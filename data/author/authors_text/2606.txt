Understanding "Each Other" 
Cla i re  Gardent  
Computat iona l  L ingu is t i cs  
Un ivers i ty  of  the  Saar land  
Saarbr f i cken ,  Germany 
claire@coli, uni-sb, de 
Kars ten  Konrad  
Computer  Sc ience  
Un ivers i ty  of  the  Saar land  
Saarbr i i cken ,  Germany 
konrad?ags, un i - sb ,  de 
Abst ract  
Although natural anguage is ambiguous, vari- 
ous linguistic and extra-linguistic factors often 
help determine a preferred reading. In this pa- 
per, we show that model generation can be used 
to model this process in the case of reciprocal 
statements. The proposed analysis builds on in- 
sights from Dalrymple t al. 98 and is shown to 
provide an integrated, computational ccount 
of the interplay between model theoretic inter- 
pretation, knowledge-based reasoning and pref- 
erences that characterises the interpretation of
reciprocals. 
1 In t roduct ion  
Although there is widespread agreement that 
inference is an essential component of natural 
language processing, little work has been done 
so far on whether existing automated reason- 
ing systems uch as theorem provers and model 
builders could be fruitfully put to work in the 
area of natural anguage interpretation. 
In this paper, we focus on the inference prob- 
lems raised by the reciprocal expression each 
other and show that model generation provides 
an adequate tool for modeling them. 
The paper is structured as follows. Section 3 
discusses the meaning of reciprocal statements 
and proposes a formal semantics for each other. 
Section 2 shows how model generation can be 
used to provide this semantics with a compu- 
tational interpretation. Section 4 compares our 
approach with the account of reciprocals which 
inspired it in the first place namely, (Dalrymple 
et al, 1998). Section 5 concludes with pointers 
for further esearch. 
2 The  mean ing  of  rec ip roca l  
s ta tements  
In the linguistic literature, the reciprocal ex- 
pression each other is often taken to denote a 
dyadic quantifier over a first-order set, which we 
will call the antecedent  set, and a binary first- 
order relation, which we will call the scope re- 
lation. In what follows, we assume an approach 
of this type and will use the symbol RcP for 
such reciprocal quantifiers o that the seman- 
tic representation f e.g. Jon and Bill saw each 
other will be: 
(1) RcP({jon, saw(x, y)) 
When antecedent sets of just two members 
are considered, each set member is required to 
stand in the scope relation to each other mem- 
ber. For larger sets however, research on recip- 
rocal statements has uncovered a variety of log- 
ical contributions that the reciprocal can pro- 
vide. Here are some examples. 
(2) The students like each other. 
Vx (std(x) -+ Vy ( x  y A std(y) -+ 
like(x,y)) 
(3) The students tare at each other in sur- 
prise. 
Vx (std(x) -+ ~y (x ? y A std(y) A 
stare_at(x, y ) ) 
(4) The students gave each other measles. 
Vx (std(x) -+ 3y (x ? y A std(y) A 
(gave_measles(x, y) V gave_measle(y, x)))) 
We can accept (2) to be true only if for each 
pair x and y of two students it holds that x 
likes y. But an analogous interpretation would 
be invalid in the case of (3) and (4) where not 
all pairs in the antecedent set the students can 
consistently stand in the scope relation (one can 
only stare at most at one person at a time, and 
319 
one can only get measles from at most one per- 
son). More generally, (Langendoen, 1978; Dal- 
rymple et al, 1998) convincingly argues that 
different reciprocal statements can have very 
different ruth conditions. The challenge to be 
addressed is thus the following: How can we 
determine a (computational) semantics for the 
reciprocal expressions each other that accounts 
for these multiplicity of meanings while predict- 
ing the specific meaning of a particular ecipro- 
cal statement? 
Clearly knowledge based reasoning plays an 
important role: only those readings are possible 
that are consistent with our knowledge about 
the situation and the world. Specifically, knowl- 
edge based reasoning constrains the strength of 
the truth conditions of a reciprocal statement. 
Thus if we abstract away from the specific scope 
relations, the truth conditions of examples uch 
as (2),(3) and (4) are ordered through entail- 
ment as follows (with A the antecedent set and 
R the scope relation): 
Vx (A(x) --~ Vy (A(y) --). R(xy)) 
Vx (A(x) ~ 9y (A(y) A e(xy)) 
Vx (A(x) --~ 3y (A(y) A (e(xy) V (e(yx))) 
Specifically, example (2), which does not in- 
volve any strong knowledge based constraint, 
has the strongest truth-conditions of the three 
examples. By contrast in (3), the knowledge 
that one can stare at most at one person, forces 
a V3 reading while in (4), a weaker meaning still 
is imposed by knowledge based constraints: the 
x gave y measles relation is asymmetric hence 
the k~/reading is ruled out; moreover, since one 
cannot be infected twice, some students in the 
group will be infected but not pass on the dis- 
ease to anyone. Hence the strongest truth con- 
ditions that can be assigned the sentence are the 
VS disjunctive reading indicated in (4). 
But are there any other constraints on the 
interpretation process than these knowledge 
based constraints? And which meaning shall 
we assign a reciprocal expression? The compu- 
tational semantics we will propose is inspired 
from (Dalrymple t al., 1998) and relies on the 
following observations. 
First, we note that (Dalrymple t al., 1998) 
identifies a lower bound for the truth conditions 
of reciprocal sentences which they dub Inclusive 
Alternative Ordering (IAO). It is exemplified by 
sentence (4) above and corresponds to the fol- 
lowing definition of RcP. 
(5) RCPIAO ~-- APAR (\[P\[ > 2 A Vx (P(x) =v 
3y p(y) ^  x # y ^  (R(x, v) v R(V, x)))) 
This definition only adequately characterises x- 
amples such as (4). It does not cover the 
stronger meanings of the reciprocal in sentences 
such as (2) and (3). However, each known form 
of reciprocity entails RCPIAO'S truth conditions, 
and RCPIAO therefore provides us with a mini- 
mal semantics for reciprocals. 
Further, we observe that given a particular 
reciprocal statement, here seems to be a pref- 
erence for consistent interpretations where the 
number of pairs that are in the scope relation 
is as large as possible. For instance in (3), not 
every student can stare at every other student 
(one can stare at at most one person), but intu- 
itively, the sentence requires that every student 
stares at some other student. While such an 
interpretation is much weaker than that of (2), 
this maximisation of the scope relation yields a 
reading that is also much stronger than the min- 
imal IAO interpretation of (4). More generally, 
while IAO provides us with a lower bound for 
the interpretation of reciprocal statements, we 
will see in section 3 that the maximisation ofthe 
scope relation that is consistent with contextual 
knowledge yields the upper bound for the inter- 
pretation of a particular eciprocal statement 
i.e., its meaning. 
Based on these observations, the principle de- 
termining the actual logical contribution of a 
reciprocal statement can be stated as follows: 
Maximise Meaning Hypothesis 
(MMH) :  The valid interpretations of 
a reciprocal sentence S in a context F 
(where I" includes knowledge about the 
previous discourse, the discourse situ- 
ation and the world) are those which 
(a) are consistent both with the IAO 
form of reciprocity and the informa- 
tion provided by F, and (b) whose con- 
tributions to the scope relation are the 
strongest. 
The MMH selects from the set of interpreta- 
tions that are consistent with IAO and contex- 
tual knowledge, those that maximise the scope 
relation. Crucially, this view of reciprocals leads 
320 
to an inference method that can actually com- 
pute the preferred interpretations of reciprocal 
sentences. We now turn to this. 
3 In terpretat ion  as Mode l  
Generat ion  
In Discourse Representation Theory (DRT, 
(Kamp, 1981; Kamp and Reyle, 1993)), a sen- 
tence with semantic representation (I) is true 
with respect o a model M iff there is an embed- 
ding of (I) onto M. Intuitively, this requirement 
says that a sub-model M'  of M must be found 
which satisfies (I). So for instance, sentence (6a) 
is true in M iff there are two individuals bugs 
and bunny in M such that bugs and bunny stand 
in the love relation; or in other words, iff the 
partial model sketched in (6b) is part of M. 
(6) a. Bugs likes Bunny. 
b. {love(bugs, bunny)} 
As shown in (Gardent and Konrad, To ap- 
pear), model generators (i.e., programs that 
compute some of the models satisfying a finite 
set of logical formulas) can be used to provide 
DRT, and more generally model-theoretic ap- 
proaches to natural anguage semantics, with a 
procedural interpretation: Given the semantic 
representation of a discourse and the relevant 
world knowledge (I) (i.e., a finite set of logical 
formulas), a model generator proves that (I) is 
satisfiable by generating some of its models. 
Intuitively, satisfying models explain how dis- 
courses can be made true. They give an 
abstract representation of how (part of) the 
world should be for a discourse to be true. 
Concretely, satisfying models can be seen as 
capturing the meaning of discourses: data-  
bases that can be queried e.g. as part of 
a query/answer system or to interpret subse- 
quent discourse. Satisfying models are also 
remininiscent of Johnson-Laird's mental mod- 
els (Johnson-Laird and Byrne, 1991) and in 
essence, mental models are very much like the 
Herbrand models we are making use of here. 
Formally, a mode l  is a mathematical struc- 
ture that describes how the symbols of a logi- 
cal theory are interpreted. Given a first-order 
language ?, a model is a pair (I, D) with D a 
non-empty set of entities (the domain  o f  indi-  
v iduals)  and I an interpretation function which 
maps relation symbols in ? to relations of ap- 
propriate arity in D and constant symbols in ?: 
to elements of D. Here we identify these mod- 
els with sets of positive assumptions that unam- 
biguously define the interpretation of the rela- 
tion symbols and fix the interpretation of terms 
to first-order entities that carry a unique name. 
These are known in the literature as Herbrand 
models. 
The set (7c) is such a model for the logical 
form (7b) which is a semantic representation f 
the sentence (7a). 
(7) a. Jon likes his cousin. 
b. 3x cousin_of(x, jon) A like(ion, x) 
c. ~'il = {cousin_of(cl,jon), 
like (jon, cl) } 
The model A41 defines an interpretation of 
the predicates cousin and like over the universe 
of discourse 7) = {jon, cl }. It can also be taken 
as a valid interpretation of (7a).There are, how- 
ever, infinitely many models for (7b) that do 
not correspond to such interpretations e.g. 
(8) .M2 = {cousin_of(jon, jon), like(jon, j on)}  
(9) Ad3 = {cousin_of(c1, jon), fike(jon, C1) , 
like( cl , jon ) } 
The model ..A42 explains the truth of (Ta) by 
declaring Jon as his own cousin. This is a re- 
sult of the inappropriate semantic representa- 
tion (7b) which fails to specify that the relation 
expressed by the noun cousin is irreflexive. In 
the case of A43, the model contains uperfluous 
information. While it is consistent o assume 
like(cl,jon) it is not necessary for explaining 
the truth of the input. 
3.1 M in imal i ty  
For applications to natural-language, weare in- 
terested in exactly those models that capture 
the meaning of a discourse, or at least capture 
the preferred interpretations that a hearer asso- 
ciates with it. As discussed in (Gardent and 
Webber, January 2000), obtaining only these 
models requires eliminating both models that 
are "too small" (e.g. A42) and models that are 
"too big" (e.g. J~43). 
Models such as A42 can be eliminated simply 
by using more appropriate truth conditions for 
NL expressions (e.g. 3x cousin(x) A of(x, jon) A 
x ~ jon A like(jon, x) for (7a)). In general how- 
ever, eliminating models that are "too small" 
is a non-trivial task which involves the interac- 
tion of model-theoretic interpretation not only 
321 
with world knowledge reasoning but also with 
syntax, prosody and pragmatics. The issue is 
discussed at some length (though not solved) in 
(Gardent and Webber, January 2000). 
To eliminate models that are "too big", some 
notion of minimality must be resorted to. For 
instance, (Gardent and Konrad, 1999; Gardent 
and Konrad, To appear) argues that local min- 
imality is an adequate form of minimality for 
interpreting definite descriptions. Local mini- 
mality is defined as follows. 
Local  Min imal i ty :  Let ~ be a set of first- 
order formulas and D be the set of Herbrand 
models of ? that use some finite domain D 
whose size is minimal. Then a model ( I ,D) E 
D is locally min imal  iff there is no other 
model (I t, D ~) E D such that I I C I. 
Locally minimal models are models that sat- 
isfy some input ? within a minimal domain :D of 
individuals and are subset-minimal with respect 
to all other domain minimal models. These 
models are the simplest in the sense of Occam's 
Razor and often the best explanation for the 
truth of an observation. In particular, if we as- 
sume that A42 is ruled out by a more appro- 
priate semantics for the word cousin, local min- 
imality rules out -1~3 as non locally minimal and 
therefore A41 is correctly identified as giving the 
preferred interpretation for example (7). 
3.2 The  MMH as a M in ima l i ty  
Constra int  
In the case of reciprocals, local minimality 
is clearly not a characterisation of preferred 
interpretations. Our semantic representation 
RCPIA 0 will only capture a reciprocal's mean- 
ing if the reciprocal group has exactly two mem- 
bers or if the input meets IAO, the weakest form 
of reciprocity. For instance, the locally minimal 
model (10c) of formula (10b) is too weak to con- 
stitute an acceptable interpretation f (10a). In- 
stead, the model capturing the meaning of (10a) 
is the model given in (10d). 
(10) a. Jon, Bill and Dan like each other. 
b. RCPIAO({jon, bill, dan})()~y)~x like(x, y)) 
c. {like(yon, bill), like(bill, dan)} 
d. {like(ion, bill), like(jon, dan), like(bill, dan), 
like(bill, jon), like (dan, bill), like(dan, ion)} 
Since the MMH ma.ximises rather than min- 
imises the logical contribution of formulas, it 
seems at first sight incompatible with local min- 
imality. However, a simple method to combine 
the MMH and model minimality is to consider 
the maximisation of reciprocal relations as a 
minimisation of their complement sets. After 
all, the difference in acceptability between (10c) 
and (10d) as models for (10a) is due to exactly 
those pairs (x, y) (with x ~ y) that are not in 
the like relation. To capture this intuition, we 
introduce a special predicate $R that indicates 
assumptions whose truth is considered "costly". 
In our case, these assumptions correspond to the 
pairs of individuals that are not in the scope re- 
lation. The semantic representation f recipro- 
cal each other is then as follows. 
(11) RcP =__ )~P)~R (RCPIAo(P)(R) A 
VxVy (e(x) A P(y) A x ? y A -~R(x, y) ~=~ 
$R(x, y))) 
The first conjunct says that a reciprocal sen- 
tence has as weakest possible meaning an IAO 
reading. Since IAO is entailed by other identi- 
fied meaning for reciprocal statements, this is 
compatible with the fact that reciprocal sen- 
tences can have other, stronger meanings. The 
second conjunct says that each pair (x, y) (with 
x ? y) that is not  in the like relation is in 
the $R relation. This encoding leads to mod- 
els like (12b) and (12c) for (12a). We say that 
model (125) has a $R-cost of 4 ($R4), while 
model (12c) has a cost of 0. 
(12) a. RcP({jon, bill, dan})(XyXx like(x, y)) 
b. {like(ion, bill), like(ion, dan), $R(bill, dan), 
$R(bill, jon), SR(dan, bill), SR(dan, ion)} 
$R4 
c. {like(ion, bill), like(ion, dan), like(bill, dan), 
like(bill, ion), like(dan, bill), like(dan, ion)} 
$RO 
We now introduce a new form of minimality 
whose definition is as follows. 
Conservat ive  Min imal i ty :  Let ~ be a set 
of first-order formulas and D be the set of Her- 
brand models of ~2 with a minimal domain T). 
Then D has a subset C of models that carry 
a minimal cost. A model ( I ,D) E C is con- 
servative minimal iff there is no other model 
(I', D') E C such that I' C. I. 
Conservative minimality is a conservative ex- 
tension of local minimality: if there are no 
costs at all, then all local minimal models are 
322 
also conservative models. Conservative mini- 
reality is a combination of local minimality and 
cost minimisation that correctly identifies the 
preferred interpretation of reciprocal sentences. 
For instance since (12c) carries a minimal cost, 
it is a conservative minimal model for (12a) 
whereas (12b) isn't. Intuitively the approach 
works as follows: the more pairs there are that 
do not stand in the scope relation of the re- 
ciprocal, the bigger the SR predicate and the 
more costly (i.e. the least preferred) the model. 
That is, the combined use of a $R-predicate and 
of conservative minimality allows us to enforce 
a preference for interpretations (i.e. models) 
maximising R. 
3.3 The  System 
KIMBA (Konrad and Wolfram, 1999) is a finite 
model generator for first-order and higher-order 
logical theories that is based on a translation 
of logics into constraint problems over finite- 
domain integer variables. KIMBA uses an effi- 
cient constraint solver to produce solutions that 
can be translated back into Herbrand models of 
the input. 
We have tailored KIMBA such that it enumer- 
ates the conservative models of its input. In- 
tuitively, this works as follows. First, KIMBA 
searches for some arbitrary model of the input 
that mentions a minimum number of individu- 
als. Then, it takes the SR-cost of this model 
as an upper bound for the cost of all successor 
models and further minimises the cost as fax 
as possible by branch-and-bound search. After 
KIMBA has determined the lowest cost possi- 
ble, it restarts the model search and eliminates 
those models from the search space that have 
a non-minimal cost. For each model .h/\[ that 
it identifies as a cost-minimal one, it proves by 
refutation that there is no other cost-minimal 
model A/l t that uses only a subset of the pos- 
itive assumptions in A/\[. Each succesful proof 
yields a conservative minimal model. 
All the examples discussed in this paper have 
been tested on Kimba and can be tried out at: 
http://www.coli.uni-sb.de/cl/ 
projects /lisa/kimba.html 
3.4 A spect rum of possib le mean ings  
Let us see in more detail what the predictions of 
our analysis are. As we saw in section 2, recip- 
rocal statements can have very different ruth 
conditions. Intuitively, these truth-conditions 
lie on a spectrum from the weakest IAO inter- 
pretation (A is the antecedent set and R the 
scope relation): 
IAl >_ 2 A Vx E A(x) 3y (A(y) A x ? y 
^(R(x, y) v R(y, 
to the strongest so-called Strong Reciprocity 
(SR) interpretation namely: 
IAI > 2AVx g(x)Vy A(y)(x ~ y ==v R(x,y)) 
We now see how the MMH allows us to cap- 
ture this spectrum. 
Let us start with example (2) whose truth- 
conditions are the strongest Strong Reciprocity 
conditions: every distinct x and y in the an- 
tecedent set are related to each other by the 
scope relation. In this case, there is no con- 
straining world knowledge hence the content of 
the like relation can be fully maximised. For 
instance if there are five students, the cheapest 
model is one in which the cardinality of like is 
twenty (and consequently the cardinatity of $R 
is zero). 
(13) {like(sl, s2),/ike(sl, s3), like(sl, s4), 
/ike(sl, sh),/ike(s2, sl),/ike(s2, s3), 
like(s2, s4), like(s2, sh), like(s3, sl),  
like(s3, s2 ) , like(s3, s4), like(s3, sh), 
I;ke(s4, sl), like(s4, s3), like(s4, s2), 
like(s4, sh), like(sh, sl),  like(sh, s3), 
like( sh, s2), like( sh, s4) 
} $RO 
By contrast, example (3) has a much weaker 
meaning. In this case there is a strong world 
knowledge constraint at work, namely that one 
can stare at only one other person at some 
time. The cheapest models compatible with this 
knowledge are models in which every student 
stare at exactly one other student. Thus in a 
universe with five students, the preferred inter- 
pretations are models in which the cardinality 
of the scope relation x stares at y in surprise 
is five. The following are example models. For 
simplicity we ommit the $R propositions and 
give the cost of the model instead (i.e. the car- 
dinality of the complement set of the scope re- 
lation). 
(14) {stare_at(sl, s2), stare_at(s2, s3), 
stare_at(s3, s4), stare_at(s4, sh), 
stare_at( sh, s3)} $R15 
323 
(15) (stare_at(sl, s2), stare_at(s2, s3), 
stare_at(s3, s4), stare_at(s4, s5), 
stare_at(s5, sl)} $R15 
Sentence (4) illustrates an intermediate case 
with respect to strength of truth conditions. 
World knowledge implies that the scope rela- 
tion x give y measles is assymetric and further 
that every individual is given measles by at most 
one other individual. Given a set of five stu- 
dents, model (16) and (17) are both acceptable 
interpretations of (4), (16) being the preferred 
interpretation. 
(16) {gave_measles(sl, s2), gave_meas\]es(sl, s3), 
gave_measles(s2, s4), gave_measles(s3, s5)} 
$R16 
(17) (gave_measles(sl, s2), gave_measles(s2, 4), 
gave_measles(s3, s5)~} $R17 
In short, these examples how the MMH at 
work. They show how given a single seman- 
tic representation for reciprocals, a variety of 
meanings can be derived as required by each 
specific reciprocal statement. Two elements are 
crucial to the account: the use of model build- 
ing, and that of minimality as an implemen- 
tation of preferences. Model building allows 
us to compute all the finite interpretations of
a sentence that are consistent with contextual 
knowledge and with an IAO interpretation of
the reciprocal expression. Preferences on the 
other hand (i.e. the use of the cost predicate 
$R and the search for conservative mininal mod- 
els), permits choosing among these interpreta- 
tions the most likely one(s) namely, the inter- 
pretation(s) maximising the scope relation. 
4 Re la ted  Work  
(Dalrymple t al., 1998) (henceforth DKKMP) 
proposes the following taxonomy of mean- 
ings for reciprocal statements (A stands for 
the antecedent set and R for the scope relation): 
Strong Reciprocity (SR) 
Vx, y E A(x ? y ~ xRy). 
Intermediate reciprocity (IR) 
Vx, y E A 3zl , . . .3Zm E A(x 
xRzl  A . . . A ZmRy) 
~= y --+ 
One-way Weak Reciprocity (OWR) 
Vx E A 3y e A (xRy) 
Intermediate Alternative Reciprocity (IAR) 
Vx, y E A3zl, . . . 3Zm E A(x ~ y -+ 
(xRzl Y zlRx) A ... A (zmRy Y yRzm)) 
Inclusive Alternative Ordering (IAO) 
Vx E A Sy E A(xRy Y yRx) 
To predict the meaning of a specific recip- 
rocal sentence, DKKMP then postulate the 
Strongest Meaning Hypothesis which says that 
the meaning of a reciprocal sentence is the log- 
ically strongest meaning consistent with world 
and contextual knowledge. 
The main difference between the DKKMP ap- 
proach and the present approach lies in how 
the best reading is determined: it is the logi- 
cally strongest of the five postulated meanings 
in DKKMP, whereas in our approach, it is that 
reading which maximises the scope relation of 
the reciprocal. This difference has both empiri- 
cal and computational consequences. 
Empirically, the predictions are the same in 
most cases because maximising the scope rela- 
tion often results in yielding a logically stronger 
meaning. In particular, as is illustrated by the 
examples in section 2, the present approach cap- 
tures the five meanings postulated by DKKMP. 
Thus model (13) exemplifies an SR reading, 
model (15) an IR reading and model (14) an 
OWR reading. Further, model (16) is an IAR 
interpretation while model (17) shows an IAO 
reading. 
But as the examples also show there are 
cases where the predictions differ. In particu- 
lar, in the DKKMP approach, sentence (3) is 
assigned the IR reading represented by model 
(15). However as they themselves observe, the 
sentence also has a natural OWR interpretation 
namely, one as depicted in model (14), in which 
some pairs of students reciprocally stare at each 
other. This is predicted by the present approach 
which says that models (14) and (15) are equally 
plausible since they both maximise the stare at 
relation to cardinality five. 
On the other hand, the DKKMP account is 
more appropriate for examples uch as: 
(18) The students at next to each other 
a. forming a nice cercle. 
324 
b. filling the bench. 
c. some to the front and others to the back 
of the church. 
An IR interpretation is predicted for (18) 
which is compatible with both continuation 
(18a) and continuation (18b). By contrast, the 
model generation approach predicts that the 
preferred interpretation is a model in which the 
students form a circle, an interpretation com- 
patible with continuation (18a) but not with 
continuations (18b-c). 
However, both approaches fail to predict he 
reading made explicit by continuation (18c) 
since this corresponds to the weaker OWR in- 
terpretation under the DKKMP account and to 
a model which fails to maximise the scope re- 
lation under the present approach. More gen- 
erally, both approaches fail to capture the se- 
mantic vagueness of reciprocal statements illus- 
trated by the following examples1: 
(19) a. The students often help each other with 
their homework. 
b. In the closing minutes of the game, the 
members of the losing team tried to encour- 
age each other. 
In both cases, the sentences can be true with- 
out maximising either the strength of its truth 
conditions (Strong Reciprocity) or the scope re- 
lation. This suggests that an empirically more 
correct analysis of reciprocals hould involve 
prototypical and probabilistic knowledge - as 
it is essentially a computational pproximation 
of the DKKMP approach, the present account 
does not integrate such information though it is 
compatible with it: just as we restrict the set 
of generated models to the set of conservative 
minimal models, we could restrict it to the set 
of models having some minimal probability. 
Computationally, the difference between the 
DKKMP and the present approach is as fol- 
lows. In essence, the DKKMP approach re- 
quires that each of the five possible readings 
(together with the relevant world knoweldge) 
be checked for consistency: some will be con- 
sistent, others will not. Since the first order 
consistency and validity problems are not de- 
cidable, we know that there can be no method 
1I am thankfu l  to an anonymous NAACL referree for 
these examples. 
guaranteed to always return a result. In order 
to implement the DKKMP approach, one must 
therefore resort to the technique advocated in 
(Blackburn et al, 1999) and use both a theo- 
rem prover and a model builder: for each possi- 
ble meaning Mi, the theorem is asked to prove 
~Mi and the model builder to satisfy Mi. Mi is 
inconsistent if the theorem prover succeeds, and 
consistent if the model builder does. Theoreti- 
cally however, cases may remain where neither 
theorem proving nor model building will return 
an answer. If these cases occur in practice, the 
approach simply is not an option. Further, the 
approach is linguistically unappealing as it in 
essence requires the reciprocal each other to be 
five-way ambiguous. 
By contrast, the model generation approach 
assigns a single semantic representation to each 
other. The approach strengthens the logical 
contribution of the weak semantic representa- 
tion as a process based on computational con- 
straints on a set of effectively enumerable mod- 
els. As a result, we will never encounter un- 
decidable logical problems as long as the repre- 
sented iscourse is consistent. The model gener- 
ator is the only computational tool that we need 
for determining preferable readings, and our ex- 
periment shows that for the examples discussed 
in this paper, it returns preferred readings in 
a few seconds on standard PCs as long as the 
background theory and the size of the domain 
remain managably small. 
5 Conc lus ion  
We have argued that model building can be used 
to provide a computational pproximation of 
DKKMP's analysis of reciprocals. 
One crucial feature of the account is that 
it permits building, comparing and ranking of 
natural-language interpretations against each 
other. In the case of reciprocals, the ranking is 
given by the size of the scope relation, but other 
ranking criteria have already been identified in 
the literature as well. For instance, (Gardent 
and Konrad, To appear) shows that in the case 
of definite descriptions, the ranking defined by 
local minimality permits capturing the prefer- 
ence of binding over bridging, over accomoda- 
tion. Similarly (Baumgartner and Kiihn, 1999) 
shows that a predicate minimisation together 
with a preference for logically consequent reso- 
325 
lutions can be used to model the interpretation 
of pronominal anaphora. 
This suggests that one of the most promising 
application of model generators i  as a device for 
developing and testing preference systems for 
the interpretation of natural language. Infer- 
ence and knowledge based reasoning are needed 
in NLP not only to check for consistency and 
informativity (as illustrated in e.g. (Blackburn 
et al, 1999)), but also to express preferences 
between, or constraints on, possible interpreta- 
tions. For this, finite model builders are natural 
tools. 
Another area that deserves further investi- 
gation concerns the use of minimality for dis- 
ambiguation. In this paper, conservative min- 
imality is used to choose among the possible 
interpretations of a particular eciprocal state- 
ment. On the other hand, (Gardent and Web- 
ber, January 2000) shows that minimality is 
also an important tool for disambiguating oun- 
compounds, logical metonymy and definite de- 
scriptions. As the paper shows though, many 
questions remains open about this use of mini- 
mality for disambiguation which are well worth 
investigating. 
In further work, we intend to look at other 
ambiguous natural anguage constructs and to 
identify and model the ranking criteria deter- 
mining their preferred interpretation. Plurals 
are a first obvious choice. But more generally, 
we hope that looking at a wider range of data 
will unveil a broader picture of what the gen- 
eral biases are which help determine a preferred 
reading - -  either in isolation, as here, or in con- 
text, as in (Gardent and Webber, January 2000) 
- -  and of how these biases can be modelled us- 
ing automated reasoning systems. 
Acknowledgement  s 
We are grateful to audiences from ITRI- 
Brighton, the Edinburgh School of Cognitive 
Science, the Paris VI TALANA seminar and the 
Amsterdam DIP colloquium for helpful com- 
ments and discussion on the material presented 
here as well as to the three NAACL anony- 
mous referrees for constructive feedback. This 
work was partially supported by the Project 
C2 (LISA) in SFB-378, grant by the Deutsche 
Forschungsgemeinschaft to the University of 
Saarbriicken. 
References  
Peter Baumgartner and Michael Kiihn. 1999. 
Abductive coreference by model construction. 
In ICoS-1 Inference in Computational Se- 
mantics, Institute for Logic, Language and 
Computation, University of Amsterdam, Au- 
gust. 
P. Blackburn, J. Bos, M. Kohlhase, and 
H. de Neville. 1999. Inference and Com- 
putational Semantics. In Third Interna- 
tional Workshop on Computational Seman- 
tics (IWCS-3), Tilburg, The Netherlands. 
Mary Dalrymple, Makoto Kanasawa, Yookyung 
Kim, Sam Mchombo, and Stanley Peters. 
1998. Reciprocal expressions and the con- 
cept of reciprocity. Linguistics and Philoso- 
phy, 21(2):159-210, April. 
Claire Gardent and Karsten Konrad. 1999. 
Definites and the proper treatment of rabbits. 
In Proceedings of ICOS. Also CLAUS Report 
111, http://www.coli.uni-sb.de/claus/. 
Claire Gardent and Karsten Konrad. To ap- 
pear. Interpreting Definites using Model 
Generation. Journal of Language and Com- 
putation. 
Claire Gardent and Bonnie Webber. Jan- 
uary 2000. Automated deduction and 
discourse disambiguation. Submitted for 
Publication. Also CLAUS Report 113, 
http://www.coli, uni-sb.de/claus/. 
P.N. Johnson-Laird and Ruth M.J. Byrne. 
1991. Deduction. Lawrence Erlbaum Asso- 
ciates Publishers. 
Hans Kamp and Uwe Reyle. 1993. From Dis- 
course to Logic. Kluwer, Dordrecht. 
Hans Kamp. 1981. A theory of truth and 
semantic representation. In J. Groenendijk, 
Th. Janssen, and M. Stokhof, editors, Formal 
Methods in the Study of Language, pages 277 
- 322. Mathematisch Centrum Tracts, Ams- 
terdam. 
Karsten Konrad and D. A. Wolfram. 1999. 
Kimba, a model generator for many-valued 
first-order logics. In Proc., 16th Interna- 
tional Conference on Automated Deduction, 
CADE 99, LNCS, forthcoming, Trento, Italy. 
Springer. 
D. Terence Langendoen. 1978. The logic of reci- 
procity. Linguistic Inquiry, 9(2):177-197. 
326 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 249?256
Manchester, August 2008
Integrating a unification-based semantics in a large scale Lexicalised Tree
Adjoining Grammar for French
Claire Gardent
CNRS / LORIA
Equipe Talaris, Bat. B
615, rue du jardin botanique
54600 Villers les Nancy
France
claire.gardent@loria.fr
Abstract
In contrast to LFG and HPSG, there is to
date no large scale Tree Adjoining Gram-
mar (TAG) equiped with a compositional
semantics. In this paper, we report on
the integration of a unification-based se-
mantics into a Feature-Based Lexicalised
TAG for French consisting of around 6 000
trees. We focus on verb semantics and
show how factorisation can be used to sup-
port a compact and principled encoding of
the semantic information that needs to be
associated with each of the verbal elemen-
tary trees. The factorisation is made possi-
ble by the use of XMG, a high-level linguis-
tic formalism designed to specify and com-
pile computational grammars and in partic-
ular, grammars based on non-local trees or
tree descriptions.
1 Introduction
Whilst there exists large scale LFGs (Lexical
Functional Grammar) and HPSGs (Head-Driven
Phrase Structure Grammar) equipped with a com-
positional semantics (Copestake et al, 2001; Frank
and van Genabith, 2001), available Tree Adjoining
Grammars remain largely syntactic.
One reason for this is that there has been, up
to recently, much debate about how best to com-
bine TAG with a compositional semantics. Should
it be based on the derived or the derivation tree
? Should Feature-Based LTAG be used or should
synchronous TAG? Many proposals have been put
forward but only recently did sufficient consensus
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
emerge to support the specification of a TAG based
compositional semantics. In a nutshell, it can be
achieved either by using a synchronous TAG (Nes-
son and Shieber, 2006) (in this case, the grammar
explicitely describes and synchronises syntax and
semantics structures) or by using Feature-Based
LTAG (in which case, the synchronisation between
syntax and semantics is mediated by the unifica-
tion of semantic indices associated with the FTAG
elementary trees).
Another more practical reason for the absence
of large scale TAGs integrating a compositional
semantics is the lack of available computational
frameworks. Up to recently, there has been
no available grammar writing environment and
parser that would support the integration of com-
positional semantics into a TAG. One step in
that direction is provided by the development of
XMG(Duchier et al, 2004), a formalism which
supports the specification of Feature-Based LT-
AGs equipped with a compositional semantics a` la
(Gardent and Kallmeyer, 2003).
In this paper, we report on the integration of a
unification-based semantics into a Feature-Based
LTAG for French which consists of around 6 000
trees. This integration is specified using XMG
and we show how this formalism can be used to
support a compact and principled encoding of the
semantic information that needs to be associated
with each of the 6 000 elementary trees.
The article is structured as follows. We start
(section 2) by presenting XMG and showing how
it supports the specification of Feature-Based LT-
AGs equipped with a compositional semantics. We
then present SEMFRAG, the FTAG grammar for
French that we developed (section 3). In section
4, we show how XMG can be used to minimise
the development cost involved in enriching such
249
a grammar with a compositional semantics. Sec-
tion 5 compares the approach with related work
and concludes with pointers for further research.
2 The XMG formalism
The XMG formalism was designed to support the
development and factorisation of computational
grammars for Natural Language. Like PATR II it
is theory neutral in that its use is not restricted
to a single grammatical theory. Unlike PATR II
however, the language provided by XMG allows
the linguist to talk about the basic building blocks
not only of rule based computational linguistic
theories such as as HPSG (Head Driven Phrase
Structure Grammar) and LFG (Lexical Functional
Grammar) but also of tree based theories such as
TAG (Tree Adjoining Grammar). As we shall see,
this involves allowing for sophisticated node nam-
ing and identification mechanisms. Other differ-
ences between PATR II and XMG include a more
general use of disjunction, the use of colours to
control tree construction and a more natural encod-
ing of trees and of semantic representation than is
permitted by PATR II. A detailed definition of the
XMG formalism is given in (Duchier et al, 2004).
In what follows, we give an intuitive presenta-
tion of XMG emphasising the points that support a
strongly factorised specification of grammars and
in particular, of SEMFRAG. We start by presenting
the basic building blocks XMG allows the linguist
to talk about (2.1). We then go on to discuss the
factorising mechanisms it supports (2.2). Finally,
we introduce the several node naming and identifi-
cation mechanisms it provides (2.3).
2.1 The basic building blocks
In XMG, the basic building blocks are CLASSES
which may be specified along three dimensions :
a syntactic dimension (SYN) which consists of a
tree description whose node variables can be dec-
orated with feature structures; a semantic dimen-
sion (SEM) consisting of a flat semantic formula;
and a syntax/semantic interface (INTERFACE) for
synchronising semantic formulae and tree descrip-
tions .
SYN. The syntactic dimension in XMG allows the
linguist to specify tree descriptions i.e., trees that
can be underspecified with respect to both domi-
nance and precedence. The trees described may
be either local or extended and their nodes may be
decorated with either one or two feature structures
(two for TAG TOP and BOTTOM feature struc-
tures).
SEM. Using the semantic dimension, the linguist
can specify unification based flat semantic formu-
lae in the sense of (Copestake et al, 2001) i.e.,
non recursive formulae describing first order for-
mulae with lambda binders replaced by unification
variables and where scope may be underspecified.
Semantic schemas can also be specified in which
predicates are replaced by unification variables
that will be instantiated during lexical lookup. For
instance, the SEM dimension may include the fol-
lowing semantic formula and schema1:
(1) a. Every: l
0
: ?(X,h
1
, h
2
), h
1
? L
1
, h
2
?
L
2
b. Binary Relation Schema: l
1
: P (E), l
1
:
Theta
1
(E,X), l
1
: Theta
2
(E,Y )
In (1a), the flat semantic formula associated
with every underspecifies scope by stating that the
scope handle h
2
scopes, directly or indirectly (?),
over (the label L
2
associated with) the scopal ar-
gument.In (1b) on the other hand, underspecifica-
tion bears on the predicate P and the theta roles
Theta
1
, Theta
2
which are unification variables
whose value will be provided by the lexicon. In
this way, this binary relation schema can be used
to represent the semantics of all verbs denoting a
binary relation. The lexicon will then specify for
each verb the relevant relation and theta roles.
INTERFACE. The third XMG dimension permits
synchronising syntax and semantics. In essence,
features that are used in SYN or in SEM can be as-
signed global names in the INTERFACE dimension
and synchronised using variable sharing. For in-
stance, given a feature-value pair F = X occuring
in the SYN dimension and a semantic parameter Y
occuring in the SEM dimension, the following in-
terface constraint permits both unifying (?synchro-
nising?) X and Y and assigning them the global
names IDX and ARG respectively :
(2) IDX = 1 X, ARG = 1 Y
As we shall see in section 4.2.2, the interface al-
lows for a natural and highly factorised means of
stating syntax/semantics linking constraints (e.g.,
the subject constituent provides the semantic index
for the first semantic argument).
1Here and in what follows, we adopt the convention that
identifiers starting with an upper case letter are unification
variables.
250
2.2 Factorising mechanisms
An important feature of a linguistic formalism is
that it supports a high level of factorisation thus
facilitating grammar development, debugging and
maintenance. In XMG, factorising can be achieved
using disjunctions, conjunctions and inheritance of
classes. As argued in (Crabbe?, 2005), classes dis-
junction supports the description of alternatives,
for instance, to describe the alternative possible re-
alisations of a subject (see below). As usual, con-
junction and inheritance of classes permits com-
bining the content of two classes2.
2.3 Node naming and identification
mechanisms
In combining tree descriptions, the linguist often
wants to identify nodes across descriptions. One
distinguishing feature of XMG it that it supports a
sophisticated treatment of node naming and node
identification (Gardent and Parmentier, 2006).
Node naming. In XMG, node names are by de-
fault local to a class. However explicit IMPORT and
EXPORT declarations can be used to make names
?visible? to children classes. An EXPORT declara-
tion makes the exported name(s) visible to all chil-
dren classes. Conversely restrictive IMPORT dec-
larations can be used either to block or to rename
exported variables that are visible through inheri-
tance.
Node identification. As we have just seen, IM-
PORT and EXPORT declarations can be used to
make names ?visible? to children classes and
thereby idendify nodes from different classes. For
instance, if class C
1
inherits from class C
2
, C
1
refers to a node variable X and C
2
exports X, then
X denotes the same node in both C
1
and C
2
.
However, this mechanism only works within a
single branch of the inheritance hierarchy. Indeed
in case of multiple inheritance (two classes C
1
and
C
2
export the same variable X to a third class in-
heriting from both C
1
and C
2
), identification will
fail (X will not be interpreted as denoting the same
node in both C
1
and C
2
). To remedy this short-
coming, XMG allows for explicit node identifica-
tions. Thus in the above case, X can be identified
using the constraint C
1
.X = C
2
.X.
2The distinction between conjunction and inheritance has
to do with some intricate issues concerning node identifica-
tions which we will not address here. See (Gardent and Par-
mentier, 2006) for a detailed discussion on this.
This concludes our informal presentation of
XMG. For a more precise definition of its syn-
tax, semantic and compilation process, we refer the
reader to (Duchier et al, 2004).
3 SemFraG
To illustrate the expressive power of XMG, we now
show how it can be used to specify SEMFRAG,
a TAG for French which integrates a unification
based compositional semantics. We start by pre-
senting the grammar produced by the XMG speci-
fication.
SEMFRAG is a unification based version of
LTAG namely, Feature-based TAG. A Feature-
based TAG (FTAG, (Vijay-Shanker and Joshi,
1988)) consists of a set of (auxiliary or initial) el-
ementary trees and of two tree composition opera-
tions: substitution and adjunction. Initial trees are
trees whose leaves are labelled with substitution
nodes (marked with a downarrow) or terminal cat-
egories. Auxiliary trees are distinguished by a foot
node (marked with a star) whose category must be
the same as that of the root node. Substitution in-
serts a tree onto a substitution node of some other
tree while adjunction inserts an auxiliary tree into
a tree. In an FTAG, the tree nodes are furthermore
decorated with two feature structures (called TOP
and BOTTOM) which are unified during derivation
as follows. On substitution, the top of the substi-
tution node is unified with the top of the root node
of the tree being substituted in. On adjunction, the
top of the root of the auxiliary tree is unified with
the top of the node where adjunction takes place;
and the bottom features of the foot node are unified
with the bottom features of this node. At the end
of a derivation, the top and bottom of all nodes in
the derived tree are unified.
To associate semantic representations with natu-
ral language expressions, the FTAG is modified as
proposed in (Gardent and Kallmeyer, 2003). Each
NP
j
John
name(j,john)
S
NP?s VPr
V
runs
run(r,s)
VPx
often VP*
often(x)
?
name(j,john), run(r,j), often(r)
Figure 1: Flat Semantics for ?John often runs?
elementary tree is associated with a flat semantic
251
representation. For instance, in Figure 1,3 the trees
for John, runs and often are associated with the se-
mantics name(j,john), run(r,s) and often(x) respec-
tively.
The arguments of a semantic functor are repre-
sented by unification variables which occur both in
the semantic representation of this functor and on
some nodes of the associated syntactic tree. For in-
stance in Figure 1, the semantic index s occurring
in the semantic representation of runs also occurs
on the subject substitution node of the associated
elementary tree.
The value of semantic arguments is then de-
termined by the unifications taking place during
derivation. For instance, the semantic index s in
the tree for runs is unified during substitution with
the semantic indices labelling the root nodes of the
tree for John. As a result, the semantics of John
often runs is
(3) {name(j,john),run(r,j),often(r)}
SEMFRAG describes a core fragment of French
and contains around 6 000 elementary trees. It cov-
ers some 35 basic verbal subcategorisation frames
and for each of these frames, the set of argument
redistributions (active, passive, middle, neuter, re-
flexivisation, impersonal, passive impersonal) and
of argument realisations (cliticisation, extraction,
omission, permutations, etc.) possible for this
frame. Predicative (adjectival, nominal and prepo-
sitional) and light verb constructions are also cov-
ered as well as subcategorising nouns and adjec-
tives. Basic descriptions are provided for the re-
maining constructions i.e., adverbs, determiners
and prepositions.
4 Implementing SEMFRAG using XMG
We now illustrate the power of XMG by show-
ing how it can be used to produce a highly fac-
torised specification of SEMFRAG, an FTAG of 6
000 trees enriched with a unification based compo-
sitional semantics. Given the space constraints, we
concentrate on the verbal trees (trees anchored by
verbs). We start (4.1) by summarising the specifi-
cation of SEMFRAG verbal syntactic trees given in
(Crabbe?, 2005). We then (4.2) show how this spec-
ification of the syntax of verbal trees can be en-
riched with a unification based compositional se-
3Cx/C
x
abbreviate a node with category C and a
top/bottom feature structure including the feature-value pair
{ index : x}.
mantics. We show in particular that this enrich-
ment can be performed using only a limited set of
general principles.
4.1 Syntax
The syntactic dimension of SEMFRAG was speci-
fied in (Crabbe?, 2005). For the verbal trees, it can
be summarised as follows.
First, tree families are specified as disjunctions
of diatheses. For instance, the N0VN1 family4 is
defined as :
n0Vn1 ? ( dian0Vn1Active
? dian0Vn1Passive
? dian0Vn1dePassive
? dian0Vn1ShortPassive
? dian0Vn1ImpersonalPassive
? dian0Vn1middle
? dian0Vn1Reflexive )
(1)
Second, diatheses are defined as conjunctions of
classes. For instance, dian0Vn1Active is defined
as:
dian0Vn1Active ? ( Subject
? ActiveV erbForm
? Object )
(2)
Third, each grammatical function appearing in the
definition of a diathesis is defined as a disjunction
of classes, each class representing a possible real-
isation of that function. For instance, the Subject
class is:
Subject ? ( CanonicalSubject
? RelativisedSubject
? WhSubject
? CleftSubject
? CliticSubject )
(3)
Fourth, each class describing a possible gram-
matical function realisation specifies the adequate
tree description. For instance, the fragments for
CanonicalSubject, ActiveVerbForm and Canon-
icalObject are sketched in Figure 25.
In sum, the XMG specification relies on a fairly
intuitive use of classes disjunctions and conjunc-
tions. Moreover, the basic leaf classes (i.e., the
most deeply embedded disjuncts and conjuncts in
the grammar specification) are defined by inheri-
tance, the inheritance hierarchy encoding the shar-
ing of tree description fragments and/or feature
4In TAG, a tree family gathers all the elementary trees
associated with verbs of a given syntactic type. Thus, the
N0VN1 family contains all the trees describing the syntactic
contexts in which a verb taking two nominal arguments (i.e.,
a transitive verb) can occur.
5Due to space constraints, these fragments are simplified
in that features are omitted.
252
S
?
N?
?
V
?
(CanSubj)
S
?
V?
?
(Active)
S
?
V
?
N?
?
(CanObj)
Figure 2: Tree fragments
structures between leaf classes. As a result, sev-
eral thousand trees are specified using only a few
hundred classes.
4.2 Semantics
Just like grammar engineering is a complex issue,
enriching a computational grammar with a com-
positional semantics is potentially time consuming
and error prone. We now show that XMG permits
this enrichment by means of a few general seman-
tic principles thus minimising both work and the
risk of errors. To enrich a purely syntactic FTAG
with the type of unification based semantics de-
scribed in section 3, three main changes need to
be carried out.
First, trees must be labelled with appropriate se-
mantic indices and labels. For instance, the sub-
ject node of a verbal tree must be labelled with a
semantic index.
Second, trees must be associated with appropri-
ate semantic schemas. For instance, the trees of the
n0Vn1 family must be associated with a semantic
schema representing a binary relation.
Third, variable sharing between semantic
schemas and syntactic trees must be enforced. For
instance, the semantic index of the subject node
of an active verb should be identified with the
first semantic argument of the associated semantic
schema.
We now provide an XMG encoding of this infor-
mation. As for the syntax, we proceed top-down
from the verb families down to argument realisa-
tion and node labelling.
4.2.1 Associating trees with semantic
formulae.
As indicated in the previous section, trees in
TAG are grouped into tree families. We use this
feature to associate in one fell swoop all the trees
of a given family with the appropriate semantic
schema. For instance, to associate transitive verbs
with a binary relation schema, the syntactic speci-
fication given in (1) is modified to:
n0Vn1 ? binaryRel ?
( dian0Vn1Active
? dian0Vn1Passive
? dian0Vn1dePassive
? dian0Vn1ShortPassive
? dian0Vn1ImpersonalPassive
? dian0Vn1middle
? dian0Vn1Reflexive )
(4)
4.2.2 Linking constraints
Next the correct syntax/semantic interface con-
straints must be specified for each diathesis. That
is, the correct mapping between syntactic and se-
mantic arguments must be enforced. This is done
in two steps.
First, we define a set of INTERFACE constraints
of the form
index
F
= V, arg
i
= V
which are meant to enforce the identification
of the semantic index (index
F
) labelling a given
tree node with grammatical function F (e.g., F =
subject) with the index (arg
i
) representing the i-
th argument in a semantic schema. For instance,
when combined with a class C containing a vari-
able X named6 arg
1
and a variable Y named
index
subject
, the SubjArg1 linking constraint
index
subject
= V, arg
1
= V
ensures that X and Y are identified. Assuming fur-
ther that arg
1
is used to name the first semantic ar-
gument and index
subject
to name the value of the
index feature labelling a subject node7, this con-
straint ensures a subject/arg
1
mapping.
Given such interface constraints, we then refine
the diathesis definitions so as to ensure the correct
bindings. For instance, the specification in (2) is
modified to :
dian0Vn1Active ? ( SubjArg1
? ObjArg2
? Subject
? ActiveV erbForm
? Object )
(5)
whilst the passive diathesis is specified as:
dian0Vn1Passive ? ( SubjArg2
? CagentArg1
? Subject
? PassiveV erbForm
? Cagent )
(6)
6As explained in section 2, interface constraints can be
used to assign global names to values inside a class.
7We will see in the next section how to ensure the appro-
priate naming of syntactic indices and semantic arguments.
253
4.2.3 Labelling trees with semantic indices.
The above scheme relies on the assumption that
tree nodes are appropriately labelled with seman-
tic indices (e.g., the subject node must be labelled
with a semantic index) and that these indices are
appropriately named (arg
1
must denote the param-
eter representing the first argument of a binary re-
lation and index
subject
the value of the index fea-
ture on a subject node). As suggested in (Gardent,
2007), a complete semantic labelling of a TAG
with the semantic features necessary to enrich this
TAG with the unification based compositional se-
mantics sketched in section 3 can be obtained by
applying the following labelling principles8 :
Argument labelling: In trees associated with se-
mantic functors, each argument node is la-
belled with a semantic index9 named after the
grammatical function of the argument node
(e.g., index
subject
for a subject node).
Anchor projection: The anchor node projects its
label up to its maximal projection.
Foot projection: A foot node projects its label up
to the root10
Controller/Controllee: In trees associated with
control verbs, the semantic index of the con-
troller is identified with the value of the con-
trolled index occuring on the sentential argu-
ment node.
As we shall now see, XMG permits a fairly direct
encoding of these principles.
Argument labelling. In the tree associated with
a syntactic functor (e.g., a verb), each tree node
representing a syntactic argument (e.g., the sub-
ject node) should be labelled with a semantic index
named after the grammatical function of that node
(e.g., index
subject
).
To label argument nodes with an appropriately
named semantic index, we first define a set of
classes encapsulating a node with an index and a
name. We then identify this node with the appro-
priate tree nodes.
More specifically, we define for each
grammatical function Function ?
8Because of space constraints, the principles required to
handle quantification are omitted.
9For simplicity, we only talk about indices here. However,
to be complete, labels are also need to be taken into account.
10The foot projection principle only applies to foot nodes
that are not argument nodes i.e., to modifiee nodes.
{subject, object, cagent, iobject, . . . }, a se-
mantic class FunctionSem which associates
with an (exported) node called xFunction the
feature value pair index = I and an interface
constraint of the form index
Function
= I. For
instance, the class SubjectSem associates the
node xSubject with the feature value pair index =
I and the interface constraint index
subject
= I.
subjectSem ? [syn] : xSubject[index = I ]
[interface] : [index
subject
= I ]
(7)
When specifying the tree fragments describing
the possible realisations of the grammatical func-
tions, the (exported) argument node is systemati-
cally named xArg.
Finally, we modify the specification of the gram-
matical functions realisations to import the ap-
propriate semantic class and identify xArg and
xFunction nodes. For instance, 3 above is
changed to:
Subject ? SubjectSem ?
xArg = xSubject ?
( CanonicalSubject
? RelativisedSubject
? WhSubject
? CleftSubject
? CliticSubject )
(8)
As a result, all xArg nodes in the tree descrip-
tions associated with a subject realisation are la-
belled with an index I feature whose global name
is index
subject
.
Controller/Controllee. Value sharing between
the semantic index of the controller (e.g., the sub-
ject of the control verb) and that of the controllee
(e.g., the empty subject of the infinitival comple-
ment) is enforced using linking constraints be-
tween the semantic index labelling the controller
node and that labelling the sentential argument
node of the control verb. Control verb definitions
then import the appropriate (object or subject con-
trol) linking constraint.
Anchor and foot projection. The anchor (foot)
projection principle stipulate the projection of se-
mantic indices from the anchor (foot) node up to
the maximal projection (root). To enforce these
principles, we define a set of anchor projection
classes as illustrated in Figure 3. We then ?glue?
these projection skeletons onto the relevant syntac-
tic trees by importing them in their definition and
explicitely identifying the anchor node of the se-
mantic projection classes with the anchor or foot
node of these trees. Since the solutions must be
trees, the nodes dominating the anchor node of the
254
?E3
E2
?
E2
E1
?
E2
E1
?
E1
E
?
E1
E
?
E1
E
??
E
? ??
E
? ??
E
?
Depth 3 Depth 2 Depth 1
?S
E2
E1
?V P
E1
E
??V
E
?
ActiveVerbForm
Figure 3: Anchor/Foot projection
projection class will deterministically be unified
with those dominating the anchor or foot node of
the trees being combined with. For instance, for
verbs, the class specifying the verbal spine (e.g.,
ActiveVerbForm, cf. 2) will import a projection
class and equate the anchor node of the verbal
spine with that of the projection skeleton. As a re-
sult, the verb projects its index up to the root (mod-
ulo the renaming made necessary by the possibility
of an adjunction) as illustrated on the right inside
of Figure 3.
4.3 Discussion
Arguably, the XMG encoding we provided to en-
rich an FTAG with a unification based composi-
tional semantics, is compact and principled.
It is principled in that it provides a direct and
transparent formulation of the main principles un-
derlying the integration in a TAG of the unification
based semantics sketched in Section 3.
It is compact in that the number of modifica-
tions needed to enrich syntax with semantics is rel-
atively small: 76 class definitions and 498 class
calls are sufficient to associate the required se-
mantic information (semantic schemas, semantic
indices and index projections) with roughly 6000
trees. Crucially, this means that the time involved
in integrating the semantics in the grammar is
small (roughly a week linguist work) and further
that the maintenance, extension and debugging of
the semantic component is greatly facilitated.
Both these points rely on the expressivity of
XMG. More in particular, the encoding heavily re-
lies on two specific features of XMG namely, gen-
eralised classes disjunctions and the possibility to
use global names not only for tree nodes but also
for feature values and semantic parameters.
Generalised classes disjunctions are used to as-
sociate large sets of trees with semantic schema
(section 4.2.1) and to label sets of tree frag-
ments with the appropriately named index (section
4.2.3). Intuitively, generalised classes disjunction
and conjunction permit factoring out the common
operand of an enumeration (e.g., instead of enu-
merating (a ? b) ? (a ? c) ? (a ? d) ? . . ., we can
specify a ? (b ? c ? d) ). In practice, this means
that the number of statements necessary to spec-
ify the grammar can be greatly reduced. For in-
stance, the association of several thousands of verb
trees with a semantic schema is enforced by a total
of 176 statements. In contrast, standard linguistic
formalisms such as PATR IIor the LKB only allow
disjunctions over atomic feature values.
Global names in turn, were used to support a di-
rect encoding of linking constraints (section 4.2.2).
37 linking constraints definitions and 255 linking
constraints calls are sufficient to ensure the ap-
propriate mapping between syntactic and seman-
tic arguments in verb trees as well as adjectival
and nominal predicative trees. More generally, the
possibility to introduce global names not only for
tree nodes as in e.g., (Vijay-Shanker and Schabes,
1992) but also for feature values and semantic pa-
rameters allows for a simple and direct encoding
of constraints applying to identifiers occuring ?far
apart? in a given tree (for instance, between the in-
dex of the subject node in a controlverb tree and
that of a PRO index of its infinitival argument).
5 Conclusion
Whilst the development of standard unification-
based grammars is well supported by the design of
formalisms such as PATR II, the XLE and the LKB,
formalisms for developing Tree-Based Grammars
have received less attention. XMG aims to remedy
this shortcoming by providing a formalism that
supports talking about trees, tree sharing and tree
labelling.
Trees of arbitrary (finite) depth can be described
using underspecified tree descriptions. Addition-
ally, trees can be combined with further linguistic
dimensions such as semantic representations and
a syntax/semantic interface to form more complex
linguistic units.
Tree sharing is supported by the inheritance, the
conjunction and the disjunction of tree descrip-
tions together with a sophisticated identifier han-
dling mechanism : identifiers are by default local
but can be made global or semi-global, on demand.
Furthermore, identifiers can be identified either ex-
plicitely (using either the interface or explicit iden-
tification constraints) or implicitely (through in-
255
heritance or through the use of colours, a mech-
anism not discussed here).
Finally, tree labelling can be expressed by as-
sociating tree nodes with one or two (for TAG)
feature structures. Importantly, feature values can
be assigned global names thereby allowing for the
specification of constraints on features that are ?far
apart from each other? within a tree.
In this paper, we have argued that these fea-
tures of XMG are effective in supporting an en-
coding of an FTAG with a unification based com-
positional semantics which is principled, transpar-
ent and compact. These features also markedly
distinguish XMG from existing formalisms used
to encode tree based grammars such as the non-
monotonic encoding of TAG proposed in (Evans
et al, 1995) (in contrast, XMG is fully monotonic)
and the tree descriptions based approaches pro-
posed in (Candito, 1996; Xia et al, 1998) where in
particular, tree descriptions can only be conjoined
(not disjoined) and where identification across tree
fragments is restricted to nodes.
More in general, we believe that expressive for-
malisms are necessary to allow not only for the
quick development of symbolic tree based gram-
mars but also for their comparison and for the fac-
toring of several grammars be they different wrt
to the language they handle (as for instance in the
HPSG Delphin or in the LFG Pargram project)
or in the semantics they integrate e.g., a glue se-
mantics as proposed in (Frank and van Genabith,
2001), a lambda-based semantics as proposed in
(Gardent, 2007) or as shown here, a unification
based flat semantics.
References
M.H. Candito. 1996. A principle-based hierarchical
representation of LTAGs. In Proc. of COLING?96,
Kopenhagen.
A. Copestake, A. Lascarides, and D. Flickinger. 2001.
An algebra for semantic construction in constraint-
based grammars. In Proc. of ACL, Toulouse, France.
B. Crabbe?. 2005. Repre?sentation informatique de
grammaires fortement lexicalise?es. Ph.D. thesis,
Universite? Henri Poincare?, Nancy.
D. Duchier, J. Le Roux, and Y. Parmentier. 2004. The
metagrammar compiler. In 2nde confrence interna-
tionale Oz/Mozart (MOZ?2004), Charleroi.
R. Evans, G. Gazdar, and D. Weir. 1995. Encoding lex-
icalized tree adjoining grammars with a nonmono-
tonic inheritance hierarchy. In Proc. of ACL.
A. Frank and J. van Genabith. 2001. Ll-based se-
mantics for ltag - and what it teaches us about lfg
and ltag. In Proc. of the LFG?01 Conference, Hong
Kong. CSLI Online Publications.
C. Gardent and L. Kallmeyer. 2003. Semantic con-
struction in ftag. In Proc. of EACL, Budapest, Hun-
gary.
C. Gardent and Y. Parmentier. 2006. Coreference han-
dling in xmg. In Proc. of COLING (Poster), Sydney,
Australia.
C. Gardent. 2007. Tree adjoining grammar, semantic
calculi and labelling invariant. In Proc. of IWCS.
Rebecca Nesson and Stuart M. Shieber. 2006. Simpler
TAG semantics through synchronization. In Pro-
ceedings of the 11th Conference on Formal Gram-
mar, Malaga, Spain, 29?30 July.
K. Vijay-Shanker and A. K. Joshi. 1988. Feature
structures based tree adjoining grammar. In Proc.
of COLING, pages 714?719, Budapest.
K. Vijay-Shanker and Y. Schabes. 1992. Structure
sharing in lexicalised tree adjoining grammar. In
Proc. of COLING 92, pages 205?211.
F. Xia, M. Palmer, K. Vijay-Shanker, and J. Rosen-
zweig. 1998. Consistent grammar development us-
ing partial-tree descriptions for lexicalized tree ad-
joining grammar. Proc. of TAG+4.
256
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 247?254,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Coreference handling in XMG
Claire Gardent
CNRS/LORIA
615, rue du jardin botanique, B.P. 101
54602 Villers le`s Nancy CEDEX
France
Claire.Gardent@loria.fr
Yannick Parmentier
INRIA Lorraine
615, rue du jardin botanique, B.P. 101
54602 Villers le`s Nancy CEDEX
France
Yannick.Parmentier@loria.fr
Abstract
We claim that existing specification lan-
guages for tree based grammars fail to
adequately support identifier managment.
We then show that XMG (eXtensible Meta-
Grammar) provides a sophisticated treat-
ment of identifiers which is effective in
supporting a linguist-friendly grammar de-
sign.
1 Specifying tree-based grammars
Whilst the development of standard unification-
based grammars is well supported by the design of
formalisms such as PATR-II, Ale or TDL (Krieger
and Schafer, 1994), the situation is less well es-
tablished for Tree-Based Grammars such as Tree
Adjoining Grammars (Joshi and Schabes, 1997),
Tree Description Grammars (Kallmeyer, 1996) or
Interaction Grammars (Perrier, 2003).
Roughly, two main types of specification for-
malism for Tree-Based Grammars can be distin-
guished: formalisms based on tree fragments and
non monotonic inheritance and formalisms based
on tree descriptions and monotonic inheritance.
The tree fragment approach is advocated in
(Evans et al, 1995) which proposes to encode lex-
icalised TAGs using the DATR representation lan-
guage1. In this approach, tree fragments are com-
bined within a non monotonic inheritance hierar-
chy. Furthermore, new fragments can be derived
from existing ones by means of lexical rules. This
first approach suffers from the procedural char-
acter of non-monotonic inheritance. In specify-
ing the grammar, the grammar writer must keep
1A tree based approach is also used in(Becker, 2000) but
this time in combination with metarules. In that particular
approach, procedural aspects also come into play as the order
in which metarules apply affect the results.
in mind the order in which non-monotonic state-
ments have been made so as to be able to pre-
dict how explicit statements interact with defaults
and non-monotonic inheritance in determining the
final output. When developing a large coverage
grammar, this rapidly become extremely cumber-
some. Moreover, as (Candito, 1996) remarks, non-
monotonicity may result in an information loss
which makes it impossible to express the relation
existing for instance between an active object and
the corresponding passive subject.
The approach based on tree descriptions (of-
ten called, the metagrammar approach) obviates
the procedural character of the non-monotonic
approach by taking tree descriptions rather than
trees to be the basic units (Candito, 1996; Xia et
al., 1999; Vijay-Shanker and Schabes, 1992). In
essence, tree fragments are described using tree
descriptions and tree descriptions are combined
through conjunction or inheritance. The idea is
that the minimal models satisfying the resulting
descriptions are TAG elementary trees. In some
cases, lexical rules are also used to derive new
trees from existing ones.
One main drawback with this second type of
approach concerns the management of node iden-
tifiers. Either nodes are represented by name-
less variables and node identification is forced by
well-formedness constraints e.g., wff-constraints
on trees and wff-constraints given by the input
tree description (cf. e.g., (Duchier and Gardent,
1999)) or nodes are named and nodes with iden-
tical names are forced to denote the same entity.
The first option is unrealistic when developing a
large core grammar as it is easy to omit a neces-
sary constraint and thereby permit overgeneration
(the description will be satisfied by more trees than
intended). The second option greatly degrades
247
modularity as the grammar writer must remem-
ber which names were used where and with which
interpretation. As we shall see below, it also has
the undesirable effect that the same tree fragment
cannot be used twice in a given tree description.
Nevertheless, this is the option that is adopted in
most grammar formalisms and grammar compil-
ers (Candito, 1996; Xia et al, 1999; Gaiffe et al,
2002).
In this paper, we present an approach which
remedies these shortcomings by combining mono-
tonic inheritance of tree descriptions with an ex-
plicit management of identifier scope and identi-
fiers equality2 . The proposed approach thus es-
chews both the inconvenients induced by a non
monotonic framework (by using tree descriptions
rather than trees) and those resulting from a global
treatment of identifiers (by providing greater ex-
pressivity wrt identifiers).
Specifically, we show that the proposed ap-
proach supports several ways of identifying (node
or feature) values, we motivate this multiplicity
and we identify the linguistic and/or technical cri-
teria for choosing among the various possibilities.
The paper starts in section 2 by introducing the
syntax of the XMG formalism. In section 3, we
show that XMG provides four different ways of
identifying two (node or variable) identifiers. In
section 4, we motivate each of these four differ-
ent ways and indicate when each of them can and
should be used.
2 The XMG formalism
We start by briefly introducing XMG (eXtended
MetaGrammar). First, we show that it supports the
description and the combination of blocks consist-
ing of tree fragments and/or semantic representa-
tions. Then, we show that it supports a sophisti-
cated treatment of identifiers.
2.1 Defining blocks
At the syntactic level, the basic units are tree de-
scriptions which are specified using the following
tree logic:
2Recently, (Villemonte de la Clergerie, 2005) has pro-
posed a highly compact representation formalism for tree-
based grammars which also features explicit identifier man-
agement. His approach differs from ours in that it includes
neither a colouring mechanism (cf. section 3.4) nor interfaces
(cf. section 3.3).
Description ::= x ? y | x ?+ y | x ?? y |
x ? y | x ?+ y | x ?? y |
x[f :E] | x = y |
Description ? Description
(1)
where x, y represent node variables, ? immediate
dominance (x is directly above y), ?+ strict dom-
inance (x is above y), and ?? large dominance3
(x is above or equal to y). Similarly ? denotes
immediate precedence, ?+ strict precedence, and
?? large precedence. Finally x[f :E] constrains
feature f with associated expression E on node
x, and x = y indicates node identification.
The XMG formalism also supports the associa-
tion of semantic representations with elementary
trees. The semantic representation language is a
flat semantic representation language (Bos, 1995)
with the following syntax:
Description ::= `:p(E1, ..., En) |
?`:p(E1, ..., En) | Ei ? Ej
Description ? Description
(2)
where ` is a label, p is a predicate and E1, .., En
are parameters. Further, ? denotes negation and
Ei ? Ej expresses a scope constraint between Ei
and Ej (Ej is in the scope of Ei).
2.2 Combining blocks
As in other existing tree-based formalisms, in
XMG, blocks can be combined using inheritance.
However, XMG additionally supports block con-
junction and block disjunction.
Specifically, a Class associates a name with a
content:
Class ::= Name ? {Content } (3)
A Content is either a Description (i.e., a tree
description, a semantic formula or both), a class
name, a conjunction or a disjunction of class
name:
Content ::= Description | Name |
Name ? Name | Name ? Name (4)
Further, XMG allows multiple inheritance: a given
class can import or inherit one or more classes
(written Ci here):
3By large, we mean the transitive reflexive closure of
dominance.
248
Class ::= Name 6 C1 ? . . . ? Cn ? {Content } (5)
The semantic of the import instruction is to in-
clude the description of the imported class within
the current one. This makes it possible to refine a
class e.g., by adding information to a node or by
adding new nodes4 .
2.3 Managing identifiers
We now introduce the treatment of identifiers sup-
ported by XMG. We show in particular, that it in-
tegrates:
? a convenient way of managing identifier
scope based on import/export declarations
inspired from standard Object Oriented Pro-
gramming techniques (section 2.3.1);
? an alternative means of identifying feature
values based on the use of unification
? polarity- (here called colour-) based node
identification as first proposed in (Muskens
and Krahmer, 1998) and later used in
(Duchier and Thater, 1999; Perrier, 2000).
The next sections will detail the linguistic and
technical motivations behind this variety of identi-
fier handling techniques.
2.3.1 Import/Export declaration
In XMG, the default scope of an identifier is the
class in which it is declared. However, export
specifications can be used to extend the scope of
a given identifier outside its declaration class. The
export of identifier ?X ouside class A is written :5
A?X ? { . . . ?X . . . }
Export declarations interact with inheritance,
conjunction and disjunction specifications as fol-
lows (where A,B,C are classes):
Inheritance: if the class A is imported either di-
rectly or indirectly by a class B, then ?X is
visible in B. In case of multiple inheritance
4Note that disjunctive inheritance is not supported which
would allow a block to be defined as importing one or more
classes from a given set of imported classes
5Similarly, import declaration can be used to restrict the
set of accessible identifiers to a subset of it.
e.g., if B 6 C1 ? . . . ? Cn , then all identi-
fiers exported by C1 ? . . . ? Cn are visible
from B provided they have distinct names.
In other words, if two (or more) classes in
C1 ? . . . ? Cn export the same identifier ?X,
then ?X is not directly visible from B. It can
be accessed though using the dot operator.
First A is identified with a local identifier
(e.g., ?T = A), then ?T.?X can be used to
refer to the identifier ?X exported by A.
Conjunction: if classes A and B are conjoined in-
side a class C, then all the identifiers exported
by A or B are visible within C using the dot
operator.
Disjunction: if classes A and B are disjoined in-
side a class C, then all the identifiers exported
by A or B are visible within C using the dot
operator. However in this case, both A and
B have to be associated with the same local
identifier.
In sum, export/import declarations permit ex-
tending/restricting the scope of an identifier within
a branch of the inheritance hierarchy whilst the
dot operator allows explicit access to an inherited
identifier in case the inheriting class either dis-
plays multiple inheritance or is combined by con-
junction or disjunction with other classes. More
specifically, inheritance allows implicit corefer-
ence (the use of an imported name ensures coref-
erence with the object referred to when declaring
this name) and the dot operator explicit corefer-
ence (through an explicit equality statement e.g.,
?A.?X = ?B.?Y).
2.3.2 Class interface
In XMG, a class can be associated with a class
interface i.e., with a feature structure. Further-
more, when two classes are related either by in-
heritance or by combination (conjunction or dis-
junction), their interfaces are unified. Hence class
interfaces can be used to ensure the unification of
identifiers across classes.
Here is an illustrating example:
A ? { . . . ?X . . . }? = [n1 = ?X]
B ? { . . . ?Y . . . }? = [n1 = ?Y]
In A (resp. B), the local identifier ?X (resp. ?Y) is
associated with an interface feature named n1. If
249
these two classes are combined either by conjunc-
tion or by inheritance, their interfaces are unified
and as a result, the local identifiers ?X and ?Y are
unified. In the case of a disjunction, the interface
of the current class (C here) is non deterministi-
cally unified with that of A or B.
In practice, interface-based identification of val-
ues is particularly useful when two distinct fea-
tures need to be assigned the same value. In (Gar-
dent, 2006) for instance, it is used to identify the
semantic index associated with e.g., the subject
node of a verbal tree and the corresponding seman-
tic index in the semantic representation associated
with that tree.
2.3.3 Colouring nodes
Finally, XMG provides a very economical way
of identifying node variables based on the use of
colours (also called polarities in the literature).
The idea is that node variables are associated with
a specific colour and that this colouring will either
prevent or trigger node identifications based on the
following identification rules:
?B ?R ?W ?
?B ? ? ?B ?
?R ? ? ? ?
?W ?B ? ?W ?
? ? ? ? ?
and on the requirement that valid trees only
have red or black nodes. In effect, node colour-
ing enforces the following constraints : (i) a white
node must be identified with a black node, (ii) a
red node cannot be identified with any other node
and (iii) a black node may be identified with one
or more white nodes.
Contrary to other means of value identification,
colours are restricted to node identifiers. Hence
they are best used to induce node identification in
those contexts where neither inheritance nor ex-
plicit identification are appropriate (see section 4).
3 XMG at work
Recall (section 1) that one main problem when de-
veloping a factorised specification of tree based
grammars is to ensure a consistent treatment of
identifiers and in particular, of identifier unifica-
tion. That is, when combining two units of infor-
mation, the grammar writer must ensure that her
specification correctly states which objects are the
same and which are distinct.
In what follows, we show that XMG supports
four different ways of identifying objects. We il-
lustrate this by demonstrating that the following
tree can be obtained in four different ways:
s
n v
Figure 1: A tree that can be derived in four ways
In section 4, we will show that these four ways
of identifying nodes and/or features values support
both explicitness and economy thereby reducing
the risks of specification errors.
3.1 Using explicit identification
The most basic way to identify two identifiers is to
explicitly state their identity. Thus the above tree
can be produced by combining the following two
classes6 :
A?X,?Y ? { ?X [cat : s] ? ?Y [cat : n] }
B1 ? { ?U [cat : s] ? ?Z [cat : v]
? A ? ?U = A.?X ? A.?Y ? ?Z }
To improve readability, we use from now on a
graphical representation. For instance, the classes
above are represented as follows (exported identi-
fiers are underlined and boxed letters indicate class
names):


 
A s ?X




B1 s ?U
n ?Y v ?Z
? A ? ?U = A.?X
? A.?Y ? ?Z
Thus, the class A describes the left branch of the
tree in Figure 1 and the class B1 its right branch.
The root of A and B are named ?X and ?U re-
spectively. Since ?X is exported, ?X is visible in
B1. The explicit identification ?U=A.?X then en-
forces that the two roots are identified thus con-
straining the solution to be the tree given in Fig-
ure 1.
3.2 Using inheritance
Using inheritance instead of conjunction, the same
nodes identification can be obtained in a more eco-
nomical way. We reuse the same class A as before,
but we now define a class B 2 as a sub-class of A:


 
A s ?X




B2 6 A s ?X
n ?Y v ?Z
? ?Y ? ?Z
6Here and in what follows, we abbreviate the conjunction
of a class identification ?T = A and a dot notation T.?X to
A.?X. That is,
?T = A ? T.?X?abbrev A.?X
250
Since the identifiers ?X and ?Y are exported by A,
they are visible in B2. Thus, in the latter we only
have to indicate the precedence relation between
?Y and ?Z.
In sum, the main difference between explicit
identification and identification through simple ex-
ports, is that whilst inheritance of exported identi-
fiers gives direct access to these identifiers, class
combination requires the use of a prefix and dot
statement. Note nevertheless that with the latter,
identifiers conflicts are a lot less likely to appear.
3.3 Using interfaces
A third possibility is to use interfaces to force node
identifications as illustrated in figure 2.


 
A s ?X




B3 s ?U
n ?Y n ?W ? v ?V
? A
[root = ?X, [root = ?U,
nNode = ?Y] nNode = ?W]
Figure 2: Structure sharing using interfaces
Class A is the same as before except that the
identifiers ?X and ?Y are no longer exported. In-
stead they are associated with the interface fea-
tures root and nNode respectively. Similarly,
class B3 associates the identifiers (?U and ?V) with
the interface features root and nNode. As the tree
fragment of class B3 is conjoined with A, the inter-
face features of A and B3 are unified so that ?X is
identified with ?U and ?Y with ?V.
3.4 Using node colours
Finally, colours can be used as illustrated in the
Figure below:


 
A s ?




B4 s ?
n ? n ? ? v ?
? A
Now, class B4 contains three nodes: two white
ones whose categories are s and n and which must
be identified with compatible black nodes in A;
and a black node that may but need not be identi-
fied with a white one. To satisfy these constraints,
the black s node in A must be identified with the
white s node in B and similarly for the n nodes.
The result is again the tree given in Figure 1.
Note that in this case, none of the identifiers
need to be exported. Importantly, the use of
colours supports a very economical way of forcing
nodes identification. Indeed, nodes that are identi-
fied through colouration need neither be exported
nor even be named.
4 Which choice when?
As shown in the previous section, XMG allows
four ways of identifying values (i.e., nodes or fea-
ture values): through simple exports, through ex-
plicit identification, through colour constraints and
through the interface. We now identify when each
of these four possibilities is best used.
4.1 Exports
As shown in section 2.3, an identifier ?X can be
explicitly exported by a class Cwith the effect that,
within all classes that inherit from C, all occur-
rences of ?X denote the same object.
In essence, exports supports variable naming
that is global to a branch of the inheritance hier-
archy. It is possible to name an identifier within
a given class C and to reuse it within any other
class that inherits from C. Thus the empirical dif-
ficulty associated with the use of exported iden-
tifiers is that associated with global names. That
is, the grammar writer must remember the names
used and their intended interpretation. When de-
veloping a large size grammar, this rapidly makes
grammar writing, maintenance and debugging an
extremely difficult task. Hence global identifiers
should be use sparingly.
But although non trivial (this was in fact one
of the main motivations for developing XMG), this
empirical limitation is not the only one. There are
two additional formal restrictions which prevent a
general use of exported identifiers.
First, as remarked upon in (Crabbe and Duchier,
2004), global names do not support multiple use
of the same class within a class. For instance, con-
sider the case illustrated in Figure 3.
s s s
v pp ? v pp pp
p n p n p n
Figure 3: Case of double prepositional phrase.
In this case, the aim is to produce the elemen-
tary tree for a verb taking two prepositional argu-
ments such as parler a` quelqu?un de quelque chose
(to tell someone about something). Ideally, this is
done by combining the verbal fragment on the left
251
with two occurrences of the PP class in the mid-
dle to yield the tree on the right. However if, as is
likely in a large size metagrammar, any of the pp,
the p or the n node bears an exported identifier,
then the two occurrences of this node will be iden-
tified so that the resulting tree will be that given in
(4).
s
v pp
p n
Figure 4: Double prepositional phrase with ex-
ported identifiers.
We will see below how colours permit a natural
account of such cases.
Second, exported modifiers do not support iden-
tifier unification in cases of conjunction, disjunc-
tion and multiple inheritance. That is, in each of
the three cases below, the various occurrences of
?X are not identified.
C1 ?X ? C2 ?X
C1 ?X ? C2 ?X
C3 ?X 6 C1 ?X ? C2 ?X
In such cases, the multiple occurrences of ?X
need to be explicitly identified (see below).
In practice then, the safest use of simple exports
(ie without explicit identifier equalities) consists in
using them
? in combination with inheritance only and
? within a linguistically motivated subpart of
the inheritance hierarchy
4.2 Colours
As discussed in section 2.3, node identifications
can be based on colours. In particular, if a node is
white, it must be identified with a black node.
The main advantage of this particular identifica-
tion mechanism is that it is extremely economical.
Not only is there no longer any need to remember
names, there is in fact no need to chose a name.
When developing a metagrammar containing sev-
eral hundreds of nodes, this is a welcome feature.
This ?no-name? aspect of the colour mecha-
nism is in particular very useful when a given class
needs to be combined with many other classes.
For instance, in SEMFRAG (Gardent, 2006), the
semantic index of a semantic functor (i.e., a verb,
an adjective, a preposition or a predicative noun)
needs to be projected from the anchor to the root
node as illustrated in Figure 5. This can be done,
as shown in the figure by conjoining CSem with CV
or CA and letting the colour unify the appropriate
nodes.
s ? s ? ?i2
np ? vp ? np ? np ? ap ? np ? ?i2i1
v ? cop ? adj ? ?
i1
v | adj




CV




CA




CSem
Figure 5: Case of semantic projections.
Colouring also solves the problem raised by the
multiple reuse of the same class in the definition
of a given class. The colouring will be as shown
in Figure 6. Since the pp, p and n nodes are black,
their two occurrences cannot be identified. The
two white s nodes however will both be unified
with the black one thus yielding the expected tree.
s ? s ? s ?
v ? pp ? ? v ? pp ? pp ?
p ? n ? p ? n ? p ? n ?
Figure 6: Case of double prepositional phrase with
coloured descriptions.
As for exports however, colours cannot always
be used to force identifications.
First, colours can only be used in combination
with conjunction or inheritance of non exported
identifiers. Indeed, inheritance does not allow the
identification of two different objects. Hence if a
class C containing a white node named ?X inherits
from another class C? exporting a black node also
named ?X, compilation will fail as a given identi-
fier can only have one colour7 . In contrast, when
solving a description containing the conjunction of
a black and a white node (where these two nodes
have either no names or distinct names), the well
formedness constraint on coloured tree will ensure
that these two nodes are in fact the same (since a
tree containing a white node is ill formed).
Second, colour based identification is non de-
terministic. For instance, in Figure 5, if the lowest
7However, different occurrences of the same unnamed
node can have distinct colours.
252
node b of CSem was not labelled cat = v | adj,
CA? CSem would yield not one but two trees: one
where b is identified with the cop node and the
other where it is identified with the adj one. In
other words, colour based unification is only pos-
sible in cases where node decorations (or explicit
node identifications) are sufficiently rich to con-
strain the possible unifications.
To sum up, colours are useful in situations
where:
? a given class needs to be combined with
many other classes ? in this case it is unlikely
that the names used in all classes to be com-
bined are consistent (ie that they are the same
for information that must be unified and that
they are different for information that must
not) and
? the nodes to be identified are unambigu-
ous (the white and the black nodes contain
enough information so that it is clear which
white node must be identified with which
black one)
4.3 Interfaces
Interfaces provide another mechanism for global
naming. They are particularly useful in cases
where two fundamentally different objects contain
non-node identifiers that must be unified.
Recall (cf. section 4.2) that exported identifiers
are best used within restricted, linguistically well
defined hierarchies. In a case where the objects
containing the two identifiers to be identified are
different, these will belong to distinct part of the
inheritance hierarchy hence identifier export is not
a good option.
Node colouring is another possibility but as the
name indicates, it only works for nodes, not for
feature values.
In such a situation then, interfaces come in
handy. This is the case for instance, when com-
bining a semantic representation with a tree. The
semantic formula and the tree are distinct objects
but in the approach to semantic construction de-
scribed in (Gardent and Kallmeyer, 2003), they
share some semantic indices. For instance, the
subject node in the tree is labelled with an index
feature whose value must be (in an active form
tree) that of the first argument occurring in the
semantic representation. The encoding of the re-
quired coreference can be sketched as follows:
Subj ?{ . . . ?X . . .}? = [subjectIdx = ?X]
Sem ?{ . . . ?Y . . .}? = [arg1 = ?Y]
Tree ?Subj? = [subjectIdx = ?Z]?
Sem? = [arg1 = ?Z]
The first two lines show the naming of the iden-
tifiers ?X and ?Y through the interface, the third
illustrates how unification can be used to identify
the values named by the interface: since the same
variable ?Z is the value of the two features arg1
and subjectIdx, the corresponding values in the
Subj and Sem classes are identified.
4.4 Explicit identification of exported
identifiers
The explicit identification of exported identifiers is
the last resort solution. It is not subject to any of
the restrictions listed above and can be combined
with conjunction, disjunction and inheritance. It
is however uneconomical and complexifies gram-
mar writing (since every node identification must
be explicitly declared). Hence it should be used as
little as possible.
In practice, explicit identification of exported
identifiers is useful :
? to further constrain colour based identifica-
tion (when the feature information present in
the nodes does not suffice to force identifica-
tion of the appropriate nodes)
? to model general principles that apply to sev-
eral subtrees in a given hierarchy
The second point is illustrated by Subject/Verb
agreement. Suppose that in the metagrammar,
we want to have a separate class to enforce this
agreement. This class consists of a subject node
?SubjAgr bearing agreement feature ?X and of
a verb node ?VerbAgr bearing the same agree-
ment feature. It must then be combined with all
verbal elementary trees described by the meta-
grammar whereby in each such combination the
nodes ?SubjAgr, ?VerbAgr must be identi-
fied with the subject and the verb node respec-
tively. This is a typical case of multiple inheri-
tance because both the subject and the verb nodes
are specified by inheritance and ?SubjAgr,
?VerbAgr must be further inherited. Since
nodes must be identified and multiple inheritance
occur, simple identifier exports cannot be used (cf.
section 2.3.1). If colours cannot be sufficiently
253
Pros Cons Practice
Export Economy Name management Use in linguistically motivated
Not with multiple inheritance sub-hierarchy
Not with conjunction
Not with disjunction
Not with multiple reuse
Colours Economy ++ Non deterministic
Multiple reuse OK Not with inheritance Use when a given class
and identically named identifiers combines with many classes
Interface Global Name management Use for Syntax/Semantic interface
Explicit identification Usable in all cases Uneconomical Last Resort solution
Figure 7: Summary of the pros and cons of sharing mechanisms.
constrained by features, then the only solution left
is explicit node identification.
Figure 7 summarises the pros and the cons of
each approach.
5 Conclusion
In this paper, we have introduced a specification
formalism for Tree-Based Grammars and shown
that its expressivity helps solving specification
problems which might be encountered when de-
velopping a large scale tree-based grammar.
This formalism has been implemented within
the XMG system and successfully used to encode
both a core TAG for French (Crabbe, 2005; Gar-
dent, 2006) and a core Interaction Grammar (Per-
rier, 2003). We are currently exploring ways
in which the XMG formalism could be extended
to automatically enforce linguistically-based well-
formedness principles such as for instance, a kind
of Head Feature Principle for TAG.
References
T. Becker. 2000. Patterns in metarules. In A. Abeille and
O. Rambow, editors, Tree Adjoining Grammars: formal,
computational and linguistic aspects. CSLI publications,
Stanford.
J. Bos. 1995. Predicate Logic Unplugged. In Proceedings of
the 10th Amsterdam Colloquium, Amsterdam.
M.H. Candito. 1996. A principle-based hierarchical rep-
resentation of LTAGs. In Proceedings of COLING?96,
Kopenhagen.
B. Crabbe and D. Duchier. 2004. Metagrammar Redux. In
Proceedings of CSLP 2004, Copenhagen.
B. Crabbe. 2005. Repre?sentation informatique de gram-
maires fortement lexicalise?es : Application a` la gram-
maire d?arbres adjoints. Ph.D. thesis, Universite? Nancy
2.
D. Duchier and C. Gardent. 1999. A constraint based treat-
ment of descriptions. In Proceedings of the 3rd IWCS,
Tilburg.
Denys Duchier and Stefan Thater. 1999. Parsing with tree
descriptions: a constraint-based approach. In NLULP,
pages 17?32, Las Cruces, New Mexico.
R. Evans, G. Gazdar, and D. Weir. 1995. Encoding lexi-
calized tree adjoining grammars with a nonmonotonic in-
heritance hierarchy. In Proceedings of the 33rd Annual
Meeting of the ACL, 77-84.
B. Gaiffe, B. Crabbe, and A. Roussanaly. 2002. A new meta-
grammar compiler. In Proceedings of TAG+6, Venice.
C. Gardent and L. Kallmeyer. 2003. Semantic construction
in FTAG. In Proceedings of EACL?03, Budapest.
C. Gardent. 2006. Inte?gration d?une dimension se?mantique
dans les grammaires d?arbres adjoints. In Actes de La
13e`me e?dition de la confe?rence sur le TALN (TALN 2006).
A. Joshi and Y. Schabes. 1997. Tree-adjoining grammars.
In G. Rozenberg and A. Salomaa, editors, Handbook of
Formal Languages, volume 3, pages 69 ? 124. Springer,
Berlin, New York.
L. Kallmeyer. 1996. Tree description grammars. In Results
of the 3rd KONVENS Conference, pages 330 ? 341. Mou-
ton de Gruyter ed., Hawthorne, NY, USA.
H.-U. Krieger and U. Schafer. 1994. TDL ? a type descrip-
tion language for constraint-based grammars. In Proceed-
ings of COLING-94, pp. 893?899.
R. Muskens and E. Krahmer. 1998. Description theory, ltags
and underspecified semantics. In TAG?4.
G. Perrier. 2000. Interaction grammars. In Proceedings of
18th International Conference on Computational Linguis-
tics (CoLing 2000), Sarrebrcken.
G. Perrier. 2003. Les grammaires d?interaction. HDR en
informatique, Universite? Nancy 2.
K. Vijay-Shanker and Y. Schabes. 1992. Structure sharing
in lexicalized tree adjoining grammars. In Proceedings of
COLING?92, Nantes, pp. 205 - 212.
E. Villemonte de la Clergerie. 2005. DyALog: a tabular
logic programming based environment for NLP. In Pro-
ceedings of CSLP?05, Barcelona.
F. Xia, M. Palmer, and K. Vijay-Shanker. 1999. To-
ward semi-automating grammar development. In Proc. of
NLPRS-99, Beijing, China.
254
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 13?16,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemTAG: a platform for specifying Tree Adjoining Grammars and
performing TAG-based Semantic Construction
Claire Gardent
CNRS / LORIA
Campus scientifique - BP 259
54 506 Vand?uvre-Le`s-Nancy CEDEX
France
Claire.Gardent@loria.fr
Yannick Parmentier
INRIA / LORIA - Nancy Universite?
Campus scientifique - BP 259
54 506 Vand?uvre-Le`s-Nancy CEDEX
France
Yannick.Parmentier@loria.fr
Abstract
In this paper, we introduce SEMTAG, a free
and open software architecture for the de-
velopment of Tree Adjoining Grammars in-
tegrating a compositional semantics. SEM-
TAG differs from XTAG in two main ways.
First, it provides an expressive grammar
formalism and compiler for factorising and
specifying TAGs. Second, it supports se-
mantic construction.
1 Introduction
Over the last decade, many of the main grammatical
frameworks used in computational linguistics were
extended to support semantic construction (i.e., the
computation of a meaning representation from syn-
tax and word meanings). Thus, the HPSG ERG
grammar for English was extended to output mini-
mal recursive structures as semantic representations
for sentences (Copestake and Flickinger, 2000); the
LFG (Lexical Functional Grammar) grammars to
output lambda terms (Dalrymple, 1999); and Clark
and Curran?s CCG (Combinatory Categorial Gram-
mar) based statistical parser was linked to a seman-
tic construction module allowing for the derivation
of Discourse Representation Structures (Bos et al,
2004).
For Tree Adjoining Grammar (TAG) on the other
hand, there exists to date no computational frame-
work which supports semantic construction. In this
demo, we present SEMTAG, a free and open soft-
ware architecture that supports TAG based semantic
construction.
The structure of the paper is as follows. First,
we briefly introduce the syntactic and semantic for-
malisms that are being handled (section 2). Second,
we situate our approach with respect to other possi-
ble ways of doing TAG based semantic construction
(section 3). Third, we show how XMG, the linguistic
formalism used to specify the grammar (section 4)
differs from existing computational frameworks for
specifying a TAG and in particular, how it supports
the integration of semantic information. Finally, sec-
tion 5 focuses on the semantic construction module
and reports on the coverage of SEMFRAG, a core
TAG for French including both syntactic and seman-
tic information.
2 Linguistic formalisms
We start by briefly introducing the syntactic and se-
mantic formalisms assumed by SEMTAG namely,
Feature-Based Lexicalised Tree Adjoining Gram-
mar and LU .
Tree Adjoining Grammars (TAG) TAG is a tree
rewriting system (Joshi and Schabes, 1997). A TAG
is composed of (i) two tree sets (a set of initial trees
and a set of auxiliary trees) and (ii) two rewriting op-
erations (substitution and adjunction). Furthermore,
in a Lexicalised TAG, each tree has at least one leaf
which is a terminal.
Initial trees are trees where leaf-nodes are labelled
either by a terminal symbol or by a non-terminal
symbol marked for substitution (?). Auxiliary trees
are trees where a leaf-node has the same label as the
root node and is marked for adjunction (?). This
leaf-node is called a foot node.
13
Further, substitution corresponds to the insertion
of an elementary tree t1 into a tree t2 at a frontier
node having the same label as the root node of t1.
Adjunction corresponds to the insertion of an auxil-
iary tree t1 into a tree t2 at an inner node having the
same label as the root and foot nodes of t1.
In a Feature-Based TAG, the nodes of the trees are
labelled with two feature structures called top and
bot. Derivation leads to unification on these nodes as
follows. Given a substitution, the top feature struc-
tures of the merged nodes are unified. Given an
adjunction, (i) the top feature structure of the inner
node receiving the adjunction and of the root node of
the inserted tree are unified, and (ii) the bot feature
structures of the inner node receiving the adjunction
and of the foot node of the inserted tree are unified.
At the end of a derivation, the top and bot feature
structures of each node in a derived tree are unified.
Semantics (LU ). The semantic representation lan-
guage we use is a unification-based extension of the
PLU language (Bos, 1995). LU is defined as fol-
lows. Let H be a set of hole constants, Lc the set
of label constants, and Lv the set of label variables.
Let Ic (resp. Iv) be the set of individual constants
(resp. variables), let R be a set of n-ary relations
over Ic? Iv?H , and let ? be a relation over H ?Lc
called the scope-over relation. Given l ? Lc ? Lv,
h ? H , i1, . . . , in ? Iv ? Ic ?H , and Rn ? R, we
have:
1. l : Rn(i1, . . . , in) is a LU formula.
2. h ? l is a LU formula.
3. ?,? is LU formula iff both ? and ? are LU
formulas.
4. Nothing else is a LU formula.
In short, LU is a flat (i.e., non recursive) version
of first-order predicate logic in which scope may be
underspecified and variables can be unification vari-
ables1.
3 TAG based semantic construction
Semantic construction can be performed either dur-
ing or after derivation of a sentence syntactic struc-
ture. In the first approach, syntactic structure and
semantic representations are built simultaneously.
This is the approach sketched by Montague and
1For mode details on LU , see (Gardent and Kallmeyer,
2003).
adopted e.g., in the HPSG ERG and in synchronous
TAG (Nesson and Shieber, 2006). In the second
approach, semantic construction proceeds from the
syntactic structure of a complete sentence, from a
lexicon associating each word with a semantic rep-
resentation and from a set of semantic rules speci-
fying how syntactic combinations relate to seman-
tic composition. This is the approach adopted for
instance, in the LFG glue semantic framework, in
the CCG approach and in the approaches to TAG-
based semantic construction that are based on the
TAG derivation tree.
SEMTAG implements a hybrid approach to se-
mantic construction where (i) semantic construction
proceeds after derivation and (ii) the semantic lexi-
con is extracted from a TAG which simultaneously
specifies syntax and semantics. In this approach
(Gardent and Kallmeyer, 2003), the TAG used in-
tegrates syntactic and semantic information as fol-
lows. Each elementary tree is associated with a for-
mula of LU representing its meaning. Importantly,
the meaning representations of semantic functors in-
clude unification variables that are shared with spe-
cific feature values occurring in the associated ele-
mentary trees. For instance in figure 1, the variables
x and y appear both in the semantic representation
associated with the tree for aime (love) and in the
tree itself.
Given such a TAG, the semantics of a tree
t derived from combining the elementary trees
t1, . . . , tn is the union of the semantics of t1, . . . , tn
modulo the unifications that results from deriving
that tree. For instance, given the sentence Jean aime
vraiment Marie (John really loves Mary) whose
TAG derivation is given in figure 1, the union of the
semantics of the elementary trees used to derived the
sentence tree is:
l0 : jean(j), l1 : aime(x, y), l2 : vraiment(h0),
ls ? h0, l3 : marie(m)
The unifications imposed by the derivations are:
{x? j, y ? m, ls ? l1}
Hence the final semantics of the sentence Jean aime
vraiment Marie is:
l0 : jean(j), l1 : aime(j,m), l2 : vraiment(h0),
l1 ? h0, l3 : marie(m)
14
S[lab:l1]
NP[idx:j] NP[idx:x,lab:l1] V[lab:l1] NP
[idx:y,lab:l1] V[lab:l2] NP[idx:m]
Jean aime V[lab:ls]? Adv Marie
vraiment
l0 : jean(j) l1 : aimer(x, y) l2 : vraiment(h0), l3 : marie(m)
ls ? h0
Figure 1: Derivation of ?Jean aime vraiment Marie?
As shown in (Gardent and Parmentier, 2005), se-
mantic construction can be performed either dur-
ing or after derivation. However, performing se-
mantic construction after derivation preserves mod-
ularity (changes to the semantics do not affect syn-
tactic parsing) and allows the grammar used to re-
main within TAG (the grammar need contain nei-
ther an infinite set of variables nor recursive feature
structures). Moreover, it means that standard TAG
parsers can be used (if semantic construction was
done during derivation, the parser would have to be
adapted to handle the association of each elemen-
tary tree with a semantic representation). Hence in
SEMTAG, semantic construction is performed after
derivation. Section 5 gives more detail about this
process.
4 The XMG formalism and compiler
SEMTAG makes available to the linguist a formalism
(XMG) designed to facilitate the specification of tree
based grammars integrating a semantic dimension.
XMG differs from similar proposals (Xia et al, 1998)
in three main ways (Duchier et al, 2004). First it
supports the description of both syntax and seman-
tics. Specifically, it permits associating each ele-
mentary tree with an LU formula. Second, XMG pro-
vides an expressive formalism in which to factorise
and combine the recurring tree fragments shared by
several TAG elementary trees. Third, XMG pro-
vides a sophisticated treatment of variables which
inter alia, supports variable sharing between seman-
tic representation and syntactic tree. This sharing is
implemented by means of so-called interfaces i.e.,
feature structures that are associated with a given
(syntactic or semantic) fragment and whose scope
is global to several fragments of the grammar speci-
fication.
To specify the syntax / semantics interface
sketched in section 5, XMG is used as follows :
1. The elementary tree of a semantic functor is
defined as the conjunction of its spine (the projec-
tion of its syntactic head) with the tree fragments
describing each of its arguments. For instance, in
figure 2, the tree for an intransitive verb is defined
as the conjunction of the tree fragment for its spine
(Active) with the tree fragment for (a canonical re-
alisation of) its subject argument (Subject).
2. In the tree fragments representing the different
syntactic realizations (canonical, extracted, etc.) of
a given grammatical function, the node representing
the argument (e.g., the subject) is labelled with an
idx feature whose value is shared with a GFidx fea-
ture in the interface (where GF is the grammatical
function).
3. Semantic representations are encapsulated as
fragments where the semantic arguments are vari-
ables shared with the interface. For instance, the ith
argument of a semantic relation is associated with
the argI interface feature.
4. Finally, the mapping between grammatical
functions and thematic roles is specified when con-
joining an elementary tree fragment with a semantic
representation. For instance, in figure 22, the inter-
face unifies the value of arg1 (the thematic role) with
that of subjIdx (a grammatical function) thereby
specifying that the subject argument provides the
value of the first semantic argument.
5 Semantic construction
As mentioned above, SEMTAG performs semantic
construction after derivation. More specifically, se-
mantic construction is supported by the following 3-
step process:
2The interfaces are represented using gray boxes.
15
Intransitive: Subject: Active: 1-ary relation:
S
NP?[idx=X] VP
l0:Rel(X)
arg0=X
subjIdx=X
?
S
NP?[idx=I] VP
subjIdx=I
?
S
VP ? l0:Rel(A)
arg0=A
Figure 2: Syntax / semantics interface within the metagrammar.
1. First, we extract from the TAG generated by
XMG (i) a purely syntactic TAG G?, and (ii) a purely
semantic TAG G?? 3 A purely syntactic (resp. seman-
tic) Tag is a TAG whose features are purely syntactic
(resp. semantic) ? in other words, G?? is a TAG with
no semantic features whilst G?? is a TAG with only
semantic features. Entries of G? and G?? are indexed
using the same key.
2. We generate a tabular syntactic parser for G?
using the DyALog system of (de la Clergerie, 2005).
This parser is then used to compute the derivation
forest for the input sentence.
3. A semantic construction algorithm is applied to
the derivation forest. In essence, this algorithm re-
trieves from the semantic TAG G?? the semantic trees
involved in the derivation(s) and performs on these
the unifications prescribed by the derivation.
SEMTAG has been used to specify a core TAG for
French, called SemFRag. This grammar is currently
under evaluation on the Test Suite for Natural Lan-
guage Processing in terms of syntactic coverage, se-
mantic coverage and semantic ambiguity. For a test-
suite containing 1495 sentences, 62.88 % of the sen-
tences are syntactically parsed, 61.27 % of the sen-
tences are semantically parsed (i.e., at least one se-
mantic representation is computed), and the average
semantic ambiguity (number of semantic represen-
tation per sentence) is 2.46.
SEMTAG is freely available at http://trac.
loria.fr/?semtag.
3As (Nesson and Shieber, 2006) indicates, this extraction in
fact makes the resulting system a special case of synchronous
TAG where the semantic trees are isomorphic to the syntactic
trees and unification variables across the syntactic and semantic
components are interpreted as synchronous links.
References
J. Bos, S. Clark, M. Steedman, J. R. Curran, and J. Hock-
enmaier. 2004. Wide-coverage semantic representa-
tions from a ccg parser. In Proceedings of the 20th
COLING, Geneva, Switzerland.
J. Bos. 1995. Predicate Logic Unplugged. In Proceed-
ings of the tenth Amsterdam Colloquium, Amsterdam.
A. Copestake and D. Flickinger. 2000. An open-
source grammar development environment and broad-
coverage english grammar using hpsg. In Proceedings
of LREC, Athens, Greece.
Mary Dalrymple, editor. 1999. Semantics and Syntax in
Lexical Functional Grammar. MIT Press.
E. de la Clergerie. 2005. DyALog: a tabular logic pro-
gramming based environment for NLP. In Proceed-
ings of CSLP?05, Barcelona.
D. Duchier, J. Le Roux, and Y. Parmentier. 2004. The
Metagrammar Compiler: An NLP Application with
a Multi-paradigm Architecture. In Proceedings of
MOZ?2004, Charleroi.
C. Gardent and L. Kallmeyer. 2003. Semantic construc-
tion in FTAG. In Proceedings of EACL?03, Budapest.
C. Gardent and Y. Parmentier. 2005. Large scale se-
mantic construction for tree adjoining grammars. In
Proceedings of LACL05, Bordeaux, France.
A. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69
? 124. Springer, Berlin, New York.
Rebecca Nesson and Stuart M. Shieber. 2006. Sim-
pler TAG semantics through synchronization. In Pro-
ceedings of the 11th Conference on Formal Grammar,
Malaga, Spain, 29?30 July.
F. Xia, M. Palmer, K. Vijay-Shanker, and J. Rosenzweig.
1998. Consistent grammar development using partial-
tree descriptions for lexicalized tree adjoining gram-
mar. Proceedings of TAG+4.
16
123
124
125
126
127
128
129
130
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 115?120,
Sydney, July 2006. c?2006 Association for Computational Linguistics
SemTAG, the LORIA toolbox for TAG-based Parsing and Generation
Eric Kow
INRIA / LORIA
Universite? Henri Poincare?
615, rue du Jardin Botanique
F-54 600 Villers-Le`s-Nancy
kow@loria.fr
Yannick Parmentier
INRIA / LORIA
Universite? Henri Poincare?
615, rue du Jardin Botanique
F-54 600 Villers-Le`s-Nancy
parmenti@loria.fr
Claire Gardent
CNRS / LORIA
615, rue du Jardin Botanique
F-54 600 Villers-Le`s-Nancy
gardent@loria.fr
Abstract
In this paper, we introduce SEMTAG, a
toolbox for TAG-based parsing and gen-
eration. This environment supports the
development of wide-coverage grammars
and differs from existing environments
for TAG such as XTAG, (XTAG-Research-
Group, 2001) in that it includes a semantic
dimension. SEMTAG is open-source and
freely available.
1 Introduction
In this paper we introduce a toolbox that allows for
both parsing and generation with TAG. This tool-
box combines existing software and aims at facili-
tating grammar development, More precisely, this
toolbox includes1:
? XMG: a grammar compiler which supports the
generation of a TAG from a factorised TAG
(Crabbe? and Duchier, 2004),
? LLP2 and DyALog: two chart parsers, one
with a friendly user interface (Lopez, 2000)
and the other optimised for efficient parsing
(Villemonte de la Clergerie, 2005)2
? GenI: a chart generator which has been
tested on a middle size grammar for French
(Gardent and Kow, 2005)
1All these tools are freely available, more information and
links at http://trac.loria.fr/?semtag.
2Note that DyALog refers in fact to a logic program-
ming language, and a tabular compiler for this language. The
DyALog system is well-adapted to the compilation of effi-
cient tabular parsers.
2 XMG, a grammar writing environment
for Tree Based Grammars
XMG provides a grammar writing environment for
tree based grammars3 with three distinctive fea-
tures. First, XMG supports a highly factorised and
fully declarative description of tree based gram-
mars. Second, XMG permits the integration in a
TAG of a semantic dimension. Third, XMG is based
on well understood and efficient logic program-
ming techniques. Moreover, it offers a graphical
interface for exploring the resulting grammar (see
Figure 1).
Factorising information. In the XMG frame-
work, a TAG is defined by a set of classes organised
in an inheritance hierarchy where classes define
tree fragments (using a tree logic) and tree frag-
ment combinations (by conjunction or disjunc-
tion). XMG furthermore integrates a sophisticated
treatment of names whereby variables scope can
be local, global or user defined (i.e., local to part
of the hierarchy).
In practice, the resulting framework supports a
very high degree of factorisation. For instance, a
first core grammar (FRAG) for French comprising
4 200 trees was produced from roughly 300 XMG
classes.
Integrating semantic information. In XMG,
classes can be multi-dimensional. That is, they
can be used to describe several levels of linguis-
tic knowledge such as for instance, syntax, seman-
tics or prosody. At present, XMG supports classes
including both a syntactic and a semantic dimen-
sion. As mentioned above, the syntactic dimen-
3Although in this paper we only mention TAG, the XMG
framework is also used to develop so called Interaction Gram-
mars i.e., grammars whose basic units are tree descriptions
rather than trees (Parmentier and Le Roux, 2005).
115
Figure 1: XMG?s graphical interface
sion is based on a tree logic and can be used to
describe (partial) tree fragments. The semantic di-
mension on the other hand, can be used to asso-
ciate with each tree a flat semantic formula. Such a
formula can furthermore include identifiers which
corefer with identifiers occurring in the associated
syntactic tree. In other words, XMG also provides
support for the interface between semantic formu-
lae and tree decorations. Note that the inclusion of
semantic information remains optional. That is, it
is possible to use XMG to define a purely syntactic
TAG.
XMG was used to develop a core grammar for
French (FRAG) which was evaluated to have 75%
coverage4 on the Test Suite for Natural Language
Processing (TSNLP, (Lehmann et al, 1996)). The
FRAG grammar was furthermore enriched with
semantic information using another 50 classes de-
scribing the semantic dimension (Gardent, 2006).
The resulting grammar (SEMFRAG) describes
both the syntax and the semantics of the French
core constructions.
Compiling an XMG specification. By build-
ing on efficient techniques from logic program-
ming and in particular, on the Warren?s Abstract
4This means that for 75 % of the sentences, a TAG parser
can build at least one derivation.
Figure 2: The LLP2 parser.
Machine idea (Ait-Kaci, 1991), the XMG com-
piler allows for very reasonable compilation times
(Duchier et al, 2004). For instance, the compila-
tion of a TAG containing 6 000 trees takes about 15
minutes with a Pentium 4 processor 2.6 GHz and
1 GB of RAM.
3 Two TAG parsers
The toolbox includes two parsing systems: the
LLP2 parser and the DyALog system. Both of
them can be used in conjunction with XMG. First
we will briefly introduce both of them, and then
show that they can be used with a semantic gram-
mar (e.g., SEMFRAG) to perform not only syntac-
tic parsing but also semantic construction.
LLP2 The LLP2 parser is based on a bottom-
up algorithm described in (Lopez, 1999). It has
relatively high parsing times but provides a user
friendly graphical parsing environment with much
statistical information (see Figure 2). It is well
suited for teaching or for small scale projects.
DyALog The DyALog system on the other
hand, is a highly optimised parsing system based
on tabulation and automata techniques (Ville-
monte de la Clergerie, 2005). It is implemented
using the DyALog programming language (i.e.,
it is bootstrapped) and is also used to compile
parsers for other types of grammars such as Tree
Insertion Grammars.
The DyALog system is coupled with a seman-
tic construction module whose aim is to associate
with each parsed string a semantic representation5.
This module assumes a TAG of the type described
in (Gardent and Kallmeyer, 2003; Gardent, 2006)
5The corresponding system is called SemConst (cf section
6).
116
Figure 3: The SemConst system
where initial trees are associated with semantic in-
formation and unification is used to combine se-
mantic representations. In such a grammar, the se-
mantic representation of a derived tree is the union
of the semantic representations of the trees enter-
ing in the derivation of that derived tree modulo
the unifications entailed by analysis. As detailed
in (Gardent and Parmentier, 2005), such grammars
support two strategies for semantic construction.
The first possible strategy is to use the full
grammar and to perform semantic construction
during derivation. In this case the parser must ma-
nipulate both syntactic trees and semantic repre-
sentations. The advantage is that the approach is
simple (the semantic representations can simply
be an added feature on the anchor node of each
tree). The drawback is that the presence of seman-
tic information might reduce chart sharing.
The second possibility involves extracting the
semantic information contained in the grammar
and storing it into a semantic lexicon. Parsing then
proceeds with a purely syntactic grammar and se-
mantic construction is done after parsing on the
basis of the parser output and of the extracted se-
mantic lexicon. This latter technique is more suit-
able for large scale semantic construction as it sup-
ports better sharing in the derivation forests. It
is implemented in the LORIA toolbox where a
module permits both extracting a semantic lexi-
con from a semantic TAG and constructing a se-
mantic representation based on this lexicon and on
the derivation forests output by DyALog (see Fig-
ure 3).
The integration of the DyALog system into the
toolbox is relatively new so that parsing evaluation
Figure 4: The GenI debugger
is still under progress. So far, evaluation has been
restricted to parsing the TSNLP with DyALog
with the following preliminary results. On sen-
tences ranging from 1 to 18 words, with an aver-
age of 7 words per sentence, and with a grammar
containing 5 069 trees, DyALog average parsing
time is of 0.38 sec with a P4 processor 2.6 GHz
and 1 GB of RAM6.
4 A TAG-based surface realiser
The surface realiser GenI takes a TAG and a flat
semantic logical form as input, and produces all
the sentences that are associated with that logi-
cal form by the grammar. It implements two bot-
tom up algorithms, one which manipulates derived
trees as items and one which is based on Earley for
TAG. Both of these algorithms integrate a number
of optimisations such as delayed adjunction and
polarity filtering (Kow, 2005; Gardent and Kow,
2005).
GenI is written in Haskell and includes a
graphical debugger to inspect the state of the gen-
erator at any point in the surface realisation pro-
cess (see Figure 4). It also integrates a test harness
for automated regression testing and benchmark-
ing of the surface realiser and the grammar. The
harness gtester is written in Python. It runs the
surface realiser on a test suite, outputting a single
document with a table of passes and failures and
various performance charts (see Figures 5 and 6).
Test suite and performance The test suite is
built with an emphasis on testing the surface re-
6These features only concern classic syntactic parsing as
the semantic construction module has not been tested on real
grammars yet.
117
test expected simple earley
t1 il le accepter pass pass
t32 il nous accepter pass pass
t83 le ingnieur le lui apprendre pass DIED
t114 le ingnieur nous le prsenter pass pass
t145 le ingnieur vous le apprendre pass pass
t180 vous venir pass pass
Figure 5: Fragment of test harness output - The
Earley algorithm timed out.
 0
 1000
 2000
 3000
 4000
 5000
 6000
 0  20  40  60  80  100  120  140  160  180  200
ch
art
_s
ize
lex_foot_nodes
chart_size for lex_foot_nodes
simpleearley
Figure 6: Automatically generated graph of per-
formance data by the test harness.
aliser?s performance in the face of increasing para-
phrastic power i.e., ambiguity. The suite consists
of semantic inputs that select for and combines
verbs with different valencies. For example, given
a hypothetical English grammar, a valency (2,1)
semantics might be realised in as Martin thinks
Faye drinks (thinks takes 2 arguments and drinks
takes 1), whereas a valency (2,3,2) one would be
Dora says that Martin tells Bob that Faye likes
music. The suite also adds a varying number of
intersective modifiers into the mix, giving us for
instance, The girl likes music, The pretty scary girl
likes indie music.
The sentences in the suite range from 2 to 15
words (8 average). Realisation times for the core
suite range from 0.7 to 2.84 seconds CPU time
(average 1.6 seconds).
We estimate the ambiguity for each test case
in two ways. The first is to count the number of
paraphrases. Given our current grammar, the test
cases in our suite have up to 669 paraphrases (av-
erage 41). The second estimate for ambiguity is
the number of combinations of lexical items cov-
ering the input semantics.
This second measure is based on optimisation
known as polarity filtering (Gardent and Kow,
2005). This optimisation detects and eliminates
combinations of lexical items that cannot be used
to build a result. It associates the syntactic re-
sources (root nodes) and requirements (substitu-
tion nodes) of the lexical items to polarities, which
are then used to build ?polarity automata?. The
automata are minimised to eliminate lexical com-
binations where the polarities do not cancel out,
that is those for which the number of root and sub-
stitution nodes for any given category do not equal
each other.
Once built, the polarity automata can also serve
to estimate ambiguity. The number of paths in the
automaton represent the number of possible com-
binations of lexical items. To determine how ef-
fective polarity filtering with respect to ambiguity,
we compare the combinations before and after po-
larity filtering. Before filtering, we start with an
initial polarity automaton in which all items are
associated with a zero polarity. This gives us the
lexical ambiguity before filtering. The polarity fil-
ter then builds upon this to form a final automaton
where all polarities are taken into account. Count-
ing the paths on this automaton gives us the am-
biguity after filtering, and comparing this number
with the lexical initial ambiguity provides an es-
timate on the usefulness of the polarity filter. In
our suite, the initial automata for each case have
1 to 800 000 paths (76 000 average). The fi-
nal automata have 1 to 6000 paths (192 average).
This can represent quite a large reduction in search
space, 4000 times in the case of the largest au-
tomaton. The effect of this search space reduc-
tion is most pronounced on the larger sentences or
those with the most modifiers. Indeed, realisation
times with and without filtering are comparable for
most of the test suite, but for the most complicated
sentence in the core suite, polarity filtering makes
surface realisation 94% faster, producing a result
in 2.35 seconds instead of 37.38.
5 Benefits of an integrated toolset
As described above, the LORIA toolbox for TAG
based semantic processing includes a lexicon, a
grammar, a parser, a semantic construction mod-
ule and a surface realiser. Integrating these into
a single platform provides some accrued benefits
which we now discuss in more details.
Simplified resource management The first ad-
vantage of an integrated toolkit is that it facilitates
118
the management of the linguistic resources used
namely the grammar and the lexicon. Indeed it is
common that each NLP tool (parser or generator)
has its own representation format. Thus, manag-
ing the resources gets tiresome as one has to deal
with several versions of a single resource. When
one version is updated, the others have to be re-
computed. Using an integrated toolset avoid such
a drawback as the intermediate formats are hidden
and the user can focus on linguistic description.
Better support for grammar development
When developing parsers or surface realisers, it is
useful to test them out by running them on large,
realistic grammars. Such grammars can explore
nooks and crannies in our implementations that
would otherwise have been overlooked by a toy
grammar. For example, it was only when we ran
GenI on our French grammar that we realised our
implementation did not account for auxiliary trees
with substitution nodes (this has been rectified).
In this respect, one could argue that XMG could al-
most be seen as a parser/realiser debugging utility
because it helps us to build and extend the large
grammars that are crucial for testing.
This perspective can also be inverted; parsers
and surface realiser make for excellent grammar-
debugging devices. For example, one possible
regression test is to run the parser on a suite of
known sentences to make sure that the modified
grammar still parses them correctly. The exact
reverse is useful as well; we could also run the
surface realiser over a suite of known semantic
inputs and make sure that sentences are gener-
ated for each one. This is useful for two reasons.
First, reading surface realiser output (sentences)
is arguably easier for human beings than reading
parser output (semantic formulas). Second, the
surface realiser can tell us if the grammar overgen-
erates because it would output nonsense sentences.
Parsers, on the other hand, are much better adapted
for testing for undergeneration because it is easier
to write sentences than semantic formulas, which
makes it easier to test phenomena which might not
already be in the suite.
Towards a reversible grammar Another ad-
vantage of using such a toolset relies on the fact
that we can manage a common resource for both
parsing and generation, and thus avoid inconsis-
tency, redundancy and offer a better flexibility as
advocated in (Neumann, 1994).
On top of these practical questions, having a
unique reversible resource can lead us further.
For instance, (Neumann, 1994) proposes an inter-
leaved parsing/realisation architecture where the
parser is used to choose among a set of para-
phrases proposed by the generator; paraphrases
which are ambiguous (that have multiple parses)
are discarded in favour of those whose meaning is
most explicit. Concretely, we could do this with a
simple pipeline using GenI to produce the para-
phrases, DyALog to parse them, and a small shell
script to pick the best result. This would only be
a simulation, of course. (Neumann, 1994) goes
as far as to interleave the processes, keeping the
shared chart and using the parser to iteratively
prune the search space as it is being explored by
the generator. The version we propose would not
have such niceties as a shared chart, but the point
is that having all the tools at our disposable makes
such experimentation possible in the first place.
Moreover, there are several other interest-
ing applications of the combined toolbox. We
could use the surface realiser to build artifi-
cial corpora. These can in turn be parsed to
semi-automatically create rich treebanks contain-
ing syntactico-semantic analyses a` la Redwoods
(Oepen et al, 2002).
Eventually, another use for the toolbox might be
in components of standard NLP applications such
as machine translation, questioning answering, or
interactive dialogue systems.
6 Availability
The toolbox presented here is open-source and
freely available under the terms of the GPL7. More
information about the requirements and installa-
tion procedure is available at http://trac.
loria.fr/?semtag. Note that this toolbox is
made of two main components: the GenI8 sys-
tem and the SemConst9 system, which respec-
tively performs generation and parsing from com-
mon linguistic resources. The first is written in
Haskell (except the XMG part written in Oz) and is
multi-platform (Linux, Windows, Mac OS). The
latter is written in Oz (except the DyALog part
which is bootstrapped and contains some Intel as-
sembler code) and is available on Unix-like plat-
7Note that XMG is released under the terms of the
CeCILL license (http://www.cecill.info/index.
en.html), which is compatible with the GPL.
8http://trac.loria.fr/?geni
9http://trac.loria.fr/?semconst
119
forms only.
7 Conclusion
The LORIA toolbox provides an integrated envi-
ronment for TAG based semantic processing: ei-
ther to construct the semantic representation of a
given sentence (parsing) or to generate a sentence
verbalising a given semantic content (generation).
Importantly, both the generator and the parsers
use the same grammar (SEMFRAG) so that both
tools can be used jointly to improve grammar pre-
cision. All the sentences outputted by the surface
realiser should be parsed to have at least the se-
mantic representation given by the test suite, and
all parses of a sentence should be realised into at
least the same sentence.
Current and future work concentrates on de-
veloping an automated error mining environment
for both parsing and generation; on extending the
grammar coverage; on integrating further optimi-
sations both in the parser (through parsing with
factorised trees) and in the generator (through
packing and accessibility filtering cf. (Carroll and
Oepen, 2005); and on experimenting with differ-
ent semantic construction strategies (Gardent and
Parmentier, 2005).
References
H. Ait-Kaci. 1991. Warren?s Abstract Machine: A Tu-
torial Reconstruction. In K. Furukawa, editor, Proc.
of the Eighth International Conference of Logic Pro-
gramming. MIT Press, Cambridge, MA.
J. Carroll and S. Oepen. 2005. High efficiency re-
alization for a wide-coverage unification grammar.
In R. Dale and K-F. Wong, editors, Proceedings of
the Second International Joint Conference on Natu-
ral Language Processing, volume 3651 of Springer
Lecture Notes in Artificial Intelligence, pages 165?
176.
B. Crabbe? and D. Duchier. 2004. Metagrammar Re-
dux. In Proceedings of CSLP 2004, Copenhagen.
D. Duchier, J. Le Roux, and Y. Parmentier. 2004. The
Metagrammar Compiler: An NLP Application with
a Multi-paradigm Architecture. In 2nd International
Mozart/Oz Conference (MOZ?2004), Charleroi.
C. Gardent and L. Kallmeyer. 2003. Semantic con-
struction in FTAG. In Proceedings of EACL?03, Bu-
dapest.
C. Gardent and E. Kow. 2005. Generating and select-
ing grammatical paraphrases. ENLG, Aberdeen.
C. Gardent and Y. Parmentier. 2005. Large scale
semantic construction for tree adjoining grammars.
In Proceedings of The Fifth International Confer-
ence on Logical Aspects of Computational Linguis-
tics (LACL05).
C. Gardent. 2006. Inte?gration d?une dimension
se?mantique dans les grammaires d?arbres adjoints.
In Actes de la confe?rence TALN?2006 Leuven.
E. Kow. 2005. Adapting polarised disambiguation
to surface realisation. In 17th European Summer
School in Logic, Language and Information - ESS-
LLI?05, Edinburgh, UK, Aug.
S. Lehmann, S. Oepen, S. Regnier-Prost, K. Netter,
V. Lux, J. Klein, K. Falkedal, F. Fouvry, D. Estival,
E. Dauphin, H. Compagnion, J. Baur, L. Balkan, and
D. Arnold. 1996. TSNLP ? Test Suites for Natural
Language Processing. In Proceedings of COLING
1996, Kopenhagen.
P. Lopez. 1999. Analyse d?e?nonce?s oraux pour le dia-
logue homme-machine a` l?aide de grammaires lex-
icalise?es d?arbres. Ph.D. thesis, Universite? Henri
Poincare? ? Nancy 1.
P. Lopez. 2000. Extended Partial Parsing for
Lexicalized Tree Grammars. In Proceedings of
the International Workshop on Parsing Technology
(IWPT2000), Trento, Italy.
G. Neumann. 1994. A Uniform Computational
Model for Natural Language Parsing and Gener-
ation. Ph.D. thesis, University of the Saarland,
Saarbru?cken.
S. Oepen, E. Callahan, C. Manning, and K. Toutanova.
2002. Lingo redwoods?a rich and dynamic tree-
bank for hpsg.
Y. Parmentier and J. Le Roux. 2005. XMG: an Exten-
sible Metagrammatical Framework. In Proceedings
of the Student Session of the 17th European Summer
School in Logic, Language and Information, Edin-
burg, Great Britain, Aug.
E. Villemonte de la Clergerie. 2005. DyALog: a tabu-
lar logic programming based environment for NLP.
In Proceedings of CSLP?05, Barcelona.
XTAG-Research-Group. 2001. A lexical-
ized tree adjoining grammar for english.
Technical Report IRCS-01-03, IRCS, Uni-
versity of Pennsylvania. Available at
http://www.cis.upenn.edu/?xtag/gramrelease.html.
120
Proceedings of the 8th International Conference on Computational Semantics, pages 359?370,
Tilburg, January 2009.
c?2009 International Conference on Computational Semantics
Semantic Normalisation : a Framework and an Experiment
Paul Bedaride Claire Gardent
INRIA/LORIA CNRS/LORIA
Universit?e Henri Poincar?e, Nancy Nancy
paul.bedaride@loria.fr claire.gardent@loria.fr
Abstract
We present a normalisation framework for linguistic representations and illustrate its use
by normalising the Stanford Dependency graphs (SDs) produced by the Stanford parser into
Labelled Stanford Dependency graphs (LSDs). The normalised representations are evaluated
both on a testsuite of constructed examples and on free text. The resulting representations
improve on standard Predicate/Argument structures produced by SRL by combining role la-
belling with the semantically oriented features of SDs. Furthermore, the proposed normalisa-
tion framework opens the way to stronger normalisation processes which should be useful in
reducing the burden on inference.
1 Introduction
In automated text understanding, there is a tradeoff between the degree of abstraction provided
by the semantic representations used and the complexity of the logical or probabilistic reasoning
involved. Thus, a system that normalises syntactic passives as actives avoids having to reason
about equivalences between grammatical dependencies. Similarly, normalising phrasal synonyms
into their one word equivalent (e.g., take a turn for the worse/worsen) or converting the semantic
representation of deverbal nominals into their equivalent verbal representations (Caesar?s destruc-
tion of the city/Caesar destroyed the city) avoids having to reason with the corresponding lexical
axioms. In short, the better, semantic representations abstract away from semantically irrelevant
distinctions, the less reasoning needs to be performed.
In this paper, we investigate a normalisation approach and present a framework for normalising
linguistic representations which we apply to converting the dependency structures output by the
Stanford parser (henceforth, Stanford Dependencies or SDs) into labelled SD graphs (LSD) that
is, dependency graphs where grammatical relations have been converted to roles.
The LSD graphs we produce and the normalisation framework we present, provide an inter-
esting alternative both for the shallow Predicate/Argument structures produced by semantic role
labelling (SRL) systems and for the complex logical formulae produced by deep parsers.
Thus as we shall see in Section 2, labelled SDs are richer than the standard Predicate/Argument
structures produced by SRL in that (i) they indicate dependencies between all parts of a sentence,
1
359
not just the verb and its arguments
1
and (ii) they inherit the semantically oriented features of
SDs namely, a detailed set of dependencies, a precise account of noun phrases and a semantically
oriented treatment of role marking prepositions, of heads and of conjunctions.
Furthermore, the normalisation framework (formal system and methodology) we present, can
be extended to model and implement more advanced normalisation steps (e.g., deverbal/verbal and
phrasal/lexical synonym normalisation) thereby potentially supporting a stronger normalisation
process than the semantic role labelling already supported by SRL systems and by deep parsers.
In sum, although the normalised SDs presented in this paper, do not exhibit a stronger normal-
isation than that available in the Predicate/Argument structures already produced by deep parsers
and by SRL systems, we believe that they are interesting in their own right in that they combine
semantic role labelling with the semantic features of SDs. Moreover, the proposed normalisation
framework opens the way for a stronger normalisation process.
The paper is structured as follows. Section 2 presents the representations handled by the system
namely, the SD graphs and their labelled versions, the LSDs. Section 3 presents the rewriting
system used and explains how SDs are converted to LSDs. Section 4 reports on evaluation. Section
5 discusses related work and concludes with pointers for further research.
2 (Normalised) Stanford Dependency graphs
Stanford Dependency graphs. SD graphs are syntactic dependency graphs where nodes are
words and edges are labelled with syntactic relations. As detailed in [dMM06, dM08], SD graphs
differ from other dependency graphs in several ways. First, they involve an extensive set of 56
dependency relations. These relations are organised in a hierarchy thereby permitting underspec-
ifying the relation between a head and its dependent (by using a very generic relation such as
dependent). Second, in contrast to other relational schemes such as the GR [CMB99] and PARC
[KCR
+
03], NP-internal dependency relations are relatively fine-grained
2
thereby permitting a de-
tailed description of NPs internal structure and providing better support for an accurate definition
of their semantics. Third, heads are constrained to be content words i.e., noun, verbs, adjectives,
adverbs but also conjunctions. In particular, contrary to the GR scheme, SD graphs take copula
be to be a dependent rather than a head. Fourth, SD graphs are further simplified in that some
nodes may be collapsed. for instance, role marking prepositions are omitted and a trace kept of
that preposition in the dependency name (e.g., prep-on).
The practical adequacy of SD graphs and their ability to support shallow semantic reasoning
is attested by a relatively high number of experiments. Thus, in 2007, 5 out of the 21 systems
submitted to the RTE (Recognising Textual Entailment) challenge used the SD representations.
SDs have been used in bioinformatics for extracting relations between genes and proteins [EOR07,
CS07]. It has furthermore been used for opinion extraction [ZJZ06], sentence-level sentiment
analysis [MP07] and information extraction [BCS
+
07].
1
In the CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies [SJM
+
08], the aim is to
produce dependency structures labelled with predicate/argument relations. Although such structures are similar to the
LSD graphs we produce, there are differences both in the precise type of structures built and in how these are built. We
discuss this point in more detail in section 5.
2
e.g., appos for apposition, nn for noun-noun compounds, num for a numeric modifier and number for an element in
a compound number.
360
love
John
Mary
nsubj dobj
love
Mary
be John
nsubjpass
agent
auxpass
love
John
Mary
arg0 arg1
Figure 1: SDs and LSDs for ?John loves Mary? and ?Mary is loved by John?
Normalised Stanford Dependency graphs. From the SDs produced by the Stanford parser, we
produce labelled SDs where the syntactic relations between a verb and its arguments are replaced
by the roles. For instance, the SDs and LSDs for the sentences ?john loves mary? and ?mary is
loved by john? are as given in Figure 1. The roles used in LSDs are those used in the PropBank for
core- and adjunct-like arguments namely, A0, A1, A2, A3, A4, AM where AM covers all PropBank
adjunct tags such as AM-TMP, AL-LOC, etc..
As mentioned in the introduction, LSD graphs combine the advantages of SD graphs with se-
mantic role labelling. From semantic role labelling, they take the more semantic predicate/argument
relations. From SD graphs, they inherit the semantic oriented features such as the deletion of con-
tent poor function words, the rich hierarchy of NP internal relations and the detailed description of
the relations holding between words other than the verb and its arguments.
In short, LSD graphs are both more semantic than SD graphs and richer than SRL Predi-
cate/Argument structures.
3 Normalising dependency trees
To normalise the SD graphs, we extend the Stanford parser with a normalisation module de-
signed to translate the grammatical relations between a verb and its arguments into roles. This
normalisation module consists of an ordered set of rewrite rules and is defined semi-automatically
in a two-step procedure as follows.
First, the rewrite rules for transitive verbs are defined. This first step is done manually and is
based on the XTAG [Gro01] inventory of possible syntactic contexts for verbs of this type.
Second, further rewrite rules for verbs of other classes (ditransitive, verbs with sentential ar-
gument, verbs with one prepositional argument, etc.) are automatically derived from the set of
rewrite rules for transitive verbs and from a small set of ?base-form rewrite rules? manually de-
fined for each class. The rules are then lexicalised using the information contained in the PropBank
Frames
3
.
3
The PropBank Frames specify for each verb sense in PropBank, the arguments it accepts and the corresponding
semantic roles.
361
xy
z
x
y
z
nsubj dobj
arg0 arg1
x
y
z
t
x
z
y
nsubjpass
agent
auxpass
arg0 arg1
Figure 2: Rewriting rules for active and passive
3.1 Defining basic rewrite rules
In the first phase, we manually define a set of rewrite rules for each possible syntactic variation of
a transitive verb.
Using the XTAG Tree Adjoining Grammar [Gro01], we start by listing these variations. In-
deed a Tree Adjoining Grammar (TAG) lists the set of all possible syntactic configurations for
basic clauses and groups them into so-called (tree) families. Thus the Tnx0Vnx1 family is a set
of trees describing the possible syntactic contexts in which a transitive verb can occur. Further,
W1nx0Vnx1 names a tree in that family which describes a syntactic context in which a transitive
verb (nx0Vnx1) occurs together with a canonical nominal subject (nx0) and a questioned object
(W1). We use the XTAG families to produce a list of basic clauses illustrating the possible syntac-
tic variations of each verb type. For instance, using the Tnx0Vnx1 XTAG family, we create a ?list
of Tnx0Vnx1 sentences? i.e.,
(1) ?John loves Mary?,?Mary is loved by John?, ?Mary, John loves?, ?It is Mary who is loved
by John?, ?It is John who loves Mary?, ?Mary who is loved by John?, ?John who loves
Mary?,etc.
We then parse these sentences using the Stanford parser and retrieve the correct dependency struc-
ture from the output thus gathering the set of dependency structures associated by the Stanford
parser with the various syntactic realisations of a given verb type.
Finally, for each distinct dependency structure found, we define a rewrite rule which maps this
dependency structure onto a unique (canonical) semantic representation. For instance, the rewrite
rules for the active and passive form of a sentence featuring a transitive verb are as sketched in
Figure 2 (see below for the exact content of these rules).
To define our rewrite rules, we resort to a standard rewriting system namely GrGen [KG07].
Used in multiple domains (e.g., formal calculus, combinatoric algebra, operational semantics),
rewriting is a technique for modelling reduction and simplification. For instance, the rewriting rule
r
1
: x ? y+ x ? z ? x ? (y+ z) permits factorising 5 ? 6+ 5 ? 7+ 5 ? 8 to 5 ? ((6+ 7)+ 8). More
generally, a rewriting system consists of a set of rewriting rules of the form l ? r where l and r
are filtering and rewriting patterns respectively. Given an object o, such a rule will apply to o if o
362
rule nx0Vnx1 {
pattern{
verb:element;
if{verb.verb != "None";}
np0:element;
np1:element;
verb -:nsubj-> np0;
verb -:dobj-> np1;
}
replace {
verb -:arg0-> np0;
verb -:arg1-> np1;
}}
rule nx1Vbynx0 {
pattern{
verb:element;
if{verb.verb != "None";}
np1:element;
be:element;
np0:element;
verb -:nsubjpass-> np1;
verb -:auxpass-> be;
verb -:agent-> np0;
}
replace {
verb -:arg0-> np0;
verb -:arg1-> np1;
}}
Figure 3: Two rewrite rules in the GrGen format
satisfies the filtering pattern l. The result of applying a rule to an object o is o where the sub-part of
o matched by l is rewritten according to the rewriting pattern r. Matching consists in looking for
a homograph homomorphism between the pattern graph l and the host graph h while the allowed
rewriting operations include information duplication, deletion and addition
4
.
In GrGen, the objects handled by rewriting are attributed typed directed multigraphs. These
are directed graphs with typed nodes and edges, where between two nodes more than one edge of
the same type and direction is permitted. According to its type, each node or edge has a defined set
of attributes associated with it. Moreover, the type system suppports multiple inheritance on node
and edge types.
Expressive and efficient, GrGen
5
is well suited to specify our normalisation rules. For instance,
the rewrite rule sketched in figure 2 can be specified as given in Figure 3. The left handside (lhs) of
the rule specifies a pattern in terms of nodes, node attributes, edge labels and conditions on nodes.
The right handside specifies how to rewrite the subgraphs matched by the lhs.
More generally, the SD graphs can be seen as attributed typed directed multigraphs where
node attributes are words and edge labels are grammatical relations. Rewrite rules can then be
used to modify, add or duplicate information present in the dependency graphs to create predicate-
argument structures.
Typically, rewriting is not confluent (different rule application orders yield different results)and
GrGen supports various sophisticated control strategies. So far however, we simply used rule
sequencing : rules are tested and fired in the order in which they are listed. They are ordered by
specificity with the most specific rules listed first. For instance, the rule rewriting a long passive
will precede that for a short passive thereby preventing the short passive rule from applying to a
4
For a more precise definition of satisfaction, matching and replacement, we refer the reader to [EHK
+
99].
5
There are other rewriting systems available such as in particular, the Tsurgen system used in the Stanford Parser to
map parse trees into dependency graphs. We opted for GrGen instead because it fitted our requirements best. GrGen
is efficient, notationally expressive (for specifying graphs but also rules and rule application strategies) and comes with
a sophisticated debugging environment. Importantly, GrGen developers are also quick to react to questions and to
integrate proposed modifications.
363
long passive sentence.
We also use GrGen ?global rewriting mode?. This ensures that whenever the rule filtering pat-
tern matches several subgraphs in the input structures, the rewriting operates on each of the filtered
subgraph. As we shall see in section 3, our rewrite rules are applied on not one but 5 dependency
graphs at a time. Moreover the same rewrite rules may be applicable to several subgraphs in a
sentence analysis (typically when the sentence contains 2 or more verbs occurring in the same
syntactic configuration). Global rewriting thereby avoids having to iterate over the rule set.
3.2 Deriving new rewrite rules
Manually specifying the normalisation rules is time consuming and error prone. To extend the
approach to all types of verbs and syntactic configurations, we semi-automatically derive new
rewrite rules from existing ones.
Let us call source class, the syntactic class from which we derive new rules, target class, the
syntactic class for which rewrite rules are being derived and base-form rewrite rule, a rewrite
rule operating on a ?base-form? that is, either on an active, a passive or a short passive form
subcategorising for canonical (i.e., non extracted) arguments.
Now, let us define the set of primitive rewrite rules used to bootstrap the process as the set of
all rewrite rules defined for the source class together with the set of base-form rewrite rules defined
for the target class.
To derive new rules from the set of primitive rewrite rules, we start by computing the differ-
ences (in terms of edges, node and labels) between a source base-form rewrite rule (RR) and either
a target, base-form RR (DIFF
+arg
) or a source non base-form RR (DIFF
+movt
). We then use the
resulting DIFFs to compute new rewrite rules which differ either from a source RR by a DIFF
+arg
patch or from a target base-form RR by a DIFF
+movt
. Figure 4 illustrates the idea on a specific
example. The RR for a ditransitive verb with questioned object (?What does John put on the ta-
ble??, W1nx0Vnx1pnx2) is derived both by applying a DIFF
+W1
patch to the nx0Vnx1pnx2
active base-form RR (?John put a book on the table.?) and by applying a DIFF
+pnx2
patch to
the source RR operating on W1nx0Vnx1 verbs with questioned object (?Who does Mary love??).
Note that in this way, the same rewrite rule (W1nx0Vnx2nx1) is derived in two different ways
namely, from the W1nx0Vnx1 RR by applying a DIFF
+pnx2
patch and from the nx0Vnx1pnx2
RR by applying a DIFF
+W1
one. We use this double derivation process to check the approach
consistency and found that in all cases, the same rule is derived by both possible paths.
Using the method just sketched, we derived 377 rules from a set of 352 primitive rewrite rules.
Although the ratio might seem weak, automating the derivation of rewrite rules facilitates system
maintenance and extension. This is because whenever a correction in the set of primitive rewrite
rules is carried out, the change automatically propagates to the related derived rules. In practice,
we found that a real feature when adapting the system to the Propbank data. We believe that it will
also be useful when extending the system to deal with nominalisations.
4 Evaluation and discussion
We evaluated our normalisation method both on a testsuite of constructed examples and on real
world data namely, the Propbank corpus.
364
nx0Vnx1
nx0Vnx1pnx2
W1nx0Vnx1
W1nx0Vnx1pnx2
+pnx2
+pnx2
DIFF
arg
+W1 +W1
DIFF
mvt
Source RR
Target RR
Base Form RR
Figure 4: Deriving new rules from existing ones
4.1 Evaluation on a testsuite of constructed examples
This first evaluation aims to provide a systematic, fine grained assessment of how well the system
normalises each of the several syntactic configurations assigned by XTAG to distinct verb types.
The emphasis is here in covering the most exhaustive set of possible syntactic configurations possi-
ble. Because constructing the examples was intricate and time consuming, we did not cover all the
possibilities described by XTAG however. Instead we concentrated on listing all the configurations
specified by XTAG for 4 very distinct families namely, Tnx0Vnx1, Tnx0Vnx2nx1,Tnx0Vplnx1
and Tnx0Vnx1pnx2. The first class is the class for transitive verbs. Because of passive, this class
permits many distinct variations. The second class is the class of verbs with 3 nominal arguments.
This class is difficult for role labelling as the distinction between the complements often relies on
semantic rather than syntactic grounds. The third class is the class of verbs with a particle and 2
nominal arguments (ring up) and the fourth, the class of ditransitive.
For these constructed sentences, we had no gold standard i.e., no role annotation. Hence we
used logical inference to check normalisation. We proceeded by grouping the test items in (non)
entailment pairs and then checked whether the associated LSDs supported the detection of the
correct entailment relation (i.e., true or false).
The testsuite. Using a restricted lexicon, a set of clauses covering the possible syntactic patterns
of the four verb classes and regular expressions describing sentence-semantics pairs, we develop
a script generating (sentence,semantics) pairs where sentences contain one or more clauses. After
having manually verified the correctness of the generated pairs, we used them to construct textual
entailment testsuite items that is, pairs of sentences annotated with TRUE or FALSE dependending
on whether the two sentences are related by entailment (TRUE) or not (FALSE). The resulting
testsuite
6
contains 4 976 items of which 2 335 are entailments between a sentence and a clause
(1V+TE, example 2), 1 019 between two complex sentences (2V+TE, example 3) and 1 622 are
non-entailments (V-TE, example 4).
(2) T
1
: John likes the book that Mary put on the table.
T
2
: John likes a book
Annotations: 1V+TE, TRUE
6
Available at http://www.loria.fr/
?
bedaride/publications/taln08-bedgar/index.html.
365
(3) T
1
: John likes the book that Mary put on the table.
T
2
: The book which is put on the table by Mary, is liked by John
Annotations: 2V+TE, TRUE
(4) T
1
: John likes the book that Mary put on the table.
T
2
: John likes a table
Annotations: V-TE, FALSE
Checking for entailment. For each testsuite item, we then checked for entailment by translating
LSDs into FOL formulae and checking entailment between the first five LSDs derived from the
parser output for the sentences contained in the testsuite item.
The translation of a LSD into a FOL formula is done as follows. Each node is associated with
an existentially quantified variable and a predication over that variable where the predicate used is
the word labelling the node. Each edge translates to a binary relation between the source and the
target node variables. The overall formula associated with an LSD is then the conjunction of the
predications introduced by each node. For instance, for the LSD given in Figure 1, the resulting
formula is ?x, y, z : love(x) ? john(y) ?mary(z) ? arg0(x, y) ? arg1(x, z).
This translation procedure is of course very basic. Nonetheless, because the testsuite builds
on a restricted syntax and vocabulary
7
, it suffices to check how well the normalisation process
succeeds in assigning syntactic variants the same semantic representation.
Results. The test procedure just described is applied to the LSD graphs produced by the normal-
isation module on the testsuite items. Table 5 gives the results. For each class of testsuite items
(1V+TE, 2V+TE, V-TE), we list the percentage of cases recognised by the system as entailment
(+TE) and non entailment (-TE). Because FOL is only semi-decidable, the reasoners do not always
return an answer. The Failure line gives the number of cases for which the reasoners fail.
The results on positive entailments (1V+TE,2V+TE) show that the proposed normalisation
method is generally successful in recognising syntax based entailments with an overall average
precision of 86.3% (and a breakdown of 94.9% for 1V+TE and 66.6% for 2V+TE cases). Impor-
tantly, the results on negative entailments (99.2% overall precision) show that the method is not
overly permissive and does not conflate semantically distinct structures. Finally, it can be seen that
the results degrade for the Tnx0Vnx2nx1 class (John gave Mary a book). This is due mainly to
genuine syntactic ambiguities which cannot be resolved without further semantic (usually ontolog-
ical) knowledge. For instance, both The book which John gave the woman and The woman whom
John gave the book are assigned the same dependency structures by the Stanford parser. Hence
the same rewrite rule applies to both structures and necessarily assigns one of them the wrong
labelling. Other sources of errors are cases where the DIFF patch used to derive a new rule fail
to adequately generalise to the target structure. In such cases, the erroneous rewrite rule can be
modified manually.
7
In particular, the testsuite contains no quantifiers.
366
family ans 1V+TE 2V+TE V-TE
+TE 585 (98.2%) 212 (72.4%) 0 (0.0%)
Tnx0Vnx1 -TE 11 (1.8%) 79 (27.0%) 57 (100.0%)
Failure 0 (0.0%) 2 (0.6%) 0 (0.0%)
+TE 513 (89.2%) 131 (55.7%) 3 (0.4%)
Tnx0Vnx2nx1 -TE 61 (10.6%) 103 (43.8%) 703 (99.6%)
Failure 1 (0.2%) 1 (0.5%) 0 (0.0%)
+TE 567 (95.5%) 169 (67.9%) 0 (0.0%)
Tnx0Vplnx1 -TE 27 (4.5%) 79 (31.7%) 198 (100.0%)
Failure 0 (0.0%) 1 (0.4%) 0 (0.0%)
+TE 550 (96.5%) 167 (69.0%) 10 (1.5%)
Tnx0Vnx1pnx2 -TE 16 (2.8%) 69 (28.5%) 651 (98.5%)
Failure 4 (0.7%) 6 (2.5%) 0 (0.0%)
+TE 2215 (94.9%) 679 (66.6%) 13 (0.8%)
all -TE 115 (4.9%) 330 (32.4%) 1609 (99.2%)
Failure 5 (0.2%) 10 (1.0%) 0 (0.0%)
Figure 5: Precision on constructed examples. Each cell gives the proportion of cases recognised as
entailment by the system. Bold face figures give the precision i.e., the proportion of answers given
by the system that are correct.
4.2 Evaluation on the PropBank
The PropBank (Proposition Bank) was created by semantic annotation of the Wall Street Journal
section of Treebank-2. Each verb occurring in the Treebank has been treated as a semantic pred-
icate and the surrounding text has been annotated for arguments and adjuncts of the predicate as
illustrated in (5).
(5) [A0 He ] [AM-MOD would ] [AM-NEG n?t ] [V accept ] [A1 anything of value ] from [A2
those he was writing about ] .
The labels used for the core and adjunct-like arguments are the following
8
. The labels A0 .. A5
designate arguments associated with a verb predicate as defined in the PropBank Frames scheme.
A0 is the agent, A1 the patient or the theme. For A2 to A5 no consistent generalisation can be
made and the annotation reflects the decisions made when defining the PropBank Frames scheme.
Further, the AM-T label describes adjunct like arguments of various sorts, where T is the type of
the adjunct. Types include locative, temporal, manner, etc.
We used the PropBank to evaluate our normalisation procedure on free text. As in the CoNLL
(Conference on Natural Language Learning) shared task for SRL, the evaluation metrics used
are precision, recall and F measure. An argument is said to be correctly recognised if the words
spanning the argument as well as its semantic role match the PropBank annotation. Precision is
the proportion of arguments predicted by a system which are correct. Recall is the proportion of
correct arguments which are predicted by a system. F-measure is the harmonic mean of precision
and recall. The results are given below.
8
This is in fact simplified. The PropBank corpus additionally provide information about R-* arguments (a reference
such as a trace to some other argument of A* type) and C-* arguments (a continuation phrase in a split argument).
367
args 0 1 2 3 4 5 a m total
recall 68.4% 68.2% 62.4% 47.2% 57.6% 5.3% 0.0% 64.4% 66.1%
precision 88.0% 80.2% 76.4% 83.1% 83.3% 50.0% ? 75.0% 80.6%
f-mesure 77.0% 73.7% 68.7% 60.2% 68.1% 9.5% ? 69.3% 72.6%
Precision (80.6%) is comparable to the results obtained in the ConLL 2005 SRL shared task
where the top 8 systems have an average precision ranging from 76.55% to 82.28%. Recall is
generally a little low (the ConLL05 recall ranged from 64.99% to 76.78%) for mainly two reasons:
either the Stanford parser, did not deliver the correct analysis or the required rewrite rule was not
present.
5 Conclusion
Our approach is akin to so-called semantic role labelling (SRL) approaches [CM05] and to sev-
eral rewriting approaches developed to modify parsing output in RTE systems [Ass07]. It differs
from the SRL approaches in that unlike most SRLs systems, it is based on a hybrid, statistic and
symbolic, framework. As a result, improving or extending the system can be done independently
of the availability of an appropriately annotated corpus. However, the quality, performance and
coverage of the system remains dependent on those of the Stanford parser
9
,
Our approach also differs from approaches that use the lambda calculus to normalise syntactic
variation. In such approaches, a compositional semantics module associates words and grammar
rules or derivation structures with lambda terms which in effect normalise variations such as for
instance, the active/passive variation. One important advantage of lambda based approaches is that
the rewriting system is confluent. The drawback however is that the specification of the appropriate
lambda terms requires expert linguistic skills. In contrast, the rewrite rule approach is compara-
tively easier to handle (the rules presented here were developed by a computer scientist) and its
use is supported by sophisticated developing environments such as GrGen which provides strong
notational expressivity (the rewrite rules can include conditions, can operate on graphs of arbi-
trary depth, etc. ), a good debugging environment and good processing times. In short although,
the lambda calculus approach is undoubtly more principled, the rewrite rule approach is arguably
easier to handle and easier to understand.
Normalisation of linguistic representations is not new. It is used in particular, in [BCC
+
07,
DBBT07, RTF07] for dealing with entailment detection in the RTE (Recognising Textual Entail-
ment) challenge. The approach we present here differs from these approaches both by its sys-
tematic treatment of syntactic variation and by its use of GrGen as a framework for specifying
transformations. More generally, our approach emphasises the following three points namely (i)
the systematic testing of all possible syntactic variations (based on the information contained in
XTAG); (ii) the use of an expressive, efficient and well-understood graph rewriting system for
defining transformations; and (iii) the development of a methodology for automatically deriving
new rewrite rules from existing ones.
By providing a well-defined framework for specifying, deriving and evaluating rewrite rules,
we strive to develop a system that normalises NLP representations in a way that best supports
9
[KM03] report a label F-mesure of 86.3% on section 23 of the Penn Treebank.
368
semantic processing. The emphasis is on aligning Predicate/Argument structures that diverge in
the surface text but that are semantically similar (e.g., John buy a car from Peter/Peter sells a car
to John). In particular, we plan to extend the system to normalise nominal dependencies (using
NomBank) and converse constructions.
References
[Ass07] Association for Computational Linguistics. Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing, Prague, Czech Republic, June 2007.
[BCC
+
07] D. G. Bobrow, C. Condoravdi, R. S. Crouch, V. de Paiva, L. Karttunen, T. H. King,
R. Nairn, L. Price, and A. Zaenen. Precision-focused textual inference. In ACL-
PASCAL Workshop on Textual Entailment and Paraphrasing, pages 16?21, Prague,
Czech Republic, June 2007.
[BCS
+
07] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open in-
formation extraction from the web. In IJCAI ?07: Proceedings of International Joint
Conference on Artificial Intelligence, pages 2670?2676, Hyderabad, India, January
2007.
[CM05] X. Carreras and L. Marquez. Introduction to the conll-2005 shared task: Semantic role
labeling. In Proceedings of the CoNLL-2005 Shared Task: Semantic Role Labeling,
pages 152?164, Ann Arbor, Michigan, June 2005.
[CMB99] J. Carroll, G. Minnen, and T. Briscoe. Corpus annotation for parser evaluation. In
EACL Workshop on Linguistically Interpreted Corpora, Bergen, Norway, June 1999.
[CS07] A. B. Clegg and A. J. Shepherd. Benchmarking natural-language parsers for biological
applications using dependency graphs. BMC Bioinformatics, 8:24, January 2007.
[DBBT07] R. Delmonte, A. Bristot, M. A. P. Boniforti, and S. Tonelli. Entailment and anaphora
resolution in rte3. In ACL-PASCAL Workshop on Textual Entailment and Paraphras-
ing, pages 48?53, Prague, Czech Republic, June 2007.
[dM08] M.-C. de Marneffe and C. D. Manning. The stanford typed dependencies representa-
tions. In COLING?08 Workshop on Cross-framework and Cross-domain Parser Eval-
uation, Manchester, England, August 2008.
[dMM06] Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. Gener-
ating typed dependency parses from phrase structure parses. In LREC ?06: Proceed-
ings of 5th International Conference on Language Resources and Evaluation, pages
449?454, Genoa, Italy, May 2006.
[EHK
+
99] H. Ehrig, R. Heckel, M. Korff, Loewe M., L. Ribeiro, A. Wagner, and A. Corradini.
Handbook of Graph Grammars and Computing by Graph Transformation., volume 1,
chapter Algebraic Approaches to Graph Transformation - Part II: Single Pushout A.
and Comparison with Double Pushout A, pages 247?312. World Scientific, 1999.
369
[EOR07] G. Erkan, A. Ozgur, and D. R. Radev. Semi-supervised classification for extracting
protein interaction sentences using dependency parsing. In EMNLP-CoNLL ?07: Pro-
ceedings of the 2007 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 228?237, Prague,
Czech Republic, June 2007. Association for Computational Linguistics.
[Gro01] XTAG Research Group. A lexicalized tree adjoining grammar for english. Technical
Report IRCS-01-03, IRCS, University of Pennsylvania, 2001.
[KCR
+
03] T. King, R. Crouch, S. Riezler, M. Dalrymple, and R. Kaplan. The parc 700 depen-
dency bank. In EACL workshop on Linguistically Interpreted Corpora, Budapest,
Hungary, April 2003.
[KG07] M. Kroll and R. Gei?. Developing graph transformations with grgen.net. Technical
report, October 2007. preliminary version, submitted to AGTIVE 2007.
[KM03] D. Klein and C. D. Manning. Accurate unlexicalized parsing. In ACL ?03: Proceedings
of the 41st Annual Meeting of the Association for Computational Linguistics, pages
423?430, Sapporo, Japan, July 2003. Association for Computational Linguistics.
[MP07] A. Meena and T. V. Prabhakar. Sentence level sentiment analysis in the presence
of conjuncts using linguistic analysis. In Ecir ?07: Proceedings of 29th European
Conference on Information Retrieval, pages 573?580, Rome, Italy, April 2007.
[RTF07] A. B. N. Reiter, S. Thater, and A. Frank. A semantic approach to textual entailment:
System evaluation and task analysis. In ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 10?15, Prague, Czech Republic, June 2007.
[SJM
+
08] M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and J. Nivre. The CoNLL-2008
shared task on joint parsing of syntactic and semantic dependencies. In CoNLL ?08:
Proceedings of the 12th Conference on Computational Natural Language Learning,
pages 159?177, Manchester, UK, August 2008.
[ZJZ06] L. Zhuang, F. Jing, and X.-Y. Zhu. Movie review mining and summarization. In
CIKM ?06: Proceedings of the 15th ACM international conference on Information
and knowledge management, pages 43?50, Arlington, Virginia, USA, November 2006.
ACM.
370
Generating with a Grammar Based on Tree Descriptions: a
Constraint-Based Approach
Claire Gardent
CNRS
LORIA, BP 239 Campus Scientifique
54506 Vandoeuvre-les-Nancy, France
claire.gardent@loria.fr
Stefan Thater
Computational Linguistics
Universita?t des Saarlandes
Saarbru?cken, Germany
stth@coli.uni-sb.de
Abstract
While the generative view of language
processing builds bigger units out of
smaller ones by means of rewriting
steps, the axiomatic view eliminates in-
valid linguistic structures out of a set of
possible structures by means of well-
formedness principles. We present a
generator based on the axiomatic view
and argue that when combined with a
TAG-like grammar and a flat seman-
tics, this axiomatic view permits avoid-
ing drawbacks known to hold either of
top-down or of bottom-up generators.
1 Introduction
We take the axiomatic view of language and show
that it yields an interestingly new perspective on
the tactical generation task i.e. the task of produc-
ing from a given semantics
 
a string with seman-
tics
 
.
As (Cornell and Rogers, To appear) clearly
shows, there has recently been a surge of interest
in logic based grammars for natural language. In
this branch of research sometimes referred to as
?Model Theoretic Syntax?, a grammar is viewed
as a set of axioms defining the well-formed struc-
tures of natural language.
The motivation for model theoretic grammars
is initially theoretical: the use of logic should sup-
port both a more precise formulation of grammars
and a different perspective on the mathematical
and computational properties of natural language.
But eventually the question must also be ad-
dressed of how such grammars could be put to
work. One obvious answer is to use a model gen-
erator. Given a logical formula
 
, a model genera-
tor is a program which builds some of the models
satisfying this formula. Thus for parsing, a model
generator can be used to enumerate the (minimal)
model(s), that is, the parse trees, satisfying the
conjunction of the lexical categories selected on
the basis of the input string plus any additional
constraints which might be encoded in the gram-
mar. And similarly for generation, a model gener-
ator can be used to enumerate the models satisfy-
ing the bag of lexical items selected by the lexical
look up phase on the basis of the input semantics.
How can we design model generators which
work efficiently on natural language input i.e. on
the type of information delivered by logic based
grammars? (Duchier and Gardent, 1999) shows
that constraint programming can be used to im-
plement a model generator for tree logic (Back-
ofen et al, 1995). Further, (Duchier and Thater,
1999) shows that this model generator can be used
to parse with descriptions based grammars (Ram-
bow et al, 1995; Kallmeyer, 1999) that is, on
logic based grammars where lexical entries are
descriptions of trees expressed in some tree logic.
In this paper, we build on (Duchier and Thater,
1999) and show that modulo some minor modi-
fications, the same model generator can be used
to generate with description based grammars.
We describe the workings of the algorithm and
compare it with standard existing top-down and
bottom-up generation algorithms. In specific, we
argue that the change of perspective offered by
the constraint-based, axiomatic approach to pro-
cessing presents some interesting differences with
the more traditional generative approach usually
pursued in tactical generation and further, that the
combination of this static view with a TAG-like
grammar and a flat semantics results in a system
which combines the positive aspects of both top-
down and bottom-up generators.
The paper is structured as follows. Sec-
tion 2 presents the grammars we are working
with namely, Description Grammars (DG), Sec-
tion 3 summarises the parsing model presented in
(Duchier and Thater, 1999) and Section 4 shows
that this model can be extended to generate with
DGs. In Section 5, we compare our generator
with top-down and bottom-up generators, Section
6 reports on a proof-of-concept implementation
and Section 7 concludes with pointers for further
research.
2 Description Grammars
There is a range of grammar formalisms which
depart from Tree Adjoining Grammar (TAG) by
taking as basic building blocks tree descriptions
rather than trees. D-Tree Grammar (DTG) is pro-
posed in (Rambow et al, 1995) to remedy some
empirical and theoretical shortcomings of TAG;
Tree Description Grammar (TDG) is introduced
in (Kallmeyer, 1999) to support syntactic and se-
mantic underspecification and Interaction Gram-
mar is presented in (Perrier, 2000) as an alterna-
tive way of formulating linear logic grammars.
Like all these frameworks, DG uses tree de-
scriptions and thereby benefits first, from the ex-
tended domain of locality which makes TAG par-
ticularly suitable for generation (cf. (Joshi, 1987))
and second, from the monotonicity which differ-
entiates descriptions from trees with respect to ad-
junction (cf. (Vijay-Shanker, 1992)).
DG differs from DTG and TDG however in
that it adopts an axiomatic rather than a genera-
tive view of grammar: whereas in DTG and TDG,
derived trees are constructed through a sequence
of rewriting steps, in DG derived trees are mod-
els satisfying a conjunction of elementary tree de-
scriptions. Moreover, DG differs from Interaction
Grammars in that it uses a flat rather than a Mon-
tague style recursive semantics thereby permitting
a simple syntax/semantics interface (see below).
A Description Grammar is a set of lexical en-
tries of the form 	 where  is a tree descrip-
tion and  is the semantic representation associ-
ated with  .
Tree descriptions. A tree description is a con-
junction of literals that specify either the label
of a node or the position of a node relative to

  NP: 
John

Generating Minimal Definite Descriptions
Claire Gardent
CNRS, LORIA, Nancy
gardent@loria.fr
Abstract
The incremental algorithm introduced in
(Dale and Reiter, 1995) for producing dis-
tinguishing descriptions does not always
generate a minimal description. In this
paper, I show that when generalised to
sets of individuals and disjunctive proper-
ties, this approach might generate unnec-
essarily long and ambiguous and/or epis-
temically redundant descriptions. I then
present an alternative, constraint-based al-
gorithm and show that it builds on existing
related algorithms in that (i) it produces
minimal descriptions for sets of individu-
als using positive, negative and disjunctive
properties, (ii) it straightforwardly gener-
alises to n-ary relations and (iii) it is inte-
grated with surface realisation.
1 Introduction
In English and in many other languages, a possible
function of definite descriptions is to identify a set
of referents1 : by uttering an expression of the form
The N, the speaker gives sufficient information to the
hearer so that s/he can identify the set of the objects
the speaker is referring to.
From the generation perspective, this means that,
starting from the set of objects to be described and
from the properties known to hold of these objects
by both the speaker and the hearer, a definite de-
scription must be constructed which allows the user
1The other well-known function of a definite is to inform the
hearer of some specific attributes the referent of the NP has.
to unambiguously identify the objects being talked
about.
While the task of constructing singular definite
descriptions on the basis of positive properties has
received much attention in the generation literature
(Dale and Haddock, 1991; Dale and Reiter, 1995;
Horacek, 1997; Krahmer et al, 2001), for a long
time, a more general statement of the task at hand re-
mained outstanding. Recently however, several pa-
pers made a step in that direction. (van Deemter,
2001) showed how to extend the basic Dale and Re-
iter Algorithm (Dale and Reiter, 1995) to generate
plural definite descriptions using not just conjunc-
tions of positive properties but also negative and
disjunctive properties; (Stone, 1998) integrates the
D&R algorithm into the surface realisation process
and (Stone, 2000) extends it to deal with collective
and distributive plural NPs.
Notably, in all three cases, the incremental struc-
ture of the D&R?s algorithm is preserved: the al-
gorithm increments a set of properties till this set
uniquely identifies the target set i.e., the set of ob-
jects to be described. As (Garey and Johnson, 1979)
shows, such an incremental algorithm while be-
ing polynomial (and this, together with certain psy-
cholinguistic observations, was one of the primary
motivation for privileging this incremental strategy)
is not guaranteed to find the minimal solution i.e.,
the description which uniquely identifies the target
set using the smallest number of atomic properties.
In this paper, I argue that this characteristic of the
incremental algorithm while reasonably innocuous
when generating singular definite descriptions using
only conjunctions of positive properties, renders it
                 Computational Linguistics (ACL), Philadelphia, July 2002, pp. 96-103.
                         Proceedings of the 40th Annual Meeting of the Association for
cognitively inappropriate when generalised to sets of
individuals and disjunctive properties. I present an
alternative approach which always produce the min-
imal description thereby avoiding the shortcomings
of the incremental algorithm. I conclude by com-
paring the proposed approach with related proposals
and giving pointers for further research.
2 The incremental approach
Dale and Reiter?s incremental algorithm (cf. Fig-
ure 1) iterates through the properties of the target
entity (the entity to be described) selecting a prop-
erty, adding it to the description being built and com-
puting the distractor set i.e., the set of elements for
which the conjunction of properties selected so far
holds. The algorithm succeeds (and returns the se-
lected properties) when the distractor set is the sin-
gleton set containing the target entity. It fails if all
properties of the target entity have been selected and
the distractor set contains more than the target entity
(i.e. there is no distinguishing description for the
target).
This basic algorithm can be refined by ordering
properties according to some fixed preferences and
thereby selecting first e.g., some base level category
in a taxonomy, second a size attribute third, a colour
attribute etc.
 
: the domain;

, the set of properties of  ;
To generate the UID   , do:
1. Initialise:  :=   ,   :=  .
2. Check success:
If 		
 return  
elseif   then fail
else goto step 3.
3. Choose property   which picks out the smallest set



ffProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 328?335,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Symbolic Approach to Near-Deterministic Surface Realisation using Tree
Adjoining Grammar
Claire Gardent
CNRS/LORIA
Nancy, France
claire.gardent@loria.fr
Eric Kow
INRIA/LORIA/UHP
Nancy, France
eric.kow@loria.fr
Abstract
Surface realisers divide into those used in
generation (NLG geared realisers) and those
mirroring the parsing process (Reversible re-
alisers). While the first rely on grammars not
easily usable for parsing, it is unclear how
the second type of realisers could be param-
eterised to yield from among the set of pos-
sible paraphrases, the paraphrase appropri-
ate to a given generation context. In this pa-
per, we present a surface realiser which com-
bines a reversible grammar (used for pars-
ing and doing semantic construction) with a
symbolic means of selecting paraphrases.
1 Introduction
In generation, the surface realisation task consists in
mapping a semantic representation into a grammati-
cal sentence.
Depending on their use, on their degree of non-
determinism and on the type of grammar they as-
sume, existing surface realisers can be divided into
two main categories namely, NLG (Natural Lan-
guage Generation) geared realisers and reversible
realisers.
NLG geared realisers are meant as modules in a
full-blown generation system and as such, they are
constrained to be deterministic: a generation system
must output exactly one text, no less, no more. In or-
der to ensure this determinism, NLG geared realisers
generally rely on theories of grammar which sys-
tematically link form to function such as systemic
functional grammar (SFG, (Matthiessen and Bate-
man, 1991)) and, to a lesser extent, Meaning Text
Theory (MTT, (Mel?cuk, 1988)). In these theories, a
sentence is associated not just with a semantic rep-
resentation but with a semantic representation en-
riched with additional syntactic, pragmatic and/or
discourse information. This additional information
is then used to constrain the realiser output.1 One
drawback of these NLG geared realisers however, is
that the grammar used is not usually reversible i.e.,
cannot be used both for parsing and for generation.
Given the time and expertise involved in developing
a grammar, this is a non-trivial drawback.
Reversible realisers on the other hand, are meant
to mirror the parsing process. They are used on a
grammar developed for parsing and equipped with a
compositional semantics. Given a string and such
a grammar, a parser will assign the input string
all the semantic representations associated with that
string by the grammar. Conversely, given a seman-
tic representation and the same grammar, a realiser
will assign the input semantics all the strings as-
sociated with that semantics by the grammar. In
such approaches, non-determinism is usually han-
dled by statistical filtering: treebank induced prob-
abilities are used to select from among the possible
paraphrases, the most probable one. Since the most
probable paraphrase is not necessarily the most ap-
propriate one in a given context, it is unclear how-
ever, how such realisers could be integrated into a
generation system.
In this paper, we present a surface realiser which
1On the other hand, one of our reviewers noted that ?de-
terminism? often comes more from defaults when input con-
straints are not supplied. One might see these realisers as being
less deterministic than advertised; however, the point is that it
is possible to supply the constraints that ensure determinism.
328
combines reversibility with a symbolic approach to
determinism. The grammar used is fully reversible
(it is used for parsing) and the realisation algorithm
can be constrained by the input so as to ensure a
unique output conforming to the requirement of a
given (generation) context. We show both that the
grammar used has a good paraphrastic power (it
is designed in such a way that grammatical para-
phrases are assigned the same semantic representa-
tions) and that the realisation algorithm can be used
either to generate all the grammatical paraphrases of
a given input or just one provided the input is ade-
quately constrained.
The paper is structured as follows. Section 2 in-
troduces the grammar used namely, a Feature Based
Lexicalised Tree Adjoining Grammar enriched with
a compositional semantics. Importantly, this gram-
mar is compiled from a more abstract specification
(a so-called ?meta-grammar?) and as we shall see, it
is this feature which permits a natural and system-
atic coupling of semantic literals with syntactic an-
notations. Section 3 defines the surface realisation
algorithm used to generate sentences from semantic
formulae. This algorithm is non-deterministic and
produces all paraphrases associated by the gram-
mar with the input semantics. We then go on to
show (section 4) how this algorithm can be used
on a semantic input enriched with syntactic or more
abstract control annotations and further, how these
annotations can be used to select from among the
set of admissible paraphrases precisely these which
obey the constraints expressed in the added annota-
tions. Section 5 reports on a quantitative evaluation
based on the use of a core tree adjoining grammar
for French. The evaluation gives an indication of the
paraphrasing power of the grammar used as well as
some evidence of the deterministic nature of the re-
aliser. Section 6 relates the proposed approach to
existing work and section 7 concludes with pointers
for further research.
2 The grammar
We use a unification based version of LTAG namely,
Feature-based TAG. A Feature-based TAG (FTAG,
(Vijay-Shanker and Joshi, 1988)) consists of a set
of (auxiliary or initial) elementary trees and of two
tree composition operations: substitution and ad-
junction. Initial trees are trees whose leaves are la-
belled with substitution nodes (marked with a dow-
narrow) or terminal categories. Auxiliary trees are
distinguished by a foot node (marked with a star)
whose category must be the same as that of the root
node. Substitution inserts a tree onto a substitution
node of some other tree while adjunction inserts an
auxiliary tree into a tree. In an FTAG, the tree nodes
are furthermore decorated with two feature struc-
tures (called top and bottom) which are unified dur-
ing derivation as follows. On substitution, the top
of the substitution node is unified with the top of the
root node of the tree being substituted in. On adjunc-
tion, the top of the root of the auxiliary tree is uni-
fied with the top of the node where adjunction takes
place; and the bottom features of the foot node are
unified with the bottom features of this node. At the
end of a derivation, the top and bottom of all nodes
in the derived tree are unified.
To associate semantic representations with natu-
ral language expressions, the FTAG is modified as
proposed in (Gardent and Kallmeyer, 2003).
NPj
John
name(j,john)
S
NP?s VPr
V
runs
run(r,s)
VPx
often VP*
often(x)
? name(j,john), run(r,j), often(r)
Figure 1: Flat Semantics for ?John often runs?
Each elementary tree is associated with a flat se-
mantic representation. For instance, in Figure 1,2
the trees for John, runs and often are associated with
the semantics name(j,john), run(r,s) and often(x) re-
spectively.
Importantly, the arguments of a semantic functor
are represented by unification variables which occur
both in the semantic representation of this functor
and on some nodes of the associated syntactic tree.
For instance in Figure 1, the semantic index s oc-
curring in the semantic representation of runs also
occurs on the subject substitution node of the asso-
ciated elementary tree.
2Cx/Cx abbreviate a node with category C and a top/bottom
feature structure including the feature-value pair { index : x}.
329
The value of semantic arguments is determined by
the unifications resulting from adjunction and sub-
stitution. For instance, the semantic index s in the
tree for runs is unified during substitution with the
semantic indices labelling the root nodes of the tree
for John. As a result, the semantics of John often
runs is
(1) {name(j,john),run(r,j),often(r)}
The grammar used describes a core fragment of
French and contains around 6 000 elementary trees.
It covers some 35 basic subcategorisation frames
and for each of these frames, the set of argument re-
distributions (active, passive, middle, neuter, reflex-
ivisation, impersonal, passive impersonal) and of ar-
gument realisations (cliticisation, extraction, omis-
sion, permutations, etc.) possible for this frame. As
a result, it captures most grammatical paraphrases
that is, paraphrases due to diverging argument real-
isations or to different meaning preserving alterna-
tion (e.g., active/passive or clefted/non-clefted sen-
tence).
3 The surface realiser, GenI
The basic surface realisation algorithm used is a bot-
tom up, tabular realisation algorithm (Kay, 1996)
optimised for TAGs. It follows a three step strat-
egy which can be summarised as follows. Given an
empty agenda, an empty chart and an input seman-
tics ?:
Lexical selection. Select all elementary trees
whose semantics subsumes (part of) ?. Store
these trees in the agenda. Auxiliary trees
devoid of substitution nodes are stored in a
separate agenda called the auxiliary agenda.
Substitution phase. Retrieve a tree from the
agenda, add it to the chart and try to combine it
by substitution with trees present in the chart.
Add any resulting derived tree to the agenda.
Stop when the agenda is empty.
Adjunction phase. Move the chart trees to the
agenda and the auxiliary agenda trees to the
chart. Retrieve a tree from the agenda, add it
to the chart and try to combine it by adjunction
with trees present in the chart. Add any result-
ing derived tree to the agenda. Stop when the
agenda is empty.
When processing stops, the yield of any syntacti-
cally complete tree whose semantics is ? yields an
output i.e., a sentence.
The workings of this algorithm can be illustrated
by the following example. Suppose that the input se-
mantics is (1). In a first step (lexical selection), the
elementary trees selected are the ones for John, runs,
often. Their semantics subsumes part of the input se-
mantics. The trees for John and runs are placed on
the agenda, the one for often is placed on the auxil-
iary agenda.
The second step (the substitution phase) consists
in systematically exploring the possibility of com-
bining two trees by substitution. Here, the tree for
John is substituted into the one for runs, and the re-
sulting derived tree for John runs is placed on the
agenda. Trees on the agenda are processed one by
one in this fashion. When the agenda is empty, in-
dicating that all combinations have been tried, we
prepare for the next phase.
All items containing an empty substitution node
are erased from the chart (here, the tree anchored by
runs). The agenda is then reinitialised to the content
of the chart and the chart to the content of the aux-
iliary agenda (here often). The adjunction phase
proceeds much like the previous phase, except that
now all possible adjunctions are performed. When
the agenda is empty once more, the items in the chart
whose semantics matches the input semantics are se-
lected, and their strings printed out, yielding in this
case the sentence John often runs.
4 Paraphrase selection
The surface realisation algorithm just sketched is
non-deterministic. Given a semantic formula, it
might produce several outputs. For instance, given
the appropriate grammar for French, the input in (2a)
will generate the set of paraphrases partly given in
(2b-2k).
(2) a. lj :jean(j) la:aime(e,j,m) lm:marie(m)
b. Jean aime Marie
c. Marie est aime?e par Jean
d. C?est Jean qui aime Marie
e. C?est Jean par qui Marie est aime?e
f. C?est par Jean qu?est aime?e Marie
g. C?est Jean dont est aime?e Marie
h. C?est Jean dont Marie est aime?e
i. C?est Marie qui est aime?e par Jean
330
j. C?est Marie qu?aime Jean
k. C?est Marie que Jean aime
To select from among all possible paraphrases of
a given input, exactly one paraphrase, NLG geared
realisers use symbolic information to encode syn-
tactic, stylistic or pragmatic constraints on the out-
put. Thus for instance, both REALPRO (Lavoie and
Rambow, 1997) and SURGE (Elhadad and Robin,
1999) assume that the input associates semantic lit-
erals with low level syntactic and lexical informa-
tion mostly leaving the realiser to just handle in-
flection, word order, insertion of grammatical words
and agreement. Similarly, KPML (Matthiessen and
Bateman, 1991) assumes access to ideational, inter-
personal and textual information which roughly cor-
responds to semantic, mood/voice, theme/rheme and
focus/ground information.
In what follows, we first show that the semantic
input assumed by the realiser sketched in the previ-
ous section can be systematically enriched with syn-
tactic information so as to ensure determinism. We
then indicate how the satisfiability of this enriched
input could be controlled.
4.1 At most one realisation
In the realisation algorithm sketched in Section 3,
non-determinism stems from lexical ambiguity:3 for
each (combination of) literal(s) l in the input there
usually is more than one TAG elementary tree whose
semantics subsumes l. Thus each (combination of)
literal(s) in the input selects a set of elementary
trees and the realiser output is the set of combi-
nations of selected lexical trees which are licensed
by the grammar operations (substitution and adjunc-
tion) and whose semantics is the input.
One way to enforce determinism consists in en-
suring that each literal in the input selects exactly
one elementary tree. For instance, suppose we want
to generate (2b), repeated here as (3a), rather than
3Given two TAG trees, there might also be several ways
of combining them thereby inducing more non-determinism.
However in practice we found that most of this non-
determinism is due either to over-generation (cases where the
grammar is not sufficiently constrained and allows for one tree
to adjoin to another tree in several places) or to spurious deriva-
tion (distinct derivations with identical semantics). The few re-
maining cases that are linguistically correct are due to varying
modifier positions and could be constrained by a sophisticated
feature decorations in the elementary tree.
any of the paraphrases listed in (2c-2k). Intuitively,
the syntactic constraints to be expressed are those
given in (3b).
(3) a. Jean aime Marie
b. Canonical Nominal Subject, Active verb form,
Canonical Nominal Object
c. lj :jean(j) la:aime(e,j,m) lm:marie(m)
The question is how precisely to formulate these
constraints, how to associate them with the seman-
tic input assumed in Section 3 and how to ensure
that the constraints used do enforce uniqueness of
selection (i.e., that for each input literal, exactly one
elementary tree is selected)? To answer this, we rely
on a feature of the grammar used, namely that each
elementary tree is associated with a linguistically
meaningful unique identifier.
The reason for this is that the grammar is com-
piled from a higher level description where tree frag-
ments are first encapsulated into so-called classes
and then explicitly combined (by inheritance, con-
junction and disjunction) to produce the grammar
elementary trees (cf. (Crabbe? and Duchier, 2004)).
More generally, each elementary tree in the gram-
mar is associated with the set of classes used to pro-
duce that tree and importantly, this set of classes
(we will call this the tree identifier) provides a dis-
tinguishing description (a unique identifier) for that
tree: a tree is defined by a specific combination of
classes and conversely, a specific combination of
classes yields a unique tree.4 Thus the set of classes
associated by the compilation process with a given
elementary tree can be used to uniquely identify that
tree.
Given this, surface realisation is constrained as
follows.
1. Each tree identifier Id(tree) is mapped into a
simplified set of tree properties TPt. There
are two reasons for this simplification. First,
some classes are irrelevant. For instance, the
class used to enforce subject-verb agreement
is needed to ensure this agreement but does
not help in selecting among competing trees.
Second, a given class C can be defined to be
4This is not absolutely true as a tree identifier only reflects
part of the compilation process. In practice, they are few ex-
ceptions though so that distinct trees whose tree identifiers are
identical can be manually distinguished.
331
equivalent to the combination of other classes
C1 . . . Cn and consequently a tree identifier
containing C,C1 . . . Cn can be reduced to in-
clude either C or C1 . . . Cn.
2. Each literal li in the input is associated with a
tree property set TPi (i.e., the input we gener-
ate from is enriched with syntactic information)
3. During realisation, for each literal/tree property
pair ?li : TPi? in the enriched input semantics,
lexical selection is constrained to retrieve only
those trees (i) whose semantics subsumes li and
(ii) whose tree properties are TPi
Since each literal is associated with a (simpli-
fied) tree identifier and each tree identifier uniquely
identifies an elementary tree, realisation produces at
most one realisation.
Examples 4a-4c illustrates the kind of constraints
used by the realiser.
(4) a. lj :jean(j)/ProperName
la:aime(e,j,m)/[CanonicalNominalSubject,
ActiveVerbForm, CanonicalNominalObject]
lm:marie(m)/ProperName
Jean aime Marie
* Jean est aime? de Marie
b. lc:le(c)/Det
lc:chien(c)/Noun
ld:dort(e1,c)/RelativeSubject
lr:ronfle(e2,c)/CanonicalSubject
Le chien qui dort ronfle
* Le chien qui ronfle dort
c. lj :jean(j)/ProperName
lp:promise(e1,j,m,e2)/[CanonicalNominalSubject,
ActiveVerbForm, CompletiveObject]
lm:marie(m)/ProperName
le2:partir(e2,j)/InfinitivalVerb
Jean promet a` marie de partir
* Jean promet a` marie qu?il partira
4.2 At least one realisation
For a realiser to be usable by a generation system,
there must be some means to ensure that its input
is satisfiable i.e., that it can be realised. How can
this be done without actually carrying out realisation
i.e., without checking that the input is satisfiable?
Existing realisers indicate two types of answers to
that dilemma.
A first possibility would be to draw on (Yang et
al., 1991)?s proposal and compute the enriched in-
put based on the traversal of a systemic network.
More specifically, one possibility would be to con-
sider a systemic network such as NIGEL, precom-
pile all the functional features associated with each
possible traversal of the network, map them onto the
corresponding tree properties and use the resulting
set of tree properties to ensure the satisfiability of
the enriched input.
Another option would be to check the well
formedness of the input at some level of the linguis-
tic theory on which the realiser is based. Thus for
instance, REALPRO assumes as input a well formed
deep syntactic structure (DSyntS) as defined by
Meaning Text Theory (MTT) and similarly, SURGE
takes as input a functional description (FD) which in
essence is an underspecified grammatical structure
within the SURGE grammar. In both cases, there
is no guarantee that the input be satisfiable since
all the other levels of the linguistic theory must be
verified for this to be true. In MTT, the DSyntS
must first be mapped onto a surface syntactic struc-
ture and then successively onto the other levels of
the theory while in SURGE, the input FD can be re-
alised only if it provides consistent information for
a complete top-down traversal of the grammar right
down to the lexical level. In short, in both cases, the
well formedness of the input can be checked with
respect to some criteria (e.g., well formedness of a
deep syntactic structure in MTT, well formedness of
a FD in SURGE) but this well formedness does not
guarantee satisfiability. Nonetheless this basic well
formedness check is important as it provides some
guidance as to what an acceptable input to the re-
aliser should look like.
We adopt a similar strategy and resort to the no-
tion of polarity neutral input to control the well
formedness of the enriched input. The proposal
draws on ideas from (Koller and Striegnitz, 2002;
Gardent and Kow, 2005) and aims to determine
whether for a given input (a set of TAG elemen-
tary trees whose semantics equate the input seman-
tics), syntactic requirements and resources cancel
out. More specifically, the aim is to determine
whether given the input set of elementary trees, each
substitution and each adjunction requirement is sat-
isfied by exactly one elementary tree of the appro-
priate syntactic category and semantic index.
332
Roughly,5 the technique consists in (automati-
cally) associating with each elementary tree a po-
larity signature reflecting its substitution/adjunction
requirements and resources and in computing the
grand polarity of each possible combination of trees
covering the input semantics. Each such combina-
tion whose total polarity is non-null is then filtered
out (not considered for realisation) as it cannot pos-
sibly lead to a valid derivation (either a requirement
cannot be satisfied or a resource cannot be used).
In the context of a generation system, polarity
checking can be used to check the satisfiability of the
input or more interestingly, to correct an ill formed
input i.e., an input which can be detected as being
unsatisfiable.
To check a given input, it suffices to compute its
polarity count. If it is non-null, the input is unsatis-
fiable and should be revised. This is not very useful
however, as the enriched input ensures determinism
and thereby make realisation very easy, indeed al-
most as easy as polarity checking.
More interestingly, polarity checking can be used
to suggest ways of fixing an ill formed input. In such
a case, the enriched input is stripped of its control
annotations, realisation proceeds on the basis of this
simplified input and polarity checking is used to pre-
select all polarity neutral combinations of elemen-
tary trees. A closest match (i.e. the polarity neutral
combination with the greatest number of control an-
notations in common with the ill formed input) to
the ill formed input is then proposed as a probably
satisfiable alternative.
5 Evaluation
To evaluate both the paraphrastic power of the re-
aliser and the impact of the control annotations on
non-determinism, we used a graduated test-suite
which was built by (i) parsing a set of sentences, (ii)
selecting the correct meaning representations from
the parser output and (iii) generating from these
meaning representations. The gradation in the test
suite complexity was obtained by partitioning the
input into sentences containing one, two or three fi-
nite verbs and by choosing cases allowing for differ-
ent paraphrasing patterns. More specifically, the test
5Lack of space prevents us from giving much details here.
We refer the reader to (Koller and Striegnitz, 2002; Gardent and
Kow, 2005) for more details.
suite includes cases involving the following types of
paraphrases:
? Grammatical variations in the realisations of
the arguments (cleft, cliticisation, question, rel-
ativisation, subject-inversion, etc.) or of the
verb (active/passive, impersonal)
? Variations in the realisation of modifiers (e.g.,
relative clause vs adjective, predicative vs non-
predicative adjective)
? Variations in the position of modifiers (e.g.,
pre- vs post-nominal adjective)
? Variations licensed by a morpho-derivational
link (e.g., to arrive/arrival)
On a test set of 80 cases, the paraphrastic level
varies between 1 and over 50 with an average of
18 paraphrases per input (taking 36 as upper cut
off point in the paraphrases count). Figure 5 gives
a more detailed description of the distribution of
the paraphrastic variation. In essence, 42% of the
sentences with one finite verb accept 1 to 3 para-
phrases (cases of intransitive verbs), 44% accept 4
to 28 paraphrases (verbs of arity 2) and 13% yield
more than 29 paraphrases (ditransitives). For sen-
tences containing two finite verbs, the ratio is 5%
for 1 to 3 paraphrases, 36% for 4 to 14 paraphrases
and 59% for more than 14 paraphrases. Finally, sen-
tences containing 3 finite verbs all accept more than
29 paraphrases.
Two things are worth noting here. First, the para-
phrase figures might seem low wrt to e.g., work by
(Velldal and Oepen, 2006) which mentions several
thousand outputs for one given input and an average
number of realisations per input varying between
85.7 and 102.2. Admittedly, the French grammar
we are using has a much more limited coverage than
the ERG (the grammar used by (Velldal and Oepen,
2006)) and it is possible that its paraphrastic power
is lower. However, the counts we give only take
into account valid paraphrases of the input. In other
words, overgeneration and spurious derivations are
excluded from the toll. This does not seem to be the
case in (Velldal and Oepen, 2006)?s approach where
the count seems to include all sentences associated
by the grammar with the input semantics.
Second, although the test set may seem small it is
important to keep in mind that it represents 80 inputs
333
with distinct grammatical and paraphrastic proper-
ties. In effect, these 80 test cases yields 1 528 dis-
tinct well-formed sentences. This figure compares
favourably with the size of the largest regression test
suite used by a symbolic NLG realiser namely, the
SURGE test suite which contains 500 input each
corresponding to a single sentence. It also compares
reasonably with other more recent evaluations (Call-
away, 2003; Langkilde-Geary, 2002) which derive
their input data from the Penn Treebank by trans-
forming each sentence tree into a format suitable for
the realiser (Callaway, 2003). For these approaches,
the test set size varies between roughly 1 000 and
almost 3 000 sentences. But again, it is worth stress-
ing that these evaluations aim at assessing coverage
and correctness (does the realiser find the sentence
used to derive the input by parsing it?) rather than
the paraphrastic power of the grammar. They fail to
provide a systematic assessment of how many dis-
tinct grammatical paraphrases are associated with
each given input.
To verify the claim that tree properties can be used
to ensure determinism (cf. footnote 4), we started
by eliminating from the output all ill-formed sen-
tences. We then automatically associated each well-
formed output with its set of tree properties. Finally,
for each input semantics, we did a systematic pair-
wise comparison of the tree property sets associated
with the input realisations and we checked whether
for any given input, there were two (or more) dis-
tinct paraphrases whose tree properties were the
same. We found that such cases represented slightly
over 2% of the total number of (input,realisations)
pairs. Closer investigation of the faulty data indi-
cates two main reasons for non-determinism namely,
trees with alternating order of arguments and deriva-
tions with distinct modifier adjunctions. Both cases
can be handled by modifying the grammar in such
a way that those differences are reflected in the tree
properties.
6 Related work
The approach presented here combines a reversible
grammar realiser with a symbolic approach to para-
phrase selection. We now compare it to existing sur-
faces realisers.
NLG geared realisers. Prominent general
purpose NLG geared realisers include REALPRO,
SURGE, KPML, NITROGEN and HALOGEN. Fur-
thermore, HALOGEN has been shown to achieve
broad coverage and high quality output on a set of 2
400 input automatically derived from the Penn tree-
bank.
The main difference between these and the
present approach is that our approach is based on a
reversible grammar whilst NLG geared realisers are
not. This has several important consequences.
First, it means that one and the same grammar and
lexicon can be used both for parsing and for gener-
ation. Given the complexity involved in developing
such resources, this is an important feature.
Second, as demonstrated in the Redwood Lingo
Treebank, reversibility makes it easy to rapidly cre-
ate very large evaluation suites: it suffices to parse a
set of sentences and select from the parser output the
correct semantics. In contrast, NLG geared realis-
ers either work on evaluation sets of restricted size
(500 input for SURGE, 210 for KPML) or require
the time expensive implementation of a preprocessor
transforming e.g., Penn Treebank trees into a format
suitable for the realisers. For instance, (Callaway,
2003) reports that the implementation of such a pro-
cessor for SURGE was the most time consuming part
of the evaluation with the resulting component con-
taining 4000 lines of code and 900 rules.
Third, a reversible grammar can be exploited to
support not only realisation but also its reverse,
namely semantic construction. Indeed, reversibility
is ensured through a compositional semantics that is,
through a tight coupling between syntax and seman-
tics. In contrast, NLG geared realisers often have
to reconstruct this association in rather ad hoc ways.
Thus for instance, (Yang et al, 1991) resorts to ad
334
hoc ?mapping tables? to associate substitution nodes
with semantic indices and ?fr-nodes? to constrain
adjunction to the correct nodes. More generally, the
lack of a clearly defined compositional semantics in
NLG geared realisers makes it difficult to see how
the grammar they use could be exploited to also sup-
port semantic construction.
Fourth, the grammar can be used both to gener-
ate and to detect paraphrases. It could be used for
instance, in combination with the parser and the se-
mantic construction module described in (Gardent
and Parmentier, 2005), to support textual entailment
recognition or answer detection in question answer-
ing.
Reversible realisers. The realiser presented here
differs in mainly two ways from existing reversible
realisers such as (White, 2004)?s CCG system or
the HPSG ERG based realiser (Carroll and Oepen,
2005).
First, it permits a symbolic selection of the out-
put paraphrase. In contrast, existing reversible re-
alisers use statistical information to select from the
produced output the most plausible paraphrase.
Second, particular attention has been paid to the
treatment of paraphrases in the grammar. Recall
that TAG elementary trees are grouped into families
and further, that the specific TAG we use is com-
piled from a highly factorised description. We rely
on these features to associate one and the same se-
mantic to large sets of trees denoting semantically
equivalent but syntactically distinct configurations
(cf. (Gardent, 2006)).
7 Conclusion
The realiser presented here, GENI, exploits a gram-
mar which is produced semi-automatically by com-
piling a high level grammar description into a Tree
Adjoining Grammar. We have argued that a side-
effect of this compilation process ? namely, the as-
sociation with each elementary tree of a set of tree
properties ? can be used to constrain the realiser
output. The resulting system combines the advan-
tages of two orthogonal approaches. From the re-
versible approach, it takes the reusability, the ability
to rapidly create very large test suites and the capac-
ity to both generate and detect paraphrases. From
the NLG geared paradigm, it takes the ability to
symbolically constrain the realiser output to a given
generation context.
GENI is free (GPL) software and is available at
http://trac.loria.fr/?geni.
References
Charles B. Callaway. 2003. Evaluating coverage for large sym-
bolic NLG grammars. In 18th IJCAI, pages 811?817, Aug.
J. Carroll and S. Oepen. 2005. High efficiency realization for a
wide-coverage unification grammar. 2nd IJCNLP.
B. Crabbe? and D. Duchier. 2004. Metagrammar redux. In
CSLP, Copenhagen.
M. Elhadad and J. Robin. 1999. SURGE: a comprehensive
plug-in syntactic realization component for text generation.
Computational Linguistics.
C. Gardent and L. Kallmeyer. 2003. Semantic construction in
FTAG. In 10th EACL, Budapest, Hungary.
C. Gardent and E. Kow. 2005. Generating and selecting gram-
matical paraphrases. ENLG, Aug.
C. Gardent and Y. Parmentier. 2005. Large scale semantic con-
struction for Tree Adjoining Grammars. LACL05.
C. Gardent. 2006. Integration d?une dimension semantique
dans les grammaires d?arbres adjoints. TALN.
M. Kay. 1996. Chart Generation. In 34th ACL, pages 200?204,
Santa Cruz, California.
A. Koller and K. Striegnitz. 2002. Generation as dependency
parsing. In 40th ACL, Philadelphia.
I. Langkilde-Geary. 2002. An empirical verification of cover-
age and correctness for a general-purpose sentence genera-
tor. In Proceedings of the INLG.
B. Lavoie and O. Rambow. 1997. RealPro?a fast, portable
sentence realizer. ANLP?97.
C. Matthiessen and J.A. Bateman. 1991. Text generation
and systemic-functional linguistics: experiences from En-
glish and Japanese. Frances Pinter Publishers and St. Mar-
tin?s Press, London and New York.
I.A. Mel?cuk. 1988. Dependency Syntax: Theorie and Prac-
tice. State University Press of New York.
Erik Velldal and Stephan Oepen. 2006. Statistical ranking in
tactical generation. In EMNLP, Sydney, Australia.
K. Vijay-Shanker and AK Joshi. 1988. Feature Structures
Based Tree Adjoining Grammars. Proceedings of the 12th
conference on Computational linguistics, 55:v2.
M. White. 2004. Reining in CCG chart realization. In INLG,
pages 182?191.
G. Yang, K. McKoy, and K. Vijay-Shanker. 1991. From func-
tional specification to syntactic structure. Computational In-
telligence, 7:207?219.
335
Paraphrastic Grammars
Claire Gardent
CNRS-LORIA, Nancy
France
Claire.Gardent@loria.fr
Marilisa Amoia
Computational Linguistics
University of Saarbruecken
Germany
amoia@coli.uni-sb.de
Evelyne Jacquey
CNRS-ATILF, Nancy
France
Evelyne.Jacquey@atilf.fr
Abstract
Arguably, grammars which associate natural lan-
guage expressions not only with a syntactic but
also with a semantic representation, should do so in
a way that capture paraphrasing relations between
sentences whose core semantics are equivalent. Yet
existing semantic grammars fail to do so. In this pa-
per, we describe an ongoing project whose aim is
the production of a ?paraphrastic grammar? that is,
a grammar which associates paraphrases with iden-
tical semantic representations. We begin by propos-
ing a typology of paraphrases. We then show how
this typology can be used to simultaneously guide
the development of a grammar and of a testsuite de-
signed to support the evaluation of this grammar.
1 Introduction
A salient feature of natural language is that it allows
paraphrases that is, it allows different verbalisations
of the same content. Thus although the various ver-
balisations in (1) may have different pragmatic or
communicative values (with respect for instance to
topicalisation, presuppositions or focus/ground par-
titioning), they all share a core semantic content, the
content approximated by a traditional montagovian
compositional semantics.
(1) a. La croisie`re cou?te cher.
Lit. the cruse is expensive
b. Le cou?t de la croisie`re est e?leve?.
Lit. the cost of the cruse is high
c. La croisie`re a un cou?t e?leve?
Lit. the cruse has a high cost
Linguists have long noticed the pervasiveness of
paraphrases in natural language and attempted to
caracterise it. Thus for instance Chomsky?s ?trans-
formations? capture the relation between one core
meaning (a deep structure in Chomsky?s terms) and
several surface realisations (for instance, between
the passive and the active form of the same sen-
tence) while (Mel?c?uk, 1988) presents sixty para-
phrastic rules designed to account for paraphrastic
relations between sentences.
More recently, work in information extraction
(IE) and question answering (QA) has triggered a
renewed research interest in paraphrases as IE and
QA systems typically need to be able to recognise
various verbalisations of the content. Because of the
large, open domain corpora these systems deal with,
coverage and robustness are key issues and much on
the work on paraphrases in that domain is based on
automatic learning techniques. For instance, (Lin
and Pantel, 2001) acquire two-argument templates
(inference rules) from corpora using an extended
version of the distributional analysis in which paths
in dependency trees that have similar arguments are
taken to be close in meaning. Similarly, (Barzi-
lay and Lee, 2003) and (Shinyanma et al, 2002)
learn sentence level paraphrase templates from a
corpus of news articles stemming from different
news source. And (Glickman and Dagan, 2003) use
clustering and similarity measures to identify sim-
ilar contexts in a single corpus and extract verbal
paraphrases from these contexts.
Such machine learning approaches have known
pros and cons. On the one hand, they produce large
scale resources at little man labour cost. On the
other hand, the degree of descriptive abstraction of-
fered by the list of inference or paraphrase rules they
output is low.
We chose to investigate an alternative research di-
rection by aiming to develop a ?paraphrastic gram-
mar? that is, a grammar which captures the para-
phrastic relations between linguistic structures1 .
Based on a computational grammar that associates
natural language expressions with both a syntactic
and a semantic representation, a paraphrastic gram-
1As we shall briefly discuss in section 4, the grammar is de-
veloped with the help of a meta-grammar (Candito, 1999) thus
ensuring an additional level of abstraction. The metagrammar
is an abstract specification of the linguistic properties (phrase
structure, valency, realisation of grammatical functions etc.)
encoded in the grammar basic units. This specification is then
compiled to automatically produce a specific grammar.
mar is a grammar that moreover associates para-
phrases with the same semantic representation. That
is, contrary to machine learning based approaches
which relate paraphrases via sentence patterns, the
paraphrastic grammar approach relates paraphrases
via a common semantic representation. In this way,
the paraphrastic approach provides an interesting al-
ternative basis for generation from conceptual rep-
resentations and for the inference-based, deep se-
mantic processing of the kind that is ultimately
needed for high quality question answering.
Specifically, we aim at developing a paraphras-
tic grammar for French, based on the Tree Adjoin-
ing Grammar (TAG) developed for this language by
Anne Abeille? (Abeille?, 2002).
The paper is structured as follows. We start
by proposing a typology of the paraphrastic means
made available by natural language. We then show
how this typology can be used to develop a testsuite
for developing and evaluating a paraphrastic gram-
mar. Finally, we highlight some of the issues arising
when developing a paraphrastic grammar.
2 Classifying paraphrases
A paraphrastic grammar should capture the vari-
ous means made available by natural language to
support paraphrasing. But what are those means?
We distinguish here between three main classes
namely, parallel, shuffling and definitional para-
phrastic means.
Parallel paraphrastic means. A parallel para-
phrase can hold either between two non predica-
tive lexical units (words or multi word expressions)
modulo negation or between two predicative units
of identical arity. If it holds between predicative
units, the mapping linking grammatical functions
(subject, objects, etc.) and thematic roles (agent,
theme, etc.) must be the same. Depending on
whether or not negation is involved, semantic equiv-
alence will futhermore obtain either through syn-
onymy or through antonymy.
As illustrated in Figure 1, synonymy can be fur-
ther divided in a number of cases depending on var-
ious morphological and syntactic criteria. The clas-
sification criteria used involve :
? Syntactic category: Do the synonyms have the
same syntactic category?
? Morphological relatedness: Do the synonyms
contain words that are morphologically re-
lated?
? Form: Are the synonyms simple lexical units
or multi word expressions?
As for antonymy, we distinguish between trans
and intracategorial antonymy:
(2) Jean est lent/Jean n?est pas rapide.
Jean is slow/Jean is not fast.
lent/rapide, intracategorial
Jean a cesser de fumer/Jean ne fume plus.
Jean has stopped smoking/Jean smokes no
more.
cesse de/ne . . . plus, transcategorial
Shuffling paraphrastic means. When a seman-
tic equivalence holds between predicative units with
distinct grammatical functions/thematic role link-
ing, we speak of shuffling paraphrases. Such para-
phrases can be realised either by means of argument
preserving alternations (in the sense of Beth Levin,
cf. (4)) or using a converse construction (cf. 3)2.
(3) a Jean donne un livre a` Marie.
Jean gives a book to Marie.
Marie rec?oit un livre de Jean
Jean receives a book from Marie.
b Jean est le parent de Marie.
Jean is the parent of Marie.
Marie est l?enfant de Jean.
Marie is the child of Jean.
(4) a. Cette cle? ouvre le coffre fort
This key opens the safe.
Le coffre fort s?ouvre avec cette cle?
The safe opens with this key.
b. Jean mange une pomme
Jean eats an apple.
une pomme est mange?e par Jean
An apple is eaten by Jean.
Il a e?te? mange? une pomme par Jean.
There has been an apple eaten by Jean.
c. L?eau remplit la cruche
The water fills the jug .
La cruche se remplit d?eau
The jug fills with water.
On remplit la cruche d?eau
One fills the jug with water.
d. Le laboratoire fusionne avec l?entreprise
The laboratory merges with the firm.
le laboratoire et l?entreprise fusionnent
The laboratory and the firm merge.
e. Jean frappe le mur avec un baton
Jean hit the wall with a stick.
2Obviously, the english translations do not reflect the ac-
ceptability of the french equivalent.
Same synt. Same morph. Form Example
categories family
yes no word/word policier, flic
yes yes word/mwe conseiller, donner conseil
yes no word/mwe s?exprimer sur, donner son avis sur
yes no mwe/mwe donner carte blanche a`, laisser tout pouvoir
no yes word/word construire, construction
no no word/word candidature a`, briguer
Figure 1: Synonymy
Jean frappe le baton sur le mur.
Jean hit the stick on the wall.
f. Je fournis des livres a` Jean
I provide books to Jean.
Je fournis Jean en livre
I provide Jean with books.
Definitional paraphrastic means. Third, we call
?definitional paraphrases? semantic equivalences
that hold between a lexical unit and a phrase con-
sisting of more than one lexical unit. The phrase
in this case, defines the meaning of the lexical unit.
Since definitions are notoriously difficult to decide
upon, we restrict ourselves here to such definitions
as can be given by derivational morphology that is,
definitions based on a word that is morphologically
linked to the definiendum (cf. 5).
(5) a. Le conducteur de la BMW est chauve
The driver of the BMW is bald.
La personne qui conduit la BMW est
chauve
The person who drives the BMW is bald.
b. Cet outil est parame?trable
This tool is parameterisable.
Cet outil peut e?tre parame?tre?
This tool can be parameterised.
3 Developing a paraphrase testsuite
Based on the above typology, we can systematically
construct a testsuite for developing and evaluating
a paraphrastic grammar. Indeed, when developing
a grammar, it is necessary to have some means of
assessing both the coverage of the grammar (does
it generate all the sentences of the described lan-
guage?) and its degree of overgeneration (does it
generate only the sentences of the described lan-
guage?) While corpus driven efforts along the PAR-
SEVAL lines (Black et al, 1991) are good at giving
some measure of a grammar coverage, they are not
suitable for finer grained analysis and in particular,
for progress evaluation, regression testing and com-
parative report generation. Another known method
consists in developing and using a test suite that is,
a set of negative and positive items against which
the grammar can be systematically tested. For en-
glish, there is for instance the 15 year old Hewlett-
Packard test suite, a simple text file listing test sen-
tences and grouping them according to linguistics
phenomena (Flickinger et al, 1987); and more re-
cently, the much more sophisticated TSNLP (Test
Suite for Natural Language Processing) which in-
cludes some 9500 test items for English, French and
German, each of them being annotated with syntac-
tic and application related information (Oepen and
Flickinger, 1998).
Yet because they do not take into account the se-
mantic dimension, none of these tools are adequate
for evaluating the paraphrastic power of a gram-
mar. To remedy this, we propose to develop a para-
phrase test suite based on the paraphrase typology
described in the previous section. In such a testsuite,
test items pair a semantic representation with a set
of paraphrases verbalising this semantics. The con-
struction and annotation of the paraphrases reflects
the paraphrase typology. In a first phase, we concen-
trate on simple, non-recursive predicate/argument
structure. Given such a structure, the construction
and annotation of a test item proceeds as follows.
First, a ?canonical verbalisation? is produced in
which the predicate is realised by the ?canonical
verb? for the given concept3 and the arguments by
the canonical nouns.
Next variants are produced by systematically try-
ing to create parallel, shuffling and definitional para-
phrases. Each of the variant is furthermore anno-
tated with labels caracterising the type of paraphras-
ing involved. Here is an example. Suppose the input
semantics is:
apply(e), agent(e,jean), theme(e,job), failure(e)
for which the canonical verbalisation is:
(6) Jean a candidate? sans succe`s sur le poste
Jean has applied in vain for the job.
3Like in a thesaurus, we assume that amongst a set of syn-
onyms, one lexical unit is ?canonical? and the others not. The
canonical unit is sometimes called a descriptor.
The parallel synonyms4 that can be used are the
following:5
candidater candidature +pred-N
poser sa +pred-vsupV
candidature
briguer +pred-V
sans succe`s e?chouer +mod-V
e?tre sans succe`s +mod-beAdv
ne pas e?tre retenu +mod-Vanton
For shuffling synonymy, two alternations are
available: the active/passive alternation for ?poser?
and the active/locative one for ?e?chouer?. There is
no converse construction. Neither is there any defi-
nition given by derivational morphology for any of
the terms occurring in the canonical verbalisation.
Based on these facts, the following variants and an-
notations can be constructed.
(7) a. Jean a brigue? le poste sans succe`s
Jean has asked for the job in vain.
+pred-Vsyn
b. Jean a pose? sa candidature sur le poste sans
succe`s
Jean has submitted his application for the
job in vain.
+pred-vsupN
c. La candidature pose?e par Jean sur le poste
a e?te? sans succe`s
The application submitted by Jean for the
job was in vain.
+pred-partAdj, +mod-beAdv
d. La candidature pose?e par Jean sur le poste
a e?choue?
The application submitted by Jean for the
job failed.
+pred-partAdj, +mod-V
e. La candidature de Jean sur le poste a e?te?
sans succe`s
Jean?s application for the job was in vain.
+pred-N, +mod-beAdv
f. La candidature de Jean sur le poste n?a pas
e?te? retenue
4As has been abundantly argued by linguists, real synonyms
are extremely rare. By synonyms, we in fact refer here to the
notion of quasi-synonyms used for instance in WordNet that is,
words that are interchangeable in a restricted set of contexts.
5The labels are the ones used for annotation. They carac-
terise variations with respect to the canonical realisation. For
instance, +pref-N indicates that the main predicate (realised by
a verb in the canonical verbalisation) is realised as a noun.
Jean?s application for the job was not suc-
cessful.
+pred-N, +mod-Vanton
g. La candidature de Jean sur le poste a
e?choue?
Jean?s application for the job failed.
+pred-N, +mod-V
h. Jean a e?choue? dans sa candidature sur le
poste.
Jean failed in his application for the job.
+pred-N, +mod-V-altLoc
Thus the typology of paraphrastic means help
guide the construction of the various paraphrases
contained in a single item. There remains the ques-
tion of how to choose the particular items of the
testsuite. In other words: which semantic repre-
sentations should we use to populate the test suite
and on the basis of which criteria? The basic aim
here is to cover the various types of possible seman-
tic combinations and the constraints they are sub-
ject to at the syntactic (realisation) level. If, as Beth
Levin argues, syntax is a reflex of semantic proper-
ties, then different semantic contents should be sub-
ject to varying syntactic constraints and the test suite
ought to cover these various types of interactions.
Accordingly test items are constructed whose main
predicate vary along the following dimensions :
(1) WordNet Verb Family; (2) Aspect; (3) Arite?
That is, items are constructed for each word-
Net family (the french WordNet counts roughly 170
such families). Within a given family, we attempt
to find examples with distinct aspectual categories
(state, accomplishment and process). Finally, given
a WN family and an aspectual category, items will
vary with respect to the arity of the main predicate
and the types of their arguments e.g., predicates of
arity one (run, cost, sleep), of arity two with non
propositional arguments (eat, hit, dug), of arity two
with a propositional argument (say, promise etc.),
etc.
4 A paraphrastic grammar
?Semantic grammars? already exist which describe
not only the syntax but also the semantics of nat-
ural language. Thus for instance, (Copestake and
Flickinger, 2000; Copestake et al, 2001) describes
a Head Driven Phrase Structure Grammar (HPSG)
which supports the parallel construction of a phrase
structure (or derived) tree and of a semantic repre-
sentation and (Dalrymple, 1999) show how to equip
Lexical Functional grammar (LFG) with a glue se-
mantics.
These grammars are both efficient and large scale
in that they cover an important fragment of the nat-
ural language they describe and can be processed by
parsers and generators in almost real time. For in-
stance, the LFG grammar parses sentences from the
Wall Street Journal and the ERG HPSG grammar
will produce semantic representations for about 83
per cent of the utterances in a corpus of some 10
000 utterances varying in length between one and
thirty words. Parsing times vary between a few ms
for short sentences and several tens of seconds for
longer ones.
Nonetheless, from a semantics viewpoint, these
grammars fail to yield a clear account of the para-
phrastic relation. Here is a simple example illustrat-
ing this shortcoming. Suppose we parse the follow-
ing paraphrases where a lexical definition (driver ?
person who drives) is involved:
(8) a. The person who drives the car is mad.
b. The driver of the car is mad.
When given these sentences, the LKB system
based on the ERG HPSG grammar returns semantic
representations which can be sketched as follows6:
(9) a. the(x, person(x) ? the(y, car(y) ?
drive(e,x,y) ? mad(x)))
a. the(y, car(y) ? the(x, driver(x,y) ? of(x,y))
? mad(x))
In other words, the grammar associates with
these paraphrases semantic representations which
are very different. It could be argued of course
that although these representations are syntactically
distinct, they can be inferred, given the appropri-
ate knowledge, to be semantically equivalent. But
a solution that avoids placing such extra burden on
the inferencing component is obviously better. In
short, one important shortcoming of existing large
scale semantic grammars is that they do not assign
semantically equivalent sentences, the same seman-
tic representation.
By contrast, we propose to develop a grammar
which whereever possible assigns identical seman-
tic representations to paraphrases and whose devel-
6These semantic representations have been simplified for
better readibility. The real representations output by the LKB
are the following:
prpstn(def(x,person(x)?prpstn(def(y,car(y),
drive(e1,v1,x,y,v2),v3)), mad(e2,x,v4),v5)
prpstn(def(x,person(x)?prpstn(def(y,car(y),
drive(e1,v1,x,y,v2),v3)), mad(e2,x,v4),v5)
prpstn(def(y,car(y)?prpstn(def(x, driver(x,y) ? of(e1,x,y,v1),
mad(e2,x,v2,v3)))))
opment is based both on semantic and syntactic con-
siderations.
4.1 Linguistic framework
Our grammar is couched within the Feature-Based
Tree Adjoining grammar (FTAG) formalism. An
FTAG consists of a set of (auxiliary or initial) ele-
mentary trees and two tree composition operations:
substitution and adjunction. Substitution is the stan-
dard tree operation used in phrase structure gram-
mars while adjunction is an operation which inserts
an auxiliary tree into a derived tree. To account for
the effect of these insertions, two feature structures
(called top and bottom) are associated with each
tree node in FTAG. The top feature structure en-
codes information that needs to be percolated up the
tree should an adjunction take place. In contrast, the
bottom feature structure encodes information that
remains local to the node at which adjunction takes
place.
The language chosen for semantic representa-
tion is a flat semantics along the line of (Bos,
1995; Copestake et al, 1999; Copestake et al,
2001). However because we are here focusing on
paraphrases rather than fine grained semantic dis-
tinctions, the underspecification and the descrip-
tion of the scope relations permitted by these se-
mantics will here be largely ignored and flat se-
mantics will be principally used as a convenient
way of describing predicate/arguments and modi-
fiers/modified relationships. Thus the semantic rep-
resentations we assume are simply set of literals of
the form P n(x1, . . . , xn) where P n is a predicate
of arity n and xi is either a constant or a unifica-
tion variable whose value will be instantiated during
processing.
Semantic construction proceeds from the derived
tree (Gardent and Kallmeyer, 2003) rather than ?
as is more common in TAG ? from the derivation
tree. This is done by associating each elementary
tree with a semantic representation and by deco-
rating relevant tree nodes with unification variables
and constants occuring in associated semantic rep-
resentation. The association between tree nodes and
unification variables encodes the syntax/semantics
interface ? it specifies which node in the tree pro-
vides the value for which variable in the final se-
mantic representation.
As trees combine during derivation, (i) variables
are unified ? both in the tree and in the associated
semantic representation ? and (ii) the semantics of
the derived tree is constructed from the conjunction
of the semantics of the combined trees. A simple
example will illustrate this.
NPj
John
name(j,john)
S
NP?x1 VP
V NP?x2 NPm
loves Mary
love(x1,x2) name(m,mary)
Figure 2: ?John loves Mary?
Suppose the elementary trees for ?John?, ?loves?
and ?Mary? are as given in Fig. 2 where a downar-
row (?) indicates a substitution node and Cx/Cx ab-
breviate a node with category C and a top/bottom
feature structure including the feature-value pair {
index : x}. On substitution, the root node of the tree
being substituted in is unified with the node at which
substitution takes place. Further, when derivation
ends, the top and bottom feature structures of each
node in the derived tree are unified. Thus in this
case, x1 is unified with j and x2 with m. Hence, the
resulting semantics is:
love(j, m), name(j, john), name(m, mary)
4.2 The signature of the semantic
representation language
Let us now come back to the paraphrases given in
example 1. To produce an identical semantic rep-
resentation of these three sentences, we first need to
ensure that synonyms be assigned the same concept.
That is, we need to fix a concept inventory and to
use this inventory in a consistent way in particular,
by assigning synonyms the same concept.
For non predicative units, we use WordNet synset
numbers or when working within a restricted do-
main with a well defined thesaurus, the descriptors
of that thesaurus.
To represent the semantics of predicative units,
we use FrameNet inventory of frames and frame el-
ements (C.Johnson et al, 2002). FrameNet is an on-
line lexical resource for English based on the prin-
ciples of Frame Semantics. In this approach, a word
evokes a frame i.e., a simple or a complex event, and
each frame is associated with a number of frame el-
ements that is, a number of participants fulfilling a
given role in the frame. Finally each frame is as-
sociated with a set of target words, the words that
evoke that frame.
Thus FrameNet associates synonyms with an
identical concept namely, the frame evoked by those
synonyms. We make use of this feature and instead
of choosing our own semantic predicates and re-
lations, draw on FrameNet frames and frame ele-
ments. For instance, the paraphrases in example 1
are taken to evoke the FrameNet COMMERCE frame
and to instantiate two of its frame elements namely,
GOODS and MONEY. The semantic representation
they will be assigned will therefore be the follow-
ing:
commerce(e,g,m), cruise(g), goods(e,g), high(m),
money(e,m)
4.3 Capturing paraphrastic relations
Given the basic signature provided by FrameNet
(and any extension of it that will prove necessary
to account for the data), the grammar must then
specify a compositional semantics which will de-
rive identical representations for the types of para-
phrases captured by our typology. In essence, this
implies assigning the same semantic representations
to synonyms, converses and alternations. Con-
cretely, this involves two different subtasks : first,
a modeling of the synonymic relation between syn-
tactically divergent constructs (e.g., between a pred-
icative noun, a support verb construction and a verb)
and second, the identification of the synonymic sets
(which are the words and multi word expressions
that stand in a parallel, shuffling or definitional para-
phrastic relation?).
Modeling intercategorial synonymic links. A
first investigation of Anne Abeille??s TAG for French
suggests that modeling the synonymic relations
across syntactic constructs is reasonably straightfor-
ward. For instance, as Figures 3, 4 and 5 show, the
FTAG trees assigned on syntactic grounds by Anne
Abeille? FTAG to predicative nouns, support verb
constructions and transitive verbs can be equiped
with a flat semantics in such a way as to assign
the three sentences in 1 a unique semantic rep-
resentation namely the one given above. Gener-
ally, the problem is not so much to state the cor-
respondances between synonymic but syntactically
different constructs as to do this in a general way
while not overgeneralising. To address this prob-
lem, we are currently working on developing a
metagrammar in the sense of (Candito, 1999). This
metagrammar allows us to factorise both syntac-
tic and semantic information. Syntactic informa-
tion is factorised in the usual way. For instance,
there will be a class NOVN1 which groups together
all the initial trees representing the possible syntac-
tic configurations in which a transitive verb with
two nominal arguments can occur. But addition-
nally there will be semantic classes such as, ?bi-
nary predicate of semantic type X? which will be
associated with the relevant syntactic classes for in-
stance, NOVN1 (the class of transitive verbs with
nominal arguments), BINARY NPRED (the class of
binary predicative nouns), NOVSUPNN1 , the class
of support verb constructions taking two nominal
arguments. By further associating semantic units
(e.g., ?cost?) with the appropriate semantic classes
(e.g., ?binary predicate of semantic type X?), we
can in this way capture both intra and intercategorial
paraphrasing links in a general way.
Constructing paraphrastic sets. Depending on
the type of paraphrastic means involved, construct-
ing a paraphrastic set (the set of all lexical items re-
lated by a paraphrastic link be it parallel, shuffling
or definitional) is more or less easy as resources for
that specific means may or may not be readily avail-
able.
Cases of intracategorial synonymy are relatively
straigthtforward as several electronic synonym dic-
tionnaries for french are available (Ploux, 1997).
Multi word expressions however remain a problem
as they are often not or only partially included in
such dictionnaries. For these or for a specific do-
main, basic synonymic dictionaries can be comple-
mented using learning methods based on distribu-
tional similarity (Pereira et al, 1993; Lin, 1998).
techniques.
For intercategorial synonymy involving a deriva-
tional morphology link, some resources are avail-
able which however are only partial in that they only
store morphological families that is, sets of items
that are morphologically related. Lexical semantics
information still need to be included.
Intercategorial synonymy not involving a deriva-
tional morphology link has been little studied and
resources are lacking. However as for other types
of synonymy, distributional analysis and clustering
techniques can be used to develop such resources.
For shuffling paraphrases, french alternations are
partially described in (Saint-Dizier, 1999) and a re-
source is available which describes alternation and
the mapping verbs/alternations for roughly 1 700
verbs. For complementing this database and for
converse constructions, the LADL tables (Gross,
1975) can furthermore be resorted to, which list
detailed syntactico-semantic descriptions for 5 000
verbs and 25 000 verbal expressions. In particu-
lar, (Gross, 1989) lists the converses of some 3 500
predicative nouns.
S
GNG ? V GAdvM ?
coute
GNX S:Commerce GAdvY
D NX ? (S,G):goods cher
la (S,M):money Y:High
NX
croisiere
X:Cruise
Figure 3: La croisie`re cou?te cher
S
GNG ? VSup? GN
a D? NGMGNX
cout
D NX ? D S:Commerce
la un (S,M):money
NX (S,G):goods
croisiere N
X:Cruise ? NY Adj
eleve
Y:High
Figure 4: La croisie`re a un cou?t e?leve?
5 Conclusion
Besides the development and evaluation of a core
paraphrastic testsuite and grammar for French, we
plan to investigate two main issues. First, how pre-
cisely should a metagrammar be structured to best
describe a paraphrastic grammar? And second: is
it possible to extract from the kind of inference
rules automatically derived in machine learning ap-
proach, information that can be used to specify this
metagrammar?
6 Acknowledgments.
This paper is based upon work suppported in part by
the project ?Des connaissances a` leurs re?alisation en
langue? within the CNRS funded TCAN program.
SGNY ? Cop GAdjY ?
GNY est eleve
D NY ? Y:High
le
NM
N GP
cout P? GNG ?
S:Commerce
(S,M):money P GNX
(S,G):goods de D NX
la croisiere
X:Cruise
Figure 5: Le cou?t de la croisie`re est e?leve?
References
A. Abeille?. 2002. Une Grammaire Electronique du
Franais. CNRS Editions.
R. Barzilay and L. Lee. 2003. Learning to
paraphrase: an unsupervised approahc using
mutliple-sequence alignment. In Proceedings of
NAACL-HLT.
A. Black, S. Abney, D. Flickinger, C. Gdaniec,
R. Grishman, P. Harrison, D. Hindel, R. INgria,
F. Jelinek, F. Klaavans, M. Liberman, M. Mar-
cus, S. Roukos, B. Santorini, and T. Strzalkowski.
1991. A procedure for quantitatively comparing
the syntactic coverage of english grammars. In
Proceedings of teh 4th DARPA Speech and Natu-
ral Language Workshop.
J. Bos. 1995. Predicate logic unplugged. In Paul
Dekker and Martin Stokhof, editors, Proceedings
of the 10th Amsterdam Colloquium, pages 133?
142.
M.H Candito. 1999. Un outil multilingue de gener-
ation de ltag : application au francais et a l?italien.
TAL, 40(1).
C.Johnson, C. Fillmore, M. Petruckand C. Baker,
M. Ellsworth, and J. Ruppenhofer. 2002.
Framenet: Theory and practice. Technical report,
Berkeley.
Ann Copestake and Dan Flickinger. 2000. An open
source grammar development environment and
broad-coverage English grammar using HPSG.
In Proceedings of the 2nd International Con-
ference on Language Resources and Evaluation,
Athens, Greece.
A. Copestake, D. Flickinger, I. Sag, and C. Pollard.
1999. Minimal Recursion Semantics. An Intro-
duction. Manuscript, Stanford University.
A. Copestake, A. Lascarides, and D. Flickinger.
2001. An algebra for semantic construction in
constraint-based grammars. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics, Toulouse, France.
M. Dalrymple. 1999. Semantics and syntax in lexi-
cal functional grammar. MIT Press.
D. Flickinger, J. Nerbonne, I. Sag, and T. Wasow.
1987. Towards evaluation of nlp systems. Tech-
nical report, Hewlett-Packard Laboratories.
C. Gardent and L. Kallmeyer. 2003. Semantic con-
struction in ftag. In Proceedings of EACL, Bu-
dapest, Hungary.
O. Glickman and I. Dagan. 2003. Identifying lexi-
cal paraphrases from a single corpus: a case study
for verbs. In Proceedings of Recent Advances in
Natural Language Processing.
M. Gross. 1975. Me?thodes en syntase. Masson,
Paris.
G. Gross. 1989. Les constructions converses du
francais. CNRS Editions.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules for question answering. Natural
Language Engineering.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of ACL/COLING,
pages 768?774.
I. Mel?c?uk. 1988. Paraphrase et lexique dans la
thorie linguistique sens-texte. Lexique, 6:13?54.
S. Oepen and D. Flickinger. 1998. Towards sys-
tematic grammar profiling. test suite technology
10 years after. Computer Speech and Language,
12:411?435.
F. Pereira, N. Tishby, and L. Lee. 1993. Distribu-
tional clustering of english words. In Proceed-
ings of the ACL, pages 183?190.
S. Ploux. 1997. Modlisation et traitement infor-
matique de la synonymi. Linguisticae Investiga-
tiones, XXI(1).
P. Saint-Dizier, 1999. Alternations and Verb Se-
mantic Classes for French: analysis and class
formation, chapter 5. Kluwer.
Y. Shinyanma, S. Sekine, K. Sudo, and R. Grish-
man. 2002. Automatic paraphrase acquisition
from news articles. In Proceedings of HLT.
Generating and selecting grammatical paraphrases
Claire Gardent
CNRS/LORIA
Nancy, France
claire.gardent@loria.fr
Eric Kow
INRIA/LORIA
Nancy, France
eric.kow@loria.fr
Abstract
Natural language has a high paraphrastic power yet
not all paraphrases are appropriate for all contexts.
In this paper, we present a TAG based surface re-
aliser which supports both the generation and the
selection of paraphrases. To deal with the combi-
natorial explosion typical of such an NP-complete
task, we introduce a number of new optimisations
in a tabular, bottom-up surface realisation algo-
rithm. We then show that one of these optimisations
supports paraphrase selection.
1 Introduction
As is well known, natural language has a very high paraphras-
tic power so that the same core meaning can be expressed in
many different ways [Gross, 1975; Mel?c?uk, 1988]. Yet not
all paraphrases are appropriate for all contexts. So for in-
stance, a sentence and its converse (1a) express the same core
meaning and so can be considered paraphrases of each other.
Yet as example (1b) illustrates, they are not interchangeable
in the context of a control verb:
(1) a. John borrowed a book from Mary.
?Mary lent a book to John
b. Peter persuaded John to borrow a book from Mary.
6? Peter persuaded Mary to lend a book to John
Similarly, a canonical and a cleft sentence (2a) communi-
cate the same core meaning yet a contrastive context (2b) only
admits the cleft version.
(2) a. John looks at Mary.
? It is Mary that John looks at
b. * It is not Sarah, John looks at Mary.
It is not Sarah, it is Mary that John looks at
More generally, the anaphoric potential (that is, the dis-
course status of the entities being talked about) of the pre-
ceding discourse, its structure, the presence of an embedding
verb or of a given subordinating or coordinating conjunction
are all factors which may restrict the use of paraphrases. To
preserve completeness, it is therefore important that a gener-
ator be able to produce paraphrases in a systematic fashion.
On the other hand, it is also well known that surface real-
isation (the task of producing the set of sentences associated
by a grammar with a given semantic representation) is NP-
complete [Brew, 1992].
In this paper, we present a TAG based surface realiser
which supports both the generation and the selection of gram-
matical paraphrases (section 2 and 3). To deal with the re-
sulting combinatorics, we introduce a number of new opti-
misations (section 4). We then show how one of these op-
timisations can be used to support the selection of contextu-
ally appropriate paraphrases (section 5). Finally, we relate
our approach to similar proposals and show that it compares
favourably in terms of efficiency (section 6 and 7).
2 The grammar
The grammar used by the surface realiser is Feature-based
TAG, a unification based version of Tree Adjoining Gram-
mar. Briefly1, a Feature-based TAG consists of a set of (aux-
iliary or initial) elementary trees and of two tree composition
operations: substitution and adjunction. Substitution inserts a
tree onto a leaf node of another tree2 while adjunction inserts
an auxiliary tree into a derived tree (i.e., either an elementary
tree or a tree resulting from the combination of two trees). In
an FTAG, each tree node which is not a substitution node is
associated with two feature structures called top and bottom
and during derivation, the following unifications take place.
? The adjunction at some node X with top features tX
and bottom features bX , of an auxiliary tree with root
top features r and foot bottom features f entails the
unification of tX with r and of bX with f .
? The substitution at some node X with top features tX
of a tree with root top features t entails the unification
of tX with t.
? At the end of a derivation, the top and bottom features
of all nodes in the derived tree are unified.
1For more details on FTAG see [Vijay-Shanker and Joshi, 1988].
2Leaf nodes where substitution can take place are graphically
distinguished by a down arrow.
In the FTAG used by the surface realisation algorithm, lin-
guistic expressions are associated with semantic representa-
tions as advocated in [Gardent and Kallmeyer, 2003]. The se-
mantic representations used are flat semantic representations
in the sense of [Copestake et al, 2001] and the semantic pa-
rameters (that is, the semantic indices representing the miss-
ing arguments of the semantic functors) are represented by
unification variables.
Further, each elementary tree is associated with a semantic
representation of the type just described and the appropriate
nodes of the elementary trees are decorated with semantic in-
dices or parameters.
More precisely, the substitution nodes of the tree associated
with a semantic functor will be associated with semantic pa-
rameters whilst root nodes and certain adjunction nodes will
be labelled with semantic indices. As trees are combined,
semantic parameters and semantic indices are unified by the
FTAG unification mechanism thus specifying which semantic
index provides the value for which semantic parameter.
Generally, the idea is that the association between tree
nodes and unification variables encodes the syntax/seman-
tics interface: it specifies which node in the tree provides the
value for which semantic parameter in the semantic represen-
tation of a semantic functor. So for instance, the trees for
John, loves and Mary will be as given in Figure 1. The tree for
loves is associated with a semantic representation including
the two semantic parameters x and y. These parameters also
label the subject and the object substitution nodes of this tree.
Conversely, the root node of the tree for John is labelled with
the semantic index j. If the string parsed is John loves Mary,
this tree will be substituted at the subject substitution node of
the loves tree thus instantiating the semantic parameter x to j.
And similarly, for the Mary tree.
S
NP?x VP
NPj V NP?y NPm
John loves Mary
name(j,john) love(x,y) name(m,mary)
? love(j,m),name(j,john),name(m,mary)
Figure 1: John loves Mary
Coverage. The grammar used describes a core fragment for
French and contains around 4 000 trees. It covers some 35
basic subcategorisation frames and for each of these frames,
the set of argument redistributions (active, passive, middle,
reflexivisation, impersonal, passive impersonal) and of argu-
ment realisations (cliticisation, extraction, omission, permu-
tations, etc.) possible for this frame. As a result, it captures
most grammatical paraphrases that is, paraphrases due to di-
verging argument realisations or to different meaning pre-
serving alternation (e.g., active/passive or clefted/non clefted
sentence).
3 The basic algorithm
The basic surface realisation algorithm used is summarised in
Figure 1 (appendix). It is a bottom up, tabular algorithm [Kay,
1996] optimised for TAGs. Its workings can be illustrated by
the following example. Suppose that the input semantics is
the following :
{camp(s,j),john(j),in(s,l),paris(l)}
Then the algorithm proceeds as follows. In a first step (lex-
ical selection), the elementary trees whose semantics sub-
sumes3 part of the input semantics are retrieved and added
to the agenda. In our simple example, the selected trees are
the trees for Jean, campe, dans and paris.
The second step (the substitution phase) consists in sys-
tematically exploring the possibility of combining two trees
by substitution. It is summarised for our example by the ta-
ble in figure 2 where each line corresponds to a processing
step. The words in each column indicate the trees present at
each step in the chart, the agenda and the agenda for auxiliary
trees (AgendaA). The combination column indicates which
tree combines with which tree by means of which operation
(? indicates a substitution, ? an adjunction). The trees result-
ing from such a combination are represented using the con-
catenation of the names of the combined trees (jeanCampe is
the tree resulting from the combination of the tree anchored
by Jean with that anchored by campe). Thus, the first line in-
dicates that the trees anchored by Jean, campe, dans and Paris
are in the agenda and that the chart is empty. The second line
shows that the next state is a state where the tree anchored
by Jean has been retrieved from the agenda and added to the
chart. The third line indicates that when the trees anchored by
campe and Jean are in the chart, they can be combined using
substitution. The result is added to the agenda etc.
More generally, the items are retrieved one by one from
the agenda to be added either to the chart or to the auxiliary
agenda (in the case of an auxiliary tree devoid of substitution
node). For each item added to the chart, all possible substitu-
tions are carried out and the resulting derived trees are added
to the agenda. The loop ends when the agenda is empty.
At this stage, all the items containing an empty substitution
node are erased from the chart (here, the trees anchored by
campe and dans are erased). The agenda is then reinitialised to
the content of the chart and the chart to the content of the aux-
iliary agenda. The third step (the adjunction phase) occurs
then in which all possible adjunctions are performed (figure
3). Finally (retrieval phase), the strings labelling the items in
the chart whose semantics is the input semantics are printed
3Subsumption is here taken to denote term unification. Hence
lexical selection is done on a very ?syntactic? basis: only these lexi-
cal entries whose semantics representation matches part of the input
semantics are selected. This is partly alleviated by taking lexical
synonymy into account while developing the grammar so that two
(intra- or inter-categorical) synonyms are assigned the same seman-
tic representation. A more complete treatment would require the in-
tegration either of a richer lexical semantics or of a lexical selection
module permitting inference so that for instance ?adult(x) male(x)
human(x)? can be inferred to be denoted by the word ?man?.
Agenda Chart Combination AgendaA
Jean,campe,dans,Paris
campe,dans,Paris Jean
dans,Paris campe,Jean ?(campe,Jean)
Paris,JeanCampe campe,Jean,dans
JeanCampe campe,Jean,dans,Paris ?(dans,Paris)
dansParis campe,Jean,dans,Paris,JeanCampe
campe,Jean,dans,Paris,JeanCampe dansParis
Figure 2: Sample run of the substitution phase
out, which in this case yields the sentence Jean campe dans
Paris.
4 Optimisations
Surface realisation is NP complete [Brew, 1992]. More-
over the paraphrastic power of natural language is enormous
[Gross, 1975; Mel?c?uk, 1988]. Hence optimisation is a key
issue and so is the possibility to select a given paraphrase
on demand. We now present a number of optimisations we
added to the algorithm just described in order to reduce the
combinatorics.
4.1 Tabulation and ordered combinations
Tabulation serves to avoid redundant computations. In analy-
sis, the use of the chart to store intermediate constituents and
avoid multiple computation of the same structure renders an
exponential task polynomial. In generation however, tabula-
tion increases efficiency by avoiding duplicate computations
but the complexity remains exponential because in particular
of multiple modifiers [Brew, 1992]. Suppose for instance
that the input semantic representation is the following:
fierce(x),little(x),cat(x),black(x)
For this input, a naive bottom-up realisation algorithm will
generate all intermediate structures that is, n! intermediate
structures with n the number of modifiers. These n! struc-
tures will furthermore be multiplied by the context so that for
instance given the input for the fierce little black cat runs, the
following structures will all be generated.
(3) a. fierce cat, fierce black cat, little cat,little black cat, fierce
little cat, black cat
b. the fierce cat, the fierce black cat, the little cat, the little
black cat, the fierce little cat, the black cat
c. the fierce cat runs, the fierce black cat runs, the little cat
runs, the little black cat runs, the fierce little cat runs, the black
cat runs
To minimise the impact of multiple modifiers, the algo-
rithm presented here performs all substitutions before con-
sidering adjunctions. In effect, this means that adjunction
only applies to syntactically complete trees and so that the
many intermediate structures induced by the modifiers do not
multiply out with other incomplete structures. In the above
example for instance, (3c) will be computed but neither (3a)
nor (3b).
4.2 Avoiding spurious derivations
Categorical grammars often allow so called spurious deriva-
tions in that one and the same syntactic structure can be
derived in several different ways [Hepple, 1991]. TAGs also
induce spurious derivations due to the fact that substitutions
and adjunctions on different nodes of the same tree can be
carried out in different relative orders all of which result in
one and the same structure. Thus for instance, given the
trees np(Marie), np(Jean), s(np?, v(aime), np?)
and the semantic aime(j,m),jean(j), marie(m), two
derivations are possibles, one where np(Jean) is first
substituted in s(np?, v(aime), np?) before the tree for
np(Marie) is ; and the other where np(Marie) is first
substituted before np(Jean) is added. More generally, for a
tree containing n substitution nodes, there will be n! possible
derivations. For instance given the sentence
(4) Jean persuade Marie de promettre a` Claire de donner un
livre a` Marie.
Jean persuades Mary to promise Claire to give Mary a
book
there will be 3! ? 2! ? 2! = 24 possible derivations all
of them produce the same syntactic tree and hence the same
sentence.
Adjunction suffers from the same shortcoming. Given a
TAG tree and n auxiliary trees that can adjoin to different
nodes of that tree, there are n! possible ways of deriving the
tree resulting from these n adjunctions.
To avoid these spurious derivations, we impose a unique
order (from left to right) on the sequences of substitutions
and adjunctions done within a given tree. Because the al-
gorithm systematically examines all pairs of items, this re-
striction does not affect completeness : the unique derivation
supported by the imposed constraints will be taken into con-
sideration by the algorithm and will therefore be produced.
A third source of spurious derivations come from the pos-
sibility of having multiple adjunctions on the same node of a
given tree for instance in the case of the little black cat. The
auxiliary trees anchored by little and black can adjoin in two
different orders on the tree anchored by cat: either little is ad-
joined to cat and black to the foot node of little in the resulting
tree or black is adjoined to cat and little to the root node of
the resulting derived tree. To eliminate this type of spurious
derivations, adjunction on a foot node is ruled out ? which is
usual in TAG parsers.
Agenda Chart Combination AgendaA
Jean,Paris,JeanCampe dansParis
Paris,JeanCampe dansParis,Jean
JeanCampe dansParis,Jean,Paris
dansParis,Jean,Paris,JeanCampe ?(JeanCampe,dansParis)
JeanCampeDansParis dansParis,Jean,Paris,JeanCampe
dansParis,Jean,Paris,JeanCampe,
JeanCampeDansParis
Figure 3: Sample run of the adjunction phase
4.3 Filtering of valid lexical sequences
The most efficient optimisation takes place between the lex-
ical selection phase and that of combination by substitution
and adjunction. At this stage, the number of combinations
that are a priori possible is
?
1?i?n ai with ai the degree of
lexical ambiguity of the i-th literal and n, the number of lit-
erals in the input semantic. That is, the search space is expo-
nential in the number of literals. To reduce the combinatorics,
we use a technique introduced for parsing by [Perrier, 2003]
called polarity based filtering.
Polarity based filtering is based on the observation that
many of the combinations of lexical items which cover the in-
put semantics are in fact syntactically invalid either because
a syntactic requirement is not fulfilled or because a syntac-
tic resource is not used. Accordingly, polarity based filtering
detects and eliminates such combinations by:
1. assigning each lexical item a polarity signature reflecting
its syntactic requirements and resources
2. computing for each possible combination of lexical
items the net sum of its syntactic requirements and re-
sources and
3. eliminating all combinations of lexical items that do not
have a net sum of zero (because such combinations can-
not possibly lead to a syntactically valid sentence)
As we shall see below, polarity based filtering is imple-
mented using finite state techniques.
Let us see how this works by running through a simple
example. Suppose that the input semantic representation is:
(5) buy(e,t,j), annoying(e), field(t),john(j)
and that the TAG trees selected for this input are the ones
given in Figure 8 (appendix).
In this figure, the literals following the tree name give the
polarities that are automatically assigned to each of these
trees on the basis of their root and substitution nodes (for in-
stance, the v achete has polarity (+p ? 2n) meaning that it
provides a sentence and requires two NPs). Since in a TAG,
substitution nodes indicates syntactic requirements whilst an
initial tree permits fulfilling a syntactic requirement, polarity
signatures can be automatically computed as follows:
? a polarity of the form +C is added to the tree polarity
signature of each initial tree with root node category C.
? a polarity of the form ?S is added to the tree polarity
signature of each initial tree for each substitution node
with category S in that tree.
Now we need to compute the polarity of all possible com-
binations of lexical items. This is done by:
1. building a polarity automaton for each polarity category
occurring in the set of possible combinations (in this
case, n and s),
2. computing the intersection of these automata and
3. minimising the resulting automaton.
In the final automaton, only the combinations that have a
null polarity are represented. These will be the combinations
actually explored by the realiser.
For the above example, the final automaton is that given in
figure 9 where each state is labelled with the cumulated polar-
ity of the path(s) leading to that state and where the transitions
are labelled with the lexical item covered. As can be seen, the
combinations that are syntactically invalid (in grey in the au-
tomaton) have been eliminated. Thus in particular, the com-
bination of the predicative tree n0Vadj with the verb ache`te
and its two complements is ruled out (as the n requirement of
n0Vadj cannot be satisfied) and conversely, the combination
of the predicative tree p0Vadj with the relational noun achat
(because the p requirement of p0Vadj cannot be satisfied)4 .
4.4 Combining polarity based filtering and
tabulation
To preserve the factorisation supported by the use of a chart,
polarity filtering must furthermore be integrated with the re-
alisation algorithm. Indeed, each path through a polarity au-
tomaton represents a combination of lexical items whose total
semantics is the input semantics and which may lead to a syn-
tactically valid expression. But some of these paths may share
some subpath(s). To avoid computing these shared subpaths
several times, each selected elementary tree is annotated with
4For lack of space, we ignore here functional words (determiners,
prepositions). In the full algorithm, their treatment is implemented
either by means of co-anchors (a verb whose comple?ment requires a
given preposition for instance, will be assigned a tree with multiple
anchors, one for the verb, the other for the preposition) or by means
of a richer semantic (contrary to what is shown here, a quantifier will
have a non nul semantics). Note further that lexical items with multi-
literal semantics are also handled as well as items whose semantics
is reduced to an index (pronouns, control verb subject, modifiers,
etc.).
the set of automaton paths it occurs in. During realisation,
two items will be compared only if the intersection of their
path sets is not empty (they appear in the same automaton
path). The result of a combination is labelled with the in-
tersection of the labels of the combined constituents. In this
way, the elementary items appearing in several paths of the
automaton are only introduced once in the chart and the fac-
torisation of both elementary and derived items that are com-
mon to several automaton path is ensured.
5 Paraphrase selection
As pointed out in the introduction, not all paraphrases are ap-
propriate in all contexts. To test the ability to generate contex-
tually appropriate sentences, we augmented the realiser with
a paraphrase selection mechanism based on the polarity fil-
tering system described in section (4.3). For instance, it is
possible to select from among the possible realisations for
regarde(j,m), jean(j), marie(m), the variant where
jean is verbalised as a cleft subject namely, C?est Jean qui re-
garde Marie (It is John who is looking at Mary).
More generally, the selection constraints allowed are
syntactico-semantic constraints of the form Synt:SemIndex
where Synt is a morpho-syntactic feature (declarative, inter-
rogative, cleft, pronoun, etc.) and SemIndex is an index oc-
curring in the input semantics.
Intuitively, a selection constraint supports the selection,
for a given semantic index, of the variant(s) obeying the
syntactico-semantic constraint set by the selection constraint
for that index.
Formally, these constraints are imposed during the polarity
filtering phase as follows. The syntactic properties supported
by the selection constraints are automatically associated dur-
ing grammar compilation to the elementary trees of the gram-
mar by means of so-called hypertags [Kinyon, 2000]. This is
made possible by the fact that the TAG used is derived from
a metagrammar [Crabbe? and Duchier, 2004] that is, from a
highly factorised way of representing the linguistic concepts
encoded in the TAG trees. Roughly, the metagrammar for-
malism is used (i) to define abstractions over these concepts
and (ii) to combine these abstractions so as to produce the el-
ementary trees of a TAG. During the metagrammar compila-
tion process, a so-called hypertag is built for each tree which
records the abstractions used to produce that tree. Thus hy-
pertags contain detailed information about the linguistic con-
tent of the TAG elementary trees. In particular, the hypertag
of the tree with clefted subject of the n0vn1 family (i.e., the
set of verbs taking two nominal arguments) will contain the
property +cleft:X where X is the semantic index associated
with the subject node.
During lexical selection, this index is instantiated by unifi-
cation with the input so that the selected elementary tree for
regarde will have the property +cleft:j.
Conversely, a restrictor is a property that a lexical item in-
tervening in the production of the generated paraphrases must
have. In the above example, the restrictor is -cleft:jmean-
ing that the j index must be realised by a clefted structure.
Paraphrase selection is implemented by parameterising the
realiser with a restrictor (for instance, -cleft:j). This re-
strictor is then used to initialise the polarity automaton and
eliminate (by polarity filtering) all these combinations which
do not contain the +cleft:j charge (since the negative
charge introduced during initialisation must be cancelled). As
a result, the realiser will only produce the variant:
(6) C?est Jean qui regarde Marie.
More generally, the polarity mechanism permits selecting
paraphrases on the basis of the information contained in the
grammar hypertags or in the TAG tree features. This infor-
mation, which is decided upon by the grammar writer, can be
both fine grained and of different natures.
Feature values can be used to control the feature values
associated with the root node of the constructed tree, typically
requiring that it is of interrogative, declarative or imperative
mood.
Hypertags can be used more generally to control the selec-
tion of the lexical items entering in the generated construct.
Importantly, the information they contain can be specified
both at the grammar and at the lexical level so that para-
phrase selection can then operate both on features determined
by syntax and on lexically determined characteristics (level
of speech, modality, type of semantic relation, thematic and
fore/backgrounding structure, etc;).
6 Implementation and Experimentation
The realiser described here has been implemented in Haskell.
It includes a graphical interface as well as a debugger so that
the user can inspect the content of the chart and of the agenda
at each step of the algorithm. It also supports batch process-
ing thus permitting a systematic evaluation of the impact of
various optimisations combinations. In what follows, we dis-
cuss the effect of polarity filtering and of paraphrase selection
in that system.
6.1 The effect of polarity filtering
To get an estimate of how our realiser compares with exist-
ing published results, we revisited the test cases discussed
in [Carroll et al, 1999] and [Koller and Striegnitz, 2002] by
producing similar sentences in French namely (7a) and (7b).
(7) a. Le directeur de ce bureau auditionne un nouveau consul-
tant d?Allemagne (The manager in that office interviews a
new consultant from Germany)
b. Le directeur organise un nouveau seminaire d?equipe heb-
domadaire special (The manager organises an unusual ad-
ditional weekly departmental conference).
The grammar used contains 2063 trees. In this grammar,
the verb organiser is associated with 107 trees and adjectives
with 8 trees. For the purpose of efficiency testing, we fur-
thermore treated the PP d?e?quipe as an adjective. As a result,
there are 107 ? 8 (856) combinations of lexical items cover-
ing the input semantics for example (7a) while for example
(7b), this number is 107? 84. The effect of polarity filtering
for these two examples is summarised in the following table.
That is, polarity filtering reduces the number of lexical
items combinations actually explored from 856 to 55 in the
first case and from 438 272 to 232 in the second.
Example 7a Example 7b
Possible combinations 856 438 272
Combinations explored 55 232
Sentences (w/o selection) 9 216
Figure 4: Filtering out combinations
Note furthermore that despite the overhead introduced by
the construction of the polarity automaton, polarity filtering
reduces overall processing times (cf. Figure 5).
Optimisations Example 7a Example 7b
none 14.8 s 93.8 s
pol 0.8 s 14.7 s
Carroll 1.8 s 4.3 s
Koller 1.4 s 0.8 s
Figure 5: Processing times
Thus, for the examples considered, processing times are
reduced by 95% and 84% respectively. The processing times
for (7a) compares favourably with those published for both
the Carroll et al and the Koller and Striegnitz realisers. This
comparison is not all that meaningful, however, since we are
using different grammars and significantly faster computers,
a 3 Ghz Pentium IV to the 700 Mhz Pentium III in [Koller
and Striegnitz, 2002].
Indeed, the poor performance of our surface realiser in ex-
ample (7b) is directly related to the degree of lexical ambi-
guity in our grammar. As illustrated in section 4.1, input se-
mantics with multiple modifiers pose a problem for surface
realisers. Although performing adjunction separately from
substitution prevents this problem from spilling over into in-
complete structures, the fact remains that n translate to n!
structures. Further aggravating the situation is that our gram-
mar provides 8 trees for every adjective, leading to 85?5!, or
3.9 million possible structures. When we modified our gram-
mar to only have one tree per adjective, our realisation times
dropped to 9s without filtering and 2.7s with. This exam-
ple calls to attention the fact that polarity filtering does not
account for lexical ambiguity in modifiers. In section 7, we
suggest some potential mechanisms for dealing with modi-
fiers, which we expect to be complementary to the filtering
technique.
6.2 Paraphrase selection
Paraphrase selection permits reducing the combinatorics one
step further. Thus introducing a cleft restrictor for examples
(7a) and (7b), causes the generator to produce fewer results,
2 sentences instead of 9 in the first example, and 18 instead
of 54 in the second.
These figures can be explained as follows. The grammar
allows 9 syntactic structures for the input considered namely:
(8) a. C?est par le directeur de ce bureau qu?un nouveau
consultant d?Allemagne est auditionne?
b. C?est le directeur de ce bureau qui auditionne un
nouveau consultant d?Allemagne
c. C?est un nouveau consultant d?Allemagne
qu?auditionne le directeur de ce bureau
d. C?est un nouveau consultant d?Allemagne que le
directeur de ce bureau auditionne
e. C?est un nouveau consultant d?Allemagne qui est
auditionne? par le directeur de ce bureau
f. Le directeur de ce bureau auditionne un nouveau
consultant d?Allemagne
g. Un nouveau consultant d?Allemagne est auditionne?
par le directeur de ce bureau
Since for the moment the grammar places no constraints on
the respective order of modifiers, there are 9 possible realisa-
tions for example (7a) and 9 ? 3! for example (7b). With the
object cleft restrictions on ?consultant?, these numbers drop
to 2 for the first example and to 2? 3! for the second.
Example 7a Example 7b
Sentences (w/o selection) 9 54
Sentences (with selection) 2 18
Figure 6: Selection
Accordingly, the processing time drops by 63% and 88%
with respect to simple polarities (cf. Figure 7).
Optimisations Example 7a Example 7b
none 14.8 s 93.8 s
pol 0.8 s 14.7 s
pol + select 0.3 s 1.8 s
Figure 7: Polarity + Selection
7 Related approaches
Several recent papers focus on improving the efficiency of
surface realisation. In this section, we relate our approach to
the HPSG based approach presented in [Carroll et al, 1999],
to the statistical and semi-statistical strategies used in [Ban-
galore and Rambow, 2000] and in [White, 2004] and to the
constraint based approach described in [Koller and Striegnitz,
2002]. We also briefly relate it to the greedy strategy used in
[Stone et al, 2003].
7.1 Copestake et al?s HPSG approach
As mentioned in section 4.1, multiple modifiers may trig-
ger an exponential number of intermediate structures. The
?adjunction after substitution? idea is inspired from the pro-
posal made in [Carroll et al, 1999] that a complete syntactic
skeleton be built before modifiers be inserted into that tree.
Because the Carroll et al proposal is set within the HPSG
framework however, extracted modifiers as in Which office
did John work in? need specific treatment. In contrast, in
TAG, all modifiers are treated using adjunction so that no spe-
cific treatment is required. All that is needed is that adjunc-
tion only be applied after all possible substitutions have been
carried out. A second, more meaningful difference is that no
such global optimisation as polarity filtering is proposed to
filter out on the basis of global information about the sets of
possible combinations, a priori invalid ones.
7.2 Statistical approaches
Interestingly, [White, 2004] proposes a treatment of modifiers
which is in some sense the converse of the ?adjunction after
substitution? treatment and where complete NPs are first built
before they are combined with the verb. This second option is
also feasible in TAG (adjunction would then apply on specific
sets of lexical entries and the results combined with the verb)
and it would be interesting to experiment and compare the
relative efficiency of both approaches within the TAG frame-
work.
Both approaches isolate the addition of modifiers to a con-
stituent, thereby avoiding spurious combinations with unre-
lated constituents; but neither directly address the fact that are
still an exponential n! ways to combine any n modifiers for
a single constituent. [White, 2004] and [Bangalore and Ram-
bow, 2000] propose statistical solutions to this problem based
on a linear n-gram language model. In White?s approach the
statistical knowledge is used to prune the chart of identical
edges representing different modifier permutations, e.g., to
choose between fierce black cat and black fierce cat. Bangalore
assumes a single derivation tree that encodes a word lattice (a
{fierce black, black fierce} cat), and uses statistical knowledge
to select the best linearilisation. Our framework does not cur-
rently implement either approach, but we hope to adopt an ap-
proach similar to Bangalore?s. Rather than directly perform-
ing adjunction, we associate each node with the set of auxil-
iary trees (modifiers) that are to be adjoined to that node. The
order in which these modifiers are adjoined can be decided
through statistical methods.
There are three other uses for probabilistic techniques: for
lexical selection, optimisation and ranking. Such techniques
are useful for guiding the surface realiser towards a single
best result (or a relatively small number thereof). On the
other hand, we aim to produce all possible paraphrases, that
is explore the entire search space of syntactic variants, and
so with the exception of modifier ordering, we eschew the
use of probabilities in favour of an ?exact method? [G. Bon-
fante, 2004]. While Bangalore uses a tree model to produce
a single most probable lexical selection, we use polarities to
filter out all the definitely impossible ones. While in White?s
system, the best paraphrase is determined on the basis of n-
gram scores that is, on the basis of frequency, in our approach
?best? means ?most contextually appropriate?. Indeed, the
restrictors we use to select a paraphrase, although they are
here given by hand, could equally be set by the context and
so permit modelling the effect of contextual constraints on
paraphrases. We believe that our approach, modulo statis-
tical handling of modifiers, would be roughly equivalent to
White?s with anytime-searching disabled.
7.3 Koller et al?s constraint-based approach
Finally, our approach has interesting connections to the
constraint-based approach proposed by [Koller and Strieg-
nitz, 2002]. In this approach, the subset of the TAG grammar
which is used for a given realisation task is translated into a
set of lexical entries in a dependency grammar defining well
formed TAG derivation trees. This set of entries is then parsed
by an efficient constraint-based dependency parser thus pro-
ducing the derivation trees associated by the grammar with
the set of input lexical entries. A post processing phase pro-
duces the derived trees on the basis of the derivation trees
output by the first step.
The main similarity between this and our approach is that
they both use a global mechanism for filtering out combina-
tions of lexical entries that cannot possibly lead to a syntac-
tically valid sequences. In the Koller et al approach, this
filtering is based on well formed derivation trees (only these
combinations of lexical entries that form a valid derivation
tree are considered) whereas in ours, it is based on polarities
and on the cancelling out of syntactic resources and require-
ments. As a preliminary evaluation shows, such a global op-
timisation is very efficient in pruning the search space.
There are differences though. In particular, while Koller et
al. explicitly ignores feature information, our algorithm han-
dles a TAG with fully specified feature structures. Further
while in our approach, the processing of the valid combina-
tions is done using a tabular algorithm optimised to avoid spu-
rious derivations, the postprocessing step producing derived
trees from derivation trees is left undefined in the Koller et al
approach. Finally, while the Koller et al approach is based
on constraint propagation, ours is based on finite state tech-
niques. These differences open up the door for interesting
comparisons and combinations. It would be interesting for
instance to combine the Koller et alapproach with the tab-
ular surface realisation algorithm presented in this paper, or
to compare run times once feature structures are taken into
account.
7.4 Stone?s greedy approach
[Stone et al, 2003] presents a greedy approach to TAG based
surface realisation. The greedy search applies iteratively to
update a single state in the search space. On each iteration,
all neighbours of the current state are produced but only one
state is chosen at the next current state, based on a heuristic
evaluation.
[Stone et al, 2003]?s search strategy is therefore markedly
different from ours. While we explore the entire search
space and use polarities to control the combinatorics, Stone?s
greedy strategy is a best first strategy which incrementally
trims the search space using heuristics. In terms of efficiency,
the greedy strategy is of course better. The goals behind the
two approaches are distinct however. Thus while Stone?s ap-
proach aims at modelling the interaction of the various mech-
anisms involved in microplanning, the present proposal is di-
rected towards generating and selecting paraphrases. In par-
ticular, we are interested in using the realiser to debug a para-
phrastic grammar that is, a grammar which alleviates the in-
ference task by assigning paraphrases the same semantics ?
this can only be done by adopting an exhaustive search strat-
egy. More generally, ?exhaustive surface realisation? pro-
vides a natural way to debug grammars and reduce their de-
gree of overgeneration. Since the combinatorics is not only
theoretically (worse case analysis) but also practically very
high, it is worth investigating ways of optimising surface re-
alisers which perform an exhaustive search.
8 Conclusion
We have presented a surface realiser for TAG which is opti-
mised to support the generation of grammatical paraphrases
while also permitting the selection, on the basis of syntactico
semantic constraints, of a particular paraphrase. The most
efficient optimisation proposed concerns polarity filtering, a
global technique that permits the elimination of combinations
of lexical items which cannot possibly lead to a syntactically
valid sentence. While used here for generating with TAG, the
technique is fully general and can be used for parsing [Perrier,
2003] but also for generating with other grammatical frame-
works.
Future work will concentrate on extending the grammar
and the lexicon to other types of paraphrases (in particu-
lar, morphoderivational or cross categorical paraphrases), on
providing a systematic evaluation of the paraphrase selection
mechanism and on using the realiser for the debugging of an
existing TAG for French.
References
[Bangalore and Rambow, 2000] S. Bangalore and O. Ram-
bow. Using TAGs, a tree model and a language model
for generation. In Proceedings of TAG+5, Paris, France,
2000.
[Brew, 1992] C. Brew. Letting the cat out of the bag: Gener-
ation for shake-and-bake MT. In Proceedings of COLING
?92, Nantes, France, 1992.
[Carroll et al, 1999] J. Carroll, A. Copestake, D. Flickinger,
and V. Paznan?ski. An efficient chart generator for (semi-
)lexicalist grammars. In Proceedings of EWNLG ?99,
1999.
[Copestake et al, 2001] A. Copestake, A. Lascarides, and
D. Flickinger. An algebra for semantic construction in
constraint-based grammars. In Proceedings of the 39th
ACL, Toulouse, France, 2001.
[Crabbe? and Duchier, 2004] B. Crabbe? and D. Duchier.
Metagrammar redux. In International Workshop on Con-
straint Solving and Language Processing - CSLP 2004,
Copenhagen, 2004.
[G. Bonfante, 2004] G. Perrier G. Bonfante, B. Guillaume.
Polarization and abstraction of grammatical formalisms as
methods for lexical disambiguation. In Proceedings of
CoLing 2004, 2004.
[Gardent and Kallmeyer, 2003] C. Gardent and
L. Kallmeyer. Semantic construction in ftag. In
Proceedings of the 10th EACL, Budapest, Hungary, 2003.
[Gross, 1975] M. Gross. Me?thodes en syntaxe. Masson,
Paris, 1975.
[Hepple, 1991] M. Hepple. Efficient incremental processing
with categorial grammar. In Proceedings of the 29th ACL,
Berkeley, 1991.
[Kay, 1996] M. Kay. Chart Generation. In 34th ACL, pages
200?204, Santa Cruz, California, 1996.
[Kinyon, 2000] A. Kinyon. Hypertags. In Proceedings COL-
ING, Sarrebruck, 2000.
[Koller and Striegnitz, 2002] A. Koller and K. Striegnitz.
Generation as dependency parsing. In Proceedings of the
40th ACL, Philadelphia, 2002.
[Mel?c?uk, 1988] I. Mel?c?uk. Paraphrase et lexique dans la
the?orie linguistique sens-texte. Lexique, 6:13?54, 1988.
[Perrier, 2003] G. Perrier. Les grammaires d?interaction,
2003. Habilitation a` diriger les recherches en informa-
tique, universite? Nancy 2.
[Stone et al, 2003] M. Stone, C. Doran, B. Webber,
T. Bleam, and M. Palmer. Microplanning with commu-
nicative intentions: the SPUD system. Computational In-
telligence, 19(4):311?381, 2003.
[Vijay-Shanker and Joshi, 1988] K. Vijay-Shanker and
A. Joshi. Feature based tags. In Proceedings of the 12th
ACL, pages 573?577, Budapest, 1988.
[White, 2004] M. White. Reining in CCG chart realization.
In INLG, pages 182?191, 2004.
A Appendix
Algorithm 1 The GenI algorithm
1: procedure GENERATE(Gram,Sem)
2: AgendaA? ?; Agenda? ?; Chart? ?
3: for all trees t ? Gram such that t?s semantics subsumes
Sem do
4: Agenda? Agenda + t
5: end for
6: while Agenda 6= ? do
7: t? any tree ? Agenda
8: delete t from Agenda
9: if t has a foot node and no substitution nodes then
10: AgendaA? AgendaA + t
11: else
12: for all trees c ? Chart which can combine with t
via substitution into a new tree ct do
13: Agenda? Agenda + ct
14: end for
15: Chart? Chart + t
16: end if
17: end while
18: delete from Chart any tree with a substitution node
19: Agenda? Chart
20: Chart? AgendaA
21: while Agenda 6= ? do
22: t? any tree ? Agenda
23: delete t from Agenda
24: if t?s semantics is Sem then
25: return the string corresponding to t
26: else
27: for all trees c ? Chart which can combine with t
via adjunction into a new tree ct do
28: Agenda? Agenda + ct
29: end for
30: end if
31: end while
32: end procedure
buy(e,j,f) annoying(e) field(f)
v ache`te (+p -2n) n0Vadj ennuyeux (+p -n) n field (+n)
Pe
N?j Ve N?f
ache`te
P
N?e V Adj
est ennuyeux
Nf
terrain
john(j)
n achat (+n -2n) p0Vadj ennuyeux (+p -p) n jean (+n)
Ne
N GP GP
achat P N?f P N?j
par de
P
P?e V Adj
est ennuyeux
Nj
jean
Figure 8: Grammar for example 5
Figure 9: A minimised polarity automaton
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 97?102,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Three reasons to adopt TAG-based surface realisation
Claire Gardent
CNRS / LORIA
615, rue du Jardin Botanique
F-54 600 Villers-Le`s-Nancy
gardent@loria.fr
Eric Kow
INRIA / LORIA
Universite? Henri Poincare?
615, rue du Jardin Botanique
F-54 600 Villers-Le`s-Nancy
kow@loria.fr
Abstract
Surface realisation from flat semantic for-
mulae is known to be exponential in the
length of the input. In this paper, we argue
that TAG naturally supports the integration
of three main ways of reducing complex-
ity: polarity filtering, delayed adjunction
and empty semantic items elimination. We
support these claims by presenting some
preliminary results of the TAG-based sur-
face realiser GenI.
1 Introduction
Surface realisation consists in producing all the
sentences associated by a grammar with a given
semantic formula. For lexicalist grammars such
as LTAG (Lexicalised Tree Adjoining Grammar),
surface realisation usually proceeds bottom-up
from a set of flat semantic literals1. However,
surface realisation from flat semantic formulae is
known to be exponential in the length of the input
(Kay96; Bre92; KS02). In this paper, we abstract
from the TAG based surface realiser for French
GenI, (GK05) and argue that TAG naturally sup-
ports the integration of various proposals made to
help reduce either surface realisation or parsing
complexity into a TAG based, lexically driven sur-
face realiser. Specifically, we show:
1. that TAG elementary trees naturally support
the implementation of a technique called po-
larity filtering used to reduce the exponen-
tial factor introduced by lexical ambiguity
(Per03),
1See e.g., (CCFP99) for a discussion summarising the rea-
sons for this choice.
2. that TAG two operations of substitution and
adjunction provides a natural framework for
implementing a delayed adjunction mecha-
nism capable of reducing the complexity due
to the lack of ordering information and
3. that TAG extended domain of locality helps
reduce the potential complexity increment in-
troduced by semantically empty items such as
infinitival ?to? or complementiser ?that?.
2 Surface realisation, flat semantics and
computational complexity
Why is surface realisation exponential in the
length of the input? As shown in (Kay96), one
reason for this is the lack of ordering information.
Contrary to parsing where the input is a string i.e.,
an ordered list of words, the input to surface re-
alisation is a set of literals. Supposing each lit-
eral selects exactly one constituent in the lexicon,
then the number of possible combinations between
these constituents will be 2n (the number of sub-
sets obtainable from a set of size n).
In practice of course, there are possible restric-
tions on constituent combination. In particular,
most existing realisers impose the constraint that
only constituents with non overlapping semantics
and compatible indices can be combined. Be-
cause of this restriction, the core of the complex-
ity stems in practice from intersective modifiers
(Bre92; Kay96). Given a set of n modifiers all
modifying the same structure, all possible inter-
mediate structures will be constructed i.e. 2n+1.
A second reason for the exponential complexity
of surface realisation is lexical ambiguity. As for
bottom-up parsing, in surface realisation from flat
semantics, the input is used to select a set of lexi-
cal entries namely all lexical entries whose seman-
97
tics subsumes one or more of the input literals. In
a realistic grammar, one literal will be associated
with more than one lexical entries. So if Lexi is the
number of lexical entries associated with literal li,
then for an input semantics comprising n literals,
the number of sets of lexical constituents covering
the input semantics is:
?i=n
i=1 Lexi
The two sources of complexity interact by mul-
tiplying out so that the potential number of combi-
nations of constituents is:
2n ?
i=n
?
i=1
Lexi
In what follows, we show that TAG naturally
supports various optimisations that have been pro-
posed to reduce the search space.
3 Polarity filtering
To restrict the impact of lexical ambiguity on pars-
ing efficiency, (Per03) introduces a method called
Polarity filtering. This method is based on the ob-
servation that many of the combinations of lexi-
cal entries which cover the input semantics are in
fact syntactically invalid either because a syntactic
requirement is not fulfilled or because a syntactic
resource is not used. Accordingly, polarity based
filtering eliminates such combinations by:
? assigning each lexical entry with a set of po-
larities reflecting its syntactic requirements
and resources,
? computing for each possible combination of
lexical entries the sum of its polarities and
? only allowing surface realisation on combi-
nations which have a net sum of zero (all re-
quirements are satisfied and all resources are
used).
By filtering the initial search space before the
tree combination phase, polarity filtering in effect
reduces the impact of lexical ambiguity i.e. de-
creases
?i=n
i=1 Lexi.
The definitory properties of TAG elementary
trees provide a natural way to assign polarities to
a TAG lexical entries: each elementary tree can be
associated with a polarity +C , where C is the cat-
egory of its root node and each substitution or foot
node in that tree, a polarity ?C is added, where C
is the category of that node.
We implemented polarity filtering in GenI
based on this way of associating lexical entries
with polarities2 . We then measured the impact of
this filtering on the initial search space (the num-
ber of sets of lexical items actually explored by
the realiser), on space (measured by the number
of chart items created) and on time.
Table 1 summarises the impact of polarity fil-
tering on the initial search space3. possible indi-
cates the number of combinations of lexical entries
which cover the input semantics and thus can po-
tentially lead to a valid syntactic tree realising the
input semantics and explored gives the number of
combinations actually explored by the surface re-
aliser after polarity filtering has ruled out combi-
nations which cannot possibly lead to a valid syn-
tactic tree).
As is to be expected, the impact increases with
the number of input literals so that while polarity
filtering divides the initial search space by 35.6 for
an input ranging between 1 and 6 literals, it divides
it by 441.6 for an input size ranging between 14
and 16 literals
literals possible explored (?)
1-6 199.10 5.60 35.6
7-9 6460.88 40.06 161.3
10-13 43028.25 137.06 313.9
14-16 292747.64 662.91 441.6
Figure 1: Polarity filtering and initial space
(Sets of initial trees covering the input semantics)
Table 2 gives the impact of polarity filtering on
space as measured by the number of created chart
items (or constituents). The first column (w/o pol.)
gives the number of created charted items when
polarity filtering is switched off and the second,
(with pol.) when polarity filtering is on. As can
be seen, the effect is particularly pronounced when
the input exceeds 10 literals.
Finally, Figure 3 shows that the overhead intro-
duced by the construction of the polarity automa-
ton means that formulae under 10 literals are re-
alised in roughly the same time with or without po-
larity filtering. However, for larger sentences, po-
larity filtering is increasingly important in keeping
realisation times reasonable. For instance, given
an input ranging between 14 and 16 literals, polar-
2See (GK05) for more details.
3For each group of input (1-6 literals, 7-9, etc.), measures
are based on an average of 15 cases.
98
literals w/o pol. with pol. (?)
1-6 146.40 83.60 1.8
7-9 3273.50 1281.25 2.6
10-13 7468.06 702.50 10.6
14-16 17502.36 1613.91 10.8
Figure 2: With and without Polarity filtering
(Chart items)
ity filtering divides realisation time by 5, that is,
yields a realisation time of 2.21 seconds instead of
11.61.
literals w/o pol. with pol. (?)
1-6 0.81 0.79 1.0
7-9 1.68 1.35 1.2
10-13 3.56 1.88 1.9
14-16 11.61 2.21 5.3
Figure 3: With and without Polarity filtering (CPU
times)
4 Substitution/adjunction distinction
One important specificity of TAG is that it includes
two combination operations namely, adjunction
and substitution. We now show that this feature
of TAG is particularly useful in improving surface
realisation performance.
4.1 Reducing the impact of intersective
modifiers
To restrict the combinatorics induced by modi-
fiers, (CCFP99; CO05) proposes either to han-
dle modifiers after a complete syntactic tree is
built (i.e., after all syntactic requirements are ful-
filled) or before the modifiee is combined with
other items (e.g., before the head noun has com-
bined with a determiner). Although the number of
intermediate structures generated is still 2n for n
modifiers, both strategies have the effect of block-
ing these 2n structures from multiplying out with
other structures in the chart. More precisely, given
an input semantics of size n where k of its liter-
als are to be realised as modifiers, the number of
intermediate structures possible in the two phase
approach is 2k + 2n?k, which can be considerably
smaller than 2n, depending on the size of k.
In TAG, we can make use of the fact that substi-
tution and adjunction apply independently of each
other to implement a two-phase generation strat-
egy where modifiers are handled only after a com-
plete syntactic tree is built. In the first phase,
only substitutions are performed and in the sec-
ond, only adjunctions. Additionally, before ad-
junction starts, all unsaturated trees (trees with
unfilled substitution sites) are discarded from the
chart thereby ensuring that modifiers do not com-
bine with structures that cannot possibly lead to a
valid result (since no constituent could be found to
fill the unsaturated substitution sites).
Since in TAG, modifiers always involve the use
of adjunction, modifiers will always be handled by
the second phase of the algorithm and thereby ad-
joined into ?saturated trees? i.e., trees devoid of
unfilled substitutions sites. In this way, the prolif-
eration of structures induced by the modifiers can
be restricted.
The substitution-before-adjunction strategy was
integrated in GenI yielding the improvements in-
dicated in Figures 4 and 5.
literals 1 phase 2 phase (?)
? 3 0.73 0.73 1.0
4 0.74 0.75 1.0
5 0.97 0.93 1.0
6 2.91 0.89 3.3
7 4.24 1.30 3.3
? 8 Time out
Figure 4: With and without SBA (CPU times)
literals 1 phase 2 phase (?)
? 3 47.00 44.33 1.1
4 107.00 108.00 1.0
5 310.00 263.00 1.2
6 1387.33 883.00 1.6
7 2293.50 761.33 3.0
Figure 5: With and without SBA (Chart items)
As table 4 shows, when there is more than 7 lit-
erals in the input, the one-phase algorithm times
out. More in general, for the data shown, the two
phase strategy leads to an average decrease in time
ranging between 1 and 3.3% and a decrease in
space varying between 1.1% and 3% respectively.
Although the poor performance of the 1 phase
algorithm is in part due to a very large and strongly
overgenerating grammar4 , the data clearly shows
that SBA is essential in supporting large scale TAG
based surface realisation.
4The grammar used is a grammar for French which con-
tains roughly 3 400 initial trees (CD04).
99
4.2 Substitution-before-adjunction combined
with Polarity Filtering
The substitution-before-adjunction strategy limits
the impact of intersective modifiers by restricting
the number of constituents the modifiers can com-
bine with within one set of lexical items. Because
polarity filtering reduces the number of sets of lex-
ical items to be considered, it trivially also reduces
the number of sets of lexical items involving ad-
junctions.
The space improvement provided by combining
the substitution-before-adjunction (SBA) strategy
with polarity filtering is illustrated in Figures 6
and 7 which show the space reduction associated
with cases ordered either according to their num-
ber of literals or according to their number of foot
nodes (i.e., adjunction cases). As should be ex-
pected, the number of foot nodes is more highly
correlated with a space reduction. Specifically,
a combined SBA/polarity strategy divides by 3.4
the space used for cases involving between 1 and
12 auxiliary trees; and by 18.8 the space used for
cases involving between 14 and 16 auxiliary trees.
literals w/o pol. with pol. (?)
1-6 367.90 109.50 3.4
7-9 6192.69 1550.19 4.0
10-13 11211.06 711.06 15.8
14-16 30660.27 1631.64 18.8
Figure 6: SBA + Polarity (Chart items)
# aux trees w/o pol. with pol. (?)
1-12 2124.27 620.82 3.4
13-120 8751.53 1786.47 4.9
121-190 11528.43 611.50 18.9
191-350 25279.75 1085.75 23.3
Figure 7: SBA + Polarity (Chart items)
4.3 Filtering out unusable trees
Another interesting aspect of TAG?s use of two
combination operations and more specifically of
the substitution-before-adjunction strategy is that
it naturally supports the inclusion of a third phase
to filter out unusable trees that is, trees which can
be determined not to be integrable in any valid
derivation. Specifically, this third phase occurs be-
tween substitution and adjunction and filters out:
? all trees with an unfilled substitution site
? all saturated trees whose root node is not la-
belled with an S category
The first filter (elimination of unsaturated trees)
is required, as indicated above, to restrict the im-
pact of intersective modifiers: by discarding them,
we restrict adjunction to saturated trees. The sec-
ond, makes use of the property of auxiliary trees
which insists that root and foot node be labelled
with the same category. Because of this property,
adjunction cannot affect the category of the tree it
adjoins to. In particular, a tree which after all pos-
sible substitutions have been performed, has root
label C with C 6= S can never lead to the creation
by adjunction of a tree with root label S. Hence it
can be discarded (provided of course, the genera-
tor is seeking to build sentences).
Figures 8 and 9 illustrate the impact of this sec-
ond filter (called the Root Node Filter, RNF) on
the chart size when polarity filtering is switched
off. As for SAB, the figures show a higher correla-
tion between the RNF and the number of adjunc-
tion nodes than with the number of literals. In-
triguingly, the impact of the filter is proportionally
higher on sentences with fewer foot nodes. Al-
though this needs to be checked more thoroughly,
the explanation for this could be the following.
The trees removed by the Root Node Filter are sat-
urated tree not rooted in S hence essentially sat-
urated NP trees. Examination of the data reveals
that the number of these trees removed by the RNF
remains almost constant (though this might be an
ad hoc property of the specific testsuite used).
Hence in proportion, the effect of the RNF dimin-
ishes.
Note however that in absolute terms, the num-
ber of trees whose derivation is avoided by the
RNF remains quite high thus contributing to an
overall better performance.
literals w/o RNF with RNF (?)
1-6 367.90 146.40 2.5
7-9 6192.69 3273.50 1.9
10-13 11211.06 7468.06 1.5
14-16 30660.27 17502.36 1.8
Figure 8: Root node filter w/o Pol (Chart Items).
As Figures 10 and 11 show, combining the Root
Node Filter with polarity filtering simply rein-
forces the biases noted above: Root Node Filtering
is proportionally more effective for short input but
can remain useful in absolute terms. A more thor-
100
# aux trees w/o RNF with RNF (?)
1-12 2124.27 527.36 4.0
13-120 8751.53 5570.33 1.6
121-190 11528.43 6490.14 1.8
191-350 25279.75 15469.17 1.6
Figure 9: Root node filter w/o Pol (Chart Items).
ough investigation of the data and further exper-
iments are needed however to determine whether
such behaviour is not tied to some ad hoc property
of our (still too limited) testsuite.
literals w/o RNF with RNF (?)
1-6 109.50 83.60 1.3
7-9 1550.19 1281.25 1.2
10-13 711.06 702.50 1.0
14-16 1631.64 1613.91 1.0
Figure 10: Root node filter + Pol (Chart Items).
# aux trees w/o RNF with RNF (?)
1-12 422 621 1.5
13-120 1627 1786 1.1
121-190 600 612 1.0
191-350 1073 1086 1.0
Figure 11: Root Node Filter + Pol (Chart Items).
5 TAG extended domain of locality
Arguably there are words such as complementiser
that or infinitival to whose semantics is empty.
These words are to surface realisation what gaps
(or empty categories) are to parsing. In a naive ap-
proach, they require that all trees with an empty
semantics be considered as potential constituent
candidate at each combining step. In terms of ef-
ficiency, this roughly means increasing the size of
the input n (just like postulating gaps at all po-
sition in an input string increases the size of that
string).
To avoid this shortcoming, a common practice
(CCFP99) consists in specifying a set of rules
which selects empty semantic items on the basis
of the input literals. However these rules fail to re-
flect the fact that empty semantic items are usually
functional words and hence governed by syntactic
rather than semantic constraints.
By contrast, in a TAG based surface realiser,
TAG elementary trees provide a natural way to
specify the syntactic environment in which empty
semantic items can occur. For instance, comple-
mentiser that occurs with verbs taking a sentential
argument which is generally captured by includ-
ing the complementiser as a co-anchor in the trees
of these verbs.
More in general, the extended domain of local-
ity provided by TAG elementary trees, together
with the possibility of specifying co-anchors
means that empty semantic items can be avoided
altogether. Hence they do not require specific
treatment and have no impact on efficiency.
6 Discussion
We have argued that TAG presents several fea-
tures that makes it particularly amenable to the
development of an optimised surface realiser. We
now summarise these features and briefly compare
TAG with CCG (Combinatory Categorial Gram-
mar) and HPSG (Head Driven Phrase Structure
Grammar) based surface realisation.
6.1 Using tree node types
The different types of tree nodes identified by TAG
can be used to support polarity filtering whereby
substitution nodes can be associated with negative
polarities (requirements) and root nodes with pos-
itive polarities (resources). As our preliminary ex-
periments show, polarity filtering has a significant
impact on the initial search space, on the space
used and on CPU times.
So far, this particular type of global filtering
on the initial search space has been used neither
in the HPSG (CCFP99; CO05) nor in the CCG
(Whi04) approach. Although it could presumably
be adapted to fit these grammars, such an adapta-
tion is in essence less straightforward than in TAG.
In CCG, the several combination rules mean
that a subcategory can function either as a re-
source or as a requirement depending on the rule
that applies. For instance, in the verbal category
(S\NP )/NP , the subcategory S\NP functions
as a resource when NPs are type raised (it satisfies
the requirement of a type raised NP with category
S/(S\NP )). However it will need to be further
decomposed into a resource and a requirement if
they are not. More in general, polarity specifica-
tion in CCG would need to take into account the
several combination rules in addition to the cate-
gory structure. In HPSG, it is the interaction of
lexical categories with lexical and phrasal rules
that will need to be taken into consideration.
101
6.2 Using rule types
The two types of tree combining operations per-
mitted by TAG can be used to structure the sur-
face realisation algorithm. As we?ve shown, per-
forming all substitutions before allowing for ad-
junction greatly reduces the exponential impact of
intersective modifiers. Moreover, combining such
a substitution-before-adjunction strategy with po-
larity filtering further improves performance.
In comparison, the HPSG and the CCG ap-
proach do not support such a natural structuring
of the algorithm and intersective modifiers induce
either a pre- or a post-processing.
In HPSG, intersective modifiers are discarded
during the chart generation phase and adjoined
into the generated structures at a later stage. This
is inelegant in that (i) intersective modifiers are ar-
tificially treated separately and (ii) structures sub-
ject to adjunction have to be non monotonically
recomputed to reflect the impact of the adjunction
in that part of the tree dominating the adjunction.
In CCG, the input logical form is chunked into
subtrees each corresponding to a separate gen-
eration subproblem to be solved independently.
Again the approach is ad hoc in that it does not
rely on a given grammatical or linguistic property.
As a result, e.g., negation needs special treatment
to avoid incompleteness (if the heuristic applies,
negated sentences cannot be generated). Similarly,
it is unclear how long distance dependencies in-
volving modifiers (e.g., Which office did you say
that Peter work in ?) are handled.
6.3 Using TAG extended domain of locality
TAG extended domain of locality means that
empty semantic items need no special treatment.
In contrast, both the HPSG and the CCG approach
resort to ad hoc filtering rules which, based on
a scan of the input semantics, add semantically
empty items to the chart.
7 Further research
Although the results presented give strong evi-
dence for the claim that TAG naturally supports
the development of an optimised surface based re-
aliser, they are based on a limited testsuite and on
a core grammar for French that heavily overgen-
erates. Hence they do not truly reflect the poten-
tial of the proposed optimisations on the perfor-
mance of a large scale surface realiser. Current
work concentrates on remedying these shortcom-
ings. In particular, we are working on develop-
ing a structured test suite which permits a pre-
cise measure of the impact of different factors both
on complexity and on the optimisations used. In
this testsuite for instance, each item is associated
with a series of indicators concerning its potential
complexity: number of literals in the correspond-
ing input semantics, number of trees, number of
nodes, number of substitutions nodes and number
of foot nodes in the corresponding selection of ini-
tial trees.
Further work also includes restricting overgen-
eration and exploring in how far, polarity filtering
can be used to select one among the many para-
phrases
References
C. Brew. Letting the cat out of the bag: Generation
for shake-and-bake MT. In Proceedings of COLING
?92, Nantes, France, 1992.
J. Carroll, A. Copestake, D. Flickinger, and
V. Paznan?ski. An efficient chart generator for
(semi-)lexicalist grammars. In Proceedings of
EWNLG ?99, 1999.
B. Crabbe? and D. Duchier. Metagrammar redux. In
International Workshop on Constraint Solving and
Language Processing - CSLP 2004, Copenhagen,
2004.
J. Carroll and S. Oepen. High efficiency realization for
a wide-coverage unification grammar. In R. Dale
and K-F. Wong, editors, Proceedings of the Sec-
ond International Joint Conference on Natural Lan-
guage Processing, volume 3651 of Springer Lec-
ture Notes in Artificial Intelligence, pages 165?176,
2005.
C. Gardent and E. Kow. Generating and select-
ing grammatical paraphrases. In Proceedings of
the 10th European Workshop on Natural Language
Generation, Aberdeen, Scotland, 2005.
M. Kay. Chart Generation. In 34th ACL, pages 200?
204, Santa Cruz, California, 1996.
A. Koller and K. Striegnitz. Generation as dependency
parsing. In Proceedings of the 40th ACL, Philadel-
phia, 2002.
G. Perrier. Les grammaires d?interaction, 2003. Ha-
bilitation a` diriger les recherches en informatique,
universite? Nancy 2.
M. White. Reining in CCG chart realization. In INLG,
pages 182?191, 2004.
102
Adjective based inference?
Marilisa Amoia
INRIA/Universite? de Nancy 1 &
University of the Saarland
Saarbru?cken Germany
amoia@coli.uni-sb.de
Claire Gardent
CNRS/Loria
Campus Scientifique BP 239
54506 Vandoeuvre-les-Nancy, France
claire.gardent@loria.fr
Abstract
In this paper, we propose a fine grained
classification of english adjectives geared
at modeling the distinct inference patterns
licensed by each adjective class. We show
how it can be implemented in description
logic and illustrate the predictions made
by a series of examples. The proposal has
been implemented using Description logic
as a semantic representation language and
the prediction verified using the DL theo-
rem prover RACER.
Topics: Textual Entailment, Adjectival Semantics
1 Introduction
Understanding a text is one of the ultimate goals
of computational linguistics. To achieve this goal,
systems need to be developed which can construct
a meaning representation for any given text and
which furthermore, can reason about the meaning
of a text. As is convincingly argued in (Ido Dagan
and Magnini, 2005), one of the major inference
task involved in that reasoning is the entailment
recognition task:
Does text T1 entail text T2?
Indeed entailment recognition can be used to
determine whether a text fragment answers a
question (e.g., in question answering application),
whether a query is entailed by a relevant document
(in information retrieval), whether a text fragment
entails a specific information nugget (in informa-
tion extraction), etc.
Because the Pascal RTE challenge focuses on
real text, the participating systems must be robust
that is, they must be able to handle unconstrained
?We thank la Re?gion Lorraine, INRIA and the University
of Sarrebruecken for partially funding the research presented
in this paper.
input. Most systems therefore are based on sta-
tistical methods (e.g., stochastic parsing and lex-
ical distance or word overlap for semantic simi-
larity) and few provide for a principled integra-
tion of lexical and compositional semantics. On
the other hand, one of the participant teams has
shown that roughly 50% of the RTE cases could
be handled correctly by a system that would ade-
quately cover semantic entailments that are either
syntax based (e.g., active/passive) or lexical se-
mantics based (e.g., bicycle/bike). Given that the
overall system accuracies hovered between 50 and
60 percent with a baseline of 50 %1, this suggests
that a better integration of syntax, compositional
and lexical semantics might improve entailment
recognition accuracy.
In this paper, we consider the case of adjectives
and, building on approaches like those described
in (Raskin and Nirenburg, 1995; Peters and Pe-
ters, 2000), we propose a classification of adjec-
tives which can account for the entailment patterns
that are supported by the interaction of their lexi-
cal and of their compositional semantics. We start
by defining a classification schema for adjectives
based on their syntactic and semantic properties.
We then associate with each class a set of axioms
schemas which translate the knowledge about lex-
ical relations (i.e. antonymy) the adjectives of the
class are involved in by extracting this information
from WordNet (Miller, 1998) and a set of seman-
tic construction rules and we show that these cor-
rectly predicts the observed entailment patterns.
For instance, the approach will account for the fol-
lowing (non)-entailment cases:
(1) a. John frightened the child
|= The child is afraid
150% of the cases were true entailment and 50% were
false ones, hence tossing a coin would get half of the cases
right.
20 KRAQ06
b. Peter claims that John is a murderer
|= John is an alledged murderer
6|= John is a murderer
c. This is a fake bicycle
|= This is a false bike
|= This is not a real bike
6|= This is a bike
d. John is not awake
|= John sleeps
6|= John does not sleep
The approach is implemented using Description
Logic as a semantic representation language and
tested on a hand-built semantic test suite of ap-
proximately 1 000 items. In the latter part of the
paper we discuss this testsuite and the philosophy
behind it.
2 A fine grained classification for
adjectives
As mentioned above, we propose a classification
of adjectives based on their lexical, their model
theoretic and their morpho-derivational properties.
To facilitate the link with compositional semantics
(the construction of a meaning representation for
sentences containing adjectives), we also take into
account syntactic properties such as the predica-
tive/attributive or the static/dynamic distinction.
We now detail each of these properties. The over-
all categorisation system is given in Figure 1.
2.1 Model theoretic properties
The main criteria for classification are given by
(Kamp, 1975; Kamp and Partee, 1995) seman-
tic classification of adjectives which is based on
whether it is possible to infer from the Adj+N
combination the Adj or the N denotation.
Intersective adjectives (e.g., red) licence the
following inference inference patterns:
A + N |= A
A + N |= N
For instance, if X is a red car then X is a car and
X is red
Subsective adjectives (e.g., big) licence the
following inference pattern:
A + N |= N
For instance, if X is a big mouse, then X is a mouse
but it is not necessarily true X is big
Privative adjectives licence the inference pattern:
A + N |= ?N
For instance, if X is a fake gun then X is not a gun
Plain non-subsective adjectives (e.g., alledged)
do not licence any inference
For instance, if X is an alleged murderer then it is
unknown whether X is a murderer or not
2.2 Lexical semantics
From the lexical semantics literature, we take
one additional classification criterion namely
antonymy. As described in (Cruse, 1986), this
term covers different kinds of opposite polarity re-
lations between adjectives namly, binary opposi-
tion, contraries and multiple oppositions.
Binary oppositions covers pairs such as wet/dry
which license the following inference pattern:
A1 ? ?A2 ? ?A1 ? A2
So that in particular:
wet ? ?dry ? ?wet ? dry
Contraries are pairs such as long/short where the
implication is unidirectional:
A1 |= ?A2 ? ?A1 6|= A2
A2 |= ?A1 ? ?A2 6|= A1
and in particular:
long |= ?short ? ?long 6|= short
short |= ?long ? ?short 6|= long
Multiple oppositions involve a finite set of adjec-
tives (e.g., linguistic/economic/mathematical/... )
which are pairwise mutually exclusive. For a set
of opposed adjectives A1 . . . An, the following ax-
ioms schemas will be licensed:
?i, j s.t. 1 ? i, j ? and i 6= j
Ai |= ?Aj and ?Ai 6|= Aj
2.2.1 Derivational morphology
We also take into account related forms that is,
whether there exists a verb (Va) or a noun that is
semantically related to the adjectives being con-
sidered. Moreover, for nominalizations we distin-
guish whether the morphologically related noun is
an event noun (Ne), a noun denoting a theta role
of the related verb (N?) or a non-event noun (Na).
As we shall see, this permits capturing entail-
ment relations between sentences containing mor-
phoderivational variants such as for instance :
21 KRAQ06
(2) a. John is asleep (Adj ? Va)
|= John sleeps
b. John is absent (Adj ? N?)
|= John is the absentee
c. John is deeply asleep (Adj ? Ne)
|= John?s sleep is deep
2.2.2 Syntactic properties
To better support the syntax/semantic interface,
we refine the adjectives classes distinguishable on
the basis of the above criteria with the following
syntactic ones taken from (Quirk et al, 1985).
Attributiveness/Predicativeness. English adjec-
tives can be divided in adjectives which can be
used only predicatively (such as alone), adjectives
which can be used only attributively (such as me-
chanical in mechanical enginner) and adjectives
which can be used in both constructions such as
red.
Modifiability by very. We distinguish between
adjectives such as nice which can be modified by
very (i.e. very nice) and adjectives such as alleged
which cannot (*very alleged).
Gradability. We distinguish between adjectives
such as big which express gradable properties and
have comparative and superlative forms (bigger,
biggest) and adjectives such as rectangular which
don?t (*more rectangular).
Staticity/Dynamicity. Dynamic adjectives can be
used in imperative constructions and in the pro-
gressive form (Be reasonable, He is being reason-
able), static adjectives cannot (*Be short, He is be-
ing short).
3 Semantic Classes and textual
entailment recognition
In order to build our classification, we have anal-
ysed a set of about 300 english adjectives each
of which was manually mapped to the WordNet
synset correspondent to the more frequent mean-
ing of the adjective. In some case, when an ad-
jective presents polysemic forms which belong to
different semantic classes more than one form has
been considered. For example, for the adjective
civil we consider two senses/forms civil1 (syn-
onym of polite, as in civil man) and civil2 (as in
civil engineer) which belong to different semantic
classes, the first being intersective and the second
subsective. As Figure 1 shows, the proposed clas-
sification includes 15 adjective classes, each with
distinct syntactic and semantic properties.
To account for these differences, we define for
each class a set of axiom schemas capturing the
model theoretic, lexical semantics and morpho-
derivational properties of that class. Lexical se-
mantics and morpho-derivational information are
derived from WordNet. For example, the axioms
describing antonymy are obtained by extracting
from WordNet the antonyms of a particular adjec-
tive and then by considering the direction of the
entailment relevant for the class the adjective be-
longs to:
asleep ? wake vs. polite <rude
Morpho-derivational information are derived from
WordNet by extracting the derivationally related
forms for the given adjective and then iterating the
extraction on nouns and verbs in order to obtain
information about their antonyms and hyponyms.
For scalar adjective like tall, WordNet contains
also a relation is a value of which offers a
pointer to the noun concept the adjective is a value
of. Moreover, WordNet links the noun concept to
a list of attributes which describe the scalar prop-
erty it represents. For example, the adjective tall
is a value of {stature,height} and attributes
of {stature,height} are tall and short.
Based on some basic syntactic patterns, we then
show that these axioms predict the observed tex-
tual entailment patterns for that class.
Before we illustrate this approach by means of
some example, we first show how we capture log-
ical entailment between NL semantic representa-
tions in a description logic setting.
3.1 Using description logic to check
entailment between NL sentences
As argued in (Gardent and Jacquey, 2003), de-
scription logic (DL) is an intuitive framework
within which to perform lexical reasoning: it is
efficient (basic versions of description logics are
decidable), it is tailored to reason about complex
taxonomies (taxonomies of descriptions) and it
is equipped with powerful, freely available auto-
mated provers (such as RACER, (Volker Haarslev,
2001)). For these reasons, we are here exploring a
DL encoding of the entailment recognition task for
the set of examples we are considering.The partic-
ular language we assume has the following syntax.
C, D ? A|>|?|?A | C u D | C unionsq D | ?R.C | ?R.C
The semantics of this language is given below with
? the domain of interpretation and I the interpre-
tation function which assigns to every atomic con-
22 KRAQ06
Adjective Class Predicative/Attributive Modifiable by very Gradability static/dynamic Antonymy Related forms Semantic class
Class 1: afloat predicative-only - - static multi-opposition Va , Ne , N? intersective
Class 2: asleep predicative-only + - static binary-opposition Va , Ne , N? intersective
Class 3: polite both + + dynamic contraries Na intersective
Class 4: dry both + + static binary-opposition Va , Ne , N? intersective
Class 5: open both - - dynamic binary-opposition Va , Ne , N? intersective
Class 6: male both - - static multi-opposition Na , Ne , intersective
Class 7: authentic both + - static binary-opposition Ne intersective
Class 8: big both + + static contraries Ne subsective
Class 9: good both + + dynamic contraries Ne subsective
Class 10: cultural attributive-only - - static multi-opposition Na subsective
Class 11: recent attributive-only + - static multi-opposition Ne subsective
Class 12: fake both - - static binary-opposition Va ,Ne privative
Class 13: former attributive-only - - static multi-opposition privative
Class 14: questionable both + - static contraries Va , Ne plain non-subsective
Class 15: alleged attributive-only - - static contraries Va plain non-subsective
Figure 1: Classes of Adjectives
cept A, a set AI ? ? and to every atomic role R
a binary relation RI ? ? ? ?.
>I = ?
?I = ?
(?A)I = ?\AI
(C u D)I = CI ? DI
(C unionsq D)I = CI ? DI
(?R.C)I = {a ? ? | ?b(a, b) ? RI ? b ? CI}
(?R.C)I = {a ? ? | ?b ? CI ? (a, b) ? RIn}
Now one basic problem with using DL to check
entailment between NL expressions, is that DL
formulae are ?directional? in that they refer to a
given set of individuals. For instance the sentence
The boat is floating might be represented by either
of the two formulae given in 3 but these two for-
mulae do not stand in an entailment relation (since
they refer to different kind of objects namely float-
ing event of a boat in 3a and boats that float in 3b).
(3) a. float u?theme.boat
b. boat u?theme?1.float
To remedy this shortcoming, we introduce the
notion of a rotation. Given a DL formula which
only contains conjunction (disjunction is trans-
lated in DL as different formulas)
? = ui=1,n Eventi uj=1,m ?Rj .Typej
a rotation of this formula is defined as:
1. ?
2. ?j ? {1, ..., m} :
Typej u ?R?1j .(ui=1,nEventi u1<k<j,j<k<m
?Rk.Typek)
so that the formula:
Event1u Event2 u ...u Eventn u?R1.Type1 u?R2.Type2 ...
u?Rn.Typen
corresponds to the following n Rotations each of
which describe the same situation from the point
of view of a particular type
0. Event u?R1.Type1 u?R2.Type2 ... u?Rn.Typen
? Event
1. Type1 u?R?11 .(Event u?R2.Type2 ... u?Rn.Typen)
? Type1
2. Type2 u?R?12 .(Event u?R1.Type1 ... u?Rn.Typen)
? Type2
...
n. Typen u?R?1n .(Event u?R1.Type1 ... u?Rn?1.Typen?1)
? Typen
So for example, the sentence Mary knows that
John is the inventor of the radio will be repre-
sented as a predicate logic formula
?x1mary(x1) ? ?x2john(x2) ? ?x3radio(x3) ? ?e1know(e1) ?
?agent(e1, x1)??topic(e1 , e2)??e2invent(e2)?agent(e2 , x2)?
patient(e2 , x3)
the denotation of this PL formula corresponds to
the set of individuals {x1, x2, x3} ? {e1, e2}. The
corresponding DL representation will be the un-
derspecified representation
know u? agent.mary u? topic.( invent u?agent.john u? pa-
tient.radio)
the denotation of which corresponds to the set
{e1} and all its rotations which permit to access
the other sets of individuals asserted in the sen-
tence. Thus for example, the set {x1} which
describes the individual Mary can be accessed
through the following rotation:
Rotation1: mary u? agent?1.(know u? topic.( invent
u?agent.john u? patient.radio))
Finally, we say that an arbitrary for-
mula/representation ?1 implies the formula
?2 iff it is possible to find a rotation Rotationi of
?1 the denotation of which describes a subset of
the denotation of ?2:
Definition
?1 |= ?2 iff ?i.Rotationi(?1) v ?2 (1)
23 KRAQ06
3.2 Example class axioms and derivations
We now illustrate our approach by looking at two
classes in more detail namely, class 1 and class 8.
3.2.1 Class 1
Syntactically, Class 1 contains adjectives like
adrift,afloat,aground which can only be used pred-
icatively, are non gradable and cannot be modified
by very. Semantically, they behave like intersec-
tive adjectives which enter in multiple opposition
relations with other adjectives. They are further-
more morphologically derived from verbs and can
be nominalized. To reflect these semantic proper-
ties we use the following axioms.
Model theoretic semantics. Adjectives of class
1 are intersective adjective. They will thus li-
cence the correponding inference patterns namely:
A + N |= A (2)
A + N |= N (3)
Lexical semantics. Adjectives of class 1 enter in
multiple opposition relations. Hence For instance:
afloat |= ? aground ?? afloat 6|= aground
aground |= ? afloat ?? aground 6|= afloat
sunken |= ? afloat ?? afloat 6|= sunken
afloat |= ? sunken ?? sunken 6|= afloat
Morpho-derivational semantics. Adjectives in
Class 1 can be related to both nouns and verbs.
Thus, for example the adjective afloat in WordNet
is related to the noun floating which is related to
the verb float, by assuming that the semantics as-
signed to the verb float is float(e), theme(e,a), the
adjective afloat is assigned the following seman-
tics:
afloat ? ? Theme?1.float
This is encoded in the following axiom schemas:
MDR 1. Adj1 < ? Adj2 If Adj1 = Anto(Adj2)
e.g., afloat < ? sunken
MDR 2. Adj1 ? ? Theme?1.V1 If Adj1 is related to V1
e.g.,afloat ? ? Theme?1.float
MDR 3. V1 < ? V2 If V1 = Anto(V2)
e.g., float < ? sink
MDR 4. N1 ? V1 If Adj1 is related to an evt denoting N1
e.g., floating ? float
MDR 5. N1 < ? N2 If N1 is an antonym of N2
e.g., floating < ? sinking
MDR 6. N11 ? ? Theme?1.V1 If Adj1 is related to a
noun N11 denoting the theme role of the verb V1
e.g., floater ? ? Theme?1.float
We make the following assumptions about the
syntax/semantic interface that is, about the seman-
tic representations associated with given sentence
patterns.
SCR 1. NP toBe Adj
ADJ u NP
SCR 2. NP toBe clearly Adj
ADJ u NP
SCR 3. Ni[+event] of NP is clear
V i u ?theme.NP
SCR 4. Nii[-event] is clear
?theme?1.V i
SCR 5. NP toBe V[+ing].
V u ?Theme.NP
Given the above axiom schemas and semantic
constructions rules, the following inference pat-
terns can be handled:
1. ADJ1 + N |= N
Ex. This boat is afloat. |= This is a boat.
2. ADJ1 + N |= ADJ1
Ex. This boat is afloat. |= This is afloat.
3. ADJ1 + N 6|= ? N
Ex. The boat is afloat. 6|= This not a boat.
4. ADJ1 + N |= ? ADJ2 u N
Ex. The boat is afloat. |= The boat is not sunken.
5. ? ADJ1 + N 6|= ADJ2 u N
Ex. The boat is not afloat. 6|= The boat is sunken.
6. ADJ1 + N |= N u?theme?1.V 1
Ex. The boat is afloat. |= The boat is the floater.
7. ADJ1 + N |= V1 u?theme.N
Ex. The boat is afloat. |= The boat is floating.
8. ADJ1 + N |= N1 u?theme.N
Ex. This boat is clearly afloat. |= The floating of the
boat is clear.
9. ADJ1 + N |= N u?theme?1.N1
Ex. This boat is clearly afloat. |= The floating of the
boat is clear (or the boat is the floating object).
10. ? (ADJ1 + N) |= ? (V1 u?theme.N) 6|= ? N
Ex. This is not a floating boat. 6|= This is not a boat.
11. ? (ADJ1 + N) 6|= ? Adj1
Ex. This is not a floating boat. 6|= This is not afloat.
12. ? (ADJ1 + N) 6|= ? V1
Ex. This is not a floating boat. 6|= This is not floating.
13. ? (ADJ1 + N) 6|= ? N1
Ex. This is not a floating boat. 6|= This is not a floating.
14. ? (ADJ1 + N) 6|= ? ? theme?1.V1
Ex. This is not a floating boat. 6|= This is not the floater.
15. ? (ADJ1 + N) 6|= ? ? theme.N
Ex. This is not a floating boat. 6|= This is not a floating.
24 KRAQ06
In the inference patterns 10 to 15, the negation
of the adjective-noun compound ? (ADJ1 + N) is
syntactically blocked, as the adjectives in this class
are used predicative only, however the equivalent
representation V1 u?theme.N can be used to mo-
tivate the inferences.
The following show in more detail how the first
three of the above (non) entailments are recog-
nised.
(4) a. The boat is afloat.
b. |= The boat is floating.
4a ? Boat u Afloat (by SCR 1) A
4b ? Float u?Theme.Boat (by SCR 5) B
Afloat ? ?Theme?1.F loat (by MDR 2) C
1 ? Boat u?Theme?1.F loat (from A and C) D
D |= B (By Defn 1) E
(5) a. The boat is afloat.
b. |= The boat is the floater.
5a ? Boat u Afloat (by SCR 1) A
5b ? Boat u?Theme?1.f loat (by SCR 4) B
Afloat ? ?Theme?1.F loat (by MDR 2) C
A |= B (from B und C) D
(6) a. The boat is afloat.
b. |= The boat is not sinking.
6a ? Boat u Afloat (by SCR 1) A
6b ? ? sink u?Theme.boat (by SCR 5) B
Afloat ? ?Theme?1.F loat (by MDR 2) C
Boat u?Theme?1.F loat (from A and C) D
float u?Theme.boat (By Defn 1) E
E |= B (by MDR 1) F
3.2.2 Class 8.
Class 8 contains adjectives like
big,fast,tall,deep which can be used attribu-
tively and predicatively, are gradable, can be
modified by very. Semantically, they are classified
as subsective adjectives and their antonyms are
contraries. They are morphologically related
to nouns which describe the particular property
denoted by the adjectives and to nouns of which
they are attributes.
Model theoretic semantics. Adjectives of
class 8 are subsective adjective. They will thus li-
cence the correponding inference patterns namely:
A + N 6|= A (4)
A + N |= N (5)
Lexical semantics. The Adjectives of class 8 en-
ter in contrary opposition relations. Hence, the fol-
lowing axioms schemas will be licensed:
Ai |= ?Anto(Ai) and ?Ai 6|= Anto(Ai)
(6)
For instance:
long |= ? small ?? long 6|= small
deep |= ? shallow ?? deep 6|= shallow
Morpho-derivational semantics. Adjectives in
Class 8 can be related to nouns but not to
verbs. Moreover, such adjectives are mapped
in WordNet to noun concepts through two dif-
ferent links: derivationally related to
and is a value of. For example, the adjec-
tive tall in WordNet is derivationally related to the
noun tallness and is a value of the concept noun
height. The adjectives in this class describe grad-
able properties so that their semantics corresponds
to:
has-property(Related Noun u?has-measure.Top)
in which the role has-measure account for the
value of the scalar property described by the adjec-
tive, which remain underspecified (Top) if the ad-
jective is used without a reference to the value of
measure. When the value of the measure is speci-
fied, for example by combining the adjective with
a noun, as for example in This is a tall man, then
the noun is assigned as a value of the measure role:
man u?has-property.(tallness
u?has-measure.man)
which translate This is tall as a man.
This is encoded in the following axiom
schemas:
MDR 1. Adj1 < ? Adj2 If Adj1 = Anto(Adj2)
Ex. tall < ? short
MDR 2. Adj1 < ? has property.(N1 u?has measure.Top)
If Adj1 is related to a noun N1 denoting the property
described by Adj1
Ex. tall < ? has property.(tallness
u?has measure.Top)
MDR 3. N1 < ? N2 If N1=Anto(N2)
Ex. tallness < ? shortness
MDR 4. N1 ? N? u?has value.Adj1
If Adj1 is an attribute of the noun N?
Ex. tallness ? height u?has value.tall
MDR 5. N2 ? N? u?has value.Adj2
If Adj2 is an attribute of the noun N?
Ex. shortness ? height u?has value.short
MDR 6. N1 < N? If N1 is an hyponym of N?
Ex. tallness < height
25 KRAQ06
MDR 7. N2 < N? If N2 is an hyponym of N?
Ex. shortness < height
MDR 8. Adj11 < Adj1 If Adj1 is a
scalar attribute with value less then Adj11 (hyponymy
is not defined for adjectives)
Ex. giant < tall
For the moment, we don?t account for the se-
mantics of comparatives forms of adjectives but
we will do that in the feature, by also introducing a
representation for scales as described in (Kennedy,
2005).
We make the following assumptions about the
semantic representations associated with basic
sentence patterns.
SCR 1. NP toBe Adj
NP u? has property.(N1 u?has measure.NP)
SCR 2. That toBe Det Adj NP
NP u? has property.(N1 u?has measure.NP)
SCR 3. NP toBe clearly Adj
NP u? has property.(N1 u?has measure.NP)
SCR 4. N1 of NP is clear
NP u? has property.(N1 u?has measure.NP)
SCR 5. The Adj N? of NP
NP u? has property.(N? u? has value.Adj
u?has measure.NP )
SCR 6. NP1 toBe Adj as a N
NP1 u N u?has property.(N? u? value.Adj u?
has measure.N)
SCR 7. NP1 toBe NP2[+measure] Adj
NP1 u?has property.(N? u? value.Adj u?
has measure.NP2)
SCR 8. NP1 toBe NP2[+measure] Adj N
NP1 u N u?has property.(N? u?has value.Adj u?
has measure.NP2)
Given the above axioms, the following exam-
ples can be handled:
(7) (a) John is a 1.50 meter tall man.
|= (b) John is 1.50 meter tall.
7a ? John u Man u?has property.(height A
uhas value.tall uhas measure(1.50 meter) )
(by SCR 8)
7b |= John u?has property.(height uhas value.tall B
uhas measure(1.50 meter) )
(by SCR 7 and from A)
A |= B C
(8) (a) John is a 1.50 meter tall man. 6|= (b) John
is a tall man.
8a ? John u Man u?has property.(height A
uhas value.tall uhas measure(1.50 meter) )
(by SCR 8)
8b |= John u Man u?has property.(height u B
has value.tall uhas measure(man) )
(by SCR1 and from A)
A 6|= B C
4 Implementation
For each of the 15 classes, we have specified a set
of axioms schemas, some basic semantic construc-
tion rules and a set of inference patterns which
could be deduced to follow from both of these.
The axioms schemas were implemented in De-
scription Logic using RACER and for each infer-
ence pattern identified, the corresponding Descrip-
tion Logic query was checked to verify that the
proposed axioms and semantic construction rules
did indeed correctly predict the deduced inference
patterns.
5 Further work and evaluation
The main contribution of this work is a detailed
analysis of the interactions between derivational
morphology, lexical and compositional semantics
and of their impact on the entailment patterns li-
censed by sentences containing adjective or their
related nouns/verbs.
To turn this analysis into a computational sys-
tem, its components need to be integrated into a
semantic analyser and the behaviour of that anal-
yser tested against a collection of data. We are
currently working on developing such an anal-
yser within a symbolic grammar framework. We
have also started to develop an evaluation test
suite geared towards entailment recognition be-
tween sentence pairs containing adjectives. At the
moment, the test suite contains about 1 000 infer-
ence pairs. Each item in the TestSuite (see fig. 2)
is annotated with a judgement about the truth of
the entailment between the pair of sentences, with
the type of inference involved and with the speci-
fication of adjective involved. Moreover, each ad-
jective is annotated with the WordNet sense corre-
sponding to the given class.
The idea behind this test suite is similar to
that underlying the creation of the TSNLP (Test
suite for natural language processing) (see (Oepen
and Netter, 1995)) or the Eurotra testsuites (see
(Arnold and des Tombe, 1987)) namely, to pro-
vide a benchmark against which to evaluate and
compare existing semantic analyzers. Thus this
26 KRAQ06
<pair id="1" value="TRUE" class="[CLASS1]" inference="Adj/Verb">
<t>The boat is <sn n="1"> afloat </sn>.</t>
<h>The boat is floating.</h>
</pair>
<pair id="2" value="FALSE" class="[CLASS6]" inference="Antonymy">
<t>This is not a <sn n="1"> rectangular </sn> table.</t>
<h>This is a <sn n="1"> round </sn> table </h>
</pair>
<pair id="3" value="TRUE" class="[CLASS8]" inference="Adj/Noun">
<t>The line is 2 meter <sn n="1"> long </sn>.</t>
<h>The length of the line is 2 meter.</h>
</pair>
<pair id="4" value="FALSE" class "[subs/intersective]" inference="Attr/Pred">
<t>The treasurer is <sn n="2"> present </sn>.</t>
<h>This is the <sn n="1"> present </sn> treasurer.</h>
</pair>
Figure 2: TestSuite
test suite illustrates the semantic and syntactic be-
haviour of adjectives and their related verbs/nouns
with respect to textual entailment. One could
imagine other test suites illustrating the seman-
tic behaviour of verbs, of quantifiers, of discourse
connectives, etc. Just as the TSNLP still proves
useful in supporting the development of new sym-
bolic parsers/grammars, hand built test suites of
artificial examples might prove useful in improv-
ing the accuracy of semantic analyser wrt textual
entailment. Indeed the Pascal RTE challenge has
shown that existing systems fares rather poortly at
the textual entailment task. Providing a set of hand
crafted semantic test suites might help in remedy-
ing this shortcoming.
Beside implementing and evaluating the anal-
ysis of adjectives presented in this paper, we are
also working on refining this analysis by combin-
ing it with a detailed analysis of noun semantics so
as to handle (non) entailments such as:
(9)
Lyon is the gastronomical capital of France
6|= Lyon is the capital of France
References
D.J. Arnold and Luis des Tombe. 1987. Basic Theory
and methodology in Eurotra. Cambridge University
Press.
DA. Cruse. 1986. Lexical Semantics. Cambridge Uni-
versity Press.
Claire Gardent and Evelyne Jacquey. 2003. Lexical
reasoning. In Proceedings of the ICON?03 (Inter-
national Conference on Natural Language Process-
ing), Mysore, India.
Oren Glickman Ido Dagan and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entailment
Challenge.
Hans Kamp and Barbara Partee. 1995. Prototype the-
ory and compositionality. Cognition, (57):129?191.
Hans Kamp. 1975. Two theories about adjectives. In
Edward L. Keenan (ed.), Formal Semantics of Nat-
ural Language, pages 123?155. Cambridge Univer-
sity Press.
Christofer Kennedy. 2005. Vagueness and grammar:
The semantics of relative and absolute gradable ad-
jectives. Ms., pages 129?191, June.
K. J. Miller. 1998. Modifiers in wordnet. In
C. Fellbaum (ed.), WordNet An Electronic Lexical
Database. Cambridge, MA, The MIT Press.
Stephan Oepen and Klaus Netter. 1995. TSNLP -
test suites for natural language processing. Gronin-
gen, The Netherlands. Conference on Linguistic
Databases.
I. Peters and W. Peters. 2000. The Treatment of Adjec-
tives in SIMPLE: Theoretical Observations. Athens.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman.
V. Raskin and S. Nirenburg. 1995. Lexical Semantics
of Adjectives, a micro-theory of adjectival meaning.
MCCS Report.
Ralf Mo?ller Volker Haarslev. 2001. Description of the
racer system and its applications. In Proceedings
International Workshop on Description Logics (DL-
2001, Stanford, USA.
27 KRAQ06
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 185?192,
Prague, June 2007. c?2007 Association for Computational Linguistics
A first order semantic approach to adjectival inference
Marilisa Amoia
INRIA/Universite? de Nancy 1 &
University of the Saarland
Saarbru?cken, Germany
amoia@coli.uni-saarland.de
Claire gardent
CNRS/Loria
Campus Scientifique BP 239
54506 Vandoeuvre-les-Nancy, France
claire.gardent@loria.fr
Abstract
As shown in the formal semantics litera-
ture, adjectives can display very different
inferential patterns depending on whether
they are intersective, privative, subsective
or plain non-subsective. Moreover, many
of these classes are often described using
second order constructs. In this paper, we
adopt Hobbs?s ontologically promiscuous
approach and present a first order treatment
of adjective semantics which opens the way
for a sophisticated treatment of adjectival
inference. The approach was implemented
and tested using first order automated rea-
soners.
1 Introduction
As has often been observed, not all of natural lan-
guage meaning can be represented by first order
logic. There are expressions such as, most, former,
I didn?t whose meaning intuitively involve higher-
order constructs.
Nevertheless, as (Hobbs, 1985) and others have
argued, semantic representations for natural lan-
guage need not be higher-order in that ontological
promiscuity can solve the problem. That is, by reify-
ing all objects that can be predicated of, it is possible
to retain a semantic representation scheme for NL
that is first-order.
This observation is crucial for computational ap-
plications for two reasons. First, logics that goes be-
yond first order are highly undecidable. Second and
more importantly, there is no off the shelf higher or-
der automated reasoners that could be put to use to
reason about the meaning of higher-order formulae.
In this paper, we present a semantics for adjec-
tives that adopts an ontologically promiscuous ap-
proach and thereby supports first order inference for
all types of adjectives including extensional ones.
Indeed, traditional semantic classifications of ad-
jectives such as (Chierchia and Connell-Ginet, 1990;
Kamp, 1975; Kamp and Partee, 1995) subdivide
adjectives into two classes namely extensional vs.
intensional adjectives, the latter grouping together
adjectives which intuitively denote functions from
properties to properties, i.e. second order objects.
We present a compositional semantics for ad-
jectives which both (i) defines a first order repre-
sentation and (ii) integrates interactions with other
sources of linguistic information such as lexical se-
mantics and morpho-derivational relations. We then
show that the proposed semantics correctly predicts
the inferential patterns observed to hold of the var-
ious adjective subclasses identified in the literature
(Chierchia and Connell-Ginet, 1990; Kamp, 1975;
Kamp and Partee, 1995; Amoia and Gardent, 2006).
This paper is structured as follows. We start by
presenting a classification of adjectives which is mo-
tivated by the different inferential patterns observed.
We then propose a compositional semantics for each
class and show that it correctly predicts their inferen-
tial behaviour. We conclude with a brief discussion
of related work and pointers for further research.
2 Inferential patterns and adjective classes
In the literature (Chierchia and Connell-Ginet, 1990;
Kamp, 1975; Kamp and Partee, 1995; Amoia and
185
Gardent, 2006), adjectives are usually divided into
four main classes namely, intersective, subsective,
privative and plain non subsective depending on
whether or not the [Adj N]AP phrase entails the
properties expressed by the noun and/or the adjec-
tive. More specifically, each of the four classes is
characterised as follows.
Intersective adjectives. This class includes com-
mon categorical (e.g., red, rectangular, French) and
tautological (e.g., real, present) adjectives. It is char-
acterised by the inferential patterns:
[A N] |= N
[A N] |= A
For instance, saying that there is a red table im-
plies both that there is something red and that there
is a table.
Subsective adjectives form an ontologically het-
erogeneous class including for instance denominal
(e.g., gastronomical) and measure (e.g. big) adjec-
tives. They are characterised by the fact that the [Adj
N]AP phrase does not entail the Adj property:
[A N] |= N
[A N] 6|= A
For instance, a big mouse is a mouse but is not
big. Instead it is ?big for a mouse?. In other words,
?bigness? cannot be directly inferred as, e.g. a big
mouse and a big elephant are big in very different
ways.
Privative adjectives denote adjectives such that
the [Adj N]AP phrase entails the negation of the N
property:
[A N] |= ?N
For instance, the former king is not the king and a
fake weapon is not a weapon.
Plain non-subsective adjectives are adjectives
which preclude any inference wrt to the N property:
[A N] |= (N ? ?N)
[A N] 6|= A
Thus, if Peter is an alleged murderer, it is impos-
sible to know whether or not he is a murderer.
Now, the class of intensional adjectives groups to-
gether adjectives with a syntactic and semantic id-
iosyncratic behaviour. Syntactically, intensional ad-
jectives are not gradable (e.g. cannot be modified
by very) and most of them can only be used attribu-
tively (He is a former president but not The presi-
dent is former). Semantically, they are usually taken
to denote second order properties, i.e. functions of
the type ??e,t?, ?e,t??.
Intensional adjectives include denominal (or rela-
tional) adjectives (e.g polar bear, atomic scientist),
manner (or adverbial) adjectives (e.g. a poor liar, a
fast car), emotive (e.g. a poor man) and modals, i.e.
all adjectives which are related to adverbs, quanti-
fiers or determiners (e.g. a feeble excuse, the specific
reason, a fake nose, etc.).
3 Assigning FOL Representation to
Intensional adjectives
We now show how adjectives can be assigned an ap-
propriate first order logic representation which ap-
propriately reflects their inferential behaviour.
Following Hobbs, we adopt a promiscuous ontol-
ogy and assume that for every predication that can
be made in natural language, there corresponds an
?eventuality?. As Hobbs has argued, this allows for
higher order predications to remain first order in that
they become predications over (first order) eventual-
ities.
Thus, in the domain there are entities which are
either eventualities or individuals and relations be-
tween individuals. Moreover like Hobbs, we assume
a model to describe a platonic universe containing
everything that can be spoken about whether or not
these things exist in the real world. To express exis-
tence in the real world, a special predicate (Exists)
is introduced.
We use the following notation:
? ei, for eventuality variables,
? xi, for individuals,
? Pi, for properties of individuals.
And the following types:
? e will denote the type of individuals,
? ev the type of eventualities and
186
? t a truth value.
3.1 The intuition
As shown in section 2, the semantics of [Adj N]AP
phrases has very different inferential properties de-
pending on the type of the adjective Adj. The differ-
ences stem from three main points.
The number of individuals introduced by the
[Adj N]AP phrase. Thus, the red table evokes a
single individual x which is both red and a table
whilst the gastronomical book refers to a book x
which is about the gastronomy concept y. More gen-
erally, the variables predicated of by the noun and by
the adjective can refer either to the same or to two
distinct individual(s).
The properties licensed by the adjective and the
noun to contribute to the meaning of the [Adj
N]AP phrase. Depending on the adjective type,
the properties denoted by Adj and N will contribute
either directly or indirectly to the meaning of the
[Adj N]AP phrase. Thus in an intersective [Adj
N]AP phrase, the meaning contributed by Adj and
N are simply the properties they denote. By con-
trast, the privative fake forces the negation of the N
property to be part of the Adj N meaning whilst the
subsective gastronomical induces a relation to the
morphoderivationally related noun concept (about
gastronomy) to be included in the the Adj N mean-
ing. More generally, the properties that compose the
meaning of the Adj N phrase can be the denotation
of Adj and/or N, the negation of N, its denotation in
the past or some property derived from it.
The existence in the real world of the entity de-
noted by the NP. In all cases the [Adj N]AP
phrase denotes a set of individuals but whilst in most
cases the [Adj N]AP phrase is neutral with respect
to the existence in the real world of these individ-
uals, plain non-subsective [Adj N]AP phrases (e.g.
alleged murderer) explicitly question it (an alleged
murderer may or not exist in the real world).
3.2 The semantics of nouns
In designing a semantics for adjectives, we assume
a semantics for nouns which reflect their possible
interactions with the different types of adjectives
(1) a. noun: ?Pol?e?x.[Pol(table(e)) ? e = x]
As we shall shortly see, the additional lambda
variable e is imposed by the treatment of adjective
semantics we propose and more specifically by the
necessity to sometimes distinguish between the indi-
vidual described by the noun and the individual de-
scribed by the adjective. The variable Pol accounts
for the polarity of the noun, i.e. whether it occurs
with the negation or not.
We give here also the semantics assigned to the
pronouns someone/something which will be used in
the derivations throughout this paper:
(2) a. someone/something: ?P?x.P (x)
3.3 The semantics of the copula
Following the proposal of Mantague, we assign a
unique representation for both the uses of the cop-
ula in identity statements (e.g. John is Mary ?
john=mary) and in predicative assertions (e.g. John
is a man ? man(john)):
(3) a. be: ?K?x.K(?y(x = y))
In the case of predicative assertions in which the
predicate is an adjective (e.g. John is brave), we
adjust the type of the argument of the copula in the
following way:
(4) a. be Adj: be(Adj(?Pol?e?x.true))
3.4 The semantics of adjectives
Given such a representation for nouns, we represent
adjectives using the schema given in Figure 1.
Briefly, schema 1 captures the observations made
in section (3.1) as follows. First it introduces an ex-
istential quantification (in the platonic universe) over
not one but two variables (ea and en) ? depending on
how the formula is instantiated (and in particular on
the value of R1 and R2) these two variables may or
not denote the same object. This accounts for the
first observation according to which an [Adj N]AP
phrase may refer to either one or two individuals.
Second, the meaning of the [Adj N]AP phrase is a
function not of the Adj and N meaning but rather of
properties derived from these meanings (A? for Adj
and N , as modified by its three arguments, for N).
This accounts for the second observation.
Third, the use of the exists predicate will permit
distinguishing between existence in the universe of
discourse and existence in the real world.
187
?N?x?ea?en.[A?(ea) ? R1(x, ea) ? R2(en, ea) ? N(Pol)(en)(x)]
with A? the property licensed by the adjective, R1, R2 two arbitrary relations licensed by the adjective,
N the property denoted by the noun and Pol a polarity argument of value either ?S.S or ?S.?S
Figure 1: Semantics schema for all adjectives
We now show how this general schema receives
different instantiations depending on the adjectival
class being considered; and how each instantiation
predicts the correct inferential pattern for the four
adjectival classes.
3.4.1 Intersective adjectives
The semantic representation of an [Adj N]AP ad-
jectival phrase involving an intersective adjective is
given in Figure 2 together with the derivation of the
[Adj N]AP phrase red table. As can be seen, in this
case, the relation R1 holding between the lambda
bound variable x and the entity introduced by the
adjective is one of identity. Similarly, the entity en
introduced is equated with x and the relation R2
is ?x, y.true (i.e. there is no modifying relation
between ea and en). Hence the [Adj N]AP phrase
licenses in effect a single entity x and the resulting
semantics is the traditional ?x.[A(x) ? N(x)] with
A the semantics of the adjective and N that of the
noun. Assuming further that determiners have the
semantics:
a/the ?P?Q?x.[P (?S.S)(x) ? Q(x)]
then the semantics of Something is a red table is
(5) ?x?ea?en.[red(ea)?x = ea?table(en)?en =
x]
which correctly entails that there is an entity x
which is both red and a table i.e.,
(5) |= ?x.[red(x)] something is red
(5) |= ?x.[table(x)] something is a table
3.4.2 Subsective adjectives
As recalled above, subsective adjectives are char-
acterised by the fact that the [Adj N]AP phrase en-
tails N but not A. Relatedly, the adjective phrase in-
troduces not one but two individuals, one linked to
the adjective and the other to the noun. For instance,
the phrase the gastronomical book refers to a book
x which is about the gastronomy concept en.
Thus in such cases, we take the R2 relation hold-
ing between x, the NP quantified variable, and ea,
the entity introduced by the adjective, to be distinct
from identity, while the R1 relation is empty.
(6) ?x?ea?en.[gastronomy(ea)?about(en, ea)?
book(en) ? en = x]
This ensures that the NP refers to two entities, one
bound by the determiner and licenced by N, the other
existentially quantified and licensed by A. For in-
stance, the sentence John read every gastronomical
books is interpreted as meaning that John read all
books that are about gastronomy.
More generally, this ensures that [A N] 6|= A (and
in fact, adjectives like gastronomical cannot be used
predicatively), e.g.
(6) |= something is a book
|= ?x.[book(x)]
(6) |= something is about gastronomy
|= ?x?ea.[about(x, ea) ? gastronomy(ea)]
(6) 6|= something is a book and a gastronomy
6|= ?x[book(x) ? gastronomy(x)]
(6) 6|= something is gastronomical
6|= ?x[gastronomical(x)]
As shown in (Amoia and Gardent, 2006), subsec-
tive adjectives can be further divided into at least
four classes. Because of space restrictions, we only
show here how to represent two of these subclasses
namely denominal (e.g. gastronomical) and mea-
sure subsective adjectives (e.g. big). In both cases,
the idea is to decompose the meaning of the adjec-
tives into a finer grained lexical meaning. Depend-
ing on the lexical meaning involved, this decompo-
sition induces different instantiation patterns for the
188
Intersective Adjectives
?N?x?ea?en.[A(ea) ? x = ea ? N(?S.S)(en)(x)]
Red table
?N?x?ea?en.[red(ea) ? x = ea ? N(?S.S)(en)(x)](?Pol?e?x.[Pol(table(e)) ? e = x])
? ?x?ea?en.[red(ea) ? x = ea ? table(en) ? en = x])
? ?x.[red(x) ? table(x)])
Figure 2: Semantics of Intersective Adjectives
Subsective Adjectives
?N?x?ea?en.[A?(ea) ? R2(en, ea) ? N(?S.S)(en)(x)]
with A? an arbitrary complex relation derived from the lexical meaning of the adjective and
R2 a relation other than identity
Gastronomical book
?N?x?ea?en.[gastronomy(ea) ? about(en, ea) ? N(?S.S)(en)(x)](?Pol?e?x.[Pol(book (e)) ? e = x])
? ?x?ea?en.[gastronomy(ea) ? about(en, ea) ? book(en) ? en = x])
Figure 3: Semantics of Subsective Adjectives
R relation mentioned in the general schema for ad-
jective semantic representation.
Thus, the meaning of the adjectival phrase
containing an adjective of measure, e.g. big mouse
will be represented as:
?N?x?ea?en.[size(ea) ? highFor(ea, C)
?has(en, ea) ? N(?S.S)(en)(x)]
(?Pol?e?x.[mouse(e) ? e = x])
? ?x?ea?en.[size(ea) ? highFor(ea, C)
?has(en, ea) ? mouse(en) ? en = x])
where C is a contextually given parameter which de-
termine the scale size is measured against. In this
case, C would be, e.g. ?mouse? so that the formula
above can be glossed as x is a mouse with a size ea
which is high for a mouse. In particular, Daisy is
a big mouse entails that Daisy is a mouse and that
Daisy is big for a mouse, but not that Daisy is big.
3.4.3 Privative adjectives
As seen above, privative adjectives entail that the
entity described by the NP is not N, e.g. a fake gun is
not a gun. For such adjectives, it is the entity intro-
duced by the adjective that is being quantified over,
hence ea is identified with x (cf. Figure 4). Fur-
ther, the N property is either denied or subject to a
modality (former, potential). As shown in Figure 4,
this is accounted for by providing the appropriate re-
lation R (e.g. R2 being the relation time introduced
by former or R1 being the identity relation x = ea
introduced by fake).
This representation presupposes that each sen-
tence in which such modality adjectives do not occur
has a default value for time and/or modality. Thus,
for instance that
(7) John is a former president. 6|= John is the pres-
ident.
(8) John is a possible president. 6|= John is the pres-
ident.
can only be accounted for if the base forms are
assigned the following default representations:
(7) ?ea?x [president(x) ? time(x, ea)
?present(ea)]
(8) ?ea?x [president(x) ? mod(x, ea)
?possible(ea)]
3.4.4 Plain non-subsective adjectives
Finally, plain non-subsective adjectives fail to
make any prediction about the existence of an in-
189
Privative Adjectives (e.g., fake,potential,former,future)
(e.g. fake, fictitious)
?N?x?ea?en.[A(ea) ? x = ea ? N(?S.?S)(en)(x)] OR
?N?x?ea?en.[A?(ea) ? mod/time(ea, en) ? N(?S.S)(en)(x)]
with R2 being the relation mod/time specifying the modality or the time indicated by the adjective
Fake gun
?N?x?ea?en.[fake(ea) ? x = ea ? N(?S.?S)(en)(x)](?Pol?e?x.[Pol(gun(e)) ? e = x])
? ?x?ea?en.[fake(ea) ? x = ea ? ?gun(en) ? en = x])
Former president
?N?x?ea?en.[former (ea) ? time(en, ea) ? N(?S.S)(en)(x)]
(?Pol?e?x.[Pol(president(e)) ? e = x])
? ?x?ea?en[former(ea) ? time(x, ea) ? president(en) ? x = en]
Figure 4: Semantics of Privative Adjectives
dividual having the N property. Thus for instance,
if John is an alleged murderer, there might or might
not exist a murderer.
To account for this fact, we follow Hobbs? ap-
proach in distinguishing between existence in the
universe of discourse and existence in the real world.
Thus, the logical existential connective ? is used to
denote existence in the discourse world while the
special predicate Exists is used to denote existence
in the real world. We assume further a theory that
permits determining when an individual exists in the
universe of discourse and when it exists in the real
world.
Given these caveats, the semantics of plain non-
subsective adjectives is as indicated in Figure 5 and
simply specifies that the alleged murderer is an in-
dividual x which exists in the universe of discourse
(but not necessarily in the real world) and which is
alleged to be a murderer. Moreover, as stated in
(Hobbs, 1985), we assume that the alleged predi-
cate is existentially opaque in its second argument.
That is, an alleged predication does not imply the
existence in the real world of its second argument.
4 Implementation
The semantics of adjectives presented in this paper
was tested using (Blackburn and Bos, 2005) compu-
tational semantics framework.
First, based on the classification of 300 English
adjectives presented in (Amoia and Gardent, 2006),
which identifies 17 different adjectival subclasses
for the four main classes proposed by (Kamp, 1975;
Kamp and Partee, 1995), we have built a test suite of
about 150 examples in the following way. We have
chosen for each class a representant adjective and
written for it the set of sentence pairs (H/T) illus-
trating the inference patterns displayed by the class
the adjective belongs to. In particular, we have built
examples which test:
1. whether the adjective partecipates in both pred-
icative and attributive constructions, so that the
resulting sentences (H and T) are paraphrastic,
2. whether the two sentences contain adjectives
which are synonyms,
3. what kind of antonymic relation links the given
adjective with its antonym,
4. which of the three inference patterns described
in (Kamp and Partee, 1995) holds for the given
adjective,
5. hyperonymy,
6. derivational morphology.
For instance, the test suite contains for an adjec-
tive such as fake, belonging to a subclass of the pri-
vative adjectives, the H/T pairs in (9).
(9) a. H:This is a fake gun / T:This gun is fake
190
Plain non subsective Adjectives (e.g., alleged)
?N?x?ea?en.[A?(ea, en) ? x = ea ? N(?S.S)(en)(en)]
with R1 being the identity relation between x and ea and R2 being the relation
introduced by the adjective A?(ea, en)
Alleged murderer
?N?x?ea?en.[alleged(ea, en) ? x = ea ? N(?S.S)(en)(en)](?Pol?e?x.[Pol(murderer (e)) ? e = x])
? ?x?ea?en.[alleged(ea, en) ? x = ea ? murderer(en) ? en = en])
Figure 5: Semantics of plain non-subsective Adjectives
b. H:This is a fake gun / T:This is a false gun
c. H:This is a fake gun / T:This gun is not gen-
uine
d. H:This is not a fake gun |= This gun is real
e. H:This is a fake gun / T:This is a gun
f. H:This is a fake gun / T:This is not a gun
g. H:This is a fake gun / T:This is fake
h. H:This is a fake gun / T:This is a fake
weapon
i. H:This is a fake gun / T:This gun is a coun-
terfeit
Second, a grammar fragment was implemented
which integrates the semantics of nouns and adjec-
tives presented here. This grammar fragment was
then used together with the appropriate lexicon to
automatically associate with each sentence of the
test suite a representation of its meaning.
Third, lexical Knowledge pertaining to each class
of adjectives is captured through a set of axioms de-
scribing the specific lexical relationships adjectives
are involved in.
Synonymy is captured introducing equality axioms
which describe the equivalence of the two proper-
ties expressed by the two adjectives Adj1 and Adj2
asserting:
?e[Adj1(e) ? Adj2(e)]
Hyponymy (for example big/giant vs.
small/minuscule) is captured by introducing
the axioms such as:
?e[Adj1(e) ? Adj2(e)]
Antonymy is captured by introducing different ax-
ioms depending on the type of opposition relation in
which the adjectives are involved, i.e. binary, con-
trary or multiple opposition. The axiom below for
example introduces a binary antonymic relation:
?e[Adj1(e) ? ? Adj2(e)]
Fourth, entailment (H|=T) was checked for each
sentence pair using the first order theorem provers
available in the system and the results compared
with the expected result. A first evaluation shows
that the methodology proposed yields the expected
results: we could correctly predict all the inferen-
tial patterns presented above from 1 to 5 (136 pairs,
89%). The results for other patterns, describing mor-
phoderivational relations of adjectives, depend on
the amount of information implemented in the gram-
mar which for the moment is very limited.
5 Perspectives and Comparison with
related works
The approach presented here lays the basis for a
computational treatment of adjectival inference in
that it provides a fine grained characterisation of the
various types of inferential patterns licenced by ad-
jectives.
In future work, we believe three main points are
worth investigating.
First, previous work (Amoia and Gardent, 2006)
has shown that the classification presented here can
be further detailed and even finer-grained classes
identified thereby permitting the creation of syn-
tactically and semantically homogeneous adjectival
191
classes. The advantages of identifying such ho-
mogeneous classes has been well demonstrated for
verbs. It permits structuring the lexicon and facil-
itates development and maintenance. Based on the
idea that syntax (and in particular, so-called syntac-
tic alternations) helps define such classes, we are
currently investigating in how far adjectival syntax
helps further refine adjectival classes.
Second, the proposed classification need to be ap-
plied and combined with ontological and lexical se-
mantic information. That is, each adjective should
be classified wrt the 4 types of model theoretic se-
mantics described here and related to such a lexical
semantics ontology as e.g., WordNet, the MikroKos-
mos ontology of the SIMPLE lexicon.
Thus (Raskin and Nirenburg, 1995) describe the
methodology used to encode adjectival entries in the
lexicon of the MikroKosmos semantic analyser. The
MikroKosmos lexicon contains 6,000 entries for En-
glish and 1,500 entries for Spanish adjectives. Ad-
jectives are organised in an ontology which distin-
guishes between the following three main adjectival
classes: (i) Scalar Adjectives, which are rep-
resented as property-value pairs, (ii) Denominal
Adjectives, (e.g. atomic, civil, gastronom-
ical) represented as nouns and (iii) Deverbal
Adjectives, (e.g. eager, abusive, readable) is re-
lated to the meaning of the verb they are derived to.
The classification of adjectives proposed in SIM-
PLE (SIMPLE, 2000) is also ontology-based. A
lexical entry for an adjective is characterised by a
set of semantic and syntactic information. Seman-
tic information describes: (i) the hierarchy of onto-
logical properties expressed by the particular adjec-
tive, for example the adjective expresses the prop-
erty of COLOUR and this is a physical property; (ii)
whether the adjective is intersective or subsective;
(iii) whether the adjective has a persistent duration
(i.e. is stable) or not. Moreover, syntactic informa-
tion describes adjectival features such as (i) predica-
tive/attributive usage, and (ii) gradability.
SIMPLE has actually added semantic information
to approximately 3,500 lexical entries (about 10,000
senses) for each of the 12 European languages con-
sidered in the project.
It would be interesting to see whether any of these
resources can be used to create an adjective lexicon
rich enough to support both syntactic processing and
semantic inference.
Finally, a third point of interest concerns the in-
tegration of the compositional semantics proposed
here for adjectives into a robust semantic processing
system. We plan to integrate this semantics into the
CCG2Sem semantic parsing system (Bos, 2005) and
to investigate in how far, this would help deal with
entailment recognition.
References
Marilisa Amoia and Claire Gardent. 2006. Adjective
based inference. In Proceedings of KRAQ?06 (Knowl-
edge and Reasoning for Answering Questions), Trento,
Italy.
Patrick Blackburn and Johan Bos. 2005. Representation
and Inference for Natiral Language. A first Course in
Computational Semantics. CSLI Studies in Computa-
tional Linguistics.
Johan Bos. 2005. Towards wide-coverage semantic in-
terpretation. In In Proceedings of the Sixth Interna-
tional Workshop on Computational Semantics IWCS-
6, pages 42?53.
G. Chierchia and S. Mc Connell-Ginet. 1990. Mean-
ing and Grammar: An Introduction to Semantics. The
MIT Press, Cambridge, MA.
Jerry R. Hobbs. 1985. Ontological promiscuity. In
Proceedings of the 23rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 61?69,
Chicago, Illinois, July.
Hans Kamp and Barbara Partee. 1995. Prototype theory
and compositionality. Cognition, (57):129?191.
Hans Kamp. 1975. Two theories about adjectives. In
Edward L. Keenan (ed.), Formal Semantics of Natu-
ral Language, pages 123?155. Cambridge University
Press.
V. Raskin and S. Nirenburg. 1995. Lexical Semantics
of Adjectives, a micro-theory of adjectival meaning.
MCCS Report.
Specification Group SIMPLE. 2000. Specification sim-
ple work package 2. Linguistic specifications deliver-
able d2.1.
192
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 367?375,
Beijing, August 2010
RTG based surface realisation for TAG
Claire Gardent
CNRS/LORIA
claire.gardent@loria.fr
Laura Perez-Beltrachini
Universite? Henri Poincare?/LORIA
laura.perez@loria.fr
Abstract
Surface realisation with grammars inte-
grating flat semantics is known to be NP
complete. In this paper, we present a new
algorithm for surface realisation based on
Feature Based Tree Adjoining Grammar
(FTAG) which draws on the observation
that an FTAG can be translated into a Reg-
ular Tree Grammar describing its deriva-
tion trees. We carry out an extensive test-
ing of several variants of this algorithm
using an automatically produced testsuite
and compare the results obtained with
those obtained using GenI, another FTAG
based surface realiser.
1 Introduction
As shown in (Brew, 1992; Koller and Striegnitz,
2002), Surface Realisation is NP-complete. Var-
ious optimisation techniques have therefore been
proposed to help improve practical runtimes. For
instance, (Kay, 1996) proposes to reduce the num-
ber of constituents built during realisation by only
considering for combination constituents with non
overlapping semantics and compatible indices.
(Kay, 1996; Carroll and Oepen, 2005; Gardent
and Kow, 2007) propose various techniques to re-
strict the combinatorics induced by intersective
modifiers all applying to the same structure. And
(Koller and Striegnitz, 2002; Gardent and Kow,
2007) describe two alternative techniques for re-
ducing the initial search space.
In this paper, we focus on the optimisation
mechanisms of two TAG based surface realisers
namely, GENI (Gardent and Kow, 2007) and the
algorithm we present in this paper namely, RT-
GEN (Perez-Beltrachini, 2009). GENI?s optimisa-
tion includes both a filtering process whose aim is
to reduce the initial search space and a two step,
?substitution before adjunction?, tree combination
phase whose effect is to delay modifier adjunc-
tion thereby reducing the number of intermediate
structures being built. In RTGEN on the other
hand, the initial FTAG is converted to a Regu-
lar Tree Grammar (RTG) describing its derivation
trees and an Earley algorithm, including sharing
and packing, is used to optimise tree combination.
We compare GENI with several variants of the
proposed RTGEN algorithm using an automati-
cally produced testsuite of 2 679 input formulae
and relate the RTGEN approach to existing work
on surface realisation optimisation.
The paper is structured as follows. We first
present the grammar used by both GENI and RT-
GEN, namely SEMXTAG (Section 2). We then de-
scribe the two surface realisation algorithms (Sec-
tion 3). In Section 4, we describe the empirical
evaluation carried out and present the results. Fi-
nally, Section 5 situates RTGEN with respect to
related work on surface realisation optimisation.
2 SemXTag
The grammar (SEMXTAG) used by GENI and
RTGEN is a Feature-Based Lexicalised Tree
Adjoining Grammar (FTAG) augmented with a
unification-based semantics as described in (Gar-
dent and Kallmeyer, 2003). We briefly introduce
each of these components and describe the gram-
mar coverage. We then show how this FTAG can
be converted to an RTG describing its derivation
trees.
367
2.1 FTAG.
A Feature-based TAG (Vijay-Shanker and Joshi,
1988) consists of a set of (auxiliary or initial) el-
ementary trees and of two tree-composition oper-
ations: substitution and adjunction. Initial trees
are trees whose leaves are labeled with substitu-
tion nodes (marked with a downarrow) or termi-
nal categories. Auxiliary trees are distinguished
by a foot node (marked with a star) whose cate-
gory must be the same as that of the root node.
Substitution inserts a tree onto a substitution node
of some other tree while adjunction inserts an aux-
iliary tree into a tree. In an FTAG, the tree nodes
are furthermore decorated with two feature struc-
tures (called top and bottom) which are unified
during derivation as follows. On substitution, the
top of the substitution node is unified with the top
of the root node of the tree being substituted in.
On adjunction, the top of the root of the auxiliary
tree is unified with the top of the node where ad-
junction takes place; and the bottom features of
the foot node are unified with the bottom features
of this node. At the end of a derivation, the top
and bottom of all nodes in the derived tree are
unified. Finally, each sentence derivation in an
FTAG is associated with both a derived tree rep-
resenting the phrase structure of the sentence and
a derivation tree recording how the correspond-
ing elementary trees were combined to form the
derived tree. Nodes in a derivation tree are la-
belled with the name of a TAG elementary tree.
Edges are labelled with a description of the opera-
tion used to combine the TAG trees whose names
label the edge vertices.
2.2 FTAG with semantics.
To associate semantic representations with natu-
ral language expressions, the FTAG is modified as
proposed in (Gardent and Kallmeyer, 2003).
NPj
John
name(j,john)
Sc
NP?s VPcb
Vba
runs
run(a,s)
VPx
often VP*
often(x)
? name(j,john), run(a,j), often(a)
Figure 1: Flat Semantics for ?John often runs?
Each elementary tree is associated with a flat
semantic representation. For instance, in Fig-
ure 1,1 the trees for John, runs and often are asso-
ciated with the semantics name(j,john), run(a,s)
and often(x) respectively. Importantly, the argu-
ments of a semantic functor are represented by
unification variables which occur both in the se-
mantic representation of this functor and on some
nodes of the associated syntactic tree. For in-
stance in Figure 1, the semantic index s occur-
ring in the semantic representation of runs also
occurs on the subject substitution node of the as-
sociated elementary tree. The value of semantic
arguments is determined by the unifications re-
sulting from adjunction and substitution. For in-
stance, the semantic index s in the tree for runs is
unified during substitution with the semantic in-
dex labelling the root node of the tree for John.
As a result, the semantics of John often runs is
{name(j,john),run(a,j),often(a)}.
2.3 SemXTAG.
SEMXTAG is an FTAG for English augmented
with a unification based compositional semantics
of the type described above. Its syntactic cover-
age approaches that of XTAG, the FTAG devel-
oped for English by the XTAG group (The XTAG
Research Group, 2001). Like this grammar, it
contains around 1300 elementary trees and cov-
ers auxiliaries, copula, raising and small clause
constructions, topicalization, relative clauses, in-
finitives, gerunds, passives, adjuncts, ditransitives
and datives, ergatives, it-clefts, wh-clefts, PRO
constructions, noun-noun modification, extraposi-
tion, sentential adjuncts, imperatives and resulta-
tives.
2.4 Converting SemXTAG to RTG
As shown in (Schmitz and Le Roux, 2008), an
FTAG can be converted to a Regular Tree Gram-
mar describing its derivation tree. In this section,
we briefly sketch this conversion process. For a
more precise description of this FTAG to RTG
conversion, the reader is referred to (Schmitz and
Le Roux, 2008).
1Cx/Cx abbreviate a node with category C and a
top/bottom feature structure including the feature-value pair
{ index : x}.
368
In the FTAG-to-RTG conversion, each SEMX-
TAG elementary tree is converted to a rule that
models its contribution to a TAG derivation tree.
A TAG derivation involves the selection of an ini-
tial tree, which has some nodes requiring substi-
tution and some permitting adjunction. Let us
think of the potential adjunction sites as requiring,
rather than permitting, adjunction, but such that
the requirement can be satisfied by ?null? adjunc-
tion. Inserting another tree into this initial tree sat-
isfies one of the substitution or adjunction require-
ments, but introduces some new requirements into
the resulting tree, in the form of its own substitu-
tion nodes and adjunction sites.
Thus, intuitively, the RTG representation of a
SEMXTAG elementary tree is a rule that rewrites
the satisfied requirement as a local tree whose root
is a unique identifier of the tree and whose leaves
are the introduced requirements. A requirement
of a substitution or adjunction of a tree of root
category X is written as XS or XA, respectively.
Here, for example, is the translation to RTG of the
FTAG tree (minus semantics) for run in Figure 1,
using the word anchoring the tree as its identifier
(the upperscripts abbreviates features structures:
b/t refers to the bottom/top feature structure and
the upper case letters to the semantic index value,
[idx : X] is abbreviated to X):
S[t:T ]S ? runs(S
[t:T,b:C]
A NP
[t:S]
S V P
[t:C,b:B]
A V
[t:B,b:A]
A )
The semantics of the SemXTAG tree are carried
over as-is to the corresponding RTG rule. Fur-
ther, the feature structures labelling the nodes of
the SemXTAG tree are converted into the RTG
rules so as to correctly interact with substitution
and adjunction (see (Schmitz and Le Roux, 2008)
for more details on this part of the conversion pro-
cess).
To account for the optionality of adjunction,
there are additional rules allowing any adjunction
requirement to be rewritten as the symbol ?, a ter-
minal symbol of the RTG.
The terminal symbols of the RTG are thus the
tree identifiers and the symbol ?, and its non-
terminals are XS and XA for each terminal or
non-terminal X of SemXTAG.
3 TAG-based surface realisation
We now present RTGEN and describe GENI, and
compare the optimisations they propose to deal
with the task complexity.
GENI and RTGEN are similar on several points.
They use the same grammar, namely SEMXTAG
(cf. Section 2). Further, they both pipeline three
main steps. First, lexical selection selects from
the grammar those elementary trees whose seman-
tics subsumes part of the input semantics. Second,
the tree combining phase systematically tries to
combine trees using substitution and adjunction.
Third, the retrieval phase extracts the yields of
the complete derived trees, thereby producing the
generated sentence(s).
GENI and RTGEN differ however with respect
to the trees they are working with (derived trees
in GENI vs derivation trees in RTGEN). They also
differ in how tree combination is handled. We now
describe these differences in more detail and ex-
plain how each approach address the complexity
issue.
3.1 GenI
The tree combining phase in GENI falls into two
main steps namely, filtering and tree combining.
Filtering. The so-called polarity filtering step
aims to reduce the initial search space. It elim-
inates from the initial search space all those sets
of TAG elementary trees which cover the input se-
mantics but cannot possibly lead to a valid derived
tree. In specific, this filtering removes all tree sets
covering the input semantics such that either the
category of a substitution node cannot be canceled
out by that of the root node of a different tree;
or a root node fails to have a matching substitu-
tion site. Importantly, this filtering relies solely
on categorial information ? feature information is
not used. Furthermore, auxiliary trees have no im-
pact on filtering since they provide and require the
same category thereby being ?polarity neutral el-
ements?.
Tree combining. The tree combining algorithm
used after filtering has taken place, is a bottom-up
tabular algorithm (Kay, 1996) optimised for TAGs.
This step, unlike the first, uses all the features
369
present in the grammar. To handle intersective
modifiers, the delayed modifiers insertion strategy
from (Carroll et al, 1999) is adapted to TAG as
follows. First, all possible derived trees are ob-
tained using only substitution. Next, adjunction
is applied. Although the number of intermediate
structures generated is still 2n for n modifiers, this
strategy has the effect of blocking these 2n struc-
tures from multiplying out with other structures in
the chart.
3.2 RTGen
RTGen synthesises different techniques that have
been observed in the past to improve surface re-
alisation runtimes. We first describe these tech-
niques i.e., the main features of RTGEN. We
then present three alternative ways of implement-
ing RTGEN which will be compared in the evalu-
ation.
3.2.1 RTGen?s main features
A main feature of RTGEN is that it focuses on
building derivation rather than derived trees. More
specifically, the first two steps of the surface real-
isation process (lexical selection, tree combining)
manipulate RTG rules describing the contribution
of the SEMXTAG elementary trees to the deriva-
tion tree rather than the elementary tree them-
selves. The derived trees needed to produce actual
sentences are only produced in the last phase i.e.,
the retrieval phase.
This strategy is inspired from a similar ap-
proach described in (Koller and Striegnitz, 2002)
which was shown to be competitive with state of
the art realisers on a small sample of example in-
put chosen for their inherent complexity. (Koller
and Striegnitz, 2002)?s approach combines trees
using a constraint based dependency parser rather
than an Earley algorithm so that it is difficult
to assess how much of the efficiency is due to
the parser and how much to the grammar con-
version. Intuitively however, the motivation un-
derlying the construction of a derivation rather
than a derived tree is that efficiency might be in-
creased because the context free derivation trees
(i) are simpler than the mildly context sensitive
trees generated by an FTAG and (ii) permit draw-
ing on efficient parsing and surface realisation al-
gorithms designed for such grammars.
Second, RTGEN makes use of the now standard
semantic criteria proposed in (Kay, 1996; Carroll
et al, 1999) to reduce the number of combinations
tried out by the realiser. On the one hand, two con-
stituents are combined by the algorithm?s infer-
ence rules only if they cover disjoint parts of the
input semantics. On the other hand, the seman-
tic indices present in both the input formula and
the lexically retrieved RTG trees are used to pre-
vent the generation of intermediate structures that
are not compatible with the input semantics. For
instance, given the input formula for ?John likes
Mary?, semantic indices will block the generation
of ?likes John? because this constituent requires
that the constituent for ?John? fills the patient slot
of ?likes? whereas the input semantics requires
that it fills the agent slot. In addition, chart items
in RTGEN are indexed by semantic indices to ef-
ficiently select chart items for combination.
Third, RTGEN implements a standard Earley
algorithm complete with sharing and packing.
Sharing allows for intermediate structures that are
common to several derivations to be represented
only once ? in addition to not being recomputed
each time. Packing means that partial derivation
trees with identical semantic coverage and similar
combinatorics (same number and type of substi-
tution and adjunction requirements) are grouped
together and that only one representative of such
groups is stored in the chart. In this way, interme-
diate structures covering the same set of intersec-
tive modifiers in a different order are only repre-
sented once and the negative impact of intersec-
tive modifiers is lessened (cf. (Brew, 1992)). . As
(Carroll and Oepen, 2005) have shown, packing
and sharing are important factors in improving ef-
ficiency. In particular, they show that an algorithm
with packing and sharing clearly outtperforms the
same algorithm without packing and sharing giv-
ing an up to 50 times speed-up for inputs with
large numbers of realizations.
3.2.2 Three ways to implement RTGen
Depending on how much linguistic information
(i.e. feature constraints from the feature struc-
tures) is preserved in the RTG rules, several RT-
GEN configurations can be tried out which each
370
reflect a different division of labour between con-
straint solving and structure building. To experi-
ment with these several configurations, we exploit
the fact that the FTAG-to-RTG conversion proce-
dure developed by Sylvain Schmitz permits spec-
ifying which features should be preserved by the
conversion.
RTGen-all. In this configuration, all the feature
structure information present in the SEMXTAG el-
ementary trees is carried over to the RTG rules.
As a result, tree combining and constraint solving
proceed simultaneously and the generated parse
forest contains the derivation trees of all the out-
put sentences.
RTGen-level0. In the RTGen-level0 configura-
tion, only the syntactic category and the seman-
tic features are preserved by the conversion. As
a result, the grammar information used by the
(derivation) tree building phase is comparable to
that used by GENI filtering step. In both cases,
the aim is to detect those sets of elementary trees
which cover the input semantics and such that all
syntactic requirements are satisfied while no syn-
tactic resource is left out. A further step is addi-
tionally needed to produce only those trees which
can be built from these tree sets when applying the
constraints imposed by other features. In GENI,
this additional step is carried out by the tree com-
bining phase, in RTGEN, it is realised by the ex-
traction phase i.e., the phase that constructs the
derived trees from the derivation trees produced
by the tree combining phase.
RTGen-selective. Contrary to parsing, surface
realisation only accesses the morphological lex-
icon last i.e., after sentence trees are built. Be-
cause throughout the tree combining phase, lem-
mas are handled rather than forms, much of the
morpho-syntactic feature information which is
necessary to block the construction of ill-formed
constituents is simply not available. It is therefore
meaningful to only include in the tree combining
phase those features whose value is available at
tree combining time. In a third experiment, we au-
tomatically identified those features from the ob-
served feature structure unification failures during
runs of the realisation algorithm. We then use only
these features (in combination with the semantic
features and with categorial information) during
tree combining.
4 Evaluation
To evaluate the impact of the different optimisa-
tion techniques discussed in the previous section,
we use two benchmarks generated automatically
from SEMXTAG (Gottesman, 2009).
The first benchmark (MODIFIERS) was de-
signed to test the realisers on cases involving in-
tersective modifiers. It includes 1 789 input for-
mulae with a varying number (from 0 to 4 modifi-
cations), type (N and VP modifications) and distri-
bution of intersective modifiers (n modifiers dis-
tributed differently over the predicate argument
structures). For instance, the formula in (1) in-
volves 2 N and 1 VP modification. Further,
it combines lexical ambiguity with modification
complexities, i.e. for the snore modifier the gram-
mar provides 10 trees.
(1) l1 : ?(x1, hr, hs), hr ? l2, hs ? l3, l2 :
man(x1), l2 : snoring(e1, x1), l2 : big(x1), l3 :
sleep(e2, x1), l4 : soundly(e2)
(A snoring big man sleeps soundly)
The second benchmark (COMPLEXITY) was
designed to test overall performance on cases of
differing complexity (input formulae of increas-
ing length, involving verbs with a various number
and types of arguments and with a varying num-
ber of and types of modifiers). It contains 890 dis-
tinct cases. A sample formula extracted from this
benchmark is shown in (2), which includes one
modification and to different verb types.
(2) h1 ? l4, l0 : want(e, h1), l1 : ?(x1, hr, hs), hr ?
l1, hs ? l0, l1 : man(x1), l1 : snoring(e1, x1), l3 :
?(x2, hp, hw , hu), hp ? l3, hw ? l4, hu ? l5, l3 :
monkey(x2), l4 : eat(e2, x2, e3), l5 : sleep(e3, x2)
(The snoring man wants the monkey to sleep)
To evaluate GENI and the various configurations
of RTGEN (RTGEN-all, RTGEN-level0, RTGEN-
selective), we ran the 4 algorithms in batch mode
on the two benchmarks and collected the follow-
ing data for each test case:
? Packed chart size : the number of chart items
built. This feature is only aplicable to RTGen
as GENI does not implement packing.
371
? Unpacked chart size : the number of interme-
diate and final structures available after un-
packing (or at the end of the tree combining
process in the case of GENI).
? Initial Search Space (ISS) : the number of all
possible combinations of elementary trees to
be explored given the result of lexical selec-
tion on the input semantics. That is, the prod-
uct of the number of FTAG elementary trees
selected by each literal in the input seman-
tics.
? Generation forest (GF) : the number of
derivation trees covering the input semantics.
The graph in Figure 2 shows the differences be-
tween the different strategies with respect to the
unpacked chart size metric.
A first observation is that RTGEN-all outper-
forms GENI in terms of intermediate structures
built . In other words, the Earley sharing and
packing strategy is more effective in reducing the
number of constituents built than the filtering and
substitution-before-adjunction optimisations used
by GENI. In fact, even when no feature informa-
tion is used at all (RTGEN-level0 plot), for more
complex test cases, packing and sharing is more
effective in reducing the chart size than filtering
and operation ordering.
Another interesting observation is that RTGEN-
all and RTGEN-selective have the same impact on
chart size (their plots coincide). This is unsurpris-
ing since the features used by RTGEN-selective
have been selected based on their ability to block
constituent combination. The features used in
RTGEN-selective mode are wh, xp, assign-comp,
mode, definite, inv, assign-case, rel-clause,
extracted and phon, in addition to the categorial
and semantic information. In other words, using
all 42 SEMXTAG grammar features has the same
impact on search space pruning as using only a
small subset of them. As explained in the previ-
ous section, this is probably due to the fact that
contrary to parsing, surface realisation only ac-
cesses the morphological lexicon after tree com-
bining takes place. Another possibility is that the
grammar is under constrained and that feature val-
ues are missing thereby inducing overgeneration.
Zooming in on cases involving three modifiers,
0 1 2 3 4 5 6 7
103
104
p
p
p
p
p
p
number of modifiers
u
n
pa
ck
ed
ch
ar
ts
iz
e
RTGEN-all
RTGEN-level0
p RTGEN-selective
GENI
Figure 2: Performance of realisation approaches
on the MODIFIERS benchmark, average unpacked
chart size as a function of the number of modifiers.
we show in Table 1 the average results for various
efficiency metrics 2. This provides a more detail
view of the performance of the differences among
the three RTGEN variants.
strategy GF chart unpacked-chart seconds
RTGen-all 15.05 918.31 2,538.98 0.99
RTGen-level0 1,118.06 2,018 6,898.28 1.41
RTGen-selective 27.08 910.34 2,531.23 0.44
Table 1: Average results on 610 test cases from
the MODIFIERS benchmark. Each test case has
3 modifications, distributed in various ways be-
tween adjectival and adverbial modifications. The
second column, Generation Forest (GF), is the
number of derivation trees present in the gener-
ated parse forest. The third and fourth columns
show the chart and unpacked chart sizes, respec-
tively. The last column shows the runtime in sec-
onds.
This data shows that running RTGEN with no
feature information leads not only to an increased
chart size but also to runtimes that are higher in
average than for full surface realisation i.e., reali-
sation using the full grammar complete with con-
2The two realisers being implemented in different
programming languages (RTGEN uses Prolog and GENI
Haskell), runtimes comparisons are not necessarily very
meaningful. Additionally, GENI does not provide time statis-
tics. After adding this functionality to GENI, we found that
overall GENI is faster on simple cases but slower on more
complex ones. We are currently working on optimising RT-
GEN prolog implementation before carrying out a full scale
runtime comparison.
372
0-
10
0
10
0-
10
00
10
00
-
50
00
50
00
-
10
00
0
10
00
0-
10
00
00
10
00
00
-
50
00
00
50
00
00
-
10
00
00
0
m
o
re
th
an
10
00
00
0
102
103
104
105
106
p
p p
p p
p p p
Initial Search Space (ISS) size
u
n
pa
ck
ed
ch
ar
ts
iz
e
RTGEN-all
RTGEN-level0
p RTGEN-selective
GENI
Figure 3: Performance of realisation approaches
on the COMPLEXITY benchmark, average un-
packed chart size as a function of the ISS com-
plexity.
straints.
Interestingly, it also shows that the selective
mode (RTGEN-selective) permits improving run-
times while achieving almost perfect disambigua-
tion in that the average number of derivation trees
(GF) produced is close to that produced when
using all features. The differences between the
two generation forests stems from packing. Using
only a subset of features favors packing, thereby
reducing the number of chart items built, but in-
troduces over- generation.
Graph 3 and Table 2 confirm the results ob-
tained using the MODIFIERS benchmark on a test-
set (COMPLEXITY) where input complexity varies
not only with respect to modification but also with
respect to the length of the input and to the de-
gree of lexical ambiguity. Typically, in a TAG, one
word or one semantic literal may be associated ei-
ther with one tree or with up to several hundred
trees (e.g., ditransitive verbs and verbs with sev-
eral subcategorisation types). By varying the type
and the number of verbs selected by the seman-
tic literals contained in the input semantics, the
COMPLEXITY benchmark provides a more exten-
sive way to test performance on cases of varying
complexity.
strategy GF chart unpacked-chart seconds
RTGen-all 14.77 693.39 2,427.82 0.81
RTGen-level0 162.02 2,114.16 6,954.84 1.09
RTGen-selective 15.31 692.9 2,427.2 0.36
Table 2: Average results on 335 cases with
10000 < ISS ? 100000, from the COMPLEXITY
benchmark. The columns show the same perfor-
mance metrics as in Table 1.
5 Related work
Much work has already been done on optimising
surface realisation. Because surface realisation
often draws on parsing techniques, work on pars-
ing optimisation is also relevant. In this section,
we briefly relate our proposal to another gram-
mar converting approach (Koller and Striegnitz,
2002); to another chart based approach (Carroll
and Oepen, 2005); and to approaches based on
statistical pruning (White, 2004; Bangalore and
Rambow, 2000).
5.1 Optimising surface realisation
Encoding into another grammatical formalism.
As already mentioned, the RTGEN approach is
closely related to the work of (Koller and Strieg-
nitz, 2002) where the XTAG grammar is con-
verted to a dependency grammar capturing its
derivation trees. This conversion enables the use
of a constraint based dependency parser, a parser
which was specifically developed for the efficient
parsing of free word order languages and is shown
to support an efficient handling of both lexical and
modifier attachment ambiguity.
Our proposal differs from this approach in three
main ways. First, contrary to XTAG, SEMX-
TAG integrates a full-fledged, unification based
compositional semantics thereby allowing for a
principled coupling between semantic represen-
tations and natural language expressions. Sec-
ond, the grammar conversion and the feature-
based RTGs used by RTGEN accurately trans-
lates the full range of unification mechanisms em-
ployed in FTAG wheras the conversion described
by (Koller and Striegnitz, 2002) does not take
into account feature structure information. Third,
the RTGEN approach was extensively tested on a
large benchmark using 3 different configurations
whilst (Koller and Striegnitz, 2002) results are re-
373
stricted to a few hand constructed example inputs.
Chart generation algorithm optimisations.
(Carroll and Oepen, 2005) provides an extensive
and detailed study of how various techniques used
to optimise parsing and surface realisation impact
the efficiency of a surface realiser based on a large
coverage Head-Driven Phrase Structure grammar.
Because they use different grammars, gram-
mar formalisms and different benchmarks, it is
difficult to compare the RTGEN and the HPSG
approach. However, one point is put forward
by (Carroll and Oepen, 2005) which it would
be interesting to integrate in RTGEN(Carroll and
Oepen, 2005) show that for packing to be effi-
cient, it is important that equivalence be checked
through subsumption, not through equality. RT-
GEN also implements a packing mechanism with
subsumption check, i.e. different ways of cov-
ering the same subset of the input semantics are
grouped together and represented in the chart by
the most general one. One difference however it
that RTGEN will pack analyses together as long
as the new ones are more specific cases. It will
not go backwards to recalculate the packing made
so far if a more general item is found (Stefan and
John, 2000). In this case the algorithm will pack
them under two different groups.
Statistical pruning. Various probabilistic tech-
niques have been proposed in surface realisation
to improve e.g., lexical selection, the handling of
intersective modifiers or ranking. For instance,
(Bangalore and Rambow, 2000) uses a tree model
to produce a single most probable lexical selec-
tion while in White?s system, the best paraphrase
is determined on the basis of n-gram scores. Fur-
ther, to address the fact that there are n! ways
to combine any n modifiers with a single con-
stituent, (White, 2004) proposes to use a language
model to prune the chart of identical edges rep-
resenting different modifier permutations, e.g., to
choose between fierce black cat and black fierce
cat. Similarly, (Bangalore and Rambow, 2000) as-
sumes a single derivation tree that encodes a word
lattice (a {fierce black, black fierce} cat), and uses
statistical knowledge to select the best linearisa-
tion. Our approach differs from these approaches
in that lexical selection is not filtered, intersective
modifiers are handled by the grammar (constraints
on the respective order of adjectives) and the chart
packing strategy (for optimisation), and ranking is
not performed. We are currently exploring the use
of Optimality Theory for ranking.
6 Conclusion
We presented RTGEN, a novel surface realiser for
FTAG grammars which builds on the observation
that an FTAG can be translated to a regular tree
grammar describing its derivation trees. Using
automatically constructed benchmarks, we com-
pared the performance of this realiser with that of
GENI, another state of the art realiser for FTAG.
We showed that RTGEN outperforms GENI in
terms of space i.e. that the Earley sharing and
packing strategy is more effective in reducing the
number of constituents built than the filtering and
substitution-before-adjunction optimisations used
by GENI. Moreover, we investigated three ways
of interleaving phrase structure and feature struc-
ture constraints and showed that, given a naive
constraint solving approach, the interleaving ap-
proach with selective features seems to provide
the best space/runtimes compromise.
Future work will concentrate on further investi-
gating the interplay in surface realisation between
phrase structure and feature structure constraints.
In particular, (Maxwell and Kaplan, 1994) shows
that a more sophisticated approach to constraint
solving and to its interaction with chart process-
ing renders the non interleaved approach more ef-
fective than the interleaved one. We plan to exam-
ine whether this observation applies to SEMXTAG
and RTGEN. Further, we intend to integrate Op-
timality Theory constraints in RTGEN so as sup-
port ranking of multiple outputs. Finally, we want
to further optimise RTGEN on intersective modi-
fiers using one the methods mentioned in Section
5.
References
Bangalore, S. and O. Rambow. 2000. Using TAGs, a
tree model and a language model for generation. In
Proceedings of TAG+5, Paris, France.
Brew, Chris. 1992. Letting the cat out of the bag:
generation for shake-and-bake mt. In Proceedings
374
of the 14th conference on Computational linguistics,
pages 610?616, Morristown, NJ, USA. Association
for Computational Linguistics.
Carroll, J. and S. Oepen. 2005. High efficiency re-
alization for a wide-coverage unification grammar.
2nd IJCNLP.
Carroll, J., A. Copestake, D. Flickinger, and
V. Paznan?ski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings of
EWNLG ?99.
Gardent, C. and L. Kallmeyer. 2003. Semantic con-
struction in FTAG. In 10th EACL, Budapest, Hun-
gary.
Gardent, C. and E. Kow. 2007. Spotting overgenera-
tion suspect. In 11th European Workshop on Natu-
ral Language Generation (ENLG).
Gottesman, B. 2009. Generating examples. Mas-
ter?s thesis, Erasmus Mundus Master Language and
Communication Technology, Saarbrucken/Nancy.
Kay, Martin. 1996. Chart generation. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 200?204, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Koller, A. and K. Striegnitz. 2002. Generation as de-
pendency parsing. In Proceedings of the 40th ACL,
Philadelphia.
Maxwell, J. and R. Kaplan. 1994. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4).
Perez-Beltrachini, L. 2009. Using regular tree
grammars to reduce the search space in surface
realisation. Master?s thesis, Erasmus Mundus
Master Language and Communication Technology,
Nancy/Bolzano.
Schmitz, S. and J. Le Roux. 2008. Feature uni-
fication in TAG derivation trees. In Gardent, C.
and A. Sarkar, editors, Proceedings of the 9th In-
ternational Workshop on Tree Adjoining Grammars
and Related Formalisms (TAG+?08), pages 141?
148, Tu?bingen, Germany.
Stefan, Oepen and Carroll John. 2000. Parser engi-
neering and performance profiling. Journal of Nat-
ural Language Engineering, 6(1):81?98.
The XTAG Research Group. 2001. A lexicalised tree
adjoining grammar for english. Technical report,
Institute for Research in Cognitive Science, Univer-
sity of Pennsylvannia.
Vijay-Shanker, K. and AK Joshi. 1988. Feature Struc-
tures Based Tree Adjoining Grammars. Proceed-
ings of the 12th conference on Computational lin-
guistics, 55:v2.
White, M. 2004. Reining in CCG chart realization. In
INLG, pages 182?191.
375
Coling 2010: Poster Volume, pages 45?53,
Beijing, August 2010
Benchmarking for syntax-based sentential inference
Paul Bedaride
INRIA/LORIA
Universite? Henri Poincare?
paul.bedaride@loria.fr
Claire Gardent
CNRS/LORIA
claire.gardent@loria.fr
Abstract
We propose a methodology for investigat-
ing how well NLP systems handle mean-
ing preserving syntactic variations. We
start by presenting a method for the semi
automated creation of a benchmark where
entailment is mediated solely by meaning
preserving syntactic variations. We then
use this benchmark to compare a seman-
tic role labeller and two grammar based
RTE systems. We argue that the proposed
methodology (i) supports a modular eval-
uation of the ability of NLP systems to
handle the syntax/semantic interface and
(ii) permits focused error mining and er-
ror analysis.
1 Introduction
First launched in 2005, the Recognising Textual
Inference Challenge (RTE)1 aims to assess in how
far computer systems can emulate a human being
in determining whether a short text fragment H
referred to as the hypothesis, follows from or is
contradicted by a text fragment T . In the RTE
benchmarks, the hypothesis is a short constructed
sentence whilst the text fragments are short pas-
sages of naturally occurring texts. As a result, the
RTE challenge permits evaluating the capacity of
NLP systems to handle local textual inference on
real data, an enabling technology for any applica-
tions involving document interpretation.
In this paper, we focus on entailments based on
meaning entailing, syntactic transformations such
as:
(1) The man gives the woman the flowers that
smell nice ? The flowers which are given
to the woman smell nice
1http://www.pascal-network.org/
Challenges/RTE
We start (Section 2) by motivating the ap-
proach. We argue that the proposed evaluation
methodology (i) interestingly complements the
RTE challenge in that it permits a modular, ana-
lytic evaluation of the ability of NLP systems to
handle syntax-based, sentential inference and (ii)
permits focused error mining and analysis .
In Section 3, we go on to describe the bench-
mark construction process. Each item of the con-
structed benchmark associates two sentences with
a truth value (true or false) indicating whether
or not the second sentence can be understood to
follow from the first. The construction of these
benchmark items relies on the use of a gram-
mar based surface realiser and we show how this
permits automatically associating with each infer-
ence item, an entailment value (true or false) and
a detailed syntactic annotation reflecting the syn-
tactic constructs present in the two sentences con-
stituting each benchmark item.
In section 4, we use the benchmark to evaluate
and compare three systems designed to recognise
meaning preserving syntactic variations namely,
a semantic role labeller, Johan Bos? Nutcracker
RTE system (where the syntax/semantic interface
is handled by a semantic construction module
working on the output of combinatory categorial
grammar parser) and the Afazio system, a hybrid
system combining statistical parsing, symbolic se-
mantic role labelling and sentential entailment de-
tection using first order logic. We give the eval-
uation figures for each system. Additionally, we
show how the detailed syntactic annotations au-
tomatically associated with each benchmark item
by the surface realiser can be used to identify the
most likely source of errors that is, the syntactic
constructs that most frequently co-occur with en
entailment recognition error.
45
2 Motivations
Arguably focusing on meaning entailing syntac-
tic transformations is very weak. Indeed, one of
the key conclusions at the second RTE Challenge
Workshop was that entailment modeling requires
vast knowledge resources that correspond to dif-
ferent types of entailment reasoning e.g., ontolog-
ical and lexical relationships, paraphrases and en-
tailment rules, meaning entailing syntactic trans-
formations and last but not least, world knowl-
edge. Further, Manning (2006) has strongly ar-
gued against circumscribing the RTE data to cer-
tain forms of inference such as for instance, infer-
ences based solely on linguistic knowledge. Fi-
nally, it is also often insisted that naturally occur-
ring data should be favored over constructed data.
While we agree that challenges such as the RTE
challenge are useful in testing systems abilities to
cope with real data, we believe there is also room
for more focused evaluation setups.
Focusing on syntax based entailments. As
mentioned above, syntax based entailment is only
one of the many inference types involved in deter-
mining textual entailment. Nevertheless, a manual
analysis of the RTE1 data by (Vanderwende et al,
2005) indicates that 37% of the examples could
be handled by considering syntax alone. Sim-
ilarly, (Garoufi, 2007) shows that 37.5% of the
RTE2 data does not involve deep reasoning and
more specifically, that 33.8% of the RTE2 data in-
volves syntactic or lexical knowledge only. Hence
although the holistic, blackbox type of evaluation
practiced in the RTE challenge is undeniably use-
ful in assessing the ability of existing systems to
handle local textual inference, a more analytic,
modular kind of evaluation targeting syntax-based
entailment reasoning is arguably also of interest.
Another interesting feature of the SSI (syntax-
based sentential entailment) task we propose is
that it provides an alternative way of evaluating
semantic role labelling (SRL) systems. Typically,
the evaluation of SRL systems relies on a hand an-
notated corpus such as PropBank or the FrameNet
corpus. The systems precision and recall are then
computed w.r.t. this reference corpus. As has been
repeatedly argued (Moll and Hutchinson, 2003;
Galliers and Jones, 1993), intrinsic evaluations
may be of very limited value. For semantically
oriented tools such as SRL systems, it is important
to also assess their results w.r.t. the task which
they are meant support namely reasoning : Do
the semantic representations built by SRL help in
making the correct inferences ? Can they be used,
for instance, to determine whether a given sen-
tence answers a given question ? or whether the
content of one sentence follow from that another ?
As explained in (Giampiccolo et al, 2007), entail-
ment recognition is a first, major step towards an-
swering these questions. Accordingly, instead of
comparing the representations produced by SRL
systems against a gold standard, the evaluation
scheme presented here, permits evaluating them
w.r.t. their ability to capture syntax based senten-
tial inference.
It is worth adding that, although the present pa-
per focuses on entailments strictly based on syn-
tax, the proposed methodology should straight-
forwardly extend to further types of entailment
such as in particular, entailments involving lexi-
cal relations (synonymy, antonymy, etc.) or entail-
ments involving more complex semantic phenom-
ena such as the interplay between different classes
of complement taking verbs, polarity and author
commitment discussed in (Nairn et al, 2006).
This is because as we shall see in section 3, our
approach is based on an extensive, hand written
grammar of English integrating syntax and se-
mantics. By modifying the grammar, the lexicon
and/or the semantics, data of varying linguistic
type and complexity can be produced and used for
evaluation.
Hand constructed vs. naturally occurring data.
Although in the 90s, hand tailored testsuites such
as (Lehmann et al, 1996; Cooper et al, 1995)
were deemed useful for evaluating NLP systems,
it is today generally assumed that, for evaluation
purposes, naturally occurring data is best. We ar-
gue that constructed data can interestingly com-
plement naturally occurring data.
To start with, we agree with (Crouch et al,
2006; Cohen et al, 2008) that science generally
benefits from combining laboratory and field stud-
ies and more specifically, that computational lin-
guistics can benefit from evaluating systems on
46
a combination of naturally occurring and con-
structed data.
Moreover, constructed data need not be hand
constructed. Interestingly, automating the produc-
tion of this data can help provide better data anno-
tation as well as better and better balanced data
coverage than both hand constructed data and nat-
urally occurring data. Indeed, as we shall show
in section 4, the benchmark creation process pre-
sented here supports a detailed and fully auto-
mated annotation of the syntactic properties as-
sociated with each benchmark item. As shown
in section 5, this in turn allows for detailed er-
ror mining making it possible to identify the most
likely causes of system errors. Additionally, the
proposed methodology permits controlling over
such benchmark parameters as the size of the data
set, the balance between true and false entail-
ments, the correlation between word overlap and
entailment value and/or the specific syntactic phe-
nomena involved. This is in contrast with the RTE
data collection process where ?the distribution of
examples is arbitrary to a large extent, being de-
termined by manual selection2? (Giampiccolo et
al., 2007). As has been repeatedly pointed out
(Burchardt et al, 2007; Garoufi, 2007), the RTE
datasets are poorly balanced w.r.t., both the fre-
quency and the coverage of the various phenom-
ena interacting with textual inference.
3 Benchmark
We now present the content of an SSI benchmark
and the method for constructing it.
An SSI benchmark item (cf. e.g., Figure 1) con-
sists of two sentences and a truth value (true or
false) indicating whether or not the second sen-
tence can be understood to follow from the first.
In addition, each sentence is associated with a de-
tailed syntactic annotation describing the syntac-
tic constructs present in the sentence.
The benchmark construction process consists
of two main steps. First, a generation bank is
built. Second, this generation bank is drawn upon
2The short texts of the RTE benchmarks are automatically
extracted from real texts using different applications (e.g.,
Q/A, summarisation, information extraction, information re-
trieval systems) but the query used to retrieve these texts is
either constructed manually or post-edited.
T: The man gives the woman the flowers that smell
nice
smell:{n0Va1,active,relSubj,canAdj}
give:{n0Vn2n1,active,canSubj,canObj,canIObj}
H: The flowers are given to the woman
give:{n0Vn1Pn2,shortPassive,canSubj,canIObj}
Entailment: TRUE
Figure 1: An SSI Benchmark item
to construct a balanced data set for SSI evaluation.
We now describe each of these processes in turn.
Constructing a generation bank We use the
term ?generation bank? to refer to a dataset whose
items are produced by a surface realiser i.e., a
sentence generator. A surface realiser in turn
is a program which associates with a given se-
mantic representation, the set of sentences ver-
balising the meaning encoded by that representa-
tion. To construct our generation bank, we use the
GenI surface realiser (Gardent and Kow, 2007).
This realiser uses a Feature based Tree Adjoining
Grammar (FTAG) augmented with a unification
sematics as proposed in (Gardent and Kallmeyer,
2003) to produce all the sentences associated by
the grammar with a given semantic representa-
tion. Interestingly, the FTAG used has been com-
piled out of a factorised representation and as a
result, each elementary grammar unit (i.e., ele-
mentary FTAG tree) and further each parse tree, is
associated with a list of items indicating the syn-
tactic construct(s) captured by that unit/tree3. In
short, GenI permits associating with a given se-
mantics, a set of sentences and further for each of
these sentences, a set of items indicating the syn-
tactic construct(s) present in the syntactic tree of
that sentence. For instance, the sentences and the
syntactic constructs associated by GenI with the
semantics given in (2) are those given in (3).
(2) A:give(B C D E) G:the(C) F:man(C)
H:the(D) I:woman(D) J:the(E) K:flower(E)
L:passive(B) L:smell(M E N) O:nice(N)
(3) a. The flower which smells nice is given to
the woman by the man
3Space is lacking to give a detailed explanation of this
process here. We refer the reader to (Gardent and Kow, 2007)
for more details on how GenI associates with a given seman-
tics, a set of sentences and for each sentence a set of items
indicating the syntactic construct(s) present in the syntactic
tree of that sentence.
47
give:n0Vn1Pn2-Passive-CanSubj-ToObj-ByAgt,
smell:n0V-active-OvertSubjectRelative
b. The flower which smells nice is given
the woman by the man
give:n0Vn2n1-Passive,
smell:n0V-active-OvertSubjectRelative
c. The flower which is given the woman by
the man smells nice
give:n0Vn2n1-Passive-CovertSubjectRelative,
smell:n0V-active
d. The flower which is given to the woman
by the man smells nice
give:n0Vn1Pn2-Passive-OvertSubjectRelative,
smell:n0V-active
e. The flower that smells nice is given to
the woman by the man
give:n0Vn1Pn2-Passive,
smell:n0V-CovertSubjectRelative
f. The flower that smells nice is given the
woman by the man
give:n0Vn2n1-Passive,
smell:n0V-CovertSubjectRelative
g. The flower that is given the woman by
the man smells nice
give:n0Vn2n1-Passive-CovertSubjectRelative,
smell:n0V-active
h. The flower that is given to the woman by
the man smells nice
give:n0Vn1Pn2-Passive-CovertSubjectRelative,
smell:n0V-active
The tagset of syntactic annotation covers the sub-
categorisation type of the verb, a specification of
the verb mood and a description of how arguments
are realised.
The semantic representation language used is
a simplified version of the flat semantics used
in e.g., (Copestake et al, 2005) which is suf-
ficient for the cases handled in the present pa-
per. The grammar and therefore the generator,
can however easily be modified to integrate the
more sophisticated version proposed in (Gardent
and Kallmeyer, 2003) and thereby provide an ad-
equate treatment of scope.
Constructing an SSI benchmark. Given a
generation bank, false and true sentential entail-
ment pairs can be automatically produced by tak-
ing pairs of sentences ?S1, S2? and comparing
their semantics: if the semantics of S2 is entailed
by the semantics of S1, the pair is marked as TRUE
else as FALSE. The syntactic annotations asso-
ciated in the generation bank with each sentence
are carried over to the SSI benchmark thereby en-
suring that the overall information contained in
each SSI benchmark is as illustrated in Figure 1
namely, two pairs of syntactically annotated sen-
tences and a truth value indicating (non) entail-
ment.
To determine whether a sentence textually en-
tails another we translate their flat semantic rep-
resentation into first order logic and check for
logical entailment. Differences in semantic rep-
resentations which are linked to functional sur-
face differences such as active/passive or the
presence/absence of a complementizer (John sees
Mary leaving/John sees that Mary leaves) are
dealt with by (automatically) removing the corre-
sponding semantic literals from the semantic rep-
resentation before translating it to first order logic.
In other words, active/passive variants of the same
sentence are deemed semantically equivalent.
Note that contrary to what is assumed in the
RTE challenge, entailment is here logical rather
than textual (i.e., determined by a human) entail-
ment. By using logical, rather than textual (i.e.,
human based) entailment, it is possible that some
cases of syntax mediated textual entailments are
not taken into account. However, intuitively, it
seems reasonable to assume that for most of the
entailments mediated by syntax alone, logical and
textual entailments coincide.
3.1 The SSI benchmark
Using the methodology just described, we first
produced a generation bank of 226 items using 81
input formula distributed over 4 verb types. From
this generation bank, a total of 6 396 SSI-pairs
were built with a ratio of 42.6% true and 57.4%
false entailments.
For our experiment, we extracted from this SSI-
suite, 1000 pairs with an equal proportion of true
and false entailments and a 7/23/30/40 distribu-
tion of four subcategorisation types namely, ad-
jectival predicative (n0Va1 e.g., The cake tastes
good), intransitive (n0V), transitive (n0Vn1) and
ditransitive (n0Vn2n1)4. We furthermore con-
4The subcategorisation type of an SSI item is determined
manually and refers either to the main verb if the sentence is
48
strained the suite to respect a neutral correlation
between word overlap and entailment. Following
(Garoufi, 2007), we define this correlation as fol-
lows. The word overlap wo(T,H) between two
sentences T and H is the ratio of common lem-
mas between T and H on the number of lemmas
in H (non content words are ignored). If entail-
ment holds, the word overlap/entailment correla-
tion value of the sentence pair is wo(T,H). Oth-
erwise it is 1 ? wo(T,H). The 1000 items of the
SSI suite used in our experiment were chosen in
such a way that the word overlap/entailment cor-
relation value of the SSI suite is 0.49.
In sum, the SSI suite used for testing exhibits
the following features. First, it is balanced w.r.t.
entailment. Second, it displays good syntactic
variability based both on the constrained distribu-
tion of the four subcategorisation types and on the
use of the XTAG grammar to construct sentences
from abstract representations (cf. the paraphrases
in (3) generated by GenI from the representation
given in (2)). Third, it contains 1000 items and
could easily be extended to cover more and more
varied data. Fourth, it is specifically tailored to
check systems on their ability to deal with syntax
based sentential entailment: word overlap is high,
syntactic variability is provided and the correla-
tion between word overlap and entailment is not
biased.
4 System evaluation and comparison
SRL and grammar based systems equipped with
a compositional semantics are primary targets for
an SSI evaluation. Indeed these systems aim to
abstract away from syntactic differences by pro-
ducing semantic representations of a text which
capture predicate/argument relations independent
of their syntactic realisation.
We evaluated three such systems on the SSI
benchmark namely, NutCracker, (Johansson and
Nugues, 2008)?s Semantic Role Labeller and the
Afazio RTE system.
4.1 Systems
Nutcracker Nutcracker is a system for recog-
nising textual entailment which uses deep seman-
a clause or to the embedded verb if the sentence is a complex
sentence.
tic processing and automated reasoning. Deep se-
mantic processing associates each sentence with a
Discourse Representation Structure (DRS (Kamp
and Reyle, 1993)) by first, using a statistical
parser to build the syntactic parse of the sentence
and second, using a symbolic semantic construc-
tion module to associate a DRS with the syn-
tactic parse. Entailment between two DRSs is
then checked by translating this DRS into a first-
order logical (FOL) formula and first trying to
find a proof. If a proof is found then the en-
tailment is set to true. Otherwise, Nutcracker
backs off with a word overlap module computed
over an abstract representation of the input sen-
tences and taking into account WordNet related
information. Nutcracker was entered in the first
RTE challenge and scored an accuraccy (percent-
age of correct judgments) of 0.562 when used as
is and 0.612 when combined with machine learn-
ing techniques. For our experiment, we use the
online version of Nutcracker and the given default
parameters.
Afazio Like Nutcracker, the Afazio system
combines a statistical parser (the Stanford parser)
with a symbolic semantic component. This com-
ponent pipelines several rewrite modules which
translate the parser output into a first order logic
formula intended to abstract away from sur-
face differences and assign syntactic paraphrases
the same representation (Bedaride and Gardent,
2009). Special emphasis is placed on captur-
ing syntax based equivalences such as syntac-
tic (e.g., active/passive) variations, redistributions
and noun/verb variants. Once the parser out-
put has been normalised into predicate/argument
representations capturing these equivalences, the
resulting structures are rewritten into first order
logic formulae. Like Nutcracker, Afazio checks
entailment using first order automated reasoners
namely, Equinox and Paradox 5.
SRL (Johansson and Nugues, 2008)?s seman-
tic role labeller achieved the top score in the
closed CoNLL 2008 challenge reaching a labeled
semantic F1 of 81.65. To allow for compari-
son with Nutcracker and Afazio, we adapted the
5http://www.cs.chalmers.se/?koen/
folkung/
49
rewrite module used in Afazio to rewrite Pred-
icate/Argument structures into FOL formula in
such a way as to fit (Johansson and Nugues,
2008)?s SRL output. We then use FOL automated
reasoner to check entailment.
4.2 Evaluation scheme and results
The results obtained by the three systems are
summarised in Table 1. TP (true positives) is
the number of entailments recognised as such by
the system and TN (true negatives) of non entail-
ments. Conversely, FN and FP indicate how often
the systems get it wrong: FP is the number of non
entailments labelled as entailments by the system
and FN, the number of entailments labelled as non
entailments. ?ERROR? refers to cases where the
CCG parser used by Nutcracker fails to find a
parse. The last three columns indicate the over-
all ability of the systems to recognise false entail-
ments (TN/N with N the number of false entail-
ment in the benchmark), true entailments (TP/P)
and all true and false entailment (Precision).
Overall, Afazio outperforms both Nutcracker
and the SRL system. This is unsurprising since
contrary to these other two systems, Afazio was
specifically designed to handle syntax based sen-
tential entailment. Its strength is that it combines
a full SRL system with a semantic construction
module designed for entailment detection. More
surprisingly, the CCG parser used by Nutcracker
often fails to find a parse.
The SRL system has a high rate of false nega-
tives. Using the error mining technique presented
in the next section, we found that the most sus-
picious syntactic constructs all included a rela-
tivised argument. A closer look at the analyses
showed that this was due to the fact that SRL sys-
tems fail to identify the antecedent of a relative
pronoun, an identification that is necessary for en-
tailment checking. Another important difference
with Afazio is that the SRL system produces a
single output. In contrast, Afazio checks entail-
ment for any of the pairs of semantic representa-
tions derived from the first 9 parses of the Stan-
ford parser. The number 9 was determined em-
pirically and proved to yield the best results over-
all although as we shall see in the error mining
section, taking such a high number of parses into
account often leads to incorrect results when the
hypothesis (H) is short.
Nutcracker, on the other hand, produces many
false positives. This is in part due to cases where
the time bound is reached and the word overlap
backoff triggered. Since the overall word overlap
of the SSI suite is high, the backoff often predicts
an entailment where in fact there is none (for in-
stance, the pair ?John gave flowers to Mary/Mary
gave flowers to John has a perfect word overlap
but entailment does not hold). When removing
the backoff results i.e., when assigning all backoff
cases a negative entailment value, overall preci-
sion approximates 60%. In other words, on cases
such as those present in the SSI benchmark where
word overlap is generally high but the correla-
tion between word overlap and entailment value is
neutral, Nutcracker should be used without back-
off.
5 Finding the source of errors
The annotations contained in the automatically
constructed testsuite can help identify the most
likely sources of failures. We use (Sagot and de
La Clergerie, 2006)?s suspicion rate to compute
the probability that a given pair of sets of syntac-
tic tags is responsible for an RTE detection failure.
The tag set pairs with highest suspicion rate in-
dicate which syntactic phenomena often cooccurs
with failure.
More specifically, we store for each testsuite
item (T,H), all tag pairs (tj , hk) such that the syn-
tactic tags tj and hk are associated with the same
predicate Pi but tj occurs in T and hk in H. That is,
we collect the tag pairs formed by taking the tags
that label the occurrence of the same predicate on
both sides of the implication. If a predicate occurs
only in H then for each syntactic tag hk labelling
this predicate, the pair (nil, hk) is created. Con-
versely, if a predicate occurs only in T, the pair
(tj , nil) is added. Furthermore, the tags describ-
ing the subcategorisation type and the form of the
verb are grouped into a single tag so as to reduce
the tagset and limit data sparseness. For instance,
given the pair of sentences in Figure (1), the fol-
lowing tag pairs are produced:
(n0Va1:active:relSubj, nil)
(n0Va1:active:canAdj, nil)
50
system ERROR TN FN TP FP TN/N TP/P Prec
afazio 0 360 147 353 140 0.7200 0.7060 71.3%
nutcracker 155 22 62 312 449 0.0467 0.8342 39.5% (60% w/o B.O.)
srl 0 487 437 63 13 0.9740 0.1260 55.0%
Table 1: Results of the three systems on the SSI-testsuite ( TN = true negatives, FN = false negatives,
TP = true positives, FP = false positives, N = TN + FP, P = TP + FN, Prec = Precision, ERROR: no
parse tree found)
(n0Vn2n1:active:canSubj,n0Vn1Pn2:shortPassive:canSubj)
(n0Vn2n1:active:canSubj,n0Vn1Pn2:shortPassive:canIObj)
(n0Vn2n1:active:canObj,n0Vn1Pn2:shortPassive:canSubj)
(n0Vn2n1:active:canObj,n0Vn1Pn2:shortPassive:canIObj)
(n0Vn2n1:active:canIObj,n0Vn1Pn2:shortPassive:canSubj)
(n0Vn2n1:active:canIObj,n0Vn1Pn2:shortPassive:canIObj)
For each tag pair, we then compute the suspi-
cion rate of that pair using (Sagot and de La Clerg-
erie, 2006)?s fix point algorithm. To also take into
account pairs of sets of tags (rather than just pairs
of single tags), we furthermore preprocess the data
according to (de Kok et al, 2009)?s proposal for
handling n-grams.
Computing the suspicion rate of a tag pair.
The error mining?s suspicion rate algorithm of
(Sagot and de La Clergerie, 2006) is a fix point al-
gorithm used to detect the possible cause of pars-
ing failures. We apply this algorithm to the pair
of annotated sentences resulting from running the
three systems on the automatically created test-
suite as follows. Each such pair consists of a pair
of sentences, a set of tag pairs, an entailment value
(true or false) and a result value namely FP (false
positive), FN (false negative), TP (true positive) or
TN (true negative). To search for the most likely
causes of failure, we consider separately entail-
ments from non entailments. If entailment holds,
the suspicion rate of a sentence pair is 0 for true
positive and 1 for false positives. Conversely, if
entailment does not hold, the suspicion rate of the
sentence pair is 0 for true negatives and 1 for false
negatives.
The aim is to detect the tag pair most likely to
make entailment detection fail6. The algorithm it-
erates between tag pair occurrences and tag pair
forms, redistributing probabilities with each itera-
tion as follows. Initially, all tag pair occurrences
6We make the simplifying hypothesis that for each entail-
ment not recognised, a single tag pair or tag pair n-gram is
the cause of the failure.
in a given sentence have the same suspicion rate
namely, the suspicion rate of the sentence (1 if the
entailment could not be recognised, 0 otherwise)
divided by the number of tag pair occurrences in
that sentence. Next, the suspicion rate of a tag
pair form is defined as the average suspicion rate
of all occurrences of that tag pair. The suspicion
rate of a tag pair occurrence within each particular
sentence is then recalculated as the suspicion rate
of that tag pair form normalised by the suspicion
rates of the other tag pair forms occurring within
the same sentence. The iteration stops when the
process reaches a fixed point where the suspicion
rates have stabilised.
Extending the approach to pairs of tag sets.
To account for entailment recognition due to more
than one tag pair, we follow (de Kok et al, 2009)
and introduce a preprocessing step which first, ex-
pands tag pair unigrams to tag pair n-grams when
there is evidence that it is useful that is, when
an n-gram has a higher suspicion rate than each
of its sub n-grams. For this preprocessing, the
suspicion of a tag pair t is defined as the ratio
of t occurrences in unrecognised entailments and
the total number of t occurrences in the corpus.
To compensate for data sparseness, an additional
expansion factor is used which depends on the
frequency of an n-gram and approaches one for
higher frequency. In this way, long n-grams that
have low frequency are not favoured. The longer
the n-gram is, the more frequent or the more sus-
picious it needs to be in order to be selected by the
preprocessing step.
We apply this extension to the SSI setting. We
first extend the set of available tag pairs with tag
set pairs such that the suspicion rate of these pairs
is higher that the suspicion rate of each of the
smaller tagset pairs that can be constructed from
these sets. We then apply (Sagot and de La Clerg-
51
n0Vs1:act:CanSubj nil 0.85
n0Vn1:act:CanObj nil 0.46
n0V:betaVn nil 0.28
Table 2: The first 3 suspects for false positives
n0V:act n0V:act:RelCSubj 0.73
n0Vs1:act:CanSubj n0Vs1:act:CanSubj 0.69
n0V:act:RelOSubj n0V:betaVn
n0Vs1:act:CanSub n0Vs1:act:CanSubj 0.69
n0V:act:CanSubj n0V:betaVn
Table 3: The first 3 suspects for false negatives
erie, 2006)?s fix point algorithm to compute the
suspicion rate of the resulting tag pairs and tag sets
pairs.
Results and discussion. We now show how er-
ror mining can help shed some light on the most
probable sources of error when using Afazio.
For false positives (non entailment labelled
as entailment by Afazio), the 3 most suspect
tag pairs are given in Table 2. The first pair
(n0Vs1:act:CanSubj,nil) points out to cases such
as Bill sees the woman give the flower to the man
/ The man gives the flower to the woman. where
T contains a verb with a sentential argument not
present in H. In such cases, we found that the sen-
tential argument in T is usually incorrectly anal-
ysed, the analyses produced are fragmented and
entailment goes through. Similarly, the second
suspect (n0Vn1:act:CanObj,nil) points to cases
such as a man sees Lisa dancing / a man dances,
where the transitive verb in T has no counterpart in
H. Here the high number of analyses relied on by
Afazio together with the small size of H leads to
entailment detection: because we consider many
possible analyses for T and H and because H is
very short, one pair of analyses is found to match.
Finally, the third suspect (n0V:betaVn,nil) points
to cases such as Bill insists for the singing man to
dance / Bill dances where the gerund is wrongly
analysed and a relation is incorrectly established
by the parser between Bill and dance (in H).
For false negatives, the first suspect indicates
incorrect analyses for cases where an intransitive
with canonical subject in H is matched by an in-
transitive with covert relative subject (e.g., Bill
sees the woman give the flower to the man / the
man gives the flower to the woman.). The sec-
ond suspect points to cases such as Bill insists for
the man who sings to dance / Bill insists that the
singing man dances. where an embedded verb
with relative overt subject in T (sings) is matched
in H by an embedded gerund. Similarly, the third
suspect points to embedded verbs with canonical
subject matched by gerund verbs as in the man
who Bill insists that dances sings / Bill insists that
the singing man dances.
6 Conclusion
The development of a linguistically principled
treatment of the RTE task requires a clear under-
standing of the strength and weaknesses of RTE
systems w.r.t. to the various types of reasoning in-
volved. The main contribution of this paper is the
specification of an evaluation methodology which
permits a focused evaluation of syntax based rea-
soning on arbitrarily many inputs. As the results
show, there is room for improvment even on that
most basic level. In future work, we plan to extend
the approach to other types of inferences required
for textual entailment recognition. A more so-
phisticated compositional semantics in the gram-
mar used by the sentence generator would allow
for entailments involving more complex semantic
phenomena such as the interplay between implica-
tive verbs, polarity and downward/upward mono-
tonicity discussed in (Nairn et al, 2006). For in-
stance, it would allow for sentence pairs such as
Ed did not forget to force Dave to leave / Dave
left to be assigned the correct entailment value.
References
Bedaride, P. and C. Gardent. 2009. Noun/verb entail-
ment. In 4th Language and Technology Conference,
Poznan, Poland.
Burchardt, A., N. Reiter, S. Thater, and A. Frank.
2007. A semantic approach to textual entailment:
System evaluation and task analysis. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 10?16.
Cohen, K., W. Baumgartner, and L. Hunter. 2008.
Software testing and the naturally occurring data as-
sumption in natural language processing. In Proc.
of ?Software engineering, testing, and quality as-
surance for natural language processing ACL Work-
shop?.
52
Cooper, R., R. Crouch, J. van Eijck, C. Fox, J. van Gen-
abith, J. Jaspars, H. Kamp, M. Pinkal, D. Milward,
M. Poesio, and S. Pulman. 1995. A framework for
computational semantics, FraCaS. Technical report.
MS. Stanford University.
Copestake, A., D. Flickinger, C. Pollard, and I. A.
Sag. 2005. Minimal recursion semantics: an intro-
duction. Research on Language and Computation,
3.4:281?332.
Crouch, R., L. Karttunen, and A. Zaenen. 2006. Cir-
cumscribing is not excluding: A reply to manning.
MS. Palo Alto Research Center.
de Kok, D., J. Ma, and G. van Noord. 2009. A gen-
eralized method for iterative error mining in parsing
results. In Proceedings of the 2009 Workshop on
Grammar Engineering Across Frameworks (GEAF
2009), pages 71?79, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Galliers, J. R. and K. Sparck Jones. 1993. Evaluat-
ing natural language processing systems. Technical
report, Computer Laboratory, University of Cam-
bridge. Technical Report 291.
Gardent, C. and L. Kallmeyer. 2003. Semantic con-
struction in ftag. In Proceedings of the 10th meet-
ing of the European Chapter of the Association for
Computational Linguistics, Budapest, Hungary.
Gardent, C. and E. Kow. 2007. A symbolic approach
to near-deterministic surface realisation using tree
adjoining grammar. In ACL07.
Garoufi, K. 2007. Towards a better understanding of
applied textual entailment: Annotation and evalua-
tion of the rte-2 dataset. Master?s thesis, Saarland
University, Saarbrcken.
Giampiccolo, D., B. Magnini, I. Dagan, and B. Dolan.
2007. The third pascal recognizing textual en-
tailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1?9.
Johansson, R. and P. Nugues. 2008. Dependency-
based syntactic-semantic analysis with propbank
and nombank. In CoNLL ?08: Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, pages 183?187, Morristown, NJ,
USA. Association for Computational Linguistics.
Kamp, H. and U. Reyle. 1993. From Discourse
to Logic. Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Kluwer.
Lehmann, S., S. Oepen, H. Baur, O. Lbdkan, and
D. Arnold. 1996. tsnlp ? test suites for natural
language processing. In In J. Nerbonne (Ed.), Lin-
guistic Databases. CSLI Publications.
Manning, C. D. 2006. Local textual inference: It?s
hard to circumscribe, but you know it when you see
it - and nlp needs it. MS. Stanford University.
Moll, D. and B. Hutchinson. 2003. Intrinsic versus
extrinsic evaluations of parsing systems. In Pro-
ceedings European Association for Computational
Linguistics (EACL), workshop on Evaluation Initia-
tives in Natural Language Processing, Budapest.
Nairn, R., C. Condoravdi, and L. Kartunen. 2006.
Computing relative polarity for textual inference. In
Proceedings of ICoS-5 (Inference in Computational
Semantics), Buxton, UK.
Sagot, B. and E. de La Clergerie. 2006. Error mining
in parsing results. In Proceedings of ACL-CoLing
06, pages 329?336, Sydney, Australie.
Vanderwende, L., D. Coughlin, and B. Dolan. 2005.
What syntax can contribute in entailment task. In
Proceedings of the First PASCAL RTE Workshop,
pages 13?17.
53
Coling 2010: Poster Volume, pages 338?346,
Beijing, August 2010
Comparing the performance of two TAG-based surface realisers using
controlled grammar traversal
Claire Gardent
CNRS/LORIA
claire.gardent@loria.fr
Benjamin Gottesman
acrolinx GmbH
ben.gottesman@acrolinx.com
Laura Perez-Beltrachini
Universite? Henri Poincare?/LORIA
laura.perez@loria.fr
Abstract
We present GENSEM, a tool for generat-
ing input semantic representations for two
sentence generators based on the same re-
versible Tree Adjoining Grammar. We
then show how GENSEM can be used
to produced large and controlled bench-
marks and test the relative performance of
these generators.
1 Introduction
Although computational grammars are mostly
used for parsing, they can also be used to gener-
ate sentences. This has been done, for instance,
to detect overgeneration by the grammar (Gardent
and Kow, 2007). Sentences that are generated but
are ungrammatical indicate flaws in the grammar.
This has also been done to test a parser (Neder-
hof, 1996; Purdom, 1972). Using the sentences
generated from the grammar ensures that the sen-
tences given to the parser are in the language it de-
fines. Hence a parse failure necessarily indicates
a flaw in the parser?s design as opposed to a lack
of coverage by the grammar.
Here we investigate a third option, namely, the
focused benchmarking of sentence realisers based
on reversible grammars, i.e. on grammars that can
be used both to produce sentences from a semantic
representation and semantic representations from
a sentence.
More specifically, we present a linguistically-
controlled grammar traversal algorithm for Tree
Adjoining Grammar (TAG) which, when applied
to a reversible TAG, permits producing arbitrarily
many of the semantic representations associated
by this TAG with the sentences it generates. We
then show that the semantic representations thus
produced can be used to compare the relative per-
formance of two sentence generators based on this
grammar.
Although the present paper concentrates on
Tree Adjoining Grammar realisers, it is worth
pointing out that the semantic representations pro-
duced could potentially be used to evaluate any
surface realiser whose input is a flat semantic for-
mula.
Section 2 discusses related work and motivates
the approach. Section 3 presents GENSEM, the
DCG-based grammar traversal algorithm we de-
veloped. We show, in particular, that the use of
a DCG permits controlling grammar traversal in
such a way as to systematically generate sets of se-
mantic representations covering certain computa-
tionally or linguistically interesting cases. Finally,
Section 4 reports on the benchmarking of two sur-
face realisers with respect to a GENSEM-produced
benchmark.
2 Motivations
Previous work on benchmark construction for
testing the performance of surface realisers falls
into two camps depending on whether or not the
realiser uses a reversible grammar, that is, a gram-
mar that can be used for both parsing and genera-
tion.
To test a surface realiser based on a large
reversible Head-Driven Phrase Structure Gram-
mar (HPSG), Carroll et al (1999) use a small
test set of two hand-constructed and 40 parsing-
derived cases to test the impact of intersective
modifiers1 on generation performance. More re-
cently, Carroll and Oepen (2005) present a perfor-
1As first noted by Brew (1992) and Kay (1996), given a
set of n modifiers all modifying the same structure, all pos-
sible intermediate structures will be constructed, i.e., 2n+1.
338
mance evaluation which uses as a benchmark the
set of semantic representations produced by pars-
ing 130 sentences from the Penn Treebank and
manually selecting the correct semantic represen-
tations. Finally, White (2004) profiles a CCG2-
based sentence realiser using two domain-focused
reversible CCGs to produce two test suites of 549
and 276 ? semantic formula, target sentence ?
pairs, respectively.
For realisers that are not based on a reversible
grammar, there are approaches which derive large
sets of realiser input from the Penn Treebank
(PTB). For example, Langkilde-Geary (2002)
proposes to translate the PTB annotations into a
format accepted by her sentence generator Halo-
gen. The output of this generator can then be au-
tomatically compared with the PTB sentence from
which the corresponding input was derived. Simi-
larly, Callaway (2003) builds an evaluation bench-
mark by transforming PTB trees into a format
suitable for the KPML realiser he uses.
In all of the above cases, the data is derived
from real world sentences, thereby exemplifying
?real world complexity?. If the corpus is large
enough (as in the case of the PTB), the data can
furthermore be expected to cover a broad range of
syntactic phenomena. Moreover, the data, being
derived from real world sentences, is not biased
towards system-specific capabilities. Nonethe-
less, there are also limits to these approaches.
First, they fail to support graduated perfor-
mance testing on constructs such as intersective
modifiers or lexical ambiguity, which are known
to be problematic for surface realisation.
Second, the construction of the benchmark is in
both cases time consuming. In the reversible ap-
proach, for each input sentence, the correct inter-
pretation must be manually selected from among
the semantic formulae produced by the parser. As
a side effect, the constructed benchmarks remain
relatively small (825 in the case of White (2004);
130 in Carroll and Oepen (2005)). In the case
of a benchmark derived by transformation from
a syntactically annotated corpus, the implemen-
tation of the converter is both time-intensive and
corpus-bound. For instance, Callaway (2003) re-
2Combinatory Categorial Grammar
ports that the implementation of such a proces-
sor for the SURGE realiser was the most time-
consuming part of the evaluation with the result-
ing component containing 4000 lines of code and
900 rules.
As we shall show in the following sections,
the GENSEM approach to benchmark construction
aims to address both of these shortcomings. By
using a DCG to implement grammar traversal, it
permits both a full automation of the benchmark
creation and some control over the type and the
distribution of the benchmark items.
3 GenSem
As mentioned above, GENSEM is a grammar
traversal algorithm for TAG. We first present the
specific TAG used for traversal, namely SEMX-
TAG (Alahverdzhieva, 2008) (section 3.1). We
then show how to automatically derive a DCG
that describes the derivation trees of this gram-
mar (section 3.2). Finally, we show how this DCG
encoding permits generating formulae while en-
abling control over the set of semantic representa-
tions to be produced (section 3.3).
3.1 SemXTAG
The SEMXTAG grammar used by GENSEM and
by the two surface realisers is a Feature-Based
Lexicalised Tree Adjoining Grammar augmented
with a unification-based semantics as described by
Gardent and Kallmeyer (2003). We briefly intro-
duce each of these components and describe the
grammar coverage.
FTAG. A Feature-based TAG (Vijay-Shanker
and Joshi, 1988) consists of a set of (auxil-
iary or initial) elementary trees and of two tree-
composition operations: substitution and adjunc-
tion. Initial trees are trees whose leaves are la-
belled with substitution nodes (marked with a
downarrow) or terminal categories. Auxiliary
trees are distinguished by a foot node (marked
with a star) whose category must be the same as
that of the root node. Substitution inserts a tree
onto a substitution node of some other tree while
adjunction inserts an auxiliary tree into a tree. In
an FTAG, the tree nodes are furthermore deco-
rated with two feature structures (called top and
339
bottom) which are unified during derivation as
follows. On substitution, the top of the substitu-
tion node is unified with the top of the root node
of the tree being substituted in. On adjunction,
the top of the root of the auxiliary tree is unified
with the top of the node where adjunction takes
place; and the bottom features of the foot node are
unified with the bottom features of this node. At
the end of a derivation, the top and bottom of all
nodes in the derived tree are unified. Finally, each
sentence derivation in an FTAG is associated with
both a derived tree representing the phrase struc-
ture of the sentence and a derivation tree record-
ing how the corresponding elementary trees were
combined to form the derived tree.
FTAG with semantics. To associate seman-
tic representations with natural language expres-
sions, the FTAG is modified as proposed by Gar-
dent and Kallmeyer (2003).
NPj
John
name(j,john)
Sb
NP?c VPba
Va
runs
run(a,c)
VPx
often VP*x
often(x)
? name(j,john), run(a,j), often(a)
Figure 1: Flat semantics for ?John often runs?
Each elementary tree is associated with a flat
semantic representation. For instance, in Fig-
ure 1,3 the trees for John, runs, and often are asso-
ciated with the semantics name(j,john), run(a,c),
and often(x), respectively. Importantly, the argu-
ments of a semantic functor are represented by
unification variables which occur both in the se-
mantic representation of this functor and on some
nodes of the associated syntactic tree. For in-
stance in Figure 1, the semantic index c occur-
ring in the semantic representation of runs also
occurs on the subject substitution node of the as-
sociated elementary tree. The value of semantic
arguments is determined by the unifications re-
sulting from adjunction and substitution. For in-
stance, the semantic index c in the tree for runs is
3Cx/Cx abbreviate a node with category C and a
top/bottom feature structure including the feature-value pair
{ index : x}.
unified during substitution with the semantic in-
dex labelling the root node of the tree for John.
As a result, the semantics of John often runs is
{name(j,john),run(a,j),often(a)}.
SemXTAG. SEMXTAG is an FTAG for En-
glish augmented with a unification-based compo-
sitional semantics of the type described above.
Its syntactic coverage approaches that of XTAG,
the FTAG developed for English by the XTAG
group (The XTAG Research Group, 2001). Like
this grammar, it contains around 1300 elementary
trees and covers auxiliaries, copula, raising and
small clause constructions, topicalization, relative
clauses, infinitives, gerunds, passives, adjuncts,
ditransitives and datives, ergatives, it-clefts, wh-
clefts, PRO constructions, noun-noun modifica-
tion, extraposition, sentential adjuncts, impera-
tives and resultatives.
3.2 Converting SemXTAG to a DCG
We would like to be able to traverse SEMXTAG in
order to generate semantic representations that are
licensed by it. In the DCG formalism, a grammar
is represented as a set of Prolog definite clauses,
and Prolog?s query mechanism provides built-in
grammar traversal. We take advantage of this by
deriving a DCG from SEMXTAG and then using
Prolog queries to generate semantic representa-
tions that are associated with sentences in the lan-
guage described by it.
Another advantage of the DCG formalism is
that arbitrary Prolog goals can be inserted into a
rule, to constrain when the rule applies or to bind
variables occurring in it. We use this to ground
derivations with lexical items, which are repre-
sented using Prolog assertions. We also use it to
control Prolog?s grammar traversal in such a way
as to generate sets of semantic formulae covering
certain computationally interesting cases (see sec-
tion 3.3).
Our algorithm for converting SEMXTAG to a
DCG is inspired by Schmitz and Le Roux (2008),
who derive from an FTAG a feature-based reg-
ular tree grammar (RTG) whose language is the
derivation trees of the FTAG. Indeed, in our im-
plementation, we derive a DCG from such an
RTG, thereby taking advantage of a SEMXTAG-
340
to-RTG converter previously implemented by Syl-
vain Schmitz.
TAG to RTG. In the conversion to RTG4, each
elementary tree in SEMXTAG is converted to a
rule that models the contribution of the tree to
a TAG derivation. A TAG derivation involves
the selection of an initial tree, which has some
nodes requiring substitution and some permitting
adjunction. Let us think of the potential adjunc-
tion sites as requiring, rather than permitting, ad-
junction, but such that the requirement can be sat-
isfied by ?null? adjunction. Inserting another tree
into this initial tree satisfies one of the substitution
or adjunction requirements, but introduces some
new requirements into the resulting tree, in the
form of its own substitution nodes and adjunction
sites.
Thus, intuitively, the RTG representation of a
SEMXTAG elementary tree is a rule that rewrites
the satisfied requirement as a local tree whose root
is a unique identifier of the tree and whose leaves
are the introduced requirements. A requirement
of a substitution or adjunction of a tree of root
category X is written as XS or XA, respectively.
Here, for example, is the translation to RTG of the
TAG tree (minus semantics) for runs in Figure 1,
using the word anchoring the tree as its identifier
(the superscripts abbreviate feature structures: b/t
refers to the bottom/top feature structure and the
upper case letters to the semantic index value, so
[idx : X] is abbreviated to X):
S[t:T ]S ? runs(S
[t:T,b:B]
A NP
[t:C]
S V P
[t:B,b:A]
A V
[t:A]
A )
The semantics of the SEMXTAG tree are carried
over as-is to the corresponding RTG rule. Fur-
ther, the feature structures labelling the nodes of
SEMXTAG trees are carried over to the RTG rules
so as to correctly interact with substitution and
adjunction (see Schmitz and Le Roux (2008) for
more details on this part of the conversion pro-
cess).
To account for the optionality of adjunction,
there are additional rules allowing any adjunction
4For a more precise description of the FTAG to RTG con-
version see Schmitz and Le Roux (2008).
requirement to be rewritten as the symbol ?, a ter-
minal symbol of the RTG.
The terminal symbols of the RTG are thus the
tree identifiers and the symbol ?, and its non-
terminals are XS and XA for each terminal or
non-terminal X of SEMXTAG.
RTG to DCG. Since the right-hand side of each
RTG rule is a local tree ? that is, a tree of depth no
more than one ? we can flatten each of them into
a list consisting of the root node followed by the
leaves without losing any structural information.
This is the insight underlying the RTG-to-DCG
conversion step. Each RTG rule is converted to
a DCG rule that is essentially identical except for
this flattening of the right-hand side. Here is the
translation to DCG of the RTG rule above5:
rule(s,init,Top,Bot,Sem;S;N;VP;V)
--> [runs],
{lexicon(runs,n0V,[run])},
rule(s,aux,Top,[B],S),
rule(np,init,[C],_,N),
rule(vp,aux,[B],[A],VP),
rule(v,aux,[A],_,V),
{Sem =.. [run,A,C]}.
We represent non-terminals of the DCG us-
ing the rule predicate, whose five (non-hidden)6
arguments, in order, are the category, the sub-
script (init for subscript S, aux for subscript
A), the top and bottom feature values, and the se-
mantics. Feature structures are represented us-
ing Prolog lists with a fixed argument position
for each attribute in the grammar (in this ex-
ample, only the index attribute). The semantics
associated with the left-hand-side symbol (here,
Sem;S;N;VP;V, with the semicolon represent-
ing semantic conjunction) are composed of the se-
mantics associated with this rule and those associ-
ated with each of the right-hand-side symbols.
The language of the resulting DCG is neither
the language of the RTG nor the language of
SEMXTAG, and indeed the language of the DCG
does not interest us but rather its derivation trees.
5In practice, the lexicon is factored out, so there is no rule
specifically for runs, but one for intransitive verbs (n0V) in
general. Each rule hooks into the lexicon, so that a given
invocation of a rule is grounded by a particular lexical item.
6The ?? > notation is syntactic sugar for the usual Pro-
log : ? definite clause notation with two hidden arguments
on each predicate. The hidden arguments jointly represent
the list of terminals dominated by the symbol.
341
These are correlated one-to-one with the trees in
the language described by the RTG, i.e. with the
derivation trees of SEMXTAG, and the latter can
be trivially reconstructed from the DCG deriva-
tions. From a SEMXTAG derivation tree, one can
compose the semantic representation of the asso-
ciated sentence, and in fact this semantic compo-
sition occurs as a side effect of a Prolog query
against the DCG, allowing semantic representa-
tions licensed by SEMXTAG to be returned as
query results.
We define a Prolog predicate for querying
against the DCG, as follows. Its one input argu-
ment, Cat, is the label of the root node of the
derivation tree (typically s), and its one output ar-
gument, Sem, is the semantic representation asso-
ciated with that tree7.
genSem(Cat,Sem) :-
rule(Cat,init,_,_,Sem,_,[]).
3.3 Control parameters
In order to give the users some control over the
sorts of semantic representations that they get
back from a query against the DCG, we augment
the DCG in such a way as to allow control over
the TAG family8 of the root tree in the derivation
tree, over the number and type of adjunctions in
the derivation, and over the depth of substitutions.
To implement control over the family is quite sim-
ple: we need merely to index the DCG rules by
family and modify the GENSEM call accordingly.
For instance, the above DCG rule becomes :
rule(s,init,Top,Bot,n0V,Sem;S;NP;VP;V)
--> [runs],
{lexicon(runs,n0V,[run])},
...
We implement restrictions on adjunctions by
adding an additional argument to the grammar
symbols, namely a vector of non-negative inte-
gers representing the number of non-null adjunc-
tions of each type that are in the derivation sub-
tree dominated by the symbol. By ?type? of ad-
junction, we mean the category of the adjunc-
7The 6th and 7th arguments of the rule call are the hidden
arguments needed by the DCG.
8TAG families group together trees which belong to-
gether, in particular, the trees associated with various real-
isation of a specific subcategorisation type. Thus, here the
notion of TAG family is equivalent to that of subcategorisa-
tion type.
tion site. In DCG terms, a non-null adjunction
of a category X is represented as the expansion of
an x/aux symbol other than as ?. So, for ex-
ample, a DCG symbol associated with the vec-
tor [1,0,0,0,0], where the five dimensions of
the vector correspond to the n, np, v, vp, and s
categories, respectively, dominates a subtree con-
taining exactly one n/aux symbol expanded by
a non-epsilon rule, and no other aux symbol ex-
panded by a non-epsilon rule. We link the vector
associated with the root of the derivation to the
query predicate.
We define a special predicate to handle the
divvying up of a mother node?s vector among the
daughters, taking advantage of the fact that the
DCG formalism permits the insertion of arbitrary
Prolog goals into a rule.
Finally, we add an additional argument to the
DCG rule and to the GENSEM?s call to control
the traversal depth with respect to the number of
substitutions applied. The overall depth of each
derivation is therefore constrained both by the
user defined adjunctions and substitution depth
constraints.
Our query predicate now has four input argu-
ments and one output argument:
genSem(Cat,Fam,[N,NP,V,VP,S],Dth,Sem):-
rule(Cat,init,_,_,Fam,
[N,NP,V,VP,S],Dth,Sem,_,[]).
4 Using GENSEM for benchmarking
We now show how GENSEM can be put to work
for comparing two TAG-based surface realisers,
namely GENI (Gardent and Kow, 2007) and RT-
GEN (Perez-Beltrachini, 2009). These two realis-
ers follow globally similar algorithms but differ in
several respects. We show how GENSEM can be
used to produce benchmarks that are tailored to
test hypotheses about how these differences might
impact performance. We then use this GENSEM-
generated benchmark to compare the performance
of the two realisers.
4.1 GenI and RTGen
Both GENI and RTGEN use the SEMXTAG gram-
mar described in section 3.1. Moreover, both re-
alisers follow an algorithm pipelining three main
phases. First, lexical selection selects from the
342
grammar those elementary trees whose semantics
subsumes part of the input semantics. Second,
the tree combining phase systematically tries to
combine trees using substitution and adjunction.
Third, the retrieval phase extracts the yields of
the complete derived trees, thereby producing the
generated sentence(s).
There are also differences however. We now
spell these out and indicate how they might im-
pact the relative performance of the two surface
realisers.
Derived vs. derivation trees. While GENI con-
structs derived trees, RTGEN uses the RTG en-
coding of SEMXTAG sketched in the previous
section to construct derivation trees. These are
then unraveled into derived trees at the final re-
trieval stage. As noted by Koller and Striegnitz
(2002), these trees are simpler than TAG elemen-
tary trees, which can favourably impact perfor-
mance.
Interleaving of feature constraint solving and
syntactic analysis. GENI integrates in the tree
combining phase a filtering step in which the ini-
tial search space is pruned by eliminating from
it all combinations of TAG elementary trees that
cover the input semantics but cannot possibly lead
to a valid derived tree. This filtering eliminates
all combinations of trees such that either the cat-
egory of a substitution node cannot be cancelled
out by that of the root node of a different tree, or
a root node fails to have a matching substitution
site. Importantly, filtering ignores feature infor-
mation and tree combining takes place after filter-
ing. RTGEN, on the other hand, directly combines
derivation trees decorated with full feature struc-
ture information.
Handling of intersective modifiers. GENI and
RTGEN differ in their strategies for handling
modification.
Adapting Carroll and Oepen?s (2005) proposal
to TAG, GENI adopts a two-step tree-combining
process such that in the first step, only substitu-
tion applies, while in the second, only adjunc-
tion is used. Although the number of intermediate
structures generated is still 2n for n modifiers, this
strategy has the effect of blocking these 2n struc-
tures from multiplying out with other structures in
the chart.
RTGEN, on the other hand, uses a standard Ear-
ley algorithm that includes sharing and packing.
Sharing allows intermediate structures common
to several derivations to be represented once only
while packing groups together partial derivation
trees with identical semantic coverage and similar
combinatorics (same number and type of substitu-
tion and adjunction requirements), keeping only
one representative of such groups in the chart.
In this way, intermediate structures covering the
same set of intersective modifiers in a different
order are only represented once and the negative
impact of intersective modifiers is lessened.
4.2 Two GENSEM benchmarks
We use GENSEM to construct two benchmarks de-
signed to test the impact of the differences be-
tween the two realisers and, more specifically, to
compare the relative performance of both realisers
(i) on cases involving intersective modifiers and
(ii) on cases of varying overall complexity.
The MODIFIERS benchmark focuses on
intersective modifiers and contains semantic
formulae corresponding to sentences in-
volving an increasing number of modifiers.
Recall that GENSEM calls are of the form
gensem(Cat,Family,[N,NP,V,VP,S],Dth,Sem)
where N,NP,V,VP,S indicates the number of
required adjunctions in N, NP, V, VP and S,
respectively, while Family constrains the subcate-
gorisation type of the root tree in the derivations
produced by GENSEM. To produce formulae
involving the lexical selection of intersective
modifiers, we set the following constraints. Cat is
set to s and Family is set to either n0V (intransitive
verbs) or n0Vn1 (transitive verbs). Furthermore,
N and V P vary from 0 to 4 thereby requiring the
adjunction of 0 to 4 N and/or VP modifiers. All
other adjunction counters are set to null. To avoid
producing formulae with identical derivation trees
but distinct lemmas, we use a restricted lexicon
containing one lemma of each syntactic type,
e.g. one transitive verb, one intransitive verb, etc.
Given these settings, GENSEM produces 1 789
formulae whose adjunction requirements vary
from 1 to 6. For instance, the semantic formula
343
{sleep(b,c),man(c),a(c),blue(c),sleep(i,c),carefully(b)} (A
sleeping blue man sleeps carefully) extracted
from the MODIFIERS benchmark contains two
NP adjunctions and one VP adjunction.
The MODIFIERS benchmark is tailored to fo-
cus on cases involving a varying number of in-
tersective modifiers. To support a comparison of
the realisers on this dimension, it displays little or
no variation w.r.t. other dimensions, such as verb
type and non-modifying adjunctions.
To measure the performance of the two realisers
on cases of varying overall complexity, we con-
struct a second benchmark (COMPLEXITY) dis-
playing such variety. The GENSEM parameters
for the construction of this suite are the follow-
ing. The verb type (Family) is one of 28 possible
verb types9. The number and type of required ad-
junctions vary from 0 to 4 for N adjunctions, 0 to
1 for NP , 0 to 4 for V P and 0 to 1 for S. The re-
sulting benchmark contains 890 semantic formu-
lae covering an extensive set of verb types and of
adjunction requirements.
4.3 Results
Using the two GENSEM-generated benchmarks,
we now compare GENI and RTGEN. We plot the
average number of chart items against both the
number of intersective modifiers present in the in-
put (Figure 3) and the size of the Initial Search
Space (ISS), i.e., the number of combinations of
elementary TAG trees covering the input seman-
tics to be explored after the lexical selection step
(Figure 2). In our case, the ISS gives a more
meaningful idea about the complexity than con-
sidering only the number of input literals. In an
FTAG, the number of elementary trees selected
9The 28 verb types are
En1V,n0BEn1,n0lVN1Pn2,n0V,n0Va1,n0VAN1,n0VAN1Pn2,
n0VDAN1,n0VDAN1Pn2,n0VDN1,n0VDN1Pn2,n0Vn1,
n0VN1,n0Vn1Pn2,n0VN1Pn2,n0Vn2n1,n0Vpl,n0Vpln1,
n0Vpn1,n0VPn1,n0Vs1,REn1VA2,REn1VPn2,Rn0Vn1A2,
Rn0Vn1Pn2,s0V,s0Vn1,s0Vton1. The notational convention
for verb types is from XTAG and reads as follows. Sub-
scripts indicate the thematic role of the verb argument. n
indicates a nominal, Pn a PP and s a sentential argument. pl
is a verbal particle. Upper case letters describe the syntactic
functor type: V is a verb, E an ergative, R a resultative and
BE the copula. Sequences of upper case letters such as
VAN in n0VAN1 indicate a multiword functor with syntactic
categories V, A, and N. For instance, n0Vn1 indicates a verb
taking two nominal arguments (e.g., like) and n0VAN1 a
verb locution such as to cry bloody murder.
0-
10
0
10
0-
10
00
10
00
-
50
00
50
00
-
10
00
0
10
00
0-
10
00
00
10
00
00
-
50
00
00
50
00
00
-
10
00
00
0
m
o
re
th
an
10
00
00
0
102
103
104
105
106
p
p p
p p
p p p
Initial Search Space (ISS) size
u
n
pa
ck
ed
ch
ar
ts
iz
e
RTGEN-all
RTGEN-level0
p RTGEN-selective
GENI
Figure 2: Performance of realisation approaches
on the COMPLEXITY benchmark, average un-
packed chart size as a function of the ISS com-
plexity.
by a given literal may vary considerably depend-
ing on the number and the size of the tree families
selected by this literal. For instance, a literal se-
lecting the n0Vn2n1 class will select many more
trees than a literal selecting the n0V family be-
cause there are many more ways of realising the
three arguments of a ditransitive verb than the sin-
gle subject argument of an intransitive one. Chart
items include all elementary trees selected by the
lexical selection step as well as the intermediate
and final structures produced by the tree combin-
ing phase. In RTGEN, we distinguish between
the number of structures built before unpacking
(packed chart) and the number of structures ob-
tained after unpacking (unpacked chart).
Both realisers are implemetned in different pro-
gramming languages, GENI is implemented in
Haskell whereas RTGEN in Prolog. As for the
time results comparison, preliminary experiments
show that GENI is faster is simple input cases. On
the other hand, in the case of more complex cases,
the point of producing much less intermediate re-
sults pays off compared to the overhead of the
chart/agenda operations.
344
Overall efficiency. The plot in Figure 2 shows
the results obtained by running both realisers on
the COMPLEXITY benchmark. Recall (cf. sec-
tion 4.2) that the COMPLEXITY benchmark con-
tains input with varying verb arity and a varying
number of required adjunctions. Hence it provides
cases of increasing complexity in terms of ISS to
be explored. Furthermore, test cases in the bench-
mark trigger sentence realisation involving certain
TAG families, which have a certain number of
trees. Those trees within a family often have iden-
tical combinatorics but different features. Conse-
quently, the COMPLEXITY benchmark also pro-
vides an appropriate testbed for testing the im-
pact of feature structure information on the two
approaches to tree combination.
The graphs show that as complexity increases,
the performance delta between GENI and RT-
GEN increases. We conjecture that as complex-
ity grows, the filtering used by GENI does not
suffice to reduce the search space to a manage-
able size. Conversely, the overhead introduced by
RTGEN?s all-in-one, tree-combining Earley with
packing strategy seems compensated throughout
by the construction of a derivation rather than a
derived tree and pays off increasingly as complex-
ity increases.
Modifiers. Figure 3 plots the results obtained by
running the realisers on the MODIFIERS bench-
mark. Here again, RTGEN outperforms GENI and
the delta between the two realisers grows with the
number of intersective modifiers to be handled. A
closer look at the data shows that the global con-
straints set by GENSEM on the number of required
adjunctions covers an important range of varia-
tion in the data complexity. For instance, there
are cases where 4 modifiers modify the same NP
(or VP) and cases where the modifiers are dis-
tributed over two NPs. Similarly, literals intro-
duced into the formula by a GENSEM adjunction
requirement vary in terms of the number of auxil-
iary trees whose selection they trigger. The steep
curve in GENI?s plot suggests that although the
delayed adjunction mechanism helps in avoiding
the proliferation of intermediate incomplete mod-
ifiers? structures, the lexical ambiguity of modi-
fiers still poses a problem. In contrast, RTGEN?s
0 1 2 3 4 5 6 7
103
104
p
p
p
p
p
p
number of modifiers
u
n
pa
ck
ed
ch
ar
ts
iz
e
RTGEN-all
RTGEN-level0
p RTGEN-selective
GENI
Figure 3: Performance of realisation approaches
on the MODIFIERS benchmark, average unpacked
chart size as a function of the number of modifiers.
packing uniformly applies to word order varia-
tions and to the cases of lexical ambiguity raised
by intersective modifiers because the items have
the same combinatoric potential and the same se-
mantics.
5 Conclusion
Surface realisers are complex systems that need to
handle diverse input and require complex compu-
tation. Testing raises among other things the issue
of coverage ? how can the potential input space
be covered? ? and of test data creation ? should
this data be hand tailored, created randomly, or
derived from real world text?
In this paper, we presented an approach which
permits automating the creation of test input for
surface realisers whose input is a flat semantic for-
mula. The approach differs from other existing
evaluation schemes in two ways. First, it permits
producing arbitrarily many inputs. Second, it sup-
ports the construction of grammar-controlled, lin-
guistically focused benchmarks.
We are currently working on further extending
GENSEM with more powerful (recursive) control
restrictions on the grammar traversal; on com-
bining GENSEM with tools for detecting grammar
overgeneration; and on producing a benchmark
that could be made available to the community for
testing surface realisers whose input is either a de-
pendency tree or a flat semantic formula.
345
References
Alahverdzhieva, K. 2008. XTAG using XMG. Mas-
ter?s thesis, U. Nancy 2. Erasmus Mundus Master
?Language and Communication Technology?.
Brew, C. 1992. Letting the cat out of the bag: Gen-
eration for shake-and-bake MT. In Proceedings of
COLING ?92, Nantes, France.
Callaway, Charles B. 2003. Evaluating coverage for
large symbolic NLG grammars. In 18th IJCAI,
pages 811?817, Aug.
Carroll, John and Stephan Oepen. 2005. High effi-
ciency realization for a wide-coverage unification
grammar. 2nd IJCNLP.
Carroll, John, A. Copestake, D. Flickinger, and
V. Paznan?ski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings of
EWNLG ?99.
Gardent, Claire and Laura Kallmeyer. 2003. Seman-
tic construction in FTAG. In 10th EACL, Budapest,
Hungary.
Gardent, Claire and Eric Kow. 2007. Spotting over-
generation suspects. In 11th European Workshop
on Natural Language Generation (ENLG).
Kay, Martin. 1996. Chart generation. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 200?204, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Koller, Alexander and Kristina Striegnitz. 2002. Gen-
eration as dependency parsing. In Proceedings of
the 40th ACL, Philadelphia.
Langkilde-Geary, Irene. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proceedings of the
INLG.
Nederhof, M.-J. 1996. Efficient generation of random
sentences. Natural Language Engineering, 2(1):1?
13.
Perez-Beltrachini, Laura. 2009. Using regular
tree grammars to reduce the search space in sur-
face realisation. Master?s thesis, Erasmus Mundus
Master Language and Communication Technology,
Nancy/Bolzano.
Purdom, Paul. 1972. A sentence generator for testing
parsers. BIT, 12(3):366?375.
Schmitz, S. and J. Le Roux. 2008. Feature uni-
fication in TAG derivation trees. In Gardent, C.
and A. Sarkar, editors, Proceedings of the 9th In-
ternational Workshop on Tree Adjoining Grammars
and Related Formalisms (TAG+?08), pages 141?
148, Tu?bingen, Germany.
The XTAG Research Group. 2001. A lexicalised tree
adjoining grammar for english. Technical report,
Institute for Research in Cognitive Science, Univer-
sity of Pennsylvannia.
Vijay-Shanker, K. and AK Joshi. 1988. Feature Struc-
tures Based Tree Adjoining Grammars. Proceed-
ings of the 12th conference on Computational lin-
guistics, 55:v2.
White, Michael. 2004. Reining in CCG chart realiza-
tion. In INLG, pages 182?191.
346
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 808?813,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Using Paraphrases and Lexical Semantics to Improve the Accuracy and the
Robustness of Supervised Models in Situated Dialogue Systems
Claire Gardent
CNRS/LORIA, Nancy
claire.gardent@loria.fr
Lina M. Rojas Barahona
Universit? de Lorraine/LORIA, Nancy
lina.rojas@loria.fr
Abstract
This paper explores to what extent lemmati-
sation, lexical resources, distributional seman-
tics and paraphrases can increase the accuracy
of supervised models for dialogue manage-
ment. The results suggest that each of these
factors can help improve performance but that
the impact will vary depending on their com-
bination and on the evaluation mode.
1 Introduction
One strand of work in dialog research targets the
rapid prototyping of virtual humans capable of con-
ducting a conversation with humans in the context
of a virtual world. In particular, question answering
(QA) characters can respond to a restricted set of
topics after training on a set of dialogs whose utter-
ances are annotated with dialogue acts (Leuski and
Traum, 2008).
As argued in (Sagae et al, 2009), the size of the
training corpus is a major factor in allowing QA
characters that are both robust and accurate. In ad-
dition, the training corpus should arguably be of
good quality in that (i) it should contain the various
ways of expressing the same content (paraphrases)
and (ii) the data should not be skewed. In sum, the
ideal training data should be large (more data is
better data) ; balanced (similar amount of data for
each class targeted by the classifier) and varied (it
should encompass the largest possible number of
paraphrases and synonyms for the utterances of each
class).
In this paper, we explore different ways of im-
proving and complementing the training data of a
supervised QA character. We expand the size and
the quality (less skewed data) of the training corpus
using paraphrase generation techniques. We com-
pare the performance obtained on lemmatised vs.
non lemmatised data. And we investigate how vari-
ous resources (synonym dictionaries, WordNet, dis-
tributional neighbours) can be used to handle unseen
words at run time.
2 Related work
Previous work on improving robustness of super-
vised dialog systems includes detecting and han-
dling out of domain utterances for generating feed-
back (Lane et al, 2004) ; using domain-restricted
lexical semantics (Hardy et al, 2004) ; and work on
manual data expansion (DeVault et al, 2011). Our
work follows up on this research but provides a sys-
tematic investigation of how data expansion, lemma-
tisation and synonym handling impacts the perfor-
mance of a supervised QA engine.
3 Experimental Setup
We run our experiments on a dialog engine de-
veloped for a serious game called Mission Plastech-
nologie. In this game, the player must interact with
different virtual humans through a sequence of 12
subdialogs, each of them occurring in a different part
of the virtual world.
Training Data. The training corpus consists of
around 1250 Human-Human dialogues which were
manually annotated with dialog moves. As the fol-
lowing dialog excerpt illustrates, the dialogs are con-
ducted in French and each dialog turn is manu-
ally annotated using a set of 28 dialog acts. For
808
a more detailed presentation of the training corpus
and of the annotation scheme, the reader is referred
to (Rojas-Barahona et al, 2012a)
dialog : 01_dialogDirecteur-Tue Jun 14 11 :04 :23 2011
>M.Jasper : Bonjour, je suis M.Jasper le directeur. || greet
(Hello, I am the director, Mr. Jasper.)
>M.Jasper : Qu?est-ce que je peux faire pour vous ? || ask(task(X))
(What can I do for you ?)
>Lucas : je dois sauver mon oncle || first_step
(I must rescue my uncle)
>M.Jasper : Pour faire votre manette, il vous faut
des plans. Allez voir dans le bureau d??tudes,
ils devraient y ?tre. || inform(do(first_step))
(To build the joystick you will need the plans.
You will find them in the Designing Office.)
>M.Jasper : Bonne Chance ! || quit
(Good Luck !)
Dialog Systems For our experiments, we use a hy-
brid dialog system similar to that described in (Ro-
jas Barahona et al, 2012b; Rojas Barahona and
Gardent, 2012). This system combines a classifier
for interpreting the players utterances with an infor-
mation state dialog manager which selects an appro-
priate system response based on the dialog move as-
signed by the classifier to the user turn. The clas-
sifier is a logistic regression classifier 1 which was
trained for each subdialog in the game. The features
used for training are the set of content words which
are associated with a given dialog move and which
remain after TF*IDF 2 filtering. Note that in this ex-
periment, we do not use contextual features such as
the dialog acts labeling the previous turns. There are
two reasons for this. First, we want to focus on the
impact of synonym handling, paraphrasing and lem-
matisation on dialog management. Removing con-
textual features allows us to focus on how content
features (content words) can be improved by these
mechanisms. Second, when evaluating on the H-C
corpus (see below), contextual features are often in-
correct (because the system might incorrectly inter-
pret and thus label a user turn). Excluding contextual
features from training allows for a fair comparison
between the H-H and the H-C evaluation.
Test Data and Evaluation Metrics We use accu-
1. We used MALLET (McCallum, 2002) for the LR classi-
fier with L1 Regularisation.
2. TF*IDF = Term Frequency*Inverse Document Fre-
quency
racy (the number of correct classifications divided
by the number of instances in the testset) to mea-
sure performance and we carry out two types of
evaluation. On the one hand, we use 10-fold cross-
validation on the EmoSpeech corpus (H-H data). On
the other hand, we report accuracy on a corpus of
550 Human-Computer (H-C) dialogues obtained by
having 22 subjects play the game against the QA
character trained on the H-H corpus. As we shall see
below, performance decreases in this second evalua-
tion suggesting that subjects produce different turns
when playing with a computer than with a human
thereby inducing a weak out-of-domain effect and
negatively impacting classification. Evaluation on
the H-H corpus therefore gives a measure of how
well the techniques explored help improving the di-
alog engine when used in a real life setting.
Correspondingly, we use two different tests for
measuring statistical significance. In the H-H eval-
uation, significance is computed using the Wilcoxon
signed rank test because data are dependent and are
not assumed to be normally distributed. When build-
ing the testset we took care of not including para-
phrases of utterances in the training partition (for
each paraphrase generated automatically we keep
track of the original utterance), however utterances
in both datasets might be generated by the same sub-
ject, since a subject completed 12 distinct dialogues
during the game. Conversely, in the H-C evaluation,
training (H-H data) and test (H-C data) sets were
collected under different conditions with different
subjects therefore significance was computed using
the McNemar sign-test (Dietterich, 1998).
4 Paraphrases, Synonyms and
Lemmatisation
We explore three main ways of modifying the
content features used for classification : lemmatising
the training and the test data ; augmenting the train-
ing data with automatically acquired paraphrases ;
and substituting unknown words with synonyms at
run time.
Lemmatisation We use the French version of
Treetagger 3 to lemmatise both the training and the
test data. Lemmas without any filtering were used
3. http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/
809
to train classifiers. We then compare performance
with and without lemmatisation. As we shall see,
the lemma and the POS tag provided by TreeTag-
ger are also used to lookup synonym dictionaries and
EuroWordNet when using synonym handling at run
time.
Paraphrases : (DeVault et al, 2011) showed that
enriching the training corpus with manually added
paraphrases increases accuracy. Here we exploit au-
tomatically acquired paraphrases and use these not
only to increase the size of the training corpus but
also to better balance it 4. We proceed as follows.
First, we generated paraphrases using a pivot ma-
chine translation approach where each user utter-
ance in the training corpus (around 3610 utterances)
was translated into some target language and back
into French. Using six different languages (English,
Spanish, Italian, German, Chinese and Arabian),
we generated around 38000 paraphrases. We used
Google Translate API for translating.
Category Train Instances Balanced Instances
greet 24 86
help 20 82
yes 92 123
no 55 117
ack 73 135
other 27 89
quit 38 100
find_plans 115 146
job 26 88
staff 15 77
studies 20 82
security_policies 24 86
? 44.08 100.92
? ?32.68 ?23.32
TABLE 1: Skewed and Balanced Data on a sample sub-
dialog. The category with lowest number of paraphrases
is greet, with 62 paraphrases, hence lp = 62. All cat-
egories were increased by 62 except find_plans and
yes that were increased by half : 31.
Second, we eliminate from these paraphrases,
words that are likely to be incorrect lexical transla-
tions by removing words with low normalized term
4. The Emospeech data is highly skewed with some classes
being populated with many utterances and others with few.
Algorithm extendingDataWithParaphrases(trainingset ts)
1. Let c be the set of categories in ts.
2. ? be the mean of train instances per category
3. ? be the standard deviation of train instances per category
4. Let Npc be the number of paraphrases per category
5. Let lp ? min Npcj
6. Repeat
7. set i ? 0
8. Ninstci be the number of instances per category ci
9. di ? Ninstci ? ?
10. if di < ? then
11. Ninstci ? lp
12. else
13. Ninstci ?
lp
2
14. end if
15. set i?i+1
16. if i>?c? then
17. terminate
18. end
FIGURE 1: Algorithm for augmenting the training data
with paraphrases.
frequency (< 0.001) across translations i.e., lexical
translations given by few translations and/or transla-
tion systems. We then preprocessed the paraphrases
in the same way the utterances of the initial train-
ing corpus were preprocessed i.e., utterances were
unaccented, converted to lower-case and stop words
were removed, the remaining words were filtered
with TF*IDF. After preprocessing, duplicates were
removed.
Third, we added the paraphrases to the training
data seeking to improve the balance between dialog
moves per dialog, as shown in Figure 1. To this end,
we look for the category c with the lowest number
of paraphrases lp (line 5). We then compute the de-
viation di for each dialog move ci from the mean
? in the original training set (line 9). If the devia-
tion di is lower than the standard deviation then we
add lp number of paraphrases instances (line 11).
Conversely, if di is higher than the standard devia-
tion, we reduce the number of instances to be added
by half lp2 (line 13). Table 1 shows the original and
the extended training data for the third sub-dialog
in the Emospeech game. In this dialogue the player
is supposed to ask information about the joystick
plans (find_plans, which is the mandatory goal).
The categories cover mandatory and optional goals
and general dialogue acts, such as greetings, asking
for help, confirm and disconfirm, acknowledgment
and out of topic questions (i.e. other).
Substituting Synonyms for Unknown Words A
word is unknown, if it is a well-formed French
810
word 5 and if it does not appear in the training cor-
pus. Conversely, a word is known if it is not un-
known.
When an unknown word w is detected in a player
utterance at runtime, we search for a word w? which
occurs in the training data and is either a synonym of
w or a distributional neighbour. After disambigua-
tion, we substitute the unknown word for the syn-
onym.
To identify synonyms, we make use of two lexical
resources namely, the French version of EuroWord-
Net (EWN) (Vossen, 1998), which includes 92833
synonyms, hyperonyms and hyponyms pairs, and a
synonym lexicon for French (DIC) 6 which contains
38505 lemmas and 254149 synonym pairs. While
words are categorised into Noun, Verbs and Adjec-
tives in EWN, DIC contains no POS tag information.
To identify distributional neighbours, we con-
structed semantic word spaces for each subdialog
in the EmoSpeech corpus 7 using random indexing
(RI) 8 on the training corpus expanded with para-
phrases. Using the cosine measure as similarity met-
rics, we then retrieve for any unknown word w, the
word w? which is most similar to w and which ap-
pear in the training corpus.
For lexical disambiguation, two methods are com-
pared. We use the POS tag provided by TreeTagger.
In this case, disambiguation is syntactic only. Or we
pick the synonym with highest probability based on
a trigram language model trained on the H-H cor-
pus 9.
5 Results and Discussion
Table 2 summarises the results obtained in four
main configurations : (i) with and without para-
phrases ; (ii) with and without synonym handling ;
(iii) with and without lemmatisation ; and (iv) when
5. A word is determined to be a well-formed French word if
it occurs in the LEFFF dictionary, a large-scale morphological
and syntactic lexicon for French (Sagot, 2010)
6. DICOSYN (http ://elsap1.unicaen.fr/dicosyn.html).
7. We also used distributional semantics from the Gigaword
corpus but the results were poor probably because of the very
different text genre and domains between the the Gigaword and
the MP game.
8. Topics are Dialog acts while documents are utterances ;
we used the S-Space Package http://code.google.com/p/
airhead-research/wiki/RandomIndexing
9. We used SRILM (http://www.speech.sri.com/
projects/srilm)
combining lemmatisation with synonym handling.
We also compare the results obtained when evalu-
ating using 10-fold cross validation on the training
data (H-H dialogs) vs. evaluating the performance
of the system on H-C interactions.
Overall Impact The largest performance gain is
obtained by a combination of the three techniques
explored in this paper namely, data expansion, syn-
onym handling and lemmatisation (+8.9 points for
the cross-validation experiment and +2.3 for the H-
C evaluation).
Impact of Lexical Substitution at Run Time Be-
cause of space restrictions, we do not report here
the results obtained using lexical resources without
lemmatisation. However, we found that lexical re-
sources are only useful when combined with lemma-
tisation. This is unsurprising since synonym dictio-
naries and EuroWordNet only contain lemmas. In-
deed when distributional neighbours are used, lem-
matisation has little impact (e.g., 65.11% using dis-
tributional neighbours without lemmatisation on the
H-H corpus without paraphrases vs. 66.41% when
using lemmatisation).
Another important issue when searching for a
word synonym concerns lexical disambiguation : the
synonym used to replace an unknown word should
capture the meaning of that word in its given con-
text. We tried using a language model trained on the
training corpus to choose between synonym candi-
dates (i.e., selecting the synonym yielding the high-
est sentence probability when substituting that syn-
onym for the unknown word) but did not obtain a
significant improvement. In contrast, it is noticeable
that synonym handling has a higher impact when us-
ing EuroWordNet as a lexical resource. Since Eu-
roWordNet contain categorial information while the
synonym dictionaries we used do not, this suggests
that the categorial disambiguation provided by Tree-
Tagger helps identifying an appropriate synonym in
EuroWordNet.
Finally, it is clear that the lexical resources used
for this experiment are limited in coverage and qual-
ity. We observed in particular that some words which
are very frequent in the training data (and thus which
could be used to replace unknown words) do not oc-
cur in the synonym dictionaries. For instance when
using paraphrases and dictionaries (fourth row and
811
H Lemmatisation
H-H Orig. Lemmas +EWN +DIC +RI
Orig. 65.70%? 5.62 66.04%? 6.49 68.17%? 6.98 67.92%? 4.51 66.83%? 5.92
Parap. 70.89%? 6.45 74.31%? 4.78* 74.60%? 5.99* 73.07%? 7.71* 72.63%? 5.82*
H-C Orig. Lemmas +EWN +DIC +RI
Orig. 59.71%? 16.42 59.88%? 7.19 61.14%? 16.65 61.41%? 16.59 60.75%? 17.39
Parap. 59.82%? 15.53 59.48%? 14.02 61.70%? 14.09* 62.01%? 14.37* 61.16%? 14.41*
TABLE 2: Accuracy on the H-H and on the H-C corpus. The star denotes statistical significance with the Wilcoxon test
(p < 0.005) used for the HH corpus and the McNemar test (p < 0.005) for the HC corpus.
fourth column in Table 2) 50% of the unknown
words were solved, 17% were illformed and 33% re-
mained unsolved. To compensate this deficiency, we
tried combining the three lexical resources in vari-
ous ways (taking the union or combining them in a
pipeline using the first resource that would yield a
synonym). However the results did not improve and
even in some cases worsened due probably to the in-
sufficient lexical disambiguation. Interestingly, the
results show that paraphrases always improves syn-
onym handling presumably because it increases the
size of the known vocabulary thereby increasing the
possibility of finding a known synonym.
In sum, synonym handling helps most when (i)
words are lemmatised and (ii) unknown words can
be at least partially (i.e., using POS tag information)
disambiguated. Moreover since data expansion in-
creases the set of known words available as potential
synonyms for unknown words, combining synonym
handling with data expansion further improves ac-
curacy.
Impact of Lemmatisation When evaluating using
cross validation on the training corpus, lemmatisa-
tion increases accuracy by up to 3.42 points indi-
cating that unseen word forms negatively impact ac-
curacy. Noticeably however, lemmatisation has no
significant impact when evaluating on the H-C cor-
pus. This in turn suggests that the lower accuracy
obtained on the H-C corpus results not from unseen
word forms but from unseen lemmas.
Impact of Paraphrases On the H-H corpus, data
expansion has no significant impact when used
alone. However it yields an increase of up to 8.27
points and in fact, has a statistically significant im-
pact, for all configurations involving lemmatisation.
Thus, data expansion is best used in combination
with lemmatisation and their combination permits
creating better, more balanced and more general
training data. On the H-C corpus however, the im-
pact is negative or insignificant suggesting that the
decrease in performance on the H-C corpus is due to
content words that are new with respect to the train-
ing data i.e., content words for which neither a syn-
onym nor a lemma can be found in the expanded
training data.
Conclusion
While classifiers are routinely trained on dialog
data to model the dialog management process, the
impact of such basic factors as lemmatisation, au-
tomatic data expansion and synonym handling has
remained largely unexplored. The empirical eval-
uation described here suggests that each of these
factors can help improve performance but that the
impact will vary depending on their combination
and on the evaluation mode. Combining all three
techniques yields the best results. We conjecture
that there are two main reasons for this. First, syn-
onym handling is best used in combination with
POS tagging and lemmatisation because these sup-
ports partial lexical semantic disambiguation. Sec-
ond, data expansion permits expanding the set of
known words thereby increasing the possibility of
finding a known synonym to replace an unknown
word with.
Acknowledgments
This work was partially supported by the EU
funded Eurostar EmoSpeech project. We thank
Google for giving us access to the University Re-
search Program of Google Translate.
812
References
David DeVault, Anton Leuski, and Kenji Sagae. 2011.
Toward learning and evaluation of dialogue policies
with text examples. In 12th SIGdial Workshop on Dis-
course and Dialogue, Portland, OR, June.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Computation, 10 :1895?1923.
Hilda Hardy, Tomek Strzalkowski, Min Wu, Cristian
Ursu, Nick Webb, Alan W. Biermann, R. Bryce In-
ouye, and Ashley McKenzie. 2004. Data-driven
strategies for an automated dialogue system. In ACL,
pages 71?78.
Ian Richard Lane, Tatsuya Kawahara, and Shinichi Ueno.
2004. Example-based training of dialogue planning
incorporating user and situation models. In INTER-
SPEECH.
Anton Leuski and David Traum. 2008. A statistical ap-
proach for text processing in virtual humans. In Pro-
ceedings of the 26th Army Science Conference.
Andrew Kachites McCallum. 2002. Mallet : A ma-
chine learning for language toolkit. http ://mal-
let.cs.umass.edu.
Lina Maria Rojas Barahona and Claire Gardent. 2012.
What should I do now ? Supporting conversations in
a serious game. In SeineDial 2012 - 16th Workshop
on the Semantics and Pragmatics of Dialogue, Paris,
France. Jonathan Ginzburg (chair), Anne Abeill?, Mar-
got Colinet, Gregoire Winterstein.
Lina M. Rojas-Barahona, Alejandra Lorenzo, and Claire
Gardent. 2012a. Building and exploiting a corpus
of dialog interactions between french speaking virtual
and human agents. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Evalu-
ation.
Lina M. Rojas Barahona, Alejandra Lorenzo, and Claire
Gardent. 2012b. An end-to-end evaluation of two
situated dialog systems. In Proceedings of the 13th
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 10?19, Seoul, South Ko-
rea, July. Association for Computational Linguistics.
K. Sagae, G. Christian, D. DeVault, , and D.R. Traum.
2009. Towards natural language understanding of par-
tial speech recognition results in dialogue systems. In
Proceedings of Human Language Technologies : The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), Companion Volume : Short Papers, pages
53?56.
Beno?t Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for French. In 7th international conference on Lan-
guage Resources and Evaluation (LREC 2010), Val-
letta, Malta.
Piek Vossen, editor. 1998. EuroWordNet : a multilin-
gual database with lexical semantic networks. Kluwer
Academic Publishers, Norwell, MA, USA.
813
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 183?191,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Incremental Query Generation
Laura Perez-Beltrachini
Faculty of Computer Science
Free University of Bozen-Bolzano
Bozen-Bolzano, Italy
laura.perez@loria.fr
Claire Gardent
CNRS/LORIA
Nancy, France
claire.gardent@loria.fr
Enrico Franconi
Faculty of Computer Science
Free University of Bozen-Bolzano
Bozen-Bolzano, Italy
franconi@inf.unibz.it
Abstract
We present a natural language genera-
tion system which supports the incremen-
tal specification of ontology-based queries
in natural language. Our contribution is
two fold. First, we introduce a chart
based surface realisation algorithm which
supports the kind of incremental process-
ing required by ontology-based querying.
Crucially, this algorithm avoids confusing
the end user by preserving a consistent
ordering of the query elements through-
out the incremental query formulation pro-
cess. Second, we show that grammar
based surface realisation better supports
the generation of fluent, natural sounding
queries than previous template-based ap-
proaches.
1 Introduction
Previous research has shown that formal ontolo-
gies could be used as a means not only to provide
a uniform and flexible approach to integrating and
describing heterogeneous data sources, but also to
support the final user in querying them, thus im-
proving the usability of the integrated system. To
support the wide access to these data sources, it is
crucial to develop efficient and user-friendly ways
to query them (Wache et al., 2001).
In this paper, we present a Natural Language
(NL) interface of an ontology-based query tool,
called Quelo1, which allows the end user to for-
mulate a query without any knowledge either of
the formal languages used to specify ontologies, or
of the content of the ontology being used. Follow-
ing the conceptual authoring approach described
in (Tennant et al., 1983; Hallett et al., 2007), this
interface masks the composition of a formal query
1krdbapp.inf.unibz.it:8080/quelo
as the composition of an English text describ-
ing the equivalent information needs using natu-
ral language generation techniques. The natural
language generation system that we propose for
Quelo?s NL interface departs from similar work
(Hallett et al., 2007; Franconi et al., 2010a; Fran-
coni et al., 2011b; Franconi et al., 2010b; Franconi
et al., 2011a) in that it makes use of standard gram-
mar based surface realisation techniques. Our con-
tribution is two fold. First, we introduce a chart
based surface realisation algorithm which supports
the kind of incremental processing required by on-
tology driven query formulation. Crucially, this
algorithm avoids confusing the end user by pre-
serving a consistent ordering of the query ele-
ments throughout the incremental query formu-
lation process. Second, we show that grammar
based surface realisation better supports the gener-
ation of fluent, natural sounding queries than pre-
vious template-based approaches.
The paper is structured as follows. Section 2
discusses related work and situates our approach.
Section 3 describes the task being addressed
namely, ontology driven query formulation. It in-
troduces the input being handled, the constraints
under which generation operates and the opera-
tions the user may perform to build her query.
In Section 4, we present the generation algo-
rithm used to support the verbalisation of possi-
ble queries. Section 5 reports on an evaluation of
the system with respect to fluency, clarity, cover-
age and incrementality. Section 6 concludes with
pointers for further research.
2 Related Work
Our approach is related to two main strands of
work: incremental generation and conceptual au-
thoring.
Incremental Generation (Oh and Rudnicky,
2000) used an n-gram language model to stochas-
183
tically generate system turns. The language model
is trained on a dialog corpus manually annotated
with word and utterance classes. The generation
engine uses the appropriate language model for
the utterance class and generates word sequences
randomly according to the language model distri-
bution. The generated word sequences are then
ranked using a scoring mechanism and only the
best-scored utterance is kept. The system is incre-
mental is that each word class to be verbalised can
yield a new set of utterance candidates. However
it supports only addition not revisions. Moreover
it requires domain specific training data and man-
ual annotation while the approach we propose is
unsupervised and generic to any ontology.
(Dethlefs et al., 2013) use Conditional Random
Fields to find the best surface realisation from a
semantic tree. They show that the resulting sys-
tem is able to modify generation results on the fly
when new or updated input is provided by the dia-
log manager. While their approach is fast to ex-
ecute, it is limited to a restricted set of domain
specific attributes; requires a training corpus of
example sentences to define the space of possi-
ble surface realisations; and is based on a large
set (800 rules) of domain specific rules extracted
semi-automatically from the training corpus. In
contrast, we use a general, small size grammar
(around 50 rules) and a lexicon which is automat-
ically derived from the input ontologies. The re-
sulting system requires no training and thus can
be applied to any ontology with any given signa-
ture of concepts and relations. Another difference
between the two approaches concerns revisions:
while our approach supports revisions anywhere
in the input, the CRF approach proposed by (Deth-
lefs et al., 2013) only supports revisions occurring
at the end of the generated string.
There is also much work (Schlangen and
Skantze, 2009; Schlangen et al., 2009) in the do-
main of spoken dialog systems geared at mod-
elling the incremental nature of dialog and in par-
ticular, at developing dialog systems where pro-
cessing starts before the input is complete. In these
approaches, the focus is on developing efficient ar-
chitectures which support the timely interleaving
of parsing and generation. Instead, our aim is to
develop a principled approach to the incremental
generation of a user query which supports revision
and additions at arbitrary points of the query being
built; generates natural sounding text; and maxi-
mally preserves the linear order of the query.
Conceptual authoring Our proposal is closely
related to the conceptual authoring approach de-
scribed in (Hallett et al., 2007). In this approach,
a text generated from a knowledge base, describes
in natural language the knowledge encoded so far,
and the options for extending it. Starting with an
initial very general query (e.g., all things), the user
can formulate a query by choosing between these
options. Similarly, (Franconi et al., 2010a; Fran-
coni et al., 2011b; Franconi et al., 2010b; Fran-
coni et al., 2011a) describes a conceptual author-
ing approach to querying semantic data where in
addition , logical inference is used to semantically
constrain the possible completions/revisions dis-
played to the user.
Our approach departs from this work in that it
makes use of standard grammars and algorithms.
While previous work was based on procedures and
templates, we rely on a Feature-Based Tree Ad-
joining Grammar to capture the link between text
and semantics required by conceptual authoring;
and we adapt a chart based algorithm to support
the addition, the revision and the substitution of
input material. To avoid confusing the user, we
additionally introduce a scoring function which
helps preserve the linear order of the NL query.
The generation system we present is in fact inte-
grated in the Quelo interface developed by (Fran-
coni et al., 2011a) and compared with their previ-
ous template-based approach.
3 Incremental Generation of Candidate
Query Extensions
The generation task we address is the following.
Given a knowledge base K , some initial formal
query q and a focus point p in that query, the rea-
soning services supported by Quelo?s query logic
framework (see (Guagliardo, 2009)) will compute
a set of new queries rev(q) formed by adding,
deleting and revising the current query q at point
p. The task of the generator is then to produce
a natural language sentence for each new formal
query q? ? rev(q) which results from this revision
process. In other words, each time the user refines
a query q to produce a new query q?, the system
computes all revisions rev(q) of q? that are com-
patible with the underlying knowledge base using
a reasoner. Each of these possible revisions is then
input to the generator and the resulting revised NL
queries are displayed to the user. In what follows,
184
we assume that formal queries are represented us-
ing Description Logics (Baader, 2003).
The following examples show a possible se-
quence of NL queries, their corresponding DL rep-
resentation and the operations provided by Quelo
that can be performed on a query (bold face is used
to indicate the point in the query at which the next
revision takes place). For instance, the query in
(1c) results from adding the concept Y oung to the
query underlying (1b) at the point highlighted by
man.
(1) a. I am looking for something (initial query)
?
b. I am looking for a man (substitute con-
cept)
Man
c. I am looking for a young man (add com-
patible concept)
Man ? Y oung
d. I am looking for a young man who is
married to a person (add relation)
Man?Y oung??isMarried.(Person)
e. I am looking for a young married man
(substitute selection)
MarriedMan ? Y oung
f. I am looking for a married man (delete
concept)
MarriedMan
4 Generating Queries
Generation of KB queries differs from standard
natural language generation algorithms in two
main ways. First it should support the revi-
sions, deletions and additions required by incre-
mental processing. Second, to avoid confusing
the user, the revisions (modifications, extensions,
deletions) performed by the user should have a
minimal effect on the linear order of the NL query.
That is the generator is not free to produce any NL
variant verbalising the query but should produce
a verbalisation that is linearly as close as possi-
ble, modulo the revision applied by the user, to the
query before revisions. Thus for instance, given
the DL query (2) and assuming a linearisation of
that formula that matches the linear order it is pre-
sented in (see Section 4.2.1 below for a definition
of the linearisation of DL formulae), sentence (2b)
will be preferred over (2c).
(2) a. Car ? ?runOn.(Diesel) ?
?equippedWith.(AirCond)
b. A car which runs on Diesel and is
equipped with air conditioning
c. A car which is equipped with air condi-
tioning and runs on Diesel
In what follows, we describe the generation al-
gorithm used to verbalise possible extensions of
user queries as proposed by the Quelo tool. We
start by introducing and motivating the underlying
formal language supported by Quelo and the input
to the generator. We then describe the overall ar-
chitecture of our generator. Finally, we present the
incremental surface realisation algorithm support-
ing the verbalisation of the possible query exten-
sions.
4.1 The Input Language
Following (Franconi et al., 2010a; Franconi et al.,
2011b; Franconi et al., 2010b; Franconi et al.,
2011a) we assume a formal language for queries
that targets the querying of various knowledge and
data bases independent of their specification lan-
guage. To this end, it uses a minimal query lan-
guage L that is shared by most knowledge repre-
sentation languages and is supported by Descrip-
tion Logic (DL) reasoners namely, the language of
tree shaped conjunctive DL queries. Let R be a
set of relations and C be a set of concepts, then the
language of tree-shaped conjunctive DL queries is
defined as follows: S ::= C | ?R.(S) | S ? S
where R ? R, C ? C, ? denotes conjunction and
? is the existential quantifier.
A tree shaped conjunctive DL query can be rep-
resented as a tree where nodes are associated with
a set of concept names (node labels) and edges are
labelled with a relation name (edge labels). Figure
1 shows some example query trees.
4.2 NLG architecture
Our generator takes as input two L formula: the
formula representing the current query q and the
formula representing a possible revision r (addi-
tion/deletion/modification) of q. Given this in-
put, the system architecture follows a traditional
pipeline sequencing a document planner which (i)
linearises the input query and (ii) partition the in-
put into sentence size chunks; a surface realiser
mapping each sentence size L formula into a sen-
tence; and a referring expression generator verbal-
ising NPs.
4.2.1 Document Planning
The document planning module linearises the in-
put query and segments the resulting linearised
185
x{Man}
(a)
x
w
{Man}
{House}
livesIn
(b)
x
w
z
{Man}
{House}
livesIn
{RichPerson}
ownedBy
(c)
x
w
z
{Man}
{House,
Beautiful}
livesIn
{RichPerson}
ownedBy
(d)
?
?
?
?
x
y
w
z
{Man}
{Person}
{House,
Beautiful}
{RichPerson}
marriedTo livesIn
ownedBy
(e)
Figure 1: Example of query tree and incremental query construction.
query into sentence size chunks.
Query Linearisation Among the different
strategies investigated in (Dongilli, 2008) to
find a good order for the content contained in a
query tree the depth-first planning, i.e. depth-first
traversal of the query tree, was found to be the
most appropriate one. Partly because it is obtained
straightforward from the query tree but mostly
due to the fact that it minimizes the changes in the
text plan that are required by incremental query
modifications. Thus, (Franconi et al., 2010a)
defines a query linearisation as a strict total order2
on the query tree that satisfies the following
conditions:
? all labels associated with the edge?s leaving
node precede the edge label
? the edge label is followed by at least one label
associated with the edge?s arriving node
? between any two labels of a node there can
only be (distinct) labels of the same node
The specific linearisation adopted in Quelo is
defined by the depth-first traversal strategy of the
query tree and a total order on the children which
is based on the query operations. That is, the la-
bels of a node are ordered according to the se-
quence applications of the add compatible
concept operation. The children of a node are
inversely ordered according to the sequence of ap-
plications of the add relation operation.
According to this linearisation definition, for
the query tree (e) in Figure 1 the following linear
order is produced:
(3) a. Man marriedTo Person livesIn House
Beautiful ownedBy RichPeron
2A strict total order can be obtained by fixing an order in
the children nodes and traversing the tree according to some
tree traversal strategy.
Query Segmentation Given a linearised query
q, the document planner uses some heuristics
based on the number and the types of rela-
tions/concepts present in q to output a sequence
of sub-formulae each of which will be verbalised
as a sentence.
4.2.2 Incremental Surface Realisation and
Linearisation Constraints
We now describe the main module of the generator
namely the surface realiser which supports both
the incremental refinement of a query and a min-
imal modification of the linear order between in-
crements. This surface realiser is caracterised by
the following three main features.
Grammar-Based We use a symbolic, grammar-
based approach rather than a statistical one for two
reasons. First, there is no training corpus available
that would consist of knowledge base queries and
their increments. Second, the approach must be
portable and should apply to any knowledge base
independent of the domain it covers and indepen-
dent of the presence of a training corpus. By com-
bining a lexicon automatically extracted from the
ontology with a small grammar tailored to produce
natural sounding queries, we provide a generator
which can effectively apply to any ontology with-
out requiring the construction of a training corpus.
Chart-Based A chart-based architecture en-
hances efficiency by avoiding the recomputation
of intermediate structures while allowing for a
natural implementation of the revisions (addition,
deletion, substitution) operations required by the
incremental formulation of user queries. We show
how the chart can be used to implement these op-
erations.
Beam search. As already mentioned, for er-
gonomic reasons, the linear order of the gener-
ated NL query should be minimally disturbed dur-
ing query formulation. The generation system
186
should also be sufficiently fast to support a timely
Man/Machine interaction. We use beam search
and a customised scoring function both to preserve
linear order and to support efficiency.
We now introduce each of these components in
more details.
Feature-Based Tree Adjoining Grammar
A tree adjoining grammar (TAG) is a tuple
??, N, I,A, S? with ? a set of terminals, N a set
of non-terminals, I a finite set of initial trees, A a
finite set of auxiliary trees, and S a distinguished
non-terminal (S ? N ). Initial trees are trees
whose leaves are labeled with substitution nodes
(marked with a down-arrow) or with terminal
categories3 . Auxiliary trees are distinguished by
a foot node (marked with a star) whose category
must be the same as that of the root node.
Two tree-composition operations are used to
combine trees: substitution and adjunction. Sub-
stitution inserts a tree onto a substitution node of
some other tree while adjunction inserts an aux-
iliary tree into a tree. In a Feature-Based Lexi-
calised TAG (FB-LTAG), tree nodes are further-
more decorated with two feature structures which
are unified during derivation; and each tree is an-
chored with a lexical item. Figure 2 shows an ex-
ample toy FB-LTAG with unification semantics.
The dotted arrows indicate possible tree combina-
tions (substitution for John, adjunction for often).
As the trees are combined, the semantics is the
union of their semantics modulo unification. Thus
given the grammar and the derivation shown, the
semantics of John often runs is as shown namely,
named(j john), run(a,j), often(a).
NPj
John
l1:john(j)
S
b
NP?c VPb
a
Va
runs
lv:run(a,j)
VP
x
often VP*
x
lo:often(x)
l1:named(j john), lv:run(a,j), lv:often(a)
Figure 2: Derivation and Semantics for ?John often runs?
Chart-Based Surface Realisation Given an
FB-LTAG G of the type described above, sen-
tences can be generated from semantic formulae
by (i) selecting all trees in G whose semantics sub-
sumes part of the input formula and (ii) combining
3For a more detailed introduction to TAG and FB-LTAG,
see (Vijay-Shanker and Joshi, 1988).
these trees using the FB-LTAG combining opera-
tions namely substitution and adjunction. Thus for
instance, in Figure 2, given the semantics l1:named(j
john), lv:run(a,j), lv:often(a), the three trees shown are
selected. When combined they produce a com-
plete phrase structure tree whose yield (John runs
often) is the generated sentence.
Following (Gardent and Perez-Beltrachini,
2011), we implement an Earley style generation
algorithm for FB-LTAG which makes use of the
fact that the derivation trees of an FB-LTAG are
context free and that an FB-LTAG can be con-
verted to a a Feature-Based Regular Tree Gram-
mar (FB-RTG) describing the derivation trees of
this FB-LTAG4.
On the one hand, this Earley algorithm en-
hances efficiency in that (i) it avoids recomput-
ing intermediate structures by storing them and
(ii) it packs locally equivalent structures into a
single representative (the most general one). Lo-
cally equivalent structures are taken to be partial
derivation trees with identical semantic coverage
and similar combinatorics (same number and type
of substitution and adjunction requirements).
On the other hand, it naturally supports the
range of revisions required for the incremental for-
mulation of ontology-based queries. Let C be the
current chart i.e., the chart built when generating a
NL query from the formal query. Then additions,
revisions and deletion can be handled as follows.
? Add concept or property X: the grammar
units selected by X are added to the agenda5
and tried for combinations with the elements
of C .
? Substitute selection X with Y : all chart items
derived from a grammar unit selected by an
element of X are removed from the chart.
Conversely, all chart items derived from a
grammar unit selected by an element of Y are
added to the agenda. All items in the agenda
are then processed until generation halts.
? Delete selection X: all chart items derived
from a grammar unit selected by an element
of X are removed from the chart. Intermedi-
ate structures that had previously used X are
moved to the agenda and the agenda is pro-
cessed until generation halts.
4For more details on this algorithm, we refer the reader to
(Gardent and Perez-Beltrachini, 2010).
5The agenda is a book keeping device which stores all
items that needs to be processed i.e., which need to be tried
for combination with elements in the chart.
187
Beam Search To enhance efficiency and favor
those structures which best preserve the word or-
der while covering maximal input, we base our
beam search on a scoring function combining lin-
ear order and semantic coverage information. This
works as follows. First, we associate each literal
in the input query with its positional information
e.g.,
(4) a. man(x)[0] marriedTo(x y)[1]
person(y)[2] livesIn(x w)[3]
house(w)[4]
This positional information is copied over to
each FB-LTAG tree selected by a given literal and
is then used to compute a word order cost (C
wo
)
for each derived tree as follows:
C
wo
(t
i+j
) = C
wo
(t
i
) + C
wo
(t
j
) + C
wo
(t
i
+ t
j
)
That is the cost of a tree t
i+j
obtained by com-
bining t
i
and t
j
is the sum of the cost of each
of these trees plus the cost incurred by combin-
ing these two trees. We define this latter cost to
be proportional to the distance separating the ac-
tual position (ap
i
) of the tree (t
i
) being substi-
tuted/adjoined in from its required position (rp
i
).
If t
i
is substituted/adjoined at position n to the
right (left) of the anchor of a tree t
j
with posi-
tion p
j
, then the actual position of t
i
is pj + n
(pj ? n) and the cost of combining t
i
with t
j
is
| pj + n ? rp
i
| /? (| pj ? n ? rp
i
| /?) where
we empirically determined ? to be 1006.
Finally, the total score of a tree reflects the rela-
tion between the cost of the built tree, i.e. its word
order cost, and its semantic coverage, i.e. nb. of
literals from the input semantics:
S(t
i
) =
{
?(|literals| ? 1) C
wo
(t
i
) = 0
C
wo
(t
i
)/(|literals| ? 1) otherwise
The total score is defined by cases. Those trees
with C
wo
= 0 get a negative value according to
their input coverage (i.e. those that cover a larger
subset of the input semantics are favored as the
trees in the agenda are ordered by increasing total
score). Conversely, those trees with C
wo
> 0 get
a score that is the word order cost proportional to
the covered input.
In effect, this scoring mechanism favors trees
with low word order cost and large semantic cov-
erage. The beam search will select those trees with
lowest score.
6In the current implementation we assume that n = 1.
Furthermore, as t
i
might be a derived tree we also add to
C
wo
(t
i
+ t
j
) the cost computed on each tree t
k
used in the
derivation of t
i
with respect to t
j
.
4.2.3 Referring Expression Generation
The referring expression (RE) module takes as
input the sequence of phrase structure trees out-
put by the surface realiser and uses heuristics to
decide for each NP whether it should be ver-
balised as a pronoun, a definite or an indefinite
NP. These heuristics are based on the linear order
and morpho-syntactic information contained in the
phrase structure trees of the generated sentences.
5 Experiments and evaluation
We conducted evaluation experiments designed to
address the following questions:
? Does the scoring mechanism appropriately
capture the ordering constraints on the gen-
erated queries ? That is, does it ensure that
the generated queries respect the strict total
order of the query tree linearisation ?
? Does our grammar based approach produce
more fluent and less ambiguous NL query
than the initial template based approach cur-
rently used by Quelo ?
? Does the automatic extraction of lexicons
from ontology support generic coverage of
arbitrary ontologies ?
We start by describing the grammar used. We
then report on the results obtained for each of these
evaluation points.
5.1 Grammar and Lexicon
We specify an FB-LTAG with unification seman-
tics which covers a set of basic constructions used
to formulate queries namely, active and passive
transitive verbs, adjectives, prepositional phrases,
relative and elliptical clauses, gerund and partici-
ple modifiers. The resulting grammar consists of
53 FB-LTAG pairs of syntactic trees and semantic
schema.
To ensure the appropriate syntax/semantic in-
terface, we make explicit the arguments of a
relation using the variables associated with the
nodes of the query tree. Thus for instance,
given the rightmost query tree shown in Figure
1, the flat semantics input to surface realisation is
{Man(x), Person(y), House(w), Beautiful(w), RichPerson(z),
marriedTo(x,y), livesIn(x,w), ownedBy(w,z)}.
For each ontology, a lexicon mapping con-
cepts and relations to FB-LTAG trees is automat-
ically derived from the ontology using (Trevisan,
2010)?s approach. We specify for each experiment
below, the size of the extracted lexicon.
188
5.2 Linearisation
In this first experiment, we manually examined
whether the incremental algorithm we propose
supports the generation of NL queries whose word
order matches the linearisation of the input query
tree.
We created four series of queries such that each
serie is a sequence q
1
. . . q
n
where q
i+1
is an in-
crement of q
i
. That is, q
i+1
is derived from q
i
by adding, removing or substituting to q
i
a con-
cept or a relation. The series were devised so as to
encompass the whole range of possible operations
at different points of the preceding query (e.g., at
the last node/edge or on some node/edge occur-
ring further to the left of the previous query); and
include 14 revisions on 4 initial queries.
For all queries, the word order of the best NL
query produced by the generator was found to
match the linearisation of the DL query.
5.3 Fluency and Clarity
Following the so-called consensus model (Power
and Third, 2010), the current, template based ver-
sion of Quelo generates one clause per relation7.
Thus for instance, template-based Quelo will gen-
erate (5a) while our grammar based approach sup-
ports the generation of arguably more fluent sen-
tences such as (5b).
(5) a. I am looking for a car. Its make should
be a Land Rover. The body style of the
car should be an off-road car. The exterior
color of the car should be beige.
b. I am looking for car whose make is a Land
Rover, whose body style is an off-road car
and whose exterior color is beige.
We ran two experiments designed to assess how
fluency impacts users. The first experiment aims
to assess how Quelo template based queries are
perceived by the users in terms of clarity and flu-
ency, the second aims to compare these template
based queries with the queries produced by our
grammar-based approach.
Assessing Quelo template-based queries Us-
ing the Quelo interface, we generated a set of
41 queries chosen to capture different combina-
tions of concepts and relations. Eight persons
(four native speakers of English, four with C2
7This is modulo aggregation of relations. Thus two sub-
ject sharing relations may be realised in the same clause.
level of competence for foreign learners of En-
glish) were then asked to classify (a binary choice)
each query in terms of clarity and fluency. Fol-
lowing (Kow and Belz, 2012), we take Fluency
to be a single quality criterion intended to cap-
ture language quality as distinct from its meaning,
i.e. how well a piece of text reads. In contrast,
Clarity/ambiguity refers to ease of understanding
(Is the sentence easy to understand?). Taking the
average of the majority vote, we found that the
judges evaluated the queries as non fluent in 50%
of the cases and as unclear in 10% of the cases.
In other words, template based queries were found
to be disfluent about half of the time and unclear
to a lesser extent. The major observation made by
most of the participants was that the generated text
is too repetitive and lacks aggregation.
Figure 3: Online Evaluation.
Comparing template- and grammar-based
queries In this second experiment, we asked 10
persons (all proficient in the English language) to
compare pairs of NL queries where one query is
produced using templates and the other using our
grammar-based generation algorithm. The evalu-
ation was done online using the LG-Eval toolkit
(Kow and Belz, 2012) and geared to collect rel-
ative quality judgements using visual analogue
scales. After logging in, judges were given a de-
scription of the task. The sentence pairs were dis-
played as shown in Figure 3 with one sentence to
the left and the other to the right. The judges were
instructed to move the slider to the left to favor
the sentence shown on the left side of the screen;
and to the right to favor the sentence appearing to
the right. Not moving the slider means that both
sentences rank equally. To avoid creating a bias,
189
the sentences from both systems were equally dis-
tributed to both sides of the screen.
For this experiment, we used 14 queries built
from two ontologies, an ontology on cars and the
other on universities. The extracted lexicons for
each of these ontology contained 465 and 297 en-
tries respectively.
The results indicate that the queries generated
by the grammar based approach are perceived as
more fluent than those produced by the template
based approach (19.76 points in average for the
grammar based approach against 7.20 for the tem-
plate based approach). Furthermore, although the
template based queries are perceived as clearer
(8.57 for Quelo, 6.87 for our approach), the dif-
ference is not statistically significant (p < 0.5).
Overall thus, the grammar based approach appears
to produce verbalisations that are better accepted
by the users. Concerning clarity, we observed that
longer sentences let through by document plan-
ning were often deemed unclear. In future work,
we plan to improve clarity by better integrating
document planning and sentence realisation.
5.4 Coverage
One motivation for the symbolic based approach
was the lack of training corpus and the need for
portability: the query interface should be usable
independently of the underlying ontology and of
the existence of a training corpus. To support
coverage, we combined the grammar based ap-
proach with a lexicon which is automatically ex-
tracted from the ontology using the methodology
described in (Trevisan, 2010). When tested on
a corpus of 200 ontologies, this approach was
shown to be able to provide appropriate verbalisa-
tion templates for about 85% of the relation iden-
tifiers present in these ontologies. 12 000 relation
identifiers were extracted from the 200 ontologies
and 13 syntactic templates were found to be suf-
ficient to verbalise these relation identifiers (see
(Trevisan, 2010) for more details on this evalua-
tion).
That is, in general, the extracted lexicons permit
covering about 85% of the ontological data. In ad-
dition, we evaluated the coverage of our approach
by running the generator on 40 queries generated
from five distinct ontologies. The domains ob-
served are cinema, wines, human abilities, dis-
abilities, and assistive devices, e-commerce on the
Web, and a fishery database for observations about
an aquatic resource. The extracted lexicons con-
tained in average 453 lexical entries and the cov-
erage (proportion of DL queries for which the gen-
erator produced a NL query) was 87%.
Fuller coverage could be obtained by manually
adding lexical entries, or by developing new ways
of inducing lexical entries from ontologies (c.f.
e.g. (Walter et al., 2013)).
6 Conclusion
Conceptual authoring (CA) allows the user to
query a knowledge base without having any
knowledge either of the formal representation lan-
guage used to specify that knowledge base or of
the content of the knowledge base. Although this
approach builds on a tight integration between
syntax and semantics and requires an efficient pro-
cessing of revisions, existing CA tools predomi-
nantly make use of ad hoc generation algorithms
and restricted computational grammars (e.g., Def-
inite Clause Grammars or templates). In this pa-
per, we have shown that FB-LTAG and chart based
surface realisation provide a natural framework in
which to implement conceptual authoring. In par-
ticular, we show that the chart based approach nat-
urally supports the definition of an incremental al-
gorithm for query verbalisation; and that the added
fluency provided by the grammar based approach
potentially provides for query interfaces that are
better accepted by the human evaluators.
In the future, we would like to investigate the
interaction between context, document structuring
and surface realisation. In our experiments we
found out that this interaction strongly impacts flu-
ency whereby for instance, a complex sentence
might be perceived as more fluent than several
clauses but a too long sentence will be perceived
as difficult to read (non fluent). Using data that
can now be collected using our grammar based
approach to query verbalisation and generalising
over FB-LTAG tree names rather than lemmas or
POS tags, we plan to explore how e.g., Conditional
Random Fields can be used to model these inter-
actions.
Acknowledgments
We would like to thank Marco Trevisan, Paolo
Guagliardo and Alexandre Denis for facilitating
the access to the libraries they developed and to
Natalia Korchagina and the judges who partici-
pated in the evaluation experiments.
190
References
Franz Baader. 2003. The description logic handbook:
theory, implementation, and applications. Cam-
bridge university press.
Nina Dethlefs, Helen Hastie, Heriberto Cuaya?huitl, and
Oliver Lemon. 2013. Conditional Random Fields
for Responsive Surface Realisation using Global
Features. Proceedings of ACL, Sofia, Bulgaria.
Paolo Dongilli. 2008. Natural language rendering of a
conjunctive query. KRDB Research Centre Techni-
cal Report No. KRDB08-3). Bozen, IT: Free Univer-
sity of Bozen-Bolzano, 2:5.
E. Franconi, P. Guagliardo, and M. Trevisan. 2010a.
An intelligent query interface based on ontology
navigation. In Workshop on Visual Interfaces to the
Social and Semantic Web, VISSW, volume 10. Cite-
seer.
E. Franconi, P. Guagliardo, and M. Trevisan. 2010b.
Quelo: a NL-based intelligent query interface. In
Pre-Proceedings of the Second Workshop on Con-
trolled Natural Languages, volume 622.
E. Franconi, P. Guagliardo, S. Tessaris, and M. Tre-
visan. 2011a. A natural language ontology-driven
query interface. In 9th International Conference on
Terminology and Artificial Intelligence, page 43.
E. Franconi, P. Guagliardo, M. Trevisan, and S. Tes-
saris. 2011b. Quelo: an Ontology-Driven Query
Interface. In Description Logics.
C. Gardent and L. Perez-Beltrachini. 2010. RTG based
Surface Realisation for TAG. In COLING?10, Bei-
jing, China.
B. Gottesman Gardent, C. and L. Perez-Beltrachini.
2011. Using regular tree grammar to enhance sur-
face realisation. Natural Language Engineering,
17:185?201. Special Issue on Finite State Methods
and Models in Natural Language Processing.
Paolo Guagliardo. 2009. Theoretical foundations of
an ontology-based visual tool for query formulation
support. Technical report, KRDB Research Centre,
Free University of Bozen-Bolzano, October.
C. Hallett, D. Scott, and R. Power. 2007. Composing
questions through conceptual authoring. Computa-
tional Linguistics, 33(1):105?133.
Eric Kow and Anja Belz. 2012. LG-Eval: A Toolkit
for Creating Online Language Evaluation Experi-
ments. In LREC, pages 4033?4037.
Alice H Oh and Alexander I Rudnicky. 2000. Stochas-
tic language generation for spoken dialogue sys-
tems. In Proceedings of the 2000 ANLP/NAACL
Workshop on Conversational systems-Volume 3,
pages 27?32. Association for Computational Lin-
guistics.
R. Power and A. Third. 2010. Expressing owl ax-
ioms by english sentences: dubious in theory, fea-
sible in practice. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 1006?1013. Association for Compu-
tational Linguistics.
David Schlangen and Gabriel Skantze. 2009. A gen-
eral, abstract model of incremental dialogue pro-
cessing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 710?718. Association
for Computational Linguistics.
David Schlangen, Timo Baumann, and Michaela At-
terer. 2009. Incremental reference resolution: The
task, metrics for evaluation, and a bayesian filtering
model that is sensitive to disfluencies. In Proceed-
ings of the SIGDIAL 2009 Conference: The 10th An-
nual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 30?37. Association for
Computational Linguistics.
H. R Tennant, K. M Ross, R. M Saenz, C. W Thomp-
son, and J. R Miller. 1983. Menu-based natural lan-
guage understanding. In Proceedings of the 21st an-
nual meeting on Association for Computational Lin-
guistics, pages 151?158. Association for Computa-
tional Linguistics.
Marco Trevisan. 2010. A Portable Menuguided Nat-
ural Language Interface to Knowledge Bases for
Querytool. Ph.D. thesis, Masters thesis, Free Uni-
versity of Bozen-Bolzano (Italy) and University of
Groningen (Netherlands).
K. Vijay-Shanker and A. Joshi. 1988. Feature based
tags. In Proceedings of the 12th International Con-
ference of the Association for Computational Lin-
guistics, pages 573?577, Budapest.
Holger Wache, Thomas Voegele, Ubbo Visser, Heiner
Stuckenschmidt, Gerhard Schuster, Holger Neu-
mann, and Sebastian Hu?bner. 2001. Ontology-
based integration of information-a survey of existing
approaches. In IJCAI-01 workshop: ontologies and
information sharing, volume 2001, pages 108?117.
Citeseer.
Sebastian Walter, Christina Unger, and Philipp Cimi-
ano. 2013. A corpus-based approach for the induc-
tion of ontology lexica. In Natural Language Pro-
cessing and Information Systems, pages 102?113.
Springer.
191
XMG: eXtensible MetaGrammar
Beno??t Crabbe??
INRIA - Universite? Paris 7
Denys Duchier??
LIFO - Universite? d?Orle?ans
Claire Gardent?
CNRS - LORIA, Nancy
Joseph Le Roux?
LIPN - Universite? Paris Nord
Yannick Parmentier?
LIFO - Universite? d?Orle?ans
In this article, we introduce eXtensible MetaGrammar (XMG), a framework for specifying
tree-based grammars such as Feature-Based Lexicalized Tree-Adjoining Grammars (FB-LTAG)
and Interaction Grammars (IG). We argue that XMG displays three features that facilitate
both grammar writing and a fast prototyping of tree-based grammars. Firstly, XMG is fully
declarative. For instance, it permits a declarative treatment of diathesis that markedly departs
from the procedural lexical rules often used to specify tree-based grammars. Secondly, the XMG
language has a high notational expressivity in that it supports multiple linguistic dimensions,
inheritance, and a sophisticated treatment of identifiers. Thirdly, XMG is extensible in that its
computational architecture facilitates the extension to other linguistic formalisms. We explain
how this architecture naturally supports the design of three linguistic formalisms, namely,
FB-LTAG, IG, and Multi-Component Tree-Adjoining Grammar (MC-TAG). We further show
how it permits a straightforward integration of additional mechanisms such as linguistic and
formal principles. To further illustrate the declarativity, notational expressivity, and extensibility
of XMG, we describe the methodology used to specify an FB-LTAG for French augmented with a
? UFR de Linguistique, Universite? Paris Diderot-Paris 7, Case 7003, 2, F-75205 Paris Cedex 13, France.
E-mail: bcrabbe@linguist.jussieu.fr.
?? Laboratoire d?Informatique Fondamentale d?Orle?ans, Ba?timent IIIA, Rue Le?onard de Vinci, B.P. 6759,
F-45067 Orle?ans Cedex 2, France. E-mail: denys.duchier@univ-orleans.fr.
? Laboratoire LORIA - CNRS, Projet Synalp, Ba?timent B, BP 239, Campus Scientifique, F-54506
Vand?uvre-Le`s-Nancy Cedex, France. E-mail: gardent@loria.fr.
? Laboratoire d?Informatique de Paris Nord, UMR CNRS 7030, Institut Galile?e - Universite? Paris-Nord, 99,
avenue Jean-Baptiste Cle?ment, F-93430 Villetaneuse, E-mail: leroux@univ-paris13.fr.
? Laboratoire d?Informatique Fondamentale d?Orle?ans, Ba?timent IIIA, Rue Le?onard de Vinci, B.P. 6759,
F-45067 Orle?ans Cedex 2, France. E-mail: yannick.parmentier@univ-orleans.fr.
Submission received: 27 March 2009; revised version received: 2 July 2012; accepted for publication:
11 August 2012.
doi:10.1162/COLI a 00144
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
unification-based compositional semantics. This illustrates both how XMG facilitates the
modeling of the tree fragment hierarchies required to specify tree-based grammars and of a
syntax/semantics interface between semantic representations and syntactic trees. Finally, we
briefly report on several grammars for French, English, and German that were implemented
using XMG and compare XMG with other existing grammar specification frameworks for
tree-based grammars.
1. Introduction
In the late 1980s and early 1990s, many grammar engineering environments were
developed to support the specification of large computational grammars for natural
language. One may, for instance, cite XLE (Kaplan and Newman 1997) for specifying
Lexical-Functional Grammars (LFG), LKB (Copestake and Flickinger 2000) for speci-
fying Head-driven Phrase Structure Grammars (HPSG), and DOTCCG (Baldridge
et al 2007) for specifying Combinatory Categorial Grammars (CCG). Concretely, such
environments usually rely on (i) a formal language used to describe a target com-
putational grammar, and (ii) a processor for this language, which aims at generating
the actual described grammar (and potentially at checking it, e.g., by feeding it to
a parser).
Although these environments were tailored for specific grammar formalisms, they
share a number of features. Firstly, they are expressive enough to characterize subsets
of natural language. Following Shieber (1984), we call this feature weak completeness.
Secondly, they are notationally expressive enough to relatively easily formalize important
theoretical notions. Thirdly, they are rigorous, that is, the semantics of their underlying
language is well defined and understood. Additionally, for an environment to be useful
in practice, it should be simple to use (by a linguist), and make it possible to detect errors
in the described target grammar.
If we consider a particular type of computational grammar, namely, tree-based
grammars?that is, grammars where the basic units are trees (or tree descriptions) of
arbitrary depth, such as Tree-Adjoining Grammar (TAG; Joshi, Levy, and Takahashi
1975), D-Tree Grammar (DTG; Rambow, Vijay-Shanker, and Weir 1995), Tree Description
Grammars (TDG; Kallmeyer 1999) or Interaction Grammars (IG; Perrier 2000)?
environments sharing all of the listed features are lacking. As we shall see in Section 7
of this article, there have been some proposals for grammar engineering environments
for tree-based grammar (e.g., Candito 1996; Xia, Palmer, and Vijay-Shanker 1999,
but these lack notational expressivity. This is partly due to the fact that tree-based
formalisms offer an extended domain of locality where one can encode constraints
between remote syntactic constituents. If one wants to define such constraints while
giving a modular and incremental specification of the grammar, one needs a high level
of notational expressivity, as we shall see throughout the article (and especially in
Section 4).
In this article, we present XMG (eXtensible MetaGrammar), a framework for
specifying tree-based grammars. Focusing mostly on Feature-Based Lexicalized Tree-
Adjoining Grammars (FB-LTAG) (but using Interaction Grammars [IG] and Multi-
Component Tree-Adjoining Grammars [MC-TAG] to illustrate flexibility), we argue that
XMG departs from other existing computational frameworks for designing tree-based
grammars in three main ways:
 First, XMG is a declarative language. In other words, grammaticality is
defined in an order-independent fashion by a set of well-formedness
592
Crabbe? et al XMG: eXtensible MetaGrammar
constraints rather than by procedures. In particular, XMG permits a
fully declarative treatment of diathesis that markedly departs from the
procedural rules (called meta-rules or lexical rules) previously used to
specify tree-based grammars.
 Second, XMG is notationally expressive. The XMG language supports full
disjunction and conjunction of grammatical units, a modular treatment
of multiple linguistic dimensions, multiple inheritance of units, and a
sophisticated treatment of identifiers. We illustrate XMG?s notational
expressivity by showing (i) how it facilitates the modeling of the tree
fragment hierarchies required to specify tree-based grammars and (ii) how
it permits a natural modeling of the syntax/semantics interface between
semantic representations and syntactic trees as can be used in FB-LTAG.
 Third, XMG is extensible in that its computational architecture facilitates
(i) the integration of an arbitrary number of linguistic dimensions (syntax,
semantics, etc.), (ii) the modeling of different grammar formalisms
(FB-LTAG, MC-TAG, IG), and (iii) the specification of general linguistic
principles (e.g., clitic ordering in French).
The article is structured as follows. Section 2 starts by giving a brief introduction
to FB-LTAG, the grammar formalism we used to illustrate most of XMG?s features. The
next three sections then go on to discuss and illustrate XMG?s three main features?
namely, declarativity, notational expressivity, and flexibility. In Section 3, we focus
on declarativity and show how XMG?s generalized disjunction permits a declarative
encoding of diathesis. We then contrast the XMG approach with the procedural methods
previously resorted to for specifying FB-LTAG. Section 4 addresses notational expressiv-
ity. We present the syntax of XMG and show how the sophisticated identifier handling
it supports or permits a natural treatment (i) of identifiers in tree based hierarchies
and (ii) of the unification-based syntax/semantics interface often used in FB-LTAG. In
Section 5, we concentrate on extensibility. We first describe the operational semantics
of XMG and the architecture of the XMG compiler. We then show how these facilitate
the adaptation of the basic XMG language to (i) different grammar formalisms (IG,
MC-TAG, FB-LTAG), (ii) the integration of specific linguistic principles such as clitic
ordering constraints, and (iii) the specification of an arbitrary number of linguistic
dimensions. In Section 6, we illustrate the usage of XMG by presenting an XMG
specification for the verbal fragment of a large scale FB-LTAG for French augmented
with a unification-based semantics. We also briefly describe the various other tree-
based grammars implemented using XMG. Section 7 discusses the limitations of other
approaches to the formal specification of tree-based grammars, and Section 8 concludes
with pointers for further research.
2. Tree-Adjoining Grammar
A Tree-Adjoining Grammar (TAG) consists of a set of auxiliary or initial elementary
trees and of two tree composition operations, namely, substitution and adjunction.
Initial trees are trees whose leaves are either substitution nodes (marked with ?) or
terminal symbols (words). Auxiliary trees are distinguished by a foot node (marked
with ) whose category must be the same as that of the root node. Substitution inserts a
tree onto a substitution node of some other tree and adjunction inserts an auxiliary tree
593
Computational Linguistics Volume 39, Number 3
N
Marie
Mary
V
V
a
has
V
S
N? V
vu
seen
N?
N
Jean
John
??
S
N
Marie
Mary
V
V
a
has
V
vu
seen
N
Jean
John
Figure 1
Sample derivation of Marie a vu Jean ?Mary has seen John? in a TAG.
into a tree. Figure 1 shows a toy TAG generating the sentence Marie a vu Jean ?Mary has
seen John? and sketches its derivation.1
Among existing variants of TAG, one commonly used in practice is Lexical-
ized FB-LTAG (Vijay-Shanker and Joshi 1988). A lexicalized TAG is such that each
elementary tree has at least one leaf labeled with a lexical item (word), whereas in
an FB-LTAG, tree nodes are additionally decorated with two feature structures (called
top and bottom). These feature structures are unified during derivation as follows. On
substitution, the top features of the substitution node are unified with the top features of
the root node of the tree being substituted in. On adjunction, the top features of the root
of the auxiliary tree are unified with the top features of the node where adjunction takes
place; and the bottom features of the foot node of the auxiliary tree are unified with the
bottom features of the node where adjunction takes place. At the end of a derivation,
the top and bottom feature structures of all nodes in the derived tree are unified.
Implementation of Tree-Adjoining Grammars. Most existing implementations of TAGs fol-
low the three-layer architecture adopted for the XTAG grammar (XTAG Research Group
2001), a feature-based lexicalized TAG for English. Thus the grammar consists of (i) a
set of so-called tree schemas (i.e., elementary trees having a leaf node labeled with a
 referring to where to anchor lexical items2), (ii) a morphological lexicon associating
words with lemmas, and (iii) a syntactic lexicon associating lemmas with tree schemas
(these are gathered into families according to syntactic properties, such as the sub-
categorization frame for verbs). Figure 2 shows some of the tree schemas associated
with transitive verbs in the XTAG grammar. The tree corresponds (a) to a declarative
sentence, (b) to a WH-question on the subject, (c) to a passive clause with a BY-agent,
and (d) to a passive clause with a WH-object. As can be seen, each tree schema contains
an anchor node (marked with ). During parsing this anchor node can be replaced by
any word morphologically related to a lemma listed in the syntactic lexicon as anchor-
ing the transitive tree family.
This concept of tree family allows us to share structural information (tree schemas)
between words having common syntactic properties (e.g., sub-categorization frames).
There still remains a large redundancy within the grammar because many elementary
tree schemas share common subtrees (large coverage TAGs usually consist of hun-
dreds, sometimes thousands, of tree schemas). An important issue when specifying
1 The elementary trees displayed in this article conform to Abeille? (2002), that is, we reject the use of a VP
constituent in French.
2 As mentioned earlier, we describe lexicalized TAG, thus every tree schema has to contain at least one
anchor (node labeled ).
594
Crabbe? et al XMG: eXtensible MetaGrammar
(a) (b)
Sr
NP0 ? VP
V NP1 ?
Sq
NP0 ?
[
wh +
]
[]
Sr
NPNA

VP
V NP1 ?
(c) (d)
Sr [][
mode 3
]
NP1 ? VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
NP0 ?
Sq
NP1 ?
[
wh +
]
[]
Sr [][
mode 3
]
NPNA

VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
NP0 ?
Figure 2
Some tree schemas for English transitive verbs.
such grammars is thus structure sharing. Being able to share structural information is
necessary not only for a faster grammar development, but also for an easier grammar
maintenance (modifications to be applied to the tree schemas would be restricted to
shared structures). In the next section, we will see how XMG declarativity can be
efficiently used to factorize TAGs. In addition, Section 4 will show how XMG notational
expressivity facilitates the specification of another commonly used tree sharing device,
namely, inheritance hierarchies of tree fragments.
Extending TAG with a Unification-Based Semantics. To extend FB-LTAG with a compo-
sitional semantics, Gardent and Kallmeyer (2003) propose to associate each elementary
tree with a flat semantic representation. For instance, in Figure 3, the trees3 for John, runs,
and often are associated with the semantics l0:name(j,john), l1:run(e,s), and l2:often(x),
respectively. Importantly, the arguments of semantic functors are represented by uni-
fication variables which occur both in the semantic representation of this functor and
on some nodes of the associated syntactic tree. Thus in Figure 3, the semantic index s
occurring in the semantic representation of runs also occurs on the subject substitution
node of the associated elementary tree. The value of semantic arguments is then deter-
mined by the unifications resulting from adjunction and substitution. For instance, the
semantic index s in the tree for runs is unified during substitution with the semantic
index j labeling the root node of the tree for John. As a result, the semantics of John often
runs is {l0:name(j,john), l1:run(e,j), l2:often(e)}.
Gardent and Kallmeyer?s (2003) proposal was applied to various semantic phe-
nomena (Kallmeyer and Romero 2004a, 2004b, 2008). Its implementation, however,
3 Cx/Cx abbreviate a node with category C and a top/bottom feature structure including the feature-value
pair { index : x}.
595
Computational Linguistics Volume 39, Number 3
NPj
John
l0:name(j,john)
Sg
NP?s VPgf
Vfe
runs
l1:run(e,s)
VPx
often VP*x
l2:often(x)
? l0:name(j,john), l1:run(e,j), l2:often(e)
Figure 3
A toy lexicalized FTAG with unification-based semantics (l0, l1, l2, e, and j are constants and
s, f, g, x are unification variables).
relies on having a computational framework that associates syntactic trees with flat
semantic formulae while allowing for shared variables between trees and formulae. In
the following sections, we will show how XMG notational expressivity makes it pos-
sible to specify an FB-LTAG equipped with a unification-based semantics.
3. Declarativity
In this section, we show how a phenomenon which is often handled in a procedural
way by existing approaches can be provided with a declarative specification in XMG.
Concretely, we show how XMG supports a declarative account of diathesis that avoids
the drawbacks of lexical rules (e.g., information erasing). We start by presenting the
lexical rule approach. We then contrast it with the XMG account.
3.1 Capturing Diathesis Using Lexical Rules
Following Flickinger (1987), redundancy among grammatical descriptions is often han-
dled using two devices: an inheritance hierarchy and a set of lexical rules. Whereas
the inheritance hierarchy permits us to encode the sharing of common substructures,
lexical rules (sometimes called meta-rules) permit us to capture relationships between
trees by deriving new trees from already specified ones. For instance, passive trees will
be derived from active ones.
Although Flickinger?s (1987) approach was developed for HPSGs, several similar
approaches have been put forward for FB-LTAG (Vijay-Shanker and Schabes 1992;
Becker 1993; Evans, Gazdar, and Weir 1995; XTAG Research Group 2001). One important
drawback of these approaches, however, is that they are procedural in that the order in
which lexical rules apply matters. For instance, consider again the set of trees given
in Figure 2. In the meta-rule representation scheme adopted by Becker (1993), the base
tree (a) would be specified in the inheritance hierarchy grouping all base trees, and
the derived trees (b, c, d) would be generated by applying one or more meta-rules on
this base tree. Figure 4 sketches these meta-rules. The left-hand side of the meta-rule
is a matching pattern replaced with the right-hand side of the meta-rule in the newly
generated tree. Symbol ??? denotes a meta-variable whose matching subtree in the input
is substituted in place of the variable in the output tree. Given these, the tree family in
Figure 2 is generated as follows: (b) and (c) are generated by application to the base
tree (a) of the Wh-Subject and Passive meta-rules, respectively. Further, (d) is generated
by applying first, the Wh-Subject meta-rule and second, the Passive meta-rule to the
base tree.
596
Crabbe? et al XMG: eXtensible MetaGrammar
Passive meta-rule Wh-Subject meta-rule
Sr
?1 NP? VP
V ?2 NP?
? Sr [][
mode 3
]
?2 NP? VP
[
mode 3
]
?
?
mode 2
passive 1
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
?1 NP?
Sr
?2NP? ? ?1
? Sq
?2NP? ?
[
wh +
]
[]
Sr
NP?NA

?1
Figure 4
Simplified meta-rules for passive and wh-subject extraction.
More generally a meta-rule is a procedural device that, given a tree instance,
generates a new tree instance by adding, suppressing (hence possibly substituting)
information in grammatical units. Prolo (2002) defines a set of meta-rules that can
be used to specify a large FB-LTAG for English. Given an ordered set of meta-rules,
however, there is no guarantee that the trees they derive are linguistically appropriate
and that the derivation process terminates. Thus, to ensure termination and consistency,
Prolo needs to additionally provide rule ordering schemes (expressed as automata).
3.2 XMG: Capturing Diathesis Using Disjunction
XMG provides an alternative account for describing tree sets such as that of Figure 2
without lexical rules and without the related ordering constraints. In essence, the
approach consists of enumerating trees by combining tree fragments using conjunction
and disjunction.
More specifically, the tree set given in Figure 2 can be generated by combining
some of the tree fragments sketched in Figure 5 using the following conjunctions and
disjunctions:4
Subject ? CanonicalSubject ? Wh-NP-Subject (1)
ActiveTransitiveVerb ? Subject ? ActiveVerb ? CanonicalObject (2)
PassiveTransitiveVerb ? Subject ? PassiveVerb ? CanonicalByObject (3)
TransitiveVerb ? ActiveTransitiveVerb ? PassiveTransitiveVerb (4)
The first clause (Subject) groups together two subtrees representing the possi-
ble realizations of a subject (canonical and wh). The next two clauses define a tree
set for active and passive transitive verbs, respectively. The last clause defines the
TransitiveVerb family as a disjunction of the two verb forms (passive or active). In sum,
the TransitiveVerb clause defines the tree set sketched in Figure 2 as a disjunction of
conjunctions of tree fragments.
One of the issues of meta-rules reported by Prolo (2002) is the handling of feature
equations. For a number of cases (including subject relativization in passive trees),
4 For now, let us consider that the tree fragments are combined in order to produce minimal trees by
merging nodes whose categories (and features) unify. In the next section, we will see how to precisely
control node identification using either node variables or node constraints.
597
Computational Linguistics Volume 39, Number 3
Canonical Subject ? Wh-NP-Subject ? Canonical Object ? Wh-NP-Object ?
Sr
NP? VP
Sq
NP?
[
wh +
]
[]
Sr
NPNA

VP
VP
V NP?
Sq
NP?
[
wh +
]
[]
Sr
VP NPNA

Canonical By Object ? Wh By Object ? Active Verb ? Passive Verb ?
VP
V PP
P
by
NP?
VP
PP
P
by
NP?
V
Sr
VP
V
Sr[]
[
mode 3
]
VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
Figure 5
Tree fragments.
ad hoc meta-rules are needed, for a unified tree transformation cannot be defined. In
a declarative approach such as the one here, dealing with feature equations can be
done relatively easily. Let us imagine that we now want to extend the trees of Figure 2
with feature equations for subject?number agreement. We can for instance do so by
defining the following tree fragment (the dashed line indicates that the VP node can be
a descendant, not only a daughter, of the S node):5
SubjAgreement ? S
NP?
[
num 1
]
[
num 1
]
VP
[
num 1
]
[
num 1
]
Then we extend the definition of Subject as follows:
Subject ? SubjAgreement ? ( CanonicalSubject ? Wh-NP-Subject ) (5)
If we want to get further with the description of transitive verbs, for instance by
taking into account wh-objects and by-objects, this can be done as follows. We first
define the elementary fragments Wh-NP-Object and Wh-By-Object (see Figure 5), and
then define the following additional combinations:6
ActiveTransitiveVerb ? CanonicalSubject ? ActiveVerb ? Wh-Np-Object (6)
PassiveTransitiveVerb ? CanonicalSubject ? PassiveVerb ? Wh-By-Object (7)
5 Note that in XMG, it is not mandatory to define any tree structure inside SubjAgreement. We could define
independent NP and VP nodes, and associate them with variables, say n1 and n2. n1 and n2 would then
be exported and reused directly in the classes CanonicalSubject and Wh-NP-Subject, respectively.
6 Note that these clauses only consider canonical subjects to avoid having both a Wh-subject and a
Wh-object. This is not entirely satisfactory, as we would prefer to define a single abstraction over objects
(as was done for subjects) and use it wherever possible. There would then be another mechanism to
capture this exception and cause the invalid combination to fail (that is, the resulting tree description not
to have any model). Such a mechanism exists in XMG, and is called linguistic principle (see Section 5).
598
Crabbe? et al XMG: eXtensible MetaGrammar
Evans, Gazdar, and Weir (1995) argue for the necessity of using lexical rules for
grammatical description based on two arguments: (i) morphology is irregular and has
to be handled by a non-monotonic device and (ii) erasing rules such as the agentless
passive (John eats an apple / An apple is eaten ) are needed to erase an argument from
the canonical base tree. Neither of these arguments holds here, however: The first
argument because we describe tree schema hence lexical and morphological issues are
ruled out; the second because agentless passive and, more generally, argument erasing
constructions can simply be defined by an additional clause such as:
AgentlessPassiveTransitiveVerb ? Subject ? PassiveVerb (8)
To summarize, using a declarative language to specify a tree-based grammar offers
an adequate level of control on the structures being described while avoiding having
to deal with ordering and termination issues. It facilitates grammar design and mainte-
nance, by providing an abstract view on grammar trees, uniquely made of monotonic
(no information removal) combinations of tree fragments.
4. Notational Expressivity
We now focus on notational expressivity and show how XMG supports a direct
encoding of (i) distinct linguistic dimensions (here syntax, semantics and the syntax/
semantics interface) and (ii) the various types of coreferences7 that arise in the devel-
opment of tree-based grammars.
The syntax of the XMG language can be formally defined as follows.
Class ::= NameC1,...,Ckx1,...,xn ? Content (9)
Content ::= ?SYN, SEM, DYN? | Name | Content ? Content | Content ? Content
(10)
SYN ::=
n1 ? n2 | n1 ?+ n2 | n1 ?? n2 | n1 ? n2 | n1 ?+ n2 | n1 ?? n2 |
n1[f1 : v1,..., fk : vk] | n1(c1 : cv1,..., cl : cvl) | n1 = n2 | x = Ci.y |
n1 (c1 : cv1,..., cl : cvl) [f1 : v1,..., fk : vk] | SYN ? SYN
(11)
SEM ::= li : p(E1,...,En) | li ? hj | SEM ? SEM (12)
DYN ::= ? f1 : v1,...,fn : vn ? (13)
Here and in what follows, we use the following notational conventions. Ci denote
variables over class names; xi, x, and y are variables ranging over tree nodes or feature
values; ni refer to node variables; f, fi are features and v, vi and feature values (constants
or variables); li, hj, p, and Ei are variables over semantic labels, semantic holes, predi-
cates, and predicate arguments in flat semantic formulae, respectively.8 [ ] are used to
associate a node variable with some feature constraint. ( ) are used to associate a node
variable with some property constraint (e.g., node colors, see Section 5). ci and cvi denote
7 By coreference, we mean the sharing of information between distinct elementary fragments of the
grammar specification.
8 See Gardent and Kallmeyer (2003) for a detailed introduction to flat semantics.
599
Computational Linguistics Volume 39, Number 3
a property constraint and a property constraint value, respectively. Ci.y denotes the y
variable declared in class Ci and = is unification; ? and ? denote linear precedence and
immediate dominance relations between nodes. Finally, +, ? represent the transitive and
transitive-reflexive closure of a relation, respectively.
The first two clauses of the formal definition here specify XMG classes and how they
combine. The next three clauses define the languages supported for describing three lin-
guistic dimensions, namely, syntax (SYN), semantics (SEM), and the syntax/semantics
interface (called DYN for dynamic interface). We now discuss each of these in more
detail starting bottom?up with the three linguistic dimensions and ending with the
control language that permits us to combine basic linguistic units into bigger ones.
SYN. The XMG formalism for syntax (copied here for convenience) is a tree description
logic similar to that proposed by Vijay-Shanker and Schabes (1992) and Rogers and
Vijay-Shanker (1994) to describe tree-based grammars.
SYN ::= n1 ? n2 | n1 ?+ n2 | n1 ?? n2 | n1 ? n2 | n1 ?+ n2 | n1 ?? n2 |
n1[f1 : v1,..., fk : vk] | n1(c1 : cv1,..., cl : cvl) | n1 = n2 | x = Ci.y |
n1 (c1 : cv1,..., cl : cvl) [f1 : v1,..., fk : vk] | SYN ? SYN
It includes tree node variables, feature names, feature values, and feature variables.
Tree node variables can be related by equality (node identification), precedence (imme-
diate or non-immediate), and dominance (immediate or non-immediate). Tree nodes
can also be labeled with feature structures of depth 2, that is, sets of feature/value
pairs where feature values are either variables, constants (e.g., syntactic category), or
non-recursive feature structure (e.g., top and bottom feature structures).
Here is a graphical illustration of how tree logic formulae can be used to describe
tree fragments: The depicted tree fragment is a model satisfying the given formula.
n1 ? n2 ? n1 ? n3 ? n2 ? n3
? n1[cat : S] ? n2(mark : subst) [cat : NP] ? n3[cat : VP]
S
NP? VP
One distinguishing feature of the XMG tree language is the introduction of node
constraints (n1(c : cv)) that generalize Muskens and Krahmer?s (1998) use of positive
and negative node markings. Concretely, node constraints are attribute-value matri-
ces, which contain information to be used when solving tree descriptions to produce
grammar trees. In other words, node constraints are used to further restrict the set
of models satisfying a tree description. As an example of node constraint, consider
node annotations in FB-LTAG (foot node, substitution node, null-adjunction, etc.). Such
annotations can be used as node constraints to allow the description solver to apply
well-formedness constraints (e.g., there is at most one foot node).
Another interesting feature of XMG concerns the inclusion of the dot operator,
which permits us to identify variables across classes in cases where name sharing cannot
be resorted to. When a variable y is declared in a class C, the latter being instantiated
within a class D, y can be accessed from D by C.y (the identifier y still being available
in D?s namespace).
600
Crabbe? et al XMG: eXtensible MetaGrammar
SEM. The semantic dimension supports a direct encoding of the flat semantic formulae
used by Gardent and Kallmeyer (2003):
SEM ::= li : p(E1,...,En) | li ? hj | SEM ? SEM
where li : p(E1,..., En) represents a predicate p with label li and arguments E1,..., En and
li ? hj is a scope constraint between label li and scope hj. Expressions (predicate argu-
ments Ei) can refer to semantic holes, constants (atomic values), or unification variables
(written x, y hereafter).
For instance, the following flat semantic formula can be used to underspecify the
meaning of the sentence ?Every dog chases a cat?:
l0 : ?(x, h1, h2) ? l1 ? h1 ? l1 : Dog(x) ? l2 ? h2 ? l2 : Chase(x, y)
? l3 : ?(y, h3, h4) ? l4 ? h3 ? l4 : Cat(y) ? l2 ? h4
(14)
This formula denotes the following two first-order logic formulae, thereby describing
the two possibles readings of this sentence.9
l0 : ?(x, l1, l3) ? l1 : Dog(x) ? l2 : Chase(x, y) ? l3 : ?(y, l4, l2) ? l4 : Cat(y) (15)
l0 : ?(x, l1, l2) ? l1 : Dog(x) ? l2 : Chase(x, y) ? l3 : ?(y, l4, l0) ? l4 : Cat(y) (16)
DYN. The DYN dimension generalizes Kinyon?s hypertag (Kinyon 2000) which is
unified whenever two tree fragments are combined. Similarly, in XMG the DYN
dimension is a feature structure that is unified whenever two XMG classes are com-
bined through inheritance or through conjunction (see the discussion on XMG control
language, subsequently).
For instance, the following constraints ensure a coreference between the index I
occurring in the syntactic dimension and the argument X occurring in the semantic
dimension (indexsubject and arg1 are feature names, and E, I, X, and V local unification
variables).
C1 ? Node [idx : I] ? ?indexsubject : I? (17)
C2 ? L : P(E) ? L : Theta1(E, X) ? ?arg1 : X? (18)
SubjectArg1 ? C1 ? C2 ? ?indexsubject : V, arg1 : V? (19)
More generally, the DYN dimension permits us to unify nodes and feature values
that belong to distinct classes and dimensions, and are thus often not related within
the inheritance hierarchy. As we shall see in Section 6, the DYN dimension permits
a modular account of the syntax/semantics interface in which linking constraints can
be stipulated separately and reused to specify the various diatheses.
In other words, the DYN feature structure allows us to extend the scope of some
specific variables so that they can be unified with variables (or values) introduced
in some other classes of the metagrammar. This concept of scope extension can be
compared with that of hook in Copestake, Lascarides, and Flickinger (2001).
9 For more details on the interpretation of flat semantics and on its association with a grammar of natural
language, see Gardent (2008).
601
Computational Linguistics Volume 39, Number 3
Control language. The linguistic units (named Content here) defined by the linguist can
be abstracted and combined as follows:
Class ::= NameC1,...,Ckx1,...,xn ? Content
Content ::= ?SYN, SEM, DYN? | Name | Content ? Content | Content ? Content
The first clause states that the linguistic information encoded in Content is abstracted in
a class named Name and that this class inherits classes C1,..., Ck and exports variables
x1,..., xn. That is, XMG allows for abstraction, inheritance, and variable exports. By
default, variables (referring to nodes and feature values) are local to a class. Export
statements extend the scope of a variable to all sub-classes, however. An exported
variable can also be accessed from outside its class in case of class instantiation (using
the dot operator introduced earlier in this section). The second clause states that an
XMG class consists of a syntactic, a semantic, and a dynamic description (each of them
possibly empty), and that XMG classes can be combined by conjunction and disjunc-
tion and reused through class instantiation. The notation ?SYN, SEM, DYN? represents
simultaneous contributions (possibly empty) to all three dimensions.10
The XMG control language differs from other frameworks used to specify tree-
based grammars (Vijay-Shanker and Schabes 1992; Xia et al 1998; Candito 1999)
in two main ways. First, it supports generalized conjunctions and disjunctions of
classes. As shown in Section 3, this permits us, inter alia, a declarative treatment of
diathesis.
Second, it allows for both local and exported variables. As mentioned in Section 3, a
common way to share structure within a tree-based grammar is to define an inheritance
hierarchy of either tree fragments (Evans, Gazdar, and Weir 1995) or tree descriptions
(Vijay-Shanker and Schabes 1992; Candito 1996; Xia 2001). When considering an FB-
LTAG augmented with unification semantics, the hierarchy will additionally contain
semantic representations and/or tuples made of tree fragments and semantic represen-
tations. In all cases, the question arises of how to handle identifiers across classes and,
more specifically, how to share them.
In Candito?s (1996) approach, tree nodes are referred to using constants so that
multiple occurrences of the same node constant refer to the same node. As pointed out
in Gardent and Parmentier (2006), global names have several non-trivial shortcomings.
First, they complicate grammar writing in that the grammar writer must remember the
names used and their intended interpretation. Second, they fail to support multiple uses
of the same class within one class. For instance, in French, some verbs sub-categorize
for two prepositional phrases (PP). A natural way of deriving the tree for such verbs
would be to combine a verbal tree fragment with two instances of a PP fragment. If,
however, the nodes in the PP fragment are labeled with global names, then the two
occurrences of these nodes will be identified thereby blocking the production of the
appropriate tree.11
A less restrictive treatment of identifiers is proposed by Vijay-Shanker and Schabes
(1992), where each tree description can be associated with a set of declared node
variables and subsets of these node variables can be referred to by descriptions in the
10 Although formally precise, this notation can be cumbersome. In the interest of legibility we adopt
throughout the convention that SYN stands for ?SYN, , ?, SEM for ? , SEM, ?, and DYN for ? , , DYN?.
11 An analogous situation may arise in English with ditransitive verbs requiring two direct objects.
602
Crabbe? et al XMG: eXtensible MetaGrammar
hierarchy that inherit from the description in which these node variables were declared.
For instance, if entity A in the hierarchy declares such a special node variable X and B
inherits from A, then X can be referred to in B using the notation A.X.12
XMG generalizes Vijay-Shanker and Schabes?s (1992) approach by integrating an
export mechanism that can be used to extend the scope of a given identifier (node
or feature value variable) to classes that inherit from the exporting class. Thus if
class B inherits from class A and class A exports variable X, then X is visible in B
and its reuse forces identity. If B inherits from several classes and two (or more) of
these inherited classes export the same variable name X, then X is not directly visible
from B. It can be accessed though using the dot operator. First A is identified with a
local variable (e.g., T = A), then T.X can be used to refer to the variable X exported
by A.
To summarize, XMG allows for local variables to be exported to sub-classes as well
as for prefixed variables?that is, variables that are prefixed (using the dot operator)
with a reference to the class in which they are declared. In this way, the pitfalls in-
troduced by global names are avoided while providing enough expressivity to handle
variable coreference (via the definition of variable namespaces). Section 6 will further
illustrate the use of the various coreference devices made available by XMG showing
how they concretely facilitate grammar writing.
Let us finally illustrate variable handling with XMG in the example of Figure 2.
Recall that we define the trees of Figure 2 as the conjunctions and disjunctions of some
tree fragments of Figure 5, such as:
Subject ? SubjAgreement ? ( CanonicalSubject ? Wh-NP-Subject ) (20)
CanonicalSubject can be defined as a tree description formula as follows (only variables
n2 and n3 are exported):
CanonicalSubjectn2,n3 ?
n1 ? n2 ? n1[cat : S] ? n2(mark : subst) [cat : NP]?
n1 ? n3 ? n3[cat : VP] ? n2 ? n3
(21)
The class Wh-NP-Subject is defined accordingly (i.e., by means of a slightly more
complex tree description formula using the n2 and n3 variable identifiers to refer to
the nodes involved in subject agreement). The class SubjAgreement is defined slightly
differently (we do not impose any tree relation between the node concerned with
number agreement):
SubjAgreementn1,n2 ?
n1 [[top : [num : x]] [bot : [num : x]]]?
n2 [[top : [num : x]] [bot : [num : x]]]
(22)
12 In fact, the notation used by Vijay-Shanker and Schabes (1992) is attr:X with attr an attribute variable
ranging over a finite set of attributes, to indicate special node variables that scope outside their class; and
attr(A) to refer to such variables from outside the entity in which they were declared. We use a different
notation here to enforce consistency with the XMG notation.
603
Computational Linguistics Volume 39, Number 3
We can then explicitly control the way the fragments combine as follows:
Subject ?
C1 = SubjAgreementn1,n2 ?
C2 = ( CanonicalSubjectn2,n3 ? Wh-NP-Subjectn2,n3 ) ?
C1.n1 = C2.n2 ? C1.n2 = C2.n3
(23)
In this example, we see how to constrain, via variable export and unification, some
given syntactic nodes to be labeled with feature structures defined somewhere else in
the metagrammar. We use XMG?s flexible management of variable scope to deal with
node coreference. Compared with previous approaches on metagrammars such as those
of Candito (1996), Xia (2001), having the possibility of handling neither only global nor
only local variables, offers a high level of expressivity along with a precise control on
the structures being described.
5. Extensibility
A third distinguishing feature of XMG is extensibility. XMG is extensible in that
(i) dimensions can be added and (ii) each dimension can be associated with its own
interpreter. In order to support an arbitrary number of dimensions, XMG relies on a
device permitting the accumulation of an arbitrary number of types of literals, namely,
Extensible Definite Clause Grammar (EDCG) (Van Roy 1990). Once literals are accumu-
lated according to their type (i.e., each type of literals is accumulated separately), they
can be fed to dedicated interpreters. Because each of these sets of literals represents
formulas of a description language, these interpreters are solvers whose role is to
compute models satisfying the accumulated formulas.
Via this concept of separated dimensions, XMG allows us (i) to describe different
levels of language (not only syntax, but also semantics and potentially morphology,13
etc.), and (ii) to define linguistic principles (well-formedness constraints to be applied on
the structures being described). These principles depend either on the dimension (e.g.,
scope constraints in flat semantics), the target formalism (e.g. cooccurrence predicate-
arguments in FB-LTAG), or the natural language (e.g., clitic ordering in Romance lan-
guages) being described.
In what follows, we start by showing how XMG handles dimensions independently
from each other introducing EDCG (Section 5.1). We then summarize the architecture
of the XMG system (Section 5.2). We finally show how different solvers can be used
to implement various constraints on each of these dimensions (Section 5.3). In partic-
ular, we discuss three kinds of extensions implemented in XMG: extension to several
grammar formalisms, integration of explicit linguistic generalizations, and inclusion of
color-based node marking to facilitate grammar writing.
5.1 XMG: Accumulating and Interpreting an Arbitrary Number of Descriptions
Accumulating (tree) descriptions. First, let us notice that XMG is nothing other than a logic
language a` la Prolog (Duchier, Parmentier, and Petitjean 2012). More precisely, an XMG
13 Recently, XMG has been used to describe the morphology of verbs in Ikota, a Bantu language spoken in
Gabon (Duchier, Parmentier, and Petitjean 2012).
604
Crabbe? et al XMG: eXtensible MetaGrammar
specification is a collection of Horn clauses, which contribute a declarative description
of what a computational tree grammar is.
Logic Program XMG Metagrammar
Clause ::= Head ? Body
Body ::= Fact | Head |
Body ? Body |
Body ? Body
Query ::= Head
Class ::= Name ? Content
Content ::= Description | Name |
Content ? Content |
Content ? Content
Axiom ::= Name
Recall that the descriptions handled by XMG are in fact tuples of the form
?SYN, SEM, DYN?. An XMG class can thus describe, in a non-exclusive way, any of these
three levels of description. If one wants to add another level of description (i.e., another
dimension), one needs to extend the arity of this tuple. Before discussing this, let us first
see how such tuples are processed by XMG.
As mentioned earlier, XMG?s control language is comparable to Horn clauses.
A common way to represent Horn clauses is by using Definite Clause Grammar
(DCG) (Pereira and Warren 1980). Concretely, a DCG is a rewriting system (namely, a
context-free grammar), where the symbols of the rewriting rules are equipped with
pairs of unification variables (these are usually called difference list or accumulator)
(Blackburn, Bos, and Striegnitz 2006, page 100). As an illustration, consider the follow-
ing toy example.
s --> np,vp. np --> det,n.
vp --> v,np. vp --> v.
det --> [the]. det --> [a].
n --> [cat]. n --> [mouse].
v --> [eats].
The string language described by this DCG can be obtained by submitting the query
s(X,[]) where X is a unification variable to be bound with lists of facts (these being the
sentences belonging to the string language). As we can easily see, this language contains
the sentences ?a cat eats,? ?the cat eats,? ?a mouse eats,? ?the mouse eats,? ?a cat eats a
mouse,? ?a mouse eats a cat,? and so on.
Similarly, we can represent XMG classes as DCG clauses. For instance, the combina-
tions of syntactic fragments given in relations (1)?(4) can be rewritten as DCG clauses
as follows:
subject --> canonicalSubject.
subject --> whNpSubject.
activeTransitiveVerb --> subject, activeVerb, canonicalObject.
passiveTransitiveVerb --> subject, passiveVerb, canonicalByObject.
transitiveVerb --> activeTransitiveVerb.
transitiveVerb --> passiveTransitiveVerb.
Disjunctions (e.g., the subject specification) translate to multiple clauses with iden-
tical heads and conjunctions (e.g., activeTransitiveVerb) to a clause body.
In our case, the terminal symbols of the underlying DCG are not just facts, but
tuples of descriptions. In other words, the DCG clause whose head is canonicalSubject
is associated with a tuple of the following form (the dots have to be replaced with
605
Computational Linguistics Volume 39, Number 3
adequate descriptions, these can contain unification variables, whose scope is by default
local to the clause):
canonicalSubject --> [desc(syn(...),sem(...),dyn(...))].
In order to allow for an extension of XMG to an arbitrary number of dimensions,
instead of compiling XMG classes into a DCG whose accumulator stores tuples with
a fixed arity, these classes are compiled into an EDCG (Van Roy 1990). EDCG are DCG
with multiple accumulators. In XMG, each dimension is thus allocated a dedicated
accumulator in the underlying EDCG.
Note that although the content of the various dimensions is accumulated separately,
dimensions may nevertheless share information either via local unification variables
(if the XMG class defines several dimensions locally), via exported unification vari-
ables (in case of class instantiation or inheritance), or via the shared unification variables
supported by the DYN dimension.
At the end of the EDCG execution, we obtain, for each axiom of the metagrammar
(i.e., for each class name to be valuated), a list of description formulas per accumulator.
These lists are grouped together into a tuple of lists of the following form (N is the
number of dimensions, and consequently of accumulators):
desc(accu1(L1),accu2(L2), ... ,accuN(LN))
Each element (i.e., list Li) of such a tuple is a complete description of a given dimension,
where shared variables have been unified (via unification with backtracking).
Solving (tree) descriptions. As illustrated earlier, interpreting XMG?s control language in
terms of an EDCG yields tuples whose arity is the number of dimensions defined by
the linguist, that is, triples of the form ?SYN, SEM, DYN? if syntax, semantics, and the
dynamic interface are described.
For each dimension D, XMG includes a constraint solver SD that computes the set of
minimal models MD = SD(dD) satisfying the description (dD) of that dimension. In other
words, each dimension is interpreted separately by a specific solver. For instance, the
syntactic dimension is handled by a tree description solver that produces, for a given
tree description, the set of trees satisfying that description, whereas the solver for the
semantic dimension simply outputs the flat semantic representation (list of semantic
literals) built by the EDCG through accumulation.
Note that, although solvers are distinct, the models computed in each dimension
may nonetheless be coupled through shared variables. In that case, these variables can
constrain the models computed by the respective solvers. For instance, shared variables
can be used for the syntactic tree description solver to be parametrized by some value
coming from the semantic input description. Note that the output of the solving process
is a Cartesian product of the sets of minimal models of each solver. As a consequence,
the worst case complexity of metagrammar compilation is that of the various solvers
associated with relevant dimensions.
In addition to having separate solvers for each dimension, the constraint-solving
approach used in XMG permits us to modularize a given solver by combining different
principles. Each such principle enforces specific constraints on the models satisfying
the description of a given dimension. For instance, for the syntactic dimension of an
FB-LTAG, a set of principles is used to enforce that the structures produced by the
compiler are trees, and that these conform to the FB-LTAG formalism (e.g., there is no
tree having two foot nodes).
606
Crabbe? et al XMG: eXtensible MetaGrammar
5.2 Architecture
The XMG compiler14 consists of the following three modules:
 A compiler that parses XMG?s concrete syntax and compiles XMG classes
into clauses of an EDCG.
 A virtual machine (VM), which interprets EDCG. This VM performs
the accumulation of dimensions along with scope management and
identifiers resolution. This VM is basically a unification engine equipped
with backtracking, and which is extended to support EDCG. Although its
architecture is inspired by the Warren Abstract Machine (A??t-Kaci 1991),
it uses structure-sharing to represent and unify prolog terms, and, given
a query on a class, processes the conjunctions, disjunctions, inheritance,
and export statements related to that class to produce its full definition,
namely, a tree description for the SYN dimension, a flat semantic formula
for the SEM dimension, and a feature structure for the DYN dimension.
 A constraint-solving phase that produces for each dimension the minimal
models satisfying the input description as unfolded by the preceding
two steps.
As already mentioned, the first part is extensible in that new linguistic dimensions
can be added by specifying additional dedicated accumulators to the underlying EDCG.
The second part is a unification engine that interprets EDCG while performing both term
unification and polarized unification (i.e., unification of polarized feature structures, as
defined by Perrier [2000], and discussed in Section 5.3.1). This extended unification is
the reason why XMG does not merely recourse to an existing Prolog engine to process
EDCG, but relies on a specific VM instead.
The third part is completely modular in that various constraint solvers can be
plugged in depending on the requirements set by the dimensions used, and the chosen
grammatical framework. For instance, the SYN dimension is solved in terms of tree
models, and the SEM dimension is solved in terms of underspecified flat semantic
formulae (i.e., the input semantics remains untouched modulo the unification of its
shared variables).
Importantly, these additional solvers can be ?turned on/off? (via a primitive of the
XMG language) so that, for instance, the same processor can be used to compile an
XMG specification for an FB-LTAG using linguistic principles such as those defined in
the next section (i.e., clitic ordering principle) or not.
5.3 Three Extensions of XMG
We now show (i) how the modular architecture of the XMG compiler permits us
to specify grammars for several tree-based linguistic formalisms; (ii) how it can be
extended to enforce language specific constraints on the syntactic trees; and (iii) how
additional formal constraints (namely node marking) can be integrated to simplify node
identifications (and consequently grammar writing).
14 The XMG compiler is open source software released under the terms of the CeCILL GPL-compliant
licence. See http://sourcesup.renater.fr/xmg.
607
Computational Linguistics Volume 39, Number 3
Eq
Up
Down
Left
Right
Figure 6
Partition of the nodes of tree models.
5.3.1 TAG, MC-TAG, and IG: Producing Trees, Tree Sets, or Tree Descriptions. XMG in-
tegrates a generic tree solver that computes minimal tree models from tree descrip-
tion logic formulae built on the language SYN introduced in Section 4. This solver
integrates the dominance solving technique proposed by Duchier and Niehren (2000)
and can be summarized as follows. A minimal tree model is described in terms of
the relative positions of its nodes. For each node n in a minimal tree model T, the
set of all the nodes of T can be partitioned in five subsets, depending on their po-
sition relative to n. Hence, for each node variable n appearing in a tree description,
it is first associated with an integer (called node id). We then define the five sets
of node ids (i.e., sets of integers) Downn, Upn, Leftn, Rightn, and Eqn referring to the
ids of the nodes located below, above, on the left, on the right, or identified with n,
respectively (see Figure 6). Note that we require that these sets are a partition of all
node ids.
Using this set-based representation of a model, we translate each node relation
from the input formula (built on the tree description language introduced in Section 4)
into constraints on the sets of node ids that must hold in a valid model. For instance,
the sub-formula n1 ?+ n2, which states that node n1 strictly precedes node n2, is
translated into:
n1 ?+ n2 ? EqDownn1 ? Leftn2 ? EqDownn2 ? Rightn1?
Rightn2 ? Rightn1 ? Leftn1 ? Leftn2
(24)
where15 EqDownx = Eqx unionmulti Downx for x ? {n1, n2}. In other words, in a valid minimal
tree model, the set of nodes below or equal to n1 is included in the set of nodes (strictly)
on the left of n2, the set of nodes below or equal to n2 is included in the set of nodes
(strictly) on the right of n1, the set of nodes on the right of n2 is included in the set of
nodes on the right of n1, and finally the set of nodes on the left of n1 is included in the
set of nodes on the left of n2.
Once all input relations are translated into set constraints, the solver uses standard
Constraint Satisfaction techniques (e.g., a first-fail exploration of the search tree) to find a
set of consistent partitions. Finally, the nodes of the models are obtained by considering
nodes with distinct Eqn.
15 unionmulti represents disjoint union.
608
Crabbe? et al XMG: eXtensible MetaGrammar
FB-LTAG trees. To support the specification of FB-LTAG trees, the XMG compiler extends
the generic tree solver described here with a set of constraints ensuring that the trees are
well-formed TAG trees. In effect, these constraints require the trees to be linear ordered
trees with appropriate decorations. Each node must be labeled with a syntactic category.
Leaf nodes are either terminal, foot, or substitution nodes. There is at most one foot
node per tree and the category of the foot node must be identical to that of the root
node. Finally, each tree must have at least one leaf node that is an anchor.
MCTAG tree sets. Where FB-LTAG consists of trees, MC-TAG (Weir 1988) consists of sets
of trees. To support the specification of MC-TAG, the sole extension needed concerns
node variables that are not dominated by any other node variable in the tree description.
Whereas for FB-LTAG, these are taken to denote either the same root node or nodes that
are connected to some other node (i.e., uniqueness of the root), for MC-TAG they can
be treated as distinct nodes, thereby allowing for models that are sets of trees rather
than trees (Parmentier et al 2007). In other words, the only modification brought to the
tree description solver is that, in MC-TAG mode, it does not enforce the uniqueness of
a root node in a model.
IG polarized tree descriptions. IG (Perrier 2000) consist of tree descriptions whose node
variables are labeled with polarized feature structures. A polarized feature structure is
a set of polarized feature triples (f, p, v) where f and v are standard features and feature
values, respectively, and p is a polarity value in {?,?,=,?}. Polarities are used to
guide parsing in that a valid derivation structure must neutralize polarities.
To support an XMG encoding of IG, two extensions are introduced, namely, (i) the
ability to output tree descriptions rather than trees, and (ii) the ability to write polarized
feature structures. The first extension is trivially realized by specifying a description
solver that ensures that any output description has at least one tree model. For the
second point, the SYN language is extended to define polarized feature structures and
the unification engine to support unification of polarized features (for instance, a ?
feature will unify with a neutral (=) feature to yield a ? polarized feature value triple).
5.3.2 Adding Specific Linguistic Constraints: The Case of Clitics. XMG can be extended
to support specific constraints on tree descriptions (e.g., constraints on node linear
order), which make it possible to describe linguistic-dependent phenomena, such as,
for instance, clitic ordering in French, at a meta-level (i.e., within the metagrammar).
According to Perlmutter (1970), clitics are subject to two hard constraints. First,
they appear in front of the verb in a fixed order according to their rank (Exam-
ples 25a and 25b).16 Second, two different clitics in front of the verb cannot have the
same rank (Example 25c).
(25) a. Jean le3 lui4 donne.
?John gives it to him.?
b. *Jean lui4 le3 donne.
*?John gives to him it.?
c. *Jean le3 la3 donne.
*?John gives it it.?
16 In (Examples 25a?c), the numbers on the clitics indicate their rank.
609
Computational Linguistics Volume 39, Number 3
S
N? ?+ V?
?
V?
Cl?3 ?+ V
?
V?
Cl?4 ?+ V
?
S
V?
V
?
S
N? V?
Cl?3 Cl?4 V
S
N? V?
Cl?4 Cl?3 V
Figure 7
Clitic ordering in French.
To support a direct encoding of Perlmutter?s observation, XMG includes both a
node uniqueness principle and a node ordering principle. The latter allows us to label
nodes with some property (let us call it rank) whose value is an integer (for instance,
one can define a node as n1(rank : 2)[cat : Cl]). When solving tree descriptions, XMG
further requires that in a valid tree model, (i) there are no two nodes with the same
rank and (ii) sibling nodes labeled with a rank are linearly ordered according to their
rank.
Accordingly, in the French grammar of Crabbe? (2005), each node labeled with a clitic
category is also labeled with a numerical node property representing its rank.17 XMG
ordering principle then ensures that the ill-formed tree crossed out in Figure 7 is not
produced. Note that in Figure 7, every type of clitic is defined locally (i.e., in a separate
class), and that the interactions between these local definitions are handled by XMG
using this rank principle, to produce only one valid description (pictured to the right of
the arrow).
That is, XMG ordering constraints permit a simple, declarative encoding of the
interaction between clitics. This again contrasts with systems based on lexical rules. As
noted by Perlmutter (1970), if clitics are assumed to be moved by transformations, then
the order in which lexical rules apply this movement must be specified.
To implement the uniqueness principle, one needs to express the fact that in a valid
model ?, there is only one node having a given property p (i.e., a parameter of the
constraint, here the value of the rank node property). This can be done by introducing,
for each node n of the description, a Boolean variable pn indicating whether the node
denoting n in the model has this property or not (i.e., are there two nodes of identical
rank?). Then, if we call V?p the set of integers referring to nodes having the property p in
a model, we have: pn ? (Eqn ? V
?
p ) = ?. Finally, if we represent pn being true with 1 and
pn being false with 0,18 and we sum pn for each n in the model, we have that in a valid
model this sum is strictly lower than 2:
?
n?? pn < 2.
To implement the ordering principle, one needs to express the fact that in a valid
model ?, two sibling nodes n1 and n2 having a given property p of type integer and
of values p1 and p2, respectively, are such that the linear precedence between these
nodes conform to the natural order between p1 and p2. This can be done by first
introducing, for each pair of nodes n, m of the description, a Boolean variable bn,m
indicating whether they have the same ancestors: bn,m ? (Upn ? Upm) = (Upn ? Upm).
For each pair of nodes that do so, we check whether they both have the property p,
17 Recall that node properties are features whose values are used by the tree description solver in order to
restrict the set of valid models. These properties may not appear in the trees produced from the input
metagrammar. For instance, the rank property is not part of the FB-LTAG formalism, and thus does not
appear in the FB-LTAG elementary trees produced by XMG.
18 These integer representations are usually called reified constraints.
610
Crabbe? et al XMG: eXtensible MetaGrammar
and if this is the case, we add to the input description a strict precedence constraint on
these nodes according to their respective values of the property p:19
bn,m ? (pn < pm) ? n ?+ m (26)
bn,m ? (pm < pn) ? m ?+ n (27)
5.3.3 Adding Color Constraints to Facilitate Grammar Writing. To further ease grammar
development, XMG supports a node coloring mechanism that permits nameless node
identification (Crabbe? and Duchier 2004), reminiscent of the polarity-based node iden-
tification first proposed by Muskens and Krahmer (1998) and later used by Duchier
and Thater (1999) and Perrier (2000). Such a mechanism offers an alternative to explicit
node identification using equations between node variables. The idea is to label node
variables with a color property, whose value (either red, black, or white) can trigger
node identifications.
This mechanism is another parameter of the tree solver. When in use, the valid
tree models must satisfy some color constraints, namely, they must only have red or
black nodes (no remaining white nodes; these have to be identified with some black
nodes). As shown in the following table, node identification must observe the following
constraints: A white node must be identified with a black node; a red node cannot be
identified with any other node; and a black node may be identified with one or more
white nodes.20
?B ?R ?W ?
?B ? ? ?B ?
?R ? ? ? ?
?W ?B ? ?W ?
? ? ? ? ?
We now briefly describe how the constraint solver sketched in Section 5.3.1 was
extended to support colors. As mentioned previously, in valid models all white nodes
are identified with a black node (at most one black node per white node). Consequently,
there is a bijection from the red and black nodes of the tree description to the nodes of
the model. In order to take this bijection into account, we add a node variable RBn to
the five sets already associated with a node variable n from Section 5.1. RBn denotes
either n if n is a black or red node, or the black node identified with n if n is a white
node. Note that all the node variables must be colored: the set of node variables in a
tree description can then be partitioned into three sets: Red, Black, and White. Basically,
we know that, for all nodes n, RBn ? Eqn (this is what the bijection is about). Again
we translate color information into constraints on node sets (these constraints help the
generic tree solver by reducing the ambiguity for the Eqn sets):
n ? Red ? (n = RBn) ? (Eqn = {n}) (28)
n ? Black ? (n = RBn) ? (Eqn\{n} ? White) (29)
n ? White ? (RBn ? Black) ? (Eqn ? Black = {RBn}) (30)
19 In fact, rather than adding strict precedence constraints to the tree description, we directly add to the
solver their equivalent set constraints on Eq, Up, Left, Right, Down, introduced earlier.
20 In other words, node colors can be seen as information on node saturation.
611
Computational Linguistics Volume 39, Number 3
Node coloring offers an alternative to complex namespace management. The main
advantage of this particular identification mechanism is its economy: Not only is there
no longer any need to remember node identifiers, there is in fact no need to choose a
name for node variables.
It is worth stressing that the XMG node identification process is reduced to a
constraint-solving problem and so it is not a sequential process. Thus the criticisms
leveled by Cohen-Sygal and Wintner (2007, 2009) against non-associative constraints
on node unification do not apply.
Briefly, in their work, Cohen-Sygal and Wintner (2007, 2009) showed that any
polarity-based tree description formalism is not associative. In other words, when
describing trees in terms of combinations of polarized structures, the order in which
the structures are combined matters (i.e., the output structures depend on the combi-
nation order). This feature makes such formalisms not appropriate for a modular and
collaborative grammar engineering, such as that of Cohen-Sygal and Wintner (2011) for
Unification Grammar.
In the XMG case, when using node colors, the tree description solver does not
rely on any specific fragment combination order. It computes all possible combination
orders. In this context, the grammar designer cannot think in terms of sequences of node
identifications. This would lead to tree overgeneration.
Again, it is important to remember that tree solving computes any valid tree model,
independently of any specific sequence of node identifications (all valid node identifica-
tions are computed). In this context, non-associativity of color-based node identification
is not an issue, but rather a feature, as it allows for a compact description of a large
number of node identifications (and thus of tree structures).
6. Writing Grammars with XMG
In this section, we first provide a detailed example showing how XMG can be used to
specify the verbal trees of a large FB-LTAG for French extended with unification-based
semantics. We then give a brief description of several large- and middle-scale grammars
that were implemented using XMG.
6.1 SEMTAG: A large FB-LTAG for French Covering Syntax and Semantics
We now outline the XMG specification for the verbal trees of SEMTAG, a large FB-LTAG
for French. This specification further illustrates how the various features of XMG (e.g.,
combined use of disjunction and conjunction, node colors) permit us to specify compact
and declarative grammar descriptions. We first discuss the syntactic dimension (SYN).
We then go on to show how the semantic dimension (SEM) and the syntax/semantic
interface (DYN) are specified.
6.1.1 The Syntactic Dimension. The methodology used to implement the verbal fragment
of SEMTAG can be summarized as follows. First, tree fragments are defined that rep-
resent either a possible realization of a verb argument or a possible realization of the
verb. The verbal elementary TAG trees of SEMTAG are then defined by appropriately
combining these tree fragments.
To maximize structure sharing, we work with four levels of abstraction. First, basic
tree fragments describing verb or verb argument realizations are defined. Second, gram-
matical functions are defined as disjunctions of argument realizations. Third, verbal
diathesis alternatives are defined as conjunctions of verb realizations and grammatical
612
Crabbe? et al XMG: eXtensible MetaGrammar
CanonSubj ?
S?W
N??R V?W CanonObj ?
S?W
V?W N??R
CanonIndirObj ?
S?W
V?W PP?R
P?R
a`?R
N??R
CanonByObj ?
S?W
V?W PP?R
P?R
par?R
N??R
RelatSubj ?
N?R
N?R S?W
N??R V?W WhObj ?
S?R
N??R S?W
V?W
WhByObj ?
S?R
PP?R
P?R
par?R
N??R
S?W
WhIndirObj ?
S?R
PP?R
P?R
a`?R
N??R
S?W
ActiveVerbForm?
S?B
V?B PassiveVerbForm?
S?B
V?B
V??B V?B
Figure 8
Elementary tree fragments used as building blocks of the grammar (nodes are colored to control
their identification when blocks are combined).
functions. Fourth, diathesis alternatives are gathered into tree families. In the next
paragraphs, we explain each of these levels in more detail.
Tree fragments. Tree fragments are the basic building blocks used to define SEMTAG.
These are the units that are shared and reused in the definition of many elementary
trees. For instance, the fragment for a canonical subject will be used by all FB-LTAG
elementary trees involving a canonical subject.
As mentioned earlier, to specify the verbal elementary trees of SEMTAG, we begin
by defining tree fragments which describe the possible syntactic realizations of the verb
arguments and of the verb itself. Figure 8 provides some illustrative examples of these
fragments. Here and in the following, we omit the feature structures decorating the trees
to facilitate reading.21
To further factorize information and facilitate grammar maintenance, the basic tree
fragments are organized in an inheritance hierarchy.22 Figure 9 shows a partial view of
21 See Crabbe? (2005) for a complete description of SEMTAG tree fragments, including feature structures.
22 Recall from Section 4 that inheritance is used to share namespaces. Thus, (node or feature) variables
introduced in a given class C can be directly reused in the sub-classes of C.
613
Computational Linguistics Volume 39, Number 3
VerbalArgument
CanonSubj CanonCompl
CanonObj CanPP
CanonIndirObj CanonByObj
Wh
WhObj WhPP
WhIndirObj WhByObj
RelatSubj
Figure 9
Organization of elementary fragments in an inheritance hierarchy.
this hierarchy illustrating how the tree fragments for argument realization depicted in
Figure 8 are organized to maximize the sharing of common information. The hierarchy
classifies the verbal arguments depicted in Figure 8 into four categories:
1. The canonical subject is a noun realized in front of the verb.
2. Canonical complements occur after the verb. The canonical object is a
noun phrase whereas prepositional complements are introduced by
specific prepositions, namely, a` for the canonical indirect object and
par for the canonical by object.
3. Wh-arguments (or questioned arguments) occur in front of a sentence
headed by a verb. A Wh-object is an extracted noun whereas questioned
prepositional objects are extracted prepositional phrases that are
introduced by a specific preposition.
4. Finally, the relativized subject is a relative pronoun realized in front
of the sentence. Extracted subjects in French cannot be realized at an
unbounded distance from the predicate.
Syntactic functions. The second level of abstraction uses syntactic function names such
as Subject and Object to group together alternative ways in which a given syntactic
function can be realized. For instance, if we make the simplifying assumption that the
possible argument realizations are limited to those given in Figure 8, the Subject, Object,
ByObject, and IndirectObject classes would be defined as follows.23
Subject ? CanonSubj ? RelatSubj (31)
Object ? CanonObj ? WhObj (32)
ByObject ? CanonByObj ? WhByObj (33)
IndirectObject ? CanonIndirObj ? WhIndirObj (34)
That is, we define the Subject class as an abstraction for talking about the set of tree
fragments that represent the possible realizations of a subject argument?namely, in
23 Note that, when these abstractions will be combined to describe for instance transitive verbs, the
combination of WhObj with WhByObj will be ruled out by using a uniqueness principle such as
introduced in Section 5.
614
Crabbe? et al XMG: eXtensible MetaGrammar
our restricted example, canonical and relativized subject. Thus, the simplified Subject
class defined in Equation (31) characterizes contexts such as the following:
(35) a. Jean mange. (canonical subject)
?John eats.?
b. Le garc?on qui mange (relativized subject)
?The boy who eats?
Similarly, the IndirectObject class abstracts over the realization of an argument intro-
duced by the preposition a` to the right of the verb (CanonIndirObj) or realized in
extracted position (possibly realized at an unbounded distance from the predicate) as
illustrated by the following examples:
(36) a. Jean parle a` Marie. (canonical indirect object)
?John talks to Mary.?
b. A` qui Jean parle-t-il ? (wh indirect object)
?To whom is John talking ??
c. A` qui Pierre croit-il que Jean parle ? (wh indirect object)
?To whom Peter thinks that John talks ??
This way of grouping tree fragments is reminiscent of the informal classification of
French syntactic functions presented by Iordanskaja and Mel?c?uk (2009) whereby each
syntactic function is associated with a set of possible syntactic constructions.
Diathesis alternations. In this third level, we take advantage of the abstractions defined
in the previous level to represent diathesis alternations. Again, we are interested here
in describing alternatives. Diathesis alternations are those alternations of mapping
between arguments and syntactic functions such as for instance the active/passive
alternation. In a diathesis alternation, the actual form of the verb constrains the way
predicate arguments are realized in syntax. Thus, in the following example, it is con-
sidered that both Examples (37a) and (37b) are alternative realizations of a predicate
argument structure such as send(John, a letter).
(37) a. Jean envoie une lettre.
?John sends a letter.?
b. Une lettre est envoye?e par Jean.
?A letter is sent by John.?
The active/passive diathesis alternation captures the fact that if the verb is in the
active form, its two arguments are realized by a subject and an object whereas if the
verb is in the passive form, then the arguments consist of a subject and a by-object.
TransitiveDiathesis ? (Subject ? ActiveVerbForm ? Object)
? (Subject ? PassiveVerbForm ? ByObject)
(38)
Finally a traditional case of ?erasing,?24 such as the agentless passive (or passive
without agent) can be expressed in our language by adding an additional alternative
24 It is often argued that a language of grammatical representation must be equipped with an ?erasing
device? like lexical rules because of phenomena such as the passive without agent. In this framework it
turns out that this kind of device is not needed because we do not grant any special status to base trees.
615
Computational Linguistics Volume 39, Number 3
where the by-object or agentive complement is not expressed. Thus Equation (39) is an
augmentation of (38) where we have added the agentless passive alternative (indicated
in boldface).
TransitiveDiathesis ? (Subject ? ActiveVerbForm ? Object)
? (Subject ? PassiveVerbForm ? ByObject)
? (Subject ? PassiveVerbForm)
(39)
This methodology can be further augmented to implement an actual linking in the
manner of Bresnan and Zaenen (1990). For the so-called erasing cases, one can map the
?erased? predicative argument to an empty realization in syntax. We refer the reader to
Crabbe? (2005) for further details.
Tree families. Finally, tree families are defined?that is, sets of trees capturing alternative
realizations of a given verb type (i.e., sub-categorization frame). Continuing with the
simplified example presented so far, we can for instance define the tree family for
verbs taking a nominal subject, a nominal object, and an indirect nominal object (i.e.,
ditransitive verbs) as follows:
DitransitiveFamily ? TransitiveDiathesis ? IndirectObject (40)
The trees generated for such a family will, among others, handle the following
contexts:25
(41) a. Jean offre des fleurs a` Marie.
?John offers flowers to Mary.?
b. A` quelle fille Jean offre-t-il des fleurs ?
?To which girl does John offer flowers ??
c. Le garc?on qui offre des fleurs a` Marie.
?The boy who offers flowers to Mary.?
d. Quelles fleurs le garc?on offre-t-il a` Marie ?
?Which flowers does the boy offer to Mary ??
e. Les fleurs sont offertes par Jean a` Marie.
?The flowers are offered by John to Mary.?
f. Par quel garc?on les fleurs sont-elles offertes a` Marie ?
?By which boy are the flowers offered to Mary ??
It is straightforward to extend the grammar with new families. Thus, for instance,
Equation (42) shows how to define the transitive family (for verbs taking a nominal
subject and a nominal object) and Equation (43), the intransitive one (alternatives of a
verb sub-categorizing for a nominal subject).
TransitiveFamily ? TransitiveDiathesis (42)
IntransitiveFamily ? Subject ? ActiveVerbForm (43)
25 Note that number and gender agreements are dealt with using coreferences between features labeling
syntactic nodes, see Crabbe? (2005).
616
Crabbe? et al XMG: eXtensible MetaGrammar
Similarly, tree families for non-verbal predicates (adjectives, nouns) can be defined
using the abstraction over grammatical functions defined for verbs. For instance, the ex-
amples in (44a?44b) can be captured using the adjectival trees defined in Equations (46)
and (47), respectively, where Subject extends the definition of subject given above with a
Wh-subject, PredAdj combines a subject tree fragment with a tree fragment describing a
predicative adjective, and PredAdjAObj extends a PredAdj tree fragment with a canonical
a`-object.
(44) a. Jean est attentif. Qui est attentif ? L?homme qui est attentif
?John is mindful. Who is mindful ? The man who is mindful?
b. Jean est attentif a` Marie. Qui est attentif a` Marie ? L?homme qui est attentif a`
Marie
?John is mindful of Mary. Who is mindful of Mary ? The man who is mindful
of Mary?
Subject ? CanonSubj ? RelatSubj ? WhSubj (45)
PredAdj ? Subject ? AdjectivalForm (46)
PredAdjAObj ? PredAdj ? CanonAObj (47)
6.1.2 The Semantic Dimension and the Syntax/Semantic Interface. We now show how to
extend the XMG specification presented in the previous section to integrate a
unification-based compositional semantics. Three main changes need to be carried out:
1. Each elementary tree must be associated with a semantic formula. This is
done using the SEM dimension.
2. The nodes of elementary trees must be labeled with the appropriate
semantic indices. This involves introducing the correct attribute-value pair
in the correct feature structure (top or bottom) on the appropriate node.
3. Syntax and semantics need to be synchronized?that is, variable sharing
between semantic formulae and tree indices need to be enforced. To this
end we use the DYN dimension.
Informing the semantic dimension. To associate each elementary tree with a formula rep-
resenting the meaning of the words potentially anchoring that tree, we use the SEM
dimension to specify a semantic schema. For instance, the TransitiveFamily class defined
in Equation (42) for verbs taking two nominal arguments is extended as follows:
TransitiveFamily ? TransitiveDiathesis ? BinaryRel (48)
where TransitiveDiathesis is the XMG class defined in Equation (39) to describe the set of
trees associated with transitive verbs and BinaryRel the class describing the following
semantic schema:
L : P(E) ? L : Theta1(E, X) ? L : Theta2(E, Y) (49)
In this semantic schema, P, Theta1, and Theta2 are unification variables that become
ground when the tree is anchored with a specific word. For instance, P, Theta1, and
Theta2 are instantiated to eat, agent, and patient, respectively, when the anchor is ate (these
617
Computational Linguistics Volume 39, Number 3
pieces of information?predicate, thematic roles?are associated with lemmas, located
in the syntactic lexicon, and unified with adequate semantic variables via anchoring
equations). Further, X, Y, E, L are unification variables representing semantic arguments.
As illustrated in Figure 3, these become ground during (or after) derivation as a side
effect of the substitutions and adjunctions taking place when trees are combined. It
is worth noting that by combining semantic schemas with diathesis classes, one such
specification assigns the specified semantic schema to many trees, namely, all the trees
described by the corresponding diathesis class. In this way, the assignment of semantic
formulae to trees is relatively economical. Indeed in SEMTAG, roughly 6,000 trees are
assigned a semantic schema using a total of 75 schema calls.
Co-indexing trees and formulae indices. Assuming that tree nodes are appropriately deco-
rated with semantic indices by the specification scheme described in the next paragraph,
we now show how to enforce the correct mapping between syntactic and semantic
arguments. This is done in two steps.
First, we define a set of interface constraints of the form ?indexF : V, argi : V? which
are used to enforce the identification of the semantic index (indexF) labeling a given tree
node with grammatical function F (e.g., F := subject) with the index (argi) representing
the i-th argument in a semantic schema. For instance, the following constraints ensure
a subject/arg1 mapping, that is, a coreference between the index labeling a subject node
and the index representing the first argument of a semantic schema:
C1 ? Node [idx : I] ? ?indexsubject : I?
C2 ? L : P(E) ? L : Theta1(E, X) ? ?arg1 : X?
SubjectArg1 ? C1 ? C2 ? ?indexsubject : V, arg1 : V?
(50)
Given such interface constraints, we refine the diathesis definitions so as to ensure the
correct bindings. For instance, the specification in Equation (38) is modified to:
TransitiveDiathesis ? TransitiveActive ? TransitivePassive
TransitiveActive ? (SubjectArg1 ? ObjectArg2?
Subject ? ActiveVerbForm ? Object)
(51)
and the passive diathesis is specified as:
TransitivePassive ? (SubjectArg2 ? ByObjectArg1?
Subject ? PassiveVerbForm ? ByObject)
(52)
Labeling tree nodes with semantic indices. This scheme relies on the assumption that tree
nodes are appropriately labeled with semantic indices (e.g., the subject node must be
labeled with a semantic index) and that these indices are appropriately named (arg1
must denote the parameter representing the first argument of a binary relation and
indexsubject the value of the index feature on a subject node). As suggested by Gardent
(2007), a complete semantic labeling of a TAG with the semantic features necessary
618
Crabbe? et al XMG: eXtensible MetaGrammar
to enrich this TAG with the unification-based compositional semantics sketched in the
previous section can be obtained by applying the following labeling principles:26
Argument labeling: In trees associated with semantic functors, each argument node
is labeled with a semantic index27 named after the grammatical function of the
argument node (e.g., indexsubject for a subject node).
Controller/Controllee: In trees associated with control verbs, the semantic index of the
controller is identified with the value of the controlled index occurring on the
sentential argument node.
Anchor projection: The anchor node projects its index up to its maximal projection.
Foot projection: A foot node projects its index up to the root.28
As we shall now see, XMG permits a fairly direct encoding of these principles.
The Argument Labeling principle states that, in the tree associated with a syntactic
functor (e.g., a verb), each node representing a syntactic argument (e.g., the subject
node) should be labeled with a semantic index named after the grammatical function of
that node (e.g., indexsubject).29
To specify this labeling, we define for each grammatical function Function ?
{Subject, Object, ByObject, IndirectObject, . . . }, a semantic class FunctionSem which as-
sociates with an (exported) node variable called FunctionNode the feature value pair
[index : I] and a DYN constraint of the form ?indexFunction : I?. For instance, the class
SubjectSem associates the node SubjectNode with the feature value pair [index : I] and
the DYN constraint ?indexsubject : I?.
SubjectSem ? SubjectNode [index : I] ? ?indexsubject : I? (53)
Additionally, in the tree fragments describing the possible realizations of the grammat-
ical functions, the (exported) variable denoting the argument node is systematically
named ArgNode.
Finally, we modify the specification of the realizations of the grammatical functions
to import the appropriate semantic class and identify ArgNode and FunctionNode. For
instance, the Subject specification given above is changed to:
Subject ? SubjectSem ? ArgNode = SubjectNode ?
(CanonSubj ? RelatSubj ? WhSubj)
(54)
26 The principles required to handle quantification are omitted. We refer the reader to Gardent (2007) for a
more extensive presentation of how semantics is implemented using XMG.
27 For simplicity, we only mention indices. To be complete, however, labels should also be used.
28 The foot projection principle only applies to foot nodes that are not argument nodes (i.e., to modifiee
nodes).
29 In other words, this argument labeling principle defines an explicit and normalized reference to any
realization of a semantic argument. Following FB-LTAG predicate?argument co-occurrence principle
(Abeille?, Candito, and Kinyon 1999), we know that any elementary tree includes a leaf node for each
realized semantic argument of its anchor. This principle thus holds in any FB-LTAG. Its implementation,
however, is closely related to the architecture of the metagrammar; here we benefit from the fact that
verbal arguments are described in dedicated classes to reach a high degree of factorization.
619
Computational Linguistics Volume 39, Number 3
E3
E2
E2
E1
E2
E1
E1
E
E1
E
E1
E
? E? ? E? ? E?
Depth 3 Depth 2 Depth 1
SE2E1
VPE1E
?VE?
ActiveVerbForm
Figure 10
Anchor/Foot projection.
As a result, all ArgNode nodes in the tree descriptions associated with a subject realiza-
tion are labeled with an index feature I whose global name is indexsubject.
Value sharing between the semantic index of the controller (e.g., the subject of
the control verb) and that of the controllee (e.g., the empty subject of the infinitival
complement) is enforced using linking constraints between the semantic index labeling
the controller node and that labeling the sentential argument node of the control verb.
Control verb definitions then import the appropriate (object or subject control) linking
constraint.
The anchor (respectively, foot) projection principle stipulates the projection of
semantic indices from the anchor (respectively, foot) node up to the maximal projection
(respectively, root). Concretely, this means that the top and bottom features of the nodes
located on this path between the anchor (respectively, foot) and the maximal projection
(respectively, root) all include an index feature whose value is shared between adjacent
nodes (see variables Ei in Figure 10).30 Once the top and bottom structures are unified,
so are the semantic indices along this path (modulo expected adjunctions realized on
the projection).
To implement these principles, we define a set of anchor projection classes
{Depth1, Depth2, Depth3} as illustrated in Figure 10. We then ?glue? these projection
skeletons onto the relevant syntactic trees by importing the skeletons in the syntactic
tree description and explicitly identifying the anchor node of the semantic projection
classes with the anchor or foot node of these syntactic tree descriptions. Because the
models must be trees, the nodes dominating the anchor node of the projection class
will deterministically be identified with those dominating the anchor or foot node of
the trees being combined with. For instance, for verbs, the class specifying the verbal
spine (e.g., ActiveVerbForm, see Figure 10) equates the anchor node of the verbal spine
with that of the projection skeleton. As a result, the verb projects its index up to
the root.
6.1.3 Some Figures About SEMTAG. As mentioned previously, SEMTAG is a large FB-LTAG
for French equipped with semantics (Gardent 2008); it extends the purely syntactic
FTAG of Crabbe? (2005) with a unification based compositional semantics as described
by Gardent and Kallmeyer (2003).31 The syntactic FTAG in essence implements Abeille??s
(2002) proposal for an FB-LTAG-based modeling of French syntax. FTAG contains
around 6,000 elementary trees built from 293 XMG classes and covers some 40 basic
30 For sake of brevity, we write E2E1 for [bot : [index : E1] top : [index : E2]]. ? ? refers to the anchor / foot.31 FTAG and SEMTAG are freely available under the terms of the GPL-compliant CeCILL license, the former
at https://sourcesup.renater.fr/scm/viewvc.php/trunk/METAGRAMMARS/FrenchTAG/?root=xmg, and
the latter on request.
620
Crabbe? et al XMG: eXtensible MetaGrammar
verbal sub-categorization frames. For each of these frames, FTAG defines a set of
argument alternations (active, passive, middle, neuter, reflexivization, impersonal,
passive impersonal) and of argument realizations (cliticization, extraction, omission,
permutations, etc.) possible for this frame. Predicative (adjectival, nominal, and
prepositional) and light verb constructions are also covered as well as some common
sub-categorizing noun and adjective constructions. Basic descriptions are provided for
the remaining constructions namely, adverbs, determiners, and prepositions.
FTAG and SEMTAG were both evaluated on the Test Suite for Natural Language Pro-
cessing (TSNLP) (Lehmann et al 1996), using a lexicon designed specifically on the test
suite, hence reducing lexical ambiguity (Crabbe? 2005; Parmentier 2007). This test suite
focuses on difficult syntactical phenomena, providing grammatical and ungrammatical
sentences. These competence grammars accept 76% of the grammatical items, reject 83%
of the ungrammatical items, and have an average ambiguity of 1.64 parses per sentence.
To give an idea of the compilation time, under architectures made of a 2-Ghz processor
with 1 Gb of RAM, it takes XMG 10 minutes to compile the whole SEMTAG (recall that
there is no semantic description solving, hence the compilation times between FTAG
and SEMTAG do not differ).32
Note that SEMTAG can be used for assigning semantic representations to sentences
when combined with an FB-LTAG parser and a semantic construction module as de-
scribed by Gardent and Parmentier (2005, 2007).33 Conversely, it can be used to verbalize
the meaning denoted by a given semantic representation when coupled with the GenI
surface realizer described by Gardent and Kow (2007).
6.2 Other Grammars Designed with XMG
XMG has been used mainly to design FB-LTAG and IG for French or English. More
recently, it has also been used to design a FB-LTAG for Vietnamese and a TreeTuple
MC-TAG for German. We now briefly describe each of these resources.
SemXTAG. The English grammar, SEMXTAG (Alahverdzhieva 2008), reimplements the
FB-LTAG developed for English at the University of Pennsylvania (XTAG Research
Group 2001) and extends it with a unification-based semantics. It contains 1,017 trees
and covers the syntactic fragment of XTAG, namely, auxiliaries, copula, raising and
small clause constructions, topicalization, relative clauses, infinitives, gerunds, pas-
sives, adjuncts, ditransitives (and datives), ergatives, it-clefts, wh-clefts, PRO con-
structions, noun?noun modification, extraposition, determiner sequences, genitives,
negation, noun?verb contractions, sentential adjuncts, imperatives, and resultatives.
The grammar was tested on a handbuilt test-suite of 998 sentences illustrating the
various syntactic constructions meant to be covered by the grammar. All sentences in
the test suite can be parsed using the grammar.
FrenchIG. The extended XMG framework was used to design a core IG for French
consisting of 2,059 tree descriptions compiled out of 448 classes (Perrier 2007). The
resulting grammar is lexicalized, and its coverage was evaluated using the previously
mentioned TSNLP. The French IG accepts 88% of the grammatical sentences and rejects
32 As a comparison, about one hour was needed by Candito?s (1999) compiler to produce a French FB-LTAG
containing about 1,000 tree schemas.
33 As an alternative way to parse FB-LTAG grammars equipped with flat semantics such as those produced
by XMG, one can use the Tu?bingen Linguistic Parsing Architecture (TuLiPA) (Kallmeyer et al 2010).
621
Computational Linguistics Volume 39, Number 3
85% of the ungrammatical sentences, although the current version of the French IG
does not yet cover all the syntactic phenomena presented in the test suite (for example,
causative and superlative constructions).
Vietnamese TAG. The XMG language was used by Le Hong, N?Guyen, and Roussanaly
(2008) to produce a core FB-LTAG for Vietnamese. Their work is rather a proof of con-
cept than a large-scale implementation. They focused on Vietnamese?s categorization
frames, and were able to produce a TAG covering the following frames: intransitive
(tree family N0V), transitive with a nominal complement (N0VN1), transitive with a
clausal complement (N0VS1), transitive with modal complement (N0V0V1), ditransi-
tive (N0VN1N2), ditransitive with a preposition (N0VN1ON2), ditransitive with a ver-
bal complement (N0V0N1V1), ditransitive with an adjectival complement (N0VN1A),
movement verbs with a nominal complement (N0V0V1N1), movement verbs with an
adjectival complement (N0V0AV1), and movement ditransitive (N0V0N1V1N2).
GerTT. Another XMG-based grammar corresponds to the German MC-TAG of
Kallmeyer et al (2008). This grammar, called GerTT, is in fact an MC-TAG with
Tree Tuples (Lichte 2007). This variant of MCTAG has been designed to model free
word order phenomena. This is done by imposing node sharing constraints on MCTAG
derivations (Kallmeyer 2005). GerTT covers phenomena such as scrambling, coherent
constructions, relative clauses, embedded questions, copula verbs, complementized
sentences, verbs with various sub-categorization frames, nouns, prepositions, determin-
ers, adjectives, and partly includes semantics. It is made of 103 tree tuples, compiled
from 109 classes.
7. Related Work
We now compare XMG with existing environments for designing tree-based grammars
and briefly report on the grammars designed with these systems.
7.1 Environments for Designing Tree-Based Grammars
Candito?s Metagrammar Compiler. The concept of metagrammar was introduced by
Candito (1996). In her paper, Candito presented a compiler for abstract specifications
of FB-LTAG trees (the so-called metagrammars). Such specifications are based on three
dimensions, each of them being encoded in a separate inheritance hierarchy of linguistic
descriptions. Dimension 1 describes canonical sub-categorization frames (e.g., transitive),
the Dimension 2 describes redistributions of syntactic functions (e.g., active to passive),
and Dimension 3 the tree descriptions corresponding to the realizations of the syntactic
functions defined in Dimension 2. This three-dimensional metagrammatical description
is then processed by a compiler to compute FB-LTAG tree schemas. In essence, these
tree schemas are produced by associating a canonical sub-categorization frame (Dimen-
sion 1) with a compatible redistribution schema (Dimension 2), and with exactly one
function realization (Dimension 3) for each function required by the sub-categorization
frame.
Candito?s (1996, 1999) approach improves on previous proposals by Vijay-Shanker
and Schabes (1992) and Evans, Gazdar, and Weir (1995) in that it provides a linguistically
principled basis for structuring the inheritance hierarchy. As shown in Section 6.1,
622
Crabbe? et al XMG: eXtensible MetaGrammar
the XMG definition of SEMTAG uses similar principles. Candito?s approach differs,
however, from the XMG account in several important ways:
 Much of the linguistic knowledge used to determine which classes to
combine is hard-coded in the compiler (unlike in XMG, there is no explicit
control on class combinations). In other words, there is no clear separation
between the linguistic knowledge needed to specify a high-level FB-LTAG
description and the algorithm used to compile an actual FB-LTAG from
this description. This makes grammar extension and maintenance by
linguists extremely difficult.
 As in Vijay-Shanker and Schabes (1992) Evans, Gazdar, and Weir (1995),
the linguistic description is non-monotonic in that some erasing classes
are used to remove information introduced by other dimensions
(e.g., agentless passive).
 The approach fails to provide an easy means to state exceptions. These
are usually encoded in the compiling algorithm.
 The tree description language used to specify classes in Dimension 3
relies on global node variables. Thus, two variables with identical names
introduced in different classes are expected to refer to the same tree node.
As argued in Section 4, this makes it hard to design large-scale
metagrammars.
The LexOrg system. An approach similar to Candito?s was presented by Xia et al
(1998), Xia (2001), and Xia, Palmer, and Vijay-Shanker (2005, 2010). As in Candito?s
approach, a TAG abstract specification relies on a three-dimensional description made
of, namely, sub-categorization frames, blocks, and lexical redistribution rules. To com-
pile this specification into a TAG, the system selects a canonical sub-categorization
frame, and applies some lexical redistribution rules to derive new frames and finally
select blocks corresponding to the resulting frames. These blocks contain tree descrip-
tions using the logic of Rogers and Vijay-Shanker (1994).
LexOrg suffers from similar limitations as Candito?s compiler. Much of the lin-
guistic knowledge is embedded in the compiling algorithm, making it difficult for
linguists to extend the grammar description and to handle exceptions. Unlike in Can-
dito?s framework, the tree description language uses local node variables and lets the
tree description solver determine node identifications. Although this avoids having to
memorize node names, this requires that the descriptions be constrained enough to
impose the required node identifications and prevent the unwanted ones. In practice,
this again complicates grammar writing. In contrast, XMG provides an intermediate
solution which, by combining local variables with export declarations, avoids having to
memorize too many node variable names (only those local to the relevant sub-hierarchy
need memorizing) while allowing for explicit node identification.
The Metagrammar Compiler of Gaiffe, Crabbe?, and Roussanaly. Gaiffe, Crabbe?, and
Roussanaly (2002) proposed a compiler for FB-LTAG that aims to remedy both the lack
of a clear separation between linguistic information and compilation algorithm, and
the lack of explicit control on the class combinations prevalent in Candito (1996), Xia
et al (1998), and Xia (2001). In their approach, the linguistic specification consists of
a single inheritance hierarchy of classes, each class containing a tree description. The
623
Computational Linguistics Volume 39, Number 3
description logic used is similar to Candito?s. That is, global node names are used. To
trigger class combinations, classes are labeled with two types of information: needs and
resources. The compiler selects all final classes of the hierarchy, performs all possible
combinations, and only keeps those combinations that neutralize the stated needs
and resources. The tree descriptions contained in these neutral combinations are then
solved to produce the expected trees.
Although this approach implements a clear separation between linguistic informa-
tion and compilation algorithm, the fully automatic derivation of FB-LTAG trees from
the inheritance hierarchy makes it difficult in practice to control overgeneration. In
contrast, XMG?s explicit definitions of class combinations by conjunction, disjunction,
and inheritance makes it easier to control the tree set that will be generated by the
compiler from the grammar specification. Additionally, the issues raised by global
variables remain (no way to instantiate twice a given class, and cumbersome definition
of variables in large metagrammars).
The MGCOMP System. More recently, Villemonte de la Clergerie (2005, 2010) proposed a
compiler for FB-LTAG that aims at preserving a high degree of factorization in both the
abstract grammar specification and the grammar which is compiled from it. Thus, the
MGCOMP system does not compute FB-LTAG elementary trees, but factorized trees.
In MGCOMP, like in Gaiffe, Crabbe?, and Roussanaly?s (2002) approach, a meta-
grammar consists of a single hierarchy of classes. The classes are labeled with needs and
resources, and final classes of the hierarchy are combined to compute tree descriptions.
The main differences with Gaiffe, Crabbe?, and Roussanaly (2002), lies in the fact that
(i) a description can include new factorizing operators, such as repetition (Kleene-star
operator), shuffling (interleaving of nodes), optionality, and disjunctions; and (ii) it offers
namespaces to specify the scope of variables. MGCOMP?s extended tree descriptions
are not completely solved by the compiler. Rather, it compiles underspecified trees (also
called factorized trees). With this approach, a large grammar is much smaller in terms of
number of grammatical structures than a classical FB-LTAG. As a result, the grammars it
compiles are only compatible with the DyALog parsing environment (Villemonte de La
Clergerie 2005). And, because the linguist designs factorized trees and not actual TAG
trees, debugging the metagrammar becomes harder.
7.2 Resources Built Using Candito, Xia, and De La Clergerie?s Systems
Candito?s system has been used by Candito (1999) herself to design a core FB-LTAG
for French and Italian, and later by Barrier (2006) to design a FB-LTAG for adjectives
in French. Xia?s system (LexOrg) has been used to semi-automatically generate XTAG
(Xia 2001). De La Clergerie?s system (MGCOMP) has been used to design a grammar
for French named FRMG (FRench MetaGrammar) (Villemonte de la Clergerie 2010).
FRMG makes use of MGCOMP?s factorizing operators (e.g., shuffling operator), thus
producing not sensu stricto a FB-LTAG, but a factorized FB-LTAG. FRMG is freely
available, contains 207 factorized trees (having optional branches, etc.) built from 279
metagrammatical classes, and covers 95% of the TSNLP.
8. Conclusion
In this article, we presented the eXtensible MetaGrammar framework and argued that,
contrary to other existing grammar writing environments for tree-based grammar,
624
Crabbe? et al XMG: eXtensible MetaGrammar
XMG is declarative, extensible, and notationally expressive. We believe that these fea-
tures make XMG particularly appropriate for a fast prototyping of the kind of deep
tree-based grammars that are used in applications requiring high precision in gram-
mar modeling (e.g., language teaching, man/machine dialogue systems, data-to-text
generation).
The XMG language is documented on-line, and its compiler is open source soft-
ware, freely available under the terms of the GPL-compliant CeCILL license.34 Many
grammars designed with XMG (FB-LTAG and IG for French and English, TT-MCTAG
for German) are also open-source and available on-line.35
Future research will focus on extensibility. So far, XMG has been used to design tree-
based grammars for different languages. We plan to extend XMG to handle other types
of formalisms36 such as dependency grammars, and to support dimensions other than
syntax and semantics such as for instance, phonology or morphology. As mentioned
here, XMG offers a modular architecture, making it possible to extend it relatively easily.
Nonetheless, in its current state, such extensions imply modifying XMG?s code. We are
exploring new extensions of the formalism, which would allow the linguist to dynam-
ically define her/his metagrammar formalism (e.g., which principles or descriptions to
use) depending on the target formalism.
Another interesting question concerns cross-language grammar engineering. So far,
the metagrammar allows for dealing with structural redundancy. As pointed out by
Kinyon et al (2006), a metagrammar can be used to capture generalizations across
languages and is surely worth further investigating.
Finally, we plan to extend XMG with features borrowed from Integrated De-
velopment Environments (IDE) for programming languages. Designing a grammar
is, in some respect, similar to programming an application. Grammar environments
should benefit from the same tools as those used for the development of applications
(incremental compilation, debugger, etc.).
Acknowledgments
We are grateful to the three anonymous
reviewers for their valuable comments.
Any remaining errors are ours.
References
Abeille?, A. 2002. Une grammaire e?lectronique
du franc?ais. CNRS Editions.
Abeille?, A., M. Candito, and A. Kinyon. 1999.
Ftag: current status and parsing scheme.
In Proceedings of Vextal ?99, pages 283?292,
Venice.
A??t-Kaci, Hassan. 1991. Warren?s Abstract
Machine: A Tutorial Reconstruction. MIT
Press, Cambridge, MA.
Alahverdzhieva, Katya. 2008. XTAG using
XMG. Masters thesis, Nancy Universite?.
Baldridge, Jason, Sudipta Chatterjee,
Alexis Palmer, and Ben Wing. 2007.
DotCCG and VisCCG: Wiki and
programming paradigms for improved
grammar engineering with OpenCCG.
In Tracy Holloway King and Emily M.
Bender, editors, Proceedings of the Grammar
Engineering Across Framework Workshop
(GEAF 07). CSLI, Stanford, CA, pages 5?25.
Barrier, Se?bastien. 2006. Une me?tagrammaire
pour les noms pre?dicatifs du franc?ais :
de?veloppement et expe?rimentations pour les
grammaires TAG. Ph.D. thesis, Universite?
Paris 7.
Becker, Tilman. 1993. HyTAG: A New Type
of Tree Adjoining Grammars for Hybrid
Syntactic Representation of Free Word Order
Language. Ph.D. thesis, Universita?t des
Saarlandes.
34 See https://sourcesup.renater.fr/xmg.
35 The French TAG and French and English IG are available on XMG?s website, and the German TreeTuple
MC-TAG is available at http://www.sfs.uni-tuebingen.de/emmy/res.html.
36 Preliminary work on cross-framework grammar engineering has been realized by Cle?ment and Kinyon
(2003), who used Gaiffe et al?s compiler to produce both a TAG and a LFG from a given metagrammar.
625
Computational Linguistics Volume 39, Number 3
Blackburn, Patrick, Johan Bos, and Kristina
Striegnitz. 2006. Learn Prolog Now!,
volume 7 of Texts in Computing. College
Publications, London.
Bresnan, Joan and Annie Zaenen. 1990. Deep
unaccusitivity in LFG. In K. Dziwirek,
P. Farell, and E. Mejias-Bikandi, editors,
Grammatical Relations: A Cross-Theoretical
Perspective. CSLI publications, Stanford,
CA, pages 45?57.
Candito, Marie. 1996. A principle-based
hierarchical representation of LTAGs.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING?96), pages 194?199, Copenhagen.
Candito, Marie. 1999. Repre?sentation modulaire
et parame?trable de grammaires e?lectroniques
lexicalise?es : application au franc?ais et a`
l?italien. Ph.D. thesis, Universite? Paris 7.
Cle?ment, Lionel and Alexandra Kinyon.
2003. Generating parallel multilingual
lfg-tag grammars from a metagrammar.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics,
pages 184?191, Sapporo.
Cohen-Sygal, Yael and Shuly Wintner. 2007.
The Non-Associativity of Polarized
Tree-Based Grammars. In Proceedings of the
Eighth International Conference on Intelligent
Text Processing and Computational Linguistics
(CICLing-2007), pages 208?217, Mexico City.
Cohen-Sygal, Yael and Shuly Wintner. 2009.
Associative grammar combination operators
for tree-based grammars. Journal of Logic,
Language and Information, 18(3):293?316.
Cohen-Sygal, Yael and Shuly Wintner. 2011.
Towards modular development of typed
unification grammars. Computational
Linguistics, 37(1):29?74.
Copestake, Ann and Dan Flickinger. 2000.
An open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings of the
Second Conference on Language Resources and
Evaluation (LREC-2000), Athens.
Copestake, Ann, Alex Lascarides, and Dan
Flickinger. 2001. An algebra for semantic
construction in constraint-based
grammars. In Proceedings of 39th Annual
Meeting of the Association for Computational
Linguistics, pages 140?147, Toulouse.
Crabbe?, Benoit. 2005. Repre?sentation
informatique de grammaires fortement
lexicalise?es : Application a` la grammaire
d?arbres adjoints. Ph.D. thesis, Universite?
Nancy 2.
Crabbe?, Beno??t and Denys Duchier. 2004.
Metagrammar redux. In Proceedings of the
Workshop on Constraint Solving for Language
Processing (CSLP 2004), pages 32?47,
Copenhagen.
Duchier, Denys, Brunelle Magnana Ekoukou,
Yannick Parmentier, Simon Petitjean, and
Emmanuel Schang. 2012. Describing
morphologically-rich languages using
metagrammars: A look at verbs in Ikota.
In Workshop on ?Language Technology for
Normalisation of Less-resourced Languages,?
8th SALTMIL Workshop on Minority
Languages and 4th Workshop on African
Language Technology, International
Conference on Language Resources and
Evaluation, LREC 2012, pages 55?60,
Istanbul.
Duchier, Denys and Joachim Niehren. 2000.
Dominance constraints with set operators.
In John W. Lloyd, Vero?nica Dahl, Ulrich
Furbach, Manfred Kerber, Kung-Kiu Lau,
Catuscia Palamidessi, Lu??s Moniz Pereira,
Yehoshua Sagiv, and Peter J. Stuckey,
editors, Proceedings of the First International
Conference on Computational Logic,
volume 1861 of Lecture Notes in Computer
Science. Springer, Berlin, pages 326?341.
Duchier, Denys, Yannick Parmentier, and
Simon Petitjean. 2012. Metagrammars as
logic programs. In International Conference
on Logical Aspects of Computational
Linguistics (LACL 2012). Proceedings of
the Demo Session, pages 1?4, Nantes.
Duchier, Denys and Stefan Thater. 1999.
Parsing with tree descriptions: A
constraint-based approach. In Proceedings
of the Sixth International Workshop on
Natural Language Understanding and Logic
Programming (NLULP?99), pages 17?32,
Las Cruces, NM.
Evans, Roger, Gerald Gazdar, and David
Weir. 1995. Encoding lexicalized tree
adjoining grammars with a nonmonotonic
inheritance hierarchy. In Proceedings of the
33rd Annual Meeting of the Association for
Computational Linguistics, pages 77?84,
Cambridge, MA.
Flickinger, Daniel. 1987. Lexical Rules in the
Hierarchical Lexicon. Ph.D. thesis, Stanford
University.
Gaiffe, Bertrand, Beno??t Crabbe?, and Azim
Roussanaly. 2002. A new metagrammar
compiler. In Proceedings of the Sixth
International Workshop on Tree Adjoining
Grammars and Related Frameworks (TAG+6),
pages 101?108, Venice.
Gardent, Claire. 2007. Tree adjoining
grammar, semantic calculi and labelling
invariants. In Proceedings of the International
Workshop on Computational Semantics
(IWCS), Tilburg.
626
Crabbe? et al XMG: eXtensible MetaGrammar
Gardent, Claire. 2008. Integrating a
unification-based semantics in a large
scale lexicalised tree adjoininig grammar
for French. In Proceedings of the 22nd
International Conference on Computational
Linguistics (COLING?08), pages 249?256,
Manchester.
Gardent, Claire and Laura Kallmeyer. 2003.
Semantic construction in feature-based
tree adjoining grammar. In Proceedings of
the 10th Conference of the European Chapter of
the Association for Computational Linguistics,
pages 123?130, Budapest.
Gardent, Claire and Eric Kow. 2007. A
symbolic approach to near-deterministic
surface realisation using tree adjoining
grammar. In 45th Annual Meeting of the
Association for Computational Linguistics,
pages 328?335, Prague.
Gardent, Claire and Yannick Parmentier.
2005. Large scale semantic construction for
tree adjoining grammars. In Proceedings
of the Fifth International Conference
on Logical Aspects of Computational
Linguistics (LACL?05), pages 131?146,
Bordeaux.
Gardent, Claire and Yannick Parmentier.
2006. Coreference Handling in XMG.
In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL
2006) Main Conference Poster Sessions,
pages 247?254, Sydney.
Gardent, Claire and Yannick Parmentier.
2007. SemTAG: A platform for specifying
tree adjoining grammars and performing
TAG-based semantic construction. In
Proceedings of the 45th Annual Meeting
of the Association for Computational
Linguistics Companion Volume Proceedings
of the Demo and Poster Sessions,
pages 13?16, Prague.
Iordanskaja, Lidija and Igor Mel?c?uk, 2009.
Establishing an inventory of surface?
syntactic relations: valence-controlled
surface-dependents of the verb in French.
In A. Polgue`re and I. A. Mel?duk, editors,
Dependency in Linguistic Description.
John Benjamins, Amsterdam,
pages 151?234.
Joshi, Aravind K., Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars.
Journal of Computer and System Sciences,
10(1):136?163.
Kallmeyer, Laura. 1999. Tree Description
Grammars and Underspecified
Representations. Ph.D. thesis,
Universita?t Tu?bingen.
Kallmeyer, Laura. 2005. Tree-local
multicomponent tree-adjoining grammars
with shared nodes. Computational
Linguistics, 31(2):187?226.
Kallmeyer, Laura, Timm Lichte, Wolfgang
Maier, Yannick Parmentier, and Johannes
Dellert. 2008. Developing a TT-MCTAG for
German with an RCG-based parser. In
Proceedings of the Sixth Language Resources
and Evaluation Conference (LREC),
pages 782?789, Marrakech.
Kallmeyer, Laura, Wolfgang Maier, Yannick
Parmentier, and Johannes Dellert. 2010.
TuLiPA?Parsing extensions of TAG with
range concatenation grammars. Bulletin of
the Polish Academy of Sciences: Technical
Sciences, 58(3):377?392.
Kallmeyer, Laura and Maribel Romero.
2004a. LTAG semantics for questions.
In Proceedings of 7th International Workshop
on Tree-Adjoining Grammar and Related
Formalisms (TAG+7), pages 186?193,
Vancouver.
Kallmeyer, Laura and Maribel Romero.
2004b. LTAG semantics with semantic
unification. In Proceedings of 7th
International Workshop on Tree-Adjoining
Grammar and Related Formalisms (TAG+7),
page 155?162, Vancouver.
Kallmeyer, Laura and Maribel Romero.
2008. Scope and situation binding in
LTAG using semantic unification.
Research on Language and Computation,
6(1):3?52.
Kaplan, Ronald and Paula Newman. 1997.
Lexical resource reconciliation in the Xerox
linguistic environment. In Proceedings
of the ACL Workshop on Computational
Environments for Grammar Development
and Linguistic Engineering, pages 54?61,
Madrid.
Kinyon, Alexandra. 2000. Hypertags.
In Proceedings of the 18th International
Conference on Computational Linguistics
(COLING?00), pages 446?452, Saarbru?cken.
Kinyon, Alexandra, Owen Rambow, Tatjana
Scheffler, SinWon Yoon, and Aravind K.
Joshi. 2006. The metagrammar goes
multilingual: A cross-linguistic look at
the v2-phenomenon. In Proceedings of
the Eighth International Workshop on Tree
Adjoining Grammar and Related Formalisms,
pages 17?24, Sydney.
Le Hong, Phuong, Thi-Min-Huyen
N?Guyen, and Azim Roussanaly. 2008.
A metagrammar for Vietnamese. In
Proceedings of the 9th International Workshop
on Tree-Adjoining Grammar and Related
Formalisms (TAG+9), Tu?bingen.
627
Computational Linguistics Volume 39, Number 3
Lehmann, Sabine, Stephan Oepen, Sylvie
Regnier-Prost, Klaus Netter, Veronika Lux,
Judith Klein, Kirsten Falkedal, Frederik
Fouvry, Dominique Estival, Eva Dauphin,
Herve? Compagnion, Judith Baur, Lorna
Balkan, and Doug Arnold. 1996. TSNLP?
Test suites for natural language processing.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING?96), pages 711?716, Copenhagen.
Lichte, Timm. 2007. An MCTAG with tuples
for coherent constructions in German.
In Proceedings of the 12th Conference on
Formal Grammar (FG 2007), 12 pages,
Dublin.
Muskens, Reinhard and Emiel Krahmer.
1998. Description theory, LTAGs and
Underspecified Semantics. In Fourth
International Workshop on Tree Adjoining
Grammars and Related Frameworks,
pages 112?115, Philadelphia, PA.
Parmentier, Yannick. 2007. SemTAG: une
plate-forme pour le calcul se?mantique a` partir
de Grammaires d?Arbres Adjoints. Ph.D.
thesis, Universite? Henri Poincare? - Nancy.
Parmentier, Yannick, Laura Kallmeyer, Timm
Lichte, and Wolfgang Maier. 2007. XMG:
eXtending MetaGrammars to MCTAG. In
Proceedings of the Workshop on High-Level
Syntactic Formalisms, 14th Conference on
Natural Language Processing (TALN?2007),
pages 473?482, Toulouse.
Pereira, Fernando and David Warren. 1980.
Definite clause grammars for language
analysis?A survey of the formalism
and a comparison to augmented
transition networks. Artificial
Intelligence, 13:231?278.
Perlmutter, David. 1970. Surface structure
constraints in syntax. Linguistic Inquiry,
1:187?255.
Perrier, Guy. 2000. Interaction grammars.
In Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2000), pages 600?606,
Saarbru?cken.
Perrier, Guy. 2007. A French interaction
grammar. In Proceedings of the 6th
Conference on Recent Advances in Natural
Language Processing (RANLP 2007),
pages 463?467, Borovets.
Prolo, Carlos A. 2002. Generating the
XTAG English grammar using metarules.
In Proceedings of the 19th International
Conference on Computational Linguistics
(COLING?2002), pages 814?820, Taipei.
Rambow, Owen, K. Vijay-Shanker, and
David Weir. 1995. D-tree grammars.
In Proceedings of the 33th Meeting of the
Association for Computational Linguistics,
pages 151?158, Cambridge, MA.
Rogers, James and K. Vijay-Shanker. 1994.
Obtaining trees from their descriptions:
An application to tree-adjoining
grammars. Computational Intelligence,
10:401?421.
Shieber, Stuart M. 1984. The design of a
computer language for linguistic
information. In Proceedings of the Tenth
International Conference on Computational
Linguistics, pages 362?366, Stanford, CA.
Van Roy, Peter. 1990. Extended DCG
notation: A tool for applicative
programming in prolog. Technical
Report UCB/CSD 90/583, University
of California, Berkeley.
Vijay-Shanker, K. and Aravind K. Joshi.
1988. Feature structures based tree
adjoining grammars. In Proceedings
of the 12th Conference on Computational
Linguistics (COLING?88), pages 714?719,
Budapest.
Vijay-Shanker, K. and Yves Schabes. 1992.
Structure sharing in lexicalized tree
adjoining grammars. In Proceedings
of the 14th International Conference on
Computational Linguistics (COLING?92),
pages 205?212, Nantes.
Villemonte de La Clergerie, E?ric. 2005.
DyALog: a tabular logic programming
based environment for NLP. In Proceedings
of 2nd International Workshop on Constraint
Solving and Language Processing (CSLP?05),
pages 18?33, Barcelona.
Villemonte de la Clergerie, E?ric. 2010.
Building factorized TAGs with
meta-grammars. In Proceedings of
the 10th International Workshop on
Tree-Adjoining Grammar and Related
Formalisms (TAG+10), pages 111?118,
New Haven, CT.
Weir, David J. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, University of Pennsylvania.
Xia, Fei. 2001. Automatic Grammar Generation
from Two Different Perspectives. Ph.D. thesis,
University of Pennsylvania.
Xia, Fei, Martha Palmer, and K. Vijay-
Shanker. 1999. Toward semi-automating
grammar development. In Proceedings of
the 5th Natural Language Processing Pacific
Rim Symposium (NLPRS-99), pages 96?101,
Beijing.
Xia, Fei, Martha Palmer, and K. Vijay-
Shanker. 2005. Automatically generating
tree adjoining grammars from abstract
specifications. Journal of Computational
Intelligence, 21(3):246?287.
628
Crabbe? et al XMG: eXtensible MetaGrammar
Xia, Fei, Martha Palmer, and
K. Vijay-Shanker. 2010. Developing
tree-adjoining grammars with lexical
descriptions. In Srinivas Bangalore and
Aravind Joshi, editors, Supertagging:
Using Complex Lexical Descriptions in
Natural Language Processing. MIT Press,
Cambridge, MA, pages 73?110.
Xia, Fei, Martha Palmer, K. Vijay-Shanker,
and Joseph Rosenzweig. 1998. Consistent
grammar development using partial-tree
descriptions for LTAGs. In Proceedings
of the 4th International Workshop on
Tree Adjoining Grammar and Related
Formalisms (TAG+ 1998), pages 180?183,
Philadelphia, PA.
XTAG Research Group. 2001. A lexicalized
tree adjoining grammar for English.
Technical Report IRCS-01-03, IRCS,
University of Pennsylvania.
629

Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 592?600,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Error Mining on Dependency Trees
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy, F-54500, France
claire.gardent@loria.fr
Shashi Narayan
Universite? de Lorraine, LORIA, UMR 7503
Villers-le`s-Nancy, F-54600, France
shashi.narayan@loria.fr
Abstract
In recent years, error mining approaches were
developed to help identify the most likely
sources of parsing failures in parsing sys-
tems using handcrafted grammars and lexi-
cons. However the techniques they use to enu-
merate and count n-grams builds on the se-
quential nature of a text corpus and do not eas-
ily extend to structured data. In this paper, we
propose an algorithm for mining trees and ap-
ply it to detect the most likely sources of gen-
eration failure. We show that this tree mining
algorithm permits identifying not only errors
in the generation system (grammar, lexicon)
but also mismatches between the structures
contained in the input and the input structures
expected by our generator as well as a few id-
iosyncrasies/error in the input data.
1 Introduction
In recent years, error mining techniques have been
developed to help identify the most likely sources
of parsing failure (van Noord, 2004; Sagot and de la
Clergerie, 2006; de Kok et al, 2009). First, the input
data (text) is separated into two subcorpora, a corpus
of sentences that could be parsed (PASS) and a cor-
pus of sentences that failed to be parsed (FAIL). For
each n-gram of words (and/or part of speech tag) oc-
curring in the corpus to be parsed, a suspicion rate is
then computed which, in essence, captures the like-
lihood that this n-gram causes parsing to fail.
These error mining techniques have been applied
with good results on parsing output and shown to
help improve the large scale symbolic grammars and
lexicons used by the parser. However the techniques
they use (e.g., suffix arrays) to enumerate and count
n-grams builds on the sequential nature of a text cor-
pus and cannot easily extend to structured data.
There are some NLP applications though where
the processed data is structured data such as trees
or graphs and which would benefit from error min-
ing. For instance, when generating sentences from
dependency trees, as was proposed recently in the
Generation Challenge Surface Realisation Task (SR
Task, (Belz et al, 2011)), it would be useful to be
able to apply error mining on the input trees to find
the most likely causes of generation failure.
In this paper, we address this issue and propose
an approach that supports error mining on trees. We
adapt an existing algorithm for tree mining which we
then use to mine the Generation Challenge depen-
dency trees and identify the most likely causes of
generation failure. We show in particular, that this
tree mining algorithm permits identifying not only
errors in the grammar and the lexicon used by gener-
ation but also a few idiosyncrasies/error in the input
data as well as mismatches between the structures
contained in the SR input and the input structures
expected by our generator. The latter is an impor-
tant point since, for symbolic approaches, a major
hurdle to participation in the SR challenge is known
to be precisely these mismatches i.e., the fact that
the input provided by the SR task fails to match the
input expected by the symbolic generation systems
(Belz et al, 2011).
The paper is structured as follows. Section 2
presents the HybridTreeMiner algorithm, a complete
and computationally efficient algorithm developed
592
AB
CD
B
C
A
B
DC
B
C
A
B
C
B
CD
A
B
C
B
DC
Figure 1: Four unordered labelled trees. The right-
most is in Breadth-First Canonical Form
by (Chi et al, 2004) for discovering frequently oc-
curring subtrees in a database of labelled unordered
trees. Section 3 shows how to adapt this algorithm
to mine the SR dependency trees for subtrees with
high suspicion rate. Section 4 presents an experi-
ment we made using the resulting tree mining algo-
rithm on SR dependency trees and summarises the
results. Section 5 discusses related work. Section 6
concludes.
2 Mining Trees
Mining for frequent subtrees is an important prob-
lem that has many applications such as XML data
mining, web usage analysis and RNA classification.
The HybridTreeMiner (HTM) algorithm presented
in (Chi et al, 2004) provides a complete and com-
putationally efficient method for discovering fre-
quently occurring subtrees in a database of labelled
unordered trees and counting them. We now sketch
the intuition underlying this algorithm1. In the next
section, we will show how to modify this algorithm
to mine for errors in dependency trees.
Given a set of trees T , the HybridTreeMiner al-
gorithm proceeds in two steps. First, the unordered
labelled trees contained in T are converted to a
canonical form called BFCF (Breadth-First Canoni-
cal Form). In that way, distinct instantiations of the
same unordered trees have a unique representation.
Second, the subtrees of the BFCF trees are enumer-
ated in increasing size order using two tree opera-
tions called join and extension and their support (the
number of trees in the database that contains each
subtree) is recorded. In effect, the algorithm builds
an enumeration tree whose nodes are the possible
subtrees of T and such that, at depth d of this enu-
meration tree, all possible frequent subtrees consist-
ing of d nodes are listed.
1For a more complete definition see (Chi et al, 2004).
The BFCF canonical form of an unordered tree
is an ordered tree t such that t has the smallest
breath-first canonical string (BFCS) encoding ac-
cording to lexicographic order. The BFCS encod-
ing of a tree is obtained by breadth-first traver-
sal of the tree, recording the string labelling each
node, ?$? to separate siblings with distinct parents
and ?#? to represent the end of the tree2. For in-
stance, the BFCS encodings of the four trees shown
in Figure 1 are ?A$BB$C$DC#?, ?A$BB$C$CD#?,
?A$BB$DC$C#? and ?A$BB$CD$C#? respectively.
Hence, the rightmost tree is the BFCF of all four
trees.
The join and extension operations used to itera-
tively enumerate subtrees are depicted in Figure 2
and can be defined as follows.
? A leg is a leaf of maximal depth.
? Extension: Given a tree t of height ht and a
node n, extending t with n yields a tree t? (a
child of t in the enumeration tree) with height
ht? such that n is a child of one of t?s legs and
ht? is ht + 1.
? Join: Given two trees t1 and t2 of same height
h differing only in their rightmost leg and such
that t1 sorts lower than t2, joining t1 and t2
yields a tree t? (a child of t1 in the enumeration
tree) of same height h by adding the rightmost
leg of t2 to t1 at level h? 1.
A
CB
D + E ?Extension
A
CB
D
E
A
CB
D +
A
C
E
B
?Join
A
C
E
B
D
Figure 2: Join and Extension Operations
To support counting, the algorithm additionally
records for each subtree a list (called occurrence list)
2Assuming ?#? sorts greater than ?$? and both sort greater
than any other alphabets in node labels.
593
of all trees in which this subtree occurs and of its po-
sition in the tree (represented by the list of tree nodes
mapped onto by the subtree). Thus for a given sub-
tree t, the support of t is the number of elements
in that list. Occurrence lists are also used to check
that trees that are combined occur in the data. For
the join operation, the subtrees being combined must
occur in the same tree at the same position (the inter-
section of their occurrence lists must be non empty
and the tree nodes must match except the last node).
For the extension operation, the extension of a tree
t is licensed for any given occurrence in the occur-
rence list only if the planned extension maps onto
the tree identified by the occurrence.
3 Mining Dependency Trees
We develop an algorithm (called ErrorTreeMiner,
ETM) which adapts the HybridTreeMiner algorithm
to mine sources of generation errors in the Gener-
ation Challenge SR shallow input data. The main
modification is that instead of simply counting trees,
we want to compute their suspicion rate. Following
(de Kok et al, 2009), we take the suspicion rate of a
given subtree t to be the proportion of cases where t
occurs in an input tree for which generation fails:
Sus(t) =
count(t|FAIL)
count(t)
where count(t) is the number of occurrences of
t in all input trees and count(t|FAIL) is the number
of occurrences of t in input trees for which no output
was produced.
Since we work with subtrees of arbitrary length,
we also need to check whether constructing a longer
subtree is useful that is, whether its suspicion rate
is equal or higher than the suspicion rate of any of
the subtrees it contains. In that way, we avoid com-
puting all subtrees (thus saving time and space). As
noted in (de Kok et al, 2009), this also permits by-
passing suspicion sharing that is the fact that, if n2
is the cause of a generation failure, and if n2 is con-
tained in larger trees n3 and n4, then all three trees
will have high suspicion rate making it difficult to
identify the actual source of failure namely n2. Be-
cause we use a milder condition however (we accept
bigger trees whose suspicion rate is equal to the sus-
picion rate of any of their subtrees), some amount of
Algorithm 1 ErrorTreeMiner(D,minsup)
Note: D consists of Dfail and Dpass
F1 ? {Frequent 1-trees}
F2 ? ?
for i? 1, ..., |F1| do
for j ? 1, ..., |F1| do
q ? fi plus legfj
if Noord-Validation(q,minsup) then
F2 ? F2 ? q
end if
end for
end for
F ? F1 ? F2
PUSH: sort(F2)? LQueue
Enum-Grow(LQueue, F,minsup)
return F
Algorithm 2 Enum-Grow(LQueue, F,minsup)
while LQueue 6= empty do
POP: pop(LQueue)? C
for i? 1, ..., |C| do
The join operation
J ? ?
for j ? i, ..., |C| do
p? join(ci, cj)
if Noord-Validation(p,minsup) then
J ? J ? p
end if
end for
F ? F ? J
PUSH: sort(J)? LQueue
The extension operation
E ? ?
for possible leg lm of ci do
for possible new leg ln(? F1) do
q ? extend ci with ln at position lm
if Noord-Validation(q,minsup) then
E ? E ? q
end if
end for
end for
F ? F ? E
PUSH: sort(E)? LQueue
end for
end while
594
Algorithm 3 Noord-Validation(tn,minsup)
Note: tn, tree with n nodes
if Sup(tn) ? minsup then
if Sus(tn) ? Sus(tn?1),?tn?1 in tn then
return true
end if
end if
return false
suspicion sharing remains. As we shall see in Sec-
tion 4.3.2, relaxing this check though allows us to
extract frequent larger tree patterns and thereby get
a more precise picture of the context in which highly
suspicious items occur.
Finally, we only keep subtrees whose support is
above a given threshold where the support Sup(t)
of a tree t is defined as the ratio between the number
of times it occurs in an input for which generation
fails and the total number of generation failures:
Sup(t) =
count(t|FAIL)
count(FAIL)
The modified algorithm we use for error mining is
given in Algorithm 1, 2 and 3. It can be summarised
as follows.
First, dependency trees are converted to Breadth-
First Canonical Form whereby lexicographic order
can apply to the word forms labelling tree nodes, to
their part of speech, to their dependency relation or
to any combination thereof3.
Next, the algorithm iteratively enumerates the
subtrees occurring in the input data in increasing
size order and associating each subtree t with two
occurrence lists namely, the list of input trees in
which t occurs and for which generation was suc-
cessful (PASS(t)); and the list of input trees in which
t occurs and for which generation failed (FAIL(t)).
This process is initiated by building trees of size
one (i.e., one-node tree) and extending them to trees
of size two. It is then continued by extending the
trees using the join and extension operations. As
explained in Section 2 above, join and extension
only apply provided the resulting trees occur in the
data (this is checked by looking up occurrence lists).
3For convenience, the dependency relation labelling the
edges of dependency trees is brought down to the daughter node
of the edge.
Each time an n-node tree tn, is built, it is checked
that (i) its support is above the set threshold and (ii)
its suspicion rate is higher than or equal to the sus-
picion rate of all (n? 1)-node subtrees of tn.
In sum, the ETM algorithm differs from the HTM
algorithm in two main ways. First, while HTM ex-
plores the enumeration tree depth-first, ETM pro-
ceeds breadth-first to ensure that the suspicion rate
of (n-1)-node trees is always available when check-
ing whether an n-node tree should be introduced.
Second, while the HTM algorithm uses support to
prune the search space (only trees with a minimum
support bigger than the set threshold are stored), the
ETM algorithm drastically prunes the search space
by additionally checking that the suspicion rate of
all subtrees contained in a new tree t is smaller or
equal to the suspicion rate of t . As a result, while
ETM looses the space advantage of HTM by a small
margin4, it benefits from a much stronger pruning of
the search space than HTM through suspicion rate
checking. In practice, the ETM algorithm allows us
to process e.g., all NP chunks of size 4 and 6 present
in the SR data (roughly 60 000 trees) in roughly 20
minutes on a PC.
4 Experiment and Results
Using the input data provided by the Generation
Challenge SR Task, we applied the error mining al-
gorithm described in the preceding Section to debug
and extend a symbolic surface realiser developed for
this task.
4.1 Input Data and Surface Realisation System
The shallow input data provided by the SR Task
was obtained from the Penn Treebank using the
LTH Constituent-to-Dependency Conversion Tool
for Penn-style Treebanks (Pennconverter, (Johans-
son and Nugues, 2007)). It consists of a set
of unordered labelled syntactic dependency trees
whose nodes are labelled with word forms, part of
speech categories, partial morphosyntactic informa-
tion such as tense and number and, in some cases, a
sense tag identifier. The edges are labelled with the
syntactic labels provided by the Pennconverter. All
words (including punctuation) of the original sen-
4ETM needs to store all (n-1)-node trees in queues before
producing n-node trees.
595
tence are represented by a node in the tree and the
alignment between nodes and word forms was pro-
vided by the organisers.
The surface realiser used is a system based on
a Feature-Based Lexicalised Tree Adjoining Gram-
mar (FB-LTAG) for English extended with a unifica-
tion based compositional semantics. Both the gram-
mars and the lexicon were developed in view of the
Generation Challenge and the data provided by this
challenge was used as a means to debug and extend
the system. Unknown words are assigned a default
TAG family/tree based on the part of speech they
are associated with in the SR data. The surface real-
isation algorithm extends the algorithm proposed in
(Gardent and Perez-Beltrachini, 2010) and adapts it
to work on the SR dependency input rather than on
flat semantic representations.
4.2 Experimental Setup
To facilitate interpretation, we first chunked the in-
put data in NPs, PPs and Clauses and performed er-
ror mining on the resulting sets of data. The chunk-
ing was performed by retrieving from the Penn Tree-
bank (PTB), for each phrase type, the yields of the
constituents of that type and by using the alignment
between words and dependency tree nodes provided
by the organisers of the SR Task. For instance, given
the sentence ?The most troublesome report may be
the August merchandise trade deficit due out tomor-
row?, the NPs ?The most troublesome report? and
?the August merchandise trade deficit due out to-
morrow? will be extracted from the PTB and the
corresponding dependency structures from the SR
Task data.
Using this chunked data, we then ran the genera-
tor on the corresponding SR Task dependency trees
and stored separately, the input dependency trees for
which generation succeeded and the input depen-
dency trees for which generation failed. Using infor-
mation provided by the generator, we then removed
from the failed data, those cases where generation
failed either because a word was missing in the lex-
icon or because a TAG tree/family was missing in
the grammar but required by the lexicon and the in-
put data. These cases can easily be detected using
the generation system and thus do not need to be
handled by error mining.
Finally, we performed error mining on the data
using different minimal support thresholds, differ-
ent display modes (sorted first by size and second by
suspicion rate vs sorted by suspicion rate) and differ-
ent labels (part of speech, words and part of speech,
dependency, dependency and part of speech).
4.3 Results
One feature of our approach is that it permits min-
ing the data for tree patterns of arbitrary size us-
ing different types of labelling information (POS
tags, dependencies, word forms and any combina-
tion thereof). In what follows, we focus on the NP
chunk data and illustrate by means of examples how
these features can be exploited to extract comple-
mentary debugging information from the data.
4.3.1 Mining on single labels (word form, POS
tag or dependency)
Mining on a single label permits (i) assessing the
relative impact of each category in a given label cat-
egory and (ii) identifying different sources of errors
depending on the type of label considered (POS tag,
dependency or word form).
Mining on POS tags Table 1 illustrates how min-
ing on a single label (in this case, POS tags) gives
a good overview of how the different categories in
that label type impact generation: two POS tags
(POS and CC) have a suspicion rate of 0.99 indicat-
ing that these categories always lead generation to
fail. Other POS tag with much lower suspicion rate
indicate that there are unresolved issues with, in de-
creasing order of suspicion rate, cardinal numbers
(CD), proper names (NNP), nouns (NN), prepositions
(IN) and determiners (DT).
The highest ranking category (POS5) points to
a mismatch between the representation of geni-
tive NPs (e.g., John?s father) in the SR Task data
and in the grammar. While our generator ex-
pects the representation of ?John?s father? to be FA-
THER(?S?(JOHN)), the structure provided by the SR
Task is FATHER(JOHN(?S?)). Hence whenever a
possessive appears in the input data, generation fails.
This is in line with (Rajkumar et al, 2011)?s finding
that the logical forms expected by their system for
possessives differed from the shared task inputs.
5In the Penn Treebank, the POS tag is the category assigned
to possessive ?s.
596
POS Sus Sup Fail Pass
POS 0.99 0.38 3237 1
CC 0.99 0.21 1774 9
CD 0.39 0.16 1419 2148
NNP 0.35 0.32 2749 5014
NN 0.30 0.81 6798 15663
IN 0.30 0.16 1355 3128
DT 0.09 0.12 1079 10254
Table 1: Error Mining on POS tags with frequency
cutoff 0.1 and displaying only trees of size 1 sorted
by decreasing suspicion rate (Sus)
The second highest ranked category is CC for co-
ordinations. In this case, error mining unveils a
bug in the grammar trees associated with conjunc-
tion which made all sentences containing a conjunc-
tion fail. Because the grammar is compiled out of
a strongly factorised description, errors in this de-
scription can propagate to a large number of trees
in the grammar. It turned out that an error occurred
in a class inherited by all conjunction trees thereby
blocking the generation of any sentence requiring
the use of a conjunction.
Next but with a much lower suspicion rate come
cardinal numbers (CD), proper names (NNP), nouns
(NN), prepositions (IN) and determiners (DT). We
will see below how the richer information provided
by mining for larger tree patterns with mixed la-
belling information permits identifying the contexts
in which these POS tags lead to generation failure.
Mining on Word Forms Because we remove
from the failure set al cases of errors due to a miss-
ing word form in the lexicon, a high suspicion rate
for a word form usually indicates a missing or incor-
rect lexical entry: the word is present in the lexicon
but associated with either the wrong POS tag and/or
the wrong TAG tree/family. To capture such cases,
we therefore mine not on word forms alone but on
pairs of word forms and POS tag. In this way, we
found for instance, that cardinal numbers induced
many generation failures whenever they were cate-
gorised as determiners but not as nouns in our lexi-
con. As we will see below, larger tree patterns help
identify the specific contexts inducing such failures.
One interesting case stood out which pointed to
idiosyncrasies in the input data: The word form $
(Sus=1) was assigned the POS tag $ in the input
data, a POS tag which is unknown to our system and
not documented in the SR Task guidelines. The SR
guidelines specify that the Penn Treebank tagset is
used modulo the modifications which are explicitly
listed. However for the $ symbol, the Penn treebank
used SYM as a POS tag and the SR Task $, but the
modification is not listed. Similarly, while in the
Penn treebank, punctuations are assigned the SYM
POS tag, in the SR data ?,? is used for the comma,
?(? for an opening bracket and so on.
Mining on Dependencies When mining on de-
pendencies, suspects can point to syntactic construc-
tions (rather than words or word categories) that are
not easily spotted when mining on words or parts
of speech. Thus, while problems with coordination
could easily be spotted through a high suspicion rate
for the CC POS tag, some constructions are linked
neither to a specific POS tag nor to a specific word.
This is the case, for instance, for apposition which
a suspicion rate of 0.19 (286F/1148P) identified as
problematic. Similarly, a high suspicion rate (0.54,
183F/155P) on the TMP dependency indicates that
temporal modifiers are not correctly handled either
because of missing or erroneous information in the
grammar or because of a mismatch between the in-
put data and the fomat expected by the surface re-
aliser.
Interestingly, the underspecified dependency rela-
tion DEP which is typically used in cases for which
no obvious syntactic dependency comes to mind
shows a suspicion rate of 0.61 (595F/371P).
4.3.2 Mining on trees of arbitrary size and
complex labelling patterns
While error mining with tree patterns of size one
permits ranking and qualifying the various sources
of errors, larger patterns often provide more detailed
contextual information about these errors. For in-
stance, Table 1 shows that the CD POS tag has a
suspicion rate of 0.39 (1419F/2148P). The larger
tree patterns identified below permits a more specific
characterization of the context in which this POS tag
co-occurs with generation failure:
TP1 CD(IN,RBR) more than 10
TP2 IN(CD) of 1991
TP3 NNP(CD) November 1
TP4 CD(NNP(CD)) Nov. 1, 1997
597
Two patterns clearly emerge: a pattern where car-
dinal numbers are parts of a date (tree patterns TP2-
TP4) and a more specific pattern (TP1) involving
the comparative construction (e.g., more than 10).
All these patterns in fact point to a missing category
for cardinals in the lexicon: they are only associated
with determiner TAG trees, not nouns, and therefore
fail to combine with prepositions (e.g., of 1991, than
10) and with proper names (e.g., November 1).
For proper names (NNP), dates also show up be-
cause months are tagged as proper names (TP3,TP4)
as well as addresses TP5:
TP5 NNP(?,?,?,?) Brooklyn, n.y.,
For prepositions (IN), we find, in addition to the
TP1-TP2, the following two main patterns:
TP6 DT(IN) those with, some of
TP7 RB(IN) just under, little more
Pattern TP6 points to a missing entry for words
such as those and some which are categorised in the
lexicon as determiners but not as nouns. TP7 points
to a mismatch between the SR data and the format
expected by the generator: while the latter expects
the structure IN(RB), the input format provided by
the SR Task is RB(IN).
4.4 Improving Generation Using the Results of
Error Mining
Table 2 shows how implementing some of the cor-
rections suggested by error mining impacts the num-
ber of NP chunks (size 4) that can be generated. In
this experiment, the total number of input (NP) de-
pendency trees is 24995. Before error mining, gen-
eration failed on 33% of these input. Correcting
the erroneous class inherited by all conjunction trees
mentioned in Section 4.3.1 brings generation failure
down to 26%. Converting the input data to the cor-
rect input format to resolve the mismatch induced
by possessive ?s (cf. Section 4.3.1) reduce gener-
ation failure to 21%6 and combining both correc-
tions results in a failure rate of 13%. In other words,
error mining permits quickly identifying two issues
which, once corrected, reduces generation failure by
20 points.
When mining on clause size chunks, other mis-
matches were identified such as in particular, mis-
matches introduced by subjects and auxiliaries:
6For NP of size 4, 3264 structures with possessive ?s were
rewritten.
NP 4 Before After
SR Data 8361 6511
Rewritten SR Data 5255 3401
Table 2: Diminishing the number of errors using in-
formation from error mining. The table compares
the number of failures on NP chunks of size 4 be-
fore (first row) and after (second row) rewriting the
SR data to the format expected by our generator and
before (second column) and after (third column) cor-
recting the grammar and lexicon errors discussed in
Section 4.3.1
while our generator expects both the subject and the
auxiliary to be children of the verb, the SR data rep-
resent the subject and the verb as children of the aux-
iliary.
5 Related Work
We now relate our proposal (i) to previous proposals
on error mining and (ii) to the use of error mining in
natural language generation.
Previous work on error mining. (van Noord,
2004) initiated error mining on parsing results with
a very simple approach computing the parsability
rate of each n-gram in a very large corpus. The
parsability rate of an n-gram wi . . . wn is the ratio
R(wi . . . wn) =
C(wi...wn|OK)
C(wi...wn)
with C(wi . . . wn)
the number of sentences in which the n-gram
wi . . . wn occurs and C(wi . . . wn | OK) the num-
ber of sentences containing wi . . . wn which could
be parsed. The corpus is stored in a suffix array
and the sorted suffixes are used to compute the fre-
quency of each n-grams in the total corpus and in the
corpus of parsed sentences. The approach was later
extended and refined in (Sagot and de la Clergerie,
2006) and (de Kok et al, 2009) whereby (Sagot and
de la Clergerie, 2006) defines a suspicion rate for n-
grams which takes into account the number of occur-
rences of a given word form and iteratively defines
the suspicion rate of each word form in a sentence
based on the suspicion rate of this word form in the
corpus; (de Kok et al, 2009) combined the iterative
error mining proposed by (Sagot and de la Clergerie,
2006) with expansion of forms to n-grams of words
and POS tags of arbitrary length.
Our approach differs from these previous ap-
598
proaches in several ways. First, error mining is per-
formed on trees. Second, it can be parameterised to
use any combination of POS tag, dependency and/or
word form information. Third, it is applied to gener-
ation input rather than parsing output. Typically, the
input to surface realisation is a structured represen-
tation (i.e., a flat semantic representation, a first or-
der logic formula or a dependency tree) rather than a
string. Mining these structured representations thus
permits identifying causes of undergeneration in sur-
face realisation systems.
Error Mining for Generation Not much work
has been done on mining the results of surface re-
alisers. Nonetheless, (Gardent and Kow, 2007) de-
scribes an error mining approach which works on
the output of surface realisation (the generated sen-
tences), manually separates correct from incorrect
output and looks for derivation items which system-
atically occur in incorrect output but not in correct
ones. In contrast, our approach works on the input
to surface realisation, automatically separates cor-
rect from incorrect items using surface realisation
and targets the most likely sources of errors rather
than the absolute ones.
More generally, our approach is the first to our
knowledge, which mines a surface realiser for un-
dergeneration. Indeed, apart from (Gardent and
Kow, 2007), most previous work on surface reali-
sation evaluation has focused on evaluating the per-
formance and the coverage of surface realisers. Ap-
proaches based on reversible grammars (Carroll et
al., 1999) have used the semantic formulae output
by parsing to evaluate the coverage and performance
of their realiser; similarly, (Gardent et al, 2010) de-
veloped a tool called GenSem which traverses the
grammar to produce flat semantic representations
and thereby provide a benchmark for performance
and coverage evaluation. In both cases however, be-
cause it is produced using the grammar exploited by
the surface realiser, the input produced can only be
used to test for overgeneration (and performance) .
(Callaway, 2003) avoids this shortcoming by con-
verting the Penn Treebank to the format expected by
his realiser. However, this involves manually iden-
tifying the mismatches between two formats much
like symbolic systems did in the Generation Chal-
lenge SR Task. The error mining approach we pro-
pose helps identifying such mismatches automati-
cally.
6 Conclusion
Previous work on error mining has focused on appli-
cations (parsing) where the input data is sequential
working mainly on words and part of speech tags.
In this paper, we proposed a novel approach to error
mining which permits mining trees. We applied it
to the input data provided by the Generation Chal-
lenge SR Task. And we showed that this supports
the identification of gaps and errors in the grammar
and in the lexicon; and of mismatches between the
input data format and the format expected by our re-
aliser.
We applied our error mining approach to the in-
put of a surface realiser to identify the most likely
sources of undergeneration. We plan to also ex-
plore how it can be used to detect the most likely
sources of overgeneration based on the output of
this surface realiser on the SR Task data. Using the
Penn Treebank sentences associated with each SR
Task dependency tree, we will create the two tree
sets necessary to support error mining by dividing
the set of trees output by the surface realiser into a
set of trees (FAIL) associated with overgeneration
(the generated sentences do not match the original
sentences) and a set of trees (SUCCESS) associated
with success (the generated sentence matches the
original sentences). Exactly which tree should popu-
late the SUCCESS and FAIL set is an open question.
The various evaluation metrics used by the SR Task
(BLEU, NIST, METEOR and TER) could be used
to determine a threshold under which an output is
considered incorrect (and thus classificed as FAIL).
Alternatively, a strict matching might be required.
Similarly, since the surface realiser is non determin-
istic, the number of output trees to be kept will need
to be experimented with.
Acknowledgments
We would like to thank Cle?ment Jacq for useful dis-
cussions on the hybrid tree miner algorithm. The
research presented in this paper was partially sup-
ported by the European Fund for Regional Develop-
ment within the framework of the INTERREG IV A
Allegro Project.
599
References
Anja Belz, Michael White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalu-
ation results. In Proceedings of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. In Proceedings of the
18th International Joint Conference on Artificial Intel-
ligence, pages 811?817, Acapulco, Mexico.
John Carroll, Ann Copestake, Dan Flickinger, and Vik-
tor Paznan?ski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings of the
7th European Workshop on Natural Language Gener-
ation, pages 86?95, Toulouse, France.
Yun Chi, Yirong Yang, and Richard R. Muntz. 2004.
Hybridtreeminer: An efficient algorithm for mining
frequent rooted trees and free trees using canonical
form. In Proceedings of the 16th International Con-
ference on and Statistical Database Management (SS-
DBM), pages 11?20, Santorini Island, Greece. IEEE
Computer Society.
Danie?l de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error mining
in parsing results. In Proceedings of the 2009 Work-
shop on Grammar Engineering Across Frameworks
(GEAF 2009), pages 71?79, Suntec, Singapore. As-
sociation for Computational Linguistics.
Claire Gardent and Eric Kow. 2007. Spotting overgen-
eration suspect. In Proceedings of the 11th European
Workshop on Natural Language Generation (ENLG),
pages 41?48, Schloss Dagstuhl, Germany.
Claire Gardent and Laura Perez-Beltrachini. 2010. Rtg
based surface realisation for tag. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING), pages 367?375, Beijing, China.
Claire Gardent, Benjamin Gottesman, and Laura Perez-
Beltrachini. 2010. Comparing the performance of
two TAG-based Surface Realisers using controlled
Grammar Traversal. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING - Poster session), pages 338?346, Beijing,
China.
Richert Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proceedings of the 16th Nordic Conference of Com-
putational Linguistics (NODALIDA), pages 105?112,
Tartu, Estonia.
Rajakrishnan Rajkumar, Dominic Espinosa, and Michael
White. 2011. The osu system for surface realization
at generation challenges 2011. In Proceedings of the
13th European Workshop on Natural Language Gen-
eration (ENLG), pages 236?238, Nancy, France.
Beno??t Sagot and E?ric de la Clergerie. 2006. Error min-
ing in parsing results. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 329?336, Sydney,
Australia.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL), pages 446?453, Barcelona, Spain.
600
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 854?863,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Classifying French Verbs Using French and English Lexical Resources
Ingrid Falk
Universite? de Lorraine/LORIA,
Nancy, France
ingrid.falk@loria.fr
Claire Gardent
CNRS/LORIA,
Nancy, France
claire.gardent@loria.fr
Jean-Charles Lamirel
Universite? de Strasbourg/LORIA,
Nancy, France
jean-charles.lamirel@loria.fr
Abstract
We present a novel approach to the automatic
acquisition of a Verbnet like classification of
French verbs which involves the use (i) of
a neural clustering method which associates
clusters with features, (ii) of several super-
vised and unsupervised evaluation metrics and
(iii) of various existing syntactic and semantic
lexical resources. We evaluate our approach
on an established test set and show that it
outperforms previous related work with an F-
measure of 0.70.
1 Introduction
Verb classifications have been shown to be useful
both from a theoretical and from a practical perspec-
tive. From the theoretical viewpoint, they permit
capturing syntactic and/or semantic generalisations
about verbs (Levin, 1993; Kipper Schuler, 2006).
From a practical perspective, they support factorisa-
tion and have been shown to be effective in various
NLP (Natural language Processing) tasks such as se-
mantic role labelling (Swier and Stevenson, 2005) or
word sense disambiguation (Dang, 2004).
While there has been much work on automatically
acquiring verb classes for English (Sun et al, 2010)
and to a lesser extent for German (Brew and Schulte
im Walde, 2002; Schulte im Walde, 2003; Schulte
im Walde, 2006), Japanese (Oishi and Matsumoto,
1997) and Italian (Merlo et al, 2002), few studies
have been conducted on the automatic classification
of French verbs. Recently however, two proposals
have been put forward.
On the one hand, (Sun et al, 2010) applied
a clustering approach developed for English to
French. They exploit features extracted from a large
scale subcategorisation lexicon (LexSchem (Mes-
siant, 2008)) acquired fully automatically from Le
Monde newspaper corpus and show that, as for En-
glish, syntactic frames and verb selectional prefer-
ences perform better than lexical cooccurence fea-
tures. Their approach achieves a F-measure of
55.1 on 116 verbs occurring at least 150 times in
Lexschem. The best performance is achieved when
restricting the approach to verbs occurring at least
4000 times (43 verbs) with an F-measure of 65.4.
On the other hand, Falk and Gardent (2011)
present a classification approach for French verbs
based on the use of Formal Concept Analysis (FCA).
FCA (Barbut and Monjardet, 1970) is a sym-
bolic classification technique which permits creating
classes associating sets of objects (eg. French verbs)
with sets of features (eg. syntactic frames). Falk
and Gardent (2011) provide no evaluation for their
results however, only a qualitative analysis.
In this paper, we describe a novel approach to the
clustering of French verbs which (i) gives good re-
sults on the established benchmark used in (Sun et
al., 2010) and (ii) associates verbs with a feature
profile describing their syntactic and semantic prop-
erties. The approach exploits a clustering method
called IGNGF (Incremental Growing Neural Gas
with Feature Maximisation, (Lamirel et al, 2011b))
which uses the features characterising each cluster
both to guide the clustering process and to label the
output clusters. We apply this method to the data
contained in various verb lexicons and we evalu-
854
ate the resulting classification on a slightly modified
version of the gold standard provided by (Sun et al,
2010). We show that the approach yields promising
results (F-measure of 70%) and that the clustering
produced systematically associates verbs with syn-
tactic frames and thematic grids thereby providing
an interesting basis for the creation and evaluation
of a Verbnet-like classification.
Section 2 describes the lexical resources used for
feature extraction and Section 3 the experimental
setup. Sections 4 and 5 present the data used for
and the results obtained. Section 6 concludes.
2 Lexical Resources Used
Our aim is to accquire a classification which covers
the core verbs of French, could be used to support
semantic role labelling and is similar in spirit to the
English Verbnet. In this first experiment, we there-
fore favoured extracting the features used for clus-
tering, not from a large corpus parsed automatically,
but from manually validated resources1. These lexi-
cal resources are (i) a syntactic lexicon produced by
merging three existing lexicons for French and (ii)
the English Verbnet.
Among the many syntactic lexicons available for
French (Nicolas et al, 2008; Messiant, 2008; Kups?c?
and Abeille?, 2008; van den Eynde and Mertens,
2003; Gross, 1975), we selected and merged three
lexicons built or validated manually namely, Dico-
valence, TreeLex and the LADL tables. The result-
ing lexicon contains 5918 verbs, 20433 lexical en-
tries (i.e., verb/frame pairs) and 345 subcategorisa-
tion frames. It also contains more detailed syntac-
tic and semantic features such as lexical preferences
(e.g., locative argument, concrete object) or thematic
role information (e.g., symmetric arguments, asset
role) which we make use of for clustering.
We use the English Verbnet as a resource for asso-
ciating French verbs with thematic grids as follows.
We translate the verbs in the English Verbnet classes
to French using English-French dictionaries2. To
1Of course, the same approach could be applied to corpus
based data (as done e.g., in (Sun et al, 2010)) thus making the
approach fully unsupervised and directly applicable to any lan-
guage for which a parser is available.
2For the translation we use the following resources: Sci-
Fran-Euradic, a French-English bilingual dictionary, built and
improved by linguists (http://catalog.elra.info/
deal with polysemy, we train a supervised classifier
as follows. We first map French verbs with English
Verbnet classes: A French verb is associated with
an English Verbnet class if, according to our dictio-
naries, it is a translation of an English verb in this
class. The task of the classifier is then to produce
a probability estimate for the correctness of this as-
sociation, given the training data. The training set
is built by stating for 1740 ?French verb, English
Verbnet class? pairs whether the verb has the the-
matic grid given by the pair?s Verbnet class3. This
set is used to train an SVM (support vector machine)
classifier4. The features we use are similar to those
used in (Mouton, 2010): they are numeric and are
derived for example from the number of translations
an English or French verb had, the size of the Verb-
net classes, the number of classes a verb is a member
of etc. The resulting classifier gives for each ?French
verb, English VN class? pair the estimated probabil-
ity of the pair?s verb being a member of the pair?s
class5. We select 6000 pairs with highest proba-
bility estimates and obtain the translated classes by
assigning each verb in a selected pair to the pair?s
class. This way French verbs are effectively asso-
ciated with one or more English Verbnet thematic
grids.
3 Clustering Methods, Evaluation Metrics
and Experimental Setup
3.1 Clustering Methods
The IGNGF clustering method is an incremental
neural ?winner-take-most? clustering method be-
longing to the family of the free topology neu-
ral clustering methods. Like other neural free
topology methods such as Neural Gas (NG) (Mar-
tinetz and Schulten, 1991), Growing Neural Gas
(GNG) (Fritzke, 1995), or Incremental Growing
Neural Gas (IGNG) (Prudent and Ennaji, 2005),
the IGNGF method makes use of Hebbian learning
product_info.php?products_id=666), Google dic-
tionary (http://www.google.com/dictionary) and
Dicovalence (van den Eynde and Mertens, 2003).
3The training data consists of the verbs and Verbnet classes
used in the gold standard presented in (Sun et al, 2010).
4We used the libsvm (Chang and Lin, 2011) implementation
of the classifier for this step.
5The accuracy of the classifier on the held out random test
set of 100 pairs was of 90%.
855
(Hebb, 1949) for dynamically structuring the learn-
ing space. However, contrary to these methods, the
use of a standard distance measure for determining a
winner is replaced in IGNGF by feature maximisa-
tion. Feature maximisation is a cluster quality metric
which associates each cluster with maximal features
i.e., features whose Feature F-measure is maximal.
Feature F-measure is the harmonic mean of Feature
Recall and Feature Precision which in turn are de-
fined as:
FRc(f) =
?
v?c
W fv
?
c??C
?
v?c?
W fv
, FPc(f) =
?
v?c
W fv
?
f ??Fc,v?c
W f
?
v
where W fx represents the weight of the feature f for
element x and Fc designates the set of features as-
sociated with the verbs occuring in the cluster c. A
feature is then said to be maximal for a given clus-
ter iff its Feature F-measure is higher for that cluster
than for any other cluster.
The IGNGF method was shown to outperform
other usual neural and non neural methods for clus-
tering tasks on relatively clean data (Lamirel et al,
2011b). Since we use features extracted from man-
ually validated sources, this clustering technique
seems a good fit for our application. In addition,
the feature maximisation and cluster labeling per-
formed by the IGNGF method has proved promising
both for visualising clustering results (Lamirel et al,
2008) and for validating or optimising a clustering
method (Attik et al, 2006). We make use of these
processes in all our experiments and systematically
compute cluster labelling and feature maximisation
on the output clusterings. As we shall see, this per-
mits distinguishing between clusterings with simi-
lar F-measure but lower ?linguistic plausibility? (cf.
Section 5). This facilitates clustering interpretation
in that cluster labeling clearly indicates the associa-
tion between clusters (verbs) and their prevalent fea-
tures. And this supports the creation of a Verbnet
style classification in that cluster labeling directly
provides classes grouping together verbs, thematic
grids and subcategorisation frames.
3.2 Evaluation metrics
We use several evaluation metrics which bear on dif-
ferent properties of the clustering.
Modified Purity and Accuracy. Following (Sun
et al, 2010), we use modified purity (mPUR);
weighted class accuracy (ACC) and F-measure to
evaluate the clusterings produced. These are com-
puted as follows. Each induced cluster is assigned
the gold class (its prevalent class, prev(C)) to which
most of its member verbs belong. A verb is then said
to be correct if the gold associates it with the preva-
lent class of the cluster it is in. Given this, purity is
the ratio between the number of correct gold verbs
in the clustering and the total number of gold verbs
in the clustering6:
mPUR =
?
C?Clustering,|prev(C)|>1 |prev(C) ? C|
VerbsGold?Clustering
,
where VerbsGold?Clustering is the total number of gold
verbs in the clustering.
Accuracy represents the proportion of gold verbs
in those clusters which are associated with a gold
class, compared to all the gold verbs in the clus-
tering. To compute accuracy we associate to each
gold class CGold a dominant cluster, ie. the cluster
dom(CGold) which has most verbs in common with
the gold class. Then accuracy is given by the follow-
ing formula:
ACC =
?
C?Gold |dom(C) ? C|
VerbsGold?Clustering
Finally, F-measure is the harmonic mean of mPUR
and ACC.
Coverage. To assess the extent to which a cluster-
ing matches the gold classification, we additionally
compute the coverage of each clustering that is, the
proportion of gold classes that are prevalent classes
in the clustering.
Cumulative Micro Precision (CMP). As pointed
out in (Lamirel et al, 2008; Attik et al, 2006), un-
supervised evaluation metrics based on cluster la-
belling and feature maximisation can prove very
useful for identifying the best clustering strategy.
Following (Lamirel et al, 2011a), we use CMP to
identify the best clustering. Computed on the clus-
tering results, this metrics evaluates the quality of a
clustering w.r.t. the cluster features rather than w.r.t.
6Clusters for which the prevalent class has only one element
are ignored
856
to a gold standard. It was shown in (Ghribi et al,
2010) to be effective in detecting degenerated clus-
tering results including a small number of large het-
erogeneous, ?garbage? clusters and a big number of
small size ?chunk? clusters.
First, the local Recall (Rfc ) and the local Preci-
sion (P fc ) of a feature f in a cluster c are defined as
follows:
Rfc =
|vfc |
|V f |
P fc =
|vfc |
|Vc|
where vfc is the set of verbs having feature f in c, Vc
the set of verbs in c and V f , the set of verbs with
feature f .
Cumulative Micro-Precision (CMP) is then de-
fined as follows:
CMP =
?
i=|Cinf |,|Csup|
1
|Ci+|2
?
c?Ci+,f?Fc P
f
c
?
i=|Cinf |,|Csup|
1
Ci+
where Ci+ represents the subset of clusters of C
for which the number of associated verbs is greater
than i, and: Cinf = argminci?C |ci|, Csup =
argmaxci?C |ci|
3.3 Cluster display, feature f-Measure and
confidence score
To facilitate interpretation, clusters are displayed as
illustrated in Table 1. Features are displayed in
decreasing order of Feature F-measure (cf. Sec-
tion 3.1) and features whose Feature F-measure is
under the average Feature F-measure of the over-
all clustering are clearly delineated from others. In
addition, for each verb in a cluster, a confidence
score is displayed which is the ratio between the sum
of the F-measures of its cluster maximised features
over the sum of the F-measures of the overall cluster
maximised features. Verbs whose confidence score
is 0 are considered as orphan data.
3.4 Experimental setup
We applied an IDF-Norm weighting scheme
(Robertson and Jones, 1976) to decrease the influ-
ence of the most frequent features (IDF component)
and to compensate for discrepancies in feature num-
ber (normalisation).
C6- 14(14) [197(197)]
???-
Prevalent Label ? = AgExp-Cause
0.341100 G-AgExp-Cause
0.274864 C-SUJ:Ssub,OBJ:NP
0.061313 C-SUJ:Ssub
0.042544 C-SUJ:NP,DEOBJ:Ssub
**********
**********
0.017787 C-SUJ:NP,DEOBJ:VPinf
0.008108 C-SUJ:VPinf,AOBJ:PP
. . .
[**de?primer 0.934345 4(0)] [affliger 0.879122 3(0)]
[e?blouir 0.879122 3(0)] [choquer 0.879122 3(0)]
[de?cevoir 0.879122 3(0)] [de?contenancer 0.879122
3(0)] [de?contracter 0.879122 3(0)] [de?sillusionner
0.879122 3(0)] [**ennuyer 0.879122 3(0)] [fasciner
0.879122 3(0)] [**heurter 0.879122 3(0)] . . .
Table 1: Sample output for a cluster produced with
the grid-scf-sem feature set and the IGNGF clustering
method.
We use K-Means as a baseline. For each cluster-
ing method (K-Means and IGNGF), we let the num-
ber of clusters vary between 1 and 30 to obtain a
partition that reaches an optimum F-measure and a
number of clusters that is in the same order of mag-
nitude as the initial number of Gold classes (i.e. 11
classes).
4 Features and Data
Features In the simplest case the features are
the subcategorisation frames (scf) associated to the
verbs by our lexicon. We also experiment with dif-
ferent combinations of additional, syntactic (synt)
and semantic features (sem) extracted from the lex-
icon and with the thematic grids (grid) extracted
from the English Verbnet.
The thematic grid information is derived from the
English Verbnet as explained in Section 2. The syn-
tactic features extracted from the lexicon are listed
in Table 1(a). They indicate whether a verb accepts
symmetric arguments (e.g., John met Mary/John and
Mary met); has four or more arguments; combines
with a predicative phrase (e.g., John named Mary
president); takes a sentential complement or an op-
tional object; or accepts the passive in se (similar to
the English middle voice Les habits se vendent bien /
The clothes sell well). As shown in Table 1(a), these
857
(a) Additional syntactic features.
Feature related VN class
Symmetric arguments amalgamate-22.2, correspond-36.1
4 or more arguments get-13.5.1, send-11.1
Predicate characterize-29.2
Sentential argument correspond-36.1, characterize-29.2
Optional object implicit theme (Randall, 2010), p. 95
Passive built with se theme role (Randall, 2010), p. 120
(b) Additional semantic features.
Feature related VN class
Location role put-9.1, remove-10.1, . . .
Concrete object hit-18.1 (eg. INSTRUMENT)
(non human role) other cos-45.4 . . .
Asset role get-13.5.1
Plural role amalgamate-22.2, correspond-36.1
Table 2: Additional syntactic (a) and semantic (b) fea-
tures extracted from the LADL and Dicovalence re-
sources and the alternations/roles they are possibly re-
lated to.
features are meant to help identify specific Verbnet
classes and thematic roles. Finally, we extract four
semantic features from the lexicon. These indicate
whether a verb takes a locative or an asset argument
and whether it requires a concrete object (non hu-
man role) or a plural role. The potential correlation
between these features and Verbnet classes is given
in Table 1(b).
French Gold Standard To evaluate our approach,
we use the gold standard proposed by Sun et al
(2010). This resource consists of 16 fine grained
Levin classes with 12 verbs each whose predomi-
nant sense in English belong to that class. Since
our goal is to build a Verbnet like classification
for French, we mapped the 16 Levin classes of the
Sun et al (2010)?s Gold Standard to 11 Verbnet
classes thereby associating each class with a the-
matic grid. In addition we group Verbnet semantic
roles as shown in Table 4. Table 3 shows the refer-
ence we use for evaluation.
Verbs For our clustering experiments we use the
2183 French verbs occurring in the translations of
the 11 classes in the gold standard (cf. Section 4).
Since we ignore verbs with only one feature the
number of verbs and ?verb, feature? pairs considered
may vary slightly across experiments.
AgExp Agent, Experiencer
AgentSym Actor, Actor1, Actor2
Theme Theme, Topic, Stimulus, Proposition
PredAtt Predicate, Attribute
ThemeSym Theme, Theme1, Theme2
Patient Patient
PatientSym Patient, Patient1, Patient2
Start Material (transformation), Source (motion,
transfer)
End Product (transformation), Destination (mo-
tion), Recipient (transfer)
Location
Instrument
Cause
Beneficiary
Table 4: Verbnet role groups.
5 Results
5.1 Quantitative Analysis
Table 4(a) includes the evaluation results for all the
feature sets when using IGNGF clustering.
In terms of F-measure, the results range from 0.61
to 0.70. This generally outperforms (Sun et al,
2010) whose best F-measures vary between 0.55 for
verbs occurring at least 150 times in the training data
and 0.65 for verbs occurring at least 4000 times in
this training data. The results are not directly com-
parable however since the gold data is slightly dif-
ferent due to the grouping of Verbnet classes through
their thematic grids.
In terms of features, the best results are ob-
tained using the grid-scf-sem feature set with an F-
measure of 0.70. Moreover, for this data set, the un-
supervised evaluation metrics (cf. Section 3) high-
light strong cluster cohesion with a number of clus-
ters close to the number of gold classes (13 clusters
for 11 gold classes); a low number of orphan verbs
(i.e., verbs whose confidence score is zero); and a
high Cumulated Micro Precision (CMP = 0.3) indi-
cating homogeneous clusters in terms of maximis-
ing features. The coverage of 0.72 indicates that ap-
proximately 8 out of the 11 gold classes could be
matched to a prevalent label. That is, 8 clusters were
labelled with a prevalent label corresponding to 8
distinct gold classes.
In contrast, the classification obtained using the
scf-synt-sem feature set has a higher CMP for the
clustering with optimal mPUR (0.57); but a lower
F-measure (0.61), a larger number of classes (16)
858
AgExp, PatientSym
amalgamate-22.2: incorporer, associer, re?unir, me?langer, me?ler, unir, assembler, combiner, lier, fusionner
Cause, AgExp
amuse-31.1: abattre, accabler, briser, de?primer, consterner, ane?antir, e?puiser, exte?nuer, e?craser, ennuyer, e?reinter, inonder
AgExp, PredAtt, Theme
characterize-29.2: appre?hender, concevoir, conside?rer, de?crire, de?finir, de?peindre, de?signer, envisager, identifier, montrer, percevoir, repre?senter, ressen-
tir
AgentSym, Theme
correspond-36.1: coope?rer, participer, collaborer, concourir, contribuer, associer
AgExp, Beneficiary, Extent, Start, Theme
get-13.5.1: acheter, prendre, saisir, re?server, conserver, garder, pre?server, maintenir, retenir, louer, affre?ter
AgExp, Instrument, Patient
hit-18.1: cogner, heurter, battre, frapper, fouetter, taper, rosser, brutaliser, e?reinter, maltraiter, corriger
other cos-45.4: me?langer, fusionner, consolider, renforcer, fortifier, adoucir, polir, atte?nuer, tempe?rer, pe?trir, fac?onner, former
AgExp, Location, Theme
light emission-43.1 briller, e?tinceler, flamboyer, luire, resplendir, pe?tiller, rutiler, rayonner, scintiller
modes of being with motion-47.3: trembler, fre?mir, osciller, vaciller, vibrer, tressaillir, frissonner, palpiter, gre?siller, trembloter, palpiter
run-51.3.2: voyager, aller, errer, circuler, courir, bouger, naviguer, passer, promener, de?placer
AgExp, End, Theme
manner speaking-37.3: ra?ler, gronder, crier, ronchonner, grogner, bougonner, maugre?er, rouspe?ter, grommeler, larmoyer, ge?mir, geindre, hurler,
gueuler, brailler, chuchoter
put-9.1: accrocher, de?poser, mettre, placer, re?partir, re?inte?grer, empiler, emporter, enfermer, inse?rer, installer
say-37.7: dire, re?ve?ler, de?clarer, signaler, indiquer, montrer, annoncer, re?pondre, affirmer, certifier, re?pliquer
AgExp, Theme
peer-30.3: regarder, e?couter, examiner, conside?rer, voir, scruter, de?visager
AgExp, Start, Theme
remove-10.1: o?ter, enlever, retirer, supprimer, retrancher, de?barasser, soustraire, de?compter, e?liminer
AgExp, End, Start, Theme
send-11.1: envoyer, lancer, transmettre, adresser, porter, expe?dier, transporter, jeter, renvoyer, livrer
Table 3: French gold classes and their member verbs presented in (Sun et al, 2010).
and a higher number of orphans (156). That is, this
clustering has many clusters with strong feature co-
hesion but a class structure that markedly differs
from the gold. Since there might be differences in
structure between the English Verbnet and the the-
matic classification for French we are building, this
is not necessarily incorrect however. Further inves-
tigation on a larger data set would be required to as-
sess which clustering is in fact better given the data
used and the classification searched for.
In general, data sets whose description includes
semantic features (sem or grid) tend to produce bet-
ter results than those that do not (scf or synt). This
is in line with results from (Sun et al, 2010) which
shows that semantic features help verb classifica-
tion. It differs from it however in that the seman-
tic features used by Sun et al (2010) are selectional
preferences while ours are thematic grids and a re-
stricted set of manually encoded selectional prefer-
ences.
Noticeably, the synt feature degrades perfor-
mance throughout: grid,scf,synt has lower F-
measure than grid,scf; scf,synt,sem than scf,sem;
and scf,synt than scf. We have no clear explanation
for this.
The best results are obtained with IGNGF method
on most of the data sets. Table 4(b) illustrates
the differences between the results obtained with
IGNGF and those obtained with K-means on the
grid-scf-sem data set (best data set). Although K-
means and IGNGF optimal model reach similar F-
measure and display a similar number of clusters,
the very low CMP (0.10) of the K-means model
shows that, despite a good Gold class coverage
(0.81), K-means tend to produce more heteroge-
neous clusters in terms of features.
Table 4(b) also shows the impact of IDF feature
weighting and feature vector normalisation on clus-
tering. The benefit of preprocessing the data appears
clearly. When neither IDF weighting nor vector nor-
malisation are used, F-measure decreases from 0.70
to 0.68 and cumulative micro-precision from 0.30
to 0.21. When either normalisation or IDF weight-
ing is left out, the cumulative micro-precision drops
by up to 15 points (from 0.30 to 0.15 and 0.18) and
the number of orphans increases from 67 up to 180.
859
(a) The impact of the feature set.
Feat. set Nbr. feat. Nbr. verbs mPUR ACC F (Gold) Nbr. classes Cov. Nbr. orphans CMP at opt (13cl.)
scf 220 2085 0.93 0.48 0.64 17 0.55 129 0.28 (0.27)
grid, scf 231 2085 0.94 0.54 0.68 14 0.64 183 0.12 (0.12)
grid, scf, sem 237 2183 0.86 0.59 0.70 13 0.72 67 0.30 (0.30)
grid, scf, synt 236 2150 0.87 0.50 0.63 14 0.72 66 0.13 (0.14)
grid, scf, synt, sem 242 2201 0.99 0.52 0.69 16 0.82 100 0.50 (0.22)
scf, sem 226 2183 0.83 0.55 0.66 23 0.64 146 0.40 (0.26)
scf, synt 225 2150 0.91 0.45 0.61 15 0.45 83 0.17 (0.22)
scf, synt, sem 231 2101 0.89 0.47 0.61 16 0.64 156 0.57 (0.11)
(b) Metrics for best performing clustering method (IGNGF) compared to K-means. Feature set is grid, scf, sem.
Method mPUR ACC F (Gold) Nbr. classes Cov. Nbr. orphans CMP at opt (13cl.)
IGNGF with IDF and norm. 0.86 0.59 0.70 13 0.72 67 0.30 (0.30)
K-means with IDF and norm. 0.88 0.57 0.70 13 0.81 67 0.10 (0.10)
IGNGF, no IDF 0.86 0.59 0.70 17 0.81 126 0.18 (0.14)
IGNGF, no norm. 0.78 0.62 0.70 18 0.72 180 0.15 (0.11)
IGNGF, no IDF, no norm. 0.87 0.55 0.68 14 0.81 103 0.21 (0.21)
Table 5: Results. Cumulative micro precision (CMP) is given for the clustering at the mPUR optimum and in paran-
theses for 13 classes clustering.
That is, clusters are less coherent in terms of fea-
tures.
5.2 Qualitative Analysis
We carried out a manual analysis of the clusters ex-
amining both the semantic coherence of each cluster
(do the verbs in that cluster share a semantic com-
ponent?) and the association between the thematic
grids, the verbs and the syntactic frames provided
by clustering.
Semantic homogeneity: To assess semantic ho-
mogeneity, we examined each cluster and sought
to identify one or more Verbnet labels character-
ising the verbs contained in that cluster. From
the 13 clusters produced by clustering, 11 clus-
ters could be labelled. Table 6 shows these eleven
clusters, the associated labels (abbreviated Verbnet
class names), some example verbs, a sample sub-
categorisation frame drawn from the cluster max-
imising features and an illustrating sentence. As
can be seen, some clusters group together several
subclasses and conversely, some Verbnet classes are
spread over several clusters. This is not necessar-
ily incorrect though. To start with, recall that we
are aiming for a classification which groups together
verbs with the same thematic grid. Given this, clus-
ter C2 correctly groups together two Verbnet classes
(other cos-45.4 and hit-18.1) which share the same
thematic grid (cf. Table 3). In addition, the features
associated with this cluster indicate that verbs in
these two classes are transitive, select a concrete ob-
ject, and can be pronominalised which again is cor-
rect for most verbs in that cluster. Similarly, cluster
C11 groups together verbs from two Verbnet classes
with identical theta grid (light emission-43.1 and
modes of being with motion-47.3) while its associ-
ated features correctly indicate that verbs from both
classes accept both the intransitive form without ob-
ject (la jeune fille rayonne / the young girl glows, un
cheval galope / a horse gallops) and with a prepo-
sitional object (la jeune fille rayonne de bonheur /
the young girl glows with happiness, un cheval ga-
lope vers l?infini / a horse gallops to infinity). The
third cluster grouping together verbs from two Verb-
net classes is C7 which contains mainly judgement
verbs (to applaud, bless, compliment, punish) but
also some verbs from the (very large) other cos-45.4
class. In this case, a prevalent shared feature is
that both types of verbs accept a de-object that is,
a prepositional object introduced by ?de? (Jean ap-
plaudit Marie d?avoir danse? / Jean applaudit Marie
for having danced; Jean de?gage le sable de la route /
Jean clears the sand of the road). The semantic fea-
tures necessary to provide a finer grained analysis of
their differences are lacking.
Interestingly, clustering also highlights classes
which are semantically homogeneous but syntac-
tically distinct. While clusters C6 and C10 both
860
contain mostly verbs from the amuse-31.1 class
(amuser,agacer,e?nerver,de?primer), their features in-
dicate that verbs in C10 accept the pronominal form
(e.g., Jean s?amuse) while verbs in C6 do not (e.g.,
*Jean se de?prime). In this case, clustering highlights
a syntactic distinction which is present in French but
not in English. In contrast, the dispersion of verbs
from the other cos-45.4 class over clusters C2 and
C7 has no obvious explanation. One reason might
be that this class is rather large (361 verbs) and thus
might contain French verbs that do not necessarily
share properties with the original Verbnet class.
Syntax and Semantics. We examined whether the
prevalent syntactic features labelling each cluster
were compatible with the verbs and with the seman-
tic class(es) manually assigned to the clusters. Ta-
ble 6 sketches the relation between cluster, syntac-
tic frames and Verbnet like classes. It shows for in-
stance that the prevalent frame of the C0 class (man-
ner speaking-37.3) correctly indicates that verbs in
that cluster subcategorise for a sentential argument
and an AOBJ (prepositional object in ?a`?) (e.g., Jean
bafouille a` Marie qu?il est amoureux / Jean stam-
mers to Mary that he is in love); and that verbs
in the C9 class (characterize-29.2) subcategorise for
an object NP and an attribute (Jean nomme Marie
pre?sidente / Jean appoints Marie president). In gen-
eral, we found that the prevalent frames associated
with each cluster adequately characterise the syntax
of that verb class.
6 Conclusion
We presented an approach to the automatic classi-
fication of french verbs which showed good results
on an established testset and associates verb clusters
with syntactic and semantic features.
Whether the features associated by the IGNGF
clustering with the verb clusters appropriately car-
acterise these clusters remains an open question. We
carried out a first evaluation using these features
to label the syntactic arguments of verbs in a cor-
pus with thematic roles and found that precision is
high but recall low mainly because of polysemy: the
frames and grids made available by the classification
for a given verb are correct for that verb but not for
the verb sense occurring in the corpus. This sug-
gests that overlapping clustering techniques need to
C0 speaking: babiller, bafouiller, balbutier
SUJ:NP,OBJ:Ssub,AOBJ:PP
Jean bafouille a` Marie qu?il l?aime / Jean stammers to Mary that he is
in love
C1 put: entasser, re?pandre, essaimer
SUJ:NP,POBJ:PP,DUMMY:REFL
Loc, Plural
Les de?chets s?entassent dans la cour / Waste piles in the yard
C2 hit: broyer, de?molir, fouetter
SUJ:NP,OBJ:NP
T-Nhum
Ces pierres broient les graines / These stones grind the seeds.
other cos: agrandir, alle?ger, amincir
SUJ:NP,DUMMY:REFL
les ae?roports s?agrandissent sans arre?t / airports grow constantly
C4 dedicate: s?engager a`, s?obliger a`,
SUJ:NP,AOBJ:VPinf,DUMMY:REFL
Cette promesse t?engage a` nous suivre / This promise commits you to
following us
C5 conjecture: penser, attester, agre?er
SUJ:NP,OBJ:Ssub
Le me?decin atteste que l?employe? n?est pas en e?tat de travailler / The
physician certifies that the employee is not able to work
C6 amuse: de?primer, de?contenancer, de?cevoir
SUJ:Ssub,OBJ:NP
SUJ:NP,DEOBJ:Ssub
Travailler de?prime Marie / Working depresses Marie
Marie de?prime de ce que Jean parte / Marie depresses because of Jean?s
leaving
C7 other cos: de?gager, vider, drainer, sevrer
judgement
SUJ:NP,OBJ:NP,DEOBJ:PP
vider le re?cipient de son contenu / empty the container of its contents
applaudir, be?nir, bla?mer,
SUJ:NP,OBJ:NP,DEOBJ:Ssub
Jean blame Marie d?avoir couru / Jean blames Mary for runnig
C9 characterise: promouvoir, adouber, nommer
SUJ:NP,OBJ:NP,ATB:XP
Jean nomme Marie pre?sidente / Jean appoints Marie president
C10 amuse: agacer, amuser, enorgueillir
SUJ:NP,DEOBJ:XP,DUMMY:REFL
Jean s?enorgueillit d?e?tre roi/ Jean is proud to be king
C11 light: rayonner,clignoter,cliqueter
SUJ:NP,POBJ:PP
Jean clignote des yeux / Jean twinkles his eyes
motion: aller, passer, fuir, glisser
SUJ:NP,POBJ:PP
glisser sur le trottoir verglace? / slip on the icy sidewalk
C12 transfer msg: enseigner, permettre, interdire
SUJ:NP,OBJ:NP,AOBJ:PP
Jean enseigne l?anglais a` Marie / Jean teaches Marie English.
Table 6: Relations between clusters, syntactic frames and
Verbnet like classes.
be applied.
We are also investigating how the approach scales
up to the full set of verbs present in the lexicon. Both
Dicovalence and the LADL tables contain rich de-
tailed information about the syntactic and semantic
properties of French verbs. We intend to tap on that
potential and explore how well the various semantic
features that can be extracted from these resources
support automatic verb classification for the full set
of verbs present in our lexicon.
861
References
M. Attik, S. Al Shehabi, and J.-C. Lamirel. 2006. Clus-
tering Quality Measures for Data Samples with Mul-
tiple Labels. In Databases and Applications, pages
58?65.
M. Barbut and B. Monjardet. 1970. Ordre et Classifica-
tion. Hachette Universite?.
C. Brew and S. Schulte im Walde. 2002. Spectral Clus-
tering for German Verbs. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 117?124, Philadelphia, PA.
C. Chang and C. Lin. 2011. LIBSVM: A library for
support vector machines. ACM Transactions on Intel-
ligent Systems and Technology, 2:27:1?27:27. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
H. T. Dang. 2004. Investigations into the role of lexical
semantics in word sense disambiguation. Ph.D. thesis,
U. Pennsylvannia, US.
I. Falk and C. Gardent. 2011. Combining Formal Con-
cept Analysis and Translation to Assign Frames and
Thematic Role Sets to French Verbs. In Amedeo
Napoli and Vilem Vychodil, editors, Concept Lattices
and Their Applications, Nancy, France, October.
B. Fritzke. 1995. A growing neural gas network learns
topologies. Advances in Neural Information Process-
ing Systems 7, 7:625?632.
M. Ghribi, P. Cuxac, J.-C. Lamirel, and A. Lelu. 2010.
Mesures de qualite? de clustering de documents : prise
en compte de la distribution des mots cle?s. In Nicolas
Be?chet, editor, E?valuation des me?thodes d?Extraction
de Connaissances dans les Donne?es- EvalECD?2010,
pages 15?28, Hammamet, Tunisie, January. Fatiha
Sa??s.
M. Gross. 1975. Me?thodes en syntaxe. Hermann, Paris.
D. O. Hebb. 1949. The organization of behavior: a
neuropsychological theory. John Wiley & Sons, New
York.
K. Kipper Schuler. 2006. VerbNet: A Broad-Coverage,
Comprehensive Verb Lexicon. Ph.D. thesis, University
of Pennsylvania.
A. Kups?c? and A. Abeille?. 2008. Growing treelex. In
Alexander Gelbkuh, editor, Computational Linguis-
tics and Intelligent Text Processing, volume 4919 of
Lecture Notes in Computer Science, pages 28?39.
Springer Berlin / Heidelberg.
J.-C. Lamirel, A. Phuong Ta, and M. Attik. 2008. Novel
Labeling Strategies for Hierarchical Representation of
Multidimensional Data Analysis Results. In AIA -
IASTED, Innbruck, Autriche.
J. C. Lamirel, P. Cuxac, and R. Mall. 2011a. A new
efficient and unbiased approach for clustering quality
evaluation. In QIMIE?11, PaKDD, Shenzen, China.
J.-C. Lamirel, R. Mall, P. Cuxac, and G. Safi. 2011b.
Variations to incremental growing neural gas algo-
rithm based on label maximization. In Neural Net-
works (IJCNN), The 2011 International Joint Confer-
ence on, pages 956 ?965.
B. Levin. 1993. English Verb Classes and Alternations:
a preliminary investigation. University of Chicago
Press, Chicago and London.
T. Martinetz and K. Schulten. 1991. A ?Neural-Gas?
Network Learns Topologies. Artificial Neural Net-
works, I:397?402.
P. Merlo, S. Stevenson, V. Tsang, and G. Allaria. 2002.
A multilingual paradigm for automatic verb classifica-
tion. In ACL, pages 207?214.
C. Messiant. 2008. A subcategorization acquisition sys-
tem for French verbs. In Proceedings of the ACL-
08: HLT Student Research Workshop, pages 55?60,
Columbus, Ohio, June. Association for Computational
Linguistics.
C. Mouton. 2010. Ressources et me?thodes semi-
supervise?es pour l?analyse se?mantique de textes en
fran cais. Ph.D. thesis, Universite? Paris 11 - Paris Sud
UFR d?informatique.
L. Nicolas, B. Sagot, E?. de La Clergerie, and J. Farre?.
2008. Computer aided correction and extension of a
syntactic wide-coverage lexicon. In Proc. of CoLing
2008, Manchester, UK, August.
A. Oishi and Y. Matsumoto. 1997. Detecting the orga-
nization of semantic subclasses of Japanese verbs. In-
ternational Journal of Corpus Linguistics, 2(1):65?89,
october.
Y. Prudent and A. Ennaji. 2005. An incremental grow-
ing neural gas learns topologies. In Neural Networks,
2005. IJCNN ?05. Proceedings. 2005 IEEE Interna-
tional Joint Conference on, volume 2, pages 1211?
1216.
J. H. Randall. 2010. Linking. Studies in Natural Lan-
guage and Linguistic Theory. Springer, Dordrecht.
S. E. Robertson and K. S. Jones. 1976. Relevance
weighting of search terms. Journal of the American
Society for Information Science, 27(3):129?146.
S. Schulte im Walde. 2003. Experiments on the Auto-
matic Induction of German Semantic Verb Classes.
Ph.D. thesis, Institut fu?r Maschinelle Sprachverar-
beitung, Universita?t Stuttgart. Published as AIMS Re-
port 9(2).
S. Schulte im Walde. 2006. Experiments on the au-
tomatic induction of german semantic verb classes.
Computational Linguistics, 32(2):159?194.
L. Sun, A. Korhonen, T. Poibeau, and C. Messiant. 2010.
Investigating the cross-linguistic potential of verbnet:
style classification. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
862
COLING ?10, pages 1056?1064, Stroudsburg, PA,
USA. Association for Computational Linguistics.
R. S. Swier and S. Stevenson. 2005. Exploiting
a verb lexicon in automatic semantic role labelling.
In HLT/EMNLP. The Association for Computational
Linguistics.
K. van den Eynde and P. Mertens. 2003. La valence :
l?approche pronominale et son application au lexique
verbal. Journal of French Language Studies, 13:63?
104.
863
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 424?434,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Surface Realisation from Knowledge-Bases
Bikash Gyawali
Universite? de Lorraine, LORIA
Villers-le`s-Nancy, F-54600, France
bikash.gyawali@loria.fr
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy, F-54500, France
claire.gardent@loria.fr
Abstract
We present a simple, data-driven approach
to generation from knowledge bases (KB).
A key feature of this approach is that
grammar induction is driven by the ex-
tended domain of locality principle of
TAG (Tree Adjoining Grammar); and that
it takes into account both syntactic and
semantic information. The resulting ex-
tracted TAG includes a unification based
semantics and can be used by an existing
surface realiser to generate sentences from
KB data. Experimental evaluation on the
KBGen data shows that our model outper-
forms a data-driven generate-and-rank ap-
proach based on an automatically induced
probabilistic grammar; and is comparable
with a handcrafted symbolic approach.
1 Introduction
In this paper we present a grammar based ap-
proach for generating from knowledge bases (KB)
which is linguistically principled and conceptually
simple. A key feature of this approach is that
grammar induction is driven by the extended do-
main of locality principle of TAG (Tree Adjoining
Grammar) and takes into account both syntactic
and semantic information. The resulting extracted
TAGs include a unification based semantics and
can be used by an existing surface realiser to gen-
erate sentences from KB data.
To evaluate our approach, we use the bench-
mark provided by the KBGen challenge (Banik
et al, 2012; Banik et al, 2013), a challenge
designed to evaluate generation from knowledge
bases; where the input is a KB subset; and where
the expected output is a complex sentence convey-
ing the meaning represented by the input. When
compared with two other systems having taken
part in the KBGen challenge, our system outper-
forms a data-driven, generate-and-rank approach
based on an automatically induced probabilis-
tic grammar; and produces results comparable to
those obtained by a symbolic, rule based approach.
Most importantly, we obtain these results using a
general purpose approach that we believe is sim-
pler and more transparent than current state of the
art surface realisation systems generating from KB
or DB data.
2 Related Work
Our work is related to work on concept to text gen-
eration.
Earlier work on concept to text generation
mainly focuses on generation from logical forms
using rule-based methods. (Wang, 1980) uses
hand-written rules to generate sentences from an
extended predicate logic formalism; (Shieber et
al., 1990) introduces a head-driven algorithm for
generating from logical forms; (Kay, 1996) de-
fines a chart based algorithm which enhances effi-
ciency by minimising the number of semantically
incomplete phrases being built; and (Shemtov,
1996) presents an extension of the chart based gen-
eration algorithm presented in (Kay, 1996) which
supports the generation of multiple paraphrases
from underspecified semantic input. In all these
approaches, grammar and lexicon are developed
manually and it is assumed that the lexicon as-
sociates semantic sub-formulae with natural lan-
guage expressions. Our approach is similar to
these approaches in that it assumes a grammar en-
coding a compositional semantics. It differs from
them however in that, in our approach, grammar
and lexicon are automatically acquired from the
data.
With the development of the semantic web and
the proliferation of knowledge bases, generation
from knowledge bases has attracted increased in-
terest and so called ontology verbalisers have
been proposed which support the generation of
text from (parts of) knowledge bases. One main
424
strand of work maps each axiom in the knowledge
base to a clause. Thus the OWL verbaliser inte-
grated in the Prote?ge? tool (Kaljurand and Fuchs,
2007) provides a verbalisation of every axiom
present in the ontology under consideration and
(Wilcock, 2003) describes an ontology verbaliser
using XML-based generation. As discussed in
(Power and Third, 2010), one important limita-
tion of these approaches is that they assume a
simple deterministic mapping between knowledge
representation languages and some controlled nat-
ural language (CNL). Specifically, the assump-
tion is that each atomic term (individual, class,
property) maps to a word and each axiom maps
to a sentence. As a result, the verbalisation of
larger ontology parts can produce very unnatural
text such as, Every cat is an animal. Every dog
is an animal. Every horse is an animal. Every
rabbit is an animal. More generally, the CNL
based approaches to ontology verbalisation gen-
erate clauses (one per axiom) rather than complex
sentences and thus cannot adequately handle the
verbalisation of more complex input such as the
KBGen data where the KB input often requires the
generation of a complex sentence rather than a se-
quence of base clauses.
To generate more complex output from KB
data, several alternative approaches have been pro-
posed.
The MIAKT project (Bontcheva and Wilks.,
2004) and the ONTOGENERATION project
(Aguado et al, 1998) use symbolic NLG tech-
niques to produce textual descriptions from some
semantic information contained in a knowledge
base. Both systems require some manual in-
put (lexicons and domain schemas). More so-
phisticated NLG systems such as TAILOR (Paris,
1988), MIGRAINE (Mittal et al, 1994), and
STOP (Reiter et al, 2003) offer tailored output
based on user/patient models. While offering
more flexibility and expressiveness, these systems
are difficult to adapt by non-NLG experts because
they require the user to understand the architec-
ture of the NLG systems (Bontcheva and Wilks.,
2004). Similarly, the NaturalOWL system (Gala-
nis et al, 2009) has been proposed to generate flu-
ent descriptions of museum exhibits from an OWL
ontology. This approach however relies on exten-
sive manual annotation of the input data.
The SWAT project has focused on producing
descriptions of ontologies that are both coherent
and efficient (Williams and Power, 2010). For in-
stance, instead of the above output, the SWAT sys-
tem would generate the sentence: The following
are kinds of animals: cats, dogs, horses and rab-
bits. . In this approach too however, the verbaliser
output is strongly constrained by a simple Definite
Clause Grammar covering simple clauses and sen-
tences verbalising aggregation patterns such as the
above. More generally, the sentences generated by
ontology verbalisers cover a limited set of linguis-
tics constructions; the grammar used is manually
defined; and the mapping between semantics and
strings is assumed to be deterministic (e.g., a verb
maps to a relation and a noun to a concept). In
constrast, we propose an approach which can gen-
erate complex sentences from KB data; where the
grammar is acquired from the data; and where no
assumption is made about the mapping between
semantics and NL expressions.
Recent work has focused on data-driven gener-
ation from frames, lambda terms and data base en-
tries.
(DeVault et al, 2008) describes an approach for
generating from the frames produced by a dialog
system. They induce a probabilistic Tree Adjoin-
ing Grammar from a training set algning frames
and sentences using the grammar induction tech-
nique of (Chiang, 2000) and use a beam search
that uses weighted features learned from the train-
ing data to rank alternative expansions at each
step.
(Lu and Ng, 2011) focuses on generating nat-
ural language sentences from logical form (i.e.,
lambda terms) using a synchronous context-free
grammar. They introduce a novel synchronous
context free grammar formalism for generating
from lambda terms; induce such a synchronous
grammar using a generative model; and extract the
best output sentence from the generated forest us-
ing a log linear model.
(Wong and Mooney, 2007; Lu et al, 2009)
focuses on generating from variable-free tree-
structured representations such as the CLANG for-
mal language used in the ROBOCUP competition
and the database entries collected by (Liang et
al., 2009) for weather forecast generation and for
the air travel domain (ATIS dataset) by (Dahl et
al., 1994). (Wong and Mooney, 2007) uses syn-
chronous grammars to transform a variable free
tree structured meaning representation into sen-
tences. (Lu et al, 2009) uses a Conditional Ran-
425
The function of a gated channel is to release particles from the endoplasmic reticulum
:TRIPLES (
(|Release-Of-Calcium646| |object| |Particle-In-Motion64582|)
(|Release-Of-Calcium646| |base| |Endoplasmic-Reticulum64603|)
(|Gated-Channel64605| |has-function||Release-Of-Calcium646|)
(|Release-Of-Calcium646| |agent| |Gated-Channel64605|))
:INSTANCE-TYPES
(|Particle-In-Motion64582| |instance-of| |Particle-In-Motion|)
(|Endoplasmic-Reticulum64603| |instance-of| |Endoplasmic-Reticulum|)
(|Gated-Channel64605| |instance-of| |Gated-Channel|)
|Release-Of-Calcium646| |instance-of| |Release-Of-Calcium|))
:ROOT-TYPES (
(|Release-Of-Calcium646| |instance-of| |Event|)
(|Particle-In-Motion64582| |instance-of| |Entity|)
(|Endoplasmic-Reticulum64603| |instance-of| |Entity|)
(|Gated-Channel64605| |instance-of| |Entity|)))
Figure 1: Example KBGEN Scenario
dom Field to generate from the same meaning rep-
resentations.
Finally, more recent papers propose approaches
which perform both surface realisation and con-
tent selection. (Angeli et al, 2010) proposes a log
linear model which decomposes into a sequence
of discriminative local decisions. The first classi-
fier determines which records to mention; the sec-
ond, which fields of these records to select; and the
third, which words to use to verbalise the selected
fields. (Kim and Mooney, 2010) uses a genera-
tive model for content selection and verbalises the
selected input using WASP
?1
, an existing gener-
ator. Finally, (Konstas and Lapata, 2012b; Kon-
stas and Lapata, 2012a) develop a joint optimi-
sation approach for content selection and surface
realisation using a generic, domain independent
probabilistic grammar which captures the struc-
ture of the database and the mapping from fields
to strings. They intersect the grammar with a lan-
guage model to improve fluency; use a weighted
hypergraph to pack the derivations; and find the
best derivation tree using Viterbi algorithm.
Our approach differs from the approaches
which assume variable free tree structured repre-
sentations (Wong and Mooney, 2007; Lu et al,
2009) and data-based entries (Kim and Mooney,
2010; Konstas and Lapata, 2012b; Konstas and
Lapata, 2012a) in that it handles graph-based, KB
input and assumes a compositional semantics. It
is closest to (DeVault et al, 2008) and (Lu and
Ng, 2011) who extract a grammar encoding syn-
tax and semantics from frames and lambda terms
respectively. It differs from the former however in
that it enforces a tighter syntax/semantics integra-
tion by requiring that the elementary trees of our
extracted grammar encode the appropriate linking
information. While (DeVault et al, 2008) extracts
a TAG grammar associating each elementary tree
with a semantics, we additionnally require that
these trees encode the appropriate linking between
syntactic and semantic arguments thereby restrict-
ing the space of possible tree combinations and
drastically reducing the search space. Although
conceptually related to (Lu and Ng, 2011), our ap-
proach extracts a unification based grammar rather
than one with lambda terms. The extraction pro-
cess and the generation algorithms are also funda-
mentally different. We use a simple mainly sym-
bolic approach whereas they use a generative ap-
proach for grammar induction and a discriminative
approach for sentence generation.
3 The KBGen Task
The KBGen task was introduced as a new shared
task at Generation Challenges 2013 (Banik et al,
2013)
1
and aimed to compare different generation
systems on KB data. Specifically, the task is to
verbalise a subset of a knowledge base. For in-
stance, the KB input shown in Figure 1 can be ver-
balised as:
(1) The function of a gated channel is to release
particles from the endoplasmic reticulum
The KB subsets forming the KBGen input data
were pre-selected from the AURA biology knowl-
edge base (Gunning et al, 2010), a knowledge
base about biology which was manually encoded
by biology teachers and encodes knowledge about
events, entities, properties and relations where
relations include event-to-entity, event-to-event,
1
http://www.kbgen.org
426
NP
GC
DT NN NN
a gated channel
instance-of(GC,Gated-Channel)
S
RoC1
NP?
GC
VP
RoC1
RoC
VBZ
RoC
NP?
PM
releases
instance-of(RoC,Release-of-Calcium)
object(RoC,PM)
agent(RoC,GC)
NP
PM
particles
instance-of(PM,Particle-In-Motion)
VP
RoC
VP
?RoC
PP
IN NP?
ER
from
base(RoC,ER)
NP
ER
DT NN NN
the endoplasmic reticulum
instance-of(ER,Endoplasmic-Reticulum)
Figure 2: Example FB-LTAG with Unification-Based Semantics. Dotted lines indicate substitution and
adjunction operations between trees. The variables decorating the tree nodes (e.g., GC) abbreviate fea-
ture structures of the form [idx : V ] where V is a unification variable shared with the semantics.
event-to-property and entity-to-property relations.
AURA uses a frame-based knowledge representa-
tion and reasoning system called Knowledge Ma-
chine (Clark and Porter, 1997) which was trans-
lated into first-order logic with equality and from
there, into multiple different formats including
SILK (Grosof, 2012) and OWL2 (Motik et al,
2009). It is available for download in various for-
mats including OWL
2
.
4 Generating from the KBGen
Knowledge-Base
To generate from the KBGen data, we induce a
Feature-Based Lexicalised Tree Adjoining Gram-
mar (FB-LTAG, (Vijay-Shanker and Joshi, 1988))
augmented with a unification-based semantics
(Gardent and Kallmeyer, 2003) from the training
data. We then use this grammar and an existing
surface realiser to generate from the test data.
4.1 Feature-Based Lexicalised Tree
Adjoining Grammar
Figure 2 shows an example FB-LTAG augmented
with a unification-based semantics.
Briefly, an FB-LTAG consists of a set of ele-
mentary trees which can be either initial or auxil-
iary. Initial trees are trees whose leaves are labeled
with substitution nodes (marked with a down-
arrow) or terminal categories. Auxiliary trees are
distinguished by a foot node (marked with a star)
2
http://www.ai.sri.com/halo/
halobook2010/exported-kb/biokb.html
whose category must be the same as that of the
root node. In addition, in an FB-LTAG, each el-
ementary tree is anchored by a lexical item (lexi-
calisation) and the nodes in the elementary trees
are decorated with two feature structures called
top and bottom which are unified during deriva-
tion. Two tree-composition operations are used
to combine trees namely, substitution and adjunc-
tion. While substitution inserts a tree in a substi-
tution node of another tree, adjunction inserts an
auxiliary tree into a tree. In terms of unifications,
substitution unifies the top feature structure of the
substitution node with the top feature structure of
the root of the tree being substituted in. Adjunc-
tion unifies the top feature structure of the root of
the tree being adjoined with the top feature struc-
ture of the node being adjoined to; and the bottom
feature structure of the foot node of the auxiliary
tree being adjoined with the bottom feature struc-
ture of the node being adjoined to.
In an FB-LTAG augmented with a unification-
based semantics, each tree is associated with a
semantics i.e., a set of literals whose arguments
may be constants or unification variables. The
semantics of a derived tree is the union of the
semantics of the tree contributing to its deriva-
tion modulo unification. Importantly, semantic
variables are shared with syntactic variables
(i.e., variables occurring in the feature structures
decorating the tree nodes) so that when trees are
combined, the appropriate syntax/semantics link-
ing is enforced. For instance given the semantics:
427
instance-of(RoC,Release-Of-Calcium),
object(RoC,PM),agent(RoC,GC),base(RoC,ER),
instance-of(ER,Endoplasmic-Reticulum),
instance-of(GC,Gated-Channel),
instance-of(PM,Particle-In-Motion)
the grammar will generate A gated channel re-
leases particles from the endoplasmic reticulum
but not e.g., Particles releases a gated channel
from the endoplasmic reticulum.
4.2 Grammar Extraction
We extract our FB-LTAG with unification seman-
tics from the KBGen training data in two main
steps. First, we align the KB data with the input
string. Second, we induce a Tree Adjoining Gram-
mar augmented with a unification-based semantics
from the aligned data.
4.2.1 Alignment
Given a Sentence/Input pair (S, I) provided by the
KBGen Challenge, the alignment procedure asso-
ciates each entity and event variable in I to a sub-
string in S. To do this, we use the entity and the
event lexicon provided by the KBGen organiser.
The event lexicon maps event types to verbs, their
inflected forms and nominalizations while the en-
tity lexicon maps entity types to a noun and its
plural form. For instance, the lexicon entries for
the event and entity types shown in Figure 1 are as
shown in Figure 3.
For each entity and each event vari-
able V in I , we retrieve the corresponding
type (e.g., Particle-In-Motion for
Particle-In-Motion64582); search
the KBGen lexicon for the corresponding phrases
(e.g., molecule in motion,molecules in motion);
and associate V with the phrase in S which
matches one of these phrases. Figure 3 shows
an example lexicon and the resulting alignment
obtained for the scenario shown in Figure 1. Note
that there is not always an exact match between
the phrase associated in the KBGen lexicon with
a type and the phrase occurring in the training
sentence. To account for this, we use some
additional similarity based heuristics to identify
the phrase in the input string that is most likely
to be associated with a variable lacking an exact
match in the input string. E.g., for entity variables
(e.g., Particle-In-Motion64582), we
search the input string for nouns (e.g., particles)
whose overlap with the variable type (e.g.,
Particle-In-Motion) is not empty.
4.2.2 Inducing a based FB-LTAG from the
aligned data
To extract a Feature-Based Lexicalised Tree
Adjoining Grammar (FB-LTAG) from the KBGen
data, we parse the sentences of the training cor-
pus; project the entity and event variables to the
syntactic projection of the strings they are aligned
with; and extract the elementary trees of the result-
ing FB-LTAG from the parse tree using semantic
information. Figure 4 shows the trees extracted
from the scenario given in Figure 1.
To associate each training example sentence
with a syntactic parse, we use the Stanford parser.
After alignment, the entity and event variables oc-
curring in the input semantics are associated with
substrings of the yield of the syntactic parse tree.
We project these variables up the syntactic tree to
reflect headedness. A variable aligned with a noun
is projected to the NP level or to the immediately
dominating PP if it occurs in the subtree domi-
nated by the leftmost daughter of that PP. A vari-
able aligned with a verb is projected to the first S
node immediately dominating that verb or, in the
case of a predicative sentence, to the root of that
sentence
3
.
Once entity and event variables have been pro-
jected up the parse trees, we extract elementary
FB-LTAG trees and their semantics from the input
scenario as follows.
First, the subtrees whose root node is indexed
with an entity variable are extracted. This results
in a set of NP and PP trees anchored with entity
names and associated with the predication true of
the indexing variable.
Second, the subtrees capturing relations be-
tween variables are extracted. To perform this ex-
traction, each input variable X is associated with a
set of dependent variables i.e., the set of variables
Y such that X is related to Y (R(X,Y )). The
minimal tree containing all and only the dependent
variables D(X) of a variable X is then extracted
and associated with the set of literals ? such that
? = {R(Y,Z) | (Y = X?Z ? D(X))?(Y,Z ?
D(X))}. This procedure extracts the subtrees re-
lating the argument variables of a semantic func-
tors such as an event or a role e.g., a tree describ-
ing a verb and its arguments as shown in the top
3
Initially, we used the head information provided by the
Stanford parser. In practice however, we found that the
heuristics we defined to project semantic variables to the cor-
responding syntactic projection were more accurate and bet-
ter supported our grammar extraction process.
428
Particle-In-Motion molecule in motion,molecules in motion
Endoplasmic-Reticulum endoplasmic reticulum,endoplasmic reticulum
Gated-Channel gated Channel,gated Channels
Release-Of-Calcium releases,release,released,release
The function of a (gated channel, Gated-Channel64605) is to (release,
Release-Of-Calcium646) (particles, Particle-In-Motion64582) from the (endoplas-
mic reticulum, Endoplasmic-Reticulum64603 )
Figure 3: Example Entries from the KBGen Lexicon and example alignment
S
RoC3
NP VP
RoC3
RoC2
NP PP VBZ S
RoC2
RoC1
DT NN IN NP?
GC
is VP
RoC1
RoC
the fn of TO VB
RoC
NP?
PM
PP
to release IN NP?
ER
from
instance-of(RoC,Release-of-Calcium)
object(RoC,PM)
base(RoC,ER)
has-function(GC,RoC)
agent(RoC,GC)
NP
GC
DT NN NN
a gated channel
instance-of(GC,Gated-Channel)
NP
PM
particles
instance-of(PM,Particle-In-Motion)
NP
ER
DT NN NN
the endoplasmic reticulum
instance-of(ER,Endoplasmic-Reticulum)
Figure 4: Extracted Grammar for ?The function of a gated channel is to release particles from the endoplasmic reticulum?.
Variable names have been abbreviated and the KBGen tuple notation converted to terms so as to fit the input format expected by
our surface realiser.
429
part of Figure 4. Note that such a tree may cap-
ture a verb occurring in a relative or a subordinate
clause (together with its arguments) thus allowing
for complex sentences including a relative or re-
lating a main and a subordinate clause.
The resulting grammar extracted from the parse
trees (cf. e.g., Figure 4) is a Feature-Based
Tree Adjoining Grammar with a Unification-based
compositional semantics as described in (Gardent
and Kallmeyer, 2003). In particular, our gram-
mars differs from the traditional probabilistic Tree
Adjoining Grammar extracted as described in e.g.,
(Chiang, 2000) in that they encode both syntax and
semantics rather than just syntax. They also differ
from the semantic FB-TAG extracted by (DeVault
et al, 2008) in that (i) they encode the linking be-
tween syntactic and semantic arguments; (ii) they
allow for elementary trees spanning discontiguous
strings (e.g., The function of X is to release Y); and
(iii) they enforce the semantic principle underly-
ing TAG namely that an elementary tree contain-
ing a syntactic functor also contains its syntactic
arguments.
4.3 Generation
To generate with the grammar extracted from the
KBGen data, we use the GenI surface realiser (Gar-
dent et al, 2007). Briefly, given an input seman-
tics and a FB-LTAG with a unification based se-
mantics, GenI selects all grammar entries whose
semantics subsumes the input semantics; com-
bines these entries using the FB-LTAG combina-
tion operations (i.e., adjunction and substitution);
and outputs the yield of all derived trees which are
syntactically complete and whose semantics is the
input semantics. To rank the generator output, we
train a language model on the GeniA corpus
4
, a
corpus of 2000 MEDLINE asbtracts about biol-
ogy containing more than 400000 words (Kim et
al., 2003) and use this model to rank the generated
sentences by decreasing probability.
Thus for instance, given the input semantics
shown in Figure 1 and the grammar depicted in
Figure 4, the surface realiser will select all of these
trees; combine them using FB-LTAG substitution
operation; and output as generated sentence the
yield of the resulting derived tree namely the sen-
tence The function of a gated channel is to release
particles from the endoplasmic reticulum.
However, this procedure only works if the en-
4
http://www.nactem.ac.uk/genia/
tries necessary to generate from the given input
are present in the grammar. To handle new, un-
seen input, we proceed in two ways. First, we try
to guess a grammar entry from the shape of the in-
put and the existing grammar. Second, we expand
the grammar by decomposing the extracted trees
into simpler ones.
4.4 Guessing new grammar entries.
Given the limited size of the training data, it is of-
ten the case that input from the test data will have
no matching grammar unit. To handle such pre-
viously unseen input, we start by partitioning the
input semantics into sub-semantics corresponding
to events, entities and role.
For each entity variable X of type Type, we
create a default NP tree whose semantics is a lit-
eral of the form instance-of(X,Type).
For event variables, we search the lexicon for
an entry with a matching or similar semantics i.e.,
an entry with the same number and same type of
literals (literals with same arity and with identical
relations). When one is found, a grammar entry is
constructed for the unseen event variable by sub-
stituting the event type of the matching entry with
the type of the event variable. For instance, given
the input semantics instance-of(C,Carry), object(C,X),
base(C,Y), has-function(Z,C), agent(C,Z), this procedure
will create a grammar entry identical to that shown
at the top of Figure 4 except that the event type
Release-of-Calcium is changed to Carry and the ter-
minal release to the word form associated in the
KBGen lexicon with this concept, namely to the
verb carry.
4.5 Expanding the Grammar
While the extracted grammar nicely captures pred-
icate/argument dependencies, it is very specific to
the items seen in the training data. To reduce over-
fitting, we generalise the extracted grammar by ex-
tracting from each event tree, subtrees that cap-
ture structures with fewer arguments and optional
modifiers.
For each event tree ? extracted from the train-
ing data which contains a subject-verb-object sub-
tree ?
?
, we add ?
?
to the grammar and associate it
with the semantics of ? minus the relations associ-
ated with the arguments that have been removed.
For instance, given the extracted tree for the sen-
tence ?Aquaporin facilitates the movement of wa-
ter molecules through hydrophilic channels.?, this
430
procedure will construct a new grammar tree cor-
responding to the subphrase ?Aquaporin facili-
tates the movement of water molecules?.
We also construct both simpler event trees and
optional modifiers trees by extracting from event
trees, PP trees which are associated with a re-
lational semantics. For instance, given the tree
shown in Figure 4, the PP tree associated with
the relation base(RoC,ET) is removed thus creating
two new trees as illustrated in Figure 5: an S tree
corresponding to the sentence The function of a
gated channel is to release particles and an aux-
iliary PP tree corresponding to the phrase from
the endoplasmic reticulum. Similarly in the above
example, a PP tree corresponding to the phrase
?through hydrophilic channels.? will be extracted.
As with the base grammar, missing grammar
entries are guessed from the expanded grammar.
However we do this only in cases where a correct
grammar entry cannot be guessed from the base
grammar.
5 Experimental Setup
We evaluate our approach on the KBGen data and
compare it with the KBGen reference and two other
systems having taken part to the KBGen challenge.
5.1 Training and test data.
Following a practice introduced by (Angeli et al,
2010), we use the term scenario to denote a KB
subset paired with a sentence. The KBGen bench-
mark contains 207 scenarii for training and 72 for
testing. Each KB subset consists of a set of triples
and each scenario contains on average 16 triples
and 17 words.
5.2 Systems
We evaluate three configurations of our approach
on the KBGen test data: one without grammar ex-
pansion (BASE); a second with a manual grammar
expansion MANEXP; and a third one with auto-
mated grammar expansion AUTEXP. We compare
the results obtained with those obtained by two
other systems participating in the KBGen chal-
lenge, namely the UDEL system, a symbolic rule
based system developed by a group of students at
the University of Delaware; and the IMS system,
a statistical system using a probabilistic grammar
induced from the training data.
5.3 Metrics.
We evaluate system output automatically, using
the BLEU-4 modified precision score (Papineni et
al., 2002) with the human written sentences as ref-
erence. We also report results from a human based
evaluation. In this evaluation, participants were
asked to rate sentences along three dimensions:
fluency (Is the text easy to read?), grammatical-
ity and meaning similarity or adequacy (Does the
meaning conveyed by the generated sentence cor-
respond to the meaning conveyed by the reference
sentence?). The evaluation was done on line us-
ing the LG-Eval toolkit (Kow and Belz, 2012),
subjects used a sliding scale from -50 to +50 and
a Latin Square Experimental Design was used to
ensure that each evaluator sees the same number
of outputs from each system and for each test set
item. 12 subjects participated in the evaluation and
3 judgments were collected for each output.
6 Results and Discussion
System All Covered Coverage # Trees
IMS 0.12 0.12 100%
UDEL 0.32 0.32 100%
Base 0.04 0.39 30.5% 371
ManExp 0.28 0.34 83 % 412
AutExp 0.29 0.29 100% 477
Figure 6: BLEU scores and Grammar Size (Num-
ber of Elementary TAG trees
Table 6 summarises the results of the automatic
evaluation and shows the size (number of elemen-
tary TAG trees) of the grammars extracted from
the KBGen data.
The average BLEU score is given with respect
to all input (All) and to those inputs for which
the systems generate at least one sentence (Cov-
ered). While both the IMS and the UDEL system
have full coverage, our BASE system strongly un-
dergenerates failing to account for 69.5% of the
test data. However, because the extracted gram-
mar is linguistically principled and relatively com-
pact, it is possible to manually edit it. Indeed, the
MANEXP results show that, by adding 41 trees to
the grammar, coverage can be increased by 52.5
points reaching a coverage of 83%. Finally, the
AUTEXP results demonstrate that the automated
expansion mechanism permits achieving full cov-
erage while keeping a relative small grammar (477
trees).
431
SRoC3
NP VP
RoC3
RoC2
NP PP VBZ S
RoC2
RoC1
DT NN IN NP?
GC
is VP
RoC1
RoC
the fn of TO VB
RoC
NP?
PM
to release
instance-of(RoC,Release-of-Calcium)
object(RoC,PM)
has-function(GC,RoC)
agent(RoC,GC)
VP
RoC
VP
?,RoC
PP
IN NP?
ER
from
base(RoC,ER)
Figure 5: Trees Added by the Expansion Process
Fluency Grammaticality Meaning Similarity
System Mean Homogeneous Subsets Mean Homogeneous Subsets Mean Homogeneous Subsets
UDEL 4.36 A 4.48 A 3.69 A
AutExp 3.45 B 3.55 B 3.65 A
IMS 1.91 C 2.05 C 1.31 B
Figure 7: Human Evaluation Results on a scale of 0 to 5. Homogeneous subsets are determined using
Tukey?s Post Hoc Test with p < 0.05
In terms of BLEU score, the best version of our
system (AUTEXP) outperforms the probabilistic
approach of IMS by a large margin (+0.17) and
produces results similar to the fully handcrafted
UDEL system (-0.03).
In sum, our approach permits obtaining BLEU
scores and a coverage which are similar to that
obtained by a hand crafted system and outper-
forms a probabilistic approach. One key feature of
our approach is that the grammar extracted from
the training data is linguistically principled in that
it obeys the extended locality principle of Tree
Adjoining Grammars. As a result, the extracted
grammar is compact and can be manually modi-
fied to fit the need of an application as shown by
the good results obtained when using the MAN-
EXP configuration.
We now turn to the results of the human eval-
uation. Table 7 summarises the results whereby
systems are grouped by letters when there is no
significant difference between them (significance
level: p < 0.05). We used ANOVAs and post-
hoc Tukey tests to test for significance. The dif-
ferences between systems are statistically signifi-
cant throughout except for meaning similarity (ad-
equacy) where UDEL and our system are on the
same level. Across the metrics, our system consis-
tently ranks second behind the symbolic, UDEL
system and before the statistical IMS one thus con-
firming the ranking based on BLEU.
7 Conclusion
In Tree Adjoining Grammar, the extended domain
of locality principle ensures that TAG trees group
together in a single structure a syntactic predi-
cate and its arguments. Moreover, the semantic
principle requires that each elementary tree cap-
tures a single semantic unit. Together these two
principles ensure that TAG elementary trees cap-
ture basic semantic units and their dependencies.
In this paper, we presented a grammar extraction
approach which ensures that extracted grammars
comply with these two basic TAG principles. Us-
ing the KBGen benchmark, we then showed that
the resulting induced FB-LTAG compares favor-
ably with competing symbolic and statistical ap-
proaches when used to generate from knowledge
base data.
In the current version of the generator, the
output is ranked using a simple language model
trained on the GENIA corpus. We observed that
this often fails to return the best output in terms
of BLEU score, fluency, grammaticality and/or
meaning. In the future, we plan to remedy this us-
ing a ranking approach such as proposed in (Vell-
dal and Oepen, 2006; White and Rajkumar, 2009).
432
References
G. Aguado, A. Ban?o?n, J. Bateman, S. Bernardos,
M. Ferna?ndez, A. Go?mez-Pe?rez, E. Nieto, A. Olalla,
R. Plaza, and A. Sa?nchez. 1998. Ontogeneration:
Reusing domain and linguistic ontologies for span-
ish text generation. In Workshop on Applications
of Ontologies and Problem Solving Methods, ECAI,
volume 98.
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 502?512. Association for Com-
putational Linguistics.
Eva Banik, Claire Gardent, Donia Scott, Nikhil Dinesh,
and Fennie Liang. 2012. Kbgen: Text generation
from knowledge bases as a new shared task. In Pro-
ceedings of the seventh International Natural Lan-
guage Generation Conference, pages 141?145. As-
sociation for Computational Linguistics.
Eva Banik, Claire Gardent, Eric Kow, et al 2013. The
kbgen challenge. In Proceedings of the 14th Eu-
ropean Workshop on Natural Language Generation
(ENLG), pages 94?97.
K. Bontcheva and Y. Wilks. 2004. Automatic re-
port generation from ontologies: the miakt ap-
proach. In Ninth International Conference on Appli-
cations of Natural Language to Information Systems
(NLDB?2004). Lecture Notes in Computer Science
3136, Springer, Manchester, UK.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th AnnualMeeting on Associa-
tion for Computational Linguistics, pages 456?463.
Association for Computational Linguistics.
Peter Clark and Bruce Porter. 1997. Building con-
cept representations from reusable components. In
AAAI/IAAI, pages 369?376. Citeseer.
Deborah A Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the atis
task: The atis-3 corpus. In Proceedings of the work-
shop on Human Language Technology, pages 43?48.
Association for Computational Linguistics.
David DeVault, David Traum, and Ron Artstein. 2008.
Making grammar-based generation easier to deploy
in dialogue systems. In Proceedings of the 9th SIG-
dial Workshop on Discourse and Dialogue, pages
198?207. Association for Computational Linguis-
tics.
D. Galanis, G. Karakatsiotis, G. Lampouras, and I. An-
droutsopoulos. 2009. An open-source natural lan-
guage generator for owl ontologies and its use in
prote?ge? and second life. In Proceedings of the 12th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Demonstra-
tions Session, pages 17?20. Association for Compu-
tational Linguistics.
Claire Gardent and Laura Kallmeyer. 2003. Semantic
construction in feature-based tag. In Proceedings of
the tenth conference on European chapter of the As-
sociation for Computational Linguistics-Volume 1,
pages 123?130. Association for Computational Lin-
guistics.
Claire Gardent, Eric Kow, et al 2007. A symbolic ap-
proach to near-deterministic surface realisation us-
ing tree adjoining grammar. In ACL, volume 7,
pages 328?335.
B. Grosof. 2012. The silk project: Semantic infer-
encing on large knowledge. Technical report, SRI.
http://silk.semwebcentral.org/.
D. Gunning, V. K. Chaudhri, P. Clark, K. Barker, Shaw-
Yi Chaw, M. Greaves, B. Grosof, A. Leung, D. Mc-
Donald, S. Mishra, J. Pacheco, B. Porter, A. Spauld-
ing, D. Tecuci, and J. Tien. 2010. Project halo up-
date - progress toward digital aristotle. AIMagazine,
Fall:33?58.
K. Kaljurand and N.E. Fuchs. 2007. Verbalizing
owl in attempto controlled english. Proceedings of
OWLED07.
Martin Kay. 1996. Chart generation. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 200?204. Association
for Computational Linguistics.
Joohyun Kim and Raymond J Mooney. 2010. Gen-
erative alignment and semantic parsing for learn-
ing from ambiguous supervision. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 543?551. Associ-
ation for Computational Linguistics.
J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsu-
jii. 2003. Genia corpusa semantically annotated
corpus for bio-textmining. Bioinformatics, 19(suppl
1):i180?i182.
Ioannis Konstas and Mirella Lapata. 2012a. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 369?378. Association for
Computational Linguistics.
Ioannis Konstas and Mirella Lapata. 2012b. Unsuper-
vised concept-to-text generation with hypergraphs.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 752?761. Association for Computational Lin-
guistics.
Eric Kow and Anja Belz. 2012. Lg-eval: A toolkit for
creating online language evaluation experiments. In
LREC, pages 4033?4037.
433
Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of the Joint Conference of the
47th AnnualMeeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1, pages
91?99. Association for Computational Linguistics.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1611?1622. Asso-
ciation for Computational Linguistics.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat-
ural language generation with tree conditional ran-
dom fields. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing: Volume 1-Volume 1, pages 400?409. As-
sociation for Computational Linguistics.
VO Mittal, G. Carenini, and JD Moore. 1994. Gen-
erating patient specific explanations in migraine. In
Proceedings of the eighteenth annual symposium on
computer applications in medical care. McGraw-
Hill Inc.
Boris Motik, Peter F Patel-Schneider, Bijan Parsia,
Conrad Bock, Achille Fokoue, Peter Haase, Rinke
Hoekstra, Ian Horrocks, Alan Ruttenberg, Uli Sat-
tler, et al 2009. Owl 2 web ontology language:
Structural specification and functional-style syntax.
W3C recommendation, 27:17.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
C.L. Paris. 1988. Tailoring object descriptions to a
user?s level of expertise. Computational Linguistics,
14(3):64?78.
R. Power and A. Third. 2010. Expressing owl ax-
ioms by english sentences: dubious in theory, fea-
sible in practice. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 1006?1013. Association for Compu-
tational Linguistics.
E. Reiter, R. Robertson, and L.M. Osman. 2003.
Lessons from a failure: Generating tailored smoking
cessation letters. Artificial Intelligence, 144(1):41?
58.
Hadar Shemtov. 1996. Generation of paraphrases from
ambiguous logical forms. In Proceedings of the 16th
conference on Computational linguistics-Volume 2,
pages 919?924. Association for Computational Lin-
guistics.
Stuart M Shieber, Gertjan Van Noord, Fernando CN
Pereira, and Robert C Moore. 1990. Semantic-
head-driven generation. Computational Linguistics,
16(1):30?42.
Erik Velldal and Stephan Oepen. 2006. Statistical
ranking in tactical generation. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 517?525. Association
for Computational Linguistics.
K. Vijay-Shanker and AK Joshi. 1988. Feature struc-
tures based tree adjoining grammars. In Proceed-
ings of the 12th International Conference on Com-
putational Linguistics, Budapest, Hungary.
Juen-tin Wang. 1980. On computational sentence gen-
eration from logical form. In Proceedings of the
8th conference on Computational linguistics, pages
405?411. Association for Computational Linguis-
tics.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for ccg realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1-
Volume 1, pages 410?419. Association for Compu-
tational Linguistics.
G. Wilcock. 2003. Talking owls: Towards an ontology
verbalizer. Human Language Technology for the Se-
mantic Web and Web Services, ISWC, 3:109?112.
Sandra Williams and Richard Power. 2010. Grouping
axioms for more coherent ontology descriptions. In
Proceedings of the 6th International Natural Lan-
guage Generation Conference (INLG 2010), pages
197?202, Dublin.
Yuk Wah Wong and Raymond J Mooney. 2007. Gen-
eration by inverting a semantic parser that uses sta-
tistical machine translation. In HLT-NAACL, pages
172?179.
434
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 435?445,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Hybrid Simplification using Deep Semantics and Machine Translation
Shashi Narayan
Universite? de Lorraine, LORIA
Villers-le`s-Nancy, F-54600, France
shashi.narayan@loria.fr
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy, F-54500, France
claire.gardent@loria.fr
Abstract
We present a hybrid approach to sentence
simplification which combines deep se-
mantics and monolingual machine transla-
tion to derive simple sentences from com-
plex ones. The approach differs from pre-
vious work in two main ways. First, it
is semantic based in that it takes as in-
put a deep semantic representation rather
than e.g., a sentence or a parse tree. Sec-
ond, it combines a simplification model
for splitting and deletion with a monolin-
gual translation model for phrase substi-
tution and reordering. When compared
against current state of the art methods,
our model yields significantly simpler out-
put that is both grammatical and meaning
preserving.
1 Introduction
Sentence simplification maps a sentence to a sim-
pler, more readable one approximating its con-
tent. Typically, a simplified sentence differs from
a complex one in that it involves simpler, more
usual and often shorter, words (e.g., use instead
of exploit); simpler syntactic constructions (e.g.,
no relative clauses or apposition); and fewer mod-
ifiers (e.g., He slept vs. He also slept). In prac-
tice, simplification is thus often modeled using
four main operations: splitting a complex sen-
tence into several simpler sentences; dropping and
reordering phrases or constituents; substituting
words/phrases with simpler ones.
As has been argued in previous work, sentence
simplification has many potential applications. It
is useful as a preprocessing step for a variety of
NLP systems such as parsers and machine trans-
lation systems (Chandrasekar et al, 1996), sum-
marisation (Knight and Marcu, 2000), sentence
fusion (Filippova and Strube, 2008) and semantic
role labelling (Vickrey and Koller, 2008). It also
has wide ranging potential societal application as a
reading aid for people with aphasis (Carroll et al,
1999), for low literacy readers (Watanabe et al,
2009) and for non native speakers (Siddharthan,
2002).
There has been much work recently on de-
veloping computational frameworks for sentence
simplification. Synchronous grammars have been
used in combination with linear integer program-
ming to generate and rank all possible rewrites of
an input sentence (Dras, 1999; Woodsend and La-
pata, 2011). Machine Translation systems have
been adapted to translate complex sentences into
simple ones (Zhu et al, 2010; Wubben et al, 2012;
Coster and Kauchak, 2011). And handcrafted
rules have been proposed to model the syntactic
transformations involved in simplifications (Sid-
dharthan et al, 2004; Siddharthan, 2011; Chan-
drasekar et al, 1996).
In this paper, we present a hybrid approach to
sentence simplification which departs from this
previous work in two main ways.
First, it combines a model encoding probabil-
ities for splitting and deletion with a monolin-
gual machine translation module which handles
reordering and substitution. In this way, we ex-
ploit the ability of statistical machine translation
(SMT) systems to capture phrasal/lexical substi-
tution and reordering while relying on a dedi-
cated probabilistic module to capture the splitting
and deletion operations which are less well (dele-
tion) or not at all (splitting) captured by SMT ap-
proaches.
Second, our approach is semantic based. While
previous simplification approaches starts from ei-
ther the input sentence or its parse tree, our model
takes as input a deep semantic representation
namely, the Discourse Representation Structure
(DRS, (Kamp, 1981)) assigned by Boxer (Curran
et al, 2007) to the input complex sentence. As we
435
shall see in Section 4, this permits a linguistically
principled account of the splitting operation in that
semantically shared elements are taken to be the
basis for splitting a complex sentence into sev-
eral simpler ones; this facilitates completion (the
re-creation of the shared element in the split sen-
tences); and this provide a natural means to avoid
deleting obligatory arguments.
When compared against current state of the art
methods (Zhu et al, 2010; Woodsend and Lapata,
2011; Wubben et al, 2012), our model yields sig-
nificantly simpler output that is both grammatical
and meaning preserving.
2 Related Work
Earlier work on sentence simplification relied
on handcrafted rules to capture syntactic sim-
plification e.g., to split coordinated and subor-
dinated sentences into several, simpler clauses
or to model active/passive transformations (Sid-
dharthan, 2002; Chandrasekar and Srinivas, 1997;
Bott et al, 2012; Canning, 2002; Siddharthan,
2011; Siddharthan, 2010). While these hand-
crafted approaches can encode precise and linguis-
tically well-informed syntactic transformation (us-
ing e.g., detailed morphological and syntactic in-
formation), they are limited in scope to purely syn-
tactic rules and do not account for lexical simpli-
fications and their interaction with the sentential
context.
Using the parallel dataset formed by Simple En-
glish Wikipedia (SWKP)1 and traditional English
Wikipedia (EWKP)2, more recent work has fo-
cused on developing machine learning approaches
to sentence simplification.
Zhu et al (2010) constructed a parallel cor-
pus (PWKP) of 108,016/114,924 complex/simple
sentences by aligning sentences from EWKP and
SWKP and used the resulting bitext to train a sim-
plification model inspired by syntax-based ma-
chine translation (Yamada and Knight, 2001).
Their simplification model encodes the probabil-
ities for four rewriting operations on the parse
tree of an input sentences namely, substitution, re-
ordering, splitting and deletion. It is combined
with a language model to improve grammatical-
ity and the decoder translates sentences into sim-
1SWKP (http://simple.wikipedia.org) is a
corpus of simple texts targeting ?children and adults who are
learning English Language? and whose authors are requested
to ?use easy words and short sentences?.
2http://en.wikipedia.org
pler ones by greedily selecting the output sentence
with highest probability.
Using both the PWKP corpus developed by
Zhu et al (2010) and the edit history of Simple
Wikipedia, Woodsend and Lapata (2011) learn a
quasi synchronous grammar (Smith and Eisner,
2006) describing a loose alignment between parse
trees of complex and of simple sentences. Fol-
lowing Dras (1999), they then generate all possi-
ble rewrites for a source tree and use integer lin-
ear programming to select the most appropriate
simplification. They evaluate their model on the
same dataset used by Zhu et al (2010) namely,
an aligned corpus of 100/131 EWKP/SWKP sen-
tences and show that they achieve better BLEU
score. They also conducted a human evaluation
on 64 of the 100 test sentences and showed again
a better performance in terms of simplicity, gram-
maticality and meaning preservation.
In (Wubben et al, 2012; Coster and Kauchak,
2011), simplification is viewed as a monolingual
translation task where the complex sentence is the
source and the simpler one is the target. To ac-
count for deletions, reordering and substitution,
Coster and Kauchak (2011) trained a phrase based
machine translation system on the PWKP corpus
while modifying the word alignment output by
GIZA++ in Moses to allow for null phrasal align-
ments. In this way, they allow for phrases to be
deleted during translation. No human evaluation
is provided but the approach is shown to result in
statistically significant improvements over a tradi-
tional phrase based approach. Similarly, Wubben
et al (2012) use Moses and the PWKP data to train
a phrase based machine translation system aug-
mented with a post-hoc reranking procedure de-
signed to rank the output based on their dissim-
ilarity from the source. A human evaluation on
20 sentences randomly selected from the test data
indicates that, in terms of fluency and adequacy,
their system is judged to outperform both Zhu et
al. (2010) and Woodsend and Lapata (2011) sys-
tems.
3 Simplification Framework
We start by motivating our approach and explain-
ing how it relates to previous proposals w.r.t.,
the four main operations involved in simplifica-
tion namely, splitting, deletion, substitution and
reordering. We then introduce our framework.
436
Sentence Splitting. Sentence splitting is ar-
guably semantic based in that in many cases, split-
ting occurs when the same semantic entity partici-
pates in two distinct eventualities. For instance, in
example (1) below, the split is on the noun bricks
which is involved in two eventualities namely,
?being resistant to cold? and ?enabling the con-
struction of permanent buildings?.
(1) C. Being more resistant to cold, bricks enabled the con-
struction of permanent buildings.
S. Bricks were more resistant to cold. Bricks enabled
the construction of permanent buildings.
While splitting opportunities have a clear coun-
terpart in syntax (i.e., splitting often occurs when-
ever a relative, a subordinate or an appositive
clause occurs in the complex sentence), comple-
tion i.e., the reconstruction of the shared element
in the second simpler clause, is arguably seman-
tically governed in that the reconstructed element
corefers with its matching phrase in the first sim-
pler clause. While our semantic based approach
naturally accounts for this by copying the phrase
corresponding to the shared entity in both phrases,
syntax based approach such as Zhu et al (2010)
and Woodsend and Lapata (2011) will often fail to
appropriately reconstruct the shared phrase and in-
troduce agreement mismatches because the align-
ment or rules they learn are based on syntax alone.
For instance, in example (2), Zhu et al (2010)
fails to copy the shared argument ?The judge? to
the second clause whereas Woodsend and Lapata
(2011) learns a synchronous rule matching (VP
and VP) to (VP. NP(It) VP) thereby failing to pro-
duce the correct subject pronoun (?he? or ?she?)
for the antecedent ?The judge?.
(2) C. The judge ordered that Chapman should receive
psychiatric treatment in prison and sentenced him to
twenty years to life.
S
1
. The judge ordered that Chapman should get psychi-
atric treatment. In prison and sentenced him to twenty
years to life. (Zhu et al, 2010)
S
2
. The judge ordered that Chapman should receive
psychiatric treatment in prison. It sentenced him to
twenty years to life. (Woodsend and Lapata, 2011)
Deletion. By handling deletion using a proba-
bilistic model trained on semantic representations,
we can avoid deleting obligatory arguments. Thus
in our approach, semantic subformulae which are
related to a predicate by a core thematic roles (e.g.,
agent and patient) are never considered for dele-
tion. By contrast, syntax based approaches (Zhu
et al, 2010; Woodsend and Lapata, 2011) do not
distinguish between optional and obligatory argu-
ments. For instance Zhu et al (2010) simplifies
(3C) to (3S) thereby incorrectly deleting the oblig-
atory theme (gifts) of the complex sentence and
modifying its meaning to giving knights and war-
riors (instead of giving gifts to knights and war-
riors).
(3) C. Women would also often give knights and warriors
gifts that included thyme leaves as it was believed to
bring courage to the bearer.
S. Women also often give knights and warriors. Gifts
included thyme leaves as it was thought to bring
courage to the saint. (Zhu et al, 2010)
We also depart from Coster and Kauchak (2011)
who rely on null phrasal alignments for deletion
during phrase based machine translation. In their
approach, deletion is constrained by the training
data and the possible alignments, independent of
any linguistic knowledge.
Substitution and Reordering SMT based ap-
proaches to paraphrasing (Barzilay and Elhadad,
2003; Bannard and Callison-Burch, 2005) and to
sentence simplification (Wubben et al, 2012) have
shown that by utilising knowledge about align-
ment and translation probabilities, SMT systems
can account for the substitutions and the reorder-
ings occurring in sentence simplification. Fol-
lowing on these approaches, we therefore rely on
phrase based SMT to learn substitutions and re-
ordering. In addition, the language model we in-
tegrate in the SMT module helps ensuring better
fluency and grammaticality.
3.1 An Example
Figure 1 shows how our approach simplifies (4C)
into (4S).
(4) C. In 1964 Peter Higgs published his second paper in
Physical Review Letters describing Higgs mechanism
which predicted a new massive spin-zero boson for the
first time.
S. Peter Higgs wrote his paper explaining Higgs mech-
anism in 1964. Higgs mechanism predicted a new ele-
mentary particle.
The DRS for (4C) produced using Boxer (Cur-
ran et al, 2007) is shown at the top of the Figure
and a graph representation3 of the dependencies
between its variables is shown immediately below.
Each DRS variable labels a node in the graph and
each edge is labelled with the relation holding be-
tween the variables labelling its end vertices. The
3The DRS to graph conversion goes through several pre-
processing steps: the relation nn is inverted making modi-
fier noun (higgs) dependent of modified noun (mechanism),
named and timex are converted to unary predicates, e.g.,
named(x, peter) is mapped to peter(x) and timex(x) =
1964 is mapped to 1964(x); and nodes are introduced for
orphan words (e.g., which).
437
((
X
0
named(X
0
, higgs, per)
named(X
0
, peter, per)
?
(
X
1
male(X
1
)
?
(
X
2
second(X
2
)
paper(X
2
)
of(X
2
, X
1
)
?
(
X
3
publish(X
3
)
agent(X
3
, X
0
)
patient(X
3
, X
2
)
;
(
X
4
named(X
4
, physical, org)
named(X
4
, review, org)
named(X
4
, letters, org)
?
X
5
thing(X
5
)
event(X
3
)
in(X
3
, X
4
)
in(X
3
, X
5
)
timex(X
5
) = 1964
)))))
;
(
X
6
;
(
X
7
, X
8
mechanism(X
8
)
nn(X
7
, X
8
)
named(X
7
, higgs, org)
?
X
9
, X
10
, X
11
, X
12
new(X
9
)
massive(X
9
)
spin-zero(X
9
)
boson(X
9
)
predict(X
10
)
event(X
10
)
describe(X
11
)
event(X
11
)
first(X
12
)
time(X
12
)
agent(X
10
, X
8
)
patient(X
10
, X
9
)
agent(X
11
, X
6
)
patient(X
11
, X
8
)
for(X
10
, X
12
)
[Discourse Representation Structure produced by BOXER]
ROOT
O
1
X
10
X
12
X
9
R
10
R
11
X
11
X
8
X
7
R
8
X
6
R
6
R
7
X
3
X
5
X
4
X
2
X
1
R
3
X
0
R
1
R
2
R
4
R
5
R
9
[DRS Graph Representation]
O
1
16 which/WDT
X
12
24, 25, 26 first/a, time/n
X
11
13
describe/v, event
X
10
17
predict/v, event
X
9
18, 19, 20
21, 22
new/a, spin-zero/a
massive/a, boson/n
X
8
14, 15 mechanism/n
X
7
14
higgs/org
X
6
6, 7, 8
??
X
5
2
thing/n, 1964
X
4
10, 11, 12
physical/org
review/org, letters/org
X
3
5
publish/v, event
X
2
6, 7, 8 second/a, paper/a
X
1
6 male/a
X
0
3, 4 higgs/per, peter/per
node pos. in S predicate/type
R
11
23
for,X
10
? X
12
R
10
17
patient,X
10
? X
9
R
9
17
agent,X
10
? X
8
R
8
?? nn,X
8
? X
7
R
7
13
patient,X
11
? X
8
R
6
13
agent,X
11
? X
6
R
5
1
in,X
3
? X
5
R
4
9
in,X
3
? X
4
R
3
6
of,X
2
? X
1
R
2
5
patient,X
3
? X
2
R
1
5
agent,X
3
? X
0
rel pos. in S predicate
ROOT
X
11
X
8
X
7
R
8
X
6
R
6
R
7
X
3
X
5
X
4
X
2
X
1
R
3
X
0
R
1
R
2
R
4
R
5
ROOT
O
1
X
10
X
12
X
9
X
8
X
7
R
8
R
9
R
10
R
11
( )
w
w
w
w

SPLIT
ROOT
X
11
X
8
X
7
R
8
X
6
R
6
R
7
X
3
X
5
X
?
2
X
1
R
3
X
0
R
1
R
2
R
5
In 1964 Peter Higgs published his
paper describing Higgs mechanism
ROOT
X
10
X
?
9
X
8
X
7
R
8
R
9
R
10
Higgs mechanism predicted
a new boson
( )
w
w
w
w

DELETION
Peter Higgs wrote his paper explaining
Higgs mechanism in 1964 .
Higgs mechanism predicted
a new elementary particle .
( )
w
w
w
w

PBMT+LM
Figure 1: Simplification of ?In 1964 Peter Higgs published his second paper in Physical Review Letters
describing Higgs mechanism which predicted a new massive spin-zero boson for the first time .?
438
two tables to the right of the picture show the pred-
icates (top table) associated with each variable and
the relation label (bottom table) associated with
each edge. Boxer also outputs the associated po-
sitions in the complex sentence for each predicate
(not shown in the DRS but in the graph tables). Or-
phan words (OW) i.e., words which have no cor-
responding material in the DRS (e.g., which at po-
sition 16), are added to the graph (node O
1
) thus
ensuring that the position set associated with the
graph exactly matches the positions in the input
sentence and thus deriving the input sentence.
Split Candidate isSplit prob.
(agent, for, patient) - (agent, in, in,
patient)
true 0.63
false 0.37
Table 1: Simplification: SPLIT
Given the input DRS shown in Figure 1, simpli-
fication proceeds as follows.
Splitting. The splitting candidates of a DRS are
event pairs contained in that DRS. More precisely,
the splitting candidates are pairs4 of event vari-
ables associated with at least one of the core the-
matic roles (e.g., agent and patient). The features
conditioning a split are the set of thematic roles as-
sociated with each event variable. The DRS shown
in Figure 1 contains three such event variables
X
3
,X
11
and X
10
with associated thematic role
sets {agent, in, in, patient}, {agent, patient} and
{agent, for, patient} respectively. Hence, there are
3 splitting candidates (X
3
-X
11
, X
3
-X
10
and X
10
-
X
11
) and 4 split options: no split or split at one of
the splitting candidates. Here the split with highest
probability (cf. Table 1) is chosen and the DRS is
split into two sub-DRS, one containing X
3
, and
the other containing X
10
. After splitting, dan-
gling subgraphs are attached to the root of the new
subgraph maximizing either proximity or position
overlap. Here the graph rooted in X
11
is attached
to the root dominating X
3
and the orphan word O
1
to the root dominating X
10
.
Deletion. The deletion model (cf. Table 2) reg-
ulates the deletion of relations and their associated
subgraph; of adjectives and adverbs; and of orphan
words. Here, the relations in between X
3
and X
4
and for between X
10
and X
12
are deleted resulting
in the deletion of the phrases ?in Physical Review
Letters? and ?for the first time? as well as the ad-
4The splitting candidates could be sets of event variables
depending on the number of splits required. Here, we con-
sider pairs for 2 splits.
jectives second, massive, spin-zero and the orphan
word which.
Substitution and Reordering. Finally the trans-
lation and language model ensures that published,
describing and boson are simplified to wrote, ex-
plaining and elementary particle respectively; and
that the phrase ?In 1964? is moved from the be-
ginning of the sentence to its end.
3.2 The Simplification Model
Our simplification framework consists of a prob-
abilistic model for splitting and dropping which
we call DRS simplification model (DRS-SM); a
phrase based translation model for substitution
and reordering (PBMT); and a language model
learned on Simple English Wikipedia (LM) for
fluency and grammaticality. Given a complex sen-
tence c, we split the simplification process into
two steps. First, DRS-SM is applied to D
c
(the
DRS representation of the complex sentence c)
to produce one or more (in case of splitting) in-
termediate simplified sentence(s) s?. Second, the
simplified sentence(s) s? is further simplified to s
using a phrase based machine translation system
(PBMT+LM). Hence, our model can be formally
defined as:
s? = argmax
s
p(s|c)
= argmax
s
p(s
?
|c)p(s|s
?
)
= argmax
s
p(s
?
|D
c
)p(s
?
|s)p(s)
where the probabilities p(s?|D
c
), p(s
?
|s) and
p(s) are given by the DRS simplification model,
the phrase based machine translation model and
the language model respectively.
To get the DRS simplification model, we com-
bine the probability of splitting with the probabil-
ity of deletion:
p(s
?
|D
c
) =
?
?:str(?(D
c
))=s
?
p(D
split
|D
c
)p(D
del
|D
split
)
where ? is a sequence of simplification opera-
tions and str(?(D
c
)) is the sequence of words as-
sociated with a DRS resulting from simplifying D
c
using ?.
The probability of a splitting operation for a
given DRS D
c
is:
p(D
split
|D
c
) =
?
?
?
SPLIT(sptruecand), split at spcand
?
spcand
SPLIT(spfalsecand), otherwise
439
relation candidate isDrop prob.
relation
word
length
range
in 0-2 true 0.22false 0.72
in 2-5 true 0.833false 0.167
mod. cand. isDrop prob.
mod word
new
true 0.22
false 0.72
massive true 0.833false 0.167
OW candidate isDrop prob.orphan
word isBoundary
and true true 0.82false 0.18
which false true 0.833false 0.167
Table 2: Simplification: DELETION (Relations, modifiers and OW respectively)
That is, if the DRS is split on the splitting candi-
date spcand, the probability of the split is then given
by the SPLIT table (Table 1) for the isSplit value
?true? and the split candidate spcand; else it is the
product of the probability given by the SPLIT table
for the isSplit value ?false? for all split candidate
considered for D
c
. As mentioned above, the fea-
tures used for determining the split operation are
the role sets associated with pairs of event vari-
ables (cf. Table 1).
The deletion probability is given by three mod-
els: a model for relations determining the deletion
of prepositional phrases; a model for modifiers
(adjectives and adverbs) and a model for orphan
words (Table 2). All three deletion models use the
associated word itself as a feature. In addition, the
model for relations uses the PP length-range as a
feature while the model for orphan words relies on
boundary information i.e., whether or not, the OW
occurs at the associated sentence boundary.
p(D
del
|D
split
) =
?
relcand
DELrel(relcand)
?
modcand
DELmod(modcand)
?
owcand
DELow(owcand)
3.3 Estimating the parameters
We use the EM algorithm (Dempster et al, 1977)
to estimate our split and deletion model parame-
ters. For an efficient implementation of EM algo-
rithm, we follow the work of Yamada and Knight
(2001) and Zhu et al (2010); and build training
graphs (Figure 2) from the pair of complex and
simple sentence pairs in the training data.
Each training graph represents a complex-
simple sentence pair and consists of two types
of nodes: major nodes (M-nodes) and operation
nodes (O-nodes). An M-node contains the DRS
representation D
c
of a complex sentence c and the
associated simple sentence(s) s
i
while O-nodes
determine split and deletion operations on their
parent M-node. Only the root M-node is consid-
ered for the split operations. For example, given
fin
del-rel?; del-mod?; del-ow?
split
root
Figure 2: An example training graph
the root M-node (D
c
, (s
1
, s
2
)), multiple success-
ful split O-nodes will be created, each one further
creating two M-nodes (D
c1
, s
1
) and (D
c2
, s
2
). For
the training pair (c, s), the root M-node (D
c
, s) is
followed by a single split O-node producing an M-
node (D
c
, s) and counting all split candidates in D
c
for failed split. The M-nodes created after split op-
erations are then tried for multiple deletion opera-
tions of relations, modifiers and OW respectively.
Each deletion candidate creates a deletion O-node
marking successful or failed deletion of the can-
didate and a result M-node. The deletion process
continues on the result M-node until there is no
deletion candidate left to process. The governing
criteria for the construction of the training graph
is that, at each step, it tries to minimize the Leven-
shtein edit distance between the complex and the
simple sentences. Moreover, for the splitting op-
eration, we introduce a split only if the reference
sentence consists of several sentences (i.e., there
is a split in the training data); and only consider
splits which maximises the overlap between split
and simple reference sentences.
We initialize our probability tables Table 1 and
Table 2 with the uniform distribution, i.e., 0.5 be-
cause all our features are binary. The EM algo-
rithm iterates over training graphs counting model
features from O-nodes and updating our probabil-
ity tables. Because of the space constraints, we
do not describe our algorithm in details. We refer
the reader to (Yamada and Knight, 2001) for more
details.
440
Our phrase based translation model is trained
using the Moses toolkit5 with its default command
line options on the PWKP corpus (except the sen-
tences from the test set) considering the complex
sentence as the source and the simpler one as the
target. Our trigram language model is trained us-
ing the SRILM toolkit6 on the SWKP corpus7.
Decoding. We explore the decoding graph sim-
ilar to the training graph but in a greedy approach
always picking the choice with maximal probabil-
ity. Given a complex input sentence c, a split O-
node will be selected corresponding to the deci-
sion of whether to split and where to split. Next,
deletion O-nodes are selected indicating whether
or not to drop each of the deletion candidate. The
DRS associated with the final M-node D
fin
is then
mapped to a simplified sentence s?
fin
which is
further simplified using the phrase-based machine
translation system to produce the final simplified
sentence s
simple
.
4 Experiments
We trained our simplification and translation mod-
els on the PWKP corpus. To evaluate perfor-
mance, we compare our approach with three other
state of the art systems using the test set provided
by Zhu et al (2010) and relying both on automatic
metrics and on human judgments.
4.1 Training and Test Data
The DRS-Based simplification model is trained
on PWKP, a bi-text of complex and simple sen-
tences provided by Zhu et al (2010). To construct
this bi-text, Zhu et al (2010) extracted complex
and simple sentences from EWKP and SWKP re-
spectively and automatically aligned them using
TF*IDF as a similarity measure. PWKP contains
108016/114924 complex/simple sentence pairs.
We tokenize PWKP using Stanford CoreNLP
toolkit8. We then parse all complex sentences
in PWKP using Boxer9 to produce their DRSs.
Finally, our DRS-Based simplification model is
trained on 97.75% of PWKP; we drop out 2.25%
of the complex sentences in PWKP which are re-
peated in the test set or for which Boxer fails to
produce DRSs.
5http://www.statmt.org/moses/
6http://www.speech.sri.com/projects/srilm/
7We downloaded the snapshots of Simple Wikipedia
dated 2013-10-30 available at http://dumps.wikimedia.org/.
8http://nlp.stanford.edu/software/corenlp.shtml
9http://svn.ask.it.usyd.edu.au/trac/candc, Version 1.00
We evaluate our model on the test set used by
Zhu et al (2010) namely, an aligned corpus of
100/131 EWKP/SWKP sentences. Boxer pro-
duces a DRS for 96 of the 100 input sentences.
These input are simplified using our simplifica-
tion system namely, the DRS-SM model and the
phrase-based machine translation system (Section
3.2). For the remaining four complex sentences,
Boxer fails to produce DRSs. These four sen-
tences are directly sent to the phrase-based ma-
chine translation system to produce simplified sen-
tences.
4.2 Automatic Evaluation Metrics
To assess and compare simplification systems, two
main automatic metrics have been used in previ-
ous work namely, BLEU and the Flesch-Kincaid
Grade Level Index (FKG).
The FKG index is a readability metric taking
into account the average sentence length in words
and the average word length in syllables. In its
original context (language learning), it was ap-
plied to well formed text and thus measured the
simplicity of a well formed sentence. In the con-
text of the simplification task however, the auto-
matically generated sentences are not necessarily
well formed so that the FKG index reduces to a
measure of the sentence length (in terms of words
and syllables) approximating the simplicity level
of an output sentence irrespective of the length
of the corresponding input. To assess simplifica-
tion, we instead use metrics that are directly re-
lated to the simplification task namely, the number
of splits in the overall (test and training) data and
in average per sentences; the number of generated
sentences with no edits i.e., which are identical to
the original, complex one; and the average Leven-
shtein distance between the system?s output and
both the complex and the simple reference sen-
tences.
BLEU gives a measure of how close a system?s
output is to the gold standard simple sentence. Be-
cause there are many possible ways of simplifying
a sentence, BLEU alone fails to correctly assess
the appropriateness of a simplification. Moreover
BLEU does not capture the degree to which the
system?s output differs from the complex sentence
input. We therefore use BLEU as a means to eval-
uate how close the systems output are to the refer-
ence corpus but complement it with further man-
ual metrics capturing other important factors when
441
evaluating simplifications such as the fluency and
the adequacy of the output sentences and the de-
gree to which the output sentence simplifies the
input.
4.3 Results and Discussion
Number of Splits Table 3 shows the proportion
of input whose simplification involved a splitting
operation. While our system splits in proportion
similar to that observed in the training data, the
other systems either split very often (80% of the
time for Zhu and 63% of the time for Woodsend)
or not at all (Wubben). In other words, when com-
pared to the other systems, our system performs
splits in proportion closest to the reference both
in terms of total number of splits and of average
number of splits per sentence.
Data Total number
of sentences % split
average split /
sentence
PWKP 108,016 6.1 1.06
GOLD 100 28 1.30
Zhu 100 80 1.80
Woodsend 100 63 2.05
Wubben 100 1 1.01
Hybrid 100 10 1.10
Table 3: Proportion of Split Sentences (% split)
in the training/test data and in average per sen-
tence (average split / sentence). GOLD is the
test data with the gold standard SWKP sentences;
Zhu, Woodsend, Wubben are the best output of the
models of Zhu et al (2010), Woodsend and Lap-
ata (2011) and Wubben et al (2012) respectively;
Hybrid is our model.
Number of Edits Table 4 indicates the edit dis-
tance of the output sentences w.r.t. both the com-
plex and the simple reference sentences as well as
the number of input for which no simplification
occur. The right part of the table shows that our
system generate simplifications which are closest
to the reference sentence (in terms of edits) com-
pared to those output by the other systems. It
also produces the highest number of simplifica-
tions which are identical to the reference. Con-
versely our system only ranks third in terms of dis-
similarity with the input complex sentences (6.32
edits away from the input sentence) behind the
Woodsend (8.63 edits) and the Zhu (7.87 edits)
system. This is in part due to the difference in
splitting strategies noted above : the many splits
applied by these latter two systems correlate with
a high number of edits.
System BLEU
Edits (Complex
to System)
Edits (System
to Simple)
LD No edit LD No edit
GOLD 100 12.24 3 0 100
Zhu 37.4 7.87 2 14.64 0
Woodsend 42 8.63 24 16.03 2
Wubben 41.4 3.33 6 13.57 2
Hybrid 53.6 6.32 4 11.53 3
Table 4: Automated Metrics for Simplification:
average Levenshtein distance (LD) to complex and
simple reference sentences per system ; number of
input sentences for which no simplification occur
(No edit).
BLEU score We used Moses support tools:
multi-bleu10 to calculate BLEU scores. The
BLEU scores shown in Table 4 show that our sys-
tem produces simplifications that are closest to the
reference.
In sum, the automatic metrics indicate that our
system produces simplification that are consis-
tently closest to the reference in terms of edit dis-
tance, number of splits and BLEU score.
4.4 Human Evaluation
The human evaluation was done online using the
LG-Eval toolkit (Kow and Belz, 2012)11. The
evaluators were allocated a trial set using a Latin
Square Experimental Design (LSED) such that
each evaluator sees the same number of output
from each system and for each test set item. Dur-
ing the experiment, the evaluators were presented
with a pair of a complex and a simple sentence(s)
and asked to rate this pair w.r.t. to adequacy (Does
the simplified sentence(s) preserve the meaning
of the input?) and simplification (Does the gen-
erated sentence(s) simplify the complex input?).
They were also asked to rate the second (sim-
plified) sentence(s) of the pair w.r.t. to fluency
(Is the simplified output fluent and grammatical?).
Similar to the Wubben?s human evaluation setup,
we randomly selected 20 complex sentences from
Zhu?s test corpus and included in the evaluation
corpus: the corresponding simple (Gold) sentence
from Zhu?s test corpus, the output of our system
(Hybrid) and the output of the other three sys-
tems (Zhu, Woodsend and Wubben) which were
provided to us by the system authors. The eval-
uation data thus consisted of 100 complex/simple
pairs. We collected ratings from 27 participants.
10http://www.statmt.org/moses/?n=Moses.SupportTools
11http://www.nltg.brighton.ac.uk/research/lg-eval/
442
All were either native speakers or proficient in En-
glish, having taken part in a Master taught in En-
glish or lived in an English speaking country for
an extended period of time.
Systems Simplification Fluency Adequacy
GOLD 3.57 3.93 3.66
Zhu 2.84 2.34 2.34
Woodsend 1.73 2.94 3.04
Wubben 1.81 3.65 3.84
Hybrid 3.37 3.55 3.50
Table 5: Average Human Ratings for simplicity,
fluency and adequacy
Table 5 shows the average ratings of the human
evaluation on a slider scale from 0 to 5. Pairwise
comparisons between all models and their statisti-
cal significance were carried out using a one-way
ANOVA with post-hoc Tukey HSD tests and are
shown in Table 6.
Systems GOLD Zhu Woodsend Wubben
Zhu ??
Woodsend ?? ??
Wubben ?N ?? ?
Hybrid N ? ?N ?N
Table 6: ?/ is/not significantly different (sig.
diff.) wrt simplicity. / is/not sig. diff. wrt
fluency. ?/N is/not sig. diff. wrt adequacy. (sig-
nificance level: p < 0.05)
With regard to simplification, our system ranks
first and is very close to the manually simpli-
fied input (the difference is not statistically signif-
icant). The low rating for Woodsend reflects the
high number of unsimplified sentences (24/100 in
the test data used for the automatic evaluation and
6/20 in the evaluation data used for human judg-
ments). Our system data is not significantly differ-
ent from the manually simplified data for simplic-
ity whereas all other systems are.
For fluency, our system rates second behind
Wubben and before Woodsend and Zhu. The
difference between our system and both Zhu
and Woodsend system is significant. In partic-
ular, Zhu?s output is judged less fluent proba-
bly because of the many incorrect splits it li-
censes. Manual examination of the data shows
that Woodsend?s system also produces incorrect
splits. For this system however, the high propor-
tion of non simplified sentences probably counter-
balances these incorrect splits, allowing for a good
fluency score overall.
Regarding adequacy, our system is against clos-
est to the reference (3.50 for our system vs.
3.66 for manual simplification). Our system, the
Wubben system and the manual simplifications
are in the same group (the differences between
these systems are not significant). The Wood-
send system comes second and the Zhu system
third (the difference between the two is signifi-
cant). Wubben?s high fluency, high adequacy but
low simplicity could be explained with their min-
imal number of edit (3.33 edits) from the source
sentence.
In sum, if we group together systems for which
there is no significant difference, our system ranks
first (together with GOLD) for simplicity; first
for fluency (together with GOLD and Wubben);
and first for adequacy (together with GOLD and
Wubben).
5 Conclusion
A key feature of our approach is that it is seman-
tically based. Typically, discourse level simplifi-
cation operations such as sentence splitting, sen-
tence reordering, cue word selection, referring ex-
pression generation and determiner choice are se-
mantically constrained. As argued by Siddharthan
(2006), correctly capturing the interactions be-
tween these phenomena is essential to ensuring
text cohesion. In the future, we would like to
investigate how our framework deals with such
discourse level simplifications i.e., simplifications
which involves manipulation of the coreference
and of the discourse structure. In the PWKP data,
the proportion of split sentences is rather low (6.1
%) and many of the split sentences are simple sen-
tence coordination splits. A more adequate but
small corpus is that used in (Siddharthan, 2006)
which consists of 95 cases of discourse simplifica-
tion. Using data from the language learning or the
children reading community, it would be interest-
ing to first construct a similar, larger scale corpus;
and to then train and test our approach on more
complex cases of sentence splitting.
Acknowledgments
We are grateful to Zhemin Zhu, Kristian Wood-
send and Sander Wubben for sharing their data.
We would like to thank our annotators for partic-
ipating in our human evaluation experiments and
to anonymous reviewers for their insightful com-
ments.
443
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics (ACL), pages 597?
604. Association for Computational Linguistics.
Regina Barzilay and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
pora. In Proceedings of the 2003 conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 25?32. Association for Computa-
tional Linguistics.
Stefan Bott, Horacio Saggion, and Simon Mille. 2012.
Text simplification tools for spanish. In Proceedings
of the 8th International Conference on Language
Resources and Evaluation (LREC), pages 1665?
1671.
Yvonne Margaret Canning. 2002. Syntactic simplifica-
tion of Text. Ph.D. thesis, University of Sunderland.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999.
Simplifying text for language-impaired readers. In
Proceedings of 9th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), volume 99, pages 269?270. Cite-
seer.
Raman Chandrasekar and Bangalore Srinivas. 1997.
Automatic induction of rules for text simplification.
Knowledge-Based Systems, 10(3):183?190.
Raman Chandrasekar, Christine Doran, and Banga-
lore Srinivas. 1996. Motivations and methods for
text simplification. In Proceedings of the 16th In-
ternational conference on Computational linguistics
(COLING), pages 1041?1044. Association for Com-
putational Linguistics.
William Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1?9. Association for Computational
Linguistics.
James R Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale NLP with C&C
and Boxer. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL) on Interactive Poster and Demonstration Ses-
sions, pages 33?36. Association for Computational
Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39(1):1?38.
Mark Dras. 1999. Tree adjoining grammar and the
reluctant paraphrasing of text. Ph.D. thesis, Mac-
quarie University NSW 2109 Australia.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference (INLG), pages 25?32. Asso-
ciation for Computational Linguistics.
Hans Kamp. 1981. A theory of truth and semantic rep-
resentation. In J.A.G. Groenendijk, T.M.V. Janssen,
B.J. Stokhof, and M.J.B. Stokhof, editors, Formal
methods in the study of language, number pt. 1 in
Mathematical Centre tracts. Mathematisch Centrum.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In Proceedings of the Seventeenth National
Conference on Artificial Intelligence (AAAI) and
Twelfth Conference on Innovative Applications of
Artificial Intelligence (IAAI), pages 703?710. AAAI
Press.
Eric Kow and Anja Belz. 2012. LG-Eval: A Toolkit
for Creating Online Language Evaluation Experi-
ments. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC), pages 4033?4037.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document sum-
marization. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING), page 896. Association for Computational Lin-
guistics.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In Proceedings of the Lan-
guage Engineering Conference (LEC), pages 64?71.
IEEE Computer Society.
Advaith Siddharthan. 2006. Syntactic simplification
and text cohesion. Research on Language and Com-
putation, 4(1):77?109.
Advaith Siddharthan. 2010. Complex lexico-syntactic
reformulation of sentences using typed dependency
representations. In Proceedings of the 6th Inter-
national Natural Language Generation Conference
(INLG), pages 125?133. Association for Computa-
tional Linguistics.
Advaith Siddharthan. 2011. Text simplification us-
ing typed dependencies: a comparison of the robust-
ness of different generation strategies. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation (ENLG), pages 2?11. Association
for Computational Linguistics.
David A Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projec-
tion of syntactic dependencies. In Proceedings of
the HLT-NAACL Workshop on Statistical Machine
Translation, pages 23?30. Association for Compu-
tational Linguistics.
444
David Vickrey and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL) and the
Human Language Technology Conference (HLT),
pages 344?352.
Willian Massami Watanabe, Arnaldo Candido Junior,
Vin??cius Rodriguez Uze?da, Renata Pontin de Mat-
tos Fortes, Thiago Alexandre Salgueiro Pardo, and
Sandra Maria Alu??sio. 2009. Facilita: reading as-
sistance for low-literacy readers. In Proceedings of
the 27th ACM international conference on Design of
communication, pages 29?36. ACM.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 409?420.
Association for Computational Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (ACL): Long Papers-Volume
1, pages 1015?1024. Association for Computational
Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting on Association for Com-
putational Linguistics (ACL), pages 523?530. Asso-
ciation for Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (COLING), pages 1353?1361, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
445
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 31?39,
Utica, May 2012. c?2012 Association for Computational Linguistics
Generation for Grammar Engineering
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy, F-54000, France
claire.gardent@loria.fr
German Kruszewski
Inria, LORIA, UMR 7503
Villers-le`s-Nancy, F-54600, France
german.kruszewski@inria.fr
Abstract
While in Computer Science, grammar engi-
neering has led to the development of various
tools for checking grammar coherence, com-
pletion, under- and over-generation, in Natu-
ral Langage Processing, most approaches de-
veloped to improve a grammar have focused
on detecting under-generation and to a much
lesser extent, over-generation. We argue that
generation can be exploited to address other
issues that are relevant to grammar engineer-
ing such as in particular, detecting grammar
incompleteness, identifying sources of over-
generation and analysing the linguistic cover-
age of the grammar. We present an algorithm
that implements these functionalities and we
report on experiments using this algorithm to
analyse a Feature-Based Lexicalised Tree Ad-
joining Grammar consisting of roughly 1500
elementary trees.
1 Introduction
Grammar engineering, the task of developing large
scale computational grammars, is known to be er-
ror prone. As the grammar grows, the interactions
between the rules and the lexicon become increas-
ingly complex and the generative power of the gram-
mar becomes increasingly difficult for the grammar
writer to predict.
While in Computer Science, grammar engineer-
ing has led to the development of various tools for
checking grammar coherence, completion, under-
and over-generation (Klint et al, 2005), in Natu-
ral Langage Processing, most approaches developed
to improve a grammar have focused on detecting
under-generation (that is cases where the grammar
and/or the lexicon fails to provide an analysis for
a given, grammatical, input) and to a lesser degree
over-generation.
In this paper, we argue that generation can be ex-
ploited to address other issues that are relevant to
grammar engineering. In particular, we claim that it
can be used to:
? Check grammar completeness: for each gram-
mar rule, is it possible to derive a syntactically
complete tree ? That is, can each grammar rule
be used to derive a constituent.
? Analyse generation and over-generation: given
some time/recursion upper bounds, what does
the grammar generate? How much of the out-
put is over-generation? Which linguistic con-
structions present in a language are covered by
the grammar?
We present a generation algorithm called GRADE
(GRAmmar DEbugger) that permits addressing
these issues. In essence, this algorithm implements
a top-down grammar traversal guided with semantic
constraints and controlled by various parameteris-
able constraints designed to ensure termination and
linguistic control.
The GRADE algorithm can be applied to any gen-
erative grammar i.e., any grammar which uses a
start symbol and a set of production rules to gen-
erate the sentences of the language described by
that grammar. We present both an abstract descrip-
tion of this algorithm and a concrete implementation
which takes advantage of Definite Clause Grammars
31
to implement grammar traversal. We then present
the results of several experiments where we use the
GRADE algorithm to examine the output of SEM-
TAG, a Feature-Based Lexicalised Tree Adjoining
Grammar (FB-LTAG) for French.
The paper is structured as follows. Section 2
summarises related work. Section 3 presents the
GRADE algorithm. Section 4 introduces the gram-
mar used for testing and describes an implementa-
tion of GRADE for FB-LTAG. Section 5 presents
the results obtained by applying the GRADE algo-
rithm to SEMTAG. We show that it helps (i) to detect
sources of grammar incompleteness (i.e., rules that
do not lead to a complete derivation) and (ii) to iden-
tify overgeneration and analyse linguistic coverage.
Section 6 concludes.
2 Related Work
Two main approaches have so far been used to im-
prove grammars: treebank-based evaluation and er-
ror mining techniques. We briefly review this work
focusing first, on approaches that are based on pars-
ing and second, on those that exploit generation.
Debugging Grammars using Parsing Over the
last two decades, Treebank-Based evaluation has be-
come the standard way of evaluating parsers and
grammars. In this framework (Black et al, 1991),
the output of a parser is evaluated on a set of sen-
tences that have been manually annotated with their
syntactic parses. Whenever the parse tree produced
by the parser differs from the manual annotation, the
difference can be traced back to the parser (timeout,
disambiguation component), the grammar and/or to
the lexicon. Conversely, if the parser fails to re-
turn an output, undergeneration can be traced back
to missing or erroneous information in the grammar
or/and in the lexicon.
While it has supported the development of ro-
bust, large coverage parsers, treebank based evalu-
ation is limited to the set of syntactic constructions
and lexical items present in the treebank. It also
fails to directly identify the most likely source of
parsing failures. To bypass these limitations, error
mining techniques have been proposed which per-
mit detecting grammar and lexicon errors by pars-
ing large quantities of data (van Noord, 2004; Sagot
and de la Clergerie, 2006; de Kok et al, 2009). The
output of this parsing process is then divided into
two sets of parsed and unparsed sentences which are
used to compute the ?suspicion rate? of n-grams of
word forms, lemmas or part of speech tags whereby
the suspicion rate of an item indicates how likely a
given item is to cause parsing to fail. Error mining
was shown to successfully help detect errors in the
lexicon and to a lesser degree in the grammar.
Debugging Grammars using Generation Most
of the work on treebank-based evaluation and error
mining target undergeneration using parsing. Re-
cently however, some work has been done which ex-
ploits generation and more specifically, surface real-
isation to detect both under- and over-generation.
Both (Callaway, 2003) and the Surface Realisa-
tion (SR) task organised by the Generation Chal-
lenge (Belz et al, 2011) evaluate the output of sur-
face realisers on a set of inputs derived from the
Penn Treebank. As with parsing, these approaches
permit detecting under-generation in that an input
for which the surface realiser fails to produce a
sentence points to shortcomings either in the sur-
face realisation algorithm or in the grammar/lexicon.
The approach also permits detecting overgeneration
in that a low BLEU score points to these inputs
for which the realiser produced a sentence that is
markedly different from the expected answer.
Error mining approaches have also been devel-
oped using generation. (Gardent and Kow, 2007) is
similar in spirit to the error mining approaches de-
veloped for parsing. Starting from a set of manu-
ally defined semantic representations, the approach
consists in running a surface realiser on these repre-
sentations; manually sorting the generated sentences
as correct or incorrect; and using the resulting two
datasets to detect grammatical structures that sys-
tematically occur in the incorrect dataset. The ap-
proach however is only partially automatised since
both the input and the output need to be manually
produced/annotated. More recently, (Gardent and
Narayan, 2012) has shown how the fully automatic
error mining techniques used for parsing could be
adapted to mine for errors in the output of a surface
realiser tested on the SR input data. In essence, they
present an algorithm which enumerate the subtrees
in the input data that frequently occur in surface re-
alisation failure (the surface realiser fails to gener-
32
ate a sentence) and rarely occur in surface realisa-
tion success. In this way, they can identify subtrees
in the input that are predominantly associated with
generation failure.
In sum, tree-bank based evaluation permits de-
tecting over- and under-generation while error min-
ing techniques permits identifying sources of er-
rors; Treebank-based evaluation requires a refer-
ence corpus while error mining techniques require
a way to sort good from bad ouput; and in all cases,
generation-based grammar debugging requires input
to be provided (while for parsing, textual input is
freely available).
Discussion The main difference between the
GRADE approach and both error mining and tree-
bank based evaluation is that GRADE is grammar
based. No other input is required for the GRADE
algorithm to work than the grammar1. Whereas ex-
isting approaches identify errors by processing large
amounts of data, GRADE identifies errors by travers-
ing the grammar. In other words, while other ap-
proaches assess the coverage of a parser or a genera-
tor on a given set of input data, GRADE permits sys-
tematically assessing the linguistic coverage and the
precision of the constructs described by the grammar
independently of any input data.
Currently, the output of GRADE needs to be man-
ually examined and the sources of error manually
identified. Providing an automatic means of sorting
GRADE ?s output into good and bad sentences is de-
veloped however, it could be combined with error
mining techniques so as to facilitate interpretation.
3 The GraDE Algorithm
How can we explore the quirks and corners of a
grammar to detect inconsistencies and incorrect out-
put?
In essence, the GRADE algorithm performs a top-
down grammar traversal and outputs the sentences
generated by this traversal. It is grammar neutral in
that it can be applied to any generative grammar i.e.,
any grammar which includes a start symbol and a
set of production rules. Starting from the string con-
sisting of the start symbol, the GRADE algorithm
recursively applies grammar rules replacing one oc-
1Although some semantic input is possible.
currence of its left-hand side in the string by its right-
hand side until a string that contains neither the start
symbol nor designated nonterminal symbols is pro-
duced.
Since NL grammars describe infinite sets of sen-
tences however, some means must be provided to
control the search and output sets of sentences that
are linguistically interesting. Therefore, the GRADE
algorithm is controlled by several user-defined pa-
rameters designed to address termination (Given that
NL grammars usually describe an infinite set of sen-
tences, how can we limit sentence generation to
avoid non termination?), linguistic control (How can
we control sentence generation so that the sentences
produced cover linguistic variations that the linguist
is interested in ?) and readibility (How can we con-
strain sentence generation in such a way that the out-
put sentences are meaningful sentences rather than
just grammatical ones?).
3.1 Ensuring termination
To ensure termination, GRADE supports three user-
defined control parameters which can be used simul-
taneously or in isolation namely: a time out parame-
ter; a restriction on the number and type of recursive
rules allowed in any derivation; and a restriction on
the depth of the derivation tree.
Each of these restrictions is implemented as a re-
striction on the grammar traversal process as fol-
lows.
Time out. The process halts when the time bound
is reached.
Recursive Rules. For each type of recursive rule,
a counter is created which is initialised to the values
set by the user and decremented each time a recur-
sive rule of the corresponding type is used. When
all counters are null, recursive rules can no longer
be used. The type of a recursive rule is simply the
main category expanded by that rule namely, N, NP,
V, VP and S. In addition, whenever a rule is applied,
the GRADE algorithm arbitrarily divides up the re-
cursion quotas of a symbol among the symbol?s chil-
dren. If it happens to divide them a way that can-
not be fulfilled, then it fails, backtracks, and divides
them some other way.
33
Derivation Depth. A counter is used to keep track
of the depth of the derivation tree and either halts (if
no other rule applies) or backtracks whenever the set
depth is reached.
3.2 Linguistic Coverage and Output
Readibility
GRADE provides several ways of controlling the lin-
guistic coverage and the readibility of the output
sentences.
Modifiers. As we shall show in Section 5, the re-
cursivity constraints mentioned in the previous sec-
tion can be used to constrain the type and the number
of modifiers present in the output.
Root Rule. Second, the ?root rule? i.e., the rule
that is used to expand the start symbol can be con-
strained in several ways. The user can specify which
rule should be used; which features should label
the lhs of that rule; which subcategorisation type it
should model; and whether or not it is a recursive
rule. For instance, given the FB-LTAG we are using,
by specifying the root rule to be used, we can con-
strain the generated sentences to be sentences con-
taining an intransitive verb in the active voice com-
bining with a canonical nominal subject. If we only
specify the subcategorisation type of the root rule
e.g., transitive, we can ensure that the main verb of
the generated sentences is a transitive verb; And if
we only constrain the features of the root rule to in-
dicative mode and active voice, then we allow for
the generation of any sentence whose main verb is
in the indicative mode and active voice.
Input Semantics. Third, in those cases where the
grammar is a reversible grammar associating sen-
tences with both a syntactic structure and a seman-
tic representation, the content of the generated sen-
tences can be controlled by providing GRADE with
an input semantics. Whenever a core semantics
is specified, only rules whose semantics includes
one or more literal(s) in the core semantics can be
used. Determiner rules however are selected inde-
pendent of their semantics. In this way, it is possi-
ble to constrain the output sentences to verbalise a
given meaning without having to specify their full
semantics (the semantic representations used in re-
versible grammars are often intricate representations
which are difficult to specify manually) and while
allowing for morphological variations (tense, num-
ber, mode and aspect can be left unspecified and will
be informed by the calls to the lexicon embedded in
the DCG rules) as well as variations on determin-
ers2. For instance, the core semantics {run(E M),
man(M)} is contained in, and therefore will gen-
erate, the flat semantics for the sentences The man
runs, The man ran, A man runs, A man ran, This
man runs, My man runs, etc..
4 Implementation
In the previous section, we provided an abstract de-
scription of the GRADE algorithm. We now describe
an implementation of that algorithm tailored for FB-
LTAGs equipped with a unification-based composi-
tional semantics. We start by describing the gram-
mar used (SEMTAG), we then summarise the im-
plementation of GRADE for FB-LTAG.
4.1 SemTAG
For our experiments, we use the FB-LTAG described
in (Crabbe?, 2005; Gardent, 2008). This grammar,
called SEMTAG, integrates a unification-based se-
mantics and can be used both for parsing and for
generation. It covers the core constructs for non
verbal constituents and most of the verbal construc-
tions for French. The semantic representations built
are MRSs (Minimal Recursion Semantic representa-
tions, (Copestake et al, 2001)).
More specifically, a tree adjoining grammar
(TAG) is a tuple ??, N, I, A, S? with ? a set of ter-
minals, N a set of non-terminals, I a finite set of
initial trees, A a finite set of auxiliary trees, and S
a distinguished non-terminal (S ? N ). Initial trees
are trees whose leaves are labeled with substitution
nodes (marked with a downarrow) or terminal cate-
gories3. Auxiliary trees are distinguished by a foot
node (marked with a star) whose category must be
the same as that of the root node.
2The rules whose semantics is not checked during derivation
are specified as a parameter of the system and can be modified
at will e.g., to include adverbs or auxiliaries. Here we choosed
to restrict underspecification to determiners.
3Due to space limitation we here give a very sketchy defini-
tion of FB-LTAG. For a more detailed presentation, see (Vijay-
Shanker and Joshi, 1988).
34
Two tree-composition operations are used to com-
bine trees: substitution and adjunction. Substitu-
tion inserts a tree onto a substitution node of some
other tree while adjunction inserts an auxiliary tree
into a tree. In a Feature-Based Lexicalised TAG
(FB-LTAG), tree nodes are furthermore decorated
with two feature structures (called top and bottom)
which are unified during derivation; and each tree
is anchored with a lexical item. Figure 1 shows an
example toy FB-LTAG with unification semantics.
NPj
John
l0:proper q(c hr hs)
l1:named(j john)
qeq(hr l1)
Sb
NP?c VPba
Va
runs
lv:run(a,j)
VPx
often VP*x
lo:often(x)
? l0:proper q(c hr hs) l1:named(j john), qeq(hr, l1),
lv:run(a,j), lv:often(a)
Figure 1: MRS for ?John often runs?
4.2 GraDe for FB-LTAG
The basic FB-LTAG implementation of GRADE is
described in detail in (Gardent et al, 2011; Gar-
dent et al, 2010). In brief, this implementation
takes advantage of the top-down, left-to-right, gram-
mar traversal implemented in Definite Clause Gram-
mars by translating the FB-LTAG to a DCG. In the
DCG formalism, a grammar is represented as a set of
Prolog clauses and Prolog?s query mechanism pro-
vides a built-in top-down, depth-first, traversal of the
grammar. In addition, the DCG formalism allows
arbitrary Prolog goals to be inserted into a rule. To
implement a controlled, top-down grammar traver-
sal of SEMTAG, we simply convert SEMTAGto a
Definite Clause Grammar (DCG) wherein arbitrary
Prolog calls are used both to ground derivations with
lexical items and to control Prolog?s grammar traver-
sal so as to respect the user defined constraints on
recursion and on linguistic coverage. In addition,
we extended the approach to handle semantic con-
straints (i.e., to allow for an input semantic to con-
strain the traversal) as discussed in Section 3. That
is, for a subset of the grammar rules, a rule will only
be applied if its semantics subsumes a literal in the
input semantics.
For more details, on the FB-LTAG implementa-
tion of the GRADE algorithm and of the conversion
from FB-LTAG to DCG, we refer the reader to (Gar-
dent et al, 2011; Gardent et al, 2010).
5 Grammar Analysis
Depending on which control parameters are used,
the GRADE algorithm can be used to explore the
grammar from different viewpoints. In what fol-
lows, we show that it can be used to check grammar
completeness (Can all rules in the grammar be used
so as to derive a constituent?); to inspect the vari-
ous possible realisations of syntactic functors and of
their arguments (e.g., Are all possible syntactic real-
isations of the verb and of its arguments generated
and correct?); to explore the interactions between
basic clauses and modifiers; and to zoom in on the
morphological and syntactic variants of a given core
semantics (e.g., Does the grammar correctly account
for all such variants ?).
5.1 Checking for Grammar Completeness
We first use GRADE to check, for each grammar
rule, whether it can be used to derive a complete
constituent i.e., whether a derivation can be found
such that all leaves of the derivation tree are ter-
minals (words). Can all trees anchored by a verb
for instance, be completed to build a syntactically
complete clause? Trees that cannot yield a complete
constituent points to gaps or inconsistencies in the
grammar.
To perform this check, we run the GRADE algo-
rithm on verb rules, allowing for up to 1 adjunc-
tion on either a noun, a verb or a verb phrase and
halting when either a derivation has been found or
all possible rule combinations have been tried. Ta-
ble 1 shows the results per verb family4. As can be
seen, there are strong differences between the fam-
ilies with e.g., 80% of the trees failing to yield a
derivation in the n0Vs1int (Verbs with interrogative
sentential complement) family against 0% in the ilV
4The notational convention for verb types is from XTAG and
reads as follows. Subscripts indicate the thematic role of the
verb argument. n indicates a nominal, Pn a PP and s a sentential
argument. pl is a verbal particle. Upper case letters describe
the syntactic functor type: V is a verb, A an adjective and BE
the copula. For instance, n0Vn1 indicates a verb taking two
nominal arguments (e.g., like) .
35
Tree Family Trees Fails Fails/Trees
CopulaBe 60 1 1%
ilV 2 0 0%
n0V 10 0 0%
n0ClV 9 0 0%
n0ClVn1 45 2 4%
n0ClVden1 36 3 8%
n0ClVpn1 29 3 10%
n0Vn1 84 3 3%
n0Vn1Adj2 24 6 25%
n0Van1 87 3 3%
n0Vden1 38 3 7%
n0Vpn1 30 3 10%
ilVcs1 2 0 0%
n0Vcs1 30 23 74%
n0Vas1 15 10 66%
n0Vn1Adj2 24 0 0%
s0Vn1 72 9 12%
n0Vs1int 15 12 80%
n0Vn1n2 24 0 0%
n0Vn1an2 681 54 7%
Table 1: Checking for Gaps in the Grammar
(impersonal with expletive subject, ?it rains?) and
the n0V (intransitive, ?Tammy sings?). In total, ap-
proximatively 10% (135/1317) of the grammar rules
cannot yield a derivation.
5.2 Functor/Argument Dependencies
To check grammar completeness, we need only find
one derivation for any given tree. To assess the de-
gree to which the grammar correctly generates all
possible realisations associated with a given syn-
tactic functor however, all realisations generated by
the grammar need to be produced. To restrict the
output to sentences illustrating functor/argument de-
pendencies (no modifiers), we constrain adjunction
to the minimum required by each functor. In most
cases, this boils down to setting the adjunction coun-
ters to null for all categories. One exception are
verbs taking a sentential argument which require one
S adjunction. We also allow for one N-adjunction
and one V-adjunction to allow for determiners and
the inverted subject clitic (t?il). In addition, the lex-
icon is restricted to avoid lexical or morphological
variants.
We show below some of the well-formed sen-
tences output by GRADE for the n0V (intransitive
verbs) family.
Elle chante (She sings), La tatou chante-
t?elle? (Does the armadillo sing? ),
La tatou chante (The armadillo sings ),
La tatou qui chante (The armadillo which
sings ), Chacun chante -t?il (Does every-
one sing? ), Chacun chante (Everyone
sings ), Quand chante chacun? (When
does everyone sing? ), Quand chante la
tatou? (When does the armadillo sing?
) Quand chante quel tatou? (When does
which armadillo sing? ), Quand chante
Tammy? (When does Tammy sing? ),
Chante-t?elle? (Does she sing? ) Chante
-t?il? (Does he sing? ), Chante! (Sing!
), Quel tatou chante ? (Which armadillo
sing? ), Quel tatou qui chante ..? (Which
armadillo who sings ..? ) Tammy chante-
t?elle? (Does Tammy sing? ), Tammy
chante (Tammy sings ), une tatou qui
chante chante (An armadillo which sings
sings ), C?est une tatou qui chante (It is an
armadillo which sings ), ...
The call on this family returned 55 distinct MRSs
and 65 distinct sentences of which only 28 were cor-
rect. Some of the incorrect cases are shown below.
They illustrate the four main sources of overgener-
ation. The agreement between the inverted subject
clitic and the subject fails to be enforced (a); the in-
verted nominal subject fails to require a verb in the
indicative mode (b); the inverted subject clitic fails
to be disallowed in embedded clauses (c); the inter-
rogative determiner quel fails to constrain its nomi-
nal head to be a noun (d,e).
(a) Chacun chante-t?elle? (Everyone
sings?) (b) Chante?e chacun? (Sung every-
one?) (c) La tatou qui chante-t?elle? (The
armadillo which does she sing?) (d) Quel
chacun chante ? (Which everyone sings?)
(e) quel tammy chante ? (Which Tammy
sings?)
5.3 Interactions with Modifiers
Once basic functor/argument dependencies have
been verified, adjunction constraints can be used to
36
explore the interactions between e.g., basic clauses
and modification5. Allowing for N-adjunctions for
instance, will produce sentences including determin-
ers and adjectives. Similarly, allowing for V ad-
junction will permit for auxiliaries and adverbs to
be used; and allowing for VP or S adjunctions will
licence the use of raising verbs and verbs subcate-
gorising for sentential argument.
We queried GRADE for derivations rooted in n0V
(intransitive verbs) and with alternatively, 1N, 2N,
1V and 1VP adjunction. Again a restricted lexicon
was used to avoid structurally equivalent but lexi-
cally distinct variants. The following table shows
the number of sentences output for each query.
0 1S 1VP 1V 1N 2N
36 170 111 65 132 638
As the examples below show, the generated sen-
tences unveil two further shortcomings in the gram-
mar: the inverted subject clitic fails to be constrained
to occur directly after the verb (1) and the order and
compatibility of determiners are unrestricted (2).
(1) a. Semble-t?il chanter? / * Semble chanter
t?il? (Does he seems to sing?)
b. Chante-t?il dans Paris? / * Chante dans
Paris-t?il? (Does he sing in Paris?)
c. Chante-t?il beaucoup? / * Chante
beaucoup-t?il? (Does he sing a lot?)
d. Veut-t?il que Tammy chante? / * Veut que
Tammy chante-t?il? (Does he want that
Tammy sings?
(2) * Un quel tatou, *Quel cette tatou, Ma quelle
tatou (Un which armadillo, Which this ar-
madillo, My which armadillo)
5.4 Inspecting Coverage and Correctness
In the previous sections, GRADE was used to gen-
erate MRSs and sentences ex nihilo. As mentioned
above however, a core semantics can be used to re-
strict the set of output sentences to sentences whose
MRS include this core semantics. This is useful for
5Recall that in FB-LTAG, adjunction is the operation which
permits applying recursive rules (i.e., auxiliary trees). Hence
allowing for adjunctions amounts to allowing for modification
with the exception already noted above of certain verbs subcat-
egorising for sentential arguments.
Tree Family MRS Sent. S/MRS
ilV 7 52 7.4
n0V 65 161 2.4
n0ClV 30 62 2.0
n0ClVn1 20 25 1.25
n0ClVden1 10 15 1.5
n0ClVpn1 40 63 1.57
n0Vn1 20 110 5.5
n0Van1 30 100 3.33
n0Vden1 5 15 3.00
n0Vpn1 25 76 3.04
ilVcs1 1 1 1.00
n0Vcs1 200 660 3.3
n0Vas1 35 120 3.42
n0Vn1Adj2 10 15 1.5
s0Vn1 4 24 6.00
n0Vn1n2 10 48 4.80
n0Vn1an2 5 45 9.00
Table 2: Producing Variants
instance, to systematically inspect all variations out-
put by the grammar on a given input. These varia-
tions include all morphological variations supported
by the lexicon (number, tense, mode variations) and
the syntactic variations supported by the grammar
for the same MRSs (e.g., active/passive). It also in-
cludes the variations supported by GRADE in that
some rules are not checked for semantic compati-
bility thereby allowing for additional materials to be
added. In effect, GRADE allows for the inclusion of
arbitrary determiners and auxiliaries.
Table 2 shows the number of MRSs and sen-
tences output for each verb family given a match-
ing core semantics and a morphological lexicon in-
cluding verbs in all simple tenses (3rd person only)
and nouns in singular and plural6. The ratio S/M of
sentences on MRSs produced by one GRADE call
shows how the underspecified core semantics per-
mits exploring a larger number of sentences gener-
ated by the grammar than could be done by gener-
ating from fully specified MRSs. For the n0Vn1an2
class, for instance, the GRADE call permits generat-
ing 9 times more sentences in average than generat-
ing from a single MRS.
6The lexicon used in this experiment includes more mor-
phological variants than in the experiment of Section 5.2 where
the focus was on syntactic rather than morphological variants.
Hence the different number of generated sentences.
37
6 Conclusion
When using a grammar for generation, it is essen-
tial, not only that it has coverage (that it does not
undergenerate) but also that it be precise (that it
does not overgenerate). Nonetheless, relatively lit-
tle work has been done on how to detect overgener-
ation. In this paper, we presented an algorithm and
a methodology to explore the sentences generated
by a grammar; we described an implementation of
this algorithm based on DCGs (GRADE ); and we
illustrated its impact by applying it to an existing
grammar. We showed that GRADE could be used
to explore a grammar from different viewpoints: to
find gaps or inconsistencies in the rule system; to
systematically analyse the grammar account of func-
tor/argument dependencies; to explore the interac-
tion between base constructions and modifiers; and
to verify the completeness and correctness of syn-
tactic and morphological variants.
There are many directions in which to pursue
this research. One issue is efficiency. Unsurpris-
ingly, the computational complexity of GRADE is
formidable. For the experiments reported here, run-
times are fair (a few seconds to a few minutes de-
pending on how much output is required and on the
size of the grammar and of the lexicon). As the com-
plexity of the generated sentences and the size of the
lexicons grow, however, it is clear that runtimes will
become unpractical. We are currently using YAP
Prolog tabling mechanism for storing intermediate
results. It would be interesting however to compare
this with the standard tabulating algorithms used for
parsing and surface realisation.
Another interesting issue is that of the interac-
tion between GRADE and error mining. As men-
tioned in Section 2, GRADE could be usefully com-
plemented by error mining techniques as a means
to automatically identify the most probable causes
of errors highlighted by GRADE and thereby of im-
proving the grammar. To support such an integration
however, some means must be provided of sorting
GRADE ?s output into ?good? and ?bad? output i.e.,
into sentences that are grammatical and sentences
that are over-generated by the grammar. We plan to
investigate whether language models could be used
to identify those sentences that are most probably
incorrect. In a first step, simple and highly con-
strained input would be used to generate from the
grammar and the lexicon a set of correct sentences
using GRADE . Next these sentences would be used
to train a language model which could be used to
detect incorrect sentences produced by GRADE on
more complex, less constrained input.
Other issues we are currently pursueing are the
use of GRADE (i) for automating the creation of
grammar exercises for learners of french and (ii) for
creating a bank of MRSs to be used for the evalua-
tion and comparison of data-to-text generators. The
various degrees of under-specification supported by
GRADE permit producing either many sentences out
of few input (e.g., generate all basic clauses whose
verb is of a given subcategorisation type as illus-
trated in Section 5.2); or fewer sentences out a more
constrained input (e.g., producing all syntactic and
morphological variants verbalising a given input se-
mantics). We are currently exploring how seman-
tically constrained GRADE calls permit producing
variants of a given meaning; and how these vari-
ants can be used to automatically construct gram-
mar exercises which illustrate the distinct syntac-
tic and morphological configurations to be acquired
by second language learners. In contrast, more un-
derspecified GRADE calls can be used to automat-
ically build a bank of semantic representations and
their associated sentences which could form the ba-
sis for an evaluation of data-to-text surface realis-
ers. The semantics input to GRADE are simplified
representations of MRSs. During grammar traver-
sal, GRADE reconstructs not only a sentence and
its associated syntactic tree but also its full MRS.
As a result, it is possible to produce a generation
bank which, like the Redwook Bank, groups to-
gether MRSs and the sentences verbalising these
MRSs. This bank however would reflect the linguis-
tic coverage of the grammar rather than the linguis-
tic constructions present in the corpus parsed to pro-
duce the MRS. It would thus provide an alternative
way to test the linguistic coverage of existing surface
realisers.
Acknowledgments
The research presented in this paper was partially
supported by the European Fund for Regional De-
velopment within the framework of the INTERREG
IVA Allegro Project.
38
References
Anja Belz, Michael White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalua-
tion results. In Proc. of the 13th European Workshop
on Natural Language Generation.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, Ingria R., F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cov-
erage of english grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop,
page 306311.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. In 18th IJCAI, pages
811?817, Aug.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An algebra for semantic construction in
constraint-based grammars. In Proceedings of the
39th Annual Meeting of the Association for Compu-
tational Linguistics, Toulouse, France.
Benoit Crabbe?. 2005. Reprsentation informatique de
grammaires d?arbres fortement lexicalise?es : le cas de
la grammaire d?arbres adjoints. Ph.D. thesis, Nancy
University.
Danie?l de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error mining
in parsing results. In ACL2009 Workshop Grammar
Engineering Across Frameworks (GEAF), Singapore.
Claire Gardent and Eric Kow. 2007. Spotting overgener-
ation suspect. In 11th European Workshop on Natural
Language Generation (ENLG).
Claire Gardent and Shashi Narayan. 2012. Error mining
on dependency trees. In Proceedings of ACL.
Claire Gardent, Benjamin Gottesman, and Laura Perez-
Beltrachini. 2010. Benchmarking surface realisers. In
COLING 2010 (Poster Session), Beijing, China.
Claire Gardent, Benjamin Gottesman, and Laura Perez-
Beltrachini. 2011. Using regular tree grammar to
enhance surface realisation. Natural Language En-
gineerin, 17:185?201. Special Issue on Finite State
Methods and Models in Natural Language Processing.
Claire Gardent. 2008. Integrating a unification-based
semantics in a large scale lexicalised tree adjoining
grammar for french. In COLING?08, Manchester, UK.
Paul Klint, Ralf La?mmel, and Chris Verhoef. 2005.
Toward an engineering discipline for grammarware.
ACM Transactions on Software Engineering Method-
ology, 14(3):331?380.
Benoit Sagot and Eric de la Clergerie. 2006. Error min-
ing in parsing results. In ACL, editor, Proceedings of
the ACL 2006, pages 329?336, Morristown, NJ, USA.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In ACL, editor, Pro-
ceedings of the ACL 2004, pages 446?454, Morris-
town, NJ, USA.
K. Vijay-Shanker and Aravind Joshi. 1988. Feature
Structures Based Tree Adjoining Grammars. Proceed-
ings of the 12th conference on Computational linguis-
tics, 55:v2.
39
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 141?145,
Utica, May 2012. c?2012 Association for Computational Linguistics
KBGen ? Text Generation from Knowledge Bases as a New Shared Task
Eva Banik1, Claire Gardent2, Donia Scott3, Nikhil Dinesh4, and Fennie Liang5
1ebanik@comp-ling.co.uk, Computational Linguistics Ltd, London, UK
2claire.gardent@loria.fr, CNRS, LORIA, Nancy, France
3D.R.Scott@sussex.ac.uk, School of Informatics, University of Sussex, Brighton, UK
4dinesh@ai.sri.com, SRI International, Menlo Park, CA
5fennie.liang@cs.man.ac.uk, School of Computer Science, University of Manchester, UK
1 Introduction
In this paper we propose a new shared task, KB-
Gen, where the aim is to produce coherent descrip-
tions of concepts and relationships in a frame-based
knowledge base (KB). We propose to use the AURA
knowledge base for the shared task which contains
information about biological entities and processes.
We describe how the AURA KB provides an appli-
cation context for NLG and illustrate how this ap-
plication context generalizes to other biology KBs.
We argue that the easy availability of input data and
a research community ? both domain experts and
knowledge representation experts ? which actively
uses these knowledge bases, along with regular eval-
uation experiments, creates an ideal scenario for a
shared task.
2 Application Context and Motivation
One of the research challenges in the knowledge rep-
resentation community is to model complex knowl-
edge in order to be able to answer complex ques-
tions from a knowledge base (see e.g. the Deep
Knowledge Representation Challenge Workshop at
KCAP 20111). There are several applications of
such knowledge bases, perhaps most recently and
most prominently in the bioinformatics and educa-
tional informatics domain, where there are available
knowledge bases and reasoners that help scientists
answer questions, explain connections between con-
cepts, visualize complex processes, and help stu-
dents learn about biology. These uses of a knowl-
edge base are however difficult to implement with-
1http://sites.google.com/site/dkrckcap2011/home
out presenting the resulting answers and explana-
tions to the user in a clear, concise and coherent way,
which often requires using natural language.
2.1 The AURA Knowledge Base
The AURA biology knowledge base developed by
SRI International (Gunning et al, 2010) encodes in-
formation from a biology textbook (Reece et al,
2010)2. The purpose of this knowledge base is
to help students understand biological concepts by
allowing them to ask questions about the material
while reading the textbook. The KB is built on top
of a generic library of concepts (CLIB, Barker et al,
2001), which are specialized and/or combined to en-
code biology-specific information, and it is orga-
nized into a set of concept maps, where each con-
cept map corresponds to a biological entity or pro-
cess. The KB is being encoded by biologists and
currently encodes over 5,000 concept maps.
The AURA KB and its question answering sys-
tem is integrated with an electronic textbook appli-
cation3. The applicaton allows the students to ask
complex questions about relationships between con-
cepts, which are answered by finding a possible path
between the two concepts. The results are presented
to the students as graphs, for example the answer
produced by the system in response to the question
?what is the relationship between glycolysis and glu-
cose?? is illustrated in Fig 1.
These graphs are simplified representations of
2The development of the AURA knowledge base and related
tools and applications was funded by Vulcan Inc.
3A demo of the application will be presented in the demo
session at INLG 2012
141
Figure 1: Relationship between glycolysis and glucose
a path in the knowledge base that connects two
concepts, because presenting the full concept map
where the path was found would make it difficult for
the students to clearly see the relationship. However,
this simplification often obscures the connection by
not showing relevant information.
Given the inclusion of a few more relations from
the concept map of glycolysis (Fig 2), the answer to
the question could be generated as a complex sen-
tence or a paragraph of text, for example: ?Phos-
phorylation of glucose is the first step of the energy
investment phase of glycolysis? or ?In the first step
of the energy investment phase of glycolysis, called
phosphorylation, hexokinase catalyses the synthesis
of glucose-6-phosphate from glucose and a phos-
phate ion.?
2.2 BioCyc
Another situation in which graph-based representa-
tions are presented to the user is metabolic pathway
and genome databases, such as the BioCyc knowl-
edge base. BioCyc describes the genome, metabolic
pathways, and other important aspects of organisms
such as molecular components and their interactions
and currently contains information from 1,763 path-
Figure 2: Concept map of glycolysis
way/genome databases4.
When users query parts of the BioCyc knowledge
base, the system automatically produces a graph
to visualize complex biological processes. For ex-
ample, Fig 3 illustrates an automatically generated
graph from the knowledge base which shows the
process of glycolysis in an E. coli cell. Hovering the
mouse over the ? and 	 signs on the graph brings
up popups with additional information about gene
expressions , detailed chemical reactions in the pro-
cess, enzymes activated by certain chemicals, etc..
Figure 3: The process of glycolysis in E.coli
3 Input Data for Generation
Although there is a clear benefit from visualizing
complex processes in a graph form, one also has to
4http://www.biocyc.org
142
be well-versed in the notation and details of biolog-
ical processes in order to make sense of these rep-
resentations. Students of biology and non-experts
would certainly benefit from a more detailed ex-
planation of the process, presented as a few para-
phraphs of text along with graphs to emphasize the
most salient features of processes.
The paths and relations returned by reasoning al-
gorithms also present a good opportunity to pro-
vide inputs for natural language generation. These
chunks of data typically contain the right amount of
data because they consist of the information needed
to answer a question or describe a concept. Ad-
ditionally, many knowledge bases (including both
BioCyc and AURA) are encoded in a frame-based
representation, which has the advantage that frames
naturally correspond to linguistic units.
Frame-based systems (Minsky, 1981) are based
around the notion of frames or classes which repre-
sent collections of concepts. Each frame has an as-
sociated set of slots or attributes which can be filled
either by specific values or by other frames. Intu-
itively, frames correspond to situations, and each ter-
minal in the frame corresponds to answers to ques-
tions that could be asked about the situation, in-
cluding the participants in the situation, causes and
consequences, preceding and following situations,
purpose, etc. Frame-based representations may ei-
ther contain frames of generic concepts or instance
frames which represent information about particular
instances. Frames also have a kind-of slot, which
allows the assertion of a frame taxonomy, and the
inheritance of slots.
In the knowledge representation community,
frame-based representations are popular because
they make the encoding process more intuitive.
From a natural language generation perspective,
each frame (or a set of slots) corresponds to a lin-
guistic unit (sentence, noun phrase, clause, verb
phrase, etc), depending on the type of the frame and
the slots it contains. This organization of concepts
and relations in the knowledge base makes it easier
to select chunks of data from which coherent texts
can be generated.
Slots in these frame-based representations also
naturally correspond to the kind of flat semantic
representations and dependency structures that have
served as input to surface realization (Koller and
Striegnitz, 2002; Carroll and Oepen, 2005; White,
2006; Gardent and Kow, 2007; Nakatsu and White,
2010).
4 The shared task
We propose two tracks for the KBGen shared task: a
?complex surface realization? track, where the task
is to generate complex sentences from shorter in-
puts, and a ?discourse generation? track, where the
task is to generate longer texts made up from several
paragraphs. In the following, we describe the data
set from which the input to generation will be se-
lected; the methology we plan to use to extract text
size input for the generation challenge; and the two
tracks making up the KBGen challenge.
4.1 The AURA knowledge base as Input
Dataset
We propose to use the AURA knowledge base as
input data for the shared task for several reasons.
AURA contains a number of relations and therefore
provides varied input for generation5. The AURA
knowledge base contains linguistic resources that
can be used for generation (a morphological lexi-
con and a list of synonyms for each concept) and
the electronic textbook provides an application con-
text to evaluate the generated texts. There are regular
evaluation efforts to assess the educational benefits
of using the textbook application, and the next round
of these experiments will involve over 400 students
and biology teachers who will use the application
over an extended period of time. The evaluation of
the outputs generated for the shared task could form
part of these experiments.
4.2 Selecting Text Size Content for the Shared
Task
We propose to select data from the knowledge base
manually or semi-automatically, by selecting a set
of concepts to be described and including relevant
relations associated with the concepts. We would
first select a set of concept maps that are encoded in
most detail and have been reviewed by the encoders
for quality assurance. The input data for each con-
cept will then be a manually selected set of frames
5If there is interest, the systems developed to generate from
AURA could also be applied to the BioCyc data, which has a
more restricted set of relations.
143
from the concept map. The selected relations will be
reviewed one more time for quality and consistency
to filter out any errors in the data.
If there is interest in the community, we can
also envision a content selection challenge which
could provide input to the generation task. Although
frames in the knowledge base correspond well to
chunks of data for generation of descriptions, con-
tent selection for other communicative goals is far
from a trivial problem. One such challenge could
be for example comparing two concepts, or explain-
ing the relation between a process and its sub-type
(another process that is taxonomically related, but
different in certain parts).
4.3 Complex Surface Realization Track
For the complex surface realization track, a small
number of frames would be selected from the knowl-
edge base along with a small number of other rel-
evant relations (e.g., important parts or properties
of certain event participants, or certain relations be-
tween them, depending on the context). The output
texts to be generated would be complex sentences
describing the central entity/event in the data, or the
relationship between two concepts, such as the gly-
colysis example in section 2.1. This task would
involve aggregation and generating intrasentential
pronouns governed by syntax where necessary, but
it would not require the generation of any discourse
anaphora or referring expressions.
This track will differ from the deep generation
track of the Surface Realization Shared Task both in
form and in content. The form of the KBGen input
is a concept map extracted from an ontology rather
than a deep semantics extracted by conversion from
dependency parse trees. Similarly, its content is that
of a biology knowledge base rather than that of the
Penn Treebank textual corpus.
4.4 Discourse Generation Track
Inputs for the discourse generation task would in-
clude most frames from the concept map of an entity
or process. The output would be longer paragraphs
or 2-3 paragraphs of text, typically a description of
the subevents, results, etc, of a biological process,
or the description of the structure and function of an
entity. This task would involve text structuring and
the generation of pronouns.
4.5 Lexical Resources and potential
multilingual tracks
The knowledge base provides a mapping from con-
cepts to lexical items and a list of synonyms. It
also provides information about how specific slots
in event frames are mapped onto prepositions.
If there is interest in the community, the lex-
ical resources corresponding to the selected con-
tent could be translated to different languages semi-
automatically: the translation could be attempted
first automatically, with the help of available biol-
ogy/medical lexicons, and then the output would be
hand-corrected. Candidate languages for a multilin-
gual challenge would be French and Spanish. To
run the multilingual tracks we would need to create
multilingual development and test data and would
need to have access to French/Spanish speaking bi-
ologists.
5 Evaluation
Evaluation of the generated texts could be done both
with automatic evaluation metrics and using human
judgements. Automatic evaluation metrics could in-
clude BLUE (Papineni et al, 2002) or measuring
Levenshtein distance (Levenshtein, 1966) from hu-
man written texts. To obtain human judgements, bi-
ologists will be asked to compose texts conveying
the same content as the input for the generated texts.
The human-written texts will be presented to sub-
jects along with the generated outputs to obtain flu-
ency judgements, but the subjects will not be told
which kind of text they are judging. The evaluation
campaign could be coordinated with the evaluation
of the knowledge base and the electronic textbook
application, and/or publicized on social networking
sites or mechanical turk.
6 Next Steps
We invite feedback on this proposal with the aim
of refining our plan and discussing a suitable input
representation for the shared task in the next few
months. If there is sufficient interest in the shared
task, we would make the input data available in the
agreed format in late 2012, with the first evaluation
taking place in 2013. We would like to hear any
comments/suggestions/critisisms about the plan and
we are actively looking for people who would be in-
144
terested in getting involved in planning and running
the challenge.
References
Barker, K., B. Porter, and P. Clark. 2001. A library of
generic concepts for composing knowledgebases.
In Proceedings of the 1st Int Conf on Knowledge
Capture (K-Cap?01), 14?21.
Carroll, J., and S. Oepen. 2005. High efficiency real-
ization for a wide-coverage unification grammar.
2nd IJCNLP .
Gardent, C., and E. Kow. 2007. A symbolic ap-
proach to near-deterministic surface realisation
using tree adjoining grammar. In In 45th Annual
Meeting of the ACL.
Gunning, D., V. K. Chaudhri, P. Clark, K. Barker,
Shaw-Yi Chaw, M. Greaves, B. Grosof, A. Leung,
D. McDonald, S. Mishra, J. Pacheco, B. Porter,
A. Spaulding, D. Tecuci, and J. Tien. 2010.
Project halo update - progress toward digital aris-
totle. AI Magazine Fall:33?58.
Koller, Alexander, and Kristina Striegnitz. 2002.
Generation as dependency parsing. In Proceed-
ings of ACL.
Levenshtein, Vladimir I. 1966. Binary codes capable
of correcting deletions, insertions, and reversals.
Soviet Physics Doklady 10:707?710.
Minsky, Marvin. 1981. Mind design, chapter A
Framework for Representing Knowledge, 95?
128. MIT Press.
Nakatsu, Crystal, and Michael White. 2010. Gen-
erating with discourse combinatory categorial
grammar. submitted to Linguistic Issues in Lan-
guage Technology .
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. 311?318.
Reece, Jane B., Lisa A. Urry, Michael L. Cain,
Steven A. Wasserman, Peter V. Minorsky, and
Robert B. Jackson. 2010. Campbell biology. Pear-
son Publishing.
White, Michael. 2006. Ccg chart realization from
disjunctive inputs. In Proceedings of the Fourth
International Natural Language Generation Con-
ference, 12?19. Sydney, Australia: Association
for Computational Linguistics.
145
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 10?19,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
An End-to-End Evaluation of Two Situated Dialog Systems
Lina M. Rojas-Barahona
Inria, LORIA, UMR 7503
Villers-le`s-Nancy
F-54600, France
lina.rojas@loria.fr
Alejandra Lorenzo
Universite? de Lorraine
LORIA, UMR 7503
Vandoeuvre-le`s-Nancy
F-54500, France
alejandra.lorenzo@loria.fr
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy
F-54500, France
claire.gardent@loria.fr
Abstract
We present and evaluate two state-of-the art
dialogue systems developed to support dialog
with French speaking virtual characters in the
context of a serious game: one hybrid statis-
tical/symbolic and one purely statistical. We
conducted a quantitative evaluation where we
compare the accuracy of the interpreter and
of the dialog manager used by each system; a
user based evaluation based on 22 subjects us-
ing both the statistical and the hybrid system;
and a corpus based evaluation where we exam-
ine such criteria as dialog coherence, dialog
success, interpretation and generation errors in
the corpus of Human-System interactions col-
lected during the user-based evaluation. We
show that although the statistical approach is
slightly more robust, the hybrid strategy seems
to be better at guiding the player through the
game.
1 Introduction
In recent years, there has been much research on cre-
ating situated conversational characters i.e., virtual
characters (VCs) that look and act like humans but
inhabit a virtual environment (Gratch et al, 2002;
Hofs et al, 2010; Traum et al, 2007; Johnson et al,
2005; Traum et al, 2008; DeVault et al, 2011).
In this paper, we focus on French speaking, situ-
ated conversational agents who interact with virtual
characters in the context of a serious game designed
to promote careers in the plastic industry (The Mis-
sion Plastechnologie game or MP). We present and
compare two state-of-the art dialogue systems. The
first system (H) is a hybrid approach that com-
bines an information-state dialogue manager (Lars-
son and Traum, 2000) with a classifier for interpret-
ing the players? phrases. The second system (QA)
is a question/answering character model which pre-
dicts the system dialog move given a player?s ut-
terance (Leuski and Traum, 2008). Both systems
use a generation-by-selection strategy (Leuski et al,
2006; Gandhe and Traum, 2007) where the system?s
utterances are selected from a corpus of possible
outputs based on the dialog manager output. While
previous work focuses on relatively short dialogs in
a static setting, in our systems we consider long in-
teractions in which dialogs occur in a setting that
dynamically evolves as the game unfolds.
We evaluate the two dialog systems in the con-
text of the 3D game they were developed for and
seek to determine the degree to which a dialog sys-
tem is operational. To answer this question, we anal-
yse both systems with respect not only to quantita-
tive metrics such as accuracy but also to user- and
corpus-based metrics. User-based metrics are com-
puted based on a questionnaire the users filled in;
while corpus-based metrics are manually extracted
from the corpus of Player-VC interactions collected
during the user-based evaluation. As suggested by
evaluation frameworks such as PARADISE (Walker
et al, 1997) and SASSI (Hone and Graham, 2000),
we show that a multiview evaluation permits a better
assessment of how well the dialog system functions
?in the real world?. The metrics proposed assess di-
alog success and coherence, as well the costs of dia-
log components.
The paper is organized as follows. In Section 2,
10
we present the MP game, the dialogue strategies
used in the different dialogs and the dialog data used
for training. Section 3 presents the two dialog sys-
tems we compare. Section 4 presents the evaluation
schemes used to compare these two systems and dis-
cusses the results obtained. Section 5 concludes with
directions for further research.
2 Dialogues in the MP Game
We begin by describing the MP game, the dialogs in
the MP game, the strategies used to guide the hybrid
dialog manager and the data used for training.
2.1 The MP Game and Dialogs
The MP game is a multi-player quest where 3
teenagers seek to build a joystick in order to free
their uncle trapped in a video game 1. To build
this joystick, the player (who alternatively repre-
sents anyone of these three teenagers) must explore
the plastic factory and achieve 17 mandatory goals
(find the plans, get the appropriate mould, retrieve
some plastic from the storing shed, etc), as well
as 11 optional goals which, when reached, provide
them with extra information about the plastic indus-
try (and therefore increases their knowledge of it).
In total, the player can achieve up to 28 game
goals by conducting 12 separate dialogs in various
parts of the virtual world. Each of the 12 dialogs
in the MP game helps players to achieve the game
goals. The player interacts with the virtual charac-
ters to obtain information that helps her to achieve
these goals and, as a consequence, to increase her
score in the game. Table 1 summarises the game
goals and the contextual parameters (player?s role,
location in the virtual world, VCs present) associ-
ated with each dialog.
2.2 Dialog Data and Annotation
To train both classifiers, the one used by the hybrid
and the one used by the QA system, we collected
Human-Machine dialog data using a Wizard-of-Oz
setting and manually annotated each turn with a di-
alog move. The resulting corpus (called Emospeech
Corpus) and the annotation scheme (as well as the
inter-annotator agreement) used are described in de-
1The MP game was created by Artefacto, http://www.
artefacto.fr/index_ok.htm
tail (Rojas-Barahona et al, 2012). Briefly, the Emo-
speech Corpus comprises 1249 dialogs, 10454 utter-
ances and 168509 words. It contains 3609 player ut-
terances consisting of 31613 word tokens and 2969
word types, with approximately 100 conversations
for each dialog in the game. Turns were annotated
with dialog moves (Traum and Larsson, 2003) cap-
turing both domain knowledge (e.g., about the goals
set by the game) and the set of core communicative
acts.
2.3 Dialog Strategies
We identified four main dialog strategies underlying
the 12 MP dialogs and used these to define the plans
guiding the rule-based discourse management in the
hybrid system. These strategies can be seen as trans-
actions made up of conversational games (Carletta et
al., 1997).
Strategy 1. This strategy is used in the first di-
alog only and consists of a single Address Request
move by the VC followed by the player?s answer:
Lucas requests Ben to find the address of the Plas-
tic Enterprise that must be hidden somewhere in the
lab. Ben can accept, reject or ask for help. Lucas
answers accordingly and ends the conversation.
Strategy 2. Nine dialogues follow this strategy.
They include several (up to 5) requests for infor-
mation and the corresponding system/player?s ex-
change. Appendix A shows an example dialog fol-
lowing this strategy.
Strategy 3: This is a confirmation strategy where
the VC first checks that the player has already
achieved a given task, before informing her about
the next step (e.g. dialogs with Melissa in Table 1).
Strategy 4. This strategy, exemplified in Ap-
pendix B, is similar to strategy 2 but additionally
includes a negotiation step where the VC asks the
player for help.
3 Dialogue Systems
The game and the two dialog systems built were in-
tegrated as agents within the Open Agent Architec-
ture as shown in Figure 1. Both systems access a
database for starting the appropriate dialogs at the
appropriate place in the virtual world while simulta-
neously storing all interactions in the database.
11
Id VC Player Goals Location
1 Lucas Ben Find the address of the enterprise. Uncle?s place.
2 M.Jasper Lucas The manufacturing first step Enterprise reception
3 Samir Julie Find the plans of the joystick Designing Office
Optional: job, staff, studies, security policies
4 Samir Julie Find out what to do next Designing Office
Optional: jobs in the enterprise, staff in the enterprise
5 Melissa Lucas Find the mould, optional where are the moulds Plant
6 Melissa Lucas Find the right machine Plant
7 Melissa Lucas Confirm you have found the right mould and machine and Plant
find out what to do next
8 Operator Julie Knowing about the material space and about the job Material Space
Optional: find out what to do in the case of failure
helping to feed a machine with the right material
9 Serge Ben Perform quality tests. Laboratory Tests
Optional: VC?s job
10 Serge Ben Find out what to do next. Laboratory Tests
Optional: know what happens with broken items
11 Sophia Julie Find the electronic components, knowing about VC?s job Finishing
12 Sophia Lucas Finishing process Finishing
Optional: know about conditioning the product
Table 1: Description of the 12 dialogs in the MP Game.
Figure 1: General Architecture for the dialog system:
modules are implemented as agents within the Open
Agent Architecture.
3.1 The Hybrid Dialogue System
The hybrid system combines an interpreter; a rule
based, Information State Approach dialog manager;
a generator; and the game/dialog communication
components i.e., the OAA interface.
The Interpreter Module In the hybrid system,
the interpreter is a classifier trained on the anno-
tated data (cf. section 2.2), which maps the player?s
utterance to a dialog move. To build the classi-
fier we experimented with both SVM (Support Vec-
tor Machine) and LR (Logistic Regression) 2 us-
ing different sets of features: utterances were pre-
processed by removing stop words and converting
content words to unaccented lowercase. Afterwards,
we experiment with and without TF*IDF (term fre-
quency*Inverse Document Frequency) filtering and
context moves (from 0 to 4 previous dialog moves).
Since the game consist of a number of different
dialogs, two options naturally arise: we could either
train a single classifier on the whole dataset or train
one classifier per dialog. Hence the data sets (and the
number of categories to be learned) differ depending
on whether we trained one classifier on the whole
game data or a classifier for each dialog.
In addition, hand-written rules are used to correct
misclassification errors as follows. The best predic-
tion given by the classifier is matched against the
expected move determined by the last move stored
in the information-state (see below the description
of the dialog manager). In case of a mismatch, the
interpreter selects a move that is valid in the current
context and updates the information state with this
move. For instance, if the last move is a yes/no ques-
tion, confirm and disconfirm dialog acts are valid
moves; for propositional questions, Goali is a valid
2We used LIBSVM (Chang and Lin, 2001) and MALLET
(McCallum, 2002) for the LR classifier with L1 Regularisation.
12
dialog move 3; etc. Since the process is non deter-
ministic, this re-interpretation process may improve
the system response but it may also be incorrect. For
instance, in the following example re-interpretation
correctly overrides the dialog move output by the
classifier to a move asking the VC (Samir) for the
next goal to be achieved.
Samir: Vous avez d?autres questions pour moi?
Do you have other questions ?
Julie: comment cela fonctionne ?
How does it work?
Samir: Eh bien , il va falloir la fabriquer maintenant cette
manette . Allez voir Me?lissa.
Well, you have to build the joystick now. Go to see Melissa.
In contrast, the dialog below shows a case where
re-interpretation fails. Instead of re-interpreting the
player?s (Julie) input as a request for the next goal,
it outputs a request for information about the staff
thereby yielding an incoherent exchange.
Samir: D? autres questions ?
Other questions?
Julie: oui qu?est-ce que je peux faire avec ces bouts de papier ?
yes, what can I do with these pieces of paper ?
Samir: Et bien sachez qu?il y a de plus en plus des femmes dans
cette industrie ...
you should know there are more and more women in this indus-
try ...
The DialogManager We designed a plan for each
dialog strategy and extended Midiki (Burke et al,
2003) to support the OAA architecture and access
the relational database specifying the configurations
of the different dialogs in the game. Each time a new
dialog is launched, the information state is loaded
with the corresponding dialog-context (e.g., speak-
ers, list of goals to be discussed) and the plan mod-
eling the corresponding dialog strategy. To support
dialog management, we implemented a set of update
and selection rules for integrating players? moves,
handling the information-state and for preparing the
agenda according to the plan. More specifically, the
following rules are executed at runtime: Integration:
integrates dialog moves (e.g., questions, answers,
acknowledgments) in the information state (ques-
tions are listed in the Question Under Discussion,
3The system asks the player for the goal to be discussed:
ask(task(X)) and the player answers one goal in the situated
dialog: Goali.
answers change the Commond Ground, player an-
swers are integrated in response to VCs questions).
Manage Plan: searches the next action in the plan.
Refill Agenda: updates the agenda with the next ac-
tion and Selection: selects the next dialog move ac-
cording to the plan. Once the system move has been
selected, the Generator searches an appropriate ver-
balisation.
The Generator As mentioned above, the gener-
ator implements a generation-by-selection strategy.
Given the dialog move output by the dialog man-
ager, the generator selects any utterance in this cor-
pus that is labeled with this dialog move and with
the identifier of the current dialog.
In addition, two types of dialog moves are
given special treatment. The first two moves of
each dialog are systematically constrained to be
a welcome greeting followed by either a request
to pursue a goal (ask(Goali) or a proposal
to help (ask(task(X))). Furthermore, proposi-
tional questions (i.e., proposals by the system to
discuss additional topics) were annotated separately
with their respective dialog goals. For example,
Samir?s sentence: Are you interested in hearing
about my job, the people that work here or the se-
curity policies?, was annotated with the goals: job,
staff and security policies. For these dialog acts, the
generator checks the list of current missing goals so
as to retrieve an appropriate propositional question.
In this way, the system can coherently direct the
player by suggesting possible topics without using
vague and repetitive sentences such as Would you
like to know more?.
3.2 The QA System
The QA system combines a classifier that matches
players? turns to system dialog moves with the same
generation-by-selection algorithm used in the hybrid
system. Like the hybrid system, the QA dialog sys-
tem maintains a constant interaction with the game
to allow for the game coherence and continuity. In
particular, at the end of each dialog, the game goals
that have been reached through the dialog are re-
turned to the game server so that the game can pro-
ceed and be updated with the appropriate score.
For the QA system, we trained a Logistic Re-
gression (LR) classifier for each dialog in the game.
13
We applied the same preprocessing and extracted the
same features used for training the interpreter in the
hybrid system.
3.3 Discussion and Interaction Examples
As shown above, while the QA system simply
matches the player?s input to a system response, the
hybrid system has a much more elaborate dialog
management policy including re-interpretation and
the use of game and dialog history information,
to guide the dialog. As a result, the QA dialogs
are generally more spontaneous, giving the player
more initiative whereas the hybrid system enforces
a more System-Driven dialog strategy thereby guid-
ing the player through the game. The following two
examples illustrate this point. Sophia is the system
and Lucas the player. The game context provides
two goals which can be discussed: one mandatory
(To find out that finishing the manufacturing process
involves conditioning the product), and one optional
(To be told what conditioning the product means).
The examples show how the hybrid system proposes
both goals to the user while the QA engine provides
the same information fluently.
Hybrid system
Sophia: Vous avez fait du bon travail les enfants!
You did a great job kids!
Sophia: Bien. Que puis-je faire pour vous ?
What can I do for you?
Lucas: J?ai fini la manette
I?ve finished the joystick
Sophia: Normalement j?ai encore quelques e?tapes a` re?aliser
avant l?envoi au client. Vous voulez en savoir plus ?
Normally I still have to condition the product. Do you want to
know more about conditioning a product?
Lucas: Oui
Yes, ok
Sophia: Conditionner le produit veut dire l?emballer soigneuse-
ment pour pouvoir l?expe?dier au client. Aldo, le logisticien as-
sure qu?on ne manque pas de stock et de ge?rer les livraisons et
les expe?ditions.
Conditioning the product means packing it carefully in order to
ship it to the customer. Aldo,the logistics coordinator ensures
there is no shortage of stock and he is also in charge of the de-
liveries and shipments.
Sophia: Je vous laisse tester votre manette . Au revoir
I leave you test out your joystick. Goodbye
Question Answer
Sophia: he? bien c?est du bon travail tout c?a.
Good! That?s a great job kids!
Sophia: Alors , ou` en sommes nous?
So where are we?
Lucas:voila` nous avons utilise? les composants. Quelle est
l?e?tape suivante?
We used the components, what is the next step?
Sophia: On finis la fabrication par conditionner le produit.
We finish the fabrication by conditioning the product.
Lucas:Ok
Sophia: Conditioner le produit veut dire l?emballer soigneuse-
ment pour pouvoir l?expe?dier au client . Le logisticien assure
qu?on ne manque pas de stock et de ge?rer les livraisons et les
expe?ditions.
Conditioning the product means packing it carefully in order to
ship it to the customer. Aldo,the logistics coordinator ensures
there is no shortage of stock and he is also in charge of the de-
liveries and shipments.
Sophia: Et voila, je vous dis au revoir
Ok, I say good bye!
4 Evaluation
In evaluating the two systems, we seek to compare
their usability: Which system is best suited for use
by real users in the context of the MP serious game?
We also seek to better understand which module
causes which errors and why. To address these ques-
tions we conducted a quantitative evaluation where
we compare the accuracy of the interpreter and the
dialog manager integrated in each system; a user
based evaluation involving 22 subjects using both
the QA and the hybrid system; and a corpus based
evaluation where we examine such criteria as dialog
coherence, dialog success, interpretation and gener-
ation errors in the corpus of Human-System interac-
tions collected during the user-based evaluation.
4.1 Quantitative Evaluation
We begin by evaluating the accuracy of the inter-
preter and the dialog manager used by the hybrid and
the QA system respectively. These two classifiers
were trained on the Emospeech corpus mentioned
above and evaluated with 30-fold cross-validation.
Hybrid System As we mentioned in section 3.1,
since the game includes different dialogs, a natu-
ral question arise: whether to implement the inter-
14
preter with a single classifier for the whole dataset,
or using a different classifier for each dialog in the
game. To answer this question, we compared the
accuracy reached in each case. The details of these
experiments are described in (Rojas-Barahona et al,
2012). The highest accuracy is reported when using
a single classifier for the whole game, reaching an
accuracy of 90.26%, as opposed to 88.22% in aver-
age for each dialog. In both cases, the classifier used
is LR, with L1 regularisation and applying the tf*idf
filtering. However, although the classifier trained on
the whole dialog data has better accuracy (learning
a model per dialog often run into the sparse data is-
sue), we observed that, in practice, it often predicted
interpretations that were unrelated to the current di-
alog thereby introducing incoherent responses in di-
alogs. For instance, in the dialog below, the player
wants to know how waste is managed in the fac-
tory. The best prediction given by the interpreter is a
goal related to another dialog thereby creating a mis-
match with the DM expectations. Re-interpretation
then fails producing a system response that informs
the player of the next goal to be pursued in the game
instead of answering the player?s request.
Ben: Comment on ge`re les dechets ici?
How is the waste managed here ?
Serge: Allez voir Sophia pour qu?elle vous fournisse les com-
posants e?lectroniques ne?cessaires a` votre manette.
Go and see Sophia, she?ll give you the electronic components
you need for your joystick.
For the user based experiment, we therefore use
the LR models with one classifier per dialog.
QA System For evaluating the QA classifier, we
also compared results with or without tf*idf filter-
ing. The best results were obtained by the LR clas-
sifier for each dialog with tf*idf filtering yielding an
accuracy of 88.27% as shown in Table 2.
4.2 Preliminary User-Based Evaluation
The accuracy of the interpreter and the dialog man-
ager used by the hybrid and the QA system only
gives partial information on the usability of the di-
alog engine in a situated setting. We therefore con-
ducted a user-based evaluation which aims to assess
the following points: interpretation quality, overall
system quality, dialog clarity, game clarity and tim-
ing. We invited 22 subjects to play the game twice,
Id w/o Tf*Idf w Tf*Idf
1 83.33 82.93
2 93.55 91.8
3 72 80.95
4 80 82.47
5 95.24 93.98
6 97.56 97.5
7 97.5 97.44
8 70.59 76
9 92.77 91.14
10 85.53 86.49
11 83.51 87.5
12 94.12 91.04
Avg. 87.14 88.27
Table 2: Results of the LR classifier for mapping play-
ers? utterances to system moves, with content-words and
a context of four previous system moves, with and with-
out tf*idf filtering.
once with one system and once with the other. The
experiment is biased however in that the players al-
ways used the hybrid system first. This is because in
practice, the QA system often fail to provide novice
players with enough guidance to play the game. This
can be fixed by having the player first use the hybrid
system. Interestingly, the game guidance made pos-
sible by the Information State approach is effective
in guiding players through the game e.g., by propos-
ing new goals to be discussed at an appropriate point
in the dialog; and by taking dialog history into ac-
count.
After playing, each user completed the question-
naire shown in Table 3. For those criteria such as
dialog and game clarity, we do not report the scores
since these are clearly impacted by how many times
the player has played the game. Table 4 shows the
mean of the quantitative scores given by the 22 sub-
jects for interpretation, overall system quality and
timing. We computed a significance test between
the scores given by the subjects, using the Wilcoxon
signed-rank test4. As shown in the Table, for all
criteria, except Q.4, the QA performs significantly
(p < 0.01) better than the Hybrid system.
4The Wilcoxon signed-rank test is the non-parametric alter-
native to the paired t-test for correlated samples, applicable, e.g.
when dealing with measures which cannot be assumed to have
equal-interval scales, as is usual with user questionnaires.
15
Interpretation
Q.1 Did you have the feeling the virtual characters understood you? (very bad 1 ... 100 very good)
Overall System Quality
Q.2 Did you find the conversations coherent? (very bad 1 . . . 100 very good)
Q.3 Did you enjoy talking with the virtual characters? (very annoying 1 ... 100 very enjoyable)
Q.4 Would you prefer playing the game without conversations with virtual characters? (yes/no)
Q.5 What is your overall evaluation of the quality of the conversations? (very bad 1 . . . 100 very good)
Dialogue clarity
Q.6 How easy was it to understand what you were supposed to ask? (very difficult 1 ... 100 very easy)
Q.7 How clear was the information given by the virtual characters? (totally unclear 1 ... 100 very clear)
Q.8 How effective were the instructions at helping you complete the game? (not effective 1 ... 100 very effective)
Game clarity
Q.9 How easy was it to understand the game? (totally unclear 1 ... 100 very clear)
Timing
Q.10 Were the system responses too slow (1) / just at the right time (2) / too fast (3)
Table 3: Questionnaire filled by the subjects that played with both dialog systems.
Interpretation. Question Q.1 aims to captures the
user?s assessment of the dialog system ability to cor-
rectly interpret the player?s utterances. The QA sys-
tem scores 0.7 points higher than the Hybrid system
suggesting better question/answer coherence for this
system. One possible reason is that while the hybrid
system detects any incoherence and either tries to
fix it using re-interpretation (which as we saw some-
times yields an incoherent dialog) or make it explicit
(using a misunderstanding dialog act i.e., a request
for rephrasing), the QA system systematically pro-
vides a direct answer to the player?s input.
The relatively low scores assigned by the user
to the interpretation capabilities of the two systems
(57.36 and 64.55 respectively) show that the high
accuracy of the interpreter and the dialog manager
is not a sufficient criteria for assessing the usability
of a dialog system.
Timing. One important factor for the usability of
a system is of course real time runtimes. The eval-
uation shows that overall the speed of the QA sys-
tem was judged more adequate. Interestingly though
the difference between the two systems stems no so
much from cases where the hybrid approach is too
slow than from cases where it is too fast. These cases
are due to the fact that while the QA system always
issues one-turn answer, the rule based dialog based
approach used in the hybrid system often produce
two consecutive turns, one answering the player and
the other attempting to guide her towards the follow-
ing game goal.
In sum, although the QA system seems more ro-
bust and better at supporting coherent dialogs, the
hybrid system seems to be more effective at guiding
Question Hybrid QA
Interpr. Q.1 57.36 64.55 (*)
Sys Qual.
Q.2 57.78 60.68 (*)
Q.3 60.77 66.45 (*)
Q.4/no 86.37 81.82
Q.5 59.54 65.68 (*)
Avg. 66.12 68.66 (*)
Timing Q.10 2.25 2.05 (*)
Table 4: Mean of the quantitative scores given by 22 in-
dividuals. (*) denotes statistical significance at p < 0.01
(two-tailed significance level).
the player through the game.
4.3 Corpus-Based Evaluation
The User-Based evaluation resulted in the collection
of 298 dialogs (690 player and 1813 system turns)
with the Hybrid system and 261 dialogs (773 player
and 1411 system turns) with the QA system. To bet-
ter understand the causes of the scores derived from
the user-filled questionnaire, we performed manual
error analysis on this data focusing on dialog inco-
herences, dialog success, dialog management and
generation errors (reported in Table 5).
DM Errors The count of dialog management
(DM) errors is the ratio WRP of wrong system re-sponses on counts of player?s input. In essence this
metrics permits comparing the accuracy of the QA
dialog manager with that of the hybrid system. On
average there is no clear distinction between the two
systems.
16
Generation Errors The system response selected
by the generation component might be contextually
inappropriate for at least two reasons. First, it may
contain information which is unrelated to the current
context. Second, it might have been imprecisely or
incorrectly annotated. For instance, in the dialog
below, the annotation of the turn Yes, thanks. What
do you want me to do? did not indicate that the turn
included a Confirm dialog move. Selecting this turn
in the absence of a yes/no question resulted in a
contextually inappropriate system response.
SYSTEM: Bonjour les petits jeunes je suis le pre?parateur
matie?re.
Hello kids, I am the raw material responsible
SYSTEM: Oui merci. Vous me voulez quoi en fait ?
Yes, thanks. What do you want me to do?
PLAYER: je veux en savoir plus sur cet endroit.
I would like to know more about this place
As shown in Table 5, for both systems, there were
few generation errors.
Id %DM H. %DM. QA %Gen H. & QA
1 0.0 4.55 0.57
2 10.81 12.00 1.02
3 10.38 12.04 1.49
4 16.22 14.86 0.32
5 10.34 2.13 1.46
6 0.0 0.0 0.94
7 9.52 4.0 0.0
8 11.68 7.08 2.06
9 2.13 26.47 0.76
10 15.63 16.13 6.08
11 11.94 8.33 3.19
12 14.29 8.16 3.17
Avg. 9.41 9.65 1.76
Table 5: DM and generation errors detected in the hybrid
and the QA systems.
Unsuccessful Dialogs We counted as unsuccess-
ful those dialogs that were closed before discussing
the mandatory goals. The results are shown in Ta-
ble 6. Overall the QA system is more robust leading
to the mandatory goals being discussed in almost all
dialogs. One exception was dialog 8, where the sys-
tem went into a loop due to the player repeating the
same sequence of dialog moves. We fixed this by
Id %Uns. H. %Inco. H. %Uns. QA. %Inc. QA.
1 0 0.0 0.0 0.0
2 0 0.0 0.0 0.0
3 6.67 3.33 7.41 0.0
4 7.14 0.0 0.0 4.0
5 3.85 0.0 0.0 0.0
6 0.0 0.0 0.0 0.0
7 21.21 0.0 0.0 0.0
8 3.70 0.0 15.63 3.13
9 0.0 0.0 0.0 4.35
10 0.0 6.67 0.0 16.67
11 3.45 6.90 0.0 3.70
12 4.17 4.17 4.55 4.55
Avg. 4.89 1.76 4.47 3.03
Table 6: Overall dialog errors, the percentage of unsuc-
cessful dialogs
integrating a loop detection step in the QA dialog
manager. For the hybrid system, dialog 7, a dialog
involving the confirmation strategy (cf. section 2)
is the most problematic. In this case, the DM rules
used to handle this strategy are inappropriate in that
whenever the system fails to identify a contextually
appropriate response, it simply says so and quits the
dialog. The example illustrates the difficulty of de-
veloping a complete and coherent DM rule system.
Incoherent Dialogs We counted as incoherent, di-
alogs where most system answers were unrelated to
the player?s input. As shown in Table 6, despite
interpretation and generation imprecisions, most di-
alogs were globally coherent. They made sense ac-
cording to the game context: they were related to the
task to be solved by the player in the game, and the
generated instructions were correctly understood.
The hybrid system produces slightly less incoher-
ent dialogs probably because of its re-interpretation
mechanism which permits correcting contextually
invalid dialog moves.
5 Conclusion
We have presented a multi-view evaluation of two
system architectures for conversational agents situ-
ated in a serious game. Although the QA system
seems more robust and is easier to deploy, the hy-
brid dialog engine seems to fare better in terms of
game logic in that it guides the player more effec-
17
tively through the game. The evaluation shows the
importance of assessing not only the dialog engine
accuracy but also its usability in the setting it was
designed for. In future work, we plan to compute
a regression model of user satisfaction for applying
reinforcement learning and find the optimal strategy.
In addition, we plan to extend the comparison to
other domains such as language learning and com-
plex negociation dialogs.
6 Acknowledgments
The research presented in this paper was partially
supported by the Eurostar EmoSpeech project and
by the European Fund for Regional Development
within the framework of the INTERREG IV A Alle-
gro Project.
References
C. Burke, C. Doran, A. Gertner, A. Gregorowicz,
L. Harper, J. Korb, and D. Loehr. 2003. Dialogue
complexity with portability?: research directions for
the information state approach. In Proceedings of the
HLT-NAACL 2003 workshop on Research directions in
dialogue processing - Volume 7.
Jean Carletta, Stephen Isard, Gwyneth Doherty-Sneddon,
Amy Isard, Jacqueline C. Kowtko, and Anne H. An-
derson. 1997. The reliability of a dialogue struc-
ture coding scheme. Comput. Linguist., 23(1):13?31,
March.
Chih C. Chang and Chih J. Lin, 2001. LIBSVM: a library
for support vector machines.
David DeVault, Anton Leuski, and Kenji Sagae. 2011.
An evaluation of alternative strategies for implement-
ing dialogue policies using statistical classification
and hand-authored rules. In 5th International Joint
Conference on Natural Language Processing (IJCNLP
2011).
Sudeep Gandhe and David Traum. 2007. Creating spo-
ken dialogue characters from corpora without annota-
tions. In Proceedings of 8th Conference in the Annual
Series of Interspeech Events, pages 2201?2204.
Jonathan Gratch, Jeff Rickel, Elisabeth Andre?, Justine
Cassell, Eric Petajan, and Norman Badler. 2002. Cre-
ating interactive virtual humans: Some assembly re-
quired. IEEE Intelligent Systems, 17:54?63, July.
Dennis Hofs, Marie?t Theune, and Rieks Akker op den.
2010. Natural interaction with a virtual guide in a
virtual environment: A multimodal dialogue system.
Journal on Multimodal User Interfaces, 3(1-2):141?
153, March. Open Access.
Kate S. Hone and Robert Graham. 2000. Towards a
tool for the subjective assessment of speech system
interfaces (sassi). Nat. Lang. Eng., 6(3-4):287?303,
September.
W. L. Johnson, H. H. Vilhja?lmsson, and S. Marsella.
2005. Serious games for language learning: How
much game, how much AI? In Artificial Intelligence
in Education.
S. Larsson and D. Traum. 2000. Information state and di-
alogue management in the TRINDI dialogue move en-
gine toolkit. Natural Language Engineering, 6:323?
340.
Anton Leuski and David Traum. 2008. A statistical ap-
proach for text processing in virtual humans. In Pro-
ceedings of the 26th Army Science Conference.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proceedings of the 7th SIG-
DIAL Workshop on Discourse and Dialogue, pages
18?27.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Lina M. Rojas-Barahona, Alejandra Lorenzo, and Claire
Gardent. 2012. Building and exploiting a corpus of di-
alog interactions between french speaking virtual and
human agents. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(to be published).
David R. Traum and Staffan Larsson. 2003. The in-
formation state approach to dialogue management. In
Jan Kuppevelt, Ronnie W. Smith, and Nancy Ide, ed-
itors, Current and New Directions in Discourse and
Dialogue, volume 22 of Text, Speech and Language
Technology, pages 325?353. Springer Netherlands.
10.1007/978-94-010-0019-2 15.
David R. Traum, Antonio Roque, Anton Leuski, Panayi-
otis Georgiou, Jillian Gerten, Bilyana Martinovski,
Shrikanth Narayanan, Susan Robinson, and Ashish
Vaswani. 2007. Hassan: A virtual human for tac-
tical questioning. In Proceedings of the 8th SIGdial
Workshop on Discourse and Dialogue, page 71?74,
September.
David Traum, Stacy C. Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal vir-
tual agents. In Proceedings of the 8th international
conference on Intelligent Virtual Agents, IVA ?08,
pages 117?130, Berlin, Heidelberg. Springer-Verlag.
Marilyn A. Walker, Donald Hindle, Jeanne Fromer,
Giuseppe Di Fabbrizio, and Craig Mestel. 1997. Eval-
uating competing agent strategies for a voice email
agent. CoRR, cmp-lg/9706019.
18
A. Example of a dialog with the hybrid system.
Translation into English of a dialog between Samir
(the system) and Julie (the player) (Strategy 2), it
corresponds to the third dialog in Table 1
Samir: Hello my name is Samir, the product designer
Samir: What are you doing here young people?
Julie: we come to build the joystick of Professor Geekman
Samir: You are in the right place. The plans are in the closet
Samir: Before would you like to hear about my job, the studies
I did or my colleagues
Julie: Tell me about your job.
Samir: Well, for instance, is me who devise and design the
items we construct in this enterprise
Samir: Do you want to know more about my colleagues or
about the security measures applied here?
Julie: What do your colleagues do?
Samir: My job is universal! In fact it can please both girls that
boys...
Samir: In your opinion, what studies should be done to get
there
Julie: Designing studies?
Samir: This is a creative job. Several training can lead to the
job arts, industrial design, computer-aided design
Samir: And does the safety standards interest you?
Julie: That is important the safety yes, tell me more
Samir: You know, here you are in a strategic place.
The novelties, the products of tomorrow, is here that they are
born. They must be protected. For that reason we control the
entrance here.
Samir: Well, Good Luck. Bye!
B. Translation to English of a dialog with the
QA system. Dialog between the raw material opera-
tor (the system) and Julie (the player), it corresponds
to the 8th dialog in Table 1(Strategy 4).
Preparateur1: Hello I am the raw material responsible. I can
provide you a lot of information about this place.
Preparateur1: What can I do for you ?
Julie: What do you do here?
Preparateur1: My role is to ensure machines are supplied
with the right materials,in good colors. I turn on the drying
ovens and the whole supply system.
Julie: How does it work the supply system?
Preparateur1: Well when we get the pellet material, they are
wet ...
Julie: What does it happen when there is a problem?
Preparateur1: In that case, I call the technician, he is the best
for fixing everything
Julie:Thanks for the information, what should I do now?
Preparateur1: It is not complicated and you will learn things.
I have a new machine to supply but I can not find the right
material. Would you give me a hand?
Julie: yes
Preparateur1: You must supply this machine with a new
generation of plastic. This machine allows the manufacture of
plastic bags
Julie: ok
Preparateur1: Let?s go!
Julie: Ok, Let?s start!
Preparateur1: Great, Thanks!
Preparateur1: You are very kind, thank you.
19
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 147?156,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Generating Grammar Exercises
Laura Perez-Beltrachini
Universite? de Lorraine
LORIA, UMR 7503
Vandoeuvre-le`s-Nancy
F-54500, France
laura.perez@loria.fr
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy
F-54500, France
claire.gardent@loria.fr
German Kruszewski
Inria, LORIA, UMR 7503
Villers-le`s-Nancy
F-54600, France
german.kruszewski@inria.fr
Abstract
Grammar exercises for language learning fall
into two distinct classes: those that are based
on ?real life sentences? extracted from exist-
ing documents or from the web; and those that
seek to facilitate language acquisition by pre-
senting the learner with exercises whose syn-
tax is as simple as possible and whose vo-
cabulary is restricted to that contained in the
textbook being used. In this paper, we in-
troduce a framework (called GramEx) which
permits generating the second type of gram-
mar exercises. Using generation techniques,
we show that a grammar can be used to
semi-automatically generate grammar exer-
cises which target a specific learning goal; are
made of short, simple sentences; and whose
vocabulary is restricted to that used in a given
textbook.
1 Introduction
Textbooks for language learning generally include
grammar exercises. Tex?s French Grammar 1 for in-
stance, includes at the end of each lecture, a set of
grammar exercises which target a specific pedagog-
ical goal such as learning the plural form of nouns
1Tex?s French Grammar http://www.laits.
utexas.edu/tex/ is an online pedagogical reference
grammar that combines explanations with surreal dialogues
and cartoon images. Tex?s French Grammar is arranged like
many other traditional reference grammars with the parts of
speech (nouns, verbs, etc.) used to categorize specific grammar
items (gender of nouns, irregular verbs). Individual grammar
items are carefully explained in English, then exemplified in a
dialogue, and finally tested in self-correcting, fill-in-the-blank
exercises.
or learning the placement of adjectives. Figure 1
shows the exercises provided by this book at the end
of the lecture on the plural formation of nouns. As
exemplified in this figure, these exercises markedly
differ from more advanced learning activities which
seek to familiarise the learner with ?real world sen-
tences?. To support in situ learning, this latter type
of activity presents the learner with sentences drawn
from the Web or from existing documents thereby
exposing her to a potentially complex syntax and to
a diverse vocabulary. In contrast, textbook grammar
exercises usually aim to facilitate the acquisition of
a specific grammar point by presenting the learner
with exercises made up of short sentences involving
a restricted vocabulary.
As shall be discussed in the next section, most ex-
isting work on the generation of grammar exercises
has concentrated on the automatic creation of the
first type of exercises i.e., exercises whose source
sentences are extracted from an existing corpus. In
this paper, we present a framework (called GramEx)
which addresses the generation of the second type of
grammar exercises used for language learning i.e.,
grammar exercises whose syntax and lexicon are
strongly controlled. Our approach uses generation
techniques to produce these exercises from an exist-
ing grammar describing both the syntax and the se-
mantics of natural language sentences. Given a ped-
agogical goal for which exercises must be produced,
the GramEx framework permits producing Fill in the
blank (FIB, the learner must fill a blank with an ap-
propriate form or phrase) and Shuffle (given a set of
lemmas or forms, the learner must use these to pro-
duce a phrase) exercises that target that specific goal.
147
Give the plural form of the noun indicated in parentheses.
Pay attention to both the article and the noun.
1. Bette aime _____ . (le bijoux)
2. Fiona aime ______ . (le cheval)
3. Joe-Bob aime ______ ame?ricaines. (la bie`re)
4. Tex n?aime pas ______ . (le choix)
5. Joe-Bob n?aime pas ______ difficiles. (le cours)
6. Tammy n?aime pas ______ . (l?ho?pital)
7. Eduard aime ______. (le tableau)
8. Bette aime ______ de Tex. (l?oeil)
9. Tex aime ______ franc?ais. (le poe`te)
10. Corey aime ______ fra??ches. (la boisson)
11. Tammy aime ______ ame?ricains. (le campus)
12. Corey n?aime pas ______ . (l?examen)
Figure 1: Grammar exercises from the Tex?s French Grammar textbook
The exercises thus generated use a simple syntax and
vocabulary similar to that used in the Tex?s French
Grammar textbook.
We evaluate the approach on several dimensions
using quantitative and qualitative metrics as well as a
small scale user-based evaluation. And we show that
the GramEx framework permits producing exercises
for a given pedagogical goal that are linguistically
and pedagogically varied.
The paper is structured as follows. We start by
discussing related work (Section 2). In Section 3,
we present the framework we developed to generate
grammar exercises. Section 4 describes the exper-
imental setup we used to generate exercise items.
Section 5 reports on an evaluation of the exercise
items produced and on the results obtained. Section
6 concludes.
2 Related Work
A prominent strand of research in Computer Aided
Language Learning (CALL) addresses the automa-
tion of exercise specifications relying on Natural
Language Processing (NLP) techniques (Mitkov et
al., 2006; Heilman and Eskenazi, 2007; Karama-
nis et al, 2006; Chao-Lin et al, 2005; Coniam,
1997; Sumita et al, 2005; Simon Smith, 2010; Lin
et al, 2007; Lee and Seneff, 2007). Mostly, this
work targets the automatic generation of so-called
objective test items i.e., test items such as multiple
choice questions, fill in the blank and cloze exercise
items, whose answer is strongly constrained and can
therefore be predicted and checked with high accu-
racy. These approaches use large corpora and ma-
chine learning techniques to automatically generate
the stems (exercise sentences), the keys (correct an-
swers) and the distractors (incorrect answers) that
are required by such test items.
Among these approaches, some proposals target
grammar exercises. Thus, (Chen et al, 2006) de-
scribes a system called FAST which supports the
semi-automatic generation of Multiple-Choice and
Error Detection exercises while (Aldabe et al, 2006)
presents the ArikiTurri automatic question genera-
tor for constructing Fill-in-the-Blank, Word Forma-
tion, Multiple Choice and Error Detection exercises.
These approaches are similar to the approach we
propose. First, a bank of sentences is built which are
automatically annotated with syntactic and morpho-
syntactic information. Second, sentences are re-
trieved from this bank based on their annotation and
on the linguistic phenomena the exercise is meant to
illustrate. Third, the exercise question is constructed
from the retrieved sentences. There are important
differences however.
First, in these approaches, the source sentences
used for building the test items are selected from
corpora. As a result, they can be very complex
and most of the generated test items are targeted
for intermediate or advanced learners. In addition,
some of the linguistic phenomena included in the
language schools curricula may be absent or insuf-
ficiently present in the source corpus (Aldabe et al,
2006). In contrast, our generation based approach
permits controlling both the syntax and the lexicon
of the generated exercices.
Second, while, in these approaches, the syntactic
and morpho-syntactic annotations associated with
the bank sentences are obtained using part-of-speech
tagging and chunking, in our approach, these are
obtained by a grammar-based generation process.
148
As we shall see below, the information thus asso-
ciated with sentences is richer than that obtained by
chunking. In particular, it contains detailed linguis-
tic information about the syntactic constructs (e.g.,
cleft subject) contained in the bank sentences. This
permits a larger coverage of the linguistic phenom-
ena that can be handled. For instance, we can re-
trieve sentences which contain a relativised cleft ob-
ject (e.g., This is the man whom Mary likes who
sleeps) by simply stipulating that the retrieved sen-
tences must be associated with the information Cleft
Object).
To sum up, our approach differs from most exist-
ing work in that it targets the production of syntac-
tically and lexically controlled grammar exercises
rather than producing grammar exercises based on
sentences extracted from an existing corpus.
3 Generating Exercises
Given a pedagogical goal (e.g., learning adjective
morphology), GramEx produces a set of exercise
items for practicing that goal. The item can be ei-
ther a FIB or a shuffle item; and GramEx produces
both the exercise question and the expected solution.
To generate exercise items, GramEx proceeds in
three main steps as follows. First, a generation
bank is constructed using surface realisation tech-
niques. This generation bank stores sentences that
have been generated together with the detailed lin-
guistic information associated by the generation al-
gorithm with each of these sentences. Next, sen-
tences that permit exercising the given pedagogical
goal are retrieved from the generation bank using a
constraint language that permits defining pedagog-
ical goals in terms of the linguistic properties as-
sociated by the generator with the generated sen-
tences. Finally, exercises are constructed from the
retrieved sentences using each retrieved sentence to
define FIB and Shuffle exercises; and the sentence
itself as the solution to the exercise.
We now discuss each of these steps in more detail.
3.1 Constructing a Generation bank
The generation bank is a database associating sen-
tences with a representation of their semantic con-
tent and a detailed description of their syntactic and
morphosyntactic properties. In other words, a gen-
Sentence realisation:
?Tammy a une voix douce?
Lemma-features pairs:
{?lemma?: ?Tammy?,
?lemma-features?: {anim:+,num:sg,det: +,wh:-,cat:n,
func:suj,xp: +, gen:f},
?trace?: {propername}},
{?lemma?: ?avoir?,
?lemma-features?: {aux-refl:-,inv:-,cat:v,pers:3,pron:-,
num:sg,mode:ind, aspect:indet,tense:pres,stemchange:-,
flexion:irreg},
?trace?: {CanonicalObject,CanonicalSubject,n0Vn1}},
{?lemma?: ?un?,
?lemma-features?: {wh:-,num:sg,mass:-,cat:d,
gen:f,def:+},
?trace?: {determiner}},
{?lemma?: ?voix?,
?lemma-features?: {bar:0,wh:-,cat:n,num:sg,
mass:-,gen:f,flexion:irreg,
?trace?: {noun}},
{?lemma?: ?doux?,
?lemma-features?: {num:sg,gen:f,flexion:irreg,cat:adj},
?trace?: {Epith,EpithPost}}
Figure 2: Morphosyntactic information associated by
GraDe with the sentence Tammy a un voix douce
eration bank is a set of (Si, Li, ?i) tuples where Si is
a sentence, Li is a set of linguistic properties true of
that sentence and ?i is its semantic representation.
To produce these tuples, we use the GraDe gram-
mar traversal algorithm described in (Gardent and
Kruszewski, 2012). Given a grammar and a set
of user-defined constraints, this algorithm gener-
ates sentences licensed by this grammar. The user-
defined constraints are either parameters designed to
constrain the search space and guarantee termina-
tion (e.g., upper-bound on the number and type of
recursive rules used or upper-bound on the depth of
the tree build by GraDe); or linguistic parameters
which permit constraining the output (e.g., by spec-
ifying a core semantics the output must verbalise or
by requiring the main verb to be of a certain type).
Here we use GraDe both to generate from manu-
ally specified semantic input; and from a grammar
(in this case an existing grammar is used and no
manual input need to be specified). As explained
in (Gardent and Kruszewski, 2012), when generat-
ing from a semantic representation, the output sen-
tences are constrained to verbalise that semantics but
the input semantics may be underspecified thereby
allowing for morpho-syntactic, syntactic and tem-
poral variants to be produced from a single se-
mantics. For instance, given the input semantics
149
L1:named(J bette n) A:le d(C RH SH) B:bijou n(C)
G:aimer v(E J C), GraDe will output among others
the following variants:
Bette aime le bijou (Bette likes the jewel),
Bette aime les bijoux (Bette likes the jewels),
C?est Bette qui aime le bijou (It is Bette who
likes the jewel), C?est Bette qui aime les bijoux
(It is Bette who likes the jewel), Bette aimait le
bijou (Bette liked the jewel), Bette aimait les
bijoux (Bette liked the jewels), ...
When generating from the grammar, the output
is even less constrained since all derivations com-
patible with the user-defined constraints will be pro-
duced irrespective of semantic content. For instance,
when setting GraDe with constraints restricting the
grammar traversal to only derive basic clauses con-
taining an intransitive verb, the output sentences in-
clude among others the following sentences:
Elle chante (She sings), La tatou chante-t?elle?
(Does the armadillo sing?), La tatou chante
(The armadillo sings), Chacun chante -t?il
(Does everyone sing? ), Chacun chante (Ev-
eryone sings), Quand chante la tatou? (When
does the armadillo sing?), ...
Figure 2 shows the linguistic properties associ-
ated with the sentence Tammy a une voix douce
(Tammy has a soft voice) by GraDe. To gener-
ate exercises, GramEx makes use of the morpho-
syntactic information associated with each lemma
i.e., the feature-value pairs occurring as values of the
lemma-features fields; and of their linguistic proper-
ties i.e., the items occurring as values of the trace
fields.
3.2 Retrieving Appropriate Sentences
To enable the retrieval of sentences that are appropri-
ate for a given pedagogical goal, we define a query
language on the linguistic properties assigned by
GraDe to sentences. We then express each peda-
gogical goal as a query in that language; and we use
these queries to retrieve from the generation bank
appropriate source sentences. For instance, to re-
trieve a sentence for building a FIB exercise where
the blank is a relative pronoun, we query the gen-
eration bank with the constraint RelativePronoun.
This will return all sentences in the generation bank
whose trace field contains the RelativePronoun
item i.e., all sentences containing a relative pronoun.
We then use this sentence to build both the exercise
question and its solution.
3.2.1 GramEx Query Language
We now define the query language used to retrieve
sentences that are appropriate to build an exercise
for a given pedagogical goal. Let B be a genera-
tion bank and let (Si, Li, ?i) be the tuples stored in
B. Then, a GramEx query q permits retrieving from
B the set of sentences Si ? (Si, Li, ?i) such that
Li satisfies q. In other words, GramEx queries per-
mit retrieving from the generation bank all sentences
whose linguistic properties satisfy those queries.
The syntax of the GramEx query language is as
follows:
BoolExpr ? BoolTerm
BoolTerm ? BoolFactor | BoolTerm ? BoolFactor
BoolFactor ? BoolUnary | BoolFactor ? BoolUnary
BoolUnary ? BoolPrimary | ? BoolPrimary
BoolPrimary ? PrimitiveCond | ( BoolExpr ) | [ BoolExpr ]
PrimitiveCond ? traceItem | feature = value
In words: the GramEx query language permits
defining queries that are arbitrary boolean con-
straints on the linguistic properties associated by
GraDe with each generated sentence. In addi-
tion, complex constraints can be named and reused
(macros); and expressions can be required to hold
on a single lexical item ([ BoolExpr] indicates that
BoolExpr should be satisfied by the linguistic prop-
erties of a single lexical item).
The signature of the language is the set of gram-
matical (traceItem) and morpho-syntactic proper-
ties (feature = value) associated by GraDe with
each generated sentence where traceItem is any
item occurring in the value of a trace field and
feature = value any feature/value pair occurring
in the value of a lemma-features field (cf. Fig-
ure 2). The Table below (Table 1) shows some of the
constraints that can be used to express pedagogical
goals in the GramEx query language.
3.2.2 Query Examples
The GramEx query language allows for very spe-
cific constraints to be expressed thereby providing
fine-grained control over the type of sentences and
therefore over the types of exercises that can be pro-
duced. The following example queries illustrate this.
150
Grammatical Properties (traceItem)
Argument Cleft, CleftSUbj, CleftOBJ, ...,
Realisation InvertedSubj
Questioned, QuSubj, ...
Relativised, RelSubj ...
Pronominalised, ProSubj, ...
Voice Active, Passive, Reflexive
Aux tse, modal, causal
Adjective Predicative,Pre/Post nominal
Adverb Sentential, Verbal
Morpho-Syntactic Properties (feature=value)
Tense present,future,past
Number mass, count, plural, singular
Inflexion reg,irreg
Table 1: Some grammatical and morpho-syntactic prop-
erties that can be used to specify pedagogical goals.
(1) a. EpithAnte
Tex pense que Tammy est une jolie tatou (Tex
thinks that Tammy is a pretty armadillo)
b. [Epith ? flexion: irreg]
Tex et Tammy ont une voix douce (Tex and
Tammy have a soft voice)
c. POBJinf ? CLAUSE
POBJinf ? (DE-OBJinf ? A-OBJinf)
CLAUSE ? Vfin??Mod ? ?CCoord? ?Sub
Tammy refuse de chanter (Tammy refuses to
sing)
Query (1a) shows a query for retrieving sentences
containing prenominal adjectives which uses the
grammatical (traceItem) property EpithAnte associ-
ated with preposed adjectives.
In contrast, Query (1b) uses both grammatical and
morpho-syntactic properties to retrieve sentences
containing a postnominal adjective with irregular in-
flexion. The square brackets in the query force the
conjunctive constraint to be satisfied by a single lex-
ical unit. That is, the query will be satisfied by sen-
tences containing a lexical item that is both a post-
nominal adjective and has irregular inflexion. This
excludes sentences including e.g., a postnominal ad-
jective and a verb with irregular inflexion.
Finally, Query (1c) shows a more complex case
where the pedagogical goal is defined in terms of
predefined macros themselves defined as GramEx
query expressions. The pedagogical goal is de-
fined as a query which retrieves basic clauses
(CLAUSE) containing a prepositional infinitival ob-
ject (POBJinf). A sentence containing a preposi-
tional infinitival object is in turn defined (second
line) as a prepositional object introduced either by
the de or the a` preposition. And a basic clause (3rd
line) is defined as a sentence containing a finite verb
and excluding modifiers, clausal or verb phrase co-
ordination (CCORD) and subordinated clauses2
3.3 Building Exercise Items
In the previous section, we saw the mechanism used
for selecting an appropriate sentence for a given
pedagogical goal. GramEx uses such selected sen-
tences as source or stem sentences to build exercise
items. The exercise question is automatically gen-
erated from the selected sentence based on its asso-
ciated linguistic properties. Currently, GramEx in-
cludes two main types of exercises namely, Fill in
the blank and Shuffle exercises.
FIB questions. FIB questions are built by remov-
ing a word from the target sentence and replacing it
with either: a blank (FIBBLNK), a lemma (FIBLEM)
or a set of features used to help the learner guess
the solution (FIBHINT). For instance, in an exercise
on pronouns, GramEx will use the gender, number
and person features associated with the pronoun by
the generation process and display them to specify
which pronominal form the learner is expected to
provide. The syntactic representation (cf. Figure 2)
associated by GraDe with the sentence is used to
search for the appropriate key word to be removed.
For instance, if the pedagogical goal is Learn Sub-
ject Pronouns and the sentence retrieved from the
generation bank is that given in (2a), GramEx will
produce the FIBHINT question in (2b) by search-
ing for a lemma with category cl (clitic) and feature
func=subj and using its gender value to provide the
learner with a hint constraining the set of possible
solutions.
(2) a. Elle adore les petits tatous
(She loves small armadillos)
b. ... adore les petits tatous (gender=fem)
Shuffle questions. Similarly to FIB questions,
shuffle exercise items are produced by inspecting
and using the target derivational information. More
specifically, lemmas are retrieved from the list of
2The expressions CCoord and Sub are themselves defined
rather than primitive expressions.
151
lemma-feature pairs. Function words are (option-
ally) deleted. And the remaining lemmas are ?shuf-
fled? (MSHUF). For instance, given the source sen-
tence (2a), the MSHUF question (2b) can be pro-
duced.
(3) a. Tammy adore la petite tatou
a. tatou / adorer / petit / Tammy
Note that in this case, there are several possible
solutions depending on which tense and number is
used by the learner. For such cases, we can either
use hints as shown above to reduce the set of pos-
sible solutions to one; or compare the learner?s an-
swer to the set of output produced by GraDe for the
semantics the sentence was produced from.
4 Experimental Setup
We carried out an experiment designed to assess the
exercises produced by GramEx. In what follows, we
describe the parameters of this experiment namely,
the grammar and lexicons used; the input and the
user-defined parameters constraining sentence gen-
eration; and the pedagogical goals being tested.
4.1 Grammar and Lexicon
The grammar used is a Feature-Based Lexicalised
Tree Adjoining Grammar for French augmented
with a unification-based compositional semantics.
This grammar contains around 1300 elementary
trees and covers auxiliaries, copula, raising and
small clause constructions, relative clauses, infini-
tives, gerunds, passives, adjuncts, wh-clefts, PRO
constructions, imperatives and 15 distinct subcate-
gorisation frames.
The syntactic and morpho-syntactic lexicons used
for generating were derived from various existing
lexicons, converted to fit the format expected by
GraDe and tailored to cover basic vocabulary as de-
fined by the lexicon used in Tex?s French Grammar.
The syntactic lexicon contains 690 lemmas and the
morphological lexicon 5294 forms.
4.2 Pedagogical Goals
We evaluate the approach on 16 pedagogical goals
taken from the Tex?s French Grammar book. For
each of these goals, we define the corresponding
linguistic characterization in the form of a GramEx
query. We then evaluate the exercises produced by
the system for each of these queries. The pedagog-
ical goals tested are the following (we indicate in
brackets the types of learning activity produced for
each teaching goal by the system):
? Adjectives: Adjective Order (MSHUF), Adjec-
tive Agreement (FIBLEM), Prenominal adjec-
tives (FIBLEM), Present and Past Participial
used as adjectives (FIBLEM), Regular and Ir-
regular Inflexion (FIBLEM), Predicative adjec-
tives (MSHUF)
? Prepositions: Prepositional Infinitival Object
(FIBBLNK), Modifier and Complement Prepo-
sitional Phrases (FIBBLNK)
? Noun: Gender (FIBLEM), Plural form (FI-
BLEM), Subject Pronoun (FIBHINT).
? Verbs: Pronominals (FIBLEM), -ir Verbs in
the present tense (FIBLEM), Simple past (FI-
BLEM), Simple future (FIBLEM), Subjunctive
Mode (FIBLEM).
4.3 GraDe?s Input and User-Defined
Parameters
GraDe?s configuration As mentioned in Sec-
tion 3, we run GraDe using two main configura-
tions. In the first configuration, GraDe search is con-
strained by an input core semantics which guides the
grammar traversal and forces the output sentence to
verbalise this core semantics. In this configuration,
GraDe will only produce the temporal variations
supported by the lexicon (the generated sentences
may be in any simple tense i.e., present, future,
simple past and imperfect) and the syntactic varia-
tions supported by the grammar for the same MRSs
(e.g., active/passive voice alternation and cleft argu-
ments).
Greater productivity (i.e., a larger output/input ra-
tio) can be achieved by providing GraDe with less
constrained input. Thus, in the second configura-
tion, we run GraDe not on core semantics but on the
full grammar. To constrain the search, we specify a
root constraint which requires that the main verb of
all output sentences is an intransitive verb. We also
set the constraints on recursive rules so as to exclude
the inclusion of modifiers. In sum, we ask GraDe to
produce all clauses (i) licensed by the grammar and
the lexicon; (ii) whose verb is intransitive; and (iii)
152
which do not include modifiers. Since the number
of sentences that can be produced under this con-
figuration is very large, we restrict the experiment
by using a lexicon containing a single intransitive
verb (chanter/To sing), a single common noun and a
single proper name. In this way, syntactically struc-
turally equivalent but lexically distinct variants are
excluded.
Input Semantics We use two different sets of in-
put semantics for the semantically guided configura-
tion: one designed to test the pedagogical coverage
of the system (Given a set of pedagogical goals, can
GramEx generate exercises that appropriately target
those goals?); and the other to illustrate linguistic
coverage (How much syntactic variety can the sys-
tem provide for a given pedagogical goal?).
The first set (D1) of semantic representations con-
tains 9 items representing the meaning of exam-
ple sentences taken from the Tex?s French Gram-
mar textbook. For instance, for the first item
in Figure 1, we use the semantic representation
L1:named(J bette n) A:le d(C RH SH) B:bijou n(C)
G:aimer v(E J C). With this first set of input seman-
tics, we test whether GramEx correctly produces the
exercises proposed in the Tex?s French Grammar
book. Each of the 9 input semantics corresponds to
a distinct pedagogical goal.
The second set (D2) of semantic representations
contains 22 semantics, each of them illustrating dis-
tinct syntactic configurations namely, intransitive,
transitive and ditransitive verbs; raising and control;
prepositional complements and modifiers; sentential
and prepositional subject and object complements;
pronominal verbs; predicative, attributive and par-
ticipial adjectives. With this set of semantics, we
introduce linguistically distinct material thereby in-
creasing the variability of the exercises i.e., making
it possible to have several distinct syntactic configu-
rations for the same pedagogical goal.
5 Evaluation, Results and Discussion
Using the experimental setup described in the previ-
ous section, we evaluate GramEx on the following
points:
? Correctness: Are the exercises produced by the
generator grammatical, meaningful and appro-
priate for the pedagogical goal they are associ-
ated with?
? Variability: Are the exercises produced linguis-
tically varied and extensive? That is, do the ex-
ercises for a given pedagogical goal instantiate
a large number of distinct syntactic patterns?
? Productivity: How much does GramEx support
the production, from a restricted number of se-
mantic input, of a large number of exercises?
Correctness To assess correctness, we randomly
selected 10 (pedagogical goal, exercise) pairs for
each pedagogical goal in Section 4.2 and asked two
evaluators to say for each pair whether the exer-
cise text and solutions were grammatical, meaning-
ful (i.e., semantically correct) and whether the ex-
ercise was adequate for the pedagogical goal. The
results are shown in Table 3 and show that the sys-
tem although not perfect is reliable. Most sources of
grammatical errors are cases where a missing word
in the lexicon fails to be inflected by the generator.
Cases where the exercise is not judged meaningful
are generally cases where a given syntactic construc-
tion seems odd for a given semantics content. For
instance, the sentence C?est Bette qui aime les bi-
joux (It is Bette who likes jewels) is fine but C?est
Bette qui aime des bijoux although not ungrammati-
cal sounds odd. Finally, cases judged inappropriate
are generally due to an incorrect feature being as-
signed to a lemma. For instance, avoir (To have) is
marked as an -ir verb in the lexicon which is incor-
rect.
Grammatical Meaningful Appropriate
91% 96% 92%
Table 3: Exercise Correctness tested on 10 randomly se-
lected (pedagogical goal, exercise pairs)
We also asked a language teacher to examine 70
exercises (randomly selected in equal number across
the different pedagogical goals) and give her judg-
ment on the following three questions:
? A. Do you think that the source sentence se-
lected for the exercise is appropriate to practice
the topic of the exercise? Score from 0 to 3 ac-
cording to the degree (0 inappropriate - 3 per-
fectly appropriate)
153
Nb. Ex. 1 2 4 5 6 12 17 18 20 21 23 26 31 37 138
Nb. Sem 1 4 6 1 4 3 1 1 1 1 1 1 1 1 1
Table 2: Exercise Productivity: Number of exercises produced per input semantics
? B. The grammar topic at hand together with
the complexity of the source sentence make
the item appropriate for which language level?
A1,A2,B1,B2,C13
? C. Utility of the exercise item: ambiguous (not
enough context information to solve it) / correct
For Question 1, the teacher graded 35 exercises as
3, 20 as 2 and 14 as 1 pointing to similar problems
as was independently noted by the annotators above.
For question B, she marked 29 exercises as A1/A2,
24 as A2, 14 as A2/B1 and 3 as A1 suggesting that
the exercises produced are non trivial. Finally, she
found that 5 out of the 70 exercises lacked context
and were ambiguously phrased.
Variability For any given pedagogical goal, there
usually are many syntactic patterns supporting learn-
ing. For instance, learning the gender of common
nouns can be practiced in almost any syntactic con-
figuration containing a common noun. We assess the
variability of the exercises produced for a given ped-
agogical goal by computing the number of distinct
morpho-syntactic configurations produced from a
given input semantics for a given pedagogical goal.
We count as distinct all exercise questions that are
derived from the same semantics but differ either
in syntax (e.g., passive/active distinction) or in mor-
phosyntax (determiner, number, etc.). Both types of
differences need to be learned and therefore produc-
ing exercises which, for a given pedagogical goal,
expose the learner to different syntactic and morpho-
syntactic patterns (all involving the construct to be
learned) is effective in supporting learning. How-
ever we did not take into account tense differences
as the impact of tense on the number of exercises
produced is shown by the experiment where we gen-
erate by traversing the grammar rather than from a
3A1, A2, B1, B2 and C1 are reference levels established
by the Common European Framework of Reference for
Languages: Learning, Teaching, Assessment (cf. http:
//en.wikipedia.org/wiki/Common_European_
Framework_of_Reference_for_Languages) for
grading an individual?s language proficiency.
semantics. Table 4 shows for each (input semantics,
teaching goal) pair the number of distinct patterns
observed. The number ranges from 1 to 21 distinct
patterns with very few pairs (3) producing a single
pattern, many (33) producing two patterns and a fair
number producing either 14 or 21 patterns.
Nb. PG 1 2 3 4 5 6
Nb. sent 213 25 8 14 10 6
Table 6: Pedagogical Productivity: Number of Teaching
Goals the source sentence produced from a given seman-
tics can be used for
Productivity When used to generate from seman-
tic representations (cf. Section 4.3), GramEx only
partially automates the production of grammar ex-
ercises. Semantic representations must be manually
input to the system for the exercises to be generated.
Therefore the issue arises of how much GramEx
helps automating exercise creation. Table 5 shows
the breakdown of the exercises produced per teach-
ing goal and activity type. In total, GramEx pro-
duced 429 exercises out of 28 core semantics yield-
ing an output/input ratio of 15 (429/28). Further, Ta-
ble 2 and 6 show the distribution of the ratio be-
tween (i) the number of exercises produced and the
number of input semantics and (ii) the number of
teaching goals the source sentences produced from
input semantics i can be used for. Table 6 (peda-
gogical productivity) shows that, in this first exper-
iment, a given input semantics can provide material
for exercises targeting up to 6 different pedagogi-
cal goals while Table 2 (exercise productivity) shows
that most of the input semantics produce between 2
and 12 exercises4.
When generating by grammar traversal, under the
constraints described in Section 4, from one input
4If the input semantics contains a noun predicate whose gen-
der is underspecified, the exercise productivity could be dou-
bled. This is the case for 4 of the input semantics in the dataset
D2; i.e. an input semantics containing the predicates tatou n(C)
petit a(C) will produce variations such as: la petite tatou (the
small armadillo (f)) and le petit tatou (the small armadillo (m)).
154
Nb. SP 1 2 3 4 5 6 7 8 9 10 14 21
(S,G) 3 33 16 7 2 4 6 1 4 1 2 6
Table 4: Variability: Distribution of the number of distinct sentential patterns that can be produced for a given peda-
gogical goal from a given input semantics
Pedagogical Goal FIBLEM FIBBLNK MSHUF FIBHINT
Preposition ? 28 ? ?
Prepositions with infinitives ? 8 ? ?
Subject pronouns?il ? ? ? 3
Noun number 11 ? ? ?
Noun gender ? 49 ? ?
Adjective order ? ? 30 ?
Adjective morphology 30 ? ? ?
Adjectives that precede the noun 24 ? ? ?
Attributive Adjectives ? ? 28 ?
Irregular adjectives 4 ? ? ?
Participles as adjectives 4 ? ? ?
Simple past 78 ? ? ?
Simple future 90 ? ? ?
-ir verbs in present 18 ? ? ?
Subjunctive mode 12 ? ? ?
Pronominal verbs 12 ? ? ?
Total 236 78 30 3
Table 5: Number and Types of Exercises Produced from the 28 input semantics
90 exercises are generated targeting 4 different ped-
agogical goals (i.e. 4 distinct linguistic phenomena).
6 Conclusion
We presented a framework (called GramEx) for gen-
erating grammar exercises which are similar to those
often used in textbooks for second language learn-
ing. These exercises target a specific learning goal;
and, they involve short sentences that make it eas-
ier for the learner to concentrate on the grammatical
point to be learned.
One distinguishing feature of the approach is the
rich linguistic information associated by the gen-
erator with the source sentences used to construct
grammar exercises. Although space restriction pre-
vented us from showing it here, this information
includes, in addition to the morphosyntactic infor-
mation and the grammatical properties illustrated in
Figure 2 and Table 1 respectively, a semantic rep-
resentation, a derivation tree showing how the parse
tree of each sentence was obtained and optionally,
an underspecified semantics capturing the core pred-
icate/argument and modifier/modifiee relationships
expressed by each sentence. We are currently ex-
ploring how this information could be used to ex-
tend the approach to transformation exercises (e.g.,
passive/active) where the relation between exercise
question and exercise solution is more complex than
in FIB exercises.
Another interesting question which needs further
investigation is how to deal with exercise items that
have multiple solutions such as example (3) above.
Here we plan to use the fact that underspecified se-
mantics in GraDe permits associating many variants
with a given semantics.
Acknowledgments
We would like to thank the language teacher, Tex?s
French Grammar developers, and the anonymous re-
viewers for their useful comments. The research
presented in this paper was partially supported
by the European Fund for Regional Development
within the framework of the INTERREG IV A Alle-
gro Project5.
5http://www.allegro-project.eu/ and http:
//talc.loria.fr/-ALLEGRO-Nancy-.html
155
References
Itziar Aldabe, Maddalen Lopez de Lacalle, Montse Mar-
itxalar, Edurne Martinez, and Larraitz Uria. 2006.
Arikiturri: an automatic question generator based on
corpora and nlp techniques. In Proceedings of the
8th international conference on Intelligent Tutoring
Systems, ITS?06, pages 584?594, Berlin, Heidelberg.
Springer-Verlag.
Liu Chao-Lin, Wang Chun-Hung, Gao Zhao-Ming, and
Huang Shang-Ming. 2005. Applications of lexical
information for algorithmically composing multiple-
choice cloze items. In Proceedings of the second
workshop on Building Educational Applications Us-
ing NLP, EdAppsNLP 05, pages 1?8, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chia-Yin Chen, Hsien-Chin Liou, and Jason S. Chang.
2006. Fast: an automatic generation system for gram-
mar tests. In Proceedings of the COLING/ACL on
Interactive presentation sessions, COLING-ACL ?06,
pages 1?4, Stroudsburg, PA, USA. Association for
Computational Linguistics.
David Coniam. 1997. A preliminary inquiry into using
corpus word frequency data in the automatic genera-
tion of english language cloze tests. CALICO Journal,
14:15?33.
Claire Gardent and German Kruszewski. 2012. Gener-
ation for grammar engineering. In 11th International
Conference on Natural Language Generation (ENLG).
Michael Heilman and Maxine Eskenazi. 2007. Ap-
plication of automatic thesaurus extraction for com-
puter generation of vocabulary questions. In Proceed-
ings of Speech and Language Technology in Education
(SLaTE2007), pages 65?68.
Nikiforos Karamanis, Le An Ha, and Ruslan Mitkov.
2006. Generating multiple-choice test items from
medical text: A pilot study. In Proceedings of the
Fourth International Natural Language Generation
Conference, pages 111?113, Sydney, Australia.
John Lee and Stephanie Seneff. 2007. Automatic gener-
ation of cloze items for prepositions. Proceedings of
Interspeech, pages 2173?2176.
Yi-Chien Lin, Li-Chun Sung, and Meng Chang Chen.
2007. An Automatic Multiple-Choice Question Gen-
eration Scheme for English Adjective Understandings.
In Workshop on Modeling, Management and Gener-
ation of Problems/Questions in eLearning, the 15th
International Conference on Computers in Education
(ICCE 2007), pages pages 137?142.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.
2006. A computer-aided environment for generating
multiple-choice test items. Natural Language Engi-
neering, 12(2):177?194.
Adam Kilgarriff Simon Smith, P.V.S Avinesh. 2010.
Gap-fill Tests for Language Learners: Corpus-Driven
Item Generation. In Proceedings of ICON-2010: 8th
International Conference on Natural Language Pro-
cessing.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring non-native speakers? pro-
ficiency of english by using a test with automatically-
generated fill-in-the-blank questions. In Proceedings
of the second workshop on Building Educational Ap-
plications Using NLP, EdAppsNLP 05, pages 61?68,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
156
Proceedings of the 14th European Workshop on Natural Language Generation, pages 40?50,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Generating Elliptic Coordination
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy, F-54500, France
claire.gardent@loria.fr
Shashi Narayan
Universite? de Lorraine, LORIA, UMR 7503
Villers-le`s-Nancy, F-54600, France
shashi.narayan@loria.fr
Abstract
In this paper, we focus on the task of gen-
erating elliptic sentences. We extract from
the data provided by the Surface Realisa-
tion (SR) Task (Belz et al, 2011) 2398 in-
put whose corresponding output sentence
contain an ellipsis. We show that 9% of the
data contains an ellipsis and that both cov-
erage and BLEU score markedly decrease
for elliptic input (from 82.3% coverage for
non-elliptic sentences to 65.3% for ellip-
tic sentences and from 0.60 BLEU score
to 0.47). We argue that elided material
should be represented using phonetically
empty nodes and we introduce a set of
rewrite rules which permits adding these
empty categories to the SR data. Finally,
we evaluate an existing surface realiser on
the resulting dataset. We show that, after
rewriting, the generator achieves a cover-
age of 76% and a BLEU score of 0.74 on
the elliptical data.
1 Introduction
To a large extent, previous work on generating el-
lipsis has assumed a semantically fully specified
input (Shaw, 1998; Harbusch and Kempen, 2009;
Theune et al, 2006). Given such input, elliptic
sentences are then generated by first producing
full sentences and second, deleting from these sen-
tences substrings that were identified to obey dele-
tion constraints.
In contrast, recent work on generation often as-
sumes input where repeated material has already
been elided. This includes work on sentence com-
pression which regenerates sentences from surface
dependency trees derived from parsing the initial
text (Filippova and Strube, 2008); Surface realisa-
tion approaches which have produced results for
regenerating from the Penn Treebank (Langkilde-
Geary, 2002; Callaway, 2003; Zhong and Stent,
2005; Cahill and Van Genabith, 2006; White and
Rajkumar, 2009); and more recently, the Surface
Realisation (SR) Task (Belz et al, 2011) which
has proposed dependency trees and graphs de-
rived from the Penn Treebank (PTB) as a com-
mon ground input representation for testing and
comparing existing surface realisers. In all these
approaches, repeated material is omitted from the
representation that is input to surface realisation.
As shown in the literature, modelling the inter-
face between the empty phonology and the syn-
tactic structure of ellipses is a difficult task. For
parsing, Sarkar and Joshi (1996), Banik (2004)
and Seddah (2008) propose either to modify the
derivation process of Tree Adjoining Grammar
or to introduce elementary trees anchored with
empty category in a synchronous TAG to accom-
modate elliptic coordinations. In HPSG (Head-
Driven Phrase Structure Grammar), Levy and Pol-
lard (2001) introduce a neutralisation mechanism
to account for unlike constituent coordination ;
in LFG (Lexical Functional Grammar), Dalrym-
ple and Kaplan (2000) employ set values to model
coordination; in CCG (Combinatory Categorial
Grammar, (Steedman, 1996)), it is the non stan-
dard notion of constituency assumed by the ap-
proach which permits accounting for coordinated
structures; finally, in TLCG (Type-Logical Cat-
egorial Grammar), gapping is treated as like-
category constituent coordinations (Kubota and
Levine, 2012).
In this paper, we focus on how surface reali-
sation handles elliptical sentences given an input
where repeated material is omitted. We extract
from the SR data 2398 input whose correspond-
ing output sentence contain an ellipsis. Based on
previous work on how to annotate and to represent
ellipsis, we argue that elided material should be
represented using phonetically empty nodes (Sec-
tion 3) and we introduce a set of rewrite rules
which permits adding these empty categories to
40
the SR data (Section 4). We then evaluate our sur-
face realiser (Narayan and Gardent, 2012b) on the
resulting dataset (Section 5) and we show that, on
this data, the generator achieves a coverage of 76%
and a BLEU score, for the generated sentences, of
0.74. Section 6 discusses related work on generat-
ing elliptic coordination. Section 7 concludes.
2 Elliptic Sentences
Elliptic coordination involves a wide range of phe-
nomena including in particular non-constituent
coordination (1, NCC) i.e., cases where sequences
of constituents are coordinated; gapping (2, G)
i.e., cases where the verb and possibly some
additional material is elided; shared subjects (3,
SS) and right node raising (4, RNR) i.e., cases
where a right most constituent is shared by two or
more clauses1.
(1) [It rose]i 4.8 % in June 1998 and i 4.7% in
June 1999. NCC
(2) Sumitomo bank [donated]i $500, 000, Tokyo
prefecture i $15, 000 and the city of Osaka i
$10, 000 . Gapping
(3) [the state agency ?s figures]i i confirm pre-
vious estimates and i leave the index at 178.9 .
Shared Subject
(4) He commissions i and splendidly interprets
i [fearsome contemporary scores]i . RNR
We refer to the non elliptic clause as the source
and to the elliptic clause as the target. In the
source, the brackets indicate the element shared
with the target while in the target, the i sign in-
dicate the elided material with co-indexing indi-
cating the antecedent/ellipsis relation. In gapping
clauses, we refer to the constituents in the gapped
clause, as remnants.
3 Representing and Annotating Elided
Material
We now briefly review how elided material is rep-
resented in the literature.
Linguistic Approaches. While Sag (1976),
Williams (1977), Kehler (2002), Merchant (2001)
1Other types of elliptic coordination include sluicing and
Verb-Phrase ellipsis. These will not be discussed here be-
cause they can be handled by the generator by having the
appropriate categories in the grammar and the lexicon e.g.,
in a Tree Adjoining Grammar, an auxiliary anchoring a verb
phrase for VP ellipsis and question words anchoring a sen-
tence for sluicing.
and van Craenenbroeck (2010) have argued for
a structural approach i.e., one which posits syn-
tactic structure for the elided material, Keenan
(1971), Hardt (1993), Dalrymple et al (1991),
Ginzburg and Sag (2000) and Culicover and Jack-
endoff (2005) all defend a non structural approach.
Although no consensus has yet been reached on
these questions, many of these approaches do pos-
tulate an abstract syntax for ellipsis. That is they
posit that elided material licenses the introduction
of phonetically empty categories in the syntax or
at some more abstract level (e.g., the logical form
of generative linguistics).
Treebanks. Similarly, in computational linguis-
tics, the treebanks used to train and evaluate
parsers propose different means of representing el-
lipsis.
For phrase structure syntax, the Penn Treebank
Bracketing Guidelines extensively describe how to
annotate coordination and missing material in En-
glish (Bies et al, 1995). For shared complements
(e.g., shared subject and right node raising con-
structions), these guidelines state that the elided
material licenses the introduction of an empty
*RNR* category co-indexed with the shared com-
plement (cf. Figure 1) while gapping construc-
tions are handled by labelling the gapping rem-
nants (i.e., the constituents present in the gapping
clause) with the index of their parallel element in
the source (cf. Figure 2).
(S
(VP (VB Do)(VP (VB avoid)
(S (VP (VPG puncturing(NP *RNR*-5))
(CC or)
(VP (VBG cutting)(PP (IN into)
(NP *RNR*-5)))
(NP-5 meats)))))
Figure 1: Penn Treebank annotation for Right
Node Raising ?Do avoid puncturing i or cutting into i
[meats]i.?
(S
(S (NP-SBJ-10 Mary)
(VP (VBZ likes) (NP-11 potatoes)))
(CC and)
(S (NP-SBJ=10 Bill)
(, ,) (NP=11 ostriches)))
Figure 2: Penn Treebank annotation for gapping
?Mary [likes]i potatoes and Bill i ostriches.?
In dependency treebanks, headless elliptic con-
structs such as gapping additionally raise the is-
41
sue of how to represent the daughters of an empty
head. Three main types of approaches have been
proposed. In dependency treebanks for German
(Daum et al, 2004; Hajic? et al, 2009) and in
the Czech treebank ( ?Cmejrek et al, 2004; Hajic?
et al, 2009), one of the dependents of the head-
less phrase is declared to be the head. This is
a rather undesirable solution because it hides the
fact that there the clause lacks a head. In contrast,
the Hungarian dependency treebank (Vincze et al,
2010) explicitly represents the elided elements in
the trees by introducing phonetically empty ele-
ments that serve as attachment points to other to-
kens. This is the cleanest solution from a linguistic
point of view. Similarly, Seeker and Kuhn (2012)
present a conversion of the German Tiger treebank
which introduces empty nodes for verb ellipses if
a phrase normally headed by a verb is lacking a
head. They compare the performance of two sta-
tistical dependency parsers on the canonical ver-
sion and the CoNLL 2009 Shared Task data and
show that the converted dependency treebank they
propose yields better parsing results than the tree-
bank not containing empty heads.
In sum, while some linguists have argued for an
approach where ellipsis has no syntactic represen-
tation, many have provided strong empirical evi-
dence for positing empty nodes as place-holders
for elliptic material. Similarly, in devising tree-
banks, computational linguists have oscillated be-
tween representations with and without empty cat-
egories. In the following section, we present the
way in which elided material is represented in the
SR data; we show that it underspecifies the sen-
tences to be generated; and we propose to mod-
ify the SR representations by making the relation-
ship between ellipsis and antecedent explicit using
phonetically empty categories and co-indexing.
4 Rewriting the SR Data
The SR Task 2011 made available two types of
data for surface realisers to be tested on: shallow
dependency trees and deep dependency graphs.
Here we focus on the shallow dependency trees
i.e., on syntactic structures.
The input data provided by the SR Task were
obtained from the Penn Treebank. They were
derived indirectly from the LTH Constituent-to-
Dependency Conversion Tool for Penn-style Tree-
banks (Pennconverter, (Johansson and Nugues,
2007)) by post-processing the CoNLL data to re-
d o n a t e
Sumitomo bank
SBJ
a n d
COORD
$ 5 0 0 , 0 0 0
OBJ
Tokyo prefecture
GAP-SBJ
$ 1 5 , 0 0 0
GAP-OBJ
Figure 3: Gapping in the SR data. ?Sumitomo bank
[donated]i $500, 000 and Tokyo prefecture i $15, 000.?
move word order, inflections etc. It consists of
a set of unordered labelled syntactic dependency
trees whose nodes are labelled with word forms,
part of speech categories, partial morphosyntac-
tic information such as tense and number and, in
some cases, a sense tag identifier. The edges are
labelled with the syntactic labels provided by the
Pennconverter. All words (including punctuation)
of the original sentence are represented by a node
in the tree. Figures 3, 4, 5 and 6 show (simplified)
input trees from the SR data.
In the SR data, the representation of ellipsis
adopted in the Penn Treebank is preserved modulo
some important differences regarding co-indexing.
Gapping is represented as in the PTB by la-
belling the remnants with a marker indicating the
source element parallel to each remnant. However
while in the PTB, this parallelism is made explicit
by co-indexing (the source element is marked with
an index i and its parallel target element with the
marker = i), in the SR data this parallelism is ap-
proximated using functions. For instance, if the
remnant is parallel to the source subject, it will be
labelled GAP-SBJ (cf. Figure 3).
commission
He
SBJ
a n d
COORD
fearsome contemporary  score
OBJ
splendidly interpret
CONJ
Figure 4: Subject Sharing and RNR in the SR
data. ?[He]j j commissions i and j splendidly interprets
i [fearsome contemporary scores]i .?
For right-node raising and shared subjects, the
coindexation present in the PTB is dropped in the
SR data. As a result, the SR representation under-
42
b e
They
SBJ
s h o w
VC
n o t
ADV
t a k e
OPRD
James Madison
OBJ
or
COORD
a puff
OBJ
light up
CONJ
Figure 5: Non shared Object ?They aren?t showing
James Madison taking a puff or lighting up?
rise
It
SBJ
a n d
COORD
4.8  %
EXT
in June 1998
TMP
4.7  %
GAP-EXT
in June 1999
GAP-TMP
Figure 6: NCC in the SR data. ?It rose 4.8 % in June
1998 and 4.7% in June 1999.?
V V
FUNC COORD FUNC COORD
X CONJ W X CONJ W
GAP-FUNC ? CONJ
Y V
FUNC
Y W
Figure 7: Gapping and Non Constituent Coordina-
tion structures rewriting (V: a verb, CONJ: a con-
junctive coordination, X, Y and W three sets of de-
pendents). The antecedent verb (V) and the source
material without counterpart in the gapping clause
(W) are copied over to the gapping clause and
marked as phonetically empty.
specifies the relation between the object and the
coordinated verbs in RNR constructions: the ob-
ject could be shared as in He commissions i and splen-
didly interprets i [fearsome contemporary scores]i . (Fig-
ure 4) or not as in They aren?t showing James Madison
taking a puff or lighting up (Figure 5). In both cases,
the representation is the same i.e., the shared ob-
ject (fearsome contemporary scores) and the unshared
object (a puff ) are both attached to the first verb.
Finally, NCC structures are handled in the same
way as gapping by having the gapping remnants
labelled with a GAP prefixed function (e.g., GAP-
SBJ) indicating which element in the source the
gapping remnant is parallel to (cf. Figure 6).
Summing up, the SR representation schema un-
derspecifies ellipsis in two ways. For gapping and
non-constituent coordination, it describes paral-
lelism between source and target elements rather
than specifying the syntax of the elided material.
For subject sharing and right node raising, it fails
to explicitly specify argument sharing.
V1 V1
SUBJ COORD SUBJ COORD
X CONJ Y1 X CONJ Y1
CONJ ? CONJ
V2 V2
SUBJ
Y2 X Y2
Figure 8: Subject sharing: the subject dependent
is copied over to the target clause and marked as
phonetically empty.
To resolve this underspecification, we rewrite
the SR data using tree rewrite rules as follows.
In Gapping and NCC structures, we copy the
source material that has no (GAP- marked) coun-
terpart in the target clause to the target clause
marking it to indicate a phonetically empty cate-
gory (cf. Figure 7).
For Subject sharing, we copy the shared subject
of the source clause in the target clause and mark it
to be a phonetically empty category (cf. Figure 8).
For Right-Node-Raising, we unfold the am-
biguity producing structures where arguments
present in the source but not in the target are op-
tionally copied over to the target (cf. Figure 9).
These rewrite rules are implemented efficiently
43
V1 V1 V1
COORD OBJ COORD OBJ COORD OBJ
X1 CONJ Y X1 CONJ Y X1 CONJ Y
CONJ ?
{
CONJ , CONJ
}
V2 V2 V2
OBJ
X2 X2 X2 Y
Figure 9: Right-Node-Raising: the object dependent is optionally copied over to the target clause and
marked as phonetically empty in the source clause.
using GrGen, an efficient graph rewriting sys-
tem (Gei?et al, 2006).
5 Generating Elliptic Coordination
5.1 The Surface Realiser
To generate sentences from the SR data, we
use our surface realiser (Narayan and Gardent,
2012b), a grammar-based generator based on a
Feature-Based Lexicalised Tree Adjoining Gram-
mar (FB-LTAG) for English. This generator first
selects the elementary FB-LTAG trees associated
in the lexicon with the lemmas and part of speech
tags associated with each node in the input de-
pendency tree. It then attempts to combine the
selected trees bottom-up taking into account the
structure of the input tree (only trees that are se-
lected by nodes belonging to the same local input
tree are tried for combination). A language model
is used to implement a beam search letting through
only the n most likely phrases at each bottom up
combination step. In this experiment, we set n to
5. The generator thus outputs at most 5 sentences
for each input.
Figure 10: Gapping after rewriting ?Sumitomo bank
[donated]i $500, 000 and Tokyo prefecture i $15, 000.?
As mentioned in the introduction, most compu-
tational grammars have difficulty accounting for
ellipses and FB-LTAG is no exception.
The difficulty stems from the fact that in ellip-
tical sentences, there is meaning without sound.
As a result, the usual form/meaning mappings that
in non-elliptic sentences allow us to map sounds
onto their corresponding meanings, break down.
For instance, in the sentence John eats apples
and Mary pear, the Subject-Verb-Object structure
which can be used in English to express a binary
relation is present in the source clause but not in
the elided one. In practice, the syntax of ellipti-
cal sentences often leads to a duplication of the
grammatical system, one system allowing for non
elliptical sentences and the other for their elided
counterpart.
For parsing with TAG, two main methods have
been proposed for processing elliptical sentences.
(Sarkar and Joshi, 1996) introduces an additional
operation for combining TAG trees which yields
derivation graphs rather than trees. (Seddah, 2008)
uses Multi-Component TAG and proposes to asso-
ciate each elementary verb tree with an elliptic tree
with different pairs representing different types of
ellipses.
We could use either of these approaches for
generation. The first approach however has the
drawback that it leads to a non standard notion of
derivation (the derivation trees become derivation
graphs). The second on the other hand, induces a
proliferation of trees in the grammar and impacts
efficiency.
Instead, we show that, given an input enriched
with empty categories as proposed in the previous
section, neither the grammar nor the tree combi-
nation operation need changing. Indeed, our FB-
LTAG surface realiser directly supports the gen-
eration of elliptic sentences. It suffices to as-
sume that an FB-LTAG elementary tree may be an-
chored by the empty string. Given an input node
marked as phonetically empty, the generator will
44
then select all FB-LTAG rules that are compatible
with the lexical and the morpho-syntactic features
labelling that node. Generation will then proceed
as usual by composing the trees selected on the ba-
sis of the input using substitution and adjunction;
and by retrieving from the generation forest those
sentences whose phrase structure tree covers the
input.
For instance, given the rewritten input shown in
Figure 10, the TAG trees associated in the lexi-
con with donate will be selected; anchored with
the empty string and combined with the TAG trees
built for Tokyo Prefecture and $15,000 thus yield-
ing the derivation shown in Figure 11.
NP
Tokyo prefecture
S
NP? VP
V NP?

NP
$15,000
Figure 11: Derivation for ?Tokyo prefecture  $15,000?
5.2 The Data
We use both the SR test data (2398 sentences) and
the SR training data (26604 sentences) to evaluate
the performance of the surface realiser on elliptic
coordination. Since the realiser we are using is
not trained on this data (the grammar was written
manually), this does not bias evaluation. Using
the training data allows us to gather a larger set of
elliptic sentences for evaluation while evaluating
also on the test data allows comparison with other
realisers.
To focus on ellipses, we retrieve those sentences
which were identified by our rewrite rules as po-
tentially containing an elliptic coordination. In
essence, these rewrite rules will identify all cases
of non-constituent coordination and gapping (be-
cause these involve GAP-X dependencies with ?X?
a dependency relation and are therefore easily de-
tected) and of shared-subjects (because the tree
patterns used to detect are unambiguous i.e., only
apply if there is indeed a shared subject). For
RNR, as discussed in the previous section, the SR
format is ambiguous and consequently, the rewrite
rules might identify as object sharing cases where
in fact the object is not shared. As noted by one
of our reviewers, the false interpretation could be
Elliptic Coordination Data
Elliptic Coordination Pass BLEU ScoresCOV ALL
RNR (384)
Before 66% 0.68 0.45
After 81% 0.70 0.57
Delta +15 +0.02 +0.12
SS (1462)
Before 70% 0.74 0.52
After 75% 0.75 0.56
Delta +5 +0.01 +0.04
SS + RNR
(456)
Before 61% 0.71 0.43
After 74% 0.73 0.54
Delta +13 +0.02 +0.11
Gapping (36)
Before 3% 0.53 0.01
After 67% 0.74 0.49
Delta +64 +0.21 +0.48
NCC (60)
Before 5% 0.68 0.03
After 73% 0.74 0.54
Delta +68 +0.06 +0.51
Total (2398)
Before 65% 0.72 0.47
After 76% 0.74 0.56
Delta +11 +0.02 +0.09
Table 1: Generation results on elliptical data be-
fore and after input rewriting (SS: Shared Subject,
NCC: Non Constituent Coordination, RNR: Right
Node Raising). The number in brackets in the first
column is the number of cases. Pass stands for
the coverage of the generator. COV and ALL in
BLEU scores column stand for BLEU scores for
the covered and the total input data.
dropped out by consulting the Penn Treebank2.
The approach would not generalise to other data
however.
In total, we retrieve 2398 sentences3 potentially
containing an elliptic coordination from the SR
training data. The number and distribution of these
sentences in terms of ellipsis types are given in Ta-
ble 1. From the test data, we retrieve an additional
182 elliptic sentences.
5.3 Evaluation
We ran the surface realiser on the SR input data
both before and after rewriting elliptic coordina-
tions; on the sentences estimated to contain ellip-
sis; on sentences devoid of ellipsis; and on all sen-
tences. The results are shown in Table 2. They
indicate coverage and BLEU score before and af-
ter rewriting. BLEU score is given both with re-
spect to covered sentences (COV) i.e., the set of
input for which generation succeeds; and for all
sentences (ALL). We evaluate both with respect to
the SR test data and with respect to the SR training
2The Penn Treebank makes the RNR interpretations ex-
plicit (refer to Figure 1).
3It is just a coincidence that the size of the SR test data
and the number of extracted elliptic sentences are the same.
45
SR Data Pass BLEU ScoresCOV ALL
Test
+E (182)
Before 58% 0.59 0.34
After 67% 0.59 0.40
Delta +9 +0.00 +0.06
-E (2216)
Before 80% 0.59 0.47
After 80% 0.59 0.48
Delta +0 +0.00 +0.01
T (2398)
Before 78% 0.58 0.46
After 79% 0.59 0.47
Delta +1 +0.01 +0.01
Training
+E (2398)
(Table 1)
Before 65% 0.72 0.47
After 76% 0.74 0.56
Delta +11 +0.02 +0.09
-E (24206)
Before 82% 0.73 0.60
After 82% 0.73 0.60
Delta +0 +0.00 +0.00
T (26604)
Before 81% 0.72 0.58
After 82% 0.73 0.60
Delta +1 +0.01 +0.02
Table 2: Generation results on SR test and SR
training data before and after input rewriting (+E
stands for elliptical data, -E for non elliptical data
and T for total.)
data. We use the SR Task scripts for the computa-
tion of the BLEU score.
The impact of ellipsis on coverage and preci-
sion. Previous work on parsing showed that co-
ordination was a main source of parsing failure
(Collins, 1999). Similarly, ellipses is an important
source of failure for the TAG generator. Ellipses
are relatively frequent with 9% of the sentences
in the training data containing an elliptic struc-
ture and performance markedly decreases in the
presence of ellipsis. Thus, before rewriting, cov-
erage decreases from 82.3% for non-elliptic sen-
tences to 80.75% on all sentences (elliptic and non
elliptic sentences) and to 65.3% on the set of el-
liptic sentences. Similarly, BLEU score decreases
from 0.60 for non elliptical sentences to 0.58 for
all sentences and to 0.47 for elliptic sentences. In
sum, both coverage and BLEU score decrease as
the number of elliptic input increases.
The impact of the input representation on cov-
erage and precision. Recent work on treebank
annotation has shown that the annotation schema
adopted for coordination impacts parsing. In par-
ticular, Maier et al (2012) propose revised annota-
tion guidelines for coordinations in the Penn Tree-
bank whose aim is to facilitate the detection of co-
ordinations. And Dukes and Habash (2011) show
that treebank annotations which include phoneti-
cally empty material for representing elided mate-
rial allows for better parsing results.
Similarly, Table 2 shows that the way in which
ellipsis is represented in the input data has a strong
impact on generation. Thus rewriting the input
data markedly extends coverage with an overall
improvement of 11 points (from 65% to 76%) for
elliptic sentences and of almost 1 point for all sen-
tences.
As detailed in Table 1 though, there are impor-
tant differences between the different types of el-
liptic constructs: coverage increases by 68 points
for NCC and 64 points for gapping against only
15, 13 and 5 points for RNR, mixed RNR-Shared
Subject and Shared Subject respectively. The rea-
son for this is that sentences are generated for
many input containing the latter types of con-
structions (RNR and Shared Subject) even with-
out rewriting. In fact, generation succeeds on the
non rewritten input for a majority of RNR (66%
PASS), Shared Subject (70% PASS) and mixed
RNR-Shared Subject (61% PASS) constructions
whereas it fails for almost all cases of gapping
(3% PASS) and of NCC (5% PASS). The reason
for this difference is that, while the grammar can-
not cope with headless constructions such as gap-
ping and NCC constructions, it can often provide
a derivation for shared subject sentences by using
the finite verb form in the source sentence and the
corresponding infinitival form in the target. Since
the infinitival does not require a subject, the tar-
get sentence is generated. Similarly, RNR con-
structions can be generated when the verb in the
source clause has both a transitive and an intran-
sitive form: the transitive form is used to gener-
ate the source clause and the intransitive for the
target clause. In short, many sentences contain-
ing a RNR or a shared subject construction can be
generated without rewriting because the grammar
overgenerates i.e., it produces sentences which are
valid sentences of English but whose phrase struc-
ture tree is incorrect.
Nevertheless, as the results show, rewriting
consistently helps increasing coverage even for
RNR (+15 points), Shared Subject (+5 points) and
mixed RNR-Shared Subject (+13 points) construc-
tions because (i) not all verbs have both a transi-
tive and an intransitive verb form and (ii) the input
for the elliptic clause may require a finite form for
the target verb (e.g., in sentences such as ?[they]i
weren?t fired but instead i were neglected? where the tar-
get clause includes an auxiliary requiring a past
46
participial which in this context requires a sub-
ject).
Precision is measured using the BLEU score.
For each input, we take the best score obtained
within the 5 derivations4 produced by the gener-
ator. Since the BLEU score reflects the degree to
which a sentence generated by the system matches
the corresponding Penn Treebank sentence, it is
impacted not just by elliptic coordination but also
by all linguistic constructions present in the sen-
tence. Nonetheless, the results show that rewrit-
ing consistently improves the BLEU score with an
overall increase of 0.09 points on the set of ellip-
tic sentences. Moreover, the consistent improve-
ment in terms of BLEU score for generated sen-
tences (COV column) shows that rewriting simul-
taneously improves both coverage and precision
that is, that for those sentences that are generated,
rewriting consistently improves precision.
Analysing the remaining failure cases. To bet-
ter assess the extent to which rewriting and the FB-
LTAG generation system succeed in generating el-
liptic coordinations, we performed error mining
on the elliptic data using our error miner described
in (Narayan and Gardent, 2012a). This method
permits highlighting the most likely sources of er-
ror given two datasets: a set of successful cases
and a set of failure cases. In this case, the suc-
cessful cases is the subset of rewritten input data
for elliptic coordination cases for which genera-
tion succeeds . The failure cases is the subset for
which generation fails. If elliptic coordination was
still a major source of errors, input nodes or edges
labelled with labels related to elliptic coordination
(e.g., the COORD and the GAP-X dependency rela-
tions or the CONJ part of speech tag) would sur-
face as most suspicious forms. In practice how-
ever, we found that the 5 top sources of errors
highlighted by error mining all include the DEP re-
lation, an unknown dependency relation used by
the Pennconverter when it fails to assign a label
to a dependency edge. In other words, most of the
remaining elliptic cases for which generation fails,
fails for reasons unrelated to ellipsis.
Comparison with other surface realisers
There is no data available on the performance
of surface realisers on elliptic input. However,
the performance of the surface realiser can be
4The language model used in the generator allows only 5
likely derivations (refer to section 5.1).
compared with those participating in the shallow
track of the SR challenge. On the SR training
data, the TAG surface realiser has an average
run time of 2.78 seconds per sentence (with an
average of 20 words per sentence), a coverage
of 82% and BLEU scores of 0.73 for covered
and 0.60 for all. On the SR test data, the realiser
achieves a coverage of 79% and BLEU scores of
0.59 for covered and 0.47 for all. In comparison,
the statistical systems in the SR Tasks achieved
0.88, 0.85 and 0.67 BLEU score on the SR test
set and the best symbolic system 0.25 (Belz et al,
2011).
6 Related work
Previous work on generating elliptic sentences has
mostly focused on identifying material that could
be elided and on defining procedures capable of
producing input structures for surface realisation
that support the generation of elliptic sentences.
Shaw (1998) developed a sentence planner
which generates elliptic sentences in 3 steps. First,
input data are grouped according to their simi-
larities. Second, repeated elements are marked.
Third, constraints are used to determine which oc-
currences of a marked element should be deleted.
The approach is integrated in the PLANDoc sys-
tem (McKeown et al, 1994) and shown to gen-
erate a wide range of elliptic constructs includ-
ing RNR, VPE and NCC using FUF/SURGE (El-
hadad, 1993), a realisation component based on
Functional Unification Grammar.
Theune et al (2006) describe how elliptic sen-
tences are generated in a story generation system.
The approach covers conjunction reduction, right
node raising, gapping and stripping and uses de-
pendency trees connected by rhetorical relations
as input. Before these trees are mapped to sen-
tences, repeated elements are deleted and their an-
tecedent (thesource element) is related by a SUB-
ORROWED relation to their governor in the ellip-
tic clause and a SUIDENTICAL relation to their
governor in the antecedent clause. This is then in-
terpreted by the surface realiser to mean that the
repeated element should be realised in the source
clause, elided in the target clause and that it li-
censes the same syntactic structure in both clauses.
Harbusch and Kempen (2009) have proposed a
module called Elleipo which takes as input unre-
duced, non-elliptic, syntactic structures annotated
with lexical identity and coreference relationships
47
between words and word groups in the conjuncts;
and returns as output structures annotated with
elision marks indicating which elements can be
elided and how (i.e., using which type of ellip-
sis). The focus is on developing a language in-
dependent module which can mediate between the
unreduced input syntactic structures produced by a
generator and syntactic structures that are enriched
with elision marks rich enough to determine the
range of possible elliptic and non elliptic output
sentences.
In CCG, grammar rules (type-raising and com-
position) permit combining non constituents into a
functor category which takes the shared element as
argument; and gapping remnants into a clause tak-
ing as argument its left-hand coordinated source
clause. White (2006) describes a chart based algo-
rithm for generating with CCG and shows that it
can efficiently realise NCC and gapping construc-
tions.
Our proposal differs from these approaches in
that it focuses on the surface realisation stage (as-
suming that the repeated elements have already
been identified) and is tested on a large corpus
of newspaper sentences rather than on hand-made
document plans and relatively short sentences.
7 Conclusion
In this paper, we showed that elliptic structures
are frequent and can impact the performance of
a surface realiser. In line with linguistic theory
and with some recent results on treebank annota-
tion, we argued that the representation of ellipsis
should involve empty categories and we provided
a set of tree rewrite rules to modify the SR data ac-
cordingly. We then evaluated the performance of a
TAG based surface realiser on 2398 elliptic input
derived by the SR task from the Penn Treebank
and showed that it achieved a coverage of 76% and
a BLEU score of 0.74 on generated sentences. Our
approach relies both on the fact that the grammar
is lexicalised (each rule is associated with a word
from the input) and on TAG extended domain of
locality (which permits using a rule anchored with
the empty string to reconstruct the missing syntax
in the elided clause thereby making it grammati-
cal).
We will release the 2398 input representations
we gathered for evaluating the generation of el-
liptic coordination so as to make it possible for
other surface realisers to be evaluated on their abil-
ity to generate ellipsis. In particular, its would
be interesting to examine how other grammar
based generators perform on this dataset such
as White?s CCG based generator (2006) (which
eschews empty categories by adopting a more
flexible notion of constituency) and Carroll and
Oepen?s HPSG based generator (2005) (whose do-
main of locality differs from that of TAG).
Acknowledgments
We would like to thank Anja Belz and Mike White
for providing us with the evaluation data and the
evaluation scripts. The research presented in this
paper was partially supported by the European
Fund for Regional Development within the frame-
work of the INTERREG IV A Allegro Project.
References
Eva Banik. 2004. Semantics of VP coordination
in LTAG. In Proceedings of the 7th International
Workshop on Tree Adjoining Grammars and Re-
lated Formalisms (TAG+), volume 7, pages 118?
125, Vancouver, Canada.
Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Ann Bies, Mark Ferguson, Katz Katz, Robert MacIn-
tyre, Victoria Tredinnick, Grace Kim, Marry Ann
Marcinkiewicz, and Britta Schasberger. 1995.
Bracketing guidelines for treebank II style penn tree-
bank project. University of Pennsylvania.
Aoife Cahill and Josef Van Genabith. 2006. Robust
pcfg-based generation using automatically acquired
lfg approximations. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
(COLING) and the 44th annual meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1033?1040, Sydney, Australia.
Charles B Callaway. 2003. Evaluating coverage for
large symbolic nlg grammars. In Proceedings of the
18th International joint conference on Artificial In-
telligence (IJCAI), volume 18, pages 811?816, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
John Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Proceedings of the 2nd International
Joint Conference on Natural Language Process-
ing (IJCNLP), pages 165?176, Jeju Island, Korea.
Springer.
48
M. ?Cmejrek, J. Hajic?, and V. Kubon?. 2004. Prague
czech-english dependency treebank: Syntactically
annotated resources for machine translation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC), Lis-
bon, Portugal.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Peter W. Culicover and Ray Jackendoff. 2005. Simpler
Syntax. Oxford University Press.
Mary Dalrymple and Ronald M. Kaplan. 2000. Fea-
ture indeterminacy and feature resolution. Lan-
guage, pages 759?798.
Mary Dalrymple, Stuart M. Sheiber, and Fernando
C. N. Pereira. 1991. Ellipsis and higher-order unifi-
cation. Linguistics and Philosophy.
Michael Daum, Kilian Foth, and Wolfgang Menzel.
2004. Automatic transformation of phrase treebanks
to dependency trees. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation (LREC), Lisbon, Portugal.
Kais Dukes and Nizar Habash. 2011. One-step sta-
tistical parsing of hybrid dependency-constituency
syntactic representations. In Proceedings of the
12th International Conference on Parsing Technolo-
gies, pages 92?103, Dublin, Ireland. Association for
Computational Linguistics.
Michael Elhadad. 1993. Using argumentation to con-
trol lexical choice: a functional unification imple-
mentation. Ph.D. thesis, Columbia University.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference (INLG), pages 25?32, Salt
Fork, Ohio, USA. Association for Computational
Linguistics.
Rubino Gei?, Gernot Veit Batz, Daniel Grund, Sebas-
tian Hack, and Adam M. Szalkowski. 2006. Grgen:
A fast spo-based graph rewriting tool. In Proceed-
ings of the 3rd International Conference on Graph
Transformation, pages 383?397. Springer. Natal,
Brasil.
Jonathan Ginzburg and Ivan Sag. 2000. Interrogative
investigations. CSLI Publications.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M.A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. ?Ste?pa?nek, et al 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 1?18.
Karin Harbusch and Gerard Kempen. 2009. Gener-
ating clausal coordinate ellipsis multilingually: A
uniform approach based on postediting. In Proceed-
ings of the 12th European Workshop on Natural Lan-
guage Generation, pages 138?145, Athens, Greece.
Association for Computational Linguistics.
Daniel Hardt. 1993. Verb phrase ellipsis: Form,
meaning and processing. Ph.D. thesis, University
of Pennsylvania.
Richert Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings of the 16th Nordic Conference
of Computational Linguistics (NODALIDA), pages
105?112, Tartu, Estonia.
Edward Keenan. 1971. Names, quantifiers, and the
sloppy identity problem. Papers in Linguistics,
4:211?232.
Andrew Kehler. 2002. Coherence in discourse. CSLI
Publications.
Yusuke Kubota and Robert Levine. 2012. Gapping
as like-category coordination. In Proceedings of the
7th international conference on Logical Aspects of
Computational Linguistics (LACL), pages 135?150,
Nantes, France. Springer-Verlag.
Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proceedings of the
12th International Natural Language Generation
Workshop, pages 17?24.
Roger Levy and Carl Pollard. 2001. Coordination and
neutralization in HPSG. Technology, 3:5.
Wolfgang Maier, Erhard Hinrichs, Julia Krivanek, and
Sandra Ku?bler. 2012. Annotating coordination
in the Penn Treebank. In Proceedings of the 6th
Linguistic Annotation Workshop (LAW), pages 166?
174, Jeju, Republic of Korea. Association for Com-
putational Linguistics.
Kathleen McKeown, Karen Kukich, and James Shaw.
1994. Practical issues in automatic documentation
generation. In Proceedings of the fourth conference
on Applied natural language processing (ANLC),
pages 7?14, Stuttgart, Germany. Association for
Computational Linguistics.
Jason Merchant. 2001. The syntax of silence: Sluicing,
islands, and the theory of ellipsis. Oxford Univer-
sity Press.
Shashi Narayan and Claire Gardent. 2012a. Error min-
ing with suspicion trees: Seeing the forest for the
trees. In Proceedings of the 24th International Con-
ference on Computational Linguistics (COLING),
Mumbai, India.
Shashi Narayan and Claire Gardent. 2012b. Structure-
driven lexicalist generation. In Proceedings of the
24th International Conference on Computational
Linguistics (COLING), Mumbai, India.
49
Ivan Sag. 1976. Deletion and logical form. Ph.D.
thesis, Massachusetts Institute of Technology, Cam-
bridge, Massachusetts.
Anoop Sarkar and Arvind Joshi. 1996. Coordination
in tree adjoining grammars: Formalization and im-
plementation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 610?
615, Copenhagen, Denmark. Association for Com-
putational Linguistics.
Djame? Seddah. 2008. The use of mctag to process
elliptic coordination. In Proceedings of The Ninth
International Workshop on Tree Adjoining Gram-
mars and Related Formalisms (TAG+ 9), volume 1,
page 2, Tu?bingen, Germany.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a ger-
man treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey.
James Shaw. 1998. Segregatory coordination and
ellipsis in text generation. In Proceedings of the
36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Con-
ference on Computational Linguistics, pages 1220?
1226, Montreal, Quebec, Canada.
Mark Steedman. 1996. Surface Structure and Inter-
pretation, volume 30. MIT press Cambridge, MA.
Marie?t Theune, Feikje Hielkema, and Petra Hendriks.
2006. Performing aggregation and ellipsis using dis-
course structures. Research on Language & Compu-
tation, 4(4):353?375.
Jeoren van Craenenbroeck. 2010. The syntax of ellip-
sis: Evidence from Dutch dialects. Oxford Univer-
sity Press.
V. Vincze, D. Szauter, A. Alma?si, G. Mo?ra, Z. Alexin,
and J. Csirik. 2010. Hungarian dependency tree-
bank. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), Valletta, Malta.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for ccg realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 410?419, Singapore. Association for Compu-
tational Linguistics.
Michael White. 2006. Efficient realization of coordi-
nate structures in combinatory categorial grammar.
Research on Language & Computation, 4(1):39?75.
Edwin Williams. 1977. Discourse and logical form.
Linguistic Inquiry.
Huayan Zhong and Amanda Stent. 2005. Building
surface realizers automatically from corpora. In
Proceedings of the Workshop on Using Corpora for
Natural Language Generation (UCNLG), volume 5,
pages 49?54.
50
Proceedings of the 14th European Workshop on Natural Language Generation, pages 94?97,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
The KBGen Challenge
Eva Banik
Computational
Linguistics Ltd
London, UK
ebanik@comp-ling.com
Claire Gardent
CNRS, LORIA
Nancy, France
claire.gardent@loria.fr
Eric Kow?
Computational
Linguistics Ltd
London, UK
kowey@comp-ling.com
1 Introduction
The KBGen 2013 natural language generation
challenge1 was intended to survey and compare
the performance of various systems which perform
tasks in the content realization stage of generation
(Banik et al, 2012). Given a set of relations which
form a coherent unit, the task is to generate com-
plex sentences which are grammatical and fluent
in English. The relations for this year?s challenge
were selected from the AURA knowledge base
(KB) (Gunning et al, 2010). In this paper we give
an overview of the KB, describe our methodology
for selecting sets of relations from the KB to pro-
vide input-output pairs for the challenge, and give
details of the development and test data set that
was provided to participating teams. Three teams
have submitted system outputs for this year?s chal-
lenge. In this paper we show BLEU and NIST
scores for outputs generated by the teams. The full
results of our evaluation, including human judge-
ments, as well as the development and test data set
are available at http://www.kbgen.org.
2 The AURA Knowledge Base
The AURA knowledge base (Gunning et al,
2010) encodes information from a biology text-
book (Reece et al, 2010). It was developed to
support a question answering system, to help stu-
dents understand biological concepts by allowing
them to ask questions about the material while
reading the textbook. AURA is a frame-based
KB which encodes events, the entities that partic-
ipate in events, properties, and roles that the en-
tities play in an event. The relations in the KB
include relations between these types, including
event-to-entity, event-to-event, event-to-property,
entity-to-property. The KB is built on top of the
?The work reported in this paper was supported by fund-
ing from Vulcan, Inc.
1http://www.kbgen.org
CLIB generic library of concepts (Barker et al,
2001). As part of the encoding process, concepts
in CLIB are specialized and/or combined to en-
code biology-specific information. AURA is or-
ganized into a set of concept maps, where each
concept map corresponds to a biological entity or
process. The KB was encoded by biology teach-
ers and contains around 5,000 concept maps. It is
available for download for academic purposes in
various formats including OWL2.
3 The Content Selection Process for
KBGen 2012
The input provided to the participants consisted
of a set of content units extracted from the KB,
and a sentence corresponding to each content unit.
The content units were semi-automatically se-
lected from AURA such that:
? the set of relations in each content unit
formed a connected graph
? each content unit can be verbalised by a
single, possibly complex sentence which is
grammatical and meaningful
? the set of content units contain as many dif-
ferent relations and concepts of different se-
mantic types (events, entities, properties, etc)
as possible.
To produce these inputs we first asked biology
teachers to provide coherent content units using
the AURA graphical interface. The basic assump-
tion behind this approach was that, since every
content unit can be expressed by a coherent sen-
tence, each set of relations will exhibit a ?coher-
ence pattern?. We then created a search space of
candidate content units by extracting patterns from
the KB which were similar to the patterns given
by the biologists. Finally, we manually selected
coherent content units.
2http://www.ai.sri.com/halo/
halobook2010/exported-kb/biokb.html
94
Figure 1: ? A hydrophobic compound attaches to a
carrier protein at a region called the binding site.?
3.1 Manual Selection of Content Units
In the first step of our process, biology teachers
manually selected parts of concept maps which
represented educationally useful information for
biology students by searching for specific con-
cepts in AURA?s graph-based user interface. For
each content unit they wrote a sentence verbalis-
ing the selected relations (Fig. 1). The biology
teachers who identified these coherent, sentence-
sized chunks of information were familiar with the
encoding practices in AURA, the underlying biol-
ogy textbook, and had experience with the Inquire
e-book application (Spaulding et al, 2011) which
displays educationally useful content from the KB.
3.2 From Graphs to Queries
In the second step, the graphical representations
produced by the biologists were manually trans-
lated to specific knowledge base queries which
were run in AURA to retrieve the instances sat-
isfying the queries. Queries consist of two parts:
a set of triples whose domain and range are vari-
ables, and a set of instance-of triples stating type
constraints on the variables. The graph shown in
Figure 1 was translated to the following query:
Type constraints:
(?CP instance-of Carrier-Protein)
(?A instance-of Attach)
(?BS instance-of Binding-Site)
(?HP instance-of Hydrophilic-Compound)
Relation constraints:
(?A object ?HP)
(?A base ?CP)
(?A site ?BS)
(?CP has-region ?BS)
3.3 From Queries to Generalized Query
Patterns
After checking that it returns an answer, each
query was generalized to a query pattern in or-
der to find other queries which involved different
concepts and relations, but still exhibited the same
general coherence pattern. To derive generalized
query patterns, specific queries were modified in
two ways: 1) by removing type constraints on con-
cepts, and 2) by replacing specific relations with
generalized relation types.
Removing type constraints
Manually specified queries were extended by re-
moving type constraints on variables. In the above
example, types were generalised to Event or En-
tity:
(?CP instance-of Entity)
(?A instance-of Event)
(?BS instance-of Entity)
(?HP instance-of Entity)
Other generalized types we used from the ontol-
ogy were Property-Values and Roles.
Generalizing relations
Each query was generalized by defining equiva-
lence classes over semantically similar relations
and replacing the specific relation in the query
with its equivalence class. The basic assumption
behind this was that if a set of relations is coherent,
we should be able to replace a relation with an-
other, semantically similar relation in the set, and
still have a coherent content unit. For example,
whether two entities are connected by has-part
or has-region is unlikely to make a difference
to the coherence of a content unit.
Following this approach we identified groups of
semantically similar relations within each relation
type (Event-to-Event, Event-to-Entity, etc). The
equivalence classes over relations were straight-
forwardly derived from distinctions made in CLIB
(Barker et al, 2001), the upper ontology and li-
brary of general concepts that AURA is built on,
although there was some manual fine-tuning re-
quired to exclude relations which were not re-
liably encoded in the KB. For example, we di-
vided Entity-to-Entity relations into three cate-
gories, based on whether they had a spatial or
meronymic sense, or expressed a specific relation
between two chemicals:
en2en-spatial: abuts is-above is-along is-at is-
inside is-opposite is-outside is-over location
95
is-across is-on is-parallel-to is-perpendicular-
to is-under is-between is-facing is-below is-
beside is-near
en2en-part: possesses has-part has-region
encloses has-basic-structural-unit has-
structural-part has-functional-part
en2en-chemical: has-solute has-solvent has-
atom has-ion has-oxidized-form has-
reduced-form has-isomer
Here the distinction between spatial relations
and meronymic relations was given by CLIB. Re-
lations in the third group were specific to our do-
main and added during the process of encoding.
Event-to-entity relations were divided into
?aux-participant? relations, which express the spa-
tial orientation of an event, and ?core-participant?
relations which describe ways in which entities
participate in the event. Here we used the cat-
egories of spatial relations and ?participant? re-
lations from CLIB. Our terminology reflects the
fact that entities connected to an event by a
core-participant relation are typically expressed as
obligatory arguments of the verb in a sentence,
whereas aux-participants would be expressed as
optional modifiers:
core-participants: agent object donor base in-
strument raw-material recipient result
aux-participants: away-from destination origin
path site toward
With these definitions, the specific query illus-
trated above in section 3.2 was translated to the
following query pattern:
(?A core-participant ?X)
(?A core-participant ?CP)
(?A aux-participant ?BS)
(?CP en2en-part ?BS)
3.4 From Query Results to Content Units
Query patterns were expanded by producing all
valid instantiations of the pattern in order to cre-
ate a search space of candidate content units, and
we ran each expanded query in AURA. The last
step was filtering the results returned by satisfi-
able queries to obtain content units which can be
verbalised in a single sentence. We used the fol-
lowing selection criteria to do this:
? A meaningful and grammatical sentence
could be formed by verbalising all concepts,
relations and properties present in the query
result.
(KBGEN-INPUT :ID "ex03c.99-1"
:TRIPLES (
(|Secretion21994| |object| |Mucus21965|)
(|Secretion21994| |base| |Earthworm21974|)
(|Secretion21994| |site| |Alimentary-Canal21978|)
(|Earthworm21974| |has-region|
|Alimentary-Canal21978|))
:INSTANCE-TYPES (
(|Mucus21965| |instance-of| |Mucus|)
(|Secretion21994| |instance-of| |Secretion|)
(|Earthworm21974| |instance-of| |Earthworm|)
(|Alimentary-Canal21978| |instance-of|
|Alimentary-Canal|))
:ROOT-TYPES (
(|Secretion21994| |instance-of| |Event|)
(|Mucus21965| |instance-of| |Entity|)
(|Earthworm21974| |instance-of| |Entity|)
(|Alimentary-Canal21978| |instance-of| |Entity|)
))
Figure 2: Input for the sentence ?Mucus is se-
creted in the alimentary canal of earthworms.?
? The set of content units should be as varied
as possible. In particular, we did not keep
a content unit if another very similar content
unit was present in the selected units. For in-
stance, if two content units contain identical
relations (modulo concept labels), only one
of these two units would be kept.
Given the pattern shown in Fig. 1 for instance,
we obtained 27 coherent content units. Each con-
tent unit was verbalized as a sentence to provide
development data for the content realization chal-
lenge. The following sentences illustrate the vari-
ation in the resulting content units:
- Polymers are digested in the lysosomes of eu-
karyotic cells.
- Mucus is secreted in the alimentary canal of
earthworms.
- Lysosomal enzymes digest proteins and poly-
mers at the lysosome of a eukaryotic cell.
- A chemical is attached to the active site of a
protein enzyme with an ionic bond.
- An enzyme substrate complex is formed
when a chemical attaches to the active site of
a protein enzyme with a hydrogen bond.
- Starch is stored in the lateral root of carrots.
4 Development Data Set
The development data set provided to participants
contained 207 input-output pairs. These inputs
96
were based on 19 different coherence patterns.
Fig. 2 shows an input-output pair based on the
pattern illustrated above. We also provided two
lexicons: a lexicon for events which gave a map-
ping from events to verbs, their inflected forms and
nominalizations and a lexicon for entities, which
provided a noun and its plural form. The rele-
vant entries in these lexicons for the input in Fig. 2
were:
Secretion,secretes,secrete,secreted,secretion
Mucus, mucus, mucus
Earthworm,earthworm,earthworms
Alimentary-Canal,alimentary canal,alimentary canals
5 Test Set
Our test data set contained 72 inputs in the same
format (and corresponding lexical resources as
above), which were divided into three categories:
(1) seen patterns, seen relations: inputs that have
exactly the same relations as some of the inputs in
the development data set, but different concepts
(2) seen patterns, unseen relations: these in-
puts are derived from patterns in the development
data set. They have similar structure, but contain
slightly different combinations of relations.
(3) unseen patterns: inputs extracted from a pre-
viously unused pattern, containing combinations
of relations not seen in the development data set.
6 Evaluation
Participants submitted two sets of outputs:
(1) outputs generated by their system as is (mod-
ulo including the lexicon provided in the test data
set) (2) outputs generated 6 days later, during
which time teams had a chance to make improve-
ments.
Each team was allowed to submit a set of 5 ranked
outputs for each input. We have evaluated all
of the submitted outputs using BLEU and NIST
scores and we are currently in the process of col-
lecting human judgements for the final system out-
puts that were ranked first. Table 1 shows the
overall results of automatic evaluation on both the
initial and final data sets for our three teams3, as
well as the coverage of the individual systems over
the 72 test inputs. More detail including the full
results of our evaluation can be found at http:
//www.kbgen.org, along with a link to download
3IMS: Stuttgart University Institute for Computational
Language Processing, LOR: LORIA, University of Nancy,
UDEL: University of Delaware, Computer and Information
Science Department
NIST BLEU coverage
HUMAN-1 10.0098 1.0000 100%
UDEL-final-1 5.9749 0.3577 97%
UDEL-initial-1 5.6030 0.3165 100%
LOR-final-1 4.8569 0.3053 84%
LOR-final-3 4.7238 0.2993 100%
LOR-final-2 4.6711 0.2945 100%
LOR-final-5 4.5720 0.2812 100%
LOR-final-4 4.4889 0.2781 100%
IMS-final-2 3.9649 0.1107 100%
IMS-final-4 3.8813 0.1140 100%
IMS-final-1 3.8670 0.1111 100%
IMS-final-3 3.7765 0.1023 100%
IMS-initial-2 3.6726 0.1117 100%
IMS-initial-3 3.6608 0.1181 100%
IMS-initial-1 3.6384 0.1173 100%
IMS-initial-4 3.5817 0.1075 100%
LOR-initial-1 0.1206 0.0822 30%
LOR-initial-3 0.1091 0.0751 100%
LOR-initial-4 0.0971 0.0732 100%
LOR-initial-2 0.0948 0.0757 100%
LOR-initial-5 0.0881 0.0714 100%
Table 1: BLEU and NIST scores of initial and final
system outputs. The digit behind the team names
refer to the output rank
the development and test data set used in the chal-
lenge, and more information about AURA and re-
lated resources.
References
E. Banik, C. Gardent, D. Scott, N. Dinesh, and
F. Liang. 2012. Kbgen text generation from knowl-
edge bases as a new shared task. In INLG 2012,
Starved Rock State Park, Illinois,USA.
K. Barker, B. Porter, and P. Clark. 2001. A library of
generic concepts for composing knowledgebases. In
Proceedings K-CAP 2001, pages 14?21.
D. Gunning, V. K. Chaudhri, P. Clark, K. Barker, Shaw-
Yi Chaw, M. Greaves, B. Grosof, A. Leung, D. Mc-
Donald, S. Mishra, J. Pacheco, B. Porter, A. Spauld-
ing, D. Tecuci, and J. Tien. 2010. Project halo up-
date - progress toward digital aristotle. AIMagazine,
Fall:33?58.
Jane B. Reece, Lisa A. Urry, Michael L. Cain,
Steven A. Wasserman, Peter V. Minorsky, and
Robert B. Jackson. 2010. Campbell Biology. Pear-
son Publishing.
A. Spaulding, A. Overholtzer, J. Pacheco, J. Tien, V. K.
Chaudhri, D. Gunning, and P. Clark. 2011. Inquire
for iPad: Bringing question-answering AI into the
classroom. In International Conference on AI in Ed-
ucation (AIED).
97
Proceedings of the 14th European Workshop on Natural Language Generation, pages 204?205,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
LOR-KBGEN, A Hybrid Approach To Generating from the KBGen
Knowledge-Base
Bikash Gyawali
Universite? de Lorraine, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy, F-54500, France
bikash.gyawali@loria.fr
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy, F-54500, France
claire.gardent@loria.fr
Abstract
This abstract describes a contribution
to the 2013 KBGen Challenge from
CNRS/LORIA and the University of Lor-
raine. Our contribution focuses on an
attempt to automate the extraction of a
Feature Based Tree Adjoining Grammar
equipped with a unification based compo-
sitional semantics which can be used to
generate from KBGen data.
Introduction Semantic grammars, i.e., gram-
mars which link syntax and semantics, have been
shown to be useful for generation and for seman-
tic parsing. This abstract outlines an attempt to
automatically extract from the KBGen data, a Fea-
ture Based Tree Adjoining Grammar which can be
used for generation from the KBGen data.
Data The KBGen data consists of sets of triples
extracted from the AURA knowledge base which
encodes knowledge contained in a college-level
biology textbook. Each set of triple was selected
to be verbalisable as a simple, possibly complex
sentence. For instance, the input shown in Fig-
ure 1 can be verbalised as1:
(1) The function of a gated channel is to release
particles from the endoplasmic reticulum
Sketch of the Overall Grammar Extraction and
Generation Procedure To generate from the
KBGen data, we parsed each input sentence us-
ing the Stanford parser; we aligned the semantic
input with a substring in the input sentence; we ex-
tracted a grammar from the parsed sentences pro-
vided with the input triples; and we generated us-
ing an existing surface realiser. In addition some
of the input were preprocessed to produce a se-
mantics more compatible with the assumption un-
derlying the syntax/semantic interace of SemTAG;
1For space reasons, we slightly simplified the KBGen in-
put and removed type information.
:TRIPLES (
(|Release-Of-Calcium646|
|object| |Particle-In-Motion64582|)
(|Release-Of-Calcium646|
|base| |Endoplasmic-Reticulum64603|)
(|Gated-Channel64605|
|has-function||Release-Of-Calcium646|)
(|Release-Of-Calcium646|
|agent| |Gated-Channel64605|))
:INSTANCE-TYPES
(|Particle-In-Motion64582|
|instance-of| |Particle-In-Motion|)
(|Endoplasmic-Reticulum64603|
|instance-of| |Endoplasmic-Reticulum|)
(|Gated-Channel64605|
|instance-of| |Gated-Channel|)
(|Release-Of-Calcium646|
|instance-of| |Release-Of-Calcium|))
and a procedure was used to guess missing lexical
entries.
Alignment and Index Projection Given a Sen-
tence/Input pair (S, I) provided by the KBGen
Challenge, we match each entity and event vari-
able in I to a substring in S. Matching uses the
variable name, the name of the unary predicate
true of that variable and the word form assigned
to that predicte in the KBGen lexicon. Digits oc-
curring in the input are removed and the string in
the input sentence which is closest to either of the
used units is decorated with that variable. Index
variables are then projected up the syntactic trees
to reflect headedness. For instance, the variable
indexed with a noun is projected to the NP level;
and the index projected to the NP of a preposi-
tional phrase is project to the PP level.
Grammar Extraction Grammar extraction pro-
ceeds in two steps as follows. First, the subtrees
whose root node are indexed with an entity vari-
able are extracted. This results in a set of NP and
PP trees anchored with entity names and associ-
ated with the predication true of the indexing vari-
able.
Second, the subtrees capturing relations be-
tween variables are extracted. To perform this ex-
204
traction, each input variableX is associated with a
set of dependent variables i.e., the set of variables
Y such that X is related to Y (R(X,Y )). The
minimal tree containing all and only the dependent
variables D(X) of a variable X is then extracted
and associated with the set of literals ? such that
? = {R(Y,Z) | (Y = X?Z ? D(X))?(Y,Z ?
D(X))}. This procedure extracts the subtrees re-
lating the argument variables of a semantics func-
tors such as an event or a role.
The extracted grammar is a Feature-Based
Tree Adjoining Grammar with a Unification-based
compositional semantics as described in (Gardent,
2008). Each entry in the grammar associates a nat-
ural language expression with a syntactic tree and
a semantic representation thereby allowing both
for semantic parsing and for generation. Figure 1
shows the tree extracted for the release predicate
in Example 1.
Pre-Processing The parse trees produced by the
Stanford parser are pre-processed to better match
TAG recursive modeling of modification. In
particular, the flat structure assigned to relative
clauses is modified into a recursive structure.
The input semantics provided by the KBGen
task is also preprocessed to allow for aggregation
and to better match the assumptions underlying the
syntax/semantics interface of SemTAG.
For aggregation, we use a rewrite rule of the
form shown below to support the production
of e.g., A cellulose-synthase which contains a
polypeptide and two glucose synthesizes cellu-
lose..
R(X,Y1), . . . , R(X,Yn), P (Y 1), . . . , P (Yn)
? R(X,Y ), P (Y ), quantity(Y, n)
For relative clauses, we rewrite input of the
form plays(X Y),in-event(Y E), P(E), R(E X) to
plays(X Y),in-event(Y E), P(E), R(E Y). This cap-
tures the fact that in sentences such as A biomem-
brane is a barrier which blocks the hydrogen ion
of a chemical., the entity variable bound by the rel-
ative clause is that associated with barrier, not that
of the main clause subject biomembrane.
Guessing Missing Lexical Entries To handle
unseen input, we start by partitioning the in-
put semantics into sub-semantics corresponding to
events, entities and role. We then search the lexi-
con for an entry with a matching or similar seman-
tics. An entry with a similar semantics is an entry
with the same number and same type of literals
(literals with same arity and with identical rela-
tions). Similar entries are then adapted to create
lexical entries for unseen data.
References
Claire Gardent. 2008. Integrating a unification-based
semantics in a large scale lexicalised tree adjoining
grammar for french. In COLING?08, Manchester,
UK.
205
Proceedings of the SIGDIAL 2013 Conference, pages 357?359,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Weakly and Strongly Constrained Dialogues for Language Learning
Claire Gardent
CNRS/LORIA, Nancy
claire.gardent@loria.fr
Alejandra Lorenzo
Universite? de Lorraine
LORIA, Nancy
alejandra.lorenzo@loria.fr
Laura Perez-Beltrachini
KRDB Research Centre
FUB, Bolzano
laura.perez@loria.fr
Lina Rojas-Barahona
Universite? de Lorraine
LORIA, Nancy
lina.rojas@loria.fr
Abstract
We present two dialogue systems for lan-
guage learning which both restrict the di-
alog to a specific domain thereby pro-
moting robustness and the learning of a
given vocabulary. The systems vary in how
much they constrain the learner?s answer :
one system places no other constrain on
the learner than that provided by the re-
stricted domain and the dialog context ; the
other provides the learner with an exercise
whose solution is the expected answer.
The first system uses supervised learning
for simulating a human tutor whilst the
second one uses natural language gener-
ation techniques to produce grammar ex-
ercises which guide the learner toward the
expected answer.
1 Introduction
Work on dialog based tutors for language learn-
ing includes both chatbot systems which maintain
a free flowing dialog with the learner (Shawar and
Atwell, 2007; Jia, 2004) and form-focused dia-
log systems which restrict the learner answer e.g.,
by providing her with an answer template to be
filled in for the dialog to continue (Wilske and
Wolska, 2011). While the former encourages lan-
guage practice with a virtual tutor and requires a
good knowledge of the language, the latter focuses
on linguistic forms and usually covers a more re-
stricted lexical field thereby being more amenable
to less advanced learners.
In these notes, we describe a dialog architecture
which (i) supports both free-flowing and form-
focused man/machine dialog ; and (ii) ensures that
in both cases, dialogs are restricted to a specific
lexical field. The free-flowing dialog system uses
supervised classification techniques to predict the
system dialog move based on the learner?s input
and does not explicitely constrain the learner?s an-
swer. In contrast, the dialog system for intermedi-
ate learners provides an exercise which the learner
must solve to construct her answer.
To restrict the dialog to a specific domain and
to improve system robustness, we make use of a
finite-state automaton (FSA) describing the range
of permissible interactions within a given domain.
This FSA serves to guide the collection of human-
human interactions necessary to train the classi-
fier ; to verify and if necessary to adjust the sys-
tem?s predicted answer ; and to support the gener-
ation of the exercise provided in the form-focused
dialog engine.
2 Finite State Automaton and Domain
Representation
To support focused dialog and promote robust-
ness, we make use of the FSA depicted in Fig-
ure 2. This FSA models a generic sequence of
(System Question, Learner Answer, System Re-
action) turns instantiating 4 main question/answer
patterns and two possible system reactions.
The system question systematically formulates
a question which explicitly lists a set of possible
answers.
The learner?s answer is then classified into four
main types. It selects one of the proposed answers ;
it chooses another alternative not mentioned in
the system?s question ; it indicates that the learner
does not know the correct answer or does not un-
derstand the question ; or it meets neither of these
categories.
Finally, for each answer, the system can con-
tinue the dialogue by making a ?confirmation?
which is used to reassure the learner in the free
dialogue system and to provide some feedback in
the guided dialogue system ; or just by ?inform-
ing? to the learner of the correct answer before
getting into the next question.
Given a specific domain, this generic FSA is
357
>S:Welcome to my house!, I?m the cat. Do you want
to play with me? || ask(ready)
>L:Ok. || answer(ready)
>S:Great! How are you doing? || ask(health)
>L: I?m doing very well, thanks!
|| answer(health, pref-option)
>S: Awesome! || confirm(health,pref-option)
>S: Come! I want to show you the kitchen!
|| inform(health)
>S:In the kitchen we prepare lunch. I prepare
soup very often. Do you prefer soup or salad?
|| ask(pref-meal)
>L:I prefer salad.
|| answer(pref-meal,pref-option)
>S:So do I! || confirm(pref-meal,pref-option)
>S:I also love cheese. || inform(pref-meal)
>S:Where do we usually keep the cheese?
|| ask(cheese where)
>L:In the fridge || answer(cheese where,pref-option)
>S:You are right || confirm(cheese where,pref-option)
>S:In the fridge || inform(cheese where)
...
>S:Thanks for this little chat! See you soon
|| quit
FIGURE 1 ? Example of a dialog (translated from
French to English)
instantiated differently for each question/answer
pair making up a dialog. In particular, in the cur-
rent demonstration system, it is instantiated to
model a dialog situated in the kitchen of a virtual
world. Figure 1 shows an example dialog.
3 Situated Dialogues for language
learning
Our dialog systems for language learning are in-
tegrated in a serious game called I-FLEG (Interac-
tive French Learning Game, (Amoia et al, 2012))
in which the learner can trigger grammar exercices
and interactive dialog sessions by clicking on the
objects present in the virtual world.
IFLEG integrates the two dialog systems for
language learning mentioned above namely, a
?free answer dialog system? where the learner an-
swer is guided only by the preceding dialog ex-
changes ; and a ?guided dialog system? which re-
stricts the set of permissible answers by providing
the learner with an exercise whose solution pro-
vides a possible answer given the current dialog
context.
3.1 Data collection
To provide the training data necessary to train
the free dialog system, we conducted a Wizard-
of-Oz experiment where language learners were
invited to engage in a conversation with the wiz-
ard, a French tutor. In these experiments, we fol-
lowed the methodology and used the tools for
data collection and annotation presented in (Rojas-
Barahona et al, 2012a). Given an FSA specifiying
a set of 5 questions the learner had to answer, the
wizard guided the learner through the dialog us-
ing this FSA. The resulting corpus consists of 52
dialogues and 1906 sentences.
3.2 Free answer Dialogue System
The free answer dialogue system simulates
the behavior of the wizard tutor by means of
a Logistic-Regression classifier, the FSA and
a generation-by-selection algorithm. The system
first uses the FSA to determine the next question
to be asked. Then for each question, the Logistic-
Regression classifier is used to map the learner an-
swer to a system dialog act. At this stage, the FSA
is used again, in two different ways. First, it is used
to ensure that the predicted system dialog act is
consistent with the states in the FSA. In case of a
mismatch, a valid dialog act is selected in the cur-
rent context. In particular, unpredicted ?preferred
options? and ?do not know? learner answers are
detected using keyword spotting methods. If the
classifier prediction conflicts with the prediction
made by key word spotting, it is ignored and the
FSA transition is prefereed.
Second, since the system has several consecu-
tive turns, and given that the classifier only pre-
dicts the next one, the FSA is used to determine
the following system dialog acts sequence. For
instance, if the predicted next system dialog act
was ?confirm?, according to the FSA the follow-
ing system dialog act is ?inform? and then eiher
the next question encoded in the FSA or ?quit?.
Training the simulator To train the classifier,
we labeled each learner sentence with the dialog
act caracterising the next system act. The features
used for trainig included context features (namely,
the four previous system dialogue acts) and the set
of content words present in the learner turns af-
ter filtering using tf*idf (Rojas Barahona et al,
2012b). Given the learner input and the current di-
alog context, the classifier predicts the next system
move.
Generation by Selection Given the system move
predicted by the dialog manager, the system turn
is produced by randomly selecting from the train-
ing corpus an utterance annotated with that dialog
move.
3.3 Guided dialogue system
Unlike the free answer dialogue, the guided di-
alogue strongly constrains the learner answer by
suggesting it in the form of a grammar exercise.
358
FIGURE 2 ? Finite-state automata that defines the different states in the dialog for each question Q X. S
defines the system, and P the learner.
In the guided dialogue system, the dialogue
paths contained in the training corpus are used to
decide on the next dialogue move. In a first step,
learner?s moves are labelled with the meaning rep-
resentation associated to them by the grammar un-
derlying the natural language generator used to
produce IFLEG grammar exercises. Given a se-
quence S/L contained in the training corpus with
S, a system turn and L the corresponding learner?s
turn, the system then constructs the exercise pro-
viding the learner?s answer using the methodology
described in (Perez-Beltrachini et al, 2012). First,
a sentence is generated from the meaning repre-
sentation of the learner answer. Next, the linguis-
tic information (syntactic tree, morpho-syntactic
information, lemmas) associated by the generator
with the generated sentence is used to build a shuf-
fle, a fill-in-the-blank or a transformation exercise.
Here is an example interaction produced by the
system :
S : Vous pre?fe?rez la soupe ou le fromage ? (Do you
prefer soup or salad ?)
Please answer using the following words : { je,
adorer, le, soupe }
This dialogue setting has several benefits. The
dialogue script provides a rich context for each
generated exercise item, learners are exposed to
example communicative interactions, and the sys-
tem can provide feedback by comparing the an-
swer entered by the learner against the expected
one.
4 Sample Dialogue
In this demo, the user will be able to interact
with both dialogue systems, situated in the kitchen
of a virtual world, and where the tutor prompts
the learner with questions about meals, drinks,
and various kitchen related activities such as floor
cleaning and food preferences.
References
M. Amoia, T. Bre?taudie`re, A. Denis, C. Gardent, and
L. Perez-Beltrachini. 2012. A Serious Game for Second
Language Acquisition in a Virtual Environment. Jour-
nal on Systemics, Cybernetics and Informatics (JSCI),
10(1) :24?34.
J. Jia. 2004. The study of the application of a web-based
chatbot system on the teaching of foreign languages. In
Society for Information Technology & Teacher Educa-
tion International Conference, volume 2004, pages 1201?
1207.
L. Perez-Beltrachini, C. Gardent, and G. Kruszewski. 2012.
Generating Grammar Exercises. In NAACL-HLT 7th
Workshop on Innovative Use of NLP for Building Educa-
tional Applications, Montreal, Canada, June.
L. M. Rojas-Barahona, A. Lorenzo, and C. Gardent. 2012a.
Building and exploiting a corpus of dialog interactions be-
tween french speaking virtual and human agents. In Pro-
ceedings of the 8th International Conference on Language
Resources and Evaluation.
L. M. Rojas Barahona, A. Lorenzo, and C. Gardent. 2012b.
An end-to-end evaluation of two situated dialog systems.
In Proceedings of the 13th Annual Meeting of the Special
Interest Group on Discourse and Dialogue, pages 10?19,
Seoul, South Korea, July. ACL.
B. Abu Shawar and E. Atwell. 2007. Chatbots : are they
really useful ? In LDV Forum, volume 22, pages 29?49.
S. Wilske and M. Wolska. 2011. Meaning versus form in
computer-assisted task-based language learning : A case
study on the german dative. JLCL, 26(1) :23?37.
359
