The Importance of Syntactic Parsing and
Inference in Semantic Role Labeling
Vasin Punyakanok??
BBN Technologies
Dan Roth??
University of Illinois at
Urbana-Champaign
Wen-tau Yih??
Microsoft Research
We present a general framework for semantic role labeling. The framework combines a machine-
learning technique with an integer linear programming?based inference procedure, which in-
corporates linguistic and structural constraints into a global decision process. Within this
framework, we study the role of syntactic parsing information in semantic role labeling. We
show that full syntactic parsing information is, by far, most relevant in identifying the argument,
especially, in the very first stage?the pruning stage. Surprisingly, the quality of the pruning
stage cannot be solely determined based on its recall and precision. Instead, it depends on the
characteristics of the output candidates that determine the difficulty of the downstream prob-
lems. Motivated by this observation, we propose an effective and simple approach of combining
different semantic role labeling systems through joint inference, which significantly improves its
performance.
Our system has been evaluated in the CoNLL-2005 shared task on semantic role labeling,
and achieves the highest F1 score among 19 participants.
1. Introduction
Semantic parsing of sentences is believed to be an important task on the road to natural
language understanding, and has immediate applications in tasks such as informa-
tion extraction and question answering. Semantic Role Labeling (SRL) is a shallow
semantic parsing task, in which for each predicate in a sentence, the goal is to identify
all constituents that fill a semantic role, and to determine their roles (Agent, Patient,
Instrument, etc.) and their adjuncts (Locative, Temporal, Manner, etc.).
? 10 Moulton St., Cambridge, MA 02138, USA. E-mail: vpunyaka@bbn.com.
?? Department of Computer Science, University of Illinois at Urbana-Champaign, 201 N. Goodwin Ave.,
Urbana, IL 61801, USA. E-mail: danr@uiuc.edu.
? One Microsoft Way, Redmond, WA 98052, USA. E-mail: scottyih@microsoft.com.
? Most of the work was done when these authors were at the University of Illinois at Urbana-Champaign.
Submission received: 15 July 2006; revised submission received: 3 May 2007; accepted for publication:
19 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
The PropBank project (Kingsbury and Palmer 2002; Palmer, Gildea, and Kingsbury
2005), which provides a large human-annotated corpus of verb predicates and their ar-
guments, has enabled researchers to apply machine learning techniques to develop SRL
systems (Gildea and Palmer 2002; Chen and Rambow 2003; Gildea and Hockenmaier
2003; Pradhan et al 2003; Surdeanu et al 2003; Pradhan et al 2004; Xue and Palmer 2004;
Koomen et al 2005). However, most systems rely heavily on full syntactic parse trees.
Therefore, the overall performance of the system is largely determined by the quality
of the automatic syntactic parsers of which the state of the art (Collins 1999; Charniak
2001) is still far from perfect.
Alternatively, shallow syntactic parsers (i.e., chunkers and clausers), although they
do not provide as much information as a full syntactic parser, have been shown to
be more robust in their specific tasks (Li and Roth 2001). This raises the very natural
and interesting question of quantifying the importance of full parsing information to
semantic parsing and whether it is possible to use only shallow syntactic information to
build an outstanding SRL system.
Although PropBank is built by adding semantic annotations to the constituents in
the Penn Treebank syntactic parse trees, it is not clear how important syntactic parsing
is for an SRL system. To the best of our knowledge, this problem was first addressed
by Gildea and Palmer (2002). In their attempt to use limited syntactic information, the
parser they used was very shallow?clauses were not available and only chunks were
used. Moreover, the pruning stage there was very strict?only chunks were considered
as argument candidates. This results in over 60% of the actual arguments being ignored.
Consequently, the overall recall in their approach was very low.
The use of only shallow parsing information in an SRL system has largely been
ignored until the recent CoNLL-2004 shared task competition (Carreras and Ma`rquez
2004). In that competition, participants were restricted to using only shallow parsing
information, which included part-of-speech tags, chunks, and clauses (the definitions of
chunks and clauses can be found in Tjong Kim Sang and Buchholz [2000] and Carreras
et al [2002], respectively). As a result, the performance of the best shallow parsing?
based system (Hacioglu et al 2004) in the competition is about 10 points in F1 below the
best system that uses full parsing information (Koomen et al 2005). However, this is not
the outcome of a true and fair quantitative comparison. The CoNLL-2004 shared task
used only a subset of the data for training, which potentially makes the problem harder.
Furthermore, an SRL system is usually complicated and consists of several stages. It
was still unclear howmuch syntactic information helps and precisely where it helps the
most.
The goal of this paper is threefold. First, we describe an architecture for an SRL
system that incorporates a level of global inference on top of the relatively common
processing steps. This inference step allows us to incorporate structural and linguistic
constraints over the possible outcomes of the argument classifier in an easy way. The
inference procedure is formalized via an Integer Linear Programming framework and
is shown to yield state-of-the-art results on this task. Second, we provide a fair com-
parison between SRL systems that use full parse trees and systems that only use shal-
low syntactic information. As with our full syntactic parse?based SRL system (Koomen
et al 2005), our shallow parsing?based SRL system is based on the system that achieves
very competitive results and was one of the top systems in the CoNLL-2004 shared
task competition (Carreras and Ma`rquez 2004). This comparison brings forward a care-
ful analysis of the significance of full parsing information in the SRL task, and provides
an understanding of the stages in the process in which this information makes the most
difference. Finally, to relieve the dependency of the SRL system on the quality of
258
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
automatic parsers, we suggest a way to improve semantic role labeling significantly by
developing a global inference algorithm, which is used to combine several SRL systems
based on different state-of-the-art full parsers. The combination process is done through
a joint inference stage, which takes the output of each individual system as input and
generates the best predictions, subject to various structural and linguistic constraints.
The underlying system architecture can largely affect the outcome of our study.
Therefore, to make the conclusions of our experimental study as applicable as possible
to general SRL systems, the architecture of our SRL system follows the most widely
used two-step design. In the first step, the system is trained to identify argument candi-
dates for a given verb predicate. In the second step, the system classifies the argument
candidates into their types. In addition, it is also a simple procedure to prune obvious
non-candidates before the first step, and to use post-processing inference to fix incon-
sistent predictions after the second step. These two additional steps are also employed
by our system.
Our study of shallow and full syntactic information?based SRL systems was done
by comparing their impact at each stage of the process. Specifically, our goal is to investi-
gate at what stage full parsing information is most helpful relative to a shallow parsing?
based system. Therefore, our experiments were designed so that the compared systems
are as similar as possible, and the addition of the full parse tree?based features is the
only difference. The most interesting result of this comparison is that although each
step of the shallow parsing information?based system exhibits very good performance,
the overall performance is significantly inferior to the system that uses full parsing
information. Our explanation is that chaining multiple processing stages to produce
the final SRL analysis is crucial to understanding this analysis. Specifically, the quality
of the information passed from one stage to the other is a decisive issue, and it is
not necessarily judged simply by considering the F-measure. We conclude that, for
the system architecture used in our study, the significance of full parsing information
comes into play mostly at the pruning stage, where the candidates to be processed later
are determined. In addition, we produce a state-of-the-art SRL system by combining
different SRL systems based on two automatic full parsers (Collins 1999; Charniak 2001),
which achieves the best result in the CoNLL-2005 shared task (Carreras and Ma`rquez
2005).
The rest of this paper is organized as follows. Section 2 introduces the task of
semantic role labeling in more detail. Section 3 describes the four-stage architecture of
our SRL system, which includes pruning, argument identification, argument classifi-
cation, and inference. The features used for building the classifiers and the learning
algorithm applied are also explained there. Section 4 explains why and where full
parsing information contributes to SRL by conducting a series of carefully designed
experiments. Inspired by the result, we examine the effect of inference in a single system
and propose an approach that combines different SRL systems based on joint inference
in Section 5. Section 6 presents the empirical evaluation of our system in the CoNLL-
2005 shared task competition. After that, we discuss the related work in Section 7 and
conclude this paper in Section 8.
2. The Semantic Role Labeling (SRL) Task
The goal of the semantic role labeling task is to discover the predicate?argument struc-
ture of each predicate in a given input sentence. In this work, we focus only on the verb
predicate. For example, given a sentence I left my pearls to my daughter-in-law in my will,
259
Computational Linguistics Volume 34, Number 2
the goal is to identify the different arguments of the verb predicate left and produce the
output:
[A0 I] [V left ] [A1 my pearls] [A2 to my daughter-in-law] [AM-LOC in my will].
Here A0 represents the leaver, A1 represents the thing left, A2 represents the beneficiary,
AM-LOC is an adjunct indicating the location of the action, and V determines the
boundaries of the predicate, which is important when a predicate contains many words,
for example, a phrasal verb. In addition, each argument can be mapped to a constituent
in its corresponding full syntactic parse tree.
Following the definition of the PropBank and CoNLL-2004 and 2005 shared tasks,
there are six different types of arguments labeled as A0?A5 and AA. These labels have
different semantics for each verb and each of its senses as specified in the PropBank
Frame files. In addition, there are also 13 types of adjuncts labeled as AM-adj where adj
specifies the adjunct type. For simplicity in our presentation, we will also refer to these
adjuncts as arguments. In some cases, an argument may span over different parts of
a sentence; the label C-arg is then used to specify the continuity of the arguments, as
shown in this example:
[A1 The pearls] , [A0 I] [V said] , [C-A1 were left to my daughter-in-law].
In some other cases, an argumentmight be a relative pronoun that in fact refers to the ac-
tual agent outside the clause. In this case, the actual agent is labeled as the appropriate
argument type, arg, while the relative pronoun is instead labeled as R-arg. For example,
[A1 The pearls] [R-A1 which] [A0 I] [V left] [A2 to my daughter-in-law] are fake.
Because each verb may have different senses producing different semantic roles
for the same labels, the task of discovering the complete set of semantic roles should
involve not only identifying these labels, but also the underlying sense for a given
verb. However, as in all current SRL work, this article focuses only on identifying the
boundaries and the labels of the arguments, and ignores the verb sense disambiguation
problem.
The distribution of these argument labels is fairly unbalanced. In the official release
of PropBank I, core arguments (A0?A5 and AA) occupy 71.26% of the arguments, where
the largest parts are A0 (25.39%) and A1 (35.19%). The rest mostly consists of adjunct
arguments (24.90%). The continued (C-arg) and referential (R-arg) arguments are rela-
tively few, occupying 1.22% and 2.63%, respectively. For more information on PropBank
and the semantic role labeling task, readers can refer to Kingsbury and Palmer (2002)
and Carreras and Ma`rquez (2004, 2005).
Note that the semantic arguments of the same verb do not overlap. We define over-
lapping arguments to be those that share some of their parts. An argument is considered
embedded in another argument if the second argument completely covers the first one.
Arguments are exclusively overlapping if they are overlapping but are not embedded.
3. SRL System Architecture
Adhering to the most common architecture for SRL systems, our SRL system consists of
four stages: pruning, argument identification, argument classification, and inference.
In particular, the goal of pruning and argument identification is to identify argument
candidates for a given verb predicate. In the first three stages, however, decisions
are independently made for each argument, and information across arguments is not
260
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
incorporated. The final inference stage allows us to use this type of information along
with linguistic and structural constraints in order to make consistent global predictions.
This system architecture remains unchanged when used for studying the impor-
tance of syntactic parsing in SRL, although different information and features are used.
Throughout this article, when full parsing information is available, we assume that
the system is presented with the full phrase-structure parse tree as defined in the Penn
Treebank (Marcus, Marcinkiewicz, and Santorini 1993) but without trace and functional
tags. On the other hand, when only shallow parsing information is available, the full
parse tree is reduced to only the chunks and the clause constituents.
A chunk is a phrase containing syntactically related words. Roughly speaking,
chunks are obtained by projecting the full parse tree onto a flat tree; hence, they are
closely related to the base phrases. Chunks were not directly defined as part of the
standard annotation of the treebank, but, rather, their definition was introduced in the
CoNLL-2000 shared task on text chunking (Tjong Kim Sang and Buchholz 2000), which
aimed to discover such phrases in order to facilitate full parsing. A clause, on the other
hand, is the clausal constituent as defined by the treebank standard. An example of
chunks and clauses is shown in Figure 1.
3.1 Pruning
When the full parse tree of a sentence is available, only the constituents in the parse
tree are considered as argument candidates. Our system exploits the heuristic rules
introduced by Xue and Palmer (2004) to filter out simple constituents that are very
unlikely to be arguments. This pruning method is a recursive process starting from the
target verb. It first returns the siblings of the verb as candidates; then it moves to the
parent of the verb, and collects the siblings again. The process goes on until it reaches
the root. In addition, if a constituent is a PP (prepositional phrase), its children are also
collected. For example, in Figure 1, if the predicate (target verb) is assume, the pruning
heuristic will output: [PP by John Smith who has been elected deputy chairman], [NP John
Smith who has been elected deputy chairman], [VB be], [MD will], and [NP His duties].
3.2 Argument Identification
The argument identification stage utilizes binary classification to identify whether a
candidate is an argument or not. When full parsing is available, we train and apply
the binary classifiers on the constituents supplied by the pruning stage. When only
shallow parsing is available, the system does not have a pruning stage, and also does
not have constituents to begin with. Therefore, conceptually, the system has to consider
all possible subsequences (i.e., consecutive words) in a sentence as potential argument
candidates. We avoid this by using a learning scheme that utilizes two classifiers, one to
predict the beginnings of possible arguments, and the other the ends. The predictions
are combined to form argument candidates. However, we can employ a simple heuristic
to filter out some candidates that are obviously not arguments. The final predication
includes those that do not violate the following constraints.
1. Arguments cannot overlap with the predicate.
2. If a predicate is outside a clause, its arguments cannot be embedded in
that clause.
3. Arguments cannot exclusively overlap with the clauses.
261
Computational Linguistics Volume 34, Number 2
Figure 1
An example of a parse tree and its predicate?argument structure.
The first constraint comes from the definition of this task that the predicate simply
cannot take itself or any constituents that contain itself as arguments. The other two
constraints are due to the fact that a clause can be treated as a unit that has its own
verb?argument structure. If a verb predicate is outside a clause, then its argument can
only be the whole clause, but may not be embedded in or exclusively overlap with the
clause.
For the argument identification classifier, the features used in full parsing and
shallow parsing settings are all binary features, which are described subsequently.
3.2.1 Features Used When Full Parsing is Available. Most of the features used in our
system are common features for the SRL task. The creation of PropBank was inspired
by the works of Levin (1993) and Levin and Hovav (1996), which discuss the relation
between syntactic and semantic information. Following this philosophy, the features
aim to indicate the properties of the predicate, the constituent which is an argument
candidate, and the relationship between them through the available syntactic infor-
mation. We explain these features herein. For further discussion of these features, we
262
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
refer the readers to the article by Gildea and Jurafsky (2002), which introduced these
features.
 Predicate and POS tag of predicate: indicate the lemma of the predicate
verb and its POS tag.
 Voice: indicates passive/active voice of the predicate.
 Phrase type: provides the phrase type of the constituent, which is the tag
of the corresponding constituent in the parse tree.
 Head word and POS tag of the head word: provides the head word of the
constituent and its POS tag. We use the rules introduced by Collins (1999)
to extract this feature.
 Position: describes if the constituent is before or after the predicate,
relative to the position in the sentence.
 Path: records the tags of parse tree nodes in the traversal path from the
constituent to the predicate. For example, in Figure 1, if the predicate is
assume and the constituent is [S who has been elected deputy chairman], the
path is S?NP?PP?VP?VBN, where ? and ? indicate the traversal direction
in the path.
 Subcategorization: describes the phrase structure around the predicate?s
parent. It records the immediate structure in the parse tree that expands to
its parent. As an example, if the predicate is elect in Figure 1, its
subcategorization is VP?(VBN)-NP while the subcategorization of the
predicate assume is VP?(VBN)-PP. Parentheses indicate the position of the
predicate.
Generally speaking, we consider only the arguments that correspond to some con-
stituents in parse trees. However, in some cases, we need to consider an argument that
does not exactly correspond to a constituent, for example, in our experiment in Sec-
tion 4.2 where the gold-standard boundaries are used with the parse trees generated by
an automatic parse. In such cases, if the information on the constituent, such as phrase
type, needs to be extracted, the deepest constituent that covers the whole argument will
be used. For example, in Figure 1, the phrase type for by John Smith is PP, and its path
feature to the predicate assume is PP?VP?VBN.
We also use the following additional features. These features have been shown
to be useful for the systems by exploiting other information in the absence of the
full parse tree information (Punyakanok et al 2004), and, hence, can be helpful in
conjunction with the features extracted from a full parse tree. They also aim to encode
the properties of the predicate, the constituent to be classified, and their relationship in
the sentence.
 Context words and POS tags of the context words: the feature
includes the two words before and after the constituent, and their
POS tags.
 Verb class: the feature is the VerbNet (Kipper, Palmer, and Rambow 2002)
class of the predicate as described in PropBank Frames. Note that a
263
Computational Linguistics Volume 34, Number 2
verb may inhabit many classes and we collect all of these classes as
features, regardless of the context-specific sense which we do not attempt
to resolve.
 Lengths: of the constituent, in the numbers of words and chunks
separately.
 Chunk: tells if the constituent ?is,? ?embeds,? ?exclusively overlaps,? or
?is embedded in? a chunk with its type. For instance, in Figure 1, if the
constituents are [NP His duties], [PP by John Smith], and [VBN elected], then
their chunk features are ?is-NP,? ?embed-PP & embed-NP,? and
?embedded-in-VP,? respectively.
 Chunk pattern: encodes the sequence of chunks from the constituent to
the predicate. For example, in Figure 1 the chunk sequence from [NP His
duties] to the predicate elect is VP-PP-NP-NP-VP.
 Chunk pattern length: the feature counts the number of chunks in the
chunk pattern feature.
 Clause relative position: encodes the position of the constituent relative
to the predicate in the pseudo-parse tree constructed only from clause
constituents, chunks, and part-of-speech tags. In addition, we label the
clause with the type of chunk that immediately precedes the clause.
This is a simple rule to distinguish the type of clause based on
the intuition that a subordinate clause often modifies the part of the
sentence immediately before it. Figure 2 shows the pseudo-parse
tree of the parse tree in Figure 1. By disregarding the chunks, there
are four configurations??target constituent and predicate are
siblings,? ?target constituent?s parent is an ancestor of predicate,?
?predicate?s parent is an ancestor of target word,? or ?otherwise.?
This feature can be viewed as a generalization of the Path feature
described earlier.
 Clause coverage: describes how much of the local clause from the
predicate is covered by the target argument.
 NEG: the feature is active if the target verb chunk has not or n?t.
 MOD: the feature is active when there is a modal verb in the verb chunk.
The rules of the NEG and MOD features are used in a baseline SRL system
developed by Erik Tjong Kim Sang (Carreras and Ma`rquez 2004).
Figure 2
The pseudo-parse tree generated from the parse tree in Figure 1.
264
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
In addition, we also use the conjunctions of features which conjoin any two features
into a new feature. For example, the conjunction of the predicate and path features
for the predicate assume and the constituent [S who has been elected deputy chairman] in
Figure 1 is (S?NP?PP?VP?VBN, assume).
3.2.2 Features Used When Only Shallow Parsing is Available. Most features used here are
similar to those used by the systemwith full parsing information. However, for features
that need full parse trees in their extraction procedures, we either try to mimic them
with some heuristic rules or discard them. The details of these features are as follows.
 Phrase type: uses a simple heuristic to identify the type of the argument
candidate as VP, PP, or NP.
 Head word and POS tag of the head word: are the rightmost word for
NP, and the leftmost word for VP and PP.
 Shallow-Path: records the traversal path in the pseudo-parse tree.
This aims to approximate the Path features extracted from the full
parse tree.
 Shallow-Subcategorization: describes the chunk and clause structure
around the predicate?s parent in the pseudo-parse tree. This aims to
approximate the Subcategorization feature extracted from the full parse
tree.
3.3 Argument Classification
This stage assigns labels to the argument candidates identified in the previous stage.
A multi-class classifier is trained to predict the types of the argument candidates. In
addition, to reduce the excessive candidates mistakenly output by the previous stage,
the classifier can also label an argument as ?null? (meaning ?not an argument?) to dis-
card it.
The features used here are the same as those used in the argument identification
stage. However, when full parsing is available, an additional feature introduced by Xue
and Palmer (2004) is used.
 Syntactic frame: describes the sequential pattern of the noun phrases and
the predicate in the sentence which aims to complement the Path and
Subcategorization features.
The learning algorithm used for training the argument classifier and argument iden-
tifier is a variation of the Winnow update rule incorporated in SNoW (Roth 1998;
Carlson et al 1999), a multi-class classifier that is tailored for large scale learning tasks.
SNoW learns a sparse network of linear functions, in which the targets (argument
border predictions or argument type predictions, in this case) are represented as linear
functions over a common feature space; multi-class decisions are done via a winner-
take-all mechanism. It improves the basic Winnow multiplicative update rule with a
regularization term, which has the effect of separating the data with a large margin
separator (Dagan, Karov, and Roth 1997; Grove and Roth 2001; Zhang, Damerau, and
Johnson 2002) and voted (averaged) weight vector (Freund and Schapire 1999; Golding
and Roth 1999).
265
Computational Linguistics Volume 34, Number 2
The softmax function (Bishop 1995) is used to convert raw activation to conditional
probabilities. If there are n classes and the raw activation of class i is acti, the posterior
estimation for class i is
Prob(i) = e
acti
?
1?j?n e
actj
Note that in training this classifier, unless specified otherwise, the argument can-
didates used to generate the training examples are obtained from the output of the
argument identifier, not directly from the gold-standard corpus. In this case, we au-
tomatically obtain the necessary examples to learn for class ?null.?
3.4 Inference
In the previous stages, decisions were always made for each argument independently,
ignoring the global information across arguments in the final output. The purpose
of the inference stage is to incorporate such information, including both linguistic
and structural knowledge, such as ?arguments do not overlap? or ?each verb takes
at most one argument of each type.? This knowledge is useful to resolve any incon-
sistencies of argument classification in order to generate final legitimate predictions.
We design an inference procedure that is formalized as a constrained optimization
problem, represented as an integer linear program (Roth and Yih 2004). It takes as
input the argument classifiers? confidence scores for each type of argument, along
with a list of constraints. The output is the optimal solution that maximizes the lin-
ear sum of the confidence scores, subject to the constraints that encode the domain
knowledge.
The inference stage can be naturally extended to combine the output of several
different SRL systems, as we will show in Section 5. In this section we first introduce
the constraints and formalize the inference problem for the semantic role labeling task.
We then demonstrate how we apply integer linear programming (ILP) to generate the
global label assignment.
3.4.1 Constraints over Argument Labeling. Formally, the argument classifiers attempt to
assign labels to a set of arguments, S1:M, indexed from 1 toM. Each argument Si can take
any label from a set of argument labels, P , and the indexed set of arguments can take a
set of labels, c1:M ? PM. If we assume that the classifiers return a score score(Si = ci) that
corresponds to the likelihood of argument Si being labeled ci then, given a sentence, the
unaltered inference task is solved by maximizing the overall score of the arguments,
c?1:M = argmax
c1:M?PM
score(S1:M = c1:M) = argmax
c1:M?PM
M
?
i=1
score(Si = ci) (1)
In the presence of global constraints derived from linguistic information and struc-
tural considerations, our system seeks to output a legitimate labeling that maximizes this
score. Specifically, it can be thought of as if the solution space is limited through the use
of a filter function, F , which eliminates many argument labelings from consideration.
266
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Here, we are concerned with global constraints as well as constraints on the arguments.
Therefore, the final labeling becomes
c?1:M = argmax
c1:M?F (PM )
M
?
i=1
score(Si = ci) (2)
When the confidence scores correspond to the conditional probabilities estimated by
the argument classifiers, the value of the objective function represents the expected
number of correct argument predictions. Hence, the solution of Equation (2) is the one
that maximizes this expected value among all legitimate outputs.
The filter function used considers the following constraints:1
1. Arguments cannot overlap with the predicate.
2. Arguments cannot exclusively overlap with the clauses.
3. If a predicate is outside a clause, its arguments cannot be embedded in
that clause.
4. No overlapping or embedding arguments.
This constraint holds because semantic arguments are labeled on
non-embedding constituents in the syntactic parse tree. In addition, as
defined in the CoNLL-2004 and 2005 shared tasks, the legitimate output of
an SRL system must satisfy this constraint.
5. No duplicate argument classes for core arguments, such as A0?A5 and AA.
The only exception is when there is a conjunction in the sentence. For
example,
[A0 I] [V left ] [A1 my pearls] [A2 to my daughter] and [A1 my gold] [A2 to
my son].
Despite this exception, we treat it as a hard constraint because it almost
always holds.
6. If there is an R-arg argument, then there has to be an arg argument. That is,
if an argument is a reference to some other argument arg, then this
referenced argument must exist in the sentence. This constraint is directly
derived from the definition of R-arg arguments.
7. If there is a C-arg argument, then there has to be an arg argument; in
addition, the C-arg argument must occur after arg. This is stricter than
the previous rule because the order of appearance also needs to be
considered. Similarly, this constraint is directly derived from the definition
of C-arg arguments.
8. Given the predicate, some argument classes are illegal (e.g., predicate
stalk can take only A0 or A1). This information can be found in
PropBank Frames.
1 There are other constraints such as ?exactly one V argument per class,? or ?V?A1?C-V pattern? as
introduced by Punyakanok et al (2004). However, we did not find them particularly helpful in our
experiments. Therefore, we exclude those constraints in the presentation here.
267
Computational Linguistics Volume 34, Number 2
This constraint comes from the fact that different predicates take
different types and numbers of arguments. By checking the
PropBank Frame file of the target verb, we can exclude some core
argument labels.
Note that constraints 1, 2, and 3 are actually implemented in the argument identifi-
cation stage (see Section 3.2). In addition, they need to be explicitly enforced only when
full parsing information is not available because the output of the pruning heuristics
never violates these constraints.
The optimization problem (Equation (2)) can be solved using an ILP solver by
reformulating the constraints as linear (in)equalities over the indicator variables that
represent the truth value of statements of the form [argument i takes label j], as described
in detail next.
3.4.2 Using Integer Linear Programming. As discussed previously, a collection of po-
tential arguments is not necessarily a valid semantic labeling because it may not
satisfy all of the constraints. We enforce a legitimate solution using the following
inference algorithm. In our context, inference is the process of finding the best (ac-
cording to Equation (1)) valid semantic labels that satisfy all of the specified con-
straints. We take a similar approach to the one previously used for entity/relation
recognition (Roth and Yih 2004), and model this inference procedure as solving an ILP
problem.
An integer linear program is a linear program with integral variables. That is,
the cost function and the (in)equality constraints are all linear in terms of the variables.
The only difference in an integer linear program is that the variables can only take
integers as their values. In our inference problem, the variables are in fact binary. A
general binary integer linear programming problem can be stated as follows.
Given a cost vector p ? 
d, a collection of variables u = (u1, . . . ,ud) and cost ma-
trices C1 ? 
c1 ?
d,C2 ? 
c2 ?
d , where c1 and c2 are the numbers of inequality and
equality constraints and d is the number of binary variables, the ILP solution u? is the
vector that maximizes the cost function,
u? = argmax
u?{0,1}d
p ? u
subject to
C1u ? b1, and C2u = b2
where b1 ? 
c1 ,b2 ? 
c2 , and for all u ? {0, 1}d.
To solve the problem of Equation (2) in this setting, we first reformulate the
original cost function
?M
i=1 score(S
i = ci) as a linear function over several binary vari-
ables, and then represent the filter function F using linear inequalities and equalities.
We set up a bijection from the semantic labeling to the variable set u. This is done
by setting u to be a set of indicator variables that correspond to the labels assigned to ar-
guments. Specifically, let uic = [S
i = c] be the indicator variable that represents whether
268
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
or not the argument type c is assigned to Si, and let pic = score(S
i = c). Equation (1) can
then be written as an ILP cost function as
argmax
uic?{0,1}:?i?[1,M],c?P
M
?
i=1
?
c?P
picuic
subject to
?
c?P
uic = 1 ?i ? [1,M]
which means that each argument can take only one type. Note that this new constraint
comes from the variable transformation, and is not one of the constraints used in the
filter function F .
Of the constraints listed earlier, constraints 1 through 3 can be evaluated on a per-
argument basis and, for the sake of efficiency, arguments that violate these constraints
are eliminated even before being given to the argument classifier. Next, we show how to
transform the constraints in the filter function into the form of linear (in)equalities over
u and use them in this ILP setting. For a more complete example of this ILP formulation,
please see Appendix A.
Constraint 4: No overlapping or embedding. If arguments Sj1 , . . . ,Sjk cover the same word
in a sentence, then this constraint ensures that at most one of the arguments is assigned
to an argument type. In other words, at least k? 1 arguments will be the special class
null. If the special class null is represented by the symbol ?, then for every set of such
arguments, the following linear equality represents this constraint.
k
?
i=1
uji? ? k? 1
Constraint 5: No duplicate argument classes. Within the same clause, several types of
arguments cannot appear more than once. For example, a predicate can only take one
A0. This constraint can be represented using the following inequality.
M
?
i=1
uiA0 ? 1
Constraint 6: R-arg arguments. Suppose the referenced argument type is A0 and the
referential type is R-A0. The linear inequalities that represent this constraint are:
?m ? {1, . . . ,M} :
M
?
i=1
uiA0 ? umR-A0
If there are ? referential types, then the total number of inequalities needed is ?M.
Constraint 7: C-arg arguments. This constraint is similar to the reference argument con-
straints. The difference is that the continued argument arg has to occur before C-arg.
269
Computational Linguistics Volume 34, Number 2
Assume that the argument pair is A0 and C-A0, and arguments are sorted by their
beginning positions, i.e., if i < k, the position of the beginning of Sjk is not before that of
the beginning of Sji . The linear inequalities that represent this constraint are:
?m ? {2, . . . ,M} :
m?1
?
i=1
ujiA0 ? ujmC-A0
Constraint 8: Illegal argument types. Given a specific verb, some argument types should
never occur. For example, most verbs do not have arguments A5. This constraint is
represented by summing all the corresponding indicator variables to be 0.
M
?
i=1
uiA5 = 0
Using ILP to solve this inference problem enjoys several advantages. Linear con-
straints are very general, and are able to represent any Boolean constraint (Gue?ret, Prins,
and Sevaux 2002). Table 1 summarizes the transformations of common constraints (most
are Boolean), which are revised from Gue?ret, Prins, and Sevaux (2002), and can be used
for constructing complicated rules.
Previous approaches usually rely on dynamic programming to resolve non-
overlapping/embedding constraints (i.e., Constraint 4) when the constraint structure
is sequential. However, they are not able to handle more expressive constraints
such as those that take long-distance dependencies and counting dependencies into
account (Roth and Yih 2005). The ILP approach, on the other hand, is flexible enough
to handle more expressive and general constraints. Although solving an ILP problem is
NP-hard in the worst case, with the help of today?s numerical packages, this problem
can usually be solved very quickly in practice. For instance, in our experiments it
only took about 10 minutes to solve the inference problem for 4,305 sentences, using
Table 1
Rules of mapping constraints to linear (in)equalities for Boolean variables.
Original constraint Linear form
exactly k of x1, x2, ? ? ? , xn x1 + x2 + ? ? ?+ xn = k
at most k of x1, x2, ? ? ? , xn x1 + x2 + ? ? ?+ xn ? k
at least k of x1, x2, ? ? ? , xn x1 + x2 + ? ? ?+ xn ? k
a ? b a ? b
a = b? a = 1? b
a ? b? a+ b ? 1
a? ? b a+ b ? 1
a ? b a = b
a ? b ? c a ? b and a ? c
a ? b ? c a ? b+ c
b ? c ? a a ? b+ c? 1
b ? c ? a a ? (b+ c)/2
a? at least k of x1, x2, ? ? ? , xn a ? (x1 + x2 + ? ? ?+ xn)/k
At least k of x1, x2, ? ? ? , xn ? a a ? (x1 + x2 + ? ? ?+ xn ? (k? 1))/(n? (k? 1))
270
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Xpress-MP (2004) running on a Pentium-III 800 MHz machine. Note that ordinary
search methods (e.g., beam search) are not necessarily faster than solving an ILP
problem and do not guarantee the optimal solution.
4. The Importance of Syntactic Parsing
We experimentally study the significance of syntactic parsing by observing the effects
of using full parsing and shallow parsing information at each stage of an SRL system.
We first describe, in Section 4.1, how we prepare the data. The comparison of full
parsing and shallow parsing on the first three stages of the process is presented in the
reverse order (Sections 4.2, 4.3, 4.4). Note that in the following sections, in addition
to the performance comparison at various stages, we present also the overall system
performance for the different scenarios. In all cases, the overall system performance is
derived after the inference stage.
4.1 Experimental Setting
We use PropBank Sections 02 through 21 as training data, Section 23 as testing, and
Section 24 as a validation set when necessary. In order to apply the standard CoNLL
shared task evaluation script, our system conforms to both the input and output format
defined in the shared task.
The goal of the experiments in this section is to understand the effective contribu-
tion of full parsing information versus shallow parsing information (i.e., using only the
part-of-speech tags, chunks, and clauses). In addition, we also compare performance
when using the correct (gold-standard) data versus using automatic parse data. The
performance is measured in terms of precision, recall, and the F1 measure. Note that
all the numbers reported here do not take into account the V arguments as it is quite
trivial to predict V and, hence, this gives overoptimistic overall performance if included.
When doing the comparison, we also compute the 95% confidence interval of F1 us-
ing the bootstrap resampling method (Noreen 1989), and the difference is considered
significant if the compared F1 lies outside this interval. The automatic full parse trees
are derived using Charniak?s parser (2001) (version 0.4). In automatic shallow parsing,
the information is generated by different state-of-the-art components, including a POS
tagger (Even-Zohar and Roth 2001), a chunker (Punyakanok and Roth 2001), and a
clauser (Carreras, Ma`rquez, and Castro 2005).
4.2 Argument Classification
To evaluate the performance gap between full parsing and shallow parsing in argument
classification, we assume the argument boundaries are known, and only train classifiers
to classify the labels of these arguments. In this stage, the only difference between the
uses of full parsing and shallow parsing information is the construction of phrase type,
head word, POS tag of the head word, path, subcategorization, and syntactic frame features.
As described in Section 3.2.2, most of these features can be approximated using chunks
and clauses, with the exception of the syntactic frame feature. It is unclear how this
feature can be mimicked because it relies on the internal structure of a full parse tree.
Therefore, it does not have a corresponding feature in the shallow parsing case.
Table 2 reports the experimental results of argument classification when argument
boundaries are known. In this case, because the argument classifier of our SRL system
does not overpredict or miss any arguments, we do not need to train with a null class,
271
Computational Linguistics Volume 34, Number 2
Table 2
The accuracy of argument classification when argument boundaries are known.
Full Parsing Shallow Parsing
Gold 91.50 ? 0.48 90.75 ? 0.45
Auto 90.32 ? 0.48 89.71 ? 0.50
and we can simply measure the performance using accuracy instead of F1. The training
examples include 90,352 propositions with a total of 332,381 arguments. The test data
contain 5,246 propositions and 19,511 arguments. As shown in the table, although the
full-parsing features are more helpful than the shallow-parsing features, the perfor-
mance gap is quite small (0.75% on gold-standard data and 0.61% with the automatic
parsers).
The rather small difference in the performance between argument classifiers using
full parsing and shallow parsing information almost disappears when their output is
processed by the inference stage. Table 3 shows the final results in recall, precision, and
F1, when the argument boundaries are known. In all cases, the differences in F1 between
the full parsing?based and the shallow parsing?based systems are not statistically
significant.
Conclusion. When the argument boundaries are known, the performance of the full
parsing?based SRL system is about the same as the shallow parsing?based SRL system.
4.3 Argument Identification
Argument identification is an important stage that effectively reduces the number of
argument candidates after the pruning stage. Given an argument candidate, an argu-
ment identifier is a binary classifier that decides whether or not the candidate should be
considered as an argument. To evaluate the influence of full parsing information in this
stage, the candidate list used here is the outputs of the pruning heuristic applied on the
gold-standard parse trees. The heuristic results in a total number of 323,155 positive and
686,887 negative examples in the training set, and 18,988 positive and 39,585 negative
examples in the test set.
Similar to the argument classification stage, the only difference between full
parsing? and shallow parsing?based systems is in the construction of some features.
Specifically, phrase type, head word, POS tag of the head word, path, and subcategorization
features are approximated using chunks and clauses when the binary classifier is trained
using shallow parsing information.
Table 4 reports the performance of the argument identifier on the test set using
the direct predictions of the trained binary classifier. The recall and precision of the
Table 3
The overall system performance when argument boundaries are known.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 91.58 91.90 91.74 ? 0.51 91.14 91.48 91.31 ? 0.51
Auto 90.71 91.14 90.93 ? 0.53 90.50 90.88 90.69 ? 0.53
272
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 4
The performance of argument identification after pruning (based on the gold standard full parse
trees).
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 96.53 93.57 95.03 ? 0.32 93.66 91.72 92.68 ? 0.38
Auto 94.68 90.60 92.59 ? 0.39 92.31 88.36 90.29 ? 0.43
full parsing?based system are around 2 to 3 percentage points higher than the shallow
parsing?based system on the gold-standard data. As a result, the F1 score is 2.5 percent-
age points higher. The performance on automatic parse data is unsurprisingly lower
but the difference between the full parsing? and the shallow parsing?based systems is
as observed previously. In terms of filtering efficiency, around 25% of the examples are
predicted as positive. In other words, both argument identifiers filter out around 75%
of the argument candidates after pruning.
Because the recall in the argument identification stage sets the upper-bound the
recall in argument classification, the threshold that determines when examples are
predicted to be positive is usually lowered to allow more positive predictions. That
is, a candidate is predicted as positive when its probability estimation is larger than
the threshold. Table 5 shows the performance of the argument identifiers when the
threshold is 0.1.2
Because argument identification is just an intermediate step in a complete system,
a more realistic evaluation method is to see how each final system performs. Using an
argument identifier with threshold = 0.1 (i.e., Table 5), Table 6 reports the final results
in recall, precision, and F1. The F1 difference is 1.5 points when using the gold-standard
data. However, when automatic parsers are used, the shallow parsing?based system is,
in fact, slightly better; although the difference is not statistically significant. This may be
due to the fact that chunk and clause predictions are very important here, and shallow
parsers are more accurate in chunk or clause predictions than a full parser (Li and Roth
2001).
Conclusion. Full parsing information helps in argument identification. However, when
the automatic parsers are used, using the full parsing information may not have better
overall results compared to using shallow parsing.
4.4 Pruning
As shown in the previous two sections, the overall performance gaps of full parsing and
shallow parsing are small. When automatic parsers are used, the difference is less than 1
point in F1 or accuracy. Therefore, we conclude that themain contribution of full parsing
is in the pruning stage. Because the shallow parsing system does not have enough in-
formation for the pruning heuristics, we train two word-based classifiers to replace the
pruning stage. One classifier is trained to predict whether a given word is the start (S) of
2 The value was determined by experimenting with the complete system using automatic full parse trees,
on the development set. In our tests, lowering the threshold in argument identification always leads to
higher overall recall and lower overall precision. As a result, the gain in F1 is limited.
273
Computational Linguistics Volume 34, Number 2
Table 5
The performance of argument identification after pruning (based on the gold-standard full parse
trees) and with threshold = 0.1.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 92.13 95.62 93.84 ? 0.37 88.54 94.81 91.57 ? 0.42
Auto 89.48 94.14 91.75 ? 0.41 86.14 93.21 89.54 ? 0.47
Table 6
The overall system performance using the output from the pruning heuristics, applied on the
gold-standard full parse trees.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 86.22 87.40 86.81 ? 0.59 84.14 85.31 84.72 ? 0.63
Auto 84.21 85.04 84.63 ? 0.63 86.17 84.02 85.08 ? 0.63
an argument; the other classifier is to predict the end (E) of an argument. If the product
of probabilities of a pair of S and E predictions is larger than a predefined threshold,
then this pair is considered as an argument candidate. The threshold used here was
obtained by using the validation set. Both classifiers use very similar features to those
used by the argument identifier as explained in Section 3.2, treating the target word as
a constituent. Particularly, the features are predicate, POS tag of the predicate, voice,
context words, POS tags of the context words, chunk pattern, clause relative position,
and shallow-path. The headword and its POS tag are replaced by the target word and its
POS tag. The comparison of using the classifiers and the heuristics is shown in Table 7.
Even without the knowledge of the constituent boundaries, the classifiers seem
surprisingly better than the pruning heuristics. Using either the gold-standard data set
or the output of automatic parsers, the classifiers achieve higher F1 scores. One possible
reason for this phenomenon is that the accuracy of the pruning strategy is limited by
the number of agreements between the correct arguments and the constituents of the
parse trees. Table 8 summarizes the statistics of the examples seen by both strategies.
The pruning strategy needs to decide which are the potential arguments among all con-
stituents. This strategy is upper-bounded by the number of correct arguments that agree
with some constituent. On the other hand, the classifiers do not have this limitation. The
number of examples they observe is the total number of words to be processed, and the
positive examples are those arguments that are annotated as such in the data set.
Table 7
The performance of pruning using heuristics and classifiers.
Full Parsing Classifier Threshold = 0.04
Prec Rec F1 Prec Rec F1
Gold 25.94 97.27 40.96 ? 0.51 29.58 97.18 45.35 ? 0.83
Auto 22.79 86.08 36.04 ? 0.52 24.68 94.80 39.17 ? 0.79
274
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 8
Statistics of the training and test examples for the pruning stage.
Words Arguments Constituents Agreements
Gold Auto Gold Auto
Train 2,575,665 332,381 4,664,954 4,263,831 327,603 319,768
Test 147,981 19,511 268,678 268,482 19,266 18,301
The Agreements column shows the number of arguments that match the boundaries of some
constituents.
Note that because each verb is processed independently, a sentence is processed
once for each verb in the sentence. Therefore, the words and constituents in each
sentence are counted as many times as the number of verbs to be processed.
As before, in order to compare the systems that use full parsing and shallow
parsing information, we need to see the impact on the overall performance. There-
fore, we built two semantic role systems based on full parsing and shallow parsing
information. The full parsing?based system follows the pruning, argument identifica-
tion, argument classification, and inference stages, as described earlier. For the shallow
parsing system, the pruning heuristic is replaced by the word-based pruning classi-
fiers, and the remaining stages are designed to use only shallow parsing as described in
previous sections. Table 9 shows the overall performance of the two evaluation systems.
As indicated in the tables, the gap in F1 between full parsing and shallow parsing?
based systems enlarges tomore than 11 points on the gold-standard data. At first glance,
this result seems to contradict our conclusion in Section 4.3. After all, if the pruning
stage of shallow parsing SRL system performs equally well or even better, the overall
performance gap in F1 should be small.
After we carefully examined the output of the word-based classifier, we realized
that it filters out easy candidates, and leaves examples that are difficult to the later
stages. Specifically, these argument candidates often overlap and differ only in one or
twowords. On the other hand, the pruning heuristic based on full parsing never outputs
overlapping candidates and consequently provides input that is easier for the next stage
to handle. Indeed, the following argument identification stage turns out to be good in
discriminating these non-overlapping candidates.
Conclusion. The most crucial contribution of full parsing is in the pruning stage. The
internal tree structure significantly helps in discriminating argument candidates, which
makes the work done by the following stages easier.
Table 9
The overall system performance.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 86.22 87.40 86.81 ? 0.59 75.34 75.28 75.31 ? 0.76
Auto 77.09 75.51 76.29 ? 0.76 75.48 67.13 71.06 ? 0.80
275
Computational Linguistics Volume 34, Number 2
5. The Effect of Inference
Our inference procedure plays an important role in improving accuracy when the local
predictions violate the constraints among argument labels. In this section, we first
present the overall system performance when most constraints are not used. We then
demonstrate how the inference procedure can be used to combine the output of several
systems to yield better performance.
5.1 Inference with Limited Constraints
The inference stage in our system architecture provides a principled way to resolve
conflicting local predictions. It is interesting to see whether this procedure improves the
performance differently for the full parsing? vs. the shallow parsing?based system, as
well as gold-standard vs. automatic parsing input.
Table 10 shows the results of using only constraints 1, 2, 3, and 4. As mentioned
previously, the first three constraints are handled before the argument classification
stage. Constraint 4, which forbids overlapping or embedding arguments, is required
in order to use the official CoNLL-2005 evaluation script and is therefore kept.
By comparing Table 9 with Table 10, we can see that the effect of adding more
constraints is quite consistent over the four settings. Precision is improved by 1 to 2 per-
centage points but recall is decreased a little. As a result, the gain in F1 is about 0.5 to 1
point. It is not surprising to see this lower recall and higher precision phenomenon after
the constraints described in Section 3.4.1 are examined. Most constraints punish false
non-null output, but do not regulate false null predictions. For example, an assignment
that has two A1 arguments clearly violates the non-duplication constraint. However, if
an assignment has no predicted arguments at all, it still satisfies all the constraints.
5.2 Joint Inference
The empirical study in Section 4 indicates that the performance of an SRL system
primarily depends on the very first stage?pruning, which is directly derived from
the full parse trees. This also means that in practice the quality of the syntactic parser
is decisive to the quality of the SRL system. To improve semantic role labeling, one
possible way is to combine different SRL systems through a joint inference stage, given
that the systems are derived using different full parse trees.
To test this idea, we first build two SRL systems that use Collins?s parser (Collins
1999)3 and Charniak?s parser (Charniak 2001), respectively. In fact, these two parsers
have noticeably different outputs. Applying the pruning heuristics on the output of
Collins?s parser produces a list of candidates with 81.05% recall. Although this number
is significantly lower than the 86.08% recall produced by Charniak?s parser, the union
of the two candidate lists still significantly improves recall to 91.37%. We construct the
two systems by implementing the first three stages, namely, pruning, argument identifi-
cation, and argument classification. When a test sentence is given, a joint inference stage
is used to resolve the inconsistency of the output of argument classification in these two
systems.
We first briefly review the objective function used in the inference procedure in-
troduced in Section 3.4. Formally speaking, the argument classifiers attempt to assign
3 We use the Collins parser implemented by Bikel (2004).
276
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 10
The impact of removing most constraints in overall system performance.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 85.07 87.50 86.27 ? 0.58 73.19 75.63 74.39 ? 0.75
Auto 75.88 75.81 75.84 ? 0.75 73.56 67.45 70.37 ? 0.80
labels to a set of arguments, S1:M, indexed from 1 toM. Each argument Si can take any
label from a set of argument labels, P , and the indexed set of arguments can take a
set of labels, c1:M ? PM. If we assume that the argument classifier returns an estimated
conditional probability distribution, Prob(Si = ci), then, given a sentence, the inference
procedure seeks a global assignment that maximizes the objective function denoted by
Equation (2), which can be rewritten as follows,
c?1:M = argmax
c1:M?F (PM )
M
?
i=1
Prob(Si = ci) (3)
where the linguistic and structural constraints are represented by the filter F . In other
words, this objective function reflects the expected number of correct argument predic-
tions, subject to the constraints.
When there are two or more argument classifiers from different SRL systems, a joint
inference procedure can take the output estimated probabilities for all these candidates
as input, although some candidates may refer to the same phrases in the sentence. For
example, Figure 3 shows the two candidate sets for a fragment of a sentence, ..., traders
say, unable to cool the selling panic in both stocks and futures. In this example, system A has
two argument candidates, a1 = traders and a4 = the selling panic in both stocks and futures;
system B has three argument candidates, b1 = traders, b2 = the selling panic, and b3 = in
both stocks and futures.
A straightforward solution to the combination is to treat each argument produced
by a system as a possible output. Each possible labeling of the argument is associated
with a variable which is then used to set up the inference procedure. However, the final
predictionwill be likely dominated by the system that producesmore candidates, which
is system B in this example. The reason is that our objective function is the sum of the
probabilities of all the candidate assignments.
This bias can be corrected by the following observation. Although system A only
has two candidates, a1 and a4, it can be treated as if it also has two additional phantom
candidates, a2 and a3, where a2 and b2 refer to the same phrase, and so do a3 and b3.
Similarly, system B has a phantom candidate b4 that corresponds to a4. Because systemA
does not really generate a2 and a3, we can assume that these two phantom candidates are
predicted by it as ?null? (i.e., not an argument). We assign the same prior distribution to
each phantom candidate. In particular, the probability of the ?null? class is set to be 0.55
based on empirical tests, and the probabilities of the remaining classes are set based on
their occurrence frequencies in the training data.
Then, we treat each possible final argument output as a single unit. Each probability
estimation by a system can be viewed as evidence in the final probability estimation and,
therefore, we can simply average their estimation. Formally, let Si be the argument set
277
Computational Linguistics Volume 34, Number 2
Figure 3
The output of two SRL systems: system A has two candidates, a1 = traders and a4 = the selling
panic in both stocks and futures; system B has three argument candidates, b1 = traders, b2 = the
selling panic, and b3 = in both stocks and futures. In addition, we create two phantom candidates a2
and a3 for system A that correspond to b2 and b3 respectively, and b4 for system B that
corresponds to a4.
output by system i, and S =
?k
i=1 Si be the set of all arguments where k is the number
of systems; let N be the cardinality of S. Our augmented objective function is then:
c?1:N = argmax
c1:N?F (PN )
N
?
i=1
Prob(Si = ci) (4)
where Si ? S, and
Prob(Si = ci) = 1
k
k
?
j=1
Probj(S
i = ci) (5)
where Probj is the probability output by system j.
Note that we may also treat the individual systems differently by applying different
priors (i.e., weights) on the estimated probabilities of the argument candidates. For
example, if the performance of system A is much better than system B, then we may
want to trust system A?s output more by multiplying the output probabilities by a
larger weight.
Table 11 reports the performance of two individual systems based on Collins?s
parser and Charniak?s parser, as well as the joint system, where the two individual
systems are equally weighted. The joint system based on this straightforward strategy
significantly improves the performance compared to the two original SRL systems in
both recall and precision, and thus achieves a much higher F1.
6. Empirical Evaluation?CoNLL Shared Task 2005
In this section, we present the detailed evaluation of our SRL system, in the competi-
tion on semantic role labeling?the CoNLL-2005 shared task (Carreras and Ma`rquez
278
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 11
The performance of individual and combined SRL systems.
Prec Rec F1
Collins? parser 75.92 71.45 73.62 ? 0.79
Charniak?s parser 77.09 75.51 76.29 ? 0.76
Combined result 80.53 76.94 78.69 ? 0.71
2005). The setting of this shared task is basically the same as it was in 2004, with
some extensions. First, it allows much richer syntactic information. In particular, full
parse trees generated using Collins?s parser (Collins 1999) and Charniak?s parser
(Charniak 2001) were provided. Second, the full parsing standard partition was used?
the training set was enlarged and covered Sections 02?21, the development set was
Section 24, and the test set was Section 23. Finally, in addition to the Wall Street Journal
(WSJ) data, three sections of the Brown corpus were used to provide cross-corpora
evaluation.
The system we used to participate in the CoNLL-2005 shared task is an enhanced
version of the system described in Sections 3 and 5. The main difference was that
the joint-inference stage was extended to combine six basic SRL systems instead of
two. Specifically for this implementation, we first trained two SRL systems that use
Collins?s parser and Charniak?s parser, respectively, because of their noticeably dif-
ferent outputs. In evaluation, we ran the system that was trained with Charniak?s
parser five times, with the top-5 parse trees output by Charniak?s parser. Together we
have six different outputs per predicate. For each parse tree output, we ran the first
three stages, namely, pruning, argument identification, and argument classification.
Then, a joint-inference stage, where each individual system is weighted equally, was
used to resolve the inconsistency of the output of argument classification in these
systems.
Table 12 shows the overall results on the development set and different test sets; the
detailed results on WSJ section 23 are shown in Table 13. Table 14 shows the results of
individual systems and the improvement gained by the joint inference procedure on the
development set.
Our system reached the highest F1 scores on all the test sets and was the best system
among the 19 participating teams. After the competition, we improved the system
slightly by tuning the weights of the individual systems in the joint inference procedure,
where the F1 scores onWSJ test section and the Brown test set are 79.59 points and 67.98
points, respectively.
Table 12
Overall CoNLL-2005 shared task results.
Prec. Rec. F1
Development 80.05 74.83 77.35
Test WSJ 82.28 76.78 79.44
Test Brown 73.38 62.93 67.75
Test WSJ+Brown 81.18 74.92 77.92
279
Computational Linguistics Volume 34, Number 2
Table 13
Detailed CoNLL-2005 shared task results on the WSJ test set.
Test WSJ Prec. Rec. F1
Overall 82.28 76.78 79.44
A0 88.22 87.88 88.05
A1 82.25 77.69 79.91
A2 78.27 60.36 68.16
A3 82.73 52.60 64.31
A4 83.91 71.57 77.25
AM-ADV 63.82 56.13 59.73
AM-CAU 64.15 46.58 53.97
AM-DIR 57.89 38.82 46.48
AM-DIS 75.44 80.62 77.95
AM-EXT 68.18 46.88 55.56
AM-LOC 66.67 55.10 60.33
AM-MNR 66.79 53.20 59.22
AM-MOD 96.11 98.73 97.40
AM-NEG 97.40 97.83 97.61
AM-PNC 60.00 36.52 45.41
AM-TMP 78.16 76.72 77.44
R-A0 89.72 85.71 87.67
R-A1 70.00 76.28 73.01
R-A2 85.71 37.50 52.17
R-AM-LOC 85.71 57.14 68.57
R-AM-TMP 72.34 65.38 68.69
In terms of the computation time, for both the argument identifier and the argument
classifier, the training of each model, excluding feature extraction, takes 50?70 minutes
using less than 1GB memory on a 2.6GHz AMD machine. On the same machine, the
average test time for each stage, excluding feature extraction, is around 2 minutes.
7. Related Work
The pioneering work on building an automatic semantic role labeler was proposed
by Gildea and Jurafsky (2002). In their setting, semantic role labeling was treated as a
tagging problem on each constituent in a parse tree, solved by a two-stage architecture
consisting of an argument identifier and an argument classifier. This is similar to our
Table 14
The results of individual systems and the result with joint inference on the development set.
Prec. Rec. F1
Charniak-1 75.40 74.13 74.76
Charniak-2 74.21 73.06 73.63
Charniak-3 73.52 72.31 72.91
Charniak-4 74.29 72.92 73.60
Charniak-5 72.57 71.40 71.98
Collins 73.89 70.11 71.95
Joint inference 80.05 74.83 77.35
280
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
main architecture with the exclusion of the pruning and inference stages. There are
two additional key differences between their system and ours. First, their system
used a back-off probabilistic model as its main engine. Second, it was trained on
FrameNet (Baker, Fillmore, and Lowe 1998)?another large corpus, besides PropBank,
that contains selected examples of semantically labeled sentences.
Later that year, the same approach was applied on PropBank by Gildea and Palmer
(2002). Their system achieved 57.7% precision and 50.0% recall with automatic parse
trees, and 71.1% precision and 64.4% recall with gold-standard parse trees. It is worth
noticing that at that time the PropBank project was not finished and the data set
available was only a fraction in size of what it is today. Since these pioneering works, the
task has gained increasing popularity and created a new line of research. The two-step
constituent-by-constituent architecture became a common blueprint for many systems
that followed.
Partly due to the expansion of the PropBank dataset, researchers have gradually
made improvement on the performance of automatic SRL systems by using new tech-
niques and new features. Some of the early systems are described in Chen and Rambow
(2003), Gildea and Hockenmaier (2003), and Surdeanu et al (2003). All are based on a
two-stage architecture similar to the one proposed by Gildea and Palmer (2002) with
the differences in the machine-learning techniques and the features used. The first
breakthrough in terms of performance was due to Pradhan et al (2003), who first
viewed the task as a massive classification problem and applied multiple SVMs to it.
Their final result (after a few more improvements) reported in Pradhan et al (2004)
achieved 84% and 75% in precision and recall, respectively.
A second significant contribution beyond the two-stage architecture is due to Xue
and Palmer (2004), who introduced the pruning heuristics to the two-stage architecture,
and remarkably reduced the number of candidate arguments a system needs to con-
sider; this approach was adopted by many systems. Another significant advancement
was in the realization that global information can be exploited and benefits the results
significantly. Inference based on an integer linear programming technique, which was
originally introduced by Roth and Yih (2004) on a relation extraction problem, was
first applied to the SRL problem by Punyakanok et al (2004). It showed that domain
knowledge can be easily encoded and contributes significantly through inference over
the output of classifiers. The idea of exploiting global information, which is detailed in
this paper, was pursued later by other researchers, in different forms.
Besides the constituent-by-constituent based architecture, others have also been
explored. The alternative frameworks include representing semantic role labeling as
a sequence-tagging problem (Ma`rquez, Pere Comas, and Catala` 2005) and tagging the
edges in the corresponding dependency trees (Hacioglu 2004). However, the most pop-
ular architecture by far is the constituent-by-constituent based multi-stage architecture,
perhaps due to its conceptual simplicity and its success. In the CoNLL-2005 shared
task competition (Carreras and Ma`rquez 2005), the majority of the systems followed
the constituent-by-constituent based two-stage architecture, and the use of the pruning
heuristics was also fairly common.
The CoNLL-2005 shared task also highlighted the importance of system combina-
tion, such as our ILP technique when used in joint inference, in order to achieve superior
performance. The top four systems, which produced significantly better results than the
rest, all used some schemes to combine the output of several SRL systems, ranging from
using a fixed combination function (Haghighi, Toutanova, and Manning 2005; Koomen
et al 2005) to using a machine-learned combination strategy (Ma`rquez, Pere Comas,
and Catala` 2005; Pradhan, Hacioglu, Ward et al 2005).
281
Computational Linguistics Volume 34, Number 2
The work of Gildea and Palmer (2002) pioneered not only the fundamental archi-
tecture of SRL, but was also the first to investigate the interesting question regarding
the significance of using full parsing for high quality SRL. They compared their full
system with another system that only used chunking, and found that the chunk-based
system performed much worse. The precision and recall dropped from 57.7% and
50.0% to 27.6% and 22.0%, respectively. That led to the conclusion that full parsing
information is necessary to solving the SRL problem, especially at the stage of argu-
ment identification?a finding that is quite similar to ours in this article. However,
their chunk-based approach was very weak?only chunks were considered as possible
candidates; hence, it is not very surprising that the boundaries of the arguments could
not be reliably found. In contrast, our shallow parse?based system does not have these
restrictions on the argument boundaries and therefore performs much better at this
stage, providing a more fair comparison.
A related comparison can be found also in the work by Pradhan, Hacioglu, Krugler
et al (2005) (their earlier version appeared in Pradhan et al [2003]), which reported
the performance on several systems using different information sources and system
architectures. Their shallow parse?based system is modeled as a sequence tagging prob-
lem while the full system is a constituent-by-constituent based two-stage system. Due
to technical difficulties, though, they reported the results of the chunk-based systems
only on a subset of the full data set. Their shallow parse?based system achieved 60.4%
precision and 51.4% recall and their full system achieved 80.6% precision and 67.1%
recall on the same data set (but 84% precision and 75% recall with the full data set).
Therefore, due to the use of different architectures and data set sizes, the questions
of ?how much one can gain from full parsing over shallow parsing when using the
full PropBank data set? and ?what are the sources of the performance gain? were left
open.
Similarly, in the CoNLL-2004 shared task (Carreras andMa`rquez 2004), participants
were asked to develop SRL systems with the restriction that only shallow parsing infor-
mation (i.e., chunks and clauses) were allowed. The performance of the best systemwas
at 72.43% precision and 66.77% recall, which was about 10 points in F1 lower than the
best system based on full parsing in the literature. However, the training examples were
derived from only 5 sections and not all the 19 sections usually used in the standard
setting. Hence, the question was not yet fully answered.
Our experimental study, on the other hand, is done with a consistent architecture,
by considering each stage in a controlled manner, and using the full data set, allowing
one to draw direct conclusions regarding the impact of this information source.
8. Conclusion
This paper studies the important task of semantic role labeling. We presented an ap-
proach to SRL and a principled and general approach to incorporating global informa-
tion in natural language decisions. Beyond presenting this approach which leads to a
state-of-the-art SRL system, we focused on investigating the significance of using full
parse tree information as input to an SRL system adhering to the most common system
architecture, and the stages in the process where this information has the most impact.
We performed a detailed and fair experimental comparison between shallow and full
parsing information and concluded that, indeed, full syntactic information can improve
the performance of an SRL system. In particular, we have shown that this information
is most crucial in the pruning stage of the system, and relatively less important in the
following stages.
282
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
In addition, we showed the importance of global inference to good performance in
this task, characterized by rich structural and linguistic constraints among the predicted
labels of the arguments. Our integer linear programming?based inference procedure
is a powerful and flexible optimization strategy that finds the best solution subject to
these constraints. As we have shown, it can be used to resolve conflicting argument
predictions in an individual system but can also serve as an effective and simple
approach to combining different SRL systems, resulting in a significant improvement
in performance.
In the future, we plan to extend our work in several directions. By adding more
constraints to the inference procedure, an SRL system may be further improved.
Currently, the constraints are provided by human experts in advance. Learning both
hard and statistical constraints from the data will be our top priority. Some work on
combining statistical and declarative constraints has already started and is reported
in Roth and Yih (2005). Another issue we want to address is domain adaptation.
It has been clearly shown in the CoNLL-2005 shared task that the performance of
current SRL systems degrades significantly when tested on a corpus different from
the one used in training. This may be due to the underlying components, especially
the syntactic parsers which are very sensitive to changes in data genre. Developing
a better model that more robustly combines these components could be a promising
direction. In addition, although the shallow parsing?based system was shown here to
be inferior, shallow parsers were shown to be more robust than full parsers (Li and
Roth 2001). Therefore, combining these two systems may bring forward both of their
advantages.
Appendix A: An ILP Formulation for SRL
In this section, we show a complete example of the ILP formulation formulated to solve
the inference problem as described in Section 3.4.
Example. Assume the sentence is four words long with the following argument
candidates, and the following illegal argument types for the predicate of interest.
Sentence: w1 w2 w3 w4
Candidates: [ S1 ] [ S2 ] [ S3 ] [ S5 ]
[ S4 ]
Illegal argument types: A3, A4, A5
Indicator Variables and Their Costs. The followings are the indicator variables and their
associated costs set up for the example.
Indicator Variables:
u1A0,u1A1, . . . ,u1AM-LOC, . . . ,u1C-A0, . . . ,u1R-A0, . . . ,u1?
u2A0,u2A1, . . . ,u2AM-LOC, . . . ,u2C-A0, . . . ,u2R-A0, . . . ,u2?
...
u5A0,u5A1, . . . ,u5AM-LOC, . . . ,u5C-A0, . . . ,u5R-A0, . . . ,u5?
Costs:
p1A0, p1A1, . . . , p1AM-LOC, . . . , p1C-A0, . . . , p1R-A0, . . . , p1?
p2A0, p2A1, . . . , p2AM-LOC, . . . , p2C-A0, . . . , p2R-A0, . . . , p2?
...
p5A0, p5A1, . . . , p5AM-LOC, . . . , p5C-A0, . . . , p5R-A0, . . . , p5?
283
Computational Linguistics Volume 34, Number 2
Objective Function. The objective function can be written as the following.
argmaxuic?{0,1}:?i?[1,5],c?P
?5
i=1
?
c?P picuic
where
P = {A0,A1, . . . , AM-LOC, . . . , C-A0, . . . , R-A0, . . . ,?}
subject to
u1A0 + u1A1 + . . .+ u1AM-LOC + . . .+ u1C-A0 + . . .+ u1R-A0 + . . .+ u1? = 1
u2A0 + u2A1 + . . .+ u2AM-LOC + . . .+ u2C-A0 + . . .+ u2R-A0 + . . .+ u2? = 1
...
u5A0 + u5A1 + . . .+ u5AM-LOC + . . .+ u2C-A0 + . . .+ u5R-A0 + . . .+ u5? = 1
Additional Constraints. The rest of the constraints can be formulated as the following.
Constraint 4: No overlapping or embedding
u3? + u4? ? 1
u4? + u5? ? 1
Constraint 5: No duplicate argument classes
u1A0 + u2A0 + . . .+ u5A0 ? 1
u1A1 + u2A1 + . . .+ u5A1 ? 1
u1A2 + u2A2 + . . .+ u5A2 ? 1
Constraint 6: R-arg arguments
u1A0 + u2A0 + . . .+ u5A0 ? u1R-A0
u1A0 + u2A0 + . . .+ u5A0 ? u2R-A0
...
u1A0 + u2A0 + . . .+ u5A0 ? u5R-A0
u1A1 + u2A1 + . . .+ u5A1 ? u1R-A1
...
u1AM-LOC + u2AM-LOC + . . .+ u5AM-LOC ? u1R-AM-LOC
...
Constraint 7: C-arg arguments
u1A0 ? u2C-A0
u1A0 + u2A0 ? u3C-A0
...
u1A0 + u2A0 + . . .+ u4A0 ? u5C-A0
u1A1 ? u2C-A1
...
u1AM-LOC ? u2C-AM-LOC
...
284
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Constraint 8: Illegal argument types
u1A3 + u2A3 + . . .+ u5A3 = 0
u1A4 + u2A4 + . . .+ u5A4 = 0
u1A5 + u2A5 + . . .+ u5A5 = 0
Acknowledgments
We thank Xavier Carreras and Llu??s Ma`rquez
for the data and scripts, Szu-ting Yi for her
help in improving our joint inference
procedure, and Nick Rizzolo as well as the
anonymous reviewers for their comments
and suggestions. We are also grateful to Dash
Optimization for the free academic use of
Xpress-MP and AMD for their equipment
donation. This research is supported by the
Advanced Research and Development
Activity (ARDA)?s Advanced Question
Answering for Intelligence (AQUAINT)
Program, a DOI grant under the Reflex
program, NSF grants ITR-IIS-0085836,
ITR-IIS-0085980, and IIS-9984168,
EIA-0224453, and an ONR MURI Award.
References
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley Framenet
project. In Proceedings of COLING-ACL,
pages 86?90, Montreal, Canada.
Bikel, Daniel M. 2004. Intricacies of Collins?
parsing model. Computational Linguistics,
30(4):479?511.
Bishop, Christopher M., 1995. Neural
Networks for Pattern Recognition, chapter
6.4: Modelling conditional distributions,
page 215. Oxford University Press,
Oxford, UK.
Carlson, Andrew J., Chad M. Cumby, Jeff L.
Rosen, and Dan Roth. 1999. The SNoW
learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer
Science Department.
Carreras, Xavier and Llu?is Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
tasks: Semantic role labeling. In Proceedings
of CoNLL-2004, pages 89?97, Boston, MA.
Carreras, Xavier and Llu?is Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 152?164, Ann Arbor, MI.
Carreras, Xavier, Llu?is Ma`rquez, and Jorge
Castro. 2005. Filtering?ranking perceptron
learning for partial parsing.Machine
Learning, 60:41?71.
Carreras, Xavier, Llu?is Ma`rquez, Vasin
Punyakanok, and Dan Roth. 2002.
Learning and inference for clause
identification. In Proceedings of the 13th
European Conference on Machine Learning
(ECML-2002), pages 35?47, Helsinki,
Finland.
Charniak, Eugene. 2001. Immediate-head
parsing for language models. In
Proceedings of the 39th Annual Meeting of the
Association of Computational Linguistics
(ACL-2001), pages 116?123, Toulouse,
France.
Chen, John and Owen Rambow. 2003. Use of
deep linguistic features for the recognition
and labeling of semantic arguments. In
Proceedings of the 2003 Conference on
Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 41?48,
Sapporo, Japan.
Collins, Michael. 1999. Head-driven
Statistical Models for Natural Language
Parsing. Ph.D. thesis, Computer Science
Department, University of Pennsylvania,
Philadelphia, PA.
Dagan, Ido, Yael Karov, and Dan Roth.
1997. Mistake-driven learning in text
categorization. In Proceedings of the
Second Conference on Empirical Methods
in Natural Language Processing
(EMNLP-1997), pages 55?63,
Providence, RI.
Even-Zohar, Yair and Dan Roth. 2001. A
sequential model for multi-class
classification. In Proceedings of the 2001
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2001),
pages 10?19, Pittsburgh, PA.
Freund, Yoav and Robert E. Schapire. 1999.
Large margin classification using the
Perceptron algorithm.Machine Learning,
37(3):277?296.
Gildea, Daniel and Julia Hockenmaier. 2003.
Identifying semantic roles using
combinatory categorial grammar. In
Proceedings of the 2003 Conference on
Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 57?64,
Sapporo, Japan.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
285
Computational Linguistics Volume 34, Number 2
Gildea, Daniel and Martha Palmer. 2002.
The necessity of parsing for predicate
argument recognition. In Proceedings
of the 40th Annual Meeting of the
Association of Computational Linguistics
(ACL-2002), pages 239?246,
Philadelphia, PA.
Golding, Andrew R. and Dan Roth. 1999.
A Winnow based approach to
context-sensitive spelling correction.
Machine Learning, 34(1-3):107?130.
Grove, Adam J. and Dan Roth. 2001. Linear
concepts and hidden variables.Machine
Learning, 42(1?2):123?141.
Gue?ret, Christelle, Christian Prins, and Marc
Sevaux. 2002. Applications of Optimization
with Xpress-MP. Dash Optimization.
Translated and revised by Susanne
Heipcke. http://www.dashoptimization.
com/home/downloads/book/booka4.pdf.
Hacioglu, Kadri. 2004. Semantic role labeling
using dependency trees. In Proceedings of
the 20th International Conference on
Computational Linguistics (COLING),
Geneva, Switzerland.
Hacioglu, Kadri, Sameer Pradhan, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2004. Semantic role labeling by
tagging syntactic chunks. In Proceedings of
CoNLL-2004, pages 110?113, Boston, MA.
Haghighi, Aria, Kristina Toutanova, and
Christopher D. Manning. 2005. A joint
model for semantic role labeling. In
Proceedings of the Ninth Conference on
Computational Natural Language
Learning (CoNLL-2005), pages 173?176,
Ann Arbor, MI.
Kingsbury, Paul and Martha Palmer. 2002.
From Treebank to PropBank. In Proceedings
of LREC-2002, Las Palmas, Canary Islands,
Spain.
Kipper, Karin, Martha Palmer, and Owen
Rambow. 2002. Extending PropBank with
VerbNet semantic predicates. In
Proceedings of Workshop on Applied
Interlinguas, Tiburon, CA.
Koomen, Peter, Vasin Punyakanok, Dan
Roth, and Wen-tau Yih. 2005. Generalized
inference with multiple semantic role
labeling systems. In Proceedings of the Ninth
Conference on Computational Natural
Language Learning (CoNLL-2005),
pages 181?184, Ann Arbor, MI.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Levin, Beth and Malka R. Hovav. 1996. From
lexical semantics to argument realization.
Unpublished manuscript.
Li, Xin and Dan Roth. 2001. Exploring
evidence for shallow parsing. In
Proceedings of CoNLL-2001, pages 107?110,
Toulouse, France.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ma`rquez, Llu?is, Jesus Gime?nez Pere Comas,
and Neus Catala`. 2005. Semantic role
labeling as sequential tagging. In
Proceedings of the Ninth Conference on
Computational Natural Language
Learning (CoNLL-2005), pages 193?196,
Ann Arbor, MI.
Noreen, Eric W. 1989. Computer-Intensive
Methods for Testing Hypotheses. New York:
John Wiley & Sons.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The proposition bank: An
annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James H. Martin,
and Daniel Jurafsky. 2005. Support vector
learning for semantic argument
classification.Machine Learning, 60:11?39.
Pradhan, Sameer, Kadri Hacioglu, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2003. Semantic role parsing
adding semantic structure to
unstructured text. In Proceedings of the
3rd IEEE International Conference on Data
Mining (ICDM 2003), pages 629?632,
Melbourne, FL.
Pradhan, Sameer, Kadri Hacioglu, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2005. Semantic role chunking
combining complementary syntactic
views. In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 217?220,
Ann Arbor, MI.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James H. Martin, and Daniel
Jurafsky. 2004. Shallow semantic parsing
using support vector machines. In
Proceedings of NAACL-HLT 2004,
pages 233?240, Boston, MA.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
and Dav Zimak. 2004. Semantic role
labeling via integer linear programming
inference. In Proceedings the 20th
International Conference on Computational
Linguistics (COLING), pages 1346?1352,
Geneva, Switzerland.
Punyakanok, Vasin and Dan Roth. 2001. The
use of classifiers in sequential inference. In
286
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Todd K. Leen, Thomas G. Dietterich, and
Volker Tresp, editors, Advances in Neural
Information Processing Systems 13,
pages 995?1001. MIT Press.
Roth, Dan. 1998. Learning to resolve
natural language ambiguities: A unified
approach. In Proceedings of the Fifteenth
National Conference on Artificial
Intelligence (AAAI-98), pages 806?813,
Madison, WI.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks. In
Proceedings of CoNLL-2004, pages 1?8,
Boston, MA.
Roth, Dan and Wen-tau Yih. 2005. Integer
linear programming inference for
conditional random fields. In Proceedings of
the 22nd International Conference on Machine
Learning (ICML-2005), pages 737?744,
Bonn, Germany.
Surdeanu, Mihai, Sanda Harabagiu, John
Williams, and Paul Aarseth. 2003. Using
predicate-argument structures for
information extraction. In Proceedings of the
41st Annual Meeting on Association for
Computational Linguistics, pages 8?15,
Sapporo, Japan.
Tjong Kim Sang, Erik F. and Sabine
Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In
Proceedings of CoNLL-2000 and LLL-2000,
pages 127?132, Lisbon, Portugal.
Xpress-MP. 2004. Dash Optimization.
Xpress-MP. http://www.
dashoptimization.com.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of the 2004
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2004),
pages 88?94, Barcelona, Spain.
Zhang, Tong, Fred Damerau, and David
Johnson. 2002. Text chunking based on a
generalization of Winnow. Journal of
Machine Learning Research, 2:615?637.
287

Applying System Combinat ion to Base Noun Phrase Identif ication 
Er ik  F. T jong  K im Sang",  Wal ter  Dae lemans  '~, Herv6  D6 jean  ~, 
Rob  Koel ingT,  Yuva l  Krymolowski /~,  Vas in  Punyakanok  '~, Dan Roth" 
~University of Antwert) 
Uifiversiteitsplohl 1 
13.-261.0 Wilri jk, Belgium 
{erikt,daelem}@uia.ua.ac.be 
r Unive.rsitiil; Tii l) ingen 
Kleine Wilhehnstrat./e 113 
I)-72074 T/il)ingen, Germany 
(lejean((~sl:q, ni)hil.ulfi-l;uebingen.de, 
7S1{,I Cambridge 
23 Millers Yard,Mil l  Lane 
Cambridge, CB2 ll{Q, UK 
koeling@caln.sri.coIn 
;~Bal'-Ilan University 
lbunat  Gan, 52900, Israel 
yuwdk(c~)macs. 1)iu.ac.il 
"University of Illinois 
1304: W. Sl)ringfield Ave. 
Url)ana, IL 61801, USA 
{lmnyakan,(lanr} ((~cs.uiuc.edu 
A1)s t rac t  
We us('. seven machine h;arning algorithms tbr 
one task: idenl;it~ying l)ase holm phrases. The 
results have 1)een t)rocessed by ditt'erent system 
combination methods and all of these (mtt)er- 
formed the t)est individual result. We have ap- 
t)lied the seven learners with the best (:omt)ina- 
tot, a majority vote of the top tive systenls, to a 
standard (lata set and lllallage(1 I;O ilnl)rov(', 1;11(' 
t)est pul)lished result %r this (lata set. 
1 In t roduct ion  
Van Haltor(m eta\ ] .  (1998) and Brill and Wu 
(1998) show that part-ofst)ee(:h tagger l)erfor- 
mance can 1)e iml)roved 1)y (:oml)ining ditl'erent 
tatters. By using te(:hni(tues su(:h as majority 
voting, errors made l)y 1;11(; minority of the tag- 
gers can 1)e r(;moved. Van Ilaltere, n et al (1998) 
rel)ort that the results of such a ('oml)ined al)- 
proach can improve ll\])Oll the aCcllracy error of 
the best individual system with as much as 19%. 
Tim positive (;tl'e(:t of system combination tbr 
non-language t)ro(:essing tasks has t)een shown 
in a large l)o(ly of mac\]fine l arning work. 
In this 1)aper we will use system (:omt)ination 
for identifying base noun 1)hrases (1)aseNt)s). 
W(; will at)l)ly seven machine learning algo- 
rithms to the same 1)aseNP task. At two l)oints 
we will al)ply confl)ination methods. We will 
start with making the systems process five out- 
trot representations and combine the l'esults t)y 
(:hoosing the majority of the outtmt tL'atures. 
Three of the seven systems use this al)l)roaeh. 
Afl, er this w(; will make an overall eoml)ination 
of the results of the seven systems. There we 
will evaluate several system combination meth- 
()(Is. The 1)est l)erforming method will 1)e at)- 
t)lied to a standard ata set tbr baseNP identi- 
tication. 
2 Methods  and exper iments  
in this se(:tion we will describe our lem:ning task: 
recognizing 1)ase noun phrases. After this we 
will (tes(:ril)e the data representations we used 
and the ma('hine learning algorithms that we 
will at)l)ly to the task. We will con(:ludc with 
an overview of the (:ombination metllo(ls that 
we will test. 
2.1 Task descript ion 
Base noun \])hrases (1)aseNPs) are n(mn phrases 
whi(:h do not (:ontain another noun l)hrase. \]?or 
cxamt)le , the sentence 
In \[early trading\] in \[ IIong Kong\] 
\[ Mo,l,tay \], \[ g,,la \] was q, loted at 
\[ $ 366. 0 \] \ [a .  o1,,,.(; \] .  
contains six baseN1)s (marked as phrases be- 
tween square 1)rackets). The phrase $ 266.50  
an  ounce  ix a holm phrase as well. However, it 
is not a baseNP since it contains two other noun 
phrases. Two baseNP data sets haw.' been put 
forward by Ramshaw and Marcus (1995). The 
main data set consist of tbur sections of the Wall 
Street Journal (WSJ) part of the Penn Tree- 
bank (Marcus et al, 1.993) as training mate- 
rial (sections 15-18, 211727 tokens) and one sec- 
tion aS test material (section 20, 47377 tokens)5. 
The data contains words, their part-of-speech 
1This Ramshaw and Marcus (1995) bascNP data set 
is availal)le via ffp://fti).cis.upe,m.edu/pub/chunker/ 
857 
(POS) tags as computed by the Brill tagger and 
their baseNP segmentation asderived from the 
%'eebank (with some modifications). 
In the baseNP identitication task, perfor- 
mance is measured with three rates. First, 
with the percentage of detected noun phrases 
that are correct (precision). Second, with the 
1)ercentage of noun phrases in the data that 
were found by the classifier (recall). And third, 
with the F#=~ rate which is equal to (2*preci- 
sion*recall)/(precision+recall). The latter rate 
has been used as the target for optimization. 
2.2 Data representat ion 
In our example sentence in section 2.1, noun 
phrases are represented by bracket structures. 
It has been shown by Mufioz et al (1999) 
that for baseNP recognition, the representa- 
tion with brackets outperforms other data rep- 
resentations. One classifier can be trained to 
recognize open brackets (O) and another can 
handle close brackets (C). Their results can be 
combined by making pairs of open and close 
brackets with large probability scores. We have 
used this bracket representation (O+C) as well. 
However, we have not used the combination 
strategy from Mufioz et al (1999) trot in- 
stead used the strategy outlined in Tjong Kim 
Sang (2000): regard only the shortest possi- 
ble phrases between candidate open and close 
brackets as base noun phrases. 
An alternative representation for baseNPs 
has been put tbrward by Ramshaw and Mar- 
cus (1995). They have defined baseNP recog- 
nition as a tagging task: words can be inside a 
baseNP (I) or outside a baseNP (O). In the case 
that one baseNP immediately follows another 
baseNP, the first word in the second baseNP 
receives tag B. Example: 
Ino early1 trading1 ino Hongi Kongi 
MondayB ,o gold1 waso quotedo ato 
$I 366.501 anu ounce1 .o 
This set of three tags is sufficient for encod- 
ing baseNP structures since these structures are 
nonrecursive and nonoverlapping. 
Tjong Kiln Sang (2000) outlines alternative 
versions of this tagging representation. First, 
the B tag can be used for tile first word of ev- 
ery baseNP (IOB2 representation). Second, in- 
stead of the B tag an E tag can be used to 
nlark the last word of a baseNP immediately 
before another baseNP (IOE1). And third, the 
E tag call be used for every noun phrase final 
word (IOE2). He used the Ramshaw and Mar- 
cus (1995) representation as well (IOB1). We 
will use these tbur tagging representations and 
the O+C representation for the system-internal 
combination experiments. 
2.a Machine learning algorithms 
This section contains a brief description of tile 
seven machine learning algorithms that we will 
apply to the baseNP identification task: AL- 
LiS, c5.0, IO~?ee, MaxEnt, MBL, MBSL and 
SNOW. 
ALLiS 2 (Architecture for Learning Linguistic 
Structures) is a learning system which uses the- 
ory refinement in order to learn non-recursive 
NP and VP structures (Ddjean, 2000). ALLiS 
generates a regular expression grammar which 
describes the phrase structure (NP or VP). This 
grammar is then used by the CASS parser (Ab- 
hey, 1996). Following the principle of theory re- 
finement, tile learning task is composed of two 
steps. The first step is the generation of an 
initial wa, mmar. The generation of this grmn- 
mar uses the notion of default values and some 
background knowledge which provides general 
expectations concerning the immr structure of 
NPs and VPs. This initial grammar provides 
an incomplete and/or incorrect analysis of tile 
data. The second step is the refinement of this 
grammar. During this step, the validity of the 
rules of the initial grammar is checked and the 
rules are improved (refined) if necessary. This 
refinement relies on the use of two operations: 
the contextualization (i which contexts uch a 
tag always belongs to the phrase) and lexical- 
ization (use of information about the words and 
not only about POS). 
05.0 a, a commercial version of 04.5 (Quin- 
lan, 1993), performs top-do,vn induction of de- 
cision trees (TDIDT). O,1 the basis of an in- 
stance base of examples, 05.0 constructs a deci- 
sion tree which compresses the classification i - 
formation in the instance base by exploiting dif- 
tbrences in relative importance of different fea- 
tures. Instances are stored in the tree as paths 
2A demo f the NP and VP ctmnker is available at 
ht;t:p: / /www.sfb441.unituebingen.de/~ dej an/chunker.h 
tml 
aAvailable fl'om http://www.rulequest.com 
858 
of commcted nodes ending in leaves which con- 
tain classification information. Nodes are con- 
nected via arcs denoting feature wflues. Feature 
inff)rmation gain (nmt;ual inforniation 1)etween 
features and class) is used to determine the or- 
der in which features are mnt)loyed as tests at all 
levels of the tree (Quinlan, 1993), With the full 
inlmt representation (words and POS tags)~ we 
were not able to run comt)lete xperiments. We 
therefore xperimented only with the POS tags 
(with a context of two left; and right). We have 
used the default parameter setting with decision 
trees coml)ined with wflue groul)ing. 
We have used a nearest neighbor algoritlml 
(IBI.-1G, here listed as MBL) and a decision tree 
algoritlmi (llG\[lh:ee) from the TiMBL learning 
package (Da(flmnans et al, 19991)). Both algo- 
rithms store the training data and ('lassi(y new 
it;eros by choosing the most frequent (:lassiti(:a- 
lion among training items which are closest to 
this new item. l)ata it(uns rare rel)resented as 
sets of thature-vahu; 1)airs. Each ti;ature recc'ives 
a weight which is t)ased on the amount of in- 
formation whi(:h it t/rovides fi)r comtmting the 
classification of t;t1(; items in the training data. 
IBI-IG uses these weights tbr comt)uting the dis- 
lance l)etween a t)air of data items and IGTree 
uses them fi)r deciding which feature-value de- 
cisions shouM t)e made in the top nod(;s of the 
decision tree (l)a(;lenJans et al, 19991)). We 
will use their det, mlt pm:amet('a:s excel)t for the 
IBI-IG t)arameter for the numl)er of exmnine(t 
m',arest n(,ighl)ors (k) whi('h we h~ve s(,t to 3 
(Daelemans et al, 1999a). The classifiers use a 
left and right context of four words and part- 
ofsl)eech tags. t~i)r |;lie four IO representations 
we have used a second i)rocessing stage which 
used a smaller context lint which included in- 
formation at)out the IO tags 1)redicted by the 
first processing phase (Tjong Kim Sang, 2000). 
When /)uilding a classifier, one must gather 
evidence ti)r predicting the correct class of an 
item from its context. The Maxinmm Entropy 
(MaxEnt) fl:mnework is especially suited tbr 
integrating evidence tiom various inti)rmal;ion 
sources. Frequencies of evidence/class combi~ 
nations (called features) are extracted fl'om a 
sample corlms and considere(t to be t)roperties 
of the classification process. Attention is con- 
strained to models with these l)roperties. The 
MaxEnt t)rinciph; now demands that among all 
1;11(; 1)robability distributions that obey these 
constraints, the most mfiform is chosen, l)ur- 
ing training, features are assigned weights in 
such a way that, given the MaxEnt principle, 
the training data is matched as well as possible. 
During evaluation it is tested which features are 
active (i.e. a feature is active when the context 
meets the requirements given by t;11(', feature). 
For every class the weights of the active fea- 
tures are combined and the best scoring class 
is chosen (Berger et al, 1996). D)r the classi- 
tier built here the surromlding words, their POS 
tags and lmseNP tags predicted for the previous 
words are used its evidence. A mixture of simple 
features (consisting of one of the mentioned in- 
formation sources) and complex features (com- 
binations thereof) were used. The left context 
never exceeded 3 words, the right context was 
maximally 2 words. The model wits (:ah:ulated 
using existing software (l)ehaspe, 1997). 
MBSL (Argalnon et al, 1999) uses POS data 
in order to identit~y t/aseNPs, hfferenee re- 
lies on a memory which contains all the o(:- 
cm:rences of P()S sequences which apt)ear in 
the t)egimfing, or the end, of a 1)aseNl? (in- 
(:hiding complete t)hrases). These sequences 
may include a thw context tags, up to a 1)re- 
st)ecifi('d max_(:ont<~:t. \])uring inti',rence, MBSL 
tries to 'tile' each POS string with parts of 
noun-l)hrases from l;he memory. If the string 
coul(1 l)e fully covered t)y the tiles, il; becomes 
l)art of a (:andidate list, anfl)iguities 1)etween 
candidates are resolved by a constraint )ropa- 
gation algorithm. Adding a (:ontext extends the 
possil)ilities for tiling, thereby giving more op- 
portunities to 1)etter candidates. The at)t)roaeh 
of MBSL to the i)rot)lem of identifying 1)aseNPs 
is sequence-1)ased rather than word-based, that 
is, decisions are taken per POS sequence, or per 
candidate, trot not for a single word. In addi- 
tion, the tiling l)rocess gives no preference to 
any (tirection in the sentence. The tiles may 1)e 
of any length, up to the maximal ength of a 
1)hrase in the training (ILl;L, which gives MBSL 
a generalization power that compensates for the 
setup of using only POS tags. The results t)re- 
seated here were obtained by optimizing MBSL 
parameters based on 5-fold CV on the training 
data. 
SNoW uses the Open/Close model, described 
in Mufioz et al (1999). As is shown there, this 
859 
section 21 
IOB1 
IOB2 
IOE1 
IOE2 
O+C 
0 
97.81% 
97.63% 
97.80% 
97.72% 
97.72% 
MBL 
Majority 98.04% 98.20% 
C Ffl=l 
97.97% 91.68 
97.96% 91.79 
97.92% 91.54 
97.94% 92.06 
98.04% 92.03 
92.82 
MaxEnt 
O C 
97.90% 98.11% 
97.81% 98.14% 
97.88% 98.12% 
97.84% 98.12% 
97.82% 98.15% 
97.94% 98.24% 
Ffl=l 
92.43 
92.14 
92.37 
92.13 
92.26 
92.60 
IGTree 
O C 
96.62% 96.89% 
97.27% 97.30% 
95.88% 96.01% 
97.19% 97.62% 
96.89% 97.49% 
97.70% 97.99% 
F\[~=1 
87.88 
90.03 
82.80 
89.98 
89.37 
91.92 
Table 1: The effects of system-internal combination by using different output representations. A 
straight-forward majority vote of the output yields better bracket accuracies and Ffl=l rates than 
any included individual classifier. The bracket accuracies in the cohmms O and C show what 
percentage of words was correctly classified as baseNP start, baseNP end or neither. 
model produced better results than the other 
paradigm evaluated there, the Inside/Outside 
paradigm. The Open/Close model consists of 
two SNoW predictors, one of which predicts the 
beginning of baseNPs (Open predictor), and the 
other predicts the end of the ptlrase (Close pre- 
dictor). The Open predictor is learned using 
SNoW (Carlson el; al., 1999; Roth, 1998) as a 
flmction of features that utilize words and POS 
tags in the sentence and, given a new sentence, 
will predict for each word whether it is the first 
word in the phrase or not. For each Open, the 
Close predictor is learned using SNoW as a func- 
tion of features that utilize the words ill the sen- 
tence, the POS tags and the open prediction. It 
will predict, tbr each word, whether it Call be 
the end of" the I)hrase, given the previously pre- 
dicted Open. Each pair of predicted Open mid 
Close forms a candidate of a baseNP. These can- 
didates may conflict due to overlapping; at this 
stage, a graph-based constraint satisfaction al- 
gorithm that uses the confidence values SNoW 
associates with its predictions i elnployed. This 
algorithln ("the combinator') produces tile list 
of" the final baseNPs fbr each sentence. Details 
of SNOW, its application in shallow parsing and 
the combinator% Mgorithm are in Mufioz et al 
(1999). 
2.4 Combinat ion techniques 
At two points in our noun phrase recognition 
process we will use system combination. We will 
start with system-internal combination: apply 
the same learning algorithm to variants of the 
task and combine the results. The approach 
we have chosen here is the same as in Tjong 
Kim Sang (2000): generate different variants 
of the task by using different representations 
of the output (IOB1, IOB2, IOE1, IOE2 and 
O+C). The five outputs will converted to the 
open bracket representation (O) and the close 
bracket; representation (C) and M'ter this, tile 
most frequent of the five analyses of each word 
will chosen (inajority voting, see below). We 
expect the systems which use this combination 
phase to perform better than their individuM 
members (Tjong Kim Sang, 2000). 
Our seven learners will generate different clas- 
sifications of tile training data and we need to 
find out which combination techniques are most 
appropriate. For the system-external combi- 
nation experiment, we have evaluated itfi;rent 
voting lllechanisms~ effectively the voting meth- 
ods as described in Van Halteren et al (1998). 
In the first method each classification receives 
the same weight and the most frequent classifi- 
cation is chosen (Majority). The second nmthod 
regards as tile weight of each individual clas- 
sification algorithm its accuracy on solne part 
of the data, tile tuning data (TotPrecision). 
The third voting method computes the preci- 
sion of each assigned tag per classifer and uses 
this value as a weight for tile classifier in those 
cases that it chooses the tag (TagPrecision). 
The fourth method uses both the precision of 
each assigned tag and tile recall of the com- 
peting tags (Precision-Recall). Finally, tile fifth 
lnethod uses not only a weight for tile current 
classification but it also computes weights tbr 
other possible classifications. The other classi- 
fications are deternfined by exalnining the tun- 
860 
ing data and registering the correct wflues for 
(;very pair of classitier esults (pair-wise voting, 
see Van Halteren et al (1998) tbr an elaborate 
explanation). 
Apart from these five voting methods we have 
also processed the output streams with two clas- 
sifters: MBL and IG%'ee. This approach is 
called classifier stacking. Like Van Halteren et 
al. (1998), we have used diff'erent intmt ver- 
sions: olle containing only the classitier Otltl)ut 
and another containing both classifier outlmt 
and a compressed representation of the data 
item tamer consideration. \]?or the latter lmr- 
pose we have used the part-of-speech tag of the 
carrent word. 
3 Resul ts  4 
We want to find out whether system combi- 
nation could improve performmlce of baseNP 
recognition and, if this is the fact, we want to 
seJect the best confl)ination technique. For this 
lmrpose we have pertbrmed an experiment with 
sections 15-18 of the WSJ part of the Prom %'ee- 
bank as training data (211727 tokens) and sec- 
tion 21 as test data (40039 tokens). Like the 
data used by Ramshaw and Marcus (1995), this 
data was retagged by the Brill tagger in order 
to obtain realistic part-of  speech (POS) tags 5. 
The data was seglnente.d into baseNP parts and 
non-lmseNP t)arts ill a similar fitshion as the 
data used 1)y Ramshaw and Marcus (1995). Of 
the training data, only 90% was used for train- 
ing. The remaining 10% was used as laming 
data for determining the weights of the combi- 
nation techniques. 
D)r three classifiers (MBL, MaxEnt and 
IGTree) we haw; used system-internal coral)i- 
nation. These learning algorithms have pro- 
cessed five dittbrent representations of the out- 
put (IOB1, IOB2, IOE1, IOE2 and O-t-C) and 
the results have been combined with majority 
voting. The test data results can 1)e fimnd in 
Table 1. In all cases, the combined results were 
better than that of the best included system. 
Tile results of ALLiS, 05.0, MB SL and SNoW 
have tmen converted to the O and the C repre- 
4Detailed results of our experiments me available on 
http: / /lcg-www.uia.ae.be/-erikt /np('oml,i / 
SThe retagging was necessary to assure that the per- 
formance rates obtained here would be similar to rates 
obtained for texts for which no Treebank POS tags are 
available. 
section 21 
Classifier 
ALLiS 
05.0 
IGTree 
MaxEnt 
MBL 
MBSL 
SNoW 
Simple Voting 
Majority 
TotPrecision 
TagPrecision 
Precision-Recall 
0 
97.87% 
97.05% 
97.70% 
97.94% 
98.04% 
97.27% 
97.78% 
98.08% 
98.08% 
98.08% 
98.08% 
C FS=j 
98.08% 92.15 
97.76% 89.97 
97.99% 91.92 
98.24% 92.60 
98.20% 92.82 
97.66% 90.71 
97.68% 91.87 
98.21% 92.95 
98.21% 92.95 
98.21% 92.95 
98.21% 92.95 
Pairwise Voting 
TagPair 98.13% 98.23% 
Memory-Based 
Tags 98.24% 98.35% 
Tags 4- P()S 98.14% 98.33% 
Deeision Trees 
Tags 98.24% 98.35% 
Tags + POS 98.13% 98.32% 
93.07 
93.39 
93.24 
93.39 
93.21 
Table 2: Bracket accuracies and Ff~=l scores 
for section WSJ 21 of the Penn ~15'eebank with 
seve, n individual classifiers and combinations of 
them. Each combination t)erforms t)etter than 
its best individual me, tuber. The stacked classi- 
tiers without COllte, xt intbrmation perform best. 
sentation. Together with the bracket; ret)resen- 
tations of the other three techniques, this gave 
us a total of seven O results and seven C results. 
These two data streams have been combined 
with the combination techniques described in 
section 2.4. After this, we built baseNPs from 
the, O and C results of each combinatkm tech- 
nique, like, described in section 2.2. The bracket 
accuracies and tile F~=I scores tbr test data can 
be found in Table 2. 
All combinations iml)rove the results of the 
best individual classifier. The best results were 
obtained with a memory-based stacked classi- 
ter. This is different from the combination re- 
sults presented in Van Ilalteren et al (1998), 
in which pairwise voting pertbrmed best. How- 
eves, in their later work stacked classifiers out- 
perIbrm voting methods as well (Van Halteren 
et al, to appear). 
861 
section 20 accuracy precision recall 
Best-five combination 0:98.32% C:98.41% 94.18% 93.55% 
Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89% 
Mufioz et al (1999) O:98.1% C:98.2% 92.4% 93.1% 
Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27% 
Argamon et al (1999) - 91.6% 91.6% 
F/3=1 
93.86 
93.26 
92.8 
92.03 
91.6 
Table 3: The overall pertbrmance of the majority voting combination of our best five systems 
(selected on tinting data perfbrnmnce) applied to the standard data set pnt tbrward by Ramshaw 
and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how 
often a word was classified correctly with the representation used (O, C or IOB1). The combined 
system outperforms all earlier reported results tbr this data set. 
Based on an earlier combination study 
(Tjong Kim Sang, 2000) we had expected the 
voting methods to do better. We suspect hat 
their pertbrmance is below that of the stacked 
classifiers because the diflhrence between tile 
best and the worst individual system is larger 
than in our earlier study. We assume that the 
voting methods might perform better if they 
were only applied to the classifiers that per- 
form well on this task. In order to test this 
hypothesis, we have repeated the combination 
experiments with the best n classitiers, where 
n took vahms from 3 to 6 and the classifiers 
were ranked based on their performance on the 
tnning data. The t)est pertbrmances were ob- 
tained with five classifiers: F/~=1=93.44 for all 
five voting methods with tile best stacked classi- 
tier reaching 93.24. With the top five classifiers, 
tile voting methods outpertbrm the best; combi- 
nation with seven systems G. Adding extra clas- 
sification results to a good combination system 
should not make overall performance worse so 
it is clear that there is some room left for im- 
provement of our combination algorithms. 
We conclude that the best results ill this 
task can be obtained with tile simplest voting 
method, majority voting, applied to the best 
five of our classifiers. Our next task was to 
apply the combination apt)roach to a standard 
data set so that we could compare our results 
with other work. For this purpose we have used 
6V~re are unaware of a good method for determining 
the significance of F~=I differences but we assume that 
this F~=I difference is not significant. However, we be- 
lieve that the fact that more colnbination methods per- 
tbrm well, shows that it easier to get a good pertbrmmlce 
out of the best; five systems than with all seven. 
tile data put tbrward by ll,amshaw and Marcus 
(1995). Again, only 90% of the training data 
was used tbr training while the remaining 11)% 
was reserved tbr ranking the classifiers. The 
seven learners were trained with the same pa- 
rameters as in the previous experiment. Three 
of the classifiers (MBL, MaxEnt and iG%'ee) 
used system-internal combination by processing 
different output representations. 
The classifier output was converted to the 
O and the C representation. Based on the 
tuning data performance, the classifiers ALLiS, 
IGTREE, MaxEnt, MBL and SNoW were se- 
lected for being combined with majority vot- 
ing. After this, the resulting O and C repre- 
sentations were combined to baseNPs by using 
the method described in section 2.2. The re- 
sults can be found in Table 3. Our combined 
system obtains an F/~=I score of 93.86 which 
corresponds to an 8% error reduction compared 
with tile best published result tbr this data set 
(93.26). 
4 Conc lud ing  remarks  
In this paper we have examined two methods for 
combining the results of machine learuing algo- 
rithms tbr identii}cing base noun phrases. Ill the 
first Inethod, the learner processed ifferent out- 
put data representations and tile results were 
combined by majority voting. This approach 
yielded better results than the best included 
classifier. Ill the second combination approach 
we have combined the results of seven learning 
systems (ALLiS, c5.0, IGTree, MaxEnt, MBL, 
MBSL and SNOW). Here we have tested d i f  
ferent confl)ination methods. Each coilfl)ination 
862 
nmthod outt)erformed the best individual learn- 
ing algorithm and a majority vote of the tol) 
five systems peribrmed best. We, have at}i}lie, d 
this approach of system-internal nd system- 
external coral}|nation to a standard ata set for 
base noun phrase identification and the 1}ertbr- 
mance of our system was 1)etter than any other 
tmblished result tbr this data set. 
Our study shows that the c, omt)ination meth- 
(}{Is that we have tested are sensitive for the in- 
clusion of classifier esults of poor quality. This 
leaves room for imt)rovement of our results t}y 
evaluating other coml}inators. Another interest- 
ing apl)roach which might lead to a l}etter t)er- 
f{}rmance is taking into a{-com~t more context 
inibrmation, for example by coral)in|rig com- 
plete 1}hrases instead of indet}endent t}ra{:kets. 
It would also be worthwhile to evaluate using 
more elaborate me, thods lbr building baseNPs 
out of ot}en and close t}ra{:ket (:an{ti{tates. 
Acknowledgements  
l)djean, Koeling and 'l?jong Kim Sang are 
funded by the TMII. 1\]etwork Learning (Jompu- 
tational Grammars r. 1}unyakanok and Roth are 
SUl)t}orted by NFS grants IIS-98{}1638 an{t SBR- 
9873450. 
Re ferences  
Steven Alm{',y. 1996. Partial t)a\]'sing via finite- 
state cascades. In l'n, l}~wce, di'ngs of the /~,gS- 
LLI '95 l?,obust 1)arsi'n9 Worlcsh, op. 
SMomo Argam(m, Ido l)agan, an(l YllV~t\] Kry- 
molowsld. 1999. A memory-1}ased at}proach 
to learning shalh}w natural anguage patterns. 
Journal of E:rperimental and Th, eovetical AL 
11(3). 
Adam L. Berge, r, SteI}hen A. l)ellaPietra, and 
Vincent J. DellaPietra. 1996. A inaximum 
entrol)y apI)roach to natural language pro- 
cessing. Computational Linguistics, 22(1). 
Eric Bri\]l and ,lun Wu. 1998. Classifier com- 
bination tbr improved lexical disaml)iguation. 
In P~vccedings o.f COLING-A 6'15 '98. Associ- 
ation for Computational Linguistics. 
A. Carlson, C. Cunfl)y, J. Rosen, and 
D. l/,oth. 1.999. The SNoW learning archi- 
tecture. Technical Report UIUCDCS-11,-99- 
2101, UIUC Computer Science Department, 
May. 
r httl): / /lcg-www.ui',,.ac.be~/ 
Walter Daelemans, A.ntal van den Bosch, and 
Jakub Zavrel. 1999a. \])brgetting exceptions 
is harmflll in language learning. Machine 
Learning, 34(1). 
Walter Daelemans, Jakub Zavrel, Ko wmder  
Sloot, and Antal van den Bosch. 1999b. 
TiMBL: Tilb'arg Memory Bused Learner, ver- 
sion 2.G Rqfi;rence Guide. ILK Te(:hnical 
th',port 99-01. http:// i lk.kub.nl/.  
Luc Dehaspe. 1997. Maximum entropy model- 
ing with clausal constraints, in PTvcecdings oJ' 
th, c 7th, 1}l, ternational Workshop on ind'uctivc 
Logic Programming. 
Hervd Ddjean. 200(I. Theory refinement and 
natural language processing. In Proceedings 
of the ColingEO00. Association for Computa- 
tional Linguistics. 
Mitchell 17 }. Marcus, Beatrice Santorini, and 
Mary Aim Marcinkiewicz. 1993. Building a 
large mmotated corpus of english: the penn 
treebank. Computational Linguistics, 19(2). 
Marcia Munoz,  Vasin Punyakanok, l)an l l,oth, 
and Day Zimak. 1999. A learning ap- 
t}roa(:h to shallow t)arsing. In P~vceedings of 
EMNLP-WVLC'99.  Asso('iation for Coml)u- 
tational Linguisti(:s. 
J. Ross Quinlan. 1993. c/t.5: Programs for Ma- 
th,|he Learning. Morgan Kauflnann. 
Lance A. Ramshaw and Mitchell P. Marcus. 
1995. Text chunking using transformation- 
l)ase{t learn|Jig. In 1}roceeding s o\[ the Th, i'rd 
A CL Worksh, op on Ve, r~.l LacTic Corpora. As- 
sociation for Comlmtational Linguistics. 
D. Roth. 1.9!t8. Learning to resolve natural an- 
guage aml}iguities: A unified approach. In 
AAAL98.  
Erik F. Tjong Kim Sang. 2000. N{mn phrase 
recognition by system {:ombination. In Pro- 
ceedings of th, e ANLP-NAA CL-2000. Seattle, 
Washington, USA. Morgan Kauflnan Pub- 
lishers. 
Hans van Halteren, Jakub Zavrel, and Wal- 
ter Daelemans. 1998. Iml)roving data driven 
wordclass tagging by system corot}|nation. In
P~veeedings of COLING-ACL '98. Associa- 
tion tbr Computational Linguistics. 
Hans van Halteren, Jakub Zavrel, and Walter 
Daelemans. to appear, hnproving accuracy 
ill nlp through coati)|nation ofmachine learn- 
ing systems. 
863 
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 6?7,
Vancouver, October 2005.
Demonstrating an Interactive Semantic Role Labeling System
Vasin Punyakanok Dan Roth Mark Sammons
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{punyakan,danr,mssammon}@uiuc.edu
Wen-tau Yih
Microsoft Research
Redmond, WA 98052, USA
scottyih@microsoft.com
Abstract
Semantic Role Labeling (SRL) is the task
of performing a shallow semantic analy-
sis of text (i.e., Who did What to Whom,
When, Where, How). This is a cru-
cial step toward deeper understanding of
text and has many immediate applications.
Preprocessed information on text, mostly
syntactic, has been shown to be impor-
tant for SRL. Current research focuses on
improving the performance assuming that
this lower level information is given with-
out any attention to the overall efficiency
of the final system, although minimizing
execution time is a necessity in order to
support real world applications. The goal
of our demonstration is to present an inter-
active SRL system that can be used both
as a research and an educational tool. Its
architecture is based on the state-of-the-
art system (the top system in the 2005
CoNLL shared task), modified to process
raw text through the addition of lower
level processors, while achieving effective
real time performance.
1 Introduction
Semantic parsing of sentences is believed to be an
important subtask toward natural language under-
standing, and has immediate applications in tasks
such information extraction and question answering.
We study semantic role labeling (SRL), defined as
follows: for each verb in a sentence, the goal is to
identify all constituents that fill a semantic role, and
to determine their roles (such as Agent, Patient or In-
strument) and their adjuncts (such as Locative, Tem-
poral or Manner). The PropBank project (Kingsbury
and Palmer, 2002), which provides a large human-
annotated corpus of semantic verb-argument rela-
tions, has opened doors for researchers to apply ma-
chine learning techniques to this task.
The focus of the research has been on improving
the performance of the SRL system by using, in ad-
dition to raw text, various syntactic and semantic in-
formation, e.g. Part of Speech (POS) tags, chunks,
clauses, syntactic parse tree, and named entities,
which is found crucial to the SRL system (Pun-
yakanok et al, 2005).
In order to support a real world application such
as an interactive question-answering system, the
ability of an SRL system to analyze text in real time
is a necessity. However, in previous research, the
overall efficiency of the SRL system has not been
considered. At best, the efficiency of an SRL sys-
tem may be reported in an experiment assuming that
all the necessary information has already been pro-
vided, which is not realistic. A real world scenario
requires the SRL system to perform all necessary
preprocessing steps in real time. The overall effi-
ciency of SRL systems that include the preproces-
sors is not known.
Our demonstration aims to address this issue. We
present an interactive system that performs the SRL
task from raw text in real time. Its architecture is
based on the top system in the 2005 CoNLL shared
task (Koomen et al, 2005), modified to process raw
text using lower level processors but maintaining
6
good real time performance.
2 The SRL System Architecture
Our system begins preprocessing raw text by
using sentence segmentation tools (available at
http://l2r.cs.uiuc.edu/?cogcomp/tools.php). Next,
sentences are analyzed by a state-of-the-art syntac-
tic parser (Charniak, 2000) the output of which pro-
vides useful information for the main SRL module.
The main SRL module consists of four stages:
pruning, argument identification, argument classifi-
cation, and inference. The following is the overview
of these four stages. Details of them can be found
in (Koomen et al, 2005).
Pruning The goal of pruning is to filter out un-
likely argument candidates using simple heuristic
rules. Only the constituents in the parse tree are
considered as argument candidates. In addition, our
system exploits a heuristic modified from that intro-
duced by (Xue and Palmer, 2004) to filter out very
unlikely constituents.
Argument Identification The argument identifi-
cation stage uses binary classification to identify
whether a candidate is an argument or not. We train
and apply the binary classifiers on the constituents
supplied by the pruning stage.
Argument Classification This stage assigns the
final argument labels to the argument candidates
supplied from the previous stage. A multi-class clas-
sifier is trained to classify the types of the arguments
supplied by the argument identification stage.
Inference The purpose of this stage is to incor-
porate some prior linguistic and structural knowl-
edge, such as ?arguments do not overlap? and ?each
verb takes at most one argument of each type.? This
knowledge is used to resolve any inconsistencies in
argument classification in order to generate legiti-
mate final predictions. The process is formulated as
an integer linear programming problem that takes as
input confidence values for each argument type sup-
plied by the argument classifier for each constituent,
and outputs the optimal solution subject to the con-
straints that encode the domain knowledge.
The system in this demonstration, however, dif-
fers from its original version in several aspects.
First, all syntactic information is extracted from the
output of the full parser, where the original version
used different information obtained from different
processors. Second, the named-entity information is
discarded. Finally, no combination of different parse
tree outputs is performed. These alterations aim to
enhance the efficiency of the system while maintain-
ing strong performance.
Currently the system runs at the average speed of
1.25 seconds/predicate. Its performance is 77.88 and
65.87 F1-score on WSJ and Brown test sets (Car-
reras and Ma`rquez, 2005) while the original system
achieves 77.11 and 65.6 on the same test sets with-
out the combination of multiple parser outputs and
79.44 and 67.75 with the combination.
3 Goal of Demonstration
The goal of the demonstration is to present the sys-
tem?s ability to perform the SRL task on raw text in
real time. An interactive interface allows users to in-
put free form text and to receive the SRL analysis
from our system. This demonstration can be found
at http://l2r.cs.uiuc.edu/?cogcomp/srl-demo.php.
Acknowledgments
We are grateful to Dash Optimization for the free
academic use of Xpress-MP. This research is sup-
ported by ARDA?s AQUAINT Program, DOI?s Re-
flex program, and an ONR MURI Award.
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
conll-2005 shared tasks: Semantic role labeling. In
Proc. of CoNLL-2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL 2000.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proc. of LREC-2002, Spain.
P. Koomen, V. Punyakanok, D. Roth, and W. Yih. 2005.
Generalized Inference with Multiple Semantic Role
Labeling Systems. In Proceedings of CoNLL-2005.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling. In
Proc. of IJCAI-2005.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proc. of the EMNLP-2004.
7
In: Proceedings of CoNLL-2000 and LLL-2000, pages 107-110, Lisbon, Portugal, 2000. 
Shallow Parsing by Inferencing with Classifiers* 
Vas in  Punyakanok  and Dan Roth  
Depar tment  of Computer  Science 
University of Illinois at Urbana-Champaign  
Urbana,  IL 61801, USA 
{punyakan, danr}@cs.uiuc.edu 
Abst ract  
We study the problem of identifying phrase 
structure. We formalize it as the problem of 
combining the outcomes of several different clas- 
sifiers in a way that provides a coherent in- 
ference that satisfies some constraints, and de- 
velop two general approaches for it. The first 
is a Markovian approach that extends stan- 
dard HMMs to allow the use of a rich obser- 
vations structure and of general classifiers to 
model state-observation dependencies. The sec- 
ond is an extension of constraint satisfaction for- 
malisms. We also develop efficient algorithms 
under both models and study them experimen- 
tally in the context of shallow parsing. 1
1 Ident i fy ing  Phrase  St ructure  
The problem of identifying phrase structure can 
be formalized as follows. Given an input string 
O =< ol, 02, . . . ,  On >, a phrase is a substring 
of consecutive input symbols oi, oi+l,. . . ,oj .  
Some external mechanism is assumed to consis- 
tently (or stochastically) annotate substrings as 
phrases 2. Our goal is to come up with a mech- 
anism that, given an input string, identifies the 
phrases in this string, this is a fundamental task 
with applications in natural language (Church, 
1988; Ramshaw and Marcus, 1995; Mufioz et 
al., 1999; Cardie and Pierce, 1998). 
The identification mechanism works by using 
classifiers that process the input string and rec- 
ognize in the input string local signals which 
* This research is supported by NSF grants IIS-9801638, 
SBR-9873450 and IIS-9984168. 
1Full version is in (Punyakanok and Roth, 2000). 
2We assume here a single type of phrase, and thus 
each input symbol is either in a phrase or outside it. All 
the methods we discuss can be extended to deal with 
several kinds of phrases in a string, including different 
kinds of phrases and embedded phrases. 
are indicative to the existence of a phrase. Lo- 
cal signals can indicate that an input symbol o 
is inside or outside a phrase (IO modeling) or 
they can indicate that an input symbol o opens 
or closes a phrase (the OC modeling) or some 
combination of the two. In any case, the lo- 
cal signals can be combined to determine the 
phrases in the input string. This process, how- 
ever, needs to satisfy some constraints for the 
resulting set of phrases to be legitimate. Sev- 
eral types of constraints, such as length and or- 
der can be formalized and incorporated into the 
mechanisms studied here. For simplicity, we fo- 
cus only on the most basic and common con- 
straint - we assume that phrases do not overlap. 
The goal is thus two-fold: to learn classifiers 
that recognize the local signals and to combine 
these in a ways that respects the constraints. 
2 Markov  Mode l ing  
HMM is a probabilistic finite state automaton 
used to model the probabilistic generation of 
sequential processes. The model consists of 
a finite set S of states, a set (9 of observa- 
tions, an initial state distribution P1 (s), a state- 
transition distribution P(s\[s') for s, # E S and 
an observation distribution P(o\[s) for o E (9 
and s 6 S. 3 
In a supervised learning task, an observa- 
tion sequence O --< o l ,o2, . . .  On > is super- 
vised by a corresponding state sequence S =< 
sl, s2,. ? ? sn >. The supervision can also be sup- 
plied, as described in Sec. 1, using the local sig- 
nals. Constraints can be incorporated into the 
HMM by constraining the state transition prob- 
ability distribution P(s\]s'). For example, set 
P(sV)  = 0 for all s, s' such that the transition 
from s ~ to s is not allowed. 
aSee (Rabiner, 1989) for a comprehensive tutorial. 
107 
Combining HMM and classifiers (artificial 
neural networks) has been exploited in speech 
recognition (Morgan and Bourlard, 1995), how- 
ever, with some differences from this work. 
2.1 HMM wi th  Classif iers 
To recover the most likely state sequence in 
HMM, we wish to estimate all the required 
probability distributions. As in Sec. 1 we as- 
sume to have local signals that indicate the 
state. That is, we are given classifiers with 
states as their outcomes. Formally, we assume 
that Pt(slo ) is given where t is the time step in 
the sequence. In order to use this information 
in the HMM framework, we compute 
Pt(o\[s) = Pt(slo)Pt(o)/Pt(s). (1) 
instead of observing the conditional probability 
Pt (ols) directly from training data, we compute 
it from the classifiers' output. Pt(s) can be cal- 
culated by Pt(s) = Es'eS P(sls')Pt- l(s ')  where 
Pl(s) and P(sls' ) are the two required distri- 
bution for the HMM. For each t, we can treat 
Pt(ols ) in Eq. 1 as a constant r/t because our goal 
is only to find the most likely sequence of states 
for given observations which are the same for 
all compared sequences. Therefore, to compute 
the most likely sequence, standard ynamic pro- 
gramming (Viterbi) can still be applied. 
2.2 P ro jec t ion  based  Markov  Mode l  
In HMMs, observations are allowed to depend 
only on the current state and long term depen- 
dencies are not modeled. Equivalently, from the 
constraint point of view, the constraint struc- 
ture is restricted by having a stationary proba- 
bility distribution of a state given the previous 
one. We attempt o relax this by allowing the 
distribution of a state to depend, in addition 
to the previous state, on the observation. For- 
mally, we make the independence assumption: 
P(  s t lS t - l  , S t -  2 ,  . . . , s l  , o t ,  o t -1 ,  . . . , 01)  
= P(stlSt_l,Ot). (2) 
Thus, we can find the most likely state sequence 
S given O by maximizing 
n 
P(SIO) = II\[P(stls~,..., 8t-1, O)\]Pl(slIO) 
t=2 
n 
= H\[P(stlst_l,ot)\]Pl(sl lOl).  (3) 
t=2 
Hence, this model generalizes the standard 
HMM by combining the state-transition prob- 
ability and the observation probability into one 
function. The most likely state sequence can 
still be recovered using the dynamic program- 
ming algorithm over the Eq.3. 
In this model, the classifiers' decisions are in- 
corporated in the terms P(sls' ,o ) and Pl(slo ). 
In learning these classifiers we project P(sls ~, o) 
to many functions Ps' (slo) according to the pre- 
vious states s ~. A similar approach has been 
developed recently in the context of maximum 
entropy classifiers in (McCallum et al, 2000). 
3 Const ra in t  Sat i s fac t ion  w i th  
Class i f ie rs  
The approach is based on an extension of 
the Boolean constraint satisfaction formal- 
ism (Mackworth, 1992) to handle variables that 
are outcomes of classifiers. As before, we as- 
sume an observed string 0 =< ol,o2,. . .  On > 
and local classifiers that, w.l.o.g., take two dis- 
tinct values, one indicating the openning a 
phrase and a second indicating closing it (OC 
modeling). The classifiers provide their outputs 
in terms of the probability P(o) and P(c), given 
the observation. 
To formalize this, let E be the set of all possi- 
ble phrases. All the non-overlapping constraints 
can be encoded in: f --/ke~ overlaps ej (-~eiV-~ej). 
Each solution to this formulae corresponds to a 
legitimate set of phrases. 
Our problem, however, is not simply to find 
an assignment  : E -+ {0, 1} that satisfies f
but rather to optimize some criterion. Hence, 
we associate a cost function c : E ~ \[0,1\] 
with each variable, and then find a solution ~- 
of f of minimum cost, c(~-) = n Ei=l 
In phrase identification, the solution to the op- 
timization problem corresponds to a shortest 
path in a directed acyclic graph constructed 
on the observation symbols, with legitimate 
phrases (the variables in E) as its edges and 
their costs as the weights. Each path in this 
graph corresponds to a satisfying assignment 
and the shortest path is the optimal solution. 
A natural cost function is to use the classi- 
fiers probabilities P(o) and P(c) and define, for 
a phrase e = (o, c), c(e) = 1 - P(o)P(c) which 
means that the error in selecting e is the er- 
ror in selecting either o or c, and allowing those 
108 
to overlap 4. The constant in 1 - P(o)P(c) bi- 
ases the minimization to prefers selecting a few 
phrases, possibly no phrase, so instead we min- 
imize -P(o) P(c). 
4 Sha l low Pars ing  
The above mentioned approaches are evaluated 
on shallow parsing tasks, We use the OC mod- 
eling and learn two classifiers; one predicting 
whether there should be a open in location t 
or not, and the other whether there should a 
close in location t or not. For technical reasons 
it is easier to keep track of the constraints if 
the cases --1 o and --1 c are separated according to 
whether we are inside or outside a phrase. Con- 
sequently, each classifier may output three pos- 
sible outcomes O, nOi,  nOo (open, not open 
inside, not open outside) and C, nCi,  nCo,  
resp. The state-transition diagram in figure 1 
captures the order constraints. Our modeling of 
the problem is a modification of our earlier work 
on this topic that has been found to be quite 
successful compared to other learning methods 
attempted on this problem (Mufioz et al, 1999) 
and in particular, better than the IO modeling 
of the problem (Mufioz et al, 1999). 
Figure 1: State-transition diagramfor the 
phrase recognition problem. 
The classifier we use to learn the states as 
a function of the observations i SNoW (Roth, 
1998; Carleson et al, 1999), a multi-class clas- 
sifter that is specifically tailored for large scale 
learning tasks. The SNoW learning architec- 
ture learns a sparse network of linear functions, 
in which the targets (states, in this case) are 
represented as linear functions over a common 
feature space. Typically, SNoW is used as a 
classifier, and predicts using a winner-take-all 
4Another solution in which the classifiers' uggestions 
inside each phrase axe also accounted for is possible. 
mechanism over the activation value of the tax- 
get classes in this case. The activation value 
itself is computed using a sigmoid function over 
the linear sum. In this case, instead, we normal- 
ize the activation levels of all targets to sum to 1 
and output the outcomes for all targets (states). 
We verified experimentally on the training data 
that the output for each state is indeed a dis- 
tribution function and can be used in further 
processing as P(slo ) (details omitted). 
5 Experiments 
We experimented both with base noun phrases 
(NP) and subject-verb patterns (SV) and show 
results for two different representations of the 
observations (that is, different feature sets for 
the classifiers) - part of speech (POS) tags only 
and POS with additional exical information 
(words). The data sets used are the standard 
data sets for this problem (Ramshaw and Max- 
cus, 1995; Argamon et al, 1999; Mufioz et 
al., 1999; Tjong Kim Sang and Veenstra, 1999) 
taken from the Wall Street Journal corpus in 
the Penn Treebank (Marcus et al, 1993). 
For each model we study three different clas- 
sifiers. The simple classifier corresponds to the 
standard HMM in which P(ols ) is estimated i- 
rectly from the data. The NB (naive Bayes) and 
SNoW classifiers use the same feature set, con- 
junctions of size 3 of POS tags (+ words) in a 
window of size 6 around the target word. 
The first important observation is that the 
SV task is significantly more difficult than the 
NP task. This is consistent for all models and 
all features ets. When comparing between dif- 
ferent models and features ets, it is clear that 
the simple HMM formalism is not competitive 
with the other two models. What is interest- 
ing here is the very significant sensitivity to the 
wider notion of observations (features) used by 
the classifiers, despite the violation of the prob- 
abilistic assumptions. For the easier NP task, 
the HMM model is competitive with the oth- 
ers when the classifiers used are NB or SNOW. 
In particular, a significant improvement in both 
probabilistic methods is achieved when their in- 
put is given by SNOW. 
Our two main methods, PMM and CSCL, 
perform very well on predicting NP and SV 
phrases with CSCL at least as good as any other 
methods tried on these tasks. Both for NPs and 
109 
Table 1: Results (F~=l) of different methods 
and comparison to previous works on NP and 
SV recognition. Notice that, in case of simple, 
the data with lexical features are too sparse to 
directly estimate the observation probability so 
we leave these entries empty. 
Method POS POS 
Model\[ Classifier only +words 
SNoW 90.64 92.89 
HMM NB 90.50 92.26 
Simple 87.83 
SNoW 90.61 92.98 
NP PMM NB 90.22 91.98 
Simple 61.44 
SNoW 90.87 92.88 
CSCL NB 90.49 91.95 
Simple 54.42 
Ramshaw & Marcus 90.6 92.0 
Argamon et al 91.6 N/A 
Mufioz et al 90.6 92.8 
Tjong Kim Sang 
Veenstra N/A 92.37 
SNoW 64.15 77.54 
HMM NB 75.40 78.43 
Simple 64.85 
SNoW 74.98 86.07 
PMM NB 74.80 84.80 
Simple 40.18 
SNoW 85.36 90.09 
CSCL NB 80.63 88.28 
Simple 59.27 
Argamon et al 86.5 N/A 
i Mufioz et al 88.1 92.0 
SV 
SVs, CSCL performs better than the probabilis- 
tic method, more significantly on the harder, 
SV, task. We attr ibute it to CSCL's ability to 
cope better with the length of the phrase and 
the long term dependencies. 
Our methods compare favorably with others 
with the exception to SV in (Mufioz et al, 
1999). Their method is fundamentally simi- 
lar to our CSCL; however, they incorporated 
the features from open in the close classifier al- 
lowing to exploit the dependencies between two 
classifiers. We believe that this is the main fac- 
tor of the significant difference in performance. 
6 Conc lus ion  
We have addressed the problem of combining 
the outcomes of several different classifiers in a 
way that provides a coherent inference that sat- 
isfies some constraints, While the probabilistic 
approach extends tandard and commonly used 
techniques for sequential decisions, it seems 
that the constraint satisfaction formalisms can 
support complex constraints and dependencies 
more flexibly. Future work will concentrate on 
these formalisms. 
Re ferences  
S. Argamon, I. Dagan, and Y. Krymolowski. 1999. 
A memory-based approach to learning shallow 
natural anguage patterns. Journal of Experimen- 
tal and Theoretical Artificial Intelligence, 10:1-22. 
C. Cardie and D. Pierce. 1998. Error-driven prun- 
ing of treebanks grammars for base noun phrase 
identification. In Proc. of ACL-98, pages 218-224. 
A. Carleson, C. Cumby, J. Rosen, and D. Roth. 
1999. The SNoW learning architecture. Tech. Re- 
port UIUCDCS-R-99-2101, UIUC Computer Sci- 
ence Department, May. 
K. W. Church. 1988. A stochastic parts program 
and noun phrase parser for unrestricted text. In 
Proe. of A CL Conference on Applied Natural Lan- 
guage Processing. 
A. K. Mackworth. 1992. Constraint Satisfaction. In 
Stuart C. Shapiro, editor, Encyclopedia of Artifi- 
cial Intelligence, pages 285-293. Vol. 1, 2 nd ed. 
M. P. Marcus, B. Santorini, and M. Marcinkiewicz. 
1993. Building a large annotated corpus of En- 
glish: The Penn Treebank. Computational Lin- 
guistics, 19(2):313-330, June. 
A. McCallum, D. Freitag, and F. Pereira. 2000. 
Maximum entropy Markov models for information 
extraction and segmentation. In Proc. of ICML- 
2000. 
N. Morgan and H. Bourlard. 1995. Continuous 
speech recognition. IEEE Signal Processing Mag- 
azine, 12(3):25-42. 
M. Mufioz, V. Punyakanok, D. Roth, and D. Zimak. 
1999. A learning approach to shallow parsing. In 
Proc. of EMNLpo VLC'99. 
V. Punyakanok and D. Roth. 2000. Inference with 
classifiers. Tech. Report UIUCDCS-R-2000-2181, 
UIUC Computer Science Department, July. 
L. R. Rabiner. 1989. A tutorial on hidden Markov 
models and selected applications in speech recog- 
nition. Proc. of the IEEE, 77(2):257-285. 
L. A. Ramshaw and M. P. Marcus. 1995. Text 
chunking using transformation-based learning. In 
Proc. of WVLC'95. 
D. Roth. 1998. Learning to resolve natural lan- 
guage ambiguities: A unified approach. In Proc. 
of AAAI'98, pages 806-813. 
E. F. Tjong Kim Sang and J. Yeenstra. 1999. Rep- 
resenting text chunks. In Proc. of EA CL'99. 
110 
Semantic Role Labeling Via Generalized Inference Over Classifiers
Vasin Punyakanok, Dan Roth, Wen-tau Yih, Dav Zimak Yuancheng Tu
Department of Computer Science Department of Linguistics
University of Illinois at Urbana-Champaign
{punyakan,danr,yih,davzimak,ytu}@uiuc.edu
Abstract
We present a system submitted to the CoNLL-
2004 shared task for semantic role labeling.
The system is composed of a set of classifiers
and an inference procedure used both to clean
the classification results and to ensure struc-
tural integrity of the final role labeling. Lin-
guistic information is used to generate features
during classification and constraints for the in-
ference process.
1 Introduction
Semantic role labeling is a complex task to discover pat-
terns within sentences corresponding to semantic mean-
ing. We believe it is hopeless to expect high levels of per-
formance from either purely manual classifiers or purely
learned classifiers. Rather, supplemental linguistic infor-
mation must be used to support and correct a learning
system. The system we present here is composed of two
phases.
First, a set of phrase candidates is produced using two
learned classifiers?one to discover beginning positions
and one to discover end positions for each argument type.
Hopefully, this phase discovers a small superset of all
phrases in the sentence (for each verb).
In the second phase, the final prediction is made. First,
candidate phrases from the first phase are re-scored using
a classifier designed to determine argument type, given
a candidate phrase. Because phrases are considered as a
whole, global properties of the candidates can be used to
discover how likely it is that a phrase is of a given ar-
gument type. However, the set of possible role-labelings
is restricted by structural and linguistic constraints. We
encode these constraints using linear functions and use
integer programming to ensure the final prediction is con-
sistent (see Section 4).
2 SNoW Learning Architecture
The learning algorithm used is a variation of the Winnow
update rule incorporated in SNoW (Roth, 1998; Roth and
Yih, 2002), a multi-class classifier that is specifically tai-
lored for large scale learning tasks. SNoW learns a sparse
network of linear functions, in which the targets (phrase
border predictions or argument type predictions, in this
case) are represented as linear functions over a common
feature space. It incorporates several improvements over
the basic Winnow update rule. In particular, a regular-
ization term is added, which has the affect of trying to
separate the data with a think separator (Grove and Roth,
2001; Hang et al, 2002). In the work presented here we
use this regularization with a fixed parameter.
Experimental evidence has shown that SNoW activa-
tions are monotonic with the confidence in the prediction
Therefore, it can provide a good source of probability es-
timation. We use softmax (Bishop, 1995) over the raw ac-
tivation values as conditional probabilities. Specifically,
suppose the number of classes is n, and the raw activa-
tion values of class i is acti. The posterior estimation for
class i is derived by the following equation.
score(i) = pi =
eacti
?
1?j?n eactj
3 First Phase: Find Argument Candidates
The first phase is to predict the phrases of a given sen-
tence that correspond to some argument (given the verb).
Unfortunately, it turns out that it is difficult to predict the
exact phrases accurately. Therefore, the goal of the first
phase is to output a superset of the correct phrases by fil-
tering out unlikely candidates.
Specifically, we learn two classifiers, one to detect
beginning phrase locations and a second to detect end
phrase locations. Each multi-class classifier makes pre-
dictions over forty-three classes ? thirty-two argument
types, ten continuous argument types, one class to detect
not begging and one class to detect not end. The follow-
ing features are used:
? Word feature includes the current word, two words
before and two words after.
? Part-of-speech tag (POS) feature includes the POS
tags of the current word, two words before and after.
? Chunk feature includes the BIO tags for chunks of
the current word, two words before and after.
? Predicate lemma & POS tag show the lemma form
and POS tag of the active predicate.
? Voice feature indicates the voice (active/passive) of
the current predicate. This is extracted with a simple
rule: a verb is identified as passive if it follows a to-
be verb in the same phrase chuck and its POS tag
is VBN(past participle) or it immediately follows a
noun phrase.
? Position feature describes if the current word is be-
fore of after the predicate.
? Chunk pattern feature encodes the sequence of
chunks from the current words to the predicate.
? Clause tag indicates the boundary of clauses.
? Clause path feature is a path formed from a semi-
parsed tree containing only clauses and chunks.
Each clause is named with the chunk immediately
preceding it. The clause path is the path from predi-
cate to target word in the semi-parsed tree.
? Clause position feature is the position of the tar-
get word relative to the predicate in the semi-parsed
tree containing only clauses. Specifically, there
are four configurations?target word and predicate
share same parent, parent of target word is ancestor
of predicate, parent of predicate is ancestor of target
word, or otherwise.
Because each phrase consists of a single beginning and
a single ending, these classifiers can be used to construct
a set of potential phrases (by combining each predicted
begin with each predicted end after it of the same type).
Although the outputs of this phase are potential ar-
gument candidates, along with their types, the second
phase re-scores the arguments using all possible types.
After eliminating the types from consideration, the first
phase achieves 98.96% and 88.65% recall (overall, with-
out verb) on the training and the development set, respec-
tively. Because these are the only candidates that are
passed to the second phase, 88.65% is an upper bound
of the recall for our overall system.
4 Second Phase: Phrase Classification
The second phase of our system assigns the final argu-
ment classes to (a subset) of the phrases supplied from the
first phase. This task is accomplished in two steps. First,
a multi-class classifier is used to supply confidence scores
corresponding to how likely individual phrases are to
have specific argument types. Then we look for the most
likely solution over the whole sentence, given the matrix
of confidences and linguistic information that serves as a
set of global constraints over the solution space.
Again, the SNoW learning architecture is used to train
a multi-class classifier to label each phrase to one of
the argument types, plus a special class ? no argument.
Training examples are created from the phrase candidates
supplied from the first phase using the following features:
? Predicate lemma & POS tag, voice, position,
clause Path, clause position, chunk pattern Same
features as the first phase.
? Word & POS tag from the phrase, including the
first/last word and tag, and the head word1.
? Named entity feature tells if the target phrase is,
embeds, overlaps, or is embedded in a named entity.
? Chunk features are the same as named entity (but
with chunks, e.g. noun phrases).
? Length of the target phrase, in the numbers of words
and chunks.
? Verb class feature is the class of the active predicate
described in the frame files.
? Phrase type uses simple heuristics to identify the
target phrase like VP, PP, or NP.
? Sub-categorization describes the phrase structure
around the predicate. We separate the clause where
the predicate is in into three part ? the predicate
chunk, segments before and after the predicate. The
sequence of the phrase types of these three segments
is our feature.
? Baseline follows the rule of identifying AM-NEG
and AM-MOD and uses them as features.
? Clause coverage describes how much of local
clause (from the predicate) is covered by the target
phrase.
? Chunk pattern length feature counts the number of
patterns in the phrase.
? Conjunctions join every pair of the above features
as new features.
? Boundary words & POS tags include one or two
words/tags before and after the target phrase.
1We use simple rules to first decide if a candidate phrase
type is VP, NP, or PP. The headword of an NP phrase is the
right-most noun. Similarly, the left-most verb/proposition of a
VP/PP phrase is extracted as the headword
? Bigrams are pairs of words/tags in the window from
two words before the target to the first word of the
target, and also from the last word to two words after
the phrase.
? Sparse colocation picks one word/tag from the two
words before the phrase, the first word/tag, the last
word/tag of the phrase, and one word/tag from the
two words after the phrase to join as features.
Alternately, we could have derived a scoring function
from the first phase confidences of the open and closed
predictors for each argument type. This method has
proved useful in the literature for shallow parsing (Pun-
yakanok and Roth, 2001). However, it is hoped that ad-
ditional global features of the phrase would be necessary
due to the variety and complexity of the argument types.
See Table 1 for a comparison.
Formally (but very briefly), the phrase classifier is at-
tempting to assign labels to a set of phrases, S1:M , in-
dexed from 1 to M . Each phrase Si can take any label
from a set of phrase labels, P , and the indexed set of
phrases can take a set of labels, s1:M ? PM . If we as-
sume that the classifier returns a score, score(Si = si),
corresponding to the likelihood of seeing label si for
phrase Si, then, given a sentence, the unaltered inference
task that is solved by our system maximizes the score of
the phrase, score(S1:M = s1:M ),
s?1:M = argmax
s1:M?PM
score(S1:M = s1:M )
= argmax
s1:M?PM
M
?
i=1
score(Si = si).
(1)
The second step for phrase identification is eliminating
labelings using global constraints derived from linguistic
information and structural considerations. Specifically,
we limit the solution space through the used of a filter
function, F , that eliminates many phrase labelings from
consideration. It is interesting to contrast this with previ-
ous work that filters individual phrases (see (Carreras and
Ma`rquez, 2003)). Here, we are concerned with global
constraints as well as constraints on the phrases. There-
fore, the final labeling becomes
s?1:M = argmax
s1:M?F(PM)
M
?
i=1
score(Si = si) (2)
The filter function used considers the following con-
straints:
1. Arguments cannot cover the predicate except those
that contain only the verb or the verb and the follow-
ing word.
2. Arguments cannot overlap with the clauses (they can
be embedded in one another).
3. If a predicate is outside a clause, its arguments can-
not be embedded in that clause.
4. No overlapping or embedding phrases.
5. No duplicate argument classes for A0-A5,V.
6. Exactly one V argument per sentence.
7. If there is C-V, then there has to be a V-A1-CV pat-
tern.
8. If there is a R-XXX argument, then there has to be a
XXX argument.
9. If there is a C-XXX argument, then there has to be
a XXX argument; in addition, the C-XXX argument
must occur after XXX.
10. Given the predicate, some argument classes are ille-
gal (e.g. predicate ?stalk? can take only A0 or A1).
Constraint 1 is valid because all the arguments of a pred-
icate must lie outside the predicate. The exception is for
the boundary of the predicate itself. Constraint 1 through
constraint 3 are actually constraints that can be evaluated
on a per-phrase basis and thus can be applied to the indi-
vidual phrases at any time. For efficiency sake, we elimi-
nate these even before the second phase scoring is begun.
Constraints 5, 8, and 9 are valid for only a subset of the
arguments.
These constraints are easy to transform into linear con-
straints (for example, for each class c, constraint 5 be-
comes
?M
i=1[Si = c] ? 1) 2. Then the optimum solution
of the cost function given in Equation 2 can be found by
integer linear programming3. A similar method was used
for entity/relation recognition (Roth and Yih, 2004).
Almost all previous work on shallow parsing and
phrase classification has used Constraint 4 to ensure that
there are no overlapping phrases. By considering addi-
tional constraints, we show improved performance (see
Table 1).
5 Results
In this section, we present results. For the second phase,
we evaluate the quality of the phrase predictor. The re-
sult first evaluates the phrase classifier, given the perfect
phrase locations without using inference (i.e. F(PM ) =
PM ). The second, adds inference to the phrase classifica-
tion over the perfect classifiers (see Table 2). We evaluate
the overall performance of our system (without assum-
ing perfect phrases) by training and evaluating the phrase
classifier on the output from the first phase (see Table 3).
Finally,since this is a tagging task, we compare this
system with the basic tagger that we have, the CLCL
2where [x] is 1 if x is true and 0 otherwise
3(Xpress-MP, 2003) was used in all experiments to solve in-
teger linear programming.
Precision Recall F1
1st Phase, non-Overlap 70.54% 61.50% 65.71
1st Phase, All Const. 70.97% 60.74% 65.46
2nd Phase, non-Overlap 69.69% 64.75% 67.13
2nd Phase, All Const. 71.96% 64.93% 68.26
Table 1: Summary of experiments on the development set.
The phrase scoring is choosen from either the first phase or the
second phase and each is evaluated by considering simply non-
overlapping constraints or the full set of linguistic constraints.
To make a fair comparison, parameters were set seperately to
optimize performance when using the first phase results. All
results are for overall performance.
Precision Recall F1
Without Inference 86.95% 87.24% 87.10
With Inference 88.03% 88.23% 88.13
Table 2: Results of second phase phrase prediction and in-
ference assuming perfect boundary detection in the first phase.
Inference improves performance by restricting label sequences
rather than restricting structural properties since the correct
boundaries are given. All results are for overall performance
on the development set.
shallow parser from (Punyakanok and Roth, 2001), which
is equivalent to using the scoring function from the first
phase with only the non-overlapping constraints. Table 1
shows how how additional constraints over the standard
non-overlapping constraints improve performance on the
development set4.
6 Conclusion
We show that linguistic information is useful for semantic
role labeling used both to derive features and to derive
hard constraints on the output. We show that it is possible
to use integer linear programming to perform inference
that incorporates a wide variety of hard constraints that
would be difficult to incorporate using existing methods.
In addition, we provide further evidence supporting the
use of scoring phrases over scoring phrase boundaries for
complex tasks.
Acknowledgments This research is supported by
NSF grants ITR-IIS-0085836, ITR-IIS-0085980 and IIS-
9984168, EIA-0224453 and an ONR MURI Award. We
also thank AMD for their equipment donation and Dash
Optimization for free academic use of their Xpress-MP
software.
References
C. Bishop, 1995. Neural Networks for Pattern Recognition,
chapter 6.4: Modelling conditional distributions, page 215.
Oxford University Press.
4The test set was not publicly available to evaluate these re-
sults.
Precision Recall F?=1
Overall 70.07% 63.07% 66.39
A0 81.13% 77.70% 79.38
A1 74.21% 63.02% 68.16
A2 54.16% 41.04% 46.69
A3 47.06% 26.67% 34.04
A4 71.43% 60.00% 65.22
A5 0.00% 0.00% 0.00
AM-ADV 39.36% 36.16% 37.69
AM-CAU 45.95% 34.69% 39.53
AM-DIR 42.50% 34.00% 37.78
AM-DIS 52.00% 67.14% 58.61
AM-EXT 46.67% 50.00% 48.28
AM-LOC 33.47% 34.65% 34.05
AM-MNR 45.19% 36.86% 40.60
AM-MOD 92.49% 94.96% 93.70
AM-NEG 85.92% 96.06% 90.71
AM-PNC 32.79% 23.53% 27.40
AM-PRD 0.00% 0.00% 0.00
AM-TMP 59.77% 56.89% 58.30
R-A0 81.33% 76.73% 78.96
R-A1 58.82% 57.14% 57.97
R-A2 100.00% 22.22% 36.36
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 54.55% 42.86% 48.00
V 98.37% 98.37% 98.37
Table 3: Results on the test set.
X. Carreras and L. Ma`rquez. 2003. Phrase recognition by filter-
ing and ranking with perceptrons. In Proceedings of RANLP-
2003.
A. Grove and D. Roth. 2001. Linear concepts and hidden vari-
ables. Machine Learning, 42(1/2):123?141.
T. Hang, F. Damerau, , and D. Johnson. 2002. Text chunking
based on a generalization of winnow. Journal of Machine
Learning Research, 2:615?637.
V. Punyakanok and D. Roth. 2001. The use of classifiers in
sequential inference. In NIPS-13; The 2000 Conference on
Advances in Neural Information Processing Systems, pages
995?1001. MIT Press.
D. Roth and W. Yih. 2002. Probabilistic reasoning for entity
& relation recognition. In COLING 2002, The 19th Interna-
tional Conference on Computational Linguistics, pages 835?
841.
D. Roth and W. Yih. 2004. A linear programming formulation
for global inference in natural language tasks. In Proc. of
CoNLL-2004.
D. Roth. 1998. Learning to resolve natural language ambigui-
ties: A unified approach. In Proc. of AAAI, pages 806?813.
Xpress-MP. 2003. Dash Optimization. Xpress-MP.
http://www.dashoptimization.com/products.html.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 181?184, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Generalized Inference with Multiple Semantic Role Labeling Systems
Peter Koomen Vasin Punyakanok Dan Roth Wen-tau Yih
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{pkoomen2,punyakan,danr,yih}@uiuc.edu
Abstract
We present an approach to semantic role
labeling (SRL) that takes the output of
multiple argument classifiers and com-
bines them into a coherent predicate-
argument output by solving an optimiza-
tion problem. The optimization stage,
which is solved via integer linear pro-
gramming, takes into account both the rec-
ommendation of the classifiers and a set
of problem specific constraints, and is thus
used both to clean the classification results
and to ensure structural integrity of the fi-
nal role labeling. We illustrate a signifi-
cant improvement in overall SRL perfor-
mance through this inference.
1 SRL System Architecture
Our SRL system consists of four stages: prun-
ing, argument identification, argument classifica-
tion, and inference. In particular, the goal of pruning
and argument identification is to identify argument
candidates for a given verb predicate. The system
only classifies the argument candidates into their
types during the argument classification stage. Lin-
guistic and structural constraints are incorporated
in the inference stage to resolve inconsistent global
predictions. The inference stage can take as its input
the output of the argument classification of a single
system or of multiple systems. We explain the infer-
ence for multiple systems in Sec. 2.
1.1 Pruning
Only the constituents in the parse tree are considered
as argument candidates. In addition, our system ex-
ploits the heuristic introduced by (Xue and Palmer,
2004) to filter out very unlikely constituents. The
heuristic is a recursive process starting from the verb
whose arguments are to be identified. It first returns
the siblings of the verb; then it moves to the parent of
the verb, and collects the siblings again. The process
goes on until it reaches the root. In addition, if a con-
stituent is a PP (propositional phrase), its children
are also collected. Candidates consisting of only a
single punctuation mark are not considered.
This heuristic works well with the correct parse
trees. However, one of the errors by automatic
parsers is due to incorrect PP attachment leading to
missing arguments. To attempt to fix this, we con-
sider as arguments the combination of any consec-
utive NP and PP, and the split of NP and PP inside
the NP that was chosen by the previous heuristics.
1.2 Argument Identification
The argument identification stage utilizes binary
classification to identify whether a candidate is an
argument or not. We train and apply the binary clas-
sifiers on the constituents supplied by the pruning
stage. Most of the features used in our system are
standard features, which include
? Predicate and POS tag of predicate indicate the lemma
of the predicate and its POS tag.
? Voice indicates tbe voice of the predicate.
? Phrase type of the constituent.
? Head word and POS tag of the head word include head
word and its POS tag of the constituent. We use rules
introduced by (Collins, 1999) to extract this feature.
? First and last words and POS tags of the constituent.
? Two POS tags before and after the constituent.
? Position feature describes if the constituent is before or
after the predicate relative to the position in the sentence.
181
? Path records the traversal path in the parse tree from the
predicate to the constituent.
? Subcategorization feature describes the phrase structure
around the predicate?s parent. It records the immediate
structure in the parse tree that expands to its parent.
? Verb class feature is the class of the active predicate de-
scribed in PropBank Frames.
? Lengths of the target constituent, in the numbers of words
and chunks separately.
? Chunk tells if the target argument is, embeds, overlaps,
or is embedded in a chunk with its type.
? Chunk pattern length feature counts the number of
chunks from the predicate to the argument.
? Clause relative position is the position of the target word
relative to the predicate in the pseudo-parse tree con-
structed only from clause constituent. There are four
configurations?target constituent and predicate share the
same parent, target constituent parent is an ancestor of
predicate, predicate parent is an ancestor of target word,
or otherwise.
? Clause coverage describes how much of the local clause
(from the predicate) is covered by the argument. It is
round to the multiples of 1/4.
1.3 Argument Classification
This stage assigns the final argument labels to the ar-
gument candidates supplied from the previous stage.
A multi-class classifier is trained to classify the
types of the arguments supplied by the argument
identification stage. To reduce the excessive candi-
dates mistakenly output by the previous stage, the
classifier can also classify the argument as NULL
(?not an argument?) to discard the argument.
The features used here are the same as those used
in the argument identification stage with the follow-
ing additional features.
? Syntactic frame describes the sequential pattern of the
noun phrases and the predicate in the sentence. This is
the feature introduced by (Xue and Palmer, 2004).
? Propositional phrase head is the head of the first phrase
after the preposition inside PP.
? NEG and MOD feature indicate if the argument is a
baseline for AM-NEG or AM-MOD. The rules of the
NEG and MOD features are used in a baseline SRL sys-
tem developed by Erik Tjong Kim Sang (Carreras and
Ma`rquez, 2004).
? NE indicates if the target argument is, embeds, overlaps,
or is embedded in a named-entity along with its type.
1.4 Inference
The purpose of this stage is to incorporate some
prior linguistic and structural knowledge, such as
?arguments do not overlap? or ?each verb takes at
most one argument of each type.? This knowledge is
used to resolve any inconsistencies of argument clas-
sification in order to generate final legitimate pre-
dictions. We use the inference process introduced
by (Punyakanok et al, 2004). The process is formu-
lated as an integer linear programming (ILP) prob-
lem that takes as inputs the confidences over each
type of the arguments supplied by the argument clas-
sifier. The output is the optimal solution that maxi-
mizes the linear sum of the confidence scores (e.g.,
the conditional probabilities estimated by the argu-
ment classifier), subject to the constraints that en-
code the domain knowledge.
Formally speaking, the argument classifier at-
tempts to assign labels to a set of arguments, S1:M ,
indexed from 1 to M . Each argument Si can take
any label from a set of argument labels, P , and the
indexed set of arguments can take a set of labels,
c1:M ? PM . If we assume that the argument classi-
fier returns an estimated conditional probability dis-
tribution, Prob(Si = ci), then, given a sentence, the
inference procedure seeks an global assignment that
maximizes the following objective function,
c?1:M = argmax
c1:M?PM
M
?
i=1
Prob(Si = ci),
subject to linguistic and structural constraints. In
other words, this objective function reflects the ex-
pected number of correct argument predictions, sub-
ject to the constraints. The constraints are encoded
as the followings.
? No overlapping or embedding arguments.
? No duplicate argument classes for A0-A5.
? Exactly one V argument per predicate considered.
? If there is C-V, then there has to be a V-A1-CV pattern.
? If there is an R-arg argument, then there has to be an arg
argument.
? If there is a C-arg argument, there must be an arg argu-
ment; moreover, the C-arg argument must occur after arg.
? Given the predicate, some argument types are illegal (e.g.
predicate ?stalk? can take only A0 or A1). The illegal
types may consist of A0-A5 and their corresponding C-
arg and R-arg arguments. For each predicate, we look
for the minimum value of i such that the class Ai is men-
tioned in its frame file as well as its maximum value j.
All argument types Ak such that k < i or k > j are
considered illegal.
182
2 Inference with Multiple SRL Systems
The inference process allows a natural way to com-
bine the outputs from multiple argument classi-
fiers. Specifically, given k argument classifiers
which perform classification on k argument sets,
{S1, . . . , Sk}. The inference process aims to opti-
mize the objective function:
c?1:N = argmax
c1:N?PN
N
?
i=1
Prob(Si = ci),
where S1:N = ?ki=1 Si, and
Prob(Si = ci) = 1k
k
?
j=1
Probj(Si = ci),
where Probj is the probability output by system j.
Note that all systems may not output with the
same set of argument candidates due to the pruning
and argument identification. For the systems that do
not output for any candidate, we assign the proba-
bility with a prior to this phantom candidate. In par-
ticular, the probability of the NULL class is set to be
0.6 based on empirical tests, and the probabilities of
the other classes are set proportionally to their oc-
currence frequencies in the training data.
For example, Figure 1 shows the two candidate
sets for a fragment of a sentence, ?..., traders say,
unable to cool the selling panic in both stocks and
futures.? In this example, system A has two argu-
ment candidates, a1 = ?traders? and a4 = ?the sell-
ing panic in both stocks and futures?; system B has
three argument candidates, b1 = ?traders?, b2 = ?the
selling panic?, and b3 = ?in both stocks and fu-
tures?. The phantom candidates are created for a2,
a3, and b4 of which probability is set to the prior.
Specifically for this implementation, we first train
two SRL systems that use Collins? parser and Char-
niak?s parser respectively. In fact, these two parsers
have noticeably different output. In evaluation, we
run the system that was trained with Charniak?s
parser 5 times with the top-5 parse trees output by
Charniak?s parser1. Together we have six different
outputs per predicate. Per each parse tree output, we
ran the first three stages, namely pruning, argument
1The top parse tree were from the official output by CoNLL.
The 2nd-5th parse trees were output by Charniak?s parser.
cool
1
b1
b4
a4
a2
2b 3b
a3
..., traders say, unable to the selling panic in both stocks and futures.
a
Figure 1: Two SRL systems? output (a1, a4, b1, b2,
and b3), and phantom candidates (a2, a3, and b4).
identification, and argument classification. Then a
joint inference stage is used to resolve the incon-
sistency of the output of argument classification in
these systems.
3 Learning and Evaluation
The learning algorithm used is a variation of the
Winnow update rule incorporated in SNoW (Roth,
1998; Roth and Yih, 2002), a multi-class classi-
fier that is tailored for large scale learning tasks.
SNoW learns a sparse network of linear functions,
in which the targets (argument border predictions
or argument type predictions, in this case) are rep-
resented as linear functions over a common feature
space. It improves the basic Winnow multiplicative
update rule with a regularization term, which has the
effect of trying to separate the data with a large mar-
gin separator (Grove and Roth, 2001; Hang et al,
2002) and voted (averaged) weight vector (Freund
and Schapire, 1999).
Softmax function (Bishop, 1995) is used to con-
vert raw activation to conditional probabilities. If
there are n classes and the raw activation of class i
is acti, the posterior estimation for class i is
Prob(i) = e
acti
?
1?j?n eactj
.
In summary, training used both full and partial
syntactic information as described in Section 1. In
training, SNoW?s default parameters were used with
the exception of the separator thickness 1.5, the use
of average weight vector, and 5 training cycles. The
parameters are optimized on the development set.
Training for each system took about 6 hours. The
evaluation on both test sets which included running
183
Precision Recall F?=1
Development 80.05% 74.83% 77.35
Test WSJ 82.28% 76.78% 79.44
Test Brown 73.38% 62.93% 67.75
Test WSJ+Brown 81.18% 74.92% 77.92
Test WSJ Precision Recall F?=1
Overall 82.28% 76.78% 79.44
A0 88.22% 87.88% 88.05
A1 82.25% 77.69% 79.91
A2 78.27% 60.36% 68.16
A3 82.73% 52.60% 64.31
A4 83.91% 71.57% 77.25
A5 0.00% 0.00% 0.00
AM-ADV 63.82% 56.13% 59.73
AM-CAU 64.15% 46.58% 53.97
AM-DIR 57.89% 38.82% 46.48
AM-DIS 75.44% 80.62% 77.95
AM-EXT 68.18% 46.88% 55.56
AM-LOC 66.67% 55.10% 60.33
AM-MNR 66.79% 53.20% 59.22
AM-MOD 96.11% 98.73% 97.40
AM-NEG 97.40% 97.83% 97.61
AM-PNC 60.00% 36.52% 45.41
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 78.16% 76.72% 77.44
R-A0 89.72% 85.71% 87.67
R-A1 70.00% 76.28% 73.01
R-A2 85.71% 37.50% 52.17
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 85.71% 57.14% 68.57
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 72.34% 65.38% 68.69
V 98.92% 97.10% 98.00
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
with all six different parse trees (assumed already
given) and the joint inference took about 4.5 hours.
Precision Recall F?=1
Charniak-1 75.40% 74.13% 74.76
Charniak-2 74.21% 73.06% 73.63
Charniak-3 73.52% 72.31% 72.91
Charniak-4 74.29% 72.92% 73.60
Charniak-5 72.57% 71.40% 71.98
Collins 73.89% 70.11% 71.95
Joint inference 80.05% 74.83% 77.35
Table 2: The results of individual systems and the
result with joint inference on the development set.
Overall results on the development and test sets
are shown in Table 1. Table 2 shows the results of
individual systems and the improvement gained by
the joint inference on the development set.
4 Conclusions
We present an implementation of SRL system which
composed of four stages?1) pruning, 2) argument
identification, 3) argument classification, and 4) in-
ference. The inference provides a natural way to
take the output of multiple argument classifiers and
combines them into a coherent predicate-argument
output. Significant improvement in overall SRL per-
formance through this inference is illustrated.
Acknowledgments
We are grateful to Dash Optimization for the free
academic use of Xpress-MP. This research is sup-
ported by ARDA?s AQUAINT Program, DOI?s Re-
flex program, and an ONR MURI Award.
References
C. Bishop, 1995. Neural Networks for Pattern Recognition,
chapter 6.4: Modelling conditional distributions, page 215.
Oxford University Press.
X. Carreras and L. Ma`rquez. 2004. Introduction to the conll-
2004 shared tasks: Semantic role labeling. In Proc. of
CoNLL-2004.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, Computer Science Depart-
ment, University of Pennsylvenia, Philadelphia.
Y. Freund and R. Schapire. 1999. Large margin classifica-
tion using the perceptron algorithm. Machine Learning,
37(3):277?296.
A. Grove and D. Roth. 2001. Linear concepts and hidden vari-
ables. Machine Learning, 42(1/2):123?141.
T. Hang, F. Damerau, and D. Johnson. 2002. Text chunking
based on a generalization of winnow. Journal of Machine
Learning Research, 2:615?637.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Seman-
tic role labeling via integer linear programming inference. In
Proc. of COLING-2004.
D. Roth and W. Yih. 2002. Probabilistic reasoning for entity &
relation recognition. In Proc. of COLING-2002, pages 835?
841.
D. Roth. 1998. Learning to resolve natural language ambigui-
ties: A unified approach. In Proc. of AAAI, pages 806?813.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proc. of the EMNLP-2004, pages 88?94,
Barcelona, Spain.
184
Semantic Role Labeling via Integer Linear Programming Inference
Vasin Punyakanok Dan Roth Wen-tau Yih Dav Zimak
Department of Computer Science
University of Illinois at Urbana-Champaign
{punyakan,danr,yih,davzimak}@uiuc.edu
Abstract
We present a system for the semantic role la-
beling task. The system combines a machine
learning technique with an inference procedure
based on integer linear programming that sup-
ports the incorporation of linguistic and struc-
tural constraints into the decision process. The
system is tested on the data provided in CoNLL-
2004 shared task on semantic role labeling and
achieves very competitive results.
1 Introduction
Semantic parsing of sentences is believed to be an
important task toward natural language understand-
ing, and has immediate applications in tasks such
information extraction and question answering. We
study semantic role labeling(SRL). For each verb in
a sentence, the goal is to identify all constituents
that fill a semantic role, and to determine their roles,
such as Agent, Patient or Instrument, and their ad-
juncts, such as Locative, Temporal or Manner.
The PropBank project (Kingsbury and Palmer,
2002) provides a large human-annotated corpus
of semantic verb-argument relations. Specifically,
we use the data provided in the CoNLL-2004
shared task of semantic-role labeling (Carreras and
Ma`rquez, 2003) which consists of a portion of the
PropBank corpus, allowing us to compare the per-
formance of our approach with other systems.
Previous approaches to the SRL task have made
use of a full syntactic parse of the sentence in or-
der to define argument boundaries and to determine
the role labels (Gildea and Palmer, 2002; Chen and
Rambow, 2003; Gildea and Hockenmaier, 2003;
Pradhan et al, 2003; Pradhan et al, 2004; Sur-
deanu et al, 2003). In this work, following the
CoNLL-2004 shared task definition, we assume that
the SRL system takes as input only partial syn-
tactic information, and no external lexico-semantic
knowledge bases. Specifically, we assume as input
resources a part-of-speech tagger, a shallow parser
that can process the input to the level of based
chunks and clauses (Tjong Kim Sang and Buch-
holz, 2000; Tjong Kim Sang and De?jean, 2001),
and a named-entity recognizer (Tjong Kim Sang
and De Meulder, 2003). We do not assume a full
parse as input.
SRL is a difficult task, and one cannot expect
high levels of performance from either purely man-
ual classifiers or purely learned classifiers. Rather,
supplemental linguistic information must be used
to support and correct a learning system. So far,
machine learning approaches to SRL have incorpo-
rated linguistic information only implicitly, via the
classifiers? features. The key innovation in our ap-
proach is the development of a principled method to
combine machine learning techniques with linguis-
tic and structural constraints by explicitly incorpo-
rating inference into the decision process.
In the machine learning part, the system we
present here is composed of two phases. First, a
set of argument candidates is produced using two
learned classifiers?one to discover beginning po-
sitions and one to discover end positions of each
argument type. Hopefully, this phase discovers a
small superset of all arguments in the sentence (for
each verb). In a second learning phase, the candi-
date arguments from the first phase are re-scored
using a classifier designed to determine argument
type, given a candidate argument.
Unfortunately, it is difficult to utilize global prop-
erties of the sentence into the learning phases.
However, the inference level it is possible to in-
corporate the fact that the set of possible role-
labelings is restricted by both structural and lin-
guistic constraints?for example, arguments cannot
structurally overlap, or, given a predicate, some ar-
gument structures are illegal. The overall decision
problem must produce an outcome that consistent
with these constraints. We encode the constraints as
linear inequalities, and use integer linear program-
ming(ILP) as an inference procedure to make a fi-
nal decision that is both consistent with the con-
straints and most likely according to the learning
system. Although ILP is generally a computation-
ally hard problem, there are efficient implementa-
tions that can run on thousands of variables and con-
straints. In our experiments, we used the commer-
cial ILP package (Xpress-MP, 2003), and were able
to process roughly twenty sentences per second.
2 Task Description
The goal of the semantic-role labeling task is to dis-
cover the verb-argument structure for a given input
sentence. For example, given a sentence ? I left my
pearls to my daughter-in-law in my will?, the goal is
to identify different arguments of the verb left which
yields the output:
[A0 I] [V left ] [A1 my pearls] [A2 to my daughter-
in-law] [AM-LOC in my will].
Here A0 represents the leaver, A1 represents the
thing left, A2 represents the benefactor, AM-LOC
is an adjunct indicating the location of the action,
and V determines the verb.
Following the definition of the PropBank, and
CoNLL-2004 shared task, there are six different
types of arguments labelled as A0-A5 and AA.
These labels have different semantics for each verb
as specified in the PropBank Frame files. In addi-
tion, there are also 13 types of adjuncts labelled as
AM-XXX where XXX specifies the adjunct type.
In some cases, an argument may span over differ-
ent parts of a sentence, the label C-XXX is used to
specify the continuity of the arguments, as shown in
the example below.
[A1 The pearls] , [A0 I] [V said] , [C-A1 were left
to my daughter-in-law].
Moreover in some cases, an argument might be a
relative pronoun that in fact refers to the actual agent
outside the clause. In this case, the actual agent is la-
beled as the appropriate argument type, XXX, while
the relative pronoun is instead labeled as R-XXX.
For example,
[A1 The pearls] [R-A1 which] [A0 I] [V left] , [A2
to my daughter-in-law] are fake.
See the details of the definition in Kingsbury and
Palmer (2002) and Carreras and Ma`rquez (2003).
3 System Architecture
Our semantic role labeling system consists of two
phases. The first phase finds a subset of arguments
from all possible candidates. The goal here is to
filter out as many as possible false argument candi-
dates, while still maintaining high recall. The sec-
ond phase focuses on identifying the types of those
argument candidates. Since the number of candi-
dates is much fewer, the second phase is able to use
slightly complicated features to facilitate learning
a better classifier. This section first introduces the
learning system we use and then describes how we
learn the classifiers in these two phases.
3.1 SNoW Learning Architecture
The learning algorithm used is a variation of the
Winnow update rule incorporated in SNoW (Roth,
1998; Roth and Yih, 2002), a multi-class classifier
that is specifically tailored for large scale learning
tasks. SNoW learns a sparse network of linear func-
tions, in which the targets (argument border predic-
tions or argument type predictions, in this case) are
represented as linear functions over a common fea-
ture space. It incorporates several improvements
over the basic Winnow multiplicative update rule.
In particular, a regularization term is added, which
has the effect of trying to separate the data with a
thick separator (Grove and Roth, 2001; Hang et al,
2002). In the work presented here we use this regu-
larization with a fixed parameter.
Experimental evidence has shown that SNoW
activations are monotonic with the confidence in
the prediction. Therefore, it can provide a good
source of probability estimation. We use soft-
max (Bishop, 1995) over the raw activation values
as conditional probabilities, and also the score of the
target. Specifically, suppose the number of classes
is n, and the raw activation values of class i is acti.
The posterior estimation for class i is derived by the
following equation.
score(i) = pi = e
acti
?
1?j?n eactj
The score plays an important role in different
places. For example, the first phase uses the scores
to decide which argument candidates should be fil-
tered out. Also, the scores output by the second-
phase classifier are used in the inference procedure
to reason for the best global labeling.
3.2 First Phase: Find Argument Candidates
The first phase is to predict the argument candidates
of a given sentence that correspond to the active
verb. Unfortunately, it turns out that it is difficult to
predict the exact arguments accurately. Therefore,
the goal here is to output a superset of the correct
arguments by filtering out unlikely candidates.
Specifically, we learn two classifiers, one to de-
tect beginning argument locations and the other
to detect end argument locations. Each multi-
class classifier makes predictions over forty-three
classes?thirty-two argument types, ten continuous
argument types, and one class to detect not begin-
ning/not end. Features used for these classifiers are:
? Word feature includes the current word, two
words before and two words after.
? Part-of-speech tag (POS) feature includes the
POS tags of all words in a window of size two.
? Chunk feature includes the BIO tags for
chunks of all words in a window of size two.
? Predicate lemma & POS tag show the lemma
form and POS tag of the active predicate.
? Voice feature is the voice (active/passive) of
the current predicate. This is extracted with a
simple rule: a verb is identified as passive if it
follows a to-be verb in the same phrase chunk
and its POS tag is VBN(past participle) or it
immediately follows a noun phrase.
? Position feature describes if the current word
is before or after the predicate.
? Chunk pattern encodes the sequence of
chunks from the current words to the predicate.
? Clause tag indicates the boundary of clauses.
? Clause path feature is a path formed from a
semi-parsed tree containing only clauses and
chunks. Each clause is named with the chunk
preceding it. The clause path is the path from
predicate to target word in the semi-parse tree.
? Clause position feature is the position of the
target word relative to the predicate in the
semi-parse tree containing only clauses. There
are four configurations ? target word and pred-
icate share the same parent, target word parent
is an ancestor of predicate, predicate parent is
an ancestor of target word, or otherwise.
Because each argument consists of a single be-
ginning and a single ending, these classifiers can be
used to construct a set of potential arguments (by
combining each predicted begin with each predicted
end after it of the same type).
Although this phase identifies typed arguments
(i.e. labeled with argument types), the second phase
will re-score each phrase using phrase-based classi-
fiers ? therefore, the goal of the first phase is sim-
ply to identify non-typed phrase candidates. In this
task, we achieves 98.96% and 88.65% recall (over-
all, without verb) on the training and the develop-
ment set, respectively. Because these are the only
candidates passed to the second phase, the final sys-
tem performance is upper-bounded by 88.65%.
3.3 Second Phase: Argument Classification
The second phase of our system assigns the final ar-
gument classes to (a subset) of the argument can-
didates supplied from the first phase. Again, the
SNoW learning architecture is used to train a multi-
class classifier to label each argument to one of the
argument types, plus a special class?no argument
(null). Training examples are created from the argu-
ment candidates supplied from the first phase using
the following features:
? Predicate lemma & POS tag, voice, position,
clause Path, clause position, chunk pattern
Same features as those in the first phase.
? Word & POS tag from the argument, includ-
ing the first,last,and head1 word and tag.
? Named entity feature tells if the target argu-
ment is, embeds, overlaps, or is embedded in a
named entity with its type.
? Chunk tells if the target argument is, embeds,
overlaps, or is embedded in a chunk with its
type.
? Lengths of the target argument, in the numbers
of words and chunks separately.
? Verb class feature is the class of the active
predicate described in PropBank Frames.
? Phrase type uses simple heuristics to identify
the target argument as VP, PP, or NP.
? Sub-categorization describes the phrase
structure around the predicate. We separate
the clause where the predicate is in into three
parts?the predicate chunk, segments before
and after the predicate, and use the sequence
of phrase types of these three segments.
? Baseline features identified not in the main
verb chunk as AM-NEG and modal verb in the
main verb chunk as AM-MOD.
? Clause coverage describes how much of the
local clause (from the predicate) is covered by
the target argument.
? Chunk pattern length feature counts the num-
ber of patterns in the argument.
? Conjunctions join every pair of the above fea-
tures as new features.
? Boundary words & POS tag include two
words/tags before and after the target argu-
ment.
? Bigrams are pairs of words/tags in the window
from two words before the target to the first
word of the target, and also from the last word
to two words after the argument.
1We use simple rules to first decide if a candidate phrase
type is VP, NP, or PP. The headword of an NP phrase is the
right-most noun. Similarly, the left-most verb/proposition of a
VP/PP phrase is extracted as the headword
? Sparse collocation picks one word/tag from
the two words before the argument, the first
word/tag, the last word/tag of the argument,
and one word/tag from the two words after the
argument to join as features.
Although the predictions of the second-phase
classifier can be used directly, the labels of argu-
ments in a sentence often violate some constraints.
Therefore, we rely on the inference procedure to
make the final predictions.
4 Inference via ILP
Ideally, if the learned classifiers are perfect, argu-
ments can be labeled correctly according to the clas-
sifiers? predictions. In reality, labels assigned to ar-
guments in a sentence often contradict each other,
and violate the constraints arising from the struc-
tural and linguistic information. In order to resolve
the conflicts, we design an inference procedure that
takes the confidence scores of each individual ar-
gument given by the second-phase classifier as in-
put, and outputs the best global assignment that
also satisfies the constraints. In this section we first
introduce the constraints and the inference prob-
lem in the semantic role labeling task. Then, we
demonstrate how we apply integer linear program-
ming(ILP) to reason for the global label assignment.
4.1 Constraints over Argument Labeling
Formally, the argument classifier attempts to assign
labels to a set of arguments, S1:M , indexed from 1
to M . Each argument Si can take any label from a
set of argument labels, P , and the indexed set of
arguments can take a set of labels, c1:M ? PM .
If we assume that the classifier returns a score,
score(Si = ci), corresponding to the likelihood of
seeing label ci for argument Si, then, given a sen-
tence, the unaltered inference task is solved by max-
imizing the overall score of the arguments,
c?1:M = argmax
c1:M?PM
score(S1:M = c1:M )
= argmax
c1:M?PM
M?
i=1
score(Si = ci).
(1)
In the presence of global constraints derived from
linguistic information and structural considerations,
our system seeks for a legitimate labeling that max-
imizes the score. Specifically, it can be viewed as
the solution space is limited through the use of a fil-
ter function, F , that eliminates many argument la-
belings from consideration. It is interesting to con-
trast this with previous work that filters individual
phrases (see (Carreras and Ma`rquez, 2003)). Here,
we are concerned with global constraints as well as
constraints on the arguments. Therefore, the final
labeling becomes
c?1:M = argmax
c1:M?F(PM )
M?
i=1
score(Si = ci) (2)
The filter function used considers the following con-
straints:
1. Arguments cannot cover the predicate except
those that contain only the verb or the verb and
the following word.
2. Arguments cannot overlap with the clauses
(they can be embedded in one another).
3. If a predicate is outside a clause, its arguments
cannot be embedded in that clause.
4. No overlapping or embedding arguments.
5. No duplicate argument classes for A0?A5,V.
6. Exactly one V argument per verb.
7. If there is C-V, then there should be a sequence
of consecutive V, A1, and C-V pattern. For ex-
ample, when split is the verb in ?split it up?,
the A1 argument is ?it? and C-V argument is
?up?.
8. If there is an R-XXX argument, then there has
to be an XXX argument. That is, if an ar-
gument is a reference to some other argument
XXX, then this referenced argument must exist
in the sentence.
9. If there is a C-XXX argument, then there has
to be an XXX argument; in addition, the C-
XXX argument must occur after XXX. This is
stricter than the previous rule because the order
of appearance also needs to be considered.
10. Given the predicate, some argument classes
are illegal (e.g. predicate ?stalk? can take only
A0 or A1). This linguistic information can be
found in PropBank Frames.
We reformulate the constraints as linear
(in)equalities by introducing indicator variables.
The optimization problem (Eq. 2) is solved using
ILP.
4.2 Using Integer Linear Programming
As discussed previously, a collection of potential ar-
guments is not necessarily a valid semantic label-
ing since it must satisfy all of the constraints. In
this context, inference is the process of finding the
best (according to Equation 1) valid semantic labels
that satisfy all of the specified constraints. We take
a similar approach that has been previously used
for entity/relation recognition (Roth and Yih, 2004),
and model this inference procedure as solving an
ILP.
An integer linear program(ILP) is basically the
same as a linear program. The cost function and the
(in)equality constraints are all linear in terms of the
variables. The only difference in an ILP is the vari-
ables can only take integers as their values. In our
inference problem, the variables are in fact binary.
A general binary integer programming problem can
be stated as follows.
Given a cost vector p ? <d, a set of variables,
z = (z1, . . . , zd) and cost matrices C1 ? <t1 ?
<d,C2 ? <t2?<d , where t1 and t2 are the numbers
of inequality and equality constraints and d is the
number of binary variables. The ILP solution z? is
the vector that maximizes the cost function,
z? = argmax
z?{0,1}d
p ? z,
subject to C1z ? b1, and C2z = b2,
where b1,b2 ? <d, and for all z ? z, z ? {0, 1}.
To solve the problem of Equation 2 in this set-
ting, we first reformulate the original cost function?M
i=1 score(Si = ci) as a linear function over sev-
eral binary variables, and then represent the filter
function F using linear inequalities and equalities.
We set up a bijection from the semantic labeling
to the variable set z. This is done by setting z to a set
of indicator variables. Specifically, let zic = [Si =
c] be the indicator variable that represents whether
or not the argument type c is assigned to Si, and
let pic = score(Si = c). Equation 1 can then be
written as an ILP cost function as
argmax
z?{0,1}d
M?
i=1
|P|?
c=1
piczic,
subject to
|P|?
c=1
zic = 1 ?zic ? z,
which means that each argument can take only one
type. Note that this new constraint comes from the
variable transformation, and is not one of the con-
straints used in the filter function F .
Constraints 1 through 3 can be evaluated on a per-
argument basis ? the sake of efficiency, arguments
that violate these constraints are eliminated even
before given the second-phase classifier. Next, we
show how to transform the constraints in the filter
function into the form of linear (in)equalities over
z, and use them in this ILP setting.
Constraint 4: No overlapping or embedding If
arguments Sj1 , . . . , Sjk occupy the same word in a
sentence, then this constraint restricts only one ar-
guments to be assigned to an argument type. In
other words, k ? 1 arguments will be the special
class null, which means the argument candidate is
not a legitimate argument. If the special class null
is represented by the symbol ?, then for every set of
such arguments, the following linear equality repre-
sents this constraint.
k?
i=1
zji? = k ? 1
Constraint 5: No duplicate argument classes
Within the same sentence, several types of argu-
ments cannot appear more than once. For example,
a predicate can only take one A0. This constraint
can be represented using the following inequality.
M?
i=1
ziA0 ? 1
Constraint 6: Exactly one V argument For each
verb, there is one and has to be one V argument,
which represents the active verb. Similarly, this con-
straint can be represented by the following equality.
M?
i=1
ziV = 1
Constraint 7: V?A1?C-V pattern This con-
straint is only useful when there are three consec-
utive candidate arguments in a sentence. Suppose
arguments Sj1 , Sj2 , Sj3 are consecutive. If Sj3 is
C-V, then Sj1 and Sj2 have to be V and A1, respec-
tively. This if-then constraint can be represented by
the following two linear inequalities.
zj3C-V ? zj1V, and zj3C-V ? zj2A1
Constraint 8: R-XXX arguments Suppose the
referenced argument type is A0 and the reference
type is R-A0. The linear inequalities that represent
this constraint are:
?m ? {1, . . . ,M} :
M?
i=1
ziA0 ? zmR-A0
If there are ? reference argument pairs, then the
total number of inequalities needed is ?M .
Constraint 9: C-XXX arguments This con-
straint is similar to the reference argument con-
straints. The difference is that the continued argu-
ment XXX has to occur before C-XXX. Assume
that the argument pair is A0 and C-A0, and argu-
ment Sji appears before Sjk if i ? k. The linear
inequalities that represent this constraint are:
?m ? {2, . . . ,M} :
j?1?
i=1
zjiA0 ? zmR-A0
Constraint 10: Illegal argument types Given a
specific verb, some argument types should never oc-
cur. For example, most verbs don?t have arguments
A5. This constraint is represented by summing all
the corresponding indicator variables to be 0.
M?
i=1
ziA5 = 0
Using ILP to solve this inference problem en-
joys several advantages. Linear constraints are
very general, and are able to represent many types
of constraints. Previous approaches usually rely
on dynamic programming to resolve non over-
lapping/embedding constraints (i.e., Constraint 4)
when the data is sequential, but are unable to han-
dle other constraints. The ILP approach is flexible
enough to handle constraints regardless of the struc-
ture of the data. Although solving an ILP prob-
lem is NP-hard, with the help of todays commer-
cial numerical packages, this problem can usually
be solved very fast in practice. For instance, it only
takes about 10 minutes to solve the inference prob-
lem for 4305 sentences on a Pentium-III 800 MHz
machine in our experiments. Note that ordinary
search methods (e.g., beam search) are not neces-
sarily faster than solving an ILP problem and do not
guarantee the optimal solution.
5 Experimental Results
The system is evaluated on the data provided in
the CoNLL-2004 semantic-role labeling shared task
which consists of a portion of PropBank corpus.
The training set is extracted from TreeBank (Mar-
cus et al, 1993) section 15?18, the development set,
used in tuning parameters of the system, from sec-
tion 20, and the test set from section 21.
We first compare this system with the basic tagger
that we have, the CSCL shallow parser from (Pun-
yakanok and Roth, 2001), which is equivalent to us-
ing the scoring function from the first phase with
only the non-overlapping/embedding constraints. In
Prec. Rec. F?=1
1st-phase, non-overlap 70.54 61.50 65.71
1st-phase, All Const. 70.97 60.74 65.46
2nd-phase, non-overlap 69.69 64.75 67.13
2nd-phase, All Const. 71.96 64.93 68.26
Table 1: Summary of experiments on the development
set. All results are for overall performance.
Precision Recall F?=1
Without Inference 86.95 87.24 87.10
With Inference 88.03 88.23 88.13
Table 2: Results of second phase phrase prediction
and inference assuming perfect boundary detection in
the first phase. Inference improves performance by re-
stricting label sequences rather than restricting structural
properties since the correct boundaries are given. All re-
sults are for overall performance on the development set.
addition, we evaluate the effectiveness of using only
this constraint versus all constraints, as in Sec. 4.
Table 1 shows how additional constraints over the
standard non-overlapping constraints improve per-
formance on the development set. The argument
scoring is chosen from either the first phase or the
second phase and each is evaluated by considering
simply the non-overlapping/embedding constraint
or the full set of linguistic constraints. To make
a fair comparison, parameters were set separately
to optimize performance when using the first phase
results. In general, using all constraints increases
F?=1 by about 1% in this system, but slightly de-
creases the performance when only the first phase
classifier is used. Also, using the two-phase archi-
tecture improves both precision and recall, and the
enhancement reflected in F?=1 is about 2.5%.
It is interesting to find out how well the second
phase classifier can perform given perfectly seg-
mented arguments. This evaluates the quality of the
argument classifier, and also provides a conceptual
upper bound. Table 2 first shows the results without
using inference (i.e. F(PM ) = PM ). The second
row shows adding inference to the phrase classifica-
tion can further improve F?=1 by 1%.
Finally, the overall result on the official test set
is given in Table 3. Note that the result here is not
comparable with the best in this domain (Pradhan et
al., 2004) where the full parse tree is assumed given.
For a fair comparison, our system was among the
best at CoNLL-04, where the best system (Hacioglu
et al, 2004) achieve a 69.49 F1 score.
6 Conclusion
We show that linguistic information is useful for se-
mantic role labeling, both in extracting features and
Dist. Prec. Rec. F?=1
Overall 100.00 70.07 63.07 66.39
A0 26.87 81.13 77.70 79.38
A1 35.73 74.21 63.02 68.16
A2 7.44 54.16 41.04 46.69
A3 1.56 47.06 26.67 34.04
A4 0.52 71.43 60.00 65.22
AM-ADV 3.20 39.36 36.16 37.69
AM-CAU 0.51 45.95 34.69 39.53
AM-DIR 0.52 42.50 34.00 37.78
AM-DIS 2.22 52.00 67.14 58.61
AM-EXT 0.15 46.67 50.00 48.28
AM-LOC 2.38 33.47 34.65 34.05
AM-MNR 2.66 45.19 36.86 40.60
AM-MOD 3.51 92.49 94.96 93.70
AM-NEG 1.32 85.92 96.06 90.71
AM-PNC 0.89 32.79 23.53 27.40
AM-TMP 7.78 59.77 56.89 58.30
R-A0 1.66 81.33 76.73 78.96
R-A1 0.73 58.82 57.14 57.97
R-A2 0.09 100.00 22.22 36.36
R-AM-TMP 0.15 54.55 42.86 48.00
Table 3: Results on the test set.
deriving hard constraints on the output. We also
demonstrate that it is possible to use integer linear
programming to perform inference that incorporates
a wide variety of hard constraints, which would be
difficult to incorporate using existing methods. In
addition, we provide further evidence supporting
the use of scoring arguments over scoring argument
boundaries for complex tasks. In the future, we plan
to use the full PropBank corpus to see the improve-
ment when more training data is provided. In addi-
tion, we would like to explore the possibility of in-
teger linear programming approach using soft con-
straints. As more constraints are considered, we ex-
pect the overall performance to improve.
7 Acknowledgments
We thank Xavier Carreras and Llu??s Ma`rquez for the
data and scripts, Martha Palmer and the anonymous
referees for their useful comments, AMD for their
equipment donation, and Dash Optimization for the
free academic use of their Xpress-MP software.
This research is supported by NSF grants ITR-IIS-
0085836, ITR-IIS-0085980 and IIS-9984168, EIA-
0224453 and an ONR MURI Award.
References
C. Bishop, 1995. Neural Networks for Pattern Recogni-
tion, chapter 6.4: Modelling conditional distributions,
page 215. Oxford University Press.
X. Carreras and L. Ma`rquez. 2003. Phrase recognition
by filtering and ranking with perceptrons. In Proc. of
RANLP-2003.
J. Chen and O. Rambow. 2003. Use of deep linguistic
features for the recognition and labeling of semantic
arguments. In Proc. of EMNLP-2003, Sapporo, Japan.
D. Gildea and J. Hockenmaier. 2003. Identifying se-
mantic roles using combinatory categorial grammar.
In Proc. of the EMNLP-2003, Sapporo, Japan.
D. Gildea and M. Palmer. 2002. The necessity of parsing
for predicate argument recognition. In Proc. of ACL
2002, pages 239?246, Philadelphia, PA.
A. Grove and D. Roth. 2001. Linear concepts and hid-
den variables. Machine Learning, 42(1/2):123?141.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2004. Semantic role labeling by tagging
syntactic chunks. In Proc. of CoNLL-04.
T. Hang, F. Damerau, and D. Johnson. 2002. Text
chunking based on a generalization of winnow. J. of
Machine Learning Research, 2:615?637.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proc. of LREC-2002, Spain.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330, June.
S. Pradhan, K. Hacioglu, W. ward, J. Martin, and D. Ju-
rafsky. 2003. Semantic role parsing adding semantic
structure to unstructured text. In Proc. of ICDM-2003,
Melbourne, FL.
S. Pradhan, W. Ward, K. Hacioglu, J. H. Martin, and
D. Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In Proc. of NAACL-HLT
2004.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS-13; The 2000
Conference on Advances in Neural Information Pro-
cessing Systems, pages 995?1001. MIT Press.
D. Roth and W. Yih. 2002. Probabilistic reasoning for
entity & relation recognition. In Proc. of COLING-
2002, pages 835?841.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proc. of CoNLL-2004.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proc. of AAAI, pages
806?813.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proc. of ACL 2003.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In
Proc. of the CoNLL-2000 and LLL-2000.
E. F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proc. of
CoNLL-2003.
E. F. Tjong Kim Sang and H. De?jean. 2001. Introduction
to the CoNLL-2001 shared task: Clause identification.
In Proc. of the CoNLL-2001.
Xpress-MP. 2003. Dash Optimization. Xpress-MP.
http://www.dashoptimization.com/products.html.
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 341?345,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language Use: What can it Tell us? 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
 
 
Abstract 
For 20 years, information extraction has fo-
cused on facts expressed in text. In contrast, 
this paper is a snapshot of research in progress 
on inferring properties and relationships 
among participants in dialogs, even though 
these properties/relationships need not be ex-
pressed as facts. For instance, can a machine 
detect that someone is attempting to persuade 
another to action or to change beliefs or is as-
serting their credibility? We report results on 
both English and Arabic discussion forums. 
1 Introduction 
Extracting explicitly stated information has been 
tested in MUC1 and ACE2 evaluations. For exam-
ple, for the text Mushaima'a, head of the opposi-
tion Haq movement, an ACE system extracts the 
relation LeaderOf(Mushaima'a, HaqMovement). In 
TREC QA3 systems answered questions, e.g.  
?When was Mozart born??, for which the answer is 
contained in one or a few extracted text phrases.  
Sentiment analysis uses implicit meaning of 
text, but has focused primarily on text known to be 
rich in opinions (product reviews, editorials) and 
delves into only one aspect of implicit meaning.  
Our long-term goal is to predict social roles in 
informal group discussion from language uses 
(LU), even if those roles are not explicitly stated; 
for example, using the communication during a 
meeting, identify the leader of a group. This paper 
provides a snapshot of preliminary, ongoing re-
search in predicting two classes of language use: 
                                                          
1
 http://www-nlpir.nist.gov/related_projects/muc/ 
2
 http://www.nist.gov/speech/tests/ace/ 
3
 http://trec.nist.gov/data/qa.html 
Establish-Credibility and Attempt-To-Persuade. 
Technical challenges include dealing with the facts 
that those LUs are rare and subjective and that hu-
man judgments have low agreement.  
Our hybrid statistical & rule-based approach 
detects those two LUs in English and Arabic. Our 
results are that (1) annotation at the message (turn) 
level provides training data useful for predicting 
rare phenomena at the discussion level while re-
ducing the requirement for turn-level predictions to 
be accurate; (2)weighing subjective judgments 
overcomes the need for high annotator consistency. 
Because the phenomena are rare, always predicting 
the absence of a LU is a very high baseline. For 
English, the system beats those baselines. For Ara-
bic, more work is required, since only 10-20% of 
the amount of training data exists so far.  
2 Language Uses (LUs) 
A language use refers to an aspect of the social 
intention of how a communicator uses language.  
The information that supports a decision about an 
implicit social action or role is likely to be distrib-
uted over more than one turn in a dialog; therefore, 
a language use is defined, annotated, and predicted 
across a thread in the dialog. Because our current 
work uses discussion forums, threads provide a 
natural, explicit unit of analysis. Our current work 
studies two language uses.  
An Attempt-to-Persuade occurs when a poster 
tries to convince other participants to change their 
beliefs or actions over the course of a thread. Typi-
cally, there is at least some resistance on the part of 
the posters being persuaded. To distinguish be-
tween actual persuasion and discussions that in-
volve differing opinions, a poster needs to engage 
341
in multiple persuasion posts (turns) to be consid-
ered exhibiting the LU.  
Establish-Credibility occurs when a poster at-
tempts to increase their standing within the group. 
This can be evidenced with any of several moves, 
e.g., explicit statements of authority, demonstration 
expertise through knowledge, providing verifiable 
information (e.g., from a trusted source or citing 
confirmable facts), or providing a justified opinion 
(e.g., a logical argument or personal experience).  
3 Challenges 
There were two significant challenges: (a) sparsity 
of the LUs, and (b) inter-annotator agreement. To 
address the sparsity of data, we tried to automati-
cally select data that was likely to contain content 
of interest. Data selection focused on the number 
of messages and posters in a thread, as well as the 
frequency of known indicators like quotations. 
(withheld). Despite these efforts, the LUs of inter-
est were rare, especially in Arabic.  
Annotation was developed using cycles of 
guideline development, annotation, evaluation of 
agreement, and revision of guidelines. Elsewhere, 
similar, iterative annotation processes have yielded 
significant improvements in agreement for word 
sense and coreference (Hovy et al, 2006). While 
LUs were annotated for a poster over the full 
thread, annotators also marked specific messages 
in the thread for presence of evidence of the lan-
guage use. Table 1 includes annotator consistency 
at both the evidence (message) and LU level.   
 English Arabic 
 Msg LU Msg LU 
 Agr # Agr # Agr # Agr # 
Per. 0.68 4722 0.75 2151 0.57 652 0.49 360 
Cred. 0.66 3594 0.68 1609 0.35 652 0.45 360 
Table 1: Number of Annotated Data Units and Annota-
tor Agreement (measured as F) 
The consistency numbers for this task were sig-
nificantly lower than we have seen in other lan-
guage processing tasks. Discussions suggested that 
disagreement did not come from a misunderstand-
ing of the task but was the result of differing intui-
tions about difficult-to-define labels. In the 
following two sections, we describe how the eval-
uation framework and system development pro-
ceeded despite low levels of consistency.  
4 Evaluation Framework 
Task. The task is to predict for every participant in 
a given thread, whether the participant exhibits 
Attempt-to-Persuade and/or Establish-Credibility. 
If there is insufficient evidence of an LU for a par-
ticipant, then the LU value for that poster is nega-
tive. The external evaluation measured LU 
predictions. Internally we measured predictions of 
message-level evidence as well. 
Corpora. For English, 139 threads from 
Google Groups and LiveJournal have been anno-
tated for Attempt-to-Persuade, and 103 threads for 
Attempt-to-Establish-Credibility. For Arabic, 
threads were collected from al-handasa.net.4 31 
threads were annotated for both tasks. Counts of 
annotated messages appear in Table 1. 
Measures. Due to low annotator agreement, at-
tempting to resolve annotation disagreement by the 
standard adjudication process was too time-
consuming. Instead, the evaluation scheme, similar 
to the pyramid scheme used for summarization 
evaluation, assigns scores to each example based 
on its level of agreement among the annotators. 
Specifically, each example is assigned positive and 
negative scores, p = n+/N and n = n-/N, where n+ is 
the number of annotators that annotate the example 
as positive, and n- for the negative. N is the total 
number of annotators. A system that outputs posi-
tive on the example results in p correct and n incor-
rect. The system gets p incorrect and n correct for 
predicting negative. Partial accuracy and F-
measure can then be computed. 
Formally, let X = {xi} be a set of examples. 
Each example xi is associated with positive and 
negative scores, pi and ni. Let ri = 1 if the system 
outputs positive for example xi and 0 for negative. 
The partial accuracy, recall, precision, and F-
measure can be computed by: 
pA = 100??i(ripi+(1-ri)ni) / ?i(pi+ni) 
pR = 100??iripi / ?ipi 
pP = 100? ?iripi / ?iri 
pF = 2 pR pP/(pR+pP) 
The maximum pA and pF may be less than 100 
when there is disagreement between annotators. To 
achieve accuracy and F scores on a scale of 100, 
pA and pF are normalized using the maximum 
achievable scores with respect to the data. 
npA = 100?pA/max(pA) 
npF = 100?pF/max(pF) 
                                                          
4
 URLs and judgments are available by email. 
342
5 System and Empirical Results 
Our architecture is shown in Figure 1. We process 
a thread in three stages: (1) linguistic analysis of 
each message (post) to yield features, (2) Predic-
tion of message-level properties using an SVM on 
the extracted features, and (3) Simple rules that 
predict language uses over the thread.  
 
Figure 1: Message and LU Prediction 
Phase 1: The SERIF Information Extraction 
Engine extracts features which are designed to cap-
ture different aspects of the posts. The features in-
clude simple features that can be extracted from 
the surface text of the posts and the structure of the 
posts within the threads. These may correlate di-
rectly or indirectly correlate to the language uses. 
In addition, more syntactic and semantic-driven 
features are also used. These can indicate the spe-
cific purpose of the sentences; specifically target-
ing directives, imperatives, or shows authority. The 
following is a partial list of features which are used 
both in isolation and in combination with each oth-
er. 
Surface and structural features: average sen-
tence length; number of names, pronouns, and dis-
tinct entities; number of sentences, URLs (links), 
paragraphs and out-of-vocabulary words; special 
styles (bold, italics, stereotypical punctuation e.g. 
!!!! ), depth in thread, and presence of a quotation. 
Syntactic and semantic features: predicate-
argument structure including the main verb, sub-
ject, object, indirect object, adverbial modifier, 
modal modifier, and negation, imperative verbs, 
injection words, subjective words, and mentions of 
attack events. 
Phase 2: Given training data from the message 
level (Section 3), an SVM predicts if the post con-
tains evidence for an LU. The motivation for this 
level is (1) Posts provide a compact unit with reli-
ably extractable, specific, explicit features. (2) 
There is more training data at the post level. (3) 
Pointing to posts offers a more clear justification 
for the predictions. (4) In our experiments, errors 
here do not seem to percolate to the thread level. In 
fact, accuracy at the message level is not directly 
predictive of accuracy at the thread level. 
Phase 3: Given the infrequency of the Attempt-
to-Persuade and Establish-Credibility LUs, we 
wrote a few rules to predict LUs over threads, giv-
en the predictions at the message level. For in-
stance, if the number of messages with evidence 
for persuasion is greater than 2 from a given partic-
ipant, then the system predicts AttemptToPer-
suade. Phase 3 is by design somewhat robust to 
errors in Phase 2. To predict that a poster is exhib-
iting the Attempt-to-Persuade LU, the system need 
not find every piece of evidence that the LU is pre-
sent, but rather just needs to find sufficient evi-
dence for identifying the LU.  
Our message level classifiers were trained with 
an SVM that optimizes F-measure (Joachims, 
2005). Because annotation disagreement is a major 
challenge, we experimented with various ways to 
account for (and make use of) noisy, dual annotat-
ed text. Initially, we resolved the disagreement au-
tomatically, i.e. removing examples with 
disagreement; treating an example as negative if 
any annotator marked the example negative; and 
treating an example as positive if any annotator 
marked the example as positive. An alternative 
(and more principled) approach is to incorporate 
positive and negative scores for each example into 
the optimization procedure. Because each example 
was annotated by the same number of annotators (2 
in this case), we are able to treat each annotator?s 
decision as an independent example without aug-
menting the SVM optimization process.  
The results below use the training procedure 
that performed best on the leave-one-thread-out 
cross validation results (Table 23 and Table 34). 
Counts of threads appear in Section 4. We compare 
our system?s performance (S) with two simple 
baselines. Baseline-A (A) always predicts absent 
for the LU/evidence. Baseline-P (P) predicts posi-
tive (present) for all messages/LUs. Table 4Table 3 
shows results for predicting message level evi-
dence of an LU (Phase 2). Table 5Table 4 shows 
performance on the task of predicting an LU for 
each poster. 
The results show significantly worse perfor-
mance in Arabic than English-- not surprising con-
sidering 5-10-fold difference in training examples. 
Additionally, Arabic messages are much shorter, 
and the phenomena is even more rare (as illustrated 
by the high npA, accuracy, of the A baseline).  
343
  Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 72.5 83.2 0.0 0.0 77.6 95.0 0.0 0.0 
P 40.4 29.7 61.1 50.7 33.9 14.4 54.5 30.9 
S 86.5 81.3 79.2 61.9 86.7 95.5 73.9 54.0 
Table 43: Performance on Message Level Evidence 
 Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 90.9 86.7 0.0 0.0 87.7 90.2 0.0 0.0 
P 12.1 27.0 23.8 48.2 18.0 21.5 33.7 41.1 
S 94.6 88.3 76.8 38.8 95.1 92.4 80.0 36.0 
Table 54: Cross Validation Performance on Poster LUs  
Table 6Table 5 shows LU prediction results 
from an external evaluation on held out data. Un-
like our dataset, each example in the external eval-
uation dataset was annotated by 3 annotators. The 
results are similar to our internal experiment. 
 Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 96.2 98.4 0.0 0.0 93.6 94.0 93.6 0.0 
P 13.1 4.2 27.6 11.7 11.1 10.1 11.1 22.2 
S 96.5 94.6 75.1 59.1 97.7 92.5 97.7 24.7 
Table 65: External, Held-Out Results on Poster LUs  
6 Related Research 
Research in authorship profiling (Chung & Penne-
baker, 2007; Argamon et al in press; and Abbasi 
and Chen, 2005) has identified traits, such as sta-
tus, sex, age, gender, and native language. Models 
and predictions in this field have primarily used 
simple word-based features, e.g. occurrence and 
frequency of function words. 
Social science researchers have studied how so-
cial roles develop in online communities (Fisher, et 
al., 2006), and have attempted to categorize these 
roles in multiple ways (Golder and Donath 2004; 
Turner et al, 2005). Welser et al (2007) have in-
vestigated the feasibility of detecting such roles 
automatically using posting frequency (but not the 
content of the messages). 
Sentiment analysis requires understanding the 
implicit nature of the text. Work on perspective 
and sentiment analysis frequently uses a corpus 
known to be rich in sentiment such as reviews or 
editorials (e.g. (Hardisty, 2010), (Somasundaran& 
Weibe, 2009). The MPQA corpus (Weibe, 2005) 
annotates polarity for sentences in newswire, but 
the focus of this corpus is at the sentence level. 
Both the MPQA corpus and the various corpora of 
editorials and reviews have tended towards more 
formal, edited, non-conversational text. Our work 
in contrast, specifically targets interactive discus-
sions in an informal setting. Work outside of com-
putational linguistics that has looked at persuasion 
has tended to examine language in a persuasive 
context (e.g. sales, advertising, or negotiations).  
Like the current work, Strzalkowski, et al 
(2010) investigates language uses over informal 
dialogue. Their work focuses on chat transcripts in 
an experimental setting designed to be rich in the 
phenomena of interest. Like our work, their predic-
tions operate over the conversation, and not a sin-
gle utterance. The specific language uses in their 
work (topic/task control, involvement, and disa-
greement) are different than those discussed here. 
Our work also differs in the data type of interest. 
We work with threaded online discussions in 
which the phenomena in question are rare. Our 
annotators and system must distinguish between 
the language use and text that is opinionated with-
out an intention to persuade or establish credibility.   
7 Conclusions and Future Work 
In this work in progress, we presented a hybrid 
statistical & rule-based approach to detecting prop-
erties not explicitly stated, but evident from lan-
guage use. Annotation at the message (turn) level 
provided training data useful for predicting rare 
phenomena at the discussion level while reducing 
the need for turn-level predictions to be accurate. 
Weighing subjective judgments overcame the need 
for high annotator consistency. For English, the 
system beats both baselines with respect to accura-
cy and F, despite the fact that because the phenom-
ena are rare, always predicting the absence of a 
language use is a high baseline. For Arabic, more 
work is required, particularly since only 10-20% of 
the amount of training data exists so far. 
This work has explored LUs, the implicit, social 
purpose behind the words of a message. Future 
work will explore incorporating LU predictions to 
predict the social roles played by the participants in 
a thread, for example using persuasion and credi-
bility to establish which participants in a discus-
sion are serving as informal leaders.  
344
Acknowledgement 
This research was funded by the Office of the Director 
of National Intelligence (ODNI), Intelligence Advanced 
Research Projects Activity (IARPA), through the _____.  
All statements of fact, opinion or conclusions contained 
herein are those of the authors and should not be con-
strued as representing the official views or policies of 
IARPA, the ODNI or the U.S. Government. 
References 
Argamon, S., Koppel, M., Pennebaker, J.W., and Schler, 
J. (2009). ?Automatically profiling the author of 
an anonymous text?. Communications of the Asso-
ciation for Computing Machinery (CACM). Vol-
ume 52 Issue 2. 
Abbasi A., and Chen H. (2005). ?Applying authorship 
analysis to extremist-group web forum messages?. 
In IEEE Intelligent Systems, 20(5), pp. 67?75. 
Boyd, D, Golder, S, and Lotan, G. (2010). ?Tweet, 
Tweet, Retweet: Conversational Aspects of Re-
tweeting on Twitter.? HICSS-43. IEEE: Kauai, HI. 
Chung, C.K., and Pennebaker, J.W. (2007). ?The psy-
chological functions of function words?. In K. 
Fiedler (Ed.), Social communication, pp. 343-359. 
New York: Psychology Press. 
Golder S., and Donath J. (2004) "Social Roles in Elec-
tronic Communities," presented at the Association 
of Internet Researchers (AoIR). Brighton, England 
Hovy E., Marcus M., Palmer M., Ramshaw L., and 
Weischedel R. (2006). ?Ontonotes: The 90% solu-
tion?. In Proceedings of the Human Language 
Technology Conference of the NAACL, Compan-
ion Volume: Short Papers, pp. 57?60. Association 
for Computational Linguistics, New York City, 
USA. 
Joachims, T. (2005), ?A Support Vector Method for 
Multivariate Performance Measures?, Proceedings 
of the International Conference on Machine 
Learning (ICML). 
Kelly, J., Fisher, D., Smith, D., (2006) ?Friends, foes, 
and fringe: norms and structure in political discus-
sion networks?, Proceedings of the 2006 interna-
tional conference on Digital government research.  
NIST Speech Group. (2008). ?The ACE 2008 evalua-
tion plan: Assessment of Detection and Recogni-
tion of Entities and Relations Within and Across 
Documents?. 
http://www.nist.gov/speech/tests/ace/2008/doc/ace
08 -evalplan.v1.2d.pdf 
Ranganath, R., Jurafsky, D., and McFarland, D. (2009) 
?It?s Not You, it?s Me: Detecting Flirting and its 
Misperception in Speed-Dates? Proceedings of the 
2009 Conference on Empirical Methods in Natural 
Language Processing, pages 334?342. 
Somasundaran, S & Wiebe, J
 
(2009). Recognizing 
Stances in Online Debates. ACL-IJCNLP 2009. 
Strzalkowski, T, Broadwell, G, Stromer-Galley, J, 
Shaikh, S, Taylor, S and Webb, N. (2010) ?Model-
ing Socio-Cultural Phenomena in Discourse?. 
Proceedings of the 23rd International Conference 
on Computational Linguistics (Coling 2010), pag-
es 1038?1046, Beijing, August 2010 
Turner T. C., Smith M. A., Fisher D., and Welser H. T. 
(2005) ?Picturing Usenet: Mapping computer-
mediated collective action?. In Journal of Com-
puter-Mediated Communication, 10(4). 
Voorhees, E. & Tice, D. (2000)."Building a Question 
Answering Test Collection", Proceedings of 
SIGIR, pp. 200-207. 
Welser H. T., Gleave E., Fisher D., and Smith M., 
(2007). "Visualizing the signatures of social roles in 
online discussion groups," In The Journal of Social 
Structure, vol. 8, no. 2. 
Wiebe, J, Wilson, T and Cardie, C (2005). Annotating 
expressions of opinions and emotions in language. 
Language Resources and Evaluation, volume 39, is-
sue 2-3, pp. 165-210. 
 
 
345
