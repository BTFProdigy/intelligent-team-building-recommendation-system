Proceedings of NAACL HLT 2009: Tutorials, pages 7?8,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extracting World and Linguistic Knowledge fromWikipedia
Simone Paolo Ponzetto
Dept. of Computational Linguistics
University of Heidelberg
Heidelberg, Germany
http://www.cl.uni-heidelberg.de/?ponzetto
Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
Heidelberg, Germany
http://www.eml-research.de/?strube
Overview
Many research efforts have been devoted to develop robust statistical modeling techniques for many NLP
tasks. Our field is now moving towards more complex tasks (e.g. RTE, QA), which require to complement
these methods with a semantically rich representation based on world and linguistic knowledge (i.e. anno-
tated linguistic data). In this tutorial we show several approaches to extract this knowledge from Wikipedia.
This resource has attracted the attention of much work in the AI community, mainly because it provides
semi-structured information and a large amount of manual annotations. The purpose of this tutorial is to in-
troduce Wikipedia as a resource to the NLP community and to provide an introduction for NLP researchers
both from a scientific and a practical (i.e. data acquisition and processing issues) perspective.
Outline
The tutorial is divided into three main parts:
1. Extracting world knowledge from Wikipedia. We review methods aiming at extracting fully struc-
tured world knowledge from the content of the online encyclopedia. We show how to take categories,
hyperlinks and infoboxes as building blocks for a semantic network with unlabeled relations between
the concepts. The task of taxonomy induction then boils down to labeling the relations between these
concepts, e.g. with isa, part-of, instance-of, located-in, etc. relations.
2. Leveraging linguistic knowledge from Wikipedia. Wikipedia provides shallow markup annotations
which can be interpreted as manual annotations of linguistic phenomena. These ?annotations? include
word boundaries, word senses, named entities, translations of concepts in many languages. Further-
more, Wikipedia can be used as a multilingual comparable corpus.
3. Future directions. Knowledge derived from Wikipedia has the potential to become a resource as
important for NLP as WordNet. Also the Wikipedia edit history provides a repository of linguistic
knowledge which is to be exploited. Potential applications of the knowledge implicitly encoded in the
edit history include spelling corrections, natural language generation, text summarization, etc.
Target audience
This tutorial is designed for students and researchers in Computer Science and Computational Linguistics.
No prior knowledge of information extraction topics is assumed.
7
Speakers? bios
Simone Paolo Ponzetto is an assistant professor at the Computational Linguistics Department of the Univer-
sity of Heidelberg, Germany. His main research interests lie in the area of information extraction, knowledge
acquisition and engineering, lexical semantics, and their application to discourse-based phenomena.
Michael Strube is group leader of the NLP group at EML Research, a privately funded research institute
in Heidelberg, Germany. The NLP group focuses on the areas of semantics, pragmatics and discourse and
applications like summarization and information extraction.
8
Tutorial Abstracts of ACL-IJCNLP 2009, page 6,
Suntec, Singapore, 2 August 2009. c?2009 ACL and AFNLP
State-of-the-art NLP Approaches to Coreference Resolution:
Theory and Practical Recipes
Simone Paolo Ponzetto
Seminar fu?r Computerlinguistik
University of Heidelberg
ponzetto@cl.uni-heidelberg.de
Massimo Poesio
DISI
University of Trento
massimo.poesio@unitn.it
1 Introduction
The identification of different nominal phrases in
a discourse as used to refer to the same (discourse)
entity is essential for achieving robust natural lan-
guage understanding (NLU). The importance of
this task is directly amplified by the field of Natu-
ral Language Processing (NLP) currently moving
towards high-level linguistic tasks requiring NLU
capabilities such as e.g. recognizing textual entail-
ment. This tutorial aims at providing the NLP
community with a gentle introduction to the task
of coreference resolution from both a theoretical
and an application-oriented perspective. Its main
purposes are: (1) to introduce a general audience
of NLP researchers to the core ideas underlying
state-of-the-art computational models of corefer-
ence; (2) to provide that same audience with an
overview of NLP applications which can benefit
from coreference information.
2 Content Overview
1. Introduction to machine learning approaches
to coreference resolution. We start by focusing
on machine learning based approaches developed
in the seminal works from Soon et al (2001) and
Ng & Cardie (2002). We then analyze the main
limitations of these approaches, i.e. their cluster-
ing of mentions from a local pairwise classifica-
tion of nominal phrases in text. We finally move
on to present more complex models which attempt
to model coreference as a global discourse phe-
nomenon (Yang et al, 2003; Luo et al, 2004;
Daume? III & Marcu, 2005, inter alia).
2. Lexical and encyclopedic knowledge for
coreference resolution. Resolving anaphors to
their correct antecedents requires in many cases
lexical and encyclopedic knowledge. We accord-
ingly introduce approaches which attempt to in-
clude semantic information into the coreference
models from a variety of knowledge sources,
e.g. WordNet (Harabagiu et al, 2001), Wikipedia
(Ponzetto & Strube, 2006) and automatically har-
vested patterns (Poesio et al, 2002; Markert &
Nissim, 2005; Yang & Su, 2007).
3. Applications and future directions. We
present an overview of NLP applications which
have been shown to profit from coreference in-
formation, e.g. question answering and summa-
rization. We conclude with remarks on future
work directions. These include: a) bringing to-
gether approaches to coreference using semantic
information with global discourse modeling tech-
niques; b) exploring novel application scenarios
which could potentially benefit from coreference
resolution, e.g. relation extraction and extracting
events and event chains from text.
References
Daume? III, H. & D. Marcu (2005). A large-scale exploration
of effective global features for a joint entity detection and
tracking model. In Proc. HLT-EMNLP ?05, pp. 97?104.
Harabagiu, S. M., R. C. Bunescu & S. J. Maiorano (2001).
Text and knowledge mining for coreference resolution. In
Proc. of NAACL-01, pp. 55?62.
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla & S. Roukos
(2004). A mention-synchronous coreference resolution al-
gorithm based on the Bell Tree. In Proc. of ACL-04, pp.
136?143.
Markert, K. & M. Nissim (2005). Comparing knowledge
sources for nominal anaphora resolution. Computational
Linguistics, 31(3):367?401.
Ng, V. & C. Cardie (2002). Improving machine learning ap-
proaches to coreference resolution. In Proc. of ACL-02,
pp. 104?111.
Poesio, M., T. Ishikawa, S. Schulte im Walde & R. Vieira
(2002). Acquiring lexical knowledge for anaphora resolu-
tion. In Proc. of LREC ?02, pp. 1220?1225.
Ponzetto, S. P. & M. Strube (2006). Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolu-
tion. In Proc. of HLT-NAACL-06, pp. 192?199.
Soon, W. M., H. T. Ng & D. C. Y. Lim (2001). A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
Yang, X. & J. Su (2007). Coreference resolution using se-
mantic relatedness information from automatically dis-
covered patterns. In Proc. of ACL-07, pp. 528?535.
Yang, X., G. Zhou, J. Su & C. L. Tan (2003). Coreference
resolution using competition learning approach. In Proc.
of ACL-03, pp. 176?183.
6
Semantic Role Labeling for Coreference Resolution
Simone Paolo Ponzetto and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp/
Abstract
Extending a machine learning based coref-
erence resolution system with a feature
capturing automatically generated infor-
mation about semantic roles improves its
performance.
1 Introduction
The last years have seen a boost of work devoted
to the development of machine learning based
coreference resolution systems (Soon et al, 2001;
Ng & Cardie, 2002; Kehler et al, 2004, inter alia).
Similarly, many researchers have explored tech-
niques for robust, broad coverage semantic pars-
ing in terms of semantic role labeling (Gildea &
Jurafsky, 2002; Carreras & Ma`rquez, 2005, SRL
henceforth).
This paper explores whether coreference reso-
lution can benefit from SRL, more specifically,
which phenomena are affected by such informa-
tion. The motivation comes from the fact that cur-
rent coreference resolution systems are mostly re-
lying on rather shallow features, such as the dis-
tance between the coreferent expressions, string
matching, and linguistic form. On the other hand,
the literature emphasizes since the very begin-
ning the relevance of world knowledge and infer-
ence (Charniak, 1973). As an example, consider
a sentence from the Automatic Content Extraction
(ACE) 2003 data.
(1) A state commission of inquiry into the sinking of the
Kursk will convene in Moscow on Wednesday, the
Interfax news agency reported. It said that the diving
operation will be completed by the end of next week.
It seems that in this example, knowing that the In-
terfax news agency is the AGENT of the report
predicate, and It being the AGENT of say, could
trigger the (semantic parallelism based) inference
required to correctly link the two expressions, in
contrast to anchoring the pronoun to Moscow.
SRL provides the semantic relationships that
constituents have with predicates, thus allowing
us to include document-level event descriptive in-
formation into the relations holding between re-
ferring expressions (REs). This layer of semantic
context abstracts from the specific lexical expres-
sions used, and therefore represents a higher level
of abstraction than predicate argument statistics
(Kehler et al, 2004) and Latent Semantic Analy-
sis used as a model of world knowledge (Klebanov
& Wiemer-Hastings, 2002). In this respect, the
present work is closer in spirit to Ji et al (2005),
who explore the employment of the ACE 2004 re-
lation ontology as a semantic filter.
2 Coreference Resolution Using SRL
2.1 Corpora Used
The system was initially prototyped using the
MUC-6 and MUC-7 data sets (Chinchor & Sund-
heim, 2003; Chinchor, 2001), using the standard
partitioning of 30 texts for training and 20-30 texts
for testing. Then, we developed and tested the
system with the ACE 2003 Training Data cor-
pus (Mitchell et al, 2003)1. Both the Newswire
(NWIRE) and Broadcast News (BNEWS) sections
where split into 60-20-20% document-based par-
titions for training, development, and testing, and
later per-partition merged (MERGED) for system
evaluation. The distribution of coreference chains
and referring expressions is given in Table 1.
2.2 Learning Algorithm
For learning coreference decisions, we used a
Maximum Entropy (Berger et al, 1996) model.
Coreference resolution is viewed as a binary clas-
sification task: given a pair of REs, the classifier
has to decide whether they are coreferent or not.
First, a set of pre-processing components includ-
1We used the training data corpus only, as the availability
of the test data was restricted to ACE participants.
143
BNEWS NWIRE
#coref ch. #pron. #comm. nouns #prop. names #coref ch. #pron. #comm. nouns #prop. names
TRAIN. 587 876 572 980 904 1037 1210 2023
DEVEL 201 315 163 465 399 358 485 923
TEST 228 291 238 420 354 329 484 712
Table 1: Partitions of the ACE 2003 training data corpus
ing a chunker and a named entity recognizer is
applied to the text in order to identify the noun
phrases, which are further taken as REs to be used
for instance generation. Instances are created fol-
lowing Soon et al (2001). During testing the
classifier imposes a partitioning on the available
REs by clustering each set of expressions labeled
as coreferent into the same coreference chain.
2.3 Baseline System Features
Following Ng & Cardie (2002), our baseline sys-
tem reimplements the Soon et al (2001) system.
The system uses 12 features. Given a pair of can-
didate referring expressions REi and REj the fea-
tures are computed as follows2.
(a) Lexical features
STRING MATCH T if REi and REj have the
same spelling, else F.
ALIAS T if one RE is an alias of the other; else
F.
(b) Grammatical features
I PRONOUN T if REi is a pronoun; else F.
J PRONOUN T if REj is a pronoun; else F.
J DEF T if REj starts with the; else F.
J DEM T if REj starts with this, that, these, or
those; else F.
NUMBER T if both REi and REj agree in num-
ber; else F.
GENDER U if REi or REj have an undefined
gender. Else if they are both defined and agree
T; else F.
PROPER NAME T if both REi and REj are
proper names; else F.
APPOSITIVE T if REj is in apposition with
REi; else F.
(c) Semantic features
WN CLASS U if REi or REj have an undefined
WordNet semantic class. Else if they both have
a defined one and it is the same T; else F.
2Possible values are U(nknown), T(rue) and F(alse). Note
that in contrast to Ng & Cardie (2002) we classify ALIAS as
a lexical feature, as it solely relies on string comparison and
acronym string matching.
(d) Distance features
DISTANCE how many sentences REi and REj
are apart.
2.4 Semantic Role Features
The baseline system employs only a limited
amount of semantic knowledge. In particular, se-
mantic information is limited to WordNet seman-
tic class matching. Unfortunately, a simple Word-
Net semantic class lookup exhibits problems such
as coverage and sense disambiguation3, which
make the WN CLASS feature very noisy. As a
consequence, we propose in the following to en-
rich the semantic knowledge made available to the
classifier by using SRL information.
In our experiments we use the ASSERT
parser (Pradhan et al, 2004), an SVM based se-
mantic role tagger which uses a full syntactic
analysis to automatically identify all verb predi-
cates in a sentence together with their semantic
arguments, which are output as PropBank argu-
ments (Palmer et al, 2005). It is often the case
that the semantic arguments output by the parser
do not align with any of the previously identified
noun phrases. In this case, we pass a semantic role
label to a RE only in case the two phrases share the
same head. Labels have the form ?ARG1 pred1 . . .
ARGn predn? for n semantic roles filled by a
constituent, where each semantic argument label
ARGi is always defined with respect to a predicate
lemma predi. Given such level of semantic infor-
mation available at the RE level, we introduce two
new features4.
I SEMROLE the semantic role argument-
predicate pairs of REi.
3Following the system to be replicated, we simply
mapped each RE to the first WordNet sense of the head noun.
4During prototyping we experimented unpairing the ar-
guments from the predicates, which yielded worse results.
This is supported by the PropBank arguments always being
defined with respect to a target predicate. Binarizing the fea-
tures ? i.e. do REi and REj have the same argument or
predicate label with respect to their closest predicate? ? also
gave worse results.
144
MUC-6 MUC-7
original R P F1 R P F1
Soon et al 58.6 67.3 62.3 56.1 65.5 60.4
duplicated
baseline 64.9 65.6 65.3 55.1 68.5 61.1
Table 2: Results on MUC
J SEMROLE the semantic role argument-
predicate pairs of REj .
For the ACE 2003 data, 11,406 of 32,502 auto-
matically extracted noun phrases were tagged with
2,801 different argument-predicate pairs.
3 Experiments
3.1 Performance Metrics
We report in the following tables the MUC
score (Vilain et al, 1995). Scores in Table 2 are
computed for all noun phrases appearing in either
the key or the system response, whereas Tables 3
and 4 refer to scoring only those phrases which ap-
pear in both the key and the response. We discard
therefore those responses not present in the key,
as we are interested here in establishing the upper
limit of the improvements given by SRL.
We also report the accuracy score for all three
types of ACE mentions, namely pronouns, com-
mon nouns and proper names. Accuracy is the
percentage of REs of a given mention type cor-
rectly resolved divided by the total number of REs
of the same type given in the key. A RE is said
to be correctly resolved when both it and its direct
antecedent are in the same key coreference class.
In all experiments, the REs given to the clas-
sifier are noun phrases automatically extracted by
a pipeline of pre-processing components (i.e. PoS
tagger, NP chunker, Named Entity Recognizer).
3.2 Results
Table 2 compares the results between our du-
plicated Soon baseline and the original system.
The systems show a similar performance w.r.t. F-
measure. We speculate that the result improve-
ments are due to the use of current pre-processing
components and another classifier.
Tables 3 and 4 show a comparison of the per-
formance between our baseline system and the
one incremented with SRL. Performance improve-
ments are highlighted in bold. The tables show
that SRL tends to improve system recall, rather
than acting as a ?semantic filter? improving pre-
cision. Semantic roles therefore seem to trigger a
R P F1 Ap Acn Apn
baseline 54.5 88.0 67.3 34.7 20.4 53.1
+SRL 56.4 88.2 68.8 40.3 22.0 52.1
Table 4: Results ACE (merged BNEWS/NWIRE)
Feature Chi-square
STR MATCH 1.0
J SEMROLE 0.2096
ALIAS 0.1852
I SEMROLE 0.1594
SEMCLASS 0.1474
DIST 0.1107
GENDER 0.1013
J PRONOUN 0.0982
NUMBER 0.0578
I PRONOUN 0.0489
APPOSITIVE 0.0397
PROPER NAME 0.0141
DEF NP 0.0016
DEM NP 0.0
Table 5: ?2 statistic for each feature
response in cases where more shallow features do
not seem to suffice (see example (1)).
The RE types which are most positively affected
by SRL are pronouns and common nouns. On the
other hand, SRL information has a limited or even
worsening effect on the performance on proper
names, where features such as string matching and
alias seem to suffice. This suggests that SRL plays
a role in pronoun and common noun resolution,
where surface features cannot account for complex
preferences and semantic knowledge is required.
3.3 Feature Evaluation
We investigated the contribution of the different
features in the learning process. Table 5 shows
the chi-square statistic (normalized in the [0, 1] in-
terval) for each feature occurring in the training
data of the MERGED dataset. SRL features show
a high ?2 value, ranking immediately after string
matching and alias, which indicates a high corre-
lation of these features to the decision classes.
The importance of SRL is also indicated by the
analysis of the contribution of individual features
to the overall performance. Table 6 shows the per-
formance variations obtained by leaving out each
feature in turn. Again, it can be seen that remov-
ing both I and J SEMROLE induces a relatively
high performance degradation when compared to
other features. Their removal ranks 5th out of
12, following only essential features such as string
matching, alias, pronoun and number. Similarly
to Table 5, the semantic role of the anaphor ranks
higher than the one of the antecedent. This re-
145
BNEWS NWIRE
R P F1 Ap Acn Apn R P F1 Ap Acn Apn
baseline 46.7 86.2 60.6 36.4 10.5 44.0 56.7 88.2 69.0 37.7 23.1 55.6
+SRL 50.9 86.1 64.0 36.8 14.3 45.7 58.3 86.9 69.8 38.0 25.8 55.8
Table 3: Results on the ACE 2003 data (BNEWS and NWIRE sections)
Feature(s) removed ? F1
all features 68.8
STR MATCH ?21.02
ALIAS ?2.96
I/J PRONOUN ?2.94
NUMBER ?1.63
I/J SEMROLE ?1.50
J SEMROLE ?1.26
APPOSITIVE ?1.20
GENDER ?1.13
I SEMROLE ?0.74
DIST ?0.69
WN CLASS ?0.56
DEF NP ?0.57
DEM NP ?0.50
PROPER NAME ?0.49
Table 6: ? F1 from feature removal
lates to the improved performance on pronouns, as
it indicates that SRL helps for linking anaphoric
pronouns to preceding REs. Finally, it should
be noted that SRL provides much more solid and
noise-free semantic features when compared to the
WordNet class feature, whose removal induces al-
ways a lower performance degradation.
4 Conclusion
In this paper we have investigated the effects
of using semantic role information within a ma-
chine learning based coreference resolution sys-
tem. Empirical results show that coreference res-
olution can benefit from SRL. The analysis of the
relevance of features, which had not been previ-
ously addressed, indicates that incorporating se-
mantic information as shallow event descriptions
improves the performance of the classifier. The
generated model is able to learn selection pref-
erences in cases where surface morpho-syntactic
features do not suffice, i.e. pronoun resolution.
We speculate that this contrasts with the disap-
pointing findings of Kehler et al (2004) since SRL
provides a more fine grained level of information
when compared to predicate argument statistics.
As it models the semantic relationship that a syn-
tactic constituent has with a predicate, it carries in-
directly syntactic preference information. In addi-
tion, when used as a feature it allows the classifier
to infer semantic role co-occurrence, thus induc-
ing deep representations of the predicate argument
relations for learning in coreferential contexts.
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a
KTF grant (09.003.2004).
References
Berger, A., S. A. Della Pietra & V. J. Della Pietra (1996). A
maximum entropy approach to natural language process-
ing. Computational Linguistics, 22(1):39?71.
Carreras, X. & L. Ma`rquez (2005). Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proc. of CoNLL-05, pp. 152?164.
Charniak, E. (1973). Jack and Janet in search of a theory
of knowledge. In Advance Papers from the Third Inter-
national Joint Conference on Artificial Intelligence, Stan-
ford, Cal., pp. 337?343.
Chinchor, N. (2001). Message Understanding Conference
(MUC) 7. LDC2001T02, Philadelphia, Penn: Linguistic
Data Consortium.
Chinchor, N. & B. Sundheim (2003). Message Understand-
ing Conference (MUC) 6. LDC2003T13, Philadelphia,
Penn: Linguistic Data Consortium.
Gildea, D. & D. Jurafsky (2002). Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?288.
Ji, H., D. Westbrook & R. Grishman (2005). Using semantic
relations to refine coreference decisions. In Proc. HLT-
EMNLP ?05, pp. 17?24.
Kehler, A., D. Appelt, L. Taylor & A. Simma (2004). The
(non)utility of predicate-argument frequencies for pro-
noun interpretation. In Proc. of HLT-NAACL-04, pp. 289?
296.
Klebanov, B. & P. Wiemer-Hastings (2002). The role of
wor(l)d knowledge in pronominal anaphora resolution. In
Proceedings of the International Symposium on Reference
Resolution for Natural Language Processing, Alicante,
Spain, 3?4 June, 2002, pp. 1?8.
Mitchell, A., S. Strassel, M. Przybocki, J. Davis, G. Dod-
dington, R. Grishman, A. Meyers, A. Brunstain, L. Ferro
& B. Sundheim (2003). TIDES Extraction (ACE) 2003
Multilingual Training Data. LDC2004T09, Philadelphia,
Penn.: Linguistic Data Consortium.
Ng, V. & C. Cardie (2002). Improving machine learning ap-
proaches to coreference resolution. In Proc. of ACL-02,
pp. 104?111.
Palmer, M., D. Gildea & P. Kingsbury (2005). The proposi-
tion bank: An annotated corpus of semantic roles. Com-
putational Linguistics, 31(1):71?105.
Pradhan, S., W. Ward, K. Hacioglu, J. H. Martin & D. Juraf-
sky (2004). Shallow semantic parsing using support vector
machines. In Proc. of HLT-NAACL-04, pp. 233?240.
Soon, W. M., H. T. Ng & D. C. Y. Lim (2001). A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly &
L. Hirschman (1995). A model-theoretic coreference scor-
ing scheme. In Proceedings of the 6th Message Under-
standing Conference (MUC-6), pp. 45?52.
146
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 192?199,
New York, June 2006. c?2006 Association for Computational Linguistics
Exploiting Semantic Role Labeling, WordNet and Wikipedia
for Coreference Resolution
Simone Paolo Ponzetto and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
In this paper we present an extension of
a machine learning based coreference res-
olution system which uses features in-
duced from different semantic knowledge
sources. These features represent knowl-
edge mined from WordNet and Wikipedia,
as well as information about semantic role
labels. We show that semantic features in-
deed improve the performance on differ-
ent referring expression types such as pro-
nouns and common nouns.
1 Introduction
The last years have seen a boost of work devoted to
the development of machine learning based coref-
erence resolution systems (Soon et al, 2001; Ng &
Cardie, 2002; Yang et al, 2003; Luo et al, 2004,
inter alia). While machine learning has proved to
yield performance rates fully competitive with rule
based systems, current coreference resolution sys-
tems are mostly relying on rather shallow features,
such as the distance between the coreferent expres-
sions, string matching, and linguistic form. How-
ever, the literature emphasizes since the very begin-
ning the relevance of world knowledge and infer-
ence for coreference resolution (Charniak, 1973).
This paper explores whether coreference resolu-
tion can benefit from semantic knowledge sources.
More specifically, whether a machine learning based
approach to coreference resolution can be improved
and which phenomena are affected by such infor-
mation. We investigate the use of the WordNet and
Wikipedia taxonomies for extracting semantic simi-
larity and relatedness measures, as well as semantic
parsing information in terms of semantic role label-
ing (Gildea & Jurafsky, 2002, SRL henceforth).
We believe that the lack of semantics in the cur-
rent systems leads to a performance bottleneck.
In order to correctly identify the discourse entities
which are referred to in a text, it seems essential to
reason over the lexical semantic relations, as well as
the event representations embedded in the text. As
an example, consider a fragment from the Automatic
Content Extraction (ACE) 2003 data.
(1) But frequent visitors say that given the sheer weight of
the country?s totalitarian ideology and generations of
mass indoctrination, changing this country?s course will
be something akin to turning a huge ship at sea. Opening
North Korea up, even modestly, and exposing people to
the idea that Westerners ? and South Koreans ? are not
devils, alone represents an extraordinary change. [...] as
his people begin to get a clearer idea of the deprivation
they have suffered, especially relative to their neighbors.
?This is a society that has been focused most of all on
stability, [...]?.
In order to correctly resolve the anaphoric expres-
sions highlighted in bold, it seems that some kind
of lexical semantic and encyclopedic knowledge is
required. This includes that North Korea is a coun-
try, that countries consist of people and are soci-
eties. The resolution requires an encyclopedia (i.e.
Wikipedia) look-up and reasoning on the content re-
latedness holding between the different expressions
(i.e. as a path measure along the links of the Word-
Net and Wikipedia taxonomies). Event representa-
tions seem also to be important for coreference res-
olution, as shown below:
(2) A state commission of inquiry into the sinking of the
Kursk will convene in Moscow on Wednesday, the
Interfax news agency reported. It said that the diving
operation will be completed by the end of next week.
192
In this example, knowing that the Interfax news
agency is the AGENT of the report predicate and It
being the AGENT of say could trigger the (seman-
tic parallelism based) inference required to correctly
link the two expressions, in contrast to anchoring
the pronoun to Moscow. SRL provides the seman-
tic relationships that constituents have with predi-
cates, thus allowing us to include such document-
level event descriptive information into the relations
holding between referring expressions (REs).
Instead of exploring different kinds of data rep-
resentations, task definitions or machine learning
techniques (Ng & Cardie, 2002; Yang et al, 2003;
Luo et al, 2004) we focus on a few promising se-
mantic features which we evaluate in a controlled
environment. That way we try to overcome the
plateauing in performance in coreference resolution
observed by Kehler et al (2004).
2 Related Work
Vieira & Poesio (2000), Harabagiu et al (2001),
and Markert & Nissim (2005) explore the use of
WordNet for different coreference resolution sub-
tasks, such as resolving bridging reference, other-
and definite NP anaphora, and MUC-style corefer-
ence resolution. All of them present systems which
infer coreference relations from a set of potential an-
tecedents by means of a WordNet search. Our ap-
proach to WordNet here is to cast the search results
in terms of semantic similarity measures. Their out-
put can be used as features for a learner. These mea-
sures are not specifically developed for coreference
resolution but simply taken ?off-the-shelf? and ap-
plied to our task without any specific tuning ? i.e.
in contrast to Harabagiu et al (2001), who weight
WordNet relations differently in order to compute
the confidence measure of the path.
To the best of our knowledge, we do not know
of any previous work using Wikipedia or SRL for
coreference resolution. In the case of SRL, this
layer of semantic context abstracts from the specific
lexical expressions used, and therefore represents a
higher level of abstraction than (still related) work
involving predicate argument statistics. Kehler et al
(2004) observe no significant improvement due to
predicate argument statistics. The improvement re-
ported by Yang et al (2005) is rather caused by their
twin-candidate model than by the semantic knowl-
edge. Employing SRL is closer in spirit to Ji et al
(2005), who explore the employment of the ACE
2004 relation ontology as a semantic filter.
3 Coreference Resolution Using Semantic
Knowledge Sources
3.1 Corpora Used
To establish a competitive coreference resolver, the
system was initially prototyped using the MUC-6
and MUC-7 data sets (Chinchor & Sundheim, 2003;
Chinchor, 2001), using the standard partitioning
of 30 texts for training and 20-30 texts for test-
ing. Then, we moved on and developed and tested
the system with the ACE 2003 Training Data cor-
pus (Mitchell et al, 2003)1. Both the Newswire
(NWIRE) and Broadcast News (BNEWS) sections
where split into 60-20-20% document-based par-
titions for training, development, and testing, and
later per-partition merged (MERGED) for system
evaluation. The distribution of coreference chains
and referring expressions is given in Table 1.
3.2 Learning Algorithm
For learning coreference decisions, we used a Maxi-
mum Entropy (Berger et al, 1996) model. This was
implemented using the MALLET library (McCal-
lum, 2002). To prevent the model from overfitting,
we employed a tunable Gaussian prior as a smooth-
ing method. The best parameter value is found by
searching in the [0,10] interval with step value of
0.5 for the variance parameter yielding the highest
MUC score F-measure on the development data.
Coreference resolution is viewed as a binary clas-
sification task: given a pair of REs, the classifier has
to decide whether they are coreferent or not. The
MaxEnt model produces a probability for each cat-
egory y (coreferent or not) of a candidate pair, con-
ditioned on the context x in which the candidate oc-
curs. The conditional probability is calculated by:
p(y|x) = 1Zx
[
?
i
?ifi(x, y)
]
1We used the training data corpus only, as the availability
of the test data is restricted to ACE participants. Therefore, the
results we report cannot be compared directly with those using
the official test data.
193
BNEWS (147 docs ? 33,479 tokens) NWIRE (105 docs ? 57,205 tokens)
#coref ch. #pron. #comm. nouns #prop. names #coref ch. #pron. #comm. nouns #prop. names
TRAIN. 587 876 572 980 904 1,037 1,210 2,023
DEVEL 201 315 163 465 399 358 485 923
TEST 228 291 238 420 354 329 484 712
TOTAL 1,016 1,482 973 1,865 1,657 1,724 2,179 3,658
TOTAL (%) 34.3% 22.5% 43.2% 22.8% 28.8% 48.4%
Table 1: Partitions of the ACE 2003 training data corpus
where fi(x, y) is the value of feature i on outcome y
in context x, and ?i is the weight associated with i in
the model. Zx is a normalization constant. The fea-
tures used in our model are all binary-valued feature
functions (or indicator functions), e.g.
fI SEMROLE(ARG0/RUN, COREF) =
?
?
?
?
?
?
?
?
?
?
?
1 if candidate pair is
coreferent and antecedent
is the semantic argument
ARG0 of predicate run
0 else
In our system, a set of pre-processing compo-
nents including a POS tagger (Gime?nez & Ma`rquez,
2004), NP chunker (Kudoh & Matsumoto, 2000)
and the Alias-I LingPipe Named Entity Recognizer2
is applied to the text in order to identify the noun
phrases, which are further taken as referring ex-
pressions (REs) to be used for instance generation.
Therefore, we use automatically extracted noun
phrases, rather than assuming perfect NP chunk-
ing. This is in contrast to other related works
in coreference resolution (e.g. Luo et al (2004),
Kehler et al (2004)).
Instances are created following Soon et al (2001).
We create a positive training instance from each pair
of adjacent coreferent REs. Negative instances are
obtained by pairing the anaphoric REs with any RE
occurring between the anaphor and the antecedent.
During testing each text is processed from left to
right: each RE is paired with any preceding RE from
right to left, until a pair labeled as coreferent is out-
put, or the beginning of the document is reached.
The classifier imposes a partitioning on the available
REs by clustering each set of expressions labeled as
coreferent into the same coreference chain.
2http://alias-i.com/lingpipe
3.3 Baseline System Features
Following Ng & Cardie (2002), our baseline sys-
tem reimplements the Soon et al (2001) system.
The system uses 12 features. Given a potential an-
tecedent REi and a potential anaphor REj the fea-
tures are computed as follows3.
(a) Lexical features
STRING MATCH T if REi and REj have the
same spelling, else F.
ALIAS T if one RE is an alias of the other; else F.
(b) Grammatical features
I PRONOUN T if REi is a pronoun; else F.
J PRONOUN T if REj is a pronoun; else F.
J DEF T if REj starts with the; else F.
J DEM T if REj starts with this, that, these, or
those; else F.
NUMBER T if both REi and REj agree in number;
else F.
GENDER U if either REi or REj have an undefined
gender. Else if they are both defined and agree
T; else F.
PROPER NAME T if both REi and REj are
proper names; else F.
APPOSITIVE T if REj is in apposition with REi;
else F.
(c) Semantic features
WN CLASS U if either REi or REj have an unde-
fined WordNet semantic class. Else if they both
have a defined one and it is the same T; else F.
(d) Distance features
DISTANCE how many sentences REi and REj are
apart.
3Possible values are U(nknown), T(rue) and F(alse). Note
that in contrast to Ng & Cardie (2002) we interpret ALIAS as
a lexical feature, as it solely relies on string comparison and
acronym string matching.
194
3.4 WordNet Features
In the baseline system semantic information is lim-
ited to WordNet semantic class matching. Unfor-
tunately, a WordNet semantic class lookup exhibits
problems such as coverage, sense proliferation and
ambiguity4, which make the WN CLASS feature
very noisy. We enrich the semantic information
available to the classifier by using semantic similar-
ity measures based on the WordNet taxonomy (Ped-
ersen et al, 2004). The measures we use include
path length based measures (Rada et al, 1989; Wu &
Palmer, 1994; Leacock & Chodorow, 1998), as well
as ones based on information content (Resnik, 1995;
Jiang & Conrath, 1997; Lin, 1998).
In our case, the measures are obtained by comput-
ing the similarity scores between the head lemmata
of each potential antecedent-anaphor pair. In order
to overcome the sense disambiguation problem, we
factorise over all possible sense pairs: given a can-
didate pair, we take the cross product of each an-
tecedent and anaphor sense to form pairs of synsets.
For each measure WN SIMILARITY, we compute
the similarity score for all synset pairs, and create
the following features.
WN SIMILARITY BEST the highest similarity
score from all ?SENSEREi,n, SENSEREj ,m? synset
pairs.
WN SIMILARITY AVG the average similarity
score from all ?SENSEREi,n, SENSEREj ,m? synset
pairs.
Pairs containing REs which cannot be mapped to
WordNet synsets are assumed to have a null simi-
larity measure.
3.5 Wikipedia Features
Wikipedia is a multilingual Web-based free-content
encyclopedia5 . The English version, as of 14 Febru-
ary 2006, contains 971,518 articles with 16.8 mil-
lion internal hyperlinks thus providing a large cover-
age available knowledge resource. In addition, since
May 2004 it provides also a taxonomy by means of
the category feature: articles can be placed in one
4Following the system to be replicated, we simply mapped
each RE to the first WordNet sense of the head noun.
5Wikipedia can be downloaded at http://download.
wikimedia.org/. In our experiments we use the English
Wikipedia database dump from 19 February 2006.
or more categories, which are further categorized to
provide a category tree. In practice, the taxonomy
is not designed as a strict hierarchy or tree of cat-
egories, but allows multiple categorisation schemes
to co-exist simultaneously. Because each article can
appear in more than one category, and each category
can appear in more than one parent category, the cat-
egories do not form a tree structure, but a more gen-
eral directed graph. As of December 2005, 78% of
the articles have been categorized into 87,000 differ-
ent categories.
Wikipedia mining works as follows (for an in-
depth description of the methods for computing
semantic relatedness in Wikipedia see Strube &
Ponzetto (2006)): given the candidate referring ex-
pressions REi and REj we first pull the pages they
refer to. This is accomplished by querying the page
titled as the head lemma or, in the case of NEs, the
full NP. We follow all redirects and check for dis-
ambiguation pages, i.e. pages for ambiguous entries
which contain links only (e.g. Lincoln). If a disam-
biguation page is hit, we first get al the hyperlinks
in the page. If a link containing the other queried RE
is found (i.e. a link containing president in the Lin-
coln page), the linked page (President of the United
States) is returned, else we return the first article
linked in the disambiguation page. Given a candi-
date coreference pair REi/j and the Wikipedia pages
PREi/j they point to, obtained by querying pages ti-
tled as TREi/j , we extract the following features:
I/J GLOSS CONTAINS U if no Wikipedia page
titled TREi/j is available. Else T if the first para-
graph of text of PREi/j contains TREj/i ; else F.
I/J RELATED CONTAINS U if no Wikipedia
page titled as TREi/j is available. Else T if at
least one Wikipedia hyperlink of PREi/j con-
tains TREj/i ; else F.
I/J CATEGORIES CONTAINS U if no Wiki-
pedia page titled as TREi/j is available. Else T if
the list of categories PREi/j belongs to contains
TREj/i ; else F.
GLOSS OVERLAP the overlap score between the
first paragraph of text of PREi and PREj . Fol-
lowing Banerjee & Pedersen (2003) we compute
the score as
?
n m2 for n phrasal m-word over-
laps.
195
Additionally, we use the Wikipedia category graph.
We ported the WordNet similarity path length based
measures to the Wikipedia category graph. How-
ever, the category relations in Wikipedia cannot only
be interpreted as corresponding to is-a links in a
taxonomy since they denote meronymic relations
as well. Therefore, the Wikipedia-based measures
are to be taken as semantic relatedness measures.
The measures from Rada et al (1989), Leacock &
Chodorow (1998) and Wu & Palmer (1994) are com-
puted in the same way as for WordNet. Path search
takes place as a depth-limited search of maximum
depth of 4 for a least common subsumer. We no-
ticed that limiting the search improves the results as
it yields a better correlation of the relatedness scores
with human judgements (Strube & Ponzetto, 2006).
This is due to the high regions of the Wikipedia cat-
egory tree being too strongly connected.
In addition, we use the measure from Resnik
(1995), which is computed using an intrinsic in-
formation content measure relying on the hierar-
chical structure of the category tree (Seco et al,
2004). Given PREi/j and the lists of categories
CREi/j they belong to, we factorise over all pos-
sible category pairs. That is, we take the cross
product of each antecedent and anaphor category to
form pairs of ?Wikipedia synsets?. For each mea-
sure WIKI RELATEDNESS, we compute the relat-
edness score for all category pairs, and create the
following features.
WIKI RELATEDNESS BEST the highest relat-
edness score from all ?CREi,n, CREj ,m? cate-
gory pairs.
WIKI RELATEDNESS AVG the average relat-
edness score from all ?CREi,n, CREj ,m? cate-
gory pairs.
3.6 Semantic Role Features
The last semantic knowledge enhancement for the
baseline system uses SRL information. In our exper-
iments we use the ASSERT parser (Pradhan et al,
2004), an SVM based semantic role tagger which
uses a full syntactic analysis to automatically iden-
tify all verb predicates in a sentence together with
their semantic arguments, which are output as Prop-
Bank arguments (Palmer et al, 2005). It is of-
ten the case that the semantic arguments output by
the parser do not align with any of the previously
identified noun phrases. In this case, we pass a
semantic role label to a RE only when the two
phrases share the same head. Labels have the form
?ARG1 pred1 . . . ARGn predn? for n semantic roles
filled by a constituent, where each semantic argu-
ment label is always defined with respect to a predi-
cate. Given such level of semantic information avail-
able at the RE level, we introduce two new features6.
I SEMROLE the semantic role argument-
predicate pairs of REi.
J SEMROLE the semantic role argument-
predicate pairs of REj .
For the ACE 2003 data, 11,406 of 32,502 automati-
cally extracted noun phrases were tagged with 2,801
different argument-predicate pairs.
4 Experiments
4.1 Performance Metrics
We report in the following tables the MUC
score (Vilain et al, 1995). Scores in Table 2 are
computed for all noun phrases appearing in either
the key or the system response, whereas Tables 3
and 4 refer to scoring only those phrases which ap-
pear in both the key and the response. We therefore
discard those responses not present in the key, as we
are interested in establishing the upper limit of the
improvements given by our semantic features. That
is, we want to define a baseline against which to es-
tablish the contribution of the semantic information
sources explored here for coreference resolution.
In addition, we report the accuracy score for all
three types of ACE mentions, namely pronouns,
common nouns and proper names. Accuracy is the
percentage of REs of a given mention type correctly
resolved divided by the total number of REs of the
same type given in the key. A RE is said to be cor-
rectly resolved when both it and its direct antecedent
are placed by the key in the same coreference class.
6During prototyping we experimented unpairing the argu-
ments from the predicates, which yielded worse results. This
is supported by the PropBank arguments always being defined
with respect to a target predicate. Binarizing the features ? i.e.
do REi and REj have the same argument or predicate label with
respect to their closest predicate? ? also gave worse results.
196
MUC-6 MUC-7
original R P F1 R P F1
Soon et al 58.6 67.3 62.3 56.1 65.5 60.4
duplicated
baseline 64.9 65.6 65.3 55.1 68.5 61.1
Table 2: Results on MUC
4.2 Feature Selection
For determining the relevant feature sets we follow
an iterative procedure similar to the wrapper ap-
proach for feature selection (Kohavi & John, 1997)
using the development data. The feature subset se-
lection algorithm performs a hill-climbing search
along the feature space. We start with a model
based on all available features. Then we train mod-
els obtained by removing one feature at a time. We
choose the worst performing feature, namely the one
whose removal gives the largest improvement based
on the MUC score F-measure, and remove it from
the model. We then train classifiers removing each
of the remaining features separately from the en-
hanced model. The process is iteratively run as long
as significant improvement is observed.
4.3 Results
Table 2 compares the results between our duplicated
Soon baseline and the original system. We assume
that the slight improvements of our system are due
to the use of current pre-processing components and
another classifier. Tables 3 and 4 show a comparison
of the performance between our baseline system and
the ones incremented with semantic features. Per-
formance improvements are highlighted in bold7.
4.4 Discussion
The tables show that semantic features improve sys-
tem recall, rather than acting as a ?semantic filter?
improving precision. Semantics therefore seems to
trigger a response in cases where more shallow fea-
tures do not seem to suffice (see examples (1-2)).
Different feature sources account for different
RE type improvements. WordNet and Wikipedia
features tend to increase performance on common
7All changes in F-measure are statistically significant at the
0.05 level or higher. We follow Soon et al (2001) in performing
a simple one-tailed, paired sample t-test between the baseline
system?s MUC score F-measure and each of the other systems?
F-measure scores on the test documents.
nouns, whereas SRL improves pronouns. Word-
Net features are able to improve by 14.3% and
7.7% the accuracy rate for common nouns on the
BNEWS and NWIRE datasets (+34 and +37 cor-
rectly resolved common nouns out of 238 and 484
respectively), whereas employing Wikipedia yields
slightly smaller improvements (+13.0% and +6.6%
accuracy increase on the same datasets). Similarly,
when SRL features are added to the baseline system,
we register an increase in the accuracy rate for pro-
nouns, ranging from 0.7% in BNEWS and NWIRE
up to 4.2% in the MERGED dataset (+26 correctly
resolved pronouns out of 620).
If semantics helps for pronouns and common
nouns, it does not affect performance on proper
names, where features such as string matching and
alias suffice. This suggests that semantics plays a
role in pronoun and common noun resolution, where
surface features cannot account for complex prefer-
ences and semantic knowledge is required.
The best accuracy improvement on pronoun res-
olution is obtained on the MERGED dataset. This
is due to making more data available to the classi-
fier, as the SRL features are very sparse and inher-
ently suffer from data fragmentation. Using a larger
dataset highlights the importance of SRL, whose
features are never removed in any feature selection
process8. The accuracy on common nouns shows
that features induced from Wikipedia are competi-
tive with the ones from WordNet. The performance
gap on all three datasets is quite small, which indi-
cates the usefulness of using an encyclopedic knowl-
edge base as a replacement for a lexical taxonomy.
As a consequence of having different knowledge
sources accounting for the resolution of different RE
types, the best results are obtained by (1) combin-
ing features generated from different sources; (2)
performing feature selection. When combining dif-
ferent feature sources, we register an accuracy im-
provement on pronouns and common nouns, as well
as an increase in F-measure due to a higher recall.
Feature selection always improves results. This
is due to the fact that our full feature set is ex-
8To our knowledge, most of the recent work in coreference
resolution on the ACE data keeps the document source sepa-
rated for evaluation. However, we believe that document source
independent evaluation provides useful insights on the robust-
ness of the system (cf. the CoNLL 2005 shared task cross-
corpora evaluation).
197
BNEWS NWIRE
R P F1 Ap Acn Apn R P F1 Ap Acn Apn
baseline 46.7 86.2 60.6 36.4 10.5 44.0 56.7 88.2 69.0 37.6 23.1 55.6
+WordNet 54.8 86.1 66.9 36.8 24.8 47.6 61.3 84.9 71.2 38.9 30.8 55.5
+Wiki 52.7 86.8 65.6 36.1 23.5 46.2 60.6 83.6 70.3 38.0 29.7 55.2
+SRL 53.3 85.1 65.5 37.1 13.9 46.2 58.0 89.0 70.2 38.3 25.0 56.0
all features 59.1 84.4 69.5 37.5 27.3 48.1 63.1 83.0 71.7 39.8 31.8 52.8
Table 3: Results on the ACE 2003 data (BNEWS and NWIRE sections)
R P F1 Ap Acn Apn
baseline 54.5 88.0 67.3 34.7 20.4 53.1
+WordNet 56.7 87.1 68.6 35.6 28.5 49.6
+Wikipedia 55.8 87.5 68.1 34.8 26.0 50.5
+SRL 56.3 88.4 68.8 38.9 21.6 51.7
all features 61.0 84.2 70.7 38.9 29.9 51.2
Table 4: Results ACE (merged BNEWS/NWIRE)
tremely redundant: in order to explore the useful-
ness of the knowledge sources we included overlap-
ping features (i.e. using best and average similar-
ity/relatedness measures at the same time), as well as
features capturing the same phenomenon from dif-
ferent point of views (i.e. using multiple measures
at the same time). In order to yield the desired per-
formance improvements, it turns out to be essential
to filter out irrelevant features.
Table 5 shows the relevance of the best perform-
ing features on the BNEWS section. As our fea-
ture selection mechanism chooses the best set of fea-
tures by removing them (see Section 4.2), we eval-
uate the contributions of the remaining features as
follows. We start with a baseline system using all
the features from Soon et al (2001) that were not
removed in the feature selection process (i.e. DIS-
TANCE). We then train classifiers combining the
current feature set with each feature in turn. We
then choose the best performing feature based on the
MUC score F-measure and add it to the model. We
iterate the process until all features are added to the
baseline system. The table indicates that all knowl-
edge sources are relevant for coreference resolution,
as it includes SRL, WordNet and Wikipedia features.
The Wikipedia features rank high, indicating again
that it provides a valid knowledge base.
5 Conclusions and Future Work
The results are somehow surprising, as one would
not expect a community-generated categorization
to be almost as informative as a well structured
Feature set F1
baseline (Soon w/o DISTANCE) 58.4%
+WIKI WU PALMER BEST +4.3%
+J SEMROLE +1.8%
+WIKI PATH AVG +1.2%
+I SEMROLE +0.8%
+WN WU PALMER BEST +0.7%
Table 5: Feature selection (BNEWS section)
lexical taxonomy such as WordNet. Nevertheless
Wikipedia offers promising results, which we expect
to improve as well as the encyclopedia goes under
further development.
In this paper we investigated the effects of using
different semantic knowledge sources within a ma-
chine learning based coreference resolution system.
This involved mining the WordNet taxonomy and
the Wikipedia encyclopedic knowledge base, as well
as including semantic parsing information, in order
to induce semantic features for coreference learning.
Empirical results show that coreference resolution
benefits from semantics. The generated model is
able to learn selectional preferences in cases where
surface morpho-syntactic features do not suffice, i.e.
pronoun and common name resolution. While the
results given by using ?the free encyclopedia that
anyone can edit? are satisfactory, major improve-
ments can come from developing efficient query
strategies ? i.e. a more refined disambiguation tech-
nique taking advantage of the context in which the
queries (e.g. referring expressions) occur.
Future work will include turning Wikipedia into
an ontology with well defined taxonomic relations,
as well as exploring its usefulness of for other NLP
applications. We believe that an interesting aspect of
Wikipedia is that it offers large coverage resources
for many languages, thus making it a natural choice
for multilingual NLP systems.
Semantics plays indeed a role in coreference
resolution. But semantic features are expensive to
198
compute and the development of efficient methods
is required to embed them into large scale systems.
Nevertheless, we believe that exploiting semantic
knowledge in the manner we described will assist
the research on coreference resolution to overcome
the plateauing in performance observed by Kehler
et al (2004).
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a
KTF grant (09.003.2004). We thank Katja Filip-
pova, Margot Mieskes and the three anonymous re-
viewers for their useful comments.
References
Banerjee, S. & T. Pedersen (2003). Extended gloss overlap as
a measure of semantic relatedness. In Proc. of IJCAI-03, pp.
805?810.
Berger, A., S. A. Della Pietra & V. J. Della Pietra (1996). A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
Charniak, E. (1973). Jack and Janet in search of a theory of
knowledge. In Advance Papers from the Third International
Joint Conference on Artificial Intelligence, Stanford, Cal.,
pp. 337?343.
Chinchor, N. (2001). Message Understanding Conference
(MUC) 7. LDC2001T02, Philadelphia, Penn: Linguistic
Data Consortium.
Chinchor, N. & B. Sundheim (2003). Message Understanding
Conference (MUC) 6. LDC2003T13, Philadelphia, Penn:
Linguistic Data Consortium.
Gildea, D. & D. Jurafsky (2002). Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
Gime?nez, J. & L. Ma`rquez (2004). SVMTool: A general POS
tagger generator based on support vector machines. In Proc.
of LREC ?04, pp. 43?46.
Harabagiu, S. M., R. C. Bunescu & S. J. Maiorano (2001). Text
and knowledge mining for coreference resolution. In Proc.
of NAACL-01, pp. 55?62.
Ji, H., D. Westbrook & R. Grishman (2005). Using semantic re-
lations to refine coreference decisions. In Proc. HLT-EMNLP
?05, pp. 17?24.
Jiang, J. J. & D. W. Conrath (1997). Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceedings of
the 10th International Conference on Research in Computa-
tional Linguistics (ROCLING).
Kehler, A., D. Appelt, L. Taylor & A. Simma (2004). The
(non)utility of predicate-argument frequencies for pronoun
interpretation. In Proc. of HLT-NAACL-04, pp. 289?296.
Kohavi, R. & G. H. John (1997). Wrappers for feature subset
selection. Artificial Intelligence Journal, 97(1-2):273?324.
Kudoh, T. & Y. Matsumoto (2000). Use of Support Vector Ma-
chines for chunk identification. In Proc. of CoNLL-00, pp.
142?144.
Leacock, C. & M. Chodorow (1998). Combining local con-
text and WordNet similarity for word sense identifica-
tion. In C. Fellbaum (Ed.), WordNet. An Electronic Lexical
Database, Chp. 11, pp. 265?283. Cambridge, Mass.: MIT
Press.
Lin, D. (1998). An information-theoretic definition of similar-
ity. In Proceedings of the 15th International Conference on
Machine Learning, pp. 296?304.
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla & S. Roukos
(2004). A mention-synchronous coreference resolution al-
gorithm based on the Bell Tree. In Proc. of ACL-04, pp.
136?143.
Markert, K. & M. Nissim (2005). Comparing knowledge
sources for nominal anaphora resolution. Computational
Linguistics, 31(3):367?401.
McCallum, A. K. (2002). MALLET: A Machine Learning for
Language Toolkit.
Mitchell, A., S. Strassel, M. Przybocki, J. Davis, G. Dodding-
ton, R. Grishman, A. Meyers, A. Brunstain, L. Ferro &
B. Sundheim (2003). TIDES Extraction (ACE) 2003 Mul-
tilingual Training Data. LDC2004T09, Philadelphia, Penn.:
Linguistic Data Consortium.
Ng, V. & C. Cardie (2002). Improving machine learning ap-
proaches to coreference resolution. In Proc. of ACL-02, pp.
104?111.
Palmer, M., D. Gildea & P. Kingsbury (2005). The proposition
bank: An annotated corpus of semantic roles. Computational
Linguistics, 31(1):71?105.
Pedersen, T., S. Patwardhan & J. Michelizzi (2004). Word-
Net::Similarity ? Measuring the relatedness of concepts. In
Companion Volume of the Proceedings of the Human Tech-
nology Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pp. 267?270.
Pradhan, S., W. Ward, K. Hacioglu, J. H. Martin & D. Juraf-
sky (2004). Shallow semantic parsing using Support Vector
Machines. In Proc. of HLT-NAACL-04, pp. 233?240.
Rada, R., H. Mili, E. Bicknell & M. Blettner (1989). Devel-
opment and application of a metric to semantic nets. IEEE
Transactions on Systems, Man and Cybernetics, 19(1):17?
30.
Resnik, P. (1995). Using information content to evaluate seman-
tic similarity in a taxonomy. In Proc. of IJCAI-95, Vol. 1, pp.
448?453.
Seco, N., T. Veale & J. Hayes (2004). An intrinsic information
content metric for semantic similarity in WordNet. In Proc.
of ECAI-04, pp. 1089?1090.
Soon, W. M., H. T. Ng & D. C. Y. Lim (2001). A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Strube, M. & S. P. Ponzetto (2006). WikiRelate! Computing
semantic relatedness using Wikipedia. In Proc. of AAAI-06.
Vieira, R. & M. Poesio (2000). An empirically-based system for
processing definite descriptions. Computational Linguistics,
26(4):539?593.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly & L. Hirschman
(1995). A model-theoretic coreference scoring scheme. In
Proceedings of the 6th Message Understanding Conference
(MUC-6), pp. 45?52.
Wu, Z. & M. Palmer (1994). Verb semantics and lexical selec-
tion. In Proc. of ACL-94, pp. 133?138.
Yang, X., J. Su & C. L. Tan (2005). Improving pronoun reso-
lution using statistics-based semantic compatibility informa-
tion. In Proc. of ACL-05, pp. 165?172.
Yang, X., G. Zhou, J. Su & C. L. Tan (2003). Coreference
resolution using competition learning approach. In Proc. of
ACL-03, pp. 176?183.
199
Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 9?12,
Rochester, April 2007. c?2007 Association for Computational Linguistics
Creating a Knowledge Base From a Collaboratively Generated Encyclopedia
Simone Paolo Ponzetto
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/?ponzetto
Abstract
We present our work on using Wikipedia
as a knowledge source for Natural Lan-
guage Processing. We first describe our
previous work on computing semantic re-
latedness from Wikipedia, and its applica-
tion to a machine learning based corefer-
ence resolution system. Our results sug-
gest that Wikipedia represents a semantic
resource to be treasured for NLP applica-
tions, and accordingly present the work di-
rections to be explored in the future.
1 Introduction
The last decade has seen statistical techniques for
Natural Language Processing (NLP) gaining the
status of standard approaches to most NLP tasks.
While advances towards robust statistical inference
methods (cf. e.g. Domingos et al (2006) and Pun-
yakanok et al (2006)) will certainly improve the
computational modelling of natural language, we
believe that crucial advances will also come from re-
discovering the use of symbolic knowledge, i.e. the
deployment of large scale knowledge bases.
Arguments for the necessity of symbolically en-
coded knowledge for AI and NLP date back at least
to McCarthy (1959). Symbolic approaches using
knowledge bases, however, are expensive and time-
consuming to maintain. They also have a limited
and arbitrary coverage. In our work we try to over-
come such problems by relying on a wide coverage
on-line encyclopedia developed by a large amount of
users, namely Wikipedia. That is, we are interested
in whether and how Wikipedia can be integrated into
NLP applications as a knowledge base. The motiva-
tion comes from the necessity to overcome the brit-
tleness and knowledge acquisition bottlenecks that
NLP applications suffer.
2 Previous Work: WikiRelate! and
Semantic Knowledge Sources for
Coreference Resolution
Ponzetto & Strube (2006) and Strube & Ponzetto
(2006) aimed at showing that ?the encyclopedia that
anyone can edit? can be indeed used as a semantic
resource for research in NLP. In particular, we as-
sumed its category tree to represent a semantic net-
work modelling relations between concepts, and we
computed measures of semantic relatedness from it.
We did not show only that Wikipedia-based mea-
sures of semantic relatedness are competitive with
the ones computed from a widely used standard
resource such as WordNet (Fellbaum, 1998), but
also that including semantic knowledge mined from
Wikipedia into an NLP system dealing with corefer-
ence resolution is in fact beneficial.
2.1 WikiRelate! Computing Semantic
Relatedness Using Wikipedia
Semantic relatedness measures have been proven to
be useful in many NLP applications such as word
sense disambiguation (Kohomban & Lee, 2005; Pat-
wardhan et al, 2005), information retrieval (Finkel-
stein et al, 2002), information extraction pattern
induction (Stevenson & Greenwood, 2005), inter-
pretation of noun compounds (Kim & Baldwin,
2005), paraphrase detection (Mihalcea et al, 2006)
and spelling correction (Budanitsky & Hirst, 2006).
Approaches to measuring semantic relatedness that
9
re
la
te
dn
es
s 
m
ea
su
re
(s)
 co
mp
uta
tio
n
search for a connecting path along the category networkpage query and retrieval, category extraction
"
Jo
hn
 Z
or
n"
 q
ue
ry
"
Fe
la
 K
ut
i" 
qu
er
y
Musicians
Composers
Jazz composers
Musical activists
page : John Zorn
page : Fela Kuti
Figure 1: Wikipedia-based semantic relatedness computation. First, target pages for the given queries are re-
trieved, possibly via disambiguation. Next, categories are extracted to provide an entry point to the category
network. Connecting paths are then searched along the category network using a depth-limited search. The
paths found are scored and the ones satisfying the measure definitions (i.e. the shortest one for path-length
measures, and the most informative one for information-content measures) are returned.
use lexical resources transform that resource into
a network or graph and compute relatedness us-
ing paths in it1. For instance, Rada et al (1989)
traverse MeSH, a term hierarchy for indexing arti-
cles in Medline, and compute semantic relatedness
as the edge distance between terms in the hierar-
chy. Jarmasz & Szpakowicz (2003) use the same
approach with Roget?s Thesaurus while Hirst & St-
Onge (1998) apply a similar strategy to WordNet.
The novel idea presented in Strube & Ponzetto
(2006) was to induce a semantic network from the
Wikipedia categorization graph to compute mea-
sures of semantic relatedness. Wikipedia, a multi-
lingual Web-based free-content encyclopedia, al-
lows for structured access by means of categories:
the encyclopedia articles can be assigned one or
more categories, which are further categorized to
provide a so-called ?category tree?. Though not de-
1An overview of lexical resource-based approaches to mea-
suring semantic relatedness is presented in Budanitsky & Hirst
(2006). Note that here we do not distinguish between seman-
tic similarity (computed using hyponymy/hyperonymy, i.e. is-
a, relations only) and semantic relatedness (using all relations
in the taxonomy, including antonymic, meronymic, functional
relations such as is-made-of, etc.), since the relations between
categories in Wikipedia are neither semantically typed nor show
a uniform semantics (see Section 3).
signed as a strict hierarchy or tree, the categories
form a graph which can be used as a taxonomy to
compute semantic relatedness. We showed (1) how
to retrieve Wikipedia articles from textual queries
and resolve ambiguous queries based on the arti-
cles? link structure; (2) compute semantic related-
ness as a function of the articles found and the paths
between them along the categorization graph (Fig-
ure 1). We evaluated the Wikipedia-based measures
against the ones computed from WordNet on bench-
marking datasets from the literature (e.g. Miller and
Charles? (1991) list of 30 noun pairs) and found
Wikipedia to be competitive with WordNet.
2.2 Semantic Knowledge Sources for
Coreference Resolution
Evaluating measures of semantic relatedness on
word pair datasets poses non-trivial problems, i.e.
all available datasets are small in size, and it is not
always clear which linguistic notion (i.e. similar-
ity vs. relatedness) underlies them. Accordingly, in
Ponzetto & Strube (2006) we used a machine learn-
ing based coreference resolution system to provide
an extrinsic evaluation of the utility of WordNet and
Wikipedia relatedness measures for NLP applica-
tions. We started with the machine learning based
10
WordNet
Wikipedia
Prince
Fela Kuti
The Minneapolis Genius
the pioneer of Afrobeat music
The artist formerly known as Prince
TAFKAP
The Artist
Raw text
he
Prince
Fela Kuti
the pioneer of Afrobeat music
The Minneapolis Genius
he
TAFKAP
The Artist
The artist formerly known as Prince
with coreference chains
Text annotated
Preprocessing
pipeline
PoS tagger
Chunker
NER
Baseline Feature Extractor
MaxEnt
classifier
Semantic Feature
extractor
SEMANTICS
(Soon et al, 2001)
Figure 2: Overview of the coreference system for extrinsic evaluation of WordNet and Wikipedia relatedness
measures. We start with a baseline system from Soon et al (2001). We then include at different times
features from WordNet and Wikipedia and register performance variations.
baseline system from Soon et al (2001), and an-
alyzed the performance variations given by includ-
ing the relatedness measures in the feature set (Fig-
ure 2). The results showed that coreference resolu-
tion benefits from information mined from seman-
tic knowledge sources and also, that using features
induced from Wikipedia gives a performance only
slightly worse than when using WordNet.
3 Future Work: Inducing an Ontology
from a Collaboratively Generated
Encyclopedia
Our results so far suggest that Wikipedia can be con-
sidered a semantic resource in its own right. Un-
fortunately, the Wikipedia categorization still suf-
fers from some limitations: it cannot be considered
an ontology, as the relations between categories are
not semantically-typed, i.e. the links between cate-
gories do not have an explicit semantics such as is-a,
part-of, etc. Work in the near future will accordingly
concentrate on automatically inducing the semantics
of the relations between Wikipedia categories. This
aims at transforming the unlabeled graph in Figure
3(a) into the semantic network in Figure 3(b), where
the links between categories are augmented with a
clearly defined semantics.
The availability of explicit semantic relations
would allow to compute semantic similarity rather
than semantic relatedness (Budanitsky & Hirst,
2006), which is more suitable for coreference res-
olution. That is, we assume that the availability
of hyponymic/hyperonymic relations will allow us
to compute lexical semantic measures which will
further increase the performance of our coreference
resolution system, as well as further bringing for-
ward Wikipedia as a direct competitor of manually-
designed resources such as WordNet.
In order to make the task feasible, we are currently
concentrating on inducing is-a vs. not-is-a semantic
relations. This simplifies the task, but still allows
us to compute measures of semantic similarity. As
we made limited use of the large amount of text in
Wikipedia, we are now trying to integrate text and
categorization. This includes extracting semantic re-
lations expressed in the encyclopedic definitions by
means of Hearst patterns (Hearst, 1992), detection
of semantic variations (Morin & Jacquemin, 1999)
between category labels, as well as using the cat-
egorized pages as bag-of-words to compute scores
of idf-based semantic overlap (Monz & de Rijke,
2001) between categories. Further work will then
concentrate on making this information available to
our coreference resolution system, e.g. via semantic
similarity computation.
Finally, since Wikipedia is available in many lan-
guages, we believe it is worth performing experi-
ments in a multilingual setting. Accordingly, we are
currently testing a website2 that will allow us to col-
lect word relatedness judgements from native speak-
2Available at http://www.eml-research.de/nlp/353-TC.
11
Cybernetics
Artificial Intelligence
Natural Language Processing
Artificial Intelligence applications Cognitive architecture
Computer Science
Computational Linguistics
Speech recognition
Cognition
Cognitive Science
Linguistics
Philosophy
Branches of philosophy
Ontology
MetaphysicsLogic
PataphysicsMathematical logic
Mathematics
Thought
Abstraction
Belief
(a) current category graph
Cybernetics
Artificial Intelligence
Natural Language Processing
 PART-OF
Artificial Intelligence applications Cognitive architecture
Computer Science
Computational Linguistics
 PART-OFSpeech recognition
 PART-OF IS-A
Cognition
Cognitive Science
Linguistics
 IS-A
Philosophy
Branches of philosophy
 IS-A
Ontology
 PART-OF
MetaphysicsLogic
 IS-A
Pataphysics
 IS-NOT
Mathematical logic
 IS-A
Mathematics
 PART-OF
Thought
Abstraction
Belief
(b) category graph augmented with semantic relations
Figure 3: Inducing explicit semantic relations between categories in Wikipedia
ers of German, French and Italian, in order to trans-
late the semantic relatedness dataset from Finkel-
stein et al (2002) and test our methodology with
languages other than English.
4 Conclusions
In this paper we presented our previous efforts on us-
ing Wikipedia as a semantic knowledge source. We
aim in the future to induce an ontology from its col-
laboratively generated categorization graph. We be-
lieve that our work opens up exciting new challenges
for the AI and NLP research community, e.g. how to
handle the noise included in such knowledge bases
and how to fully structure the information given in
the form of only partially structured text and rela-
tions between knowledge base entries.
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The author has been supported by a KTF
grant (09.003.2004).
References
Budanitsky, A. & G. Hirst (2006). Evaluating WordNet-based
measures of semantic distance. Computational Linguistics,
32(1).
Domingos, P., S. Kok, H. Poon, M. Richardson & P. Singla
(2006). Unifying logical and statistical AI. In Proc. of AAAI-
06, pp. 2?7.
Fellbaum, C. (Ed.) (1998). WordNet: An Electronic Lexical
Database. Cambridge, Mass.: MIT Press.
Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan,
G. Wolfman & E. Ruppin (2002). Placing search in context:
The concept revisited. ACM Transactions on Information
Systems, 20(1):116?131.
Hearst, M. A. (1992). Automatic acquisition of hyponyms from
large text corpora. In Proc. of COLING-92, pp. 539?545.
Hirst, G. & D. St-Onge (1998). Lexical chains as repre-
sentations of context for the detection and correction of
malapropisms. In C. Fellbaum (Ed.), WordNet: An Elec-
tronic Lexical Database, pp. 305?332. Cambridge, Mass.:
MIT Press.
Jarmasz, M. & S. Szpakowicz (2003). Roget?s Thesaurus and
semantic similarity. In Proc. of RANLP-03, pp. 212?219.
Kim, S. N. & T. Baldwin (2005). Automatic interpretation
of noun compounds using WordNet similarity. In Proc. of
IJCNLP-05, pp. 945?956.
Kohomban, U. S. & W. S. Lee (2005). Learning semantic
classes for word sense disambiguation. In Proc. of ACL-05,
pp. 34?41.
McCarthy, J. (1959). Programs with common sense. In Pro-
ceedings of the Teddington Conference on the Mechanization
of Thought Processes, pp. 75?91.
Mihalcea, R., C. Corley & C. Strapparava (2006). Corpus-based
and knowledge-based measures of text semantic similarity.
In Proc. of AAAI-06, pp. 775?780.
Miller, G. A. & W. G. Charles (1991). Contextual correlates
of semantic similarity. Language and Cognitive Processes,
6(1):1?28.
Monz, C. & M. de Rijke (2001). Light-weight entailment
checking for computational semantics. In Proc. of ICoS-3,
pp. 59?72.
Morin, E. & C. Jacquemin (1999). Projecting corpus-based se-
mantic links on a thesaurus. In Proc. of ACL-99, pp. 389?
396.
Patwardhan, S., S. Banerjee & T. Pedersen (2005). SenseRe-
late::TargetWord ? A generalized framework for word sense
disambiguation. In Proc. of AAAI-05.
Ponzetto, S. P. & M. Strube (2006). Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolution.
In Proc. of HLT-NAACL-06, pp. 192?199.
Punyakanok, V., D. Roth, W. Yih & D. Zimak (2006). Learning
and inference over constrained output. In Proc. of IJCAI-05,
pp. 1117?1123.
Rada, R., H. Mili, E. Bicknell & M. Blettner (1989). Devel-
opment and application of a metric to semantic nets. IEEE
Transactions on Systems, Man and Cybernetics, 19(1):17?
30.
Soon, W. M., H. T. Ng & D. C. Y. Lim (2001). A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Stevenson, M. & M. Greenwood (2005). A semantic approach
to IE pattern induction. In Proc. of ACL-05, pp. 379?386.
Strube, M. & S. P. Ponzetto (2006). WikiRelate! Computing
semantic relatedness using Wikipedia. In Proc. of AAAI-06,
pp. 1419?1424.
12
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 49?52,
Prague, June 2007. c?2007 Association for Computational Linguistics
An API for Measuring the Relatedness of Words in Wikipedia
Simone Paolo Ponzetto andMichael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We present an API for computing the seman-
tic relatedness of words in Wikipedia.
1 Introduction
The last years have seen a large amount of work in
Natural Language Processing (NLP) using measures
of semantic similarity and relatedness. We believe
that the extensive usage of such measures derives
also from the availability of robust and freely avail-
able software that allows to compute them (Pedersen
et al, 2004, WordNet::Similarity).
In Ponzetto & Strube (2006) and Strube &
Ponzetto (2006) we proposed to take the Wikipedia
categorization system as a semantic network which
served as basis for computing the semantic related-
ness of words. In the following we present the API
we used in our previous work, hoping that it will en-
courage further research in NLP using Wikipedia1.
2 Measures of Semantic Relatedness
Approaches to measuring semantic relatedness that
use lexical resources transform these resources into
a network or graph and compute relatedness using
paths in it (see Budanitsky & Hirst (2006) for an ex-
tensive review). For instance, Rada et al (1989)
traverse MeSH, a term hierarchy for indexing ar-
ticles in Medline, and compute semantic related-
ness straightforwardly in terms of the number of
edges between terms in the hierarchy. Jarmasz &
Szpakowicz (2003) use the same approach with Ro-
get?s Thesauruswhile Hirst & St-Onge (1998) apply
a similar strategy to WordNet.
1The software can be freely downloaded at http://www.
eml-research.de/nlp/download/wikipediasimilarity.php.
3 The Application Programming Interface
The API computes semantic relatedness by:
1. taking a pair of words as input;
2. retrieving the Wikipedia articles they refer to
(via a disambiguation strategy based on the link
structure of the articles);
3. computing paths in the Wikipedia categoriza-
tion graph between the categories the articles are
assigned to;
4. returning as output the set of paths found,
scored according to some measure definition.
The implementation includes path-length (Rada
et al, 1989; Wu & Palmer, 1994; Leacock &
Chodorow, 1998), information-content (Resnik,
1995; Seco et al, 2004) and text-overlap (Lesk,
1986; Banerjee & Pedersen, 2003) measures, as de-
scribed in Strube & Ponzetto (2006).
The API is built on top of several modules and can
be used for tasks other than Wikipedia-based relat-
edness computation. On a basic usage level, it can be
used to retrieve Wikipedia articles by name, option-
ally using disambiguation patterns, as well as to find
a ranked set of articles satisfying a search query (via
integration with the Lucene2 text search engine).
Additionally, it provides functionality for visualiz-
ing the computed paths along the Wikipedia cate-
gorization graph as either Java Swing components
or applets (see Figure 1), based on the JGraph li-
brary3, and methods for computing centrality scores
of the Wikipedia categories using the PageRank al-
gorithm (Brin & Page, 1998). Finally, it currently
2http://lucene.apache.org
3http://www.jgraph.com
49
Figure 1: Shortest path between computer and key-
board in the English Wikipedia.
provides multilingual support for the English, Ger-
man, French and Italian Wikipedias and can be eas-
ily extended to other languages4.
4 Software Architecture
Wikipedia is freely available for download, and can
be accessed using robust Open Source applications,
e.g. the MediaWiki software5, integrated within a
Linux, Apache, MySQL and PHP (LAMP) software
bundle. The architecture of the API consists of the
following modules:
1. RDBMS: at the lowest level, the encyclopedia
content is stored in a relational database manage-
ment system (e.g. MySQL).
2. MediaWiki: a suite of PHP routines for interact-
ing with the RDBMS.
3. WWW-Wikipedia Perl library6: responsible for
4In contrast to WordNet::Similarity, which due to the struc-
tural variations between the respective wordnets was reimple-
mented for German by Gurevych & Niederlich (2005).
5http://www.mediawiki.org
6http://search.cpan.org/dist/WWW-Wikipedia
querying MediaWiki, parsing and structuring the
returned encyclopedia pages.
4. XML-RPC server: an intermediate communica-
tion layer between Java and the Perl routines.
5. Java wrapper library: provides a simple inter-
face to create and access the encyclopedia page
objects and compute the relatedness scores.
The information flow of the API is summarized by
the sequence diagram in Figure 2. The higher in-
put/output layer the user interacts with is provided
by a Java API from which Wikipedia can be queried.
The Java library is responsible for issuing HTTP re-
quests to an XML-RPC daemon which provides a
layer for calling Perl routines from the Java API.
Perl routines take care of the bulk of querying ency-
clopedia entries to the MediaWiki software (which
in turn queries the database) and efficiently parsing
the text responses into structured objects.
5 Using the API
The API provides factory classes for querying
Wikipedia, in order to retrieve encyclopedia entries
as well as relatedness scores for word pairs. In
practice, the Java library provides a simple pro-
grammatic interface. Users can accordingly ac-
cess the library using only a few methods given
in the factory classes, e.g. getPage(word)
for retrieving Wikipedia articles titled word or
getRelatedness(word1,word2), for com-
puting the relatedness between word1 and word2,
and display(path) for displaying a path found
between two Wikipedia articles in the categorization
graph. Examples of programmatic usage of the API
are presented in Figure 3. In addition, the software
distribution includes UNIX shell scripts to access
the API interactively from a terminal, i.e. it does not
require any knowledge of Java.
6 Application scenarios
Semantic relatedness measures have proven use-
ful in many NLP applications such as word sense
disambiguation (Kohomban & Lee, 2005; Patward-
han et al, 2005), information retrieval (Finkelstein
et al, 2002), information extraction pattern induc-
tion (Stevenson & Greenwood, 2005), interpretation
of noun compounds (Kim & Baldwin, 2005), para-
50
:W
e
b
s
r
v
e
r
:
M
e
d
i
a
W
i
k
:
J
a
v
w
r
a
p
e
r
l
i
b
r
a
y
:
W

i
k
p
e
d
i
a
:
X
M
L

R
P
C
d
a
e
m
o
n
:
D
a
t
b
s
e
R
e
s
u
l
t
s
e
t
1
.
R
e
t
r
i
v
e
W
i
k
p
e
d
i
a
c
t
e
g
o
r
y
t
r
e
2
.
C
r
e
a
t
c
a
t
e
g
o
r
y
t
r
e
J
a
v
d
a
t
s
t
r
u
c
t
r
e
3
.
W
i
k
p
e
d
i
a
p
g
e
s
l
o
k
u
p
l
o
p
:
f
o
r
e
a
c
h
w
o
r
d
R
e
s
u
l
t
s
e
t
X
M
L

R
P
C
r
e
s
p
o
n
s
e
P
e
r
l
o
b
j
e
c
t
W
i
k
m
a
r
k
u
p
t
e
x
t
P
H
A
r
t
i
c
l
e
o
b
j
e
c
t
4
.
R
e
l
a
t
d
n
e
s
c
o
r
e
c
o
m
p
u
t
a
i
o
n
:
S
Q
L
q
u
e
r
y
(
c
a
t
e
g
o
r
i
e
s
a
n
d
l
i
k
s
)
:
H
T
P
r
e
q
u
s
t
:
P
e
r
l
m
o
d
u
l
e
c
a
l
:
H
T
P
r
e
q
u
s
t
:
P
H
m
o
d
u
l
e
c
a
l
:
S
Q
L
q
u
e
r
y
(
p
a
g
e
)
:
a
r
t
i
c
l
e
l
o
k
u
p
:
C
r
e
a
t
g
r
a
p
h
f
r
o
m
c
a
t
e
g
o
r
y
t
r
e
q
u
e
r
y
:
C
a
t
e
g
o
r
y
e
x
t
r
a
c
t
i
o
n
a
d
p
a
t
h
s
e
a
r
c
h
Figure 2: API processing sequence diagram. Wikipedia pages and relatedness measures are accessed
through a Java API. The wrapper communicates with a Perl library designed for Wikipedia access and pars-
ing through an XML-RPC server. WWW-Wikipedia in turn accesses the database where the encyclopedia
is stored by means of appropriate queries to MediaWiki.
51
// 1. Get the English Wikipedia page titled "King" using "chess" as disambiguation
WikipediaPage page = WikipediaPageFactory.getInstance().getWikipediaPage("King","chess");
// 2. Get the German Wikipedia page titled "Ufer" using "Kueste" as disambiguation
WikipediaPage page = WikipediaPageFactory.getInstance().getWikipediaPage("Ufer","Kueste",Language.DE);
// 3a. Get the Wikipedia-based path-length relatedness measure between "computer" and "keyboard"
WikiRelatedness relatedness = WikiRelatednessFactory.getInstance().getWikiRelatedness("computer","keyboard");
double shortestPathMeasure = relatedness.getShortestPathMeasure();
// 3b. Display the shortest path
WikiPathDisplayer.getInstance().display(relatedness.getShortestPath());
// 4. Score the importance of the categories in the English Wikipedia using PageRank
WikiCategoryGraph<DefaultScorableGraph<DefaultEdge>> categoryTree =
WikiCategoryGraphFactory.getCategoryGraphForLanguage(Language.EN);
categoryTree.getCategoryGraph().score(new PageRank());
Figure 3: Java API sample usage.
phrase detection (Mihalcea et al, 2006) and spelling
correction (Budanitsky & Hirst, 2006). Our API
provides a flexible tool to include such measures
into existing NLP systems while using Wikipedia
as a knowledge source. Programmatic access to the
encyclopedia makes also available in a straightfor-
ward manner the large amount of structured text in
Wikipedia (e.g. for building a language model), as
well as its rich internal link structure (e.g. the links
between articles provide phrase clusters to be used
for query expansion scenarios).
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.003.2004). We thank our colleagues Katja
Filippova and Christoph Mu?ller for helpful feed-
back.
References
Banerjee, S. & T. Pedersen (2003). Extended gloss overlap as
a measure of semantic relatedness. In Proc. of IJCAI-03, pp.
805?810.
Brin, S. & L. Page (1998). The anatomy of a large-scale hyper-
textual web search engine. Computer Networks and ISDN
Systems, 30(1?7):107?117.
Budanitsky, A. & G. Hirst (2006). Evaluating WordNet-based
measures of semantic distance. Computational Linguistics,
32(1).
Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan,
G. Wolfman & E. Ruppin (2002). Placing search in context:
The concept revisited. ACM Transactions on Information
Systems, 20(1):116?131.
Gurevych, I. & H. Niederlich (2005). Accessing GermaNet data
and computing semantic relatedness. In Comp. Vol. to Proc.
of ACL-05, pp. 5?8.
Hirst, G. & D. St-Onge (1998). Lexical chains as repre-
sentations of context for the detection and correction of
malapropisms. In C. Fellbaum (Ed.), WordNet: An Elec-
tronic Lexical Database, pp. 305?332. Cambridge, Mass.:
MIT Press.
Jarmasz, M. & S. Szpakowicz (2003). Roget?s Thesaurus and
semantic similarity. In Proc. of RANLP-03, pp. 212?219.
Kim, S. N. & T. Baldwin (2005). Automatic interpretation
of noun compounds using WordNet similarity. In Proc. of
IJCNLP-05, pp. 945?956.
Kohomban, U. S. & W. S. Lee (2005). Learning semantic
classes for word sense disambiguation. In Proc. of ACL-05,
pp. 34?41.
Leacock, C. & M. Chodorow (1998). Combining local con-
text and WordNet similarity for word sense identifica-
tion. In C. Fellbaum (Ed.), WordNet. An Electronic Lexical
Database, Chp. 11, pp. 265?283. Cambridge, Mass.: MIT
Press.
Lesk, M. (1986). Automatic sense disambiguation using ma-
chine readable dictionaries: How to tell a pine cone from an
ice cream cone. In Proceedings of the 5th Annual Confer-
ence on Systems Documentation, Toronto, Ontario, Canada,
pp. 24?26.
Mihalcea, R., C. Corley & C. Strapparava (2006). Corpus-based
and knowledge-based measures of text semantic similarity.
In Proc. of AAAI-06, pp. 775?780.
Patwardhan, S., S. Banerjee & T. Pedersen (2005). SenseRe-
late::TargetWord ? A generalized framework for word sense
disambiguation. In Proc. of AAAI-05.
Pedersen, T., S. Patwardhan & J. Michelizzi (2004). Word-
Net::Similarity ? Measuring the relatedness of concepts. In
Comp. Vol. to Proc. of HLT-NAACL-04, pp. 267?270.
Ponzetto, S. P. & M. Strube (2006). Exploiting semantic role
labeling, WordNet andWikipedia for coreference resolution.
In Proc. of HLT-NAACL-06, pp. 192?199.
Rada, R., H. Mili, E. Bicknell & M. Blettner (1989). Devel-
opment and application of a metric to semantic nets. IEEE
Transactions on Systems, Man and Cybernetics, 19(1):17?
30.
Resnik, P. (1995). Using information content to evaluate seman-
tic similarity in a taxonomy. In Proc. of IJCAI-95, Vol. 1, pp.
448?453.
Seco, N., T. Veale & J. Hayes (2004). An intrinsic information
content metric for semantic similarity in WordNet. In Proc.
of ECAI-04, pp. 1089?1090.
Stevenson, M. & M. Greenwood (2005). A semantic approach
to IE pattern induction. In Proc. of ACL-05, pp. 379?386.
Strube, M. & S. P. Ponzetto (2006). WikiRelate! Computing
semantic relatedness using Wikipedia. In Proc. of AAAI-06,
pp. 1419?1424.
Wu, Z. & M. Palmer (1994). Verb semantics and lexical selec-
tion. In Proc. of ACL-94, pp. 133?138.
52
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 9?12,
Columbus, June 2008. c?2008 Association for Computational Linguistics
BART: A Modular Toolkit for Coreference Resolution
Yannick Versley
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Simone Paolo Ponzetto
EML Research gGmbH
ponzetto@eml-research.de
Massimo Poesio
University of Essex
poesio@essex.ac.uk
Vladimir Eidelman
Columbia University
vae2101@columbia.edu
Alan Jern
UCLA
ajern@ucla.edu
Jason Smith
Johns Hopkins University
jsmith@jhu.edu
Xiaofeng Yang
Inst. for Infocomm Research
xiaofengy@i2r.a-star.edu.sg
Alessandro Moschitti
University of Trento
moschitti@dit.unitn.it
Abstract
Developing a full coreference system able
to run all the way from raw text to seman-
tic interpretation is a considerable engineer-
ing effort, yet there is very limited avail-
ability of off-the shelf tools for researchers
whose interests are not in coreference, or for
researchers who want to concentrate on a
specific aspect of the problem. We present
BART, a highly modular toolkit for de-
veloping coreference applications. In the
Johns Hopkins workshop on using lexical
and encyclopedic knowledge for entity dis-
ambiguation, the toolkit was used to ex-
tend a reimplementation of the Soon et al
(2001) proposal with a variety of additional
syntactic and knowledge-based features, and
experiment with alternative resolution pro-
cesses, preprocessing tools, and classifiers.
1 Introduction
Coreference resolution refers to the task of identify-
ing noun phrases that refer to the same extralinguis-
tic entity in a text. Using coreference information
has been shown to be beneficial in a number of other
tasks, including information extraction (McCarthy
and Lehnert, 1995), question answering (Morton,
2000) and summarization (Steinberger et al, 2007).
Developing a full coreference system, however, is
a considerable engineering effort, which is why a
large body of research concerned with feature en-
gineering or learning methods (e.g. Culotta et al
2007; Denis and Baldridge 2007) uses a simpler but
non-realistic setting, using pre-identified mentions,
and the use of coreference information in summa-
rization or question answering techniques is not as
widespread as it could be. We believe that the avail-
ability of a modular toolkit for coreference will sig-
nificantly lower the entrance barrier for researchers
interested in coreference resolution, as well as pro-
vide a component that can be easily integrated into
other NLP applications.
A number of systems that perform coreference
resolution are publicly available, such as GUITAR
(Steinberger et al, 2007), which handles the full
coreference task, and JAVARAP (Qiu et al, 2004),
which only resolves pronouns. However, literature
on coreference resolution, if providing a baseline,
usually uses the algorithm and feature set of Soon
et al (2001) for this purpose.
Using the built-in maximum entropy learner
with feature combination, BART reaches 65.8%
F-measure on MUC6 and 62.9% F-measure on
MUC7 using Soon et al?s features, outperforming
JAVARAP on pronoun resolution, as well as the
Soon et al reimplementation of Uryupina (2006).
Using a specialized tagger for ACE mentions and
an extended feature set including syntactic features
(e.g. using tree kernels to represent the syntactic
relation between anaphor and antecedent, cf. Yang
et al 2006), as well as features based on knowledge
extracted from Wikipedia (cf. Ponzetto and Smith, in
preparation), BART reaches state-of-the-art results
on ACE-2. Table 1 compares our results, obtained
using this extended feature set, with results from
Ng (2007). Pronoun resolution using the extended
feature set gives 73.4% recall, coming near special-
ized pronoun resolution systems such as (Denis and
Baldridge, 2007).
9
Figure 1: Results analysis in MMAX2
2 System Architecture
The BART toolkit has been developed as a tool to
explore the integration of knowledge-rich features
into a coreference system at the Johns Hopkins Sum-
mer Workshop 2007. It is based on code and ideas
from the system of Ponzetto and Strube (2006), but
also includes some ideas from GUITAR (Steinberger
et al, 2007) and other coreference systems (Versley,
2006; Yang et al, 2006). 1
The goal of bringing together state-of-the-art ap-
proaches to different aspects of coreference res-
olution, including specialized preprocessing and
syntax-based features has led to a design that is very
modular. This design provides effective separation
of concerns across several several tasks/roles, in-
cluding engineering new features that exploit dif-
ferent sources of knowledge, designing improved or
specialized preprocessing methods, and improving
the way that coreference resolution is mapped to a
machine learning problem.
Preprocessing To store results of preprocessing
components, BART uses the standoff format of the
MMAX2 annotation tool (Mu?ller and Strube, 2006)
with MiniDiscourse, a library that efficiently imple-
ments a subset of MMAX2?s functions. Using a
generic format for standoff annotation allows the use
of the coreference resolution as part of a larger sys-
tem, but also performing qualitative error analysis
using integrated MMAX2 functionality (annotation
1An open source version of BART is available from
http://www.sfs.uni-tuebingen.de/?versley/BART/.
diff, visual display).
Preprocessing consists in marking up noun
chunks and named entities, as well as additional in-
formation such as part-of-speech tags and merging
these information into markables that are the start-
ing point for the mentions used by the coreference
resolution proper.
Starting out with a chunking pipeline, which
uses a classical combination of tagger and chun-
ker, with the Stanford POS tagger (Toutanova et al,
2003), the YamCha chunker (Kudoh and Mat-
sumoto, 2000) and the Stanford Named Entity Rec-
ognizer (Finkel et al, 2005), the desire to use richer
syntactic representations led to the development of
a parsing pipeline, which uses Charniak and John-
son?s reranking parser (Charniak and Johnson, 2005)
to assign POS tags and uses base NPs as chunk
equivalents, while also providing syntactic trees that
can be used by feature extractors. BART also sup-
ports using the Berkeley parser (Petrov et al, 2006),
yielding an easy-to-use Java-only solution.
To provide a better starting point for mention de-
tection on the ACE corpora, the Carafe pipeline
uses an ACE mention tagger provided by MITRE
(Wellner and Vilain, 2006). A specialized merger
then discards any base NP that was not detected to
be an ACE mention.
To perform coreference resolution proper, the
mention-building module uses the markables cre-
ated by the pipeline to create mention objects, which
provide an interface more appropriate for corefer-
ence resolution than the MiniDiscourse markables.
These objects are grouped into equivalence classes
by the resolution process and a coreference layer is
written into the document, which can be used for de-
tailed error analysis.
Feature Extraction BART?s default resolver goes
through all mentions and looks for possible an-
tecedents in previous mentions as described by Soon
et al (2001). Each pair of anaphor and candi-
date is represented as a PairInstance object,
which is enriched with classification features by fea-
ture extractors, and then handed over to a machine
learning-based classifier that decides, given the fea-
tures, whether anaphor and candidate are corefer-
ent or not. Feature extractors are realized as sepa-
rate classes, allowing for their independent develop-
10
Figure 2: Example system configuration
ment. The set of feature extractors that the system
uses is set in an XML description file, which allows
for straightforward prototyping and experimentation
with different feature sets.
Learning BART provides a generic abstraction
layer that maps application-internal representations
to a suitable format for several machine learning
toolkits: One module exposes the functionality of
the the WEKA machine learning toolkit (Witten
and Frank, 2005), while others interface to special-
ized state-of-the art learners. SVMLight (Joachims,
1999), in the SVMLight/TK (Moschitti, 2006) vari-
ant, allows to use tree-valued features. SVM Classi-
fication uses a Java Native Interface-based wrapper
replacing SVMLight/TK?s svm classify pro-
gram to improve the classification speed. Also in-
cluded is a Maximum entropy classifier that is
based upon Robert Dodier?s translation of Liu and
Nocedal?s (1989) L-BFGS optimization code, with
a function for programmatic feature combination.2
Training/Testing The training and testing phases
slightly differ from each other. In the training phase,
the pairs that are to be used as training examples
have to be selected in a process of sample selection,
whereas in the testing phase, it has to be decided
which pairs are to be given to the decision function
and how to group mentions into equivalence rela-
tions given the classifier decisions.
This functionality is factored out into the en-
2see http://riso.sourceforge.net
coder/decoder component, which is separate from
feature extraction and machine learning itself. It
is possible to completely change the basic behav-
ior of the coreference system by providing new
encoders/decoders, and still rely on the surround-
ing infrastructure for feature extraction and machine
learning components.
3 Using BART
Although BART is primarily meant as a platform for
experimentation, it can be used simply as a corefer-
ence resolver, with a performance close to state of
the art. It is possible to import raw text, perform
preprocessing and coreference resolution, and either
work on the MMAX2-format files, or export the re-
sults to arbitrary inline XML formats using XSL
stylesheets.
Adapting BART to a new coreferentially anno-
tated corpus (which may have different rules for
mention extraction ? witness the differences be-
tween the annotation guidelines of MUC and ACE
corpora) usually involves fine-tuning of mention cre-
ation (using pipeline and MentionFactory settings),
as well as the selection and fine-tuning of classi-
fier and features. While it is possible to make rad-
ical changes in the preprocessing by re-engineering
complete pipeline components, it is usually possi-
ble to achieve the bulk of the task by simply mix-
ing and matching existing components for prepro-
cessing and feature extraction, which is possible by
modifying only configuration settings and an XML-
11
BNews NPaper NWire
Recl Prec F Recl Prec F Recl Prec F
basic feature set 0.594 0.522 0.556 0.663 0.526 0.586 0.608 0.474 0.533
extended feature set 0.607 0.654 0.630 0.641 0.677 0.658 0.604 0.652 0.627
Ng 2007? 0.561 0.763 0.647 0.544 0.797 0.646 0.535 0.775 0.633
?: ?expanded feature set? in Ng 2007; Ng trains on the entire ACE training corpus.
Table 1: Performance on ACE-2 corpora, basic vs. extended feature set
based description of the feature set and learner(s)
used.
Several research groups focusing on coreference
resolution, including two not involved in the ini-
tial creation of BART, are using it as a platform
for research including the use of new information
sources (which can be easily incorporated into the
coreference resolution process as features), different
resolution algorithms that aim at enhancing global
coherence of coreference chains, and also adapting
BART to different corpora. Through the availability
of BART as open source, as well as its modularity
and adaptability, we hope to create a larger com-
munity that allows both to push the state of the art
further and to make these improvements available to
users of coreference resolution.
Acknowledgements We thank the CLSP at Johns
Hopkins, NSF and the Department of Defense for
ensuring funding for the workshop and to EML
Research, MITRE, the Center for Excellence in
HLT, and FBK-IRST, that provided partial support.
Yannick Versley was supported by the Deutsche
Forschungsgesellschaft as part of SFB 441 ?Lin-
guistic Data Structures?; Simone Paolo Ponzetto has
been supported by the Klaus Tschira Foundation
(grant 09.003.2004).
References
Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc. ACL
2005.
Culotta, A., Wick, M., and McCallum, A. (2007). First-order
probabilistic models for coreference resolution. In Proc.
HLT/NAACL 2007.
Denis, P. and Baldridge, J. (2007). A ranking approach to pro-
noun resolution. In Proc. IJCAI 2007.
Finkel, J. R., Grenager, T., and Manning, C. (2005). Incorpo-
rating non-local information into information extraction sys-
tems by Gibbs sampling. In Proc. ACL 2005, pages 363?370.
Joachims, T. (1999). Making large-scale SVM learning prac-
tical. In Scho?lkopf, B., Burges, C., and Smola, A., editors,
Advances in Kernel Methods - Support Vector Learning.
Kudoh, T. and Matsumoto, Y. (2000). Use of Support Vector
Machines for chunk identification. In Proc. CoNLL 2000.
Liu, D. C. and Nocedal, J. (1989). On the limited memory
method for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
McCarthy, J. F. and Lehnert, W. G. (1995). Using decision trees
for coreference resolution. In Proc. IJCAI 1995.
Morton, T. S. (2000). Coreference for NLP applications. In
Proc. ACL 2000.
Moschitti, A. (2006). Making tree kernels practical for natural
language learning. In Proc. EACL 2006.
Mu?ller, C. and Strube, M. (2006). Multi-level annotation of
linguistic data with MMAX2. In Braun, S., Kohn, K., and
Mukherjee, J., editors, Corpus Technology and Language
Pedagogy: New Resources, New Tools, New Methods. Peter
Lang, Frankfurt a.M., Germany.
Ng, V. (2007). Shallow semantics for coreference resolution. In
Proc. IJCAI 2007.
Petrov, S., Barett, L., Thibaux, R., and Klein, D. (2006). Learn-
ing accurate, compact, and interpretable tree annotation. In
COLING-ACL 2006.
Ponzetto, S. P. and Strube, M. (2006). Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolution.
In Proc. HLT/NAACL 2006.
Qiu, L., Kan, M.-Y., and Chua, T.-S. (2004). A public reference
implementation of the RAP anaphora resolution algorithm.
In Proc. LREC 2004.
Soon, W. M., Ng, H. T., and Lim, D. C. Y. (2001). A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K. (2007).
Two uses of anaphora resolution in summarization. Informa-
tion Processing and Management, 43:1663?1680. Special
issue on Summarization.
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proc. NAACL 2003, pages 252?259.
Uryupina, O. (2006). Coreference resolution with and without
linguistic knowledge. In Proc. LREC 2006.
Versley, Y. (2006). A constraint-based approach to noun phrase
coreference resolution in German newspaper text. In Kon-
ferenz zur Verarbeitung Natu?rlicher Sprache (KONVENS
2006).
Wellner, B. and Vilain, M. (2006). Leveraging machine read-
able dictionaries in discriminative sequence models. In Proc.
LREC 2006.
Witten, I. and Frank, E. (2005). Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufmann.
Yang, X., Su, J., and Tan, C. L. (2006). Kernel-based pronoun
resolution with structured syntactic knowledge. In Proc.
CoLing/ACL-2006.
12
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 213?216, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling Using Lexical Statistical Information
Simone Paolo Ponzetto and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp/
Abstract
Our system for semantic role labeling is
multi-stage in nature, being based on tree
pruning techniques, statistical methods for
lexicalised feature encoding, and a C4.5
decision tree classifier. We use both shal-
low and deep syntactic information from
automatically generated chunks and parse
trees, and develop a model for learning
the semantic arguments of predicates as a
multi-class decision problem. We evalu-
ate the performance on a set of relatively
?cheap? features and report an F1 score of
68.13% on the overall test set.
1 Introduction
This paper presents a system for the CoNLL 2005
Semantic Role Labeling shared task (Carreras &
Ma`rquez, 2005), which is based on the current re-
lease of the English PropBank data (Palmer et al,
2005). For the 2005 edition of the shared task are
available both syntactic and semantic information.
Accordingly, we make use of both clausal, chunk
and deep syntactic (tree structure) features, named
entity information, as well as statistical representa-
tions for lexical item encoding.
The set of features and their encoding reflect the
necessity of limiting the complexity and dimension-
ality of the input space. They also provide the clas-
sifier with enough information. We explore here the
use of a minimal set of compact features for seman-
tic role prediction, and show that a feature-based
statistical encoding of lexicalised features such as
predicates, head words, local contexts and PoS by
means of probability distributions provides an effi-
cient way of representing the data, with the feature
vectors having a small dimensionality and allowing
to abstract from single words.
2 System description
2.1 Preprocessing
During preprocessing the predicates? semantic argu-
ments are mapped to the nodes in the parse trees, a
set of hand-crafted shallow tree pruning rules are ap-
plied, probability distributions for feature represen-
tation are generated from training data1, and feature
vectors are extracted. Those are finally fed into the
classifier for semantic role classification.
2.1.1 Tree node mapping of semantic
arguments and named entities
Following Gildea & Jurafsky (2002), (i) labels
matching more than one constituent due to non-
branching nodes are taken as labels of higher con-
stituents, (ii) in cases of labels with no correspond-
ing parse constituent, these are assigned to the par-
tial match given by the constituent spanning the
shortest portion of the sentence beginning at the la-
bel?s span left boundary and lying entirely within it.
We drop the role or named entity label if such suit-
able constituent could not be found2.
1All other processing steps assume a uniform treatment of
both training and test data.
2The percentage of roles for which no valid tree node could
be found amounts to 3% for the training and 7% for the devel-
opment set. These results are compatible with the performance
of the employed parser (Collins, 1999).
213
2.1.2 Tree pruning
The tagged trees are further processed by applying
the following pruning rules:
? All punctuation nodes are removed. This is for
removing punctuation information, as well as
for aligning spans of the syntactic nodes with
PropBank constituents3.
? If a node is unary branching and its daughter is
also unary branching, the daughter is removed.
This allows to remove redundant nodes span-
ning the same tokens in the sentence.
? If a node has only preterminal children, these
are removed. This allows to internally collapse
base phrases such as base NPs.
Tree pruning was carried out in order to reduce the
number of nodes from which features were to be ex-
tracted later. This limits the number of candidate
constituents for role labeling, and removes redun-
dant information produced by the pipeline of previ-
ous components (i.e. PoS tags of preterminal labels),
as well as the sparseness and fragmentation of the
input data. These simple rules reduce the number
of constituents given by the parser output by 38.4%
on the training set, and by 38.7% on the develop-
ment set, at the cost of limiting the coverage of the
system by removing approximately 2% of the tar-
get role labeled constituents. On the development
set, the number of constituents remaining on top of
pruning is 81,193 of which 7,558 are semantic ar-
guments, with a performance upper-bound of 90.6%
F1.
2.1.3 Features
Given the pruned tree structures, we traverse the tree
bottom-up left-to-right. For each non-terminal node
whose span does not overlap the predicate we extract
the following features:
Phrase type: the syntactic category of the con-
stituent (NP, PP, ADVP, etc.). In order to reduce
the number of phrase labels, we retained only
3We noted during prototyping that in many cases no tree
node fully matching a role constituent could be found, as the
latter did not include punctuation tokens, whereas in Collins?
trees the punctuation terminals are included within the preced-
ing phrases. This precludes a priori the output to align to the
gold standard PropBank annotation and we use therefore prun-
ing as a recovery strategy.
those labels which account for at least 0.1% of
the overall available semantic arguments in the
training data. We replace the label for every
phrase type category below this threshold with
a generic UNK label. This reduces the number
of labels from 72 to 18.
Position: the position of the constituent with re-
spect to the target predicate (before or after).
Adjacency: whether the right (if before) or left (if
after) boundary of the constituent is adjacent,
non-adjacent or inside the predicate?s chunk.
Clause: whether the constituent belongs to the
clause of the predicate or not.
Proposition size: measures relative to the proposi-
tion size, such as (i) the number of constituents
and (ii) predicates in the proposition.
Constituent size: measures relative to the con-
stituent size, namely (i) the number of tokens
and (ii) subconstituents (viz., non-leaf rooted
subtrees) of the constituent.
Predicate: the predicate lemma, represented as the
probability distribution P (r|p) of the predicate
p of taking one of the available r semantic
roles. For unseen predicates we assume a uni-
form distribution.
Voice: whether the predicate is in active or passive
form. Passive voice is identified if the predi-
cate?s PoS tag is VBN and either it follows a
form of to be or to get, or it does not belong to
a VP chunk, or is immediately preceded by an
NP chunk.
Head word: the head word of the constituent,
represented as the probability distribution
P (r|hw) of the head word hw of heading a
phrase filling one of the available r seman-
tic roles. For unseen words we back off on a
phrasal model by using the probability distri-
bution P (r|pt) of the phrase type pt of filling a
semantic slot r.
Head word PoS: the PoS of the head word of the
constituent, similarly represented as the proba-
bility distribution P (r|pos) of a PoS pos of be-
longing to a constituent filling one of the avail-
able r semantic roles.
Local lexical context: the words in the constituent
other than the head word, represented as the
214
averaged probability distributions of each i-
th non-head word wi of occurring in one
of the available r semantic roles, namely
1
m
?m
i=1 P (r|wi) for m non-head words in the
constituent. For each unseen word we back off
by using the probability distribution P (r|posi)
of the PoS posi of filling a semantic role r4.
Named entities: the label of the named entity
which spans the same words as the constituent,
as well as the label of the largest named en-
tity embedded within the constituent. Both val-
ues are set to NULL if such labels could not be
found.
Path: the number of intervening NPB, NP, VP, VP-
A, PP, PP-A, S, S-A and SBAR nodes along the
path from the constituent to the predicate.
Distance: the distance from the target predicate,
measured as (i) the number of nodes from the
constituent to the lowest node in the tree dom-
inating both the constituent and the predicate,
(ii) the number of nodes from the predicate to
the former common dominating node5, (iii) the
number of chunks between the base phrase of
the constituent?s head and the predicate chunk,
(iv) the number of tokens between the head of
the constituent and the predicate.
2.2 Classifier
We used the YaDT6 implementation of the C4.5 de-
cision tree algorithm (Quinlan, 1993). Parameter
selection (99% pruning confidence, at least 10 in-
stances per leaf node) was carried out by performing
10-fold cross-validation on the development set.
Data preprocessing and feature vector generation
took approximately 2.5 hours (training set, including
probability distribution generation), 5 minutes (de-
velopment) and 7 minutes (test) on a 2GHz Opteron
4This feature was introduced as the information provided by
lexical heads does not seem to suffice in many cases. This is
shown by head word ambiguities, such as LOC and TMP ar-
guments occurring in similar prepositional syntactic configu-
rations ? i.e. the preposition in, which can be head of both
AM-TMP and AM-LOC constituents, as in in October and in
New York. The idea is therefore to look at the words in the con-
stituents other than the head, and build up an overall constituent
representation, thus making use of statistical lexical information
for role disambiguation.
5These distance measures along the tree path between the
constituent and the predicate were kept separate, in order to in-
directly include embedding level information into the model.
6http://www.di.unipi.it/?ruggieri/software.html
dual processor server with 2GB memory7. Training
time was of approximately 17 minutes. The final
system was trained using all of the available training
data from sections 2?21 of the Penn TreeBank. This
amounts to 2,250,887 input constituents of which
10% are non-NULL examples. Interestingly, during
prototyping we first limited ourselves to training and
drawing probability distributions for feature repre-
sentation from sections 15?18 only. This yielded
a very low performance (57.23% F1, development
set). A substantial performance increase was given
by still training on sections 15?18, but using the
probability distributions generated from sections 2?
21 (64.43% F1, development set). This suggests that
the system is only marginally sensitive to the train-
ing dataset size, but pivotally relies on taking proba-
bility distributions from a large amount of data.
In order to make the task easier and overcome the
uneven role class distribution, we limited the learner
to classify only those 16 roles accounting for at least
0.5% of the total number of semantic arguments in
the training data8.
2.3 Post-processing
As our system does not build an overall sen-
tence contextual representation, it systematically
produced errors such as embedded role labeling. In
particular, since no embedding is observed for the
semantic arguments of predicates, in case of (multi-
ple) embeddings the classifier output was automat-
ically post-processed to retain only the largest em-
bedding constituent. Evaluation on the development
set has shown that this does not significantly im-
prove performance, still it provides a much more
?sane? output. Besides, we make use of a simple
technique for avoiding multiple A0 or A1 role as-
signments within the same proposition, based on
constituent position and predicate voice. In case of
multiple A0 labels, if the predicate is in active form,
the second A0 occurrence is replaced with A1, else
we replace the first occurrence. Similarly, in case of
multiple A1 labels, if the predicate is in active form,
the first A1 occurrence is replaced with A0, else we
7We used only a single CPU at runtime, since the implemen-
tation is not parallelised.
8These include numbered arguments (A0 to A4), adjuncts
(ADV, DIS, LOC, MNR, MOD, NEG, PNC, TMP), and references
(R-A0 and R-A1).
215
Precision Recall F?=1
Development 71.82% 61.60% 66.32
Test WSJ 75.05% 64.81% 69.56
Test Brown 66.69% 52.14% 58.52
Test WSJ+Brown 74.02% 63.12% 68.13
Test WSJ Precision Recall F?=1
Overall 75.05% 64.81% 69.56
A0 78.52% 72.52% 75.40
A1 75.53% 65.39% 70.10
A2 62.28% 52.07% 56.72
A3 63.81% 38.73% 48.20
A4 73.03% 63.73% 68.06
A5 0.00% 0.00% 0.00
AM-ADV 60.00% 42.69% 49.88
AM-CAU 0.00% 0.00% 0.00
AM-DIR 0.00% 0.00% 0.00
AM-DIS 75.97% 73.12% 74.52
AM-EXT 0.00% 0.00% 0.00
AM-LOC 54.09% 47.38% 50.51
AM-MNR 58.67% 46.22% 51.71
AM-MOD 97.43% 96.37% 96.90
AM-NEG 97.78% 95.65% 96.70
AM-PNC 42.17% 30.43% 35.35
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 75.41% 71.11% 73.20
R-A0 82.09% 73.66% 77.65
R-A1 72.03% 66.03% 68.90
R-A2 0.00% 0.00% 0.00
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 0.00% 0.00% 0.00
V 98.63% 98.63% 98.63
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
replace the second occurrence.
3 Results
Table 1 shows the results on the test set. Problems
are inherently related with the skewed distribution of
role classes, so that roles which have a limited num-
ber of occurrences are harder to classify correctly.
This explains the performance gap on the A0 and
A1 roles on one hand, and the A2, A3, A4, AM- ar-
guments on the other.
One advantage of using a decision tree learning
algorithm is that it outputs a model which includes a
feature ranking, since the most informative features
are those close to the root of the tree. In the present
case, the most informative features were both dis-
tance/position metrics (distance and adjacency) and
lexicalized features (head word and predicate).
4 Conclusion
Semantic role labeling is a difficult task, and accord-
ingly, how to achieve an accurate and robust perfor-
mance is still an open question. In our work we
used a limited set of syntactic tree based distance
and size metrics coupled with raw lexical statistics,
and showed that such ?lazy learning? configuration
can still achieve a reasonable performance.
We concentrated on reducing the complexity
given by the number and dimensionality of the in-
stances to be classified during learning. This is the
core motivation behind performing tree pruning and
statistical feature encoding. This also helped us to
avoid the use of sparse features such as the explicit
path in the parse tree between the candidate con-
stituent and the predicate, and the predicate?s sub-
categorization rule (cf. e.g. Pradhan et al (2004)).
Future work will concentrate on benchmarking
this approach within alternative architectures (i.e.
two-phase with filtering) and different learning
schemes (i.e. vector-based methods such as Support
Vector Machines and Artificial Neural Networks).
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.003.2004).
References
Carreras, Xavier & Llu??s Ma`rquez (2005). Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling. In Pro-
ceedings of CoNLL-2005.
Collins, Michael (1999). Head-driven statistical models for nat-
ural language parsing, (Ph.D. thesis). Philadelphia, Penn.,
USA: University of Pennsylvania.
Gildea, Daniel & Daniel Jurafsky (2002). Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?288.
Palmer, Martha, Dan Gildea & Paul Kingsbury (2005). The
proposition bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?105.
Pradhan, Sameer, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin & Daniel Jurafsky (2004). Support vec-
tor learning for semantic argument classification. Journal
of Machine Learning, Special issue on Speech and Natural
Language Processing. To appear.
Quinlan, J. Ross (1993). C4.5: programs for machine learn-
ing. San Francisco, Cal., USA: Morgan Kaufmann Publish-
ers Inc.
216
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1399?1410, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation
Roberto Navigli and Simone Paolo Ponzetto
Dipartimento di Informatica
Sapienza Universita` di Roma
{navigli,ponzetto}@di.uniroma1.it
Abstract
We present a multilingual joint approach
to Word Sense Disambiguation (WSD). Our
method exploits BabelNet, a very large mul-
tilingual knowledge base, to perform graph-
based WSD across different languages, and
brings together empirical evidence from these
languages using ensemble methods. The re-
sults show that, thanks to complementing
wide-coverage multilingual lexical knowledge
with robust graph-based algorithms and com-
bination methods, we are able to achieve the
state of the art in both monolingual and multi-
lingual WSD settings.
1 Introduction
Nowadays the textual information needed by a user
accessing websites for content such as news re-
ports, commentaries and encyclopedic knowledge
is provided in an increasingly wide range of lan-
guages. For example, even though English is still
the majority language of the Web, the Chinese and
Spanish languages are moving fast to capture their
?juicy share?, and more languages are about to join
them in the near future. This language explosion
clearly forces researchers to focus on the challeng-
ing problem of being able to analyze and under-
stand text written in any language. However, it also
opens up novel perspectives for multilingual Natural
Language Processing (NLP) such as, for instance,
the development of approaches aimed at ?joining
forces? and taking advantage of the lexico-semantic
knowledge provided in the different languages to
improve text understanding. These two aspects are
strongly intertwined: on the one hand, enabling
language-independent text understanding would al-
low for the harvesting of more knowledge in arbi-
trary languages, while, on the other hand, bringing
together the lexical and semantic information avail-
able in different languages would improve the qual-
ity of text understanding in arbitrary languages.
However, these two goals have hitherto never
been achieved, as is attested to by the fact that re-
search in a core language understanding task such as
Word Sense Disambiguation (Navigli, 2009, WSD)
has always been focused mostly on English. His-
torically, English became established as the lan-
guage used and understood by the scientific com-
munity and, consequently, most resources were de-
veloped for it, including large-scale computational
lexicons like WordNet (Fellbaum, 1998) and sense-
tagged corpora like SemCor (Miller et al 1993).
As a result WSD in other languages was hindered
by a lack of resources, which in turn led to poor re-
sults or low involvement on the part of the research
community (Magnini et al 2004; Ma`rquez et al
2004; Orhan et al 2007; Okumura et al 2010).
Nonetheless, already in the 1990s it had been re-
marked that WSD could be improved by means of
multilingual information: a recurring idea proposed
by several researchers was that plausible transla-
tions of a word in context would restrict its pos-
sible senses to a manageable subset of meanings
(Dagan et al 1991; Gale et al 1992; Resnik and
Yarowsky, 1999). While the lack of resources at that
time hampered the development of effective multi-
lingual approaches to WSD, recently this idea has
been revamped with the organization of SemEval
tasks dealing with cross-lingual WSD (Lefever and
Hoste, 2010) and cross-lingual lexical substitution
(Mihalcea et al 2010). At the same time, new re-
1399
search on the topic has been done, including the use
of statistical translations of sentences into many lan-
guages as features for supervised models (Banea and
Mihalcea, 2011; Lefever et al 2011), and the pro-
jection of monolingual knowledge onto another lan-
guage (Khapra et al 2011).
Yet the above two goals, i.e., disambiguating in
an arbitrary language and using lexical and seman-
tic knowledge from many languages in a joint way
to improve the WSD task, have not hitherto been
attained. In this paper, we address both objectives
and propose a graph-based approach to multilingual
joint Word Sense Disambiguation. Our proposal
brings together the lexical knowledge from differ-
ent languages by exploiting empirical evidence for
disambiguation from each of them, and then com-
bining this information in a synergistic way: each
language provides a piece of sense evidence for the
meaning of a target word in context, and subsequent
integration of these various pieces enables them to
(soft) constrain each other. The results show that
this way we are able to improve over previous, high-
performing graph-based methods in both a monolin-
gual and multilingual setting, thus showing for the
first time the beneficial effects of exploiting multi-
lingual knowledge in a joint fashion.
2 Related Work
Parallel corpora have been used in the literature
for the automatic creation of a sense-tagged dataset
for supervised WSD in different languages (Gale
et al 1992; Chan and Ng, 2005; Zhong and Ng,
2009). Other approaches include the use of a coher-
ence index for identifying the tendency to lexicalize
senses differently across languages (Ide, 2000) and
the clustering of source words which translate into
the same target word, then used to perform WSD
using a similarity measure (Diab, 2003). A histori-
cal approach (Brown et al 1991) uses bilingual cor-
pora to perform unsupervised word alignment and
determine the most appropriate translation for a tar-
get word from a set of contextual features.
All the above approaches to multilingual or cross-
lingual WSD rely on bilingual corpora, including
those which exploit existing multilingual WordNet-
like resources (Ide et al 2002), or use automatically
induced multilingual co-occurrence graphs (Silberer
and Ponzetto, 2010). However, this requirement is
often very hard to satisfy, especially if we need wide
coverage. To overcome this limitation, in this work
we make use of BabelNet (Navigli and Ponzetto,
2010), a very large multilingual lexical knowledge
base. This resource ? complementary in nature
to other recent efforts presented by de Melo and
Weikum (2010), Nastase et al(2010) and Meyer and
Gurevych (2012), inter alia ? provides a truly multi-
lingual semantic network by combining Wikipedia?s
multilinguality with the output of a state-of-the-art
machine translation system to achieve high cover-
age for all languages. The key insight here is that
Word Sense Disambiguation and Machine Transla-
tion (MT) are highly intertwined tasks, as previously
shown by Carpuat and Wu (2007) and Chan et al
(2007), who successfully used sense information to
boost state-of-the-art statistical MT. In this work we
focus instead on the benefits of using multilingual
information for WSD by exploiting the structure of
a multilingual semantic network.
3 Multilingual Joint WSD
We present our methodology for multilingual WSD:
we first introduce BabelNet, the resource used in our
work (Section 3.1) and then present our algorithm
for multilingual joint WSD (Section 3.2), including
its main components, namely graph-based WSD, en-
semble methods and translation weighting (sections
3.3, 3.4 and 3.5).
3.1 BabelNet
BabelNet (Navigli and Ponzetto, 2010) follows the
structure of a traditional lexical knowledge base and,
accordingly, consists of a labeled directed graph
whose nodes represent concepts and named entities,
and whose edges express semantic relations between
them. Concepts and relations are harvested from
the largest available semantic lexicon of English,
i.e., WordNet, and a wide-coverage collaboratively-
edited encyclopedia, i.e., Wikipedia1, thus making
BabelNet a multilingual ?encyclopedic dictionary?
which combines lexicographic information with en-
cyclopedic knowledge on the basis of an unsuper-
vised mapping framework. In addition to a core
1http://www.wikipedia.org. In the following, we
refer to Wikipedia pages and senses using SMALL CAPS.
1400
semantic network, BabelNet provides a multilin-
gual lexical dimension. Each of its nodes, called
Babel synsets, contains a set of lexicalizations of
the concept for different languages, e.g., { bankENn ,
BankDEn , bancaITn , . . . , bancoESn }2. Multilin-
gual lexicalizations for all concepts are collected
from Wikipedia?s inter-language links (e.g., the En-
glish Wikipedia page BANK links to the Italian
BANCA), as well as by acquiring missing trans-
lations by means of a statistical machine transla-
tion system applied to sense-tagged data from Sem-
Cor and Wikipedia itself ? for instance, most oc-
currences of bank1n in SemCor3 are translated into
German and Italian as Ufer and riva, respectively.
As a result of combining human-edited translations
from Wikipedia and automatically generated ones
from sense-labeled data, BabelNet is able to achieve
wide coverage for all its languages (Catalan, En-
glish, French, German, Italian and Spanish): accord-
ingly, we chose it to perform graph-based WSD in
a multilingual setting since it is specifically focused
on lexical knowledge. In addition, BabelNet is avail-
able for any language required to perform standard
SemEval cross-lingual disambiguation tasks (e.g.,
Spanish, in order to perform cross-lingual lexical
substitution). Since previous work in knowledge-
based WSD shows the benefits of using rich lexical
resources (Navigli and Lapata, 2010; Ponzetto and
Navigli, 2010), BabelNet is a suitable choice for per-
forming graph-based multilingual WSD.
3.2 Exploiting multilingual information in a
knowledge-based WSD framework
We present a multilingual approach to WSD
which exploits three main factors:
i) the fact that translations of a target word pro-
vide complementary information on the range
of its candidate senses in context;
ii) the wide-coverage, multilingual lexical knowl-
edge stored in BabelNet;
iii) the support for disambiguation from different
languages in a synergistic, unified way.
2BabelNet senses are referred to with wlp, namely the sense
of a word w in a language l with part of speech p.
3We denote WordNet senses with wip, namely the i-th sense
of a word w with part of speech p.
Algorithm 1 Multilingual joint WSD
Input: a word sequence ? = (w1, . . . , wn)
a target word w ? ?
BabelNet BN
an ensemble method M
Output: a distribution of scores for the senses of w
( indicates a comment)
1: S ? SynsetsBN (w)
2: T ? {w}
3: for each s ? S
4: T ? T ? getTranslations(s)
5: ctx? ? ? {w}
6:  LScore := {lScorei,j}i=1,...,|T |, j=1,...,|S|
7: for each ti ? T
8: ?? ? {ti} ? ctx
9:  Gi := (Vi, Ei)
10: Gi ? createGraph(??, BN)
11: for each sj ? S ? Vi
12: lScorei,j ? score(Gi, sj)
13:  Score := (score1, . . . , score|S|)
14: Score?M(LScore)
15: return Score
We call this approach multilingual joint WSD,
since disambiguation is performed by exploiting dif-
ferent languages together at the same time. To this
end, we first perform graph-based WSD using the
target word in context as input, and then combine
sense evidence from its translations using an ensem-
ble method. The key idea of our joint approach is
that sense evidence from different translations pro-
vides complementary views for the senses of a tar-
get word in context. Therefore, combining such ev-
idence should produce more accurate sense predic-
tions. We view WSD as a sense ranking problem.
Given a word sequence ? = (w1, . . . , wn), we dis-
ambiguate a target word w ? ? by scoring each of
its senses and selecting the highest-ranking one:
s? = argmax
s ? SynsetsBN (w)
score(s) , (1)
where SynsetsBN (w) is the set of Babel synsets con-
taining the different senses for w.4 We score these
4Babel synsets unambiguously identify different senses
of the target word, e.g., { bankENn , BankDEn , bancoESn . . . ,
bancaITn } corresponds to the ?financial institute? sense of
bankENn (i.e., bank2n in WordNet).
1401
synsets using Algorithm 1, which we illustrate in
the following by means of the example sentence
?bank bonuses are paid in stock?, where we focus on
bankENn as the target word and { bonusENn , payENv ,
stockENn } as its context. The following steps are
performed:
Initialization. We start by gathering the data re-
quired for disambiguation (lines 1?5). First, we
collect in line 1 the set S of Babel synsets corre-
sponding to the different senses of the target word w
? namely, the synsets containing the ?financial in-
stitution?, ?money container?, ?building? senses of
bankENn , among others. Next, we obtain the multi-
lingual lexicalizations of the target word: to this end,
we first include in T the word w itself (line 2), and
then iterate through each synset s ? S to collect the
translations of each of its senses in the languages of
interest (lines 3?4). For instance, given the English
word bankENn , we collect its sense-specific German,
Italian and Spanish translations and obtain a set of
multilingual terms T = { bankENn , . . . , BankDEn ,
Sparbu?chseDEn , Bankgeba?udeDEn , . . . , bancaITn ,
salvadanaioITn , . . . , bancoESn , huchaESn }. Finally,
we create a disambiguation context ctx by taking the
word sequence ? and removing w from it (line 5, as
a result, e.g., ctx = { bonusENn , payENv , stockENn }).
Collecting sense distributions. In the next phase
(lines 6?12), we collect a scoring distribution over
the different synsets S of w for each term ti ? T .
Each distribution quantifies the empirical support for
the different senses of the target word, obtained us-
ing ti and the context ctx: we store this informa-
tion in a |T | ? |S| matrix LScore, where each cell
lScorei,j quantifies the support for synset sj ? S,
computed using the term in ti ? T . We calculate the
scores as follows:
- We select at each step an element ti from T (line
7), for instance bancoESn .
- Next, we create a multilingual context ?? by com-
bining ti with the words in ctx (line 8, e.g., we set
?? = { bancoESn , bonusENn , payENv , stockENn }.
- We use ?? to build a graph Gi = (Vi, Ei) by
computing the paths in BabelNet which connect
the synsets of ti with those of the other words
in ?? (line 10, see Section 3.3 for details on the
createGraph function). Note that by selecting at
each step a different element from T we create a
new graph where different sets of Babel synsets
get activated by the context words in ctx. In our
example, Figures 1(a)?(c) show the graphs ob-
tained by setting at different steps ti to bankENn ,
bancoESn and BankDEn , respectively (we show ex-
cerpts by using only stockENn as context word for
ease of readability).
- Finally, we compute the support from term ti for
each synset sj ? S of the target word by applying
a graph connectivity measure to Gi and store the
result in lScorei,j (lines 11?12). For instance, us-
ing degree as graph measure, we can compute the
following scores from the graph in Figure 1(b):
bank2n bank8n bank9n
bancoESn 2 0 1
By repeating the process for each term in T (lines 7?
12) we compute all values in the matrixLScore. For
instance, given T = {bankENn , bancoESn , BankDEn },
we create the set of graphs in Figures 1(a)?(c), and
compute from each of them the following scores
(again, using degree as scoring measure):
LScore =
bank2n bank8n bank9n
bankENn
?
?
2 2 1
?
?bancoESn 2 0 1
BankDEn 2 0 0
Combining sense distributions. In the last step
(line 14) we aggregate the scores associated with
each term of T using an ensemble method M (see
Section 3.4 for details). For instance, M could sim-
ply consist of summing the scores associated with
each sense over all distributions and thus return a
score of 6, 2, and 2 for bank2n, bank8n and bank9n,
respectively. As a result of the execution of Al-
gorithm 1, the combined scoring distribution is re-
turned (line 15). This sense distribution in turn can
be used to select the best sense using Equation 1.
The main hunch behind our approach is that using
information from different languages improves dis-
ambiguation performance, as in the example of Fig-
ure 1 where more accurate disambiguation is per-
formed by combining scores computed from trans-
lations in different languages, as opposed to using
1402
(a) Disambiguation graph using the target word bankENn .
bank2nBankDEbancoESbancaIT
bank8nSparbu?chseDEhuchaESsalvadanaioIT
bank9nBankgeba?udeDEbancoES bancaIT
stock1nAktienDEaccionesESazioniIT
stock4nAktienzertifikatDEaccionesES azioniIT
stock17nViehDEganadoESbestiameIT
commercial
bank
investment
banking
stock
broker trader
piggy
bank pig
building abattoir
(b) Disambiguation graph using bancoESn as translation.
bank2nBankDEbancoESbancaIT
bench1nBankDEbancoESpanchinaIT
bank8nSparbu?chseDEhuchaESsalvadanaioIT
bank9nBankgeba?udeDEbancoES bancaIT
stock1nAktienDEaccionesESazioniIT
stock4nAktienzertifikatDEaccionesES azioniIT
stock17nViehDEganadoESbestiameIT
commercial
bank
investment
banking
stock
broker trader
piggy
bank pig
building abattoir
(c) Disambiguation graph using BankDEn as translation.
bank2nBankDEbancoESbancaIT
bench1n BankDEbancoES panchinaIT
bed4n BankDEestratoES lettoIT
bank8nSparbu?chseDEhuchaESsalvadanaioIT
bank9nBankgeba?udeDEbancoES bancaIT
stock1nAktienDEaccionesESazioniIT
stock4nAktienzertifikatDEaccionesES azioniIT
stock17nViehDEganadoESbestiameIT
commercial
bank
investment
banking
stock
broker trader
piggy
bank pig
building abattoir
(d) List of corresponding WordNet senses and their glosses
bank2n financial institution that accepts deposits and channels
the money into lending activities
bank8n a container (usually with a slot in the top) for keeping
money at home
bank9n a building in which the business of banking transacted
stock1n the capital raised by a corporation through the issue
of shares entitling holders to an ownership interest
stock4n a certificate documenting the shareholder?s
ownership in the corporation
stock17n any animals kept for use or profit
Figure 1: Multilingual graph construction for the input sentence ?bank bonuses are paid in stock?. We show excerpts
using only stockENn as context word for ease of readability.
monolingual sense evidence only. Figure 1(a) shows
the graph created to disambiguate the English target
word bankENn in our example sentence. In the graph,
some of the possible senses of this word are acti-
vated, including the correct one (bank2n) but also re-
lated, yet incorrect ones such as bank8n and bank9n.
Figure 1(b) and 1(c) show instead the graphs ob-
tained from replacing the target word with its Span-
ish and German translations, respectively. In these
graphs, different subsets of the senses of bankENn
are activated, together with others pertaining to the
translations only (e.g., the meaning of bancoESn cor-
responding to the English bench1n). However, the
sense that is consistently activated across all graphs
is the correct one ? i.e., bankENn as financial insti-
tution ? which is in fact the sense selected by our
multilingual approach by means of combining the
scoring distributions from all these graphs.
3.3 Graph-based WSD
We use graph-based algorithms to exploit multilin-
gual knowledge from BabelNet for WSD. These are
a natural choice for our approach, since BabelNet is
a semantic network, and such algorithms have been
shown to achieve high performance across domains
(Agirre et al 2009; Navigli et al 2011), as well
as to compete with supervised methods on a vari-
ety of lexical disambiguation tasks (Ponzetto and
Navigli, 2010). To this end, we use the method
of Navigli and Lapata (2010) and construct a di-
rected graphG = (V,E) for an input word sequence
? = (w1, . . . , wn)5 using the lexical and semantic
relations found in BabelNet. The result of this pro-
cedure is a subgraph of BabelNet containing (1) the
senses of the words in context, (2) all edges and in-
termediate senses found in BabelNet alg all paths
that connect them. Given G, a target word w ? ?
and its set of senses in BabelNet S ? V , we com-
pute a score distribution (score1, . . . , score|S|) over
S, where scorej refers to the confidence score for
the j-th sense of w, e.g. bank2n, based on some con-
nectivity measure applied to G. In this paper, we
specifically focus on two such measures.
5In our experiments we always take ? to be a single sen-
tence, thus disambiguating on a sentence-by-sentence basis.
1403
Degree Centrality (Degree): The first measure
ranks the senses of a given word in the graph based
on the number of their incident edges, namely:
scorej = |{{sj , v} ? E : v ? V }| .
This standard connectivity measure weights a sense
as more appropriate if it has a higher degree. We
chose context-based Degree since, albeit simple, it
had previously been shown to yield a highly com-
petitive performance on various WSD tasks (Navigli
and Lapata, 2010; Ponzetto and Navigli, 2010).
Inverse path length sum (PLength): We then de-
veloped a graph connectivity measure which scores
each sense by summing over the inverse length of all
paths which connect it to other senses in the graph:
scorej =
?
p? paths(sj)
1
elength(p)?1
,
where paths(sj) is the set of simple paths con-
necting sj to the senses of other context words,
length(p) is the number of edges in the path p and
each path is scored with the exponential inverse de-
cay of the path length. This measure overcomes the
locality of Degree by aggregating over all paths be-
tween a sense of the target word and those of the
context words, thus being able to capture the rich-
ness of the BabelNet subgraph and the semantic den-
sity of the underlying knowledge base.
3.4 Ensemble methods for multilingual WSD
At the core of our algorithm lies the combination of
the scores generated using the different translations
of the target word w. For this purpose, we apply so-
called ensemble methods, which have been shown
to improve the performance of both supervised (Flo-
rian et al 2002) and unsupervised WSD systems
(Brody et al 2006). Given |T | lexicalizations and
|S| senses for w, the input to the combination com-
ponent consists of a |T |? |S|matrix LScore, where
each cell lScorei,j quantifies the empirical support
for sense sj from a term ti ? T (see Section 3.2 for
an example). The ensemble method computes from
this translation-sense matrix a combined scoring, ex-
pressing the joint confidence across terms in differ-
ent languages over the set of senses S. In this work,
we use the ?Probability Mixture? (PMixture) method
proposed by Brody et al(2006), which they show
to be the best performing for WSD. This method
takes the scores associated with each term, normal-
izes and combines them by summing across distri-
butions. Formally, it computes the score for the j-th
sense of w as follows:
scorej =
|T |?
i=1
p(si,j), p(si,j) =
lScorei,j
?|S|
s=1 lScorei,s
.
For instance, using the (normalized) sense distribu-
tions from our example, the ensemble distribution
will be the following:
bank2n bank8n bank9n
bankENn 0.40 0.40 0.20
bancoESn 0.67 0.00 0.33
BankDEn 1.00 0.00 0.00
PMixture 2.07 0.40 0.53
3.5 Weighting multilingual sense distribution
Computing a sense distribution for each translation
using the same graph connectivity measure assumes
that all translations are equal. However, a leitmotif
of multilingual WSD research is that translations re-
strict the set of candidate senses of the target word
in the source language. In our example of Figure
1, for instance, BankDEn provides structural support
only for the financial sense of English bank, since
this is the only sense it covers. Within our frame-
work this can potentially lead to skewed sense dis-
tributions when only some senses of the target word
have a translation. In such cases, in fact, scores tend
to be concentrated mostly on the senses covered by
the translations, with the result that sense evidence
for uncovered English senses is disregarded. In or-
der to cope with this issue, we weight the elements
of each sense distribution lScorei for the i-th trans-
lation ti ? T by a factor of 1+log2 cov(ti, w), where
cov(ti, w) is the number of Babel synsets where ti
co-occurs with the target word w ? i.e., the number
of senses of w that it covers (we use the log func-
tion to dampen the effect of high coverage values).
This is to say, in order to level off the effects of un-
balanced sense coverage we assume that, all things
being equal, the more senses a translation covers, the
stronger the disambiguation evidence it provides in
context for specific senses. As a result, the contri-
butions of each translation are weighted differently
1404
and we are thus able to dampen the effects of a
highly skewed distribution like, for instance, that of
BankDEn :
bank2n bank8n bank9n
bankENn 1.72 1.72 0.86
bancoESn 1.34 0.00 0.66
BankDEn 1.00 0.00 0.00
Weighted PMixture 4.04 1.70 1.52
4 Experiments
We evaluate our approach in two different settings,
namely a monolingual all-words WSD task in Sec-
tion 4.1, as well as two different cross-lingual dis-
ambiguation gold standards in Section 4.2.
4.1 Monolingual WSD
Experimental setting. We first evaluate the per-
formance of multilingual joint WSD on a standard
monolingual dataset, namely the SemEval-2010 do-
main WSD task 17 (Agirre et al 2010), since it
provides the latest dataset for fine-grained WSD in
English. We opt for an English all-words task for
two main reasons: first, it is a well-established and
widely-participated task in the WSD community ?
thus ensuring a comparison of our method with a
wide range of state-of-the-art approaches, includ-
ing other graph-based techniques (e.g., Personalized
PageRank), as well as weakly-supervised and super-
vised approaches (see Agirre et al(2010) for de-
tails on the participating systems); second, we want
to assess whether a multilingual approach benefits
lexical disambiguation in all settings, namely even
in a standard monolingual one. We use in our ex-
periments the dataset?s nouns-only subset (1032 in-
stances), since BabelNet currently contains multi-
lingual lexicalizations for nouns only (and thus no
multilingual strategy can be applied to other parts
of speech). We perform graph-based WSD with
BabelNet in two different configurations, namely a
monolingual and multilingual setting. The multi-
lingual system performs WSD by means of the full
joint multilingual approach described in Algorithm
1. The monolingual approach, instead, simply uses
the English input sentence for disambiguation ? that
is, we skip lines 3?4 of Algorithm 1. Knowledge-
based systems typically suffer from a low recall ?
i.e., they cannot provide an answer if no information
Algorithm P R F1
Monolingual Degree 50.6 45.2 47.7
graph PLength 51.0 47.3 49.1
Multilingual Degree? 53.9 48.6 51.1
ensemble PLength? 54.3 50.2 52.2
SemCor MFS 51.9 51.2 51.5
Random 25.3 25.3 25.3
Table 1: Performance on SemEval-2010 all-words do-
main WSD (nouns only subset). Best results for each
measure are bolded. ? indicates statistically significant
differences with respect to the monolingual setting.
can be found with senses of the context words. To
overcome this issue, in both settings we use a type-
based fallback strategy which assigns to the target
word the sense which has been most frequently as-
signed by the system to other instances of the word
in the dataset.
Results and discussion. We report our results in
terms of precision (P), recall (R) and F1 measure in
Table 1, where we compare the monolingual vari-
ant (rows 1?2 of the table) with our multilingual
approach (rows 3?4). Following standard practice,
(1) we benchmark our method against two baselines,
namely a random sense assignment and the most fre-
quent sense (MFS) from SemCor; (2) we test for sta-
tistical significance by computing a 95% confidence
interval on the recall score (i.e., the main evaluation
measure for the WSD task) using bootstrap resam-
pling (Noreen, 1989).
The results show that our multilingual approach
improves over the monolingual one by a substan-
tial (i.e., statistically significant) margin. Combining
multilingual information from different languages
yields a higher precision (+3.3 for both graph algo-
rithms) and recall (+3.4 and +2.9 for Degree and
PLength, respectively). Manual inspection of the
output reveals that these increases in precision are
due to translations in different languages constrain-
ing each other ? e.g., an implausible English sense is
?ruled out? from the sense distributions of the other
languages (cf. the example in Figure 1). The in-
creases in recall, instead, indicate that using trans-
lations triggers responses in those cases where no
sense of the English target word can be connected
to the senses of the context words ? i.e., some trans-
1405
Algorithm P R F1
Monolingual Degree 52.0 51.3 51.6
graph PLength 55.0 54.2 54.6
Multilingual Degree? 61.6 59.5 60.5
ensemble PLength? 62.5 60.4 61.4
CFILT 61.4 59.4 60.4
IIITH 56.4 55.3 55.8
Table 2: Performance on SemEval-2010 all-words do-
main WSD (nouns only subset) using the most frequent
sense assigned by the system as back-off strategy when
no sense assignment is attempted.
lations activate senses in the knowledge base which
are closer to the senses of the context words. The
result is an overall increase in F1 measure of 3.4
and 3.1 points for Degree and PLength, respectively,
which makes it possible for us to beat the MFS
baseline (notably a difficult competitor for WSD
systems). Among the different graph algorithms,
PLength consistently outperforms Degree: however,
the differences are not statistically significant.
In order to better understand the impact of our ap-
proach we follow previous work (e.g., Navigli and
Lapata (2010)) and explore a weakly-supervised set-
ting where the system attempts no sense assignment
if the highest score among those assigned to the
senses of a target word is below a certain threshold.
If this is the case, in order to provide an answer for
all items, we output the most frequent sense assigned
by the system to other instances of the target word,
and fall back to SemCor?s MFS if no assignment has
been attempted. We estimate the optimal value for
the threshold by maximizing F1 on a development
set obtained by combining the Senseval-2 (Palmer et
al., 2001) and Senseval-3 (Snyder and Palmer, 2004)
English all-words datasets. The results for this set-
ting are shown in Table 2, where we also compare
with the top-performing systems from the SemEval
competition, namely CFILT (Kulkarni et al 2010)
and IIITH (Reddy et al 2010).
By complementing our multilingual method with
the MFS heuristic we achieve a performance compa-
rable with the state of the art on this task. Again, the
multilingual ensemble approach consistently outper-
forms the monolingual one and enables us to achieve
the best overall results for this dataset: without mul-
tilingual information, in fact, we achieve only aver-
age performance above the MFS level, whereas by
effectively combining sense evidence from multilin-
gual translations we are able to boost the F1 measure
by a 6-8 point margin, and thus outperform the top-
ranking SemEval systems. While differences with
CFILT are not statistically significant, we still take
this to be good news, since our system is general
purpose in nature and, accordingly, does not use any
domain information such as manually-labeled exam-
ples for the most frequent domain words (CFILT) or
a domain-specific sense ranking (IIITH).
4.2 Cross-lingual lexical disambiguation
Using a multilingual lexical resource makes it possi-
ble to perform WSD in any of its languages. Ac-
cordingly, we complement our evaluation on En-
glish texts with a second set of experiments where
we quantify the impact of our approach on a lex-
ical disambiguation task in a multilingual setting.
To this end, we use the SemEval-2010 cross-lingual
lexical substitution (Mihalcea et al 2010, CL-LS,
henceforth) and WSD (Lefever et al 2011, CL-
WSD) tasks and evaluate our methodology on per-
forming disambiguation across different languages.
Both cross-lingual WSD tasks cast disambiguation
as a word translation problem: given an English pol-
ysemous noun in context as input, the system dis-
ambiguates it by providing a translation into another
language (translations are deemed correct if they
preserve the meaning of the source word in the target
language). Their main difference, instead, lies in the
range of translations which are assumed to be valid:
that is, while CL-LS assumes no predefined sense in-
ventory (i.e., any translation can be potentially cor-
rect), CL-WSD makes use of a sense inventory built
on the basis of the Europarl corpus (Koehn, 2005).
Our approach to lexical disambiguation involves
two steps: first, given a target word in context, we
disambiguate it as usual to the highest-ranked Ba-
bel synset; next, given the translations in the se-
lected synset, we return the most suitable lexical-
ization in the language of interest. Since the se-
lected synset can contain multiple translations in a
target language for the input English word, we ex-
plore using an unsupervised strategy to select the
most reliable translation from multiple candidates.
To this end, we return for each test instance only the
1406
Algorithm P/R/F1
Baseline 23.80
Monolingual Degree 30.52
graph PLength 30.64
Multilingual Degree 32.21
ensemble PLength 32.47
UBA-T 32.17
Table 3: Performance on SemEval-2010 lexical substitu-
tion (best results are bolded).
most frequent translation found in the Babel synset.
Given that the two tasks make different assumptions
on the sense inventory (no fixed inventory for CL-
LS vs. Europarl-based for CL-WSD), the frequency
of a translation is calculated as either the number
of Babel synsets in which it occurs (CL-LS), or its
frequency of alignment with the target word, as ob-
tained by applying GIZA++ (Och and Ney, 2003) to
Europarl (CL-WSD). To provide an answer for all
instances, we return this most frequent translation
even when no sense assignment is attempted ? i.e.,
no sense of the target word is connected to any other
sense of the context words ? or a tie occurs.
Results and discussion. We report our results for
CL-LS and CL-WSD in Tables 3 and 4. We evalu-
ate using the nouns-only subset of the CL-LS dataset
and the full CL-WSD dataset, consisting of 300 and
1,000 instances of nouns in context, respectively.
The evaluation scheme is based on the SemEval-
2007 English lexical substitution task (McCarthy
and Navigli, 2009), and consists of an adaptation of
the metrics of precision and recall for the translation
setting. For each task, we compare our monolingual
and multilingual approaches against the best per-
forming SemEval systems for these tasks, namely
UBA-T (Basile and Semeraro, 2010) and UVT-v
(van Gompel, 2010) for CL-LS and CL-WSD, re-
spectively, as well as a recent supervised proposal
that exploits automatically generated multilingual
features from parallel text and translated contexts
(Lefever et al 2011, Parasense). For each task
we also report its official baseline, namely the first
translation from an online-dictionary6 for CL-LS,
and the most frequent word alignment obtained by
6www.spanishdict.com
applying GIZA++ to the Europarl data for CL-WSD.
Our cross-lingual results confirm all trends of the
English monolingual evaluation, namely that: a) our
joint multilingual approach substantially improves
over the simple monolingual graph-based approach;
b) it enables us to achieve state-of-the-art perfor-
mance for these tasks. In the case of both CL-
LS and CL-WSD, using a rich multilingual knowl-
edge base like BabelNet makes it possible to achieve
a respectable performance already with the simple
monolingual approach, thus indicating the viability
of a knowledge-rich approach to sense-driven word
translation. The use of multilingual ensembles al-
ways improves the monolingual setting for all lan-
guages, and allows us to achieve the best overall re-
sults for both CL-LS and CL-WSD. Similarly to the
case of monolingual WSD, manual inspection of the
output reveals that translations help us rule out in-
correct senses and let the disambiguation algorithm
focus on the more coherent set of senses for the in-
put context in a way similar to the one highlighted
by the example in Figure 1. As a result of this we
are able to improve the performance of both mono-
lingual Degree and PLength, and compete with the
state of the art on all disambiguation tasks.
5 Conclusions
In this paper we presented a multilingual joint ap-
proach to WSD. Key to our methodology is the ef-
fective use of a wide-coverage multilingual knowl-
edge base, BabelNet, which we exploit to perform
graph-based WSD across languages and combine
complementary sense evidence from translations in
different languages using an ensemble method. This
is the first proposal to exploit structured multilingual
information within a joint, knowledge-rich frame-
work for WSD. The APIs to perform multilingual
WSD using BabelNet are freely available for re-
search purposes (Navigli and Ponzetto, 2012b).
Thanks to multilingual joint WSD we achieve
state-of-the-art performance on three different gold
standards. The good news about these results is that
not only can further advances be achieved by using
multilingual lexical knowledge, but, more impor-
tantly, that combining multilingual sense evidence
from different languages at the same time yields
consistent improvements over a monolingual ap-
1407
French German Italian Spanish
P/R/F1 P/R/F1 P/R/F1 P/R/F1
Baseline 21.25 13.16 15.18 19.74
UvT-v N/A N/A N/A 23.39
Parasense 24.54 16.88 18.03 22.80
Monolingual Degree 22.94 17.15 18.03 22.48
graph PLength 23.42 17.72 18.19 22.76
Multilingual Degree 24.02 18.07 18.93 23.51
ensemble PLength 24.61 18.26 19.05 23.65
Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
proach in both monolingual and cross-lingual lexical
disambiguation tasks ? that is, ?joining forces pays
off?. Effectively leveraging multilingual knowledge
for WSD helps overcome the shortcomings of the
underlying resource (noise, coverage, etc.), thus in-
dicating that further performance boosts can come
in the future from even better multilingual lexical
resources. Moreover, our methodology is general-
purpose and can be adapted to tasks other than
WSD: in fact, we have already taken the first steps
in this direction by showing the beneficial effects of
a joint multilingual approach to computing semantic
relatedness (Navigli and Ponzetto, 2012a). In ad-
dition, we plan in the very near future to general-
ize our multilingual joint approach and apply it to
high-end tasks such as multilingual textual entail-
ment (Mehdad et al 2011) and sentiment analysis
(Lu et al 2011) ? so as to provide a general frame-
work for knowledge-rich multilingual NLP.
Acknowledgments
The authors gratefully acknowl-
edge the support of the ERC Start-
ing Grant MultiJEDI No. 259234.
BabelNet and its API are available for download at
http://lcl.uniroma1.it/babelnet.
References
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
performing better than generic supervised WSD. In
Proceedings of the 21st International Joint Confer-
ence on Artificial Intelligence (IJCAI-09), pages 1501?
1506.
Eneko Agirre, Oier Lo?pez de Lacalle, Christiane Fell-
baum, Shu-Kai Hsieh, Maurizio Tesconi, Monica
Monachini, Piek Vossen, and Roxanne Segers. 2010.
Semeval-2010 task 17: All-words Word Sense Disam-
biguation on a specific domain. In Proceedings of the
5th International Workshop on Semantic Evaluations
(SemEval-2010), pages 75?80.
Carmen Banea and Rada Mihalcea. 2011. Word Sense
Disambiguation with multilingual features. In Pro-
ceedings of the 9th International Conference on Com-
putational Semantics (IWCS 2011), pages 25?34.
Pierpaolo Basile and Giovanni Semeraro. 2010. UBA:
Using automatic translation and Wikipedia for cross-
lingual lexical substitution. In Proceedings of the
5th International Workshop on Semantic Evaluations
(SemEval-2010), pages 242?247.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised WSD.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL-06), pages 97?104.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1991. Word-sense dis-
ambiguation using statistical methods. In Proceed-
ings of the 29th Annual Meeting of the Association for
Computational Linguistics (ACL-91), pages 264?270.
Marine Carpuat and Dekai Wu. 2007. Improving Sta-
tistical Machine Translation using Word Sense Dis-
ambiguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Language Learning
(EMNLP-CoNLL-07), pages 61?72.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up
Word Sense Disambiguation via parallel texts. In Pro-
ceedings of the 20th National Conference on Artificial
Intelligence (AAAI-05), pages 1037?1042.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation improves Statistical Ma-
1408
chine Translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-07), pages 33?40.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proceed-
ings of the 29th Annual Meeting of the Association for
Computational Linguistics (ACL-91), pages 130?137.
Gerard de Melo and Gerhard Weikum. 2010. MENTA:
Inducing multilingual taxonomies from Wikipedia. In
Proceedings of the 19th ACM Conference on Informa-
tion and Knowledge Management (CIKM-10), pages
1099?1108.
Mona Diab. 2003. Word Sense Disambiguation within a
Multilingual Framework. Ph.D. thesis, University of
Maryland, College Park, Maryland.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Radu Florian, Silviu Cucerzan, Charles Schafer, and
David Yarowsky. 2002. Combining classifiers for
Word Sense Disambiguation. Natural Language En-
gineering, 8(4):1?14.
William A. Gale, Kenneth Church, and David Yarowsky.
1992. Using bilingual materials to develop Word
Sense Disambiguation methods. In Proceedings of the
Fourth International Conference on Theoretical and
Methodological Issues in Machine Translation, pages
101?112.
Nancy Ide, Tomaz Erjavec, and Dan Tufis?. 2002. Sense
discrimination with parallel corpora. In Proceedings
of the ACL-02 Workshop on WSD: Recent Successes
and Future Directions, pages 54?60.
Nancy Ide. 2000. Cross-lingual sense determination:
Can it work? Computers and the Humanities, 34:223?
234.
Mitesh M. Khapra, Salil Joshi, Arindam Chatterjee, and
Pushpak Bhattacharyya. 2011. Together we can:
Bilingual bootstrapping for WSD. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics, (ACL-11), pages 561?569.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of Ma-
chine Translation Summit X.
Anup Kulkarni, Mitesh Khapra, Saurabh Sohoney, and
Pushpak Bhattacharyya. 2010. CFILT: Resource
conscious approaches for all-words domain specific
WSD. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), pages
421?426.
Els Lefever and Veronique Hoste. 2010. SemEval-2010
task 3: Cross-lingual Word Sense Disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluations (SemEval-2010), pages 15?20.
Els Lefever, Ve?ronique Hoste, and Martine De Cock.
2011. Parasense or how to use parallel corpora for
Word Sense Disambiguation. In Proceedings of the
49th Annual Meeting of the Association for Computa-
tional Linguistics, (ACL-11), pages 317?322.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin
K. Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proceedings of the
49th Annual Meeting of the Association for Computa-
tional Linguistics, (ACL-11), pages 320?330.
Bernardo Magnini, Danilo Giampiccolo, and Alessan-
dro Vallin. 2004. The Italian lexical sample task at
Senseval-3. In Proceedings of the 3rd International
Workshop on the Evaluation of Systems for the Seman-
tic Analysis of Text (SENSEVAL-3), pages 17?20.
Lluis Ma`rquez, Mariona Taule?, Antonia Mart??, Nu?ria
Artigas, Mar Garc??a, Francis Real, and Dani Ferre?s.
2004. Senseval-3: The Spanish lexical sample task.
In Proceedings of the 3rd International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text (SENSEVAL-3), pages 21?24.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics, (ACL-11), pages 1336?1345.
Christian M. Meyer and Iryna Gurevych. 2012.
Ontowiktionary ? Constructing an ontology from
the collaborative online dictionary Wiktionary. In
Maria Teresa Pazienza and Armando Stellato, edi-
tors, Semi-Automatic Ontology Development: Pro-
cesses and Resources. IGI Global, Hershey, Penn.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
Semeval-2010 task 2: Cross-lingual lexical substitu-
tion. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), pages
9?14.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross Bunker. 1993. A semantic concordance. In Pro-
ceedings of the 3rd DARPA Workshop on Human Lan-
guage Technology, pages 303?308.
Vivi Nastase, Michael Strube, Benjamin Bo?rschinger,
Caecilia Zirn, and Anas Elghafari. 2010. WikiNet:
A very large scale multi-lingual concept network. In
Proceedings of the 7th International Conference on
Language Resources and Evaluation, (LREC ?10).
Roberto Navigli and Mirella Lapata. 2010. An exper-
imental study on graph connectivity for unsupervised
Word Sense Disambiguation. IEEE Transactions on
Pattern Anaylsis and Machine Intelligence, 32(4):678?
692.
1409
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belNet: Building a very large multilingual semantic
network. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
(ACL-10), pages 216?225.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. Ba-
belRelate! A joint multilingual approach to computing
semantic relatedness. In Proceedings of the 26th Con-
ference on Artificial Intelligence (AAAI-12).
Roberto Navigli and Simone Paolo Ponzetto. 2012b.
Multilingual WSD with just a few lines of code: The
BabelNet API. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics, (ACL-12). System Demonstrations.
Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier Lopez
de Lacalle, and Eneko Agirre. 2011. Two birds with
one stone: Learning semantic models for Text Cate-
gorization and Word Sense Disambiguation. In Pro-
ceedings of the 20th ACM Conference on Informa-
tion and Knowledge Management (CIKM-11), pages
2317?2320.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Eric W. Noreen, editor. 1989. Computer-intensive meth-
ods for testing hypotheses: an introduction. New
York, N.Y.: John Wiley.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya, and
Hikaru Yokono. 2010. SemEval-2010 task: Japanese
WSD. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), pages
69?74.
Zeynep Orhan, Emine C?elik, and Demirgu?c? Neslihan.
2007. SemEval-2007 task 12: Turkish lexical sample
task. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007), pages
59?63.
Martha Palmer, Christiane Fellbaum, Scott Cotton, Lau-
ren Delfs, and Hoa Trang Dang. 2001. English tasks:
All-words and verb lexical sample. In Proceedings of
the 2nd International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-2), pages
21?24.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rivaling
supervised system. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, (ACL-10), pages 1522?1531.
Siva Reddy, Abhilash Inumella, Diana McCarthy, and
Mark Stevenson. 2010. IIITH: Domain specific
Word Sense Disambiguation. In Proceedings of the
5th International Workshop on Semantic Evaluations
(SemEval-2010), pages 387?391.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: new evaluation
methods for Word Sense Disambiguation. Journal of
Natural Language Engineering, 5(2):113?133.
Carina Silberer and Simone Paolo Ponzetto. 2010. UHD:
Cross-lingual Word Sense Disambiguation using mul-
tilingual co-occurrence graphs. In Proceedings of the
5th International Workshop on Semantic Evaluations
(SemEval-2010), pages 134?137.
Benjamin Snyder and Martha Palmer. 2004. The English
all-words task. In Proceedings of the 3rd International
Workshop on the Evaluation of Systems for the Seman-
tic Analysis of Text (SENSEVAL-3), pages 41?43.
Maarten van Gompel. 2010. UvT-WSD1: A cross-
lingual word sense disambiguation system. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluations (SemEval-2010), pages 238?241.
Zhi Zhong and Hwee Tou Ng. 2009. Word Sense Dis-
ambiguation for all words without hard labor. In Pro-
ceedings of the 21st International Joint Conference on
Artificial Intelligence (IJCAI-09), pages 1616?1622.
1410
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 216?225,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
BabelNet: Building a Very Large Multilingual Semantic Network
Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
navigli@di.uniroma1.it
Simone Paolo Ponzetto
Department of Computational Linguistics
Heidelberg University
ponzetto@cl.uni-heidelberg.de
Abstract
In this paper we present BabelNet ? a
very large, wide-coverage multilingual se-
mantic network. The resource is automat-
ically constructed by means of a method-
ology that integrates lexicographic and en-
cyclopedic knowledge from WordNet and
Wikipedia. In addition Machine Transla-
tion is also applied to enrich the resource
with lexical information for all languages.
We conduct experiments on new and ex-
isting gold-standard datasets to show the
high quality and coverage of the resource.
1 Introduction
In many research areas of Natural Language Pro-
cessing (NLP) lexical knowledge is exploited to
perform tasks effectively. These include, among
others, text summarization (Nastase, 2008),
Named Entity Recognition (Bunescu and Pas?ca,
2006), Question Answering (Harabagiu et al,
2000) and text categorization (Gabrilovich and
Markovitch, 2006). Recent studies in the diffi-
cult task of Word Sense Disambiguation (Nav-
igli, 2009b, WSD) have shown the impact of the
amount and quality of lexical knowledge (Cuadros
and Rigau, 2006): richer knowledge sources can
be of great benefit to both knowledge-lean systems
(Navigli and Lapata, 2010) and supervised classi-
fiers (Ng and Lee, 1996; Yarowsky and Florian,
2002).
Various projects have been undertaken to make
lexical knowledge available in a machine read-
able format. A pioneering endeavor was Word-
Net (Fellbaum, 1998), a computational lexicon of
English based on psycholinguistic theories. Sub-
sequent projects have also tackled the significant
problem of multilinguality. These include Eu-
roWordNet (Vossen, 1998), MultiWordNet (Pianta
et al, 2002), the Multilingual Central Repository
(Atserias et al, 2004), and many others. How-
ever, manual construction methods inherently suf-
fer from a number of drawbacks. First, maintain-
ing and updating lexical knowledge resources is
expensive and time-consuming. Second, such re-
sources are typically lexicographic, and thus con-
tain mainly concepts and only a few named enti-
ties. Third, resources for non-English languages
often have a much poorer coverage since the con-
struction effort must be repeated for every lan-
guage of interest. As a result, an obvious bias ex-
ists towards conducting research in resource-rich
languages, such as English.
A solution to these issues is to draw upon
a large-scale collaborative resource, namely
Wikipedia1. Wikipedia represents the perfect com-
plement to WordNet, as it provides multilingual
lexical knowledge of a mostly encyclopedic na-
ture. While the contribution of any individual user
might be imprecise or inaccurate, the continual in-
tervention of expert contributors in all domains re-
sults in a resource of the highest quality (Giles,
2005). But while a great deal of work has been re-
cently devoted to the automatic extraction of struc-
tured information from Wikipedia (Wu and Weld,
2007; Ponzetto and Strube, 2007; Suchanek et
al., 2008; Medelyan et al, 2009, inter alia), the
knowledge extracted is organized in a looser way
than in a computational lexicon such as WordNet.
In this paper, we make a major step towards the
vision of a wide-coverage multilingual knowledge
resource. We present a novel methodology that
produces a very large multilingual semantic net-
work: BabelNet. This resource is created by link-
ing Wikipedia to WordNet via an automatic map-
ping and by integrating lexical gaps in resource-
1http://download.wikipedia.org. We use the
English Wikipedia database dump from November 3, 2009,
which includes 3,083,466 articles. Throughout this paper, we
use Sans Serif for words, SMALL CAPS for Wikipedia pages
and CAPITALS for Wikipedia categories.
216
high wind
blow gas
gasbag
windhot-air
balloon
gas
cluster
ballooning
Montgolfier
brothers
Fermi gas
is-
a
has-part
is-a
is-a
Wikipedia WordNet
balloon
BABEL SYNSET
balloonEN, BallonDE,
aerostatoES, globusCA,
pallone aerostaticoIT,
ballonFR, montgolfie`reFR
WIKIPEDIA SENTENCES
...world?s first hydrogen balloon flight.
...an interim balloon altitude record...
...from a British balloon near Be?court...
+
SEMCOR SENTENCES
...look at the balloon and the...
...suspended like a huge balloon, in...
...the balloon would go up...
Machine Translation system
Figure 1: An illustrative overview of BabelNet.
poor languages with the aid of Machine Transla-
tion. The result is an ?encyclopedic dictionary?,
that provides concepts and named entities lexical-
ized in many languages and connected with large
amounts of semantic relations.
2 BabelNet
We encode knowledge as a labeled directed graph
G = (V,E) where V is the set of vertices ? i.e.
concepts2 such as balloon ? andE ? V ?R?V is
the set of edges connecting pairs of concepts. Each
edge is labeled with a semantic relation from R,
e.g. {is-a, part-of , . . . , }, where  denotes an un-
specified semantic relation. Importantly, each ver-
tex v ? V contains a set of lexicalizations of the
concept for different languages, e.g. { balloonEN,
BallonDE, aerostatoES, . . . , montgolfie`reFR }.
Concepts and relations in BabelNet are har-
vested from the largest available semantic lexi-
con of English, WordNet, and a wide-coverage
collaboratively edited encyclopedia, the English
Wikipedia (Section 3.1). We collect (a) from
WordNet, all available word senses (as concepts)
and all the semantic pointers between synsets (as
relations); (b) from Wikipedia, all encyclopedic
entries (i.e. pages, as concepts) and semantically
unspecified relations from hyperlinked text.
In order to provide a unified resource, we merge
the intersection of these two knowledge sources
(i.e. their concepts in common) by establishing a
mapping between Wikipedia pages and WordNet
senses (Section 3.2). This avoids duplicate con-
cepts and allows their inventories of concepts to
complement each other. Finally, to enable mul-
tilinguality, we collect the lexical realizations of
the available concepts in different languages by
2Throughout the paper, unless otherwise stated, we use
the general term concept to denote either a concept or a
named entity.
using (a) the human-generated translations pro-
vided in Wikipedia (the so-called inter-language
links), as well as (b) a machine translation sys-
tem to translate occurrences of the concepts within
sense-tagged corpora, namely SemCor (Miller et
al., 1993) ? a corpus annotated with WordNet
senses ? and Wikipedia itself (Section 3.3). We
call the resulting set of multilingual lexicalizations
of a given concept a babel synset. An overview of
BabelNet is given in Figure 1 (we label vertices
with English lexicalizations): unlabeled edges are
obtained from links in the Wikipedia pages (e.g.
BALLOON (AIRCRAFT) links to WIND), whereas
labeled ones from WordNet3 (e.g. balloon1n has-
part gasbag1n). In this paper we restrict ourselves
to concepts lexicalized as nouns. Nonetheless, our
methodology can be applied to all parts of speech,
but in that case Wikipedia cannot be exploited,
since it mainly contains nominal entities.
3 Methodology
3.1 Knowledge Resources
WordNet. The most popular lexical knowledge
resource in the field of NLP is certainly WordNet,
a computational lexicon of the English language.
A concept in WordNet is represented as a synonym
set (called synset), i.e. the set of words that share
the same meaning. For instance, the concept wind
is expressed by the following synset:
{ wind1n, air current1n, current of air1n },
where each word?s subscripts and superscripts in-
dicate their parts of speech (e.g. n stands for noun)
3We use in the following WordNet version 3.0. We de-
note with wip the i-th sense of a word w with part of speech
p. We use word senses to unambiguously denote the corre-
sponding synsets (e.g. plane1n for { airplane1n, aeroplane1n,
plane1n }). Hereafter, we use word sense and synset inter-
changeably.
217
and sense number, respectively. For each synset,
WordNet provides a textual definition, or gloss.
For example, the gloss of the above synset is: ?air
moving from an area of high pressure to an area of
low pressure?.
Wikipedia. Our second resource, Wikipedia,
is a Web-based collaborative encyclopedia. A
Wikipedia page (henceforth, Wikipage) presents
the knowledge about a specific concept (e.g. BAL-
LOON (AIRCRAFT)) or named entity (e.g. MONT-
GOLFIER BROTHERS). The page typically con-
tains hypertext linked to other relevant Wikipages.
For instance, BALLOON (AIRCRAFT) is linked to
WIND, GAS, and so on. The title of a Wikipage
(e.g. BALLOON (AIRCRAFT)) is composed of
the lemma of the concept defined (e.g. balloon)
plus an optional label in parentheses which speci-
fies its meaning if the lemma is ambiguous (e.g.
AIRCRAFT vs. TOY). Wikipages also provide
inter-language links to their counterparts in other
languages (e.g. BALLOON (AIRCRAFT) links to
the Spanish page AEROSTATO). Finally, some
Wikipages are redirections to other pages, e.g.
the Spanish BALO?N AEROSTA?TICO redirects to
AEROSTATO.
3.2 Mapping Wikipedia to WordNet
The first phase of our methodology aims to estab-
lish links between Wikipages and WordNet senses.
We aim to acquire a mapping ? such that, for each
Wikipage w, we have:
?(w) =
?
?
?
s ? SensesWN(w) if a link can be
established,
 otherwise,
where SensesWN(w) is the set of senses of the
lemma of w in WordNet. For example, if our map-
ping methodology linked BALLOON (AIRCRAFT)
to the corresponding WordNet sense balloon1n,
we would have ?(BALLOON (AIRCRAFT)) = bal-
loon1n.
In order to establish a mapping between the
two resources, we first identify the disambigua-
tion contexts for Wikipages (Section 3.2.1) and
WordNet senses (Section 3.2.2). Next, we inter-
sect these contexts to perform the mapping (see
Section 3.2.3).
3.2.1 Disambiguation Context of a Wikipage
Given a Wikipage w, we use the following infor-
mation as disambiguation context:
? Sense labels: e.g. given the page BALLOON
(AIRCRAFT), the word aircraft is added to the
disambiguation context.
? Links: the titles? lemmas of the pages linked
from the target Wikipage (i.e., outgoing links).
For instance, the links in the Wikipage BAL-
LOON (AIRCRAFT) include wind, gas, etc.
? Categories: Wikipages are typically classi-
fied according to one or more categories.
For example, the Wikipage BALLOON (AIR-
CRAFT) is categorized as BALLOONS, BAL-
LOONING, etc. While many categories are
very specific and do not appear in Word-
Net (e.g., SWEDISH WRITERS or SCIEN-
TISTS WHO COMMITTED SUICIDE), we
use their syntactic heads as disambiguation con-
text (i.e. writer and scientist, respectively).
Given a Wikipage w, we define its disambiguation
context Ctx(w) as the set of words obtained from
all of the three sources above.
3.2.2 Disambiguation Context of a WordNet
Sense
Given a WordNet sense s and its synset S, we col-
lect the following information:
? Synonymy: all synonyms of s in S. For in-
stance, given the sense airplane1n and its cor-
responding synset { airplane1n, aeroplane1n,
plane1n }, the words contained therein are in-
cluded in the context.
? Hypernymy/Hyponymy: all synonyms in the
synsets H such that H is either a hypernym
(i.e., a generalization) or a hyponym (i.e., a
specialization) of S. For example, given bal-
loon1n, we include the words from its hypernym
{ lighter-than-air craft1n } and all its hyponyms
(e.g. { hot-air balloon1n }).
? Sisterhood: words from the sisters of S. A sis-
ter synset S? is such that S and S? have a com-
mon direct hypernym. For example, given bal-
loon1n, it can be found that { balloon1n } and
{ airship1n, dirigible1n } are sisters. Thus air-
ship and dirigible are included in the disam-
biguation context of s.
? Gloss: the set of lemmas of the content words
occurring within the WordNet gloss of S.
We thus define the disambiguation context Ctx(s)
of sense s as the set of words obtained from all of
the four sources above.
218
3.2.3 Mapping Algorithm
In order to link each Wikipedia page to a WordNet
sense, we perform the following steps:
? Initially, our mapping ? is empty, i.e. it links
each Wikipage w to .
? For each Wikipage w whose lemma is monose-
mous both in Wikipedia and WordNet we map
w to its only WordNet sense.
? For each remaining Wikipage w for which no
mapping was previously found (i.e., ?(w) = ),
we assign the most likely sense to w based on
the maximization of the conditional probabili-
ties p(s|w) over the senses s ? SensesWN(w)
(no mapping is established if a tie occurs).
To find the mapping of a Wikipage w, we need
to compute the conditional probability p(s|w) of
selecting the WordNet sense s given w. The sense
s which maximizes this probability is determined
as follows:
?(w) = argmax
s?SensesWN(w)
p(s|w) = argmax
s
p(s, w)
p(w)
= argmax
s
p(s, w)
The latter formula is obtained by observing that
p(w) does not influence our maximization, as it is
a constant independent of s. As a result, determin-
ing the most appropriate sense s consists of find-
ing the sense s that maximizes the joint probability
p(s, w). We estimate p(s, w) as:
p(s, w) =
score(s, w)
?
s??SensesWN(w),
w??SensesWiki(w)
score(s?, w?)
,
where score(s, w) = |Ctx(s)?Ctx(w)|+ 1 (we
add 1 as a smoothing factor). Thus, in our al-
gorithm we determine the best sense s by com-
puting the intersection of the disambiguation con-
texts of s and w, and normalizing by the scores
summed over all senses of w in Wikipedia and
WordNet. More details on the mapping algorithm
can be found in Ponzetto and Navigli (2010).
3.3 Translating Babel Synsets
So far we have linked English Wikipages to Word-
Net senses. Given a Wikipage w, and provided it
is mapped to a sense s (i.e., ?(w) = s), we cre-
ate a babel synset S ?W , where S is the WordNet
synset to which sense s belongs, and W includes:
(i) w; (ii) all its inter-language links (that is, trans-
lations of the Wikipage to other languages); (iii)
the redirections to the inter-language links found
in the Wikipedia of the target language. For in-
stance, given that ?(BALLOON) = balloon1n, the
corresponding babel synset is { balloonEN, Bal-
lonDE, aerostatoES, balo?n aerosta?ticoES, . . . ,
pallone aerostaticoIT }. However, two issues
arise: first, a concept might be covered only in
one of the two resources (either WordNet or
Wikipedia), meaning that no link can be estab-
lished (e.g., FERMI GAS or gasbag1n in Figure
1); second, even if covered in both resources, the
Wikipage for the concept might not provide any
translation for the language of interest (e.g., the
Catalan for BALLOON is missing in Wikipedia).
In order to address the above issues and thus
guarantee high coverage for all languages we de-
veloped a methodology for translating senses in
the babel synset to missing languages. Given a
WordNet word sense in our babel synset of interest
(e.g. balloon1n) we collect its occurrences in Sem-
Cor (Miller et al, 1993), a corpus of more than
200,000 words annotated with WordNet senses.
We do the same for Wikipages by retrieving sen-
tences in Wikipedia with links to the Wikipage of
interest. By repeating this step for each English
lexicalization in a babel synset, we obtain a col-
lection of sentences for the babel synset (see left
part of Figure 1). Next, we apply state-of-the-art
Machine Translation4 and translate the set of sen-
tences in all the languages of interest. Given a spe-
cific term in the initial babel synset, we collect the
set of its translations. We then identify the most
frequent translation in each language and add it to
the babel synset. Note that translations are sense-
specific, as the context in which a term occurs is
provided to the translation system.
3.4 Example
We now illustrate the execution of our method-
ology by way of an example. Let us focus on
the Wikipage BALLOON (AIRCRAFT). The word
is polysemous both in Wikipedia and WordNet.
In the first phase of our methodology we aim
to find a mapping ?(BALLOON (AIRCRAFT)) to
an appropriate WordNet sense of the word. To
4We use the Google Translate API. An initial prototype
used a statistical machine translation system based on Moses
(Koehn et al, 2007) and trained on Europarl (Koehn, 2005).
However, we found such system unable to cope with many
technical names, such as in the domains of sciences, litera-
ture, history, etc.
219
this end we construct the disambiguation context
for the Wikipage by including words from its la-
bel, links and categories (cf. Section 3.2.1). The
context thus includes, among others, the follow-
ing words: aircraft, wind, airship, lighter-than-
air. We now construct the disambiguation context
for the two WordNet senses of balloon (cf. Sec-
tion 3.2.2), namely the aircraft (#1) and the toy
(#2) senses. To do so, we include words from
their synsets, hypernyms, hyponyms, sisters, and
glosses. The context for balloon1n includes: air-
craft, craft, airship, lighter-than-air. The con-
text for balloon2n contains: toy, doll, hobby. The
sense with the largest intersection is #1, so the
following mapping is established: ?(BALLOON
(AIRCRAFT)) = balloon1n. After the first phase,
our babel synset includes the following English
words from WordNet plus the Wikipedia inter-
language links to other languages (we report Ger-
man, Spanish and Italian): { balloonEN, BallonDE,
aerostatoES, balo?n aerosta?ticoES, pallone aero-
staticoIT }.
In the second phase (see Section 3.3), we col-
lect all the sentences in SemCor and Wikipedia in
which the above English word sense occurs. We
translate these sentences with the Google Trans-
late API and select the most frequent transla-
tion in each language. As a result, we can en-
rich the initial babel synset with the following
words: mongolfie`reFR, globusCA, globoES, mon-
golfieraIT. Note that we had no translation for
Catalan and French in the first phase, because the
inter-language link was not available, and we also
obtain new lexicalizations for the Spanish and Ital-
ian languages.
4 Experiment 1: Mapping Evaluation
Experimental setting. We first performed an
evaluation of the quality of our mapping from
Wikipedia to WordNet. To create a gold stan-
dard for evaluation we considered all lemmas
whose senses are contained both in WordNet and
Wikipedia: the intersection between the two re-
sources contains 80,295 lemmas which corre-
spond to 105,797 WordNet senses and 199,735
Wikipedia pages. The average polysemy is 1.3
and 2.5 for WordNet senses and Wikipages, re-
spectively (2.8 and 4.7 when excluding monose-
mous words). We then selected a random sam-
ple of 1,000 Wikipages and asked an annotator
with previous experience in lexicographic annota-
P R F1 A
Mapping algorithm 81.9 77.5 79.6 84.4
MFS BL 24.3 47.8 32.2 24.3
Random BL 23.8 46.8 31.6 23.9
Table 1: Performance of the mapping algorithm.
tion to provide the correct WordNet sense for each
page (an empty sense label was given, if no correct
mapping was possible). The gold-standard dataset
includes 505 non-empty mappings, i.e. Wikipages
with a corresponding WordNet sense. In order to
quantify the quality of the annotations and the dif-
ficulty of the task, a second annotator sense tagged
a subset of 200 pages from the original sample.
Our annotators achieved a ? inter-annotator agree-
ment (Carletta, 1996) of 0.9, indicating almost
perfect agreement.
Results and discussion. Table 1 summarizes the
performance of our mapping algorithm against
the manually annotated dataset. Evaluation is per-
formed in terms of standard measures of preci-
sion, recall, and F1-measure. In addition we calcu-
late accuracy, which also takes into account empty
sense labels. As baselines we use the most fre-
quent WordNet sense (MFS), and a random sense
assignment.
The results show that our method achieves al-
most 80% F1 and it improves over the baselines by
a large margin. The final mapping contains 81,533
pairs of Wikipages and word senses they map to,
covering 55.7% of the noun senses in WordNet.
As for the baselines, the most frequent sense is
just 0.6% and 0.4% above the random baseline in
terms of F1 and accuracy, respectively. A ?2 test
reveals in fact no statistical significant difference
at p < 0.05. This is related to the random distri-
bution of senses in our dataset and the Wikipedia
unbiased coverage of WordNet senses. So select-
ing the first WordNet sense rather than any other
sense for each target page represents a choice as
arbitrary as picking a sense at random.
5 Experiment 2: Translation Evaluation
We perform a second set of experiments concern-
ing the quality of the acquired concepts. This is as-
sessed in terms of coverage against gold-standard
resources (Section 5.1) and against a manually-
validated dataset of translations (Section 5.2).
220
Language Word senses Synsets
German 15,762 9,877
Spanish 83,114 55,365
Catalan 64,171 40,466
Italian 57,255 32,156
French 44,265 31,742
Table 2: Size of the gold-standard wordnets.
5.1 Automatic Evaluation
Datasets. We compare BabelNet against gold-
standard resources for 5 languages, namely: the
subset of GermaNet (Lemnitzer and Kunze, 2002)
included in EuroWordNet for German, Multi-
WordNet (Pianta et al, 2002) for Italian, the Mul-
tilingual Central Repository for Spanish and Cata-
lan (Atserias et al, 2004), and WOrdnet Libre
du Franc?ais (Beno??t and Fis?er, 2008, WOLF) for
French. In Table 2 we report the number of synsets
and word senses available in the gold-standard re-
sources for the 5 languages.
Measures. Let B be BabelNet, F our gold-
standard non-English wordnet (e.g. GermaNet),
and let E be the English WordNet. All the gold-
standard non-English resources, as well as Babel-
Net, are linked to the English WordNet: given a
synset SF ? F , we denote its corresponding babel
synset as SB and its synset in the English Word-
Net as SE . We assess the coverage of BabelNet
against our gold-standard wordnets both in terms
of synsets and word senses. For synsets, we calcu-
late coverage as follows:
SynsetCov(B,F) =
?
SF?F
?(SB, SF )
|{SF ? F}|
,
where ?(SB, SF ) = 1 if the two synsets SB and
SF have a synonym in common, 0 otherwise. That
is, synset coverage is determined as the percentage
of synsets of F that share a term with the corre-
sponding babel synsets. For word senses we cal-
culate a similar measure of coverage:
WordCov(B,F) =
?
SF?F
?
sF?SF
??(sF , SB)
|{sF ? SF : SF ? F}|
,
where sF is a word sense in synset SF and
??(sF , SB) = 1 if sF ? SB, 0 otherwise. That
is we calculate the ratio of word senses in our
gold-standard resource F that also occur in the
corresponding synset SB to the overall number of
senses in F .
However, our gold-standard resources cover
only a portion of the English WordNet, whereas
the overall coverage of BabelNet is much higher.
We calculate extra coverage for synsets as follows:
SynsetExtraCov(B,F) =
?
SE?E\F
?(SB, SE)
|{SF ? F}|
.
Similarly, we calculate extra coverage for word
senses in BabelNet corresponding to WordNet
synsets not covered by the reference resource F .
Results and discussion. We evaluate the cov-
erage and extra coverage of word senses and
synsets at different stages: (a) using only the inter-
language links from Wikipedia (WIKI Links); (b)
and (c) using only the automatic translations of the
sentences from Wikipedia (WIKI Transl.) or Sem-
Cor (WN Transl.); (d) using all available transla-
tions, i.e. BABELNET.
Coverage results are reported in Table 3. The
percentage of word senses covered by BabelNet
ranges from 52.9% (Italian) to 66.4 (Spanish)
and 86.0% (French). Synset coverage ranges from
73.3% (Catalan) to 76.6% (Spanish) and 92.9%
(French). As expected, synset coverage is higher,
because a synset in the reference resource is con-
sidered to be covered if it shares at least one word
with the corresponding synset in BabelNet.
Numbers for the extra coverage, which pro-
vides information about the percentage of word
senses and synsets in BabelNet but not in the gold-
standard resources, are given in Figure 2. The re-
sults show that we provide for all languages a high
extra coverage for both word senses ? between
340.1% (Catalan) and 2,298% (German) ? and
synsets ? between 102.8% (Spanish) and 902.6%
(German).
Table 3 and Figure 2 show that the best results
are obtained when combining all available trans-
lations, i.e. both from Wikipedia and the machine
translation system. The performance figures suf-
fer from the errors of the mapping phase (see Sec-
tion 4). Nonetheless, the results are generally high,
with a peak for French, since WOLF has been cre-
ated semi-automatically by combining several re-
sources, including Wikipedia. The relatively low
word sense coverage for Italian (55.4%) is, in-
stead, due to the lack of many common words in
the Italian gold-standard synsets. Examples in-
clude whipEN translated as staffileIT but not as the
more common frustaIT, playboyEN translated as
vitaioloIT but not gigolo`IT, etc.
221
0%	 ?
500%	 ?
1000%	 ?
1500%	 ?
2000%	 ?
2500%	 ?
German	 ? Spanish	 ? Catalan	 ? Italian	 ? French	 ?
Wiki	 ?Links	 ?
Wiki	 ?Transl.	 ?
WN	 ?	 ?Transl.	 ?
BabelNet	 ?
(a) word senses
0%	 ?
100%	 ?
200%	 ?
300%	 ?
400%	 ?
500%	 ?
600%	 ?
700%	 ?
800%	 ?
900%	 ?
1000%	 ?
German	 ? Spanish	 ? Catalan	 ? Italian	 ? French	 ?
Wiki	 ?Links	 ?
Wiki	 ?Transl.	 ?
WN	 ?	 ?Transl.	 ?
BabelNet	 ?
(b) synsets
Figure 2: Extra coverage against gold-standard wordnets: word senses (a) and synsets (b).
Resource Method SENSES SYNSETS
G
er
m
an WIKI
{ Links 39.6 50.7
Transl. 42.6 58.2
WN Transl. 21.0 28.6
BABELNET All 57.6 73.4
S
pa
ni
sh WIKI
{ Links 34.4 40.7
Transl. 47.9 56.1
WN Transl. 25.2 30.0
BABELNET All 66.4 76.6
C
at
al
an
WIKI
{ Links 20.3 25.2
Transl. 46.9 54.1
WN Transl. 25.0 29.6
BABELNET All 64.0 73.3
It
al
ia
n
WIKI
{ Links 28.1 40.0
Transl. 39.9 58.0
WN Transl. 19.7 28.7
BABELNET All 52.9 73.7
F
re
nc
h WIKI
{ Links 70.0 72.4
Transl. 69.6 79.6
WN Transl. 16.3 19.4
BABELNET All 86.0 92.9
Table 3: Coverage against gold-standard wordnets
(we report percentages).
5.2 Manual Evaluation
Experimental setup. The automatic evaluation
quantifies how much of the gold-standard re-
sources is covered by BabelNet. However, it
does not say anything about the precision of the
additional lexicalizations provided by BabelNet.
Given that our resource has displayed a remark-
ably high extra coverage ? ranging from 340%
to 2,298% of the national wordnets (see Figure
2) ? we performed a second evaluation to assess
its precision. For each of our 5 languages, we
selected a random set of 600 babel synsets com-
posed as follows: 200 synsets whose senses ex-
ist in WordNet only, 200 synsets in the intersec-
tion between WordNet and Wikipedia (i.e. those
mapped with our method illustrated in Section
3.2), 200 synsets whose lexicalizations exist in
Wikipedia only. Therefore, our dataset included
600? 5 = 3,000 babel synsets. None of the synsets
was covered by any of the five reference wordnets.
The babel synsets were manually validated by ex-
pert annotators who decided which senses (i.e.
lexicalizations) were appropriate given the corre-
sponding WordNet gloss and/or Wikipage.
Results and discussion. We report the results in
Table 4. For each language (rows) and for each
of the three regions of BabelNet (columns), we
report precision (i.e. the percentage of synonyms
deemed correct) and, in parentheses, the over-
all number of synonyms evaluated. The results
show that the different regions of BabelNet con-
tain translations of different quality: while on av-
erage translations for WordNet-only synsets have
a precision around 72%, when Wikipedia comes
into play the performance increases considerably
(around 80% in the intersection and 95% with
Wikipedia-only translations). As can be seen from
the figures in parentheses, the number of trans-
lations available in the presence of Wikipedia is
higher. This quantitative difference is due to our
method collecting many translations from the redi-
rections in the Wikipedia of the target language
(Section 3.3), as well as to the paucity of examples
in SemCor for many synsets. In addition, some of
the synsets in WordNet with no Wikipedia coun-
terpart are very difficult to translate. Examples
include terms like stammel, crape fern, base-
ball clinic, and many others for which we could
222
Language WN WN ?Wiki Wiki
German 73.76 (282) 78.37 (777) 97.74 (709)
Spanish 69.45 (275) 78.53 (643) 92.46 (703)
Catalan 75.58 (258) 82.98 (517) 92.71 (398)
Italian 72.32 (271) 80.83 (574) 99.09 (552)
French 67.16 (268) 77.43 (709) 96.44 (758)
Table 4: Precision of BabelNet on synonyms in
WordNet (WN), Wikipedia (Wiki) and their inter-
section (WN ? Wiki): percentage and total num-
ber of words (in parentheses) are reported.
not find translations in major editions of bilingual
dictionaries. In contrast, good translations were
produced using our machine translation method
when enough sentences were available. Examples
are: chaudre?e de poissonFR for fish chowderEN,
grano de cafe?ES for coffee beanEN, etc.
6 Related Work
Previous attempts to manually build multilingual
resources have led to the creation of a multi-
tude of wordnets such as EuroWordNet (Vossen,
1998), MultiWordNet (Pianta et al, 2002), Balka-
Net (Tufis? et al, 2004), Arabic WordNet (Black
et al, 2006), the Multilingual Central Repository
(Atserias et al, 2004), bilingual electronic dic-
tionaries such as EDR (Yokoi, 1995), and fully-
fledged frameworks for the development of multi-
lingual lexicons (Lenci et al, 2000). As it is of-
ten the case with manually assembled resources,
these lexical knowledge repositories are hindered
by high development costs and an insufficient cov-
erage. This barrier has led to proposals that ac-
quire multilingual lexicons from either parallel
text (Gale and Church, 1993; Fung, 1995, inter
alia) or monolingual corpora (Sammer and Soder-
land, 2007; Haghighi et al, 2008). The disam-
biguation of bilingual dictionary glosses has also
been proposed to create a bilingual semantic net-
work from a machine readable dictionary (Nav-
igli, 2009a). Recently, Etzioni et al (2007) and
Mausam et al (2009) presented methods to pro-
duce massive multilingual translation dictionaries
from Web resources such as online lexicons and
Wiktionaries. However, while providing lexical
resources on a very large scale for hundreds of
thousands of language pairs, these do not encode
semantic relations between concepts denoted by
their lexical entries.
The research closest to ours is presented by de
Melo and Weikum (2009), who developed a Uni-
versal WordNet (UWN) by automatically acquir-
ing a semantic network for languages other than
English. UWN is bootstrapped from WordNet and
is built by collecting evidence extracted from ex-
isting wordnets, translation dictionaries, and par-
allel corpora. The result is a graph containing
800,000 words from over 200 languages in a hier-
archically structured semantic network with over
1.5 million links from words to word senses. Our
work goes one step further by (1) developing an
even larger multilingual resource including both
lexical semantic and encyclopedic knowledge, (2)
enriching the structure of the ?core? semantic net-
work (i.e. the semantic pointers from WordNet)
with topical, semantically unspecified relations
from the link structure of Wikipedia. This result
is essentially achieved by complementing Word-
Net with Wikipedia, as well as by leveraging the
multilingual structure of the latter. Previous at-
tempts at linking the two resources have been pro-
posed. These include associating Wikipedia pages
with the most frequent WordNet sense (Suchanek
et al, 2008), extracting domain information from
Wikipedia and providing a manual mapping to
WordNet concepts (Auer et al, 2007), a model
based on vector spaces (Ruiz-Casado et al, 2005),
a supervised approach using keyword extraction
(Reiter et al, 2008), as well as automatically
linking Wikipedia categories to WordNet based
on structural information (Ponzetto and Navigli,
2009). In contrast to previous work, BabelNet
is the first proposal that integrates the relational
structure of WordNet with the semi-structured in-
formation from Wikipedia into a unified, wide-
coverage, multilingual semantic network.
7 Conclusions
In this paper we have presented a novel methodol-
ogy for the automatic construction of a large multi-
lingual lexical knowledge resource. Key to our ap-
proach is the establishment of a mapping between
a multilingual encyclopedic knowledge repository
(Wikipedia) and a computational lexicon of En-
glish (WordNet). This integration process has
several advantages. Firstly, the two resources
contribute different kinds of lexical knowledge,
one is concerned mostly with named entities, the
other with concepts. Secondly, while Wikipedia
is less structured than WordNet, it provides large
223
amounts of semantic relations and can be lever-
aged to enable multilinguality. Thus, even when
they overlap, the two resources provide comple-
mentary information about the same named enti-
ties or concepts. Further, we contribute a large
set of sense occurrences harvested from Wikipedia
and SemCor, a corpus that we input to a state-of-
the-art machine translation system to fill in the gap
between resource-rich languages ? such as English
? and resource-poorer ones. Our hope is that the
availability of such a language-rich resource5 will
enable many non-English and multilingual NLP
applications to be developed.
Our experiments show that our fully-automated
approach produces a large-scale lexical resource
with high accuracy. The resource includes millions
of semantic relations, mainly from Wikipedia
(however, WordNet relations are labeled), and
contains almost 3 million concepts (6.7 labels per
concept on average). As pointed out in Section
5, such coverage is much wider than that of ex-
isting wordnets in non-English languages. While
BabelNet currently includes 6 languages, links to
freely-available wordnets6 can immediately be es-
tablished by utilizing the English WordNet as an
interlanguage index. Indeed, BabelNet can be ex-
tended to virtually any language of interest. In
fact, our translation method allows it to cope with
any resource-poor language.
As future work, we plan to apply our method
to other languages, including Eastern European,
Arabic, and Asian languages. We also intend to
link missing concepts in WordNet, by establish-
ing their most likely hypernyms ? e.g., a` la Snow
et al (2006). We will perform a semi-automatic
validation of BabelNet, e.g. by exploiting Ama-
zon?s Mechanical Turk (Callison-Burch, 2009) or
designing a collaborative game (von Ahn, 2006)
to validate low-ranking mappings and translations.
Finally, we aim to apply BabelNet to a variety of
applications which are known to benefit from a
wide-coverage knowledge resource. We have al-
ready shown that the English-only subset of Ba-
belNet alows simple knowledge-based algorithms
to compete with supervised systems in standard
coarse-grained and domain-specific WSD settings
(Ponzetto and Navigli, 2010). We plan in the near
future to apply BabelNet to the challenging task of
cross-lingual WSD (Lefever and Hoste, 2009).
5BabelNet can be freely downloaded for research pur-
poses at http://lcl.uniroma1.it/babelnet.
6http://www.globalwordnet.org.
References
Jordi Atserias, Luis Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
Vossen. 2004. The MEANING multilingual central
repository. In Proc. of GWC-04, pages 80?210.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ive.
2007. Dbpedia: A nucleus for a web of open data.
In Proceedings of 6th International Semantic Web
Conference joint with 2nd Asian Semantic Web Con-
ference (ISWC+ASWC 2007), pages 722?735.
Sagot Beno??t and Darja Fis?er. 2008. Building a free
French WordNet from multilingual resources. In
Proceedings of the Ontolex 2008 Workshop.
William Black, Sabri Elkateb Horacio Rodriguez,
Musa Alkhalifa, Piek Vossen, and Adam Pease.
2006. Introducing the Arabic WordNet project. In
Proc. of GWC-06, pages 295?299.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proc. of EACL-06, pages 9?16.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. of EMNLP-09, pages 286?
295.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Montse Cuadros and German Rigau. 2006. Quality
assessment of large scale knowledge resources. In
Proc. of EMNLP-06, pages 534?541.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proc. of CIKM-09, pages 513?522.
Oren Etzioni, Kobi Reiter, Stephen Soderland, and
Marcus Sammer. 2007. Lexical translation with ap-
plication to image search on the Web. In Proceed-
ings of Machine Translation Summit XI.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Pascale Fung. 1995. A pattern matching method
for finding noun and proper noun translations from
noisy parallel corpora. In Proc. of ACL-95, pages
236?243.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
Wikipedia: Enhancing text categorization with
encyclopedic knowledge. In Proc. of AAAI-06,
pages 1301?1306.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Jim Giles. 2005. Internet encyclopedias go head to
head. Nature, 438:900?901.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. of ACL-08,
pages 771?779.
224
Sanda M. Harabagiu, Dan Moldovan, Marius Pas?ca,
Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu,
Roxana Girju, Vasile Rus, and Paul Morarescu.
2000. FALCON: Boosting knowledge for answer
engines. In Proc. of TREC-9, pages 479?488.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Comp. Vol. to Proc. of ACL-07, pages 177?180.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit X.
Els Lefever and Veronique Hoste. 2009. Semeval-
2010 task 3: Cross-lingual Word Sense Disambigua-
tion. In Proc. of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 82?87, Boulder, Colorado.
Lothar Lemnitzer and Claudia Kunze. 2002. Ger-
maNet ? representation, visualization, application.
In Proc. of LREC ?02, pages 1485?1491.
Alessandro Lenci, Nuria Bel, Federica Busa, Nico-
letta Calzolari, Elisabetta Gola, Monica Monachini,
Antoine Ogonowski, Ivonne Peters, Wim Peters,
Nilda Ruimy, Marta Villegas, and Antonio Zam-
polli. 2000. SIMPLE: A general framework for the
development of multilingual lexicons. International
Journal of Lexicography, 13(4):249?263.
Mausam, Stephen Soderland, Oren Etzioni, Daniel
Weld, Michael Skinner, and Jeff Bilmes. 2009.
Compiling a massive, multilingual dictionary via
probabilistic inference. In Proc. of ACL-IJCNLP-
09, pages 262?270.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. Int. J. Hum.-Comput. Stud., 67(9):716?
754.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross Bunker. 1993. A semantic concordance. In
Proceedings of the 3rd DARPA Workshop on Human
Language Technology, pages 303?308, Plainsboro,
N.J.
Vivi Nastase. 2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
activation spreading. In Proc. of EMNLP-08, pages
763?772.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study on graph connectivity for unsuper-
vised Word Sense Disambiguation. IEEE Transac-
tions on Pattern Anaylsis and Machine Intelligence,
32(4):678?692.
Roberto Navigli. 2009a. Using cycles and quasi-
cycles to disambiguate dictionary glosses. In Proc.
of EACL-09, pages 594?602.
Roberto Navigli. 2009b. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1?69.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In Proc. of
ACL-96, pages 40?47.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: Developing an aligned
multilingual database. In Proc. of GWC-02, pages
21?25.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating Wikipedia. In Proc. of IJCAI-09,
pages 2083?2088.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rival-
ing supervised system. In Proc. of ACL-10.
Simone Paolo Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from Wikipedia. In
Proc. of AAAI-07, pages 1440?1445.
Nils Reiter, Matthias Hartung, and Anette Frank.
2008. A resource-poor approach for linking ontol-
ogy classes to Wikipedia articles. In Johan Bos and
Rodolfo Delmonte, editors, Semantics in Text Pro-
cessing, volume 1 of Research in Computational Se-
mantics, pages 381?387. College Publications, Lon-
don, England.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Ad-
vances in Web Intelligence, volume 3528 of Lecture
Notes in Computer Science. Springer Verlag.
Marcus Sammer and Stephen Soderland. 2007. Build-
ing a sense-distinguished multilingual lexicon from
monolingual corpora and bilingual lexicons. In Pro-
ceedings of Machine Translation Summit XI.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. In Proc. of COLING-ACL-06, pages 801?
808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203?217.
Dan Tufis?, Dan Cristea, and Sofia Stamou. 2004.
BalkaNet: Aims, methods, results and perspectives.
a general overview. Romanian Journal on Science
and Technology of Information, 7(1-2):9?43.
Luis von Ahn. 2006. Games with a purpose. IEEE
Computer, 6(39):92?94.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Kluwer, Dordrecht, The Netherlands.
Fei Wu and Daniel Weld. 2007. Automatically se-
mantifying Wikipedia. In Proc. of CIKM-07, pages
41?50.
David Yarowsky and Radu Florian. 2002. Evaluat-
ing sense disambiguation across diverse parameter
spaces. Natural Language Engineering, 9(4):293?
310.
Toshio Yokoi. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42?44.
225
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522?1531,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems
Simone Paolo Ponzetto
Department of Computational Linguistics
Heidelberg University
ponzetto@cl.uni-heidelberg.de
Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
navigli@di.uniroma1.it
Abstract
One of the main obstacles to high-
performance Word Sense Disambigua-
tion (WSD) is the knowledge acquisi-
tion bottleneck. In this paper, we present
a methodology to automatically extend
WordNet with large amounts of seman-
tic relations from an encyclopedic re-
source, namely Wikipedia. We show
that, when provided with a vast amount
of high-quality semantic relations, sim-
ple knowledge-lean disambiguation algo-
rithms compete with state-of-the-art su-
pervisedWSD systems in a coarse-grained
all-words setting and outperform them on
gold-standard domain-specific datasets.
1 Introduction
Knowledge lies at the core of Word Sense Dis-
ambiguation (WSD), the task of computation-
ally identifying the meanings of words in context
(Navigli, 2009b). In the recent years, two main
approaches have been studied that rely on a fixed
sense inventory, i.e., supervised and knowledge-
based methods. In order to achieve high perfor-
mance, supervised approaches require large train-
ing sets where instances (target words in con-
text) are hand-annotated with the most appropri-
ate word senses. Producing this kind of knowl-
edge is extremely costly: at a throughput of one
sense annotation per minute (Edmonds, 2000)
and tagging one thousand examples per word,
dozens of person-years would be required for en-
abling a supervised classifier to disambiguate all
the words in the English lexicon with high accu-
racy. In contrast, knowledge-based approaches ex-
ploit the information contained in wide-coverage
lexical resources, such as WordNet (Fellbaum,
1998). However, it has been demonstrated that
the amount of lexical and semantic information
contained in such resources is typically insuffi-
cient for high-performance WSD (Cuadros and
Rigau, 2006). Several methods have been pro-
posed to automatically extend existing resources
(cf. Section 2) and it has been shown that highly-
interconnected semantic networks have a great im-
pact on WSD (Navigli and Lapata, 2010). How-
ever, to date, the real potential of knowledge-rich
WSD systems has been shown only in the presence
of either a large manually-developed extension of
WordNet (Navigli and Velardi, 2005) or sophisti-
cated WSD algorithms (Agirre et al, 2009).
The contributions of this paper are two-fold.
First, we relieve the knowledge acquisition bot-
tleneck by developing a methodology to extend
WordNet with millions of semantic relations. The
relations are harvested from an encyclopedic re-
source, namely Wikipedia. Wikipedia pages are
automatically associated with WordNet senses,
and topical, semantic associative relations from
Wikipedia are transferred to WordNet, thus pro-
ducing a much richer lexical resource. Sec-
ond, two simple knowledge-based algorithms that
exploit our extended WordNet are applied to
standard WSD datasets. The results show that
the integration of vast amounts of semantic re-
lations in knowledge-based systems yields per-
formance competitive with state-of-the-art super-
vised approaches on open-text WSD. In addition,
we support previous findings from Agirre et al
(2009) that in a domain-specific WSD scenario
knowledge-based systems perform better than su-
pervised ones, and we show that, given enough
knowledge, simple algorithms perform better than
more sophisticated ones.
2 Related Work
In the last three decades, a large body of work
has been presented that concerns the develop-
ment of automatic methods for the enrichment of
existing resources such as WordNet. These in-
1522
clude proposals to extract semantic information
from dictionaries (e.g. Chodorow et al (1985)
and Rigau et al (1998)), approaches using lexico-
syntactic patterns (Hearst, 1992; Cimiano et al,
2004; Girju et al, 2006), heuristic methods based
on lexical and semantic regularities (Harabagiu et
al., 1999), taxonomy-based ontologization (Pen-
nacchiotti and Pantel, 2006; Snow et al, 2006).
Other approaches include the extraction of seman-
tic preferences from sense-annotated (Agirre and
Martinez, 2001) and raw corpora (McCarthy and
Carroll, 2003), as well as the disambiguation of
dictionary glosses based on cyclic graph patterns
(Navigli, 2009a). Other works rely on the dis-
ambiguation of collocations, either obtained from
specialized learner?s dictionaries (Navigli and Ve-
lardi, 2005) or extracted by means of statistical
techniques (Cuadros and Rigau, 2008), e.g. based
on the method proposed by Agirre and de Lacalle
(2004). But while most of these methods represent
state-of-the-art proposals for enriching lexical and
taxonomic resources, none concentrates on aug-
menting WordNet with associative semantic rela-
tions for many domains on a very large scale. To
overcome this limitation, we exploit Wikipedia, a
collaboratively generated Web encyclopedia.
The use of collaborative contributions from vol-
unteers has been previously shown to be beneficial
in the Open Mind Word Expert project (Chklovski
and Mihalcea, 2002). However, its current status
indicates that the project remains a mainly aca-
demic attempt. In contrast, due to its low en-
trance barrier and vast user base, Wikipedia pro-
vides large amounts of information at practically
no cost. Previous work aimed at transforming
its content into a knowledge base includes open-
domain relation extraction (Wu and Weld, 2007),
the acquisition of taxonomic (Ponzetto and Strube,
2007a; Suchanek et al, 2008; Wu andWeld, 2008)
and other semantic relations (Nastase and Strube,
2008), as well as lexical reference rules (Shnarch
et al, 2009). Applications using the knowledge
contained in Wikipedia include, among others,
text categorization (Gabrilovich and Markovitch,
2006), computing semantic similarity of texts
(Gabrilovich and Markovitch, 2007; Ponzetto and
Strube, 2007b; Milne and Witten, 2008a), coref-
erence resolution (Ponzetto and Strube, 2007b),
multi-document summarization (Nastase, 2008),
and text generation (Sauper and Barzilay, 2009).
In our work we follow this line of research and
show that knowledge harvested from Wikipedia
can be used effectively to improve the perfor-
mance of a WSD system. Our proposal builds on
previous insights from Bunescu and Pas?ca (2006)
and Mihalcea (2007) that pages in Wikipedia can
be taken as word senses. Mihalcea (2007) manu-
ally maps Wikipedia pages to WordNet senses to
perform lexical-sample WSD. We extend her pro-
posal in three important ways: (1) we fully autom-
atize the mapping between Wikipedia pages and
WordNet senses; (2) we use the mappings to en-
rich an existing resource, i.e. WordNet, rather than
annotating text with sense labels; (3) we deploy
the knowledge encoded by this mapping to per-
form unrestricted WSD, rather than apply it to a
lexical sample setting.
Knowledge from Wikipedia is injected into a
WSD system by means of a mapping to Word-
Net. Previous efforts aimed at automatically link-
ing Wikipedia to WordNet include full use of the
first WordNet sense heuristic (Suchanek et al,
2008), a graph-based mapping of Wikipedia cat-
egories to WordNet synsets (Ponzetto and Nav-
igli, 2009), a model based on vector spaces (Ruiz-
Casado et al, 2005) and a supervised approach
using keyword extraction (Reiter et al, 2008).
These latter methods rely only on text overlap
techniques and neither they take advantage of the
input from Wikipedia being semi-structured, e.g.
hyperlinked, nor they propose a high-performing
probabilistic formulation of the mapping problem,
a task to which we turn in the next section.
3 Extending WordNet
Our approach consists of two main phases: first,
a mapping is automatically established between
Wikipedia pages and WordNet senses; second, the
relations connecting Wikipedia pages are trans-
ferred to WordNet. As a result, an extended ver-
sion of WordNet is produced, that we call Word-
Net++. We present the two resources used in our
methodology in Section 3.1. Sections 3.2 and 3.3
illustrate the two phases of our approach.
3.1 Knowledge Resources
WordNet. Being the most widely used compu-
tational lexicon of English in Natural Language
Processing, WordNet is an essential resource for
WSD. A concept in WordNet is represented as a
synonym set, or synset, i.e. the set of words which
share a common meaning. For instance, the con-
1523
cept of soda drink is expressed as:
{ pop2n, soda2n, soda pop1n, soda water2n, tonic2n }
where each word?s subscripts and superscripts in-
dicate their parts of speech (e.g. n stands for noun)
and sense number1, respectively. For each synset,
WordNet provides a textual definition, or gloss.
For example, the gloss of the above synset is: ?a
sweet drink containing carbonated water and fla-
voring?.
Wikipedia. Our second resource, Wikipedia, is
a collaborative Web encyclopedia composed of
pages2. A Wikipedia page (henceforth, Wikipage)
presents the knowledge about a specific concept
(e.g. SODA (SOFT DRINK)) or named entity (e.g.
FOOD STANDARDS AGENCY). The page typi-
cally contains hypertext linked to other relevant
Wikipages. For instance, SODA (SOFT DRINK)
is linked to COLA, FLAVORED WATER, LEMON-
ADE, and many others. The title of a Wikipage
(e.g. SODA (SOFT DRINK)) is composed of the
lemma of the concept defined (e.g. soda) plus
an optional label in parentheses which specifies
its meaning in case the lemma is ambiguous
(e.g. SOFT DRINK vs. SODIUM CARBONATE). Fi-
nally, some Wikipages are redirections to other
pages, e.g. SODA (SODIUM CARBONATE) redirects
to SODIUM CARBONATE.
3.2 Mapping Wikipedia to WordNet
During the first phase of our methodology we aim
to establish links between Wikipages and Word-
Net senses. Formally, given the entire set of pages
SensesWiki and WordNet senses SensesWN, we aim
to acquire a mapping:
? : SensesWiki ? SensesWN,
such that, for each Wikipage w ? SensesWiki:
?(w) =
?
??
??
s ? SensesWN(w) if a link can be
established,
 otherwise,
where SensesWN(w) is the set of senses of the
lemma of w in WordNet. For example, if our
1We use WordNet version 3.0. We use word senses to un-
ambiguously denote the corresponding synsets (e.g. plane1n
for { airplane1n, aeroplane1n, plane1n }).
2http://download.wikipedia.org. We use the
English Wikipedia database dump from November 3, 2009,
which includes 3,083,466 articles. Throughout this paper, we
use Sans Serif for words, SMALL CAPS for Wikipedia pages
and CAPITALS for Wikipedia categories.
mapping methodology linked SODA (SOFT DRINK)
to the corresponding WordNet sense soda2n, we
would have ?(SODA (SOFT DRINK)) = soda2n.
In order to establish a mapping between the
two resources, we first identify different kinds of
disambiguation contexts for Wikipages (Section
3.2.1) and WordNet senses (Section 3.2.2). Next,
we intersect these contexts to perform the mapping
(see Section 3.2.3).
3.2.1 Disambiguation Context of a Wikipage
Given a target Wikipage w which we aim to map
to a WordNet sense of w, we use the following
information as a disambiguation context:
? Sense labels: e.g. given the page SODA (SOFT
DRINK), the words soft and drink are added to
the disambiguation context.
? Links: the titles? lemmas of the pages linked
from the Wikipage w (outgoing links). For in-
stance, the links in the Wikipage SODA (SOFT
DRINK) include soda, lemonade, sugar, etc.
? Categories: Wikipages are classified accord-
ing to one or more categories, which repre-
sent meta-information used to categorize them.
For instance, the Wikipage SODA (SOFT DRINK)
is categorized as SOFT DRINKS. Since many
categories are very specific and do not appear in
WordNet (e.g., SWEDISH WRITERS or SCI-
ENTISTS WHO COMMITTED SUICIDE),
we use the lemmas of their syntactic heads as
disambiguation context (i.e. writer and scien-
tist). To this end, we use the category heads
provided by Ponzetto and Navigli (2009).
Given a Wikipage w, we define its disambiguation
context Ctx(w) as the set of words obtained from
some or all of the three sources above.
3.2.2 Disambiguation Context of a WordNet
Sense
Given a WordNet sense s and its synset S, we use
the following information as disambiguation con-
text to provide evidence for a potential link in our
mapping ?:
? Synonymy: all synonyms of s in synset S. For
instance, given the synset of soda2n, all its syn-
onyms are included in the context (that is, tonic,
soda pop, pop, etc.).
1524
? Hypernymy/Hyponymy: all synonyms in the
synsets H such that H is either a hypernym
(i.e., a generalization) or a hyponym (i.e., a spe-
cialization) of S. For example, given soda2n,
we include the words from its hypernym { soft
drink1n }.
? Sisterhood: words from the sisters of S. A sister
synset S? is such that S and S? have a common
direct hypernym. For example, given soda2n, it
can be found that bitter lemon1n and soda2n are
sisters. Thus the words bitter and lemon are in-
cluded in the disambiguation context of s.
? Gloss: the set of lemmas of the content words
occurring within the gloss of s. For instance,
given s = soda2n, defined as ?a sweet drink
containing carbonated water and flavoring?, we
add to the disambiguation context of s the fol-
lowing lemmas: sweet, drink, contain, carbon-
ated, water, flavoring.
Given a WordNet sense s, we define its disam-
biguation context Ctx(s) as the set of words ob-
tained from some or all of the four sources above.
3.2.3 Mapping Algorithm
In order to link each Wikipedia page to a Word-
Net sense, we developed a novel algorithm, whose
pseudocode is shown in Algorithm 1. The follow-
ing steps are performed:
? Initially (lines 1-2), our mapping ? is empty, i.e.
it links each Wikipage w to .
? For each Wikipage w whose lemma is monose-
mous both in Wikipedia and WordNet (i.e.
|SensesWiki(w)| = |SensesWN(w)| = 1) we map
w to its only WordNet sense w1n (lines 3-5).
? Finally, for each remaining Wikipage w for
which no mapping was previously found (i.e.,
?(w) = , line 7), we do the following:
? lines 8-10: for each Wikipage d which is a
redirection to w, for which a mapping was
previously found (i.e. ?(d) 6= , that is, d is
monosemous in both Wikipedia and Word-
Net) and such that it maps to a sense ?(d) in
a synset S that also contains a sense of w, we
map w to the corresponding sense in S.
? lines 11-14: if a Wikipage w has not been
linked yet, we assign the most likely sense
to w based on the maximization of the con-
ditional probabilities p(s|w) over the senses
Algorithm 1 The mapping algorithm
Input: SensesWiki, SensesWN
Output: a mapping ? : SensesWiki ? SensesWN
1: for each w ? SensesWiki
2: ?(w) := 
3: for each w ? SensesWiki
4: if |SensesWiki(w)| = |SensesWN(w)| = 1 then
5: ?(w) := w1n
6: for each w ? SensesWiki
7: if ?(w) =  then
8: for each d ? SensesWiki s.t. d redirects to w
9: if ?(d) 6=  and ?(d) is in a synset of w then
10: ?(w) := sense ofw in synset of ?(d); break
11: for each w ? SensesWiki
12: if ?(w) =  then
13: if no tie occurs then
14: ?(w) := argmax
s?SensesWN(w)
p(s|w)
15: return ?
s ? SensesWN(w) (no mapping is established
if a tie occurs, line 13).
As a result of the execution of the algorithm, the
mapping ? is returned (line 15). At the heart of the
mapping algorithm lies the calculation of the con-
ditional probability p(s|w) of selecting the Word-
Net sense s given the Wikipage w. The sense s
which maximizes this probability can be obtained
as follows:
?(w) = argmax
s?SensesWN(w)
p(s|w) = argmax
s
p(s, w)
p(w)
= argmax
s
p(s, w)
The latter formula is obtained by observing that
p(w) does not influence our maximization, as it is
a constant independent of s. As a result, the most
appropriate sense s is determined by maximizing
the joint probability p(s, w) of sense s and page w.
We estimate p(s, w) as:
p(s, w) =
score(s, w)
?
s??SensesWN(w),
w??SensesWiki(w)
score(s?, w?)
,
where score(s, w) = |Ctx(s)?Ctx(w)|+1 (we add
1 as a smoothing factor). Thus, in our algorithm
we determine the best sense s by computing the in-
tersection of the disambiguation contexts of s and
w, and normalizing by the scores summed over all
senses of w in Wikipedia and WordNet.
3.2.4 Example
We illustrate the execution of our mapping algo-
rithm by way of an example. Let us focus on the
1525
Wikipage SODA (SOFT DRINK). The word soda
is polysemous both in Wikipedia and WordNet,
thus lines 3?5 of the algorithm do not concern
this Wikipage. Lines 6?14 aim to find a mapping
?(SODA (SOFT DRINK)) to an appropriateWordNet
sense of the word. First, we check whether a redi-
rection exists to SODA (SOFT DRINK) that was pre-
viously disambiguated (lines 8?10). Next, we con-
struct the disambiguation context for the Wikipage
by including words from its label, links and cate-
gories (cf. Section 3.2.1). The context includes,
among others, the following words: soft, drink,
cola, sugar. We now construct the disambiguation
context for the two WordNet senses of soda (cf.
Section 3.2.2), namely the sodium carbonate (#1)
and the drink (#2) senses. To do so, we include
words from their synsets, hypernyms, hyponyms,
sisters, and glosses. The context for soda1n in-
cludes: salt, acetate, chlorate, benzoate. The
context for soda2n contains instead: soft, drink,
cola, bitter, etc. The sense with the largest inter-
section is #2, so the following mapping is estab-
lished: ?(SODA (SOFT DRINK)) = soda2n.
3.3 Transferring Semantic Relations
The output of the algorithm presented in the previ-
ous section is a mapping between Wikipages and
WordNet senses (that is, implicitly, synsets). Our
insight is to use this alignment to enable the trans-
fer of semantic relations from Wikipedia to Word-
Net. In fact, given a Wikipage w we can collect
all Wikipedia links occurring in that page. For
any such link from w to w?, if the two Wikipages
are mapped to WordNet senses (i.e., ?(w) 6= 
and ?(w?) 6= ), we can transfer the correspond-
ing edge (?(w), ?(w?)) to WordNet. Note that ?(w)
and ?(w?) are noun senses, as Wikipages describe
nominal concepts or named entities. We refer to
this extended resource as WordNet++.
For instance, consider the Wikipage SODA
(SOFT DRINK). This page contains, among oth-
ers, a link to the Wikipage SYRUP. Assuming
?(SODA (SODA DRINK)) = soda2n and ?(SYRUP) =
syrup1n, we can add the corresponding semantic
relation (soda2n, syrup1n) to WordNet3.
Thus, WordNet++ represents an extension of
WordNet which includes semantic associative re-
lations between synsets. These are originally
3Note that such relations are unlabeled. However, for our
purposes this has no impact, since our algorithms do not dis-
tinguish between is-a and other kinds of relations in the lexi-
cal knowledge base (cf. Section 4.2).
found in Wikipedia and then integrated into Word-
Net by means of our mapping. In turn, Word-
Net++ represents the English-only subset of a
larger multilingual resource, BabelNet (Navigli
and Ponzetto, 2010), where lexicalizations of the
synsets are harvested for many languages using
the so-called Wikipedia inter-language links and
applying a machine translation system.
4 Experiments
We perform two sets of experiments: we first eval-
uate the intrinsic quality of our mapping (Section
4.1) and then quantify the impact of WordNet++
for coarse-grained (Section 4.2) and domain-
specific WSD (Section 4.3).
4.1 Evaluation of the Mapping
Experimental setting. We first conducted an
evaluation of the mapping quality. To create
a gold standard for evaluation, we started from
the set of all lemmas contained both in Word-
Net and Wikipedia: the intersection between the
two resources includes 80,295 lemmas which cor-
respond to 105,797 WordNet senses and 199,735
Wikipedia pages. The average polysemy is 1.3 and
2.5 for WordNet senses and Wikipages, respec-
tively (2.8 and 4.7 when excluding monosemous
words). We selected a random sample of 1,000
Wikipages and asked an annotator with previous
experience in lexicographic annotation to provide
the correct WordNet sense for each page title (an
empty sense label was given if no correct mapping
was possible). 505 non-empty mappings were
found, i.e. Wikipedia pages with a corresponding
WordNet sense. In order to quantify the quality
of the annotations and the difficulty of the task,
a second annotator sense tagged a subset of 200
pages from the original sample. We computed the
inter-annotator agreement using the kappa coeffi-
cient (Carletta, 1996) and found out that our anno-
tators achieved an agreement coefficient ? of 0.9,
indicating almost perfect agreement.
Table 1 summarizes the performance of our dis-
ambiguation algorithm against the manually anno-
tated dataset. Evaluation is performed in terms of
standard measures of precision (the ratio of cor-
rect sense labels to the non-empty labels output
by the mapping algorithm), recall (the ratio of
correct sense labels to the total of non-empty la-
bels in the gold standard) and F1-measure ( 2PRP+R ).
We also calculate accuracy, which accounts for
1526
P R F1 A
Structure 82.2 68.1 74.5 81.1
Gloss 81.1 64.2 71.7 78.8
Structure + Gloss 81.9 77.5 79.6 84.4
MFS BL 24.3 47.8 32.2 24.3
Random BL 23.8 46.8 31.6 23.9
Table 1: Performance of the mapping algorithm.
empty sense labels (that is, calculated on all 1,000
test instances). As baseline we use the most fre-
quent WordNet sense (MFS), as well as a ran-
dom sense assignment. We evaluate the map-
ping methodology described in Section 3.2 against
different disambiguation contexts for the Word-
Net senses (cf. Section 3.2.2), i.e. structure-based
(including synonymy, hypernymy/hyponymy and
sisterhood), gloss-derived evidence, and a combi-
nation of the two. As disambiguation context of
a Wikipage (Section 3.2.1) we use all information
available, i.e. sense labels, links and categories4.
Results and discussion. The results show that
our method improves on the baseline by a large
margin and that higher performance can be
achieved by using more disambiguation informa-
tion. That is, using a richer disambiguation con-
text helps to better choose the most appropriate
WordNet sense for a Wikipedia page. The combi-
nation of structural and gloss information attains a
slight variation in terms of precision (?0.3% and
+0.8% compared to Structure and Gloss respec-
tively), but a significantly high increase in recall
(+9.4% and +13.3%). This implies that the differ-
ent disambiguation contexts only partially overlap
and, when used separately, each produces differ-
ent mappings with a similar level of precision. In
the joint approach, the harmonic mean of preci-
sion and recall, i.e. F1, is in fact 5 and 8 points
higher than when separately using structural and
gloss information, respectively.
As for the baselines, the most frequent sense is
just 0.6% and 0.4% above the random baseline in
terms of F1 and accuracy, respectively. A ?2 test
reveals in fact no statistically significant difference
at p < 0.05. This is related to the random distri-
bution of senses in our dataset and the Wikipedia
unbiased coverage of WordNet senses. So select-
4We leave out the evaluation of different contexts for a
Wikipage for the sake of brevity. During prototyping we
found that the best results were given by using the largest
context available, as reported in Table 1.
ing the most frequent sense rather than any other
sense for each target page represents a choice as
arbitrary as picking a sense at random.
The final mapping contains 81,533 pairs of
Wikipages and word senses they map to, covering
55.7% of the noun senses in WordNet.
Using our best performing mapping we are
able to extend WordNet with 1,902,859 semantic
edges: of these, 97.93% are deemed novel, i.e. no
direct edge could previously be found between the
synsets. In addition, we performed a stricter eval-
uation of the novelty of our relations by check-
ing whether these can still be found indirectly by
searching for a connecting path between the two
synsets of interest. Here we found that 91.3%,
87.2% and 78.9% of the relations are novel to
WordNet when performing a graph search of max-
imum depth of 2, 3 and 4, respectively.
4.2 Coarse-grained WSD
Experimental setting. We extrinsically evalu-
ate the impact of WordNet++ on the Semeval-
2007 coarse-grained all-words WSD task (Nav-
igli et al, 2007). Performing experiments in a
coarse-grained setting is a natural choice for sev-
eral reasons: first, it has been argued that the fine
granularity of WordNet is one of the main obsta-
cles to accurate WSD (cf. the discussion in Nav-
igli (2009b)); second, the meanings of Wikipedia
pages are intuitively coarser than those in Word-
Net5. For instance, mapping TRAVEL to the first
or the second sense in WordNet is an arbitrary
choice, as the Wikipage refers to both senses. Fi-
nally, given their different nature, WordNet and
Wikipedia do not fully overlap. Accordingly,
we expect the transfer of semantic relations from
Wikipedia to WordNet to have sometimes the side
effect to penalize some fine-grained senses of a
word.
We experiment with two simple knowledge-
based algorithms that are set to perform coarse-
grained WSD on a sentence-by-sentence basis:
? Simplified Extended Lesk (ExtLesk): The first
algorithm is a simplified version of the Lesk
5Note that our polysemy rates from Section 4.1 also in-
clude Wikipages whose lemma is contained in WordNet, but
which have out-of-domain meanings, i.e. encyclopedic en-
tries referring to specialized named entities such as e.g., DIS-
COVERY (SPACE SHUTTLE) or FIELD ARTILLERY (MAGA-
ZINE). We computed the polysemy rate for a random sample
of 20 polysemous words by manually removing these NEs
and found that Wikipedia?s polysemy rate is indeed lower
than that of WordNet ? i.e. average polysemy of 2.1 vs. 2.8.
1527
algorithm (Lesk, 1986), that performs WSD
based on the overlap between the context sur-
rounding the target word to be disambiguated
and the definitions of its candidate senses (Kil-
garriff and Rosenzweig, 2000). Given a tar-
get word w, this method assigns to w the
sense whose gloss has the highest overlap (i.e.
most words in common) with the context of w,
namely the set of content words co-occurring
with it in a pre-defined window (a sentence in
our case). Due to the limited context provided
by the WordNet glosses, we follow Banerjee
and Pedersen (2003) and expand the gloss of
each sense s to include words from the glosses
of those synsets in a semantic relation with s.
These include all WordNet synsets which are
directly connected to s, either by means of the
semantic pointers found in WordNet or through
the unlabeled links found in WordNet++.
? Degree Centrality (Degree): The second algo-
rithm is a graph-based approach that relies on
the notion of vertex degree (Navigli and Lap-
ata, 2010). Starting from each sense s of the tar-
get word, it performs a depth-first search (DFS)
of the WordNet(++) graph and collects all the
paths connecting s to senses of other words in
context. As a result, a sentence graph is pro-
duced. A maximum search depth is established
to limit the size of this graph. The sense of the
target word with the highest vertex degree is se-
lected. We follow Navigli and Lapata (2010)
and run Degree in a weakly supervised setting
where the system attempts no sense assignment
if the highest degree score is below a certain
(empirically estimated) threshold. The optimal
threshold and maximum search depth are es-
timated by maximizing Degree?s F1 on a de-
velopment set of 1,000 randomly chosen noun
instances from the SemCor corpus (Miller et
al., 1993). Experiments on the development
dataset using Degree on WordNet++ revealed
a performance far lower than expected. Error
analysis showed that many instances were in-
correctly disambiguated, due to the noise from
weak semantic links, e.g. the links from SODA
(SOFT DRINK) to EUROPE or AUSTRALIA. Ac-
cordingly, in order to improve the disambigua-
tion performance, we developed a filter to rule
out weak semantic relations from WordNet++.
Given a WordNet++ edge (?(w), ?(w?)) where
w and w? are both Wikipages and w links to w?,
Resource Algorithm
Nouns only
P R F1
WordNet
ExtLesk 83.6 57.7 68.3
Degree 86.3 65.5 74.5
Wikipedia
ExtLesk 82.3 64.1 72.0
Degree 96.2 40.1 57.4
WordNet++
ExtLesk 82.7 69.2 75.4
Degree 87.3 72.7 79.4
MFS BL 77.4 77.4 77.4
Random BL 63.5 63.5 63.5
Table 2: Performance on Semeval-2007 coarse-
grained all-words WSD (nouns only subset).
we first collect all words from the category la-
bels of w and w? into two bags of words. We re-
move stopwords and lemmatize the remaining
words. We then compute the degree of overlap
between the two sets of categories as the num-
ber of words in common between the two bags
of words, normalized in the [0, 1] interval. We fi-
nally retain the link for the DFS if such score is
above an empirically determined threshold. The
optimal value for this category overlap thresh-
old was again estimated by maximizing De-
gree?s F1 on the development set. The final
graph used by Degree consists of WordNet, to-
gether with 152,944 relations from our semantic
relation enrichment method (cf. Section 3.3).
Results and discussion. We report our results in
terms of precision, recall and F1-measure on the
Semeval-2007 coarse-grained all-words dataset
(Navigli et al, 2007). We first evaluated ExtLesk
and Degree using three different resources: (1)
WordNet only; (2) Wikipedia only, i.e. only those
relations harvested from the links found within
Wikipedia pages; (3) their union, i.e. WordNet++.
In Table 2 we report the results on nouns only. As
common practice, we compare with random sense
assignment and the most frequent sense (MFS)
from SemCor as baselines. Enriching WordNet
with encyclopedic relations from Wikipedia yields
a consistent improvement against using WordNet
(+7.1% and +4.9% F1 for ExtLesk and Degree)
or Wikipedia (+3.4% and +22.0%) alone. The
best results are obtained by using Degree with
WordNet++. The better performance of Wikipedia
against WordNet when using ExtLesk (+3.7%)
highlights the quality of the relations extracted.
However, no such improvement is found with De-
1528
Algorithm
Nouns only All words
P/R/F1 P/R/F1
ExtLesk 81.0 79.1
Degree 85.5 81.7
SUSSX-FR 81.1 77.0
TreeMatch N/A 73.6
NUS-PT 82.3 82.5
SSI 84.1 83.2
MFS BL 77.4 78.9
Random BL 63.5 62.7
Table 3: Performance on Semeval-2007 coarse-
grained all-words WSD with MFS as a back-off
strategy when no sense assignment is attempted.
gree, due to its lower recall. Interestingly, Degree
on WordNet++ beats the MFS baseline, which is
notably a difficult competitor for unsupervised and
knowledge-lean systems.
We finally compare our two algorithms using
WordNet++ with state-of-the-art WSD systems,
namely the best unsupervised (Koeling and Mc-
Carthy, 2007, SUSSX-FR) and supervised (Chan
et al, 2007, NUS-PT) systems participating in
the Semeval-2007 coarse-grained all-words task.
We also compare with SSI (Navigli and Velardi,
2005) ? a knowledge-based system that partici-
pated out of competition ? and the unsupervised
proposal from Chen et al (2009, TreeMatch). Ta-
ble 3 shows the results for nouns (1,108) and
all words (2,269 words): we use the MFS as a
back-off strategy when no sense assignment is at-
tempted. Degree with WordNet++ achieves the
best performance in the literature6. On the noun-
only subset of the data, its performance is com-
parable with SSI and significantly better than the
best supervised and unsupervised systems (+3.2%
and +4.4% F1 against NUS-PT and SUSSX-FR).
On the entire dataset, it outperforms SUSSX-FR
and TreeMatch (+4.7% and +8.1%) and its re-
call is not statistically different from that of SSI
and NUS-PT. This result is particularly interest-
ing, given that WordNet++ is extended only with
relations between nominals, and, in contrast to
SSI, it does not rely on a costly annotation effort
to engineer the set of semantic relations. Last but
not least, we achieve state-of-the-art performance
with a much simpler algorithm that is based on the
notion of vertex degree in a graph.
6The differences between the results in bold in each col-
umn of the table are not statistically significant at p < 0.05.
Algorithm
Sports Finance
P/R/F1 P/R/F1
k-NN ? 30.3 43.4
Static PR ? 20.1 39.6
Personalized PR ? 35.6 46.9
ExtLesk 40.1 45.6
Degree 42.0 47.8
MFS BL 19.6 37.1
Random BL 19.5 19.6
Table 4: Performance on the Sports and Finance
sections of the dataset from Koeling et al (2005):
? indicates results from Agirre et al (2009).
4.3 Domain WSD
The main strength of Wikipedia is to provide wide
coverage for many specific domains. Accord-
ingly, on the Semeval dataset our system achieves
the best performance on a domain-specific text,
namely d004, a document on computer science
where we achieve 82.9% F1 (+6.8% when com-
pared with the best supervised system, namely
NUS-PT). To test whether our performance on the
Semeval dataset is an artifact of the data, i.e. d004
coming from Wikipedia itself, we evaluated our
system on the Sports and Finance sections of the
domain corpora from Koeling et al (2005). In Ta-
ble 4 we report our results on these datasets and
compare them with Personalized PageRank, the
state-of-the-art system from Agirre et al (2009)7,
as well as Static PageRank and a k-NN supervised
WSD system trained on SemCor.
The results we obtain on the two domains with
our best configuration (Degree using WordNet++)
outperform by a large margin k-NN, thus sup-
porting the findings from Agirre et al (2009)
that knowledge-based systems exhibit a more ro-
bust performance than their supervised alterna-
tives when evaluated across different domains. In
addition, our system achieves better results than
Static and Personalized PageRank, indicating that
competitive disambiguation performance can still
be achieved by a less sophisticated knowledge-
based WSD algorithm when provided with a rich
amount of high-quality knowledge. Finally, the
results show that WordNet++ enables competitive
performance also in a fine-grained domain setting.
7We compare only with those system configurations per-
forming token-based WSD, i.e. disambiguating each instance
of a target word separately, since our aim is not to perform
type-based disambiguation.
1529
5 Conclusions
In this paper, we have presented a large-scale
method for the automatic enrichment of a com-
putational lexicon with encyclopedic relational
knowledge8. Our experiments show that the large
amount of knowledge injected into WordNet is of
high quality and, more importantly, it enables sim-
ple knowledge-based WSD systems to perform as
well as the highest-performing supervised ones in
a coarse-grained setting and to outperform them
on domain-specific text. Thus, our results go
one step beyond previous findings (Cuadros and
Rigau, 2006; Agirre et al, 2009; Navigli and La-
pata, 2010) and prove that knowledge-rich dis-
ambiguation is a competitive alternative to super-
vised systems, even when relying on a simple al-
gorithm. We note, however, that the present con-
tribution does not show which knowledge-rich al-
gorithm performs best with WordNet++. In fact,
more sophisticated approaches, such as Personal-
ized PageRank (Agirre and Soroa, 2009), could be
still applied to yield even higher performance. We
leave such exploration to future work. Moreover,
while the mapping has been used to enrich Word-
Net with a large amount of semantic edges, the
method can be reversed and applied to the ency-
clopedic resource itself, that is Wikipedia, to per-
form disambiguation with the corresponding sense
inventory (cf. the task of wikification proposed
by Mihalcea and Csomai (2007) and Milne and
Witten (2008b)). In this paper, we focused on
English Word Sense Disambiguation. However,
since WordNet++ is part of a multilingual seman-
tic network (Navigli and Ponzetto, 2010), we plan
to explore the impact of this knowledge in a mul-
tilingual setting.
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proc. of LREC ?04.
Eneko Agirre and David Martinez. 2001. Learning
class-to-class selectional preferences. In Proceed-
ings of CoNLL-01, pages 15?22.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank forWord Sense Disambiguation. In Proc.
of EACL-09, pages 33?41.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
8The resulting resource, WordNet++, is freely available at
http://lcl.uniroma1.it/wordnetplusplus for
research purposes.
performing better than generic supervised WSD. In
Proc. of IJCAI-09, pages 1501?1506.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlap as a measure of semantic relatedness.
In Proc. of IJCAI-03, pages 805?810.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proc. of EACL-06, pages 9?16.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
NUS-ML: Exploiting parallel texts for Word Sense
Disambiguation in the English all-words tasks. In
Proc. of SemEval-2007, pages 253?256.
Ping Chen, Wei Ding, Chris Bowes, and David Brown.
2009. A fully unsupervised Word Sense Disam-
biguation method using dependency knowledge. In
Proc. of NAACL-HLT-09, pages 28?36.
Tim Chklovski and Rada Mihalcea. 2002. Building a
sense tagged corpus with Open Mind Word Expert.
In Proceedings of the ACL-02 Workshop on WSD:
Recent Successes and Future Directions at ACL-02.
Martin Chodorow, Roy Byrd, and George E. Heidorn.
1985. Extracting semantic hierarchies from a large
on-line dictionary. In Proc. of ACL-85, pages 299?
304.
Philipp Cimiano, Siegfried Handschuh, and Steffen
Staab. 2004. Towards the self-annotating Web. In
Proc. of WWW-04, pages 462?471.
Montse Cuadros and German Rigau. 2006. Quality
assessment of large scale knowledge resources. In
Proc. of EMNLP-06, pages 534?541.
Montse Cuadros and German Rigau. 2008. KnowNet:
building a large net of knowledge from the Web. In
Proc. of COLING-08, pages 161?168.
Philip Edmonds. 2000. Designing a task for
SENSEVAL-2. Technical report, University of
Brighton, U.K.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
Wikipedia: Enhancing text categorization with
encyclopedic knowledge. In Proc. of AAAI-06,
pages 1301?1306.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proc. of IJCAI-
07, pages 1606?1611.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83?135.
Sanda M. Harabagiu, George A. Miller, and Dan I.
Moldovan. 1999. WordNet 2 ? a morphologically
and semantically enhanced resource. In Proceed-
ings of the SIGLEX99 Workshop on Standardizing
Lexical Resources, pages 1?8.
1530
Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In Proc. of
COLING-92, pages 539?545.
Adam Kilgarriff and Joseph Rosenzweig. 2000.
Framework and results for English SENSEVAL.
Computers and the Humanities, 34(1-2).
Rob Koeling and Diana McCarthy. 2007. Sussx: WSD
using automatically acquired predominant senses.
In Proc. of SemEval-2007, pages 314?317.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proc. of HLT-
EMNLP-05, pages 419?426.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the 5th Annual Conference on Systems Documen-
tation, Toronto, Ontario, Canada, pages 24?26.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Rada Mihalcea and Andras Csomai. 2007. Wikify!
Linking documents to encyclopedic knowledge. In
Proc. of CIKM-07, pages 233?242.
Rada Mihalcea. 2007. Using Wikipedia for automatic
Word Sense Disambiguation. In Proc. of NAACL-
HLT-07, pages 196?203.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross Bunker. 1993. A semantic concordance. In
Proceedings of the 3rd DARPA Workshop on Human
Language Technology, pages 303?308, Plainsboro,
N.J.
David Milne and Ian H. Witten. 2008a. An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the Work-
shop on Wikipedia and Artificial Intelligence: An
Evolving Synergy at AAAI-08, pages 25?30.
David Milne and Ian H. Witten. 2008b. Learning to
link with Wikipedia. In Proc. of CIKM-08, pages
509?518.
Vivi Nastase and Michael Strube. 2008. Decoding
Wikipedia category names for knowledge acquisi-
tion. In Proc. of AAAI-08, pages 1219?1224.
Vivi Nastase. 2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
activation spreading. In Proc. of EMNLP-08, pages
763?772.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study on graph connectivity for unsuper-
vised Word Sense Disambiguation. IEEE Transac-
tions on Pattern Anaylsis and Machine Intelligence,
32(4):678?692.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a very large multilingual seman-
tic network. In Proc. of ACL-10.
Roberto Navigli and Paola Velardi. 2005. Struc-
tural Semantic Interconnections: a knowledge-based
approach to Word Sense Disambiguation. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 27(7):1075?1088.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained English all-words task. In Proc. of SemEval-
2007, pages 30?35.
Roberto Navigli. 2009a. Using cycles and quasi-
cycles to disambiguate dictionary glosses. In Proc.
of EACL-09, pages 594?602.
Roberto Navigli. 2009b. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1?69.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proc. of COLING-
ACL-06, pages 793?800.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating Wikipedia. In Proc. of IJCAI-09,
pages 2083?2088.
Simone Paolo Ponzetto and Michael Strube. 2007a.
Deriving a large scale taxonomy from Wikipedia. In
Proc. of AAAI-07, pages 1440?1445.
Simone Paolo Ponzetto and Michael Strube. 2007b.
Knowledge derived from Wikipedia for computing
semantic relatedness. Journal of Artificial Intelli-
gence Research, 30:181?212.
Nils Reiter, Matthias Hartung, and Anette Frank.
2008. A resource-poor approach for linking ontol-
ogy classes to Wikipedia articles. In Johan Bos and
Rodolfo Delmonte, editors, Semantics in Text Pro-
cessing, volume 1 of Research in Computational Se-
mantics, pages 381?387. College Publications, Lon-
don, England.
German Rigau, Horacio Rodr??guez, and Eneko Agirre.
1998. Building accurate semantic taxonomies from
monolingual MRDs. In Proc. of COLING-ACL-98,
pages 1103?1109.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Ad-
vances in Web Intelligence, volume 3528 of Lecture
Notes in Computer Science. Springer Verlag.
Christina Sauper and Regina Barzilay. 2009. Automat-
ically generating Wikipedia articles: A structure-
aware approach. In Proc. of ACL-IJCNLP-09, pages
208?216.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proc. of ACL-IJCNLP-09, pages 450?458.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. In Proc. of COLING-ACL-06, pages 801?
808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203?217.
Fei Wu and Daniel Weld. 2007. Automatically se-
mantifying Wikipedia. In Proc. of CIKM-07, pages
41?50.
FeiWu and Daniel Weld. 2008. Automatically refining
the Wikipedia infobox ontology. In Proc. of WWW-
08, pages 635?644.
1531
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 67?72,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Multilingual WSD with Just a Few Lines of Code: the BabelNet API
Roberto Navigli and Simone Paolo Ponzetto
Dipartimento di Informatica
Sapienza Universita` di Roma
{navigli,ponzetto}@di.uniroma1.it
Abstract
In this paper we present an API for program-
matic access to BabelNet ? a wide-coverage
multilingual lexical knowledge base ? and
multilingual knowledge-rich Word Sense Dis-
ambiguation (WSD). Our aim is to provide the
research community with easy-to-use tools to
perform multilingual lexical semantic analysis
and foster further research in this direction.
1 Introduction
In recent years research in Natural Language Pro-
cessing (NLP) has been steadily moving towards
multilingual processing: the availability of ever
growing amounts of text in different languages, in
fact, has been a major driving force behind re-
search on multilingual approaches, from morpho-
syntactic (Das and Petrov, 2011) and syntactico-
semantic (Peirsman and Pado?, 2010) phenomena to
high-end tasks like textual entailment (Mehdad et
al., 2011) and sentiment analysis (Lu et al, 2011).
These research trends would seem to indicate the
time is ripe for developing methods capable of per-
forming semantic analysis of texts written in any
language: however, this objective is still far from be-
ing attained, as is demonstrated by research in a core
language understanding task such as Word Sense
Disambiguation (Navigli, 2009, WSD) continuing to
be focused primarily on English. While the lack of
resources has hampered the development of effec-
tive multilingual approaches to WSD, recently this
idea has been revamped with the organization of
SemEval tasks on cross-lingual WSD (Lefever and
Hoste, 2010) and cross-lingual lexical substitution
(Mihalcea et al, 2010). In addition, new research on
the topic has explored the translation of sentences
into many languages (Navigli and Ponzetto, 2010;
Lefever et al, 2011; Banea and Mihalcea, 2011),
as well as the projection of monolingual knowledge
onto another language (Khapra et al, 2011).
In our research we focus on knowledge-based
methods and tools for multilingual WSD, since
knowledge-rich WSD has been shown to achieve
high performance across domains (Agirre et al,
2009; Navigli et al, 2011) and to compete with su-
pervised methods on a variety of lexical disambigua-
tion tasks (Ponzetto and Navigli, 2010). Our vi-
sion of knowledge-rich multilingual WSD requires
two fundamental components: first, a wide-coverage
multilingual lexical knowledge base; second, tools
to effectively query, retrieve and exploit its informa-
tion for disambiguation. Nevertheless, to date, no
integrated resources and tools exist that are freely
available to the research community on a multi-
lingual scale. Previous endeavors are either not
freely available (EuroWordNet (Vossen, 1998)), or
are only accessible via a Web interface (cf. the Mul-
tilingual Research Repository (Atserias et al, 2004)
and MENTA (de Melo and Weikum, 2010)), thus
providing no programmatic access. And this is de-
spite the fact that the availability of easy-to-use li-
braries for efficient information access is known to
foster top-level research ? cf. the widespread use of
semantic similarity measures in NLP, thanks to the
availability of WordNet::Similarity (Peder-
sen et al, 2004).
With the present contribution we aim to fill this
gap in multilingual tools, providing a multi-tiered
contribution consisting of (a) an Application Pro-
gramming Interface (API) for efficiently accessing
the information available in BabelNet (Navigli and
67
bn:00008364n WIKIWN 08420278n 85 WN:EN:bank WIKI:EN:Bank WIKI:DE:Bank WIKI:IT:Banca
WIKIRED:DE:Finanzinstitut WN:EN:banking_company
WNTR:ES:banco WNTR:FR:socie?te?_bancaire WIKI:FR:Banque ...
35 1_7 2_3,4,9 6_8 ...
228 r bn:02945246n r bn:02854884n|FROM_IT @ bn:00034537n ...
Figure 1: The Babel synset for bank2n, i.e. its ?financial? sense (excerpt, formatted for ease of readability).
Ponzetto, 2010), a very large knowledge repository
with concept lexicalizations in 6 languages (Cata-
lan, English, French, German, Italian and Spanish),
at the lexicographic (i.e., word senses), encyclope-
dic (i.e., named entities) and conceptual (i.e., con-
cepts and semantic relations) levels; (b) an API to
perform graph-based WSD with BabelNet, thus pro-
viding, for the first time, a freely-available toolkit for
performing knowledge-basedWSD in a multilingual
and cross-lingual setting.
2 BabelNet
BabelNet follows the structure of a traditional lex-
ical knowledge base and accordingly consists of a
labeled directed graph where nodes represent con-
cepts and named entities and edges express semantic
relations between them. Concepts and relations are
harvested from the largest available semantic lexi-
con of English, i.e., WordNet (Fellbaum, 1998), and
a wide-coverage collaboratively-edited encyclope-
dia, i.e., Wikipedia1, thus making BabelNet a mul-
tilingual ?encyclopedic dictionary? which automati-
cally integrates fine-grained lexicographic informa-
tion with large amounts of encyclopedic knowledge
by means of a high-performing mapping algorithm
(Navigli and Ponzetto, 2010). In addition to this
conceptual backbone, BabelNet provides a multilin-
gual lexical dimension. Each of its nodes, called
Babel synsets, contains a set of lexicalizations of
the concept for different languages, e.g., { bankEN,
BankDE, bancaIT, . . . , bancoES }.
Similar in spirit to WordNet, BabelNet consists,
at its lowest level, of a plain text file. An ex-
cerpt of the entry for the Babel synset containing
bank2n is shown in Figure 12. The record contains
(a) the synset?s id; (b) the region of BabelNet
where it lies (e.g., WIKIWN means at the intersec-
1http://www.wikipedia.org
2We denote with wip the i-th WordNet sense of a word w
with part of speech p.
tion of WordNet and Wikipedia); (c) the correspond-
ing (possibly empty) WordNet 3.0 synset offset;
(d) the number of senses in all languages and
their full listing; (e) the number of translation re-
lations and their full listing; (f) the number of se-
mantic pointers (i.e., relations) to other Babel
synsets and their full listing. Senses encode in-
formation about their source ? i.e., whether they
come from WordNet (WN), Wikipedia pages (WIKI)
or their redirections (WIKIRED), or are automatic
translations (WNTR / WIKITR) ? and about their
language and lemma. In addition, translation rela-
tions among lexical items are represented as a map-
ping from source to target senses ? e.g., 2 3,4,9
means that the second element in the list of senses
(the English word bank) translates into items #3
(German Bank), #4 (Italian banca), and #9 (French
banque). Finally, semantic relations are encoded
using WordNet?s pointers and an additional sym-
bol for Wikipedia relations (r), which can also
specify the source of the relation (e.g., FROM IT
means that the relation was harvested from the Ital-
ian Wikipedia). In Figure 1, the Babel synset in-
herits the WordNet hypernym (@) relation to finan-
cial institution1n (offset bn:00034537n), as well
as Wikipedia relations to the synsets of FINAN-
CIAL INSTRUMENT (bn:02945246n) and ETH-
ICAL BANKING (bn:02854884n, from Italian).
3 An API for multilingual WSD
BabelNet API. BabelNet can be effectively ac-
cessed and automatically embedded within applica-
tions by means of a programmatic access. In order
to achieve this, we developed a Java API, based on
Apache Lucene3, which indexes the BabelNet tex-
tual dump and includes a variety of methods to ac-
cess the four main levels of information encoded in
BabelNet, namely: (a) lexicographic (information
about word senses), (b) encyclopedic (i.e. named en-
3http://lucene.apache.org
68
1 BabelNet bn = BabelNet.getInstance();
2 System.out.println("SYNSETS WITH English word: \"bank\"");
3 List<BabelSynset> synsets = bn.getSynsets(Language.EN, "bank");
4 for (BabelSynset synset : synsets) {
5 System.out.print(" =>(" + synset.getId() + ") SOURCE: " + synset.getSource() +
6 "; WN SYNSET: " + synset.getWordNetOffsets() + ";\n" +
7 " MAIN LEMMA: " + synset.getMainLemma() + ";\n SENSES (IT): { ");
8 for (BabelSense sense : synset.getSenses(Language.IT))
9 System.out.print(sense.toString()+" ");
10 System.out.println("}\n -----");
11 Map<IPointer, List<BabelSynset>> relatedSynsets = synset.getRelatedMap();
12 for (IPointer relationType : relatedSynsets.keySet()) {
13 List<BabelSynset> relationSynsets = relatedSynsets.get(relationType);
14 for (BabelSynset relationSynset : relationSynsets) {
15 System.out.println(" EDGE " + relationType.getSymbol() +
16 " " + relationSynset.getId() +
17 " " + relationSynset.toString(Language.EN));
18 }
19 }
20 System.out.println(" -----");
21 }
Figure 2: Sample BabelNet API usage.
tities), (c) conceptual (the semantic network made
up of its concepts), (d) and multilingual level (in-
formation about word translations). Figure 2 shows
a usage example of the BabelNet API. In the code
snippet we start by querying the Babel synsets for
the English word bank (line 3). Next, we access dif-
ferent kinds of information for each synset: first, we
print their id, source (WordNet, Wikipedia, or both),
the corresponding, possibly empty, WordNet offsets,
and ?main lemma? ? namely, a compact string rep-
resentation of the Babel synset consisting of its cor-
responding WordNet synset in stringified form, or
the first non-redirection Wikipedia page found in it
(lines 5?7). Then, we access and print the Italian
word senses they contain (lines 8?10), and finally
the synsets they are related to (lines 11?19). Thanks
to carefully designed Java classes, we are able to ac-
complish all of this in about 20 lines of code.
Multilingual WSD API. We use the BabelNet API
as a framework to build a toolkit that allows the
user to performmultilingual graph-based lexical dis-
ambiguation ? namely, to identify the most suitable
meanings of the input words on the basis of the se-
mantic connections found in the lexical knowledge
base, along the lines of Navigli and Lapata (2010).
At its core, the API leverages an in-house Java li-
brary to query paths and create semantic graphs
with BabelNet. The latter works by pre-computing
off-line paths connecting any pair of Babel synsets,
which are collected by iterating through each synset
in turn, and performing a depth-first search up to a
maximum depth ? which we set to 3, on the basis of
experimental evidence from a variety of knowledge
base linking and lexical disambiguation tasks (Nav-
igli and Lapata, 2010; Ponzetto and Navigli, 2010).
Next, these paths are stored within a Lucene index,
which ensures efficient lookups for querying those
paths starting and ending in a specific synset. Given
a set of words as input, a semantic graph factory
class searches for their meanings within BabelNet,
looks for their connecting paths, and merges such
paths within a single graph. Optionally, the paths
making up the graph can be filtered ? e.g., it is possi-
ble to remove loops, weighted edges below a certain
threshold, etc. ? and the graph nodes can be scored
using a variety of methods ? such as, for instance,
their outdegree or PageRank value in the context of
the semantic graph. These graph connectivity mea-
sures can be used to rank senses of the input words,
thus performing graph-based WSD on the basis of
the structure of the underlying knowledge base.
We show in Figure 3 a usage example of our
disambiguation API. The method which performs
WSD (disambiguate) takes as input a col-
lection of words (i.e., typically a sentence), a
KnowledgeBase with which to perform dis-
69
1 public static void disambiguate(Collection<Word> words,
2 KnowledgeBase kb, KnowledgeGraphScorer scorer) {
3 KnowledgeGraphFactory factory = KnowledgeGraphFactory.getInstance(kb);
4 KnowledgeGraph kGraph = factory.getKnowledgeGraph(words);
5 Map<String, Double> scores = scorer.score(kGraph);
6 for (String concept : scores.keySet()) {
7 double score = scores.get(concept);
8 for (Word word : kGraph.wordsForConcept(concept))
9 word.addLabel(concept, score);
10 }
11 for (Word word : words) {
12 System.out.println("\n\t" + word.getWord() + " -- ID " + word.getId() +
13 " => SENSE DISTRIBUTION: ");
14 for (ScoredItem<String> label : word.getLabels()) {
15 System.out.println("\t [" + label.getItem() + "]:" +
16 Strings.format(label.getScore()));
17 }
18 }
19 }
20
21 public static void main(String[] args) {
22 List<Word> sentence = Arrays.asList(
23 new Word[]{new Word("bank", ?n?, Language.EN), new Word("bonus", ?n?, Language.EN),
24 new Word("pay", ?v?, Language.EN), new Word("stock", ?n?, Language.EN)});
25 disambiguate(sentence, KnowledgeBase.BABELNET, KnowledgeGraphScorer.DEGREE);
26 }
Figure 3: Sample Word Sense Disambiguation API usage.
ambiguation, and a KnowledgeGraphScorer,
namely a value from an enumeration of different
graph connectivity measures (e.g., node outdegree),
which are responsible for scoring nodes (i.e., con-
cepts) in the graph. KnowledgeBase is an enu-
meration of supported knowledge bases: currently, it
includes BabelNet, as well as WordNet++ (namely,
an EnglishWordNet-based subset of it (Ponzetto and
Navigli, 2010)) and WordNet. Note that, while Ba-
belNet is presently the only lexical knowledge base
which allows for multilingual processing, our frame-
work can easily be extended to work with other ex-
isting lexical knowledge resources, provided they
can be wrapped around Java classes and implement
interface methods for querying senses, concepts, and
their semantic relations. In the snippet we start in
line 3 by obtaining an instance of the factory class
which creates the semantic graphs for a given knowl-
edge base. Next, we use this factory to create the
graph for the input words (line 4). We then score the
senses of the input words occurring within this graph
(line 5?10). Finally, we output the sense distribu-
tions of each word in lines 11?18. The disambigua-
tion method, in turn, can be called by any other Java
program in a way similar to the one highlighted by
the main method of lines 21?26, where we disam-
biguate the sample sentence ?bank bonuses are paid
in stocks? (note that each input word can be written
in any of the 6 languages, i.e. we could mix lan-
guages).
4 Experiments
We benchmark our API by performing knowledge-
based WSD with BabelNet on standard SemEval
datasets, namely the SemEval-2007 coarse-grained
all-words (Navigli et al, 2007, Coarse-WSD, hence-
forth) and the SemEval-2010 cross-lingual (Lefever
and Hoste, 2010, CL-WSD) WSD tasks. For
both experimental settings we use a standard graph-
based algorithm, Degree (Navigli and Lapata, 2010),
which has been previously shown to yield a highly
competitive performance on different lexical disam-
biguation tasks (Ponzetto and Navigli, 2010). Given
a semantic graph for the input context, Degree se-
lects the sense of the target word with the highest
vertex degree. In addition, in the CL-WSD setting
we need to output appropriate lexicalization(s) in
different languages. Since the selected Babel synset
can contain multiple translations in a target language
for the given English word, we use for this task an
70
Algorithm Nouns only All words
NUS-PT 82.3 82.5
SUSSX-FR 81.1 77.0
Degree 84.7 82.3
MFS BL 77.4 78.9
Random BL 63.5 62.7
Table 1: Performance on SemEval-2007 coarse-grained
all-words WSD (Navigli et al, 2007).
unsupervised approach where we return for each test
instance only the most frequent translation found in
the synset, as given by its frequency of alignment
obtained from the Europarl corpus (Koehn, 2005).
Tables 1 and 2 summarize our results in terms
of recall (the primary metric for WSD tasks): for
each SemEval task, we benchmark our disambigua-
tion API against the best unsupervised and super-
vised systems, namely SUSSX-FR (Koeling and
McCarthy, 2007) and NUS-PT (Chan et al, 2007)
for Coarse-WSD, and T3-COLEUR (Guo and Diab,
2010) and UvT-v (van Gompel, 2010) for CL-WSD.
In the Coarse-WSD task our API achieves the best
overall performance on the nouns-only subset of
the data, thus supporting previous findings indicat-
ing the benefits of using rich knowledge bases like
BabelNet. In the CL-WSD evaluation, instead, us-
ing BabelNet alows us to surpass the best unsuper-
vised system by a substantial margin, thus indicating
the viability of high-performing WSD with a multi-
lingual lexical knowledge base. While our perfor-
mance still lags behind the application of supervised
techniques to this task (cf. also results from Lefever
and Hoste (2010)), we argue that further improve-
ments can still be obtained by exploiting more com-
plex disambiguation strategies. In general, using our
toolkit we are able to achieve a performance which
is competitive with the state of the art for these tasks,
thus supporting previous findings on knowledge-rich
WSD, and confirming the robustness of our toolkit.
5 Related Work
Our work complements recent efforts focused on vi-
sual browsing of wide-coverage knowledge bases
(Tylenda et al, 2011; Navigli and Ponzetto, 2012)
by means of an API which allows the user to pro-
grammatically query and search BabelNet. This
knowledge resource, in turn, can be used for eas-
Degree T3-Coleur UvT-v
Dutch 15.52 10.56 17.70
French 22.94 21.75 ?
German 17.15 13.05 ?
Italian 18.03 14.67 ?
Spanish 22.48 19.64 23.39
Table 2: Performance on SemEval-2010 cross-lingual
WSD (Lefever and Hoste, 2010).
ily performing multilingual and cross-lingual WSD
out-of-the-box. In comparison with other contribu-
tions, our toolkit for multilingual WSD takes pre-
vious work from Navigli (2006), in which an on-
line interface for graph-based monolingual WSD is
presented, one step further by adding a multilin-
gual dimension as well as a full-fledged API. Our
work also complements previous attempts by NLP
researchers to provide the community with freely
available tools to perform state-of-the-art WSD us-
ing WordNet-based measures of semantic related-
ness (Patwardhan et al, 2005), as well as supervised
WSD techniques (Zhong and Ng, 2010). We achieve
this by building upon BabelNet, a multilingual ?en-
cyclopedic dictionary? bringing together the lexico-
graphic and encyclopedic knowledge from WordNet
and Wikipedia. Other recent projects on creating
multilingual knowledge bases from Wikipedia in-
clude WikiNet (Nastase et al, 2010) and MENTA
(de Melo and Weikum, 2010): both these resources
offer structured information complementary to Ba-
belNet ? i.e., large amounts of facts about entities
(MENTA), and explicit semantic relations harvested
from Wikipedia categories (WikiNet).
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting Grant
MultiJEDI No. 259234.
BabelNet and its API are available for download at
http://lcl.uniroma1.it/babelnet.
References
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
performing better than generic supervised WSD. In
Proc. of IJCAI-09, pages 1501?1506.
71
Jordi Atserias, Luis Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
Vossen. 2004. The MEANING multilingual central
repository. In Proc. of GWC-04, pages 22?31.
Carmen Banea and Rada Mihalcea. 2011. Word Sense
Disambiguation with multilingual features. In Proc.
of IWCS-11, pages 25?34.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
NUS-ML: Exploiting parallel texts for Word Sense
Disambiguation in the English all-words tasks. In
Proc. of SemEval-2007, pages 253?256.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL-11, pages 600?609.
Gerard de Melo and Gerhard Weikum. 2010. MENTA:
inducing multilingual taxonomies from Wikipedia. In
Proc. of CIKM-10, pages 1099?1108.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Weiwei Guo and Mona Diab. 2010. COLEPL and COL-
SLM: An unsupervised WSD approach to multilingual
lexical substitution, tasks 2 and 3 SemEval 2010. In
Proc. of SemEval-2010, pages 129?133.
Mitesh M. Khapra, Salil Joshi, Arindam Chatterjee, and
Pushpak Bhattacharyya. 2011. Together we can:
Bilingual bootstrapping for WSD. In Proc. of ACL-
11, pages 561?569.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of Ma-
chine Translation Summit X.
Rob Koeling and Diana McCarthy. 2007. Sussx: WSD
using automatically acquired predominant senses. In
Proc. of SemEval-2007, pages 314?317.
Els Lefever and Veronique Hoste. 2010. SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation. In
Proc. of SemEval-2010, pages 15?20.
Els Lefever, Ve?ronique Hoste, and Martine De Cock.
2011. Parasense or how to use parallel corpora for
Word Sense Disambiguation. In Proc. of ACL-11,
pages 317?322.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin
K. Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proc. of ACL-11,
pages 320?330.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proc. of ACL-11, pages
1336?1345.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
SemEval-2010 Task 2: Cross-lingual lexical substitu-
tion. In Proc. of SemEval-2010, pages 9?14.
Vivi Nastase, Michael Strube, Benjamin Bo?rschinger,
Caecilia Zirn, and Anas Elghafari. 2010. WikiNet:
A very large scale multi-lingual concept network. In
Proc. of LREC ?10.
Roberto Navigli and Mirella Lapata. 2010. An exper-
imental study on graph connectivity for unsupervised
Word Sense Disambiguation. IEEE Transactions on
Pattern Analysis andMachine Intelligence, 32(4):678?
692.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belNet: Building a very large multilingual semantic
network. In Proc. of ACL-10, pages 216?225.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNetXplorer: a platform for multilingual lexical
knowledge base access and exploration. In Comp. Vol.
to Proc. of WWW-12, pages 393?396.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-grained
English all-words task. In Proc. of SemEval-2007,
pages 30?35.
Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier Lopez
de Lacalle, and Eneko Agirre. 2011. Two birds with
one stone: learning semantic models for Text Catego-
rization and Word Sense Disambiguation. In Proc. of
CIKM-11, pages 2317?2320.
Roberto Navigli. 2006. Online word sense disambigua-
tion with structural semantic interconnections. In
Proc. of EACL-06, pages 107?110.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2005. SenseRelate::TargetWord ? a generalized
framework for Word Sense Disambiguation. In Comp.
Vol. to Proc. of ACL-05, pages 73?76.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity ? Measuring the re-
latedness of concepts. In Comp. Vol. to Proc. of HLT-
NAACL-04, pages 267?270.
Yves Peirsman and Sebastian Pado?. 2010. Cross-
lingual induction of selectional preferences with bilin-
gual vector spaces. In Proc. of NAACL-HLT-10, pages
921?929.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rivaling
supervised system. In Proc. of ACL-10, pages 1522?
1531.
Tomasz Tylenda, Mauro Sozio, and Gerhard Weikum.
2011. Einstein: physicist or vegetarian? Summariz-
ing semantic type graphs for knowledge discovery. In
Proc. of WWW-11, pages 273?276.
Maarten van Gompel. 2010. UvT-WSD1: A cross-
lingual word sense disambiguation system. In Proc.
of SemEval-2010, pages 238?241.
Piek Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer,
Dordrecht, The Netherlands.
Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage Word Sense Disambiguation system
for free text. In Proc. of ACL-10 System Demonstra-
tions, pages 78?83.
72
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 5?6,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploiting Social Media for Natural Language Processing:
Bridging the Gap between Language-centric and Real-world Applications
Simone Paolo Ponzetto
Research Group Data and Web Science
University of Mannheim
Mannheim, Germany
simone@informatik.uni-mannheim.de
Andrea Zielinski
Fraunhofer IOSB
Fraunhoferstra?e 1
Karlsruhe, Germany
andrea.zielinski@iosb.fraunhofer.de
Introduction
Social media like Twitter and micro-blogs provide
a goldmine of text, shallow markup annotations
and network structure. These information sources
can all be exploited together in order to automat-
ically acquire vast amounts of up-to-date, wide-
coverage structured knowledge. This knowledge,
in turn, can be used to measure the pulse of a va-
riety of social phenomena like political events, ac-
tivism and stock prices, as well as to detect emerg-
ing events such as natural disasters (earthquakes,
tsunami, etc.).
The main purpose of this tutorial is to introduce
social media as a resource to the Natural Language
Processing (NLP) community both from a scien-
tific and an application-oriented perspective. To
this end, we focus on micro-blogs such as Twitter,
and show how it can be successfully mined to per-
form complex NLP tasks such as the identification
of events, topics and trends. Furthermore, this in-
formation can be used to build high-end socially
intelligent applications that tap the wisdom of the
crowd on a large scale, thus successfully bridging
the gap between computational text analysis and
real-world, mission-critical applications such as fi-
nancial forecasting and natural crisis management.
Tutorial Outline
1. Social media and the wisdom of the crowd.
We review the resources which will be the focus
of the tutorial, i.e. Twitter and micro-blogging in
general, and present their most prominent and dis-
tinguishing aspects (Kwak et al, 2010; Gouws et
al., 2011), namely: (i) instant short-text messag-
ing, including its specific linguistic characteris-
tics (e.g., non-standard spelling, shortenings, lo-
gograms, etc.) and other features ? i.e., mentions
(@), hashtags (#), shortened URLs, etc.; (ii) a dy-
namic network structure where users are highly
inter-connected and author profile information is
provided along with other metadata. We intro-
duce these properties by highlighting the differ-
ent trade-offs related to resources of this kind,
as well as their comparison with alternative data
publishing platforms ? for instance, highly un-
structured text vs. rich network structure, semi-
structured metadata tagging (like hashtags) vs.
fully-structured linked open data, etc.
2. Analyzing and extracting structured infor-
mation from social media. We provide an in-
depth overview of contributions aimed at tapping
the wealth of information found within Twitter
and other micro-blogs. We first show how so-
cial media can be used for many different NLP
tasks, ranging from pre-processing tasks like PoS
tagging (Gimpel et al, 2011) and Named Entity
Recognition (Ritter et al, 2011) through high-end
discourse (Ritter et al, 2010) and information ex-
traction applications like event detection (Popescu
et al, 2011; Ritter et al, 2012) and topic track-
ing (Lin et al, 2011). We then focus on novel
tasks and challenges opened up by social media
such as geoparsing, which aims to predict the lo-
cation (including its geographic coordinates) of a
message or user based on his posts (Gelernter and
Mushegian, 2011; Han et al, 2012), and methods
to automatically establish the credibility of user-
generated content by making use of contextual and
metadata features (Castillo et al, 2011).
3. Exploiting social media for real-world appli-
cations: trend detection, social sensing and cri-
sis management. We present methods to detect
emerging events and breaking news from social
media (Mathioudakis et al, 2010; Petrovic? et al,
2010, inter alia). Thanks to their highly dynamic
environment and continuously updated content, in
fact, micro-blogs and social networks are capable
of providing real-time information for a wide vari-
5
ety of different social phenomena, including con-
sumer confidence and presidential job approval
polls (O?Connor et al, 2010), as well as stock mar-
ket prices (Bollen et al, 2011; Ruiz et al, 2012).
We focus in particular on applications that use so-
cial media for health surveillance in order to mon-
itor, for instance, flu epidemics (Aramaki et al,
2011), as well as crisis management systems that
leverage them for tracking natural disasters like
earthquakes (Sakaki et al, 2010; Neubig et al,
2011) and tsunami (Zielinski and Bu?rgel, 2012;
Zielinski et al, 2013).
References
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: detecting influenza
epidemics using Twitter. In Proc. of EMNLP-11,
pages 1568?1576.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1?8.
Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on Twitter.
In Proc of WWW-11, pages 675?684.
Judith Gelernter and Nikolai Mushegian. 2011. Geo-
parsing messages from microtext. Transactions in
GIS, 15(6):753?773.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proc. of ACL-11, pages 42?47.
Stephan Gouws, Donald Metzler, Congxing Cai, and
Eduard Hovy. 2011. Contextual bearing on lin-
guistic variation in social media. In Proceedings of
the Workshop on Language in Social Media (LSM
2011), pages 20?29.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Ge-
olocation prediction in social media data by finding
location indicative words. In Proc. of COLING-12,
pages 1045?1062.
Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is Twitter, a social network
or a news media? In Proc of WWW-10, pages 591?
600.
Jimmy Lin, Rion Snow, and William Morgan. 2011.
Smoothing techniques for adaptive online language
models: topic tracking in tweet streams. In Proc. of
KDD-11, pages 422?429.
Michael Mathioudakis, Nick Koudas, and Peter Mar-
bach. 2010. Early online identification of attention
gathering items in social media. In Proc. of WSDM-
10, pages 301?310.
Graham Neubig, Yuichiroh Matsubayashi, Masato
Hagiwara, and Koji Murakami. 2011. Safety in-
formation mining ? what can NLP do in a disaster ?.
In Proceedings of IJCNLP-11, pages 965?973.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: linking text sentiment to
public opinion time series. In Proc. of ICWSM-10,
pages 122?129.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to Twitter. In Proc. of NAACL-10, pages 181?
189.
Ana-Maria Popescu, Marco Pennacchiotti, and Deepa
Paranjpe. 2011. Extracting events and event de-
scriptions from Twitter. In Comp. Vol. to Proc. of
WWW-11, pages 105?106.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of Twitter conversations. In
Proc. of NAACL-10, pages 172?180.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an exper-
imental study. In Proc. of EMNLP-11, pages 1524?
1534.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from Twitter.
In Proc. of KDD-12, pages 1104?1112.
Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo,
Aristides Gionis, and Alejandro Jaimes. 2012. Cor-
relating financial time series with micro-blogging
activity. In Proc. of WSDM-12, pages 513?522.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proc. of WWW-
10, pages 851?860.
Andrea Zielinski and Ulrich Bu?rgel. 2012. Multilin-
gual analysis of Twitter news in support of mass
emergency events. In Proc. of ISCRAM-12.
Andrea Zielinski, Stuart E. Middleton, Laurissa
Tokarchuk, and Xinyue Wang. 2013. Social-media
text mining and network analysis to support decision
support for natural crisis management. In Proc. of
ISCRAM-13.
6
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104?107,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BART: A Multilingual Anaphora Resolution System
Samuel Broscheit?, Massimo Poesio?, Simone Paolo Ponzetto?, Kepa Joseba Rodriguez?,
Lorenza Romano?, Olga Uryupina?, Yannick Versley?, Roberto Zanoli?
?Seminar fu?r Computerlinguistik, University of Heidelberg
?CiMeC, University of Trento
?Fondazione Bruno Kessler
?SFB 833, University of Tu?bingen
broscheit@cl.uni-heidelberg.de, massimo.poesio@unitn.it,
ponzetto@cl.uni-heidelberg.de, kepa.rodriguez@unitn.it,
romano@fbk.eu, uryupina@gmail.com,
versley@sfs.uni-tuebingen.de, zanoli@fbk.eu
Abstract
BART (Versley et al, 2008) is a highly mod-
ular toolkit for coreference resolution that
supports state-of-the-art statistical approaches
and enables efficient feature engineering. For
the SemEval task 1 on Coreference Resolu-
tion, BART runs have been submitted for Ger-
man, English, and Italian.
BART relies on a maximum entropy-based
classifier for pairs of mentions. A novel entity-
mention approach based on Semantic Trees is
at the moment only supported for English.
1 Introduction
This paper presents a multilingual coreference reso-
lution system based on BART (Versley et al, 2008).
BART is a modular toolkit for coreference resolution
that supports state-of-the-art statistical approaches
to the task and enables efficient feature engineer-
ing. BART has originally been created and tested
for English, but its flexible modular architecture en-
sures its portability to other languages and domains.
In SemEval-2010 task 1 on Coreference Resolution,
BART has shown reliable performance for English,
German and Italian.
In our SemEval experiments, we mainly focus on
extending BART to cover multiple languages. Given
a corpus in a new language, one can re-train BART
to obtain baseline results. Such a language-agnostic
system, however, is only used as a starting point:
substantial improvements can be achieved by incor-
porating language-specific information with the help
of the Language Plugin. This design provides ef-
fective separation between linguistic and machine
learning aspects of the problem.
2 BART Architecture
The BART toolkit has five main components: pre-
processing pipeline, mention factory, feature extrac-
tion module, decoder and encoder. In addition, an
independent LanguagePlugin module handles all the
language specific information and is accessible from
any component. The architecture is shown on Figure
1. Each module can be accessed independently and
thus adjusted to leverage the system?s performance
on a particular language or domain.
The preprocessing pipeline converts an input doc-
ument into a set of lingustic layers, represented
as separate XML files. The mention factory uses
these layers to extract mentions and assign their
basic properties (number, gender etc). The fea-
ture extraction module describes pairs of mentions
{M
i
,M
j
}, i < j as a set of features.
The decoder generates training examples through
a process of sample selection and learns a pairwise
classifier. Finally, the encoder generates testing ex-
amples through a (possibly distinct) process of sam-
ple selection, runs the classifier and partitions the
mentions into coreference chains.
3 Language-specific issues
Below we briefly describe our language-specific ex-
tensions to BART. These issues are addressed in
more details in our recent papers (Broscheit et al,
2010; Poesio et al, 2010).
3.1 Mention Detection
Robust mention detection is an essential component
of any coreference resolution system. BART sup-
ports different pipelines for mention detection. The
104
Parser
Dep-to-Const
Converter
Morphology
Preprocessing
Mention
Factory
Decoder
Basic features
Syntactic features
Knowledge-based
features
MaxEnt
Classifier
Mention
(with basic
 properties):
- Number
- Gender
- Mention Type
- Modifiers
Unannotated
Text
Coreference
Chains
LanguagePlugin
Figure 1: BART architecture
choice of a pipeline depends crucially on the avail-
ability of linguistic resources for a given language.
For English and German, we use the Parsing
Pipeline and Mention Factory to extract mentions.
The parse trees are used to identify minimal and
maximal noun projections, as well as additional fea-
tures such as number, gender, and semantic class.
For English, we use parses from a state-of-the-art
constituent parser (Petrov et al, 2006) and extract
all base noun phrases as mentions. For German,
the SemEval dependency tree is transformed to a
constituent representation and minimal and maxi-
mal phrases are extracted for all nominal elements
(pronouns, common nouns, names), except when the
noun phrase is in a non-referring syntactic position
(for example, expletive ?es?, predicates in copula
constructions).
For Italian, we use the EMD Pipeline and Men-
tion Factory. The Typhoon (Zanoli et al, 2009)
and DEMention (Biggio et al, 2009) systems were
used to recognize mentions in the test set. For each
mention, its head and extension were considered.
The extension was learned by using the mention an-
notation provided in the training set (13th column)
whereas the head annotation was learned by exploit-
ing the information produced by MaltParser (Nivre
et al, 2007). In addition to the features extracted
from the training set, such as prefixes and suffixes
(1-4 characters) and orthographic information (capi-
talization and hyphenation), a number of features ex-
tracted by using external resources were used: men-
tions recognized by TextPro (http://textpro.fbk.eu),
gazetteers of generic proper nouns extracted from
the Italian phone-book and Wikipedia, and other fea-
tures derived from WordNet. Each of these features
was extracted in a local context of ?2 words.
3.2 Features
We view coreference resolution as a binary classifi-
cation problem. Each classification instance consists
of two markables, i.e. an anaphor and potential an-
tecedent. Instances are modeled as feature vectors
(cf. Table 1) and are handed over to a binary clas-
sifier that decides, given the features, whether the
anaphor and the candidate are coreferent or not. All
the feature values are computed automatically, with-
out any manual intervention.
Basic feature set. We use the same set of rela-
tively language-independent features as a backbone
of our system, extending it with a few language-
specific features for each subtask. Most of them are
used by virtually all the state-of-the-art coreference
resolution systems. A detailed description can be
found, for example, in (Soon et al, 2001).
English. Our English system is based on a novel
model of coreference. The key concept of our model
is a Semantic Tree ? a filecard associated with each
discourse entity containing the following fields:
? Types: the list of types for mentions of a given
entity. For example, if an entity contains the
mention ?software from India?, the shallow
predicate ?software? is added to the types.
? Attributes: this field collects the premodifiers.
For instance, if one of the mentions is ?the ex-
pensive software? the shallow attribute ?expen-
sive? is added to the list of attributes.
? Relations: this field collects the prepositional
postmodifiers. If an entity contains the men-
tion ?software from India?, the shallow relation
?from(India)? is added to the list of relations.
105
For each mention BART creates such a filecard
using syntactic information. If the classifier decides
that both mentions are corefering, the filecard of
the anaphora is merged into the filecard of the an-
tecedent (cf. Section 3.3 below).
The SemanticTreeCompatibility feature
extractor checks whether individual slots of the
anaphor?s filecard are compatible with those of the
antecedent?s.
The StrudelRelatedness feature relies on
Strudel ? a distributional semantic model (Baroni et
al., 2010). We compute Strudel vectors for the sets
of types of the anaphor and the antecedent. The re-
latedness value is determined as the cosine between
the two.
German. We have tested extra features for Ger-
man in our previous study (Broscheit et al, 2010).
The NodeDistance feature measures the num-
ber of clause nodes (SIMPX, R-SIMPX) and preposi-
tional phrase nodes (PX) along the path between M
j
and M
i
in the parse tree.
The PartialMorphMatch feature is a sub-
string match with a morphological extension for
common nouns. In German the frequent use of
noun composition makes a simple string match for
common nouns unfeasible. The feature checks for
a match between the noun stems of M
i
and M
j
.
We extract the morphology with SMOR/Morphisto
(Schmid et al, 2004).
The GermanetRelatedness feature uses the
Pathfinder library for GermaNet (Finthammer and
Cramer, 2008) that computes and discretizes raw
scores into three categories of semantic relatedness.
In our experiments we use the measure from Wu and
Palmer (1994), which has been found to be the best
performing on our development data.
Italian. We have designed a feature to cover Ital-
ian aliasing patterns. A list of company/person des-
ignators (e.g., ?S.p.a? or ?D.ssa?) has been manually
crafted. We have collected patterns of name variants
for locations. Finally, we have relaxed abbreviation
constraints, allowing for lower-case characters in the
abbreviations. Our pilot experiments suggest that,
although a universal aliasing algorithm is able to re-
solve some coreference links between NEs, creating
a language-specific module boosts the system?s per-
formance for Italian substantially.
Basic feature set
MentionType(M
i
),MentionType(M
j
)
SemanticClass(M
i
), SemanticClass(M
j
)
GenderAgreement(M
i
,M
j
)
NumberAgreement(M
i
,M
j
)
AnimacyAgreement(M
i
,M
j
)
StringMatch(M
i
,M
j
)
Distance(M
i
,M
j
)
Basic features used for English and Italian
Alias(M
i
,M
j
)
Apposition(M
i
,M
j
)
FirstMention(M
i
)
English
IsSubject(M
i
)
SemanticTreeCompatibility(M
i
,M
j
)
StrudelRelatedness(M
i
,M
j
)
German
InQuotedSpeech(M
i
), InQuotedSpeech(M
j
)
NodeDistance(M
i
,M
j
)
PartialMorphMatch(M
i
,M
j
)
GermanetRelatedness(M
i
,M
j
)
Italian
AliasItalian(M
i
,M
j
)
Table 1: Features used by BART: each feature describes
a pair of mentions {M
i
,M
j
}, i < j, where M
i
is a can-
didate antecedent and M
j
is a candidate anaphor
3.3 Resolution Algorithm
The BART toolkit supports several models of coref-
erence (pairwise modeling, rankers, semantic trees),
as well as different machine learning algorithms.
Our final setting relies on a pairwise maximum en-
tropy classifier for Italian and German.
Our English system is based on an entity-mention
model of coreference. The key concept of our model
is a Semantic Tree - a filecard associated to each dis-
course entity (cf. Section 3.2). Semantic trees are
used for both computing feature values and guiding
the resolution process.
We start by creating a Semantic Tree for each
mention. We process the document from left to
right, trying to find an antecedent for each men-
tion (candidate anaphor). When the antecedent is
found, we extend its Semantic Tree with the types,
attributes and relations of the anaphor, provided
they are mutually compatible. Consider, for ex-
106
ample, a list of mentions, containing, among oth-
ers, ?software from India?, ?the software? and ?soft-
ware from China?. Initially, BART creates the fol-
lowing semantic trees: ?(type: software) (relation:
from(India))?, ?(type: software)? and ?(type: soft-
ware) (relation: from(China))?. When the second
mention gets resolved to the first one, their seman-
tic trees are merged to ?(type: software) (relation:
from(India)?. Therefore, when we attempt to resolve
the third mention, both candidate antecedents are re-
jected, as their relation attributes are incompatible
with ?from(China)?. This approach helps us avoid
erroneous links (such as the link between the second
and the third mentions in our example) by leveraging
entity-level information.
4 Evaluation
The system was evaluated on the SemEval task 1
corpus by using the SemEval scorer.
First, we have evaluated our mention detection
modules: the system?s ability to recognize both the
mention extensions and the heads in the regular set-
ting. BART has achieved the best score for men-
tion detection in German and has shown reliable
figures for English. For Italian, the moderate per-
formance level is due to the different algorithms
for identifying the heads: the MaltParser (trained
on TUT: http://www.di.unito.it/?tutreeb) produces a
more semantic representation, while the SemEval
scorer seems to adopt a more syntactic approach.
Second, we have evaluated the quality of our
coreference resolution modules. For German, BART
has shown better performance than all the other sys-
tems on the regular track.
For English, the only language targeted by all sys-
tems, BART shows good performance over all met-
rics in the regular setting, usually only outperformed
by systems that were tuned to a particular metric.
Finally, the Italian version of BART shows re-
liable figures for coreference resolution, given the
mention alignment problem discussed above.
5 Conclusion
We have presented BART ? a multilingual toolkit
for coreference resolution. Due to its highly modu-
lar architecture, BART allows for efficient language-
specific feature engineering. Our effort represents
the first steps towards building a freely available
coreference resolution system for many languages.
References
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222?254.
Silvana Marianela Bernaola Biggio, Claudio Giuliano,
Massimo Poesio, Yannick Versley, Olga Uryupina, and
Roberto Zanoli. 2009. Local entity detection and
recognition task. In Proc. of Evalita-09.
Samuel Broscheit, Simone Paolo Ponzetto, Yannick Ver-
sley, and Massimo Poesio. 2010. Extending BART to
provide a coreference resolution system for German.
In Proc. of LREC ?10.
Marc Finthammer and Irene Cramer. 2008. Explor-
ing and navigating: Tools for GermaNet. In Proc. of
LREC ?08.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov, Leon Barett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of COLING-ACL-06.
Massimo Poesio, Olga Uryupina, and Yannick Versley.
2010. Creating a coreference resolution system for
Italian. In Proc. of LREC ?10.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology cover-
ing derivation, composition and inflection. In Proc. of
LREC ?04.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics (Special Issue on Computational Anaphora
Resolution), 27(4):521?544.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Proceedings of the Linguistic Coreference Work-
shop at the International Conference on Language Re-
sources and Evaluation (LREC-2008).
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proc. of ACL-94, pages 133?
138.
Roberto Zanoli, Emiliano Pianta, and Claudio Giuliano.
2009. Named entity recognition through redundancy
driven classifier. In Proc. of Evalita-09.
107
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 134?137,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
UHD: Cross-Lingual Word Sense Disambiguation Using
Multilingual Co-occurrence Graphs
Carina Silberer and Simone Paolo Ponzetto
Department of Computational Linguistics
Heidelberg University
{silberer,ponzetto}@cl.uni-heidelberg.de
Abstract
We describe the University of Heidelberg
(UHD) system for the Cross-Lingual Word
Sense Disambiguation SemEval-2010 task
(CL-WSD). The system performs CL-
WSD by applying graph algorithms pre-
viously developed for monolingual Word
Sense Disambiguation to multilingual co-
occurrence graphs. UHD has participated
in the BEST and out-of-five (OOF) eval-
uations and ranked among the most com-
petitive systems for this task, thus indicat-
ing that graph-based approaches represent
a powerful alternative for this task.
1 Introduction
This paper describes a graph-based system for
Cross-Lingual Word Sense Disambiguation, i.e.
the task of disambiguating a word in context by
providing its most appropriate translations in dif-
ferent languages (Lefever and Hoste, 2010, CL-
WSD henceforth). Our goal at SemEval-2010 was
to assess whether graph-based approaches, which
have been successfully developed for monolingual
Word Sense Disambiguation, represent a valid
framework for CL-WSD. These typically trans-
form a knowledge resource such as WordNet (Fell-
baum, 1998) into a graph and apply graph algo-
rithms to perform WSD. In our work, we follow
this line of research and apply graph-based meth-
ods to multilingual co-occurrence graphs which
are automatically created from parallel corpora.
2 Related Work
Our method is heavily inspired by previous pro-
posals from V?eronis (2004, Hyperlex) and Agirre
et al (2006). Hyperlex performs graph-based
WSD based on co-occurrence graphs: given a
monolingual corpus, for each target word a graph
is built where nodes represent content words co-
occurring with the target word in context, and
edges connect the words which co-occur in these
contexts. The second step iteratively selects the
node with highest degree in the graph (root hub)
and removes it along with its adjacent nodes. Each
such selection corresponds to isolating a high-
density component of the graph, in order to select
a sense of the target word. In the last step the root
hubs are linked to the target word and the Mini-
mum Spanning Tree (MST) of the graph is com-
puted to disambiguate the target word in context.
Agirre et al (2006) compare Hyperlex with an al-
ternative method to detect the root hubs based on
PageRank (Brin and Page, 1998). PageRank has
the advantage of requiring less parameters than
Hyperlex, whereas the authors ascertain equal per-
formance of the two methods.
3 Graph-based Cross-Lingual WSD
We start by building for each target word a mul-
tilingual co-occurrence graph based on the target
word?s aligned contexts found in parallel corpora
(Sections 3.1 and 3.2). Multilingual nodes are
linked by translation edges, labeled with the target
word?s translations observed in the corresponding
contexts. We then use an adapted PageRank al-
gorithm to select the nodes which represent the
target word?s different senses (Section 3.3) and,
given these nodes, we compute the MST, which
is used to select the most relevant words in con-
text to disambiguate a given test instance (Section
3.4). Translations are finally given by the incom-
ing translation edges of the selected context words.
134
3.1 Monolingual Graph
Let C
s
be all contexts of a target word w in
a source language s, i.e. English in our case,
within a (PoS-tagged and lemmatized) monolin-
gual corpus. We first construct a monolingual co-
occurrence graph G
s
= ?V
s
, E
s
?. We collect all
pairs (cw
i
, cw
j
) of co-occurring nouns or adjec-
tives in C
s
(excluding the target word itself) and
add each word as a node into the initially empty
graph. Each co-occurring word pair is connected
with an edge (v
i
, v
j
) ? E
s
, which is assigned a
weight w(v
i
, v
j
) based on the strength of associa-
tion between the respective words cw
i
and cw
j
:
w(v
i
, v
j
) = 1?max [p(cw
i
|cw
j
), p(cw
j
|cw
i
)].
The conditional probability of word cw
i
given
word cw
j
is estimated by the number of contexts
in which cw
i
and cw
j
co-occur divided by the
number of contexts containing cw
j
.
3.2 Multilingual Graph
Given a set of target languages L, we then ex-
tend G
s
to a labeled multilingual graph G
ML
=
?V
ML
, E
ML
? where:
1. V
ML
= V
s
?
?
l?L
V
l
is a set of nodes represent-
ing content words from either the source (V
s
) or
the target (V
l
) languages;
2. E
ML
= E
s
?
?
l?L
{E
l
? E
s,l
} is a set of
edges. These include (a) co-occurrence edges
E
l
? V
l
?V
l
between nodes representing words
in a target language (V
l
), weighted in the same
way as the edges in the monolingual graph;
(b) labeled translation edges E
s,l
which repre-
sent translations of words from the source lan-
guage into a target language. These edges are
assigned a complex label t ? T
w,l
compris-
ing a translation of the word w in the target
language l and its frequency of translation, i.e.
E
s,l
? V
s
? T
w,l
? V
l
.
The multilingual graph is built based on a word-
aligned multilingual parallel corpus and a multi-
lingual dictionary. The pseudocode is presented in
Algorithm 1. We start with the monolingual graph
from the source language (line 1) and then for each
target language l ? L in turn, we add the transla-
tion edges (v
s
, t, v
l
) ? E
s,l
of each word in the
source language (lines 5-15). In order to include
the information about the translations of w in the
different target languages, each translation edge
Algorithm 1 Multilingual co-occurrence graph.
Input: target word w and its contexts C
s
monolingual graph G
s
= ?V
s
, E
s
?
set of target languages L
Output: a multilingual graph G
ML
1: G
ML
= ?V
ML
, E
ML
? ? G
s
= ?V
s
, E
s
?
2: for each l ? L
3: V
l
? ?
4: C
l
:= aligned sentences of C
s
in lang. l
5: for each v
s
? V
s
6: T
v
s
,l
:= translations of v
s
found in C
l
7: C
v
s
? C
s
:= contexts containing w and v
s
8: for each translation v
l
? T
v
s
,l
9: C
v
l
:= aligned sentences of C
v
s
in lang. l
10: T
w,C
v
l
? translation labels of w from C
v
l
11: if v
l
/? V
ML
then
12: V
ML
? V
ML
? v
l
13: V
l
? V
l
? v
l
14: for each t ? T
w,C
v
l
15: E
ML
? E
ML
? (v
s
, t, v
l
)
16: for each v
i
? V
l
17: for each v
j
? V
l
, i 6= j
18: if v
i
and v
j
co-occur in C
l
then
19: E
ML
? E
ML
? (v
i
, v
j
)
20: return G
ML
(v
s
, t, v
l
) receives a translation label t. Formally,
let C
v
s
? C
s
be the contexts where v
s
and w co-
occur, and C
v
l
the word-aligned contexts in lan-
guage l of C
v
s
, where v
s
is translated as v
l
. Then
each edge between nodes v
s
and v
l
is labeled with
a translation label t (lines 14-15): this includes a
translation of w in C
v
l
, its frequency of transla-
tion and the information of whether the transla-
tion is monosemous, as found in a multilingual
dictionary, i.e. EuroWordNet (Vossen, 1998) and
PanDictionary (Mausam et al, 2009). Finally, the
multilingual graph is further extended by inserting
all possible co-occurrence edges (v
i
, v
j
) ? E
l
be-
tween the nodes for the target language l (lines 16-
19, i.e. we apply the step from Section 3.1 to l and
C
l
). As a result of the algorithm, the multilingual
graph is returned (line 20).
3.3 Computing Root Hubs
We compute the root hubs in the multilingual
graph to discriminate the senses of the target word
in the source language. Hubs are found using the
adapted PageRank from Agirre et al (2006):
135
PR(v
i
) = (1? d) + d
?
j?deg(v
i
)
w
ij
?
k?deg(v
j
)
w
jk
PR(v
j
)
where d is the so-called damping factor (typically
set to 0.85), deg(v
i
) is the number of adjacent
nodes of node v
i
and w
ij
is the weight of the co-
occurrence edge between nodes v
i
and v
j
.
Since this step aims to induce the senses for
the target word, only nodes referring to words
in English can become root hubs. However, in
order to use additional evidence from other lan-
guages, we furthermore include in the computa-
tion of PageRank co-occurrence edges from the
target languages, as long as these occur in con-
texts with ?safe?, i.e. monosemous, translations of
the target word. Given an English co-occurrence
edge (v
s,i
, v
s,j
) and translation edges (v
s,i
, v
l,i
)
and (v
s,j
, v
l,j
) to nodes in the target language
l, labeled with monosemous translations, we in-
clude the co-occurrence edge (v
l,i
, v
l,j
) in the
PageRank computation. For instance, animal and
biotechnology are translated in German as Tier
and Biotechnologie, both with edges labeled with
the monosemous Pflanze: accordingly, we in-
clude the edge (Tier,Biotechnologie) in the com-
putation of PR(v
i
), where v
i
is either animal or
biotechnology.
Finally, following V?eronis (2004), a MST is
built with the target word as its root and the root
hubs of G
ML
forming its first level. By using a
multilingual graph, we are able to obtain MSTs
which contain translation nodes and edges.
3.4 Multilingual Disambiguation
Given a context W for the target word w in the
source language, we use the MST to find the most
relevant words in W for disambiguating w. We
first map each content word cw ? W to nodes
in the MST. Since each word is dominated by ex-
actly one hub, we can find the relevant nodes by
computing the correct hub disHub (i.e. sense) and
then only retain those nodes linked to disHub. Let
W
h
be the set of mapped content words dominated
by hub h. Then, disHub can be found as:
disHub = argmax
h
?
cw?W
h
d(cw)
dist(cw, h) + 1
where d(cw) is a function which assigns a weight
to cw according to its distance to w, i.e. the more
words occur between w and cw within W , the
smaller the weight, and dist(cw, h) is given by
the number of edges between cw and h in the
MST. Finally, we collect the translation edges of
the retained context nodes W
disHub
and we sum
the translation counts to rank each translation.
4 Results and Analysis
Experimental Setting. We submitted two runs
for the task (UHD-1 and UHD-2 henceforth).
Since we were interested in assessing the impact
of using different resources with our methodology,
we automatically built multilingual graphs from
different sentence-aligned corpora, i.e. Europarl
(Koehn, 2005) for UHD-1, augmented with the
JRC-Acquis corpus (Steinberger et al, 2006) for
UHD-2
1
. Both corpora were tagged and lemma-
tized with TreeTagger (Schmid, 1994) and word
aligned using GIZA++ (Och and Ney, 2003). For
German, in order to avoid the sparseness deriving
from the high productivity of compounds, we per-
formed a morphological analysis using Morphisto
(Zielinski et al, 2009).
To build the multilingual graph (Section 3.2),
we used a minimum frequency threshold of 2 oc-
currences for a word to be inserted as a node,
and retained only those edges with a weight less
or equal to 0.7. After constructing the multilin-
gual graph, we additionally removed those trans-
lations with a frequency count lower than 10 (7
in the case of German, due to the large amount
of compounds). Finally, the translations gener-
ated for the BEST evaluation setting were ob-
tained by applying the following rule onto the
ranked answer translations: add translation tr
i
while count(tr
i
) ? count(tr
i?1
)/3, where i is
the i-th ranked translation.
Results and discussion. The results for the
BEST and out-of-five (OOF) evaluations are pre-
sented in Tables 1 and 2 respectively. Results are
computed using the official scorer (Lefever and
Hoste, 2010) and no post-processing is applied to
the system?s output, i.e. we do not back-off to the
baseline most frequent translation in case the sys-
tem fails to provide an answer for a test instance.
For the sake of brevity, we present the results for
UHD-1, since we found no statistically significant
difference in the performance of the two systems
(e.g. UHD-2 outperforms UHD-1 only by +0.7%
on the BEST evaluation for French).
1
As in the case of Europarl, only 1-to-1-aligned sentences
were extracted.
136
Language P R Mode P Mode R
FRENCH 20.22 16.21 17.59 14.56
GERMAN 12.20 9.32 11.05 7.78
ITALIAN 15.94 12.78 12.34 8.48
SPANISH 20.48 16.33 28.48 22.19
Table 1: BEST results (UHD-1).
Language P R Mode P Mode R
FRENCH 39.06 32.00 37.00 26.79
GERMAN 27.62 22.82 25.68 21.16
ITALIAN 33.72 27.49 27.54 21.81
SPANISH 38.78 31.81 40.68 32.38
Table 2: OOF results (UHD-1).
Overall, in the BEST evaluation our system
ranked in the middle for those languages where
the majority of systems participated ? i.e. sec-
ond and fourth out of 7 submissions for FRENCH
and SPANISH. When compared against the base-
line, i.e. the most frequent translation found in
Europarl, our method was able to achieve in the
BEST evaluation a higher precision for ITALIAN
and SPANISH (+1.9% and +2.1%, respectively),
whereas FRENCH and GERMAN lie near below the
baseline scores (?0.5% and?1.0%, respectively).
The trade-off is a recall always below the base-
line. In contrast, we beat the Mode precision base-
line for all languages, i.e. up to +5.1% for SPAN-
ISH. The fact that our system is strongly precision-
oriented is additionally proved by a low perfor-
mance in the OOF evaluation, where we always
perform below the baseline (i.e. the five most fre-
quent translations in Europarl).
5 Conclusions
We presented in this paper a graph-based system
to perform CL-WSD. Key to our approach is the
use of a co-occurrence graph built from multilin-
gual parallel corpora, and the application of well-
studied graph algorithms for monolingual WSD
(V?eronis, 2004; Agirre et al, 2006). Future work
will concentrate on extensions of the algorithms,
e.g. computing hubs in each language indepen-
dently and combining them as a joint problem, as
well as developing robust techniques for unsuper-
vised tuning of the graph weights, given the obser-
vation that the most frequent translations tend to
receive too much weight and accordingly crowd
out more appropriate translations. Finally, we
plan to investigate the application of our approach
directly to multilingual lexical resources such as
PanDictionary (Mausam et al, 2009) and Babel-
Net (Navigli and Ponzetto, 2010).
References
Eneko Agirre, David Mart??nez, Oier L?opez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art WSD. In Proc. of EMNLP-06,
pages 585?593.
Sergey Brin and Lawrence Page. 1998. The
anatomy of a large-scale hypertextual web search
engine. Computer Networks and ISDN Systems,
30(1?7):107?117.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit X.
Els Lefever and Veronique Hoste. 2010. SemEval-
2010 Task 3: Cross-lingual Word Sense Disam-
biguation. In Proc. of SemEval-2010.
Mausam, Stephen Soderland, Oren Etzioni, Daniel
Weld, Michael Skinner, and Jeff Bilmes. 2009.
Compiling a massive, multilingual dictionary via
probabilistic inference. In Proc. of ACL-IJCNLP-
09, pages 262?270.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a very large multilingual seman-
tic network. In Proc. of ACL-10.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing (NeMLaP ?94), pages 44?49.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma?z Erjavec, Dan Tufis?, and
D?aniel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proc. of LREC ?06.
Jean V?eronis. 2004. Hyperlex: lexical cartography
for information retrieval. Computer Speech & Lan-
guage, 18(3):223?252.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Kluwer, Dordrecht, The Netherlands.
Andrea Zielinski, Christian Simon, and Tilman Wittl.
2009. Morphisto: Service-oriented open source
morphology for German. In State of the Art in Com-
putational Morphology, volume 41 of Communica-
tions in Computer and Information Science, pages
64?75. Springer.
137
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 93?101,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Assessing the Challenge of Fine-Grained
Named Entity Recognition and Classification
Asif Ekbal, Eva Sourjikova, Anette Frank and Simone Paolo Ponzetto
Department of Computational Linguistics
Heidelberg University, Germany
{ekbal,sourjikova,frank,ponzetto}@cl.uni-heidelberg.de
Abstract
Named Entity Recognition and Classi-
fication (NERC) is a well-studied NLP
task typically focused on coarse-grained
named entity (NE) classes. NERC for
more fine-grained semantic NE classes has
not been systematically studied. This pa-
per quantifies the difficulty of fine-grained
NERC (FG-NERC) when performed at
large scale on the people domain. We
apply unsupervised acquisition methods
to construct a gold standard dataset for
FG-NERC. This dataset is used to bench-
mark methods for classifying NEs at var-
ious levels of fine-grainedness using clas-
sical NERC techniques and global contex-
tual information inspired fromWord Sense
Disambiguation approaches. Our results
indicate high difficulty of the task and pro-
vide a ?strong? baseline for future research.
1 Introduction
Named Entity Recognition and Classification (cf.
Nadeau and Sekine (2007)) is a well-established
NLP task relevant for nearly all semantic process-
ing and information access applications. NERC
has been investigated using supervised (McCallum
and Li, 2003), unsupervised (Etzioni et al, 2005)
and semi-supervised (Pas?ca et al, 2006b) learning
methods. It has been investigated in multilingual
settings (Tjong Kim Sang, 2002; Tjong Kim Sang
and De Meulder, 2003) and special domains, e.g.
biomedicine (Ananiadou et al, 2004).
The classical NERC task is confined to coarse-
grained named entity (NE) classes established
in the MUC (MUC-7, 1998) or CoNLL (Tjong
Kim Sang, 2002) competitions, typically PERS,
LOC, ORG, MISC. While most recent work con-
centrates on feature engineering and robust statis-
tical models for various domains, few researchers
addressed the problem of recognizing and catego-
rizing fine-grained NE classes (such as biologist,
composer, or athlete) in an open-domain setting.
Fine-grained NERC is expected to be benefi-
cial for a wide spectrum of applications, includ-
ing Information Retrieval (Mandl and Womser-
Hacker, 2005), Information Extraction (Pas?ca et
al., 2006a) or Question-Answering (Pizzato et
al., 2006). However, manually compiling wide-
coverage gazetteers for fine-grained NE classes is
time-consuming and error-prone. Also, without an
extrinsic evaluation, it is difficult to define a priori
which classes are relevant for a particular domain
or task. Finally, prior research in FG-NERC is dif-
ficult to evaluate, due to the diversity of NE classes
and datasets used.
Accordingly, in the interest of a general ap-
proach, we address the challenge of capturing a
broad range of NE classes at various levels of con-
ceptual granularity. By turning FG-NERC into
a widely applicable task, applications are free to
choose relevant NE categories for specific needs.
Also, establishing a gold standard dataset for this
task enables comparative benchmarking of meth-
ods. However, the envisaged task is far from triv-
ial, given that the set of possible semantic classes
for a given NE comprises the full space of NE
classes, whereas descriptive nouns may be am-
biguous between a fixed set of meanings only.
The paper aims to establish a general frame-
work for FG-NERC by addressing two goals: (i)
we automatically build a gold standard dataset of
NE instances classified in context with fine-grain-
ed semantic class labels; (ii) we develop strong
baseline methods, to assess the aptness of standard
NLP approaches for this task. The two efforts are
strongly interleaved: a standardized dataset is not
only essential for (comparative) evaluation, but
also a prerequisite for classification approaches
based on supervised learning, the most successful
techniques for sequential labeling problems.
93
2 Related work
An early approach to FG-NERC is Alfonseca and
Manandhar (2002), who identify it as a problem
related to Word Sense Disambiguation (WSD).
They jointly address concept hierarchy learning
and instance classification using topic signatures,
yet the experiments are restricted to a small on-
tology of 9 classes. Similarly, Fleischman and
Hovy (2002) extend previous work from Fleis-
chman (2001) on locations and address the ac-
quisition of instances for 8 fine-grained person
classes. For supervised training they compile a
web corpus which is filtered using high-confident
classifications from an initial classifier trained on
seeds. Due to the limitations of their method to
create a good sample of training data, the perfor-
mance could not be generalized to held-out data.
Recent work takes the task of FG-NERC one
step further by (i) extending the number of classes,
(ii) relating them to reference concept hierar-
chies and (iii) exploring methods for building
training and evaluation data, or applying weakly
and unsupervised learning based on high-volume
data. Tanev and Magnini (2006) selected 10 NE-
subclasses of person and location using Word-
Net as a reference. Datasets were automatically
acquired and manually filtered. They compare
word and pattern-based supervised and a semi-
supervised approach based on syntactic features.
Giuliano & Gliozzo (2007, 2008) perform NE
classification against the People Ontology, an ex-
cerpt of the WordNet hierarchy, comprising 21
people classes populated with at least 40 instances.
Using minimally supervised lexical substitution
methods, they cast NE classification as an ontol-
ogy population task ? as opposed to recognition
and classification in context. In a similar setting,
Giuliano (2009) explores semi-supervised classifi-
cation of the People Ontology classes using latent
semantic kernels, comparing models built from
Wikipedia and from a news corpus. In a differ-
ent line of research Pas?ca (2007) and Pas?ca and
van Durme (2008) make use of query logs to ac-
quire NEs on a large scale. While Pas?ca (2007)
extracts NEs for 10 target classes, Pas?ca and van
Durme (2008) combine web query logs and web
documents to acquire both NE-concept pairs and
concept attributes using seeds.
But while these more recent approaches all of-
fer substantially novel contributions for many NE
acquisition subtasks, none of them addresses the
full task of FG-NERC, i.e., recognition and clas-
sification of NE tokens in context. Compared to
ontology population, focusing on types, classifica-
tion in raw texts needs to consider any token and
cannot rely on special contexts offering indicative
clues for class membership.
Bunescu and Pas?ca (2006) also perform dis-
ambiguation and classification of NEs in context,
yet in a different setup. Disambiguation is per-
formed into one of the known possible classes
for a NE, as determined from Wikipedia disam-
biguation pages. Contexts for training and testing
are acquired from Wikipedia pages, as opposed
to general text. Disambiguation is performed us-
ing vectors of co-occurring terms and a taxonomy-
based kernel that integrates word-category corre-
lations. Evaluation is performed on the task of
predicting, for a given NE in a Wikipedia page
context, the correct class from among its known
classes, including one experiment that included
10% of out-of-Wikipedia entities. The category
space was confined to People by occupation, with
8,202 subclasses. Classification considered 110
broad classes, 540 highly populated classes (w/o
out-of-Wikipedia entities), and 2,847 classes in-
cluding less populated ones. This setup is diffi-
cult to compare given the sense granularities em-
ployed and the special Wikipedia text genre. Even
though classification is performed in context, the
task does not evaluate recognition.
To summarize, the field has developed robust
methods for acquisition and fine-grained classifi-
cation of NEs on a large scale. But, the full task
of NE recognition and classification in context still
remains to be addressed for a wide-coverage, fine-
grained semantic class inventory that can serve as
a common benchmark for future research.
3 Fine-grained NERC on a large-scale
We present experiments that assess the difficulty
of open-domain FG-NERC pursued at a large
scale. We concentrate on instances and classes re-
ferring to people, since it is a well-studied domain
(see Section 2) and structured fine-grained infor-
mation can be readily applied to a well-defined
end-user task such as IR, cf. the Web People
Search task (Artiles et al, 2008). Our method
is general in that it requires only a (PoS tagged
and chunked) corpus and a reference taxonomy
to provide a concept hierarchy. Given a map-
ping between automatically extracted class labels
94
and concepts in a taxonomic resource, it can be
further extended to other domains, e.g. locations
or the biomedical domain by leveraging open-
domain taxonomies such as Yago (Suchanek et
al., 2008) or WikiTaxonomy (Ponzetto and Strube,
2007). The contribution of this work is two-fold:
(i) We develop an unsupervised method for ac-
quiring a comprehensive dataset for FG-NERC by
applying linguistically motivated patterns to a cor-
pus harvested from the Web (Section 4). Large
amounts of NEs are acquired together with their
contexts of occurrence and with their fine-grained
class labels which are mapped to synsets in Word-
Net. The controlled sense inventory and the tax-
onomic structure offered by WordNet enables an
evaluation of FG-NERC performance at different
levels of concept granularity, as given by the depth
at which the concepts are found. As our extraction
patterns reflect a wide-spread grammatical con-
struct, the method can be applied to many lan-
guages and extended to other domains.
(ii) Given this automatically acquired dataset,
we assess the problem of FG-NERC in a sys-
tematic series of experiments, exploring the per-
formance of NERC methods on different levels
of granularities. For recognition and classifica-
tion we apply standard sequential labeling tech-
niques ? i.e. a Maximum Entropy (MaxEnt) tag-
ger (Section 5.1) ? which we adapt to this hier-
archical classification problem (Section 5.2). To
test the hypothesis of whether a sequential la-
beler represents a valid choice to perform FG-
NERC, we compare the latter to a MaxEnt system
trained on a more semantically informed feature
set, and a gloss-overlap method inspired by WSD
approaches (Section 5.3).
4 Acquisition of a FG-NERC dataset
We present an unsupervised method that simul-
taneously acquires NEs, their semantic class and
contexts of occurrence from large textual re-
sources. In order to develop a clean resource of
properly disambiguated NEs, we develop acqui-
sition patterns for a grammatical construction that
unambiguously associates proper names with their
corresponding semantic class.
Pattern-based extraction of NE-concept pairs.
NEs are often introduced by so-called apposi-
tional structures as in (1), which overtly ex-
press which semantic class (here, painter) the NE
(Kandinsky) belongs to. Appositions involving
proper names can be captured by extraction pat-
terns as given in (2).
(1) . . . writings of the abstract painter Kandinsky
frequently explored similarities between . . .
(2) a. [the|The]? [JJ|NN]* [NN] [NP]
the abstract painter Kandinsky
b. [NP] [,]? [a|an|the]* [JJ|NN]* [NN]
W. Kandinsky, a Russian-born painter, ..
Contexts like (2.a) provide a less noisy se-
quence for extraction, due to the class and instance
labels being adjacent ? in contrast to (2.b) where
any number of modifiers can intervene between
the two. Accordingly, we apply in our experiments
only a restricted version of (2.a) ? with a deter-
miner ? to UKWAC, an English web-based cor-
pus (Baroni et al, 2009) that comes in a cleaned,
PoS-tagged and lemmatized form. Due to its size
(>2 billion tokens) and mixed genres, the corpus
is ideally suited for acquiring large quantities of
NEs pertaining to a broad variety of open-domain
semantic classes.
Filtering heuristics. The apposition patterns are
subject to noise, due to PoS-tagging errors, as
well as special constructions, e.g. reduced rela-
tive clauses. The former can be controlled by fre-
quency filters, the latter can be circumvented by
using chunk boundary information1. A more chal-
lenging problem is to recognize whether an ex-
tracted nominal is in fact a valid semantic class for
NEs. Besides, class labels can be ambiguous, so
there is uncertainty as to which class an extracted
entity should be assigned to. We apply two fil-
tering strategies: we set a frequency threshold ft
on the number of extracted NE tokens per class,
to remove infrequent class label extractions; we
then filter invalid semantic classes using informa-
tion from WordNet: given the WordNet PERSON
supersense, i.e. the lexicographer file for nouns de-
noting people, we check whether the first sense of
the class label candidate is found in PERSON.
Mapping to the WordNet person domain. In
order to perform a hierarchical classification of
people, we need a taxonomy for the domain at
hand. We achieve this by mapping the extracted
class labels to WordNet synsets. In our setting, we
map against all synsets found under person#n#1,
1We use YamCha (Kudo and Matsumoto, 2000) to per-
form phrase chunking.
95
which are direct hypernyms of at least one in-
stance in WordNet (CWN pers+Inst).2 Since our
goal is to map class labels to synsets (i.e. our fu-
ture NE classes), we check each class label candi-
date against all synonyms contained in the synset.
At this point we have to deal with two cases: two
extracted class label candidates (synonyms such
as doctor, physician) will map to a single synset,
while ambiguous class labels (e.g. director) can be
mapped to more than one synset. In the latter case,
we heuristically choose the synset which domi-
nates the highest number of instances in WordNet.
Mapping evaluation. We evaluated the cover-
age of our mapping for two sets of class labels
extracted for two different frequency thresholds:
ft = 40 and ft = 1. With ft = 40, we cover
31.1% of the synsets found under person#n#1 in
WordNet, i.e. the set of classes CWN pers+Inst;
conversely, 45.8% of the extracted class labels can
be successfully mapped to CWN pers+Inst. For
threshold ft = 1, we are able to map to 87.9%
of CWN pers+Inst, with only 20.1% of extracted
classes mapped to CWN pers+Inst. For the re-
maining 79.9% of class labels (e.g. goalkeeper,
chancellor, superstar) that have no instances in
WordNet, we manually inspected 20 classes, in 20
contexts each, and established that 76% of them
are appropriate NE person classes.
For threshold ft = 40, we obtain 153 class labels
which are mapped to 146 synsets. Ten class labels
are mapped to more than one synset. Using our
mapping heuristic based on the majority instance
class, we successfully disambiguate all of them.
However, since we only map to CWN pers+Inst,
we introduce errors for 5 classes. E.g. ?manager?
incorrectly gets mapped to manager#n#2, since
the latter is the only synset containing instances.
For these cases we manually corrected the auto-
matic mapping.
A taxonomy for FG-NERC. We create our gold
standard taxonomy of semantic classes by start-
ing with the 146 synsets obtained from the map-
ping, including the 5 classes that were manually
corrected. Since we concentrate on the people
domain, we additionally remove 5 classes that
can refer to other domains as well (e.g. carrier,
guide). Given the remaining 141 synsets, we se-
lect the portion of WordNet rooted at person#n#1
2We use WordNet version 3.0. With w#p#i we denote the
i-th sense of a wordw with part of speech p. E.g., person#n#1
is defined as { person, individual . . .}).
Level #C #C w/inst #inst #inst/C % of inst
1 1 0 0 - -
2 29 8 2,662 332 5.49
3 57 37 18,229 493 37.58
4 63 46 18,422 401 37.94
5 37 30 6,231 208 12.84
6 18 13 2,366 182 4.88
7 6 5 423 85 0.87
8 2 2 179 90 0.36
all 213 141 48,512 344 100
Table 1: Level-wise statistics of classes and in-
stances across the FG-NERC person taxonomy.
which contains them, together with any interven-
ing synset found along the WordNet hierarchy.
Given this WordNet excerpt, the extracted NE to-
kens are then appended to the respective synsets in
the hierarchy. Statistics of the resulting WordNet
fragment augmented with instances are given in
Table 1. The taxonomy has a maximum depth of 8,
and contains 213 synsets, i.e. NE classes (see col-
umn 2). 83.5% of the 31,819 extracted instances
(type-level) sit in leaf nodes. The classes automat-
ically refer back to the acquired appositional con-
texts. Table 1 gives statistics about the number of
instances (token-level) acquired for classes at dif-
ferent embedding levels. In total we have at our
disposal 48,512 instances (token-level) in apposi-
tional contexts. The type-token ratio is 1.52.
Gold standard validation. To create a gold
standard dataset of entities in context labeled with
fine-grained classes, we first randomly select 20
classes, as well as an additional 18 which are
also found in the People Ontology (Giuliano and
Gliozzo, 2008). For each class, we randomly se-
lect 40 occurrences of instances in context, i.e.
the words co-occurring in a window of 60 tokens
before and after the instance. We asked four an-
notators to label these extractions for correctness,
and to provide the correct label for the incorrect
cases, if one was available. Only 52 contexts out
of 1520 were labeled as incorrect, thus giving us
96.58% accuracy on our automatically extracted
data. The manually validated dataset is used to
provide a ground-truth for FG-NERC. However,
the noun (e.g. hunter) denoting the NE class is re-
moved from these contexts for training and testing
in all experiments. This is because, due to the ex-
traction method based on POS-patterns denoting
appositions, class labels are known a priori to oc-
cur in the context of an instance and thus identify
them with high precision.
96
5 Methodology for FG-NERC
We develop methods to perform FG-NERC using
standard techniques developed for coarse-grained
NERC and WSD. These are applied to our dataset
from Section 4, in order to measure performance
at different levels of semantic class granularity, i.e.
corresponding to the depth of the semantic classes
found in our WordNet fragment. We start in Sec-
tion 5.1 to present a Maximum Entropy model to
perform coarse-grained NERC and we extend it
to perform multiclass classification in a hierarchi-
cal taxonomy (Section 5.2). We then present in
Section 5.3 an alternative proposal to perform FG-
NERC using global context information, as found
in state-of-the-art approaches to supervised and
unsupervised WSD.
5.1 NERC using a MaxEnt tagger
Our baseline system is modeled following a Maxi-
mum Entropy approach (Bender et al, 2003, inter
alia). The MaxEnt model produces a probability
for each class label t (the NE tag) of a classifica-
tion instance, conditioned on its context of occur-
rence h. This probability is calculated by:
P (t|h) =
1
Z(h)
exp
?
?
n?
j=1
?jfj(h, t)
?
? (1)
where fj(h, t) is the j-th feature with associated
weight ?j and Z(h) is a normalization constant to
ensure a proper probability distribution.3 Given a
word wi to be classified as Beginning, Inside or
Outside (IOB) of a NE, we extract as features:
1. Context words. The words occurring within
the context window wi+2i?2 = wi?2 . . . wi+2.
2. Word prefix and suffix. Word prefix and suffix
character sequences of length up to n.
3. Infrequent word. A feature that fires if wi oc-
curs in the training set less frequently than a
given threshold (i.e. below 10 occurrences).
4. Part-of-Speech (PoS) and chunk informa-
tion. The PoS and chunk labels of wi.
5. Capitalization. A binary feature that checks
whether wi starts with a capital letter or not.
6. Word length. A binary feature that fires if
the length of wi is smaller than a pre-defined
threshold (i.e. less than 5 characters).
3In our implementation, we use the OpenNLP MaxEnt li-
brary (http://maxent.sourceforge.net).
7. Digit and symbol features. Three features
check whether wi contains digit strings, non-
characters (e.g. slashes) or number expressions.
8. Dynamic feature. The tag ti?1 of the word
wi?1 preceding wi in the sequence wn1 .
5.2 MaxEnt extension for FG-NERC
Extension to hierarchical classification. We
apply our baseline NERC system to FG-NERC.
Given a word in context, the task consists of recog-
nizing it as a NE, and classifying it into the appro-
priate semantic class from our person taxonomy.
We approach this as a hierarchical classification
task by generating a binary classifier4 with sepa-
rate training and test sets for each node in the tree.
To perform level-wise classification from coarse
to fine-grained classes, we need to adjust the class
labels and their corresponding training and test in-
stances for each experiment. For classification at
the deepest level, each concept contains the in-
stances of the original dataset. For classification
at higher levels we leverage the semantics of the
WordNet hyponym relations and expand the set
of target classes (i.e. synsets) of a given level to
contain all instances of hyponym synsets. Given
a set I of classification instances for a given tar-
get class c, we add all instances labeled with the
hyponyms of c to I . All other instances (not in
that subtree) are labeled as being Outside (O-) a
NE. This approach ensures that, for each node, the
dataset contains two classes (NE and O) only, and
implicitly ?propagates? the instances up the tree.
As a result, non-leaf nodes that did not have any
instance in the original dataset become populated.
Also, the classification of classes at higher levels
is based on larger datasets.
Extension to multiclass classification. Since
we train a binary classifier for each node of the
tree, we apply two methods to infer multiclass de-
cisions from these binary classifiers, namely level-
wise and global multiclass classification. In both
paradigms, we combine the single decisions of
the individual classifiers with the winner-takes-all
strategy, using weighted voting. The weights are
calculated based on the confidence value for the
corresponding class, i.e., its conditional probabil-
ity according to Equation (1). The output label is
selected randomly in case of ties.
4The IOB tagging scheme normally assigns three different
labels, i.e. Inside (I-), Outside (O-) and Beginning (B-) of
a chunk. However, our dataset does not have any instance
labeled as B-, since it does not contain any adjacent NEs.
97
For level-wise classification, we combine only
classifiers at the same level of embedding. Given
n concepts at level l, we have n possible out-
put labels for each word. The output label for a
classification instance is determined by the highest
weighted vote among all binary classifiers at level
l. For global classification we combine all binary
classifiers of the entire tree using weighted voting
to determine the winning class label. The weights
are calculated based on the product of confidence
value and depth of the corresponding class in the
tree.
5.3 FG-NERC using global contexts
FG-NERC is a more demanding task than ?classi-
cal? NERC, due to the larger amount of classes,
the paucity of examples for each class, and the
increasingly subtle semantic differences between
these classes. For such a task contextual informa-
tion is expected to be very informative ? e.g. if an
entity co-occurs in context with ?Nobel prize?, this
provides evidence that it is likely to be a scien-
tist or scholar. However, the context window used
by our baseline MaxEnt tagger is very local, in-
cluding at most the two preceding and succeeding
words. Hence, the classifier is not able to capture
informative contextual clues in a larger context.
Previous work has related FG-NERC to WSD
approaches (Alfonseca and Manandhar, 2002).
Accordingly, we investigate two context-sensitive
approaches inspired from WSD proposals, which
consider a more global context for classification.
We first define a new feature set to induce a new
MaxEnt model (MaxEnt-B) which only uses lexi-
cal features from a larger context window, as used
in standard supervised WSD (Lee and Ng, 2002):
1. PoS context. The part-of-speech occur-
ring within the context window posi+3i?3 =
posi?3 . . . posi+3.
2. Local collocation. Local collocations Cnm sur-
rounding wi. We use C?2,?1 and C1,2.
3. Content words in surrounding context. We
consider all unigrams in contexts wi+3i?3 =
wi?3 . . . wi+3 of wi (crossing sentence bound-
aries) for the entire training data. We convert to-
kens to lower case, remove stopwords, numbers
and punctuation symbols. We define a feature
vector of length 10 using the 10 most frequent
content words. Given a classification instance,
the feature corresponding to token t is set to 1
iff the context wi+3i?3 of wi contains t.
In addition, we use a Lesk-like method (Lesk,
1986) which labels instances in context with the
WordNet synset whose gloss has the maximum
overlap with the glosses of the senses of its words
in context. Given the small context provided by
theWordNet glosses, we follow Banerjee and Ped-
ersen (2003) and expand these to also include the
words from the glosses of the hypernym and hy-
ponym synsets.
6 Experiments
6.1 Benchmarking on coarse-grained NERC
We benchmark the performance of our baseline
MaxEnt classifier using the feature set from Sec-
tion 5.1 (MaxEnt-A henceforth) on the CoNLL-
2003 shared task dataset (Tjong Kim Sang and
De Meulder, 2003), the de-facto standard for eval-
uating coarse-grained NERC systems.
In MaxEnt modeling, feature selection is a cru-
cial problem and key to improving classification
performance. MaxEnt, however, does not provide
methods for automatic feature selection. We there-
fore experimented with various combinations of
features standardly used for NERC (1-8 of Section
5.1). Model parameters are computed with 200
iterations without feature frequency cutoff. The
best configuration is found by optimizing the F1
measure on the development data with various fea-
ture representations. The chosen features are: 1, 2
(with n = 3), 4, 5, 6, 7 and 8. Evaluation on the
test set is performed blindly, using this feature set.
The results are presented in Table 2.
TheMaxEnt labeler achieves performance com-
parable with the CoNLL-2003 task participants,
ranking 12th among the 16 systems participating
in the task, with a 2 point margin off the F1 of the
most similar system of Bender et al (2003) and
7 points below the best-performing system (Flo-
rian et al, 2003). The former used a relatively
complex set of features and different gazetteers
extracted from unannotated data. The latter com-
bined four diverse classifiers, namely a robust lin-
ear classifier, maximum entropy, transformation-
based learning and a hidden Markov model. They
used different feature sets, unannotated data and
an additional NE tagger. In comparison, our
NERC system is simpler and based on a small set
of features that can be easily obtained for many
languages. Besides, it does not make use of any
external resources and still shows state-of-the-art
performance on the overall data.
98
Recall Precision F?=1
PER 83.02% 81.40% 82.21%
LOC 88.47% 88.19% 88.23%
ORG 77.20% 68.03% 72.23%
MISC 81.20% 83.92% 82.54%
Overall 83.11% 80.47% 81.77%
Table 2: Results on the CoNLL-2003 test data.
Set # tokens # NEs
Training 2,431,041 38,810
Development 478,871 9,702
Test 181,490 1,520
Table 3: Statistics for training, dev and test sets.
6.2 Evaluating FG-NERC
Experimental setting. For the task of FG-
NERC, we compare the performance of MaxEnt-
A with the MaxEnt-B model from Section 5.3 and
the Lesk method. The data is partitioned into train-
ing and development sets by randomly selecting
80%-20% of the contexts in which the NEs occur.
We use the held-out, manually validated gold stan-
dard from Section 4 for blind evaluation. Statistics
for the dataset are reported in Table 3.
We build a MaxEnt model for each FG-NE
class, using the features that performed best on
the CoNLL task, except the digit and dynamic
NE features (MaxEnt-A), and context features 1-
3 of Section 5.3 (MaxEnt-B). Model parameters
are computed in the same way as for coarse-
grained NERC. Table 3 shows that our training set
is highly unbalanced. The ratio between positive
(NEs) and negative examples (i.e. O classification
instances) at the topmost level is 63:1. Also, with
increasing levels of fine-grainedness, the number
of negative (-O) NE classes is increasing for each
binary classifier. We observed on the develop-
ment set that this skewed distribution heavily bi-
ases the classifiers towards the negative category,
and accordingly investigated sampling techniques
to make the ratio of positive and negative examples
more balanced. We experiment with a sampling
strategy that over-samples the positive examples
and under-samples the negative ones. We define
various ratios of over-sampling depending upon
the number of positive examples in the original
training set. Table 4 lists the factors (f ) of over-
sampling applied to the original positive samples
(P ), with minimum and maximum sizes of the ob-
factor f size of P min P ? max P ?
20 ? P 1 ? 2K 20 40K
15 ? P 2K ? 5K 30K 75K
10 ? P 5K ? 10K 50K 100K
5 ? P 10K ? 20K 50K 100K
2 ? P 20K ? 50K 40K 100K
P 50K ? . . . 50K >50K
Table 4: Oversampling of positive samples.
MaxEnt-A MaxEnt-BLevel
R P F1 R P F1
1 98.7 85.0 91.4 95.1 83.0 88.6
2 96.0 65.5 77.9 48.1 46.3 47.2
3 95.3 54.3 69.3 43.3 41.1 42.2
4 86.8 52.8 65.6 41.1 37.2 39.1
5 90.4 45.9 60.9 49.2 21.5 29.9
6 91.6 36.9 52.6 51.7 13.2 21.1
7 89.5 31.8 46.9 42.2 10.2 16.4
8 100.0 19.9 66.7 87.1 8.1 14.7
global 85.1 43.2 57.3 61.9 26.6 37.2
hierarchical 87.7 44.8 59.4 64.5 29.5 40.5
Table 6: Level-wise NE recognition & classifica-
tion evaluation (in %).
tained oversampled sets P ? for different ranges of
original sizes of P .5 Oversampling is done with-
out replacement. The number of negative instan-
ces is always downsampled on the basis of P ? to
yield a 1:5 ratio of positive and negative samples,
a ratio we estimated from the CoNLL-2003 data.
Level-wise evaluation results on the FG-NE
classification-only (NEC) task for the MaxEnt
classifiers and Lesk are given in Table 5. Table
6 reports results for the evaluation of the MaxEnt
model performing both classification and recog-
nition. As for coarse-grained NERC, we evaluate
using the standard metrics of recall (R), precision
(P) and balanced F-measure (F1). As baseline, we
use a majority class assignment ? i.e. at each level,
we label all instances with the most frequent class
label. For global FG-NE classification, reported in
Table 5, the original fine-grained classes are con-
sidered, across the entire class hierarchy. Global
evaluation is performed by counting exact label
predictions on the entire hierarchy (global) and us-
ing the evaluation metric of Melamed and Resnik
(2000, hierarchical). As baseline we assume the
most frequent class label in the training set.
Discussion. All methods perform reasonably
well, indicating the feasibility of the task. For the
MaxEnt models, Table 5 shows a general high re-
call and decreasing precision as we move down the
hierarchy. Degradation in the overall F1 score is
5Sampling ratios are determined on the development set.
99
Baseline MaxEnt-A MaxEnt-B LeskLevel
R P F1 R P F1 R P F1 R P F1
1 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
2 28.4 25.9 27.1 85.8 88.6 87.0 79.5 84.9 82.2 16.4 19.7 17.9
3 27.9 23.1 25.2 83.9 88.1 85.9 75.5 79.8 77.5 16.2 16.2 16.2
4 18.8 20.4 19.5 74.6 85.0 79.5 65.4 71.3 68.2 11.3 11.3 11.3
5 25.8 19.0 21.9 78.8 83.4 80.9 78.6 74.1 76.3 13.5 14 13.8
6 24.7 7.8 11.9 88.5 73.6 80.4 78.7 74.1 75.7 33.2 37.5 35.2
7 19.1 5.34 8.3 79.2 76.5 77.8 78.1 72.7 75.3 49.4 49.4 49.4
8 34.2 2.9 5.5 82.8 73.8 78.1 81.1 71.1 75.8 0.1 0.1 0.1
global 34.6 18.5 24.1 81.1 84.2 82.6 78.0 74.2 76.6 36.5 38.6 37.5
hierarchical 33.0 21.2 25.8 83.5 86.2 84.8 78.2 77.8 78.1 36.6 38.7 37.6
Table 5: Level-wise evaluation of fine-grained NE classification techniques (in %).
given by the increasingly limited amount of class
instances found towards the low regions of the tree
(down to an average of 85 and 90 instances per
class for levels 7 and 8, respectively) (cf. Table 1).
The ?classical? feature set (MaxEnt-A) yields bet-
ter performance compared to the semantic feature
set (MaxEnt-B). However MaxEnt-B still achieves
a respectable performance, given that it contains a
few semantic features only.
The MaxEnt classifiers achieve a far better per-
formance than Lesk. This is in-line with previ-
ous findings in WSD, namely unsupervised fine-
grained disambiguation methods rarely perform-
ing above the baseline, and suggests that Lesk can
be merely used as a ?strong? baseline. Error anal-
ysis showed that it performs poorly due to the lim-
ited context provided by the WordNet glosses, and
the limited impact of gloss expansions deriving
from the low connectivity between synsets.
Comparison of Tables 5 and 6 shows that per-
formance decreases considerably for a classifier
that not only assigns fine-grained classes, but also
detects which tokens actually are NEs. As for
the classification-only task, the performance de-
creases as one moves to lower levels. This in-
dicates that the complexity of the task is propor-
tional to the fine-grainedness of the class inven-
tory. MaxEnt-B, lacking ?classical? NER features,
shows dramatic losses, compared to MaxEnt-A.
Comparison to other work. We compared the
performance of our system based on global classi-
fication (one vs. rest) against the figures reported
for individual categories in Giuliano (2009). The
MaxEnt-A system compares favorably, although it
considers (i) more classes at each level ? i.e. 213
vs. 21 ? and (ii) classifies NEs at finer-grained lev-
els ? i.e. 8 vs. 4 maximum depth in the respec-
tive WordNet fragments. We achieve overall mi-
cro average R, P and F1 values of 87.5%, 85.7%
and 86.6%, respectively, compared to Giuliano?s
79.6%, 80.9% and 80.2%. Due to the different se-
tups and data used, these figures do not offer a ba-
sis for true comparison. However, the figures sug-
gest that our system achieves respectable perfor-
mance on a more complex classification problem.
7 Conclusions
We presented a method to perform FG-NERC on
a large scale. Our contribution lies in the def-
inition of a benchmarking setup for this task in
terms of gold standard datasets and strong base-
line methods provided by a MaxEnt classifier. We
proposed a pattern-based approach for the acqui-
sition of fined-grained NE semantic classes and
instances. This corpus-based method relies only
on the availability of large text corpora, such as
the WaCky corpora, in contrast to resources diffi-
cult to obtain, such as query-logs (Pas?ca and van
Durme, 2008). It makes use of a very large Web
corpus to extract instances from open-domain con-
texts ? in contrast to standard NERC approaches,
which are tailored for newswire data and do not
generalize well across domains. Our gold stan-
dard training and test datasets are currently based
only on appositional patterns6. Therefore, it does
not include the full spectrum of constructions in
which instances can be found in context. Future
work will investigate semi-supervised and heuris-
tics (e.g. ?one sense per discourse?) methods to ex-
pand the data with examples from follow-up men-
tions, e.g. co-occurring in the same document.
Our MaxEnt models still perform very local
classification decisions, relying on separate mod-
els for each semantic class. We accordingly plan to
explore both global models operating on the over-
all hierarchy, and more informative feature sets.
6The data are available for research purposes at http:
//www.cl.uni-heidelberg.de/fgnerc.
100
References
Enrique Alfonseca and Suresh Manandhar. 2002.
An unsupervised method for general named entity
recognition and automated concept discovery. In
Proc. of GWC-02.
S. Ananiadou, C. Friedman, and J.I. Tsujii. 2004.
Special issue on named entity recognition in
biomedicine. Journal of Biomedical Informatics,
37(6).
Javier Artiles, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search. In Proc. of LREC ?08.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlap as a measure of semantic relatedness.
In Proc. of IJCAI-03, pages 805?810.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
A collection of very large linguistically processed
web-crawled corpora. Journal of Language Re-
sources and Evaluation, 43(3):209?226.
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named entity
recognition. In Proc. of CoNLL-03, pages 148?151.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proc. of EACL-06, pages 9?16.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91?134.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proc. of
COLING-02, pages 1?7.
Michael Fleischman. 2001. Automated subcategoriza-
tion of named entities. In Proc. of the ACL 2001
Student Workshop.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Proc. of CoNLL-
03, pages 168?171.
Claudio Giuliano and Alfio Gliozzo. 2007. Instance
based lexical entailment for ontology population. In
Proc. of ACL-07, pages 248?256.
Claudio Giuliano and Alfio Gliozzo. 2008. Instance-
based ontology population exploiting named-entity
substitution. In Proc. of COLING-ACL-08, pages
265?272.
Claudio Giuliano. 2009. Fine-grained classification of
named entities exploiting latent semantic kernels. In
Proc. of CoNLL-09, pages 201?209.
Taku Kudo and Yuji Matsumoto. 2000. Use of Support
Vector Machines for chunk identification. In Proc.
of CoNLL-00, pages 142?144.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning
algorithms for word sense disambiguation. In Proc.
of EMNLP-02, pages 41?48.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings
of the ACL-SIGDOC Conference, pages 24?26.
Thomas Mandl and Christa Womser-Hacker. 2005.
The effect of named entities on effectiveness in
cross-language information retrieval evaluation. In
Proc. of ACM SAC 2005, pages 1059?1064.
Andrew McCallum and Andrew Li. 2003. Early re-
sults for named entity recognition with conditional
random fields, features induction and web-enhanced
lexicons. In Proc. of CoNLL-03, pages 188?191.
Dan Melamed and Philip Resnik. 2000. Tagger evalu-
ation given hierarchical tag sets. Computers and the
Humanities, pages 79?84.
MUC-7. 1998. Proceedings of the Seventh Mes-
sage Understanding Conference (MUC-7). Morgan
Kaufmann, San Francisco, Cal.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Journal
of Linguisticae Investigationes, 30(1).
Marius Pas?ca and Benjamin van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proc. of ACL-08, pages 19?27.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006a. Names and similarities on the web: Fact ex-
traction in the fast lane. In Proc. of COLING-ACL-
06, pages 809?816.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006b. Organizing and
searching the world wide web of facts ? Step one:
The one-million fact extraction challenge. In Proc.
of AAAI-06, pages 1400?1405.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proc. of
CIKM-2007, pages 683?690.
Luiz Augusto Pizzato, Diego Molla, and Ce?cile Paris.
2006. Pseudo relevance feedback using named enti-
ties for question answering. In Proc. of ALTW-2006,
pages 83?90.
Simone Paolo Ponzetto andMichael Strube. 2007. De-
riving a large scale taxonomy from Wikipedia. In
Proc. of AAAI-07, pages 1440?1445.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A Large Ontology from
Wikipedia and WordNet. Elsevier Journal of Web
Semantics, 6(3):203?217.
Hristo Tanev and Bernardo Magnini. 2006. Weakly
supervised approaches for ontology population. In
Proc. of EACL-06, pages 17?24.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-independent Named Entity Recog-
nition. In Proc. of CoNLL-03, pages 127?132.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-independent
Named Entity Recognition. In Proc. of CoNLL-02,
pages 155?158.
101
Proceedings of the 25th International Conference on Computational Linguistics, pages 68?73,
Dublin, Ireland, August 23-29 2014.
Weakly supervised construction of a repository of iconic images
Lydia Weiland and Wolfgang Effelsberg and Simone Paolo Ponzetto
University of Mannheim
Mannheim, Germany
{lydia,effelsberg,simone}@informatik.uni-mannheim.de
Abstract
We present a first attempt at semi-automatically harvesting a dataset of iconic images, namely
images that depict objects or scenes, which arouse associations to abstract topics. Our method
starts with representative topic-evoking images from Wikipedia, which are labeled with relevant
concepts and entities found in their associated captions. These are used to query an online image
repository (i.e., Flickr), in order to further acquire additional examples of topic-specific iconic
relations. To this end, we leverage a combination of visual similarity measures, image clustering
and matching algorithms to acquire clusters of iconic images that are topically connected to the
original seed images, while also allowing for various degrees of diversity. Our first results are
promising in that they indicate the feasibility of the task and that we are able to build a first
version of our resource with minimal supervision.
1 Introduction
Figurative language and images are a pervasive phenomenon associated with human communication.
For instance, images used in news articles (especially on hot and sensitive topics) often make use of
non-literal visual representations like iconic images, which are aimed at capturing the reader?s attention.
For environmental topics, for instance, a windmill in an untouched and bright landscape surrounded by
a clear sky is typically associated by humans with environmental friendliness, and accordingly causes
positive emotions. In a similar way, images of a polar bear on a drifting ice floe are typically associated
with the topic of global warming (O?Neill and Smith, 2014).
But while icons represent a pervasive device for visual communication, to date, there exists to the best
of our knowledge no approach aimed at their computational modeling. In order to enable the overarching
goal of producing such kind of models from real-world data, we focus, in this work, on the preliminary
task of semi-automatically compiling an electronic database of iconic images. These consist, in our
definition, of images produced to create privileged associations between a particular visual representation
and a referent. Iconic images are highly recognizable for media users and typically induce negative or
positive emotions that have an impact on viewers? attitudes and actions. In order to model them from a
computational perspective, we initially formulate iconic image acquisition as a clustering task in which,
given a set of initial, manually-selected ?seed? images ? e.g., a photo of a polar bear on a drifting ice floe
for the topic of global warming, a smokestack for the topic of pollution, etc. ? we use their associated
textual descriptions in order to collect related images from the Web. We then process these images using
state-of-the-art image understanding techniques to produce clusters of semantically similar, yet different
images depicting the same topic in an iconic way.
The acquisition of a database of iconic images represents the first step towards a full-fledged model to
computationally capture the phenomenon of iconic images in context. Our long-term vision is to cover
all three aspects of content (what makes an image iconic?), usage (in which context are iconic images
used?), and effects (which negative/positive emotions do iconic images evoke on viewers?) of iconic
images. To make this challenging problem feasible, we opt in this preliminary step for an approach that
views the task of understanding iconic images as the ability to build a dataset for further research.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
68
Figure 1: Our framework for the semi-automatic acquisition of iconic images from the Web.
2 Method
Our method for the semi-automatic acquisition of iconic images consists of five phases (Figure 1):
Seed selection. In the first phase of our approach we provide our pipeline with human-selected exam-
ples of iconic images that help us bootstrap the image harvesting process. To this end, we initially focus
on a wide range of twelve different abstract topics that can be typically represented using iconic images
(Table 1). Selecting initial examples describing a visual iconic representation of a topic can be a daunting
task, due to their volatile and topic-dependent nature. In this work, we explore the use of Web encyclo-
pedic resources in order to collect our initial examples. We start with the encyclopedic entries from
National Geographic Education
1
, an on-line resource in which human expert editors make explicit use
of prototypical images to visually represent encyclopedic entries like ?agriculture?, ?climate change?,
etc.. For instance, the encyclopedic entry for ?air pollution? contains images of smokestacks, cooling
towers release steam, and so on (cf. Table 1). We use these (proprietary) images to provide us with
human-validated examples of iconic images, and use these to identify (freely available) similar images
within Wikipedia pages based on a Google image search restricted to Wikipedia ? e.g., by searching for
smokestack site:wikipedia.org. We then use Wikipedia to create an initial dataset of iconic
visuals associated with the textual descriptions found in their captions.
Text-based image search. In the next step, we make use of a query-by-text approach in order to collect
additional data and enlarge our dataset with additional images depicting iconic relations. To this end, we
start by collecting the entities annotated within the image captions (e.g., ?Cumberland Power Plant at
Cumberland City?), and manually determine their relevance to the associated topic (e.g., smokestacks
and air pollution). This is because, to build a good query, we need to provide the search systems with
a good lexicalization (i.e., keywords) of the underlying information need (i.e., the topic). Consequently,
we extract entities from each caption of our initial set of images and use these to query additional data.
For each seed, we generate a query by concatenating the entity labels in the captions and send it to
Flickr
2
. We then filter the data by retaining only photos with title, description, and tags where both, tags
and description (caption and title) contain the query words. This method provides us with around 4000
additional images and text pairs.
Image clustering. Text-based image search results can introduce noise in the dataset, e.g., cases of
?semantic mismatch? where the caption and tags do not appropriately describe the scene found in the
image. In this work, we explore the application of image clustering techniques to cope with this issue.
For each topic we start with a set of instances made up of the seed images and the crawled one, and
group them into clusters based on image similarity measures. Clusters are built by calculating the linear
correlation ? i.e., which we take as a proxy for a similarity measure ? from the HSV-histograms of each
image, and applying the K-Means algorithm. Clustering on the basis of HSV-histograms does not take
into account the semantic content of images, since images with different meanings can still have the
same HSV-histogram. Nevertheless, this approach makes it possible to spot those outliers in the image
sets that do not relate well to the other images retrieved with the same query.
Image filtering. The next processing step focuses instead on rule-driven filtering to improve the initial
clustering-based filtering. We first apply a face detection and HoG (histogram of gradients) descriptor for
1
http://education.nationalgeographic.com
2
http://flickr.com
69
Topic Themes of seed images
Adaption hummingbird, king snake, koala
Agriculture cattle, ploughing, rice terraces, tropical fruits
Air balloon, sky view
Air Pollution smokestack, Three Mile Island, wildfire
Biodiversity Amazonas, blue starfish, cornflowers, fungi, Hopetoun Falls
Capital Capitol Hill, Prac?a Dos Tr?es, Washington Monument
Climate Mykonos (mild climate), Sonoran Desert, tea plantation (cool climate)
Climate Change polar bear, volcano, dry lake
Climate Refugee climate refugees from Indonesia, Haiti, Pakistan, etc.
Ecosystem bison, flooded forest, Flynn Reef, harp seal, rainforest, thorn tree
Global Warming deforestation, flooding, smokestack
Greenhouse Effect smokestack, steam engine train (smoke emissions)
Table 1: Overview of our covered topics and the themes associated with their seed images.
detecting people (Viola and Jones, 2001; Dalal and Triggs, 2005)
3
. Next, we filter our data as follows.
If faces or people are recognized in the picture, and the caption is judged to be related to entities of type
person (e.g., farmers iconically depicting the topic of agriculture), the instance is retained in the dataset.
On the other hand, if faces and/or people are recognized, but the caption is not related to entities of type
person (e.g., a blue linckia, which is a physical object), we filter out the image from the dataset.
Image matching. The filtered clusters we built so far are still problematic in that they do not account
for diversity ? i.e., we do not want as the outcome of our method to end up with clusters made up only of
various pictures of the very same object (e.g., the cooling towers of the Three Mile Island power complex
for the topic of air pollution, possibly seen from different perspectives, times of the day, etc.). That is,
in our scenario we would like to promote heterogeneous clusters which still retain a high-level semantic
match with the initial seeds (e.g., smokestacks or cooling towers belonging to different plants). To this
end, we explore in this work an approach that leverages different image matching methods together at
the same time to automatically capture these visual semantic matches.
Initially, for each cluster we select the image that minimizes the sum over all squared distances from
the other images in the cluster. That is, given a cluster C = {c
1
. . . c
n
}, we collect the image c? =
argmin
c
i
?C
?
c
j
?C?{c
i
}
(c
i
? c
j
)
2
. We call this the prototype of the cluster. Several image processing
methods are then used to compare the prototype of each cluster with the original seed images, with the
aim to detect high-level content similarity (i.e., distinct, yet similar objects such as the smokestacks of
different plants, etc.) and account for diversity with respect to our initial seeds. The first method is a
template matching approach, based on minimum and maximum values of gray levels, which, together
with their location are used to detect similar textures. The matching method is based on a correlation
coefficient matching (Brunelli, 2009). In parallel, we explore an alternative approach where images and
prototypes are compared using SIFT-based features (Lowe, 2004). Finally, we apply a contour matching
method: we use a manually chosen threshold of the largest 10% of contours of an image to reduce the
noise from non-characteristic contours like dots, points or other smaller structures. The matching of
contours is based on rotation invariant moments (Hu, 1962). When a good match is found, bounding
boxes are drawn around the contours.
The three methods provide evidence for potential matches between regions of each input prototype and
seed pair. Information from each single method is then combined by intersecting their respective outputs:
i) the patch, where the template matching is found is compared against the coordinates where relevant
SIFT features are detected (SIFT-Template); ii) the template matching patch is tested for intersection with
the bounding boxes of the matched contours (Template-Contour); iii) the bounding boxes of the contours
3
We focus on face and people detection since these are both well studied areas in computer vision for which a wide range
of state-of-the-art methods exist.
70
2-matches all matches
Topic
P R F
1
P R F
1
Adaption 100.0 57.2 72.8 66.7 10.9 18.7
Agriculture 50.0 35.4 41.4 0.0 0.0 0.0
Air 84.2 75.6 79.7 66.7 15.8 25.5
Air Pollution 65.9 83.7 73.7 65.1 32.4 43.3
Biodiversity 54.0 40.6 46.3 34.4 8.1 13.1
Capital 61.7 54.6 57.9 50.6 12.6 20.2
Climate 93.7 81.6 87.2 89.1 20.0 32.7
Climate Change 88.5 78.1 83.0 50.0 21.4 30.0
Climate Refugee 40.0 50.0 44.4 0.0 0.0 0.0
Ecosystem 73.7 61.7 67.2 43.3 11.4 18.0
Global Warming 65.9 71.0 68.3 43.0 21.7 28.8
Greenhouse Effect 100.0 81.6 89.9 100.0 34.2 51.0
Table 2: Performance results per topic on iconic image detection (percentages).
are checked for relevant SIFT features (SIFT-Contour). Finally, we group together the prototype with
the seed icon of the corresponding topic in case at least two or three of the single matching strategies
in i?iii) identify the same regions in the images. This process is repeated until all prototypes have been
examined: prototypes for which the no match can be found are filtered out as being not iconic.
3 Evaluation
Dataset statistics. We first provide statistics on the size of the datasets created with our approach.
Using HSV correlation we initially generate 1232 clusters with an average size of 27.37 elements per
cluster. Additional filtering based on at least two of our image matching methods produces 870 clusters
(19.33 elements on average), whereas the more restrictive clustering based on all three methods gives
261 small-sized clusters of only 5.8 instances on average. This is because, naturally, applying matching-
based filtering tends to produce a smaller number of clusters with fewer elements.
Gold standard and filtering evaluation. To produce a gold standard for our task, we annotated all of
the 4,000 images we retrieved from Flickr. Each image is associated with a keyword query (Section 2):
accordingly, we annotated each instance as being iconic or not with respect to the topic expressed by the
keywords ? e.g., given a picture of Hopetoun Falls, whether it captures the concept of waterfall or not.
This is because, in our work, we take keywords as proxies of the underlying topics (e.g., biodiversity is
depicted using waterfalls): in this setting, negative instances consist of mismatches between the query
text and the picture ? e.g., a photography taken near Hopetoun Falls, showing beech trees and thus
capturing a query search for ?forest? rather than ?waterfalls?.
We next evaluate our system on the binary classification task of detecting whether an image is iconic
or not. In our case, we can quantify performance by taking all images not filtered out in the last step of
image matching (and thus deemed as iconic in the final system output), and comparing them against our
gold-standard annotations. This way we can compute standard metrics of precision, recall and balanced
F-measure. Our results indicate that combining the output of two image matching techniques allows us to
reach 59.5% recall and 68.5% precision, whereas requiring all three methods to match reduces precision
(46.9%) while drastically decreasing recall (14.3%). The results show that our system is precision-
oriented, and that filtering based on the combination of all methods leads to an overall performance
degradation. This is because requiring all methods to match gives an over-constrained filtering: our
methods, in fact, tend to match all together only with those images which are highly similar to the seeds,
thus not being able to produce heterogeneous clusters.
We finally compute performance metrics for each single topic in turn, in order to experimentally
investigate the different degrees of performance of our system, and determine whether some topics are
71
more difficult than others (Table 2). Our results indicate that some topics are indeed more difficult than
others ? e.g., our system exhibits perfect precision on ?adaptation? and ?greenhouse effect? vs. much
poorer one on ?biodiversity? or ?climate refugee?. This is because some topics are bootstrapped from
less heterogeneous, and hence ?easier?, sets of seed images (e.g., all smokestacks, as in ?greenhouse
effect?, are very similar to each other). In general, this seems to point out that one of the key challenges
in our scenario is to produce highly precise clusters, while allowing for image diversity as a trade-off.
Error analysis. We finally looked at the output of our system, in order to better understand its per-
formance, as well as problems and future challenges. Examples of a few sample clusters are shown in
Figure 2. These clusters show that, thanks to our method, we are able to collect quite diverse, yet iconic
images retaining a topical affinity with the original seeds ? e.g., the poster on fighting deforestation or
the drawing used to depict air pollution. Due to the noise of our base image processing components, how-
ever, we also suffer from wrong matches such as the picture of a mobile phone for the topic of wildfire,
where the meaning of a rapidly spreading conflagration is related to air pollution, whereas the mobile
phone is not. Based on a random sample of 10% of the output clusters, we manually identified the main
sources of errors as related to: i) false image matching due to problems with contour detection; ii) SIFT
performing best for detecting different images of the same objects, but exhibiting lower performance on
the more complex task of detecting similar objects; iii) we applied our image matching methods using
default parameters and thresholds: further improvements could be obtained by in-domain tuning.
4 Conclusions
In this work, we presented some initial steps in developing a methodology to computationally model
the challenging phenomenon of iconic images. More specifically, we focused on the task of building a
repository of iconic images in a minimally supervised way by bootstrapping based on images found on
Web encyclopedic resources.
As future work, we plan to better combine heterogeneous information from text and images, as well
as use deeper representations for both information sources ? cf. joint semantic representations such as
specific LDA-based topic models and bags of visual (SIFT) features (Rasiwasia et al., 2010; Feng and
Lapata, 2010). This, in turn, can be applied to a variety of different tasks such as the automatic se-
mantification of captions, query generation and expansion. On the computer vision side, we are instead
particularly interested in exploring region-growing algorithms to detect textured or homogeneous re-
gions, and to allow for a segmentation of textured regions without contours, e.g., a cloudy sky or a view
of a forest landscape.
Downloads The dataset presented in this paper is freely available for research purposes at https:
//madata.bib.uni-mannheim.de/87/.
References
Roberto Brunelli. 2009. Template Matching Techniques in Computer Vision: Theory and Practice. Wiley Pub-
lishing.
Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human detection. In Proc. of CVPR,
pages 886?893.
Yansong Feng and Mirella Lapata. 2010. Topic models for image annotation and text illustration. In Proc. of HLT
?10, pages 831?839.
Ming-Kuei Hu. 1962. Visual pattern recognition by moment invariants. IRE Transactions on Information Theory,
8(2):179?187.
David G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Com-
puter Vision, 60(2):91?110.
S. O?Neill and Nicholas Smith. 2014. Climate change and visual imagery. Wiley Interdisciplinary Reviews:
Climate Change, 5(1):73?87.
Nikhil Rasiwasia, Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle, Gert R.G. Lanckriet, Roger Levy, and
Nuno Vasconcelos. 2010. A new approach to cross-modal multimedia retrieval. In Proc. of MM ?10, pages
251?260.
Paul Viola and Michael Jones. 2001. Rapid object detection using a boosted cascade of simple features. In Proc.
of CVPR, pages 511?518.
72
Figure 2: Sample iconic image clusters. Above a poor cluster on wildfire, below two good clusters on
pollution and deforestation.
73
