Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 38?47,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Natural Language Descriptions of Visual Scenes:
Corpus Generation and Analysis
Muhammad Usman Ghani Khan Rao Muhammad Adeel Nawab Yoshihiko Gotoh
University of Sheffield, United Kingdom
{ughani, r.nawab, y.gotoh}@dcs.shef.ac.uk
Abstract
As video contents continue to expand, it is
increasingly important to properly annotate
videos for effective search, mining and re-
trieval purposes. While the idea of anno-
tating images with keywords is relatively
well explored, work is still needed for anno-
tating videos with natural language to im-
prove the quality of video search. The fo-
cus of this work is to present a video dataset
with natural language descriptions which is
a step ahead of keywords based tagging.
We describe our initial experiences with a
corpus consisting of descriptions for video
segments crafted from TREC video data.
Analysis of the descriptions created by 13
annotators presents insights into humans?
interests and thoughts on videos. Such re-
source can also be used to evaluate auto-
matic natural language generation systems
for video.
1 Introduction
This paper presents our experiences in manu-
ally constructing a corpus, consisting of natural
language descriptions of video segments crafted
from a small subset of TREC video1 data. In
a broad sense the task can be considered one
form of machine translation as it translates video
streams into textual descriptions. To date the
number of studies in this field is relatively small
partially because of lack of appropriate dataset
for such task. Another obstacle may be inher-
ently larger variation for descriptions that can be
produced for videos than a conventional transla-
tion from one language to another. Indeed hu-
mans are very subjective while annotating video
1
www-nlpir.nist.gov/projects/trecvid/
streams, e.g., two humans may produce quite dif-
ferent descriptions for the same video. Based on
these descriptions we are interested to identify the
most important and frequent high level features
(HLFs); they may be ?keywords?, such as a par-
ticular object and its position/moves, used for a
semantic indexing task in video retrieval. Mostly
HLFs are related to humans, objects, their moves
and properties (e.g., gender, emotion and action)
(Smeaton et al, 2009).
In this paper we present these HLFs in the form
of ontologies and provides two hierarchical struc-
tures of important concepts ? one most relevant
for humans and their actions, and another for non
human objects. The similarity of video descrip-
tions is quantified using a bag of word model. The
notion of sequence of events in a video was quan-
tified using the order preserving sequence align-
ment algorithm (longest common subsequence).
This corpus may also be used for evaluation of
automatic natural language description systems.
1.1 Background
The TREC video evaluation consists of on-going
series of annual workshops focusing on a list of
information retrieval (IR) tasks. The TREC video
promotes research activities by providing a large
test collection, uniform scoring procedures, and
a forum for research teams interested in present-
ing their results. The high level feature extrac-
tion task aims to identify presence or absence of
high level semantic features in a given video se-
quence (Smeaton et al, 2009). Approaches to
video summarisation have been explored using
rushes video2 (Over et al, 2007).
2Rushes are the unedited video footage, sometimes re-
ferred to as a pre-production video.
38
TREC video also provides a variety of meta
data annotations for video datasets. For the HLF
task, speech recognition transcripts, a list of mas-
ter shot references, and shot IDs having HLFs in
them are provided. Annotations are created for
shots (i.e., one camera take) for the summarisa-
tion task. Multiple humans performing multiple
actions in different backgrounds can be shown in
one shot. Annotations typically consist of a few
phrases with several words per phrase. Human
related features (e.g., their presence, gender, age,
action) are often described. Additionally, camera
motion and camera angle, ethnicity information
and human?s dressing are often stated. On the
other hand, details relating to events and objects
are usually missing. Human emotion is another
missing information in many of such annotations.
2 Corpus Creation
We are exploring approaches to natural language
descriptions of video data. The step one of the
study is to create a dataset that can be used for
development and evaluation. Textual annotations
are manually generated in three different flavours,
i.e., selection of HLFs (keywords), title assign-
ment (a single phrase) and full description (mul-
tiple phrases). Keywords are useful for identifica-
tion of objects and actions in videos. A title, in a
sense, is a summary in the most compact form; it
captures the most important content, or the theme,
of the video in a short phrase. On the other hand,
a full description is lengthy, comprising of several
sentences with details of objects, activities and
their interactions. Combination of keywords, a ti-
tle, and a full descriptions will create a valuable
resource for text based video retrieval and sum-
marisation tasks. Finally, analysis of this dataset
provides an insight into how humans generate nat-
ural language description for video.
Most of previous datasets are related to spe-
cific tasks; PETS (Young and Ferryman, 2005),
CAVIAR (Fisher et al, 2005) and Terrascope
(Jaynes et al, 2005) are for surveillance videos.
KTH (Schuldt et al, 2004) and the Hollywood ac-
tion dataset (Marszalek et al, 2009) are for hu-
man action recognition. MIT car dataset is for
identification of cars (Papageorgiou and Poggio,
1999). Caltech 101 and Caltech 256 are image
datasets with 101 and 256 object categories re-
spectively (Griffin et al, 2007) but there is no
information about human actions or emotions.
There are some datasets specially generated for
scene settings such as MIT outdoor scene dataset
(Oliva and Torralba, 2009). Quattoni and Tor-
ralba (2009) created indoor dataset with 67 differ-
ent scenes categories. For most of these datasets
annotations are available in the form of keywords
(e.g., actions such as sit, stand, walk). They were
developed for keyword search, object recognition
or event identification tasks. Rashtchian et al
(2010) provided an interesting dataset of 1000 im-
ages which contain natural language descriptions
of those images.
In this study we select video clips from TREC
video benchmark for creating annotations. They
include categories such as news, meeting, crowd,
grouping, indoor/outdoor scene settings, traffic,
costume, documentary, identity, music, sports and
animals videos. The most important and proba-
bly the most frequent content in these videos ap-
pears to be a human (or humans), showing their
activities, emotions and interactions with other
objects. We do not intend to derive a dataset
with a full scope of video categories, which is be-
yond our work. Instead, to keep the task manage-
able, we aim to create a compact dataset that can
be used for developing approaches to translating
video contents to natural language description.
Annotations were manually created for a small
subset of data prepared form the rushes video
summarisation task and the HLF extraction task
for the 2007 and 2008 TREC video evaluations.
It consisted of 140 segments of videos ? 20 seg-
ments for each of the following seven categories:
Action videos: Human posture is visible and hu-
man can be seen performing some action
such as ?sitting?, ?standing?, ?walking? and
?running?.
Close-up: Human face is visible. Facial expres-
sions and emotions usually define mood of
the video (e.g., happy, sad).
News: Presence of an anchor or reporters. Char-
acterised by scene settings such as weather
boards at the background.
Meeting: Multiple humans are sitting and com-
municating. Presence of objects such as
chairs and a table.
Grouping: Multiple humans interaction scenes
that do not belong to a meeting scenario. A
39
table or chairs may not be present.
Traffic: Presence of vehicles such as cars, buses
and trucks. Traffic signals.
Indoor/Outdoor: Scene settings are more obvi-
ous than human activities. Examples may be
park scenes and office scenes (where com-
puters and files are visible).
Each segment contained a single camera shot,
spanning between 10 and 30 seconds in length.
Two categories, ?Close-up? and ?Action?, are
mainly related to humans? activities, expressions
and emotions. ?Grouping? and ?Meeting? de-
pict relation and interaction between multiple hu-
mans. ?News? videos explain human activities
in a constrained environment such as a broad-
cast studio. Last two categories, ?Indoor/Outdoor?
and ?Traffic?, are often observed in surveillance
videos. They often shows for humans? interac-
tion with other objects in indoor and outdoor set-
tings. TREC video annotated most video seg-
ments with a brief description, comprising of mul-
tiple phrases and sentences. Further, 13 human
subjects prepared additional annotation for these
video segments, consisting of keywords, a title
and a full description with multiple sentences.
They are referred to as hand annotations in the
rest of this paper.
2.1 Annotation Tool
There exist several freely available video anno-
tation tools. One of the popular video anno-
tation tool is Simple Video Annotation tool3.
It allows to place a simple tag or annotation
on a specified part of the screen at a particular
time. The approach is similar to the one used by
YouTube4. Another well-known video annotation
tool is Video Annotation Tool5. A video can be
scrolled for a certain time period and place anno-
tations for that part of the video. In addition, an
annotator can view a video clip, mark a time seg-
ment, attach a note to the time segment on a video
timeline, or play back the segment. ?Elan? anno-
tation tool allows to create annotations for both
audio and visual data using temporal information
(Wittenburg et al, 2006). During that annotation
process, a user selects a section of video using the
3
videoannotation.codeplex.com/
4
www.youtube.com/t/annotations about
5dewey.at.northwestern.edu/ppad2/documents/help/video.html
Figure 1: Video Description Tool (VDT). An anno-
tator watches one video at one time, selects all HLFs
present in the video, describes a theme of the video as
a title and creates a full description for important con-
tents in the video.
timeline capability and writes annotation for the
specific time.
We have developed our own annotation tool be-
cause of a few reasons. None of existing annota-
tion tools provided the functionality of generat-
ing a description and/or a title for a video seg-
ment. Some tools allows selection of keywords in
a free format, which is not suitable for our pur-
pose of creating a list of HLFs. Figure 1 shows
a screen shot of the video annotation tool devel-
oped, which is referred to as Video Description
Tool (VDT). VDT is simple to operate and assist
annotators in creating quality annotations. There
are three main items to be annotated. An anno-
tator is shown one video segment at one time.
Firstly a restricted list of HLFs is provided for
each segment and an annotator is required to se-
lect all HLFs occurring in the segment. Second,
a title should be typed in. A title may be a theme
of the video, typically a phrase or a sentence with
several words. Lastly, a full description of video
contents is created, consisting of several phrases
and sentences. During the annotation, it is pos-
sible to stop, forward, reverse or play again the
same video if required. Links are provided for
navigation to the next and the previous videos. An
annotator can delete or update earlier annotations
if required.
40
2.2 Annotation Process
A total of 13 annotators were recruited to create
texts for the video corpus. They were undergradu-
ate or postgraduate students and fluent in English.
It was expected that they could produce descrip-
tions of good quality without detailed instructions
or further training. A simple instruction set was
given, leaving a wide room for individual inter-
pretation about what might be included in the de-
scription. For quality reasons each annotator was
given one week to complete the full set of videos.
Each annotator was presented with a complete
set of 140 video segments on the annotation tool
VDT. For each video annotators were instructed
to provide
? a title of one sentence long, indicating the
main theme of the video;
? description of four to six sentences, related
to what are shown in the video;
? selection of high level features (e.g., male,
female, walk, smile, table).
The annotations are made with open vocabulary
? that is, they can use any English words as long
as they contain only standard (ASCII) characters.
They should avoid using any symbols or computer
codes. Annotators were further guided not to use
proper nouns (e.g., do not state the person name)
and information obtained from audio. They were
also instructed to select all HLFs appeared in the
video.
3 Corpus Analysis
13 annotators created descriptions for 140 videos
(seven categories with 20 videos per category),
resulting in 1820 documents in the corpus. The
total number of words is 30954, hence the av-
erage length of one document is 17 words. We
counted 1823 unique words and 1643 keywords
(nouns and verbs).
Figure 2 shows a video segment for a meet-
ing scene, sampled at 1 fps (frame per second),
and three examples for hand annotations. They
typically contain two to five phrases or sentences.
Most sentences are short, ranging between two to
six words. Descriptions for human, gender, emo-
tion and action are commonly observed. Occa-
sionally minor details for objects and events are
also stated. Descriptions for the background are
Hand annotation 1
(title) interview in the studio;
(description) three people are sitting on a red ta-
ble; a tv presenter is interviewing his guests; he is
talking to the guests; he is reading from papers in
front of him; they are wearing a formal suit;
Hand annotation 2
(title) tv presenter and guests
(description) there are three persons; the one is
host; others are guests; they are all men;
Hand annotation 3
(title) three men are talking
(description) three people are sitting around the
table and talking each other;
Figure 2: A montage showing a meeting scene in a
news video and three sets of hand annotations. In
this video segment, three persons are shown sitting on
chairs around a table ? extracted from TREC video
?20041116 150100 CCTV4 DAILY NEWS CHN33050028?.
often associated with objects rather than humans.
It is interesting to observe the subjectivity with the
task; the variety of words were selected by indi-
vidual annotators to express the same video con-
tents. Figure 3 shows another example of a video
segment for a human activity and hand annota-
tions.
3.1 Human Related Features
After removing function words, the frequency for
each word was counted in hand annotations. Two
classes are manually defined; one class is related
directly to humans, their body structure, identity,
action and interaction with other humans. (An-
other class represents artificial and natural objects
and scene settings, i.e., all the words not directly
related to humans, although they are important
for semantic understanding of the visual scene ?
described further in the next section.) Note that
some related words (e.g., ?woman? and ?lady?)
were replaced with a single concept (?female?);
concepts were then built up into a hierarchical
structure for each class.
Figure 4 presents human related information
observed in hand annotations. Annotators paid
full attention to human gender information as the
number of occurrences for ?female? and ?male? is
41
Figure 4: Human related information found in 13 hand annotations. Information is divided into structures (gen-
der, age, identity, emotion, dressing, grouping and body parts) and activities (facial, hand and body). Each box
contains a high level concept (e.g., ?woman? and ?lady? are both merged into ?female?) and the number of its
occurrences.
Hand annotation 1
(title) outdoor talking scene;
(description) young woman is sitting on chair in
park and talking to man who is standing next to
her;
Hand annotation 2
(title) A couple is talking;
(description) two person are talking; a lady is sit-
ting and a man is standing; a man is wearing a
black formal suit; a red bus is moving in the street;
people are walking in the street; a yellow taxi is
moving in the street;
Hand annotation 3
(title) talk of two persons;
(description) a man is wearing dark clothes; he is
standing there; a woman is sitting in front of him;
they are saying to each other;
Figure 3: A montage of video showing a human activ-
ity in an outdoor scene and three sets of hand annota-
tions. In this video segment, a man is standing while
a woman is sitting in outdoor ? from TREC video
?20041101 160000 CCTV4 DAILY NEWS CHN 41504210?.
the highest among HLFs. This highlights our con-
clusion that most interesting and important HLF
is humans when they appear in a video. On the
other hand age information (e.g., ?old ?, ?young?,
?child ?) was not identified very often. Names for
human body parts have mixed occurrences rang-
ing from high (?hand ?) to low (?moustache?). Six
basic emotions ? anger, disgust, fear, happiness,
sadness, and surprise as discussed by Paul Ek-
man6 ? covered most of facial expressions.
Dressing became an interesting feature when
a human was in a unique dress such as a formal
suit, a coloured jacket, an army or police uni-
form. Videos with multiple humans were com-
mon, and thus human grouping information was
frequently recognised. Human body parts were
involved in identification of human activities; they
included actions such as standing, sitting, walk-
ing, moving, holding and carrying. Actions re-
lated to human body and posture were frequently
identified. It was rare that unique human identi-
ties, such as police, president and prime minister,
were described. This may indicate that a viewer
might want to know a specific type of an object
to describe a particular situation instead of gener-
alised concepts.
3.2 Objects and Scene Settings
Figure 5 shows the hierarchy created for HLFs
that did not appear in Figure 4. Most of the words
are related to artificial objects. Humans inter-
act with these objects to complete an activity ?
6
en.wikipedia.org/wiki/Paul Ekman
42
Figure 5: Artificial and natural objects and scene set-
tings were summarised into six groups.
e.g., ?man is sitting on a chair?, ?she is talking
on the phone?, ?he is wearing a hat ?. Natural ob-
jects were usually in the background, providing
the additional context of a visual scene ? e.g.,
?human is standing in the jungle, ?sky is clear to-
day?. Place and location information (e.g., room,
office, hospital, cafeteria) were important as they
show the position of humans or other objects in
the scene ? e.g., ?there is a car on the road, ?peo-
ple are walking in the park ?.
Colour information often plays an important
part in identifying separate HLFs ? e.g., ?a man
in black shirt is walking with a woman with green
jacket?, ?she is wearing a white uniform ?. The
large number of occurrences for colours indicates
human?s interest in observing not only objects but
also their colour scheme in a visual scene. Some
hand descriptions reflected annotator?s interest in
scene settings shown in the foreground or in the
background. Indoor/outdoor scene settings were
also interested in by some annotators. These ob-
servations demonstrate that a viewer is interested
in high level details of a video and relationships
between different prominent objects in a visual
scene.
3.3 Spatial Relations
Figure 6 presents a list of the most frequent words
and phrases related to spatial relations found in
hand annotations. Spatial relations between HLFs
are important when explaining the semantics of
visual scenes. Their effective use leads to the
smooth description. Spatial relations can be cate-
gorised into
in (404); with (120); on (329); near (68); around
(63); at (55); on the left (35); in front of (24);
down (24); together (24); along (16); beside (16);
on the right (16); into (14); far (11); between (10);
in the middle (10); outside (8); off (8); over (8);
pass-by (8); across (7); inside (7); middle (7); un-
der (7); away (6); after (7)
Figure 6: List of frequent spatial relations with their
counts found in hand annotations.
static: relations between stationary objects;
dynamic: direction and path of moving objects;
inter-static and dynamic: relations between moving
and not moving objects.
Static relations can establish the scene settings
(e.g., ?chairs around a table? may imply an indoor
scene). Dynamic relations are used for finding ac-
tivities present in the video (e.g., ?a man is run-
ning with a dog?). Inter-static and dynamic rela-
tions are a mixture of stationary and non station-
ary objects; they explain semantics of the com-
plete scene (e.g., ?persons are sitting on the chairs
around the table? indicates a meeting scene).
3.4 Temporal Relations
Video is a class of time series data formed with
highly complex multi dimensional contents. Let
video X be a uniformly sampled frame sequence
of length n, denoted by X = {x1, . . . , xn}, and
each frame xi gives a chronological position of
the sequence (Figure 7). To generate full de-
scription of video contents, annotators use tempo-
ral information to join descriptions of individual
frames. For example,
A man is walking. After sometime he en-
ters the room. Later on he is sitting on the
chair.
Based on the analysis of the corpus, we describe
temporal information in two flavors:
1. temporal information extracted from activi-
ties of a single human;
2. interactions between multiple humans.
Most common relations in video sequences are
?before?, ?after?, ?start ? and ?finish ? for single hu-
mans, and ?overlap?, ?during? and ?meeting? for
multiple humans.
Figure 8 presents a list of the most frequent
words in the corpus related to temporal relations.
It can be observed that annotators put much focus
43
Figure 7: Illustration of a video as a uniformly sam-
pled sequence of length n. A video frame is denoted
by xi, whose spatial context can be represented in the
d dimensional feature space.
single human:
then (25); end (24); before (22); after (16); next
(12); later on (12); start (11); previous (11);
throughout (10); finish (8); afterwards (6); prior
to (4); since (4)
multiple humans:
meet (114); while (37); during (27); at the same
time (19); overlap (12); meanwhile (12); through-
out (7); equals (4)
Figure 8: List of frequent temporal relations with their
counts found in hand annotations.
on keywords related to activities of multiple hu-
mans as compared to single human cases. ?Meet?
keyword has the highest frequency, as annota-
tors usually consider most of the scenes involving
multiple humans as the meeting scene. ?While?
keyword is mostly used for showing separate ac-
tivities of multiple humans such as ?a man is walk-
ing while a woman is sitting?.
3.5 Similarity between Descriptions
A well-established approach to calculating human
inter-annotator agreement is kappa statistics (Eu-
genio and Glass, 2004). However in the current
task it is not possible to compute inter-annotator
agreement using this approach because no cat-
egory was defined for video descriptions. Fur-
ther the description length for one video can vary
among annotators. Alternatively the similarity be-
tween natural language descriptions can be calcu-
lated; an effective and commonly used measure
to find the similarity between a pair of documents
is the overlap similarity coefficient (Manning and
Schu?tze, 1999):
Simoverlap(X,Y ) =
|S(X,n) ? S(Y, n)|
min(|S(X,n)|, |S(Y, n)|)
where S(X,n) and S(Y, n) are the set of distinct
n-grams in documents X and Y respectively. It is
a similarity measure related to the Jaccard index
(Tan et al, 2006). Note that when a set X is a sub-
set of Y or the converse, the overlap coefficient is
equal to one. Values for the overlap coefficient
range between 0 and 1, where ?0? presents the sit-
uation where documents are completely different
and ?1? describes the case where two documents
are exactly the same.
Table 1 shows the average overlap similarity
scores for seven scene categories within 13 hand
annotations. The average was calculated from
scores for individual description, that was com-
pared with the rest of descriptions in the same
category. The outcome demonstrate the fact that
humans have different observations and interests
while watching videos. Calculation were repeated
with two conditions; one with stop words re-
moved and Porter stemmer (Porter, 1993) applied,
but synonyms NOT replaced, and the other with
stop words NOT removed, but Porter stemmer ap-
plied and synonyms replaced. It was found the
latter combination of preprocessing techniques re-
sulted in better scores. Not surprisingly synonym
replacement led to increased performance, indi-
cating that humans do express the same concept
using different terms.
The average overlap similarity score was higher
for ?Traffic? videos than for the rest of categories.
Because vehicles were the major entity in ?Traf-
fic? videos, rather than humans and their actions,
contributing for annotators to create more uniform
descriptions. Scores for some other categories
were lower. It probably means that there are more
aspects to pay attention when watching videos in,
e.g., ?Grouping? category, hence resulting in the
wider range of natural language expressions pro-
duced.
3.6 Sequence of Events Matching
Video is a class of time series data which can
be partitioned into time aligned frames (images).
These frames are tied together sequentially and
temporally. Therefore, it will be useful to know
how a person captures the temporal information
present in a video. As the order is preserved in a
sequence of events, a suitable measure to quantify
sequential and temporal information of a descrip-
tion is the longest common subsequence (LCS).
This approach computes the similarity between
a pair of token (i.e., word) sequences by simply
counting the number of edit operations (insertions
and deletions) required to transform one sequence
into the other. The output is a sequence of com-
mon elements such that no other longer string is
44
Action Close-up Indoor Grouping Meeting News Traffic
unigram (A) 0.3827 0.3913 0.4217 0.3809 0.3968 0.4378 0.4687
(B) 0.4135 0.4269 0.4544 0.4067 0.4271 0.4635 0.5174
bigram (A) 0.1483 0.1572 0.1870 0.1605 0.1649 0.1872 0.1765
(B) 0.2490 0.2616 0.2877 0.2619 0.2651 0.2890 0.2825
trigram (A) 0.0136 0.0153 0.0301 0.0227 0.0219 0.0279 0.0261
(B) 0.1138 0.1163 0.1302 0.1229 0.1214 0.1279 0.1298
Table 1: Average overlapping similarity scores within 13 hand annotations. For each of unigram, bigram and
trigram, scores are calculated for seven categories in two conditions: (A) stop words removed and Porter stemmer
applied, but synonyms NOT replaced; (B) stop words NOT removed, but Porter stemmer applied and synonyms
replaced.
raw synonym keyword
Action 0.3782 0.3934 0.3955
Close-up 0.4181 0.4332 0.4257
Indoor 0.4248 0.4386 0.4338
Grouping 0.3941 0.4104 0.3832
Meeting 0.3939 0.4107 0.4124
News 0.4382 0.4587 0.4531
Traffic 0.4036 0.4222 0.4093
Table 2: Similarity scores based on the longest com-
mon subsequence (LCS) in three conditions: scores
without any preprocessing (raw), scores after synonym
replacement (synonym), and scores by keyword com-
parison (keyword). For keyword comparison, verbs
and nouns were presented as keywords after stemming
and removing stop words.
available. In the experiments, the LCS score be-
tween word sequences is normalised by the length
of the shorter sequence.
Table 2 presents results for identifying se-
quences of events in hand descriptions using the
LCS similarity score. Individual descriptions
were compared with the rest of descriptions in the
same category and the average score was calcu-
lated. Relatively low scores in the table indicate
the great variation in annotators? attention on the
sequence of events, or temporal information, in a
video. Events described by one annotator may not
have been listed by another annotator. The News
videos category resulted in the highest similarity
score, confirming the fact that videos in this cate-
gory are highly structured.
3.7 Video Classification
To demonstrate the application of this corpus with
natural language descriptions, a supervised docu-
ment classification task is outlined. Tf-idf score
can express textual document features (Dumais et
al., 1998). Traditional tf-idf represents the rela-
tion between term t and document d. It provides
a measure of the importance of a term within a
particular document, calculated as
tfidf(t, d) = tf(t, d) ? idf(d) (1)
where the term frequency tf(t, d) is given by
tf(t, d) =
Nt,d
?
k
Nk,d
(2)
In the above equation Nt,d is the number of occur-
rences of term t in document d, and the denomina-
tor is the sum of the number of occurrences for all
terms in document d, that is, the size of the docu-
ment |d|. Further the inverse document frequency
idf(d) is
idf(d) = log
N
W (t)
(3)
where N is the total number of documents in the
corpus and W (t) is the total number of document
containing term t.
A term-document matrix X is presented by
T ? D matrix tfidf(t, d). In the experiment
Naive Bayes probabilistic supervised learning al-
gorithm was applied for classification using Weka
machine learning library (Hall et al, 2009). Ten-
fold cross validation was applied. The perfor-
mance was measured using precision, recall and
F1-measure (Table 3). F1-measure was low for
?Grouping? and ?Action? videos, indicating the
difficulty in classifying these types of natural lan-
guage descriptions. Best classification results
were achieved for ?Traffic? and ?Indoor/Outdoor?
scenes. Absence of humans and their actions
might have contributed obtaining the high clas-
sification scores. Human actions and activities
were present in most videos in various categories,
hence the ?Action? category resulted in the low-
est results. ?Grouping? category also showed
45
precision recall F1-measure
Action 0.701 0.417 0.523
Close-up 0.861 0.703 0.774
Grouping 0.453 0.696 0.549
Indoor 0.846 0.915 0.879
Meeting 0.723 0.732 0.727
News 0.679 0.823 0.744
Traffic 0.866 0.869 0.868
average 0.753 0.739 0.736
Table 3: Results for supervised classification using the
tf-idf features.
Figure 9: The average overlap similarity scores for
titles and for descriptions. ?uni?, ?bi?, and ?tri? indi-
cate the unigram, bigram, and trigram based similarity
scores, respectively. They were calculated without any
preprocessing such as stop word removal or synonym
replacement.
weaker result; it was probably because process-
ing for interaction between multiple people, with
their overlapped actions, had not been fully devel-
oped. Overall classification results are encourag-
ing which demonstrates that this dataset is a good
resource for evaluating natural language descrip-
tion systems of short videos.
3.8 Analysis of Title and Description
A title may be considered a very short form of
summary. We carried out further experiments to
calculate the similarity between a title and a de-
scription manually created for a video. The length
of a title varied between two to five words. Figure
9 shows the average overlapping similarity scores
between titles and descriptions. It can be ob-
served that, in general, scores for titles were lower
than those for descriptions, apart from ?News?
and ?Meeting? videos. It was probably caused by
the short length of titles; by inspection we found
phrases such as ?news video? and ?meeting scene?
for these categories.
Another experiment was performed for classi-
fication of videos based on title information only.
Figure 10 shows comparison of classification per-
Figure 10: Video classification by titles, and by de-
scriptions.
formance with titles and with descriptions. We
were able to make correct classification in many
videos with titles alone, although the performance
was slightly less for titles only than for descrip-
tions.
4 Conclusion and Future Work
This paper presented our experiments using a cor-
pus created for natural language description of
videos. For a small subset of TREC video data in
seven categories, annotators produced titles, de-
scriptions and selected high level features. This
paper aimed to characterise the corpus based on
analysis of hand annotations and a series of exper-
iments for description similarity and video clas-
sification. In the future we plan to develop au-
tomatic machine annotations for video sequences
and compare them against human authored anno-
tations. Further, we aim to annotate this corpus
in multiple languages such as Arabic and Urdu
to generate a multilingual resource for video pro-
cessing community.
Acknowledgements
M U G Khan thanks University of Engineering &
Technology, Lahore, Pakistan and R M A Nawab
thanks COMSATS Institute of Information Tech-
nology, Lahore, Pakistan for funding their work
under the Faculty Development Program.
46
References
S. Dumais, J. Platt, D. Heckerman, and M. Sahami.
1998. Inductive learning algorithms and represen-
tations for text categorization. In Proceedings of
the seventh international conference on Information
and knowledge management, pages 148?155. ACM.
B.D. Eugenio and M. Glass. 2004. The kappa statis-
tic: A second look. Computational linguistics,
30(1):95?101.
R. Fisher, J. Santos-Victor, and J. Crowley. 2005.
Caviar: Context aware vision using image-based ac-
tive recognition.
G. Griffin, A. Holub, and P. Perona. 2007. Caltech-
256 object category dataset.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data min-
ing software: an update. ACM SIGKDD Explo-
rations Newsletter, 11(1):10?18.
C. Jaynes, A. Kale, N. Sanders, and E. Gross-
mann. 2005. The terrascope dataset: A scripted
multi-camera indoor video surveillance dataset with
ground-truth. In Proceedings of the IEEE Workshop
on VS PETS, volume 4. Citeseer.
Christopher Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
M. Marszalek, I. Laptev, and C. Schmid. 2009. Ac-
tions in context.
A. Oliva and A. Torralba. 2009. Mit outdoor scene
dataset.
P. Over, A.F. Smeaton, and P. Kelly. 2007. The
trecvid 2007 bbc rushes summarization evaluation
pilot. In Proceedings of the international work-
shop on TRECVID video summarization, pages 1?
15. ACM.
C. Papageorgiou and T. Poggio. 1999. A trainable
object detection system: Car detection in static im-
ages. Technical Report 1673, October. (CBCL
Memo 180).
M.F. Porter. 1993. An algorithm for suffix stripping.
Program: electronic library and information sys-
tems, 14(3):130?137.
A. Quattoni and A. Torralba. 2009. Recognizing in-
door scenes.
C. Rashtchian, P. Young, M. Hodosh, and J. Hocken-
maier. 2010. Collecting image annotations using
amazon?s mechanical turk. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical
Turk, pages 139?147. Association for Computa-
tional Linguistics.
C. Schuldt, I. Laptev, and B. Caputo. 2004. Recog-
nizing human actions: A local svm approach. In
Pattern Recognition, 2004. ICPR 2004. Proceed-
ings of the 17th International Conference on, vol-
ume 3, pages 32?36. IEEE.
A.F. Smeaton, P. Over, and W. Kraaij. 2009. High-
level feature detection from video in trecvid: a
5-year retrospective of achievements. Multimedia
Content Analysis, pages 1?24.
P.N. Tan, M. Steinbach, V. Kumar, et al 2006. Intro-
duction to data mining. Pearson Addison Wesley
Boston.
P. Wittenburg, H. Brugman, A. Russel, A. Klassmann,
and H. Sloetjes. 2006. Elan: a professional frame-
work for multimodality research. In Proceedings of
LREC, volume 2006. Citeseer.
D.P. Young and J.M. Ferryman. 2005. Pets metrics:
On-line performance evaluation service. In Joint
IEEE International Workshop on Visual Surveil-
lance and Performance Evaluation of Tracking and
Surveillance (VS-PETS), pages 317?324.
47
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 27?35,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Describing Video Contents in Natural Language
Muhammad Usman Ghani Khan
University of Sheffield
United Kingdom
ughani@dcs.shef.ac.uk
Yoshihiko Gotoh
University of Sheffield
United Kingdom
y.gotoh@dcs.shef.ac.uk
Abstract
This contribution addresses generation of
natural language descriptions for human ac-
tions, behaviour and their relations with
other objects observed in video streams.
The work starts with implementation of
conventional image processing techniques
to extract high level features from video.
These features are converted into natural
language descriptions using context free
grammar. Although feature extraction pro-
cesses are erroneous at various levels, we
explore approaches to putting them to-
gether to produce a coherent description.
Evaluation is made by calculating ROUGE
scores between human annotated and ma-
chine generated descriptions. Further we
introduce a task based evaluation by human
subjects which provides qualitative evalua-
tion of generated descriptions.
1 Introduction
In recent years video has established its domi-
nance in communication and has become an in-
tegrated part of our everyday life ranging from
hand-held videos to broadcast news video (from
unstructured to highly structured). There is a need
for formalising video semantics to help users gain
useful and refined information relevant to their
demands and requirements. Human language is
a natural way of communication. Useful entities
extracted from videos and their inter-relations can
be presented by natural language in a syntactically
and semantically correct formulation.
While literature relating to object recognition
(Galleguillos and Belongie, 2010), human action
recognition (Torralba et al, 2008), and emotion
detection (Zheng et al, 2010) are moving towards
maturity, automatic description of visual scenes
is still in its infancy. Most studies in video re-
trieval have been based on keywords (Bolle et
al., 1998). An interesting extension to a key-
word based scheme is natural language textual de-
scription of video streams. They are more human
friendly. They can clarify context between key-
words by capturing their relations. Descriptions
can guide generation of video summaries by con-
verting a video to natural language. They can pro-
vide basis for creating a multimedia repository for
video analysis, retrieval and summarisation tasks.
Kojima et al (2002) presented a method for
describing human activities in videos based on
a concept hierarchy of actions. They described
head, hands and body movements using natural
language. For a traffic control application, Nagel
(2004) investigated automatic visual surveillance
systems where human behaviour was presented
by scenarios, consisting of predefined sequences
of events. The scenario was evaluated and auto-
matically translated into a text by analysing the
visual contents over time, and deciding on the
most suitable event. Lee et al (2008) introduced
a framework for semantic annotation of visual
events in three steps; image parsing, event infer-
ence and language generation. Instead of humans
and their specific activities, they focused on ob-
ject detection, their inter-relations and events that
were present in videos. Baiget et al (2007) per-
formed human identification and scene modelling
manually and focused on human behaviour de-
scription for crosswalk scenes. Yao et al (2010)
introduced their work on video to text descrip-
tion which is dependent on the significant amount
of annotated data, a requirement that is avoided
in this paper. Yang et al (2011) presented a
27
framework for static images to textual descrip-
tions where they contained to image with up to
two objects. In contrast, this paper presents a
work on video streams, handling not only objects
but also other features such as actions, age, gender
and emotions.
The study presented in this paper is concerned
with production of natural language description
for visual scenes in a time series using a bottom-
up approach. Initially high level features (HLFs)
are identified in video frames. They may be ?key-
words?, such as a particular object and its posi-
tion/moves, used for a semantic indexing task in
video retrieval. Spatial relations between HLFs
are important when explaining the semantics of
visual scene. Extracted HLFs are then presented
by syntactically and semantically correct expres-
sions using a template based approach. Image
processing techniques are far from perfect; there
can be many missing, misidentified and erro-
neously extracted HLFs. We present scenarios
to overcome these shortcomings and to generate
coherent natural descriptions. The approach is
evaluated using video segments drafted manually
from the TREC video dataset. ROUGE scores is
calculated between human annotated and machine
generated descriptions. A task based evaluation is
performed by human subjects, providing qualita-
tive evaluation of generated descriptions.
2 Dataset Creation
The dataset was manually created from a sub-
set of rushes and HLF extraction task videos in
2007/2008 TREC video evaluations (Over et al,
2007). It consists of 140 segments, with each seg-
ment containing one camera shot, spanning 10 to
30 seconds in length. There are 20 video segments
for each of the seven categories:
Action: Human can be seen performing some action
(e.g., sit, walk)
Closeup: Facial expressions/emotions can be seen
(e.g., happy, sad)
News: Anchor/reporter may be seen; particular scene
settings (e.g., weather board in the background)
Meeting: Multiple humans are seen interacting; pres-
ence of objects such as chairs and a table
Grouping: Multiple humans are seen but not in meet-
ing scenarios; chairs and table may not be present
Traffic: Vehicles (e.g., car, bus, truck) / traffic signals
are seen
Indoor/Outdoor: Scene settings are more obvious
than human activities (e.g., park scene, office)
13 human subjects individually annotated these
videos in one to seven short sentences. They are
referred to as hand annotations in the rest of this
paper.
3 Processing High Level Features
Identification of human face or body can prove
the presence of human in a video. The method
by Kuchi et al (2002) is adopted for face detec-
tion using colour and motion information. The
method works against variations in lightning con-
ditions, skin colours, backgrounds, face sizes and
orientations. When the background is close to the
skin colour, movement across successive frames
is tested to confirm the presence of a human face.
Facial features play an important role in identify-
ing age, gender and emotion information (Maglo-
giannis et al, 2009). Human emotion can be esti-
mated using eyes, lips and their measures (gradi-
ent, distance of eyelids or lips). The same set of
facial features and measures can be used to iden-
tify a human gender1.
To recognise human actions the approach based
on a star skeleton and a hidden Markov model
(HMM) is implemented (Chen et al, 2006). Com-
monly observed actions, such as ?walking?, ?run-
ning?, ?standing?, and ?sitting?, can be identified.
Human body is presented in the form of sticks to
generate features such as torso, arm length and an-
gle, leg angle and stride (Sundaresan et al, 2003).
Further Haar features are extracted and classifiers
are trained to identify non-human objects (Viola
and Jones, 2001). They include car, bus, motor-
bike, bicycle, building, tree, table, chair, cup, bot-
tle and TV-monitor. Scene settings ? indoor or
outdoor ? can be identified based on the edge
oriented histogram (EOH) and the colour oriented
histogram (COH) (Kim et al, 2010).
3.1 Performance of HLF Extraction
In the experiments, video frames were extracted
using ffmpeg2, sampled at 1 fps (frame per sec-
ond), resulting in 2520 frames in total. Most of
1
www.virtualffs.co.uk/In a Nutshell.html
2Ffmpeg is a command line tool composed of a col-
lection of free software and open source libraries. It can
record, convert and stream digital audio and video in nu-
merous formats. The default conversion rate is 25 fps. See
http://www.ffmpeg.org/
28
(ground truth) (ground truth)
exist not exist male female
exist 1795 29 male 911 216
not exist 95 601 female 226 537
(a) human detection (b) gender identification
Table 1: Confusion tables for (a) human detection and
(b) gender identification. Columns show the ground
truth, and rows indicate the automatic recognition re-
sults. The human detection task is biased towards exis-
tence of human, while in the gender identification pres-
ence of male and female are roughly balanced.
HLFs required one frame to evaluate. Human ac-
tivities were shown in 45 videos and they were
sampled at 4 fps, yielding 3600 frames. Upon
several trials, we decided to use eight frames
(roughly two seconds) for human action recogni-
tion. Consequently tags were assigned for each
set of eight frames, totalling 450 sets of actions.
Table 1(a) presents a confusion matrix for hu-
man detection. It was a heavily biased dataset
where human(s) were present in 1890 out of 2520
frames. Of these 1890, misclassification occurred
on 95 occasions. On the other hand gender iden-
tification is not always an easy task even for hu-
mans. Table 1(b) shows a confusion matrix for
gender identification. Out of 1890 frames in
which human(s) were present, frontal faces were
shown in 1349 images. The total of 3555 humans
were present in 1890 frames (1168 frames con-
tained multiple humans), however the table shows
the results when at least one gender is correctly
identified. Female identification was often more
difficult due to make ups, variety of hair styles
and wearing hats, veils and scarfs.
Table 2 shows the human action recognition
performance tested with a set of 450 actions. It
was difficult to recognise ?sitting? actions, proba-
bly because HMMs were trained on postures of a
complete human body, while a complete posture
was often not available when a person was sit-
ting. ?Hand waving? and ?clapping? were related
to movements in upper body parts, and ?walking?
and ?running? were based on lower body move-
ments. In particular ?waving? appeared an easy
action to identify because of its significant moves
of upper body parts. Table 3 shows the confu-
sion for human emotion recognition. ?Serious?,
?happy? and ?sad? were most common emotions
in this dataset, in particular ?happy? emotion was
most correctly identified.
There were 15 videos where human or any
(ground truth)
stand sit walk run wave clap
stand 98 12 19 3 0 0
sit 0 68 0 0 0 0
walk 22 9 105 8 0 0
run 4 0 18 27 0 0
wave 2 5 0 0 19 2
clap 0 0 0 0 4 9
Table 2: Confusion table for human action recogni-
tion. Columns show the ground truth, and rows indi-
cate the automatic recognition results. Some actions
(e.g., ?standing?) were more commonly seen than oth-
ers (e.g., ?waving?).
(ground truth)
angry serious happy sad surprised
angry 59 0 0 15 16
serious 0 661 0 164 40
happy 0 35 427 27 8
sad 61 13 0 281 2
surprised 9 19 0 0 53
Table 3: Confusion table for human emotion recogni-
tion. Columns show the ground truth, and rows indi-
cate the automatic recognition results.
other moving HLF (e.g., car, bus) were absent.
Out of these 15 videos, 12 were related to outdoor
environments where trees, greenery, or buildings
were present. Three videos showed indoor set-
tings with objects such as chairs, tables and cups.
All frames from outdoor scenes were correctly
identified; for indoor scenes 80% of frames were
correct. Presence of multiple objects seems to
have caused negative impact on EOH and COH
features, hence resulted in some erroneous clas-
sifications. The recognition performances for
non-human objects were also evaluated with the
dataset. We found their average precision3 scores
ranging between 44.8 (table) and 77.8 (car).
3.2 Formalising Spatial Relations
To develop a grammar robust for describing hu-
man related scenes, there is a need for formalis-
ing spatial relations among multiple HLFs. Their
effective use leads to smooth description of visual
scenes. Spatial relations can be categorised into
static: relations between not moving objects;
dynamic: direction and path of moving objects;
inter-static and dynamic: relations between moving
and not moving objects.
3defined by Everingham et al (2010).
29
Figure 1: Procedure for calculating the ?between? rela-
tion. Obj 1 and 2 are the two reference objects, while
Obj 3, 4 and 5 are the target objects.
Static relations can establish the scene settings
(e.g., ?chairs around a table? may imply an indoor
scene). Dynamic relations are used for finding ac-
tivities present in the video (e.g., ?a man is run-
ning with a dog?). Inter-static and dynamic rela-
tions are a mixture of stationary and non station-
ary objects; they explain semantics of the com-
plete scene (e.g., ?persons are sitting on the chairs
around the table? indicates a meeting scene).
Spatial relations are estimated using positions
of humans and other objects (or their bounding
boxes, to be more precise). Following relation-
ships can be recognised between two or three ob-
jects: ?in front of?, ?behind?, ?to the left?, ?to the
right?, ?beside?, ?at?, ?on?, ?in?, and ?between?.
Figure 1 illustrates steps for calculating the three-
place relationship ?between?. Schirra et al (1987)
explained the algorithm:
? Calculate the two tangents g1 and g2 between
the reference objects using their closed-rectangle
representation;
? If (1) both tangents cross the target or its rectan-
gle representation (see Obj 4 in the figure), or (2)
the target is totally enclosed by the tangents and
the references (Obj 3), the relationship ?between?
is true.
? If only one tangent intersects the subject (Obj 5),
the applicability depends on its penetration depth
in the area between the tangents, thus calculate:
max(a/(a+b), a/(a+c))
? Otherwise ?between? relation does not hold.
3.3 Predicates for Sentence Generation
Figure 2 presents a list of predicates to be used for
natural language generation. Some predicates are
derived by combining multiple HLFs extracted,
e.g., ?boy? may be inferred when a human is a
Human structure related
human (yes, no)
gender (male, female)
age (baby, child, young, old)
body parts (hand, head, body)
grouping (one, two, many)
Human actions and emotions
action (stand, sit, walk, run, wave, clap)
emotion (happy, sad, serious, surprise, angry)
Objects and scene settings
scene setting (indoor, outdoor)
objects (car, cup, table, chair, bicycle, TV-monitor)
Spatial relations among objects
in front of, behind, to the left, to the right, beside,
at, on, in, between
Figure 2: Predicates for single human scenes.
?male? and a ?child?. Apart from objects, only one
value can be selected from candidates at one time,
e.g., gender can be male or female, action can
be only one of those listed. Note that predicates
listed in Figure 2 are for describing single human
scenes; combination of these predicates may be
used if multiple humans are present.
4 Natural Language Generation
HLFs acquired by image processing require ab-
straction and fine tuning for generating syntacti-
cally and semantically sound natural language ex-
pressions. Firstly, a part of speech (POS) tag is
assigned to each HLF using NLTK4 POS tagger.
Further humans and objects need to be assigned
proper semantic roles. In this study, a human is
treated as a subject, performing a certain action.
Other HLFs are treated as objects, affected by hu-
man?s activities. These objects are usually helpful
for description of background and scene settings.
A template filling approach based on context
free grammar (CFG) is implemented for sentence
generation. A template is a pre-defined structure
with slots for user specified parameters. Each
template requires three parts for proper function-
ing: lexicons, template rules and grammar. Lex-
icon is a vocabulary containing HLFs extracted
from a video stream (Figure 3). Grammar assures
syntactical correctness of the sentence. Template
rules are defined for selection of proper lexicons
4
www.nltk.org/
30
Noun ? man | woman | car | cup | table|
chair | cycle | head | hand | body
Verb ? stand | walk | sit | run | wave
Adjective ? happy | sad | serious | surprise |
angry | one | two | many | young
old | middle-aged | child | baby
Pronoun ? me | i | you | it | she | he
Determiner ? the | a | an | this | these | that
Preposition ? from | on | to | near | while
Conjunction ? and | or | but
Figure 3: Lexicons and their POS tags.
with well defined grammar.
4.1 Template Rules
Template rules are employed for the selection of
appropriate lexicons for sentence generation. Fol-
lowings are some template rules used in this work:
Base returns a pre-defined string (e.g., when no HLF
is detected)
If same as an if-then statement of programming lan-
guages, returning a result when the antecedent of
the rule is true
Select 1 same as a condition statement of program-
ming languages, returning a result when one of
antecedent conditions is true
Select n is used for returning a result while more than
one antecedent conditions is true
Concatenation appends the the result of one template
rule with the results of a second rule
Alternative is used for selecting the most specific
template when multiple templates can be used
Elaboration evaluates the value of a template slot
Figure 4 illustrates template rules selection pro-
cedure. This example assumes human presence
in the video. If-else statements are used for fit-
ting proper gender in the template. Human can
be performing only one action at a time referred
by Select 1. There can be multiple objects which
are either part of background or interacting with
humans. Objects are selected by Select n rule.
These values can be directly attained from HLFs
extraction step. Elaboration rule is used for
generating new words by joining multiple HLFs.
?Driving? is achieved by combing ?person is in-
side car? and ?car is moving?.
4.2 Grammar
Grammar is the body of rules that describe the
structure of expressions in any language. We
If (gender == male) then man else woman
Select 1 (Action == walk, run, wave, clap, sit, stand )
Select n (Object == car, chair, table, bike)
Elaboration (If ?the car is moving? and ?person is
inside the car?) then ?person is driving the car?
Figure 4: Template rules applied for creating a sen-
tence ?man is driving the car?.
make use of context free grammar (CFG) for the
sentence generation task. CFG based formulation
enables us to define a hierarchical presentation for
sentence generation; e.g., a description for multi-
ple humans is comprised of single human actions.
CFG is formalised by 4-tuple:
G = (T,N, S,R)
where T is set of terminals (lexicon) shown in
Figure 3, N is a set of non-terminals (usually POS
tags), S is a start symbol (one of non-terminals).
Finally R is rules / productions of the form X ?
?, where X is a non-terminal and ? is a se-
quence of terminals and non-terminals which may
be empty.
For implementing the templates, simpleNLG is
used (Gatt and Reiter, 2009). It also performs
some extra processing automatically: (1) the first
letter of each sentence is capitalised, (2) ?-ing? is
added to the end of a verb as the progressive as-
pect of the verb is desired, (3) all words are put
together in a grammatical form, (4) appropriate
white spaces are inserted between words, and (5)
a full stop is placed at the end of the sentence.
4.3 Hierarchical Sentence Generation
In this work we define a CFG based presenta-
tion for expressing activities by multiple humans.
Ryoo and Aggarwal (2009) used CFG for hierar-
chical presentation of human actions where com-
plex actions were composed of simpler actions.
In contrast we allow a scenario where there is no
interaction between humans, i.e., they perform in-
dividual actions without a particular relation ?
imagine a situation whereby three people are sit-
ting around a desk while one person is passing
behind them.
Figure 5 shows an example for sentence gen-
eration related to a single human. This mech-
anism is built with three blocks when only one
subject5 is present. The first block expresses a
5Non human subject is also allowed in the mechanism.
31
Figure 5: A scenario with a single human.
Figure 6: A scenario with two humans.
human subject with age, gender and emotion in-
formation. The second block contains a verb de-
scribing a human action, to explain the relation
between the first and the third blocks. Spatial re-
lation between the subject and other objects can
also be presented. The third block captures other
objects which may be either a part of background
or a target for subject?s action.
The approach is hierarchical in the sense that
we start with creating a single human grammar,
then build up to express interactions between two
or more than two humans as a combination of sin-
gle human activities. Figure 6 presents examples
involving two subjects. There can be three scenar-
ios; firstly two persons interact with each other to
generate some common single activity (e.g., ?hand
shake? scene). The second scenario involves two
related humans performing individual actions but
they do not create a single action (e.g., both per-
sons are walking together, sitting or standing). Fi-
nally two persons happen to be in the same scene
at the same time, but there is no particular relation
between them (e.g., one person walks, passing be-
hind the other person sitting on a chair). Figure 7
shows an example that involves an extension of a
Figure 7: A scenario with multiple humans.
Figure 8: Template selection: (a) subject + subject +
verb: ?man and woman are waving hands?; (b) subject
+ subject + object: ?two persons around the table?; (c)
subject + verb, noun phrase / subject, noun phrase /
subject: ?a man is standing; a person is present; there
are two chairs?; (d) subject + subject + subject + verb:
?multiple persons are present ?.
single human scenario to more than two subjects.
Similarly to two-human scenarios, multiple sub-
jects can create a single action, separate actions,
or different actions altogether.
4.4 Application Scenarios
This section overviews different scenarios for ap-
plication of the sentence generation framework.
Figure 8 presents examples for template selec-
tion procedure. Although syntactically and se-
mantically correct sentences can be generated in
all scenes, immaturity of image processing would
cause some errors and missing information.
Missing HLFs. For example, action (?sitting?)
was not identified in Figure 8(b). Further, detec-
Figure 9: Image processing can be erroneous: (a) only
three cars are identified although there are many ve-
hicles prominent, (b) five persons (in red rectangles)
are detected although four are present; (c) one male
is identified correctly, other male is identified as ?fe-
male?; (d) detected emotion is ?smiling? though he
shows a serious face.
32
Figure 10: Closeup of a man talking to someone in the outdoor
scene ? seen in ?MS206410? from the 2007 rushes summarisation
task. Machine annotation: A serious man is speaking; There are
humans in the background. Hand annotation 1: A man is talking
to someone; He is wearing a formal suit; A police man is standing
behind him; Some people in the background are wearing hats. Hand
annotation 2: A man with brown hair is talking to someone; He is
standing at some outdoor place; He is wearing formal clothes; He
looks serious; It is windy.
tion of food on the table might have led to more
semantic description of the scene (e.g., ?dinning
scene?). In 8(d), fourth human and actions by
two humans (?raising hands?) were not extracted.
Recognition of the road and many more vehicles
in Figure 9(a) could have produced more semantic
expression (e.g., ?heavy traffic scene?).
Non human subjects. Suppose a human is ab-
sent, or failed to be extracted, the scene is ex-
plained on the basis of objects. They are treated as
subjects for which sentences are generated. Fig-
ure 9(a) presents such a scenario; description gen-
erated was ?multiple cars are moving?.
Errors in HLF extraction. In Figure 9(c), one
person was found correctly but the other was er-
roneously identified as female. Description gen-
erated was ?a smiling adult man is present with
a woman?. Detected emotion was ?smile? in 9(d)
though real emotion was ?serious?. Description
generated was ?a man is smiling?.
5 Experiments
5.1 Machine Generated Annotation Samples
Figures 10 to 12 present machine generated an-
notation and two hand annotations for randomly
selected videos related to three categories from
dataset.
Face closeup (Figure 10). Main interest was
to find human gender and emotion information.
Machine generated description was able to cap-
ture human emotion and background information.
Hand annotations explained the sequence more,
e.g., dressing, identity of a person as policeman,
hair colour and windy outdoor scene settings.
Traffic scene (Figure 11). Humans were absent
in most of traffic video. Object detector was able
to identify most prominent objects (e.g., car, bus)
Figure 11: A traffic scene with many vehicles ? seen in
?20041101 110000 CCTV4 NEWS3 CHN? from the HLF extrac-
tion task. Machine annotation: Many cars are present; Cars are
moving; A bus is present. Hand annotation 1: There is a red bus,
one yellow and many other cars on the highway; This is a scene of
daytime traffic; There is a blue road sign on the big tower; There is
also a bridge on the road. Hand annotation 2: There are many cars;
There is a fly-over; Some buses are running on the fly-over; There is
vehicle parapet; This is a traffic scene on a highway.
Figure 12: An action scene of two humans ? seen in
?20041101 160000 CCTV4 DAILY NEWS CHN? from the HLF
extraction task. Machine annotation: A woman is sitting while
a man is standing; There is a bus in the background; There is a car in
the background. Hand annotation 1: Two persons are talking; One
is a man and other is woman; The man is wearing formal clothes;
The man is standing and woman is sitting; A bus is travellings be-
hind. Hand annotation 2: Young woman is sitting on a chair in a
park and talking to man who is standing next to her.
for description. Hand annotations produced fur-
ther details such as colours of car and other ob-
jects (e.g., flyover, bridge). This sequence was
also described as a highway.
Action scene (Figure 12). Main interest was
to find humans and their activities. Successful
recognition of man, woman and their actions (e.g.,
?sitting?, ?standing?) led to well phrased descrip-
tion. The bus and the car at the background were
also identified. In hand annotations dressing was
noted and location was reported as a park.
5.2 Evaluation with ROUGE
Difficulty in evaluating natural language descrip-
tions stems from the fact that it is not a simple
task to define the criteria. We adopted ROUGE,
widely used for evaluating automatic summarisa-
tion (Lin, 2004), to calculate the overlap between
machine generated and hand annotations. Table
4 shows the results where higher ROUGE score
indicates closer match between them.
In overall scores were not very high, demon-
strating the fact that humans have different ob-
servations and interests while watching the same
video. Descriptions were often subjective, de-
33
Action Closeup In/Outdoor Grouping Meeting News Traffic
ROUGE-1 0.4369 0.5385 0.2544 0.3067 0.3330 0.4321 0.3121
ROUGE-2 0.3087 0.3109 0.1877 0.2619 0.2462 0.3218 0.1268
ROUGE-3 0.2994 0.2106 0.1302 0.1229 0.2400 0.2219 0.1250
ROUGE-L 0.4369 0.4110 0.2544 0.3067 0.3330 0.3321 0.3121
ROUGE-W 0.4147 0.4385 0.2877 0.3619 0.3265 0.3318 0.3147
ROUGE-S 0.3563 0.4193 0.2302 0.2229 0.2648 0.3233 0.3236
ROUGE-SU 0.3686 0.4413 0.2544 0.3067 0.2754 0.3419 0.3407
Table 4: ROUGE scores between machine generated descriptions (reference) and 13 hand annotations (model).
ROUGE 1-3 shows n-gram overlap similarity between reference and model descriptions. ROUGE-L is based on
longest common subsequence (LCS). ROUGE-W is for weighted LCS. ROUGE-S skips bigram co-occurrence
without gap length. ROUGE-SU shows results for skip bigram co-occurrence with unigrams.
pendent on one?s perception and understanding,
that might have been affected by their educa-
tional and professional background, personal in-
terests and experiences. Nevertheless ROUGE
scores were not hopelessly low for machine gen-
erated descriptions; Closeup, Action and News
videos had higher scores because of presence of
humans with well defined actions and emotions.
Indoor/Outdoor videos show the poorest results
due to the limited capability of image processing
techniques.
5.3 Task Based Evaluation by Human
Similar to human in the loop evaluation (Nwogu
et al, 2011), a task based evaluation was per-
formed to make qualitative evaluation of the gen-
erated descriptions. Given a machine generated
description, human subjects were instructed to
find a corresponding video stream out of 10 can-
didate videos having the same theme (e.g., a de-
scription of a Closeup against 10 Closeup videos).
Once a choice was made, each subject was pro-
vided with the correct video stream and a ques-
tionnaire. The first question was how well the de-
scription explained the actual video, rating from
?explained completely?, ?satisfactorily?, ?fairly?,
?poorly?, or ?does not explain?. The second ques-
tion was concerned with the ranking of usefulness
for including various visual contents (e.g., human,
objects, their moves, their relations, background)
in the description.
Seven human subjects conducted this evalua-
tion searching a corresponding video for each of
ten machine generated descriptions. They did not
involve creation of the dataset, hence they saw
these videos for the first time. On average, they
were able to identify correct videos for 53%6 of
6It is interesting to note the correct identification rate
descriptions. They rated 68%, 48%, and 40% of
descriptions explained the actual video ?fairly?,
?satisfactorily?, and ?completely?. Because mul-
tiple videos might have very similar text descrip-
tions, it was worth testing meaningfulness of de-
scriptions for choosing the corresponding video.
Finally, usefulness of visual contents had mix re-
sults. For about 84% of descriptions, subjects
were able to identify videos based on information
related to humans, their actions, emotions and in-
teractions with other objects.
6 Conclusion
This paper explored the bottom up approach to
describing video contents in natural language.
The conversion from quantitative information to
qualitative predicates was suitable for conceptual
data manipulation and natural language genera-
tion. The outcome of the experiments indicates
that the natural language formalism makes it pos-
sible to generate fluent, rich descriptions, allow-
ing for detailed and refined expressions. Future
works include detection of groups, extension of
behavioural models, more complex interactions
among humans and other objects.
Acknowledgements
Muhammad Usman Ghani Khan thanks Univer-
sity of Engineering & Technology, Lahore, Pak-
istan for funding his work under the Faculty De-
velopment Program.
went up to 70% for three subjects who also conducted cre-
ation of the dataset.
34
References
P. Baiget, C. Ferna?ndez, X. Roca, and J. Gonza`lez.
2007. Automatic learning of conceptual knowledge
in image sequences for human behavior interpre-
tation. Pattern Recognition and Image Analysis,
pages 507?514.
R.M. Bolle, B.L. Yeo, and M.M. Yeung. 1998. Video
query: Research directions. IBM Journal of Re-
search and Development, 42(2):233?252.
H.S. Chen, H.T. Chen, Y.W. Chen, and S.Y. Lee. 2006.
Human action recognition using star skeleton. In
Proceedings of the 4th ACM international workshop
on Video surveillance and sensor networks, pages
171?178. ACM.
M. Everingham, L. Van Gool, C.K.I. Williams,
J. Winn, and A. Zisserman. 2010. The pascal vi-
sual object classes (voc) challenge. International
Journal of Computer Vision, 88(2):303?338.
C. Galleguillos and S. Belongie. 2010. Context based
object categorization: A critical survey. Computer
Vision and Image Understanding, 114(6):712?722.
A. Gatt and E. Reiter. 2009. SimpleNLG: A real-
isation engine for practical applications. In Pro-
ceedings of the 12th European Workshop on Nat-
ural Language Generation, pages 90?93. Associa-
tion for Computational Linguistics.
W. Kim, J. Park, and C. Kim. 2010. A novel method
for efficient indoor?outdoor image classification.
Journal of Signal Processing Systems, pages 1?8.
A. Kojima, T. Tamura, and K. Fukunaga. 2002. Nat-
ural language description of human activities from
video images based on concept hierarchy of ac-
tions. International Journal of Computer Vision,
50(2):171?184.
P. Kuchi, P. Gabbur, P. SUBBANNA BHAT, et al
2002. Human face detection and tracking using skin
color modeling and connected component opera-
tors. IETE journal of research, 48(3-4):289?293.
M.W. Lee, A. Hakeem, N. Haering, and S.C. Zhu.
2008. Save: A framework for semantic annota-
tion of visual events. In Computer Vision and Pat-
tern Recognition Workshops. CVPRW?08, pages 1?
8. IEEE.
C.Y. Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. In WAS.
I. Maglogiannis, D. Vouyioukas, and C. Aggelopoulos.
2009. Face detection and recognition of natural hu-
man emotion using markov random fields. Personal
and Ubiquitous Computing, 13(1):95?101.
H.H. Nagel. 2004. Steps toward a cognitive vision
system. AI Magazine, 25(2):31.
I. Nwogu, Y. Zhou, and C. Brown. 2011. Disco: De-
scribing images using scene contexts and objects.
In Twenty-Fifth AAAI Conference on Artificial In-
telligence.
P. Over, W. Kraaij, and A.F. Smeaton. 2007. Trecvid
2007: an introduction. In TREC Video retrieval
evaluation online proceedings.
M.S. Ryoo and J.K. Aggarwal. 2009. Semantic rep-
resentation and recognition of continued and recur-
sive human activities. International journal of com-
puter vision, 82(1):1?24.
J.R.J. Schirra, G. Bosch, CK Sung, and G. Zimmer-
mann. 1987. From image sequences to natural
language: a first step toward automatic perception
and description of motions. Applied Artificial Intel-
ligence an International Journal, 1(4):287?305.
A. Sundaresan, A. RoyChowdhury, and R. Chellappa.
2003. A hidden markov model based framework for
recognition of humans from gait sequences. In In-
ternational Conference on Image Processing, ICIP
2003, volume 2. IEEE.
A. Torralba, K.P. Murphy, W.T. Freeman, and M.A.
Rubin. 2008. Context-based vision system for
place and object recognition. In Ninth IEEE In-
ternational Conference on Computer Vision, pages
273?280. IEEE.
P. Viola and M. Jones. 2001. Rapid object detection
using a boosted cascade of simple features. In IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition, volume 1.
Y. Yang, C.L. Teo, H. Daume? III, C. Fermu?ller, and
Y. Aloimonos. 2011. Corpus-guided sentence ger-
eration of natural images. In EMNLP.
B.Z. Yao, X. Yang, L. Lin, M.W. Lee, and S.C. Zhu.
2010. I2t: Image parsing to text description. Pro-
ceedings of the IEEE, 98(8):1485?1508.
W. Zheng, H. Tang, Z. Lin, and T. Huang. 2010. Emo-
tion recognition from arbitrary view facial images.
Computer Vision?ECCV 2010, pages 490?503.
35
