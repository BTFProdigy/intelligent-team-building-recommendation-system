Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 11?20,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
It?s a Contradiction?No, it?s Not:
A Case Study using Functional Relations
Alan Ritter, Doug Downey, Stephen Soderland and Oren Etzioni
Turing Center
Department of Computer Science and Engineering
University of Washington
Box 352350
Seattle, WA 98195, USA
{aritter,ddowney,soderlan,etzioni}@cs.washington.edu
Abstract
Contradiction Detection (CD) in text is a
difficult NLP task. We investigate CD
over functions (e.g., BornIn(Person)=Place),
and present a domain-independent algorithm
that automatically discovers phrases denoting
functions with high precision. Previous work
on CD has investigated hand-chosen sentence
pairs. In contrast, we automatically harvested
from the Web pairs of sentences that appear
contradictory, but were surprised to find that
most pairs are in fact consistent. For example,
?Mozart was born in Salzburg? does not con-
tradict ?Mozart was born in Austria? despite
the functional nature of the phrase ?was born
in?. We show that background knowledge
about meronyms (e.g., Salzburg is in Austria),
synonyms, functions, and more is essential for
success in the CD task.
1 Introduction and Motivation
Detecting contradictory statements is an important
and challenging NLP task with a wide range of
potential applications including analysis of politi-
cal discourse, of scientific literature, and more (de
Marneffe et al, 2008; Condoravdi et al, 2003;
Harabagiu et al, 2006). De Marneffe et al present a
model of CD that defines the task, analyzes different
types of contradictions, and reports on a CD system.
They report 23% precision and 19% recall at detect-
ing contradictions in the RTE-3 data set (Voorhees,
2008). Although RTE-3 contains a wide variety of
contradictions, it does not reflect the prevalence of
seeming contradictions and the paucity of genuine
contradictions, which we have found in our corpus.
1.1 Contradictions and World Knowledge
Our paper is motivated in part by de Marneffe et al?s
work, but with some important differences. First,
we introduce a simple logical foundation for the CD
task, which suggests that extensive world knowl-
edge is essential for building a domain-independent
CD system. Second, we automatically generate a
large corpus of apparent contradictions found in ar-
bitrary Web text. We show that most of these appar-
ent contradictions are actually consistent statements
due to meronyms (Alan Turing was born in London
and in England), synonyms (George Bush is mar-
ried to both Mrs. Bush and Laura Bush), hypernyms
(Mozart died of both renal failure and kidney dis-
ease), and reference ambiguity (one John Smith was
born in 1997 and a different John Smith in 1883).
Next, we show how background knowledge enables
a CD system to discard seeming contradictions and
focus on genuine ones.
De Marneffe et al introduced a typology of con-
tradiction in text, but focused primarily on contra-
dictions that can be detected from linguistic evi-
dence (e.g. negation, antonymy, and structural or
lexical disagreements). We extend their analysis to
a class of contradictions that can only be detected
utilizing background knowledge. Consider for ex-
ample the following sentences:
1) ?Mozart was born in Salzburg.?
2) ?Mozart was born in Vienna.?
3) ?Mozart visited Salzburg.?
4) ?Mozart visited Vienna.?
Sentences 1 & 2 are contradictory, but 3 & 4 are
not. Why is that? The distinction is not syntactic.
Rather, sentences 1 and 2 are contradictory because
11
the relation expressed by the phrase ?was born in?
can be characterized here as a function from peo-
ple?s names to their unique birthplaces. In contrast,
?visited? does not denote a functional relation.1
We cannot assume that a CD system knows, in
advance, all the functional relations that might ap-
pear in a corpus. Thus, a central challenge for a
function-based CD system is to determine which re-
lations are functional based on a corpus. Intuitively,
we might expect that ?functional phrases? such as
?was born in? would typically map person names
to unique place names, making function detection
easy. But, in fact, function detection is surprisingly
difficult because name ambiguity (e.g., John Smith),
common nouns (e.g., ?dad? or ?mom?), definite de-
scriptions (e.g., ?the president?), and other linguistic
phenomena can mask functions in text. For example,
the two sentences ?John Smith was born in 1997.?
and ?John Smith was born in 1883.? can be viewed
as either evidence that ?was born in? does not de-
note a function or, alternatively, that ?John Smith?
is ambiguous.
1.2 A CD System Based on Functions
We report on the AUCONTRAIRE CD system, which
addresses each of the above challenges. First, AU-
CONTRAIRE identifies ?functional phrases? statis-
tically (Section 3). Second, AUCONTRAIRE uses
these phrases to automatically create a large cor-
pus of apparent contradictions (Section 4.2). Fi-
nally, AUCONTRAIRE sifts through this corpus to
find genuine contradictions using knowledge about
synonymy, meronymy, argument types, and ambi-
guity (Section 4.3).
Instead of analyzing sentences directly, AUCON-
TRAIRE relies on the TEXTRUNNER Open Informa-
tion Extraction system (Banko et al, 2007; Banko
and Etzioni, 2008) to map each sentence to one or
more tuples that represent the entities in the sen-
tences and the relationships between them (e.g.,
was born in(Mozart,Salzburg)). Using extracted tu-
ples greatly simplifies the CD task, because nu-
merous syntactic problems (e.g., anaphora, rela-
tive clauses) and semantic challenges (e.g., quantifi-
cation, counterfactuals, temporal qualification) are
1Although we focus on function-based CD in our case study,
we believe that our observations apply to other types of CD as
well.
delegated to TEXTRUNNER or simply ignored. Nev-
ertheless, extracted tuples are a convenient approxi-
mation of sentence content, which enables us to fo-
cus on function detection and function-based CD.
Our contributions are the following:
? We present a novel model of the Contradiction
Detection (CD) task, which offers a simple log-
ical foundation for the task and emphasizes the
central role of background knowledge.
? We introduce and evaluate a new EM-style al-
gorithm for detecting whether phrases denote
functional relations and whether nouns (e.g.,
?dad?) are ambiguous, which enables a CD sys-
tem to identify functions in arbitrary domains.
? We automatically generate a corpus of seem-
ing contradictions from Web text, and report
on a set of experiments over this corpus, which
provide a baseline for future work on statistical
function identification and CD. 2
2 A Logical Foundation for CD
On what basis can a CD system conclude that two
statements T and H are contradictory? Logically,
contradiction holds when T |= ?H . As de Marneffe
et al point out, this occurs when T and H contain
antonyms, negation, or other lexical elements that
suggest that T and H are directly contradictory. But
other types of contradictions can only be detected
with the help of a body of background knowledge
K: In these cases, T and H alone are mutually con-
sistent. That is,
T |=\ ?H ?H |=\ ?T
A contradiction between T and H arises only in
the context of K. That is:
((K ? T ) |= ?H) ? ((K ?H) |= ?T )
Consider the example of Mozart?s birthplace in
the introduction. To detect a contradiction, a CD
system must know that A) ?Mozart? refers to the
same entity in both sentences, that B) ?was born in?
denotes a functional relation, and that C) Vienna and
Salzburg are inconsistent locations.
2The corpus is available at http://www.cs.
washington.edu/research/aucontraire/
12
Of course, world knowledge, and reasoning about
text, are often uncertain, which leads us to associate
probabilities with a CD system?s conclusions. Nev-
ertheless, the knowledge base K is essential for CD.
We now turn to a probabilistic model that helps
us simultaneously estimate the functionality of re-
lations (B in the above example) and ambiguity of
argument values (A above). Section 4 describes the
remaining components of AUCONTRAIRE.
3 Detecting Functionality and Ambiguity
This section introduces a formal model for comput-
ing the probability that a phrase denotes a function
based on a set of extracted tuples. An extracted tuple
takes the form R(x, y) where (roughly) x is the sub-
ject of a sentence, y is the object, and R is a phrase
denoting the relationship between them. If the re-
lation denoted by R is functional, then typically the
object y is a function of the subject x. Thus, our dis-
cussion focuses on this possibility, though the anal-
ysis is easily extended to the symmetric case.
Logically, a relation R is functional in a vari-
able x if it maps it to a unique variable y:
?x, y1, y2 R(x, y1) ? R(x, y2) ? y1 = y2. Thus,
given a large random sample of ground instances of
R, we could detect with high confidence whether R
is functional. In text, the situation is far more com-
plex due to ambiguity, polysemy, synonymy, and
other linguistic phenomena. Deciding whether R is
functional becomes a probabilistic assessment based
on aggregated textual evidence.
The main evidence that a relation R(x, y) is func-
tional comes from the distribution of y values for
a given x value. If R denotes a function and x is
unambiguous, then we expect the extractions to be
predominantly a single y value, with a few outliers
due to noise. We aggregate the evidence that R is
locally functional for a particular x value to assess
whether R is globally functional for all x.
We refer to a set of extractions with the same
relation R and argument x as a contradiction set
R(x, ?). Figure 1 shows three example contradic-
tion sets. Each example illustrates a situation com-
monly found in our data. Example A in Figure 1
shows strong evidence for a functional relation. 66
out of 70 TEXTRUNNER extractions for was born in
(Mozart, PLACE) have the same y value. An am-
biguous x argument, however, can make a func-
tional relation appear non-functional. Example B
depicts a distribution of y values that appears less
functional due to the fact that ?John Adams? refers
to multiple, distinct real-world individuals with that
name. Finally, example C exhibits evidence for a
non-functional relation.
A. was born in(Mozart, PLACE):
Salzburg(66), Germany(3), Vienna(1)
B. was born in(John Adams, PLACE):
Braintree(12), Quincy(10), Worcester(8)
C. lived in(Mozart, PLACE):
Vienna(20), Prague(13), Salzburg(5)
Figure 1: Functional relations such as example A have a
different distribution of y values than non-functional rela-
tions such as C. However, an ambiguous x argument as in
B, can make a functional relation appear non-functional.
3.1 Formal Model of Functions in Text
To decide whether R is functional in x for all x,
we first consider how to detect whether R is lo-
cally functional for a particular value of x. The local
functionality of R with respect to x is the probabil-
ity that R is functional estimated solely on evidence
from the distribution of y values in a contradiction
set R(x, ?).
To decide the probability that R is a function, we
define global functionality as the average local func-
tionality score for each x, weighted by the probabil-
ity that x is unambiguous. Below, we outline an EM-
style algorithm that alternately estimates the proba-
bility that R is functional and the probability that x
is ambiguous.
Let R?x indicate the event that the relation R is
locally functional for the argument x, and that x is
locally unambiguous for R. Also, let D indicate
the set of observed tuples, and define DR(x,?) as the
multi-set containing the frequencies for extractions
of the form R(x, ?). For example the distribution of
extractions from Figure 1 for example A is
Dwas born in(Mozart,?) = {66, 3, 1}.
Let ?fR be the probability that R(x, ?) is locally
functional for a random x, and let ?f be the vector
of these parameters across all relations R. Likewise,
?ux represents the probability that x is locally unam-
biguous for random R, and ?u the vector for all x.
13
We wish to determine the maximum a pos-
teriori (MAP) functionality and ambiguity pa-
rameters given the observed data D, that is
arg max?f ,?u P (?
f ,?u|D). By Bayes Rule:
P (?f ,?u|D) =
P (D|?f ,?u)P (?f ,?u)
P (D)
(1)
We outline a generative model for the data,
P (D|?f ,?u). Let us assume that the event R?x de-
pends only on ?fR and ?
u
x , and further assume that
given these two parameters, local ambiguity and lo-
cal functionality are conditionally independent. We
obtain the following expression for the probability
of R?x given the parameters:
P (R?x|?
f ,?u) = ?fR?
u
x
We assume each set of data DR(x,?) is gener-
ated independently of all other data and parameters,
given R?x. From this and the above we have:
P (D|?f ,?u) =
?
R,x
(
P (DR(x,?)|R
?
x)?
f
R?
u
x
+P (DR(x,?)|?R
?
x)(1? ?
f
R?
u
x)
)
(2)
These independence assumptions allow us to ex-
press P (D|?f ,?u) in terms of distributions over
DR(x,?) given whether or not R
?
x holds. We use the
URNS model as described in (Downey et al, 2005)
to estimate these probabilities based on binomial
distributions. In the single-urn URNS model that we
utilize, the extraction process is modeled as draws of
labeled balls from an urn, where the labels are either
correct extractions or errors, and different labels can
be repeated on varying numbers of balls in the urn.
Let k = maxDR(x,?), and let n =
?
DR(x,?);
we will approximate the distribution over DR(x,?)
in terms of k and n. If R(x, ?) is locally func-
tional and unambiguous, there is exactly one cor-
rect extraction label in the urn (potentially repeated
multiple times). Because the probability of correct-
ness tends to increase with extraction frequency, we
make the simplifying assumption that the most fre-
quently extracted element is correct.3 In this case, k
is the number of correct extractions, which by the
3As this assumption is invalid when there is not a unique
maximal element, we default to the prior P (R?x) in that case.
URNS model has a binomial distribution with pa-
rameters n and p, where p is the precision of the ex-
traction process. If R(x, ?) is not locally functional
and unambiguous, then we expect k to typically take
on smaller values. Empirically, the underlying fre-
quency of the most frequent element in the?R?x case
tends to follow a Beta distribution.
Under the model, the probability of the evidence
given R?x is:
P (DR(x,?)|R
?
x) ? P (k, n|R
?
x) =
(
n
k
)
pk(1? p)n?k
And the probability of the evidence given ?R?x is:
P (DR(x,?)|?R
?
x) ? P (k, n|?R
?
x)
=
(n
k
) ? 1
0
p?k+?f?1(1?p?)n+?f?1?k
B(?f ,?f )
dp?
=
(n
k
)
?(n? k + ?f )?(?f + k)
B(?f , ?f )?(?f + ?f + n)
(3)
where n is the sum over DR(x,?), ? is the Gamma
function and B is the Beta function. ?f and ?f are
the parameters of the Beta distribution for the ?R?x
case. These parameters and the prior distributions
are estimated empirically, based on a sample of the
data set of relations described in Section 5.1.
3.2 Estimating Functionality and Ambiguity
Substituting Equation 3 into Equation 2 and apply-
ing an appropriate prior gives the probability of pa-
rameters ?f and ?u given the observed data D.
However, Equation 2 contains a large product of
sums?with two independent vectors of coefficients,
?f and ?u?making it difficult to optimize analyti-
cally.
If we knew which arguments were ambiguous,
we would ignore them in computing the function-
ality of a relation. Likewise, if we knew which rela-
tions were non-functional, we would ignore them in
computing the ambiguity of an argument. Instead,
we initialize the ?f and ?u arrays randomly, and
then execute an algorithm similar to Expectation-
Maximization (EM) (Dempster et al, 1977) to arrive
at a high-probability setting of the parameters.
Note that if ?u is fixed, we can compute the ex-
pected fraction of locally unambiguous arguments x
for which R is locally functional, using DR(x?,?) and
14
Equation 3. Likewise, for fixed ?f , for any given
x we can compute the expected fraction of locally
functional relations R that are locally unambiguous
for x.
Specifically, we repeat until convergence:
1. Set ?fR =
1
sR
?
x P (R
?
x|DR(x,?))?
u
x for all R.
2. Set ?ux =
1
sx
?
R P (R
?
x|DR(x,?))?
f
R for all x.
In both steps above, the sums are taken over only
those x or R for which DR(x,?) is non-empty. Also,
the normalizer sR =
?
x ?
u
x and likewise sx =?
R ?
f
R.
As in standard EM, we iteratively update our pa-
rameter values based on an expectation computed
over the unknown variables. However, we alter-
nately optimize two disjoint sets of parameters (the
functionality and ambiguity parameters), rather than
just a single set of parameters as in standard EM.
Investigating the optimality guarantees and conver-
gence properties of our algorithm is an item of future
work.
By iteratively setting the parameters to the expec-
tations in steps 1 and 2, we arrive at a good setting
of the parameters. Section 5.2 reports on the perfor-
mance of this algorithm in practice.
4 System Overview
AUCONTRAIRE identifies phrases denoting func-
tional relations and utilizes these to find contradic-
tory assertions in a massive, open-domain corpus of
text.
AUCONTRAIRE begins by finding extractions of
the form R(x, y), and identifies a set of relations
R that have a high probability of being functional.
Next, AUCONTRAIRE identifies contradiction sets
of the form R(x, ?). In practice, most contradiction
sets turned out to consist overwhelmingly of seem-
ing contradictions?assertions that do not actually
contradict each other for a variety of reasons that
we enumerate in section 4.3. Thus, a major chal-
lenge for AUCONTRAIRE is to tease apart which
pairs of assertions in R(x, ?) represent genuine con-
tradictions.
Here are the main components of AUCONTRAIRE
as illustrated in Figure 2:
Extractor: Create a set of extracted assertions E
from a large corpus of Web pages or other docu-
ments. Each extraction R(x, y) has a probability p
Figure 2: AUCONTRAIRE architecture
of being correct.
Function Learner: Discover a set of functional re-
lations F from among the relations in E . Assign to
each relation in F a probability pf that it is func-
tional.
Contradiction Detector: Query E for assertions
with a relation R in F , and identify sets C of po-
tentially contradictory assertions. Filter out seeming
contradictions in C by reasoning about synonymy,
meronymy, argument types, and argument ambigu-
ity. Assign to each potential contradiction a proba-
bility pc that it is a genuine contradiction.
4.1 Extracting Factual Assertions
AUCONTRAIRE needs to explore a large set of
factual assertions, since genuine contradictions are
quite rare (see Section 5). We used a set of extrac-
tions E from the Open Information Extraction sys-
tem, TEXTRUNNER (Banko et al, 2007), which was
run on a set of 117 million Web pages.
TEXTRUNNER does not require a pre-defined set
of relations, but instead uses shallow linguistic anal-
ysis and a domain-independent model to identify
phrases from the text that serve as relations and
phrases that serve as arguments to that relation.
TEXTRUNNER creates a set of extractions in a sin-
gle pass over the Web page collection and provides
an index to query the vast set of extractions.
Although its extractions are noisy, TEXTRUNNER
provides a probability that the extractions are cor-
15
rect, based in part on corroboration of facts from
different Web pages (Downey et al, 2005).
4.2 Finding Potential Contradictions
The next step of AUCONTRAIRE is to find contra-
diction sets in E .
We used the methods described in Section 3 to
estimate the functionality of the most frequent rela-
tions in E . For each relation R that AUCONTRAIRE
has judged to be functional, we identify contradic-
tion sets R(x, ?), where a relation R and domain ar-
gument x have multiple range arguments y.
4.3 Handling Seeming Contradictions
For a variety of reasons, a pair of extractions
R(x, y1) and R(x, y2) may not be actually contra-
dictory. The following is a list of the major sources
of false positives?pairs of extractions that are not
genuine contradictions, and how they are handled
by AUCONTRAIRE. The features indicative of each
condition are combined using Logistic Regression,
in order to estimate the probability that a given pair,
{R(x, y1), R(x, y2)} is a genuine contradiction.
Synonyms: The set of potential contradictions
died from(Mozart,?) may contain assertions that
Mozart died from renal failure and that he died from
kidney failure. These are distinct values of y, but
do not contradict each other, as the two terms are
synonyms. AUCONTRAIRE uses a variety of knowl-
edge sources to handle synonyms. WordNet is a re-
liable source of synonyms, particularly for common
nouns, but has limited recall. AUCONTRAIRE also
utilizes synonyms generated by RESOLVER (Yates
and Etzioni, 2007)? a system that identifies syn-
onyms from TEXTRUNNER extractions. Addition-
ally, AUCONTRAIRE uses edit-distance and token-
based string similarity (Cohen et al, 2003) between
apparently contradictory values of y to identify syn-
onyms.
Meronyms: For some relations, there is no con-
tradiction when y1 and y2 share a meronym,
i.e. ?part of? relation. For example, in the set
born in(Mozart,?) there is no contradiction be-
tween the y values ?Salzburg? and ?Austria?, but
?Salzburg? conflicts with ?Vienna?. Although this
is only true in cases where y occurs in an up-
ward monotone context (MacCartney and Manning,
2007), in practice genuine contradictions between
y-values sharing a meronym relationship are ex-
tremely rare. We therefore simply assigned contra-
dictions between meronyms a probability close to
zero. We used the Tipster Gazetteer4 and WordNet
to identify meronyms, both of which have high pre-
cision but low coverage.
Argument Typing: Two y values are not contra-
dictory if they are of different argument types. For
example, the relation born in can take a date or a
location for the y value. While a person can be
born in only one year and in only one city, a per-
son can be born in both a year and a city. To avoid
such false positives, AUCONTRAIRE uses a sim-
ple named-entity tagger5 in combination with large
dictionaries of person and location names to as-
sign high-level types (person, location, date, other)
to each argument. AUCONTRAIRE filters out ex-
tractions from a contradiction set that do not have
matching argument types.
Ambiguity: As pointed out in Section 3, false con-
tradictions arise when a single x value refers to mul-
tiple real-world entities. For example, if the con-
tradiction set born in(John Sutherland, ?) includes
birth years of both 1827 and 1878, is one of these a
mistake, or do we have a grandfather and grandson
with the same name? AUCONTRAIRE computes the
probability that an x value is unambiguous as part
of its Function Learner (see Section 3). An x value
can be identified as ambiguous if its distribution of
y values is non-functional for multiple functional re-
lations.
If a pair of extractions, {R(x, y1), R(x, y2)}, does
not fall into any of the above categories and R is
functional, then it is likely that the sentences under-
lying the extractions are indeed contradictory. We
combined the various knowledge sources described
above using Logistic Regression, and used 10-fold
cross-validation to automatically tune the weights
associated with each knowledge source. In addi-
tion, the learning algorithm also utilizes the follow-
ing features:
? Global functionality of the relation, ?fR
? Global unambiguity of x, ?ux
4http://crl.nmsu.edu/cgi-bin/Tools/CLR/
clrcat
5http://search.cpan.org/?simon/
Lingua-EN-NamedEntity-1.1/NamedEntity.pm
16
? Local functionality of R(x, ?)
? String similarity (a combination of token-based
similarity and edit-distance) between y1 and y2
? The argument types (person, location, date, or
other)
The learned model is then used to estimate how
likely a potential contradiction {R(x, y1), R(x, y2)}
is to be genuine.
5 Experimental Results
We evaluated several aspects of AUCONTRAIRE:
its ability to detect functional relations and to de-
tect ambiguous arguments (Section 5.2); its preci-
sion and recall in contradiction detection (Section
5.3); and the contribution of AUCONTRAIRE?s key
knowledge sources (Section 5.4).
5.1 Data Set
To evaluate AUCONTRAIRE we used TEXTRUN-
NER?s extractions from a corpus of 117 million Web
pages. We restricted our data set to the 1,000 most
frequent relations, in part to keep the experiments
tractable and also to ensure sufficient statistical sup-
port for identifying functional relations.
We labeled each relation as functional or not,
and computed an estimate of the probability it is
functional as described in section 3.2. Section 5.2
presents the results of the Function Learner on this
set of relations. We took the top 2% (20 relations)
as F , the set of functional relations in our exper-
iments. Out of these, 75% are indeed functional.
Some examples include: was born in, died in, and
was founded by.
There were 1.2 million extractions for all thou-
sand relations, and about 20,000 extractions in 6,000
contradiction sets for all relations in F .
We hand-tagged 10% of the contradiction sets
R(x, ?) where R ? F , discarding any sets with over
20 distinct y values since the x argument for that
set is almost certainly ambiguous. This resulted in a
data set of 567 contradiction sets containing a total
of 2,564 extractions and 8,844 potentially contradic-
tory pairs of extractions.
We labeled each of these 8,844 pairs as contradic-
tory or not. In each case, we inspected the original
sentences, and if the distinction was unclear, con-
sulted the original source Web pages, Wikipedia ar-
ticles, and Web search engine results.
In our data set, genuine contradictions over func-
tional relations are surprisingly rare. We found only
110 genuine contradictions in the hand-tagged sam-
ple, only 1.2% of the potential contradiction pairs.
5.2 Detecting Functionality and Ambiguity
We ran AUCONTRAIRE?s EM algorithm on the
thousand most frequent relations. Performance con-
verged after 5 iterations resulting in estimates of the
probability that each relation is functional and each
x argument is unambiguous. We used these proba-
bilities to generate the precision-recall curves shown
in Figure 3.
The graph on the left shows results for function-
ality, while the graph on the right shows precision at
finding unambiguous arguments. The solid lines are
results after 5 iterations of EM, and the dashed lines
are from computing functionality or ambiguity with-
out EM (i.e. assuming uniform values of ?c when
computing ?f and vice versa). The EM algorithm
improved results for both functionality and ambigu-
ity, increasing area under curve (AUC) by 19% for
functionality and by 31% for ambiguity.
Of course, the ultimate test of how well AUCON-
TRAIRE can identify functional relations is how well
the Contradiction Detector performs on automati-
cally identified functional relations.
5.3 Detecting Contradictions
We conducted experiments to evaluate how well
AUCONTRAIRE distinguishes genuine contradic-
tions from false positives.
The bold line in Figure 4 depicts AUCONTRAIRE
performance on the distribution of contradictions
and seeming contradictions found in actual Web
data. The dashed line shows the performance of AU-
CONTRAIRE on an artificially ?balanced? data set
that we constructed to contain 50% genuine contra-
dictions and 50% seeming ones.
Previous research in CD presented results on
manually selected data sets with a relatively bal-
anced mix of positive and negative instances. As
Figure 4 suggests, this is a much easier problem than
CD ?in the wild?. The data gathered from the Web
is badly skewed, containing only 1.2% genuine con-
tradictions.
17
Functionality
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
AuContraireNo Iteration
Ambiguity
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
AuContraireNo Iteration
Figure 3: After 5 iterations of EM, AUCONTRAIRE achieves a 19% boost to area under the precision-recall curve
(AUC) for functionality detection, and a 31% boost to AUC for ambiguity detection.
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Web DistributionBalanced Data
Figure 4: Performance of AUCONTRAIRE at distinguish-
ing genuine contradictions from false positives. The bold
line is results on the actual distribution of data from the
Web. The dashed line is from a data set constructed to
have 50% positive and 50% negative instances.
5.4 Contribution of Knowledge Sources
We carried out an ablation study to quantify how
much each knowledge source contributes to AU-
CONTRAIRE?s performance. Since most of the
knowledge sources do not apply to numeric argu-
ment values, we excluded the extractions where y
is a number in this study. As shown in Figure 5,
performance of AUCONTRAIRE degrades with no
knowledge of synonyms (NS), with no knowledge
of meronyms (NM), and especially without argu-
ment typing (NT). Conversely, improvements to any
of these three components would likely improve the
performance of AUCONTRAIRE.
The relatively small drop in performance from
no meronyms does not indicate that meronyms are
not essential to our task, only that our knowledge
sources for meronyms were not as useful as we
hoped. The Tipster Gazetteer has surprisingly low
coverage for our data set. It contains only 41% of
the y values that are locations. Many of these are
matches on a different location with the same name,
which results in incorrect meronym information. We
estimate that a gazetteer with complete coverage
would increase area under the curve by approxi-
mately 40% compared to a system with meronyms
from the Tipster Gazetteer and WordNet.
AuContraire NS NM NT
Percentage AUC
0
20
40
60
80
100
Figure 5: Area under the precision-recall curve for the
full AUCONTRAIRE and for AUCONTRAIRE with knowl-
edge removed. NS has no synonym knowledge; NM has
no meronym knowledge; NT has no argument typing.
To analyze the errors made by AUCONTRAIRE,
we hand-labeled all false-positives at the point of
maximum F-score: 29% Recall and 48% Precision.
18
Figure 6 reveals the central importance of world
knowledge for the CD task. About half of the errors
(49%) are due to ambiguous x-arguments, which we
found to be one of the most persistent obstacles to
discovering genuine contradictions. A sizable por-
tion is due to missing meronyms (34%) and missing
synonyms (14%), suggesting that lexical resources
with broader coverage than WordNet and the Tipster
Gazetteer would substantially improve performance.
Surprisingly, only 3% are due to errors in the extrac-
tion process.
Extraction Errors (3%)
Missing Synonyms (14%)
Missing Meronyms (34%)
Ambiguity (49%)
Figure 6: Sources of errors in contradiction detection.
All of our experimental results are based on the
automatically discovered set of functions F . We
would expect AUCONTRAIRE?s performance to im-
prove substantially if it were given a large set of
functional relations as input.
6 Related Work
Condoravdi et al (2003) first proposed contradiction
detection as an important NLP task, and Harabagiu
et al (2006) were the first to report results on con-
tradiction detection using negation, although their
evaluation corpus was a balanced data set built
by manually negating entailments in a data set
from the Recognizing Textual Entailment confer-
ences (RTE) (Dagan et al, 2005). De Marneffe et
al. (2008) reported experimental results on a contra-
diction corpus created by annotating the RTE data
sets.
RTE-3 included an optional task, requiring sys-
tems to make a 3-way distinction: {entails, contra-
dicts, neither} (Voorhees, 2008). The average per-
formance for contradictions on the RTE-3 was preci-
sion 0.11 at recall 0.12, and the best system had pre-
cision 0.23 at recall 0.19. We did not run AUCON-
TRAIRE on the RTE data sets because they contained
relatively few of the ?functional contradictions? that
AUCONTRAIRE tackles. On our Web-based data
sets, we achieved a precision of 0.62 at recall 0.19,
and precision 0.92 at recall 0.51 on the balanced data
set. Of course, comparisons across very different
data sets are not meaningful, but merely serve to un-
derscore the difficulty of the CD task.
In contrast to previous work, AUCONTRAIRE is
the first to do CD on data automatically extracted
from the Web. This is a much harder problem than
using an artificially balanced data set, as shown in
Figure 4.
Automatic discovery of functional relations has
been addressed in the database literature as Func-
tional Dependency Mining (Huhtala et al, 1999;
Yao and Hamilton, 2008). This focuses on dis-
covering functional relationships between sets of at-
tributes, and does not address the ambiguity inherent
in natural language.
7 Conclusions and Future Work
We have described a case study of contradiction de-
tection (CD) based on functional relations. In this
context, we introduced and evaluated the AUCON-
TRAIRE system and its novel EM-style algorithm
for determining whether an arbitrary phrase is func-
tional. We also created a unique ?natural? data set
of seeming contradictions based on sentences drawn
from a Web corpus, which we make available to the
research community.
We have drawn two key lessons from our case
study. First, many seeming contradictions (approx-
imately 99% in our experiments) are not genuine
contradictions. Thus, the CD task may be much
harder on natural data than on RTE data as sug-
gested by Figure 4. Second, extensive background
knowledge is necessary to tease apart seeming con-
tradictions from genuine ones. We believe that these
lessons are broadly applicable, but verification of
this claim is a topic for future work.
Acknowledgements
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, ONR grant N00014-
08-1-0431 as well as gifts from the Utilika Founda-
tion and Google, and was carried out at the Univer-
sity of Washington?s Turing Center.
19
References
M. Banko and O. Etzioni. 2008. The tradeoffs between
traditional and open relation extraction. In Proceed-
ings of ACL.
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
W.W. Cohen, P. Ravikumar, and S.E. Fienberg. 2003.
A comparison of string distance metrics for name-
matching tasks. In IIWeb.
Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Rein-
hard Stolle, and Daniel G. Bobrow. 2003. Entailment,
intensionality and text understanding. In Proceedings
of the HLT-NAACL 2003 workshop on Text meaning,
pages 38?45, Morristown, NJ, USA. Association for
Computational Linguistics.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
Marie-Catherine de Marneffe, Anna Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In ACL 2008.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical Society Se-
ries B, 39(1):1?38.
D. Downey, O. Etzioni, and S. Soderland. 2005. A Prob-
abilistic Model of Redundancy in Information Extrac-
tion. In Procs. of IJCAI.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing. In AAAI.
Yka? Huhtala, Juha Ka?rkka?inen, Pasi Porkka, and Hannu
Toivonen. 1999. TANE: An efficient algorithm for
discovering functional and approximate dependencies.
The Computer Journal, 42(2):100?111.
B. MacCartney and C.D. Manning. 2007. Natural Logic
for Textual Inference. In Workshop on Textual Entail-
ment and Paraphrasing.
Ellen M. Voorhees. 2008. Contradictions and justifica-
tions: Extensions to the textual entailment task. In
Proceedings of ACL-08: HLT, pages 63?71, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hong Yao and Howard J. Hamilton. 2008. Mining func-
tional dependencies from data. Data Min. Knowl. Dis-
cov., 16(2):197?219.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
20
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583?593,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Data-Driven Response Generation in Social Media
Alan Ritter
Computer Sci. & Eng.
University of Washington
Seattle, WA 98195
aritter@cs.washington.edu
Colin Cherry
National Research Council Canada
Ottawa, Ontario, K1A 0R6
Colin.Cherry@nrc-cnrc.gc.ca
William B. Dolan
Microsoft Research
Redmond, WA 98052
billdol@microsoft.com
Abstract
We present a data-driven approach to generat-
ing responses to Twitter status posts, based on
phrase-based Statistical Machine Translation.
We find that mapping conversational stimuli
onto responses is more difficult than translat-
ing between languages, due to the wider range
of possible responses, the larger fraction of
unaligned words/phrases, and the presence of
large phrase pairs whose alignment cannot be
further decomposed. After addressing these
challenges, we compare approaches based on
SMT and Information Retrieval in a human
evaluation. We show that SMT outperforms
IR on this task, and its output is preferred over
actual human responses in 15% of cases. As
far as we are aware, this is the first work to
investigate the use of phrase-based SMT to di-
rectly translate a linguistic stimulus into an ap-
propriate response.
1 Introduction
Recently there has been an explosion in the number
of people having informal, public conversations on
social media websites such as Facebook and Twit-
ter. This presents a unique opportunity to build
collections of naturally occurring conversations that
are orders of magnitude larger than those previously
available. These corpora, in turn, present new op-
portunities to apply data-driven techniques to con-
versational tasks.
We investigate the problem of response genera-
tion: given a conversational stimulus, generate an
appropriate response. Specifically, we employ a
large corpus of status-response pairs found on Twit-
ter to create a system that responds to Twitter status
posts. Note that we make no mention of context, in-
tent or dialogue state; our goal is to generate any re-
sponse that fits the provided stimulus; however, we
do so without employing rules or templates, with the
hope of creating a system that is both flexible and
extensible when operating in an open domain.
Success in open domain response generation
could be immediately useful to social media plat-
forms, providing a list of suggested responses to a
target status, or providing conversation-aware auto-
complete for responses in progress. These features
are especially important on hand-held devices (Has-
selgren et al, 2003). Response generation should
also be beneficial in building ?chatterbots? (Weizen-
baum, 1966) for entertainment purposes or compan-
ionship (Wilks, 2006). However, we are most ex-
cited by the future potential of data-driven response
generation when used inside larger dialogue sys-
tems, where direct consideration of the user?s utter-
ance could be combined with dialogue state (Wong
and Mooney, 2007; Langner et al, 2010) to generate
locally coherent, purposeful dialogue.
In this work, we investigate statistical machine
translation as an approach for response generation.
We are motivated by the following observation: In
naturally occurring discourse, there is often a strong
structural relationship between adjacent utterances
(Hobbs, 1985). For example, consider the stimulus-
response pair from the data:
Stimulus: I?m slowly making this soup
...... and it smells gorgeous!
583
Response: I?ll bet it looks delicious too!
Haha
Here ?it? in the response refers to ?this soup? in
the status by co-reference; however, there is also a
more subtle relationship between the ?smells? and
?looks?, as well as ?gorgeous? and ?delicious?. Par-
allelisms such as these are frequent in naturally oc-
curring conversations, leading us to ask whether it
might be possible to translate a stimulus into an ap-
propriate response. We apply SMT to this problem,
treating Twitter as our parallel corpus, with status
posts as our source language and their responses as
our target language. However, the established SMT
pipeline cannot simply be applied out of the box.
We identify two key challenges in adapting SMT
to the response generation task. First, unlike bilin-
gual text, stimulus-response pairs are not semanti-
cally equivalent, leading to a wider range of possible
responses for a given stimulus phrase. Furthermore,
both sides of our parallel text are written in the same
language. Thus, the most strongly associated word
or phrase pairs found by off-the-shelf word align-
ment and phrase extraction tools are identical pairs.
We address this issue with constraints and features to
limit lexical overlap. Secondly, in stimulus-response
pairs, there are far more unaligned words than in
bilingual pairs; it is often the case that large portions
of the stimulus are not referenced in the response
and vice versa. Also, there are more large phrase-
pairs that cannot be easily decomposed (for example
see figure 2). These difficult cases confuse the IBM
word alignment models. Instead of relying on these
alignments to extract phrase-pairs, we consider all
possible phrase-pairs in our parallel text, and apply
an association-based filter.
We compare our approach to response genera-
tion against two Information Retrieval or nearest
neighbour approaches, which use the input stimu-
lus to select a response directly from the training
data. We analyze the advantages and disadvantages
of each approach, and perform an evaluation using
human annotators from Amazon?s Mechanical Turk.
Along the way, we investigate the utility of SMT?s
BLEU evaluation metric when applied to this do-
main. We show that SMT-based solutions outper-
form IR-based solutions, and are chosen over actual
human responses in our data in 15% of cases. As far
as we are aware, this is the first work to investigate
the feasibility of SMT?s application to generating re-
sponses to open-domain linguistic stimuli.
2 Related Work
There has been a long history of ?chatterbots?
(Weizenbaum, 1966; Isbell et al, 2000; Shaikh et
al., 2010), which attempt to engage users, typically
leading the topic of conversation. They usually limit
interactions to a specific scenario (e.g. a Rogerian
psychotherapist), and use a set of template rules for
generating responses. In contrast, we focus on the
simpler task of generating an appropriate response
to a single utterance. We leverage large amounts of
conversational training data to scale to our Social
Media domain, where conversations can be on just
about any topic.
Additionally, there has been work on generat-
ing more natural utterances in goal-directed dia-
logue systems (Ratnaparkhi, 2000; Rambow et al,
2001). Currently, most dialogue systems rely on ei-
ther canned responses or templates for generation,
which can result in utterances which sound very
unnatural in context (Chambers and Allen, 2004).
Recent work has investigated the use of SMT in
translating internal dialogue state into natural lan-
guage (Langner et al, 2010). In addition to dialogue
state, we believe it may be beneficial to consider
the user?s utterance when generating responses in or-
der to generate locally coherent discourse (Barzilay
and Lapata, 2005). Data-driven generation based on
users? utterances might also be a useful way to fill in
knowledge gaps in the system (Galley et al, 2001;
Knight and Hatzivassiloglou, 1995).
Statistical machine translation has been applied to
a smo?rga?sbord of NLP problems, including question
answering (Echihabi and Marcu, 2003), semantic
parsing and generation (Wong and Mooney, 2006;
Wong and Mooney, 2007), summarization (Daume?
III and Marcu, 2009), generating bid-phrases in on-
line advertising (Ravi et al, 2010), spelling correc-
tion (Sun et al, 2010), paraphrase (Dolan et al,
2004; Quirk et al, 2004) and query expansion (Rie-
zler et al, 2007). Most relevant to our efforts is the
work by Soricut and Marcu (2006), who applied the
IBM word alignment models to a discourse order-
ing task, exploiting the same intuition investigated
584
in this paper: certain words (or phrases) tend to trig-
ger the usage of other words in subsequent discourse
units. As far as we are aware, ours is the first work
to explore the use of phrase-based translation in gen-
erating responses to open-domain linguistic stimuli,
although the analogy between translation and dia-
logue has been drawn (Leuski and Traum, 2010).
3 Data
For learning response-generation models, we use
a corpus of roughly 1.3 million conversations
scraped from Twitter (Ritter et al, 2010; Danescu-
Niculescu-Mizil et al, 2011). Twitter conversations
don?t occur in real-time as in IRC; rather as in email,
users typically take turns responding to each other.
Twitter?s 140 character limit, however, keeps con-
versations chat-like. In addition, the Twitter API
maintains a reference from each reply to the post
it responds to, so unlike IRC, there is no need for
conversation disentanglement (Elsner and Charniak,
2008; Wang and Oard, 2009). The first message of a
conversation is typically unique, not directed at any
particular user but instead broadcast to the author?s
followers (a status message). For the purposes of
this paper, we limit the data set to only the first two
utterances from each conversation. As a result of
this constraint, any system trained with this data will
be specialized for responding to Twitter status posts.
4 Response Generation as Translation
When applied to conversations, SMT models the
probability of a response r given the input status-
post s using a log-linear combination of feature
functions. Most prominent among these features
are the conditional phrase-translation probabilities
in both directions, P (s|r) and P (r|s), which ensure
r is an appropriate response to s, and the language
model P (r), which ensures r is a well-formed re-
sponse. As in translation, the response models are
estimated from counts of phrase pairs observed in
the training bitext, and the language model is built
using n-gram statistics from a large set of observed
responses. To find the best response to a given input
status-post, we employ the Moses phrase-based de-
coder (Koehn et al, 2007), which conducts a beam
search for the best response given the input, accord-
ing to the log-linear model.
what . . .  
time . . .  
u  . . . .
get .  . . .
out . .  . .
? . . . . .
i ge
t
off at 5
Figure 1: Example from the data where word alignment
is easy. There is a clear correspondence between words
in the status and the response.
4.1 Challenge: Lexical Repetition
When applied directly to conversation data, off-the-
shelf MT systems simply learn to parrot back the
input, sometimes with slight modification. For ex-
ample, directly applying Moses with default settings
to the conversation data produces a system which
yields the following (typical) output on the above
example:
Stimulus: I?m slowly making this soup
...... and it smells gorgeous!
Response: i?m slowly making this soup
...... and you smell gorgeous!
This ?paraphrasing? phenomenon occurs because
identical word pairs are frequently observed together
in the training data. Because there is a wide range
of acceptable responses to any status, these identical
pairs have the strongest associations in the data, and
therefore dominate the phrase table. In order to dis-
courage lexically similar translations, we filter out
all phrase-pairs where one phrase is a substring of
the other, and introduce a novel feature to penalize
lexical similarity:
?lex(s, t) = J(s, t)
Where J(s, t) is the Jaccard similarity between the
set of words in s and t.
4.2 Challenge: Word Alignment
Alignment is more difficult in conversational data
than bilingual data (Brown et al, 1990), or textual
entailment data (Brockett, 2006; MacCartney et al,
2008). In conversational data, there are some cases
in which there is a decomposable alignment between
585
if . . . .
anyones . . . .
still . . . .
awake . . . .
lets . . . .
play . . . .
a . . . .
game. . . . .
name    .
3    .
kevin    .
costner    .
movies    .
that    .
dont    .
suck    .
. . . . 
eas
ier
qu
est
ion
ple
ase
.
Figure 2: Example from the data where word alignment
is difficult (requires alignment between large phrases in
the status and response).
words, as seen in figure 1, and some difficult cases
where alignment between large phrases is required,
for example figure 2. These difficult sentence pairs
confuse the IBM word alignment models which have
no way to distinguish between the easy and hard
cases.
We aligned words in our parallel data using the
widely used tool GIZA++ (Och and Ney, 2003);
however, the standard growing heuristic resulted in
very noisy alignments. Precision could be improved
considerably by using the intersection of GIZA++
trained in two directions (s? r, and r ? s), but the
alignment also became extremely sparse. The aver-
age number of alignments-per status/response pair
in our data was only 1.7, as compared to a dataset
of aligned French-English sentence pairs (the WMT
08 news commentary data) where the average num-
ber of intersection alignments is 14.
Direct Phrase Pair Extraction
Because word alignment in status/response pairs is
a difficult problem, instead of relying on local align-
ments for extracting phrase pairs, we exploit infor-
mation from all occurrences of the pair in determin-
C(s, t) C(s,?t) C(s)
C(?s, t) C(?s,?t) N ? C(s)
C(t) N ? C(t) N
Figure 3: Contingency table for phrase pair (s,t). Fisher?s
Exact Test estimates the probability of seeing this event,
or one more extreme assuming s and t are independent.
ing whether its phrases form a valid mapping.
We consider all possible phrase-pairs in the train-
ing data,1 then use Fisher?s Exact Test to filter out
pairs with low correlation (Johnson et al, 2007).
Given a source and target phrase s and t, we consider
the contingency table illustrated in figure 3, which
includes co-occurrence counts for s and t, the num-
ber of sentence-pairs containing s, but not t and vice
versa, in addition to the number of pairs containing
neither s nor t. Fisher?s Exact Test provides us with
an estimate of the probability of observing this table,
or one more extreme, assuming s and t are indepen-
dent; in other words it gives us a measure of how
strongly associated they are. In contrast to statistical
tests such as ?2, or the G2 Log Likelihood Ratio,
Fisher?s Exact Test produces accurate p-values even
when the expected counts are small (as is extremely
common in our case).
In Fisher?s Exact Test, the hypergeometric proba-
bility distribution is used to compute the exact prob-
ability of a particular joint frequency assuming a
model of independence:
C(s)!C(?s)!C(t)!C(?t)!
N !C(s, t)!C(?s, t)!C(s,?t)!C(?s,?t)!
The statistic is computed by summing the prob-
ability for the joint frequency in Table 3, and ev-
ery more extreme joint frequency consistent with the
marginal frequencies. Moore (2004) illustrates sev-
eral tricks which make this computation feasible in
practice.
We found that this approach generates phrase-
table entries which appear quite reasonable upon
manual inspection. The top 20 phrase-pairs (after fil-
tering out identical source/target phrases, substrings,
1We define a possible phrase-pair as any pair of phrases
found in a sentence-pair from our training corpus, where both
phrases consist of 4 tokens or fewer. The total number of phrase
pairs in a sentence pair (s, r) is O(|s| ? |r|).
586
Source Target
rt [retweet] thanks for the
potter harry
ice cream
how are you you ?
good morning
chuck norris
watching movie
i miss miss you too
are you i ?m
my birthday happy birthday
wish me luck good luck
how was it was
miss you i miss
swine flu
i love you love you too
how are are you ?
did you i did
jackson michael
how are you i ?m good
michael mj
Table 1: Top 20 Phrase Pairs ranked by the Fisher Exact
Test statistic. Slight variations (substrings or symmetric
pairs) were removed to show more variety. See the sup-
plementary materials for the top 10k (unfiltered) pairs.
and symmetric pairs) are listed in Table 1.2 Our ex-
periments in ?6 show that using the phrase table pro-
duced by Fisher?s Exact Test outperforms one gen-
erated based on the poor quality IBM word align-
ments.
4.3 System Details
For the phrase-table used in the experiments (?6) we
used the 5M phrases with highest association ac-
cording the Fisher Exact Test statistic.3 To build
the language model, we used all of the 1.3M re-
sponses from the training data, along with roughly
1M replies collected using Twitter?s streaming API.
2See the supplementary materials for the top 10k (unfiltered)
phrase pairs.
3Note that this includes an arbitrary subset of the (1,1,1)
pairs (phrase pairs where both phrases were only observed once
in the data). Excluding these (1,1,1) pairs yields a rather small
phrase table, 201K phrase-pairs after filtering, while including
all of them led to a table which was too large for the memory of
the machine used to conduct the experiments.
We do not use any form of SMT reordering
model, as the position of the phrase in the response
does not seem to be very correlated with the corre-
sponding position in the status. Instead we let the
language model drive reordering.
We used the default feature weights provided by
Moses.4 Because automatic evaluation of response
generation is an open problem, we avoided the use of
discriminative training algorithms such as Minimum
Error-Rate Training (Och, 2003).
5 Information Retrieval
One straightforward data-driven approach to re-
sponse generation is nearest neighbour, or informa-
tion retrieval. This general approach has been ap-
plied previously by several authors (Isbell et al,
2000; Swanson and Gordon, 2008; Jafarpour and
Burges, 2010), and is used as a point of compari-
son in our experiments. Given a novel status s and a
training corpus of status/response pairs, two retrieval
strategies can be used to return a best response r?:
IR-STATUS [rargmaxi sim(s,si)] Retrieve the re-sponse ri whose associated status message si
is most similar to the user?s input s.
IR-RESPONSE [rargmaxi sim(s,ri)] Retrieve the re-sponse ri which has highest similarity when di-
rectly compared to s.
At first glance, IR-STATUS may appear to be the
most promising option; intuitively, if an input status
is very similar to a training status, we might expect
the corresponding training response to pair well with
the input. However, as we describe in ?6, it turns
out that directly retrieving the most similar response
(IR-RESPONSE) tends to return acceptable replies
more reliably, as judged by human annotators. To
implement our two IR response generators, we rely
on the default similarity measure implemented in the
Lucene5 Information Retrieval Library, which is an
IDF-weighted Vector-Space similarity.
6 Experiments
In order to compare various approaches to auto-
mated response generation, we used human evalu-
4The language model weight was set to 0.5, the translation
model weights in both directions were both set to 0.2, the lexical
similarity weight was set to -0.2.
5http://lucene.apache.org/
587
ators from Amazon?s Mechanical Turk (Snow et al,
2008). Human evaluation also provides us with data
for a preliminary investigation into the feasibility
of automatic evaluation metrics. While automated
evaluation has been investigated in the area of spo-
ken dialogue systems (Jung et al, 2009), it is unclear
how well it will correlate with human judgment in
open-domain conversations where the range of pos-
sible responses is very large.
6.1 Experimental Conditions
We performed pairwise comparisons of several
response-generation systems. Similar work on eval-
uating MT output (Bloodgood and Callison-Burch,
2010) has asked Turkers to rank more than two
choices, but in order to keep our evaluation as
straightforward as possible, we limited our experi-
ments to pairwise comparisons.
For each experiment comparing 2 systems (a and
b), we built a test set by selecting a random sam-
ple of 200 tweets which had received responses,
and which had a length between 4 and 20 words.
These tweets were selected from conversations col-
lected from a later, non-overlapping time-period
from those used in training. Each experiment used
a different random sample of 200 tweets. For each
of the 200 statuses, we generated a response using
method a and b, then showed the status and both re-
sponses to the Turkers, asking them to choose the
best response. The order of the systems used to
generate a response was randomized, and each of
the 200 HITs was submitted to 3 different Turkers.
Turkers were paid 1? per judgment.
The Turkers were instructed that an appropriate
response should be on the same topic as the sta-
tus, and should also ?make sense? in response to it.
While this is an inherently subjective task, from in-
specting the results, we found Turkers to be quite
competent in judging between two responses.
The systems used in these pairwise comparisons
are summarized in table 2, and example output gen-
erated by each system is presented in Table 3.
6.2 Results
The results of the experiments are summarized in
Table 4. For each experiment we show the fraction
of HITs where the majority of annotators agreed sys-
tem a was better. We also show the p-value from an
System Name Description
RND-BASELINE Picks randomly from the set of
responses which are observed at
least twice in the training data.
The assumption is these are
likely very general responses
IR-STATUS rargmaxi sim(s,si) as describedin ?5
IR-RESPONSE rargmaxi sim(s,ri) as describedin ?5
MT-CHAT Phrase-based translation system
as described in ?4
MT-BASELINE Exactly the same as MT-CHAT,
except using a phrase table ex-
tracted based on word align-
ments from GIZA++
HUMAN Actual responses from the test
data.
Table 2: Summary of systems compared experimentally
exact Binomial significance test; note that all dif-
ferences are significant with above 95% confidence.
Table 4 also shows the S coefficient (Bennett et al,
1954) as a measure of agreement between annotators
for each experiment. S is equivalent the commonly
used Kappa, except it assumes the uniform distribu-
tion when computing expected or chance agreement
(Artstein and Poesio, 2008). The uniform distribu-
tion is appropriate in our setup, since annotators are
not told which system generated each output, and the
order of choices is randomized. If annotators were
indeed choosing randomly, they would be expected
to agree in 50% of cases. In most comparisons the
value of S is in the range of 0.2 to 0.4, which can
be considered ?Fair agreement? (Landis and Koch,
1977).
Note that agreement between annotators is lower
than typically reported in corpus annotation tasks.
When annotating which of two automatically gen-
erated outputs is better, there is not always a clear
answer; both responses might be good or bad. We
can expect strong agreement only in cases where
one response is clearly better. Strong agreement is
not required, however, as we are using many anno-
tations to compare each pair of systems, and the hu-
man judgments are not intended to be used as train-
ing data. Similar agreement was reported in an eval-
uation of automatically generated MT output as part
588
Status MT-CHAT MT-BASELINE IR-STATUS HUMAN RND-BASELINE IR-RESPONSE
riding to the
arc on my bike.
Hopefully I won?t
melt into the
pavement.
good luck on your
bike ride arc
i ca n?t wait for it
! i ?m arc is a year
.
grab some re-
freshing drinks
... and a towel ...
lol ... then eat ice
cream and suika
later ...
make sure to ride
in the middle of
the street
lmfaoooo ! u sure the cup wo
n?t melt ?
I really hate
watching movies
in the living
room.
me too , what
movies are you
doing in room ?
me too ! i love
those ? ) in my
mouth .
:( cheer up ! you
need to invest in
one of those turbo
fans
Why? no comfy
spots or just too
open to public
tas fazendo oq ? someone was
watching in the
living room kasi e
. :) i was too lazy
to go up to my
room . :) )
Hello Did I miss
David Getting
his teen choice
award? Or did
they not show it :(
hello miss ? *
he was getting his
will probably win
.
it ?s just like what
you just did the
exact date hello ?
it did n?t show yet
-ha- -lol-
nothing yet...he
presented though
he is so darn cute
you wish . are they gon na
show teen choice
awards again ? i
missed it
I can think the im-
possible is possi-
ble at any given
moment.
i think you can
do it , anything
is given impossi-
ble dreams really
specs
i ca n?t wait to
see if there is no
point in the end
but if you do a
few hours ...
i hope you use
those powers for
evil . i mean good
! good . that ?s
what i meant .
for sure Eric, and
that makes it not
?impossible? any
longer.
no -user- ? nobody with
whom i might
apply the blue
cobalt ? rabbit ?
at the moment .
given the econ-
omy , perhaps
-url- ( any of em )
I want to enter my
parents house into
a home makeover
contest. Any help
out there?
me too ! thanks
for the win , par-
ents are so smart
, just got home
makeover
oh man , i do
n?t want to come
back to my mom
is getting that
i put troy on the
potty everyday at
lunch and b4 u
know it he was
going on his own
... only took a
week .
check TLC I?m
pretty sure it was
them who were
recently posting
about looking
for houses to be
nominated!
good job you want to do
laundry with me
at my parents
house after i get
off ? maybe get a
free meal out of it
? !
Table 3: Example responses from each system. We tried to pick examples where most (or all) systems generate
reasonable responses for illustration purposes.
System A System B Fraction A p-value Agreement
(S)
System A
BLEU
System B
BLEU
MT-CHAT? IR-STATUS 0.645 5.0e-05 0.347 1.15 0.57
MT-CHAT? IR-RESPONSE 0.593 1.0e-02 0.333 0.84 1.53
IR-STATUS IR-RESPONSE? 0.422 3.3e-02 0.330 0.40 1.59
MT-CHAT? MT-BASELINE 0.577 3.8e-02 0.160 1.23 1.14
MT-CHAT HUMAN? 0.145 2.2e-16 0.433 N/A N/A
MT-CHAT? RND-BASELINE 0.880 2.2e-16 0.383 1.17 0.10
Table 4: Results of pairwise comparisons between various response-generation methods. Each row presents a com-
parison between systems a and b on 200 randomly selected tweets. The column Fraction A lists the fraction of HITs
where the majority of annotators agreed System A?s response was better. The winning system is indicated with an
asterisk?. All differences are significant.
589
of the WMT09 shared tasks (Callison-Burch et al,
2009).6
The results of the paired evaluations provide a
clear ordering on the automatic systems: IR-STATUS
is outperformed by IR-RESPONSE, which is in turn
outperformed by MT-CHAT. These results are
somewhat surprising. We had expected that match-
ing status to status would create a more natural and
effective IR system, but in practice, it appears that
the additional level of indirection employed by IR-
STATUS created only more opportunity for confu-
sion and error. Also, we did not necessarily expect
MT-CHAT?s output to be preferred by human anno-
tators: the SMT system is the only one that generates
a completely novel response, and is therefore the
system most likely to make fluency errors. We had
expected human annotators to pick up on these flu-
ency errors, giving the the advantage to the IR sys-
tems. However, it appears that MT-CHAT?s ability
to tailor its response to the status on a fine-grained
scale overcame the disadvantage of occasionally in-
troducing fluency errors.7
Given MT-CHAT?s success over the IR systems,
we conducted further experiments to validate its out-
put. In order to test how close MT-CHAT?s responses
come to human-level abilities, we compared its out-
put to actual human responses from our dataset. In
some cases the human responses change the topic of
conversation, and completely ignore the initial sta-
tus. For instance, one frequent type of response we
noticed in the data was a greeting: ?How have you
been? I haven?t talked to you in a while.? For the
purposes of this evaluation, we manually filtered out
cases where the human response was completely off-
topic from the status, selecting 200 pairs at random
that met our criteria and using the actual responses
as the HUMAN output.
When compared to the actual human-generated
response, MT-CHAT loses. However, its output is
preferred over the human responses 15% of the time,
a fact that is particularly surprising given the very
small ? by MT standards ? amount of data used to
train the model. A few examples where MT-CHAT?s
output were selected over the human response are
6See inter annotator agreement in table 4.
7Also, as one can see from the example exchanges in Ta-
ble 3, fluency errors are rampant across all systems, including
the gold-standard human responses.
listed in Table 5.
We also evaluated the effect of filtering all possi-
ble phrase pairs using Fisher?s Exact Test, which we
did instead of conducting phrase extraction accord-
ing to the very noisy word alignments. We altered
our MT-CHAT system to use the standard Moses
phrase-extraction pipeline, creating the system de-
noted as MT-BASELINE. We compared this to the
complete MT-CHAT system. Note that both systems
account for lexical repetition as described in ?4.1.
MT-CHAT?s output is preferred 58% of the time over
MT-BASELINE, indicating that direct phrase extrac-
tion is useful in this conversational setting.
Finally, as an additional baseline, we compared
MT-CHAT?s output to random responses selected
from those observed 2 or more times in the train-
ing data. One might argue that short, common re-
sponses are very general, and that a reply like ?lol?
could be considered a good response to almost any
status. However, the human evaluation shows a clear
preference for MT-CHAT?s output: raters favour re-
sponses that are tailored to the stimulus.
6.3 Automatic Evaluation
The field of SMT has benefited greatly from
the existence of an automatic evaluation metric,
BLEU (Papineni et al, 2002), which grades an out-
put candidate according to n-gram matches to one or
more reference outputs. To evaluate whether BLEU
is an appropriate automatic evaluation measure for
response generation, we attempted to measure its
agreement with the human judgments. We calcu-
late BLEU using a single reference derived from our
parallel corpus. We show the smoothed BLEU 1-4
scores for each system on each dataset evaluated in
Table 4. Although these scores are extremely low,
the overall BLEU scores agree with overall annota-
tor judgments in all cases except when comparing
MT-CHAT and IR-RESPONSE. It would seem that
BLEU has some agreement with human judgments
on this task, but perhaps not enough to be immedi-
ately useful.
7 Conclusions
We have presented several data-driven approaches,
based on information retrial and phrase-based SMT,
to the task of automatically responding to open-
590
Status MT-CHAT HUMAN
i want to go to a bonfire right about
now.
that does sound fun bonfire i wan na
go !
did my tweet bring that upon you ?
hehe
I?m the invisible man. man , you could never be invisible i can still see your face surrounded
in blue !
put ya number in my phone ..and
ima call you up !
call me when u do that in your phone
call :) for what ?s up ?
o well hw kum u dnt eva call or txt
me :) ? ! ? ! lolz jpn hw r ya tho :)
? ! ? !
Gray Maynard with hair? I did not
approve this.
maynard with grey hair ! i take it back, the hair isn?t working
for maynard.
Table 5: Examples where MT-CHAT output was preferred over HUMAN response by Turker annotators
domain linguistic stimuli.
Our experiments show that SMT techniques are
better-suited than IR approaches on the task of re-
sponse generation. Our system, MT-CHAT, pro-
duced responses which were preferred by human an-
notators over actual human responses 15% of the
time. Although this is still far from human-level
performance, we believe there is much room for
improvement: from designing appropriate word-
alignment and decoding algorithms that account for
the selective nature of response in dialogue, to sim-
ply adding more training data.
We described the many challenges posed by
adapting phrase-based SMT to dialogue, and pre-
sented initial solutions to several, including direct
phrasal alignment, and phrase-table scores discour-
aging responses that are lexically similar to the sta-
tus. Finally, we have provided results from an initial
experiment to evaluate the BLEU metric when ap-
plied to response generation, showing that though
the metric as is does not work well, there is suffi-
cient correlation to suggest that a similar, dialogue-
focused approach may be feasible.
By generating responses to Tweets out of context,
we have demonstrated that the models underlying
phrase-based SMT are capable of guiding the con-
struction of appropriate responses. In the future, we
are excited about the role these models could po-
tentially play in guiding response construction for
conversationally-aware chat input schemes, as well
as goal-directed dialogue systems.
Acknowledgments
We would like to thank Oren Etzioni, Michael
Gamon, Jerry Hobbs, Dirk Hovy, Yun-Cheng Ju,
Kristina Toutanova, Saif Mohammad, Patrick Pan-
tel, and Luke Zettlemoyer, in addition to the anony-
mous reviewers for helpful discussions and com-
ments on a previous draft. The first author is sup-
pored by a National Defense Science and Engineer-
ing Graduate (NDSEG) Fellowship 32 CFR 168a.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput. Lin-
guist., 34:555?596, December.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: an entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05.
E. M. Bennett, R. Alpert, and A. C. Goldstein. 1954.
Communications through limited-response question-
ing. Public Opinion Quarterly, 18(3):303?308.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using mechanical turk to build machine translation
evaluation sets. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, CSLDAMT
?10, pages 208?211, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Chris Brockett. 2006. Aligning the rte 2006 corpus. In
Microsoft Research Techincal report MSR-TR-2007-
77.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Comput.
Linguist., 16:79?85, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation, StatMT ?09.
591
Nathanael Chambers and James Allen. 2004. Stochas-
tic language generation in a dialogue system: Toward
a domain independent generator. In Michael Strube
and Candy Sidner, editors, Proceedings of the 5th SIG-
dial Workshop on Discourse and Dialogue, pages 9?
18, Cambridge, Massachusetts, USA, April 30 - May
1. Association for Computational Linguistics.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark my words! Linguistic
style accommodation in social media. In Proceedings
of WWW.
Hal Daume? III and Daniel Marcu. 2009. Induction of
word and phrase alignments for automatic document
summarization. CoRR, abs/0907.0804.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics, COLING ?04, Morristown, NJ,
USA. Association for Computational Linguistics.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics - Volume 1, ACL
?03, pages 16?23, Morristown, NJ, USA. Association
for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2008. You talking
to me? a corpus and algorithm for conversation disen-
tanglement. In Proceedings of ACL-08: HLT, June.
Michel Galley, Eric Fosler-Lussier, and Alexandros
Potamianos. 2001. Hybrid natural language gener-
ation for spoken dialogue systems. In Proceedings
of the 7th European Conference on Speech Commu-
nication and Technology (EUROSPEECH?01), pages
1735?1738, Aalborg, Denmark, September.
Jon Hasselgren, Erik Montnemery, Pierre Nugues, and
Markus Svensson. 2003. Hms: a predictive text entry
method using bigrams. In Proceedings of the 2003
EACL Workshop on Language Modeling for Text Entry
Methods, TextEntry ?03.
Jerry R. Hobbs. 1985. On the coherence and structure of
discourse.
Charles Lee Isbell, Jr., Michael J. Kearns, Dave Ko-
rmann, Satinder P. Singh, and Peter Stone. 2000.
Cobot in lambdamoo: A social statistics agent. In Pro-
ceedings of the Seventeenth National Conference on
Artificial Intelligence and Twelfth Conference on In-
novative Applications of Artificial Intelligence, pages
36?41. AAAI Press.
Sina Jafarpour and Christopher J. C. Burges. 2010. Fil-
ter, rank, and transfer the knowledge: Learning to chat.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Comput. Speech Lang.,
23:479?509, October.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings of
the 33rd annual meeting on Association for Computa-
tional Linguistics, ACL ?95.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL. The
Association for Computer Linguistics.
J R Landis and G G Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics.
Brian Langner, Stephan Vogel, and Alan W. Black. 2010.
Evaluating a dialog language generation system: com-
paring the mountain system to other nlg approaches.
In INTERSPEECH.
Anton Leuski and David R. Traum. 2010. Practical
language processing for virtual humans. In Twenty-
Second Annual Conference on Innovative Applications
of Artificial Intelligence (IAAI-10).
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 802?811, Morristown,
NJ, USA. Association for Computational Linguistics.
Robert C. Moore. 2004. On log-likelihood-ratios and the
significance of rare events. In EMNLP.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL, pages 311?318.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 142?149.
592
Owen Rambow, Srinivas Bangalore, and Marilyn Walker.
2001. Natural language generation in dialog systems.
In Proceedings of the first international conference on
Human language technology research, HLT ?01, pages
1?4, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference.
Sujith Ravi, Andrei Broder, Evgeniy Gabrilovich, Vanja
Josifovski, Sandeep Pandey, and Bo Pang. 2010. Au-
tomatic generation of bid phrases for online advertis-
ing. In Proceedings of the third ACM international
conference on Web search and data mining, WSDM
?10.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
464?471, Prague, Czech Republic, June. Association
for Computational Linguistics.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 172?180,
Morristown, NJ, USA. Association for Computational
Linguistics.
Samira Shaikh, Tomek Strzalkowski, Sarah Taylor, and
Nick Webb. 2010. Vca: an experiment with a mul-
tiparty virtual chat agent. In Proceedings of the 2010
Workshop on Companionable Dialogue Systems.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, COLING-ACL ?06.
Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk.
2010. Learning phrase-based spelling error models
from clickthrough data. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 266?274, Morristown,
NJ, USA. Association for Computational Linguistics.
Reid Swanson and Andrew S. Gordon. 2008. Say any-
thing: A massively collaborative open domain story
writing companion. In Proceedings of the 1st Joint
International Conference on Interactive Digital Story-
telling: Interactive Storytelling, ICIDS ?08, pages 32?
40, Berlin, Heidelberg. Springer-Verlag.
Lidan Wang and Douglas W. Oard. 2009. Context-based
message expansion for disentanglement of interleaved
text conversations. In HLT-NAACL.
Joseph Weizenbaum. 1966. Eliza: a computer program
for the study of natural language communication be-
tween man and machine. Commun. ACM, 9:36?45,
January.
Yorick Wilks. 2006. Artificial companions as a new kind
of interface to the future internet. In OII Research Re-
port No. 13.
Yuk Wah Wong and Raymond Mooney. 2006. Learning
for semantic parsing with statistical machine transla-
tion. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Main Conference.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
593
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1524?1534,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Named Entity Recognition in Tweets:
An Experimental Study
Alan Ritter, Sam Clark, Mausam and Oren Etzioni
Computer Science and Engineering
University of Washington
Seattle, WA 98125, USA
{aritter,ssclark,mausam,etzioni}@cs.washington.edu
Abstract
People tweet more than 100 Million times
daily, yielding a noisy, informal, but some-
times informative corpus of 140-character
messages that mirrors the zeitgeist in an un-
precedented manner. The performance of
standard NLP tools is severely degraded on
tweets. This paper addresses this issue by
re-building the NLP pipeline beginning with
part-of-speech tagging, through chunking, to
named-entity recognition. Our novel T-NER
system doubles F1 score compared with the
Stanford NER system. T-NER leverages the
redundancy inherent in tweets to achieve this
performance, using LabeledLDA to exploit
Freebase dictionaries as a source of distant
supervision. LabeledLDA outperforms co-
training, increasing F1 by 25% over ten com-
mon entity types.
Our NLP tools are available at: http://
github.com/aritter/twitter_nlp
1 Introduction
Status Messages posted on Social Media websites
such as Facebook and Twitter present a new and
challenging style of text for language technology
due to their noisy and informal nature. Like SMS
(Kobus et al, 2008), tweets are particularly terse
and difficult (See Table 1). Yet tweets provide a
unique compilation of information that is more up-
to-date and inclusive than news articles, due to the
low-barrier to tweeting, and the proliferation of mo-
bile devices.1 The corpus of tweets already exceeds
1See the ?trending topics? displayed on twitter.com
the size of the Library of Congress (Hachman, 2011)
and is growing far more rapidly. Due to the vol-
ume of tweets, it is natural to consider named-entity
recognition, information extraction, and text mining
over tweets. Not surprisingly, the performance of
?off the shelf? NLP tools, which were trained on
news corpora, is weak on tweet corpora.
In response, we report on a re-trained ?NLP
pipeline? that leverages previously-tagged out-of-
domain text, 2 tagged tweets, and unlabeled tweets
to achieve more effective part-of-speech tagging,
chunking, and named-entity recognition.
1 The Hobbit has FINALLY started filming! I
cannot wait!
2 Yess! Yess! Its official Nintendo announced
today that they Will release the Nintendo 3DS
in north America march 27 for $250
3 Government confirms blast n nuclear plants n
japan...don?t knw wht s gona happen nw...
Table 1: Examples of noisy text in tweets.
We find that classifying named entities in tweets is
a difficult task for two reasons. First, tweets contain
a plethora of distinctive named entity types (Compa-
nies, Products, Bands, Movies, and more). Almost
all these types (except for People and Locations) are
relatively infrequent, so even a large sample of man-
ually annotated tweets will contain few training ex-
amples. Secondly, due to Twitter?s 140 character
limit, tweets often lack sufficient context to deter-
mine an entity?s type without the aid of background
2Although tweets can be written on any subject, following
convention we use the term ?domain? to include text styles or
genres such as Twitter, News or IRC Chat.
1524
knowledge.
To address these issues we propose a distantly su-
pervised approach which applies LabeledLDA (Ra-
mage et al, 2009) to leverage large amounts of unla-
beled data in addition to large dictionaries of entities
gathered from Freebase, and combines information
about an entity?s context across its mentions.
We make the following contributions:
1. We experimentally evaluate the performance of
off-the-shelf news trained NLP tools when ap-
plied to Twitter. For example POS tagging
accuracy drops from about 0.97 on news to
0.80 on tweets. By utilizing in-domain, out-
of-domain, and unlabeled data we are able to
substantially boost performance, for example
obtaining a 52% increase in F1 score on seg-
menting named entities.
2. We introduce a novel approach to distant super-
vision (Mintz et al, 2009) using Topic Models.
LabeledLDA is applied, utilizing constraints
based on an open-domain database (Freebase)
as a source of supervision. This approach in-
creases F1 score by 25% relative to co-training
(Blum and Mitchell, 1998; Yarowsky, 1995) on
the task of classifying named entities in Tweets.
The rest of the paper is organized as follows.
We successively build the NLP pipeline for Twitter
feeds in Sections 2 and 3. We first present our ap-
proaches to shallow syntax ? part of speech tagging
(?2.1), and shallow parsing (?2.2). ?2.3 describes a
novel classifier that predicts the informativeness of
capitalization in a tweet. All tools in ?2 are used
as features for named entity segmentation in ?3.1.
Next, we present our algorithms and evaluation for
entity classification (?3.2). We describe related work
in ?4 and conclude in ?5.
2 Shallow Syntax in Tweets
We first study two fundamental NLP tasks ? POS
tagging and noun-phrase chunking. We also discuss
a novel capitalization classifier in ?2.3. The outputs
of all these classifiers are used in feature generation
for named entity recognition in the next section.
For all experiments in this section we use a dataset
of 800 randomly sampled tweets. All results (Tables
Accuracy Error
Reduction
Majority Baseline (NN) 0.189 -
Word?s Most Frequent Tag 0.760 -
Stanford POS Tagger 0.801 -
T-POS(PTB) 0.813 6%
T-POS(Twitter) 0.853 26%
T-POS(IRC + PTB) 0.869 34%
T-POS(IRC + Twitter) 0.870 35%
T-POS(PTB + Twitter) 0.873 36%
T-POS(PTB + IRC + Twitter) 0.883 41%
Table 2: POS tagging performance on tweets. By training
on in-domain labeled data, in addition to annotated IRC
chat data, we obtain a 41% reduction in error over the
Stanford POS tagger.
2, 4 and 5) represent 4-fold cross-validation experi-
ments on the respective tasks.3
2.1 Part of Speech Tagging
Part of speech tagging is applicable to a wide range
of NLP tasks including named entity segmentation
and information extraction.
Prior experiments have suggested that POS tag-
ging has a very strong baseline: assign each word
to its most frequent tag and assign each Out of Vo-
cabulary (OOV) word the most common POS tag.
This baseline obtained a 0.9 accuracy on the Brown
corpus (Charniak et al, 1993). However, the appli-
cation of a similar baseline on tweets (see Table 2)
obtains a much weaker 0.76, exposing the challeng-
ing nature of Twitter data.
A key reason for this drop in accuracy is that Twit-
ter contains far more OOV words than grammatical
text. Many of these OOV words come from spelling
variation, e.g., the use of the word ?n? for ?in? in Ta-
ble 1 example 3. Although NNP is the most frequent
tag for OOV words, only about 1/3 are NNPs.
The performance of off-the-shelf news-trained
POS taggers also suffers on Twitter data. The state-
of-the-art Stanford POS tagger (Toutanova et al,
2003) improves on the baseline, obtaining an accu-
racy of 0.8. This performance is impressive given
that its training data, the Penn Treebank WSJ (PTB),
is so different in style from Twitter, however it is a
huge drop from the 97% accuracy reported on the
3We used Brendan O?Connor?s Twitter tokenizer
1525
Gold Predicted Stanford
Error
T-POS Error Error
Reduction
NN NNP 0.102 0.072 29%
UH NN 0.387 0.047 88%
VB NN 0.071 0.032 55%
NNP NN 0.130 0.125 4%
UH NNP 0.200 0.036 82%
Table 3: Most common errors made by the Stanford POS
Tagger on tweets. For each case we list the fraction of
times the gold tag is misclassified as the predicted for
both our system and the Stanford POS tagger. All verbs
are collapsed into VB for compactness.
PTB. There are several reasons for this drop in per-
formance. Table 3 lists common errors made by
the Stanford tagger. First, due to unreliable capi-
talization, common nouns are often misclassified as
proper nouns, and vice versa. Also, interjections
and verbs are frequently misclassified as nouns. In
addition to differences in vocabulary, the grammar
of tweets is quite different from edited news text.
For instance, tweets often start with a verb (where
the subject ?I? is implied), as in: ?watchng american
dad.?
To overcome these differences in style and vocab-
ulary, we manually annotated a set of 800 tweets
(16K tokens) with tags from the Penn TreeBank tag
set for use as in-domain training data for our POS
tagging system, T-POS.4 We add new tags for the
Twitter specific phenomena: retweets, @usernames,
#hashtags, and urls. Note that words in these cate-
gories can be tagged with 100% accuracy using sim-
ple regular expressions. To ensure fair comparison
in Table 2, we include a postprocessing step which
tags these words appropriately for all systems.
To help address the issue of OOV words and
lexical variations, we perform clustering to group
together words which are distributionally similar
(Brown et al, 1992; Turian et al, 2010). In particu-
lar, we perform hierarchical clustering using Jcluster
(Goodman, 2001) on 52 million tweets; each word
is uniquely represented by a bit string based on the
path from the root of the resulting hierarchy to the
word?s leaf. We use the Brown clusters resulting
from prefixes of 4, 8, and 12 bits. These clusters are
often effective in capturing lexical variations, for ex-
4Using MMAX2 (Mu?ller and Strube, 2006) for annotation.
ample, following are lexical variations on the word
?tomorrow? from one cluster after filtering out other
words (most of which refer to days):
?2m?, ?2ma?, ?2mar?, ?2mara?, ?2maro?,
?2marrow?, ?2mor?, ?2mora?, ?2moro?, ?2mo-
row?, ?2morr?, ?2morro?, ?2morrow?, ?2moz?,
?2mr?, ?2mro?, ?2mrrw?, ?2mrw?, ?2mw?,
?tmmrw?, ?tmo?, ?tmoro?, ?tmorrow?, ?tmoz?,
?tmr?, ?tmro?, ?tmrow?, ?tmrrow?, ?tm-
rrw?, ?tmrw?, ?tmrww?, ?tmw?, ?tomaro?,
?tomarow?, ?tomarro?, ?tomarrow?, ?tomm?,
?tommarow?, ?tommarrow?, ?tommoro?, ?tom-
morow?, ?tommorrow?, ?tommorw?, ?tomm-
row?, ?tomo?, ?tomolo?, ?tomoro?, ?tomorow?,
?tomorro?, ?tomorrw?, ?tomoz?, ?tomrw?,
?tomz?
T-POS uses Conditional Random Fields5 (Laf-
ferty et al, 2001), both because of their ability to
model strong dependencies between adjacent POS
tags, and also to make use of highly correlated fea-
tures (for example a word?s identity in addition to
prefixes and suffixes). Besides employing the Brown
clusters computed above, we use a fairly standard set
of features that include POS dictionaries, spelling
and contextual features.
On a 4-fold cross validation over 800 tweets,
T-POS outperforms the Stanford tagger, obtaining a
26% reduction in error. In addition we include 40K
tokens of annotated IRC chat data (Forsythand and
Martell, 2007), which is similar in style. Like Twit-
ter, IRC data contains many misspelled/abbreviated
words, and also more pronouns, and interjections,
but fewer determiners than news. Finally, we also
leverage 50K POS-labeled tokens from the Penn
Treebank (Marcus et al, 1994).
Overall T-POS trained on 102K tokens (12K from
Twitter, 40K from IRC and 50K from PTB) results
in a 41% error reduction over the Stanford tagger,
obtaining an accuracy of 0.883. Table 3 lists gains
on some of the most common error types, for ex-
ample, T-POS dramatically reduces error on inter-
jections and verbs that are incorrectly classified as
nouns by the Stanford tagger.
2.2 Shallow Parsing
Shallow parsing, or chunking is the task of identi-
fying non-recursive phrases, such as noun phrases,
5We use MALLET (McCallum, 2002).
1526
Accuracy Error
Reduction
Majority Baseline (B-NP) 0.266 -
OpenNLP 0.839 -
T-CHUNK(CoNLL) 0.854 9%
T-CHUNK(Twitter) 0.867 17%
T-CHUNK(CoNLL + Twitter) 0.875 22%
Table 4: Token-Level accuracy at shallow parsing tweets.
We compare against the OpenNLP chunker as a baseline.
verb phrases, and prepositional phrases in text. Ac-
curate shallow parsing of tweets could benefit sev-
eral applications such as Information Extraction and
Named Entity Recognition.
Off the shelf shallow parsers perform noticeably
worse on tweets, motivating us again to annotate in-
domain training data. We annotate the same set of
800 tweets mentioned previously with tags from the
CoNLL shared task (Tjong Kim Sang and Buchholz,
2000). We use the set of shallow parsing features de-
scribed by Sha and Pereira (2003), in addition to the
Brown clusters mentioned above. Part-of-speech tag
features are extracted based on cross-validation out-
put predicted by T-POS. For inference and learning,
again we use Conditional Random Fields. We utilize
16K tokens of in-domain training data (using cross
validation), in addition to 210K tokens of newswire
text from the CoNLL dataset.
Table 4 reports T-CHUNK?s performance at shal-
low parsing of tweets. We compare against the off-
the shelf OpenNLP chunker6, obtaining a 22% re-
duction in error.
2.3 Capitalization
A key orthographic feature for recognizing named
entities is capitalization (Florian, 2002; Downey et
al., 2007). Unfortunately in tweets, capitalization
is much less reliable than in edited texts. In addi-
tion, there is a wide variety in the styles of capital-
ization. In some tweets capitalization is informative,
whereas in other cases, non-entity words are capital-
ized simply for emphasis. Some tweets contain all
lowercase words (8%), whereas others are in ALL
CAPS (0.6%).
To address this issue, it is helpful to incorporate
information based on the entire content of the mes-
6http://incubator.apache.org/opennlp/
P R F1
Majority Baseline 0.70 1.00 0.82
T-CAP 0.77 0.98 0.86
Table 5: Performance at predicting reliable capitalization.
sage to determine whether or not its capitalization
is informative. To this end, we build a capitaliza-
tion classifier, T-CAP, which predicts whether or not
a tweet is informatively capitalized. Its output is
used as a feature for Named Entity Recognition. We
manually labeled our 800 tweet corpus as having
either ?informative? or ?uninformative? capitaliza-
tion. The criteria we use for labeling is as follows:
if a tweet contains any non-entity words which are
capitalized, but do not begin a sentence, or it con-
tains any entities which are not capitalized, then its
capitalization is ?uninformative?, otherwise it is ?in-
formative?.
For learning , we use Support Vector Ma-
chines.7 The features used include: the frac-
tion of words in the tweet which are capitalized,
the fraction which appear in a dictionary of fre-
quently lowercase/capitalized words but are not low-
ercase/capitalized in the tweet, the number of times
the word ?I? appears lowercase and whether or not
the first word in the tweet is capitalized. Results
comparing against the majority baseline, which pre-
dicts capitalization is always informative, are shown
in Table 5. Additionally, in ?3 we show that fea-
tures based on our capitalization classifier improve
performance at named entity segmentation.
3 Named Entity Recognition
We now discuss our approach to named entity recog-
nition on Twitter data. As with POS tagging and
shallow parsing, off the shelf named-entity recog-
nizers perform poorly on tweets. For example, ap-
plying the Stanford Named Entity Recognizer to one
of the examples from Table 1 results in the following
output:
[Yess]ORG! [Yess]ORG! Its official
[Nintendo]LOC announced today that they
Will release the [Nintendo]ORG 3DS in north
[America]LOC march 27 for $250
7http://www.chasen.org/?taku/software/
TinySVM/
1527
The OOV word ?Yess? is mistaken as a named en-
tity. In addition, although the first occurrence of
?Nintendo? is correctly segmented, it is misclassi-
fied, whereas the second occurrence is improperly
segmented ? it should be the product ?Nintendo
3DS?. Finally ?north America? should be segmented
as a LOCATION, rather than just ?America?. In gen-
eral, news-trained Named Entity Recognizers seem
to rely heavily on capitalization, which we know to
be unreliable in tweets.
Following Collins and Singer (1999), Downey et
al. (2007) and Elsner et al (2009), we treat classi-
fication and segmentation of named entities as sepa-
rate tasks. This allows us to more easily apply tech-
niques better suited towards each task. For exam-
ple, we are able to use discriminative methods for
named entity segmentation and distantly supervised
approaches for classification. While it might be ben-
eficial to jointly model segmentation and (distantly
supervised) classification using a joint sequence la-
beling and topic model similar to that proposed by
Sauper et al (2010), we leave this for potential fu-
ture work.
Because most words found in tweets are not part
of an entity, we need a larger annotated dataset to ef-
fectively learn a model of named entities. We there-
fore use a randomly sampled set of 2,400 tweets for
NER. All experiments (Tables 6, 8-10) report results
using 4-fold cross validation.
3.1 Segmenting Named Entities
Because capitalization in Twitter is less informative
than news, in-domain data is needed to train models
which rely less heavily on capitalization, and also
are able to utilize features provided by T-CAP.
We exhaustively annotated our set of 2,400 tweets
(34K tokens) with named entities.8 A convention on
Twitter is to refer to other users using the @ sym-
bol followed by their unique username. We deliber-
ately choose not to annotate @usernames as entities
in our data set because they are both unambiguous,
and trivial to identify with 100% accuracy using a
simple regular expression, and would only serve to
inflate our performance statistics. While there is am-
biguity as to the type of @usernames (for example,
8We found that including out-of-domain training data from
the MUC competitions lowered performance at this task.
P R F1 F1 inc.
Stanford NER 0.62 0.35 0.44 -
T-SEG(None) 0.71 0.57 0.63 43%
T-SEG(T-POS) 0.70 0.60 0.65 48%
T-SEG(T-POS, T-CHUNK) 0.71 0.61 0.66 50%
T-SEG(All Features) 0.73 0.61 0.67 52%
Table 6: Performance at segmenting entities varying the
features used. ?None? removes POS, Chunk, and capital-
ization features. Overall we obtain a 52% improvement
in F1 score over the Stanford Named Entity Recognizer.
they can refer to people or companies), we believe
they could be more easily classified using features
of their associated user?s profile than contextual fea-
tures of the text.
T-SEG models Named Entity Segmentation as a
sequence-labeling task using IOB encoding for rep-
resenting segmentations (each word either begins, is
inside, or is outside of a named entity), and uses
Conditional Random Fields for learning and infer-
ence. Again we include orthographic, contextual
and dictionary features; our dictionaries included a
set of type lists gathered from Freebase. In addition,
we use the Brown clusters and outputs of T-POS,
T-CHUNK and T-CAP in generating features.
We report results at segmenting named entities in
Table 6. Compared with the state-of-the-art news-
trained Stanford Named Entity Recognizer (Finkel
et al, 2005), T-SEG obtains a 52% increase in F1
score.
3.2 Classifying Named Entities
Because Twitter contains many distinctive, and in-
frequent entity types, gathering sufficient training
data for named entity classification is a difficult task.
In any random sample of tweets, many types will
only occur a few times. Moreover, due to their
terse nature, individual tweets often do not contain
enough context to determine the type of the enti-
ties they contain. For example, consider following
tweet:
KKTNY in 45min..........
without any prior knowledge, there is not enough
context to determine what type of entity ?KKTNY?
refers to, however by exploiting redundancy in the
data (Downey et al, 2010), we can determine it is
likely a reference to a television show since it of-
1528
ten co-occurs with words such as watching and pre-
mieres in other contexts.9
In order to handle the problem of many infre-
quent types, we leverage large lists of entities and
their types gathered from an open-domain ontology
(Freebase) as a source of distant supervision, allow-
ing use of large amounts of unlabeled data in learn-
ing.
Freebase Baseline: Although Freebase has very
broad coverage, simply looking up entities and their
types is inadequate for classifying named entities in
context (0.38 F-score, ?3.2.1). For example, accord-
ing to Freebase, the mention ?China? could refer to
a country, a band, a person, or a film. This prob-
lem is very common: 35% of the entities in our data
appear in more than one of our (mutually exclusive)
Freebase dictionaries. Additionally, 30% of entities
mentioned on Twitter do not appear in any Freebase
dictionary, as they are either too new (for example a
newly released videogame), or are misspelled or ab-
breviated (for example ?mbp? is often used to refer
to the ?mac book pro?).
Distant Supervision with Topic Models: To
model unlabeled entities and their possible types, we
apply LabeledLDA (Ramage et al, 2009), constrain-
ing each entity?s distribution over topics based on
its set of possible types according to Freebase. In
contrast to previous weakly supervised approaches
to Named Entity Classification, for example the Co-
Training and Na??ve Bayes (EM) models of Collins
and Singer (1999), LabeledLDA models each entity
string as a mixture of types rather than using a single
hidden variable to represent the type of each men-
tion. This allows information about an entity?s dis-
tribution over types to be shared across mentions,
naturally handling ambiguous entity strings whose
mentions could refer to different types.
Each entity string in our data is associated with a
bag of words found within a context window around
all of its mentions, and also within the entity itself.
As in standard LDA (Blei et al, 2003), each bag of
words is associated with a distribution over topics,
Multinomial(?e), and each topic is associated with a
distribution over words, Multinomial(?t). In addi-
tion, there is a one-to-one mapping between topics
and Freebase type dictionaries. These dictionaries
9Kourtney & Kim Take New York.
constrain ?e, the distribution over topics for each en-
tity string, based on its set of possible types, FB[e].
For example, ?Amazon could correspond to a distribu-
tion over two types: COMPANY, and LOCATION,
whereas ?Apple might represent a distribution over
COMPANY, and FOOD. For entities which aren?t
found in any of the Freebase dictionaries, we leave
their topic distributions ?e unconstrained. Note that
in absence of any constraints LabeledLDA reduces
to standard LDA, and a fully unsupervised setting
similar to that presented by Elsner et. al. (2009).
In detail, the generative process that models our
data for Named Entity Classification is as follows:
for each type: t = 1 . . . T do
Generate ?t according to symmetric Dirichlet
distribution Dir(?).
end for
for each entity string e = 1 . . . |E| do
Generate ?e over FB[e] according to Dirichlet
distribution Dir(?FB[e]).
for each word position i = 1 . . . Ne do
Generate ze,i from Mult(?e).
Generate the word we,i from Mult(?ze,i).
end for
end for
To infer values for the hidden variables, we apply
Collapsed Gibbs sampling (Griffiths and Steyvers,
2004), where parameters are integrated out, and the
ze,is are sampled directly.
In making predictions, we found it beneficial to
consider ?traine as a prior distribution over types forentities which were encountered during training. In
practice this sharing of information across contexts
is very beneficial as there is often insufficient evi-
dence in an isolated tweet to determine an entity?s
type. For entities which weren?t encountered dur-
ing training, we instead use a prior based on the dis-
tribution of types across all entities. One approach
to classifying entities in context is to assume that
?traine is fixed, and that all of the words inside theentity mention and context, w, are drawn based on
a single topic, z, that is they are all drawn from
Multinomial(?z). We can then compute the poste-
rior distribution over types in closed form with a
simple application of Bayes rule:
P (z|w) ?
?
w?w
P (w|z : ?)P (z : ?traine )
During development, however, we found that rather
than making these assumptions, using Gibbs Sam-
1529
Type Top 20 Entities not found in Freebase dictionaries
PRODUCT nintendo ds lite, apple ipod, generation black, ipod nano, apple iphone, gb black, xperia, ipods, verizon
media, mac app store, kde, hd video, nokia n8, ipads, iphone/ipod, galaxy tab, samsung galaxy, playstation
portable, nintendo ds, vpn
TV-SHOW pretty little, american skins, nof, order svu, greys, kktny, rhobh, parks & recreation, parks & rec, dawson
?s creek, big fat gypsy weddings, big fat gypsy wedding, winter wipeout, jersey shores, idiot abroad, royle,
jerseyshore, mr . sunshine, hawaii five-0, new jersey shore
FACILITY voodoo lounge, grand ballroom, crash mansion, sullivan hall, memorial union, rogers arena, rockwood
music hall, amway center, el mocambo, madison square, bridgestone arena, cat club, le poisson rouge,
bryant park, mandalay bay, broadway bar, ritz carlton, mgm grand, olympia theatre, consol energy center
Table 7: Example type lists produced by LabeledLDA. No entities which are shown were found in Freebase; these are
typically either too new to have been added, or are misspelled/abbreviated (for example rhobh=?Real Housewives of
Beverly Hills?). In a few cases there are segmentation errors.
pling to estimate the posterior distribution over types
performs slightly better. In order to make predic-
tions, for each entity we use an informative Dirich-
let prior based on ?traine and perform 100 iterations of
Gibbs Sampling holding the hidden topic variables
in the training data fixed (Yao et al, 2009). Fewer
iterations are needed than in training since the type-
word distributions, ? have already been inferred.
3.2.1 Classification Experiments
To evaluate T-CLASS?s ability to classify entity
mentions in context, we annotated the 2,400 tweets
with 10 types which are both popular on Twitter,
and have good coverage in Freebase: PERSON,
GEO-LOCATION, COMPANY, PRODUCT, FACIL-
ITY, TV-SHOW, MOVIE, SPORTSTEAM, BAND,
and OTHER. Note that these type annotations are
only used for evaluation purposes, and not used dur-
ing training T-CLASS, which relies only on distant
supervision. In some cases, we combine multi-
ple Freebase types to create a dictionary of entities
representing a single type (for example the COM-
PANY dictionary contains Freebase types /busi-
ness/consumer company and /business/brand). Be-
cause our approach does not rely on any manually
labeled examples, it is straightforward to extend it
for a different sets of types based on the needs of
downstream applications.
Training: To gather unlabeled data for inference,
we run T-SEG, our entity segmenter (from ?3.1), on
60M tweets, and keep the entities which appear 100
or more times. This results in a set of 23,651 dis-
tinct entity strings. For each entity string, we col-
lect words occurring in a context window of 3 words
from all mentions in our data, and use a vocabulary
of the 100K most frequent words. We run Gibbs
sampling for 1,000 iterations, using the last sample
to estimate entity-type distributions ?e, in addition
to type-word distributions ?t. Table 7 displays the
20 entities (not found in Freebase) whose posterior
distribution ?e assigns highest probability to selected
types.
Results: Table 8 presents the classification re-
sults of T-CLASS compared against a majority base-
line which simply picks the most frequent class
(PERSON), in addition to the Freebase baseline,
which only makes predictions if an entity appears
in exactly one dictionary (i.e., appears unambigu-
ous). T-CLASS also outperforms a simple super-
vised baseline which applies a MaxEnt classifier us-
ing 4-fold cross validation over the 1,450 entities
which were annotated for testing. Additionally we
compare against the co-training algorithm of Collins
and Singer (1999) which also leverages unlabeled
data and uses our Freebase type lists; for seed rules
we use the ?unambiguous? Freebase entities. Our
results demonstrate that T-CLASS outperforms the
baselines and achieves a 25% increase in F1 score
over co-training.
Tables 9 and 10 present a breakdown of F1 scores
by type, both collapsing types into the standard
classes used in the MUC competitions (PERSON,
LOCATION, ORGANIZATION), and using the 10
popular Twitter types described earlier.
Entity Strings vs. Entity Mentions: DL-Cotrain
and LabeledLDA use two different representations
for the unlabeled data during learning. LabeledLDA
groups together words across all mentions of an en-
1530
System P R F1
Majority Baseline 0.30 0.30 0.30
Freebase Baseline 0.85 0.24 0.38
Supervised Baseline 0.45 0.44 0.45
DL-Cotrain 0.54 0.51 0.53
LabeledLDA 0.72 0.60 0.66
Table 8: Named Entity Classification performance on the
10 types. Assumes segmentation is given as in (Collins
and Singer, 1999), and (Elsner et al, 2009).
Type LL FB CT SP N
PERSON 0.82 0.48 0.65 0.83 436
LOCATION 0.74 0.21 0.55 0.67 372
ORGANIZATION 0.66 0.52 0.55 0.31 319
overall 0.75 0.39 0.59 0.49 1127
Table 9: F1 classification scores for the 3 MUC types
PERSON, LOCATION, ORGANIZATION. Results are
shown using LabeledLDA (LL), Freebase Baseline (FB),
DL-Cotrain (CT) and Supervised Baseline (SP). N is the
number of entities in the test set.
Type LL FB CT SP N
PERSON 0.82 0.48 0.65 0.86 436
GEO-LOC 0.77 0.23 0.60 0.51 269
COMPANY 0.71 0.66 0.50 0.29 162
FACILITY 0.37 0.07 0.14 0.34 103
PRODUCT 0.53 0.34 0.40 0.07 91
BAND 0.44 0.40 0.42 0.01 54
SPORTSTEAM 0.53 0.11 0.27 0.06 51
MOVIE 0.54 0.65 0.54 0.05 34
TV-SHOW 0.59 0.31 0.43 0.01 31
OTHER 0.52 0.14 0.40 0.23 219
overall 0.66 0.38 0.53 0.45 1450
Table 10: F1 scores for classification broken down by
type for LabeledLDA (LL), Freebase Baseline (FB), DL-
Cotrain (CT) and Supervised Baseline (SP). N is the num-
ber of entities in the test set.
P R F1
DL-Cotrain-entity 0.47 0.45 0.46
DL-Cotrain-mention 0.54 0.51 0.53
LabeledLDA-entity 0.73 0.60 0.66
LabeledLDA-mention 0.57 0.52 0.54
Table 11: Comparing LabeledLDA and DL-Cotrain
grouping unlabeled data by entities vs. mentions.
System P R F1
COTRAIN-NER (10 types) 0.55 0.33 0.41
T-NER(10 types) 0.65 0.42 0.51
COTRAIN-NER (PLO) 0.57 0.42 0.49
T-NER(PLO) 0.73 0.49 0.59
Stanford NER (PLO) 0.30 0.27 0.29
Table 12: Performance at predicting both segmentation
and classification. Systems labeled with PLO are evalu-
ated on the 3 MUC types PERSON, LOCATION, ORGA-
NIZATION.
tity string, and infers a distribution over its possi-
ble types, whereas DL-Cotrain considers the entity
mentions separately as unlabeled examples and pre-
dicts a type independently for each. In order to
ensure that the difference in performance between
LabeledLDA and DL-Cotrain is not simply due to
this difference in representation, we compare both
DL-Cotrain and LabeledLDA using both unlabeled
datasets (grouping words by all mentions vs. keep-
ing mentions separate) in Table 11. As expected,
DL-Cotrain performs poorly when the unlabeled ex-
amples group mentions; this makes sense, since Co-
Training uses a discriminative learning algorithm,
so when trained on entities and tested on individual
mentions, the performance decreases. Additionally,
LabeledLDA?s performance is poorer when consid-
ering mentions as ?documents?. This is likely due
to the fact that there isn?t enough context to effec-
tively learn topics when the ?documents? are very
short (typically fewer than 10 words).
End to End System: Finally we present the end
to end performance on segmentation and classifica-
tion (T-NER) in Table 12. We observe that T-NER
again outperforms co-training. Moreover, compar-
ing against the Stanford Named Entity Recognizer
on the 3 MUC types, T-NER doubles F1 score.
4 Related Work
There has been relatively little previous work on
building NLP tools for Twitter or similar text styles.
Locke and Martin (2009) train a classifier to recog-
nize named entities based on annotated Twitter data,
handling the types PERSON, LOCATION, and OR-
GANIZATION. Developed in parallel to our work,
Liu et al (2011) investigate NER on the same 3
types, in addition to PRODUCTs and present a semi-
1531
supervised approach using k-nearest neighbor. Also
developed in parallel, Gimpell et al (2011) build a
POS tagger for tweets using 20 coarse-grained tags.
Benson et. al. (2011) present a system which ex-
tracts artists and venues associated with musical per-
formances. Recent work (Han and Baldwin, 2011;
Gouws et al, 2011) has proposed lexical normaliza-
tion of tweets which may be useful as a preprocess-
ing step for the upstream tasks like POS tagging and
NER. In addition Finin et. al. (2010) investigate
the use of Amazon?s Mechanical Turk for annotat-
ing Named Entities in Twitter, Minkov et. al. (2005)
investigate person name recognizers in email, and
Singh et. al. (2010) apply a minimally supervised
approach to extracting entities from text advertise-
ments.
In contrast to previous work, we have demon-
strated the utility of features based on Twitter-
specific POS taggers and Shallow Parsers in seg-
menting Named Entities. In addition we take a dis-
tantly supervised approach to Named Entity Classi-
fication which exploits large dictionaries of entities
gathered from Freebase, requires no manually anno-
tated data, and as a result is able to handle a larger
number of types than previous work. Although we
found manually annotated data to be very beneficial
for named entity segmentation, we were motivated
to explore approaches that don?t rely on manual la-
bels for classification due to Twitter?s wide range of
named entity types. Additionally, unlike previous
work on NER in informal text, our approach allows
the sharing of information across an entity?s men-
tions which is quite beneficial due to Twitter?s terse
nature.
Previous work on Semantic Bootstrapping has
taken a weakly-supervised approach to classifying
named entities based on large amounts of unla-
beled text (Etzioni et al, 2005; Carlson et al, 2010;
Kozareva and Hovy, 2010; Talukdar and Pereira,
2010; McIntosh, 2010). In contrast, rather than
predicting which classes an entity belongs to (e.g.
a multi-label classification task), LabeledLDA esti-
mates a distribution over its types, which is then use-
ful as a prior when classifying mentions in context.
In addition there has been been work on Skip-
Chain CRFs (Sutton, 2004; Finkel et al, 2005)
which enforce consistency when classifying multi-
ple occurrences of an entity within a document. Us-
ing topic models (e.g. LabeledLDA) for classifying
named entities has a similar effect, in that informa-
tion about an entity?s distribution of possible types
is shared across its mentions.
5 Conclusions
We have demonstrated that existing tools for POS
tagging, Chunking and Named Entity Recognition
perform quite poorly when applied to Tweets. To
address this challenge we have annotated tweets and
built tools trained on unlabeled, in-domain and out-
of-domain data, showing substantial improvement
over their state-of-the art news-trained counterparts,
for example, T-POS outperforms the Stanford POS
Tagger, reducing error by 41%. Additionally we
have shown the benefits of features generated from
T-POS and T-CHUNK in segmenting Named Entities.
We identified named entity classification as a par-
ticularly challenging task on Twitter. Due to their
terse nature, tweets often lack enough context to
identify the types of the entities they contain. In ad-
dition, a plethora of distinctive named entity types
are present, necessitating large amounts of training
data. To address both these issues we have presented
and evaluated a distantly supervised approach based
on LabeledLDA, which obtains a 25% increase in F1
score over the co-training approach to Named En-
tity Classification suggested by Collins and Singer
(1999) when applied to Twitter.
Our POS tagger, Chunker Named Entity Rec-
ognizer are available for use by the research
community: http://github.com/aritter/
twitter_nlp
Acknowledgments
We would like to thank Stephen Soderland, Dan
Weld and Luke Zettlemoyer, in addition to the
anonymous reviewers for helpful comments on a
previous draft. This research was supported in part
by NSF grant IIS-0803481, ONR grant N00014-11-
1-0294, Navy STTR contract N00014-10-M-0304, a
National Defense Science and Engineering Graduate
(NDSEG) Fellowship 32 CFR 168a and carried out
at the University of Washington?s Turing Center.
1532
References
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In The
49th Annual Meeting of the Association for Computa-
tional Linguistics, Portland, Oregon, USA. To appear.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn. Res.
Avrim Blum and Tom M. Mitchell. 1998. Combining
labeled and unlabeled sata with co-training. In COLT,
pages 92?100.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka, Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the third ACM interna-
tional conference on Web search and data mining,
WSDM ?10.
Eugene Charniak, Curtis Hendrickson, Neil Jacobson,
and Mike Perkowitz. 1993. Equations for part-of-
speech tagging. In AAAI, pages 784?789.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Empirical
Methods in Natural Language Processing.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating complex named entities in web text.
In Proceedings of the 20th international joint confer-
ence on Artifical intelligence.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2010. Analysis of a probabilistic model of redundancy
in unsupervised information extraction. Artif. Intell.,
174(11):726?748.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, NAACL ?09.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL Workshop on
Creating Speech and Text Language Data With Ama-
zon?s Mechanical Turk. Association for Computational
Linguistics, June.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?05.
Radu Florian. 2002. Named entity recognition as a house
of cards: classifier stacking. In Proceedings of the 6th
conference on Natural language learning - Volume 20,
COLING-02.
Eric N. Forsythand and Craig H. Martell. 2007. Lexical
and discourse analysis of online chat dialog. In Pro-
ceedings of the International Conference on Semantic
Computing.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In ACL.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. Technical report, Microsoft Research.
Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-
uard Hovy. 2011. Contextual bearing on linguistic
variation in social media. In ACL Workshop on Lan-
guage in Social Media, Portland, Oregon, USA. To
appear.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, April.
Mark Hachman. 2011. Humanity?s tweets: Just 20 ter-
abytes. In PCMAG.COM.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
The 49th Annual Meeting of the Association for Com-
putational Linguistics, Portland, Oregon, USA. To ap-
pear.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing sms: are two metaphors
better than one ? In COLING, pages 441?448.
Zornitsa Kozareva and Eduard H. Hovy. 2010. Not all
seeds are equal: Measuring the quality of text mining
seeds. In HLT-NAACL.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In ACL.
Brian Locke and James Martin. 2009. Named entity
recognition: Adapting to microblogging. In Senior
Thesis, University of Colorado.
1533
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. In http://mallet.
cs.umass.edu.
Tara McIntosh. 2010. Unsupervised discovery of nega-
tive categories in lexicon bootstrapping. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: apply-
ing named entity recognition to informal text. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, HLT ?05, pages 443?450, Morristown, NJ,
USA. Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-IJCNLP
2009.
Christoph Mu?ller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy: New Re-
sources, New Tools, New Methods, pages 197?214. Pe-
ter Lang, Frankfurt a.M., Germany.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled lda: a super-
vised topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 248?256,
Morristown, NJ, USA. Association for Computational
Linguistics.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text anal-
ysis applications. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 377?387, Morristown,
NJ, USA. Association for Computational Linguistics.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology - Volume 1, NAACL ?03.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In Human Language Technologies:
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT).
Charles Sutton. 2004. Collective segmentation and la-
beling of distant entities in information extraction.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1473?1481. Associ-
ation for Computational Linguistics.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: chunking.
In Proceedings of the 2nd workshop on Learning lan-
guage in logic and the 4th conference on Computa-
tional natural language learning - Volume 7, ConLL
?00.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference on
streaming document collections. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, ACL ?95.
1534
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1997?2007,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Major Life Event Extraction from Twitter based on
Congratulations/Condolences Speech Acts
Jiwei Li
1
, Alan Ritter
2
, Claire Cardie
3
and Eduard Hovy
4
1
Computer Science Department, Stanford University, Stanford, CA 94305, USA
2
Department of Computer Science and Engineering, the Ohio State University, OH 43210, USA
3
Computer Science Department, Cornell University, Ithaca, NY 14853, USA
4
Language Technology Institute, Carnegie Mellon University, PA 15213, USA
jiweil@stanford.edu ritter.1492@osu.edu
cardie@cs.cornell.edu ehovy@andrew.cmu.edu
Abstract
Social media websites provide a platform
for anyone to describe significant events
taking place in their lives in realtime.
Currently, the majority of personal news
and life events are published in a tex-
tual format, motivating information ex-
traction systems that can provide a struc-
tured representations of major life events
(weddings, graduation, etc. . . ). This pa-
per demonstrates the feasibility of accu-
rately extracting major life events. Our
system extracts a fine-grained description
of users? life events based on their pub-
lished tweets. We are optimistic that our
system can help Twitter users more easily
grasp information from users they take in-
terest in following and also facilitate many
downstream applications, for example re-
altime friend recommendation.
1 Introduction
Social networking websites such as Facebook and
Twitter have recently challenged mainstream me-
dia as the freshest source of information on im-
portant news events. In addition to an important
source for breaking news, social media presents a
unique source of information on private events, for
example a friend?s engagement or college gradua-
tion (examples are presented in Figure 1). While
a significant amount of previous work has inves-
tigated event extraction from Twitter (e.g., (Rit-
ter et al., 2012; Diao et al., 2012)), existing ap-
proaches mostly focus on public bursty event ex-
traction, and little progress has been made towards
the problem of automatically extracting the major
life events of ordinary users.
A system which can automatically extract ma-
jor life events and generate fine-grained descrip-
tions as in Figure 1 will not only help Twitter
users with the problem of information overload by
summarizing important events taking place in their
friends lives, but could also facilitate downstream
applications such as friend recommendation (e.g.,
friend recommendation in realtime to people who
were just admitted into the same university, get
the same jobs or internships), targeted online ad-
vertising (e.g., recommend baby care products to
newly expecting mothers, or wedding services to
new couples), information extraction, etc.
Before getting started, we first identify a num-
ber of key challenges in extracting significant life
events from user-generated text, which account the
reason for the lack of previous work in this area:
Challenge 1: Ambiguous Definition for Ma-
jor Life Events Major life event identification
is an open-domain problem. While many types of
events (e.g., marriage, engagement, finding a new
job, giving birth) are universally agreed to be im-
portant, it is difficult to robustly predefine a list of
characteristics for important life events on which
algorithms can rely for extraction or classification.
Challenge 2: Noisiness of Twitter Data: The
user-generated text found in social media websites
such as Twitter is extremely noisy. The language
used to describe life events is highly varied and
ambiguous and social media users frequently dis-
cuss public news and mundane events from their
daily lives, for instance what they ate for lunch.
Even for a predefined life event category, such
as marriage, it is still difficult to accurately iden-
tify mentions. For instance, a search for the
keyphrase ?get married? using Twitter Search
1
re-
sults in a large number of returned results that do
not correspond to a personal event:
? I want to get married once. No divorce & no
cheating, just us two till the end.
(error: wishes)
1
https://twitter.com/search?q=
get
?
married
1997
Figure 1: Examples of users mentioning personal life events on Twitter.
? Can Adam Sandler and Drew Barrymore just
drop the pretense and get married already?
(error: somebody else)
? I got married and had kids on purpose
(error: past)
Challenge 3: the Lack of Training Data Col-
lecting sufficient training data in this task for ma-
chine learning models is difficult for a number of
reasons: (1) A traditional, supervised learning ap-
proach, requires explicit annotation guidelines for
labeling, though it is difficult to know which cat-
egories are most representative in the data apriori.
(2) Unlike public events which are easily identi-
fied based on message volume, significant private
events are only mentioned by one or several users
directly involved in the event. Many important cat-
egories are relatively infrequent, so even a large
annotated dataset may contain just a few or no ex-
amples of these categories, making classification
difficult.
In this paper, we present a pipelined system that
addresses these challenges and extracts a struc-
tured representation of individual life events based
on users? Twitter feeds. We exploit the insight to
automatically gather large volumes of major life
events which can be used as training examples for
machine learning models. Although personal life
events are difficult to identify using traditional
approaches due to their highly diverse nature, we
noticed that users? followers often directly reply
to such messages with CONGRATULATIONS or
CONDOLENCES speech acts, for example:
User1: I got accepted into Harvard !
User2: Congratulations !
These speech acts are easy to identify with high
precision because the possible ways to express
them are relatively constrained. Instead of directly
inspecting tweets to determine whether they corre-
spond to major life events, we start by identifying
replies corresponding to CONGRATULATIONS or
CONDOLENCES, and then retrieve the message
they are in response to, which we assume refer to
important life events.
The proposed system automatically identifies
major life events and then extracts correspondent
event properties. Through the proposed system,
we demonstrate that it is feasible to automatically
reconstruct a detailed list of individual life events
based on users? Twitter streams. We hope that
work presented in this paper will facilitate down-
stream applications and encourage follow-up work
on this task.
2 System Overview
An overview of the components of the system is
presented in Figure 2. Pipeline1 first identifies
the major life event category the input tweet talks
about and filters out the irrelevant tweets and will
be described in Section 4. Next, Pipeline2, as,
demonstrated in Section 5, identifies whether the
speaker is directly involved in the life event. Fi-
nally, Pipeline3 extracts the property of event and
will be illustrated in Section 6.
Section 3 serves as the preparing step for the
pipelined system, describing how we collect train-
ing data in large-scale. The experimental evalua-
tion regarding each pipeline of the system is pre-
sented in the corresponding section (i.e., Section
4,5,6) and the end-to-end evaluation will be pre-
1998
Figure 2: System Overview. Blue: original input tweets. Red: filtered out tweets. Magenta: life event
category. Green: life event property. Pipeline 1 identifies the life category the input tweet talks about
(e.g., marriage, graduation) and filter out irrelevant tweets (e.g., I had beef stick for lunch). Pipeline 2
identifies whether the speaker is directly involved in the event. It will preserve self-reported information
(i.e. ?I got married?) and filtered out unrelated tweets (e.g., ?my friend Chris got married?). Pipeline
3 extracts the property of event (e.g. to whom the speaker married or the speaker admitted by which
university).
sented in Section 7.
3 Personal Life Event Clustering
In this section, we describe how we identify com-
mon categories of major life events by leverag-
ing large quantities of unlabeled data and obtain
a collection of tweets corresponding to each type
of identified event.
3.1 Response based Life Event Detection
While not all major life events will elicit CON-
GRATULATIONS or CONDOLENCES from a user?s
followers, this technique allows us to collect large
volumes of high-precision personal life events
which can be used to train models to recognize the
diverse categories of major life events discussed
by social media users.
3.2 Life Event Clustering
Based on the above intuition, we develop an ap-
proach to obtain a list of individual life event clus-
ters. We first define a small set of seed responses
which capture common CONGRATULATIONS and
CONDOLENCES, including the phrases: ?Congrat-
ulations?, ?Congrats?, ?Sorry to hear that?, ?Awe-
some?, and gather tweets that were observed with
seed responses. Next, an LDA (Blei et al., 2003)
2
based topic model is used to cluster the gathered
2
Topic Number is set to 120.
tweets to automatically identify important cate-
gories of major life events in an unsupervised way.
In our approach, we model the whole conversation
dialogue as a document
3
with the response seeds
(e.g., congratulation) masked out. We furthermore
associate each sentence with a single topic, fol-
lowing strategies adopted by (Ritter et al., 2010;
Gruber et al., 2007). We limit the words in our
document collection to verbs and nouns which
we found to lead to clearer topic representations,
and used collapsed Gibbs Sampling for inference
(Griffiths and Steyvers, 2004).
Next one of the authors manually inspected the
resulting major life event types inferred by the
model, and manually assigned them labels such
as ?getting a job?, ?graduation? or ?marriage?
and discarded incoherent topics
4
. Our methodol-
ogy is inspired by (Ritter et al., 2012) that uses
a LDA-CLUSTERING+HUMAN-IDENTIFICATION
strategy to identify public events from Twitter.
Similar strategies have been widely used in un-
supervised information extraction (Bejan et al.,
2009; Yao et al., 2011) and selectional preference
3
Each whole conversation usually contains multiple
tweets and users.
4
While we applied manual labeling and coherence eval-
uation in this work, an interesting direction for future work
is automatically labeling major life event categories follow-
ing previous work on labeling topics in traditional document-
based topic models (Mimno et al., 2011; Newman et al.,
2010).
1999
Figure 3: Illustration of bootstrapping process.
Input: Reply seed list E = {e}, Tweet conversation col-
lection T = {t}, Retrieved Tweets Collection D = ?.
Identified topic list L=?
Begin
While not stopping:
1. For unprocessed conversation t ? T
if t contains reply e ? E,
? add t to D: D = D + t.
? remove t from T : T = T ? t
2. Run streaming LDA (Yao et al., 2009) on newly added
tweets in D.
3. Manually Identify meaningful/trash topics, giving label
to meaningful topics.
4. Add newly detected meaningful topic l to L.
5. For conversation t belonging to trash topics
? remove t from D: D = D ? t
6. Harvest more tweets based on topic distribution.
7. Manually identify top 20 responses to tweets harvested
from Step 6.
8. Add meaningful responses to E.
End
Output: Identified topic list L. Tweet collection D.
Figure 4: Bootstrapping Algorithm for Response-
based Life event identification.
modeling (Kozareva and Hovy, 2010a; Roberts
and Harabagiu, 2011).
Conversation data was extracted from the CMU
Twitter Warehouse of 2011 which contains a total
number of 10% of all published tweets in that year.
3.3 Expanding dataset using Bootstrapping
While our seed patterns for identifying mes-
sages expressing CONGRATULATIONS and CON-
DOLENCES are very high precision, they don?t
cover all the possible ways these speech acts
can be expressed. We therefore adopt a semi-
supervised bootstrapping approach to expand our
reply seeds and event-related tweets. Our boot-
strapping approach is related to previous work
on semi-supervised information harvesting (e.g.,
(Kozareva and Hovy, 2010b; Davidov et al.,
2007)). To preserve the labeled topics from the
first iteration, we apply a streaming approach to
inference (Yao et al., 2009) over unlabeled tweets
(those which did not match one of the response
Figure 5: Illustration of data retrieved in each step
of bootstrapping.
congratulations (cong, congrats); (that?s) fantastic; (so) cool;
(I?m) (very) sorry to hear that; (that?s) great (good) new;
awesome; what a pity; have fun; great; that sucks; too
bad; (that?s) unfortunate; how sad; fabulous; (that?s)
terrific; (that?s) (so) wonderful; my deepest condolences;
Table 1: Responses retrieved from Bootstrapping.
seeds). We collect responses to the newly added
tweets, then select the top 20 frequent replies
5
.
Next we manually inspect and filter the top ranked
replies, and use them to harvest more tweets. This
process is then repeated with another round of
inference in LDA including manual labeling of
newly inferred topics, etc... An illustration of our
approach is presented in Figure 3 and the details
are presented in Figure 4. The algorithm outputs
a collection of personal life topics L, and a collec-
tion of retrieved tweets D. Each tweet d ? D is
associated with a life event topic l, l ? L.
We repeat the bootstrapping process for 4 iter-
ations and end up with 30 different CONGRATU-
LATIONS and CONDOLENCES patterns (shown in
Table 1) and 42 coherent event types which refer to
significant life events (statistics for harvested data
from each step is shown in Figure 5). We show
examples of the mined topics with correspondent
human labels in Table 3, grouped according to a
specific kind of resemblance.
3.4 Summary and Discussion
The objective of this section is (1) identifying a
category of life events (2) identifying tweets asso-
ciated with each event type which can be used as
candidates for latter self reported personal infor-
mation and life event category identification.
We understand that the event list retrieved from
our approach based on replies in the conversation
is far from covering all types of personal events
(especially the less frequent life events). But our
5
We only treat the first sentence that responds to the be-
ginning of the conversation as replies.
2000
Life Event Proportion
Birthday 9.78
Job 8.39
Wedding
Engagement
7.24
Award 6.20
Sports 6.08
Anniversary 5.44
Give Birth 4.28
Graduate 3.86
Death 3.80
Admission 3.54
Interview
Internship
3.44
Moving 3.26
Travel 3.24
Illness 2.45
Life Event Proportion
Vacation 2.24
Relationship 2.16
Exams 2.02
Election 1.85
New Car 1.65
Running 1.42
Surgery 1.20
Lawsuit 0.64
Acting 0.50
Research 0.48
Essay 0.35
Lost Weight 0.35
Publishing 0.28
Song 0.22
OTHER 15.31
Table 2: List of automatically discovered life event
types with percentage (%) of data covered.
list is still able to cover a large proportion of IM-
PORTANT and COMMON life events. Our latter
work is focused on given a random tweet, identi-
fying whether it corresponds to one of the 42 types
of life events in our list.
Another thing worth noting here is that, while
current section is not focused on self-reported in-
formation identification, we have already obtained
a relatively clean set of data with a large pro-
portion of non self-reported information related
tweets being screened: people do not usually re-
spond to non self-reported information with com-
monly used replies, or in other words, with replies
that will pass our next step human test
6
. These non
self-reported tweets would therefore be excluded
from training data.
4 Life Event Identification
In this section, we focused on deciding whether a
given tweet corresponds to one of the 42 prede-
fined life events.
Our training dataset consists of approximately
72,000 tweets from 42 different categories of life
events inferred by our topic model as described
in Section 3. We used the top 25% of tweets for
which our model assigned highest probability to
each topic. For sparsely populated topics we used
the top 50% of tweets to ensure sufficient cover-
age.
We further collected a random sample of about
10 million tweets from Twitter API
7
as non-life
6
For example, people don?t normally respond to ?I want
to get married once? (example in Challenge 2, Section 1)
with ?Congratulations?.
7
https://dev.twitter.com/
Human Label Top words
Wedding
&engagement
wedding, love, ring, engagement,
engaged, bride, video, marrying
Relationship
Begin
boyfriend, girlfriend, date, check,
relationship, see, look
Anniversary anniversary, years, year, married,
celebrating, wife, celebrate, love
Relation End/
Devoice
relationship, ended, hurt, hate, de-
voice, blessings, single
Graduation graduation, school, college, gradu-
ate, graduating, year, grad
Admission admitted, university, admission, ac-
cepted, college, offer, school
Exam passed, exam, test, school,
semester, finished, exams,
midterms
Research research, presentation, journalism,
paper, conference, go, writing
Essay & Thesis essay, thesis, reading, statement,
dissertation, complete, project
Job job, accepted, announce, join, join-
ing, offer, starting, announced,
work
Interview& In-
ternship
interview, position, accepted, in-
ternship, offered, start, work
Moving house, moving, move, city, home,
car, place, apartment, town, leaving
Travel leave, leaving, flight, home, miss,
house, airport, packing, morning
Vacation vocation, family, trip, country, go,
flying, visited, holiday, Hawaii
Winning Award won, award, support, awards, win-
ning, honor, scholarship, prize
Election/
Promotion/
Nomination
president, elected, run, nominated,
named, promotion, cel, selected,
business, vote
Publishing book, sold, writing, finished, read,
copy, review, release, books, cover
Contract signed, contract, deal, agreements,
agreed, produce, dollar, meeting
song/ video/ al-
bum release
video, song, album, check, show,
see, making, radio, love
Acting play, role, acting, drama, played,
series, movie, actor, theater
Death dies, passed, cancer, family, hospi-
tal, dad, grandma, mom, grandpa
Give Birth baby, born, boy, pregnant, girl, lbs,
name, son, world, daughter, birth
Illness ill, hospital, feeling, sick, cold, flu,
getting, fever, doctors, cough
Surgery surgery, got, test, emergency, blood,
tumor, stomachs, hospital, pain,
brain
Sports win, game, team, season, fans,
played, winning, football, luck
Running run, race, finished, race, marathon,
ran, miles, running, finish, goal
New Car car, buy, bought, cars, get, drive,
pick, seat, color, dollar, meet
Lost Weight weight, lost, week, pounds, loss,
weeks, gym, exercise, running
Birthday birthday, come, celebrate, party,
friends, dinner, tonight, friend
Lawsuit sue, sued, file, lawsuit, lawyer, dol-
lars, illegal, court, jury.
Table 3: Example event types with top words dis-
covered by our model.
2001
event examples and trained a 43-class maximum
entropy classifier based on the following features:
? Word: The sequence of words in the tweet.
? NER: Named entity Tag.
? Dictionary: Word matching a dictionaries of
the top 40 words for each life event category
(automatically inferred by the topic model).
The feature value is the term?s probability
generated by correspondent event.
? Window: If a dictionary term exists, left and
right context words within a window of 3
words and their part-of-speech tags.
Name entity tag is assigned from Ritter et al?s
Twitter NER system (Ritter et al., 2011). Part-of-
Speech tags are assigned based on Twitter POS
package (Owoputi et al., 2013) developed by
CMU ARK Lab. Dictionary and Window are
constructed based on the topic-term distribution
obtained from the previous section.
The average precision and recall are shown in
Table 4. And as we can observe, the dictionary
(with probability) contributes a lot to the perfor-
mance and by taking into account a more compre-
hensive set of information around the key word,
classifier on All feature setting generate signifi-
cantly better performance, with 0.382 prevision
and 0.48 recall, which is acceptable considering
(1) This is is a 43-way classification with much
more negative data than positive (2) Some types of
events are very close to each other (e.g., Leaving
and Vocation). Note that recall is valued more than
precision here as false-positive examples will be
further screened in self-reported information iden-
tification process in the following section.
Feature Setting Precision Recall
Word+NER 0.204 0.326
Word+NER+Dictionary 0.362 0.433
All 0.382 0.487
Table 4: Average Performance of Multi-Class
Classifier on Different Feature Settings. Negative
examples (non important event type) are not con-
sidered.
5 Self-Reported Information
Identification
Although a message might refer to a topic cor-
responding to a life event such as marriage, the
event still might be one in which the speaker is
not directly involved. In this section we describe
the self reported event identification portion of our
pipeline, which takes output from Section 4 and
further identifies whether each tweet refers to an
event directly involving the user who publishes it.
Direct labeling of randomly sampled Twitter
messages is infeasible for the following reasons:
(1) Class imbalance: self-reported events are rela-
tively rare in randomly sampled Twitter messages.
(2) A large proportion of self-reported information
refers to mundane, everyday topics (e.g., ?I just
finished dinner!?). Fortunately, many of the tweets
retrieved from Section 3 consist of self-reported
information and describe major life events. The
candidates for annotation are therefore largely nar-
rowed down.
We manually annotated 800 positive examples
of self-reported events distributed across the event
categories identified in Section 3. We ensured
good coverage by first randomly sampling 10 ex-
amples from each category, the remainder were
sampled from the class distribution in the data.
Negative examples of self-reported information
consisted of a combination of examples from the
original dataset
8
and randomly sampled messages
gathered by searching for the top terms in each of
the pre-identified topics using the Twitter Search
interface
9
. Due to great varieties of negative sce-
narios, the negative dataset constitutes about 2500
tweets.
5.1 Features
Identifying self-reported tweet requires sophisti-
cated feature engineering. Let u denote the term
within the tweet that gets the highest possibility
generated by the correspondent topic. We experi-
mented with combinations of the following types
of features (results are presented in Table ??):
? Bigram: Bigrams within each tweet (punctu-
ation included).
? Window: A window of k ? {0, 1, 2} words
adjacent to u and their part-of-speech tags.
? Tense: A binary feature indicating past tense
identified in by the presence of past tense
verb (VBD).
? Factuality: Factuality denotes whether one
expression is presented as corresponding to
real situations in the world (Saur?? and Puste-
jovsky, 2007). We use Stanford PragBank
10
,
8
Most tweets in the bootstrapping output are positive.
9
The majority of results returned by Twitter Search are
negative examples.
10
http://compprag.christopherpotts.net/
factbank.html
2002
an extension of FactBank (Saur?? and Puste-
jovsky, 2009) which contains a list of modal
words such as ?might?, ?will?, ?want to?
etc
11
.
? I: Whether the subject of the tweet is first per-
son singular.
? Dependency: If the subject is first person
singular and the u is a verb, the dependency
path between the subject and u (or non-
dependency).
Tweet dependency paths were obtained from
(Kong et al., 2014). As the tweet parser we use
only supports one-to-one dependency path iden-
tification but no dependency properties, Depen-
dency is a binary feature. The subject of each
tweet is determined by the dependency link to the
root of the tweet from the parser.
Among the features we explore, Word encodes
the general information within the tweet. Win-
dow addresses the information around topic key
word. The rest of the features specifically address
each of the negative situations described in Chal-
lenge 2, Section 1: Tense captures past event de-
scription, Factuality filters out wishes or imagi-
nation, I and Dependency correspond to whether
the described event involves the speaker. We built
a linear SVM classifier using SVM
light
package
(Joachims, 1999).
5.2 Evaluation
Feature Setting Acc Pre Rec
Bigram+Window 0.76 0.47 0.44
Bigram+Window
+Tense+Factuality
0.77 0.47 0.46
all 0.82 0.51 0.48
Table 5: Performance for self-report information
identification regarding different feature settings.
We report performance on the task of identi-
fying self-reported information in this subsection.
We employ 5-fold cross validation and report Ac-
curacy (Accu), Prevision (Prec) and Recall (Rec)
regarding different feature settings. The Tense,
Factuality, I and Dependency features positively
contribute to performance respectively and the
best performance is obtained when all types of fea-
tures are included.
11
Due to the colloquial property of tweets, we also intro-
duced terms such as ?gonna?, ?wanna?, ?bona?.
precision recall F1
0.82 0.86 0.84
Table 7: Performance for identifying properties.
6 Event Property Extraction
Thus far we have described how to automatically
identify tweets referring to major life events. In
addition, it is desirable to extract important prop-
erties of the event, for example the name of the
university the speaker was admitted to (See Figure
1). In this section we take a supervised approach to
event property extraction, based on manually an-
notated data for a handfull of the major life event
categories automatically identified by our system.
While this approach is unlikely to scale to the di-
versity of important personal events Twitter users
are discussing, our experiments demonstrate that
event property extraction is indeed feasible.
We cast the problem of event property extrac-
tion as a sequence labeling task, using Conditional
Random Fields (Lafferty et al., 2001) for learning
and inference. To make best use of the labeled
data, we trained a unified CRF model for closely
related event categories which often share proper-
ties; the full list is presented in Table 6 and we
labeled 300 tweets in total. Features we used in-
clude:
? word token, capitalization, POS
? left and right context words within a window
of 3 and the correspondent part-of-speech
tags
? word shape, NER
? a gazetteer of universities and employers bor-
rowed from NELL
12
.
We use 5-fold cross-validation and report results
in Table 7.
7 End-to-End Experiment
The evaluation for each part of our system has
been demonstrated in the corresponding section.
We now present a real-world evaluation: to what
degree can our trained system automatically iden-
tify life events in real world.
7.1 Dataset
We constructed a gold-standard life event dataset
using annotators from Amazon?s Mechanical Turk
(Snow et al., 2008) using 2 approaches:
12
http://rtw.ml.cmu.edu/rtw/kbbrowser/
2003
Life Event Property
(a) Acceptance, Graduation Name of University/College
(b) Wedding, Engagement, Falling love Name of Spouse/ partner/ bf/ gf
(c) Getting a job, interview, internship Name of Enterprise
(d) Moving to New Places, Trip, Vocation, Leaving Place, Origin, Destination
(e) Winning Award Name of Award, Prize
Table 6: Labeling Event Property.
? Ask Twitter users to label their own tweets
(Participants include friends, colleagues of
the authors and Turkers from Amazon Me-
chanical Turk
13
).
? Ask Turkers to label other people?s tweets.
For option 1, we asked participants to directly la-
bel their own published tweets. For option 2, for
each tweet, we employed 2 Turkers. Due to the
ambiguity in defining life events, the value co-
hen?s kappa
14
as a measure of inter-rater agree-
ment is 0.54; this does not show significant inter-
annotator agreement. The authors examined dis-
agreements and also verified all positively labeled
tweets. The resulting dataset contains around 900
positive tweets and about 60,000 negative tweets.
To demonstrate the advantage of leveraging
large quantities of unlabeled data, the first base-
line we investigate is a Supervised model which is
trained on the manually annotated labeled dataset,
and evaluated using 5 fold cross validation. Our
Supervised baseline consists of a linear SVM
classifier using bag of words, NER and POS fea-
tures. We also tested a second baseline that
combines Supervised algorithm with an our self-
reported information classifier, denoted as Super-
vised+Self.
Results are reported in Table 8; as we can ob-
serve, the fully supervised approach is not suitable
for this task with only one digit F1 score. The
explanations are as follows: (1) the labeled data
can only cover a small proportion of life events
(2) supervised learning does not separate impor-
tant event categories and will therefore classify
any tweet with highly weighted features (e.g., the
mention of ?I? or ?marriage?) as positive. By us-
ing an additional self-reported information classi-
fier in Supervised+Self, we get a significant boost
in precision with a minor recall loss.
13
https://www.mturk.com/mturk/welcome
14
http://en.wikipedia.org/wiki/Cohen?s_
kappa
Approach Precision Recall
Our approach 0.62 0.48
Supervised 0.13 0.20
Supervised+Self 0.25 0.18
Table 8: Performance for different approaches for
identifying life events in real world.
Approach Precision Recall
Step 1 0.65 0.36
Step 2 0.64 0.43
Step 3 0.62 0.48
Table 9: Performance for different steps of boot-
strapping for identifying life events in real world.
Another interesting question is to what degree
the bootstrapping contributes to the final results.
We keep the self-reported information classifier
fixed (though it?s based the ultimate identified
data source), and train the personal event classifier
based on topic distributions identified from each
of the three steps of bootstrapping
15
. Precision
and recall at various stages of bootstrapping are
presented in Table 9. As bootstrapping continues,
the precision remains roughly constant, but recall
increases as more life events and CONGRATULA-
TIONS and CONDOLENCES are discovered.
8 Related Work
Our work is related to three lines of NLP re-
searches. (1) user-level information extraction on
social media (2) public event extraction on social
media. (3) Data harvesting in Information Extrac-
tion, each of which contains large amount of re-
lated work, to which we can not do fully justice.
User Information Extraction from Twitter
Some early approaches towards understanding
user level information on social media is focused
on user profile/attribute prediction (e.g.,(Ciot et
al., 2013)) user-specific content extraction (Diao
15
which are 24, 38, 42-class classifiers, where 24, 38, 42
denoted the number of topics discovered in each step of boot-
strapping (see Figure 5).
2004
et al., 2012; Diao and Jiang, 2013; Li et al., 2014)
or user personalization (Low et al., 2011) identifi-
cation.
The problem of user life event extraction was
first studied by Li and Cardie?s (2014). They at-
tempted to construct a chronological timeline for
Twitter users from their published tweets based on
two criterion: a personal event should be personal
and time-specific. Their system does not explic-
itly identify a global category of life events (and
tweets discussing correspondent event) but identi-
fies the topics/events that are personal and time-
specific to a given user using an unsupervised ap-
proach, which helps them avoids the nuisance of
explicit definition for life event characteristics and
acquisition of labeled data. However, their sys-
tem has the short-coming that each personal topic
needs to be adequately discussed by the user and
their followers in order to be detected
16
.
Public Event Extraction from Twitter Twitter
serves as a good source for event detection owing
to its real time nature and large number of users.
These approaches include identifying bursty pub-
lic topics (e.g.,(Diao et al., 2012)), topic evolution
(Becker et al., 2011) or disaster outbreak (Sakaki
et al., 2010; Li and Cardie, 2013) by spotting the
increase/decrease of word frequency. Some other
approaches are focused on generating a structured
representation of events (Ritter et al., 2012; Ben-
son et al., 2011).
Data Acquisition in Information Extraction
Our work is also related with semi-supervised data
harvesting approaches, the key idea of which is
that some patterns are learned based on seeds.
They are then used to find additional terms, which
are subsequently used as new seeds in the patterns
to search for additional new patterns (Kozareva
and Hovy, 2010b; Davidov et al., 2007; Riloff
et al., 1999; Igo and Riloff, 2009; Kozareva et
al., 2008). Also related approaches are distant or
weakly supervision (Mintz et al., 2009; Craven et
al., 1999; Hoffmann et al., 2011) that rely on avail-
able structured data sources as a weak source of
supervision for pattern extraction from related text
corpora.
16
The reason is that topic models use word frequency for
topic modeling.
9 Conclusion and Discussion
In this paper, we propose a pipelined system for
major life event extraction from Twitter. Experi-
mental results show that our model is able to ex-
tract a wide variety of major life events.
The key strategy adopted in this work is to ob-
tain a relatively clean training dataset from large
quantity of Twitter data by relying on minimum
efforts of human supervision, and sometimes is at
the sacrifice of recall. To achieve this goal, we rely
on a couple of restrictions and manual screenings,
such as relying on replies, LDA topic identifica-
tion and seed screening. Each part of system de-
pends on the early steps. For example, topic clus-
tering in Section 3 not only offers training data for
event identification in Section 4, but prepares the
training data for self-information identification in
Section 5. .
We acknowledge that our approach is not
perfect due to the following ways: (1) The system
is only capable of discovering a few categories
of life events with many others left unidentified.
(2) Each step of the system will induce errors and
negatively affected the following parts. (3) Some
parts of evaluations are not comprehensive due
to the lack of gold-standard data. (4) Among all
pipelines, event property identification in Section
6 still requires full supervision in CRF model,
making it hard to scale to every event type
17
.
How to address these aspects and generate a more
accurate, comprehensive and fine-grained life
event list for Twitter users constitute our further
work.
Acknowledgements
A special thanks is owned to Myle Ott for sug-
gestions on bootstrapping procedure in data har-
vesting. The authors want to thank Noah Smith,
Chris Dyer and Alok Kothari for useful com-
ments, discussions and suggestions regarding dif-
ferent steps of the system and evaluations. We
thank Lingpeng Kong and members of Noah?s
ARK group at CMU for providing the tweet de-
pendency parser. All data used in this work is ex-
tracted from CMU Twitter Warehouse maintained
by Brendan O?Connor, to whom we want to ex-
press our gratitude.
17
We view weakly supervised life event property extrac-
tion as an interesting direction for future work.
2005
References
Hila Becker, Mor Naaman, and Luis Gravano. 2011.
Beyond trending topics: Real-world event identifi-
cation on twitter. ICWSM, 11:438?441.
Cosmin Adrian Bejan, Matthew Titsworth, Andrew
Hickl, and Sanda M Harabagiu. 2009. Nonparamet-
ric bayesian models for unsupervised event corefer-
ence resolution. In NIPS, pages 73?81.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 389?398. As-
sociation for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18?21.
Mark Craven, Johan Kumlien, et al. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77?86.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Annual
Meeting-Association For Computational Linguis-
tics, volume 45, page 232.
Qiming Diao and Jing Jiang. 2013. A unified model
for topics, events and users on twitter. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1869?1879.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng
Lim. 2012. Finding bursty topics from microblogs.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 536?544. Association for
Computational Linguistics.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.
2007. Hidden topic markov models. In Inter-
national Conference on Artificial Intelligence and
Statistics, pages 163?170.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-
Volume 1, pages 541?550. Association for Compu-
tational Linguistics.
Sean P Igo and Ellen Riloff. 2009. Corpus-based se-
mantic lexicon induction with web-based corrobora-
tion. In Proceedings of the Workshop on Unsuper-
vised and Minimally Supervised Learning of Lexical
Semantics, pages 18?26. Association for Computa-
tional Linguistics.
Thorsten Joachims. 1999. Making large scale svm
learning practical.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah Smith. 2014. A dependency parser for tweets.
In EMNLP.
Zornitsa Kozareva and Eduard Hovy. 2010a. Learn-
ing arguments and supertypes of semantic relations
using recursive patterns. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1482?1491. Association
for Computational Linguistics.
Zornitsa Kozareva and Eduard Hovy. 2010b. Not
all seeds are equal: Measuring the quality of text
mining seeds. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 618?626. Association for Computa-
tional Linguistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL, volume 8,
pages 1048?1056.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Jiwei Li and Claire Cardie. 2013. Early stage
influenza detection from twitter. arXiv preprint
arXiv:1309.7340.
Jiwei Li and Claire Cardie. 2014. Timeline generation:
Tracking individuals on twitter. WWW, 2014.
Jiwei Li, Alan Ritter, and Eduard Hovy. 2014.
Weakly supervised user profile extraction from twit-
ter. ACL.
Yucheng Low, Deepak Agarwal, and Alexander J
Smola. 2011. Multiple domain user personaliza-
tion. In Proceedings of the 17th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 123?131. ACM.
David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
2006
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 262?
272. Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003?1011. Association for
Computational Linguistics.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100?108. Association for Computa-
tional Linguistics.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Ellen Riloff, Rosie Jones, et al. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI/IAAI, pages 474?479.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524?1534. Association for Computational Linguis-
tics.
Alan Ritter, Oren Etzioni, Sam Clark, et al. 2012.
Open domain event extraction from twitter. In Pro-
ceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 1104?1112. ACM.
Kirk Roberts and Sanda M Harabagiu. 2011. Unsuper-
vised learning of selectional restrictions and detec-
tion of argument coercions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 980?990. Association for
Computational Linguistics.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th international conference on World wide
web, pages 851?860. ACM.
Roser Saur?? and James Pustejovsky. 2007. Deter-
mining modality and factuality for text entailment.
In Semantic Computing, 2007. ICSC 2007. Interna-
tional Conference on, pages 509?516. IEEE.
Roser Saur?? and James Pustejovsky. 2009. Factbank:
A corpus annotated with event factuality. Language
resources and evaluation, 43(3):227?268.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254?263. Association for Computational
Linguistics.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1456?1466. Association
for Computational Linguistics.
2007
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 172?180,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Modeling of Twitter Conversations
Alan Ritter?
Computer Sci. & Eng.
University of Washington
Seattle, WA 98195
aritter@cs.washington.edu
Colin Cherry?
National Research Council Canada
Ottawa, Ontario, K1A 0R6
Colin.Cherry@nrc-cnrc.gc.ca
Bill Dolan
Microsoft Research
Redmond, WA 98052
billdol@microsoft.com
Abstract
We propose the first unsupervised approach
to the problem of modeling dialogue acts in
an open domain. Trained on a corpus of
noisy Twitter conversations, our method dis-
covers dialogue acts by clustering raw utter-
ances. Because it accounts for the sequential
behaviour of these acts, the learned model can
provide insight into the shape of communica-
tion in a new medium. We address the chal-
lenge of evaluating the emergent model with a
qualitative visualization and an intrinsic con-
versation ordering task. This work is inspired
by a corpus of 1.3 million Twitter conversa-
tions, which will be made publicly available.
This huge amount of data, available only be-
cause Twitter blurs the line between chatting
and publishing, highlights the need to be able
to adapt quickly to a new medium.
1 Introduction
Automatic detection of dialogue structure is an im-
portant first step toward deep understanding of hu-
man conversations. Dialogue acts1 provide an
initial level of structure by annotating utterances
with shallow discourse roles such as ?statement?,
?question? and ?answer?. These acts are useful in
many applications, including conversational agents
(Wilks, 2006), dialogue systems (Allen et al, 2007),
dialogue summarization (Murray et al, 2006), and
flirtation detection (Ranganath et al, 2009).
Dialogue act tagging has traditionally followed an
annotate-train-test paradigm, which begins with the
?This work was conducted at Microsoft Research.
1Also called ?speech acts?
design of annotation guidelines, followed by the col-
lection and labeling of corpora (Jurafsky et al, 1997;
Dhillon et al, 2004). Only then can one train a tag-
ger to automatically recognize dialogue acts (Stol-
cke et al, 2000). This paradigm has been quite suc-
cessful, but the labeling process is both slow and
expensive, limiting the amount of data available for
training. The expense is compounded as we con-
sider new methods of communication, which may
require not only new annotations, but new annota-
tion guidelines and new dialogue acts. This issue be-
comes more pressing as the Internet continues to ex-
pand the number of ways in which we communicate,
bringing us e-mail, newsgroups, IRC, forums, blogs,
Facebook, Twitter, and whatever is on the horizon.
Previous work has taken a variety of approaches
to dialogue act tagging in new media. Cohen et al
(2004) develop an inventory of dialogue acts specific
to e-mail in an office domain. They design their in-
ventory by inspecting a large corpus of e-mail, and
refine it during the manual tagging process. Jeong et
al. (2009) use semi-supervised learning to transfer
dialogue acts from labeled speech corpora to the In-
ternet media of forums and e-mail. They manually
restructure the source act inventories in an attempt
to create coarse, domain-independent acts. Each ap-
proach relies on a human designer to inject knowl-
edge into the system through the inventory of avail-
able acts.
As an alternative solution for new media, we pro-
pose a series of unsupervised conversation models,
where the discovery of acts amounts to clustering
utterances with similar conversational roles. This
avoids manual construction of an act inventory, and
allows the learning algorithm to tell us something
about how people converse in a new medium.
172
There is surprisingly little work in unsupervised
dialogue act tagging. Woszczyna and Waibel (1994)
propose an unsupervised Hidden Markov Model
(HMM) for dialogue structure in a meeting schedul-
ing domain, but model dialogue state at the word
level. Crook et al (2009) use Dirichlet process mix-
ture models to cluster utterances into a flexible num-
ber of acts in a travel-planning domain, but do not
examine the sequential structure of dialogue.2
In contrast to previous work, we address the prob-
lem of discovering dialogue acts in an informal,
open-topic domain, where an unsupervised learner
may be distracted by strong topic clusters. We also
train and test our models in a new medium: Twit-
ter. Rather than test against existing dialogue inven-
tories, we evaluate using qualitative visualizations
and a novel conversation ordering task, to ensure our
models have the opportunity to discover dialogue
phenomena unique to this medium.
2 Data
To enable the study of large-data solutions to di-
alogue modeling, we have collected a corpus of
1.3 million conversations drawn from the micro-
blogging service, Twitter. 3 To our knowledge,
this is the largest corpus of naturally occurring chat
data that has been available for study thus far. Sim-
ilar datasets include the NUS SMS corpus (How
and Kan, 2005), several IRC chat corpora (Elsner
and Charniak, 2008; Forsyth and Martell, 2007),
and blog datasets (Yano et al, 2009; Gamon et al,
2008), which can display conversational structure in
the blog comments.
As it characterizes itself as a micro-blog, it should
not be surprising that structurally, Twitter conversa-
tions lie somewhere between chat and blogs. Like
blogs, conversations on Twitter occur in a public en-
vironment, where they can be collected for research
purposes. However, Twitter posts are restricted to be
no longer than 140 characters, which keeps interac-
tions chat-like. Like e-mail and unlike IRC, Twit-
ter conversations are carried out by replying to spe-
cific posts. The Twitter API provides a link from
each reply to the post it is responding to, allowing
2The Crook et al model should be able to be combined with
the models we present here.
3Will be available at http://www.cs.washington.
edu/homes/aritter/twitter_chat/
1 2 3 4 5
0
2
4
6
8
10
12
14
log length
log 
freq
uen
cy
Figure 1: Conversation length versus frequency
accurate thread reconstruction without requiring a
conversation disentanglement step (Elsner and Char-
niak, 2008). The proportion of posts on Twitter that
are conversational in nature are somewhere around
37% (Kelly, 2009).
To collect this corpus, we crawled Twitter using
its publicly available API. We monitored the public
timeline4 to obtain a sample of active Twitter users.
To expand our user list, we also crawled up to 10
users who had engaged in dialogue with each seed
user. For each user, we retrieved all posts, retain-
ing only those that were in reply to some other post.
We recursively followed the chain of replies to re-
cover the entire conversation. A simple function-
word-driven filter was used to remove non-English
conversations.
We crawled Twitter for a 2 month period during
the summer of 2009. The resulting corpus consists
of about 1.3 million conversations, with each con-
versation containing between 2 and 243 posts. The
majority of conversations on Twitter are very short;
those of length 2 (one status post and a reply) ac-
count for 69% of the data. As shown in Figure 1, the
frequencies of conversation lengths follow a power-
law relationship.
While the style of writing used on Twitter is
widely varied, much of the text is very similar to
SMS text messages. This is likely because many
users access Twitter through mobile devices. Posts
are often highly ungrammatical, and filled with
spelling errors. In order to illustrate the spelling
variation found on Twitter, we ran the Jcluster word
clustering algorithm (Goodman, 2001) on our cor-
4http://twitter.com/public_timeline pro-
vides the 20 most recent posts on Twitter
173
coming comming
enough enought enuff enuf
be4 b4 befor before
yuhr yur your yor ur youur yhur
msgs messages
couldnt culdnt cldnt cannae cudnt couldent
about bou abt abour abut bowt
Table 1: A sample of Twitter spelling variation.
pus, and manually picked out clusters of spelling
variants; a sample is displayed in Table 1.
Twitter?s noisy style makes processing Twitter
text more difficult than other domains. While mov-
ing to a new domain (e.g. biomedical text) is a chal-
lenging task, at least the new words found in the
vocabulary are limited mostly to verbs and nouns,
while function words remain constant. On Twit-
ter, even closed-class words such as prepositions and
pronouns are spelled in many different ways.
3 Dialogue Analysis
We propose two models to discover dialogue acts in
an unsupervised manner. An ideal model will give
insight into the sorts of conversations that happen
on Twitter, while providing a useful tool for later
processing. We first introduce the summarization
technology we apply to this task, followed by two
Bayesian extensions.
3.1 Conversation model
Our base model structure is inspired by the con-
tent model proposed by Barzilay and Lee (2004)
for multi-document summarization. Their sentence-
level HMM discovers the sequence of topics used
to describe a particular type of news event, such as
earthquakes. A news story is modeled by first gen-
erating a sequence of hidden topics according to a
Markov model, with each topic generating an ob-
served sentence according to a topic-specific lan-
guage model. These models capture the sequential
structure of news stories, and can be used for sum-
marization tasks such as sentence extraction and or-
dering.
Our goals are not so different: we wish to dis-
cover the sequential dialogue structure of conversa-
tion. Rather than learning a disaster?s location is
followed by its death toll, we instead wish to learn
that a question is followed by an answer. An initial
a
0
a
1
a
2
w
0,j
w
1,j
w
2,j
W
0
W
1
W
2
C
k
Figure 2: Conversation Model
a
0
a
1
a
2
w
0,j
w
1,j
w
2,j
W
0
W
1
W
2
C
k
s
0,j
s
1,j
s
2,j
?
k
?
E
pi
k
Figure 3: Conversation + Topic Model
conversation model can be created by simply apply-
ing the content modeling framework to conversation
data. We rename the hidden states acts, and assume
each post in a Twitter conversation is generated by
a single act.5 During development, we found that a
unigram language model performed best as the act
emission distribution.
The resulting conversation model is shown as a
plate diagram in Figure 2. Each conversation C is
a sequence of acts a, and each act produces a post,
represented by a bag of words shown using the W
plates. The number of acts available to the model
is fixed; we experimented with between 5 and 40.
Starting with a random assignment of acts, we train
our conversation model using EM, with forward-
backward providing act distributions during the ex-
pectation step. The model structure in Figure 2 is
5The short length of Twitter posts makes this assumption
reasonable.
174
sadly no. some pasta bake, but coffee and pasta bake is not a
contender for tea and toast... .
yum! Ground beef tacos? We ?re grilling out. Turkey dogs for
me, a Bubba Burger for my dh, and combo for the kids.
ha! They gotcha! You had to think about Arby?s to write that tweet.
Arby?s is conducting a psychlogical study. Of roast beef.
Rumbly tummy soon to be tamed by Dominos for lunch! Nom
nom nom!
Table 2: Example of a topical cluster discovered by
the EM Conversation Model.
similar to previous HMMs for supervised dialogue
act recognition (Stolcke et al, 2000), but our model
is trained unsupervised.
3.2 Conversation + Topic model
Our conversations are not restricted to any partic-
ular topic: Twitter users can and will talk about
anything. Therefore, there is no guarantee that our
model, charged with discovering clusters of posts
that aid in the prediction of the next cluster, will nec-
essarily discover dialogue acts. The sequence model
could instead partition entire conversations into top-
ics, such as food, computers and music, and then pre-
dict that each topic self-transitions with high proba-
bility: if we begin talking about food, we are likely
to continue to do so. Since we began with a content
model, it is perhaps not surprising that our Conversa-
tion Model tends to discover a mixture of dialogue
and topic structure. Several high probability posts
from a topic-focused cluster discovered by EM are
shown in Table 2. These clusters are undesirable, as
they have little to do with dialogue structure.
In general, unsupervised sentence clustering tech-
niques need some degree of direction when a par-
ticular level of granularity is desired. Barzilay and
Lee (2004) mask named entities in their content
models, forcing their model to cluster topics about
earthquakes in general, and not instances of specific
earthquakes. This solution is not a good fit for Twit-
ter. As explained in Section 2, Twitter?s noisiness
resists off-the-shelf tools, such as named-entity rec-
ognizers and noun-phrase chunkers. Furthermore,
we would require a more drastic form of prepro-
cessing in order to mask all topic words, and not
just alter the topic granularity. During development,
we explored coarse methods to abstract away con-
tent while maintaining syntax, such as replacing to-
kens with either parts-of-speech or automatically-
generated word clusters, but we found that these ap-
proaches degrade model performance.
Another approach to filtering out topic informa-
tion leaves the data intact, but modifies the model
to account for topic. To that end, we adopt a Latent
Dirichlet Allocation, or LDA, framework (Blei et al,
2003) similar to approaches used recently in sum-
marization (Daume? III and Marcu, 2006; Haghighi
and Vanderwende, 2009). The goal of this extended
model is to separate content words from dialogue in-
dicators. Each word in a conversation is generated
from one of three sources:
? The current post?s dialogue act
? The conversation?s topic
? General English
The extended model is shown in Figure 3.6 In addi-
tion to act emission and transition parameters, the
model now includes a conversation-specific word
multinomial ?k that represents the topic, as well as a
universal general English multinomial ?E . A new
hidden variable, s determines the source of each
word, and is drawn from a conversation-specific dis-
tribution over sources pik. Following LDA conven-
tions, we place a symmetric Dirichlet prior over
each of the multinomials. Dirichlet concentration
parameters for act emission, act transition, conver-
sation topic, general English, and source become the
hyper-parameters of our model.
The multinomials ?k, pik and ?E create non-local
dependencies in our model, breaking our HMM dy-
namic programing. Therefore we adopt Gibbs sam-
pling as our inference engine. Each hidden vari-
able is sampled in turn, conditioned on a complete
assignment of all other hidden variables through-
out the data set. Again following LDA convention,
we carry out collapsed sampling, where the various
multinomials are integrated out, and are never ex-
plicitly estimated. This results in a sampling se-
quence where for each post we first sample its act,
and then sample a source for each word in the post.
The hidden act and source variables are sampled ac-
cording to the following transition distributions:
6This figure omits hyperparameters as well as act transition
and emission multinomials to reduce clutter. Dirichlet priors are
placed over all multinomials.
175
Ptrans(ai|a?i, s,w) ?
P (ai|a?i)
Wi?
j=1
P (wi,j |a, s,w?(i,j))
Ptrans(si,j |a, s?(i,j),w) ?
P (si,j |s?(i,j))P (wi,j |a, s,w?(i,j))
These probabilities can be computed analogously to
the calculations used in the collapsed sampler for a
bigram HMM (Goldwater and Griffiths, 2007), and
those used for LDA (Griffiths and Steyvers, 2004).
Note that our model contains five hyperparame-
ters. Rather than attempt to set them using an ex-
pensive grid search, we treat the concentration pa-
rameters as additional hidden variables and sample
each in turn, conditioned on the current assignment
to all other variables. Because these variables are
continuous, we apply slice sampling (Neal, 2003).
Slice sampling is a general technique for drawing
samples from a distribution by sampling uniformly
from the area under its density function.
3.3 Estimating Likelihood on Held-Out Data
In Section 4.2 we evaluate our models by comparing
their probability on held-out test conversations. As
computing this probability exactly is intractable in
our model, we employ a recently proposed Chibb-
style estimator (Murray and Salakhutdinov, 2008;
Wallach et al, 2009). Chibb estimators estimate the
probability of unseen data, P (w) by selecting a high
probability assignment to hidden variables h?, and
taking advantage of the following equality which
can be easily derived from the definition of condi-
tional probability:
P (w) =
P (w,h?)
P (h?|w)
As the numerator can be computed exactly, this re-
duces the problem of estimating P (w) to the eas-
ier problem of estimating P (h?|w). Murray and
Salakhutdinov (2008) provide an unbiased estimator
for P (h?|w), which is calculated using the station-
ary distribution of the Gibbs sampler.
3.4 Bayesian Conversation model
Given the infrastructure necessary for the Conver-
sation+Topic model described above, it is straight-
forward to also implement a Bayesian version of
of the conversation model described in Section 3.1.
This amounts to replacing the add-x smoothing of
dialogue act emission and transition probabilities
with (potentially sparse) Dirichlet priors, and replac-
ing EM with Gibbs sampling. There is reason to
believe that integrating out multinomials and using
sparse priors will improve the performance of the
conversation model, as improvements have been ob-
served when using a Bayesian HMM for unsuper-
vised part-of-speech tagging (Goldwater and Grif-
fiths, 2007).
4 Experiments
Evaluating automatically discovered dialogue acts
is a difficult problem. Unlike previous work, our
model automatically discovers an appropriate set of
dialogue acts for a new medium; these acts will
not necessarily have a close correspondence to di-
alogue act inventories manually designed for other
corpora. Instead of comparing against human anno-
tations, we present a visualization of the automati-
cally discovered dialogue acts, in addition to mea-
suring the ability of our models to predict post order
in unseen conversations. Ideally we would evaluate
performance using an end-use application such as a
conversational agent; however as this is outside the
scope of this paper, we leave such an evaluation to
future work.
For all experiments we train our models on a set of
10,000 randomly sampled conversations with con-
versation length in posts ranging from 3 to 6. Note
that our implementations can likely scale to larger
data by using techniques such as SparseLDA (Yao
et al, 2009). We limit our vocabulary to the 5,000
most frequent words in the corpus.
When using EM, we train for 100 iterations, eval-
uating performance on the test set at each iteration,
and reporting the maximum. Smoothing parameters
are set using grid search on a development set.
When performing inference with Gibbs Sam-
pling, we use 1,000 samples for burn-in and take
10 samples at a lag of 100. Although using multi-
ple samples introduces the possibility of poor results
due to ?act drift?, we found this not to be a problem
in practice; in fact, taking multiple samples substan-
tially improved performance during development.
Recall that we infer hyperparameters using slice
176
sampling. The concentration parameters chosen in
this manner were always sparse (< 1), which pro-
duced a moderate improvement over an uninformed
prior.
4.1 Qualitative Evaluation
We are quite interested in what our models can tell
us about how people converse on Twitter. To vi-
sualize and interpret our competing models, we ex-
amined act-emission distributions, posts with high-
confidence acts, and act-transition diagrams. Of
the three competing systems, we found the Conver-
sation+Topic model by far the easiest to interpret:
the 10-act model has 8 acts that we found intuitive,
while the other 2 are used only with low probabil-
ity. Conversely, the Conversation model, whether
trained by EM or Gibbs sampling, suffered from
the inclusion of general terms and from the confla-
tion of topic and dialogue. For example, the EM-
trained conversation model discovered an ?act? that
was clearly a collection of posts about food, with no
underlying dialogue theme (see Table 2).
In the remainder of this section, we reproduce
our visualization for the 10-act Conversation+Topic
model. Word lists summarizing the discovered dia-
logue acts are shown in Table 3. For each act, the
top 40 words are listed in order of decreasing emis-
sion probability. An example post, drawn from the
set of highest-confidence posts for that act, is also
included. Figure 4 provides a visualization of the
matrix of transition probabilities between dialogue
acts. An arrow is drawn from one act to the next
if the probability of transition is above 0.15.7 Note
that a uniform model would transition to each act
with probability 0.10. In both Table 3 and Figure 4,
we use intuitive names in place of cluster numbers.
These are based on our interpretations of the clus-
ters, and are provided only to benefit the reader when
interpreting the transition diagram.8
From inspecting the transition diagram (Figure 4),
one can see that the model employs three distinct
acts to initiate Twitter conversations. These initial
acts are quite different from one another, and lead to
7After setting this threshold, two Acts were cut off from the
rest of the graph (had no incoming edges), and were therefore
removed
8In some cases, the choice in name is somewhat arbitrary,
ie: answer versus response, reaction versus comment.
Figure 4: Transitions between dialogue acts. See
table 3 for word lists and example posts for each act
different sets of possible responses. We discuss each
of these in turn.
The Status act appears to represent a post in which
the user is broadcasting information about what they
are currently doing. This can be seen by the high
amount of probability mass given to words like I
and my, in addition to verbs such as go and get, as
well as temporal nouns such as today, tomorrow and
tonight.
The Reference Broadcast act consists mostly of
usernames and urls.9 Also prominent is the word rt,
which has special significance on Twitter, indicating
that the user is re-posting another user?s post. This
act represents a user broadcasting an interesting link
or quote to their followers. Also note that this node
transitions to the Reaction act with high probability.
Reaction appears to cover excited or appreciative re-
sponses to new information, assigning high proba-
bility to !, !!, !!!, lol, thanks, and haha.
Finally Question to Followers represents a user
asking a question to their followers. The presence
of the question mark and WH question words indi-
cate a question, while words like anyone and know
indicate that the user is asking for information or an
opinion. Note that this is distinct from the Question
act, which is in response to an initial post.
Another interesting point is the alternation be-
9As part of the preprocessing of our corpus we replaced all
usernames and urls with the special tokens -usr- and -url-.
177
Status I . to ! my , is for up in ... and going was today so at go get back day got this am but Im now tomorrow night work
tonight off morning home had gon need !! be just getting
I just changed my twitter page bkgornd and now I can?t stop looking at it, lol!!
Question to Followers ? you is do I to -url- what -usr- me , know if anyone why who can ? this or of that how does - : on your are need
any rt u should people want get did have would tell
anyone using google voice? just got my invite, should i?? don?t know what it is? -url- for the video and break
down
Reference Broadcast -usr- ! -url- rt : -usr-: - ? my the , is ( you new ? ? !! ) this for at in follow of on ? lol u are twitter your thanks via
!!! by :) here 2 please check
rt -usr-: -usr- word that mac lip gloss give u lock jaw! lol
Question ? you what ! are is how u do the did your that , lol where why or ?? hey about was have who it in so haha on
doing going know good up get like were for there :) can
DWL!! what song is that??
Reaction ! you I :) !! , thanks lol it haha that love so good too your thank is are u !!! was for :d me -usr- ? hope ? my 3 omg
... oh great hey awesome - happy now aww
sweet! im so stoked now!
Comment you I . to , ! do ? it be if me your know have we can get will :) but u that see lol would are so want go let up well
need - come ca make or think them
why are you in tx and why am I just now finding out about it?! i?m in dfw, till I get a job. i?ll have to come to
Htown soon!
Answer . I , you it ? that ? is but do was he the of a they if not would know be did or does think ) like ( as have what in are
- no them said who say ?
my fave was ?keeping on top of other week?
Response . I , it was that lol but is yeah ! haha he my know yes you :) like too did well she so its ... though do had no - one
as im thanks they think would not good oh
nah im out in maryland, leaving for tour in a few days.
Table 3: Word lists and example posts for each Dialogue Act. Words are listed in decreasing order of
probability given the act. Example posts are in italics.
tween the personal pronouns you and I in the acts
due to the focus of conversation and speaker. The
Status act generates the word I with high probability,
whereas the likely response state Question generates
you, followed by Response which again generates I.
4.2 Quantitative Evaluation
Qualitative evaluations are both time-consuming
and subjective. The above visualization is useful for
understanding the Twitter domain, but it is of little
use when comparing model variants or selecting pa-
rameters. Therefore, we also propose a novel quan-
titative evaluation that measures the intrinsic qual-
ity of a conversation model by its ability to predict
the ordering of posts in a conversation. This mea-
sures the model?s predictive power, while requiring
no tagged data, and no commitment to an existing
tag inventory.
Our test set consists of 1,000 randomly selected
conversations not found in the training data. For
each conversation in the test set, we generate all
n! permutations of the posts. The probability of
each permutation is then evaluated as if it were an
unseen conversation, using either the forward algo-
rithm (EM) or the Chibb-style estimator (Gibbs).
Following work from the summarization community
(Barzilay and Lee, 2004), we employ Kendall?s ? to
measure the similarity of the max-probability per-
mutation to the original order.
The Kendall ? rank correlation coefficient mea-
sures the similarity between two permutations based
on their agreement in pairwise orderings:
? =
n+ ? n?
(n
2
)
where n+ is the number of pairs that share the same
order in both permutations, and n? is the number
that do not. This statistic ranges between -1 and +1,
where -1 indicates inverse order, and +1 indicates
identical order. A value greater than 0 indicates a
positive correlation.
Predicting post order on open-domain Twitter
conversations is a much more difficult task than on
topic-focused news data (Barzilay and Lee, 2004).
We found that a simple bigram model baseline does
very poorly at predicting order on Twitter, achieving
only a weak positive correlation of ? = 0.0358 on
our test data as compared with 0.19-0.74 reported by
Barzilay and Lee on news data.
Note that ? is not a perfect measure of model qual-
ity for conversations; in some cases, multiple order-
178
5 10 15 20 25 30 35 40
EM ConversationConversation+TopicBayesian Conversation
# acts
tau
0.0
0.1
0.2
0.3
0.4
Figure 5: Performance at conversation ordering task.
ings of the same set of posts may form a perfectly
acceptable conversation. On the other hand, there
are often strong constraints on the type of response
we might expect to follow a particular dialogue act;
for example, answers follow questions. We would
expect an effective model to use these constraints to
predict order.
Performance at the conversation ordering task
while varying the number of acts for each model is
displayed in Figure 5. In general, we found that us-
ing Bayesian inference outperforms EM. Also note
that the Bayesian Conversation model outperforms
the Conversation+Topic model at predicting conver-
sation order. This is likely because modeling conver-
sation content as a sequence can in some cases help
to predict post ordering; for example, adjacent posts
are more likely to contain similar content words. Re-
call though that we found the Conversation+Topic
model to be far more interpretable.
Additionally we compare the likelihood of these
models on held out test data in Figure 6. Note that
the Bayesian methods produce models with much
higher likelihood.10 For the EM models, likelihood
tends to decrease on held out test data as we increase
the number of hidden states, due to overfitting.
5 Conclusion
We have presented an approach that allows the
unsupervised induction of dialogue structure from
naturally-occurring open-topic conversational data.
10Likelihood of the test data is estimated using the Chibb
Style estimator described in (Murray and Salakhutdinov, 2008;
Wallach et al, 2009). This method under-estimates likelihood
in expectation. The maximum likelihood (EM) likelihoods are
exact.
5 10 15 20 25 30 35 40
EM ConversationConversation+TopicBayesian Conversation
# acts
negat
ive lo
g like
lihood
3100
003
1500
032
0000
3250
003
3000
033
5000
3400
00
Figure 6: Negative log likelihood on held out test
data (smaller values indicate higher likelihood).
By visualizing the learned models, coherent patterns
emerge from a stew of data that human readers find
difficult to follow. We have extended a conversa-
tion sequence model to separate topic and dialogue
words, resulting in an interpretable set of automat-
ically generated dialogue acts. These discovered
acts have interesting differences from those found
in other domains, and reflect Twitter?s nature as a
micro-blog.
We have introduced the task of conversation or-
dering as an intrinsic measure of conversation model
quality. We found this measure quite useful in
the development of our models and algorithms, al-
though our experiments show that it does not nec-
essarily correlate with interpretability. We have di-
rectly compared Bayesian inference to EM on our
conversation ordering task, showing a clear advan-
tage for Bayesian methods.
Finally, we have collected a corpus of 1.3 million
Twitter conversations, which we will make available
to the research community, and which we hope will
be useful beyond the study of dialogue. In the fu-
ture, we wish to scale our models to the full corpus,
and extend them with more complex notions of dis-
course, topic and community. Ultimately, we hope
to put the learned conversation structure to use in the
construction of a data-driven, conversational agent.
Acknowledgements
We are grateful to everyone in the NLP and TMSN
groups at Microsoft Research for helpful discussions
and feedback. We thank Oren Etzioni, Michael Ga-
mon, Mausam and Fei Wu, and the anonymous re-
viewers for helpful comments on a previous draft.
179
References
James Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Taysom. 2007. Plow: a collaborative task
learning agent. In Proceedings of AAAI.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
HLT-NAACL, pages 113?120.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of EMNLP.
Nigel Crook, Ramon Granell, and Stephen Pulman.
2009. Unsupervised classification of dialogue acts us-
ing a Dirichlet process mixture model. In Proceedings
of SIGDIAL, pages 341?348.
Hal Daume? III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of ACL.
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and Eliz-
abeth Shriberg. 2004. Meeting recorder project: Dia-
log act labeling guide. Technical report, International
Computer Science Institute.
Micha Elsner and Eugene Charniak. 2008. You talking
to me? A corpus and algorithm for conversation dis-
entanglement. In Proceedings of ACL-HLT.
Eric N. Forsyth and Craig H. Martell. 2007. Lexical and
discourse analysis of online chat dialog. In Proceed-
ings of ICSC.
Michael Gamon, Sumit Basu, Dmitriy Belenko, Danyel
Fisher, Matthew Hurst, and Arnd Christian Knig.
2008. Blews: Using blogs to provide context for news
articles. In Proceedings of ICWSM.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of ACL, pages 744?751.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. Technical report.
T. L. Griffiths and M. Steyvers. 2004. Finding scientific
topics. Proc Natl Acad Sci, 101 Suppl 1:5228?5235.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of HLT-NAACL, pages 362?370.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Proceedings of HCII.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Semi-supervised speech act recognition in
emails and forums. In Proceedings of EMNLP, pages
1250?1259.
Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.
Switchboard swbd-damsl shallow-discourse-function
annotation coders manual, draft 13. Technical report,
University of Colorado Institute of Cognitive Science.
Ryan Kelly. 2009. Pear analytics twitter study. Whitepa-
per, August.
Iain Murray and Ruslan Salakhutdinov. 2008. Evalu-
ating probabilities under high-dimensional latent vari-
able models. In Proceedings of NIPS, pages 1137?
1144.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2006. Incorporating speaker and discourse
features into speech summarization. In Proceedings of
HLT-NAACL, pages 367?374.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Rajesh Ranganath, Dan Jurafsky, and Dan Mcfarland.
2009. It?s not you, it?s me: Detecting flirting and
its misperception in speed-dates. In Proceedings of
EMNLP, pages 334?342.
Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David M. Mimno. 2009. Evaluation methods for
topic models. In Proceedings of ICML, page 139.
Yorick Wilks. 2006. Artificial companions as a new kind
of interface to the future internet. In OII Research Re-
port No. 13.
M. Woszczyna and A. Waibel. 1994. Inferring linguistic
structure in spoken language. In Proceedings of IC-
SLP.
Tae Yano, William W. Cohen, and Noah A. Smith. 2009.
Predicting response to political blog posts with topic
models. In Proceedings of NAACL, pages 477?485.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference on
streaming document collections. In Proceedings of
KDD, pages 937?946.
180
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424?434,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Latent Dirichlet Allocation method for Selectional Preferences
Alan Ritter, Mausam and Oren Etzioni
Department of Computer Science and Engineering
Box 352350, University of Washington, Seattle, WA 98195, USA
{aritter,mausam,etzioni}@cs.washington.edu
Abstract
The computation of selectional prefer-
ences, the admissible argument values for
a relation, is a well-known NLP task with
broad applicability. We present LDA-SP,
which utilizes LinkLDA (Erosheva et al,
2004) to model selectional preferences.
By simultaneously inferring latent top-
ics and topic distributions over relations,
LDA-SP combines the benefits of pre-
vious approaches: like traditional class-
based approaches, it produces human-
interpretable classes describing each re-
lation?s preferences, but it is competitive
with non-class-based methods in predic-
tive power.
We compare LDA-SP to several state-of-
the-art methods achieving an 85% increase
in recall at 0.9 precision over mutual in-
formation (Erk, 2007). We also eval-
uate LDA-SP?s effectiveness at filtering
improper applications of inference rules,
where we show substantial improvement
over Pantel et al?s system (Pantel et al,
2007).
1 Introduction
Selectional Preferences encode the set of admissi-
ble argument values for a relation. For example,
locations are likely to appear in the second argu-
ment of the relation X is headquartered in Y and
companies or organizations in the first. A large,
high-quality database of preferences has the po-
tential to improve the performance of a wide range
of NLP tasks including semantic role labeling
(Gildea and Jurafsky, 2002), pronoun resolution
(Bergsma et al, 2008), textual inference (Pantel
et al, 2007), word-sense disambiguation (Resnik,
1997), and many more. Therefore, much atten-
tion has been focused on automatically computing
them based on a corpus of relation instances.
Resnik (1996) presented the earliest work in
this area, describing an information-theoretic ap-
proach that inferred selectional preferences based
on the WordNet hypernym hierarchy. Recent work
(Erk, 2007; Bergsma et al, 2008) has moved away
from generalization to known classes, instead
utilizing distributional similarity between nouns
to generalize beyond observed relation-argument
pairs. This avoids problems like WordNet?s poor
coverage of proper nouns and is shown to improve
performance. These methods, however, no longer
produce the generalized class for an argument.
In this paper we describe a novel approach to
computing selectional preferences by making use
of unsupervised topic models. Our approach is
able to combine benefits of both kinds of meth-
ods: it retains the generalization and human-
interpretability of class-based approaches and is
also competitive with the direct methods on pre-
dictive tasks.
Unsupervised topic models, such as latent
Dirichlet alocation (LDA) (Blei et al, 2003) and
its variants are characterized by a set of hidden
topics, which represent the underlying semantic
structure of a document collection. For our prob-
lem these topics offer an intuitive interpretation ?
they represent the (latent) set of classes that store
the preferences for the different relations. Thus,
topic models are a natural fit for modeling our re-
lation data.
In particular, our system, called LDA-SP, uses
LinkLDA (Erosheva et al, 2004), an extension of
LDA that simultaneously models two sets of dis-
tributions for each topic. These two sets represent
the two arguments for the relations. Thus, LDA-SP
is able to capture information about the pairs of
topics that commonly co-occur. This information
is very helpful in guiding inference.
We run LDA-SP to compute preferences on a
massive dataset of binary relations r(a1, a2) ex-
424
tracted from the Web by TEXTRUNNER (Banko
and Etzioni, 2008). Our experiments demon-
strate that LDA-SP significantly outperforms state
of the art approaches obtaining an 85% increase
in recall at precision 0.9 on the standard pseudo-
disambiguation task.
Additionally, because LDA-SP is based on a for-
mal probabilistic model, it has the advantage that
it can naturally be applied in many scenarios. For
example, we can obtain a better understanding of
similar relations (Table 1), filter out incorrect in-
ferences based on querying our model (Section
4.3), as well as produce a repository of class-based
preferences with a little manual effort as demon-
strated in Section 4.4. In all these cases we obtain
high quality results, for example, massively out-
performing Pantel et al?s approach in the textual
inference task.1
2 Previous Work
Previous work on selectional preferences can
be broken into four categories: class-based ap-
proaches (Resnik, 1996; Li and Abe, 1998; Clark
and Weir, 2002; Pantel et al, 2007), similarity
based approaches (Dagan et al, 1999; Erk, 2007),
discriminative (Bergsma et al, 2008), and genera-
tive probabilistic models (Rooth et al, 1999).
Class-based approaches, first proposed by
Resnik (1996), are the most studied of the four.
They make use of a pre-defined set of classes, ei-
ther manually produced (e.g. WordNet), or auto-
matically generated (Pantel, 2003). For each re-
lation, some measure of the overlap between the
classes and observed arguments is used to iden-
tify those that best describe the arguments. These
techniques produce a human-interpretable output,
but often suffer in quality due to an incoherent tax-
onomy, inability to map arguments to a class (poor
lexical coverage), and word sense ambiguity.
Because of these limitations researchers have
investigated non-class based approaches, which
attempt to directly classify a given noun-phrase
as plausible/implausible for a relation. Of these,
the similarity based approaches make use of a dis-
tributional similarity measure between arguments
and evaluate a heuristic scoring function:
Srel(arg)=
?
arg??Seen(rel)
sim(arg, arg?) ? wtrel(arg)
1Our repository of selectional preferences is available
at http://www.cs.washington.edu/research/
ldasp.
Erk (2007) showed the advantages of this ap-
proach over Resnik?s information-theoretic class-
based method on a pseudo-disambiguation evalu-
ation. These methods obtain better lexical cover-
age, but are unable to obtain any abstract represen-
tation of selectional preferences.
Our solution fits into the general category
of generative probabilistic models, which model
each relation/argument combination as being gen-
erated by a latent class variable. These classes
are automatically learned from the data. This re-
tains the class-based flavor of the problem, with-
out the knowledge limitations of the explicit class-
based approaches. Probably the closest to our
work is a model proposed by Rooth et al (1999),
in which each class corresponds to a multinomial
over relations and arguments and EM is used to
learn the parameters of the model. In contrast,
we use a LinkLDA framework in which each re-
lation is associated with a corresponding multi-
nomial distribution over classes, and each argu-
ment is drawn from a class-specific distribution
over words; LinkLDA captures co-occurrence of
classes in the two arguments. Additionally we
perform full Bayesian inference using collapsed
Gibbs sampling, in which parameters are inte-
grated out (Griffiths and Steyvers, 2004).
Recently, Bergsma et. al. (2008) proposed the
first discriminative approach to selectional prefer-
ences. Their insight that pseudo-negative exam-
ples could be used as training data allows the ap-
plication of an SVM classifier, which makes use of
many features in addition to the relation-argument
co-occurrence frequencies used by other meth-
ods. They automatically generated positive and
negative examples by selecting arguments having
high and low mutual information with the rela-
tion. Since it is a discriminative approach it is
amenable to feature engineering, but needs to be
retrained and tuned for each task. On the other
hand, generative models produce complete prob-
ability distributions of the data, and hence can be
integrated with other systems and tasks in a more
principled manner (see Sections 4.2.2 and 4.3.1).
Additionally, unlike LDA-SP Bergsma et al?s sys-
tem doesn?t produce human-interpretable topics.
Finally, we note that LDA-SP and Bergsma?s sys-
tem are potentially complimentary ? the output of
LDA-SP could be used to generate higher-quality
training data for Bergsma, potentially improving
their results.
425
Topic models such as LDA (Blei et al, 2003)
and its variants have recently begun to see use
in many NLP applications such as summarization
(Daume? III and Marcu, 2006), document align-
ment and segmentation (Chen et al, 2009), and
inferring class-attribute hierarchies (Reisinger and
Pasca, 2009). Our particular model, LinkLDA, has
been applied to a few NLP tasks such as simul-
taneously modeling the words appearing in blog
posts and users who will likely respond to them
(Yano et al, 2009), modeling topic-aligned arti-
cles in different languages (Mimno et al, 2009),
and word sense induction (Brody and Lapata,
2009).
Finally, we highlight two systems, developed
independently of our own, which apply LDA-style
models to similar tasks. O? Se?aghdha (2010) pro-
poses a series of LDA-style models for the task
of computing selectional preferences. This work
learns selectional preferences between the fol-
lowing grammatical relations: verb-object, noun-
noun, and adjective-noun. It also focuses on
jointly modeling the generation of both predicate
and argument, and evaluation is performed on a
set of human-plausibility judgments obtaining im-
pressive results against Keller and Lapata?s (2003)
Web hit-count based system. Van Durme and
Gildea (2009) proposed applying LDA to general
knowledge templates extracted using the KNEXT
system (Schubert and Tong, 2003). In contrast,
our work uses LinkLDA and focuses on modeling
multiple arguments of a relation (e.g., the subject
and direct object of a verb).
3 Topic Models for Selectional Prefs.
We present a series of topic models for the task of
computing selectional preferences. These models
vary in the amount of independence they assume
between a1 and a2. At one extreme is Indepen-
dentLDA, a model which assumes that both a1 and
a2 are generated completely independently. On
the other hand, JointLDA, the model at the other
extreme (Figure 1) assumes both arguments of a
specific extraction are generated based on a single
hidden variable z. LinkLDA (Figure 2) lies be-
tween these two extremes, and as demonstrated in
Section 4, it is the best model for our relation data.
We are given a set R of binary relations and a
corpus D = {r(a1, a2)} of extracted instances for
these relations. 2 Our task is to compute, for each
argument ai of each relation r, a set of usual ar-
gument values (noun phrases) that it takes. For
example, for the relation is headquartered in the
first argument set will include companies like Mi-
crosoft, Intel, General Motors and second argu-
ment will favor locations like New York, Califor-
nia, Seattle.
3.1 IndependentLDA
We first describe the straightforward application
of LDA to modeling our corpus of extracted rela-
tions. In this case two separate LDA models are
used to model a1 and a2 independently.
In the generative model for our data, each rela-
tion r has a corresponding multinomial over topics
?r, drawn from a Dirichlet. For each extraction, a
hidden topic z is first picked according to ?r, and
then the observed argument a is chosen according
to the multinomial ?z .
Readers familiar with topic modeling terminol-
ogy can understand our approach as follows: we
treat each relation as a document whose contents
consist of a bags of words corresponding to all the
noun phrases observed as arguments of the rela-
tion in our corpus. Formally, LDA generates each
argument in the corpus of relations as follows:
for each topic t = 1 . . . T do
Generate ?t according to symmetric Dirich-
let distribution Dir(?).
end for
for each relation r = 1 . . . |R| do
Generate ?r according to Dirichlet distribu-
tion Dir(?).
for each tuple i = 1 . . . Nr do
Generate zr,i from Multinomial(?r).
Generate the argument ar,i from multi-
nomial ?zr,i .
end for
end for
One weakness of IndependentLDA is that it
doesn?t jointly model a1 and a2 together. Clearly
this is undesirable, as information about which
topics one of the arguments favors can help inform
the topics chosen for the other. For example, class
pairs such as (team, game), (politician, political is-
sue) form much more plausible selectional prefer-
ences than, say, (team, political issue), (politician,
game).
2We focus on binary relations, though the techniques pre-
sented in the paper are easily extensible to n-ary relations.
426
3.2 JointLDA
As a more tightly coupled alternative, we first
propose JointLDA, whose graphical model is de-
picted in Figure 1. The key difference in JointLDA
(versus LDA) is that instead of one, it maintains
two sets of topics (latent distributions over words)
denoted by ? and ?, one for classes of each ar-
gument. A topic id k represents a pair of topics,
?k and ?k, that co-occur in the arguments of ex-
tracted relations. Common examples include (Per-
son, Location), (Politician, Political issue), etc.
The hidden variable z = k indicates that the noun
phrase for the first argument was drawn from the
multinomial ?k, and that the second argument was
drawn from ?k. The per-relation distribution ?r is
a multinomial over the topic ids and represents the
selectional preferences, both for arg1s and arg2s
of a relation r.
Although JointLDA has many desirable proper-
ties, it has some drawbacks as well. Most notably,
in JointLDA topics correspond to pairs of multi-
nomials (?k, ?k); this leads to a situation in which
multiple redundant distributions are needed to rep-
resent the same underlying semantic class. For
example consider the case where we we need to
represent the following selectional preferences for
our corpus of relations: (person, location), (per-
son, organization), and (person, crime). Because
JointLDA requires a separate pair of multinomials
for each topic, it is forced to use 3 separate multi-
nomials to represent the class person, rather than
learning a single distribution representing person
and choosing 3 different topics for a2. This results
in poor generalization because the data for a single
class is divided into multiple topics.
In order to address this problem while maintain-
ing the sharing of influence between a1 and a2, we
next present LinkLDA, which represents a com-
promise between IndependentLDA and JointLDA.
LinkLDA is more flexible than JointLDA, allow-
ing different topics to be chosen for a1, and a2,
however still models the generation of topics from
the same distribution for a given relation.
3.3 LinkLDA
Figure 2 illustrates the LinkLDA model in the
plate notation, which is analogous to the model
in (Erosheva et al, 2004). In particular note that
each ai is drawn from a different hidden topic zi,
however the zi?s are drawn from the same distri-
bution ?r for a given relation r. To facilitate learn-
?
a
1
a
2
?
|R|
N
?
?
1
?
T
?
2
z
Figure 1: JointLDA
?
z
1
z
2
a
1
a
2
?
|R|
N
?
?
1
?
T
?
2
Figure 2: LinkLDA
ing related topic pairs between arguments we em-
ploy a sparse prior over the per-relation topic dis-
tributions. Because a few topics are likely to be
assigned most of the probability mass for a given
relation it is more likely (although not necessary)
that the same topic number k will be drawn for
both arguments.
When comparing LinkLDA with JointLDA the
better model may not seem immediately clear. On
the one hand, JointLDA jointly models the gen-
eration of both arguments in an extracted tuple.
This allows one argument to help disambiguate
the other in the case of ambiguous relation strings.
LinkLDA, however, is more flexible; rather than
requiring both arguments to be generated from one
of |Z| possible pairs of multinomials (?z, ?z), Lin-
kLDA allows the arguments of a given extraction
to be generated from |Z|2 possible pairs. Thus,
instead of imposing a hard constraint that z1 =
z2 (as in JointLDA), LinkLDA simply assigns a
higher probability to states in which z1 = z2, be-
cause both hidden variables are drawn from the
same (sparse) distribution ?r. LinkLDA can thus
re-use argument classes, choosing different com-
binations of topics for the arguments if it fits the
data better. In Section 4 we show experimentally
that LinkLDA outperforms JointLDA (and Inde-
pendentLDA) by wide margins. We use LDA-SP
to refer to LinkLDA in all the experiments below.
3.4 Inference
For all the models we use collapsed Gibbs sam-
pling for inference in which each of the hid-
den variables (e.g., zr,i,1 and zr,i,2 in LinkLDA)
are sampled sequentially conditioned on a full-
assignment to all others, integrating out the param-
eters (Griffiths and Steyvers, 2004). This produces
robust parameter estimates, as it allows computa-
tion of expectations over the posterior distribution
427
as opposed to estimating maximum likelihood pa-
rameters. In addition, the integration allows the
use of sparse priors, which are typically more ap-
propriate for natural language data. In all exper-
iments we use hyperparameters ? = ?1 = ?2 =
0.1. We generated initial code for our samplers us-
ing the Hierarchical Bayes Compiler (Daume III,
2007).
3.5 Advantages of Topic Models
There are several advantages to using topic mod-
els for our task. First, they naturally model the
class-based nature of selectional preferences, but
don?t take a pre-defined set of classes as input.
Instead, they compute the classes automatically.
This leads to better lexical coverage since the is-
sue of matching a new argument to a known class
is side-stepped. Second, the models naturally han-
dle ambiguous arguments, as they are able to as-
sign different topics to the same phrase in different
contexts. Inference in these models is also scalable
? linear in both the size of the corpus as well as
the number of topics. In addition, there are several
scalability enhancements such as SparseLDA (Yao
et al, 2009), and an approximation of the Gibbs
Sampling procedure can be efficiently parallelized
(Newman et al, 2009). Finally we note that, once
a topic distribution has been learned over a set of
training relations, one can efficiently apply infer-
ence to unseen relations (Yao et al, 2009).
4 Experiments
We perform three main experiments to assess the
quality of the preferences obtained using topic
models. The first is a task-independent evaluation
using a pseudo-disambiguation experiment (Sec-
tion 4.2), which is a standard way to evaluate the
quality of selectional preferences (Rooth et al,
1999; Erk, 2007; Bergsma et al, 2008). We use
this experiment to compare the various topic mod-
els as well as the best model with the known state
of the art approaches to selectional preferences.
Secondly, we show significant improvements to
performance at an end-task of textual inference in
Section 4.3. Finally, we report on the quality of
a large database of Wordnet-based preferences ob-
tained after manually associating our topics with
Wordnet classes (Section 4.4).
4.1 Generalization Corpus
For all experiments we make use of a corpus
of r(a1, a2) tuples, which was automatically ex-
tracted by TEXTRUNNER (Banko and Etzioni,
2008) from 500 million Web pages.
To create a generalization corpus from this
large dataset. We first selected 3,000 relations
from the middle of the tail (we used the 2,000-
5,000 most frequent ones)3 and collected all in-
stances. To reduce sparsity, we discarded all tu-
ples containing an NP that occurred fewer than 50
times in the data. This resulted in a vocabulary of
about 32,000 noun phrases, and a set of about 2.4
million tuples in our generalization corpus.
We inferred topic-argument and relation-topic
multinomials (?, ?, and ?) on the generalization
corpus by taking 5 samples at a lag of 50 after
a burn in of 750 iterations. Using multiple sam-
ples introduces the risk of topic drift due to lack
of identifiability, however we found this to not be
a problem in practice. During development we
found that the topics tend to remain stable across
multiple samples after sufficient burn in, and mul-
tiple samples improved performance. Table 1 lists
sample topics and high ranked words for each (for
both arguments) as well as relations favoring those
topics.
4.2 Task Independent Evaluation
We first compare the three LDA-based approaches
to each other and two state of the art similarity
based systems (Erk, 2007) (using mutual informa-
tion and Jaccard similarity respectively). These
similarity measures were shown to outperform the
generative model of Rooth et al (1999), as well
as class-based methods such as Resnik?s. In this
pseudo-disambiguation experiment an observed
tuple is paired with a pseudo-negative, which
has both arguments randomly generated from the
whole vocabulary (according to the corpus-wide
distribution over arguments). The task is, for each
relation-argument pair, to determine whether it is
observed, or a random distractor.
4.2.1 Test Set
For this experiment we gathered a primary corpus
by first randomly selecting 100 high-frequency re-
lations not in the generalization corpus. For each
relation we collected all tuples containing argu-
ments in the vocabulary. We held out 500 ran-
domly selected tuples as the test set. For each tu-
3Many of the most frequent relations have very weak se-
lectional preferences, and thus provide little signal for infer-
ring meaningful topics. For example, the relations has and is
can take just about any arguments.
428
Topic t Arg1 Relations which assign
highest probability to t
Arg2
18 The residue - The mixture - The reaction
mixture - The solution - the mixture - the re-
action mixture - the residue - The reaction -
the solution - The filtrate - the reaction - The
product - The crude product - The pellet -
The organic layer - Thereto - This solution
- The resulting solution - Next - The organic
phase - The resulting mixture - C. )
was treated with, is
treated with, was
poured into, was
extracted with, was
purified by, was di-
luted with, was filtered
through, is disolved in,
is washed with
EtOAc - CH2Cl2 - H2O - CH.sub.2Cl.sub.2
- H.sub.2O - water - MeOH - NaHCO3 -
Et2O - NHCl - CHCl.sub.3 - NHCl - drop-
wise - CH2Cl.sub.2 - Celite - Et.sub.2O -
Cl.sub.2 - NaOH - AcOEt - CH2C12 - the
mixture - saturated NaHCO3 - SiO2 - H2O
- N hydrochloric acid - NHCl - preparative
HPLC - to0 C
151 the Court - The Court - the Supreme Court
- The Supreme Court - this Court - Court
- The US Supreme Court - the court - This
Court - the US Supreme Court - The court
- Supreme Court - Judge - the Court of Ap-
peals - A federal judge
will hear, ruled in, de-
cides, upholds, struck
down, overturned,
sided with, affirms
the case - the appeal - arguments - a case -
evidence - this case - the decision - the law
- testimony - the State - an interview - an
appeal - cases - the Court - that decision -
Congress - a decision - the complaint - oral
arguments - a law - the statute
211 President Bush - Bush - The President -
Clinton - the President - President Clinton
- President George W. Bush - Mr. Bush -
The Governor - the Governor - Romney -
McCain - The White House - President -
Schwarzenegger - Obama
hailed, vetoed, pro-
moted, will deliver,
favors, denounced,
defended
the bill - a bill - the decision - the war - the
idea - the plan - the move - the legislation -
legislation - the measure - the proposal - the
deal - this bill - a measure - the program -
the law - the resolution - efforts - the agree-
ment - gay marriage - the report - abortion
224 Google - Software - the CPU - Clicking -
Excel - the user - Firefox - System - The
CPU - Internet Explorer - the ability - Pro-
gram - users - Option - SQL Server - Code
- the OS - the BIOS
will display, to store, to
load, processes, cannot
find, invokes, to search
for, to delete
data - files - the data - the file - the URL -
information - the files - images - a URL - the
information - the IP address - the user - text
- the code - a file - the page - IP addresses -
PDF files - messages - pages - an IP address
Table 1: Example argument lists from the inferred topics. For each topic number t we list the most
probable values according to the multinomial distributions for each argument (?t and ?t). The middle
column reports a few relations whose inferred topic distributions ?r assign highest probability to t.
ple r(a1, a2) in the held-out set, we removed all
tuples in the training set containing either of the
rel-arg pairs, i.e., any tuple matching r(a1, ?) or
r(?, a2). Next we used collapsed Gibbs sampling
to infer a distribution over topics, ?r, for each of
the relations in the primary corpus (based solely
on tuples in the training set) using the topics from
the generalization corpus.
For each of the 500 observed tuples in the test-
set we generated a pseudo-negative tuple by ran-
domly sampling two noun phrases from the distri-
bution of NPs in both corpora.
4.2.2 Prediction
Our prediction system needs to determine whether
a specific relation-argument pair is admissible ac-
cording to the selectional preferences or is a ran-
dom distractor (D). Following previous work, we
perform this experiment independently for the two
relation-argument pairs (r, a1) and (r, a2).
We first compute the probability of observing
a1 for first argument of relation r given that it is
not a distractor, P (a1|r,?D), which we approx-
imate by its probability given an estimate of the
parameters inferred by our model, marginalizing
over hidden topics t. The analysis for the second
argument is similar.
P (a1|r,?D) ? PLDA(a1|r) =
TX
t=0
P (a1|t)P (t|r)
=
TX
t=0
?t(a1)?r(t)
A simple application of Bayes Rule gives the
probability that a particular argument is not a
distractor. Here the distractor-related proba-
bilities are independent of r, i.e., P (D|r) =
P (D), P (a1|D, r) = P (a1|D), etc. We estimate
P (a1|D) according to their frequency in the gen-
eralization corpus.
P (?D|r, a1) =
P (?D|r)P (a1|r,?D)
P (a1|r)
?
P (?D)PLDA(a1|r)
P (D)P (a1|D) + P (?D)PLDA(a1|r)
4.2.3 Results
Figure 3 plots the precision-recall curve for the
pseudo-disambiguation experiment comparing the
three different topic models. LDA-SP, which uses
LinkLDA, substantially outperforms both Inde-
pendentLDA and JointLDA.
Next, in figure 4, we compare LDA-SP with
mutual information and Jaccard similarities us-
ing both the generalization and primary corpus for
429
0.0 0.2 0.4 0.6 0.8 1.00
.4
0.6
0.8
1.0
recall
prec
ision
LDA?SPIndependentLDAJointLDA
Figure 3: Comparison of LDA-based approaches
on the pseudo-disambiguation task. LDA-SP (Lin-
kLDA) substantially outperforms the other mod-
els.
0.0 0.2 0.4 0.6 0.8 1.00
.4
0.6
0.8
1.0
recall
prec
ision
LDA?SPJaccardMutual Information
Figure 4: Comparison to similarity-based selec-
tional preference systems. LDA-SP obtains 85%
higher recall at precision 0.9.
computation of similarities. We find LDA-SP sig-
nificantly outperforms these methods. Its edge is
most noticed at high precisions; it obtains 85%
more recall at 0.9 precision compared to mutual
information. Overall LDA-SP obtains an 15% in-
crease in the area under precision-recall curve over
mutual information. All three systems? AUCs are
shown in Table 2; LDA-SP?s improvements over
both Jaccard and mutual information are highly
significant with a significance level less than 0.01
using a paired t-test.
In addition to a superior performance in se-
lectional preference evaluation LDA-SP also pro-
duces a set of coherent topics, which can be use-
ful in their own right. For instance, one could use
them for tasks such as set-expansion (Carlson et
al., 2010) or automatic thesaurus induction (Et-
LDA-SP MI-Sim Jaccard-Sim
AUC 0.833 0.727 0.711
Table 2: Area under the precision recall curve.
LDA-SP?s AUC is significantly higher than both
similarity-based methods according to a paired t-
test with a significance level below 0.01.
zioni et al, 2005; Kozareva et al, 2008).
4.3 End Task Evaluation
We now evaluate LDA-SP?s ability to improve per-
formance at an end-task. We choose the task of
improving textual entailment by learning selec-
tional preferences for inference rules and filtering
inferences that do not respect these. This applica-
tion of selectional preferences was introduced by
Pantel et. al. (2007). For now we stick to infer-
ence rules of the form r1(a1, a2) ? r2(a1, a2),
though our ideas are more generally applicable to
more complex rules. As an example, the rule (X
defeats Y) ? (X plays Y) holds when X and Y
are both sports teams, however fails to produce a
reasonable inference if X and Y are Britain and
Nazi Germany respectively.
4.3.1 Filtering Inferences
In order for an inference to be plausible, both re-
lations must have similar selectional preferences,
and further, the arguments must obey the selec-
tional preferences of both the antecedent r1 and
the consequent r2.4 Pantel et al (2007) made
use of these intuitions by producing a set of class-
based selectional preferences for each relation,
then filtering out any inferences where the argu-
ments were incompatible with the intersection of
these preferences. In contrast, we take a proba-
bilistic approach, evaluating the quality of a spe-
cific inference by measuring the probability that
the arguments in both the antecedent and the con-
sequent were drawn from the same hidden topic
in our model. Note that this probability captures
both the requirement that the antecedent and con-
sequent have similar selectional preferences, and
that the arguments from a particular instance of the
rule?s application match their overlap.
We use zri,j to denote the topic that generates
the jth argument of relation ri. The probability
that the two arguments a1, a2 were drawn from
the same hidden topic factorizes as follows due to
the conditional independences in our model:5
P (zr1,1 = zr2,1, zr1,2 = zr2,2|a1, a2) =
P (zr1,1 = zr2,1|a1)P (zr1,2 = zr2,2|a2)
4Similarity-based and discriminative methods are not ap-
plicable to this task as they offer no straightforward way
to compare the similarity between selectional preferences of
two relations.
5Note that all probabilities are conditioned on an estimate
of the parameters ?, ?, ? from our model, which are omitted
for compactness.
430
To compute each of these factors we simply
marginalize over the hidden topics:
P (zr1,j = zr2,j |aj) =
TX
t=1
P (zr1,j = t|aj)P (zr2,j = t|aj)
where P (z = t|a) can be computed using
Bayes rule. For example,
P (zr1,1 = t|a1) =
P (a1|zr1,1 = t)P (zr1,1 = t)
P (a1)
=
?t(a1)?r1(t)
P (a1)
4.3.2 Experimental Conditions
In order to evaluate LDA-SP?s ability to filter in-
ferences based on selectional preferences we need
a set of inference rules between the relations in
our corpus. We therefore mapped the DIRT In-
ference rules (Lin and Pantel, 2001), (which con-
sist of pairs of dependency paths) to TEXTRUN-
NER relations as follows. We first gathered all in-
stances in the generalization corpus, and for each
r(a1, a2) created a corresponding simple sentence
by concatenating the arguments with the relation
string between them. Each such simple sentence
was parsed using Minipar (Lin, 1998). From
the parses we extracted all dependency paths be-
tween nouns that contain only words present in
the TEXTRUNNER relation string. These depen-
dency paths were then matched against each pair
in the DIRT database, and all pairs of associated
relations were collected producing about 26,000
inference rules.
Following Pantel et al (2007) we randomly
sampled 100 inference rules. We then automati-
cally filtered out any rules which contained a nega-
tion, or for which the antecedent and consequent
contained a pair of antonyms found in WordNet
(this left us with 85 rules). For each rule we col-
lected 10 random instances of the antecedent, and
generated the consequent. We randomly sampled
300 of these inferences to hand-label.
4.3.3 Results
In figure 5 we compare the precision and recall of
LDA-SP against the top two performing systems
described by Pantel et al (ISP.IIM-? and ISP.JIM,
both using the CBC clusters (Pantel, 2003)). We
find that LDA-SP achieves both higher precision
and recall than ISP.IIM-?. It is also able to achieve
the high-precision point of ISP.JIM and can trade
precision to get a much larger recall.
0.0 0.2 0.4 0.6 0.8 1.00
.4
0.6
0.8
1.0
recall
prec
isio
n
X
O
 
XO
LDA?SPISP.JIMISP.IIM?OR
Figure 5: Precision and recall on the inference fil-
tering task.
Top 10 Inference Rules Ranked by LDA-SP
antecedent consequent KL-div
will begin at will start at 0.014999
shall review shall determine 0.129434
may increase may reduce 0.214841
walk from walk to 0.219471
consume absorb 0.240730
shall keep shall maintain 0.264299
shall pay to will notify 0.290555
may apply for may obtain 0.313916
copy download 0.316502
should pay must pay 0.371544
Bottom 10 Inference Rules Ranked by LDA-SP
antecedent consequent KL-div
lose to shall take 10.011848
should play could do 10.028904
could play get in 10.048857
will start at move to 10.060994
shall keep will spend 10.105493
should play get in 10.131299
shall pay to leave for 10.131364
shall keep return to 10.149797
shall keep could do 10.178032
shall maintain have spent 10.221618
Table 3: Top 10 and Bottom 10 ranked inference
rules ranked by LDA-SPafter automatically filter-
ing out negations and antonyms (using WordNet).
In addition we demonstrate LDA-SP?s abil-
ity to rank inference rules by measuring the
Kullback Leibler Divergence6 between the topic-
distributions of the antecedent and consequent, ?r1
and ?r2 respectively. Table 3 shows the top 10 and
bottom 10 rules out of the 26,000 ranked by KL
Divergence after automatically filtering antonyms
(using WordNet) and negations. For slight varia-
tions in rules (e.g., symmetric pairs) we mention
only one example to show more variety.
6KL-Divergence is an information-theoretic measure of
the similarity between two probability distributions, and de-
fined as follows: KL(P ||Q) =
P
x P (x) log
P (x)
Q(x) .
431
4.4 A Repository of Class-Based Preferences
Finally we explore LDA-SP?s ability to produce a
repository of human interpretable class-based se-
lectional preferences. As an example, for the re-
lation was born in, we would like to infer that
the plausible arguments include (person, location)
and (person, date).
Since we already have a set of topics, our
task reduces to mapping the inferred topics to an
equivalent class in a taxonomy (e.g., WordNet).
We experimented with automatic methods such
as Resnik?s, but found them to have all the same
problems as directly applying these approaches to
the SP task.7 Guided by the fact that we have a
relatively small number of topics (600 total, 300
for each argument) we simply chose to label them
manually. By labeling this small number of topics
we can infer class-based preferences for an arbi-
trary number of relations.
In particular, we applied a semi-automatic
scheme to map topics to WordNet. We first applied
Resnik?s approach to automatically shortlist a few
candidate WordNet classes for each topic. We then
manually picked the best class from the shortlist
that best represented the 20 top arguments for a
topic (similar to Table 1). We marked all incoher-
ent topics with a special symbol ?. This process
took one of the authors about 4 hours to complete.
To evaluate how well our topic-class associa-
tions carry over to unseen relations we used the
same random sample of 100 relations from the
pseudo-disambiguation experiment.8 For each ar-
gument of each relation we picked the top two top-
ics according to frequency in the 5 Gibbs samples.
We then discarded any topics which were labeled
with ?; this resulted in a set of 236 predictions. A
few examples are displayed in table 4.
We evaluated these classes and found the accu-
racy to be around 0.88. We contrast this with Pan-
tel?s repository,9 the only other released database
of selectional preferences to our knowledge. We
evaluated the same 100 relations from his website
and tagged the top 2 classes for each argument and
evaluated the accuracy to be roughly 0.55.
7Perhaps recent work on automatic coherence ranking
(Newman et al, 2010) and labeling (Mei et al, 2007) could
produce better results.
8Recall that these 100 were not part of the original 3,000
in the generalization corpus, and are, therefore, representative
of new ?unseen? relations.
9http://demo.patrickpantel.com/
Content/LexSem/paraphrase.htm
arg1 class relation arg2 class
politician#1 was running for leader#1
people#1 will love show#3
organization#1 has responded to accusation#2
administrative unit#1 has appointed administrator#3
Table 4: Class-based Selectional Preferences.
We emphasize that tagging a pair of class-based
preferences is a highly subjective task, so these re-
sults should be treated as preliminary. Still, these
early results are promising. We wish to undertake
a larger scale study soon.
5 Conclusions and Future Work
We have presented an application of topic mod-
eling to the problem of automatically computing
selectional preferences. Our method, LDA-SP,
learns a distribution over topics for each rela-
tion while simultaneously grouping related words
into these topics. This approach is capable of
producing human interpretable classes, however,
avoids the drawbacks of traditional class-based ap-
proaches (poor lexical coverage and ambiguity).
LDA-SP achieves state-of-the-art performance on
predictive tasks such as pseudo-disambiguation,
and filtering incorrect inferences.
Because LDA-SP generates a complete proba-
bilistic model for our relation data, its results are
easily applicable to many other tasks such as iden-
tifying similar relations, ranking inference rules,
etc. In the future, we wish to apply our model
to automatically discover new inference rules and
paraphrases.
Finally, our repository of selectional pref-
erences for 10,000 relations is available at
http://www.cs.washington.edu/
research/ldasp.
Acknowledgments
We would like to thank Tim Baldwin, Colin
Cherry, Jesse Davis, Elena Erosheva, Stephen
Soderland, Dan Weld, in addition to the anony-
mous reviewers for helpful comments on a previ-
ous draft. This research was supported in part by
NSF grant IIS-0803481, ONR grant N00014-08-
1-0431, DARPA contract FA8750-09-C-0179, a
National Defense Science and Engineering Grad-
uate (NDSEG) Fellowship 32 CFR 168a, and car-
ried out at the University of Washington?s Turing
Center.
432
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
ACL-08: HLT.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional pref-
erence from unlabeled text. In EMNLP.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL, pages 103?111,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Andrew Carlson, Justin Betteridge, Richard C. Wang,
Estevam R. Hruschka Jr., and Tom M. Mitchell.
2010. Coupled semi-supervised learning for infor-
mation extraction. In WSDM 2010.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Global models of document
structure using latent permutations. In NAACL.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Comput. Linguist.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. In Machine Learning.
Hal Daume? III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics.
Hal Daume III. 2007. hbc: Hierarchical bayes com-
piler. http://hal3.name/hbc.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. Proceedings of the National Academy of
Sciences of the United States of America.
Oren Etzioni, Michael Cafarella, Doug Downey,
Ana maria Popescu, Tal Shaked, Stephen Soderl,
Daniel S. Weld, and Alex Yates. 2005. Unsuper-
vised named-entity extraction from the web: An ex-
perimental study. Artificial Intelligence.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Comput. Linguist.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proc Natl Acad Sci U S A.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Comput.
Linguist.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL-08: HLT.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the mdl principle.
Comput. Linguist.
Dekang Lin and Patrick Pantel. 2001. Dirt-discovery
of inference rules from text. In KDD.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proc. Workshop on the Evaluation of
Parsing Systems.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In KDD.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. JMLR.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In NAACL-HLT.
Diarmuid O? Se?aghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard H. Hovy. 2007.
Isp: Learning inferential selectional preferences. In
HLT-NAACL.
Patrick Andre Pantel. 2003. Clustering by commit-
tee. Ph.D. thesis, University of Alberta, Edmonton,
Alta., Canada.
Joseph Reisinger and Marius Pasca. 2009. Latent vari-
able models of concept-attribute attachment. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
P. Resnik. 1996. Selectional constraints: an
information-theoretic model and its computational
realization. Cognition.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proc. of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why,
What, and How?
433
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
Proceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics on Computa-
tional Linguistics.
Lenhart Schubert and Matthew Tong. 2003. Extract-
ing and evaluating general world knowledge from
the brown corpus. In In Proc. of the HLT-NAACL
Workshop on Text Meaning, pages 7?13.
Benjamin Van Durme and Daniel Gildea. 2009. Topic
models for corpus-centric knowledge generalization.
In Technical Report TR-946, Department of Com-
puter Science, University of Rochester, Rochester.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In NAACL.
L. Yao, D. Mimno, and A. Mccallum. 2009. Effi-
cient methods for topic model inference on stream-
ing document collections. In KDD.
434
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 165?174,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Weakly Supervised User Profile Extraction from Twitter
Jiwei Li
1
, Alan Ritter
2
, Eduard Hovy
1
1
Language Technology Institute,
2
Machine Learning Department
Carnegie Mellon University, Pittsburgh, PA 15213, USA
bdlijiwei@gmail.com, rittera@cs.cmu.edu, ehovy@andrew.cmu.edu
Abstract
While user attribute extraction on social
media has received considerable attention,
existing approaches, mostly supervised,
encounter great difficulty in obtaining gold
standard data and are therefore limited
to predicting unary predicates (e.g., gen-
der). In this paper, we present a weakly-
supervised approach to user profile extrac-
tion from Twitter. Users? profiles from so-
cial media websites such as Facebook or
Google Plus are used as a distant source
of supervision for extraction of their at-
tributes from user-generated text. In addi-
tion to traditional linguistic features used
in distant supervision for information ex-
traction, our approach also takes into ac-
count network information, a unique op-
portunity offered by social media. We test
our algorithm on three attribute domains:
spouse, education and job; experimental
results demonstrate our approach is able
to make accurate predictions for users? at-
tributes based on their tweets.
1
1 Introduction
The overwhelming popularity of online social me-
dia creates an opportunity to display given as-
pects of oneself. Users? profile information in
social networking websites such as Facebook
2
or
Google Plus
3
provides a rich repository personal
information in a structured data format, making it
amenable to automatic processing. This includes,
for example, users? jobs and education, and pro-
vides a useful source of information for applica-
tions such as search
4
, friend recommendation, on-
1
Both code and data are available at http://aclweb.
org/aclwiki/index.php?title=Profile_data
2
https://www.facebook.com/
3
https://plus.google.com/
4
https://www.facebook.com/about/
graphsearch
@[shanenicholson] has taken all the kids today so
I can go shopping-CHILD FREE! #iloveyoushano
#iloveyoucreditcard
Tamworth promo day with my handsome classy husband
@[shanenicholson]
Spouse: shanenicholson
I got accepted to be part of the UofM engineering safety
pilot program in [FSU]
Here in class. (@ [Florida State University] - Williams
Building)
Don?t worry , guys ! Our beloved [FSU] will always con-
tinue to rise ? to the top !
Education: Florida State University (FSU)
first day of work at [HuffPo], a sports bar woo come visit
me yo..
start to think we should just add a couple desks to the
[HuffPo] newsroom for Business Insider writers
just back from [HuffPo], what a hell !
Job: HuffPo
Table 1: Examples of Twitter message clues for
user profile inference.
line advertising, computational social science and
more.
Although profiles exist in an easy-to-use, struc-
tured data format, they are often sparsely popu-
lated; users rarely fully complete their online pro-
files. Additionally, some social networking ser-
vices such as Twitter don?t support this type of
structured profile data. It is therefore difficult to
obtain a reasonably comprehensive profile of a
user, or a reasonably complete facet of information
(say, education level) for a class of users. While
many users do not explicitly list all their personal
information in their online profile, their user gen-
erated content often contains strong evidence to
suggest many types of user attributes, for example
education, spouse, and employment (See Table 1).
Can one use such information to infer more de-
tails? In particular, can one exploit indirect clues
from an unstructured data source like Twitter to
obtain rich, structured user profiles?
In this paper we demonstrate that it is feasi-
ble to automatically extract Facebook-style pro-
165
files directly from users? tweets, thus making
user profile data available in a structured format
for upstream applications. We view user profile
inference as a structured prediction task where
both text and network information are incorpo-
rated. Concretely, we cast user profile predic-
tion as binary relation extraction (Brin, 1999),
e.g., SPOUSE(User
i
, User
j
), EDUCATION(User
i
,
Entity
j
) and EMPLOYER(User
i
, Entity
j
). Inspired
by the concept of distant supervision, we collect
training tweets by matching attribute ground truth
from an outside ?knowledge base? such as Face-
book or Google Plus.
One contribution of the work presented here is
the creation of the first large-scale dataset on three
general Twitter user profile domains (i.e., EDUCA-
TION, JOB, SPOUSE). Experiments demonstrate
that by simultaneously harnessing both text fea-
tures and network information, our approach is
able to make accurate user profile predictions. We
are optimistic that our approach can easily be ap-
plied to further user attributes such as HOBBIES
and INTERESTS (MOVIES, BOOKS, SPORTS or
STARS), RELIGION, HOMETOWN, LIVING LOCA-
TION, FAMILY MEMBERS and so on, where train-
ing data can be obtained by matching ground truth
retrieved from multiple types of online social me-
dia such as Facebook, Google Plus, or LinkedIn.
Our contributions are as follows:
? We cast user profile prediction as an informa-
tion extraction task.
? We present a large-scale dataset for this task
gathered from various structured and unstruc-
tured social media sources.
? We demonstrate the benefit of jointly rea-
soning about users? social network structure
when extracting their profiles from text.
? We experimentally demonstrate the effective-
ness of our approach on 3 relations: SPOUSE,
JOB and EDUCATION.
The remainder of this paper is organized as fol-
lows: We summarize related work in Section 2.
The creation of our dataset is described in Section
3. The details of our model are presented in Sec-
tion 4. We present experimental results in Section
5 and conclude in Section 6.
2 Related Work
While user profile inference from social media has
received considerable attention (Al Zamal et al,
2012; Rao and Yarowsky, 2010; Rao et al, 2010;
Rao et al, 2011), most previous work has treated
this as a classification task where the goal is to pre-
dict unary predicates describing attributes of the
user. Examples include gender (Ciot et al, 2013;
Liu and Ruths, 2013; Liu et al, 2012), age (Rao et
al., 2010), or political polarity (Pennacchiotti and
Popescu, 2011; Conover et al, 2011).
A significant challenge that has limited previous
efforts in this area is the lack of available training
data. For example, researchers obtain training data
by employing workers from Amazon Mechanical
Turk to manually identify users? gender from pro-
file pictures (Ciot et al, 2013). This approach is
appropriate for attributes such as gender with a
small numbers of possible values (e.g., male or fe-
male), for which the values can be directly iden-
tified. However for attributes such as spouse or
education there are many possible values, making
it impossible to manually search for gold standard
answers within a large number of tweets which
may or may not contain sufficient evidence.
Also related is the Twitter user timeline extrac-
tion algorithm of Li and Cardie (2013). This work
is not focused on user attribute extraction, how-
ever.
Distant Supervision Distant supervision, also
known as weak supervision, is a method for learn-
ing to extract relations from text using ground
truth from an existing database as a source of
supervision. Rather than relying on mention-
level annotations, which are expensive and time
consuming to generate, distant supervision lever-
ages readily available structured data sources as
a weak source of supervision for relation ex-
traction from related text corpora (Craven et
al., 1999). For example, suppose r(e
1
, e
2
) =
IsIn(Paris, France) is a ground tuple in the
database and s =?Paris is the capital of France?
contains synonyms for both ?Paris? and ?France?,
then we assume that s may express the fact
r(e
1
, e
2
) in some way and can be used as pos-
itive training examples. In addition to the wide
use in text entity relation extraction (Mintz et al,
2009; Ritter et al, 2013; Hoffmann et al, 2011;
Surdeanu et al, 2012; Takamatsu et al, 2012),
distant supervision has been applied to multiple
166
Figure 1: Illustration of Goolge Plus ?knowledge
base?.
fields such as protein relation extraction (Craven
et al, 1999; Ravikumar et al, 2012), event extrac-
tion from Twitter (Benson et al, 2011), sentiment
analysis (Go et al, 2009) and Wikipedia infobox
generation (Wu and Weld, 2007).
Homophily Online social media offers a rich
source of network information. McPherson et
al. (2001) discovered that people sharing more
attributes such as background or hobby have
a higher chance of becoming friends in social
media. This property, known as HOMOPHILY
(summarized by the proverb ?birds of a feather
flock together?) (Al Zamal et al, 2012) has been
widely applied to community detection (Yang and
Leskovec, 2013) and friend recommendation (Guy
et al, 2010) on social media. In the user attribute
extraction literature, researchers have considered
neighborhood context to boost inference accuracy
(Pennacchiotti and Popescu, 2011; Al Zamal et al,
2012), where information about the degree of their
connectivity to their pre-labeled users is included
in the feature vectors. A related algorithm by Mis-
love et al (2010) crawled Facebook profiles of
4,000 Rice University students and alumni and in-
ferred attributes such as major and year of ma-
triculation purely based on network information.
Mislove?s work does not consider the users? text
stream, however. As we demonstrate below, rely-
ing solely on network information is not enough to
enable inference about attributes.
3 Dataset Creation
We now describe the generation of our distantly
supervised training dataset in detail. We make
use of Google Plus and Freebase to obtain ground
facts and extract positive/negative bags of post-
ings from users? twitter streams according to the
ground facts.
Figure 2: Example of fetching tweets containing
entity USC mention from Miranda Cosgrove (an
American actress and singer-songwriter)?s twitter
stream.
Education/Job We first used the Google Plus
API
5
(shown in Figure 1) to obtain a seed set
of users whose profiles contain both their educa-
tion/job status and a link to their twitter account.
6
Then, we fetched tweets containing the mention of
the education/job entity from each correspondent
user?s twitter stream using Twitter?s search API
7
(shown in Figure 2) and used them to construct
positive bags of tweets expressing the associated
attribute, namely EDUCATION(User
i
, Entity
j
), or
EMPLOYER(User
i
, Entity
j
). The Freebase API
8
is employed for alias recognition, to match terms
such as ?Harvard University?, ?Harvard?, ?Har-
vard U? to a single The remainder of each corre-
sponding user?s entire Twitter feed is used as neg-
ative training data.
9
We expanded our dataset from the seed users
according to network information provided by
Google Plus and Twitter. Concretely, we crawled
circle information of users in the seed set from
both their Twitter and Google Plus accounts and
performed a matching to pick out shared users
between one?s Twitter follower list and Google
Plus Circle. This process assures friend identity
and avoids the problem of name ambiguity when
matching accounts across websites. Among candi-
date users, those who explicitly display Job or Ed-
ucation information on Google Plus are preserved.
We then gathered positive and negative data as de-
scribed above.
Dataset statistics are presented in Table 2. Our
5
https://developers.google.com/+/api/
6
An unambiguous twitter account link is needed here be-
cause of the common phenomenon of name duplication.
7
https://twitter.com/search
8
http://wiki.freebase.com/wiki/
Freebase_API
9
Due to Twitter user timeline limit, we crawled at most
3200 tweets for each user.
167
education dataset contains 7,208 users, 6,295 of
which are connected to others in the network. The
positive training set for the EDUCATION is com-
prised of 134,060 tweets.
Spouse Facebook is the only type of social me-
dia where spouse information is commonly dis-
played. However, only a tiny amount of individ-
ual information is publicly accessible from Face-
book Graph API
10
. To obtain ground truth for the
spouse relation at large scale, we turned to Free-
base
11
, a large, open-domain database, and gath-
ered instances of the /PEOPLE/PERSON/SPOUSE
relation. Positive/negative training tweets are ob-
tained in the same way as was previously de-
scribed for EDUCATION and JOB. It is worth
noting that our Spouse dataset is not perfect, as
individuals retrieved from Freebase are mostly
celebrities, and thus it?s not clear whether this
group of people are representative of the general
population.
SPOUSE is an exception to the ?ho-
mophily? effect. But it exhibits another
unique property, known as, REFLEXIVITY: fact
IsSpouseOf(e
1
, e
2
) and IsSpouseOf(e
2
, e
1
)
will hold or not hold at the same time. Given train-
ing data expressing the tuple IsSpouseOf(e
1
, e
2
)
from user e
1
?s twitter stream, we also gather user
e
2
?s tweet collection, and fetch tweets with the
mention of e
1
. We augment negative training
data from e
2
as in the case of Education and Job.
Our Spouse dataset contains 1,636 users, where
there are 554 couples (1108 users). Note that
the number of positive entities (3,121) is greater
than the number of users as (1) one user can have
multiple spouses at different periods of time (2)
multiple entities may point to the same individual,
e.g., BarackObama, Barack Obama and Barack.
4 Model
We now describe our approach to predicting user
profile attributes.
4.1 Notation
Message X: Each user i ? [1, I] is associ-
ated with his Twitter ID and his tweet corpus
X
i
. X
i
is comprised of a collection of tweets
X
i
= {x
i,j
}
j=N
i
j=1
, where N
i
denotes the number
of tweets user i published.
10
https://developers.facebook.com/docs/
graph-api/
11
http://www.freebase.com/
Education Job Spouse
#Users 7,208 1,806 1,636
#Users Con-
nected
6,295 1,407 1,108
#Edges 11,167 3,565 554
#Pos Entities 451 380 3121
#Pos Tweets 124,801 65,031 135,466
#Aver Pos
Tweets per User
17.3 36.6 82.8
#Neg Entity 6,987,186 4,405,530 8,840,722
#Neg Tweets 16,150,600 10,687,403 12,872,695
Table 2: Statistics for our Dataset
Tweet Collection L
e
i
: L
e
i
denotes the collection
of postings containing the mention of entity e from
user i. L
e
i
? X
i
.
Entity attribute indicator z
k
i,e
and z
k
i,x
: For
each entity e ? X
i
, there is a boolean variable z
k
i,e
,
indicating whether entity e expresses attribute k of
user i. Each posting x ? L
e
i
is associated with at-
tribute indicator z
k
i,x
indicating whether posting x
expresses attribute k of user i. z
k
i,e
and z
k
i,x
are
observed during training and latent during testing.
Neighbor set F
k
i
: F
k
i
denotes the neighbor set
of user i. For Education (k = 0) and Job (k = 1),
F
k
i
denotes the group of users within the network
that are in friend relation with user i. For Spouse
attribute, F
k
i
denote current user?s spouse.
4.2 Model
The distant supervision assumes that if entity e
corresponds to an attribute for user i, at least one
posting from user i?s Twitter stream containing a
mention of emight express that attribute. For user-
level attribute prediction, we adopt the following
two strategies:
(1) GLOBAL directly makes aggregate (entity)
level prediction for z
k
i,e
, where features for all
tweets from L
e
i
are aggregated to one vector for
training and testing, following Mintz et al (2009).
(2) LOCAL makes local tweet-level predictions
for each tweet z
e
i,x
, x ? L
k
i
in the first place, mak-
ing the stronger assumption that all mentions of an
entity in the users? profile are expressing the asso-
ciated attribute. An aggregate-level decision z
k
i,e
is
then made from the deterministic OR operators.
z
e
i,x
=
{
1 ?x ? L
e
i
, s.t.z
k
i,x
= 1
0 Otherwise
(1)
The rest of this paper describes GLOBAL in de-
tail. The model and parameters with LOCAL are
identical to those in GLOBAL except that LOCAL
168
encode a tweet-level feature vector rather than an
aggregate one. They are therefore excluded for
brevity. For each attribute k, we use a model that
factorizes the joint distribution as product of two
distributions that separately characterize text fea-
tures and network information as follows:
?(z
k
i,e
, X
i
, F
k
i
: ?) ?
?
text
(z
k
i,e
, X
i
)?
Neigh
(z
k
i,e
, F
k
i
)
(2)
Text Factor We use ?
text
(z
k
e
, X
i
) to capture the
text related features which offer attribute clues:
?
text
(z
k
e
, , X
i
) = exp[(?
k
text
)
T
? ?
text
(z
k
i,e
, X
i
)]
(3)
The feature vector ?
text
(z
k
i,e
, X
i
) encodes the fol-
lowing standard general features:
? Entity-level: whether begins with capital let-
ter, length of entity.
? Token-level: for each token t ? e, word iden-
tity, word shape, part of speech tags, name
entity tags.
? Conjunctive features for a window of k
(k=1,2) words and part of speech tags.
? Tweet-level: All tokens in the correspondent
tweet.
In addition to general features, we employ
attribute-specific features, such as whether the en-
tity matches a bag of words observed in the list
of universities, colleges and high schools for Edu-
cation attribute, whether it matches terms in a list
of companies for Job attribute
12
. Lists of universi-
ties and companies are taken from knowledge base
NELL
13
.
Neighbor Factor For Job and Education, we
bias friends to have a larger possibility to share
the same attribute. ?
Neigh
(z
k
i,e
, F
k
i
) captures such
influence from friends within the network:
?
Neigh
(z
k
i,e
, F
k
i
) =
?
j?F
k
i
?
Neigh
(z
k
e
, X
j
)
?
Neigh
(z
k
i,e
, X
j
)
= exp[(?
k
Neigh
)
T
? ?
Neigh
(z
k
i,e
, X
j
)]
(4)
Features we explore include the whether entity e
is also the correspondent attribute with neighbor
user j, i.e., I(z
e
j,k
= 0) and I(z
e
j,k
= 1).
12
Freebase is employed for alias recognition.
13
http://rtw.ml.cmu.edu/rtw/kbbrowser/
Input: Tweet Collection {X
i
}, Neighbor set
{F
k
i
}
Initialization:
? for each user i:
for each candidate entity e ? X
i
z
k
i,e
= argmax
z
?
?(z
?
, X
i
) from text
features
End Initialization
while not convergence:
? for each user i:
update attribute values for j ? F
k
i
for each candidate entity e ? X
i
z
k
i,e
= argmax
z
?
?(z
?
, X
i
, F
k
i
)
end while:
Figure 3: Inference for NEIGH-LATENT setting.
For Spouse, we set F
spouse
i
= {e} and the
neighbor factor can be rewritten as:
?
Neigh
(z
k
i,e
, X
j
) = ?
Neigh
(C
i
, X
e
) (5)
It characterizes whether current user C
i
to be the
spouse of user e (if e corresponds to a Twitter
user). We expect clues about whether C
i
being en-
tity e?s spouse from e?s Twitter corpus will in turn
facilitate the spouse inference procedure of user i.
?
Neigh
(C
i
, X
e
) encodes I(C
i
? S
e
), I(C
i
6? S
e
).
Features we explore also include whether C
i
?s
twitter ID appears in e?s corpus.
4.3 Training
We separately trained three classifiers regarding
the three attributes. All variables are observed
during training; we therefore take a feature-based
approach to learning structure prediction models
inspired by structure compilation (Liang et al,
2008). In our setting, a subset of the features
(those based on network information) are com-
puted based on predictions that will need to be
made at test time, but are observed during train-
ing. This simplified approach to learning avoids
expensive inference; at test time, however, we still
need to jointly predict the best attribute values for
friends as is described in section 4.4.
4.4 Inference
Job and Education Our inference algorithm
for Job/Education is performed on two settings,
depending on whether neighbor information is
169
observed (NEIGH-OBSERVED) or latent (NEIGH-
LATENT). Real world applications, where network
information can be partly retrieved from all types
of social networks, can always falls in between.
Inference in the NEIGH-OBSERVED setting is
trivial; for each entity e ? G
i
, we simply predict
it?s candidate attribute values using Equ.6.
z
k
i,e
= argmax
z
?
?(z
?
, X
i
, F
k
i
) (6)
For NEIGH-LATENT setting, attributes for each
node along the network are treated latent and user
attribute prediction depends on attributes of his
neighbors. The objective function for joint infer-
ence would be difficult to optimize exactly, and
algorithms for doing so would be unlikely to scale
to network of the size we consider. Instead, we use
a sieve-based greedy search approach to inference
(shown in Figure 3) inspired by recent work on
coreference resolution (Raghunathan et al, 2010).
Attributes are initialized using only text features,
maximizing ?
text
(e,X
i
), and ignoring network
information. Then for each user we iteratively re-
estimate their profile given both their text features
and network features (computed based on the cur-
rent predictions made for their friends) which pro-
vide additional evidence.
In this way, highly confident predictions will be
made strictly from text in the first round, then the
network can either support or contradict low con-
fidence predictions as more decisions are made.
This process continues until no changes are made
at which point the algorithm terminates. We em-
pirically found it to work well in practice. We ex-
pect that NEIGH-OBSERVED performs better than
NEIGH-LATENT since the former benefits from
gold network information.
Spouse For Spouse inference, if candidate entity
e has no correspondent twitter account, we directly
determine z
k
i,e
= argmax
z
?
?(z
?
, X
i
) from text
features. Otherwise, the inference of z
k
i,e
depends
on the z
k
e,C
i
. Similarly, we initialize z
k
i,e
and z
k
e,C
i
by maximizing text factor, as we did for Educa-
tion and Job. Then we iteratively update z
k
given
by the rest variables until convergence.
5 Experiments
In this Section, we present our experimental re-
sults in detail.
Education Job
AFFINITY 74.3 14.5
Table 3: Affinity values for Education and Job.
5.1 Preprocessing and Experiment Setup
Each tweet posting is tokenized using Twitter NLP
tool introduced by Noah?s Ark
14
with # and @
separated following tokens. We assume that at-
tribute values should be either name entities or
terms following @ and #. Name entities are ex-
tracted using Ritter et al?s NER system (2011).
Consecutive tokens with the same named entity
tag are chunked (Mintz et al, 2009). Part-of-
speech tags are assigned based on Owoputi et als
tweet POS system (Owoputi et al, 2013).
Data is divided in halves. The first is used as
training data and the other as testing data.
5.2 Friends with Same Attribute
Our network intuition is that users are much more
likely to be friends with other users who share at-
tributes, when compared to users who have no at-
tributes in common. In order to statistically show
this, we report the value of AFFINITY defined by
Mislove et al(2010), which is used to quantita-
tively evaluate the degree of HOMOPHILY in the
network. AFFINITY is the ratio of the fraction of
links between attribute (k)-sharing users (S
k
), rel-
ative to what is expected if attributes are randomly
assigned in the network (E
k
).
S
k
=
?
i
?
j?F
k
i
I(P
k
i
= P
k
j
)
?
i
?
j?F
k
i
I
E
k
=
?
m
T
k
m
(T
k
m
? 1)
U
k
(U
k
? 1)
(7)
where T
k
m
denotes the number of users with m
value for attribute k and U
k
=
?
m
T
k
m
. Table 3
shows the affinity value of the Education and Job.
As we can see, the property of HOMOPHILY in-
deed exists among users in the social network with
respect to Education and Job attribute, as signifi-
cant affinity is observed. In particular, the affinity
value for Education is 74.3, implying that users
connected by a link in the network are 74.3 times
more likely affiliated in the same school than as
expected if education attributes are randomly as-
signed. It is interesting to note that Education ex-
hibits a much stronger HOMOPHILY property than
14
https://code.google.com/p/
ark-tweet-nlp/downloads/list
170
Job. Such affinity demonstrates that our approach
that tries to take advantage of network information
for attribute prediction of holds promise.
5.3 Evaluation and Discussion
We evaluate settings described in Section 4.2 i.e.,
GLOBAL setting, where user-level attribute is pre-
dicted directly from jointly feature space and LO-
CAL setting where user-level prediction is made
based on tweet-level prediction along with differ-
ent inference approaches described in Section 4.4,
i.e. NEIGH-OBSERVED and NEIGH-LATENT, re-
garding whether neighbor information is observed
or latent.
Baselines We implement the following base-
lines for comparison and use identical processing
techniques for each approach for fairness.
? Only-Text: A simplified version of our algo-
rithm where network/neighbor influence is ig-
nored. Classifier is trained and tested only based
on text features.
? NELL: For Job and Education, candidate is se-
lected as attribute value once it matches bag of
words in the list of universities or companies
borrowed from NELL. For Education, the list is
extended by alias identification based on Free-
base. For Job, we also fetch the name abbrevia-
tions
15
. NELL is only implemented for Educa-
tion and Job attribute.
For each setting from each approach, we report
the (P)recision, (R)ecall and (F)1-score. For LO-
CAL setting, we report the performance for both
entity-level prediction (Entity) and posting-level
prediction (Tweet). Results for Education, Job and
Spouse from different approaches appear in Table
4, 5 and 6 respectively.
Local or Global For horizontal comparison, we
observe that GLOBAL obtains a higher Precision
score but a lower Recall than LOCAL(ENTITY).
This can be explained by the fact that LOCAL(U)
sets z
k
i,e
= 1 once one posting x ? L
e
i
is identified
as attribute related, while GLOBAL tend to be more
meticulous by considering the conjunctive feature
space from all postings.
Homophile effect In agreement with our ex-
pectation, NEIGH-OBSERVED performs better than
NEIGH-LATENT since erroneous predictions in
15
http://www.abbreviations.com/
NEIGH-LATENT setting will have negative in-
fluence on further prediction during the greedy
search process. Both NEIGH-OBSERVED and
NEIGH-LATENT where network information is
harnessed, perform better than Only-Text, which
the prediction is made independently on user?s text
features. The improvement of NEIGH-OBSERVED
over Only-Text is 22.7% and 6.4% regarding F-
1 score for Education and Job respectively, which
further illustrate the usefulness of making use of
Homophile effect for attribute inference on online
social media. It is also interesting to note the im-
provement much more significant in Education in-
ference than Job inference. This is in accord with
what we find in Section 5.2, where education net-
work exhibits stronger HOMOPHILE property than
Job network, enabling a significant benefit for ed-
ucation inference, but limited for job inference.
Spouse prediction also benefits from neighbor-
ing effect and the improvement is about 12% for
LOCAL(ENTITY) setting. Unlike Education and
Job prediction, for which in NEIGH-OBSERVED
setting all neighboring variables are observed, net-
work variables are hidden during spouse predic-
tion. By considering network information, the
model benefits from evident clues offered by tweet
corpus of user e?s spouse when making prediction
for e, but also suffers when erroneous decision are
made and then used for downstream predictions.
NELL Baseline Notably, NELL achieves high-
est Recall score for Education inference. It is
also worth noting that most of education men-
tions that NELL fails to retrieve are those in-
volve irregular spellings, such as HarvardUniv and
Cornell U, which means Recall score for NELL
baseline would be even higher if these irregular
spellings are recognized in a more sophisticated
system. The reason for such high recall is that as
our ground truths are obtained from Google plus,
the users from which are mostly affiliated with de-
cent schools found in NELL dictionary. However,
the high recall from NELL is sacrificed at preci-
sion, as users can mention school entities in many
of situations, such as paying a visit or reporting
some relevant news. NELL will erroneously clas-
sify these cases as attribute mentions.
NELL does not work out for Job, with a fairly
poor 0.0156 F1 score for LOCAL(ENTITY) and
0.163 for LOCAL(TWEET). Poor precision is ex-
pected for as users can mention firm entity in a
great many of situations. The recall score for
171
GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F
Our approach
NEIGH-OBSERVED 0.804 0.515 0.628 0.524 0.780 0.627 0.889 0.729 0.801
NEIGH-LATENT 0.755 0.440 0.556 0.420 0.741 0.536 0.854 0.724 0.783
Only-Text ?- 0.735 0.393 0.512 0.345 0.725 0.467 0.809 0.724 0.764
NELL ?- ?- ?- ?- 0.170 0.798 0.280 0.616 0.848 0.713
Table 4: Results for Education Prediction
GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F
Our approach
NEIGH-OBSERVED 0.643 0.330 0.430 0.374 0.620 0.467 0.891 0.698 0.783
NEIGH-LATENT 0.617 0.320 0.421 0.226 0.544 0.319 0.804 0.572 0.668
Only-Text ?- 0.602 0.304 0.404 0.155 0.501 0.237 0.764 0.471 0.583
NELL ?- ?- ?- ?- 0.0079 0.509 0.0156 0.094 0.604 0.163
Table 5: Results for Job Prediction
GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F
Our approach ?- 0.870 0.560 0.681 0.593 0.857 0.701 0.904 0.782 0.839
Only-Text ?- 0.852 0.448 0.587 0.521 0.781 0.625 0.890 0.729 0.801
Table 6: Results for Spouse Prediction
NELL in job inference is also quite low as job
related entities exhibit a greater diversity of men-
tions, many of which are not covered by the NELL
dictionary.
Vertical Comparison: Education, Job and
Spouse Job prediction turned out to be much
more difficult than Education, as shown in Ta-
bles 4 and 5. Explanations are as follows: (1)
Job contains a much greater diversity of mentions
than Education. Education inference can benefit a
lot from the dictionary relevant feature which Job
may not. (2) Education mentions are usually asso-
ciated with clear evidence such as homework, ex-
ams, studies, cafeteria or books, while situations
are much more complicated for job as vocabular-
ies are usually specific for different types of jobs.
(3) The boundary between a user working in and
a fun for a specific operation is usually ambigu-
ous. For example, a Google engineer may con-
stantly update information about outcome prod-
ucts of Google, so does a big fun. If the aforemen-
tioned engineer barely tweets about working con-
ditions or colleagues (which might still be ambigu-
ous), his tweet collection, which contains many of
mentions about outcomes of Google product, will
be significantly similar to tweets published by a
Google fun. Such nuisance can be partly solved
by the consideration of network information, but
not totally.
The relatively high F1 score for spouse predic-
tion is largely caused by the great many of non-
individual related entities in the dataset, the iden-
tification of which would be relatively simpler. A
deeper look at the result shows that the classifier
frequently makes wrong decisions for entities such
as userID and name entities. Significant as some
spouse relevant features are, such as love, hus-
band, child, in most circumstances, spouse men-
tions are extremely hard to recognize. For exam-
ple, in tweets ?Check this out, @alancross, it?s
awesome bit.ly/1bnjYHh.? or ?Happy Birth-
day @alancross !?. alancross can reasonably be
any option among current user?s friend, colleague,
parents, child or spouse. Repeated mentions add
no confidence. Although we can identify alan-
cross as spouse attribute once it jointly appear
with other strong spouse indicators, they are still
many cases where they never co-appear. How to
integrate more useful side information for spouse
recognition constitutes our future work.
6 Conclusion and Future Work
In this paper, we propose a framework for user at-
tribute inference on Twitter. We construct the pub-
licly available dataset based on distant supervision
and experiment our model on three useful user
profile attributes, i.e., Education, Job and Spouse.
Our model takes advantage of network informa-
tion on social network. We will keep updating the
dataset as more data is collected.
One direction of our future work involves ex-
ploring more general categories of user profile at-
172
tributes, such as interested books, movies, home-
town, religion and so on. Facebook would an
ideal ground truth knowledge base. Another direc-
tion involves incorporating richer feature space for
better inference performance, such as multi-media
sources (i.e. pictures and video).
7 Acknowledgments
A special thanks is owned to Dr. Julian McAuley
and Prof. Jure Leskovec from Stanford University
for the Google+ circle/network crawler, without
which the network analysis would not have been
conducted. This work was supported in part by
DARPA under award FA8750-13-2-0005.
References
Faiyaz Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of twitter users from neighbors. In
ICWSM.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 389?398. As-
sociation for Computational Linguistics.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. In The World Wide Web
and Databases.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18?21.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Mark Craven and Johan Kumlien 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77?86.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
Ido Guy, Naama Zwerdling, Inbal Ronen, David
Carmel, and Erel Uziel. 2010. Social media recom-
mendation based on people and tags. In Proceedings
of the 33rd international ACM SIGIR conference on
Research and development in information retrieval,
pages 194?201. ACM.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In ACL, pages 541?550.
Jiwei Li and Claire Cardie. 2013. Timeline generation:
Tracking individuals on twitter. Proceedings of the
23rd international conference on World wide web.
Percy Liang, Hal Daum?e III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning.
Wendy Liu and Derek Ruths. 2013. Whats in a name?
using first names as features for gender inference in
twitter. In 2013 AAAI Spring Symposium Series.
Wendy Liu, Faiyaz Zamal, and Derek Ruths. 2012.
Using social media to infer gender composition of
commuter populations. In Proceedings of the When
the City Meets the Citizen Workshop, the Interna-
tional Conference on Weblogs and Social Media.
Miller McPherson, Lynn Smith-Lovin, and James M
Cook. 2001. Birds of a feather: Homophily in social
networks. Annual review of sociology, pages 415?
444.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003?1011. Association for
Computational Linguistics.
Alan Mislove, Bimal Viswanath, Krishna Gummadi,
and Peter Druschel. 2010. You are who you know:
inferring user profiles in online social networks. In
Proceedings of the third ACM international confer-
ence on Web search and data mining, pages 251?
260. ACM.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Marco Pennacchiotti and Ana Popescu. 2011. A ma-
chine learning approach to twitter user classification.
In ICWSM.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing.
Delip Rao and David Yarowsky. 2010. Detecting latent
user properties in social media. In Proc. of the NIPS
MLSN Workshop.
173
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2nd in-
ternational workshop on Search and mining user-
generated contents, pages 37?44. ACM.
Delip Rao, Michael Paul, Clayton Fink, David
Yarowsky, Timothy Oates, and Glen Coppersmith.
2011. Hierarchical bayesian models for latent at-
tribute detection in social media. In ICWSM.
Haibin Liu, Michael Wall, Karin Verspoor, et al 2012.
Literature mining of protein-residue associations
with graph rules learned through distant supervision.
Journal of biomedical semantics, 3(Suppl 3):S2.
Alan Ritter, Sam Clark, Mausam, Oren Etzioni, et al
2011. Named entity recognition in tweets: an ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1524?1534. Association for Compu-
tational Linguistics.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455?
465. Association for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
721?729. Association for Computational Linguis-
tics.
Fei Wu and Daniel S Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the six-
teenth ACM conference on Conference on infor-
mation and knowledge management, pages 41?50.
ACM.
Jaewon Yang and Jure Leskovec. 2013. Overlapping
community detection at scale: A nonnegative matrix
factorization approach. In Proceedings of the sixth
ACM international conference on Web search and
data mining, pages 587?596. ACM.
174
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 2: Sentiment Analysis in Twitter
Preslav Nakov
QCRI, Qatar Foundation
pnakov@qf.org.qa
Zornitsa Kozareva
USC Information Sciences Institute
kozareva@isi.edu
Alan Ritter
University of Washington
aritter@cs.washington.edu
Sara Rosenthal
Columbia University
sara@cs.columbia.edu
Veselin Stoyanov
JHU HLTCOE
ves@cs.jhu.edu
Theresa Wilson
JHU HLTCOE
taw@jhu.edu
Abstract
In recent years, sentiment analysis in social
media has attracted a lot of research interest
and has been used for a number of applica-
tions. Unfortunately, research has been hin-
dered by the lack of suitable datasets, com-
plicating the comparison between approaches.
To address this issue, we have proposed
SemEval-2013 Task 2: Sentiment Analysis in
Twitter, which included two subtasks: A, an
expression-level subtask, and B, a message-
level subtask. We used crowdsourcing on
Amazon Mechanical Turk to label a large
Twitter training dataset alng with additional
test sets of Twitter and SMS messages for both
subtasks. All datasets used in the evaluation
are released to the research community. The
task attracted significant interest and a total
of 149 submissions from 44 teams. The best-
performing team achieved an F1 of 88.9% and
69% for subtasks A and B, respectively.
1 Introduction
In the past decade, new forms of communication,
such as microblogging and text messaging have
emerged and become ubiquitous. Twitter messages
(tweets) and cell phone messages (SMS) are often
used to share opinions and sentiments about the sur-
rounding world, and the availability of social con-
tent generated on sites such as Twitter creates new
opportunities to automatically study public opinion.
Working with these informal text genres presents
new challenges for natural language processing be-
yond those encountered when working with more
traditional text genres such as newswire.
Tweets and SMS messages are short in length: a
sentence or a headline rather than a document. The
language they use is very informal, with creative
spelling and punctuation, misspellings, slang, new
words, URLs, and genre-specific terminology and
abbreviations, e.g., RT for re-tweet and #hashtags.1
How to handle such challenges so as to automati-
cally mine and understand the opinions and senti-
ments that people are communicating has only very
recently been the subject of research (Jansen et al,
2009; Barbosa and Feng, 2010; Bifet et al, 2011;
Davidov et al, 2010; O?Connor et al, 2010; Pak and
Paroubek, 2010; Tumasjan et al, 2010; Kouloumpis
et al, 2011).
Another aspect of social media data, such as Twit-
ter messages, is that they include rich structured in-
formation about the individuals involved in the com-
munication. For example, Twitter maintains infor-
mation about who follows whom. Re-tweets (re-
shares of a tweet) and tags inside of tweets provide
discourse information. Modeling such structured in-
formation is important because it provides means for
empirically studying social interactions where opin-
ion is conveyed, e.g., we can study the properties of
persuasive language or those associated with influ-
ential users.
Several corpora with detailed opinion and senti-
ment annotation have been made freely available,
e.g., the MPQA corpus (Wiebe et al, 2005) of
newswire text. These corpora have proved very
valuable as resources for learning about the lan-
guage of sentiment in general, but they did not focus
on social media.
1Hashtags are a type of tagging for Twitter messages.
312
Twitter RT @tash jade: That?s really sad, Charlie RT ?Until tonight I never realised how fucked up I was? -
Charlie Sheen #sheenroast
SMS Glad to hear you are coping fine in uni... So, wat interview did you go to? How did it go?
Table 1: Examples of sentences from each corpus that contain subjective phrases.
While some Twitter sentiment datasets have al-
ready been created, they were either small and pro-
prietary, such as the i-sieve corpus (Kouloumpis
et al, 2011), or they were created only for Span-
ish like the TASS corpus2 (Villena-Roma?n et al,
2013), or they relied on noisy labels obtained from
emoticons and hashtags. They further focused on
message-level sentiment, and no Twitter or SMS
corpus with expression-level sentiment annotations
has been made available so far.
Thus, the primary goal of our SemEval-2013 task
2 has been to promote research that will lead to a
better understanding of how sentiment is conveyed
in Tweets and SMS messages. Toward that goal,
we created the SemEval Tweet corpus, which con-
tains Tweets (for both training and testing) and SMS
messages (for testing only) with sentiment expres-
sions annotated with contextual phrase-level polar-
ity as well as an overall message-level polarity. We
used this corpus as a testbed for the system evalua-
tion at SemEval-2013 Task 2.
In the remainder of this paper, we first describe
the task, the dataset creation process, and the evalu-
ation methodology. We then summarize the charac-
teristics of the approaches taken by the participating
systems and we discuss their scores.
2 Task Description
We had two subtasks: an expression-level subtask
and a message-level subtask. Participants could
choose to participate in either or both subtasks. Be-
low we provide short descriptions of the objectives
of these two subtasks.
Subtask A: Contextual Polarity Disambiguation
Given a message containing a marked instance
of a word or a phrase, determine whether that
instance is positive, negative or neutral in that
context. The boundaries for the marked in-
stance were provided: this was a classification
task, not an entity recognition task.
2http://www.daedalus.es/TASS/corpus.php
Subtask B: Message Polarity Classification
Given a message, decide whether it is of
positive, negative, or neutral sentiment. For
messages conveying both a positive and a
negative sentiment, whichever is the stronger
one was to be chosen.
Each participating team was allowed to submit re-
sults for two different systems per subtask: one con-
strained, and one unconstrained. A constrained sys-
tem could only use the provided data for training,
but it could also use other resources such as lexi-
cons obtained elsewhere. An unconstrained system
could use any additional data as part of the training
process; this could be done in a supervised, semi-
supervised, or unsupervised fashion.
Note that constrained/unconstrained refers to the
data used to train a classifier. For example, if other
data (excluding the test data) was used to develop
a sentiment lexicon, and the lexicon was used to
generate features, the system would still be con-
strained. However, if other data (excluding the test
data) was used to develop a sentiment lexicon, and
this lexicon was used to automatically label addi-
tional Tweet/SMS messages and then used with the
original data to train the classifier, then such a sys-
tem would be unconstrained.
3 Dataset Creation
In the following sections we describe the collection
and annotation of the Twitter and SMS datasets.
3.1 Data Collection
Twitter is the most common micro-blogging site on
the Web, and we used it to gather tweets that express
sentiment about popular topics. We first extracted
named entities using a Twitter-tuned NER system
(Ritter et al, 2011) from millions of tweets, which
we collected over a one-year period spanning from
January 2012 to January 2013; we used the public
streaming Twitter API to download tweets.
313
Instructions: Subjective words are ones which convey an opinion. Given a sentence, identify whether it is objective,
positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the sentence and mark
the position of its start and end in the text boxes below. The number above each word indicates its position. The
word/phrase will be generated in the adjacent textbox so that you can confirm that you chose the correct range.
Choose the polarity of the word or phrase by selecting one of the radio buttons: positive, negative, or neutral. If a
sentence is not subjective please select the checkbox indicating that ?There are no subjective words/phrases?. Please
read the examples and invalid responses before beginning if this is your first time answering this hit.
Figure 1: Instructions provided to workers on Mechanical Turk followed by a screenshot.
Average # of Total Phrase Count Vocabulary
Corpus Words Characters Positive Negative Neutral Size
Twitter - Training 25.4 120.0 5,895 3,131 471 20,012
Twitter - Dev 25.5 120.0 648 430 57 4,426
Twitter - Test 25.4 121.2 2,734 1,541 160 11,736
SMS - Test 24.5 95.6 1,071 1,104 159 3,562
Table 2: Statistics for Subtask A.
We then identified popular topics as those named
entities that are frequently mentioned in association
with a specific date (Ritter et al, 2012). Given this
set of automatically identified topics, we gathered
tweets from the same time period which mentioned
the named entities. The testing messages had differ-
ent topics from training and spanned later periods.
To identify messages that express sentiment to-
wards these topics, we filtered the tweets us-
ing SentiWordNet (Baccianella et al, 2010). We
removed messages that contained no sentiment-
bearing words, keeping only those with at least one
word with positive or negative sentiment score that
is greater than 0.3 in SentiWordNet for at least one
sense of the words. Without filtering, we found class
imbalance to be too high.3
Twitter messages are rich in social media features,
including out-of-vocabulary (OOV) words, emoti-
cons, and acronyms; see Table 1. A large portion of
the OOV words are hashtags (e.g., #sheenroast)
and mentions (e.g., @tash jade).
3Filtering based on an existing lexicon does bias the dataset
to some degree; however, note that the text still contains senti-
ment expressions outside those in the lexicon.
Corpus Positive Negative Objective
/ Neutral
Twitter - Training 3,662 1,466 4,600
Twitter - Dev 575 340 739
Twitter - Test 1,573 601 1,640
SMS - Test 492 394 1,208
Table 3: Statistics for Subtask B.
We annotated the same Twitter messages with an-
notations for subtask A and subtask B. However,
the final training and testing datasets overlap only
partially between the two subtasks since we had
to throw away messages with low inter-annotator
agreement, and this differed between the subtasks.
For testing, we also annotated SMS messages, taken
from the NUS SMS corpus4 (Chen and Kan, 2012).
Tables 2 and 3 show statistics about the corpora we
created for subtasks A and B.
4http://wing.comp.nus.edu.sg/SMSCorpus/
314
A B
Lower Avg. Upper Avg.
Twitter - Train 64.7 82.4 90.8 82.7
Twitter - Dev 51.2 74.7 87.8 78.4
Twitter - Test 68.8 83.6 90.9 76.9
SMS - Test 66.5 88.5 81.2 77.6
Table 4: Bounds for datasets in subtasks A and B.
3.2 Annotation Guidelines
The instructions provided to the annotators, along
with an example, are shown in Figure 1. We pro-
vided several additional examples to the annotators,
shown in Table 5.
In addition, we filtered spammers by considering
the following kinds of annotations invalid:
? containing overlapping subjective phrases;
? subjective but without a subjective phrase;
? marking every single word as subjective;
? not having the overall sentiment marked.
3.3 Annotation Process
Our datasets were annotated for sentiment on Me-
chanical Turk. Each sentence was annotated by five
Mechanical Turk workers (Turkers). In order to
qualify for the hits, the Turker had to have an ap-
proval rate greater than 95% and have completed 50
approved hits. Each Turker was paid three cents
per hit. The Turker had to mark all the subjec-
tive words/phrases in the sentence by indicating their
start and end positions and say whether each subjec-
tive word/phrase was positive, negative, or neutral
(subtask A). They also had to indicate the overall
polarity of the sentence (subtask B).
Figure 1 shows the instructions and an exam-
ple provided to the Turkers. The first five rows
of Table 6 show an example of the subjective
words/phrases marked by each of the workers.
For subtask A, we combined the annotations of
each of the workers using intersection as indicated
in the last row of Table 6. A word had to appear
in 2/3 of the annotations in order to be considered
subjective. Similarly, a word had to be labeled with
a particular polarity (positive, negative, or neutral)
2/3 of the time in order to receive that label.
We also experimented with combining annota-
tions by computing the union of the sentences, and
taking the sentence of the worker who annotated the
most hits, but we found that these methods were
not as accurate. Table 4 shows the lower, average,
and upper bounds for all the hits by computing the
bounds for each hit and averaging them together.
This gives a good indication about how well we can
expect the systems to perform. For example, even if
we used the best annotator each time, it would still
not be possible to get perfect accuracy.
For subtask B, the polarity of the entire sentence
was determined based on the majority of the labels.
If there was a tie, the sentence was discarded. In
order to reduce the number of sentences lost, we
combined the objective and the neutral labels, which
Turkers tended to mix up. Table 4 shows the aver-
age bound for subtask B by computing the bounds
for each hit and averaging them together. Since the
polarity is chosen based on the majority, the upper
bound is 100%.
4 Scoring
For both subtasks, the participating systems were
required to perform a three-way classification ? a
particular marked phrase (for subtask A) or an en-
tire message (for subtask B) was to be classified as
positive, negative, or objective. For each system,
we computed a score for predicting positive/negative
phrases/messages vs. the other two classes.
For instance, to compute positive precision, Ppos,
we find the number of phrases/messages that a sys-
tem correctly predicted to be positive, and we divide
that number by the total number of messages it pre-
dicted to be positive. To compute recall, for the pos-
itive class, Rpos, we find the number of messages
correctly predicted to be positive and we divide that
number by the total number of positive messages in
the gold standard.
We then calculate F-score for the positive labels,
the harmonic average of precision and recall as fol-
lows Fpos = 2
PposRpos
Ppos+Rpos
. We carry out a similar
computation to calculate Fneg, which is F1 for neg-
ative messages.
The overall score for each system run is then
given by the average of the F1-scores for the posi-
tive and negative classes: F = (Fpos + Fneg)/2.
315
Authorities are only too aware that Kashgar is 4,000 kilometres (2,500 miles) from Beijing but only a tenth of
the distance from the Pakistani border, and are desperate to ensure instability or militancy does not leak over the
frontiers.
Taiwan-made products stood a good chance of becoming even more competitive thanks to wider access to overseas
markets and lower costs for material imports, he said.
?March appears to be a more reasonable estimate while earlier admission cannot be entirely ruled out,? according
to Chen, also Taiwan?s chief WTO negotiator.
friday evening plans were great, but saturday?s plans didnt go as expected ? i went dancing & it was an ok club,
but terribly crowded :-(
WHY THE HELL DO YOU GUYS ALL HAVE MRS. KENNEDY! SHES A FUCKING DOUCHE
AT&T was okay but whenever they do something nice in the name of customer service it seems like a favor, while
T-Mobile makes that a normal everyday thin
obama should be impeached on TREASON charges. Our Nuclear arsenal was TOP Secret. Till HE told our enemies
what we had. #Coward #Traitor
My graduation speech: ?I?d like to thanks Google, Wikipedia and my computer! :D #iThingteens
Table 5: List of example sentences with annotations that were provided to the annotators. All subjective phrases are
italicized. Positive phrases are in green, negative phrases are in red, and neutral phrases are in blue.
Worker 1 I would love to watch Vampire Diaries :) and some Heroes! Great combination 9/13
Worker 2 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Worker 3 I would love to watch Vampire Diaries :) and some Heroes! Great combination 10/13
Worker 4 I would love to watch Vampire Diaries :) and some Heroes! Great combination 13/13
Worker 5 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Intersection I would love to watch Vampire Diaries :) and some Heroes! Great combination
Table 6: Example of a sentence annotated for subjectivity on Mechanical Turk. Words and phrases that were marked as
subjective are italicized and highlighted in bold. The first five rows are annotations provided by Turkers, and the final
row shows their intersection. The final column shows the accuracy for each annotation compared to the intersection.
Note that ignoring Fneutral does not reduce the
task to predicting positive vs. negative labels only
(even though some participants have chosen to do
so) since the gold standard still contains neutral
labels which are to be predicted: Fpos and Fneg
would suffer if these examples are labeled as posi-
tive and/or negative instead of neutral.
We provided participants with a scorer. In addi-
tion to outputting the overall F-score, it produced
a confusion matrix for the three prediction classes
(positive, negative, and objective), and it also vali-
dated the data submission format.
5 Participants and Results
The results for subtask A are shown in Tables 7 and
8 for Twitter and for SMS messages, respectively;
those for subtask B are shown in Table 9 for Twit-
ter and in Table 10 for SMS messages. Systems are
ranked by their scores for the constrained runs; the
ranking based on scores for unconstrained runs is
shown as a subindex.
For both subtasks, there were teams that only sub-
mitted results for the Twitter test set. Some teams
submitted both a constrained and an unconstrained
version (e.g., AVAYA and teragram). As one would
expect, the results on the Twitter test set tended to be
better than those on the SMS test set since the SMS
data was out-of-domain with respect to the training
(Twitter) data.
Moreover, the results for subtask A were signifi-
cantly better than those for subtask B, which shows
that it is a much easier task, probably because there
is less ambiguity at the phrase-level.
5.1 Subtask A: Contextual Polarity
Table 7 shows that subtask A, Twitter, attracted 23
teams, who submitted 21 constrained and 7 uncon-
strained systems. Five teams submitted both a con-
strained and an unconstrained system, and two other
teams submitted constrained systems that are on
the boundary between being constrained and uncon-
strained.
316
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 88.93 yes yes
AVAYA 86.98 87.38(1) yes yes
BOUNCE 86.79 yes yes
LVIC-LIMSI 85.70 yes yes
FBM 85.50 yes semi
GU-MLT-LT 85.19 yes yes
UNITOR 84.60 yes yes
USNA 81.31 yes yes
Serendio 80.04 yes yes
ECNUCS 79.48 80.15(2) yes yes
TJP 78.16 yes yes
?columbia-nlp 74.94 yes yes
teragram 74.89(3) yes yes
sielers 74.41 yes yes
KLUE 73.74 yes yes
OPTWIMA 69.17 36.91(6) yes yes
swatcs 67.19 63.86(5) no yes
Kea 63.94 yes yes
senti.ue-en 62.79 71.38(4) yes yes
uottawa 60.20 yes yes
IITB 54.80 yes yes
SenselyticTeam 53.88 yes yes
SU-sentilab 34.73(7) no yes
Majority Baseline 38.10 N/A N/A
Table 7: Results for subtask A on the Twitter dataset. The
? marks a team that includes a task coorganizer, and the
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
One system was semi-supervised, and the rest
were supervised. The supervised systems used clas-
sifiers such as SVM (8 systems), Naive Bayes (7 sys-
tems), and Maximum Entropy (3 systems). Other
approaches used include an ensemble of classifiers,
manual rules, and a linear classifier. Two of the sys-
tems chose not to predict neutral as a possible clas-
sification label.
The average F1-measure on the Twitter test set
was 74.1% for constrained systems and 60.5% for
unconstrained ones; this does not mean that using
additional data does not help, it just shows that the
best teams only participated with a constrained sys-
tem. NRC-Canada had the best constrained system
with an F1-measure of 88.9%, and AVAYA had the
best unconstrained one with F1=87.4%.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
GU-MLT-LT 88.37 yes yes
NRC-Canada 88.00 yes yes
?AVAYA 83.94 85.79(1) yes yes
UNITOR 82.49 yes yes
TJP 81.23 yes yes
LVIC-LIMSI 80.16 yes yes
USNA 79.82 yes yes
ECNUCS 76.69 77.34(2) yes yes
sielers 73.48 yes yes
FBM 72.95 no semi
teragram 72.83 72.83(4) yes yes
KLUE 70.54 yes yes
?columbia-nlp 70.30 yes yes
senti.ue-en 66.09 74.13(3) yes yes
swatcs 66.00 67.68(5) no yes
Kea 63.27 yes yes
uottawa 55.89 yes yes
SU-sentilab 55.38(6) no yes
SenselyticTeam 51.13 yes yes
OPTWIMA 37.32 36.38(7) yes yes
Majority Baseline 31.50 N/A N/A
Table 8: Results for subtask A on the SMS dataset. The
? indicates a late submission, the ? marks a team that
includes a task co-organizer, and the  indicates a sys-
tem submitted as constrained but which used additional
Tweets or additional sentiment-annotated text to collect
statistics that were then used as a feature.
Table 8 shows the results for the SMS test set,
where 20 teams submitted 19 constrained and 7 un-
constrained systems (again, this included two teams
that submitted boundary systems, marked accord-
ingly). The average F-measure on this test set
was 70.8% for constrained systems and 65.7% for
unconstrained systems. The best constrained sys-
tem was that of GU-MLT-LT with an F-measure of
88.4%, and AVAYA had the best unconstrained sys-
tem with an F1 of 85.8%.
5.2 Subtask B: Message Polarity
Table 9 shows that subtask B, Twitter, attracted 38
teams, who submitted 36 constrained and 15 uncon-
strained systems (and two boundary ones).
The average F1-measure was 53.7% for the con-
strained and 54.6% for the unconstrained systems.
317
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 69.02 yes yes
GU-MLT-LT 65.27 yes yes
teragram 64.86 64.86(1) yes yes
BOUNCE 63.53 yes yes
KLUE 63.06 yes yes
AMI&ERIC 62.55 61.17(3) yes yes/semi
FBM 61.17 yes yes
AVAYA 60.84 64.06(2) yes yes/semi
SAIL 60.14 61.03(4) yes yes
UT-DB 59.87 yes yes
FBK-irst 59.76 yes yes
nlp.cs.aueb.gr 58.91 yes yes
UNITOR 58.27 59.50(5) yes semi
LVIC-LIMSI 57.14 yes yes
Umigon 56.96 yes yes
NILC USP 56.31 yes yes
DataMining 55.52 yes semi
ECNUCS 55.05 58.42(6) yes yes
nlp.cs.aueb.gr 54.73 yes yes
ASVUniOfLeipzig 54.56 yes yes
SZTE-NLP 54.33 53.10(9) yes yes
CodeX 53.89 yes yes
Oasis 53.84 yes yes
NTNU 53.23 50.71(10) yes yes
UoM 51.81 45.07(15) yes yes
SSA-UO 50.17 yes no
SenselyticTeam 50.10 yes yes
UMCC DLSI (SA) 49.27 48.99(12) yes yes
bwbaugh 48.83 54.37(8) yes yes/semi
senti.ue-en 47.24 47.85(13) yes yes
SU-sentilab 45.75(14) yes yes
OPTWIMA 45.40 54.51(7) yes yes
REACTION 45.01 yes yes
uottawa 42.51 yes yes
IITB 39.80 yes yes
IIRG 34.44 yes yes
sinai 16.28 49.26(11) yes yes
Majority Baseline 29.19 N/A N/A
Table 9: Results for subtask B on the Twitter dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
These averages are much lower than those for sub-
task A, which indicates that subtask B is harder,
probably because a message can contain parts ex-
pressing both positive and negative sentiment.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 68.46 yes yes
GU-MLT-LT 62.15 yes yes
KLUE 62.03 yes yes
AVAYA 60.00 59.47(1) yes yes/semi
teragram 59.10(2) yes yes
NTNU 57.97 54.55(6) yes yes
CodeX 56.70 yes yes
FBK-irst 54.87 yes yes
AMI&ERIC 53.63 52.62(7) yes yes/semi
ECNUCS 53.21 54.77(5) yes yes
UT-DB 52.46 yes yes
SAIL 51.84 51.98(8) yes yes
UNITOR 51.22 48.88(10) yes semi
SZTE-NLP 51.08 55.46(3) yes yes
SenselyticTeam 51.07 yes yes
NILC USP 50.12 yes yes
REACTION 50.11 yes yes
SU-sentilab 49.57(9) no yes
nlp.cs.aueb.gr 49.41 55.28(4) yes yes
LVIC-LIMSI 49.17 yes yes
FBM 47.40 yes yes
ASVUniOfLeipzig 46.50 yes yes
senti.ue-en 44.65 46.72(12) yes yes
SSA UO 44.39 yes no
UMCC DLSI (SA) 43.39 40.67(14) yes yes
UoM 42.22 35.22(15) yes yes
OPTWIMA 40.98 47.15(11) yes yes
uottawa 40.51 yes yes
bwbaugh 39.73 43.43(13) yes yes/semi
IIRG 22.16 yes yes
Majority Baseline 19.03 N/A N/A
Table 10: Results for subtask B on the SMS dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
Once again, NRC-Canada had the best con-
strained system with an F1-measure of 69%, fol-
lowed by teragram, which had the best uncon-
strained system with an F1-measure of 64.9%.
As Table 10 shows, the average F1-measure on
the SMS test set was 50.2% for constrained and
50.3% for unconstrained systems. NRC-Canada had
the best constrained system with an F1=68.5%, and
AVAYA had the best unconstrained one with F1-
measure of 59.5%.
318
5.3 Overall
Overall, the results achieved by the best teams were
very strong, especially for the simpler subtask A:
? F1=88.93, NRC-Canada on subtask A, Twitter;
? F1=88.37, GU-MLT-LT on subtask A, SMS;
? F1=69.02, NRC-Canada on subtask B, Twitter;
? F1=68.46, NRC-Canada on subtask B, SMS.
We can see that the strongest team overall was that
of NRC-Canada, which was ranked first on three of
the four conditions; and it was second on subtask A,
SMS. There were two other teams that were strong
across both tasks and on both test sets: GU-MLT-LT
and AVAYA. Three other teams, namely teragram,
BOUNCE and KLUE, were ranked in the top-3 in at
least one subtask and test set.
6 Discussion
We have seen that most participants restricted them-
selves to the provided data and submitted con-
strained systems. Indeed, the best systems for each
of the two subtasks and for each of the two testing
datasets were constrained systems; of course, this
does not mean that additional data would not be use-
ful. Curiously, in some cases where a team submit-
ted a constrained and unconstrained run, the uncon-
strained run actually performed worse.
Not surprisingly, most systems were supervised;
there were only five semi-supervised systems, and
there was only one unsupervised system. One ad-
ditional team declared their system as unsupervised
since it was not making use of the training data; we
still classified it as supervised though since it did use
supervision ? in the form of manual rules.
Most participants predicted all three labels (posi-
tive, negative and neutral), even though some partic-
ipants opted for not predicting neutral, which made
some sense since the final F1-score was averaged
over the positive and the negative predictions only.
The most popular classifiers included SVM, Max-
Ent, linear classifier, Naive Bayes; in some cases,
manual rules or ensembles of classifiers were used.
A variety of features were used, including word-
related (e.g., words, stems, n-grams, word clus-
ters), word-shape (e.g., punctuation, capitalization),
syntactic (e.g., POS tags, dependency relations),
Twitter-specific (e.g., repeated characters, emoti-
cons, URLs, hashtags, slang, abbreviations), and
sentiment-related (e.g., negation); one team also
used discourse relations. Almost all participants re-
lied heavily of various sentiment lexicons, the most
popular ones being MPQA and SentiWordNet, as
well as AFINN and Bing Liu?s Opinion Lexicon;
some participants used their own lexicons ? preex-
isting or built from the provided data.
Given that Twitter messages are noisy, most par-
ticipants did some preprocessing, including tok-
enization, stemming, lemmatization, stopword re-
moval, normalization/removal of URLs, hashtags,
users, slang, emoticons, repeated vowels, punctua-
tion; some even did pronoun resolution.
7 Conclusion
We have described a new task that entered SemEval-
2013: task 2 on Sentiment Analysis on Twitter. The
task has attracted a very high number of participants:
149 submissions from 44 teams.
We believe that the datasets that we have created
as part of the task and which we have released to the
community5 under a Creative Commons Attribution
3.0 Unported License,6 will be found useful by re-
searchers beyond SemEval.
Acknowledgments
The authors would like to thank Kathleen McKeown
for her insight in creating the Amazon Mechanical
Turk annotation task.
Funding for the Amazon Mechanical Turk anno-
tations was provided by the JHU Human Language
Technology Center of Excellence and by the Of-
fice of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab. All
statements of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
5http://www.cs.york.ac.uk/semeval-2013/task2/
6http://creativecommons.org/licenses/by/3.0/
319
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Nicoletta Calzolari (chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh International Conference
on Language Resources and Evaluation, LREC ?10,
pages 2200?2204, Valletta, Malta.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer, and
Ricard Gavalda`. 2011. Detecting sentiment change in
Twitter streaming data. Journal of Machine Learning
Research - Proceedings Track, 17:5?11.
Tao Chen and Min-Yen Kan. 2012. Creating a live, pub-
lic short message service corpus: the NUS SMS cor-
pus. Language Resources and Evaluation, pages 1?
37.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
Twitter and Amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 107?116, Upp-
sala, Sweden.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The Good
the Bad and the OMG! In Lada A. Adamic, Ricardo A.
Baeza-Yates, and Scott Counts, editors, Proceedings of
the Fifth International Conference on Weblogs and So-
cial Media, ICWSM? 11, pages 538?541, Barcelona,
Spain.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In William W. Cohen and
Samuel Gosling, editors, Proceedings of the Fourth
International Conference on Weblogs and Social
Media, ICWSM ?10, Washington, DC, USA.
Alexander Pak and Patrick Paroubek. 2010. Twitter
based system: Using Twitter for disambiguating senti-
ment ambiguous adjectives. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 436?439, Los Angeles, CA, USA.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an exper-
imental study. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?11, pages 1524?1534, Edinburgh, United
Kingdom.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?12, pages 1104?1112, Beijing, China.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about po-
litical sentiment. In William W. Cohen and Samuel
Gosling, editors, Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, pages 178?185, Washington, DC, USA.
The AAAI Press.
Julio Villena-Roma?n, Sara Lana-Serrano, Euge-
nio Mart??nez-Ca?mara, and Jose? Carlos Gonza?lez
Cristo?bal. 2013. TASS - Workshop on Sentiment
Analysis at SEPLN. Procesamiento del Lenguaje
Natural, 50:37?44.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
320
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73?80,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 9: Sentiment Analysis in Twitter
Sara Rosenthal
Columbia University
sara@cs.columbia.edu
Preslav Nakov
Qatar Computing Research Institute
pnakov@qf.org.qa
Alan Ritter
Carnegie Mellon University
rittera@cs.cmu.edu
Veselin Stoyanov
Johns Hopkins University
ves@cs.jhu.edu
Abstract
We describe the Sentiment Analysis in
Twitter task, ran as part of SemEval-2014.
It is a continuation of the last year?s task
that ran successfully as part of SemEval-
2013. As in 2013, this was the most popu-
lar SemEval task; a total of 46 teams con-
tributed 27 submissions for subtask A (21
teams) and 50 submissions for subtask B
(44 teams). This year, we introduced three
new test sets: (i) regular tweets, (ii) sarcas-
tic tweets, and (iii) LiveJournal sentences.
We further tested on (iv) 2013 tweets, and
(v) 2013 SMS messages. The highest F1-
score on (i) was achieved by NRC-Canada
at 86.63 for subtask A and by TeamX at
70.96 for subtask B.
1 Introduction
In the past decade, new forms of communica-
tion have emerged and have become ubiquitous
through social media. Microblogs (e.g., Twitter),
Weblogs (e.g., LiveJournal) and cell phone mes-
sages (SMS) are often used to share opinions and
sentiments about the surrounding world, and the
availability of social content generated on sites
such as Twitter creates new opportunities to au-
tomatically study public opinion.
Working with these informal text genres
presents new challenges for natural language pro-
cessing beyond those encountered when work-
ing with more traditional text genres such as
newswire. The language in social media is very
informal, with creative spelling and punctuation,
misspellings, slang, new words, URLs, and genre-
specific terminology and abbreviations, e.g., RT
for re-tweet and #hashtags
1
.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
Hashtags are a type of tagging for Twitter messages.
Moreover, tweets and SMS messages are short:
a sentence or a headline rather than a document.
How to handle such challenges so as to automat-
ically mine and understand people?s opinions and
sentiments has only recently been the subject of
research (Jansen et al., 2009; Barbosa and Feng,
2010; Bifet et al., 2011; Davidov et al., 2010;
O?Connor et al., 2010; Pak and Paroubek, 2010;
Tumasjan et al., 2010; Kouloumpis et al., 2011).
Several corpora with detailed opinion and sen-
timent annotation have been made freely avail-
able, e.g., the MPQA newswire corpus (Wiebe et
al., 2005), the movie reviews corpus (Pang et al.,
2002), or the restaurant and laptop reviews cor-
pora that are part of this year?s SemEval Task 4
(Pontiki et al., 2014). These corpora have proved
very valuable as resources for learning about the
language of sentiment in general, but they do not
focus on tweets. While some Twitter sentiment
datasets were created prior to SemEval-2013, they
were either small and proprietary, such as the i-
sieve corpus (Kouloumpis et al., 2011) or focused
solely on message-level sentiment.
Thus, the primary goal of our SemEval task is
to promote research that will lead to better un-
derstanding of how sentiment is conveyed in So-
cial Media. Toward that goal, we created the Se-
mEval Tweet corpus as part of our inaugural Sen-
timent Analysis in Twitter Task, SemEval-2013
Task 2 (Nakov et al., 2013). It contains tweets
and SMS messages with sentiment expressions an-
notated with contextual phrase-level and message-
level polarity. This year, we extended the corpus
by adding new tweets and LiveJournal sentences.
Another interesting phenomenon that has been
studied in Twitter is the use of the #sarcasm hash-
tag to indicate that a tweet should not be taken lit-
erally (Gonz?alez-Ib?a?nez et al., 2011; Liebrecht et
al., 2013). In fact, sarcasm indicates that the mes-
sage polarity should be flipped. With this in mind,
this year, we also evaluate on sarcastic tweets.
73
In the remainder of this paper, we first describe
the task, the dataset creation process and the eval-
uation methodology. We then summarize the char-
acteristics of the approaches taken by the partici-
pating systems, and we discuss their scores.
2 Task Description
As SemEval-2013 Task 2, we included two sub-
tasks: an expression-level subtask and a message-
level subtask. Participants could choose to partici-
pate in either or both. Below we provide short de-
scriptions of the objectives of these two subtasks.
Subtask A: Contextual Polarity Disambiguation
Given a message containing a marked in-
stance of a word or a phrase, determine
whether that instance is positive, negative or
neutral in that context. The instance bound-
aries were provided: this was a classification
task, not an entity recognition task.
Subtask B: Message Polarity Classification
Given a message, decide whether it is of
positive, negative, or neutral sentiment.
For messages conveying both positive and
negative sentiment, the stronger one is to be
chosen.
Each participating team was allowed to submit
results for two different systems per subtask: one
constrained, and one unconstrained. A constrained
system could only use the provided data for train-
ing, but it could also use other resources such as
lexicons obtained elsewhere. An unconstrained
system could use any additional data as part of
the training process; this could be done in a super-
vised, semi-supervised, or unsupervised fashion.
Note that constrained/unconstrained refers to
the data used to train a classifier. For example,
if other data (excluding the test data) was used to
develop a sentiment lexicon, and the lexicon was
used to generate features, the system would still
be constrained. However, if other data (excluding
the test data) was used to develop a sentiment lexi-
con, and this lexicon was used to automatically la-
bel additional Tweet/SMS messages and then used
with the original data to train the classifier, then
such a system would be considered unconstrained.
3 Datasets
In this section, we describe the process of collect-
ing and annotating the 2014 testing tweets, includ-
ing the sarcastic ones, and LiveJournal sentences.
Corpus Positive Negative Objective
/ Neutral
Twitter2013-train 5,895 3,131 471
Twitter2013-dev 648 430 57
Twitter2013-test 2,734 1,541 160
SMS2013-test 1,071 1,104 159
Twitter2014-test 1,807 578 88
Twitter2014-sarcasm 82 37 5
LiveJournal2014-test 660 511 144
Table 1: Dataset statistics for Subtask A.
3.1 Datasets Used
For training and development, we released the
Twitter train/dev/test datasets from SemEval-2013
task 2, as well as the SMS test set, which uses mes-
sages from the NUS SMS corpus (Chen and Kan,
2013), which we annotated for sentiment in 2013.
We further added a new 2014 Twitter test set,
as well as a small set of tweets that contained
the #sarcasm hashtag to determine how sarcasm
affects the tweet polarity. Finally, we included
sentences from LiveJournal in order to determine
how systems trained on Twitter perform on other
sources. The statistics for each dataset and for
each subtask are shown in Tables 1 and 2.
Corpus Positive Negative Objective
/ Neutral
Twitter2013-train 3,662 1,466 4,600
Twitter2013-dev 575 340 739
Twitter2013-test 1,572 601 1,640
SMS2013-test 492 394 1,207
Twitter2014-test 982 202 669
Twitter2014-sarcasm 33 40 13
LiveJournal2014-test 427 304 411
Table 2: Dataset statistics for Subtask B.
3.2 Annotation
We annotated the new tweets as in 2013: by iden-
tifying tweets from popular topics that contain
sentiment-bearing words by using SentiWordNet
(Baccianella et al., 2010) as a filter. We altered the
annotation task for the sarcastic tweets, displaying
them to the Mechanical Turk annotators without
the #sarcasm hashtag; the Turkers had to deter-
mine whether the tweet is sarcastic on their own.
Moreover, we asked Turkers to indicate the degree
of sarcasm as (a) definitely sarcastic, (b) probably
sarcastic, and (c) not sarcastic.
As in 2013, we combined the annotations using
intersection, where a word had to appear in 2/3
of the annotations to be accepted. An annotated
example from each source is shown in Table 3.
74
Source Example Polarity
Twitter Why would you [still]- wear shorts when it?s this cold?! I [love]+ how Britain see?s a
bit of sun and they?re [like ?OOOH]+ LET?S STRIP!?
positive
SMS [Sorry]- I think tonight [cannot]- and I [not feeling well]- after my rest. negative
LiveJournal [Cool]+ posts , dude ; very [colorful]+ , and [artsy]+ . positive
Twitter Sarcasm [Thanks]+ manager for putting me on the schedule for Sunday negative
Table 3: Example of polarity for each source of messages. The target phrases are marked in [. . .], and
are followed by their polarity; the sentence-level polarity is shown in the last column.
3.3 Tweets Delivery
We did not deliver the annotated tweets to the par-
ticipants directly; instead, we released annotation
indexes, a list of corresponding Twitter IDs, and
a download script that extracts the correspond-
ing tweets via the Twitter API.
2
We provided the
tweets in this manner in order to ensure that Twit-
ter?s terms of service are not violated. Unfor-
tunately, due to this restriction, the task partici-
pants had access to different number of training
tweets depending on when they did the download-
ing. This varied between a minimum of 5,215
tweets and the full set of 10,882 tweets. On av-
erage the teams were able to collect close to 9,000
tweets; for teams that did not participate in 2013,
this was about 8,500. The difference in training
data size did not seem to have had a major impact.
In fact, the top two teams in subtask B (coooolll
and TeamX) trained on less than 8,500 tweets.
4 Scoring
The participating systems were required to per-
form a three-way classification for both subtasks.
A particular marked phrase (for subtask A) or an
entire message (for subtask B) was to be classi-
fied as positive, negative or objective/neutral. We
scored the systems by computing a score for pre-
dicting positive/negative phrases/messages. For
instance, to compute positive precision, p
pos
, we
find the number of phrases/messages that a sys-
tem correctly predicted to be positive, and we di-
vide that number by the total number it predicted
to be positive. To compute positive recall, r
pos
,
we find the number of phrases/messages correctly
predicted to be positive and we divide that number
by the total number of positives in the gold stan-
dard. We then calculate F1-score for the positive
class as follows F
pos
=
2(p
pos
+r
pos
)
p
pos
?r
pos
. We carry
out a similar computation for F
neg
, for the nega-
tive phrases/messages. The overall score is then
F = (F
pos
+ F
neg
)/2.
2
https://dev.twitter.com
We used the two test sets from 2013 and the
three from 2014, which we combined into one test
set and we shuffled to make it hard to guess which
set a sentence came from. This guaranteed that
participants would submit predictions for all five
test sets. It also allowed us to test how well sys-
tems trained on standard tweets generalize to sar-
castic tweets and to LiveJournal sentences, with-
out the participants putting extra efforts into this.
The participants were also not informed about the
source the extra test sets come from.
We provided the participants with a scorer that
outputs the overall score F and a confusion matrix
for each of the five test sets.
5 Participants and Results
The results are shown in Tables 4 and 5, and the
team affiliations are shown in Table 6. Tables 4
and 5 contain results on the two progress test sets
(tweets and SMS messages), which are the official
test sets from the 2013 edition of the task, and on
the three new official 2014 testsets (tweets, tweets
with sarcasm, and LiveJournal). The tables fur-
ther show macro- and micro-averaged results over
the 2014 datasets. There is an index for each re-
sult showing the relative rank of that result within
the respective column. The participating systems
are ranked by their score on the Twitter-2014 test-
set, which is the official ranking for the task; all
remaining rankings are secondary.
As we mentioned above, the participants were
not told that the 2013 test sets would be included
in the big 2014 test set, so that they do not over-
tune their systems on them. However, the 2013
test sets were made available for development, but
it was explicitly forbidden to use them for training.
Still, some participants did not notice this restric-
tion, which resulted in their unusually high scores
on Twitter2013-test; we did our best to identify
all such cases, and we asked the authors to submit
corrected runs. The tables mark such resubmis-
sions accordingly.
75
Most of the submissions were constrained, with
just a few unconstrained: 7 out of 27 for subtask
A, and 8 out of 50 for subtask B. In any case, the
best systems were constrained. Some teams par-
ticipated with both a constrained and an uncon-
strained system, but the unconstrained system was
not always better than the constrained one: some-
times it was worse, sometimes it performed the
same. Thus, we decided to produce a single rank-
ing, including both constrained and unconstrained
systems, where we mark the latter accordingly.
5.1 Subtask A
Table 4 shows the results for subtask A, which at-
tracted 27 submissions from 21 teams. There were
seven unconstrained submissions: five teams sub-
mitted both a constrained and an unconstrained
run, and two teams submitted an unconstrained
run only. The best systems were constrained. All
participating systems outperformed the majority
class baseline by a sizable margin.
5.2 Subtask B
The results for subtask B are shown in Table 5.
The subtask attracted 50 submissions from 44
teams. There were eight unconstrained submis-
sions: six teams submitted both a constrained and
an unconstrained run, and two teams submitted an
unconstrained run only. As for subtask A, the best
systems were constrained. Again, all participating
systems outperformed the majority class baseline;
however, some systems were very close to it.
6 Discussion
Overall, we observed similar trends as in
SemEval-2013 Task 2. Almost all systems used
supervised learning. Most systems were con-
strained, including the best ones in all categories.
As in 2013, we observed several cases of a team
submitting a constrained and an unconstrained run
and the constrained run performing better.
It is unclear why unconstrained systems did not
outperform constrained ones. It could be because
participants did not use enough external data or
because the data they used was too different from
Twitter or from our annotation method. Or it could
be due to our definition of unconstrained, which
labels as unconstrained systems that use additional
tweets directly, but considers unconstrained those
that use additional tweets to build sentiment lexi-
cons and then use these lexicons.
As in 2013, the most popular classifiers were
SVM, MaxEnt, and Naive Bayes. Moreover, two
submissions used deep learning, coooolll (Harbin
Institute of Technology) and ThinkPositive (IBM
Research, Brazil), which were ranked second and
tenth on subtask B, respectively.
The features used were quite varied, includ-
ing word-based (e.g., word and character n-
grams, word shapes, and lemmata), syntactic, and
Twitter-specific such as emoticons and abbrevia-
tions. The participants still relied heavily on lex-
icons of opinion words, the most popular ones
being the same as in 2013: MPQA, SentiWord-
Net and Bing Liu?s opinion lexicon. Popular this
year was also the NRC lexicon (Mohammad et
al., 2013), created by the best-performing team in
2013, which is top-performing this year as well.
Preprocessing of tweets was still a popular tech-
nique. In addition to standard NLP steps such
as tokenization, stemming, lemmatization, stop-
word removal and POS tagging, most teams ap-
plied some kind of Twitter-specific processing
such as substitution/removal of URLs, substitu-
tion of emoticons, word normalization, abbrevi-
ation lookup, and punctuation removal. Finally,
several of the teams used Twitter-tuned NLP tools
such as part of speech and named entity taggers
(Gimpel et al., 2011; Ritter et al., 2011).
The similarity of preprocessing techniques,
NLP tools, classifiers and features used in 2013
and this year is probably partially due to many
teams participating in both years. As Table 6
shows, 18 out of the 46 teams are returning teams.
Comparing the results on the progress Twit-
ter test in 2013 and 2014, we can see that NRC-
Canada, the 2013 winner for subtask A, have
now improved their F1 score from 88.93 to 90.14,
which is the 2014 best score. The best score on the
Progress SMS in 2014 of 89.31 belongs to ECNU;
this is a big jump compared to their 2013 score of
76.69, but it is less compared to the 2013 best of
88.37 achieved by GU-MLT-LT. For subtask B, on
the Twitter progress testset, the 2013 winner NRC-
Canada improves their 2013 result from 69.02 to
70.75, which is the second best in 2014; the win-
ner in 2014, TeamX, achieves 72.12. On the SMS
progress test, the 2013 winner NRC-Canada im-
proves its F1 score from 68.46 to 70.28. Overall,
we see consistent improvements on the progress
testset for both subtasks: 0-1 and 2-3 points abso-
lute for subtasks A and B, respectively.
76
Uncon- 2013: Progress 2014: Official 2014: Average
# System strain.? Tweet SMS Tweet Tweet Live- Macro Micro
sarcasm Journal
1 NRC-Canada 90.14
1
88.03
4
86.63
1
77.13
5
85.49
2
83.08
2
85.61
1
2 SentiKLUE 90.11
2
85.16
8
84.83
2
79.32
3
85.61
1
83.25
1
85.15
2
3 CMUQ-Hybrid
?
88.94
4
87.98
5
84.40
3
76.99
6
84.21
3
81.87
3
84.05
3
4 CMU-Qatar
?
89.85
3
88.08
3
83.45
4
78.07
4
83.89
5
81.80
4
83.56
4
5 ECNU X 87.29
6
89.26
2
82.93
5
73.71
8
81.69
7
79.44
7
81.85
6
6 ECNU 87.28
7
89.31
1
82.67
6
73.71
9
81.67
8
79.35
8
81.75
7
7 Think Positive X 88.06
5
87.65
6
82.05
7
76.74
7
80.90
12
79.90
6
81.15
9
8 Kea
?
84.83
10
84.14
10
81.22
8
65.94
17
81.16
11
76.11
13
80.70
10
9 Lt 3 86.28
8
85.26
7
81.02
9
70.76
13
80.44
13
77.41
11
80.33
13
10 senti.ue 84.05
11
78.72
16
80.54
10
82.75
1
81.90
6
81.73
5
81.47
8
11 LyS 85.69
9
81.44
12
79.92
11
71.67
10
83.95
4
78.51
10
82.21
5
12 UKPDIPF 80.45
15
79.05
14
79.67
12
65.63
18
81.42
9
75.57
14
80.33
11
13 UKPDIPF X 80.45
16
79.05
15
79.67
13
65.63
19
81.42
10
75.57
15
80.33
12
14 TJP 81.13
14
84.41
9
79.30
14
71.20
12
78.27
15
76.26
12
78.39
15
15 SAP-RI 80.32
17
80.26
13
77.26
15
70.64
14
77.68
18
75.19
17
77.32
16
16 senti.ue
?
X 83.80
12
82.93
11
77.07
16
80.02
2
79.70
14
78.93
9
78.83
14
17 SAIL 78.47
18
74.46
20
76.89
17
65.56
20
70.62
22
71.02
21
72.57
21
18 columbia nlp

81.50
13
74.55
19
76.54
18
61.76
22
78.19
16
72.16
19
77.11
18
19 IIT-Patna 76.54
20
75.99
18
76.43
19
71.43
11
77.99
17
75.28
16
77.26
17
20 Citius X 76.59
19
69.31
21
75.21
20
68.40
15
75.82
20
73.14
18
75.38
19
21 Citius 74.71
21
61.44
25
73.03
21
65.18
21
71.64
21
69.95
22
71.90
22
22 IITPatna 70.91
23
77.04
17
72.25
22
66.32
16
76.03
19
71.53
20
74.45
20
23 SU-sentilab 74.34
22
62.58
24
68.26
23
53.31
25
69.53
23
63.70
24
68.59
23
24 Univ. Warwick
?
62.25
26
60.12
26
67.28
24
58.08
24
64.89
25
63.42
25
65.48
25
25 Univ. Warwick
?
X 64.91
25
63.01
23
67.17
25
60.59
23
67.46
24
65.07
23
67.14
24
26 DAEDALUS 67.42
24
63.92
22
60.98
26
45.27
27
61.01
26
55.75
26
60.50
26
27 DAEDALUS X 61.95
27
55.97
27
58.11
27
49.19
26
58.65
27
55.32
27
58.17
27
Majority baseline 38.1 31.5 42.2 39.8 33.4
Table 4: Results for subtask A. The
?
indicates system resubmissions (because they initially trained on
Twitter2013-test), and the

indicates a system that includes a task co-organizer as a team member. The
systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets
are indicated with a subscript. The last two columns show macro- and micro-averaged results across the
three 2014 test datasets.
Finally, note that for both subtasks, the best sys-
tems on the Twitter-2014 dataset are those that per-
formed best on the 2013 progress Twitter dataset:
NRC-Canada for subtask A, and TeamX (Fuji Xe-
rox Co., Ltd.) for subtask B.
It is interesting to note that the best results
for Twitter2014-test are lower than those for
Twitter2013-test for both subtask A (86.63 vs.
90.14) and subtask B (70.96 vs 72.12). This is
so despite the baselines for Twitter2014-test be-
ing higher than those for Twitter2013-test: 42.2 vs.
38.1 for subtask A, and 34.6 vs. 29.2 for subtask
B. Most likely, having access to Twitter2013-test
at development time, teams have overfitted on it. It
could be also the case that some of the sentiment
dictionaries that were built in 2013 have become
somewhat outdated by 2014.
Finally, note that while some teams such as
NRC-Canada performed well across all test sets,
other such as TeamX, which used a weighting
scheme tuned specifically for class imbalances in
tweets, were only strong on Twitter datasets.
7 Conclusion
We have described the data, the experimental
setup and the results for SemEval-2014 Task 9.
As in 2013, our task was the most popular one at
SemEval-2014, attracting 46 participating teams:
21 in subtask A (27 submissions) and 44 in sub-
task B (50 submissions).
We introduced three new test sets for 2014: an
in-domain Twitter dataset, an out-of-domain Live-
Journal test set, and a dataset of tweets contain-
ing sarcastic content. While the performance on
the LiveJournal test set was mostly comparable
to the in-domain Twitter test set, for most teams
there was a sharp drop in performance for sarcas-
tic tweets, highlighting better handling of sarcas-
tic language as one important direction for future
work in Twitter sentiment analysis.
We plan to run the task again in 2015 with the
inclusion of a new sub-evaluation on detecting sar-
casm with the goal of stimulating research in this
area; we further plan to add one more test domain.
77
Uncon- 2013: Progress 2014: Official 2014: Average
# System strain.? Tweet SMS Tweet Tweet Live- Macro Micro
sarcasm Journal
1 TeamX 72.12
1
57.36
26
70.96
1
56.50
3
69.44
15
65.63
3
69.99
5
2 coooolll 70.40
3
67.68
2
70.14
2
46.66
24
72.90
5
63.23
12
70.51
2
3 RTRGO 69.10
5
67.51
3
69.95
3
47.09
23
72.20
6
63.08
13
70.15
3
4 NRC-Canada 70.75
2
70.28
1
69.85
4
58.16
1
74.84
1
67.62
1
71.37
1
5 TUGAS 65.64
13
62.77
11
69.00
5
52.87
12
69.79
13
63.89
6
68.84
8
6 CISUC KIS
?
67.56
8
65.90
6
67.95
6
55.49
5
74.46
2
65.97
2
70.02
4
7 SAIL 66.80
11
56.98
28
67.77
7
57.26
2
69.34
17
64.79
4
68.06
10
8 SWISS-CHOCOLATE 64.81
18
66.43
5
67.54
8
49.46
16
73.25
4
63.42
10
69.15
6
9 Synalp-Empathic 63.65
23
62.54
12
67.43
9
51.06
15
71.75
9
63.41
11
68.57
9
10 Think Positive X 68.15
7
63.20
9
67.04
10
47.85
21
66.96
24
60.62
18
66.47
15
11 SentiKLUE 69.06
6
67.40
4
67.02
11
43.36
30
73.99
3
61.46
14
68.94
7
12 JOINT FORCES X 66.61
12
62.20
13
66.79
12
45.40
26
70.02
12
60.74
17
67.39
12
13 AMI ERIC 70.09
4
60.29
20
66.55
13
48.19
20
65.32
26
60.02
21
65.58
20
14 AUEB 63.92
21
64.32
8
66.38
14
56.16
4
70.75
11
64.43
5
67.71
11
15 CMU-Qatar
?
65.11
17
62.95
10
65.53
15
40.52
38
65.63
25
57.23
27
64.87
24
16 Lt 3 65.56
14
64.78
7
65.47
16
47.76
22
68.56
20
60.60
19
66.12
17
17 columbia nlp

64.60
19
59.84
21
65.42
17
40.02
40
68.79
19
58.08
25
65.96
19
18 LyS 66.92
10
60.45
19
64.92
18
42.40
33
69.79
14
59.04
22
66.10
18
19 NILC USP 65.39
15
61.35
16
63.94
19
42.06
34
69.02
18
58.34
24
65.21
21
20 senti.ue 67.34
9
59.34
23
63.81
20
55.31
6
71.39
10
63.50
7
66.38
16
21 UKPDIPF 60.65
29
60.56
17
63.77
21
54.59
7
71.92
7
63.43
8
66.53
13
22 UKPDIPF X 60.65
30
60.56
18
63.77
22
54.59
8
71.92
8
63.43
9
66.53
14
23 SU-FMI
?
60.96
28
61.67
15
63.62
23
48.34
19
68.24
21
60.07
20
64.91
23
24 ECNU 62.31
27
59.75
22
63.17
24
51.43
14
69.44
16
61.35
15
65.17
22
25 ECNU X 63.72
22
56.73
29
63.04
25
49.33
17
64.08
31
58.82
23
63.04
27
26 Rapanakis 58.52
32
54.02
35
63.01
26
44.69
27
59.71
37
55.80
31
61.28
32
27 Citius X 63.25
24
58.28
24
62.94
27
46.13
25
64.54
29
57.87
26
63.06
26
28 CMUQ-Hybrid
?
63.22
25
61.75
14
62.71
28
40.95
37
65.14
27
56.27
30
63.00
28
29 Citius 62.53
26
57.69
25
61.92
29
41.00
36
62.40
33
55.11
33
61.51
31
30 KUNLPLab 58.12
33
55.89
31
61.72
30
44.60
28
63.77
32
56.70
29
62.00
29
31 senti.ue
?
X 65.21
16
56.16
30
61.47
31
54.09
9
68.08
22
61.21
16
63.71
25
32 UPV-ELiRF 63.97
20
55.36
33
59.33
32
37.46
42
64.11
30
53.63
37
60.49
33
33 USP Biocom 58.05
34
53.57
36
59.21
33
43.56
29
67.80
23
56.86
28
61.96
30
34 DAEDALUS X 58.94
31
54.96
34
57.64
34
35.26
44
60.99
35
51.30
39
58.26
35
35 IIT-Patna 52.58
40
51.96
37
57.25
35
41.33
35
60.39
36
52.99
38
57.97
36
36 DejaVu 57.43
36
55.57
32
57.02
36
42.46
32
64.69
28
54.72
34
59.46
34
37 GPLSI 57.49
35
46.63
42
56.06
37
53.90
10
57.32
41
55.76
32
56.47
37
38 BUAP 56.85
37
44.27
44
55.76
38
51.52
13
53.94
44
53.74
36
54.97
39
39 SAP-RI 50.18
44
49.00
41
55.47
39
48.64
18
57.86
40
53.99
35
56.17
38
40 UMCC DLSI Sem 51.96
41
50.01
38
55.40
40
42.76
31
53.12
45
50.43
40
54.20
42
41 IBM EG 54.51
38
46.62
43
52.26
41
34.14
46
59.24
38
48.55
43
54.34
41
42 Alberta 53.85
39
49.05
40
52.06
42
40.40
39
52.38
46
48.28
44
51.85
44
43 lsis lif 46.38
46
38.56
47
52.02
43
34.64
45
61.09
34
49.25
41
54.90
40
44 SU-sentilab 50.17
45
49.60
39
49.52
44
31.49
47
55.11
42
45.37
47
51.09
45
45 SINAI 50.59
42
57.34
27
49.50
45
31.15
49
58.33
39
46.33
46
52.26
43
46 IITPatna 50.32
43
40.56
46
48.22
46
36.73
43
54.68
43
46.54
45
50.29
46
47 Univ. Warwick 39.17
48
29.50
49
45.56
47
39.77
41
39.60
49
41.64
48
43.19
48
48 UMCC DLSI Graph 43.24
47
36.66
48
45.49
48
53.15
11
47.81
47
48.82
42
46.56
47
49 Univ. Warwick X 34.23
50
24.63
50
45.11
49
31.40
48
29.34
50
35.28
49
38.88
49
50 DAEDALUS 36.57
49
40.86
45
33.03
50
28.96
50
40.83
48
34.27
50
35.81
50
Majority baseline 29.2 19.0 34.6 27.7 27.2
Table 5: Results for subtask B. The
?
indicates system resubmissions (because they initially trained on
Twitter2013-test), and the

indicates a system that includes a task co-organizer as a team member. The
systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets
are indicated with a subscript. The last two columns show macro- and micro-averaged results across the
three 2014 test datasets.
In the 2015 edition of the task, we might also
remove the constrained/unconstrained distinction.
Finally, as there are multiple opinions about a
topic in Twitter, we would like to focus on detect-
ing the sentiment trend towards a topic.
Acknowledgements
We would like to thank Kathleen McKeown and
Smaranda Muresan for funding the 2014 Twitter
test sets. We also thank the anonymous reviewers.
78
Subtasks Team Affiliation 2013?
B Alberta University of Alberta
B AMI ERIC AMI Software R&D and Universit?e de Lyon (ERIC LYON 2) yes
B AUEB Athens University of Economics and Business yes
B BUAP Benem?erita Universidad Aut?onoma de Puebla
B CISUC KIS University of Coimbra
A, B Citius University of Santiago de Compostela
A, B CMU-Qatar Carnegie Mellon University, Qatar
A, B CMUQ-Hybrid Carnegie Mellon University, Qatar (different from the above)
A, B columbia nlp Columbia University yes
B cooolll Harbin Institute of Technology
A, B DAEDALUS Daedalus
B DejaVu Indian Institute of Technology, Kanpur
A, B ECNU East China Normal University yes
B GPLSI University of Alicante
B IBM EG IBM Egypt
A, B IITPatna Indian Institute of Technology, Patna
A, B IIT-Patna Indian Institute of Technology, Patna (different from the above)
B JOINT FORCES Zurich University of Applied Sciences
A Kea York University, Toronto yes
B KUNLPLab Koc? University
B lsis lif Aix-Marseille University yes
A, B Lt 3 Ghent University
A, B LyS Universidade da Coru?na
B NILC USP University of S?ao Paulo yes
A, B NRC-Canada National Research Council Canada yes
B Rapanakis Stamatis Rapanakis
B RTRGO Retresco GmbH and University of Gothenburg yes
A, B SAIL Signal Analysis and Interpretation Laboratory yes
A, B SAP-RI SAP Research and Innovation
A, B senti.ue Universidade de
?
Evora yes
A, B SentiKLUE Friedrich-Alexander-Universit?at Erlangen-N?urnberg yes
B SINAI University of Ja?en yes
B SU-FMI Sofia University
A, B SU-sentilab Sabanci University yes
B SWISS-CHOCOLATE ETH Zurich
B Synalp-Empathic University of Lorraine
B TeamX Fuji Xerox Co., Ltd.
A, B Think Positive IBM Research, Brazil
A TJP University of Northumbria at Newcastle Upon Tyne yes
B TUGAS Instituto de Engenharia de Sistemas e Computadores, yes
Investigac??ao e Desenvolvimento em Lisboa
A, B UKPDIPF Ubiquitous Knowledge Processing Lab
B UMCC DLSI Graph Universidad de Matanzas and Univarsidad de Alicante yes
B UMCC DLSI Sem Universidad de Matanzas and Univarsidad de Alicante (different from above) yes
A, B Univ. Warwick University of Warwick
B UPV-ELiRF Universitat Polit`ecnica de Val`encia
B USP Biocom University of S?ao Paulo and Federal University of S?ao Carlos
Table 6: Participating teams, their affiliations, subtasks they have taken part in, and an indication about
whether the team participated in SemEval-2013 Task 2.
79
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordNet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation,
LREC ?10, Valletta, Malta.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on Twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 36?44, Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavald`a. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine Learning Research, Proceedings Track, 17:5?
11.
Tao Chen and Min-Yen Kan. 2013. Creating a
live, public short message service corpus: the NUS
SMS corpus. Language Resources and Evaluation,
47(2):299?335.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcasm in Twitter
and Amazon. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, CoNLL ?10, pages 107?116, Uppsala, Sweden.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, ACL-HLT ?11, pages 42?
47, Portland, Oregon, USA.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in Twit-
ter: a closer look. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Short Pa-
pers, ACL-HLT ?11, pages 581?586, Portland, Ore-
gon, USA.
Bernard Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The
good the bad and the OMG! In Proceedings of
the Fifth International Conference on Weblogs and
Social Media, ICWSM ?11, Barcelona, Catalonia,
Spain.
Christine Liebrecht, Florian Kunneman, and Antal
Van den Bosch. 2013. The perfect solution for de-
tecting sarcasm in tweets #not. In Proceedings of
the 4th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 29?37, Atlanta, Georgia, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of tweets. In Proceed-
ings of the Seventh international workshop on Se-
mantic Evaluation Exercises, SemEval-2013, pages
321?327, Atlanta, Georgia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation, SemEval ?13, pages 312?320,
Atlanta, Georgia, USA.
Brendan O?Connor, Ramnath Balasubramanyan, Bryan
Routledge, and Noah Smith. 2010. From tweets
to polls: Linking text sentiment to public opinion
time series. In Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, Washington, DC, USA.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter based system: Using Twitter for disambiguating
sentiment ambiguous adjectives. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, SemEval ?10, pages 436?439, Uppsala, Swe-
den.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing - Volume 10, EMNLP ?02, pages
79?86.
Maria Pontiki, Harris Papageorgiou, Dimitrios Gala-
nis, Ion Androutsopoulos, John Pavlopoulos, and
Suresh Manandhar. 2014. SemEval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation, SemEval ?14, Dublin, Ireland.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1524?1534, Edinburgh,
Scotland, UK.
Andranik Tumasjan, Timm Sprenger, Philipp Sandner,
and Isabell Welpe. 2010. Predicting elections with
Twitter: What 140 characters reveal about politi-
cal sentiment. In Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, Washington, DC, USA.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
80
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 20?29,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
A Preliminary Study of Tweet Summarization using Information Extraction
Wei Xu, Ralph Grishman, Adam Meyers
Computer Science Department
New York University
New York, NY 10003, USA
{xuwei,grishman,meyers}@cs.nyu.edu
Alan Ritter
Computer Science and Engineering
University of Washington
Seattle, WA 98125, USA
aritter@cs.washington.edu
Abstract
Although the ideal length of summaries dif-
fers greatly from topic to topic on Twitter, pre-
vious work has only generated summaries of
a pre-fixed length. In this paper, we propose
an event-graph based method using informa-
tion extraction techniques that is able to cre-
ate summaries of variable length for different
topics. In particular, we extend the Pagerank-
like ranking algorithm from previous work to
partition event graphs and thereby detect fine-
grained aspects of the event to be summarized.
Our preliminary results show that summaries
created by our method are more concise and
news-worthy than SumBasic according to hu-
man judges. We also provide a brief survey of
datasets and evaluation design used in previ-
ous work to highlight the need of developing a
standard evaluation for automatic tweet sum-
marization task.
1 Introduction
Tweets contain a wide variety of useful information
from many perspectives about important events tak-
ing place in the world. The huge number of mes-
sages, many containing irrelevant and redundant in-
formation, quickly leads to a situation of informa-
tion overload. This motivates the need for automatic
summarization systems which can select a few mes-
sages for presentation to a user which cover the most
important information relating to the event without
redundancy and filter out irrelevant and personal in-
formation that is not of interest beyond the user?s
immediate social network.
Although there is much recent work focusing on
the task of multi-tweet summarization (Becker et al,
2011; Inouye and Kalita, 2011; Zubiaga et al, 2012;
Liu et al, 2011a; Takamura et al, 2011; Harabagiu
and Hickl, 2011; Wei et al, 2012), most previous
work relies only on surface lexical clues, redun-
dancy and social network specific signals (e.g. user
relationship), and little work has considered taking
limited advantage of information extraction tech-
niques (Harabagiu and Hickl, 2011) in generative
models. Because of the noise and redundancy in
social media posts, the performance of off-the-shelf
news-trained natural language process systems is de-
graded while simple term frequency is proven pow-
erful for summarizing tweets (Inouye and Kalita,
2011). A natural and interesting research question
is whether it is beneficial to extract named entities
and events in the tweets as has been shown for clas-
sic multi-document summarization (Li et al, 2006).
Recent progress on building NLP tools for Twitter
(Ritter et al, 2011; Gimpel et al, 2011; Liu et al,
2011b; Ritter et al, 2012; Liu et al, 2012) makes
it possible to investigate an approach to summariz-
ing Twitter events which is based on Information Ex-
traction techniques.
We investigate a graph-based approach which
leverages named entities, event phrases and their
connections across tweets. A similar idea has been
studied by Li et al (2006) to rank the salience
of event concepts in summarizing news articles.
However, the extreme redundancy and simplicity of
tweets allows us to explicitly split the event graph
into subcomponents that cover various aspects of the
initial event to be summarized to create comprehen-
20
Work Dataset (size of each clus-
ter)
System Output Evaluation Metrics
Inouye and
Kalita (2011)
trending topics (approxi-
mately 1500 tweets)
4 tweets ROUGE and human (over-
all quality comparing to
human summary)
Sharifi et al
(2010)
same as above 1 tweet same as above
Rosa et al
(2011)
segmented hashtag top-
ics by LDA and k-means
clustering (average 410
tweets)
1, 5, 10 tweets Precision@k (relevance to
topic)
Harabagiu and
Hickl (2011)
real-word event topics (a
minimum of 2500 tweets)
top tweets until a limit of
250 words was reached
human (coverage and co-
herence)
Liu et al
(2011a)
general topics and hash-
tag topics (average 1.7k
tweets)
same lengths as of the
human summary, vary
for each topic (about 2 or
3 tweets)
ROUGE and human (con-
tent coverage, grammat-
icality, non-redundancy,
referential clarity, focus)
Wei et al
(2012)
segmented hashtag top-
ics according to burstiness
(average 10k tweets)
10 tweets ROUGE, Precison/Recall
(good readability and rich
content)
Takamura et al
(2011)
specific soccer games
(2.8k - 5.2k tweets)
same lengths as the hu-
man summary, vary for
each topic (26 - 41
tweets)
ROUGE (considering
only content words)
Chakrabarti and
Punera (2011)
specific football games
(1.8k tweets)
10 - 70 tweets Precision@k (relevance to
topic)
Table 1: Summary of datasets and evaluation metrics used in several previous work on tweet summarization
sive and non-redundant summaries. Our work is the
first to use a Pagerank-like algorithm for graph parti-
tioning and ranking in the context of summarization,
and the first to generate tweet summaries of variable
length which is particularly important for tweet sum-
marization. Unlike news articles, the amount of in-
formation in a set of topically clustered tweets varies
greatly, from very repetitive to very discrete. For ex-
ample, the tweets about one album release can be
more or less paraphrases, while those about another
album by a popular singer may involve rumors and
release events etc. In the human study conducted by
Inouye and Kalita (2011), annotators strongly prefer
different numbers of tweets in a summary for dif-
ferent topics. However, most of the previous work
produced summaries of a pre-fixed length and has
no evaluation on conciseness. Liu et al (2011a)
and Takamura et al (2011) also noticed the ideal
length of summaries can be very different from topic
to topic, and had to use the length of human refer-
ence summaries to decide the length of system out-
puts, information which is not available in practice.
In contrast, we developed a system that is capable
of detecting fine-grained sub-events and generating
summaries with the proper number of representative
tweets accordingly for different topics.
Our experimental results show that with informa-
tion extraction it is possible to create more mean-
ingful and concise summaries. Tweets that contain
real-world events are usually more informative and
readable. Event-based summarization is especially
beneficial in this situation due to the fact that tweets
are short and self-contained with simple discourse
structure. The boundary of 140 characters makes it
efficient to extract semi-structured events with shal-
low natural language processing techniques and re-
21
Tweets (Date Created) Named Entity Event Phrases Date Mentioned
Nooooo.. Season premiere of Doctor Who is on
Sept 1 world wide and we?ll be at World Con
(8/22/2012)
doctor who,
world con
season, is on,
premiere
sept 1
(9/1/2012)
guess what I DON?T get to do tomorrow!
WATCH DOCTOR WHO (8/31/2012)
doctor who watch tomorrow
(9/1/2012)
As I missed it on Saturday, I?m now catching up
on Doctor Who (9/4/2012)
doctor who missed,
catching up
saturday
(9/1/2012)
Rumour: Nokia could announce two WP8 de-
vices on September 5 http://t.co/yZUwDFLV (via
@mobigyaan)
nokia, wp8 announce september 5
(9/5/2012)
Verizon and Motorola won?t let Nokia have all
the fun ; scheduling September 5th in New York
http://t.co/qbBlYnSl (8/19/2012)
nokia, verizon,
motorola,
new york
scheduling september 5th
(9/5/2012)
Don?t know if it?s excitement or rooting for the
underdog, but I am genuinely excited for Nokia
come Sept 5: http://t.co/UhV5SUMP (8/7/2012)
nokia rooting,
excited
sept 5
(9/5/2012)
Table 2: Event-related information extracted from tweets
duces the complexity of the relationship (or no re-
lationship) between events according to their co-
occurrence, resulting in differences in constructing
event graphs from previous work in news domain
(Li et al, 2006).
2 Issues in Current Research on Tweet
Summarization
The most serious problem in tweet summarization
is that there is no standard dataset, and conse-
quently no standard evaluation methodology. Al-
though there are more than a dozen recent works on
social media summarization, astonishingly, almost
each research group used a different dataset and a
different experiment setup. This is largely attributed
to the difficulty of defining the right granularity of a
topic in Twitter. In Table 1, we summarize the exper-
iment designs of several selective works. Regardless
of the differences, researchers generally agreed on :
? clustering tweets topically and temporally
? generating either a very short summary for a
focused topic or a long summary for large-size
clusters
? difficulty and necessity to generate summaries
of variable length for different topics
Although the need of variable-length summaries
have been raised in previous work, none has pro-
vide a good solution (Liu et al, 2011a; Takamura
et al, 2011; Inouye and Kalita, 2011). In this pa-
per, our focus is study the feasibility of generating
concise summaries of variable length and improv-
ing meaningfulness by using information extraction
techniques. We hope this study can provide new in-
sights on the task and help in developing a standard
evaluation in the future.
3 Approach
We first extract event information including named
entities and event phrases from tweets and construct
event graphs that represent the relationship between
them. We then rank and partition the events using
PageRank-like algorithms, and create summaries of
variable length for different topics.
3.1 Event Extraction from Tweets
As a first step towards summarizing popular events
discussed on Twitter, we need a way to identify
events from Tweets. We utilize several natural lan-
guage processing tools that specially developed for
noisy text to extract text phrases that bear essential
event information, including named entities (Ritter
et al, 2011), event-referring phrases (Ritter et al,
22
2012) and temporal expressions (Mani and Wilson,
2000). Both the named entity and event taggers uti-
lize Conditional Random Fields models (Lafferty,
2001) trained on annotated data, while the temporal
expression resolver uses a mix of hand-crafted and
machine-learned rules. Example event information
extracted from Tweets are presented in Table 2.
The self-contained nature of tweets allows effi-
cient extraction of event information without deep
analysis (e.g. co-reference resolution). On the other
hand, individual tweets are also very terse, often
lacking sufficient context to access the importance
of events. It is crucial to exploit the highly redun-
dancy in Twitter. Closely following previous work
by Ritter et al (2012), we group together sets of
topically and temporally related tweets, which men-
tion the same named entity and a temporal refer-
ence resolved to the same unique calendar date. We
also employ a statistical significance test to measure
strength of association between each named entity
and date, and thereby identify important events dis-
cussed widely among users with a specific focus,
such as the release of a new iPhone as opposed to in-
dividual users discussing everyday events involving
their phones. By discarding frequent but insignifi-
cant events, we can produce more meaningful sum-
maries about popular real-world events.
3.2 Event Graphs
Since tweets have simple discourse and are self-
contained, it is a reasonable assumption that named
entities and event phrases that co-occurred together
in a single tweet are very likely related. Given a col-
lection of tweets, we represent such connections by
a weighted undirected graph :
? Nodes: named entities and event phrases are
represented by nodes and treated indifferently.
? Edges: two nodes are connected by an undi-
rected edge if they co-occurred in k tweets, and
the weight of edge is k.
We find it helpful to merge named entities and
event phrases that have lexical overlap if they are fre-
quent but not the topic of the tweet cluster. For ex-
ample, ?bbc?, ?radio 1?, ?bbc radio 1? are combined
together in a set of tweets about a band. Figure 1
shows a very small toy example of event graph. In
the experiments of this paper, we also exclude the
edges with k < 2 to reduce noise in the data and
calculation cost.
Figure 1: A toy event graph example built from the three
sentences of the event ?Nokia - 9/5/2012? in Table 2
3.3 Event Ranking and Partitioning
Graph-based ranking algorithms are widely used in
automatic summarization to decide salience of con-
cepts or sentences based on global information re-
cursively drawn from the entire graph. We adapt the
PageRank-like algorithm used in TextRank (Mihal-
cea and Tarau, 2004) that takes into account edge
weights when computing the score associated with a
vertex in the graph.
Formally, let G = (V,E) be a undirected graph
with the set of vertices V and set of edges E, whereE is a subset of V ? V . For a given vertex Vi, letAd(Vi) be the set of vertices that adjacent to it. The
weight of the edge between Vi and Vj is denoted aswij , and wij = wji. The score of a vertex Vi is
defined as follows:S(Vi) = (1  d) + d? X
Vj2Ad(Vi)
wij ? S(Vj)P
Vk2Ad(Vj) wjk
where d is a damping factor that is usually set to 0.85
(Brin and Page, 1998), and this is the value we are
also using in our implementation.
23
Starting from arbitrary values assigned to each
node in the graph, the computation iterates until con-
vergence. Note that the final salience score of each
node is not affected by the choice of the initial val-
ues assigned to each node in the graph, but rather the
weights of edges.
In previous work computed scores are then used
directly to select text fractions for summaries (Li et
al., 2006). However, the redundancy and simplic-
ity of tweets allow further exploration into sub-event
detection by graph partitioning. The intuition is that
the correlations between named entities and event
phrases within same sub-events are much stronger
than between sub-events. This phenomena is more
obvious and clear in tweet than in news articles,
where events are more diverse and complicated re-
lated to each other given lengthy context.
As theoretically studied in local partitioning prob-
lem (Andersen et al, 2006), a good partition of the
graph can be obtained by separating high ranked ver-
tices from low ranked vertices, if the nodes in the
graph have ranks that are distinguishable. Utilizing
a similar idea, we show that a simple greedy algo-
rithm is efficient to find important sub-events and
generate useful summaries in our tasks. As shown
in Figure 2 and 3, the high ranked nodes (whose
scores are greater than 1, the average score of all
nodes in the graph) in tweet event graphs show the
divisions within a topic. We search for strongly con-
nected sub-graphs, as gauged by parameter ?, from
the highest ranked node to lower ranked ones.The
proportion of tweets in a set that are related to a
sub-event is then estimated according to the ratio be-
tween the sum of node scores in the sub-graph ver-
sus the entire graph. We select one tweet for each
sub-event that best covers the related nodes with the
highest sum of node scores normalized by length as
summaries. By adding a cutoff (parameter  ) on
proportion of sub-event required to be included into
summaries, we can produce summaries with the ap-
propriate length according to the diversity of infor-
mation in a set of tweets.
In Figure 2, 3 and 4, the named entity which is
also the topic of tweet cluster is omitted since it is
connected with every node in the event graph. The
size of node represents the salience score, while the
shorter, straighter and more vertical the edge is, the
higher its weight. The nodes with rectangle shapes
Algorithm 1 Find important sub-events
Require: Ranked event graph G = (V,E), the
named entity V0 which is the topic of event
cluster, parameters ? and   that can be set
towards user preference over development data
1: Initialize the pool of high ranked nodesV?  {Vi|8Vi 2 V, S(Vi) > 1}   V0 and the
total weight W  PVi2V? S(Vi)
2: while V? 6= ; do
3: Pop the highest ranked node Vm from V?
4: Put Vm to a temporary sub-event e  {Vm}
5: for all Vn in V? do
6: if wmn/w0m > ? and w0n/w0m > ?
then
7: e  e [ {Vn}
8: end if
9: end for
10: We  PVi2e S(Vi)
11: if We/W >   then
12: Successfully find a sub-event e
13: Remove all nodes in e from V?
14: end if
15: end while
are named entities, while round shaped ones are
event phrases. Note that in most cases, sub-events
correspond to connected components in the event
graph of high ranked nodes as in Figure 2 and 3.
However, our simple greedy algorithm also allows
multiple sub-events for a single connected compo-
nent that can not be covered by one tweet in the
summary. For example, in Figure 4, two sub-eventse1 = {sell, delete, start, payment} and e2 =
{facebook, share user data, privacy policy, debut}
are chosen to accommodate the complex event.
4 Experiments
4.1 Data
We gathered tweets over a 4-month period spanning
November 2012 to February 2013 using the Twitter
Streaming API. As described in more details in pre-
vious work on Twitter event extraction by Ritter et
al. (2012), we grouped together all tweets which
mention the same named entity (recognized using
24
Figure 2: Event graph of ?Google - 1/16/2013?, an example of event cluster with multiple focuses
Figure 3: Event graph of ?Instagram - 1/16/2013?, an example of event cluster with a single but complex focus
25
Figure 4: Event graph of ?West Ham - 1/16/2013?, an
example of event cluster with a single focus
a Twitter specific name entity tagger1) and a refer-
ence to the same unique calendar date (resolved us-
ing a temporal expression processor (Mani and Wil-
son, 2000)). Tweets published during the whole pe-
riod are aggregated together to find top events that
happen on each calendar day. We applied the G2
test for statistical significance (Dunning, 1993) to
rank the event clusters, considering the corpus fre-
quency of the named entity, the number of times the
date has been mentioned, and the number of tweets
which mention both together. We randomly picked
the events of one day for human evaluation, that is
the day of January 16, 2013 with 38 events and an
average of 465 tweets per event cluster.
For each cluster, our systems produce two ver-
sions of summaries, one with a fixed number (set
to 3) of tweets and another one with a flexible num-
ber (vary from 1 to 4) of tweets. Both ? and   are
set to 0.1 in our implementation. All parameters are
set experimentally over a small development dataset
consisting of 10 events in Twitter data of September
2012.
1
https://github.com/aritter/twitter_nlp
4.2 Baseline
SumBasic (Vanderwende et al, 2007) is a simple
and effective summarization approach based on term
frequency, which we use as our baseline. It uses
word probabilities with an update function to avoid
redundancy to select sentences or posts in a social
media setting. It is shown to outperform three other
well-known multi-document summarization meth-
ods, namely LexRank (Erkan and Radev, 2004),
TextRank (Mihalcea and Tarau, 2004) and MEAD
(Radev et al, 2004) on tweets in (Inouye and Kalita,
2011), possibly because that the relationship be-
tween tweets is much simpler than between sen-
tences in news articles and can be well captured by
simple frequency methods. The improvement over
the LexRank model on tweets is gained by consid-
ering the number of retweets and influential users is
another side-proof (Wei et al, 2012) of the effective-
ness of frequency.
EventRank?Flexible EventRank?Fixed SumBasic
Annotator 1
0
1
2
3
4
5 compactnesscompletenessoverall
EventRank?Flexible EventRank?Fixed SumBasic
Annotator 2
0
1
2
3
4
5 compactnesscompletenessoverall
Figure 5: human judgments evaluating tweet summariza-
tion systems
26
Event System Summary
- Google ?s home page is a Zamboni game in celebration of Frank Zam-
boni ?s birthday January 16 #GameOn
EventRank
(Flexible)
- Today social , Tomorrow Google ! Facebook Has Publicly Redefined
Itself As A Search Company http://t.co/dAevB2V0 via @sai
Google
1/16/2013
- Orange says has it has forced Google to pay for traffic . The Head of
the Orange said on Wednesday it had ... http://t.co/dOqAHhWi
- Tomorrow?s Google doodle is going to be a Zamboni! I may have to
take a vacation day.
SumBasic - the game on google today reminds me of hockey #tooexcited #saturday
- The fact that I was soooo involved in that google doodle game says
something about this Wednesday #TGIW You should try it!
EventRank
(Flexible)
- So Instagram can sell your pictures to advertisers without u knowing
starting January 16th I?m bout to delete my instagram !
- Instagram debuts new privacy policy , set to share user data with Face-
book beginning January 16
Instagram
1/16/2013
- Instagram will have the rights to sell your photos to Advertisers as of
jan 16
SumBasic - Over for Instagram on January 16th
- Instagram says it now has the right to sell your photos unless you delete
your account by January 16th http://t.co/tsjic6yA
EventRank
(Flexible)
- RT @Bassa_Mufc : Wayne Rooney and Nani will feature in the FA Cup
replay with West Ham on Wednesday - Sir Alex Ferguson
West Ham
1/16/2013
- Wayne Rooney could be back to face West Ham in next Wednesday?s
FA Cup replay at Old Trafford. #BPL
SumBasic - Tomorrow night come on West Ham lol
- Nani?s fit abd WILL play tomorrow against West Ham! Sir Alex con-
firmed :)
Table 3: Event-related information extracted from tweets
4.3 Preliminary Results
We performed a human evaluation in which two an-
notators were asked to rate the system on a five-
point scale (1=very poor, 5=very good) for com-
pleteness and compactness. Completeness refers to
how well the summary cover the important content
in the tweets. Compactness refers to how much
meaningful and non-redundant information is in the
summary. Because the tweets were collected ac-
cording to information extraction results and ranked
by salience, the readability of summaries generated
by different systems are generally very good. The
top 38 events of January 16, 2013 are used as test
set. The aggregate results of the human evaluation
are displayed in Figure 5. Agreement between an-
notators measured using Pearson?s Correlation Co-
efficient is 0.59, 0.62, 0.62 respectively for compact-
ness, completeness and overall judgements.
Results suggest that the models described in this
paper produce more satisfactory results as the base-
line approaches. The improvement of EventRank-
Flexible over SumBasic is significant (two-tailedp < 0.05) for all three metrics according to stu-
dent?s t test. Example summaries of the events in
Figure 2, 3 and 4 are presented respectively in Table
3. The advantages of our method are the follow-
ing: 1) it finds important facts of real-world events
2) it prefers tweets with good readability 3) it in-
cludes the right amount of information with diversity
and without redundancy. For example, our system
picked only one tweet about ?West Ham -1/16/2013?
that convey the same message as the three tweets to-
27
gether of the baseline system. For another example,
among the tweets about Google around 1/16/2013,
users intensively talk about the Google doodle game
with a very wide range of words creatively, giving
word-based methods a hard time to pick up the di-
verse and essential event information that is less fre-
quent.
5 Conclusions and Future Work
We present an initial study of feasibility to gen-
erate compact summaries of variable lengths for
tweet summarization by extending a Pagerank-like
algorithm to partition event graphs. The evalua-
tion shows that information extraction techniques
are helpful to generate news-worthy summaries of
good readability from tweets.
In the future, we are interested in improving the
approach and evaluation, studying automatic met-
rics to evaluate summarization of variable length
and getting involved in developing a standard eval-
uation for tweet summarization tasks. We wonder
whether other graph partitioning algorithms may im-
prove the performance. We also consider extending
this graph-based approach to disambiguate named
entities or resolve event coreference in Twitter data.
Another direction of future work is to extend the
proposed approach to different data, for example,
temporal-aware clustered tweets etc.
Acknowledgments
This research was supported in part by NSF grant
IIS-0803481, ONR grant N00014-08-1-0431, and
DARPA contract FA8750- 09-C-0179, and carried
out at the University of Washington?s Turing Center.
We thank Mausam and Oren Etzioni of University
of Washington, Maria Pershina of New York Univer-
sity for their advice.
References
Reid Andersen, Fan Chung, and Kevin Lang. 2006.
Local graph partitioning using pagerank vectors. In
Foundations of Computer Science, 2006. FOCS?06.
47th Annual IEEE Symposium on, pages 475?486.
IEEE.
Hila Becker, Mor Naaman, and Luis Gravano. 2011. Se-
lecting quality twitter content for events. In Proceed-
ings of the Fifth International AAAI Conference onWe-
blogs and Social Media (ICWSM?11).
Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Computer
networks and ISDN systems, 30(1):107?117.
Deepayan Chakrabarti and Kunal Punera. 2011. Event
summarization using tweets. In Proceedings of the
Fifth International AAAI Conference on Weblogs and
Social Media, pages 66?73.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
G?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. J. Artif. Intell. Res. (JAIR), 22:457?479.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In ACL.
Sanda Harabagiu and Andrew Hickl. 2011. Relevance
modeling for microblog summarization. In Fifth In-
ternational AAAI Conference on Weblogs and Social
Media.
David Inouye and Jugal K Kalita. 2011. Comparing twit-
ter summarization algorithms for multiple post sum-
maries. In Privacy, security, risk and trust (passat),
2011 ieee third international conference on and 2011
ieee third international conference on social comput-
ing (socialcom), pages 298?306. IEEE.
John Lafferty. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. pages 282?289. Morgan Kaufmann.
Wenjie Li, Wei Xu, Chunfa Yuan, Mingli Wu, and Qin
Lu. 2006. Extractive summarization using inter- and
intra- event relevance. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, ACL-44, pages 369?376,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why is
?sxsw? trending? exploring multiple text sources for
twitter topic summarization. ACL HLT 2011, page 66.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In ACL.
Xiaohua Liu, Furu Wei, Ming Zhou, et al 2012. Quick-
view: Nlp-based tweet search. In Proceedings of the
ACL 2012 System Demonstrations, pages 13?18. As-
sociation for Computational Linguistics.
Inderjeet Mani and GeorgeWilson. 2000. Robust tempo-
ral processing of news. In Proceedings of the 38th An-
28
nual Meeting on Association for Computational Lin-
guistics, ACL ?00, pages 69?76, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of EMNLP, vol-
ume 4, pages 404?411. Barcelona, Spain.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu
Liu, et al 2004. Mead-a platform for multidocument
multilingual text summarization. In Proceedings of
LREC, volume 2004.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
KDD, pages 1104?1112. ACM.
Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole Gersh-
man, and Robert Frederking. 2011. Topical clustering
of tweets. Proceedings of the ACM SIGIR: SWSM.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K
Kalita. 2010. Experiments in microblog summariza-
tion. In Proc. of IEEE Second International Confer-
ence on Social Computing.
Hiroya Takamura, Hikaru Yokono, and Manabu Oku-
mura. 2011. Summarizing a document stream. Ad-
vances in Information Retrieval, pages 177?188.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett, and
Ani Nenkova. 2007. Beyond sumbasic: Task-focused
summarization with sentence simplification and lex-
ical expansion. Information Processing & Manage-
ment, 43(6):1606?1618.
Furu Wei, Ming Zhou, and Heung-Yeung Shum. 2012.
Twitter topic summarization by ranking tweets using
social influence and content quality. In COLING.
Arkaitz Zubiaga, Damiano Spina, Enrique Amig?, and
Julio Gonzalo. 2012. Towards real-time summariza-
tion of scheduled events from twitter streams. In Pro-
ceedings of the 23rd ACM conference on Hypertext
and social media, pages 319?320. ACM.
29
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 121?128,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Gathering and Generating Paraphrases from Twitter
with Application to Normalization
Wei Xu+ Alan Ritter? Ralph Grishman+
+New York University, New York, NY, USA
{xuwei, grishman}@cs.nyu.edu
?University of Washington, Seattle, WA, USA
aritter@cs.washington.edu
Abstract
We present a new and unique para-
phrase resource, which contains meaning-
preserving transformations between infor-
mal user-generated text. Sentential para-
phrases are extracted from a compara-
ble corpus of temporally and topically
related messages on Twitter which of-
ten express semantically identical infor-
mation through distinct surface forms. We
demonstrate the utility of this new re-
source on the task of paraphrasing and
normalizing noisy text, showing improve-
ment over several state-of-the-art para-
phrase and normalization systems 1.
1 Introduction
Social media services provide a massive amount
of valuable information and demand NLP tools
specifically developed to accommodate their noisy
style. So far not much success has been reported
on a key NLP technology on social media data:
paraphrasing. Paraphrases are alternative ways to
express the same meaning in the same language
and commonly employed to improve the perfor-
mance of many other NLP applications (Madnani
and Dorr, 2010). In the case of Twitter, Petrovic? et
al. (2012) showed improvements on first story de-
tection by using paraphrases extracted from Word-
Net.
Learning paraphrases from tweets could be es-
pecially beneficial. First, the high level of in-
formation redundancy in Twitter provides a good
opportunity to collect many different expressions.
Second, tweets contain many kinds of paraphrases
not available elsewhere including typos, abbre-
viations, ungrammatical expressions and slang,
1Our Twitter paraphrase models are available
online at https://github.com/cocoxu/
twitterparaphrase/
which can be particularly valuable for many appli-
cations, such as phrase-based text normalization
(Kaufmann and Kalita, 2010) and correction of
writing mistakes (Gamon et al, 2008), given the
difficulty of acquiring annotated data. Paraphrase
models that are derived from microblog data could
be useful to improve other NLP tasks on noisy
user-generated text and help users to interpret a
large range of up-to-date abbreviations (e.g. dlt ?
Doritos Locos Taco) and native expressions (e.g.
oh my god ? {oh my goodness | oh my gosh | oh
my gawd | oh my jesus}) etc.
This paper presents the first investigation into
automatically collecting a large paraphrase cor-
pus of tweets, which can be used for building
paraphrase systems adapted to Twitter using tech-
niques from statistical machine translation (SMT).
We show experimental results demonstrating the
benefits of an in-domain parallel corpus when
paraphrasing tweets. In addition, our paraphrase
models can be applied to the task of normalizing
noisy text where we show improvements over the
state-of-the-art.
Relevant previous work has extracted sentence-
level paraphrases from news corpora (Dolan et
al., 2004; Barzilay and Lee, 2003; Quirk et al,
2004). Paraphrases gathered from noisy user-
generated text on Twitter have unique character-
istics which make this comparable corpus a valu-
able new resource for mining sentence-level para-
phrases. Twitter also has much less context than
news articles and much more diverse content, thus
posing new challenges to control the noise in min-
ing paraphrases while retaining the desired super-
ficial dissimilarity.
2 Related Work
There are several key strands of related work, in-
cluding previous work on gathering parallel mono-
lingual text from topically clustered news articles,
normalizing noisy Twitter text using word-based
121
models, and applying out-of-domain paraphrase
systems to improve NLP tasks in Twitter.
On the observation of the lack of a large para-
phrase corpus, Chen and Dolan (2011) have re-
sorted to crowdsourcing to collect paraphrases by
asking multiple independent users for descriptions
of the same short video. As we show in ?5, how-
ever, this data is very different from Twitter, so
paraphrase systems trained on in-domain Twitter
paraphrases tend to perform much better.
The task of paraphrasing tweets is also related
to previous work on normalizing noisy Twitter text
(Han and Baldwin, 2011; Han et al, 2012; Liu
et al, 2012). Most previous work on normaliza-
tion has applied word-based models. While there
are challenges in applying Twitter paraphrase sys-
tems to the task of normalization, access to paral-
lel text allows us to make phrase-based transfor-
mations to the input string rather than relying on
word-to-word mappings (for more details see ?4).
Also relevant is recent work on collecting bilin-
gual parallel data from Twitter (Jehl et al, 2012;
Ling et al, 2013). In contrast, we focus on mono-
lingual paraphrases rather than multilingual trans-
lations.
Finally we highlight recent work on apply-
ing out-of-domain paraphrase systems to improve
performance at first story detection in Twitter
(Petrovic? et al, 2012). By building better para-
phrase models adapted to Twitter, it should be pos-
sible to improve performance at such tasks, which
benefit from paraphrasing Tweets.
3 Gathering A Parallel Tweet Corpus
There is a huge amount of redundant information
on Twitter. When significant events take place in
the world, many people go to Twitter to share,
comment and discuss them. Among tweets on
the same topic, many will convey similar mean-
ing using widely divergent expressions. Whereas
researchers have exploited multiple news reports
about the same event for paraphrase acquisition
(Dolan et al, 2004), Twitter contains more vari-
ety in terms of both language forms and types of
events, and requires different treatment due to its
unique characteristics.
As described in ?3.1, our approach first identi-
fies tweets which refer to the same popular event
as those which mention a unique named entity and
date, then aligns tweets within each event to con-
struct a parallel corpus. To generate paraphrases,
we apply a typical phrase-based statistical MT
pipeline, performing word alignment on the paral-
lel data using GIZA++ (Och and Ney, 2003), then
extracting phrase pairs and performing decoding
uses Moses (Koehn et al, 2007).
3.1 Extracting Events from Tweets
As a first step towards extracting paraphrases from
popular events discussed on Twitter, we need a
way to identify Tweets which mention the same
event. To do this we follow previous work by Rit-
ter et al (2012), extracting named entities and
resolving temporal expressions (for example ?to-
morrow? or ?on Wednesday?). Because tweets are
compact and self-contained, those which mention
the same named entity and date are likely to refer-
ence the same event. We also employ a statistical
significance test to measure strength of association
between each named entity and date, and thereby
identify important events discussed widely among
users with a specific focus, such as the release of
a new iPhone as opposed to individual users dis-
cussing everyday events involving their phones.
By gathering tweets based on popular real-world
events, we can efficiently extract pairwise para-
phrases within a small group of closely related
tweets, rather than exploring every pair of tweets
in a large corpus. By discarding frequent but in-
significant events, such as ?I like my iPhone? and
?I like broke my iPhone?, we can reduce noise
and encourage diversity of paraphrases by requir-
ing less lexical overlap. Example events identified
using this procedure are presented in Table 1.
3.2 Extracting Paraphrases Within Events
Twitter users are likely to express the same mean-
ing in relation to an important event, however not
every pair of tweets mentioning the same event
will have the same meaning. People may have
opposite opinions and complicated events such as
presidential elections can have many aspects. To
build a useful monolingual paraphrase corpus, we
need some additional filtering to prevent unrelated
sentence pairs.
If two tweets mention the same event and also
share many words in common, they are very likely
to be paraphrases. We use the Jaccard distance
metric (Jaccard, 1912) to identify pairs of sen-
tences within an event that are similar at the lexical
level. Since tweets are extremely short with little
context and include a broad range of topics, using
only surface similarity is prone to unrelated sen-
122
Entity/Date Example Tweets
Vote for Obama on November
6th!
Obama
11/6/2012
OBAMA is #winning his 2nd
term on November 6th 2012.
November 6th we will re-elect
Obama!!
Bought movie tickets to see
James Bond tomorrow. I?m a
big #007 fan!
James Bond
11/9/2012
Who wants to go with me and
see that new James Bond movie
tomorrow?
I wanna go see James Bond to-
morrow
North Korea Announces De-
cember 29 Launch Date for
Rocket
North Korea
12/29/2012
Pyongyang reschedules launch
to December 29 due to ?techni-
cal deficiency?
North Korea to extend rocket
launch period to December 29
Table 1: Example sentences taken from automat-
ically identified significant events extracted from
Twitter. Because many users express similar in-
formation when mentioning these events, there are
many opportunities for paraphrase.
tence pairs. The average sentence length is only
11.9 words in our Twitter corpus, compared to
18.6 words in newswire (Dolan et al, 2004) which
also contains additional document-level informa-
tion. Even after filtering tweets with both their
event cluster and lexical overlap, some unrelated
sentence pairs remain in the parallel corpus. For
example, names of two separate music venues in
the same city might be mismatched together if they
happen to have concerts on the same night that
people tweeted using a canonical phrasing like ?I
am going to a concert at in Austin tonight?.
4 Paraphrasing Tweets for
Normalization
Paraphrase models built from grammatical text are
not appropriate for the task of normalizing noisy
text. However, the unique characteristics of the
Twitter data allow our paraphrase models to in-
clude both normal and noisy language and conse-
quently translate between them. Our models have
a tendency to normalize because correct spellings
and grammar are most frequently used,2 but there
is still danger of introducing noise. For the pur-
poses of normalization, we therefore biased our
models using a language model built using text
taken from the New York Times which is used to
represent grammatical English.
Previous work on microblog normalization is
mostly limited to word-level adaptation or out-of-
domain annotated data. Our phrase-based mod-
els fill the gap left by previous studies by exploit-
ing a large, automatically curated, in-domain para-
phrase corpus.
Lexical normalization (Han and Baldwin, 2011)
only considers transforming an out-of-vocabulary
(OOV) word to its standard form, i.e. in-
vocabulary (IV) word. Beyond word-to-word con-
versions, our phrase-based model is also able to
handle the following types of errors without re-
quiring any annotated data:
Error type Ill form Standard
form
1-to-many everytime every time
incorrect IVs can?t want
for
can?t wait for
grammar I?m going a
movie
I?m going to
a movie
ambiguities 4 4 / 4th / for /
four
Kaufmann and Kalita (2010) explored machine
translation techniques for the normalization task
using an SMS corpus which was manually anno-
tated with grammatical paraphrases. Microblogs,
however, contain a much broader range of content
than SMS and have no in-domain annotated data
available. In addition, the ability to gather para-
phrases automatically opens up the possibility to
build normalization models from orders of mag-
nitude more data, and also to produce up-to-date
normalization models which capture new abbrevi-
ations and slang as they are invented.
5 Experiments
We evaluate our system and several baselines
at the task of paraphrasing Tweets using pre-
viously developed automatic evaluation metrics
which have been shown to have high correlation
with human judgments (Chen and Dolan, 2011).
2Even though misspellings and grammatical errors are
quite common, there is much more variety and less agree-
ment.
123
In addition, because no previous work has evalu-
ated these metrics in the context of noisy Twitter
data, we perform a human evaluation in which an-
notators are asked to choose which system gen-
erates the best paraphrase. Finally we evaluate
our phrase-based normalization system against a
state-of-the-art word-based normalizer developed
for Twitter (Han et al, 2012).
5.1 Paraphrasing Tweets
5.1.1 Data
Our paraphrase dataset is distilled from a large
corpus of tweets gathered over a one-year period
spanning November 2011 to October 2012 using
the Twitter Streaming API. Following Ritter et
al. (2012), we grouped together all tweets which
mention the same named entity (recognized using
a Twitter specific name entity tagger3) and a ref-
erence to the same unique calendar date (resolved
using a temporal expression processor (Mani and
Wilson, 2000)). Then we applied a statistical sig-
nificance test (the G test) to rank the events, which
considers the corpus frequency of the named en-
tity, the number of times the date has been men-
tioned, and the number of tweets which mention
both together. Altogether we collected more than
3 million tweets from the 50 top events of each day
according to the p-value from the statistical test,
with an average of 229 tweets per event cluster.
Each of these tweets was passed through a Twit-
ter tokenizer4 and a simple sentence splitter, which
also removes emoticons, URLs and most of the
hashtags and usernames. Hashtags and usernames
that were in the middle of sentences and might
be part of the text were kept. Within each event
cluster, redundant and short sentences (less than 3
words) were filtered out, and the remaining sen-
tences were paired together if their Jaccard simi-
larity was no less than 0.5. This resulted in a par-
allel corpus consisting of 4,008,946 sentence pairs
with 800,728 unique sentences.
We then trained paraphrase models by applying
a typical phrase-based statistical MT pipeline on
the parallel data, which uses GIZA++ for word
alignment and Moses for extracting phrase pairs,
training and decoding. We use a language model
trained on the 3 million collected tweets in the de-
coding process. The parameters are tuned over de-
3https://github.com/aritter/twitter_
nlp
4https://github.com/brendano/
tweetmotif
velopment data and the exact configuration are re-
leased together with the phrase table for system
replication.
Sentence alignment in comparable corpora is
more difficult than between direct translations
(Moore, 2002), and Twitter?s noisy style, short
context and broad range of content present ad-
ditional complications. Our automatically con-
structed parallel corpus contains some proportion
of unrelated sentence pairs and therefore does re-
sult in some unreasonable paraphrases. We prune
out unlikely phrase pairs using a technique pro-
posed by Johnson et al (2007) with their recom-
mended setting, which is based on the significance
testing of phrase pair co-occurrence in the parallel
corpus (Moore, 2004). We further prevent unrea-
sonable translations by adding additional entries
to the phrase table to ensure every phrase has an
option to remain unchanged during paraphrasing
and normalization. Without these noise reduction
steps, our system will produce paraphrases with
serious errors (e.g. change a person?s last name)
for 100 out of 200 test tweets in the evaluation in
?5.1.5.
At the same time, it is also important to promote
lexical dissimilarity in the paraphrase task. Fol-
lowing Ritter et. al. (2011) we add a lexical sim-
ilarity penalty to each phrase pair in our system,
in addition to the four basic components (transla-
tion model, distortion model, language model and
word penalty) in SMT.
5.1.2 Evaluation Details
The beauty of lexical similarity penalty is that it
gives control over the degree of paraphrasing by
adjusting its weight versus the other components.
Thus we can plot a BLEU-PINC curve to express
the tradeoff between semantic adequacy and lexi-
cal dissimilarity with the input, where BLUE (Pa-
pineni et al, 2002) and PINC (Chen and Dolan,
2011) are previously proposed automatic evalua-
tion metrics to measure respectively the two crite-
ria of paraphrase quality.
To compute these automatic evaluation met-
rics, we manually prepared a dataset of gold para-
phrases by tracking the trending topics on Twitter5
and gathering groups of paraphrases in November
2012. In total 20 sets of sentences were collected
and each set contains 5 different sentences that ex-
press the same meaning. Each sentence is used
5https://support.twitter.com/articles/
101125-faqs-about-twitter-s-trends
124
Input Output
Hostess is going outta biz hostess is going out of business
REPUBLICAN IMMIGRATION REFORM IS A
THING NOW
gop imigration law is a thing now
Freedom Writers will always be one of my fav
movies
freedom writers will forever be one of my favorite
movies
sources confirm that Phil Jackson has cancelled
all weekend plans and upcoming guest appear-
ances, will meet with LAL front office
source confirms that phil jackson has canceled all
weekend plans , upcomin guest appearances and
will meet with lakers front office
Table 2: Example paraphrases generated by our system on the test data.
once as input while other 4 sentences in the same
set serve as reference translation for automatic
evaluation of semantic adequacy using BLEU.
5.1.3 Baselines
We consider two state-of-the-art paraphrase sys-
tems as baselines, both of which are trained on
parallel corpora of aligned sentences. The first one
is trained on a large-scale corpus gathered by ask-
ing users of Amazon?s Mechanical Turk Service
(Snow et al, 2008) to write a one-sentence de-
scription of a short video clip (Chen and Dolan,
2011). We combined a phrase table and distor-
tion table extracted from this parallel corpus with
the same Twitter language model, applying the
Moses decoder to generate paraphrases. The ad-
ditional noise removal steps described in ?5.1.1
were found helpful for this model during devel-
opment and were therefore applied. The second
baseline uses the Microsoft Research paraphrase
tables that are automatically extracted from news
articles in combination with the Twitter language
model.6
5.1.4 Results
Figure 1 compares our system against both base-
lines, varying the lexical similarity penalty for
each system to generate BLEU-PINC curves.
Our system trained on automatically gathered
in-domain Twitter paraphrases achieves higher
BLEU at equivalent PINC for the entire length of
the curves. Table 2 shows some sample outputs of
our system on real Twitter data.
One novel feature of our approach, compared
to previous work on paraphrasing, is that it cap-
tures many slang terms, acronyms, abbreviations
and misspellings that are otherwise hard to learn.
6No distortion table or noisy removal process is applied
because the parallel corpus is not available.
lll ll l l l l
l l
l
l
l
l
l
l l
l
0 20 40 60
0
5
10
15
20
PINC
BLE
U
l OursVideoMSR
Figure 1: Results from automatic paraphrase eval-
uation. PINC measures n-gram dissimilarity from
the source sentence, whereas BLEU roughly mea-
sures n-gram similarity to the reference para-
phrases.
Several examples are shown in table 3. The rich
semantic redundancy in Twitter helps generate a
large variety of typical paraphrases as well (see an
example in table 4).
5.1.5 Human Evaluation
In addition to automatic evaluation, we also per-
formed a human evaluation in which annotators
were asked to pick which system generated the
best paraphrase. We used the same dataset of
200 tweets gathered for the automatic evaluation
and generated paraphrases using the 3 systems in
Figure 1 with the highest BLEU which achieve a
PINC of at least 40. The human annotators were
then asked to pick which of the 3 systems gener-
ated the best paraphrase using the criteria that it
should be both different from the original and also
125
Input Top-ranked Outputs
amped pumped
lemme kno let me know
bb bigbang, big brother
snl nbcsnl, saturday night live
apply 4 tix apply for tickets, ask for tickets,
applying for tickets
the boys one direction (a band, whose
members are often referred as
?the boys?), they, the boy, the
gys, the lads, my boys, the direc-
tion (can be used to refer to the
band ?one direction?), the onedi-
rection, our boys, our guys
oh my god oh my gosh, omfg, thank the
lord, omg, oh my lord, thank you
god, oh my jesus, oh god
can?t wait cant wait, cant wait, cannot wait,
i cannot wait, so excited, cnt
wait, i have to wait, i can?wait,
ready, so ready, so pumped, seri-
ously can?wait, really can?t wait
Table 3: Example paraphrases of noisy phrases
and slang commonly found on Twitter
Input Top-ranked Outputs
who want
to get a
beer
wants to get a beer, so who wants
to get a beer, who wants to go
get a beer, who wants to get beer,
who want to get a beer, trying to
get a beer, who wants to buy a
beer, who wants to get a drink,
who wants to get a rootbeer, who
trying to get a beer, who wants to
have a beer, who wants to order
a beer, i want to get a beer, who
wants to get me a beer, who else
wants to get a beer, who wants to
win a beer, anyone wants to get
a beer, who wanted to get a beer,
who wants to a beer, someone to
get a beer, who wants to receive a
beer, someone wants to get a beer
Table 4: Example paraphrases of a given sentence
?who want to get a beer?
Ours Video MSR
0
20
40
60
80
100
120
annotator 1annotator 2
Figure 2: Number of paraphrases (200 in total)
preferred by the annotators for each system
capture as much of the original meaning as pos-
sible. The annotators were asked to abstain from
picking one as the best in cases where there were
no changes to the input, or where the resulting
paraphrases totally lost the meaning.
Figure 2 displays the number of times each an-
notator picked each system?s output as the best.
Annotator 2 was somewhat more conservative
than annotator 1, choosing to abstain more fre-
quently and leading to lower overall frequencies,
however in both cases we see a clear advantage
from paraphrasing using in-domain models. As
a measure of inter-rater agreement, we computed
Cohen?s Kappa between the annotators judgment
as to whether the Twitter-trained system?s output
best. The value of Cohen?s Kappa in this case was
0.525.
5.2 Phrase-Based Normalization
Because Twitter contains both normal and noisy
language, with appropriate tuning, our models
have the capability to translate between these two
styles, e.g. paraphrasing into noisy style or nor-
malizing into standard language. Here we demon-
strate its capability to normalize tweets at the
sentence-level.
5.2.1 Baselines
Much effort has been devoted recently for devel-
oping normalization dictionaries for Microblogs.
One of the most competitive dictionaries avail-
able today is HB-dict+GHM-dict+S-dict used by
Han et al (2012), which combines a manually-
constructed Internet slang dictionary , a small
(Gouws et al, 2011) and a large automatically-
126
derived dictionary based on distributional and
string similarity. We evaluate two baselines using
this large dictionary consisting of 41181 words;
following Han et. al. (2012), one is a simple dic-
tionary look up. The other baseline uses the ma-
chinery of statistical machine translation using this
dictionary as a phrase table in combination with
Twitter and NYT language models.
5.2.2 System Details
Our base normalization system is the same as
the paraphrase model described in ?5.1.1, except
that the distortion model is turned off to exclude
reordering. We tuned the system towards cor-
rect spelling and grammar by adding a language
model built from all New York Times articles
written in 2008. We also filtered out the phrase
pairs which map from in-vocabulary to out-of-
vocabulary words. In addition, we integrated the
dictionaries by linear combination to increase the
coverage of phrase-based SMT model (Bisazza et
al., 2011).
5.2.3 Evaluation Details
We adopt the normalization dataset of Han and
Baldwin (2011), which was initially annotated
for the token-level normalization task, and which
we augmented with sentence-level annotations.
It contains 549 English messages sampled from
Twitter API from August to October, 2010.
5.2.4 Results
Normalization results are presented in figure 5.
Using only our phrase table extracted from Twit-
ter events we achieve poorer performance than the
state-of-the-art dictionary baseline, however we
find that by combining the normalization dictio-
nary of Han et. al. (2012) with our automatically
constructed phrase-table we are able to combine
the high coverage of the normalization dictionary
with the ability to perform phrase-level normaliza-
tions (e.g. ?outta? ? ?out of? and examples in
?4) achieving both higher PINC and BLEU than
the systems which rely exclusively on word-level
mappings. Our phrase table also contains many
words that are not covered by the dictionary (e.g.
?pts? ? ?points?, ?noms? ? ?nominations?).
6 Conclusions
We have presented the first approach to gather-
ing parallel monolingual text from Twitter, and
built the first in-domain models for paraphrasing
BLEU PINC
No-Change 60.00 0.0
SMT+TwitterLM 62.54 5.78
SMT+TwitterNYTLM 65.72 9.23
Dictionary 75.07 22.10
Dicionary+TwitterNYTLM 75.12 20.26
SMT+Dictionary+TwitterNYTLM 77.44 25.33
Table 5: Normalization performance
tweets. By paraphrasing using models trained
on in-domain data we showed significant per-
formance improvements over state-of-the-art out-
of-domain paraphrase systems as demonstrated
through automatic and human evaluations. We
showed that because tweets include both normal
and noisy language, paraphrase systems built from
Twitter can be fruitfully applied to the task of nor-
malizing noisy text, covering phrase-based nor-
malizations not handled by previous dictionary-
based normalization systems. We also make our
Twitter-tuned paraphrase models publicly avail-
able. For future work, we consider developing ad-
ditional methods to improve the accuracy of tweet
clustering and paraphrase pair selection.
Acknowledgments
This research was supported in part by NSF grant
IIS-0803481, ONR grant N00014-08-1-0431, and
DARPA contract FA8750- 09-C-0179.
References
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In Proceedings of the
2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology - Volume 1, NAACL
?03.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based smt adaptation. In International Work-
shop on Spoken Language Translation (IWSLT), San
Francisco, CA.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2011),
Portland, OR, June.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of Coling 2004.
127
Michael Gamon, Jianfeng Gao, Chris Brockett,
Alexander Klementiev, William B. Dolan, Dmitriy
Belenko, and Lucy Vanderwende. 2008. Using con-
textual speller techniques and language modeling for
esl error correction. IJCNLP.
S. Gouws, D. Hovy, and D. Metzler. 2011. Unsu-
pervised mining of lexical variants from noisy text.
In Proceedings of the First workshop on Unsuper-
vised Learning in NLP, pages 82?90. Association
for Computational Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, volume 1, pages 368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 421?432, Stroudsburg, PA,
USA.
P. Jaccard. 1912. The distribution of the flora in the
alpine zone. New Phytologist, 11(2):37?50.
Laura Jehl, Felix Hieber, and Stefan Riezler. 2012.
Twitter translation using translation-based cross-
lingual retrieval. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
410?421. Association for Computational Linguis-
tics.
J.H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007.
Improving translation quality by discarding most of
the phrasetable.
Max Kaufmann and Jugal Kalita. 2010. Syntac-
tic normalization of twitter messages. In Interna-
tional Conference on Natural Language Processing,
Kharagpur, India.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
broadcoverage normalization system for social me-
dia language. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2012), Jeju, Republic of Korea.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Comput. Linguist.
Inderjeet Mani and George Wilson. 2000. Robust tem-
poral processing of news. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 69?76, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
the 5th Conference of the Association for Machine
Translation in the Americas on Machine Transla-
tion: From Research to Real Users, AMTA ?02.
Robert C. Moore. 2004. On log-likelihood-ratios and
the significance of rare events. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 333?340.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2012. Using paraphrases for improving first story
detection in news and twitter.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of EMNLP 2004.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter.
In KDD, pages 1104?1112. ACM.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08.
128
