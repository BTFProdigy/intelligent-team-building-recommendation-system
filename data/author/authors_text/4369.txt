An HPSG-to-CFG Approximation of Japanese 
Bernd Kiefer, Hans-Ulrich Krieger, Melanie Siegel 
German Research Center for Artificial Intell igence (DFKI )  
Stuhlsatzenhausweg 3, D-66123 Saarbri icken 
{kiefer, krieger, siegel}@dfki, de 
Abstract 
We present asimple approximation method for turn- 
ing a Head-Driven Phrase Structure Grammar into a 
context-free grammar. The approximation method 
can be seen as the construction of the least fixpoint 
of a certain monotonic hmction. We discuss an ex- 
periment with a large HPSG for Japanese. 
1 In t roduct ion  
This paper presents a simple approximation 
method for turning an HPSG (Pollard and Sag, 
1994) into a context-free grmnmar. The the- 
oretical underpinning is established through a 
least fixpoint construction over a certain mono- 
tonic function, similar to the instantiation of 
a rule in a bottom-up passive chart parser or 
to partial evaluation in logic programming; see 
(Kiefer and Krieger, 2000a). 
1.1 Bas ic  Idea  
The intuitive idea underlying our approach is 
to generalize in a first step the set of all lexicon 
entries. The resulting structures form equiv- 
alence classes, since they abstract from word- 
specific information, such as FORN or STEM. The 
abstraction is specified by means of a restrictor 
(Shiet)er, 1985), the so-called lexicon rcstrictor. 
The grammar rules/schemata are then instan- 
tiated via unification, using the abstracted lexi- 
con entries, yielding derivation trees of depth 1. 
We apply the rule restrictor to each resulting 
feature structure, which removes all information 
contained only in the daughters of the rule. Due 
to the Locality Principle of HPSG, this deletion 
does not alter the set of derivable feature struc- 
tures. Since we are interested in a finite fixpoint 
from a practical point of view, the restriction 
also gets rid of information that will lead to in- 
finite growth of feature structures during deriva- 
tion. Additionally, we throw away information 
that will not restrict the search space (typically, 
parts of tile semantics). The restricted fea- 
ture structures (together with older ones) then 
serve as tile basis for the next instantiation step. 
Again, this gives us feature structures encoding 
a derivation, and again we are applying the rule 
restrictor. We proceed with the iteration, until 
we reach a fixpoint, meaning that further itera- 
tion steps will not add (or remove) new (o1" old) 
feature structures. 
Our goal, however, is to obtain a context-fl'ee 
grammar, trot since we have reached a fixpoint, 
we can use the entire feature structures as (com- 
plex) context-free symbols (e.g., by nlapping 
them to integers). By instantiating the HPSG 
rules a final time with feature structures from 
the fixpoint, applying the rule restrictor and 
finally classifying the resulting structure (i.e., 
find tile right structure from the fixpoint), one 
can easily obtain tile desired context-free grain- 
mar (CFG). 
1.2 Why is it Wor th?  
Approximating an HPSG through a CFG ~ is 
interesting for the following practical reason: 
assuming that we have a CFG that comes close 
to an HPSG, we can use the CFG as a cheap fil- 
ter (running time complexity is O(IGI 2 x n 3) for 
an arbitrary sentence of length n). The main 
idea is to use the CFG first and then let the 
HPSG deterministically replay the derivations 
licensed by the CFG. The important point here 
is that one can find for every CF production 
exactly one and only one HPSG rule. (Kasper 
et al, 1996) describe such an approach for word 
graph parsing which employs only the relatively 
unspecific CF backbone of an HPSG-like grmn- 
mar. (Diagne et al, 1995) replaces the CF back- 
bone through a restriction of the original HPSG. 
This grammar, however, is still an unification- 
1046 
based grammar, since it employs coreference 
constraints. 
1..3 Content  of  Paper 
In tile next section, we describe the Japanese 
HPSG that is used in Verbmobil, a project that 
deals with the translation of spontaneously spo- 
ken dialogues between English, German, and 
Japanese speakers. After that, section 3 ex- 
plains a simplified, albeit correct version of the 
implemented algorithm. Section 4 then dis- 
cusses the outcome of the approximation pro- 
cess .  
2 Japanese  Grammar  
The grammar was developed for machine trans- 
lation of spoken dialogues. It is capable of deal- 
ing with spoken language phenomena nd un- 
grammatical or corrupted input. This leads on 
the one hand to the necessity of robustness and 
on the other hand to mnbiguitics that must be 
dealt with. Being used in an MT system for spo- 
ken language, the grammar must firstly accept 
fragmentary input and bc able to deliver partial 
analyses, where no spanning analysis is awdl- 
able. A coinplete fragmentary utterance could, 
e.g., be: 
dai~oubu 
OKay 
This is an adjective without any noun or (cop- 
ula) verb. There is still an analysis available. 
If an utterance is corrupted by not being fully 
recognized~ the grammar delivers analyses for 
those parts that could be understood. An ex- 
ample would be the following transliteration of
input to the MT system: 
son desu ne watakushi 
so COP TAG i 
no hou wa dai~oubu 
GEN side 'FOP okay 
desu da ga kono hi 
COP but this day 
wa kayoubi desu ~te 
TOP Tuesday COP TAG 
(lit.: Well, it is okay for my side, but 
this day is ~l~msday, isn't it?) 
Here, analyses for the following fragments arc 
delivered (where the parser found opera wa in 
the word lattice of the speech recognizer): 
sou dcsu nc watakushi 
so COP TAG I 
no hou wa dai{oubu 
GEN side TOP okay 
dCSlt 
COP 
(Well, it is okay for my side.) 
era  TOP 
(The opera) 
hone hi wa kayoubi 
this day TOP Tlmsday 
desu nc 
COP TAG 
(This (lay is 3hmsday, isn't it?) 
Another necessity for partial analysis comes 
fl'om real-time restrictions imposed by the MT 
system. If tile parser is not allowed to produce 
a spanning analysis, it delivers best partial frag- 
ments. 
rl'tle grammar must also be applicable to phe- 
nomena of spoken language. A typical problem 
is tile extensive use of topicalization and even 
omission of particles. Also serialization of parti- 
cles occur nlore often than in written language, 
as described in (Siegel, 1999). A well-defined 
type hierarchy of Japanese particles is necessary 
here to describe their functions in the dialogues. 
Extensive use of honorification is another sig- 
nificance of spoken Japanese. A detailed de- 
scription is necessary for different purposes in 
an MT system: honorification is a syntactic 
restrictor in subject-verb agreement and com- 
plement sentences, l~lrthermore, it is a very 
useflfl source of information for the solution 
of zero pronominalization (Metzing and Siegel, 
1994). It is finally necessary for Japanese gener- 
ation in order to tind the appropriate honorific 
forms. The sign-based in%rmation structure of 
HPSG (Pollard and Sag, 1994) is predestined 
to describe honorification on the different levels 
of linguistics: on the syntactic level for agree- 
ment phenomena, on tile contextual level for 
anaphora resolution and connection to speaker 
and addressee reference, and via co-indexing on 
the semantic level. Connected to honorification 
is the extensive use of auxiliary and light verb 
constructions that require solutions in the areas 
of morphosyntax, semantics, and context (see 
(Siegel, 2000) for a more detailled description). 
Finally, a severe problem of tile Japanese 
grammar in the MT system is the high po- 
1047 
tential of ambiguity arising from the syntax of 
Japanese itself, and especially from the syntax 
of Japanese spoken language. For example, the 
Japanese particle ga marks verbal argmnents in 
most cases. There are, however, occurrences of 
ga that are assigned to verbal adjuncts. Allow- 
ing g a in any case to mark arguments or ad- 
juncts would lead to a high potential of (spuri- 
ous) ambiguity. Thus, a restriction was set on 
the adjunctive g a, requiring the modified verb 
not to have any unsaturated ga arguments. 
The Japanese language allows many verbal 
arguments to be optional. For example, pro- 
nouns are very often not uttered. This phe- 
nomenon is basic for spoken Japanese, such that 
a syntax urgently needs a clear distinction be- 
tween optional and obligatory (and adjacent) 
arguments. We therefore used a description 
of subcategorization that differs from standard 
HPSG description in that it explicitly states the 
optionality of arguments. 
3 Bas ic  Algor i thm 
We stm't with the description of the top-level 
function HPSG2CFG which initiates the ap- 
proximation process (cf. section 1.1 for the 
main idea). Let 7~ be the set of all rules/rule 
schemata, 12 the set of all lexicon entries, R 
the rule restrictor, and L the lexicon restrictor. 
We begin the approximation by first abstract- 
ing from the lexicon entries /2 with the help of 
the lexicon restrictor L (line 5 of the algorithm). 
This constitutes our initial set To (line 6). Fi- 
nally, we start the fixpoint iteration calling It- 
crate with the necessary parameters. 
1 HPSG2CFG(T~, 12, R, L) :~==~ 
2 local To; 
3 T0 := (~; 
4 for each  l E/2 
5 l :=  L(1); 
6 To := To u {l}; 
7 Iterate(T~, R, To). 
After that, the instantiation of the rule 
schemata with rule/lexicon-restricted elements 
from the previous iteration Ti begins (line 11- 
14). Instantiation via unification is performed 
by Fill-Daughters which takes into account a 
single rule r and Ti, returning successful instan- 
tiations (line 12) to which we apply the rule 
restrictor (line 13). The outcome of this restric- 
tion is added to the actual set of rule-restricted 
feature structures Ti+l iff it is new (remember 
how set union works; line 14). In case that re- 
ally new feature structures have not been added 
during the current iteration (line 15), meaning 
that we have reached a fixpoint, we immediately 
exit with T/ (line 16) from which we generate 
the context-free rules as indicated in section 1.1. 
Otherwise, we proceed with the iteration (line 
17). 
8 Iterate(g, R, Ti) :?==v 
9 local Ti+j; 
10 Ti+~ := Ti; 
11 for each r E T~ 
12 for each t C Fill-Daughters(r, Ti) do 
13 t := R(t); 
14 Ti+I := Ti+I U {t}; 
15 i f  Ti = T/+I 
16 then  re turn  Cornpute-CF-Rules(TG i)
17 else Iterate(7~, R, Ti+l). 
We note here that the pseudo code above is 
only a naYve version of the implemented algo- 
rithm. It is still correct, but not computation- 
ally tractable when dealing with large HPSG 
grammars. Technical details and optimizations 
of the actual algorithm, together with a descrip- 
tion of the theoretical foundations are described 
in (Kiefer and Krieger, 2000a). Due to space 
limitations, we can only give a glimpse of the 
actual implementation. 
Firstly, the most obvious optimization applies 
to the function Fill-Daughters (line 12), where 
the number of unifications is reduced by avoid- 
ing recomputation of combinations of daugh- 
ters and rules that already have been checked. 
To do this in a simple way, we split the set Ti 
into Ti \ T/.-1 and T/_I and fill a rule with only 
those permutations of daughters which contain 
at least one element from T/ \ r / _  1 . This guaran- 
tees checking of only those configurations which 
were enabled by the last iteration. 
Secondly, we use techniques developed in 
(Kiefer et al, 1999), namely the so-called rule 
filter and the quick-check method. The rule fil- 
ter precomputes the applicability of rules into 
each other and thus is able to predict a fail- 
ing unification using a simple and fast table 
lookup. The quick-check method exploits the 
1048 
flint that unification fails snore often at cer- 
tain points in feature structnres than at oth- 
ers. In an off  line stage, we parse a test cor- 
pus, using a special unifier that records all fail- 
ures instead of bailing out after the first one 
in order to determine the most prominent fail- 
ure points/paths. These points constitute the 
so-called quick-check vector. When executing a
unification during approximation, those points 
are efficiently accessed and checked using type 
unification prior to the rest of the structure. Ex- 
actly these quick-check points are used to build 
the lexicon and the rule restrictor as described 
earlier (see fig. 1). During ore: experinmnts, 
nearly 100% of all failing unifications in Fill- 
Daughters could be quickly detected using the 
above two techniques. 
Thirdly, instead of using set union we use 
tlhe more elaborate operation during the addi- 
tion of new feature structures to T/.+I. In fact, 
we add a new structure only if it is not sub- 
sumed by some structure already in tile set. To 
do this efficiently, tile quick-check vectors de- 
scribed above are employed here: before per- 
fl)rming full feature structure subsnmption, we 
pairwise check the elements of the vectors us- 
ing type subsumption and only if this succeeds 
do a full subsmnption test. If we add a new 
structure, we also remove all those structures in 
7)ql that are subsumed by the slew structure 
in order to keep the set small. This does not 
change the language of tile resulting CF gram- 
mar because a more general structure can be 
put into at least those daughter positions which 
can be fillcd by the more specific one. Conse- 
quently, fbr each production that employs the 
more specific structure, there will be a (pos- 
sibly) more general production employing the 
more general structure in the same daughter po- 
sitions. Extending feature structure subsump- 
lion by quick-check subsumption definitely pays 
off: more than 98% of all failing subsumptions 
could be detected early. 
Further optimizations to make the algorithm 
works in practice are described in (Kiefer and 
Krieger, 2000b). 
4 Eva luat ion  
The Japanese HPSG grammar used in our ex- 
periment consists of 43 rule sdmmata (28 unary, 
15 binary), 1,208 types and a test lexicon of 
2,781 highly diverse entries. The lexicon restric- 
tot, as introduced in section 1.1 and depicted in 
figure 1, maps these entries onto 849 lexical ab- 
stractions. This restrictor tells us which parts of 
a feature structure have to be deleted---it s the 
kind of restrictor which we are usually going to 
use. We call this a negative restrictor, contrary 
to tile positive restrictors used in the PATR- 
II system that specii\[y those parts of a feature 
structure which will survive after restricting it. 
Since a restrictor could have reentrance points, 
one can even define a reeursivc (or cyclic) re- 
strictor to foresee recursive mbeddings as is the 
case in HPSG. 
The rule restrictor looks quite silnilar, cut- 
ling off additionally information contained only 
in the daughters. Since both restrictors remove 
the CONTENT feature (and hence the semantics 
which is a source of infinite growth), it hal> 
pened that two very productive head-adjunct 
schemata could be collapsed into a single rule. 
Tiffs has helped to keep the number of feature 
structures in the fixpoint relatively small. 
We reached the fixpoint after 5 iteration 
steps, obtaining 10,058 featnre structures. The 
comtmtation of the fixpoint took about 27.3 
CPU hours on a 400MHz SUN Ultrasparc 2with 
t~?anz Allegro Common Lisp under Solaris 2.5. 
Given tim feature structures from the fixpoint, 
the 43 rules might lead to 28 x 10,058-t- 15 x 
10,058 x 10,058 = 1, 51.7,732,084 CF produc- 
tions in the worst case. Our method produces 
19,198,592 productions, i.e., 1.26% of all pos- 
sible ones. We guess that the enormous et of 
productions is due tile fact that the grammar 
was developed for spoken Japanese (recall sec- 
tion 2 on the mnbiguity of Japanese). Likewise, 
the choice of a 'wrong' restrictor often leads to a 
dramatic increase of structures in the fixpoint, 
and hence of CF rules--we are not sure at this 
point whether our restrictor is a good compro- 
mise between tile specificity of the context-free 
language and the number of context-free rules. 
We are currently implementing a CF parser that 
can handle such an enormous et of CF rules. 
In (Kiefer and Krieger, 2000b), we report on 
a similar experiment that we carried out using 
the English Verbmobil grmnmar, developed at 
CSLI, Stanford. In this paper, we showed that 
the workload on the HPSG side can be drasti- 
cally reduced by using a CFG filter, obtained 
1049 
-PHON 
FORM 
SYNSEM LOCAL 
-CONTENT 
CONTEXT 
HEAD 
CAT 
SUBCAT 
3PEC ~\ ]  
"NONLOCAL 
-CONTENT 
CONTEXT 
LOCAL 
~AT 
M0D 
MARK \[~\] 
FORMAL 
MODUS 
P0S 
PTYPE 
10BJIII I 
I OBJ2 I i I I 
VAL ~ 1 
I SpR l J  ! 
I SUBJ I 1 I I 
SUBCAT 
HEAD 
POS 11 SPEC FORMAl MARK 
MOD 
Figure 1: The lexicon restrictor used during the approximation of the Japanese grammar. In 
addition, the rule restrictor cuts off the DAUGHTERS feature. 
from the HPSG. Our hope is that these results 
can be carried over to the Japanese grammar. 
Acknowledgments  
This research was supported by the German 
Ministry for Education, Science, Research, and 
Technology under grant no. 01 IV 701 V0. 
Re ferences  
Abdel Kader Diagne, Walter Kasper, and Hans- 
Ulrich Krieger. 1995. Distributed parsing with 
HPSG grammars. In Proceedings of the ~th Inter- 
national Workshop on Parsing Technologies~ IW- 
PT'95, pages 79-86. 
Walter Kasper, Hans-Ulrich Krieger, JSrg Spilker, 
and Hans Webcr. 1996. From word hypotheses to
logical form: An efficient interleaved approach. In 
D. Gibbon, editor, Natural Language Processing 
and Speech Technology, pages 77-88. Mouton de 
Gruyter, Berlin. 
Bernd Kiefer and Hans-Ulrich Krieger. 2000a. 
A context-free approximation of Head-Driven 
Phrase Structure Grmnmar. In Proceedings of the 
6th International Workshop on Parsing Technolo- 
gies, IWPT2000, pages 135-146. 
Bernd Kicfcr and Hans-UMch Kricger. 2000b. Ex- 
periments with an HPSG-to-CFG approximation. 
Research report. 
Bernd Kicfcr, Hans-Ulrich Kricger, John Carroll, 
and Rob Malouf. 1999. A bag of useful techniques 
for emcient and robust parsing. In Proceedings of 
the 37th Annual Meeting of the Association for 
Computational Linguistics, pages 473-480. 
Dieter Metzing and Melanin Siegel. 1994. Zero pro- 
noun processing: Some requirements tbr a Verb- 
mobil system. Verbmobil-Memo 46. 
Carl Pollard and Ivan A. Sag. 1994. Head-Driven 
Phrase Structure Grammar. Studies in Contem- 
porary Linguistics. University of Chicago Press, 
Chicago. 
Stuart M. Shiebcr. 1985. Using restriction to extend 
parsing algorithms for complex-feature-based for- 
malisms. In Proceedings of the 23rd Annual Meet- 
ing of the Association for Computational Linguis- 
tics, pages 145-152. 
Melanin Siegel. 1999. The syntactic processing of 
particles in Japanese spoken language. In PTv- 
cecdings of the 13th Pacific Asia Confcrcncc on 
Language, Information and Computation, pages 
313-320. 
Melanic Siegel. 2000. Japanese honorification i an 
HPSG framework. In Proceedings of the l~th Pa- 
cific Asia Conference on Language, Information 
and Computation, pages 289-300. 
1050 
A Novel Disambiguation Method For Unification-Based Grammars Using
Probabilistic Context-Free Approximations
Bernd Kiefer, Hans-Ulrich Krieger, Detlef Prescher
 
kiefer|krieger|prescher  @dfki.de
Language Technology Lab, DFKI GmbH
Stuhlsatzenhausweg 3
66123 Saarbru?cken, Germany
Abstract
We present a novel disambiguation method for
unification-based grammars (UBGs). In contrast to other
methods, our approach obviates the need for probability
models on the UBG side in that it shifts the responsibil-
ity to simpler context-free models, indirectly obtained
from the UBG. Our approach has three advantages:
(i) training can be effectively done in practice, (ii)
parsing and disambiguation of context-free readings
requires only cubic time, and (iii) involved probability
distributions are mathematically clean. In an experiment
for a mid-size UBG, we show that our novel approach is
feasible. Using unsupervised training, we achieve 88%
accuracy on an exact-match task.
1 Introduction
This paper deals with the problem of how to dis-
ambiguate the readings of sentences, analyzed by a
given unification-based grammar (UBG).
Apparently, there are many different approaches
for almost as many different unification-based
grammar formalisms on the market that tackle this
difficult problem. All approaches have in common
that they try to model a probability distribution over
the readings of the UBG, which can be used to
rank the competing analyses of a given sentence;
see, e.g., Briscoe and Carroll (1993), Eisele (1994),
Brew (1995), Abney (1997), Goodman (1997), Bod
and Kaplan (1998), Johnson et al (1999), Riezler et
al. (2000), Osborne (2000), Bouma et al (2001), or
Schmid (2002).
Unfortunately, most of the proposed probability
models are not mathematically clean in that the
probabilities of all possible UBG readings do not
sum to the value 1, a problem which is discussed
intensively by Eisele (1994), Abney (1997), and
Schmid (2002).
In addition, many of the newer approaches use
log-linear (or exponential) models. Schmid (2002)
outlines a serious problem for these models: log-
linear models prevent the application of dynamic
programming methods for the computation of the
most probable parse, if complex features are incor-
porated. Therefore the run-time complexity of the
disambiguation algorithm is linear in the number of
parses of a sentence. If the number of parses grows
exponentially with the length of the sentence, these
approaches are simply impractical.
Our approach obviates the need for such models
on the UBG side in that it shifts the responsibility
to simpler CF models, indirectly obtained from the
UBG. In more detail, the kernel of our novel disam-
biguation method for UBGs consists of the appli-
cation of a context-free approximation for a given
UBG (Kiefer and Krieger, 2000) and the exploita-
tion of the standard probability model for CFGs.
In contrast to earlier approaches to disambigua-
tion for UBGs, our approach has several advantages.
Firstly, probabilistic modeling/training of context-
free grammars is theoretically well-understood and
can be effectively done in practice, using the inside-
outside algorithm (Lari and Young, 1990). Sec-
ondly, the Viterbi algorithm enables CFG pars-
ing and disambiguation in cubic time, exploiting
dynamic programming techniques to specify the
maximum-probability parse of a given sentence.
Thirdly, probability distributions over the CFG trees
are mathematically clean, if some weak conditions
for this desired behaviour are fulfilled (Booth and
Thompson, 1973).
In the rest of the paper, we present the context-
free approximation, our novel disambiguation ap-
proach, and an experiment, showing that the ap-
proach is feasible.
2 Context-Free Approximation
In this section, we briefly review a simple and intu-
itive approximation method for turning unification-
based grammars, such as HPSG (Pollard and Sag,
UBG
CFG approximation
   	
 
    
. . .    	 
   
a
b c
X
S
. . .
a
b c
X
S
a
b c
X
S
. . .
a
b c
X
S
. . . a
b c
X
S
. . .
a
b c
X
S
Figure 1: The readings of a sentence, analyzed by a UBG (top) and its CFG approximation (bottom). The picture
illustrates that (i) each UBG reading of the sentence is associated with a non-empty set of syntax trees according to
the CFG approximation, and (ii) that the sentence may have CFG trees, which can not be replayed by the UBG, since
the CFG overgenerates (or at best is a correct approximation of the UBG).
1994) or PATR-II (Shieber, 1985) into context-free
grammars (CFG). The method was introduced by
Kiefer and Krieger (2000).
The approximation method can be seen as the
construction of the least fixpoint of a certain mono-
tonic function and shares similarities with the in-
stantiation of rules in a bottom-up passive chart
parser or with partial evaluation in logic program-
ming. The basic idea of the approach is as follows.
In a first step, one generalizes the set of all lexicon
entries. The resulting structures form equivalence
classes, since they abstract from word-specific in-
formation, such as FORM or STEM. The abstraction
is specified by means of a restrictor (Shieber, 1985),
the so-called lexicon restrictor. After that, the gram-
mar rules are instantiated by unification, using the
abstracted lexicon entries and resulting in deriva-
tion trees of depth 1. The rule restrictor is applied
to each resulting feature structure (FS), removing
all information contained only in the daughters of a
rule. Additionally, the restriction gets rid of infor-
mation that will either lead to infinite growth of the
FSs or that does not constrain the search space. The
restricted FSs (together with older ones) then serve
as the basis for the next instantiation step. Again,
this gives FSs encoding a derivation, and again the
rule restrictor is applied. This process is iterated un-
til a fixpoint is reached, meaning that further itera-
tion steps will not add (or remove) new (or old) FSs
to the set of computed FSs.
Given the FSs from the fixpoint, it is then easy
to generate context-free productions, using the com-
plete FSs as symbols of the CFG; see Kiefer and
Krieger (2002). We note here that adding (and per-
haps removing) FSs during the iteration can be
achieved in different ways: either by employing
feature structure equivalence  (structural equiva-
lence) or by using FS subsumption  . It is clear that
the resulting CFGs will behave differently (see fig-
ure 4). An in-depth description of the method, con-
taining lots of details, plus a mathematical under-
pinning is presented in (Kiefer and Krieger, 2000)
and (Kiefer and Krieger, 2002). The application of
the method to a mid-size UBG of English, and large-
size HPSGs of English and Japanese is described in
(Kiefer and Krieger, 2002) and (Kiefer et al, 2000).
3 A Novel Disambiguation for UBGs
(Kiefer and Krieger, 2000) suggest that, given a
UBG, the approximated CFG can be used as a cheap
filter during a two-stage parsing approach. The idea
is to let the CFG explore the search space, whereas
the UBG deterministically replays the derivations,
proposed by the CFG. To be able to carry out the
replay, during the creation of the CF grammar, each
CF production is correlated with the UBG rules it
was produced from.
The above mentioned two-stage parsing approach
not only speeds up parsing (see figure 4), but can
also be a starting point for an efficient stochastic
parsing model, even though the UBG might encode
an infinite number of categories. Given a training
corpus, the idea is to move from the approximated
CFG to a PCFG which predicts probabilities for the
CFG trees. Clearly, the probabilities can be used for
disambiguation, and more important, for ranking of
CFG trees. The idea is, that the ranked parsing trees
can be replayed one after another by the UBG (pro-
cessing the most probable CFG trees first), estab-
lishing an order of best UBG parsing trees. Since the
approximation always yields a CFG that is a super-
set of the UBG, it might be possible that derivation
trees proposed by the PCFG can not be replayed by
the UBG. Nevertheless, this behavior does not al-
ter the ranking of reconstructed UBG parsing trees.
Figure 1 gives an overview, displaying the readings
 












v lex
TAKES TIME PP 6
TAKES LOC PP 5
TAKES ATTRIB PP 4
SUBJ SEM N TYPE 3
SUBCAT 7
OBJ SEM N TYPE 8
AGR 2
VP VFORM 9
SEM P TYPE 10
PP SEM PP TYPE 11
INVn
STEM  measure 
VFORM 12













 










vbar rule type
VP VFORM 9
SEM P TYPE 10
PP SEM PP TYPE 11
AGR 2
OBJ SEM N TYPE 8 env
SUBCAT 7 nx0vnx1
SUBJ SEM N TYPE 3
TAKES ATTRIB PP 4
TAKES LOC PP 5
TAKES TIME PP 6
INV 13
VFORM 12











 






nn lex
TAKES LOC PP 26
AGR 24
TAKES DET TYPEnull
TAKES TIME PP 28
TAKES ATTRIB PP 27
STEM  temperature 
N POST MOD TYPEnone
SEM N TYPE 8







 








np rule type
WH 23
SEM N TYPE 8
TAKES REL 25
TAKES LOC PP 26 y
AGR 24
TAKES ATTRIB PP 27 y
TAKES TIME PP 28 y
CONJn
GAPSOUT 1
GAPSIN 1









 


p lex
STEM  at 
SEM P TYPEnot onoff
OBJ SEM N TYPE 16 loc
POSTPOSITIONn
SEM PP TYPE 15



 


d lex
PRENUMBERy
AGR1pl 	 2pl 	 3pl
WHn
STEM  all 
DET TYPEnormal



 

number lex
NUM TYPEdigit
AGR1pl 	 2pl 	 3pl
TIME TYPEhour
STEM  three 


 

d rule type
DET TYPE 21 numeric
AGR 17
PRENUMBERn
WH 22


 






nn lex
N POST MOD TYPEnone
STEM  decks 
AGR 17
TAKES ATTRIB PP 18
TAKES LOC PP 19
TAKES TIME PP 20
TAKES DET TYPE 21
SEM N TYPE 16







 








np rule type
CONJn
TAKES RELy
TAKES TIME PP 20 n
TAKES LOC PP 19 n
TAKES ATTRIB PP 18 n
AGR 17 3pl
SEM N TYPE 16
GAPSIN 14
GAPSOUT 14
WH 22









 

pp rule type
GAPn
WH 22 n
SEM PP TYPE 15 loc
GAPSOUT 14 null
GAPSIN 14


 







np rule type
CONJyes no
TAKES REL 25 y
TAKES TIME PPn
TAKES LOC PPn
TAKES ATTRIB PPn
AGR 24 3sg
SEM N TYPE 8
WH 23 n
GAPSIN 1
GAPSOUT 1








 








vp rule type
VFORM 12 base
INV 13 n
GAPSOUT 1 null
GAPSIN 1
AGR 2 1pl 	 1sg 	 2pl 	 2sg 	 3pl
SUBJ SEM N TYPE 3 agent
TAKES ATTRIB PP 4
TAKES LOC PP 5 y
TAKES TIME PP 6 n
TAKES GAP PPyes no
 








.
.
.
Figure 2: One of the two readings for the sentence measure temperature at all three decks, analyzed by the Gemini
grammar. Note that the vertical dots at the top indicate an incomplete FS derivation tree. Furthermore, the FSs at the
tree nodes are massively simplified.
of a sentence, analyzed by a UBG and its CFG ap-
proximation. Using this figure, it should be clear
that a ranking of CFG trees induces a ranking of
UBG readings, even if not all CFG trees have an
associated UBG reading. We exemplify our idea in
section 4, where we disambiguate a sentence with a
PP-attachment ambiguity.
As a nice side effect, our proposed stochastic
parsing model should usually not explore the full
search space, nor should it statically estimate the
parsing results afterwards, assuming we are in-
terested in the most probable parse (or say, the
two most probable results)?the disambiguation of
UBG results is simply established by the dynamic
ordering of most probable CFG trees during the first
parsing stage.
measure
89
1058
temperature
72
1028
at
7
all
60
three
55
1018
decks
10
1033
951
929
960
873
687
304
1017
S
measure
89
1058
temperature
72
1028
960
at
7
all
60
three
55
1018
decks
10
1033
951
183
873
687
304
1017
S
Figure 3: Alternative readings licensed by the context-free approximation of the Gemini grammar.
4 Experiments
Approximation. (Dowding et al, 2001) com-
pared (Moore, 1999)?s approach to grammar ap-
proximation to (Kiefer and Krieger, 2000). As a ba-
sis for the comparison, they chose an English gram-
mar written in the Gemini/CLE formalism. The mo-
tivation for this enterprise comes from the use of
the resulting CFG as a context-free language model
for the Nuance speech recognizer. John Dowding
kindly provided the Gemini grammar and a corpus
of 500 sentences, allowing us to measure the quality
of our approximation method for a realistic mid-size
grammar, both under  and  (see section 2).1
The Gemini grammar consisted of 57 unification
rules and a small lexicon of 216 entries which ex-
panded into 425 full forms. Since the grammar al-
lows for atomic disjunctions (and makes heavy use
of them), we ended in overall 1,886 type definitions
in our system. Given the 500 sentences, the Gem-
ini grammar licensed 720 readings. We only deleted
the ARGS feature (the daughters) during the iter-
ation and found that the original UBG encodes a
context-free language, due to the fact that the iter-
ation terminates under  . This means that we have
even obtained a correct approximation of the Gem-
ini grammar. Table 4 presents the relevant numbers,
both under  and  , and shows that the ambiguity
rate for  goes up only mildly.
We note, however, that these numbers differ from
those presented in (Dowding et al, 2001). We could
not find out why their implementation produces
worse results than ours. They suggested that the use
of  is the reason for the bad behaviour of the re-
sulting grammar, but, as our figures show, this is not
1A big thank you is due to Mark-Jan Nederhof who has writ-
ten the Gemini-to-   converter and to John Dowding and Ja-
son Baldridge for fruitful discussions.
Gemini  
# readings 720 720 747
ambiguity rate 1.44 1.44 1.494
#terminals ? 152 109
#nonterminals ? 3,158 998
#rules 57 24,101 5,269
#useful rules 57 19,618 4,842
running time (secs) 32.9 14.6 9.5
run time speed-up (%) 0 55.6 71.1
Figure 4: A comparison of the approximated CFGs de-
rived under  and  . The fixpoint for  (  ) was reached
after 9 (8) iteration steps and took 5 minutes (34 seconds)
to be computed, incl. post-processing time to compute
the CF productions. The run time speed-up for two-stage
parsing is given in the last row. The measurements were
conducted on a 833 MHz Linux workstation.
true, at least not for this grammar. Of course, us-
ing  instead of  can lead to substantially less re-
strictive grammars, but when dealing with complex
grammars, there is?at the moment?no alternative
to using  due to massive space and time require-
ments of the approximation process.
Figure 2 displays one of the two readings for the
sentence measure temperature at all three decks, an-
alyzed by the Gemini grammar. The sentence is one
of the 500 sentences provided by John Dowding.
The vertical dots simply indicate that some less rele-
vant nodes of the FS derivation tree have been omit-
ted. The figure shows the reading, where the PP at
all three decks is attached to the NP temperature.
Due to space constraints, we do not show the second
reading, where the PP is attached to the VP measure
temperature.
Figure 3 shows the two syntax trees for the sen-
tence, analyzed by the context-free approximation
of the Gemini grammar, obtained by using  . It
S   1017 (0.995)
1017   304 (0.472)
304   687 (0.980)
687   873 (1.000)
873   960 (0.542)
873   183 (0.330)
960   1058 929 (0.138)
960   1058 1028 (0.335)
183   960 951 (0.042)
1058   89 (1.000)
89   measure (0.941)
929   1028 951 (0.938)
1028   72 (0.278)
72   temperature (0.635)
951   7 1033 (0.286)
7   at (0.963)
1033   1018 10 (0.706)
1018   60 55 (0.581)
60   all (0.818)
55   three (0.111)
10   decks (1.000)
Figure 5: Fragment of the PCFG. The values in paren-
thesis are probabilities for grammar rules, gathered after
two training iterations with the inside-outside algorithm.
is worth noting that both readings of the CFG ap-
proximation differ in PP attachment, in the same
manner as the readings analyzed by the UBG it-
self. In the figure, all non-terminals are simply dis-
played as numbers, but each number represents a
fairly complex feature structure, which is, in gen-
eral, slightly less informative than an associated tree
node of a possible FS derivation tree of the given
Gemini grammar for two reasons. Firstly, the use
of the  operation as a test generalizes informa-
tion during the approximation process. In a more
complex UBG grammar, the restrictors would have
deleted even more information. Secondly, the flow
of information in a local tree from the mother to the
daughter node will not be reflected because the ap-
proximation process works strictly bottom up from
the lexicon entries.
Training of the CFG approximation. A sample
of sentences serves as input to the inside-outside
algorithm, the standard algorithm for unsupervised
training of PCFGs (Lari and Young, 1990). The
given corpus of 500 sentences was divided into a
training corpus (90%, i.e., 450 sentences) and a test-
ing corpus (10%, i.e., 50 sentences). This standard
procedure enables us (i) to apply the inside-outside
algorithm to the training corpus, and (ii) to eval-
uate the resulting probabilistic context-free gram-
mars on the testing corpus. We linguistically eval-
uated the maximum-probability parses of all sen-
tences in the testing corpus (see section 5). For un-
supervised training and parsing, we used the imple-
mentation of Schmid (1999).
Figure 5 shows a fragment of the probabilistic
context-free approximation. The probabilities of the
grammar rules are extracted after several training it-
erations with the inside-outside algorithm using the
training corpus of 450 sentences.
Disambiguation using maximum-probability
parses. In contrast to most approaches to stochas-
tic modeling of UBGs, PCFGs can be very easily
used to assign probabilities to the readings of a
given sentence: the probability of a syntax tree (the
reading) is the product of the probabilities of all
context-free rules occurring in the tree.
For example, the two readings of the sentence
measure temperature at all three decks, as dis-
played in figure 3, have the following probabilities:

	 (first reading on the left-hand side)
and 		 (second reading on the right-hand
side). The maximum-probability parse is therefore
the syntax-tree on the left-hand side of figure 3,
which is the reading, where the PP at all three decks
is attached to the NP temperature.
A closer look on the PCFG fragment shows that
the main contribution to this result comes from the
two rules 929   1028 951 (0.938) and 183   960 951
(0.042). Here, the probabilities encode the statistical
finding that PP-to-NP attachments can be expected
more frequently than PP-to-VP attachments, if the
context-free approximation of the Gemini grammar
is used to analyze the given corpus of 500 sentences.
5 Evaluation
Evaluation task. To evaluate our models, we used
the testing corpus mentioned in section 4. In a next
step, the correct parse was indicated by a human dis-
ambiguator, according to the intended reading. The
average ambiguity of this corpus is about 1.4 parses
per sentence, for sentences with about 5.8 words on
average.
Our statistical disambiguation method was tested
on an exact match task, where exact correspondence
of the manually annotated correct parse and the
most probable parse is checked. Performance on this
evaluation task was assessed according to the fol-
lowing evaluation measure:
precision An Integrated Architecture for Shallow and Deep Processing
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,
Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker and Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg 3
Saarbru?cken, Germany
whiteboard@dfki.de
Abstract
We present an architecture for the integra-
tion of shallow and deep NLP components
which is aimed at flexible combination
of different language technologies for a
range of practical current and future appli-
cations. In particular, we describe the inte-
gration of a high-level HPSG parsing sys-
tem with different high-performance shal-
low components, ranging from named en-
tity recognition to chunk parsing and shal-
low clause recognition. The NLP com-
ponents enrich a representation of natu-
ral language text with layers of new XML
meta-information using a single shared
data structure, called the text chart. We de-
scribe details of the integration methods,
and show how information extraction and
language checking applications for real-
world German text benefit from a deep
grammatical analysis.
1 Introduction
Over the last ten years or so, the trend in application-
oriented natural language processing (e.g., in the
area of term, information, and answer extraction)
has been to argue that for many purposes, shallow
natural language processing (SNLP) of texts can
provide sufficient information for highly accurate
and useful tasks to be carried out. Since the emer-
gence of shallow techniques and the proof of their
utility, the focus has been to exploit these technolo-
gies to the maximum, often ignoring certain com-
plex issues, e.g. those which are typically well han-
dled by deep NLP systems. Up to now, deep natural
language processing (DNLP) has not played a sig-
nificant role in the area of industrial NLP applica-
tions, since this technology often suffers from insuf-
ficient robustness and throughput, when confronted
with large quantities of unrestricted text.
Current information extractions (IE) systems
therefore do not attempt an exhaustive DNLP analy-
sis of all aspects of a text, but rather try to analyse or
?understand? only those text passages that contain
relevant information, thereby warranting speed and
robustness wrt. unrestricted NL text. What exactly
counts as relevant is explicitly defined by means
of highly detailed domain-specific lexical entries
and/or rules, which perform the required mappings
from NL utterances to corresponding domain knowl-
edge. However, this ?fine-tuning? wrt. a particular
application appears to be the major obstacle when
adapting a given shallow IE system to another do-
main or when dealing with the extraction of com-
plex ?scenario-based? relational structures. In fact,
(Appelt and Israel, 1997) have shown that the cur-
rent IE technology seems to have an upper perfor-
mance level of less than 60% in such cases. It seems
reasonable to assume that if a more accurate analy-
sis of structural linguistic relationships could be pro-
vided (e.g., grammatical functions, referential rela-
tionships), this barrier might be overcome. Actually,
the growing market needs in the wide area of intel-
ligent information management systems seem to re-
quest such a break-through.
In this paper we will argue that the quality of cur-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448.
                         Proceedings of the 40th Annual Meeting of the Association for
rent SNLP-based applications can be improved by
integrating DNLP on demand in a focussed manner,
and we will present a system that combines the fine-
grained anaysis provided by HPSG parsing with a
high-performance SNLP system into a generic and
flexible NLP architecture.
1.1 Integration Scenarios
Owing to the fact that deep and shallow technologies
are complementary in nature, integration is a non-
trivial task: while SNLP shows its strength in the
areas of efficiency and robustness, these aspects are
problematic for DNLP systems. On the other hand,
DNLP can deliver highly precise and fine-grained
linguistic analyses. The challenge for integration is
to combine these two paradigms according to their
virtues.
Probably the most straightforward way to inte-
grate the two is an architecture in which shallow and
deep components run in parallel, using the results of
DNLP, whenever available. While this kind of ap-
proach is certainly feasible for a real-time applica-
tion such as Verbmobil, it is not ideal for processing
large quantities of text: due to the difference in pro-
cessing speed, shallow and deep NLP soon run out
of sync. To compensate, one can imagine two possi-
ble remedies: either to optimize for precision, or for
speed. The drawback of the former strategy is that
the overall speed will equal the speed of the slow-
est component, whereas in case of the latter, DNLP
will almost always time out, such that overall preci-
sion will hardly be distinguishable from a shallow-
only system. What is thus called for is an integrated,
flexible architecture where components can play at
their strengths. Partial analyses from SNLP can be
used to identify relevant candidates for the focussed
use of DNLP, based on task or domain-specific crite-
ria. Furthermore, such an integrated approach opens
up the possibility to address the issue of robustness
by using shallow analyses (e.g., term recognition)
to increase the coverage of the deep parser, thereby
avoiding a duplication of efforts. Likewise, integra-
tion at the phrasal level can be used to guide the
deep parser towards the most likely syntactic anal-
ysis, leading, as it is hoped, to a considerable speed-
up.
shallow
NLP
components
NLP
deep
components internal repr.
layer
multi
chart
annot.
XML
external repr.
generic OOP
component
interface
WHAM
application
specification
input and
result
Figure 1: The WHITEBOARD architecture.
2 Architecture
The WHITEBOARD architecture defines a platform
that integrates the different NLP components by en-
riching an input document through XML annota-
tions. XML is used as a uniform way of represent-
ing and keeping all results of the various processing
components and to support a transparent software
infrastructure for LT-based applications. It is known
that interesting linguistic information ?especially
when considering DNLP? cannot efficiently be
represented within the basic XML markup frame-
work (?typed parentheses structure?), e.g., linguistic
phenomena like coreferences, ambiguous readings,
and discontinuous constituents. The WHITEBOARD
architecture employs a distributed multi-level repre-
sentation of different annotations. Instead of trans-
lating all complex structures into one XML docu-
ment, they are stored in different annotation layers
(possibly non-XML, e.g. feature structures). Hyper-
links and ?span? information together support effi-
cient access between layers. Linguistic information
of common interest (e.g. constituent structure ex-
tracted from HPSG feature structures) is available in
XML format with hyperlinks to full feature struc-
ture representations externally stored in correspond-
ing data files.
Fig. 1 gives an overview of the architecture of
the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and
a specification describing the components and con-
figuration options requested. The core WHAM en-
gine has an XML markup storage (external ?offline?
representation), and an internal ?online? multi-level
annotation chart (index-sequential access). Follow-
ing the trichotomy of NLP data representation mod-
els in (Cunningham et al, 1997), the XML markup
contains additive information, while the multi-level
chart contains positional and abstraction-based in-
formation, e.g., feature structures representing NLP
entities in a uniform, linguistically motivated form.
Applications and the integrated components ac-
cess the WHAM results through an object-oriented
programming (OOP) interface which is designed
as general as possible in order to abstract from
component-specific details (but preserving shallow
and deep paradigms). The interfaces of the actu-
ally integrated components form subclasses of the
generic interface. New components can be inte-
grated by implementing this interface and specifying
DTDs and/or transformation rules for the chart.
The OOP interface consists of iterators that walk
through the different annotation levels (e.g., token
spans, sentences), reference and seek operators that
allow to switch to corresponding annotations on a
different level (e.g., give all tokens of the current
sentence, or move to next named entity starting
from a given token position), and accessor meth-
ods that return the linguistic information contained
in the chart. Similarily, general methods support
navigating the type system and feature structures of
the DNLP components. The resulting output of the
WHAM can be accessed via the OOP interface or as
XML markup.
The WHAM interface operations are not only
used to implement NLP component-based applica-
tions, but also for the integration of deep and shallow
processing components itself.
2.1 Components
2.1.1 Shallow NL component
Shallow analysis is performed by SPPC, a rule-
based system which consists of a cascade of
weighted finite?state components responsible for
performing subsequent steps of the linguistic anal-
ysis, including: fine-grained tokenization, lexico-
morphological analysis, part-of-speech filtering,
named entity (NE) recognition, sentence bound-
ary detection, chunk and subclause recognition,
see (Piskorski and Neumann, 2000; Neumann and
Piskorski, 2002) for details. SPPC is capable of pro-
cessing vast amounts of textual data robustly and ef-
ficiently (ca. 30,000 words per second in standard
PC environment). We will briefly describe the SPPC
components which are currently integrated with the
deep components.
Each token identified by a tokenizer as a poten-
tial word form is morphologically analyzed. For
each token, its lexical information (list of valid read-
ings including stem, part-of-speech and inflection
information) is computed using a fullform lexicon
of about 700,000 entries that has been compiled out
from a stem lexicon of about 120,000 lemmas. Af-
ter morphological processing, POS disambiguation
rules are applied which compute a preferred read-
ing for each token, while the deep components can
back off to all readings. NE recognition is based on
simple pattern matching techniques. Proper names
(organizations, persons, locations), temporal expres-
sions and quantities can be recognized with an av-
erage precision of almost 96% and recall of 85%.
Furthermore, a NE?specific reference resolution is
performed through the use of a dynamic lexicon
which stores abbreviated variants of previously rec-
ognized named entities. Finally, the system splits
the text into sentences by applying only few, but
highly accurate contextual rules for filtering implau-
sible punctuation signs. These rules benefit directly
from NE recognition which already performs re-
stricted punctuation disambiguation.
2.1.2 Deep NL component
The HPSG Grammar is based on a large?scale
grammar for German (Mu?ller, 1999), which was
further developed in the VERBMOBIL project for
translation of spoken language (Mu?ller and Kasper,
2000). After VERBMOBIL the grammar was adapted
to the requirements of the LKB/PET system (Copes-
take, 1999), and to written text, i.e., extended with
constructions like free relative clauses that were ir-
relevant in the VERBMOBIL scenario.
The grammar consists of a rich hierarchy of
5,069 lexical and phrasal types. The core grammar
contains 23 rule schemata, 7 special verb move-
ment rules, and 17 domain specific rules. All rule
schemata are unary or binary branching. The lexicon
contains 38,549 stem entries, from which more than
70% were semi-automatically acquired from the an-
notated NEGRA corpus (Brants et al, 1999).
The grammar parses full sentences, but also other
kinds of maximal projections. In cases where no full
analysis of the input can be provided, analyses of
fragments are handed over to subsequent modules.
Such fragments consist of maximal projections or
single words.
The HPSG analysis system currently integrated
in the WHITEBOARD system is PET (Callmeier,
2000). Initially, PET was built to experiment
with different techniques and strategies to process
unification-based grammars. The resulting sys-
tem provides efficient implementations of the best
known techniques for unification and parsing.
As an experimental system, the original design
lacked open interfaces for flexible integration with
external components. For instance, in the beginning
of the WHITEBOARD project the system only ac-
cepted fullform lexica and string input. In collabora-
tion with Ulrich Callmeier the system was extended.
Instead of single word input, input items can now
be complex, overlapping and ambiguous, i.e. essen-
tially word graphs. We added dynamic creation of
atomic type symbols, e.g., to be able to add arbitrary
symbols to feature structures. With these enhance-
ments, it is possible to build flexible interfaces to
external components like morphology, tokenization,
named entity recognition, etc.
3 Integration
Morphology and POS The coupling between the
morphology delivered by SPPC and the input needed
for the German HPSG was easily established. The
morphological classes of German are mapped onto
HPSG types which expand to small feature struc-
tures representing the morphological information in
a compact way. A mapping to the output of SPPC
was automatically created by identifying the corre-
sponding output classes.
Currently, POS tagging is used in two ways. First,
lexicon entries that are marked as preferred by the
shallow component are assigned higher priority than
the rest. Thus, the probability of finding the cor-
rect reading early should increase without excluding
any reading. Second, if for an input item no entry is
found in the HPSG lexicon, we automatically create
a default entry, based on the part?of?speech of the
preferred reading. This increases robustness, while
avoiding increase in ambiguity.
Named Entity Recognition Writing HPSG gram-
mars for the whole range of NE expressions etc. is
a tedious and not very promising task. They typi-
cally vary across text sorts and domains, and would
require modularized subgrammars that can be easily
exchanged without interfering with the general core.
This can only be realized by using a type interface
where a class of named entities is encoded by a gen-
eral HPSG type which expands to a feature structure
used in parsing. We exploit such a type interface for
coupling shallow and deep processing. The classes
of named entities delivered by shallow processing
are mapped to HPSG types. However, some fine-
tuning is required whenever deep and shallow pro-
cessing differ in the amount of input material they
assign to a named entity.
An alternative strategy is used for complex syn-
tactic phrases containing NEs, e.g., PPs describ-
ing time spans etc. It is based on ideas from
Explanation?based Learning (EBL, see (Tadepalli
and Natarajan, 1996)) for natural language analy-
sis, where analysis trees are retrieved on the basis
of the surface string. In our case, the part-of-speech
sequence of NEs recognised by shallow analysis is
used to retrieve pre-built feature structures. These
structures are produced by extracting NEs from a
corpus and processing them directly by the deep
component. If a correct analysis is delivered, the
lexical parts of the analysis, which are specific for
the input item, are deleted. We obtain a sceletal
analysis which is underspecified with respect to the
concrete input items. The part-of-speech sequence
of the original input forms the access key for this
structure. In the application phase, the underspeci-
fied feature structure is retrieved and the empty slots
for the input items are filled on the basis of the con-
crete input.
The advantage of this approach lies in the more
elaborate semantics of the resulting feature struc-
tures for DNLP, while avoiding the necessity of
adding each and every single name to the HPSG lex-
icon. Instead, good coverage and high precision can
be achieved using prototypical entries.
Lexical Semantics When first applying the origi-
nal VERBMOBIL HPSG grammar to business news
articles, the result was that 78.49% of the miss-
ing lexical items were nouns (ignoring NEs). In
the integrated system, unknown nouns and NEs can
be recognized by SPPC, which determines morpho-
syntactic information. It is essential for the deep sys-
tem to associate nouns with their semantic sorts both
for semantics construction, and for providing se-
mantically based selectional restrictions to help con-
straining the search space during deep parsing. Ger-
maNet (Hamp and Feldweg, 1997) is a large lexical
database, where words are associated with POS in-
formation and semantic sorts, which are organized in
a fine-grained hierarchy. The HPSG lexicon, on the
other hand, is comparatively small and has a more
coarse-grained semantic classification.
To provide the missing sort information when re-
covering unknown noun entries via SPPC, a map-
ping from the GermaNet semantic classification to
the HPSG semantic classification (Siegel et al,
2001) is applied which has been automatically ac-
quired. The training material for this learning pro-
cess are those words that are both annotated with se-
mantic sorts in the HPSG lexicon and with synsets
of GermaNet. The learning algorithm computes a
mapping relevance measure for associating seman-
tic concepts in GermaNet with semantic sorts in the
HPSG lexicon. For evaluation, we examined a cor-
pus of 4664 nouns extracted from business news
that were not contained in the HPSG lexicon. 2312
of these were known in GermaNet, where they are
assigned 2811 senses. With the learned mapping,
the GermaNet senses were automatically mapped to
HPSG semantic sorts. The evaluation of the map-
ping accuracy yields promising results: In 76.52%
of the cases the computed sort with the highest rel-
evance probability was correct. In the remaining
20.70% of the cases, the correct sort was among the
first three sorts.
3.1 Integration on Phrasal Level
In the previous paragraphs we described strategies
for integration of shallow and deep processing where
the focus is on improving DNLP in the domain of
lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for
the integration of shallow and deep analysis at the
length cover- complete LP LR 0CB   2CB
age match
  40 100 80.4 93.4 92.9 92.1 98.9
 40 99.8 78.6 92.4 92.2 90.7 98.5
Training: 16,000 NEGRA sentences
Testing: 1,058 NEGRA sentences
Figure 2: Stochastic topological parsing: results
level of phrasal syntax by guiding the deep syntac-
tic parser towards a partial pre-partitioning of com-
plex sentences provided by shallow analysis sys-
tems. This strategy can reduce the search space, and
enhance parsing efficiency of DNLP.
Stochastic Topological Parsing The traditional
syntactic model of topological fields divides basic
clauses into distinct fields: so-called pre-, middle-
and post-fields, delimited by verbal or senten-
tial markers. This topological model of German
clause structure is underspecified or partial as to
non-sentential constituent boundaries, but provides
a linguistically well-motivated, and theory-neutral
macrostructure for complex sentences. Due to its
linguistic underpinning the topological model pro-
vides a pre-partitioning of complex sentences that is
(i) highly compatible with deep syntactic structures
and (ii) maximally effective to increase parsing ef-
ficiency. At the same time (iii) partiality regarding
the constituency of non-sentential material ensures
the important aspects of robustness, coverage, and
processing efficiency.
In (Becker and Frank, 2002) we present a corpus-
driven stochastic topological parser for German,
based on a topological restructuring of the NEGRA
corpus (Brants et al, 1999). For topological tree-
bank conversion we build on methods and results
in (Frank, 2001). The stochastic topological parser
follows the probabilistic model of non-lexicalised
PCFGs (Charniak, 1996). Due to abstraction from
constituency decisions at the sub-sentential level,
and the essentially POS-driven nature of topologi-
cal structure, this rather simple probabilistic model
yields surprisingly high figures of accuracy and cov-
erage (see Fig.2 and (Becker and Frank, 2002) for
more detail), while context-free parsing guarantees
efficient processing.
The next step is to elaborate a (partial) map-
ping of shallow topological and deep syntactic struc-
tures that is maximally effective for preference-gui-
Topological Structure:
CL-V2
VF-TOPIC LK-FIN MF RK-t
NN VVFIN ADV NN PREP NN VVFIN
[ 	 [ 
	 Peter] [ 
 i?t] [ 
 gerne Wu?rstchen mit Kartoffelsalat] [ Integrating Information Extraction and Automatic Hyperlinking 
 
Stephan Busemann, Witold'UR G \ VNL Hans-Ulrich Krieger,  
Jakub Piskorski, Ulrich Sch?fer, Hans Uszkoreit, Feiyu Xu 
German Research Center for Artificial Intelligence (DFKI GmbH) 
Stuhlsatzenhausweg 3, D-66123 Saarbr?cken, Germany 
sprout@dfki.de 
 
 
Abstract 
This paper presents a novel information sys-
tem integrating advanced information extrac-
tion technology and automatic hyper-linking. 
Extracted entities are mapped into a domain 
ontology that relates concepts to a selection of 
hyperlinks. For information extraction, we use 
SProUT, a generic platform for the develop-
ment and use of multilingual text processing 
components. By combining finite-state and 
unification-based formalisms, the grammar 
formalism used in SProUT offers both pro-
cessing efficiency and a high degree of decal-
rativeness. The ExtraLink demo system show-
cases the extraction of relevant concepts from 
German texts in the tourism domain, offering 
the direct connection to associated web docu-
ments on demand.   
1 Introduction 
The utilization of language technology for the 
creation of hyperlinks has a long history (e.g., 
Allen et al, 1993). Information extraction (IE) is a 
technology that can be applied to identifying both 
sources and targets of new hyperlinks. IE systems 
are becoming commercially viable in supporting 
diverse information discovery and management 
tasks. Similarly, automatic hyperlinking is a matu-
ring technology designed to interrelate pieces of 
information, using ontologies to define the rela-
tionships. With ExtraLink, we present a novel 
information system that integrates both technolo-
gies in order to reach at an improved level of 
informativeness and comfort. Extraction and link 
generation occur completely in the background. 
Entities identified by the IE system are mapped 
into a domain ontology that relates concepts to a 
structured selection of predefined hyperlinks, 
which can be directly visualized on demand using 
a standard web browser. This way, the user can, 
while reading a text, immediately link up textual 
information to the Internet or to any other docu-
ment base without accessing a search engine.  
The quality of the link targets is much higher 
than with standard search engines since, first of all, 
only domain-specific interpretations are sought, 
and second, the ontology provides additional 
structure, including related information. 
ExtraLink uses as its IE system SProUT, a gene-
ric multilingual shallow analysis platform, which 
currently provides linguistic processing resources 
for English, German, Italian, French, Spanish, 
Czech, Polish, Japanese, and Chinese (Becker et 
al., 2002). SProUT is used for tokenization, mor-
phological analysis, and named entity recognition 
in free texts. In Section 2 to 4, we describe innova-
tive features of SProUT. Section 5 gives details 
about the ExtraLink demonstrator. 
2 Integrating Typed Feature Structures 
and Finite State Machines 
The main motivation for developing SProUT 
comes from the need to have a system that (i) 
allows a flexible integration of different processing 
modules and (ii) to find a good trade-off between 
processing efficiency and linguistic expressive-
ness. On the one hand, very efficient finite state 
devices have been successfully applied to real-
world applications. On the other hand, unification-
based grammars (UBGs) are designed to capture 
fine-grained syntactic and semantic constraints, 
resulting in better descriptions of natural language 
phenomena. In contrast to finite state devices, 
unification-based grammars are also assumed to be 
more transparent and more easily modifiable. 
SProUT?s mission is to take the best from these 
two worlds, having a finite state machine that 
operates on typed feature structures (TFSs). I.e., 
transduction rules in SProUT do not rely on simple 
atomic symbols, but instead on TFSs, where the 
left-hand side of a rule is a regular expression over 
TFSs, representing the recognition pattern, and the 
right-hand side is a sequence of TFSs, specifying 
the output structure. Consequently, equality of 
atomic symbols is replaced by unifiability of TFSs 
and the output is constructed using TFS unification 
w.r.t. a type hierarchy. Such rules not only recog-
nize and classify patterns, but also extract frag-
ments embedded in the patterns and fill output 
templates with them. 
Standard finite state techniques such as minimi-
zation and determinization are no longer applicable 
here, due to the fact that edges in our automata are 
annotated by TFSs, instead of atomic symbols. 
However, not every outgoing edge in such an 
automaton must be analyzed, since TFS annota-
tions can be arranged under subsumption, and the 
failure of a general edge automatically causes the 
failure of several, more specialized edges, without 
applying the unifiability test. Such information can 
in fact be precompiled. This and other optimization 
techniques are described in (Krieger and Piskorski, 
2003). 
When compared to symbol-based finite state 
approaches, our method leads to smaller grammars 
and automata, which usually better approximate a 
given language.  
3 XTDL ? The Formalism in SProUT 
XTDL combines two well-known frameworks, 
viz., typed feature structures and regular ex-
pressions. XTDL is defined on top of TDL, a defi-
nition language for TFSs (Krieger and Sch?fer, 
1994) that is used as a descriptive device in several 
grammar systems (LKB, PAGE, PET).  
Apart from the integration into the rule 
definitions, we also employ TDL in SProUT for 
the establishment of a type hierarchy of linguistic 
entities. In the example definition below, the 
morph type inherits from sign and introduces three 
more morphologically motivated attributes with 
the corresponding typed values: 
morph := sign & [ POS  atom, STEM atom, INFL infl ]. 
A rule in XTDL is straightforwardly defined as 
a recognition pattern on the left-hand side, written 
as a regular expression, and an output description 
on the right-hand side. A named label serves as a 
handle to the rule. Regular expressions over TFSs 
describe sequential successions of linguistic signs. 
We provide a couple of standard operators. Con-
catenation is expressed by consecutive items. Dis-
junction, Kleene star, Kleene plus, and optionality 
are represented by the operators |, *, +, and ?, resp. 
{n} after an expression denotes an n-fold repetition. 
{m,n} repeats at least m times and at most n times. 
The XTDL grammar rule below may illustrate 
the syntax. It describes a sequence of morphologi-
cally analyzed tokens (of type morph). The first 
TFS matches one or zero items (?) with part-of-
speech Determiner. Then, zero or more Adjective 
items are matched (*). Finally, one or two Noun 
items ({1,2}) are consumed. The use of a variable 
(e.g., #1) in different places establishes a 
coreference between features. This example enfor-
ces agreement in case, number, and gender for the 
matched items. Eventually, the description on the 
RHS creates a feature structure of type phrase, 
where the category is coreferent with the category 
Noun of the right-most token(s), and the agreement 
features corefer to features of the morph tokens. 
 np :> 
   (morph & [ POS  Determiner, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )?  
   (morph & [ POS  Adjective, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )*  
   (morph & [ POS  Noun & #4, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] ){1,2} 
 -> phrase & [CAT #4, 
 AGR agr & [CASE #1, NUM #2, GEN #3 ]]. 
 
The choice of TDL has a couple of advantages. 
TFSs as such provide a rich descriptive language 
over linguistic structures and allow for a fine-
grained inspection of input items. They represent a 
generalization over pure atomic symbols. Unifia-
bility as a test criterion in a transition is a generali-
zation over symbol equality. Coreferences in 
feature structures express structural identity. Their 
properties are exploited in two ways. They provide 
a stronger expressiveness, since they create 
dynamic value assignments on the automaton 
transitions and thus exceed the strict locality of 
constraints in an atomic symbol approach. Further-
more, coreferences serve as a means of information 
transport into the output description on the RHS of 
the rule. Finally, the choice of feature structures as 
primary citizens of the information domain makes 
composition of modules very simple, since input 
and output are all of the same abstract data type.  
Functional (in contrast to regular) operators are 
a door to the outside world of SProUT.  They 
either serve as predicates, helping to locate 
complex tests that might cancel a rule application, 
or they construct new material, involving pieces of 
information from the LHS of a rule.  The sketch of 
a rule below transfers numerals into their 
corresponding digits using the functional operator 
normalize() that is defined externally. For instance, 
"one" is mapped onto "1", "two" onto "2", etc. 
 
  ?  numeral & [ SURFACE #surf, ... ] .?  -> 
  digit & [ ID #id, ... ],  where #id = normalize(#surf). 
4 The SProUT System  
The core of SProUT comprises of the following 
components: (i) a finite-state machine toolkit for 
building, combining, and optimizing finite-state 
devices; (ii) a flexible XML-based regular com-
piler for converting regular patterns into their cor-
responding compressed finite-state representation 
(Piskorski et al, 2002); (iii) a JTFS package which 
provides standard operations for constructing and 
manipulating TFSs; and (iv) an XTDL grammar 
interpreter. 
Currently, SProUT offers three online compo-
nents: a tokenizer, a gazetteer, and a morphological 
analyzer. The tokenizer maps character sequences 
to tokens and performs fine-grained token classifi-
cation. The gazetteer recognizes named entities 
based on static named entity lexica.  
The morphology unit provides lexical resources 
for English, German (equipped with online shallow 
compound recognition), French, Italian, and 
Spanish, which were compiled from the full form 
lexica of MMorph (Petitpierre and Russell, 1995). 
Considering Slavic languages, a component for 
Czech presented in (Haji?, 2001), and Morfeusz 
(Przepi?rkowski and Wolinski, 2003) for Polish. 
For Asian languages, we integrated Chasen 
(Asahara and Matsumoto, 2000) for Japanese and 
Shanxi (Liu, 2000) for Chinese.  
The XTDL-based grammar engineering plat-
form has been used to define grammars for 
English, German, French, Spanish, Chinese and 
Japanese allowing for named entity recognition 
and extraction. To guarantee a comparable 
coverage, and to ease evaluation, an extension of 
the MUC-7 standard for entities has been adopted.   
 
ne-person := enamex & [ TITLE list-of-strings, 
                          GIVEN_NAME list-of-strings, 
                          SURNAME list-of-strings, 
                                  P-POSITION list-of-strings, 
                          NAME-SUFFIX string, 
                                    DESCRIPTOR string ]. 
 
Given the expressiveness of XTDL expressions, 
MUC-7/MET-2 named entity types can be 
enhanced with more complex internal structures. 
For instance, a person name ne-person is defined 
as a subtype of enamex with the above structure. 
The named entity grammars can handle types 
such as person, location, organization, time point, 
time span (instead of date and time defined by 
MUC), percentage, and currency.  
The core system together with the grammars 
forms a basis for developing applications. SProUT 
is being used by several sites in both research and 
industrial contexts. 
A component for resolving coreferent named 
entities disambiguates and classifies incomplete 
named entities via dynamic lexicon search, e.g., 
Microsoft is coreferent with Microsoft corporation 
and is thus correctly classified as an organization. 
5 ExtraLink: Integrating Information 
Extraction and Automatic Hyperlinking  
A methodology for automatically enriching web 
documents with typed hyperlinks has been develo-
ped and applied to several domains, among them 
the domain of tourism information. A core compo-
nent is a domain ontology describing tourist sites 
in terms of sights, accommodations, restaurants, 
cultural events, etc. The ontology was specialized 
for major European tourism sites and regions (see 
Figure 1). It is associated with a large selection of  
 
 
 
Figure 1: Link Target Page (excerpt). The instance the 
web document is associated to (Isle of Capri) is shown 
on the left, together with neighboring concepts in the 
ontology, which the user can navigate through. 
 
link targets gathered, intellectually selected and 
continuously verified. Although language techno-
logy could also be employed to prime target 
selection, for most applications quality require-
ments demand the expertise of a domain specialist. 
In the case of the tourism domain, the selection 
was performed by a travel business professional. 
The system is equipped with an XML interface and 
accessible as a server. 
The ExtraLink GUI marks the relevant entities 
(usually locations) identified by SProUT (see 
second window on the left in Figure 2). Clicking 
on a marked expression causes a query related to 
the entity being shipped to the server. Coreferent 
concepts are handled as expanded queries. The 
server returns a set of links structured according to 
the ontology, which is presented in the ExtraLink 
GUI (Figure 2). The user can choose to visualize 
any link target in a new browser window that also 
shows the respective subsection of the ontology in 
an indented tree notation (see Figure 1).  
 
 
 
Figure 2: ExtraLink GUI. The links in the right-hand 
window are generated after clicking on the marked 
named entity for Lisbon (marked in dark). The bottom 
left window shows the SProUT result for ?Lissabon?. 
 
The ExtraLink demonstrator has been imple-
mented in Java and C++, and runs under both MS 
Windows and Linux. It is operational for German, 
but it can easily be extended to other languages 
covered by SProUT. This involves the adaptation 
of the mapping into the ontology and a multi-
lingual presentation of the ontology in the link 
target page. 
Acknowledgements 
Work on ExtraLink has been partially funded 
through grants by the German Ministry for 
Education, Science, Research and Technology 
(BMBF) to the project Whiteboard (contract 01 IW 
002), by the EC to the project Airforce (contract 
IST-12179), and by the state of the Saarland to the 
project SATOURN. We are indebted to Tim vor 
der Br?ck, Thierry Declerck, Adrian Raschip, and 
Christian Woldsen for their contributions to 
developing ExtraLink. 
References 
J. Allen, J. Davis, D. Krafft, D. Rus, and D. Subrama-
nian. Information agents for building hyperlinks. J. 
Mayfield and C. Nicholas: Proceedings of the Work-
shop on Intelligent Hypertext, 1993. 
M. Asahara and Y. Matsumoto. Extended models and 
tools for high-performance part-of-speech tagger. 
Proceedings of  COLING, 21-27, 2000. 
0 %HFNHU : 'UR G \ VNL +-U. Krieger, J. 
Piskorski, U. Sch?fer, F. Xu. SProUT?Shallow Pro-
cessing with Typed Feature Structures and Unifica-
tion. In Proceedings of  ICON, 2002. 
J. +DML? Disambiguation of rich inflection?compu-
tational morphology of Czech. Prague Karolinum, 
Charles University Press, 2001. 
H.-U. Krieger and U. Sch?fer. TDL?A Type Description 
Language for Constraint-Based Grammars. Procee-
dings of COLING, 893-899, 1994. 
H.-U. Krieger and J. Piskorski. Speed-up methods for 
complex annotated finite state grammars. DFKI 
Report, 2003. 
K. Liu. Research of automatic Chinese word segmen-
tation. Proceedings of ILT&CIP, 2001. 
D. Petitpierre and G. Russell. MMORPH?the Multext 
morphology program. Multext deliverable report 
2.3.1. ISSCO, University of Geneva, 1995. 
J. PiskRUVNL:'UR G \ VNL );X DQG2 6FKHUIA 
flexible XML-based regular compiler for creation 
and converting linguistic resources. Proceedings of 
LREC 2002, Las Palmas, Spain, 2002. 
A. Przepi?rkowski and M. Wolinski. The Unbearable 
Lightness of Tagging: A Case Study in Morphosyn-
tactic Tagging of Polish. Proceedings of the Work-
shop on Linguistically Interpreted Corpora, 2003. 
 
SDL?A Description Language for Building NLP Systems
Hans-Ulrich Krieger
Language Technology Lab
German Research Center for Arti?cial Intelligence (DFKI)
Stuhlsatzenhausweg 3, D-66123 Saarbru?cken, Germany
krieger@dfki.de
Abstract
We present the system description language
SDL that offers a declarative way of specify-
ing new complex NLP systems from already
existing modules with the help of three oper-
ators: sequence, parallelism, and unrestricted
iteration. Given a system description and mod-
ules that implement a minimal interface, the
SDL compiler returns a running Java program
which realizes exactly the desired behavior of
the original speci?cation. The execution se-
mantics of SDL is complemented by a precise
formal semantics, de?ned in terms of concepts
of function theory. The SDL compiler is part
of the SProUT shallow language platform, a
system for the development and processing of
multilingual resources.
1 Introduction
In this paper, we focus on a general system description
language, called SDL, which allows the declarative spec-
i?cation of NLP systems from a set of already existing
base modules. Assuming that each initial module imple-
ments a minimal interface of methods, a new complex
system is composed with the help of three operators, re-
alizing a sequence of two modules, a (quasi-)parallel ex-
ecution of several modules, and a potentially unrestricted
self-application of a single module. Communication be-
tween independent modules is decoupled by a mediator
which is sensitive to the operators connecting the mod-
ules and to the modules themselves. To put it in an-
other way: new systems can be de?ned by simply putting
together existing independent modules, sharing a com-
mon interface. The interface assumes functionality which
modules usually already provide, such as set input, clear
internal state, start computation, etc. It is clear that such
an approach permits ?exible experimentation with dif-
ferent software architectures during the set up of a new
(NLP) system. The use of mediators furthermore guar-
antees that an independently developed module will stay
independent when integrated into a new system. In the
worst case, only the mediator needs to be modi?ed or up-
graded, resp. In many cases, not even a modi?cation of
the mediator is necessary. The execution semantics of
SDL is complemented by an abstract semantics, de?ned
in terms of concepts of function theory, such as Cartesian
product, functional composition & application, Lambda
abstraction, and unbounded minimization. Contrary to
an interpreted approach to system speci?cation, our ap-
proach compiles a syntactically well-formed SDL expres-
sion into a Java program. This code might then be incor-
porated into a larger system or might be directly compiled
by the Java compiler, resulting in an executable ?le. This
strategy has two advantages: ?rstly, the compiled Java
code is faster than an interpretation of the corresponding
SDL expression, and secondly, the generated Java code
can be modi?ed or even extended by additional software.
The structure of this paper is as follows. In the next
section, we motivate the development of SDL and give
a ?avor of how base expressions can be compiled. We
then come up with an EBNF speci?cation of the concrete
syntax for SDL in section 3 and explain SDL with the
help of an example. Since modules can be seen as func-
tions in the mathematical sense, we argue in section 4
that a system speci?cation can be given a precise formal
semantics. We also clarify the formal status of the me-
diators and show how they are incorporated in the de?-
nition of the abstract semantics. Section 5 then de?nes
the programming interfaces and their default implemen-
tation, both for modules and for mediators. In the ?nal
section, we present some details of the compilation pro-
cess.
2 Motivation & Idea
The shallow text processing system SProUT (Becker
et al, 2002) developed at DFKI is a complex plat-
form for the development and processing of multilin-
gual resources. SProUT arranges processing components
(e.g., tokenizer, gazetteer, named entity recognition) in
a strictly sequential fashion, as is known from standard
cascaded ?nite-state devices (Abney, 1996).
In order to connect such (independently developed) NL
components, one must look at the application program-
mer interface (API) of each module, hoping that there are
API methods which allow, e.g., to call a module with a
speci?c input, to obtain the result value, etc. In the best
case, API methods from different modules can be used di-
rectly without much programming overhead. In the worst
case, however, there is no API available, meaning that we
have to inspect the programming code of a module and
have to write additional code to realize interfaces between
modules (e.g., data transformation). Even more demand-
ing, recent hybrid NLP systems such as WHITEBOARD
(Crysmann et al, 2002) implement more complex inter-
actions and loops, instead of using a simple pipeline of
modules.
We have overcome this in?exible behavior by imple-
menting the following idea. Since we use typed feature
structures (Carpenter, 1992) in SProUT as the sole data
interchange format between processing modules, the con-
struction of a new system can be reduced to the interpre-
tation of a regular expression of modules. Because the ?
sign for concatenation can not be found on a keyboard,
we have given the three characters +, |, and ? the follow-
ing meaning:
? sequence or concatenation
m1+m2 expresses the fact that (1) the input tom1+
m2 is the input given tom1, (2) the output of module
m1 serves as the input to m2, and (3) that the ?nal
output of m1 + m2 is equal to the output of m2.
This is the usual ?ow of information in a sequential
cascaded shallow NL architecture.
? concurrency or parallelism
| denotes a quasi-parallel computation of indepen-
dent modules, where the ?nal output of each mod-
ule serves as the input to a subsequent module (per-
haps grouped in a structured object, as we do by de-
fault). This operator has far reaching potential. We
envisage, e.g., the parallel computation of several
morphological analyzers with different coverage or
the parallel execution of a shallow topological parser
and a deep HPSG parser (as in WHITEBOARD). In
a programming language such as Java, the execution
of modules can even be realized by independently
running threads.
? unrestricted iteration or ?xpoint computation
m? has the following interpretation. Module m
feeds its output back into itself, until no more
changes occur, thus implementing a kind of a ?x-
point computation (Davey and Priestley, 1990). It is
clear that such a ?xpoint might not be reached in ?-
nite time, i.e., the computation must not stop. A pos-
sible application was envisaged in (Braun, 1999),
where an iterative application of a base clause mod-
ule was necessary to model recursive embedding
of subordinate clauses in a system for parsing Ger-
man clause sentential structures. Notice that unre-
stricted iteration would even allow us to simulate an
all-paths context-free parsing behavior, since such
a feedback loop can in principle simulate an un-
bounded number of cascade stages in a ?nite-state
device (each level of a CF parse tree has been con-
structed by a single cascade stage).
We have de?ned a Java interface of methods which
each module must ful?ll that will be incorporated in the
construction of a new system. Implementing such an in-
terface means that a module must provide an implementa-
tion for all methods speci?ed in the interface with exactly
the same method name and method signature, e.g., set-
Input(), clear(), or run(). To ease this implementa-
tion, we have also implemented an abstract Java class that
provides a default implementation for all these methods
with the exception of run(), the method which starts the
computation of the module and which delivers the ?nal
result.
The interesting point now is that a new system, declar-
atively speci?ed by means of the above apparatus, can be
automatically compiled into a single Java class. Even the
newly generated Java class implements the above inter-
face of methods. This Java code can then be compiled by
the Java compiler into a running program, realizing ex-
actly the intended behavior of the original system speci-
?cation. The execution semantics for an arbitrary mod-
ule m is de?ned to be always the execution of the run()
method of m, written in Java as m.run()
Due to space limitations, we can only outline the basic
idea and present a simpli?ed version of the compiled code
for a sequence of two module instances m1 +m2, for the
independent concurrent computation m1 | m2, and for
the unbounded iteration of a single module instance m?.
Note that we use the typewriter font when referring to
the concrete syntax or the implementation, but use italics
to denote the abstract syntax.
(m1 + m2)(input) ?
m1.clear();
m1.setInput(input);
m1.setOutput(m1.run(m1.getInput()));
m2.clear();
m2.setInput(seq(m1, m2));
m2.setOutput(m2.run(m2.getInput()));
return m2.getOutput();
(m1 | m2)(input) ?
m1.clear();
m1.setInput(input);
m1.setOutput(m1.run(m1.getInput()));
m2.clear();
m2.setInput(input);
m2.setOutput(m2.run(m2.getInput()));
return par(m1, m2);
(m?)(input) ?
m.clear();
m.setInput(input);
m.setOutput(fix(m));
return m.getOutput();
The pseudo code above contains three methods,
seq(), par(), and fix(), methods which mediate be-
tween the output of one module and the input of a suc-
ceeding module. Clearly, such functionality should not
be squeezed into independently developed modules, since
otherwise a module m must have a notion of a ?xpoint
during the execution of m? or must be sensitive to the
output of every other module, e.g., during the processing
of (m1 | m2) + m. Note that the mediators take mod-
ules as input, and so having access to their internal in-
formation via the public methods speci?ed in the module
interface (the API).
The default implementation for seq is of course the
identity function (speaking in terms of functional compo-
sition). par wraps the two results in a structured object
(default implementation: a Java array). fix() imple-
ments a ?xpoint computation (see section 5.3 for the Java
code). These mediators can be made speci?c to special
module-module combinations and are an implementation
of the mediator design pattern, which loosely couples in-
dependent modules by encapsulating their interaction in
a new object (Gamma et al, 1995, pp. 273). I.e., the
mediators do not modify the original modules and only
have read access to input and output via getInput()
and getOutput().
In the following, we present a graphical representation
for displaying module combination. Given such pictures,
it is easy to see where the mediators come into play. De-
picting a sequence of two modules is, at ?rst sight, not
hard.
m1 m2
Now, if the input format of m2 is not compatible with
the output of m1, must we change the programming code
for m2? Even more serious, if we would have another
expression m3 + m2, must m2 also be sensitive to the
output format of m3? In order to avoid these and other
cases, we decouple module interaction and introduce a
special mediator method for the sequence operator (seq
in the above code), depicted by ?.
m1 + m2
? connects two modules. This fact is re?ected by mak-
ing seq a binary method which takes m1 and m2 as input
parameters (see example code).
Let us now move to the parallel execution of several
modules (not necessarily two, as in the above example).
mk
m1
.
.
.
There is one problem here. What happens to the output
of each module when the lines come together, meeting in
the outgoing arrow? The next section has a few words
on this and presents a solution. We only note here that
there exists a mediator method par, which, by default,
groups the output in a structured object. Since par does
not know the number of modules in advance, it takes as
its parameter an array of modules. Note further that the
input arrows are ?ne?every module gets the same data.
Hence, we have the following modi?ed picture.
mk
m1
.
.
.
|
Now comes the ? operator. As we already said, the
module feeds itself with its own output, until a ?xpoint
has been reached, i.e., until input equals output. Instead
of writing
m
we make the mediator method for ? explicit, since it em-
bodies the knowledge about ?xpoints (and not the mod-
ule):
m ?
3 Syntax
A new system is built from an initial set of already ex-
isting modules M with the help of the three operators +,
|, and ?. The set of all syntactically well-formed module
descriptions D in SDL is inductively de?ned as follows:
? m ?M ? m ? D
? m1,m2 ? D ? m1 +m2 ? D
? m1, . . . ,mk ? D ? (| m1 . . .mk) ? D
? m ? D ? m? ? D
Examples in the concrete syntax are written using the
typewriter font, e.g., module. All operators have the
same priority. Succeeding modules are written from left
to right, using in?x notation, e.g., m1 + m2.
Parallel executed modules must be put in parentheses
with the | operator ?rst, for instance (| m1 m2). Note
that we use the pre?x notation for the concurrency op-
erator | to allow for an arbitrary number of arguments,
e.g., (| m1 m2 m3). This technique furthermore cir-
cumvents notorious grouping ambiguities which might
lead to different results when executing the modules. No-
tice that since | must neither be commutative nor must it
be associative, the result of (| m1 m2 m3) might be dif-
ferent to (| m1 (| m2 m3)), to (| (| m1 m2) m3),
or even to (| m2 (| m1 m3)), etc. Whether | is com-
mutative or associative is determined by the implemen-
tation of concurrency mediator par. Let us give an ex-
ample. Assume, for instance, that m1, m2, and m3 would
return typed feature structures and that par() would join
the results by using uni?cation. In this case, | is clearly
commutative and associative, since uni?cation is commu-
tative and associative (and idempotent).
Finally, the unrestricted self-application of a module
should be expressed in the concrete syntax by using the
module name, pre?xed by the asterisk sign, and grouped
using parentheses, e.g., (* module). module here
might represent a single module or a complex expression
(which itself must be put in parentheses).
Making | and ? pre?x operators (in contrast to +) ease
the work of the syntactical analysis of an SDL expression.
The EBNF for a complete system description system is
given by ?gure 1. A concrete running example is shown
in ?gure 2.
The example system from ?gure 2 should be read
as de?ne a new module de.dfki.lt.test.System
as (| rnd1 rnd2 rnd3) + inc1 + ..., varia-
bles rnd1, rnd2, and rnd3 refer to instances
of module de.dfki.lt.sdl.test.Randomize,
module Randomize belongs to package
de.dfki.lt.sdl.test, the value of rnd1 should
be initialized with ("foo", "bar", "baz"), etc.
Every single line must be separated by the newline
character.
The use of variables (instead of using directly module
names, i.e., Java classes) has one important advantage:
variables can be reused (viz., rnd2 and rnd3 in the ex-
ample), meaning that the same instances are used at sev-
eral places throughout the system description, instead of
using several instances of the same module (which, of
course, can also be achieved; cf. rnd1, rnd2, and rnd3
which are instances of module Randomize). Notice that
the value of a variable can not be rede?ned during the
course of a system description.
4 Modules as Functions
Before we start the description of the implementation in
the next section, we will argue that a system description
can be given a precise formal semantics, assuming that
the initial modules, which we call base modules are well
de?ned. First of all, we only need some basic mathemat-
ical knowledge from secondary school, viz., the concept
of a function.
A function f (sometimes called a mapping) from S to
T , written as f : S ?? T , can be seen as a special
kind of relation, where the domain of f is S (written as
DOM(f) = S), and for each element in the domain of f ,
there is at most one element in the range (or codomain)
RNG(f). If there always exists an element in the range,
we say that f is a total function (or well de?ned) and write
f ?. Otherwise, f is said to be a partial function, and for
an s ? S for which f is not de?ned, we then write f(s)?.
Since S itself might consist of ordered n-tuples and
thus is the Cartesian product of S1, . . . , Sn, depicted as
?ni=1Si, we use the vector notation and write f(~s) instead
of f(s). The n-fold functional composition of f : S ??
T (n ? 0) is written as fn and has the following induc-
tive de?nition: f 0(~s) := ~s and f i+1(~s) := f(f i(~s)).
s ? S is said to be a ?xpoint of f : S ?? S iff
f(f(s)) =S f(s) (we use =S to denote the equality rela-
tion in S).
Assuming that m is a module for which a proper run()
method has been de?ned, we will, from now on, refer to
the function m as abbreviating m.run(), the execution
of method run() from module m. Hence, we de?ne the
execution semantics of m to be equivalent to m.run().
4.1 Sequence
Let us start with the sequence m1 + m2 of two mod-
ules, regarded as two function m1 : S1 ?? T1 and
m2 : S2 ?? T2. + here is the analogue to functional
composition ?, and so we de?ne the meaning (or abstract
semantics) [[ ? ]] of m1 +m2 as
[[m1 +m2]](~s) := (m2 ?m1)(~s) = m2(m1(~s))
m1 +m2 then is well-de?ned if m1 ?, m2 ?, and T1 ?
S2 is the case, due to the following biconditional:
m1 ?,m2 ?, T1 ? S2 ?? (m1 ?m2 : S1 ?? T2)?
4.2 Parallelism
We now come to the parallel execution of k modules mi :
Si ?? Ti (1 ? i ? k), operating on the same input. As
already said, the default mediator for | returns an ordered
system ? de?nition {command}? variables
de?nition ?module "=" regexpr newline
module ? a fully quali?ed Java class name
regexpr ? var | "(" regexpr ")" | regexpr "+" regexpr | "(" "|" {regexpr}+ ")" | "(" "*" regexpr ")"
newline ? the newline character
command?mediator | threaded
mediator ? "Mediator =" med newline
med ? a fully quali?ed Java class name
threaded ? "Threaded =" {"yes" | "no"} newline
variables ?{vareq newline}+
vareq ? var "=" module [initexpr]
var ? a lowercase symbol
initexpr ? "(" string {"," string}? ")"
string ? a Java string
Figure 1: The EBNF for the syntax of SDL.
de.dfki.lt.test.System = (| rnd1 rnd2 rnd3) + inc1 + inc2 + (* i5ut42) + (* (rnd3 + rnd2))
Mediator = de.dfki.lt.sdl.test.MaxMediator
Threaded = Yes
rnd1 = de.dfki.lt.sdl.test.Randomize("foo", "bar", "baz")
rnd2 = Randomize("bar", "baz")
rnd3 = de.dfki.lt.sdl.test.Randomize("baz")
inc1 = de.dfki.lt.sdl.test.Increment
inc2 = de.dfki.lt.sdl.test.Increment
i5ut42 = de.dfki.lt.sdl.test.Incr5UpTo42
Figure 2: An example in the concrete syntax of SDL.
sequence of the results of m1, . . . ,mk, hence is similar
to the Cartesian product ?:
[[(| m1 . . . mk)]](~s) := ?m1(~s), . . . ,mk(~s) ?
(| m1 . . . mk) is well-de?ned if each module is well
de?ned and the domain of each module is a superset of
the domain of the new composite module:
m1 ?, . . . ,mk ? =?
(m1? . . .?mk : (S1 ? . . . ? Sk)k ?? T1? . . .?Tk)?
4.3 Iteration
A proper de?nition of unrestricted iteration, however, de-
serves more attention and a bit more work. Since a mod-
ule m feeds its output back into itself, it is clear that the
iteration (m?)(~s) must not terminate. I.e., the question
whetherm?? holds, is undecidable in general. Obviously,
a necessary condition for m?? is that S ? T , and so if
m : S ?? T and m? holds, we have m? : S ?? S.
Since m is usually not a monotonic function, it must not
be the case that m has a least and a greatest ?xpoint. Of
course, m might not possess any ?xpoint at all.
Within our very practical context, we are interested in
?nitely-reachable ?xpoints. From the above remarks, it is
clear that given ~s ? S, (m?)(~s) terminates in ?nite time
iff no more changes occur during the iteration process,
i.e.,
?n ? N . mn(~s) =S mn?1(~s)
We can formalize the meaning of ? with the help of
Kleene?s ? operator, known from recursive function the-
ory (Hermes, 1978). ? is a functional and so, given a
function f as its input, returns a new function ?(f), the
unbounded minimization of f . Originally employed to
precisely de?ne (partial) recursive functions of natural
numbers, we need a slight generalization, so that we can
apply ? to functions, not necessarily operating on natural
numbers.
Let f : Nk+1 ?? N (k ? N). ?(f) : Nk ?? N is
given by
?(f)(~x) :=
?
?
?
n if f(~x, n) = 0 and f(~x, i) > 0,
for all 0 ? i ? n? 1
? otherwise
I.e., ?(f)(~x) returns the least n for which f(~x, n) = 0.
Such an n, of course, must not exist.
We now move from the natural numbers N to an arbi-
trary (structured) set S with equality relation =S . The
task of ? here is to return the number of iteration steps
n for which a self-application of module m no longer
changes the output, when applied to the original input
~s ? S. And so, we have the following de?nitional equa-
tion for the meaning of m?:
[[m?]](~s) := m?(m)(~s)(~s)
Obviously, the number of iteration steps needed to ob-
tain a ?xpoint is given by ?(m)(~s), where ? : (S ??
S) ?? N. Given m, we de?ne ?(m) as
?(m)(~s) :=
?
?
?
?
?
?
?
n if mn(~s) =S mn?1(~s) and
mi(~s) 6=S mi?1(~s),
for all 0 ? i ? n? 1
? otherwise
Compare this de?nition with the original ?(f)(~x) on
natural numbers above. Testing for zero is replaced here
by testing for equality in S. This last de?nition completes
the semantics for m?.
4.4 Incorporating Mediators
The above formalization does not include the use of
mediators. The effects the mediators have on the in-
put/output of modules are an integral part of the de?nition
for the meaning of m1 +m2, (| m1 . . . mk), and m?. In
case we explicitly want to represent (the default imple-
mentation of) the mediators in the above de?nitions, we
must, ?rst of all, clarify their status.
Let us focus, for instance, on the mediator for the se-
quence operator +. We already said that the mediator for
+ uses the output of m1 to feed m2, thus can be seen as
the identity function id, speaking in terms of functional
composition. Hence, we might rede?ne [[(m1 +m2)]](~s)
as
[[(m1 +m2)]](~s) :=
(m2 ? id ?m1)(~s) = m2(id(m1(~s))) = m2(m1(~s))
If so, mediators were functions and would have the
same status as modules. Clearly, they pragmatically dif-
fer from modules in that they coordinate the interaction
between independent modules (remember the mediator
metaphor). However, we have also said that the media-
tor methods take modules as input. When adopting this
view, a mediator is different from a module: it is a func-
tional (as is ?), taking functions as arguments (the mod-
ules) and returning a function. Now, let S be the mediator
for the + operator. We then obtain a different semantics
for m1 +m2.
[[(m1 +m2)]](~s) := (m2 ? S(m1,m2) ?m1)(~s)
and
S(m1,m2) := id
is the case in the default implementation for +. This view,
in fact, precisely corresponds to the implementation.
Let us quickly make the two other de?nitions re?ect
this new view and let P and F be the functionals for |
and ?, resp. For |, we now have
[[(| m1 . . . mk)]](~s) := (P(m1, . . . ,mk)?(?ki=1mi))(~s k)
(?ki=1mi)(~s k) denotes the ordered sequence
?m1(~s), . . . ,mk(~s) ? to which function P(m1, . . . ,mk)
is applied. At the moment,
P(m1, . . . ,mk) := ?ki=1id
i.e., the identity function is applied to the result
of each mi(~s), and so in the end, we still obtain
?m1(~s), . . . ,mk(~s) ?.
The adaption of m? is also not hard: F is exactly the
?(m)(~x)-fold composition of m, given value ~x. Since ~x
are free variables, we use Church?s Lambda abstraction
(Barendregt, 1984), make them bound, and write
F(m) := ?~x .m?(m)(~x)(~x)
Thus
[[m?]](~s) := (F(m))(~s)
It is clear that the above set of de?nitions is still not
complete, since it does not cover the cases where a mod-
ule m consists of several submodules, as does the syntax
of SDL clearly admit. This leads us to the ?nal four in-
ductive de?nitions which conclude this section:
? [[m]](~s) := m(~s) iff m is a base module
? [[(m1 +m2)]](~s) :=
([[m2]] ? S([[m1]], [[m2]]) ? [[m1]])(~s)
? [[(| m1 . . . mk)]](~s) :=
(P([[m1]], . . . , [[mk]]) ? (?ki=1[[mi]]))(~s k)
? [[m?]](~s) := (F([[m]]))(~s),
whereas F([[m]]) := ?~x . [[m]]?([[m]])(~x)(~x)
Recall that the execution semantics of m(~s) has not
changed after all and is still m.run(s), whereas s abbre-
viates the Java notation for the k-tuple ~s.
5 Interfaces
This section gives a short scetch of the API methods
which every module must implement and presents the de-
fault implementation of the mediator methods.
5.1 Module Interface IModule
The following seven methods must be implemented by a
module which should contribute to a new system. The
next subsection provides a default implementation for
six of them. The exception is the one-argument method
run() which is assumed to execute a module.
? clear() clears the internal state of the module it
is applied to. clear() is useful when a module
instance is reused during the execution of a sys-
tem. clear() might throw a ModuleClearError
in case something goes wrong during the clearing
phase.
? init() initializes a given module by providing
an array of init strings. init() might throw a
ModuleInitError.
? run() starts the execution of the module to which
it belongs and returns the result of this computa-
tion. An implementation of run() might throw
a ModuleRunError. Note that run() should not
store the input nor the output of the computation.
This is supposed to be done independently by using
setInput() and setOutput() (see below).
? setInput() stores the value of parameter input
and returns this value.
? getInput() returns the input originally given to
setInput().
? setOutput() stores the value of parameter
output and returns this value.
? getOutput() returns the output originally given to
setOutput().
5.2 Module Methods
Six of the seven module methods are provided by a
default implementation in class Modules which imple-
ments interface IModule (see above). New modules are
advised to inherit from Modules, so that only run()
must actually be speci?ed. Input and output of a module
is memorized by introducing the two additional private
instance ?elds input and output.
public abstract class Modules implements IModule {
private Object input, output;
protected Modules() {
this.input = null;
this.output = null; }
public Object run(Object input) throws
UnsupportedOperationException {
throw new UnsupportedOperationException("..."); }
public void clear() {
this.input = null;
this.output = null; }
public void init(String[] initArgs) {
}
public Object setInput(Object input) {
return (this.input = input); }
public Object getInput() {
return this.input; }
public Object setOutput(Object output) {
return (this.output = output); }
public Object getOutput() {
return this.output; }
}
5.3 Mediator Methods
The public class Mediators provides a default imple-
mentation for the three mediator methods, speci?ed in
interface IMediator. It is worth noting that although
fix() returns the ?xpoint, it relocates its computation
into an auxiliary method fixpoint() (see below), due
to the fact that mediators are not allowed to change the
internal state of a module. And thus, the input ?eld still
contains the original input, whereas the output ?eld refers
to the ?xpoint, at last.
public class Mediators implements IMediator {
public Mediators() {
}
public Object seq(IModule module1, IModule module2) {
return module1.getOutput(); }
public Object par(IModule[] modules) {
Object[] result = new Object[modules.length];
for (int i = 0; i < modules.length; i++)
result[i] = modules[i].getOutput();
return result; }
public Object fix(IModule module) {
return fixpoint(module, module.getInput()); }
private Object fixpoint(IModule module, Object input) {
Object output = module.run(input);
if (output.equals(input))
return output;
else
return fixpoint(module, output); }
}
6 Compiler
In section 2, we have already seen how basic expressions
are compiled into a sequence of instructions, consisting
of API methods from the module and mediator interface.
Here, we like to glance at the compilation of more com-
plex SDL expressions.
First of all, we note that complex expressions are de-
composed into ?at basic expressions which are not fur-
ther structured. Each subexpression is associated with a
new module variable and these variables are inserted into
the original system description which will also then be-
come ?at. In case of the example from ?gure 2, we have
the following subexpressions together with their vari-
ables (we pre?x every variable by the dollar sign): $1
= (| $rnd1 $rnd2 $rnd3), $2 = (* $i5ut42), $3
= ($rnd3 + $rnd2), and $4 = (* $3). As a result,
the original system description reduces to $1 + $inc1
+ $inc2 + $2 + $4 and thus is normalized as $1, . . .,
$4 are. The SDL compiler then introduces so-called local
or inner Java classes for such subexpressions and locates
them in the same package to which the newly de?ned
system belongs. Clearly, each new inner class must also
ful?ll the module interface IModule (see section 5) and
the SDL compiler produces the corresponding Java code,
similar to the default implementation in class Modules
(section 5), together with the right constructors for the
inner classes.
For each base module and each newly introduced inner
class, the compiler generates a private instance ?eld (e.g.,
private Randomize $rnd1) and a new instance (e.g.,
this.$rnd1 = new Randomize()) to which the API
methods can be applied. Each occurence of the operators
+, |, and * corresponds to the execution of the mediator
methods seq, par, and fix (see below).
Local variables (pre?xed by the low line character) are
also introduced for the individual run() methods ( 15,
. . ., 23 below). These variables are introduced by the
SDL compiler to serve as handles (or anchors) to already
evaluated subexpression, helping to establish a proper
?ow of control during the recursive compilation process.
We ?nish this paper by presenting the generated code
for the run() method for system System from ?gure 2.
public Object run(Object input)
throws ModuleClearError, ModuleRunError {
this.clear();
this.setInput(input);
IMediator _med = new MaxMediator();
this.$1.clear();
this.$1.setInput(input);
Object _15 = this.$1.run(input);
this.$1.setOutput(_15);
Object _16 = _med.seq(this.$1, this.$inc1);
this.$inc1.clear();
this.$inc1.setInput(_16);
Object _17 = this.$inc1.run(_16);
this.$inc1.setOutput(_17);
Object _18 = _med.seq(this.$inc1, this.$inc2);
this.$inc2.clear();
this.$inc2.setInput(_18);
Object _19 = this.$inc2.run(_18);
this.$inc2.setOutput(_19);
Object _20 = _med.seq(this.$inc2, this.$2);
this.$2.clear();
this.$2.setInput(_20);
Object _21 = this.$2.run(_20);
this.$2.setOutput(_21);
Object _22 = _med.seq(this.$2, this.$4);
this.$4.clear();
this.$4.setInput(_22);
Object _23 = this.$4.run(_22);
this.$4.setOutput(_23);
return this.setOutput(_23);
}
We always generate a new mediator object ( med) for
each local class in order to make the parallel execution
of modules thread-safe. Note that in the above code, the
mediator method seq() is applied four times due to the
fact that + occurs four times in the original speci?cation.
The full code generated by the SDL compiler
for the example from ?gure 2 can be found under
http://www.dfki.de/?krieger/public/. The di-
rectory also contains the Java code of the involved mod-
ules, plus the default implementation of the mediator and
module methods. In the workshop, we hope to further
report on the combination of WHAT (Scha?fer, 2003), an
XSLT-based annotation transformer, with SDL.
Acknowledgement
I am grateful to my colleagues Bernd Kiefer, Markus
Pilzecker, and Ulrich Scha?fer, helping me to make things
clear. Thanks to the anonymous reviewers who have iden-
ti?ed weak points. This work was supported by the Ger-
man Federal Ministry for Education, Science, Research,
and Technology under grant no. 01 IW C02 (QUETAL)
and by an EU grant under no. IST 12179 (Airforce).
References
S. Abney. 1996. Partial parsing via ?nite-state cascades. Natu-
ral Language Engineering, 2(4):337?344.
H. Barendregt. 1984. The Lambda Calculus, its Syntax and
Semantics. North-Holland.
M. Becker, W. Dro?zd?zyn?ski, H.-U. Krieger, J. Piskorski, U.
Scha?fer, and F. Xu. 2002. SProUT?shallow processing
with uni?cation and typed feature structures. In Proceedings
of ICON.
C. Braun. 1999. Flaches und robustes Parsen Deutscher
Satzgefu?ge. Master?s thesis, Universita?t des Saarlandes. In
German.
B. Carpenter. 1992. The Logic of Typed Feature Structures.
Cambridge University Press.
B. Crysmann, A. Frank, B. Kiefer, S. Mu?ller, G. Neumann, J.
Piskorski, U. Scha?fer, M. Siegel, H. Uszkoreit, F. Xu, M.
Becker, and H.-U. Krieger. 2002. An integrated architecture
for shallow and deep processing. In Proceedings of ACL,
pages 441?448.
B.A. Davey and H.A. Priestley. 1990. Introduction to Lattices
and Order. Cambridge University Press.
E. Gamma, R. Helm, R. Johnson, and J. Vlissides. 1995. De-
sign Patterns. Elements of Reusable Object-Oriented Soft-
ware. Addison-Wesley.
H. Hermes. 1978. Aufza?hlbarkeit, Entscheidbarkeit, Berechen-
barkeit: Einfu?hrung in die Theorie der rekursiven Funktio-
nen. Springer, 3rd ed. In German. Also as Enumerability,
Decidability, Computability: An Introduction to the Theory
of Recursive Functions.
U. Scha?fer. 2003. WHAT: an XSLT-based infrastructure for the
integration of natural language processing components. In
Proceedings of SEALTS.
Ellipsis Resolution by Controlled Default Unification for Multi-modal and
Speech Dialog Systems
Michael Streit
German Research Center for
Artificial Intelligence - DFKI
Stuhlsatzenhausweg 3
Saarbru?cken
Germany
streit@dfki.de
HansUlrich Krieger
German Research Center for
Artificial Intelligence - DFKI
Stuhlsatzenhausweg 3
Saarbru?cken
Germany
krieger@dfki.de
Abstract
We present a default-unification-based approach to
ellipsis resolution that is based on experience in
long running multimodal dialog projects, where it
played an essential role in discourse processing. We
extend default unification to non-parallel structures,
which is important for speech and multimodal di-
alog systems. We introduce new control mecha-
nisms for ellipsis resolution by considering dialog
structure with respect to specification, variation and
results of tasks and combine this with the analysis
of relations between the information elements con-
tained in antecedent and elliptic structures.
1 Introduction
The application of default unification (Carpenter,
1993) or priority union (Kaplan, 1987; Calder,
1991) to discourse is attractive, because these re-
lated concepts meet the intuition that new informa-
tion extends, corrects, or modifies old information,
instead of deleting it, by keeping what is consis-
tent.1 The use of default unification as a means
for ellipsis resolution has been discussed in the first
half of the nineties (Pru?st et al, 1994; Grover et al,
1994). Later, the discussion silted up, perhaps be-
cause the conditions on parallelism that have been
imposed occured to be too strong (cf. (Hobbs and
Kehler, 1997)).
1.1 Applications in Dialog Systems
Since this discussion, default unification-based el-
lipsis resolution has been applied in working sys-
tems of at least two projects, where it played an es-
sential role in discourse processing.
The first implementations have been provided in
the second half of the nineties at Siemens, where the
DIAMOD project developed a serial of prototypes
for multi-modal human machine dialog (cf. (Streit,
2001)).
1Priority union as introduced by Kaplan bears essentially
the same idea as Carpenter?s default unification.
The DIAMOD project realized applications for
appointment management and driver assistance, but
also for controlling machines and other technical
devices (Streit, 1999). The DIAMOD systems pro-
vided the user with a quite natural dialog, includ-
ing clarification and repair dialogs and even multi-
tasking (e,g,. the user could deal with different ap-
pointments in parallel).
Applied under appropriate conditions, default
unification turned out to be a robust and efficient
method for VP ellipsis and even NP ellipsis. It was
also successfully used to inherit information in situ-
ations without any syntactically recognizable ellip-
sis.
Later in the SmartKom project (Wahlster et al,
2001), default unification and its application on
ellipsis resolution was reinvented under the label
overlay (Alexandersson and Becker, 2003). Over-
lay basically consists of priority unions applied to
frame-like semantic representations without consid-
ering reentrencies. This inhibits providing sloppy
readings of an ellipsis (without impairing the dialog
performed in the systems very much).
1.2 Problems with Default Unification
Default unification without additional control
shows an inherent tendency to over-accumulate in-
formation. Even worse, the method may accumulate
information that is semantically inconsistent (but
not recognized as such) or at least is practically ab-
surd. Such inconsistencies or absurdities typically
arise from dependencies between information ele-
ments that are not expressed (or not expressible) in
the type hierarchy, domain model, or ontology that
is underlying the default unification process. For
instance, it may occur that by introducing a name
of a new object, the address of an old object of the
same kind is inherited, which is pragmatically ab-
surd. Or a numeric date or time specification may
be wrongly combined with a deictic reference to an-
other date or time, which is semantically inconsis-
tent. On the other hand, the intrinsic parallelism
of default unification does inhibit the handling of
fragmentary and other non-parallel ellipsis, which
is very common in spoken dialog.
1.3 Problems with Ellipsis Resolution in
Information Seeking Dialog
In dialog systems that serve for browsing informa-
tion sources, it is significant that the user modifies
and varies her query, be it spontaneously or system-
atically. As is discussed mainly in section 5, re-
moving and inheriting old information are equally
important for ellipsis resolution in this type of in-
teraction. We want to notice, that the removal of
information is independent from the question if in-
formation has been grounded (Traum, 1994) or not.
Up to now, studies hardly consider these problems.
1.4 Overview
In this paper we will present a revised and fairly ex-
tended version of the methods developed in the DI-
AMOD project.2 We discuss two main problems.
On the one hand we show how default unification
can be applied to non-parallel or fragmentary struc-
tures, on the other hand we discuss dependencies of
ellipsis resolution from the structure of information
and tasks, that are rarely addressed in the literature.
Especially we discuss the following problems.
? The extension of default unification to frag-
ments.
? Control of ellipsis resolution by considering
dialog structure w.r.t. specification, variation
and results of tasks.
? Control of ellipsis resolution by considering
relations between old and new information.
? Handling of set-valued features in specifying
vs. varying dialog phases.
We will couch our approach in terms of typed
feature structures. Thereby features correspond to
slots, and types correspond to frames.
We assume feature structures being well-typed,
but not totally well-typed (Carpenter, 1992). This
means that for every type it is defined which fea-
tures it can have, and which types come into ques-
tion as values of these features. But it is not re-
quired that every possible feature is present or has
a value. In order to use feature structures for the
purpose of encoding semantic representations, we
need set-valued features (which are semantically in-
terpreted as conjunctions of its elements). For in-
stance, a movie may be described by a conjunction
2The quite simple but effective means for controlling de-
fault unification that have been introduced in the DIAMOD
systems have not been published yet.
of genres (e.g., crime and science fiction), and an
appointment usually has more than one participant.3
In this paper we will mainly refer to examples
taken from DIAMOD, but also make use of material
taken from SmartKom user input. We note here that
the methods described in this paper are not imple-
mented in SmartKom. We also consider examples
as they are discussed in the literature.
2 Default unification
Default unification is a method to inherit defeasible
(in our case old) information which does not contra-
dict strict (in our case new) information. As already
mentioned, the consistency criterion is to weak, but
the basic approach is useful. There are two forms of
default unification: credulous and skeptical default
unification. Credulous default unification tries to
maintain as much old information as possible. Due
to structure sharing, there are often different alter-
natives for achieving a maximal amount of old in-
formation. Skeptical default unification takes only
the information that is common to all credulous so-
lutions. We are interested in getting every maximal
solution, which correspond to the strict, sloppy or
mixed readings of ellipsis. By mixed readings we
mean readings that contain a strict reading in one
part, and a sloppy reading in another.
We follow the definition of credulous de-
fault unification provided by Carpenter (Carpenter,
1992). But we take the most general type as the top
element of the type lattice, while Carpenter takes it
as the bottom element.
If O is the old, defeasible information and N is
new, strict information, then the credulous default
unification of O and N is the unification of O? with
N, where O? is a minimal structure that subsumes O
such that O? and N unify:
O
>
uc N = {O?uN |O? w O minimal s.t. O?uN 6= ?}
The following example, shows how default
unification can be used in ellipsis resolution.
1. John revises his paper.
?
?
?
?
revise
AGENT 1 john
OBJECT
[
paper
AUTHOR 1
]
?
?
?
?
3The first case concerns a closed class of values. In prin-
ciple, this case could technically be solved without set-valued
features by introducing a highly differentiated (rather artificial)
type system with a type for every value combination. The sec-
ond case concerns an open class of entities that cannot be com-
piled in a type system.
2. And Bill does too.
[
event-agentive
AGENT bill
]
The analysis of these utterances is slightly
simplified. John would get a more complicated pre-
sentation with john being the value of the NAME
feature of the type person. The verb do is consid-
ered as being the most general verb with an agent.
We use here event-agentive as a supertype of activi-
ties.
In this example the types of the top nodes are on
a comparable ?level?. By being on a comparable
level we mean that the top node of the one item is
a supertype of the top node type of the other item.
Notice that due to the well-typing condition, types
and features may not be mixed arbitrarily . For in-
stance, the most general type of the type hierarchy
(and many others too), cannot be combined with the
feature agent. Otherwise our level condition would
be meaningless.
We find two minimal upper bounds of (1.) that
unify with (2.).
(1?)
?
?
?
revise
AGENT john
OBJECT
[
paper
AUTHOR john
]
?
?
?
(1?)
?
?
?
?
revise
AGENT 1
OBJECT
[
paper
AUTHOR 1
]
?
?
?
?
We get by unifying (1?) with (2) the strict reading
(2?), while we get the sloppy reading (2?) by using
(1?).
(2?)
?
?
?
revise
AGENT bill
OBJECT
[
paper
AUTHOR john
]
?
?
?
(2?)
?
?
?
?
revise
AGENT 1 bill
OBJECT
[
paper
AUTHOR 1 bill
]
?
?
?
?
3 Default Unification on Substructures
While classical studies focus on parallism, the im-
portance of non-parallel and fragmentary ellipsis is
shown by empirical analysis of spoken dialog (cf.
(Fernandz and Ginzburg, 2002)). The focus of an
elliptic utterance often has no direct counterpart in
the antecedent, which makes Rooth?s matching con-
dition not directly applicable (cf. (Rooth, 1992),
(Hardt and Romero, 2001)). Grammatically re-
quired verbs (e.g., the semantically weak verb do)
may be omitted in dialog ellipsis. In German spo-
ken language, this is also possible in single and se-
quential utterances of one speaker.
We take an example from TALKY, which is the
appointment management multimodal dialog sys-
tem that was developed in the framework of DI-
AMOD. The reaction of the system to the first ut-
terance of the user is not necessarily important, be-
cause users often proceed with (3) without waiting
for the system?s answer (i.e., by barge in) or with-
out paying much attention to the system?s reaction
(in case of an experienced user).
1. USER: Ich mo?chte einen Termin eintragen. (I
want to enter an appointment)
2. SYSTEM: presents a new appointment entry
3. USER: mit Schmid (with Schmid)
We achieve the following two representations of
the user?s utterances.
(1)
?
?
?
?
?
?
want
AGENT user
TOPIC
?
?
enter
AGENT system
OBJECT appointment
?
?
?
?
?
?
?
?
(3)
[
thing-with-participant
PARTICIPANT schmid
]
?Matching? cannot be achieved by assuming that
there is a hidden attitude connected to very utterance
which could be inserted.
Instead, we search for ?matching? nodes with
comparable types before normal default unification
is applied: thing-with-participant unifies with ap-
pointment, which leads to:
?
?
?
?
?
?
?
want
AGENT user
TOPIC
?
?
?
enter
AGENT system
OBJECT
[
appointment
PARTICIPANT schmid
]
?
?
?
?
?
?
?
?
?
?
In principle, it is quite possible that thing-with-
participant describes a certain (collective) type of
agents. In this case, the processing would produce
an ambiguity. In the DIAMOD system as in many
other dialog systems, the agent role is usually re-
stricted to the user and to incarnations of the system.
It is not alway posible to find a matching type.
In this case we try to find paths that connect termi-
nal nodes of the antecedent structure with the top
node of the elliptic structure. It is important, that
such connection paths do not introduce new struc-
tures corresponding to verbal complements or sub-
ordinated sentences.
If no match is achieved we get simply the new
structure back, which is the normal result of apply-
ing default unification to non-matching structures.
4 Task Completion as a Barrier for
Elliptic reference
In the following example (taken from Talky), the
user performs her specification in a stepwise man-
ner by extensively using ellipsis.
1. USER: Ich mo?chte am Montag ein Treffen ein-
tragen. (I want to enter a meeting at monday)
2. SYSTEM: Presents an empty appointment en-
try
3. USER: Im Bananensaal (In the ?banana
room?)
4. SYSTEM: Presents appointment entry with
banana room
5. USER: Ich meine im Raum Leibniz (I mean in
room Leibniz)
6. SYSTEM: Presents appointment entry with
room Leibniz
7. USER: um sechs Uhr (at six o?clock)
8. SYSTEM: Presents appointment with room
and begin time 6 a.m
9. USER: abends (at the evening)
10. SYSTEM: Presents appointment with room
and begin time 6 p.m
Some information has been corrected or clarified,
but there was no information removed implicitly.
Locally, most steps could be considerd as a case of
fragmentary elaboration of the preceding utterances
(cf. (Schlangen and Lascarides, 2003)). But this
classification depends on more general properties of
the dialog. When the task is finished, the availabil-
ity of old information has changed:
1 USER: Bitte das Treffen jetzt eintragen.
(Please enter now the meeting
2 SYSTEM: Indicates that the meeting is stored
3 USER
a Bitte jetzt ein Treffen am Freitag eintra-
gen please enter a meeting at Friday now
b Und am Freitag. And at Friday!
c Am Freitag. At Friday!
In case of 3a, the the old information is removed.
With 3b we recognize that the activity (entering a
meeting in a schedule) is still available for being in-
herited elliptically, while further information, accu-
mulated before, is no longer relevant. If the user
wants to keep the more elements of the old informa-
tion, she has to use anaphoric references, e.g.,
4 Und dasselbe am Freitag (And the same at Fri-
day).
The elliptic reading in (3b) is very clear, (3c)
is rather an incomplete utterance that has to be
clarified. This is also quite different from the
specification phase of the meeting.
Task completion is a barrier for fragmentary elab-
oration. 4 After task completion, an elliptic re-
lation has to be be marked (e.g., by clue words as
und (and). Even then, ellipsis does not refer to the
whole information accumulated before, but rather to
the utterance that introduced the specification phase
of the preceding task.
5 Information Browsing
Typically, information request are answered after
every user input without a lengthly specification
phase. As in the case of elliptic specifications,
clarification dialog does not affect the elliptic rela-
tions between subsequent user queries. If the system
actively proposes an action, this will be different.
Browsing means to vary requests either because it is
not clear in advance which information is relevant,
how exactly it can be obtained, or because the user
wants to gather broad information in some area.
In browsing dialog, ellipsis is controlled by re-
lations between the informational content of the an-
tecedent and the elliptic utterance. According to our
remarks at the beginn of the section, we omit the re-
actions of the system in the subsequent examples.
By a group, we understand a collection of infor-
mation that is orthogonal to other information. By
4The reader may recognize a certain similarity of the con-
siderations in this section with the approach of (Grosz and Sid-
ner, 1986). An example: We restrict ourself to some remarks:
Grosz & Sidner focus on the segmentation of discourse along
the hierarchical structure of a task, while we focus on problems
concerning repetition (this section) and variation of tasks (next
section). Grosz & Sidner are mainly concerned with anaphoric
reference while we are concerned with ellipsis and related im-
plicit inheritance of information. In our approach, structural
relations between information is as much important as aspects
concerning the processing of tasks. Furthermore, we discuss
problems in relation to a special resolution mechanism, i.e., de-
fault unification.
Figure 1: Ontology for Searching Information about
Performances (simplified)
orthogonal we mean independent and not ?compet-
ing?. For instance, we consider TIME, LOCATION
and CONTENT as basic groups of the information
that belongs to a performance. Independence is not
a sufficient criterion. Actor and genre are indepen-
dent, but as our examples may show there are con-
sidered as competing. We have no formal means to
recognize a group. The knowledge about groups has
to be provided.
We use the term information element (IE) of a
feature structure as follows: An IE consists of two
parts: a role path and a semantic content. Differing
from the usual definition of paths (Carpenter, 1992),
a role path is a sequence of alternating types and
features (T1F1...TnFn with Types Ti and Features
fi). The semantic content is expressed by the sub-
structure which is identified by applying the subse-
quence of the features of the role path (accordingly
to standard definition). Role paths can be translated
directly in an obvious way into feature structures.
We speak of an terminal information element
(TIE), if the substructure is a type without further
specification. A TIE is atomic, if its semantic con-
tent is atomic. We represent TIEs as extended role
paths by taking the type which expresses their se-
mantic content as last element of the path.
Two TIEs (or IEs) are of the same sort, if their
role path has a common prefix. Two TIEs are of the
same terminal sort, if their role paths are identical.
TIE1 is more general as TIE2 if TIE1 subsumes
TIE2. TIE1 subsumes TIE2 if the subsumption re-
lation holds between their translations to feature
structures. It will turn out that this definition is to
narrow and does not cover the intuitive meaning of
being more general.
The TIEs in elliptic expressions are usually less
specific or have a shorter role path than the TIEs
in the antecedent. Subsequently we assume that
the matching process (as described in former sec-
tions) has already been applied and that the TIEs of
the elliptic expression are extendend by appending
the role path from the root of the antecedent to the
matching node. Otherwise we could not correctly
determine if an IE is subsumed by another or if they
belong to the same group etc.
We only consider readings of elliptic expressions
that amount to a new request, ignoring other read-
ings of elliptic expressions, e.g., as positive or neg-
ative feedback.
1 USER: Welch filme laufen heute Abend in
Saarbru?cken? (Which movies are on today
evening at Saarbru?cken? )
2 USER: Welche Krimis kommen? / Krimis! /
(?)Und Krimis! (Which crime (movies) are
on? / Crime (movies) / And crime (movies))
3 USER:
a Welche Sience fiction filme laufen? / Und
science fiction? / Science fiction! (Which
science fiction movies are on? / And sci-
ence fiction? / science fiction)
b Sind Science fiction filme dabei? (Are
science fiction movies among them?)
In (2) the general information movies
(i.e., the TIE informationSearch:TOPIC:-
performance?in?cinema:CONTENT:movie) is
replaced by the coresponding concrete information
crime movies (i.e., the TIE informationSearch:-
TOPIC:performance?in?cinema:CONTENT:-
movie:GENRE:crime). All other information
belongs to different groups and is retained. In (3)
the information crime movies is replaced by infor-
mation of the same terminal sort. The specification
crime movies is deleted. GENRE is a set-valued
feature. Note that set-valued features act quite
differently depending on the context (information
browsing vs. task specification). If the information
crime should be retained, this has to be indicated,
e.g. by an anaphorical relation to the result of query
(2) as is done in (3b). The reading of (2) and (3)
is not affected by the form of the ellipsis, but the
strong indication of parallelism that is expressed
with ?Und Krimis? (?And crime (movies)?) seems
not acceptable due to the proper subsumption
relation between movies and movies with genre
crime.
1 USER: Welche Science fiction laufen heute
abend in Saarbru?cken?. (Which science fiction
(movies) are on today evening at Saarbru?cken
)
4 USER:
a Mit Bruce Willis? (With Bruce Willis?)
b Und mit Bruce Willis / Welche filme
mit Bruce Willis laufen (And with Bruce
Willis? / Which movies with Bruce Willis
are on)
In (4), the new information element Bruce Willis
does not belong to the same terminal sort as any el-
ement in the antecedent, but by contributing to the
specification of movies it belongs to the same group
as science fiction. It is a competing element of ?sci-
ence fiction?, and its effect on the information ele-
ment ?science fiction? is a mixture of the effect of
elements of the same sort and elements of a differ-
ent group (as may be expected). 4b is an an accept-
able utterance in this context and it has the effect of
deleting the genre information, while 4a without ex-
plicit ellipsis indication could also count as adding
a specification.
1 USER: Welche Krimis kommen heute abend
in Saarbru?cken (Which crime (movies) are on
today evening at saarbru?cken?)
5 USER:
a Und in Saarlouis (And at Saarlouis?)
b Welche filme laufen in Saarlouis? (Which
movies are on at Saarlouis?)
In (5) the information Saarbru?cken is replaced by
an information of the same terminal sort. 5a has
the reading crime movies in Saarlouis. In 5b crime
movies is replaced by a more general informa-
tion. This is an indication that the specification
crime should be removed. But Welche filme (which
movies) has two other (less preferred) readings: an
anaphoric reading which (of those) movies are (also)
running at Saarlouis, or even an elliptic (or E-type)
reading which crime movies are on at Saarlouis.
That movie is more general than crime movie can
directly inferred from examining the ontology, i.e.
by subsumption.
1 USER: Welche Krimis kommen heute abend in
Saarbru?cken (Which crime (movies) are on?)
6 USER:
a Und im Scala (And at the Scala (movie
theater))
b Welche filme laufen im Scala (Which
movies are on at the Scala movie theater)
In (6), Saarbru?cken is replaced by a more concrete
information of the sort location. The Scala movie
theater is expected to be in Saarbru?cken except for
Scala is a aforementioned cinema in another town.
The readings are quite similar to (5). But there is
one difference: assume (1) gets an empty result.
Than (5a) is still appropriate while (6a) is quite odd.
(5b remains (slightly) ambiguous, while (6b) has
only one reading. The problem with these findings
is, that we cannot recognize by subsumption that
Scala is more specific than Saarbru?cken.
In information browsing, the relations between
the information elements contained in the an-
tecedent and the information elements provided by
the ellipsis expression are relevant for resolution.
Concrete Information Rule If the elliptic expres-
sion contains a more concrete TIE than the an-
tecedent, old specifications that belong to an-
other group are retained.
General Information Rule If the elliptic expres-
sion contains more general information than
the antecedent, then the general information
tends to be understood as deleting the corre-
sponding concrete Information. The more gen-
eral TIE introduces a choice points for default
unification. Default unification has to produce
a reading (usually the more likely one) that ac-
cepts general information elements as potential
barriers for default unification and removes old
information which is beyond the barriers.
Same Sort Rule If the elliptic expression contains
information of the same terminal sort, the old
information is deleted, even if the information
elements belong to a set-valued feature, except
it is made explicit that the feature should be
added.
Competing Information Rule If the elliptic ex-
pression indicates parallelism and contains
?competing? information of the same group,
but not the same terminal sort, the old infor-
mation is deleted. Otherwise, competing infor-
mation can be understood as adding a further
specification.
Negative Result Condition Ambiguous readings
are sensible for the result of the antecedent
query. Negative (empty) results excludes
readings that make the specification more
concrete.
We only consider relations between an antecedent
query and a subsequent elliptic query. We do not
discuss here relations that come into play if a longer
history is considered. The examples show, that de-
fault unification has to be controlled by relations be-
tween information elements.
6 Conclusion and Problems
We presented an approach for the resolution of non-
parallel ellipsis by default uni?cation, which is in-
herently a parallel method. We discussed the de-
pendence of ellipsis interpretation on the state of
the dialog in respect to task processing, but also
on relation between the informational content of
antecedent structures and elliptic structures, which
leads to a removal of inforamation, which is up to
now not considered in studies on ellipsis. We also
addressed the interplay of these dependencies with
indications of parallism that are customarily viewed
as the main factors of ellipsis interpretation. and We
demonstrated how these insights are realized by us-
ing default unification as efficient base processing.
A topic of further research is the relation of gen-
eral and concrete information. For instance, the on-
tology shown in figure 1 resembles the ontology
used in SmartKom. The location of a cinema is
specified by using a common format for addresses,
in which country and town are on the same level
and the name of the object not directly related to the
address. Formally (if groups are already defined)
these informations would considered as competing.
This would prevent the Scala movie theater to be
transferred to Saarlouis (in most cases the compe-
tition criterion would exclude this possibility, for a
certain type of elliptic expression it would recog-
nize an ambiguity). But the criterion would also
delete the specification that the Scala cinema is
in Saarbru?cken if the information element Scala is
introduced elliptical after asking a question about
Saarbru?cken. This kind of problems is not exclu-
sively a problem of locations.
? Wo la?uft Matrix? (Where is Matrix on?)
? Western? / Wo laufen Western? (Where are
western (movies) on
The phrase ?Western? shows no indications for par-
allelism, hence the competition criterion would in
this case accept the reading that the user is looking
for a western named Matrix.
As a practical solution, we introduced a rule that
comprises that names and other ?identifiers? of indi-
viduals are considered as being more concrete than
any other information elements, but further explo-
ration of the problem is necessary.
Also, the study of larger pieces of dialog as con-
sidered here is an important topic of further re-
search.
At several occasions we noticed that anaphoric
relations interact with elliptic relation. The inter-
action of anaphors and ellipsis is another important
topic of research.
References
J. Alexandersson and T. Becker. 2003. The formal
foundations underlaying overlay. In Proceedings
of the Fifth International Workshop on Computa-
tional Semantics, Tilburg.
J. Calder. 1991. Some notes on priority unions. In
ACQILEX Workshop on Default Inheritance in
the Lexicon, Cambridge, England.
B. Carpenter, editor. 1992. The Logic of Typed Fea-
ture Structures. Cambridge University Press.
B. Carpenter. 1993. Skeptical and credulous de-
fault uni?cation with applicationns to templates
and inheritance. In T. Briscoe, A. Copestake, and
V. de Pavia, editors, Inheritance, Defaults and
the Lexicon, pages 13?37. Cambridge University
Press.
R. Fernandz and J. Ginzburg. 2002. A Corpus
Study of Non-sentential Utterances in Dialogue.
Tratement Automatique de Languages, 43(2).
J. Grosz and C.L. Sidner. 1986. Attention, inten-
tions, and the structure of discourse. Computa-
tional Linguistics, 12:175?204.
C. Grover, Ch. Brew, S. Manandhar, and M. Moens.
1994. Priority union and generalization in dis-
course grammars. In 32nd. Annual Meeting of the
Association for Computational Linguistics, pages
17 ? 24, Las Cruces, NM. Association for Com-
putational Linguistics.
D. Hardt and Maribel Romero. 2001. Ellipsis and
the structure of discourse. In Sinn und Bedeutung
VI, Osnabru?ck, Germany.
J.R. Hobbs and A. Kehler. 1997. A Theory of Par-
allelism and the Case of VP Ellipsis. In P. R. Co-
hen and W. Wahlster, editors, Proceedings of the
35th Annual Meeting of the ACL, pages 394?401.
Association for Computational Linguistics.
R. Kaplan. 1987. Three seductions of com-
putational psycholinguistics. In P. Whitelock,
H. Somers, P. Bennett, R. Johnson, and
M. McGee Wood, editors, Linguistic Theory and
Computer Applications, pages 149?188. Aca-
demic Press.
H. Pru?st, R. Scha, and M. van den Berg. 1994. Dis-
course grammar and verb phrase anaphora. Lin-
guistics and Philosophy, 17:261?327.
M. Rooth. 1992. A theory of focus interpreattion.
Natural Language Semantics, 1:75?116.
D. Schlangen and A. Lascarides. 2003. The
Interpretation of Non-Sentential Utterances in
Dialogue. In Proceedings of the 4th SIGdial
Workshop on Discourse and Dialogue, Sapporo,
Japan.
M. Streit. 1999. The interaction of speech, deixis,
and graphics in the multimodal of?ce agent talky.
In P. Dalsgaar, CH. Lee, P. Heisterkamp, and
R. Cole, editors, Proceedings of the ESCA Tuto-
rial and Research Workshop on Interactive Dia-
log in Multi-Modal Systems.
M. Streit. 2001. Why are multimodal systems so
dif?cult to build. In Harry Bunt and Robbert-Jan
Beun, editors, Cooperative Multimodal Commu-
nication, volume 2155 of Lecture Notes in Com-
puter Science. Springer.
D. Traum. 1994. A Computational Theory
of Grounding in Natural Language Conversa-
tion. Ph.D. thesis, Computer Science Dept., U.
Rochester.
W. Wahlster, N. Reithinger, and A. Blocher. 2001.
Multimodal Communication with a Life-Like
Character. In Proc. of the 7th Proc. European
Conf. on Speech Communication and Technol-
ogy.
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 1?8,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
Contextual phenomena and thematic relations in database QA dialogues:
results from a Wizard-of-Oz Experiment
Nu?ria Bertomeu, Hans Uszkoreit
Saarland University
Saarbru?cken, Germany
uszkoreit|bertomeu@coli.uni-sb.de
Anette Frank, Hans-Ulrich Krieger and Brigitte Jo?rg
German Research Center of Artificial Intelligence
Saarbru?cken, Germany
frank|krieger|joerg@dfki.de
Abstract
Considering data obtained from a corpus
of database QA dialogues, we address the
nature of the discourse structure needed
to resolve the several kinds of contextual
phenomena found in our corpus. We look
at the thematic relations holding between
questions and the preceding context and
discuss to which extent thematic related-
ness plays a role in discourse structure.
1 Introduction
As pointed out by several authors (Kato et al, 2004),
(Chai and Ron, 2004), the information needs of
users interacting with QA systems often go beyond
a single stand-alone question. Often users want to
research about a particular topic or event or solve
a specific task. In such interactions we can expect
that the individual user questions will be themati-
cally connected, giving the users the possibility of
reusing part of the context when formulating new
questions.
That users implicitly refer to and even omit ma-
terial which can be recovered from the context
has already been replicated in several Wizard-of-
Oz experiments simulating natural language inter-
faces to databases, (Carbonell, 1983), (Dahlba?ck
and Jo?nsson, 1989), the most frequent contextual
phenomena being ellipsis, anaphora and definite de-
scriptions.
A big challenge for interactive QA systems is,
thus, the resolution of contextual phenomena. In or-
der to be able to do so a system has to keep track of
the user?s focus of attention as the interaction pro-
ceeds. The attentional state at a given point in the
interaction is given by the discourse structure. An
open issue, however, is the nature of the discourse
structure model needed in a QA system. Ahrenberg
et al (1990) argue that the discourse structure in NL
interfaces is, given the limited set of actions to be
performed by the system and the user, simpler than
the one underlying human-human dialogue. Upon
Ahrenberg et al (1990) this is given by the discourse
goals, rather than the overall goals of the user, as is
the case in task-oriented dialogues, (Grosz and Sid-
ner, 1986). Following Ahrenberg et al (1990), the
QA discourse is structured in segments composed
by a pair of initiative-response units, like question-
answer, or question-assertion, in the absence of an
answer. A segment can be embedded in another seg-
ment if it is composed by a clarification request and
its corresponding answer. The local context of a
segment is given by the immediately preceding seg-
ment. Upon Ahrenberg et al (1990), the latter re-
liably limits up the search space for antecedents of
anaphoric devices and ellipsis. However, as we will
see, there are few cases where the antecedents of
contextual phenomena are to be found beyond the
immediately preceding segments. This suggests that
a more complex approach to discourse structure for
QA systems is needed.
In more recent studies of interactive QA special
attention has been paid to the thematic relatedness of
questions, (Chai and Ron, 2004), (Kato et al, 2004).
Chai and Ron (2004) propose a discourse model-
ing for QA interactions in which they keep track
of thematic transitions between questions. Although
1
the applications of tracking thematic transitions be-
tween questions have not been investigated in depth,
Sun and Chai (2006) report on an experiment which
shows that the use of a model of topic transitions
based on Centering Theory improves query expan-
sion for context questions. However, these previous
studies on the thematic relations between questions
are not based on collections of interactive data, but
on questions centered around a topic that were col-
lected in non-interactive environments. This means
that they do not consider the answers to the ques-
tions, to which following questions can be related.
This paper presents data on different kinds of con-
textual phenomena found in a corpus of written nat-
ural language QA exchanges between human users
and a human agent representing an interactive infor-
mation service. We address two issues: the kinds
and frequencies of thematic relations holding be-
tween the user questions and the preceding context,
on the one hand, and the location of antecedents for
the different contextual phenomena, on the other.
We also discuss the question whether thematic rela-
tions can contribute to determine discourse structure
and, thus, to the resolution of the contextual phe-
nomena.
In the next section we present our data collection
and the aspects of the annotation scheme which are
relevant to the current work. In section 3 we present
data regarding the overall thematic cohesion of the
QA sessions. In section 4 we report on data regard-
ing the co-occurrence of discourse phenomena and
thematic relations and the distance between the phe-
nomena and their antecedents. Finally, we discuss
our findings with regard to their relevance with re-
spect to the nature of discourse structure.
2 Corpus and methodology
2.1 Experimental set-up
In order to obtain a corpus of natural QA inter-
actions, we designed a Wizard-of-Oz experiment.
The experiment was set up in such a way that the
exchanges between users and information system
would be as representative as possible for the inter-
action between users and QA systems. We chose an
ontology database instead of a text based closed do-
main QA system, however, because in order to simu-
late a real system short time responses were needed.
30 subjects took part in the experiment, which
consisted in solving a task by querying LT-WORLD,
an ontology containing information about language
technology1, in English. The modality of interaction
was typing through a chat-like interface.
Three different tasks were designed: two of them
concentrated on information browsing, the other one
on information gathering. In the first task sub-
jects had to find three traineeships at three different
projects in three different institutions each on a dif-
ferent topic, and obtain some information about the
chosen projects, like a contact address, a descrip-
tion, etc. In the second task, subjects had to find
three conferences in the winter term and three con-
ferences in the summer term, each one on a differ-
ent topic and they had to obtain some information on
the chosen conferences such as deadline, place, date.
etc. Finally, the third task consisted of finding infor-
mation for writing a report on European language
technology in the last ten years. To this end, subjects
had to obtain quantitative information on patents, or-
ganizations, conferences, etc.
The Wizard was limited to very few types of re-
sponses. The main response was answering a ques-
tion. In addition, she would provide intermediate
information about the state of processing if the re-
trieval took too long. She could also make state-
ments about the contents of the database when it did
not contain the information asked for or when the
user appeared confused about the structure of the
domain. Finally, she could ask for clarification or
more specificity when the question could not be un-
derstood. Yet the Wizard was not allowed to take
the initiative by offering information that was not
explicitely asked for. Thus all actions of the Wiz-
ard were directly dependent on those of the user.
As a result we obtained a corpus of 33 logs (30
plus 3 pilot experiments) containing 125.534 words
in 2.534 turns, 1.174 of which are user turns.
2.2 Annotation scheme
The corpus received a multi-layer annotaton2 con-
sisting of five levels. The levels of turns and part-of-
speech were automatically annotated. The level of
turns records information about the speaker and time
1See http://www.lt-world.org.
2We employed the annotation tool MMAX2 developed at
EML Research, Heidelberg.
2
stamp. For the other levels - the questions level, the
utterances level, and the entities level - a specific an-
notation scheme was developed. For these, we only
explain the aspects relevant for the present study.
2.2.1 Questions
This level was conceived to keep track of the
questions asked by the user which correspond to
queries to the database. With the aim of annotating
thematic relatedness between questions we distin-
guished two main kinds of thematic relations: those
holding between a question and a previous ques-
tion, quest(ion)-to-quest(ion)-rel(ation), and those
holding between a question and a previous answer,
quest(ion)-to-answ(er)-rel(ation).
Quest-to-quest-rels can be of the following types:
? refinement if the current question asks for the
same type of entity as some previous question,
but the restricting conditions are different, ask-
ing, thus, for a subset, superset or disjoint set
of the same class.
(1) US: How many projects on language tech-
nologies are there right now?
US: How many have been done in the
past?
? theme-entity if the current question is about the
same entity as some previous question.
(2) US: Where will the conference take place?
US: What is the dead-line for applicants?
? theme-property if the current question asks for
the same property as the immediately preced-
ing question but for another entity.
(3) US: Dates of TALK project?
US: Dates of DEREKO?
? paraphrase if the question is the rephrasing of
some previous question.
? overlap if the content of a question is subsumed
by the content of some previous question.
We distinguish the following quest-to-answ-rels:
? refinement if the current question asks for a
subset of the entities given in the previous an-
swer.
(4) LT: 3810.
US: How many of them do research on
language technology?
? theme if the current question asks about an en-
tity first introduced in some previous answer.
(5) LT: Semaduct, ...
US: What language technology topics
does the Semaduct project investigate?
Although Chai and Jin (2004) only consider tran-
sitions among questions in dialogues about events,
most of our relations have a correspondence with
theirs. Refinement corresponds to their constraint
refinement, theme-property to their participant-shift,
and theme-entity to their topic exploration.
2.2.2 Utterances
Utterances are classified according to their
speech-act: question, answer, assertion, or request.
Our annotation of discourse structure is identical in
spirit to the one proposed by Ahrenberg et al (1990).
A segment is opened with a user question to the
database and is closed with its corresponding an-
swer or an assertion by the system. Clarification
requests and their corresponding answers form seg-
ments which are embedded in other segments. Re-
quests to wait and assertions about the processing of
a question are also embedded in the segment opened
by the question.
Fragmentary utterances are annotated at this level.
We distinguish between fragments with a full lin-
guistic source, fragments with a partial source,
and fragments showing a certain analogy with the
source. The first group corresponds to fragments
which are structurally identical to the source and
can, thus, be resolved by substitution or extension.
(6) US: Are there any projects on spell checking in
Europe in the year 2006?
US: And in the year 2005?
Fragments with a partial source implicitly refer to
some entity previously introduced, but some infer-
ence must be done in order to resolve them.
(7) US: How is the contact for that project?
US: Homepage?
3
The last group is formed by fragments which show
some kind of parallelism with the source but which
cannot be resolved by substitution.
(8) US: Which conferences are offered in this win-
ter term in the subject of English language?
US: Any conferences concerning linguistics in
general?
2.2.3 Reference
We distinguish the following types of reference
to entities: identity or co-reference, subset/superset
and bridging.
Co-reference occurs when two or more expres-
sions denote the same entity. Within this group we
found the following types of implicit co-referring
expressions which involve different degrees of ex-
plicitness: elided NPs, anaphoric and deictic pro-
nouns, deictic NPs, and co-referent definite NPs.
Elided NPs are optional arguments, that is, they
don?t need to be in the surface-form of the sentence,
but are present in the semantic interpretation. In (9)
there is an anaphoric pronoun and an elided NP both
referring to the conference Speech TEK West 2006.
(9) US: The Speech TEK West 2006, when does it
take place?
LT: 2006-03-30 - 2006-04-01.
US: Until when can I hand in a paper [ ]?
Bridging is a definite description which refers to
an entity related to some entity in the focus of at-
tention. The resolution of bridging requires some
inference to be done in order to establish the con-
nection between the two entities. In example (2) in
subsection 2.2.1 there is an occurrence of bridging,
where the dead-line is meant to be the dead-line of
the conference currently under discussion.
Finally, subset/superset reference takes place
when a linguistic expression denotes a subset or su-
perset of the set of entities denoted by some previ-
ous linguistic expression. Subset/superset reference
is sometimes expressed through two interesting con-
textual phenomena: nominal ellipsis3, also called se-
mantic ellipsis, and one-NPs4. Nominal ellipsis oc-
curs within an NP and it is namely the noun what
3Note, however, that nominal ellipsis does not necessarily
always denote a subset, but sometimes it can denote a disjoint
set, or just lexical material which is omitted.
4One-NPs are a very rare in our corpus, so we are not con-
sidering them in the present study.
is missing and must be recovered from the context.
Here follows an example:
(10) US: Show me the three most important.
3 Thematic follow-up
When looking at the thematic relatedness of the
questions it?s striking how well structured the in-
teractions are regarding thematic relatedness. From
1047 queries to the database, 948 (90.54%) follow-
up on some previous question or answer, or both.
Only 99 questions (9.46%) open a new topic. 725
questions (69.25% of the total, 76.48% of the con-
nected questions) are related to other questions, 332
(31.71% of the total, 35.02% of the connected ques-
tions) are related to answers, and 109 (10.41% of the
total, 11.49% of the connected questions) are con-
nected to both questions and answers. These num-
bers don?t say much about how well structured the
discourse is, since the questions could be far away
from the questions or answers they are related to.
However, this is very seldom the case. In 60% of
the cases where the questions are thematically con-
nected, they immediately follow the question they
are related to, that is, the two questions are consecu-
tive5. In 16.56% of the cases the questions immedi-
ately follow the answer they are related to. 74.58%
of the questions, thus, immediately follow up the
question or/and answer they are thematically related
to6.
Table 1 shows the distribution of occurrences
and distances in segments for each of the rela-
tions described in subsection 2.2.1. We found that
the most frequent question-to-question relation is
theme-entity, followed by the question-to-answer re-
lation theme. As you can see, for all the relations ex-
cept theme, most occurrences are between very close
standing questions or questions and answers, most
of them holding between consecutive questions or
questions and answers. The occurrences of the re-
lation theme, however, are distributed along a wide
range of distances, 29.70% holding between ques-
tions and answers that are 2 and 14 turns away from
5By consecutive we mean that there is no intervening query
to the database between the two questions. This doesn?t imply
that there aren?t several intervening utterances and turns.
69 questions are consecutive to the question and answer they
are related to, respectively, that?s why the total percentage of
related consecutive questions is not 76.56%.
4
REF. Q. THEME E. Q. THEME P. Q. PARA. Q. OVERL. Q. REF. A. THEME A.
TOTAL 74 338 107 174 29 29 303
(7.80%) (35.65%) (11.29%) (18.35%) (3.06%) (3.06%) (31.96%)
1 SEGM. 88.73% 81.65% 100% 60.92% 78.57% 83.34% 46.39%
2 SEGM. 5.63% 1.86% 0% 8.09% 21.43% 13.33% 10.20%
Table 1: Occurrences of the different thematic relations
REL. / PHEN. THEME E. Q. THEME P. Q. THEME A. REF. Q. REF. A. CONNECTED TOTAL
FRAGMENT 53 (54.08%) 17 (16.32%) 3 (3.06%) 21 (21.42%) 0 97 (85.08%) 114
BRIDGING 40 (74.07%) 0 3 (5.55%) 1 (1.85%) 0 54 (58.69%) 92
DEFINITE NP 26 (78.78%) 0 4 (12.21%) 2 (6.10%) 0 33 (66%) 50
DEICTIC NP 19 (51.35%) 0 13 (35.13%) 2 (5.40%) 1 (2.70%) 37 (78.72%) 47
ANAPHORIC PRON. 13 (39.39%) 2 (6.06%) 10 (30.30%) 0 5 (15.15%) 33 (39.75%) 83
DEICTIC PRON. 2 (75%) 0 1 (25%) 0 0 3 (25%) 12
ELIDED NP 9 (69.23%) 0 2 (15.38%) 0 0 13 (61.90%) 21
NOMINAL ELLIPSIS 0 1 (7.69%) 6 (46.15%) 1 (7.69%) 5 (38.46%) 13 (81.25%) 16
Table 2: Contextual phenomena and the thematic relations holding between the questions containing them
and the questions or answers containing the antecedents.
each other. This is because often several entities
are retrieved with a single query and addressed later
on separately, obtaining all the information needed
about each of them before turning to the next one.
We found also quite long distances for paraphrases,
which means that the user probably forgot that he
had asked that question, since he could have also
scrolled back.
These particular distributions of thematic rela-
tions seem to be dependent on the nature of the
tasks. We found some differences across tasks: the
information gathering task elicited more refinement,
while the information browsing tasks gave rise to
more theme relations. It is possible that in an in-
teraction around an event or topic we may find ad-
ditional kinds of thematic relations and different
distributions. We also observed different strategies
among the subjects. The most common was to ask
everything about an entity before turning to the next
one, but some subjects preferred to ask about the
value of a property for all the entities under discus-
sion before turning to the next property.
4 Contextual phenomena: distances and
thematic relatedness
There are 1113 user utterances in our corpus, 409 of
which exhibit some kind of discourse phenomenon,
i.e., they are context-dependent in some way. This
amounts to 36.16% of the user utterances, a pro-
portion which is in the middle of those found in the
several corpora analyzed by Dahlba?ck and Jo?nsson
(1989)7. The amount of context-dependent user ut-
terances, as Dahlba?ck and Jo?nsson (1989) already
pointed out, as well as the distribution of the dif-
ferent relations among questions and answers ex-
plained above, may be dependent on the nature of
the task attempted in the dialogue.
Table 2 shows the distribution of the most fre-
quent thematic relations holding between the ques-
tions containing the contextual phenomena consid-
ered in our study and the questions or answers con-
taining their antecedents. The rightmost column
shows the number of occurrences of each of the con-
textual phenomena described in subsection 2.2.3.
The second column on the right shows the number
of occurrences in which the antecedent is located
in some previous segment and the question contain-
ing the contextual phenomenon is related through a
thematic relation to the question or answer contain-
ing the antecedent. The percentages shown for each
phenomenon are out of the total number of its oc-
currences. The remaining columns show frequen-
7They found a high variance according to the kind of task
carried out in the different dialogues. Dialogues from tasks
where there was the possibility to order something contained
a higher number of context-dependent user initiatives, up to
54.62%, while information browsing dialogues contained a
smaller number of context-dependent user initiatives, 16.95%
being the lowest amount found.
5
cies of co-occurrence for each of the phenomena and
thematic relations. The percentages shown for each
phenomenon are out of the total number of its con-
nected occurrences.
For the majority of investigated phenomena we
observe that most questions exhibiting them stand
in a thematic relation to the question or answer con-
taining the antecedent. Although there may be sev-
eral intermediate turns, the related questions are al-
most always consecutive, that is, the segment con-
taining the contextual phenomenon immediately fol-
lows the segment containing the antecedent. In the
remainder of the cases, the contextual phenomenon
and its antecedent are usually in the same segment.
However, this is not the case for deictic and
anaphoric pronouns. In most cases their antecedents
are in the same segment and even in the same utter-
ance or just one utterance away. This suggests that
pronouns are produced in a more local context than
other phenomena and their antecedents are first to be
looked for in the current segment.
For almost all the phenomena the most frequent
relation holding between the question containing
them and the question or answer containing the an-
tecedent is the question-to-question relation theme-
entity, followed by the question-to-answer relation
theme. This is not surprising, since we refer back to
entities because we keep speaking about them.
However, fragments and nominal ellipsis show a
different distribution. Fragments are related to their
sources through the question-to-question relations
theme-property and refinement, as well. Regarding
the distribution of relations across the three differ-
ent types of fragments we distinguish in our study,
we find that the relations refinement and theme-
property only hold between fragments with a full
source and fragments of type analogy, and their re-
spective sources. On the other hand, practically all
fragments with a partial-source stand in a theme-
entity relation to their source. Questions containing
nominal ellipsis are mostly related to the preceding
answer both through the relations theme and refine-
ment.
4.1 Antecedents beyond the boundaries of the
immediately preceding segment
As we have seen, the antecedents of more implicit
co-referring expressions, like pronouns, are very of-
ten in the same segment as the expressions. The
antecedents of less explicit co-referring expressions,
like deictic and definite NPs, are mostly in the im-
mediately preceding segment, but also often in the
same segment. About 50% are 2 utterances away,
20% between 3 and 5, although we find distances up
to 41 utterances for definite NPs.
However, there is a small number (11) of cases in
which the antecedents are found across the bound-
aries of the immediately preceding segment. This
poses a challenge to systems since the context
needed for recovering these antecedent is not as lo-
cal. The following example is a case of split an-
tecedents. The antecedent of the elided NP is to be
found across the two immediately preceding ques-
tions. Moreover, as you can see, the Wizard is not
sure about how to interpret the missing argument,
which can be because of the split antecedents, but
also because of the amount of time passed, and/or
because one of the answers is still missing, that is,
more than one segment is open at the same time.
(11) US: Which are the webpages for European
Joint Conferences on Theory and Practice
of Software and International Conference on
Linguistic Evidence?
LT: Please wait... (waiting time)
US: Which are the webpages for International
Joint Conference on Neural Networks and
Translating and the Computer 27?
LT: http://www.complang.ac, ... (1st answer)
US: Up to which date is it possible to send a
paper, an abstract [ ]?
LT: http://uwb.edu/ijcnn05/, ... (2nd answer)
LT: For which conference?
US: For all of the conferences I got the web-
pages.
In the following example the antecedent of the
definite NP is also to be found beyond the bound-
aries of the immediately preceding segment.
(12) US: What is the homepage of the project?
LT: http://dip.semanticweb.org
USER: What is the email address of Christoph
Bussler?
LT: The database does not contain this informa-
tion.
US: Where does the project take place?
6
Here the user asks about the email address of a per-
son who was previously introduced in the discourse
as the coordinator of the project under discussion
and then keeps on referring to the project with a def-
inite NP. The intervening question is somehow re-
lated to the project, but not directly. There is a topic
shift, as defined by Chai and Jin (2004), where the
main topic becomes an entity related to the entity the
preceding question was about. However, this topic
shift is only at a very local level, since the dialogue
participants keep on speaking about the project, that
is, the topic at a more general level keeps on being
the same. We can speak here of thematic nesting,
since the second question is about an entity intro-
duced in relation to the entity in focus of attention
in the first question, and the third question is again
about the same entity as the first. The project has not
completely left the focus, but has remained in sec-
ondary focus during the second segment, to become
again the main focus in the third segment. It seems
that as long as the entity to which the focus of atten-
tion has shifted is related to the entity previously in
focus of attention, the latter still also remains within
the focus of attention.
5 Conclusions
The possibility of using contextual phenomena is
given by certain types of thematic relatedness - espe-
cially theme-entity and theme, for co-reference and
bridging, and refinement, theme-entity and theme-
property, for fragments -, and contiguity of ques-
tions. As we have seen, the immediately preced-
ing segment is in most cases the upper limit of the
search space for the last reference to the entity, or
the elided material in fragments. The directions of
the search for antecedents, however, can vary de-
pending on the phenomena, since for more implicit
referring expressions antecedents are usually to be
found in the same segment, while for less implicit
referring expressions they are to be found in the pre-
ceding one.
These data are in accordance with what Ahren-
berg et al (1990) predict in their model. Just to
consider the immediately preceding segment as the
upper limit of the search space for antecedents is
enough and, thus, no tracking of thematic relations
is needed to resolve discourse phenomena. How-
ever, there are occurrences of more explicit types
of co-reference expressions, where the antecedent
is beyond the immediately preceding segment. As
we have observed, in these cases the intervening
segment/s shift the focus of attention to an entity
(maybe provided in some previous answer) closely
related to the one in focus of attention in the pre-
ceding segment. It seems that as long as this rela-
tion exists, even if there are many segments in be-
tween8, the first entity remains in focus of attention
and can be referred to by an implicit deictic or defi-
nite NP without any additional retrieval cue. We can
speak of thematic nesting of segments, which seems
to be analogous to the intentional structure in task-
oriented dialogues as in (Grosz and Sidner, 1986),
also allowing for reference with implicit devices to
entities in the superordinate segments after the sub-
ordinated ones have been closed. It seems, thus, that
thematic structure, like the discourse goals, also im-
poses structure on the discourse.
These cases, although not numerous, suggest that
a more complex discourse structure is needed for
QA interactions than one simply based on the dis-
course goals. The local context is given by the dis-
course segments, which are determined by the dis-
course goals, but a less local context may encompass
several segments. As we have seen, reference with
implicit devices to entities in the less local context
is still possible. What seems to determine this less
local context is a unique theme, about which all the
segments encompassed by the context directly or in-
directly are. So, although it does not seem necessary
to track all the thematic transitions between the seg-
ments, it seems necessary to categorize the segments
as being about a particular more global theme.
In a system like the one we simulated, having spe-
cific tasks in mind and querying structured data, a
possible approach to model this extended context,
or focus of attention, would be in terms of frames.
Every time a new entity is addressed a new frame
is activated. The frame encompasses the entity it-
self and the properties holding of it and other enti-
ties, as well as those entities. This would already
allow us to successfully resolve bridging and frag-
ments with a partial source. If the focus of atten-
8We found up to five intervening segments, one of them be-
ing a subsegment.
7
tion then shifts to one of the related entities, the user
demanding particular information about it, then its
frame is activated, but the previous frame also re-
mains somehow active, although to a lesser degree.
As long as there is a connection between the enti-
ties being talked about and a frame is not explicitly
closed, by switching to speak about a different en-
tity of the same class, for example, frames remain
somehow active and implicit references will be ac-
commodated within the activation scope.
In principle, the closer the relation to the entity
currently in focus, the higher the degree of activation
of the related entities. Yet, there may be cases of
ambiguity, where only inferences about the goals of
the user may help to resolve the reference, as in (13):
(13) US: How is the contact for that project?
LT: daelem@uia.ua.ac.be
US: What is the institute?
LT: Centrum voor Nederlandse Taal en Spraak.
US: Homepage?
Here the property ?Homepage? could be asked about
the institution or the project, the institution being
more active. However, the Wizard interpreted it as
referring to the project without hesitation because
she knew that subjects were interested in projects,
not in organizations. In order to resolve the ambigu-
ity, we would need a system customized for tasks or
make inferences about the goals of the users based
on the kind of information they?ve been asking for.
Determining at which level of nesting some expres-
sion has to be interpreted may involve plan recogni-
tion.
However, for open domain systems not having a
knowledge-base with structured data it may be much
more difficult to keep track of the focus of attention
beyond the strictly local context. For other kinds
of interactions which don?t have such a structured
nature as our tasks, this may also be the case. For
example, in the information browsing tasks in (Kato
et al, 2004), there is not a global topic encompass-
ing the whole interaction, but the information needs
of the user are given by the information he is en-
countering as the interaction proceeds, that is, he is
browsing the information in a free way, without hav-
ing particular goals or particular pieces of informa-
tion he wants to obtain in mind. In such cases it
may be difficult to determine how long frames are
active if the nesting goes very far, as well as making
any inferences about the user?s plans. However, it
might also be the case, that in that kind of interac-
tions no implicit referring expressions are used be-
yond the segmental level, because there is no such
an extended context. In order to find out, a study
with interactive data should be carried out.
Acknowledgements
The research reported here has been conducted in
the projects QUETAL and COLLATE II funded by
the German Ministry for Education and Research,
grants no. 01IWC02 and 01INC02, respectively. We
are also grateful to Bonnie Webber for her helpful
comments on the contents of this paper.
References
Ahrenberg Lars, Dahlba?ck Nils and Arne Jo?nsson 1990.
Discourse representation and discourse management
for natural language interfaces. Proceeding of the
Second Nordic Conference on Text Comprehension in
Man and Machine, Ta?by, Sweeden, 1990.
Jaime G. Carbonell. 1983. Discourse pragmatics and
ellipsis resolution in task-oriented natural language
interfaces. Proceedings of the 21st annual meeting
on Association for Computational Linguistics, Cam-
bridge, Massachusetts, 1983
Chai Joyce Y. and Ron Jin. 2004. Discourse Status
for Context Questions. HLT-NAACL 2004 Workshop
on Pragmatics in Question Answering (HLT-NAACL
2004) Boston, MA, USA, May 3-7, 2004
Dahlba?ck Nils and Arne Jo?nsson. 1989. Empirical
Studies of Discourse Representations for Natural Lan-
guage Interfaces. Proceedings of the Fourth Confer-
ence of the European Chapter of the ACL (EACL?89),
Manchester.
Grosz Barbara and Candance Sidner. 1986. Attention,
Intention and the Structure of Discourse. Computa-
tional Linguistics 12(3): 175-204.
Kato Tsuneaki, Fukumoto Junichi, Masui Fumito and
Noriko Kando. 2004. Handling Information Access
Dialogue through QA Technologies - A novel chal-
lenge for open-domain question answering. HLT-
NAACL 2004 Workshop on Pragmatics in Question
Answering (HLT-NAACL 2004) Boston, MA, USA,
May 3-7, 2004
Sun Mingyu and Joycie J. Chai. 2006. Towards Intel-
ligent QA Interfaces: Discourse Processing for Con-
text Questions. International Conference on Intelligent
User Interfaces, Sydney, Australia, January 2006
8
Coling 2010: Poster Volume, pages 588?596,
Beijing, August 2010
DL Meet FL: A Bidirectional Mapping between Ontologies and
Linguistic Knowledge?
Hans-Ulrich Krieger and Ulrich Scha?fer
Language Technology Lab
German Research Center for Artificial Intelligence (DFKI)
{krieger|ulrich.schaefer}@dfki.de
Abstract
We present a transformation scheme that me-
diates between description logics (DL) or
RDF-encoded ontologies and type hierar-
chies in feature logics (FL). The DL-to-FL
direction is illustrated by an implemented
offline procedure that maps ontologies with
large, dynamically maintained instance data
to named entity (NE) and information ex-
traction (IE) resources encoded in typed fea-
ture structures. The FL-to-DL translation is
exemplified by a (currently manual) trans-
lation of so-called MRS (Minimal Recur-
sion Semantics) representations into OWL
instances that are based on OWL classes,
generated from the the type hierarchy of a
deep linguistic grammar. The paper will
identify parts of knowledge which can be
translated from one formalism into the other
without loosing information and parts which
can only be approximated. The work de-
scribed here is important for the Seman-
tic Web to become a reality, since semantic
annotations of natural language documents
(DL) can be automatically generated by shal-
low and deep natural language parsing sys-
tems (FL).
1 Introduction and motivation
Ontologies on the one hand and resources for natu-
ral language processing (lingware) on the other hand,
though closely related, are often maintained indepen-
dently, thus constituting a duplication of work.
In the first part of this paper, we describe an im-
plemented offline procedure that can be used to map
concepts and instance information from ontologies to
lingware resources for named entity recognition and
information extraction systems. The approach (i) im-
proves NE/IE precision and recall in closed domains,
?The work described in this paper has been carried out
in the TAKE project (Technologies for Advanced Knowl-
edge Extraction), funded by the German Federal Min-
istry of Education and Research under contract number
01IW08003.
(ii) exploits linguistic knowledge for identifying on-
tology instances in texts more robustly, (iii) gives full
access to ontology instances and concepts in natu-
ral language processing results, and (iv) avoids du-
plication of work in development and maintenance of
ontologies and lingware. The advantages of this ap-
proach for Semantic Web and natural language (NL)
processing-based applications come from a cross-
fertilization effect. While ontology instance data can
improve precision and recall of, e.g., named entity
recognition (NER) and information extraction (IE)
in closed domains, linguistic knowledge contained in
NER and IE components can help to recognize ontol-
ogy instances (or concepts) occurring in text, e.g., by
taking into account inflection, anaphora, and context.
Furthermore, (Haghighi and Klein, 2009) and others
have shown that incorporating finer-grained seman-
tic information on entities occurring in text (e.g., for
antecedent filtering) helps to improve performance of
coreference resolution systems.
If both resources would be managed jointly at a
single place (in the ontology), they could be eas-
ily kept up-to-date and in sync, and their mainte-
nance would be less time-consuming. When ontol-
ogy concepts and instances are recognized in text,
their name or ID can be used by applications to
support subsequent queries, navigation, or inference
in the ontology using an ontology query language
(e.g., SPARQL). The procedure we describe here,
preserves hierarchical concept information and links
to ontology concepts and instances. Applications are,
e.g., hybrid deep-shallow question answering (Frank
et al, 2007), automatic typed hyperlinking (Buse-
mann et al, 2003) of instances and concepts occur-
ring in documents, or other innovative applications
that combine Semantic Web and NL processing tech-
nologies, e.g., for semantic search (Scha?fer et al,
2008).
The second part of this paper outlines the inverse
transformation from feature logics (FL) into descrip-
tion logics (DL). Walking along this direction has
the big advantage of potentially applying subsequent
description logic reasoners to the lexical semantics
of natural language input text in order to infer new
knowledge, e.g., in interactive natural language ques-
588
tion answering. As an example, we will carefully de-
velop the (approximate) translation of so-called ro-
bust minimal recursion semantic (RMRS) structures
(Copestake, 2003) into OWL descriptions (McGuin-
ness and van Harmelen, 2004). RMRS structures
are the semantic output of various NL processing
engines, encoded in typed feature structures (TFS).
Since NL processors (e.g., taggers, chunkers, deep
parsers) only build up structure, subsequent process-
ing steps are either not realized or implemented in ad
hoc way,
? dealing with merging & normalization of
RMRS,
? infering new knowledge (e.g., w.r.t. the forego-
ing dialog),
? taking into account extralinguistic knowledge
for reasoning.
Now, by moving from a specialized ?designer lan-
guage? (RMRS) to OWL, we can take advantage of
years of solid theoretical and practical work in logic,
especially in description logics. Since OWL is an
instance of the description logics family and the de-
facto language for the Semantic Web, we can utilize
the built-in reasoning capabilities of OWL and (rule-
based) description logic reasoners.
The structure of this paper is as follows. In the
next section, we outline the relationship between de-
scription logics and feature logics, trying to make
clear what they have in common, but at the same
time explaining their differences. Section 3 describes
the syntactic mapping process from the ontology
to feature structure descriptions. In Section 4, we
present an example where recognized named entities
enriched with ontology information are used in hy-
brid NL processing and subsequent applications. Af-
ter that, Section 5 explains the mapping of RMRS
structures into OWL descriptions. Finally, Section 6
shows that a subsequent description logic reasoner
can utilize these descriptions to infer new knowledge.
2 The relationship between description
and feature logics
Description logics (DL) (Baader et al, 2003) and fea-
ture logics (FL) (Carpenter, 1992) have been pursued
independently for quite a while. Their close relation-
ship was recognized by (Nebel and Smolka, 1990).
Instances of both families of knowledge representa-
tion formalisms are usually decidable two-variable
fragments of first-order predicate logic. Even though
DL dialects usually have an intractable worst-case
complexity, average-case reasoning is usually fast,
due to the availability of highly-optimized tableaux
reasoners. When adding seemingly easy constructs
such as ?role-value maps? (the analog to reentran-
cies), the underlying logical calculus becomes unde-
cidable.
From an abstract viewpoint, both DL and FL em-
ploy unary and binary predicates for which the two
communities invented different names (we only list
some of them):
arity description logic feature logic
unary concept, class type, category
binary role, property feature, attribute
Though these names are different, both represen-
tation families (usually) vary in further, not so subtle
details:
description logic feature logic
open world assumption closed world assumption
full Boolean concept logic only conjunctions
relational properties functional properties
role-value maps forbidden reentrancies allowed
Let us be more verbose here to see the descrip-
tional consequences of both approaches in terms of
a mutual translation. We note here that we take
OWL (McGuinness and van Harmelen, 2004) as an
instance of DL and TDL (type description language)
(Krieger and Scha?fer, 1994) as an example of FL.
OWL, the outcome of the DAML+OIL standard-
ization, is regarded to be the de-facto language for
the Semantic Web. OWL still makes use of con-
structs from RDF and RDFS, but restricts the ex-
pressive power of RDFS, thereby ensuring decidabil-
ity of the standard inference problems. Compared to
RDF(S), OWL provides more fine-grained modelling
constructs, such as intersectionOf or unionOf.
Within the Head-Driven Phrase Structure Gram-
mar (HPSG) (Pollard and Sag, 1994) paradigm in
modern computational linguistics (CL), TDL is a
language that has been employed in various imple-
mented systems, such as PAGE, LKB, PET, or
SProUT .
Before going into the details of our approximate
transformation schema, let us quickly explain how
to atomize a typed feature structure (TFS) in terms
of description logic primitives, using OWL. Consider
the following TFS which is a gross simplification of
the Head-Feature Principle in HPSG. In terms of
the ?one-dimensional? line-based TDL notation, we
write
phrase1 := phrase &
[HEAD #h1, HEAD-DTR|HEAD #h1],
or as a two-dimensional AVM (attribute-value ma-
trix) notation, we have
589
phrase1 ?
[ phrase
HEAD h1
HEAD-DTR|HEAD h1
]
Assuming that this is an individual of class phrase,
we can obtain a meaning-preserving OWL represen-
tation (we assume that HEAD and HEAD-DTR are
functional OWL object properties):
<owl:Thing rdf:ID="h1"/>
<rdf:Description rdf:about="hdtr1">
<rdf:type rdf:resource="owl:Thing"/>
<HEAD rdf:resource="h1"/>
</rdf:Description>
<rdf:Description rdf:about="phr1">
<rdf:type rdf:resource="phrase"/>
<HEAD rdf:resource="h1"/>
<HEAD-DTR rdf:resource="hdtr1"/>
</rdf:Description>
Note that only the top-level structure is explic-
itly typed (phrase); every other substructure thus
is assigned the most general type, which trans-
lates into the OWL class owl:Thing. Note also
the sharing of information under paths HEAD and
HEAD-DTR|HEAD?this is realized by refering to the
the name h1 in the above RDF/OWL description for
phr1 and hdtr1.
Given a set of OWL descriptions, obtaining the in-
verse direction from DL to FL should now be clear. It
is important here to group statements that are related
to a specific class, viz., inheritance information (e.g.,
intersectionOf) together with property informa-
tion about roles that are ?introduced? on a given class
(as given by the value of rdfs:domain ). In Sec-
tion 3, we focus on this inverse direction (DL-to-FL),
whereas Section 5 exemplifies the FL-to-DL direc-
tion.
Let us finally elaborate fundamental differences
between the DL and FL families that can only be ap-
proximated in terms of ?less expressive? constructs.
Open vs. closed world assumption. Typed feature
logics usually ?live? in a closed world, meaning that
if two types t1 and t2 do not share a common sub-
type (having a greatest lower bound), the unifica-
tion (conjuntion) is assumed to be the bottom type
(OWL: owl:Nothing), meaning that no individual
exists which is of both t1 and t2 at the same time.
This is totally different to the DL point of view: what
can not proven to be true (whether the conjunction
of t1 and t2 denotes the empty set) is not believed
to be false. Thus we either have to introduce a new
type t on the FL side, abbreviating the conjunction of
t1 and t2 (TDL: t := t1 & t2.), or to close the sub-
class hierarchy on the DL side: ? ? t1 u t2 (OWL:
disjointWith). This decision clearly depends on
the direction of the transformation.
Boolean vs. conjunctive description logic. Typed
feature logics rarely provide more than conjunc-
tions of feature-value constraints. This is due to the
fact that disjunctive descriptions render almost linear
(conjunctive) unification exponential. A full Boolean
calculus, such as OWL DL, even has an NEXPTIME
complexity. Thus it is clear that the direction from
DL to FL can only be approximated. The inverse di-
rection is clearly trivial with the notable exception of
reentrancies (see below).
To flesh out our point, consider the DL axiom
human ? man unionsq woman that fully determines (?)
human in terms of the union of the concepts man
and woman. Given the syntax of TDL, we can ap-
proximate parts of the intended meaning of the de-
scription by man :< human and woman :< human,
since the above DL axiom entails that man v human
and woman v human is the case. This is ex-
actly specified by the above two TDL type defini-
tions. Further, not so trivial approximations can be
found in (Flickinger, 2002). The idea here is that
foreseeable disjunctions of DL concepts can be emu-
lated by introducing additional FL types (in the worst
case, exponentially-many new types, however). Even
negated concepts can be simulated this way, since FL
lives in a closed world (see above).
Relational vs. functional properties. By default,
roles in DL are relational properties, meaning that
for a fixed individual in the domain of a given role,
the number of individuals in the range needs not to
be 0 or 1. DL further allows to impose cardinality
(or number) restrictions on roles, so that we might
write ? 0 livingParents u ? 2 livingParents which
says that one can have at least 0 and at most 2 liv-
ing parents. This is in sharp contrast to FL which
usually assume functional roles (so-called features),
making such roles essentially partial functions. A
partial workaround has been proposed in CL systems
by using (ordered) difference lists to collect informa-
tion. Other systems, such as SProUT (Krieger et al,
2004), come up with bags (or multisets) that even vi-
olate the foundational axiom (a set must not contain
itself) in order to achieve runtime efficiency.
Summarizing, the FL-to-DL direction of translat-
ing features into roles is easy, since features in FL
can be easily defined as functional roles in DL (OWL
even provides the owl:FunctionalProperty char-
acteristics). The inverse direction is only a gross ap-
proximation in that cardinality constraints can not be
590
stated on the FL side.
Role-value maps & reentrancies. The above Head-
Feature Principle example seems to indicate that role-
value maps can be easily represented in DL, simply
by using the name of an individual to specify iden-
tity. In fact, this is true, but only for the ABox of
a knowledge base, i.e., only for the set of individu-
als (or instances). However, the notion of role-value
maps in DL or reentrencies in FL refers to the TBox
and the set of concept definitions, resp. Thus, one
can not intensionally specify identity of information
for a potentially infinite number of individuals via a
class axiom in DL, but needs to extensionally spec-
ify identity of information for each individual in the
ABox.
3 OntoNERdIE: from OWL to TDL
In this section, we describe an instantiation of the DL-
to-FL mapping. OntoNERdIE is an offline procedure
that maps ontology concept and instance information
to lingware resources (Scha?fer, 2006). The approach
has been implemented for the language technology
ontology that backs up the LT World web portal
(http://www.lt-world.org), but can be easily adapted
to other domains and ontologies, since it is fully au-
tomated, except for the choice of relevant main con-
cepts and properties that are going to be mapped
which is a matter of configuration.
The target named entity recognition and infor-
mation extraction tool we employ here is SProUT
(Droz?dz?yn?ski et al, 2004), a shallow multilingual,
multi-purpose NL processor. The advantage of
SProUT in the described approach for named en-
tity recognition and information extraction is that
it comes with (1) a type system and typed feature
structures as the basic data type, (2) a powerful,
declarative rule mechanism with regular expressions
over typed feature structures, and (3) a highly effi-
cient gazetteer module with fine-grained, customiz-
able classification of recognized entities.
SProUT provides additional modules such as mor-
phology or a reference resolver that can be exploited
in the rule system, e.g., to use context or morpholog-
ical variation for improved NER. Through automat-
ically generated mappings, SProUT output enriched
with ontology information can be used for robust, hy-
brid deep-shallow parsing, and semantic analysis.
In this section, we describe the offline process-
ing steps of the OntoNERdIE approach. The on-
line part in applications is described in Section 4.
The approach heavily relies on XSLT transformations
(Clark, 1999) of the XML representation formats,
both in the offline mapping and in the online appli-
cation.
3.1 RDF preprocessing
Input to the mapping procedure is an OWL on-
tology file, containing both concept and instance
descriptions. The RDF file is pre-processed with
a generic XSLT stylesheet sorting and merging
rdf:Descriptions that are distributed over the file
but which belong together. We use XSLT?s key and
generate-id functions. Depending on the appli-
cation, the next two processing stages take a list of
concepts as filter because it will typically not be de-
sirable to extract all concepts or instances available
in the ontology. In both cases, resource files are gen-
erated as output that can be used to extend existing
named entity recognition resources. E.g., while gen-
eral rules can recognize domain-independent named
entities (e.g., any person name), the extended re-
source contains specific, and potentially more de-
tailed information for domain-specific entities.
3.2 Extracting inheritance
The second stylesheet converts RDFS subClassOf
statements from output step 1 (Section 3.1) into a
set of TDL type definitions that can be immediately
imported by the SProUT named entity recognition
grammar. Currently 1,260 type definitions for the
same number of subClassOf statements in the LT
World ontology are generated, e.g.,
NL_Parsing := Written_Language &
Language_Analysis.
This is of course a lossy conversion because not
all relations supported in an OWL ontology (such as
unionOf, disjointWith, intersectionOf) are
mapped. However, we think that for NE classifi-
cations, the subClassOf taxonomy mappings will
be sufficient. Other relations could be formulated
as direct (though slower) ontology queries using the
OBJID mechanism described in the next step. If
the target of OntoNERdIE is a NER system differ-
ent from SProUT and without a type hierarchy, this
step can be omitted. The subClassOf information
can always be gained by querying the ontology ap-
propriately on the basis of the concept name.
3.3 Generating gazetteer entries
The next stylesheet selects statements about instances
of relevant concepts via the rdf:type information
and converts them to structured gazetteer source files
for the SProUT gazetteer compiler (or into a differ-
ent format in case of another NER system). In the
following example, one of the approximately 20,000
converted entries for LT World is shown.
591
Bernd Kiefer | GTYPE: lt_person |
SNAME: "Kiefer" | GNAME: "Bernd" |
CONCEPT: Active_Person |
OBJID: "obj_62893"
The attribute CONCEPT contains a TDL type gener-
ated in step 2 (described in Section 3.2). For con-
venience, several ontology concepts are mapped (de-
fined manually as part of the configuration of the
stylesheet) to only a few named entity classes (under
attribute GTYPE). For the LT World ontology, these
classes are person, organization, event, project, prod-
uct, and technology. The advantage of this simplifica-
tion is that NER context rules from existing SProUT
named entity grammars can be re-used for improved
robustness and disambiguation.
The rules, e.g., recognize name variants with title
like Prof. Kiefer, Dr. Kiefer, or Mr. Kiefer with or
without a first name. Moreover, context (e.g., prepo-
sitions with location names, verbs), morphology and
reference resolution information can be exploited in
these rules.
The following SProUT rule lt-event (extended
TDL syntax) simply copies the slots of a matched
gazetteer entry for events (e.g., a conference) to the
output as a recognized named entity.
lt-event :> gazetteer &
[GTYPE lt_event, SURFACE #name,
CONCEPT #concept, OBJID #objid,
GABBID #abbrev]
->
ne-event & [EVENTNAME #name,
CONCEPT #concept, OBJID #objid,
GABBID #abbrev].
OBJID contains the object identifier of the instance
in the ontology. It can be used as a link back to the
full knowledge stored in the ontology, e.g., for sub-
sequent queries, like Who else participated in project
[with OBJID obj 4789]?.
In case multiple instances with same names but dif-
ferent object IDs occur in the ontology (which actu-
ally happens to be the case in LT World), multiple al-
ternatives are generated as output which is probably
the expected and desired behavior (e.g., for frequent
names such as John Smith). On the other hand, if
product or event names with an abbreviated variant
exist in the ontology, they both point to the same ob-
ject ID (provided they are stored appropriately in the
ontology).
4 Application to hybrid deep-shallow
parsing
We now describe and exemplify how the named en-
tities enriched with ontology information are em-
ployed in a robust, hybrid deep-shallow architec-
ture, combining domain-specific shallow named en-
tity recognition with deep, broad-coverage, domain-
independent, unification-based parsing for generating
a semantic representation of the meaning of parsed
sentences. An application of this scenario is deep
question analysis for question answering of struc-
tured knowledge sources, encoded as an OWL ontol-
ogy (Frank et al, 2007).
The output of SProUT for a recognized named en-
tity is a typed feature structure in XML containing
the instantiated RHS of the recognition rule as shown
in step 3 (Section 3.3) with the copied structured
gazetteer data, plus some additional information like
character span, named entity type, etc. The mapping
of recognized named entities to generic lexicon en-
tries of the deep grammar, in this case the English Re-
source Grammar (Flickinger, 2002), for hybrid pro-
cessing are performed through an XSLT stylesheet,
automatically generated from the SProUT type hi-
erarchy. Analogous mappings are supported for
other grammars available in the DELPH-IN reposi-
tory (see http://www.delph-in.net). The mapping ba-
sically transports the surface string, a character span,
and a generic lexicon type of the deep grammar for a
chart item to be generated in an XML format, read-
able by the deep parser. A sample output of the se-
mantic representation generated by the deep parser is
shown in Figure 1. The semantic representation for-
mat, called RMRS, is described in (Copestake, 2003)
and in Section 5.3 below.
In addition to the basic named entity type mapping
for default lexicon entries, the recognized concepts
are also useful for constraining the semantic sort in
the deep grammar in a more fine-grained way (e.g.,
for disambiguation). The deep parser?s XML input
chart format foresees ?injection? of such types into
deep structures. Here, OBJID and other structured in-
formation, like given name and surname, can be pre-
served in the representation. The advantage of the
RMRS format is that it can also be combined ex post
with analyses from other deep or shallow NLP com-
ponents, e.g., with partial analyses when a full parse
fails.
5 (R)MRS2OWL: from TDL to OWL
This section is devoted to the translation of MRSs
which are encoded as TFSs into a set of OWL expres-
sions. An example of a variant of MRS, a so-called
robust MRS (RMRS) has already been depicted in
Figure 1. RMRS will be explained in more detail in
Section 5.3.
592
?
??????????????????????????
TEXT ?Did Bernd Kiefer present a paper at IJCAI 2005??
TOP h1
RELS
?
??????????????????
??????????????????
?
??
int m rel
LBL h1
ARG0 e2
MARG h1
?
??
?
??
prpstn m rel
LBL h1001
ARG0 e2
MARG h5
?
??
?
????
proper q rel
LBL h6
ARG0 x8
RSTR h7
BODY h9
?
????
?
????
named rel
LBL h10
ARG0 x8
CARG BerndKiefer
?
????
?
????????
present v
LBL h11
ARG0 e2 tense=past
ARG1 x8 num=sgpers=3
ARG2 x12 num=sgpers=3
ARG3 u13
?
????????
?
????
a q
LBL h14
ARG0 x12
RSTR h15
BODY h16
?
????
?
?
paper n
LBL h17
ARG0 x12
?
?
?
?????
at p
LBL h1002
ARG0 e19 tense=u
ARG1 e2
ARG2 x18 num=sgpers=3
?
?????
?
????
proper q rel
LBL h20
ARG0 x18
RSTR h21
BODY h22
?
????
?
??
named rel
LBL h23
ARG0 x18
CARG IJCAI 2005
?
??
?
??????????????????
??????????????????
HCONS {h5 qeq h11, h7 qeq h10, h15 qeq h17, h21 qeq h23}
ING {h1 ing h1001, h11 ing h1002}
?
??????????????????????????
Figure 1: RMRS generated through hybrid parsing.
5.1 Some words on MRSs
There exist good linguistic reasons for assuming
that the semantics of a sentence like Kim ate a
cookie is not past(eat(kim?, cookie?), but instead
something like ?e . eating(e) ? subject(e, kim?) ?
object(e, cookie?)?before(e,now). This approach to
NL semantics is often called Event or Davidsonian
semantics (named after the American philosopher
Donald Davidson). HPSG has incorporated ideas
from event semantics by defining so-called Minimal
Recursion Semantics (MRS) structures (Copestake et
al., 2005) that are constructed in parallel with the syn-
tactic structure. MRS as such provides a flat com-
positional semantics and maximizes splitting using
equality constraints. Structural ambiguities, as can
be found in the famous sentence Every farmer who
owns a donkey beats it, are not spelled out, but in-
stead quantifier scope is underspecified. By impos-
ing constraints on the scope, specific analysis trees
can be reconstructed. Robust MRS (RMRS) (Copes-
take, 2003), derived from MRS, was designed as an
abstract language that supports the integration of par-
tial and total analysis results from deep and shallow
processors and provides a good tradeoff between ro-
bustness and accuracy (see (Frank et al, 2004) for an
example).
5.2 Why the translation is useful
NL processors (e.g., tokenizer, POS tagger, shallow
chunk parser, deep parser, etc.) that are geared to-
wards (R)MRSs (or another common language) have
the potential of combining their output on the level
of semantics. However, these engines do not provide
any form of reasoning, i.e., they only build up struc-
ture.
Consider, for instance, a deep unification-based
parser that might return analyses represented as typed
feature structures, where both syntax and semantics
(the MRS) has been constructed with the help of uni-
fication. Now, to bring structures together and to
perform deductive and abductive forms of reason-
ing, subsequent computational steps are necessary,
but these steps strictly go beyond the power of or-
dinary parsing.
In order to perform these subsequent steps, we
need a concrete implemented (and hopefully stan-
dardized) representation language for which editing,
displaying, and reasoning tools are available. Ex-
actly OWL accomplishes these requirements. Hence
we think that the described below translation process
from (R)MRSs into OWL is worthwhile, especially
when one is interested in interfacing linguistic knowl-
edge (the (R)MRSs) with extralinguistic ontologies
for specific domains.
5.3 The translation process
In order to explain the translation process, we will
analyze the RMRS depicted in Figure 1. The RMRS
was derived from the MRS of the deep unification-
based parser. We see that an RMRS contains four dis-
tinguished attributes (the TEXT attribute is only added
for illustration):
1. TOP: a handle (pointer) to the top-level structure.
2. RELS (relations): a set of so-called elementary
predications (EP), encoded as TFSs, each ex-
pressing an atomic semantic unit that can not be
593
further decomposed; due to the lack of sets, TFS
grammars use a list here.
3. HCONS (handle constraints): a set of so-called
qeq constraints (equality modulo quantifiers);
the left side of a qeq constraints (a handle h in
an argument position) is always related to a label
l of an EP, (i) either directly (h = l) or (ii) indi-
rectly, in case h dominates a quantifier q, such
that BODY(q) = l or again another quantifier,
where condition (ii) is recursively applied again.
4. ING (in group): a set of relations used to express
a conjunction of EPs from the set RELS.
Giving this information, it should now be clear that
the TFS from Figure 1 must be realized as an instance
of the OWL class RMRS and that the features TOP and
RELS must be implemented as roles in OWL, all de-
fined on RMRS through the use of rdfs:domain:
<owl:Class rdf:ID="RMRS"/>
<owl:ObjectProperty rdf:ID="TOP">
<rdf:type rdf:resource=
"&owl;FunctionalProperty"/>
<rdfs:domain rdf:resource="#RMRS"/>
<rdfs:range rdf:resource=
"#HandleVar"/>
</owl:ObjectProperty>
<owl:ObjectProperty rdf:ID="RELS">
<rdfs:domain rdf:resource="#RMRS"/>
<rdfs:range rdf:resource="#EP"/>
</owl:ObjectProperty>
TOP takes exactly one argument, hence we use OWL?s
FunctionalProperty characteristics mechanism
here. Since RELS (as well as HCONS and ING, see
below) might take more than one argument, we do
not impose a property restriction here, so they are re-
lational by default. TOP maps to a special variable
class (see below), and RELS to EPs.
TOP. The TOP property always takes a handle vari-
able; other variable classes, such as label vars are
used for restricting properties:
<owl:Class rdf:ID="Var"/>
<owl:Class rdf:ID="HandleVar">
<rdfs:subClassOf rdf:resource=
"#Var"/>
</owl:Class>
<owl:Class rdf:ID="LabelVar">
<rdfs:subClassOf rdf:resource=
"#Var"/>
</owl:Class>
Actually, this modelling is mere window-dressing
and clearly verbose, since an OWL instance of
class RMRS is always assigned a name (<RMRS
rdf:ID="...">), and in fact, this name can be
taken to be the TOP handle. This means that we can
in principle forbear from the TOP property. However,
if we want to utilize morpho-syntactical information
in subsequent inference steps, we have to enrich the
above variable classes with further properties/roles,
such as tense, pers, or num (see, e.g., the ?struc-
tured? variables in the structure for present v in
Figure 1).
RELS. Elements of RELS, i.e., concrete EPs are
essentially ?slimed? instances of feature structure
types. Overall, this means that we have to represent
the relevant types of the linguistic type hierarchy and
their subsumption relationship as OWL classes. As
shown in Section 3, this process can be automated
and only some guidance from a knowledge engineer
is necessary to mark the features that should not be
taken over to the DL side.
HCONS and ING. HCONS essentially specifies a
ternary relation, but since OWL (and DL in general)
are restricted to unary and binary relations, one way
to model a qeq constraint is to define a binary prop-
erty, consisting of a left-hand and a right-hand side.
From what has been said above, the left-hand side is a
handle and the right-hand side a label, hence we have
the following declaration for qeq:
<owl:ObjectProperty rdf:ID="qeq">
<rdfs:domain rdf:resource=
"#HandleVar"/>
<rdfs:range rdf:resource=
"#LabelVar"/>
</owl:ObjectProperty>
Given this way of modelling, it is now impossible to
define a property HCONS (as well as ING) on class
RMRS, since properties can only take instances of
classes, but not instances of other properties. How-
ever, since we assume that our variables (instances
of class Var) are always unique at runtime, it is in
principle not necessary to group the qeq constraint
inside an (R)MRS?note that there is still a connec-
tion between EPs and qeq constraints through the use
of variables. However, if we want to talk about/want
to access the qeq constraints of a specific (R)MRS
instance directly, this kind of modelling is somewhat
unhandy.
To overcome this seemingly wrong representation
(we are neutral about this), we have to ?reify? or
?wrap? qeq property instances. This would mean that
qeq would no longer be a property, but instead be-
comes a class, say QEQ, consisting of a right-hand
and a left-hand side. With this in mind, we can eas-
ily model, e.g., the first qeq constraint qeq1 from the
above figure:
<RMRS rdf:ID="rmrs1">
<TOP rdf:resource="#h1"/>
<RELS rdf:resource="#ep1"/>
<HCONS rdf:resource="#qeq1"/>
594
...
</RMRS>
<QEQ rdf:ID="qeq1">
<LHS rdf:resource="#h5"/>
<RHS rdf:resource="#h11"/>
</QEQ>
<HandleVar rdf:ID="h1"/>
<HandleVar rdf:ID="h5"/>
<LabelVar rdf:ID="h11"/>
<int_m_rel rdf:ID="ep1">
<LBL rdf:resource="#h1"/>
<ARG0 rdf:resource="#e2"/>
<MARG rdf:resource="#h1"/>
</int_m_rel>
What we have said about qeq constraints so far do
hold for in-group constraints as well.
6 DL reasoning: a small example
We have already said that the OWL representation of
RMRS structures are a good starting point to imple-
ment some useful forms of reasoning. Consider the
sentence Did Bernd Kiefer present a paper at IJCAI
2005? from Figure 1. From the resulting EPs and
with the help of an in-group constraint, we can infer
the fact that Bernd Kiefer was (physically) at IJCAI
2005, assuming he has presented a paper (which he
did). The inference rule achieving this can be stated
informally as presenting a paper at a conference en-
tails being at the conference. A more formal rep-
resentation in terms of feature structures is given in
Figure 2.
Clearly this rule can be rewritten to operatate on
OWL expressions (as is proposed in SWRL (Hor-
rocks et al, 2004)) or on the underlying RDF triple
notation (which, for instance, OWLIM (Kiryakov,
2006) assumes). Note the use of logical variables
in the above rule in order to formulate the transport
of information from the LHS to the RHS. The above
rule abstract away from concrete persons and loca-
tions through the use of logic variables ?p and ?l.
Note further that the resulting RHS output structure
is no longer a RMRS but a domain-specific represen-
tation (somewhat simplified in this example) that can
be queried for or can be employed in subsequent rea-
soning tasks.
In (Frank et al, 2007), an implemented approach is
described that utilizes an additional frame represen-
tation layer (Ruppenhofer et al, 2006) in which rules
of the above kind are applied, using the term rewrit-
ing system of (Crouch, 2005).
7 Summary
Our paper returned to mind that there exists a close
relationship between feature logics as used in compu-
tational linguistics and description logics employed
in the Semantic Web community. This relation-
ship can be utilized to obtain more and better se-
mantic annotations through information extraction
and deep parsing of text documents. We have in-
dicated that specific language constructs in FL and
DL can be mutally transformed without losing any
meaning, whereas others can only be approximated
(esp., role-value maps/reentrencies and functional
features/relational roles).
We have described an implemented procedure that
maps ontology instances and concepts to named en-
tity recognition and information extraction resources.
As argued in the paper, the benefits for minimized
domain-specific and linguistic knowledge engineer-
ing are manifold. An application using hybrid shal-
low and deep NL processing on the basis of the
mapped ontology data has been successfully imple-
mented for question answering. This application
(Frank et al, 2007) employs an additional frame se-
mantics layer (cf. Section 6) on which light forms
of reasoning take place. In order to make this addi-
tional layer superflous, we have described a transfor-
mation scheme that maps (R)MRS into OWL descrip-
tions. Given these descriptions, rules of the above
kind (Section 6) can directly operate on OWL, and
no additional translation is necessary to query the in-
stance data, encoded in RDF/OWL.
References
Baader, Franz, Diego Calvanese, Deborah McGuinness,
Daniele Nardi, and Peter Patel-Schneider. 2003. The
Description Logic Handbook. Cambridge University
Press, Cambridge.
Busemann, Stephan, Witold Droz?dz?yn?ski, Hans-Ulrich
Krieger, Jakub Piskorski, Ulrich Scha?fer, Hans Uszko-
reit, and Feiyu Xu. 2003. Integrating Information Ex-
traction and Automatic Hyperlinking. In Proceedings of
the Interactive Posters/Demonstration at ACL-03, pages
117?120.
Carpenter, Bob. 1992. The Logic of Typed Feature Struc-
tures. Tracts in Theoretical Computer Science. Cam-
bridge University Press, Cambridge.
Clark, James, 1999. XSL Transformations (XSLT). W3C,
http://w3c.org/TR/xslt.
Copestake, Ann, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: An in-
troduction. Research on Language and Computation,
3(4):281?332, 12. DOI 10.1007/s11168-006-6327-9.
Copestake, Ann. 2003. Report on the Design of RMRS.
Technical Report D1.1b, University of Cambridge, Cam-
bridge, UK.
595
?
??
present v
LBL ?h1
ARG1 ?s
ARG2 ?o
?
?? &
[
paper n
ARG0 ?o
]
&
[ named rel
ARG0 ?s
CARG ?p
]
&
[ at p
LBL ?h2
ARG2 ?x
]
&
[ named rel
ARG0 ?x
CARG ?l
]
& (?h1 ing ?h2) =?
[
PERSON ?p
LOCATION ?l
]
Figure 2: RMRS rule over EPs and in-group constraint.
Crouch, Richard. 2005. Packed rewriting for map-
ping semantics to KR. In Proceedings of the Interna-
tional Workshop on Computational Semantics (IWCS) 6,
Tilburg.
Droz?dz?yn?ski, Witold, Hans-Ulrich Krieger, Jakub Pisko-
rski, Ulrich Scha?fer, and Feiyu Xu. 2004. Shallow Pro-
cessing with Unification and Typed Feature Structures?
Foundations and Applications. KI, 04(1):17?23.
Flickinger, Dan. 2002. On building a more efficient gram-
mar by exploiting types. In Oepen, S. D. Flickinger,
J. Tsuji, and H. Uszkoreit, editors, Collaborative Lan-
guage Engineering. A Case Study in Efficient Grammar-
based Processing, pages 1?17. CSLI Publications.
Frank, Anette, Kathrin Spreyer, Witold Droz?dz?yn?ski, Hans-
Ulrich Krieger, and Ulrich Scha?fer. 2004. Constraint-
Based RMRS Construction from Shallow Grammars. In
Mu?ller, Stefan, editor, Proceedings of the HPSG04 Con-
ference Workshop on Semantics in Grammar Engineer-
ing, pages 393?413. CSLI Publications, Stanford, CA.
Frank, Anette, Hans-Ulrich Krieger, Feiyu Xu, Hans
Uszkoreit, Berthold Crysmann, and Ulrich Scha?fer.
2007. Question answering from structured knowledge
sources. Journal of Applied Logics, Special Issue on
Questions and Answers: Theoretical and Applied Per-
spectives, 5(1):20?48.
Haghighi, Aria and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1152?1161.
Horrocks, Ian, Peter F. Patel-Schneider, Harold Boley, Said
Tabet, Benjamin Grosof, and Mike Dean. 2004. SWRL:
A semantic web rule language combining OWL and
RuleML. W3C Member Submission.
Kiryakov, Atanas. 2006. OWLIM: balancing between
scalable repository and light-weight reasoner. Presen-
tation of the Developer?s Track of WWW2006.
Krieger, Hans-Ulrich and Ulrich Scha?fer. 1994. TDL?A
Type Description Language for Constraint-Based Gram-
mars. In Proceedings of the 15th International Confer-
ence on Computational Linguistics, COLING-94, pages
893?899.
Krieger, Hans-Ulrich, Witold Droz?dz?yn?ski, Jakub Pisko-
rski, Ulrich Scha?fer, and Feiyu Xu. 2004. A Bag of Use-
ful Techniques for Unification-Based Finite-State Trans-
ducers. In Proceedings of KONVENS 2004, pages 105?
112.
McGuinness, Deborah L. and Frank van Harmelen. 2004.
OWL Web Ontology Language Overview. Technical re-
port, W3C. 10 February.
Nebel, Bernhard and Gert Smolka. 1990. Represen-
tation and reasoning with attributive descriptions. In
Bla?sius, K.-H., U. Hedtstu?ck, and C.-R. Rollinger, ed-
itors, Sorts and Types in Artificial Intelligence, pages
112?139. Springer, Berlin. Also available as IWBS Re-
port 81, IBM Germany, September 1989.
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. Studies in Contemporary Linguis-
tics. University of Chicago Press, Chicago.
Ruppenhofer, Josef, Michael Ellsworth, Miriam R.L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: extended theory and prac-
tice. Technical report, International Computer Sci-
ence Institute (ICSI), University of California, Berkley.
http://framenet.icsi.berkley.edu/book/book.pdf.
Scha?fer, Ulrich, Hans Uszkoreit, Christian Federmann,
Torsten Marek, and Yajing Zhang. 2008. Extracting
and querying relations in scientific papers on language
technology. In Proceedings of LREC-2008.
Scha?fer, Ulrich. 2006. OntoNERdIE ? mapping and link-
ing ontologies to named entity recognition and infor-
mation extraction resources. In Proceedings of the 5th
International Conference on Language Resources and
Evaluation LREC-2006, pages 1756?1761, Genoa, Italy.
596
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 30?34,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Ontology-Based Incremental Annotation of Characters in Folktales
Thierry Declerck
DFKI GmbH
Stuhlsatzenhausweg, 3
66123 Saarbru?cken, Germany
declerck@dfki.de
Nikolina Koleva
DFKI GmbH
Stuhlsatzenhausweg, 3
66123 Saarbru?cken, Germany
Nikolina.Koleva@dfki.de
Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg, 3
66123 Saarbru?cken, Germany
krieger@dfki.de
Abstract
We present on-going work on the auto-
mated ontology-based detection and recog-
nition of characters in folktales, restricting
ourselves for the time being to the anal-
ysis of referential nominal phrases occur-
ring in such texts. Focus of the presently
reported work was to investigate the inter-
action between an ontology and linguistic
analysis of indefinite and indefinite nomi-
nal phrase for both the incremental annota-
tion of characters in folktales text, includ-
ing some inference based co-reference res-
olution, and the incremental population of
the ontology. This in depth study was done
at this early stage using only a very small
textual base, but the demonstrated feasibil-
ity and the promising results of our small-
scale experiment are encouraging us to de-
ploy the strategy on a larger text base, cov-
ering more linguistic phenomena in a mul-
tilingual fashion.
1 Introduction
In this submission we present on-going work
dealing with the automatic annotation of charac-
ters in folktales. Focus of the investigation lies in
using an iterative approach that combines an in-
cremental ontology population and an incremen-
tal linguistic annotation. Our starting point is
given by an in-house developed ontology, which
is having as its core the description of family rela-
tions, but also some typical elements of folktales,
like supernatural entities, etc.
The use of ontologies in the field of folktales is
not new, but to our knowledge no attempt has been
done so far to use ontologies in combination with
natural language processing for automatizing the
annotation of folktales, or for automatically popu-
lating a knowledge base of characters of folktales.
The work by (Peinado et al, 2004) is dealing in
first line with the Proppian functions that charac-
ter can play and is also geared towards generation
of interactive stories. (Zoelllner-Weber, 2008) is
much closer to our aim, and in fact the author is
proposing lines of research we want to implement
in the near future, but her work is explicitly not
dealing with the automation of annotation, and
she is also not concerned with linguistic annota-
tion in particular, but with general TEI annota-
tion1 of text structures.
At the present stage of development, we re-
stricted ourselves to investigate the role indefi-
nite and definite nominal phrases (NPs) can play
for the detection of characters and their storage
as instances of ontology classes. This decision
is echoing well-established investigations on one
possible function of indefinite NPs, namely to in-
troduce a new referent in a discourse (see among
others (von Heusinger, 2000)), whereas indefinite
NPs can be used in the subsequent text to refer
back to the introduced referential entities. This
fact has also been acknowledged in the field of
folktales and narratives and (Herman, 2000), for
example, stressed the importance of analyzing se-
quences of referring expressions for achieving a
more complete and accurate view on the role of
participants in narratives.
Discourse models resulting from the sequence
of referring expressions can thus support the com-
prehension of narratives (see (Herman, 2000),p.
962). Agreeing with this study, we further think
that the automated analysis of referential expres-
1TEI stands for ?Text Encoding Initiative, see www.tei-
c.org
30
sions in folktales, delivering essential elements
for the character models used in the interpretation
of narratives, can be of help in the automated anal-
ysis of the whole folktale, and more generally for
the automated analysis of narratives.
While (Herman, 2000) treats the role of
anaphora used in transcripts of ghost stories, we
deal (for the time being) only with the relation
between characters introduced by indefinite NPs
and their subsequent enunciation by definite NPs.
In the next sections we present the main com-
ponents of the current version of our system. We
discuss also the results of a first evaluation study,
and conclude with indication on future work.
2 The Ontology
As mentioned above, our starting point is an on-
tology, developed at our lab. This ontology will
be made publicly available, after merging it with
further ontological elements relevant to the field
of narratives, as those are for example described
in (Zoelllner-Weber, 2008), and associating its
classes and relations with elements relevant for
the linguistic and semantic annotation of folk-
tales, as described for example in (Scheidel and
Declerck, 2010).
The class hierarchy and the associated relations
(or properties) are equipped with natural language
labels and comments. The labels are available in
four languages: Bulgarian, English, German and
Russian. But our work is dealing for the time be-
ing only with English.
An example of class of the ontology, with its
labels is given just below:
<owl:Class rdf:about="#BiolDaughter">
<owl:equivalentClass>
<owl:Class>
<owl:intersectionOf rdf:parseType="Collection">
<owl:Restriction>
<owl:onProperty rdf:resource="#hasBiolParent"/>
<owl:onClass rdf:resource="#BiolParent"/>
<owl:minQualifiedCardinality
rdf:datatype="&xsd;nonNegativeInteger">
1</owl:minQualifiedCardinality>
</owl:Restriction>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasGender"/>
<owl:hasValue>f</owl:hasValue>
</owl:Restriction>
</owl:intersectionOf>
</owl:Class>
</owl:equivalentClass>
<rdfs:subClassOf rdf:resource="#BiolChild"/>
<rdfs:subClassOf rdf:resource="#Daughter"/>
<rdfs:comment>The class of biological daughther is
a subclass of biological child and of daughter.
This class designates all biological daughters.
Each member of this class has gender female
and at least one biological parent.
</rdfs:comment>
<dc:language xml:lang="bg">
&#1073;&#1080;&#1086;&#1083;&#1086;&#1075;&#1080;&#1095;
&#1085;&#1072; &#1044;&#1098;&#1097;&#1077;&#1088;&#1103;
</dc:language>
<dc:language xml:lang="de">
biologische Tochter
</dc:language>
<dc:language xml:lang="en">
biological Daughter
</dc:language>
<dc:language xml:lang="ru">
&#1073;&#1080;&#1086;&#1083;&#1086;&#1075;&#1080;
&#1095;&#1077;&#1089;&#1082;&#1072;&#1103; &#1044;
&#1086;&#1095;&#1100;
</dc:language>
</owl:Class>
The ontology also encodes inference rules that
allow establishing automatically family relations
between entities (characters) that have been stored
at the instance level, which is the process of on-
tology population resulting from detection in the
text.
1. hasParent(?x, ?x1), hasParent(?x, ?x2), hasParent(?y,
?x1), hasParent(?y, ?x2), hasGender(?x, ?f?), notE-
qual(?x, ?y)? Sister(?x)
2. Daughter(?d) , Father(?f) , Son(?s) ? hasBrother(?d,
?s), hasChild(?f, ?s), hasChild(?f, ?d), hasSister(?s,
?d)
The first rule is about class inference. It states
that if two different individuals in the ontology
(represented by the variables x and y) share the
same parents (represented by the variables x1 and
x2) and if the gender of one of the two individuals
is female, then this individual can be considered
as an instance of the ontology class Sister.
According to the second rule, various relations
(or properties) can be inferred from the fact that
we have three individuals (represented by the vari-
ables d, f and s) that are instances of the classes
31
Daughter, Father and Son respectively. The first
inferred relations state that the Daughter has the
Son as her Brother and the Son reciprocally has
the Daughter as his Sister. In addition, the Father
is being assigned twice the HASCHILD property,
once for the Daughter and second for the Son.
3 Processing Steps
We submit first a folktale, here the ?Magic Swan
Geese?2, to a linguistic processing engine (the
NooJ platform, see (Silberztein, 2003)), applying
to the text nominal phrases recognition rules (in-
cluding coordination), which are differentiated in
being either indefinite or definite (we do not con-
sider pronouns for the time being). All annotated
NPs are indexed with ID numbers.
In the following step, our algorithm extracts
from the whole annotated text the nominal heads
of the indefinite NPs and compares them with the
labels present in the ontology. In case a match can
be established, the nominal head of this phrase is
used for populating the corresponding ontology
class as an individual and the text is annotated
with the nominal head being a (potential) char-
acter, as can be seen in the annotation example
below, where the reader can observe that the (po-
tential) characters are also enumerated, this time
on the base of the (unique) ID they get during
the ontology population phase. In this step thus,
all candidate characters in text are automatically
marked-up with both linguistic and character in-
formation derived from the ontology.
<text>
There lived
<NPCOORD id="z_coord_ph1" Nb="p" HEAD1="man"
HEAD2="woman" Type="and">
<NP id="indef_ph1" SPEC="a" HEAD="man"
Gender="m" Num="s">
<CHAR id="ch1" TYPE="man" Gender="m"
Num="s">
an old man</CHAR>
</NP>
and
<NP id="indef_ph2" SPEC="a" HEAD="woman"
2http://en.wikipedia.org/wiki/The Magic Swan Geese
Gender="f" Num="s">
<CHAR id="ch2" TYPE="woman" Gender="f"
Num="s">
an old woman</CHAR>
</NP>
</NPCOORD>
;
they had
<NPCOORD id="z_coord_ph2" Nb="p" HEAD1=
"daughter" HEAD2="son" Type="and">
<NP id="indef_ph3" SPEC="a"
HEAD="daughter" Gender="f" Num="s">
<CHAR id="ch3" TYPE="daughter"
Gender="f" Num="s">
a daughter</CHAR>
</NP>
and
<NP id="indef_ph4" SPEC="a" HEAD="son"
Gender="m" Num="s">
<CHAR id="ch4" TYPE="son" Gender="m"
Num="s">
a little son</CHAR>
</NP>
</NPCOORD>
In the next step, the inference rules of the ontol-
ogy are applied to the candidate characters (the in-
dividuals stored so far in the knowledge base). In
the particular tale we are analysing the class asso-
ciated with the potential character ch2 (Woman)
can be equated with the class Mother and its as-
sociated string in the label (mother), so that all
occurrences of the two strings in the text can be
marked as referring to the same character.
Also some relations are established between
the individuals by the application of the inference
rules described in the ontology section above:
Wife of, Mother Of, etc. (together with the
strings listed in the labels). If the related strings
are found in definite NPs in the text, the cor-
responding segment can then be annotated by
32
our linguistic processing engine with the origi-
nal character identifier. In the example below, the
reader can see that on the base of the inference
rules, the string mother in the definite NP (with
ID DEF PH6) is referred to ch2 (see the first an-
notation example above for the first occurrence of
ch2).
<NP id="def_ph6" SPEC="the" HEAD="mother"
Gender="f" Num="s">
the mother</NP>
said: "Daughter, daughter, we are going
to work; we shall bring you back
<NP id="indef_ph7" SPEC="a" HEAD="bun"
Gender="n" Num="s">
<CHAR id="ch6" TYPE="bun" Gender="n"
Num="s">a little bun</CHAR>
</NP>
, sew you <NP id="indef_ph8" SPEC="a"
HEAD="dress" Gender="n" Num="s">
<CHAR id="ch7" TYPE="dress"
Gender="n" Num="s">
a little dress</CHAR>
</NP>
The same remark is valid for the ch3 (?a
daughter? introduced in the first sentence of the
tale). In the definite NP with ID 12, the string
(daughter) is occurring in the context of a def-
inite NP, and thus marked as referring to ch3.
The string (girl) is occurring four times in the
context of definite NPs (with IDs 18, 25, 56 and
60) and for all those 4 occurrences the inference
driven mark-up of the nominal head with ch3
turns out to be correct.
In this annotation example, the reader can also
see that the heads of all indefinite NPs are first
considered as potential characters. A preliminary
filtering of such expression like dress is not pos-
sible, since in folktales, every object can be an
actant. So for example in this tale, an oven, an
apple tree or a river of milk are play-
ing an important role, and are characters involved
in specific actions. Our filtering is rather taking
place in a post-processing phase: strings that get
only once related to a potential character ID and
which are not involved in an action are at the end
discarded.
The next steps are dealing with finding other
occurrences of the potential characters (within
definite NPs), or to exclude candidates from the
set.
4 Evaluation of the approach
In order to be able to evaluate our approach,
even considering that we are working on a very
small text base, we designed a first basic test data
and annotated manually the folktale ?The magic
swan geese? with linguistic and character anno-
tation. The linguistic annotation is including co-
referential information. In the longer term, we
plan to compare our work applied to more folk-
tales with a real gold standard, the UMIREC cor-
pus (http://dspace.mit.edu/handle/1721.1/57507)
Our evaluation study shows results in terms of
correct detection of tale characters in compari-
son with the manually annotated data. Eight of
the real characters were correctly classified by the
tool. Three of the instances are actually charac-
ters but they were not detected. One candidate is
not a character according to the manually anno-
tated data, but the system classified it as charac-
ter. Seven entities were correctly detected as non
characters. On this small basis, we calculated the
accuracy of the tool, which is 79%. We also com-
puted the precision, the recall and the F-measure.
The precision amounts to 88%; the recall to 73%;
and the value of the balanced F-measure is 80%.
So these metrics confirm what the accuracy has
been already expressing: the results are encour-
aging.
Looking at the errors made by the tool, we know
that it does not consider the characters that are
mentioned only one time. In our text, a hedge-
hog occurs only once. However, the human intu-
ition is that it is a character and differs from the
phrases a bun and a dress, which have just de-
scriptive function. In a next version of the tool,
it will be checked if the head of an indefinite NP,
which is present only once in the text, is having
an active semantic role, like Agent. In this case, it
can be considered as a character.
Another problem of our actual approach is that
we do not consider yet the possessive phrases and
pronominal expressions. Precise analysis of these
anaphoric expressions will improve the approach
33
in augmenting the number of occurrences of can-
didate characters. We also expect the availability
of related instances in the knowledge base to help
in resolving pronominal co-reference phenomena.
The applied method does not detect one of the
main characters in the sample text namely the
swan-geese. The swan-geese are introduced in the
discourse only via a definite noun phrase. If there
are some equivalent phrases, for example occur-
ring in the title of the tale, they can be annotated
as character by the tool. An additional problem
we have, is the fact that our NP grammar has ana-
lyzed the words swan and geese as separate nouns
and not as a compound noun. So that the linguis-
tic analysis for English compounds has to be im-
proved.
5 Conclusion and future work
Our in depth investigation of the interaction of an
ontology and language processing tools for the
detection of folktale characters and their use for
incrementally populating an ontology seems to be
promising, and it has allowed for example to asso-
ciate a unique character ID to occurrences of dif-
ferent nominal heads, on the base of their inferred
semantic identity. A possible result of our work
would lie in the constitution of larger database
containing characters of narratives extracted au-
tomatically from text.
We plan to tackle the processing of pronominal
and possessive expressions for completing the co-
reference task. We plan also to extend our work to
other languages, and we already started to do this
for anaother folktale in German, in which much
more complex family relationships are involved
(the German version of the tale ?Father Frost?).
But more challenging will be to deal with lan-
guages, which do not know have the difference
between indefinite and definite NPs.
Acknowledgments
The work presented in this paper has been partly
supported by the R&D project. ?Monnet?, which
is co-funded by the European Union under Grant
No. 248458.
References
Marc Cavazza and David Pizzi. 2006. Narratology for
interactive storytelling: A critical introduction. In
TIDSE, pages 72?83.
Maja Hadzic, Pornpit Wongthongtham, Tharam Dil-
lon, Elizabeth Chang, Maja Hadzic, Pornpit
Wongthongtham, Tharam Dillon, and Elizabeth
Chang. 2009. Introduction to ontology. In
Ontology-Based Multi-Agent Systems, volume 219
of Studies in Computational Intelligence, pages 37?
60. Springer Berlin / Heidelberg.
Knut Hartmann, Sandra Hartmann, and Matthias
Feustel. 2005. Motif definition and classification
to structure non-linear plots and to control the nar-
rative flow in interactive dramas. In International
Conference on Virtual Storytelling, pages 158?167.
David Herman. 2000. Pragmatic constraints on narra-
tive processing: Actants and anaphora resolution in
a corpus of north carolina ghost stories. Journal of
Pragmatics, 32(7):959 ? 1001.
Harry R. Lewis and Christos H. Papadimitriou. 1998.
Elements of the theory of computation. Prentice-
Hall.
Deborah L. McGuinness and Frank van Harmelen.
10 February 2004. OWL Web Ontology Language
Overview. W3C Recommendation.
Federico Peinado, Pablo Gerva?s, and Bele?n D??az-
Agudo. 2004. A description logic ontology for
fairy tale generation. In Language Resources for
Linguistic Creativity Workshop, 4th LREC Confer-
ence, pages 56?61.
Vladimir IA. Propp, American Folklore Society., and
Indiana University. 1968. Morphology of the
folktale / by V. Propp ; first edition translated by
Laurence Scott ; with an introduction by Svatava
Pirkova-Jakobson. University of Texas Press,
Austin :, 2nd ed. / revised and edited with a pref-
ace by louis a. wagner ; new introduction by alan
dundes. edition.
Antonia Scheidel and Thierry Declerck. 2010. Apftml
- augmented proppian fairy tale markup language.
In Sa?ndor Dara?nyi and Piroska Lendvai, editors,
First International AMICUS Workshop on Auto-
mated Motif Discovery in Cultural Heritage and
Scientific Communication Texts: Poster session. In-
ternational Workshop on Automated Motif Discov-
ery in Cultural Heritage and Scientific Communica-
tion Texts (AMICUS-10), located at Supporting the
Digital Humanities conference 2010 (SDH-2010),
October 21, Vienna, Austria. Szeged University,
Szeged, Hungary, 10.
Max Silberztein. 2003. Nooj manual. available for
download at: www.nooj4nlp.net.
Klaus von Heusinger. 2000. The reference of indefi-
nites. In K. von Heusinger and U. Egli, editors, Ref-
erence and Anaphoric Relations, pages 247?265.
Kluwer.
Amelie Zoelllner-Weber. 2008. Noctua literaria -
A Computer-Aided Approach for the Formal De-
scription of Literary Characters Using an Ontol-
ogy. Ph.D. thesis, University of Bielefeld, Biele-
feld, Germany, may.
34
