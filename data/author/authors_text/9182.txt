Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 478?486,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Acquiring Translation Equivalences of Multiword Expressions by 
Normalized Correlation Frequencies 
Ming-Hong Bai1,2 Jia-Ming You1 Keh-Jiann Chen1 Jason S. Chang2 
1 Institute of Information Science, Academia Sinica, Taiwan 
2 Department of Computer Science, National Tsing-Hua University, Taiwan 
mhbai@sinica.edu.tw, swimming@hp.iis.sinica.edu.tw, 
kchen@iis.sinica.edu.tw, jschang@cs.nthu.edu.tw 
 
Abstract 
In this paper, we present an algorithm for ex-
tracting translations of any given multiword 
expression from parallel corpora. Given a 
multiword expression to be translated, the 
method involves extracting a short list of tar-
get candidate words from parallel corpora 
based on scores of normalized frequency, 
generating possible translations and filtering 
out common subsequences, and selecting the 
top-n possible translations using the Dice 
coefficient. Experiments show that our ap-
proach outperforms the word alignment-
based and other naive association-based me-
thods. We also demonstrate that adopting the 
extracted translations can significantly im-
prove the performance of the Moses machine 
translation system. 
1 Introduction 
Translation of multiword expressions (MWEs), 
such as compound words, phrases, collocations 
and idioms, is important for many NLP tasks, 
including the techniques are helpful for dictio-
nary compilation, cross language information 
retrieval, second language learning, and machine 
translation. (Smadja et al, 1996; Gao et al, 2002; 
Wu and Zhou, 2003). However, extracting exact 
translations of MWEs is still an open problem, 
possibly because the senses of many MWEs are 
not compositional (Yamamoto and Matsumoto, 
2000), i.e., their translations are not composi-
tions of the translations of individual words. For 
example, the Chinese idiom ???? should be 
translated as ?turn a blind eye,? which has no 
direct relation with respect to the translation of 
each constituent (i.e., ?to sit?, ?to see? and ?to 
ignore?) at the word level.  
Previous SMT systems (e.g., Brown et al, 
1993) used a word-based translation model 
which assumes that a sentence can be translated 
into other languages by translating each word 
into one or more words in the target language. 
Since many concepts are expressed by idiomatic 
multiword expressions instead of single words, 
and different languages may realize the same 
concept using different numbers of words (Ma et 
al., 2007; Wu, 1997), word alignment based me-
thods, which are highly dependent on the proba-
bility information at the lexical level, are not 
well suited for this type of translation.  
To address the above problem, some methods 
have been proposed for extending word align-
ments to phrase alignments. For example, Och et 
al. (1999) proposed the so-called grow-diag-
final heuristic method for extending word 
alignments to phrase alignments. The method is 
widely used and has achieved good results for 
phrase-based statistical machine translation. 
(Och et al, 1999; Koehn et al, 2003; Liang et al, 
2006). Instead of using heuristic rules, Ma et al 
(2008) showed that syntactic information, e.g., 
phrase or dependency structures, is useful in ex-
tending the word-level alignment. However, the 
above methods still depend on word-based 
alignment models, so they are not well suited to 
extracting the translation equivalences of seman-
tically opaque MWEs due to the lack of word 
level relations between the translational corres-
pondences. Moreover, the aligned phrases are 
not precise enough to be used in many NLP ap-
plications like dictionary compilation, which 
require high quality translations. 
Association-based methods, e.g., the Dice 
coefficient, are widely used to extract transla-
tions of MWEs. (Kupiec, 1993; Smadja et al, 
1996; Kitamura and Matsumoto, 1996; Yama-
moto and Matsumoto, 2000; Melamed, 2001). 
The advantage of such methods is that associa-
tion relations are established at the phrase level 
instead of the lexical level, so they have the po-
tential to resolve the above-mentioned transla-
tion problem. However, when applying associa-
tion-based methods, we have to consider the fol-
lowing complications. The first complication, 
which we call the contextual effect, causes the 
extracted translation to contain noisy words. For 
478
example, translations of the Chinese idiom ??
?? (best of both worlds) extracted by a naive 
association-based method may contain noisy 
collocation words like difficult, try and cannot, 
which are not part of the translation of the idiom. 
They are actually translations of its collocation 
context, such as ??(difficult), ??(try), and 
??(cannot). This problem arises because naive 
association methods do not deal with the effect 
of strongly collocated contexts carefully. If we 
can incorporate lexical-level information to dis-
count the noisy collocation words, the contextual 
effect could be resolved. 
 
English (y) fy fx,y Dice(x,y)
quote out of context 22 19 0.56 
take out of context 17 11 0.35 
interpret out of context 2 2 0.08 
out of context 53 32 0.65 
Table 1. The Dice coefficient tends to select a com-
mon subsequence of translations. (The frequency of
???? ,fx, is 46.) 
 
The second complication, which we call the 
common subsequence problem, is that the Dice 
coefficient tends to select the common subse-
quences of a set of similar translations instead of 
the full translations. Consider the translations of 
???? (quote out of context) shown in the 
first three rows of Table 1. The Dice coefficient 
of each translation is smaller than that of the 
common subsequence ?out of context? in the last 
row. If we can tell common subsequence apart 
from correct translations, the common subse-
quence problem could be resolved. 
In this paper, we propose an improved preci-
sion method for extracting MWE translations 
from parallel corpora. Our method is similar to 
that of Smadja et al (1996), except that we in-
corporate lexical-level information into the asso-
ciation-based method. The algorithm works ef-
fectively for various types of MWEs, such as 
phrases, single words, rigid word sequences (i.e., 
no gaps) and gapped word sequences. Our expe-
riment results show that the proposed translation 
extraction method outperforms word alignment-
based methods and association-based methods. 
We also demonstrate that precise translations 
derived by our method significantly improve the 
performance of the Moses machine translation 
system. 
The remainder of this paper is organized as 
follows. Section 2 describes the methodology for   
extracting translation equivalences of MWEs. 
Section 3 describes the experiment and presents 
the results. In Section 4, we consider the appli-
cation of our results to machine translation. Sec-
tion 5 contains some concluding remarks. 
2 Extracting Translation Equivalences   
Our MWE translation extraction method is simi-
lar to the two-phase approach proposed by 
Smadja et al (1996). The two phases can be 
briefly described as follows:  
Phase 1: Extract candidate words correlated to 
the given MWE from parallel text. 
Phase 2:  
1. Generate possible translations for the 
MWE by combining the candidate words. 
2. Select possible translations by the Dice 
coefficient. 
We propose an association function, called the 
normalized correlation frequency, to extract 
candidate words in the phase 1. This method 
incorporates lexical-level information with asso-
ciation measure to overcome the contextual ef-
fect. In phase 2, we also propose a weighted fre-
quency function to filter out false common sub-
sequences from possible translations. The filter-
ing step is applied before the translation select-
ing step of phase 2.   
Before describing our extraction method, we 
define the following important terms used 
throughout the paper. 
Focused corpus (FC): This is the corpus 
created for each targeted MWE. It is a subset of 
the original parallel corpora, and is comprised of 
the selected aligned sentence pairs that contain 
the source MWE and its translations. 
Candidate word list (CW): A list of extracted 
candidate words for the translations of the 
source MWE. 
2.1 Selecting Candidate Words 
For a source MWE, we try to extract from the 
FC a set of k candidate words CW that are high-
ly correlated to the source MWE. We then as-
sume that the target translation is a combination 
of some words in CW. As noted by Smadja et al 
(1996), this two-step approach drastically reduc-
es the search space. 
However, translations of collocated context 
words in the source word sequence create noisy 
candidate words, which might cause incorrect 
extraction of target translations by naive statis-
tical correlation measures, such as the Dice coef-
479
ficient used by Smadja et al (1996). The need to 
avoid this context effect motivates us to propose 
a candidate word selection method that uses the 
normalized correlation frequency as an associa-
tion measure. 
The rationale behind the proposed method is 
as follows. When counting the word frequency, 
each word in the target corpus normally contri-
butes a frequency count of one. However, we are 
only interested in the word counts correlated to a 
MWE. Therefore, intuitively, we define the 
normalized count of a target word e as the trans-
lation probability of e given the MWE.  
We explain the concept of normalizing the 
correlation count in Section 2.1.1 and the com-
putation of the normalized correlation frequency 
in Section 2.1.2. 
2.1.1 Normalizing Correlation Count 
We propose an association measure called the 
normalized correlation frequency, which ranks 
the association strength of target words with the 
source MWE. For ease of explanation, we use 
the following notations: let f=f1,f2,?,fm and 
e=e1,e2,?,en be a pair of parallel Chinese and 
English sentences; and let w=t1,t2,?,tr be the 
Chinese source MWE. Hence, w is a subse-
quence of f.  
When counting the word frequency, each 
word in the target corpus normally contributes a 
frequency count of one. However, since we are 
interested in the word counts that correlate to w, 
we adopt the concept of the translation model 
proposed by Brown et al(1993). Each word e in 
a sentence e might be generated by some words, 
denoted as r, in the source sentence f. If r is 
non-empty the relation between r and w should 
fit one of the following cases: 
 
1) All words in r belong to w, i.e., wr ? , so 
we say that e is only generated by w. 
2) No words in r belong to w, i.e., wfr ?? , 
so we say that e is only generated by context 
words.  
3) Some words in r belong to w, while others 
are context words. 
 
Intuitively, In Cases 1 and 2, the correlation 
count of an instance e should be 1 and 0 respec-
tively. In Case 3, the normalized count of e is 
the expected frequency generated by w divided 
by the expected frequency generated by f. With 
that in mind, we define the weighted correlation 
count, wcc, as follows:  
 
?
?
??
??
?+
?+=
f
w
f
w
wfe
j
i
f j
f i
fep
fep
ewcc
||)|(
||)|(
),,;( , 
 
where ? is a very small smoothing factor in case 
e is not generated by any word in f. The proba-
bility p(e | f) is the word translation probability 
trained by IBM Model 1 on the whole parallel 
corpus. 
The rationale behind the weighted correlation 
count, wcc, is that if e is part of the translation of 
w, then its association with w should be stronger 
than other words in the context. Hence its wcc 
should be closer to 1. Otherwise, the association 
is weaker and the wcc should be closer to 0. 
2.1.2 Normalized Correlation 
Once the weighted correlation counts wcc is 
computed for each word in FC, we compute the 
normalized correlation frequency for each word 
e as the total sum of the  of all w 
in bilingual sentences (e, f)  in FC. The norma-
lized correlation frequency (ncf) is defined as 
follows: 
),,;( wfeewcc
 
?
=
=
n
i
iiewccencf
1
)()( ),,;();( wfew . 
 
We choose the top-n English words ranked by 
ncf as our candidate words and filter out those 
whose ncf is less than a pre-defined threshold. 
Table 2 shows the candidate words for the Chi-
nese term ???? (quote/take/interpret out of 
context) sorted by their ncf values. To illustrate 
the effectiveness ncf, we also display candidate 
words of the term with their Dice values in 
Tables 3. As shown in the tables, noise words 
such as justify, meaning and unfair are ranked 
lower using ncf than using Dice, while correct 
candidates, such as out, take and remark are 
ranked higher.  We present more experimental 
results in Section 3. 
 
2.2 Generation and Ranking of Candi-
date Translations 
After determining the candidate words, candi-
date translations of w can be generated by mark-
ing the candidate words in each sentence of FC. 
The word sequences marked in each sentence 
are deemed possible translations. At the same 
time, the weakly associated function words,  
480
Candidate words e freq ncf(e,w) 
context 54 31.55 
out 58 24.58 
quote 26 5.84 
take 23 4.81 
remark 8 1.84 
interpret 3 1.38 
piecemeal 1 0.98 
deliberate 3 0.98 
Table 2. Candidate words for the Chinese term 
???? sorted by their global normalized correla-
tion frequencies. 
 
Candidate words e freq dice(e,w) 
context 54 0.0399 
quote 26 0.0159 
deliberate 3 0.0063 
justify 3 0.0034 
interpretation 7 0.0032 
meaning 3 0.0029 
cite 3 0.0025 
unfair 4 0.0023 
Table 3. Candidate words for the Chinese term ??
??  sorted by their Dice coefficient values. 
 
which we fail to select in the candidate word 
selection stage, should be recovered. The rule is 
quite simple: if a function word is adjacent to 
any candidate word, it should be recovered. For 
example, in the following sentence, the function 
word of would be recovered and added to the 
marked sequence: 
 
?The financial secretary has 
been quoted out of context. 
??? ?? ? ?? ? ????.?  
 
 The marked words are shown in boldface.  
2.2.1 Generating Possible Translations 
Although we have selected a reliable candidate 
word list, it may still contain some noisy words 
due to the MWE?s collocation context. Consider 
the following example: 
 
...as quoted in the audit 
report, if taken out of con-
text...  
 
In this instance, quoted is a false positive; there-
fore, the marked word sequence m ?quoted tak-
en out of context? is not the correct translation. 
To avoid such false positives, we include m and 
all its subsequences as possible translations.  
quoted taken out of context 
quoted taken out of 
quoted taken out context 
quoted taken of context 
quoted out of context 
taken out of context 
? 
quoted out 
taken out 
quoted 
taken 
out 
context
Table 4. Example subsequences generated of w and 
add them to the candidate translation list.  
 
Table 4 shows the subsequences of m in the 
above example. The generation process is used 
to increase the coverage of correct translations in 
the candidate list; otherwise, many correct trans-
lations will be lost. However, the process may 
also trigger the side effect of the common sub-
sequence problem described in Section 1.  Since 
all candidates compete for the best translations 
by comparing their association strength with w, 
the common subsequences will have an advan-
tage. 
 
2.2.2 Filtering Common Subsequences 
To resolve the common subsequence effect prob-
lem, we evaluate each candidate translation, in-
cluding its subsequences, by a concept similar to 
the normalized correlation frequency. As men-
tioned in Section 1, the Dice coefficient tends to 
select the common subsequences of some candi-
dates because they have higher frequencies. To 
avoid this problem, we use the normalized corre-
lation frequency to filter out false common sub-
sequences from the candidate translation list. 
Here, we also use the weighted correlation count 
wcc to weight the frequency count of a candidate 
translation. Suppose we have a marked sequence 
in a sentence, m, whose subsequences are gener-
ated in the way described in the previous section. 
If the weighted count of m is assigned the score 
1, the weighted count (wc) of a subsequence t is 
then defined as follows: 
 
?
???
?=
tm
wfewmfet
e
ewccwc )),,;(1(),,,;( . 
 
The underlying concept of wc is that the original 
marked sequence m is supposed to be the most 
481
likely translation of w and the weighted count is 
set to 1. Then, if a subsequence t is generated by 
removing a word e from m, the weighted count 
of the subsequence is reduced by multiplying the 
complement probability of e generated by w. 
Note that the weighted correlation count wcc is 
the probability of the word e generated by w. 
After all  in each sentence of 
the FC have been computed, the weighted fre-
quency for a sequence t can be determined by 
summing the weighted frequencies over FC as 
follows:  
),,,;( wmfetwc
 
?
??
=
FC
wcwf
),(
),,,;();(
fe
wmfetwt . 
 
We compute the wf for each candidate transla-
tion and then sort the candidate translations by 
their wf values. 
Next, we filter out common subsequences 
based on the following rule: for a sequence t, if 
there is a super-sequence t' on the sorted candi-
date translation list and the wf value of t is less 
than that of t', then t is assumed be a common 
subsequence of real translations and removed 
from the list. 
 
candidate translation list freq wf 
quote out of context 19 17.55 
of context 35 15.45 
out of context 32 14.82 
quote of context 19 13.32 
out 35 11.92 
quote 23 11.63 
quote out 19 9.42 
Table 5. Part of the candidate translation list for the 
Chinese idiom, ????, sorted by the wf values. 
 
Table 5 shows an example of the rule?s appli-
cation. The candidate translation list is sorted by 
the translations? wf values. Then, candidates 2-7 
are removed because they are subsequences of 
the first candidate and their wf values are smaller 
than that of the first candidate. 
2.3 Selection of Candidate Translations 
Having removed the common subsequences of 
real translations from the candidate translation 
list of w, we can select the best translations by 
comparing their association strength with w for 
the remaining candidates.  The Dice coefficient 
is a good measure for assessing the association 
strength and selecting translations from the can-
didate list. For a candidate translation t, the Dice 
coefficient is defined as follows: 
 
)()(
),(2
),(
wt
wt
wt
pp
p
Dice += . 
 
Where p(t,w), p(t), p(w) are probabilities of  
(t,w), t, w derived from the training corpus.  
After obtaining the Dice coefficients of the 
candidate translations, we select the top-n candi-
date translations as possible translations of w. 
 
3 Experiments 
In our experiments, we use the Hong Kong Han-
sard and the Hong Kong News parallel corpora 
as training data. The training data was prepro-
cessed by Chinese word segmentation to identify 
words and parsed by Chinese parser to extract 
MWEs. To evaluate the proposed approach, we 
randomly extract 309 Chinese MWEs from 
training data, including dependent word pairs 
and rigid idioms. We then randomly select 103 
of those MWEs as the development set and use 
the other 206 as the test set. The reference trans-
lations of each Chinese MWE are manually ex-
tracted from the parallel corpora. 
 
3.1 Evaluation of Word Candidates 
To evaluate the method for selecting candidate 
words, we use the coverage rate, which is de-
fined as follows: 
 
?
?
?=
w w
ww
||
||1
A
CA
n
coverage , 
 
where n is the number of MWEs in the test set, 
Aw denotes the word set of the reference transla-
tions of w, and Cw denotes a candidate word list 
extracted by the system.  
Table 6 shows the coverage of our method, 
NCF, compared with the coverage of the IBM 
model 1 and the association-based methods MI, 
Chi-square, and Dice. As we can see, the top-10 
candidate words of NCF cover almost 90% of 
the words in the reference translations. Whereas, 
the coverage of the association-based methods 
and IBM model 1 is much lower than 90%. The 
result implies that the candidate extraction me-
thod can extract a more precise candidate set 
than other methods. 
 
482
Method Top10 Top20 Top30 
MI 0.514 0.684 0.760 
Chi-square 0.638 0.765 0.828 
Dice 0.572 0.735 0.803 
IBM 1 0.822 0.900 0.948
NCF 0.899 0.962 0.973 
Table 6. The coverage rates of the candidate words 
extracted by the compared methods 
 
Figure 1 shows the curve diagram of the cov-
erage rate of each method. As the figure shows, 
when the size of the candidate list is increased, 
the coverage rate of using NCF rises rapidly as n 
increases but levels off after n=20. Whereas, the 
coverage rates of other measures grow much 
slowly.  
 
 
Figure 1. The curve diagram of the coverage of 
the candidate word list compiled by each method. 
 
From the evaluation of candidate word selec-
tion, we find that the ncf method, which incorpo-
rates lexical-level information into association-
based measure, can effectively filter out noisy 
words and generates a highly reliable list of can-
didate words for a given MWE. 
 
3.2 Evaluating Extracted Translations 
To evaluate the quality of MWE translations 
extracted automatically, we use the following 
three criteria: 
 
1) Translation accuracy: 
This criterion is used to evaluate the top-n 
translations of the system. It treats each 
translation produced as a string and com-
pares the whole string with the given ref-
erence translations. If any one of the top-n 
hypothesis translations is included in the 
reference translations, it is deemed correct.   
2) WER (word error rate): 
This criterion compares the top-1 hypo-
thesis translation with the reference trans-
lations by computing the edit distance (i.e., 
the minimum number of substitutions, in-
sertions, and deletions) between the hypo-
thesis translation and the given reference 
translations. 
3) PER (position-independent word error 
rate): 
This criterion ignores the word order and 
computes the edit distance between the 
top-1 hypothesis translation and the given 
reference translations. 
 
We also use the MT task to evaluated our me-
thod with other systems. For that, we use the 
GIZA++ toolkit (Och et al, 2000 ) to align the 
Hong Kong Hansard and Hong Kong News pa-
rallel corpora. Then, we extract the translations 
of the given source sequences from the aligned 
corpus as the baseline. We use the following two 
methods to extract translations from the aligned 
results. 
 
1) Uni-directional alignment  
We mark all English words that were 
linked to any constituent of w in the pa-
rallel Chinese-English aligned corpora. 
Then, we extract the marked sequences 
from the corpora and compute the fre-
quency of each sequence. The top-n high 
frequency sequences are returned as the 
possible translations of w. 
2) Bi-directional alignments 
We use the grow-diag-final heuristic (Och 
et al, 1999) to combine the Chinese-
English and English-Chinese alignments, 
and then extract the top-n high frequency 
sequences as described in method 1. 
 
To determine the effect of the common subse-
quence filtering method, FCS, we divide the 
evaluation of our system into two phases: 
 
1) NCF+Dice: 
This system uses the normalized correla-
tion frequency, NCF, to select candidate 
words as described in Section 2.1. It then 
extracts candidate translations (described 
in Section 2.2), but FCS is not used. 
2) NCF+FCS+Dice: 
This is similar to system 1, but it uses 
FCS to filter out common subsequences 
(described in subsection 2.2.2). 
483
Method WER(%) PER(%) 
Uni-directional 4.84 4.02 
Bi-directional 5.84 5.12 
NCF+Dice 3.55 3.24 
NCF+FCS+Dice 2.45 2.23 
Table 7. Translation error rates of the systems. 
 
 
Method Top1 Top2 Top3 
Uni-directional 67.5 79.6 83.0 
Bi-directional 65.5 77.7 81.1 
NCF+Dice 72.8 85.9 88.3 
NCF+FCS+Dice 78.2 89.3 91.7 
Table 8. Translation accuracy rates of the systems. 
(%) 
 
Table 7 shows the word error rates for the 
above systems. As shown in the first and second 
rows, the translations extracted from uni-
directional alignments are better than those ex-
tracted from bi-directional alignments. This 
means that the grow-diag-final heuristic reduces 
the accuracy rate when extracting MWE transla-
tions.  
The results in the third row show that the 
NCF+Dice system outperforms the methods 
based on GIZA++. In other words, the NCF me-
thod can effectively resolve the difficulties of 
extracting MWE translations discussed in Sec-
tion 1. 
In addition, the fourth row shows that the 
NCF+FCS+Dice system also outperforms the 
NCF+Dice system.  Thus, the FCS method can 
resolve the common subsequence problem effec-
tively. 
Table 8 shows the translation accuracy rates 
of each system. The NCF+FCS+Dice system 
achieves the best translation accuracy. Moreover, 
it significantly improves the performance of 
finding MWE translation equivalences. 
 
4 Applying  MWE Translations to MT 
To demonstrate the usefulness of extracted 
MWE translations to existing statistical machine 
translation systems, we use the XML markup 
scheme provided by the Moses decoder, which 
allows the specification of translations for parts 
of a sentence. The procedure for this experiment 
consists of three steps: (1) the extracted MWE 
translations are added to the test set with the 
XML markup scheme, (2) after which the data is 
input to the Moses decoder to complete the 
translation task, (3) the results are evaluated 
 Moses  MWE +Moses
NIST06-sub 23.12 23.49 
NIST06 21.57 21.79 
 Table 9. BLEU scores of the translation results. 
 
using the BLEU metric (Papineni et al, 2002). 
4.1 Experimental Settings 
To train a translation model for Moses, we use 
the Hong Kong Hansard and the Hong Kong 
News parallel corpora as training data 
(2,222,570 sentence pairs). We also use the 
same parallel corpora to extract translations of 
MWEs. The NIST 2008 evaluation data (1,357 
sentences, 4 references) is used as development 
set and NIST 2006 evaluation data (1,664 sen-
tences, 4 references) is used as test set. 
4.2 Selection of MWEs 
Due to the limitation of the XML markup 
scheme, we only consider two types of MWEs: 
continuous bigrams and idioms. Since the goal 
of this experiment is not focus on extraction of 
MWEs, simple methods are applied to extract 
MWEs from the training data: (1) we collect all 
continuous bigrams from Chinese sentences in 
the training data and then simply filter out the 
bigrams by mutual information (MI) with a thre-
shold1, (2) we also extract all idioms from Chi-
nese sentences of the training data by collecting 
all 4-syllables words from the training data and 
filtering out obvious non-idioms, such as deter-
minative-measure words and temporal words by 
their part-of-speeches, because most Chinese 
idioms are 4-syllables words.  
In total, 33,767 Chinese bigram types and 
20,997 Chinese idiom types were extracted from 
training data; and the top-5 translations of each 
MWE were extracted by the method described in 
Section 2. Meanwhile 1,171 Chinese MWEs 
were added to the translations in the test set. The 
Chinese words covered by the MWEs in test 
data set were 2,081 (5.3%). 
 
4.3 Extra Information 
When adding the translations to the test data, 
two extra types of information are required by 
the Moses decoder. The first type comprises the 
function words between the translation and its 
context. For example, if ??  ??/economic 
cooperation is added to the test data, possible  
                                                 
1 We set the threshold at 5. 
484
source sentence ... ????<MWE>????</MWE>????? ... 
Moses ... entered blinded by the colourful community ... 
MWE+Moses ... entered the colourful community ... 
reference ... entered the colorful society ... 
source sentence ... ?????  <MWE>???  ??</MWE> ??? ... 
Moses ... do not want to see an escalation of crisis ... 
MWE+Moses ... do not want to see a further escalation of crisis ... 
reference ... don 't want to see the further escalation of the crisis ... 
source sentence ... ????????<MWE>?????</MWE> ... 
Moses ... the people 's interests ... 
MWE+Moses ... the people of the fundamental interests ... 
reference ... the fundamental interests of the masses ... 
Table 10. Examples of improved translation quality with the MWE translation equivalences. 
 
function words, such as ?in? or ?with?, should be 
provided for the translation. Because the Moses 
decoder does not generate function words that 
are context dependent, it treats a function word 
as a part of the translation. Therefore, we collect 
possible function words for each translation 
from the corpora when the conditional probabili-
ty is larger than a threshold2. 
The second type of information is the phrase 
translation probability and lexical weighting. 
Computing the phrase translation probability is 
trivial in the training corpora, but lexical weight-
ing (Koehn et al, 2003) needs lexical-level 
alignment. For convenience, we assume that 
each word in an MWE links to each word in the 
translations. Under this assumption, the lexical 
weighting is simplified as follows:   
 
??
??= ?= aji ji
n
i
w efpajij
ap
),(1
)|(
|}),(|{|
1
),|( ef
        ? ?
= ??
?
n
i e
ji
j
efp
1
)|(
||
1
ee
. 
 
Then, it is trivial to compute the simplified lexi-
cal weighting of each MWE correspondence 
when the word translation probability table is 
provided. Here, we use the IBM model 1 to learn 
the table from the training data. 
4.4 Evaluation Results 
We trained a model using Moses toolkit (Koehn 
et al, 2007) on the training data as our baseline 
system.  
Table 9 shows the influence of adding the 
MWE translations to the test data. In the first 
row (NIST06-sub), we only consider sentences 
containing MWE translations for BLEU score 
evaluation (726 sentences). In the second row, 
we took the whole NIST 2006 evaluation set 
into consideration (1,664 sentences). The Chi-
nese words covered by the MWEs in NIST06-
sub and NIST06 were 9.9% and 5.3% respec-
tively. 
Adding MWE translations to the test data sta-
tistically significantly lead to better results than 
those of the baseline. Significance was tested 
using a paired bootstrap (Koehn, 2004) with 
1000 samples (p<0.02). Although the improve-
ment in BLEU score seems small, it is actually 
reasonably good given that the MWEs account 
for only 5% of the NIST06 test set. Examples of 
improved translations are shown in Table 10. 
There is still room for improvement of the pro-
posed MWE extraction method in order to pro-
vide more MWE translation pairs or design a 
feasible way to incorporate discontinuous bilin-
gual MWEs to the decoder. 
5 Conclusions and Future Work 
We have proposed a high precision algorithm for 
extracting translations of multiword expressions 
from parallel corpora. The algorithm can be used 
to translate any language pair and any type of 
word sequence, including rigid sequences and 
discontinuous sequences. Our evaluation results 
show that the algorithm can cope with the diffi-
culties caused by indirect association and the 
common subsequence effects, leading to signifi-
cant improvement over the word alignment-
based extraction methods used by the state of the 
art systems and other association-based extrac-
tion methods. We also demonstrate that ex-
tracted translations significantly improve the                                                  
2 We set the threshold at 0.1. 
485
performance of the Moses machine translation 
system. 
In future work, it would be interesting to de-
velop a machine translation model that can be 
integrated with the translation acquisition algo-
rithm in a more effective way. Using the norma-
lized-frequency score to help phrase alignment 
tasks, as the grow-diag-final heuristic, would 
also be interesting direction to explore. 
 
Acknowledgement 
This research was supported in part by the Na-
tional Science Council of Taiwan under the NSC 
Grants: NSC 96-2221-E-001-023-MY3. 
References  
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Parame-
ter Estimation. Computational Linguistics, 
19(2):263-311.  
Gao, Jianfeng, Jian-Yun Nie, Hongzhao He, Weijun 
Chen, Ming Zhou. 2002. Resolving Query Trans-
lation Ambiguity using a Decaying Co-occurrence 
Model and Syntactic Dependence Relations. In 
Proc. of SIGIR?02. pp. 183 -190. 
Kitamura, Mihoko and Yuji Matsumoto. 1996. Au-
tomatic Extraction of Word Sequence Correspon-
dences in Parallel Corpora. In Proc. of the 4th An-
nual Workshop on Very Large Corpora. pp. 79-87. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Proc. 
of HLT/NAACL?03. pp. 127-133. 
Koehn, Philipp. 2004. Statistical significance tests for 
machine translation evaluation. In Proc. 
EMNLP?04. pp. 388-395. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses: 
Open source toolkit for statistical machine transla-
tion. In ACL?07, demonstration session. 
Kupiec, Julian. 1993. An Algorithm for Finding 
Noun Phrase Correspondences in Bilingual Corpo-
ra. In Proc. of ACL?93 . pp. 17-22. 
Liang, Percy, Ben Taskar, Dan Klein. 2006. Align-
ment by Agreement. In Proc. of HLT/NAACL?06. 
pp. 104-111. 
Ma, Yanjun, Nicolas Stroppa, Andy Way. 2007. 
Bootstrapping Word Alignment via Word Packing. 
In Proc. of ACL?07. pp. 304-311. 
Ma, Yanjun, Sylwia Ozdowska, Yanli Sun, and Andy 
Way. 2008. Improving Word Alignment Using 
Syntactic Dependencies. In Proc. of ACL/HLT?08 
Second Workshop on Syntax and Structure in Sta-
tistical Translation. pp. 69-77. 
Melamed, Ilya Dan. 2001. Empirical Methods for 
Exploiting parallel Texts. MIT press. 
Och, Franz Josef and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proc. of ACL?00. 
pp. 440-447. 
Och, Franz Josef, Christoph Tillmann, and Hermann 
Ney. 1999. Improved Alignment Models for Sta-
tistical Machine Translation. In Proc. of 
EMNLP/VLC?99. pp. 20-28. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Proc. 
of ACL?02. pp. 311-318. 
Smadja, Frank, Kathleen R. McKeown, and Vasileios 
Hatzivassiloglou. 1996. Translating Collocations 
for Bilingual Lexicons: A Statistical Approach. 
Computational Linguistics, 22(1):1-38. 
Wu, Dekai. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-403.  
Wu, Hua, Ming Zhou. 2003. Synonymous Colloca-
tion Extraction Using Translation Information. In 
Proc. of ACL?03. pp. 120-127. 
Yamamoto, Kaoru, Yuji Matsumoto. 2000. Acquisi-
tion of Phrase-level Bilingual Correspondence us-
ing Dependency Structure. In Proc. of COL-
ING?00. pp. 933-939. 
 
486
Automatic Semantic Role Assignment for a Tree Structure 
 
Jia-Ming You 
Institute of Information Science 
Academia Sinica 
swimming@hp.iis.sinica.edu.tw 
Keh-Jiann Chen 
Institute of Information Science 
Academia Sinica 
Kchen@iis.sinica.edu.tw 
 
Abstract 
  We present an automatic semantic roles labeling 
system for structured trees of Chinese sentences. It 
adopts dependency decision making and 
example-based approaches. The training data and 
extracted examples are from the Sinica Treebank, 
which is a Chinese Treebank with semantic role 
assigned for each constituent. It used 74 abstract 
semantic roles including thematic roles, such as 
?agent?; ?theme?, ?instrument?, and secondary roles of 
?location?, ?time?, ?manner? and roles for nominal 
modifiers. The design of role assignment algorithm is 
based on the different decision features, such as 
head-argument/modifier, case makers, sentence 
structures etc. It labels semantic roles of parsed 
sentences. Therefore the practical performance of the 
system depends on a good parser which labels the 
right structures of sentences. The system achieves 
92.71% accuracy in labeling the semantic roles for 
pre-structure- bracketed texts which is considerably 
higher than the simple method using probabilistic 
model of head-modifier relations. 
1. Introduction  
  For natural language understanding, the process of 
fine-grain semantic role assignment is one of the 
prominent steps, which provides semantic relations 
between constituents. The sense and sense relations 
between constituents are core meaning of a sentence. 
  Conventionally there are two kinds of methods 
for role assignments, one is using only statistical 
information (Gildea and Jurafsky, 2002) and the 
other is combining with grammar rules (Gildea and 
Hockenmaier, 2003). However using only grammar 
rules to assign semantic roles could lead to low 
coverage. On the other hand, performance of 
statistical methods relies on significant dependent 
features. Data driven is a suitable strategy for 
semantic roles assignments of general texts. We use 
the Sinica Treebank as information resource because 
of its various domains texts including politics, 
society, literature?etc and it is a Chinese Treebank 
with semantic role assigned for each constituent 
(Chen etc., 2003). It used 74 abstract semantic roles 
including thematic roles, such as ?agent?; ?theme?, 
?instrument?, and secondary roles of ?location?, 
?time?, ?manner? and modifiers of nouns, such as  
 
?quantifier?, ?predication?, ?possessor?, etc. The 
design of role assignment algorithm is based on 
the different decision features, such as 
head-argument/modifier, case makers, sentence 
structures etc. It labels semantic roles of parsed 
sentences by example-based probabilistic models. 
 
1.1 Sinica Treebank 
  The Sinica Treebank has been developed and 
released to public since 2000 by Chinese 
Knowledge Information Processing (CKIP) group 
at Academia Sinica. The Sinica Treebank version 
2.0 contains 38944 structural trees and 240,979 
words in Chinese. Each structural tree is annotated 
with words, part-of-speech of words, syntactic 
structure brackets, and semantic roles. For 
conventional structural trees, only syntactic 
information was annotated. However, it is very 
important and yet difficult for Chinese to identify 
word relations with purely syntactic constraints 
(Xia et al, 2000). Thus, partial semantic 
information, i.e. semantic role for each constituent, 
was annotated in Chinese structural trees. The 
grammatical constraints are expressed in terms of 
linear order of semantic roles and their syntactic 
and semantic restrictions. Below is an example 
sentence of the Sinica Treebank.  
 
Original sentence:  
? ?Ta?? ?yao? ?? ?ZhangSan?? ?jian? ? 
?qiu?? 
He ask Zhang San to pick up the ball. 
 
Parsed tree: 
S(agent:NP(Head:Nhaa:??He?)|Head:VF2:??ask? 
|goal:NP(Head:Nba:???Zhang San?) 
|theme:VP(Head:VC2:??pick?|goal:NP(Head:Nab:?
'?ball?))) 
Figure 1: An example sentence of Sinica Treebank 
  In the Sinica Treebank, not only the semantic 
relations of a verbal predicate but also the 
modifier head relations were marked. There are 74 
different semantic roles, i.e. the task of semantic 
role assignment has to establish the semantic 
relations among phrasal heads and their 
arguments/modifiers within 74 different choices. 
The set of semantic roles used in the Sinica Treebank 
is listed in the appendix.  
 
2.  Example-based Probabilistic Models for 
Assigning Semantic Roles 
  The idea of example-based approaches is that 
semantic roles are preserved for the same event 
frames. For a target sentence, if we can find same 
examples in the training corpus, we can assign the 
same semantic role for each constituent of the target 
sentence as the examples. However reoccurrence of 
exact same surface structures for a sentence is very 
rare, i.e. the probability of finding same example 
sentences in a corpus is very low. In fact, by 
observing structures of parsed trees, we find that 
most of semantic roles are uniquely determined by 
semantic relations between phrasal heads and their 
arguments/modifiers and semantic relations are 
determined by syntactic category, semantic class of 
related words. For example:  
 
Original sentence:  
?? ?wo men? ? ?du? ?? ?xi huan? ?? ?hu 
die?? 
 
We all like butterflies.  
 
Parsed tree:  
S(experiencer:NP(Head:Nhaa:?? 
?we? )|quantity:Dab:? ?all? |Head:VK1:?? ?like? 
|goal:NP(Head:Nab:?? ?butterflies?))? 
Figure 2: The illustration of the parsed tree.  
 
  In Figure2, ?? ?like? is the sentential head; ?
? ?we? and ?? ?butterflies? are the arguments; 
? ?all? is the modifier. As a result, the semantic role 
?experiencer? of ??  ?we? is deduced from the 
relation between ?? ?we? and?? ?like?, since 
the event frame of?? ?like? has the two arguments 
of experiencer and goal and the experiencer usually 
takes the subject position. The semantic roles of ?
? ?butterflies? and ? ?all? are assigned by the 
same way. For the task of automatic role 
assignment, once phrase boundaries and phrasal 
head are known, the semantic relations will be 
resolved by looking for similar 
head-argument/modifier pairs in training data. 
 
2.1  Example Exaction  
  To extract head-argument/modifier examples 
from the Sinica Treebank is trivial, since phrase 
boundaries and semantic roles, including phrasal 
head, are labeled. The extracted examples are 
pairs of head word and target word. The target 
word is represented by the head of the 
argument/modifier, since the semantic relations 
are established between the phrasal head and the 
head of argument/modifier phrase. An extracted 
word pair includes the following features. 
 
Target word:  
  The head word of argument/modifier.  
 
Target POS:  
  The part-of-speech of the target word. 
 
Target semantic role:  
  Semantic role of the constituent contains the 
target word as phrasal head. 
 
Head word:  
  The phrasal head. 
 
Head POS:  
  The part-of-speech of the head word. 
 
Phrase type:  
  The phrase which contains the head word and 
the constituent containing target word. 
 
Position:  
  Shows whether target word appears before or 
after head word. 
 
The examples we extracted from Figure 2 are 
listed below.  
 
Table 1: The three head-argument/modifier pairs 
extracted from Figure 2. 
 
 
S
goal
NP
experiencer
NP
Head
VK1
quantity
Dab
Head
Nhaa
Head
Nab
??
W e
?
all
??
like
??
Butterflies.
 
 
 
 
Table 2: Coverage and accuracy of different features combinations 
 
2.2 Probabilistic Model for Semantic Role 
Assignment
  It is possible that conflicting examples (or 
ambiguous role assignments) occur in the training 
data. We like to assign the most probable roles. The 
probability of each semantic role in a constituent 
with different features combinations are estimated 
from extract examples. 
 
 
position) pt,  t_pos, t,h_pos, h,(#
position) pt,  t_pos,, t h_pos, h,(r, #
) position pt,t_pos, t,h_pos, h, |r (
)|(
=
= P
tconstituenrP
1 
 
  Due to the sparseness of the training data, it?s not 
possible to have example feature combinations 
matched all input cases. Therefore the similar 
examples will be matched. A back off process will 
be carried out to reduce feature constraints during 
the example matching. We will evaluate 
performances for various features combinations to 
see which features combinations are best suited for 
semantic roles assignments. 
  We choose four different feature combinations. 
Each has relatively high accuracy. The four 
classifiers will be back off in sequence. If none of 
the four classifiers is applicable, a baseline model 
of assigning the most common semantic role of 
target word is applied. 
 
if # of (h,h_pos,t,t_pos,pt,position) > threshold  
P(r|constituent)=P(r|h,h_pos,t,t_pos,pt,position) 
 
Else 
if # of (h_pos,t,t_pos,pt,position) > threshold 
P(r|constituent)=P(r|h_pos,t,t_pos,pt,position) 
 
Else  
                                                 
1  r: semantic role; h: the head word; 
   h_pos: part-of-speech of head word;  
   t: the target word;  
   t_pos: part-of-speech of target word; 
   pt: the phrase type. 
if # of (h,h_pos,t_pos,pt,position) > threshold  
P(r|constituent)=P(r|h,h_pos,t_pos,pt,position) 
 
Else 
if # of (h_pos,t_pos,pt,position) > threshold 
P(r|constituent)=P(r|h_pos,t_pos,pt,position) 
 
Else 
Baseline model: 
P(r|constituent)=P(r| t, t_pos,pt) 
 
3. Experiments  
  We adopt the Sinica Treebank as both training 
and testing data. It contains about 40,000 parsed 
sentences. We use 35,000 sentences as training data 
and the rest 5,000 as testing data. The table 2 shows 
the coverage of each classifier, their accuracies, and 
performance of each individual classifier without 
back off process. The table 3 shows combined 
performance of the four classifiers after back off 
processes in sequence. The baseline algorithm is the 
simple unigram approach to assign the most 
common role for the target word. Because the 
accuracy of the four classifiers is considerably high, 
instead of using linear probability combinations we 
will rather use the most reliable classifier for each 
different features combination. 
 
 
 
 
 
 
Table 3: The accuracy of our backoff method and 
the base line (the most common semantic roles) 
 
3.1 Error Analyses 
  Although the accuracy of back off model is 
relatively high to the baseline model, it still has 
quite a room for improvement. After analyzed the 
errors, we draw following conclusions. 
 
Method Accuracy 
Backoff 90.29% 
Baseline:  68.68% 
a) Semantic head vs. syntactic head 
 
  A semantic role for a prepositional phrase (PP) is 
mainly determined by the syntactic head of PP, i.e. 
preposition, and the semantic head of PP, i.e. the 
head word of the DUMMY-argument of PP. For 
example, in Figure 3, the two sentences are almost 
the same, only the contents of PP are different. 
Obviously, the semantic roles of PP (? ?in? ?? 
?Indonesia?) is location, and the semantic role of PP 
(? ?in? ?? ?this year?) is time. Therefore the 
semantic roles of the two PPs should be determined 
only within the scope of PP and not relevant to 
matrix verb.  
 
 
S
Head
VC31
agent
NP
Head
Nca
Head
P21
Head
Nca
manner
Dh
?? ? ?? ?? ??
Taipei  speed-up  the  investments   in  Indonesia.
Dummy
NP
location
PP
S
Head
VC31
agent
NP
Head
Nca
Head
P21
Head
Nca
manner
Dh
?? ? ?? ?? ??
Taipei  speed-up   the  investments   this year.
Dummy
NP
time
PP
 
Figure 3: Parsed trees of ??????????? 
and ??????????? 
 
b) Structure-dependent semantic roles assignments 
 
  Complex structures are always the hardest part of 
semantic roles assignments. For example, the 
sentences with passive voice are the typical 
complex structures. In Figure 4, the semantic role 
of ?? ?Butterflies? is not solely determined by 
the head verb ?? ?attracted? and itself. Instead 
we should inspect the existence of passive voice 
and then reverse the roles of subject and object. 
 
 
 
 
Figure 4: A parsed tree of passive sentence ????
?????? 
  
4 Refined Models 
  Chen & Huang (1996) had studied the task of 
semantic assignment during Chinese sentence 
parsing. They concluded that semantic roles are 
determined by the following 4 parameters. 
 
1. Syntactic and semantic categories of the target 
word, 
2. Case markers, i.e. prepositions and 
postpositions 
3. Phrasal head, and 
4. Sub-categorization frame and its syntactic 
patterns. 
   
  Therefore head-modifier/argument examples 
only resolve most of semantic role assignments. 
Some of complex cases need other parameters to 
determine their semantic roles. For instance, the 
argument roles of Bei sentences (passive sentence) 
should be determined by all four parameters.   
  The refined model contain two parts, one is the 
refinements of features data which provide more 
precisely information and the other is the 
improvements of back off process to deal with 
special semantic roles assignments. 
 
4.1 Refinement of Features Extractions 
  The refinements of features extractions focus on 
two different cases, one is the features extractions 
of case-marked structures, such as PP and GP 
(postpositional phrases), and the other is the general 
semantic class identifications of synonyms.  
  The features of PP/GP include two different 
S
Head
VJ1
goal
NP
Head
P02
evaluation
Dbb
Head
Nab
Head
Nad
?? ? ? ?? ??
Butterflies   are   also   be   attracted   by   the voice.
Dummy
NP
theme
PP
feature types: the internal and the external features. 
The internal features of phrases compose of phrasal 
head and Dummy-head; the external features are 
heads (main verbs) of the target phrases. 
 
 
Figure 5: A parsed tree for demonstrating features 
extractions of PP 
 
Table 4: The internal/external relations of Figure 5.  
 
  The semantic class identifications of synonyms 
are crucial for solving data sparseness problems. 
Some type of  words are very productive, such as 
numbers, DM (determinative measurement), proper 
names. They need to be classified into different 
semantic classes. We use some tricks to classify 
them into specific word classes. For example we 
label  1 ??  ?one kilogram?, 2 ??  ?two 
kilograms? as their canonical form ???  ?n 
kilograms?; ??? ?the first day?, ??? ?the 
second day ? as ???  ?the nth days?;  ?
?  ?Zhang San?, ??  ?Li Si? as a personal 
name?etc. With this method, we can increase the 
number of matched examples and resolve the 
problem of occurrences of unknown words in a 
large scale.   
 
4.2  Dependency Decisions and Refined Back 
off Processes 
  The refined back off model aimed to solve 
semantic roles assignments for certain special 
structures. Using only head-modifier features could 
result into decision making with insufficient 
information. As illustrated before, the semantic role 
of ?? ?butterflies? in Figure 4 is ?agent? observed 
from the head-argument feature.  But in fact the 
feature of passive voice ? ?passive? tells us that 
the subject role of ?? ?butterflies? should be the 
semantic role ?goal? instead of the usual role of 
?agent?. 
  Therefore we enhanced our back off process by 
adding some dependency decisions. The 
dependency conditions include special grammar 
usage like passive form, quotation, topical 
sentences? etc. In the refined back off process, 
first we have to detect which dependency condition 
is happened and resolved it by using dependency 
features. For example, if the feature word ? 
?passive? occurs in a sentence, we realize that the 
subjective priority of semantic roles should be 
reversed. For instance, ?goal? will take subject 
position instead of ?agent? (?goal? appears before 
?agent?). 
4.3 Experiment Results 
  The experiments were carried out for the refined 
back off model with the same set of training data 
and testing data as in the previous experiments. 
Table 5 shows that the refined back off model gains 
2.4 % accuracy rate than the original back off 
model. However most of the improvement is due to 
the refinements of features extractions and 
canonical representation for certain classes of words. 
A few improvements were contributed to the 
decision making on the cases of structure 
dependency.   
 
 
Method Accuracy 
Refined Backoff  92.71% 
Backoff 90.29% 
Baseline 68.68% 
Table 5: Role assignment accuracies of refined 
backoff, backoff, and baseline models. 
 
5  Conclusion and Future Works 
  Semantic roles are determined by the following 4 
parameters. 
 
1. Syntactic and semantic categories of the target 
word, 
2. Case markers, i.e. prepositions and 
postpositions, 
3. Phrasal head, and 
4. Sub-categorization frame and its syntactic 
patterns. 
 
S
Head
VC31
agent
NP
Head
Nca
Head
P21
Head
Nca
manner
Dh
?? ? ?? ?? ??
Taipei  speed-up  the  investments   in  Indonesia.
Dummy
NP
location
PP
  We present an automatic semantic roles labeling 
system. It adopts dependency decision making and 
example-based approaches, which makes decision 
on the amount of parameters by observing the 
occurrence of dependency features and to utilize the 
minimal amount of feature combinations to assign 
semantic roles. It labels semantic roles of parsed 
sentences. Therefore the practical performance of 
the system depends on a good parser which labels 
the right structures of sentences. The system 
achieves 92.71% accuracy in labeling the semantic 
roles for pre-structure- bracketed texts which is 
considerably higher than the simple method using 
probabilistic model of head-modifier relations. 
In the future, we will consider fine-grain semantic 
role assignment problems. The current semantic 
roles assignment is focus on one sentence. However, 
the occurrences of frame elements are not limited to 
a single sentence. For instance, ?John bought the 
books from Mary?. The semantic roles of ?John? 
and ?Mary? are agent and theme respectively. 
According to Fillmore?s FrameNet, the frame 
element assignment for the above sentence should 
be ?John? the buyer, ?Mary? the seller, ?the books? 
the goods. The precondition of buy-frame says that 
the seller should be the owner of the goods. 
Therefore after the sentence parsing and logical 
reasoning, the following semantic relations should 
be established. 
 
Event frame: Commerce-buy 
Buyer: John 
Seller: Mary 
Goods: books 
Additional frame: Own 
 
Before the buy event 
Owner: Mary 
Possession: books 
After the buy event 
Owner: John 
Possession: books 
 
  The semantic roles assignment is a process of 
crossing phrasal and sentential boundaries. Some 
semantic roles of an event might occur at left or 
right context. Therefore we have to analyze the 
relation between two consecutive events. The 
relations include causal relation, temporal relation, 
resultant relation, etc. How to resolve the above 
problems will be our future studies. 
 
References 
 
Chen, Keh-Jiann, Chu-Ren Huang. 1996.  
Information-based Case Grammar: A 
Unification-based Formalism for Parsing Chinese. 
Journal of Chinese Linguistics Monograph Series 
No. 9.  
 
Chen, Keh-Jiann, Chu-Ren Huang, Feng-Yi Chen, 
Chi-Ching Luo,Ming-Chung Chang, Chao-Jan Chen, 
and Zhao-Ming Gao, 2003. Sinica Treebank: Design 
Criteria, Representational Issues and 
Implementation. In Anne Abeille (Ed.) Treebanks 
Building and Using Parsed Corpora. Language and 
Speech series. Dordrecht:Kluwer, pp231-248. 
Chu-Ren Huang, Keh-Jiann Chen, and Benjamin K. 
T?sou Eds. Readings in Chinese Natural Language 
Processing. 23-45. Berkeley: JCL. 
 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics, 28(3):245-288 
 
Daniel Gildea and Julia Hockenmaier. 2003. Identifying 
Semantic Roles Using Combinatory Categorial 
Grammar. Conference on Empirical Methods in 
Natural Language Processing (EMNLP). 
 
Xia, Fei, 2000, The Part-of-Speech Tagging Guidelines 
for the Penn Chinese Treebank (3.0). IRCS Report 
00-07. Philadelphia, PA: University of Pennsylvania. 
Appendix: 
 
  Figure 6: The detail classification of semantic roles in the Sinica Treebank 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving Context Vector Models by Feature Clustering for Auto-
matic Thesaurus Construction 
 
Jia-Ming You 
Institute of Information Science 
Academia Sinica 
swimming@hp.iis.sinica.edu.tw  
Keh-Jiann Chen 
Institute of Information Science 
Academia Sinica 
kchen@iis.sinica.edu.tw  
Abstract 
Thesauruses are useful resources for NLP; 
however, manual construction of thesau-
rus is time consuming and suffers low 
coverage. Automatic thesaurus construc-
tion is developed to solve the problem. 
Conventional way to automatically con-
struct thesaurus is by finding similar 
words based on context vector models 
and then organizing similar words into 
thesaurus structure. But the context vec-
tor methods suffer from the problems of 
vast feature dimensions and data sparse-
ness. Latent Semantic Index (LSI) was 
commonly used to overcome the prob-
lems. In this paper, we propose a feature 
clustering method to overcome the same 
problems. The experimental results show 
that it performs better than the LSI mod-
els and do enhance contextual informa-
tion for infrequent words. 
1 Introduction 
Thesaurus is one of the most useful linguistic 
resources. It provides information more than just 
synonyms. For example, in WordNet (Fellbaum, 
1998), it also builds up relations between syno-
nym sets, such as hyponym, hypernym. There are 
two Chinese thesauruses Cilin(1983) and 
Hownet1. Cilin provides synonym sets with sim-
ple hierarchical structure. Hownet uses some 
primitive senses to describe word meanings. The 
common primitive senses provide additional re-
lations between words implicitly. However, 
many words occurred in contemporary news cor-
pora are not covered by Chinese thesauruses.  
 
                                                 
1 http://www.HowNet.com(Dong Zhendong, Dong 
Qiang:HowNet) 
 
Therefore, we intend to create a thesaurus 
based on contemporary news corpora. The com-
mon steps to automatically construct a thesaurus 
include a) contextual information extraction, b) 
finding synonym words and c) organizing syno-
nym words into a thesaurus. The approach is 
based upon the fact that word meaning lays on its 
contextual behavior. If words act similarly in 
context, they may share the same meaning.  
However, the method can only handle frequent 
words rather than infrequent ones. In fact most of 
vocabularies occur infrequently, one has to dis-
cover extend information to overcome the data 
sparseness problem. We will introduce the con-
ventional approaches for automatic thesaurus 
construction in section 2. Follow a discussion 
about the problems and solutions of context vec-
tor models in section 3.  In section 4, we use two 
performance evaluation metrics, i.e. discrimina-
tion and nonlinear interpolated precision, to 
evaluate our proposed method. 
2 Conventional approaches for auto-
matic thesaurus construction  
The conventional approaches for automatic the-
saurus construction include three steps: (1) Ac-
quire contextual behaviors of words from cor-
pora. (2) Calculate the similarity between words. 
(3) Finding similar words and then organizing 
into a thesaurus structure.  
2.1 Acquire word sense knowledge 
One can model word meanings by their co-
occurrence context. The common ways to extract 
co-occurrence contextual words include simple 
window based and syntactic dependent based 
(You, 2004). Obviously, syntactic dependent 
relations carry more accurate information than 
window based. Also, it can bring additional in-
formation, such as POS (part of speech) and se-
mantic roles etc. To extract the syntactic de-
1
;1 )(log)()rdentropy(wo ?= -=
m
k
i
kwordp
i
kwordpi
pended relation, a raw text has to be segmented, 
POS tagged, and parsed. Then the relation ex-
tractor identifies the head-modifier relations 
and/or head-argument relations. Each relation 
could be defined as a triple (w, r, c), where w is 
the thesaurus term, c is the co-occurred context 
word and r is the relation between w and c.  
Then context vector of a word is represented 
differently by different models, such as: tf, 
weight-tf, Latent Semantic Indexing (LSI) 
(Deerwester, S.,et al, 1990) and Probabilistic 
LSI (Hofmann, 1999). The context vectors of 
word x can be express by:     
 
a) tf model: word x = }tf...,,2tf,1{tf xnxx ,where xitf is 
the term frequency of the ith context word when 
given word x.  
 
b) weight-tf model: assume there are n contex-
tual words and m target words. word x= 
 
,where weighti,  we used here, is defined as  
[logm-entropy(wordi)]/logm 
   
)(wordikp      
is the co-occurrence probability of wordk when 
given wordi. 
 
c) LSI or PLSI models: using tf or weighted-tf 
co-occurrence matrix and by adopting LSI or 
PLSI to reduce the dimension of the matrix. 
 
2.2 Similarity between words  
The common similarity functions include  
a) Adopting simple frequency feature, such as 
cosine, which computes the angle between two 
context vectors;  
 
b) Represent words by the probabilistic distribu-
tion among contexts, such as Kull-Leiber diver-
gence (Cover and Thomas, 1991). 
The first step is to convert the co-occurrence 
matrix into a probabilistic matrix by simple for-
mula.  
? =
==
? =
===
= n 1k yktf
y
itfxiq},
x
n,...q
x
2q,
x
1{q wordy 
n
1k
x
ktf
x
itfxip},
x
n,...p
x
2p,
x
1{pwordx
 q
p
 
 
Then calculate the distance between probabil-
istic vectors by sums up the all probabilistic dif-
ference among each context word so called cross 
entropy. 
 
Due to the original KL distance is asymmetric 
and is not defined when zero frequency occurs. 
Some enhanced KL models were developed to 
prevent these problems such as Jensen-Shannon 
(Jianhua, 1991), which introducing a probabilis-
tic variable m, or ? -Skew Divergence (Lee, 
1999), by adopting adjustable variable ?. Re-
search shows that Skew Divergence achieves 
better performance than other measures. (Lee, 
2001) 
 
))1(||(yxS  rgence)D(SkewDive yxxKL aaa -+==  
 
2/)(                                  
,2/)}||()||({y)x,(JS Shannon)-D(Jensen
yxm
myKLmxKL
+=
+==
 
To convert distance to similarity value, we 
adopt the formula inspired by Mochihashi, and 
Matsumoto 2002.   
 
 
2.3 Organize similar words into thesaurus  
 
There are several clustering methods can be used 
to cluster similar words. For example, by select-
ing N target words as the entries of a thesaurus, 
then extract top-n similar words for each entry; 
adopting HAC(Hierarchical agglomerative clus-
tering, E.M. Voorhees,1986) method to cluster 
the most similar word pairs in each clustering 
loop. Eventually, these similar words will be 
formed into synonyms sets.  
 
3 Difficulties and Solutions 
There are two difficulties of using context vector 
models. One is the enormous dimensions of con-
}weight...tf2weight2tf,1weight1{tf n
xxx
n ???
yx
xy)cos(x,
?
?= y
)},distance(exp{wordy)(wordx,similarity yx?-= l
i
i
21 q
plog)(q)KL(p, :Distance KL ??
=
= ipn
i
2
textual words, and the other is data sparseness 
problem. Conventionally LSI or PLSI methods 
are used to reduce feature dimensions by map-
ping literal words into latent semantic classes. 
The researches show that it?s a promising 
method (April Kontostathis, 2003). However the 
latent semantic classes also smooth the informa-
tion content of feature vectors. Here we proposed 
a different approach to cope with the feature re-
duction and data sparseness problems. 
 
3.1 Feature Clustering 
Reduced feature dimensions and data sparseness 
cause the problem of inaccurate contextual in-
formation. In general, one has to reduce the fea-
ture dimensions for computational feasibility and 
also to extend the contextual word information to 
overcome the problem of insufficient context 
information. 
In our experiments, we took the clustered-
feature approaches instead of LSI to cope with 
these two problems and showed better perform-
ances. The idea of clustered-feature approaches 
is by adopting the classes of clustering result of 
the frequent words as the new set of features 
which has less feature dimensions and context 
words are naturally extend to their class mem-
bers. We followed the steps described in section 
2 to develop the synonyms sets. First, the syntac-
tic dependent relations were extracted to create 
the context vectors for each word. We adopted 
the skew divergence as the similarity function, 
which is reported to be the suitable similarity 
function (Masato, 2005), to measure the distance 
between words.  
 
We used HAC algorithm to develop the syno-
nyms classes, which is a greedy method, simply 
to cluster the most similar word pairs at each 
clustering iteration.  
 
The HAC clustering process: 
 
While  the similarity of the most similar word pair 
(wordx, wordy) is greater than a threshold ? 
 
then cluster wordx, wordy together and replace it with 
the centroid   between wordx and wordy 
 
Recalculate the similarity between other words and 
the  centroid   
 
3.2 Clustered-Feature Vectors 
We obtain the synonyms sets S from above HAC 
method. Let the extracted synonyms sets S = { S1, 
S2,?SR} which contains R synonym classes; 
i
jS stands for the jth element of the ith synonym 
class;  the ith synonym class Si contains Qi ele-
ments.  
 
 
The feature extension processing transforms 
the coordination from literal words to synonyms 
sets. Assume there are N contextual words 
{C1,C2,?CN}, and the first step is to transform 
the context vector of of Ci to the distribution vec-
tor among S. Then the new feature vector is the 
summation of the distribution vectors among S 
of its all contextual words. 
 
The new feature vector of wordj = 
?= ?
N
i 1
 jitf Distribution_Vector_among_S( iC ) 
,where jitf  is the term frequency of the context 
word Ci occurs with wordj.  
Distribution_Vector_among_S( iC )= { }RSiPSiPSiP ,..., 21 ,
.S synonyms  at the  of rdscontext wo
 ofon distributi  themeans,(Ci)
1
),(
  where,
jjthCi
freq
Qj
q
CijqSfreqS
iP
j
?
==
 
Due to the transformed coordination no longer 
stands for either frequency or probability, we use 
simple cosine function to measure the similarity 
between these transformed clustered-feature vec-
tors.  
4 Evaluation  
To evaluate the performance of the feature clus-
tering method, we had prepared two sets of test-
ing data with high and low frequency words re-
spectively. We want to see the effects of feature 
reduction and feature extension for both frequent 
and infrequent words. 
 
??
??
?
?
?
??
??
?
?
?
=
R
QR
RR
Q
Q
SSS
SSS
SSS
...
............
...
...
 S 
21
2
2
2
2
2
1
1
1
1
2
1
1
3
4.1 Discrimination Rates  
The discrimination rate is used to examine the 
capability of distinguishing the correlation be-
tween words. Given a word pair (wordi,wordj), 
one has to decide whether the word pair is simi-
lar or not. Therefore, we will arrange two differ-
ent word pair sets, related and unrelated, to esti-
mate the discrimination. By given the formula 
below  
 
 
,where Na and Nb are respectively the numbers 
of synonym word pairs and unrelated word pairs. 
As well as, na and nb are the numbers of correct 
labeled pairs in synonyms and unrelated words.   
4.2 Nonlinear interpolated precision  
 
The Nap evaluation is used to measure the per-
formance of restoring words to taxonomy, a 
similar task of restoring words in WordNet 
(Dominic Widdows, 2003).  
The way we adopted Nap evaluation is to re-
construct a partial Chinese synonym set, and 
measure the structure resemblance between 
original synonyms and the reconstructed one. By 
doing so, one has to prepare certain number of 
synonyms sets from Chinese taxonomy, and try 
to reclassify these words.  
Assume there are n testing words distributed 
in R synonyms sets.  Let i1R stands for the repre-
sented word of the ith synonyms set. Then we 
will compute the similarity ranking between each 
represented word and the rest n-1 testing words. 
By given formula  
 
i
jS  represents the jth similar word of i1R  among 
the rest n-1 words 
 
??
?
??
?= 0
  synonym are R and  S if,1 i1iji
jZ  
 
The NAP value means how many percent 
synonyms can be identified. The maximum value 
of NAP is 1, means the extracted similar words 
are exactly match to the synonyms.  
5 Experiments 
The context vectors were derived from a 10 
year news corpus from The Central News 
Agency. It contains nearly 33 million sentences, 
234 million word tokens, and we extracted 186 
million syntactic relations from this corpus. Due 
to the low reliability of infrequent data, only the 
relation triples (w, r, c), which occurs more than 
3 times and POS of w and c must be noun or 
verb, are used. It results that nearly 30,000 high 
frequent nouns and verbs are used as the contex-
tual features. And with feature clustering2, the 
contextual dimensions were reduced from 30,988 
literal words to 12,032 semantic classes. 
In selecting testing data, we consider the 
words that occur more than 200 times as high 
frequent words and the frequencies range from 
40 to 200 as low frequent words.    
 
Discrimination  
 
For the discrimination experiments, we randomly 
extract high frequent word pairs which include 
500 synonym pairs and 500 unrelated word pairs 
from Cilin (Mei et. al, 1983). At the mean time, 
we also prepare equivalent low frequency data.  
We use a mathematical technique Singular 
Value Decomposition (SVD) to derive principal 
components and to implement LSI models with 
respect to different feature dimensions from 100 
to 1000. We compare the performances of differ-
ent models. The results are shown in the follow-
ing figures. 
 
Figure1.  Discrimination for high frequent words 
 
The result shows that for the high frequent 
data, although the feature clustering method did 
not achieve the best performance, it perform-
ances better at related data and a balanced per-
formance at unrelated data. The tradeoffs be-
                                                 
2 Some feature clustering results are listed in the Ap-
pendix  
??
???
? += Nb
nb
Na
na
2
1ratetion Discrimina
,1R
1NAP
1
1
1n
1j
R
1i
???
?
???
? += ??? -
=
-
==
j
k
i
k
i
j Zj
Z
4
tween related recalls and unrelated recalls are 
clearly shown. Another observation is that no 
matter of using LSI or literal word features (tf or 
weight_tf), the performances are comparable. 
Therefore, we could simply use any method to 
handle the high frequent words.  
 
Figure2 Discrimination for low frequent word 
 
For the infrequent words experiments, neither 
LSI nor weighted-tf performs well due to insuffi-
cient contextual information. But by introducing 
feature clustering method, one can gain more 6% 
accuracy for the related data. It shows feature 
clustering method could help gather more infor-
mation for the infrequent words.  
 
Nonlinear interpolated precision 
 
For the Nap evaluation, we prepared two testing 
data from Cilin and Hownet. In the high frequent 
words experiments, we extract 1311 words 
within 352 synonyms sets from Cilin and 2981 
words within 570 synonyms sets from Hownet.  
 
Figure 3. Nap performance for high frequent words 
 
In high frequent experiments, the results show 
that the models retaining literal form perform 
better than dimension reduction methods. It 
means in the task of measuring similarity of high 
frequent words using literal contextual feature 
vectors is more precise than using dimension 
reduction feature vectors. 
In the infrequent words experiments, we can 
only extract 202 words distributed in 62 syno-
nyms sets from Cilin and 1089 words within 222 
synonyms sets. Due to fewer testing words, LSI 
was not applied in this experiment. 
 
Figure 4. Nap performance for low frequent words 
 
It shows with insufficient contextual informa-
tion, the feature clustering method could not help 
in recalling synonyms because of dimensional 
reduction.  
 
6. Error Analysis and Conclusion  
 
Using context vector models to construct thesau-
rus suffers from the problems of large feature 
dimensions and data sparseness. We propose a 
feature clustering method to overcome the prob-
lems. The experimental results show that it per-
forms better than the LSI models in distinguish-
ing related/unrelated pairs for the infrequent data, 
and also achieve relevant scores on other evalua-
tions.  
Feature clustering method could raise the abil-
ity of discrimination, but not robust enough to 
improve the performance in extracting synonyms. 
It also reveals the truth that it?s easy to distin-
guish whether a pair is related or unrelated once 
the word pair shares the same sense in their 
senses. However, it?s not the case when seeking 
synonyms. One has to discriminate each sense 
for each word first and then compute the similar-
ity between these senses to achieve synonyms.  
Because feature clustering method lacks the abil-
ity of senses discrimination of a word, the 
method can handle the task of distinguishing cor-
relation pairs rather than synonyms identification. 
 
Also, after analyzing discrimination errors 
made by context vector models, we found that 
some errors are not due to insufficient contextual 
information. Certain synonyms have dissimilar 
contextual contents for different reasons. We 
observed some phenomenon of these cases:  
 
5
a) Some senses of synonyms in testing data are 
not their dominant senses.  
 
Take guang1hua2 (??) for example, it has a 
sense of ?splendid? which is similar to the sense 
of guang1mang2 (?? ). Guang1hua2 and 
guang1mang2 are certainly mutually changeable 
in a certain degree, guang1hua2jin4shi4 (???
?) and guang1mang2jin4shi4 (????), or 
xi2ri4guang1hua2 ( ? ? ? ? ) and 
xi2ri4guang1mang2 (????). However, the 
dominated contextual sense of guang1hua2 is 
more likely to be a place name, like 
guang1hua2shi4chang3( ? ? ? ? ) or 
hua1lian2guang1hua2 (????) etc3.  
 
b) Some synonyms are different in usages for 
pragmatic reasons.  
 
Synonyms with different contextual vectors 
could be result from different perspective views. 
For example, we may view wai4jie4 (??) as a 
container image with viewer inside, but on the 
other hand, yi3wai4 (??) is an omnipotence 
perspective. This similar meaning but different 
perspective makes distinct grammatical usage 
and different collocations.  
 
 
 Similarly, zhong1shen1 (??) and sheng1ping2 
( ? ? ) both refer to ?life-long time?. 
zhong1shen1 explicates things after a time point, 
which differs from sheng1ping2, showing mat-
ters before a time point.  
 
 
 
 
 
c) Domain specific usages.  
 
 For example, in medical domain news ,wa1wa1 
(??) occurs frequently with bo1li2 (??) refer 
                                                 
3 This may due to different genres. In newspapers the 
proper noun usage of guang1hua2 is more common 
than in a literature text. 
to kind of illness. Then the corpus reinterpret 
wa1wa1 (??) as a sick people, due to it occurs 
with medical term. But the synonym of wa1wa1 
(?? ), xiao3peng2you3(??? ) stands for 
money in some finance news.  Therefore, the 
meanings of words change from time to time. It?s 
hard to decide whether meaning is the right an-
swer when finding synonyms.  
  
With above observations, our future researches 
will be how to distinguish different word senses 
from its context features. Once we could distin-
guish the corresponding features for different 
senses, it will help us to extract more accurate 
synonyms for both frequent and infrequent words.  
 
 
References 
 
April Kontostathis,  William M. Pottenger 2003. ,  A 
Framework for Understanding LSI Performance ,  In 
the Proceedings of the ACM SIGIR Workshop on 
Mathematical/Formal Methods in Information Re-
trieval, Annual International SIGIR Conference, 2003. 
 
Christiance Fellbaum, editor 1998,WordNet: An alec-
tronic lwxical database. MIT press, Cambrige MA.  
 
Deerwester, S.,et al 1990 Indexing by Latent Seman-
tic Analysis. Jorunal of the American Society for In-
formation Science, 41(6):391-407 
 
Dominic Widdows. 2003. Unsupervised methods for 
developing taxonomies by combining syntactic and 
statistical information. In Proceeding of HLT-NAACL 
2003 Main papers, pp, 197-204.  
 
E.M. Voorhees, ?Implement agglomerative hierarchi-
cal clustering algorithm for use in document re-
trieval?, Information Processing & Management. , no. 
22 pp.46-476,1986 
 
Hofmann, T.1999. Probabilistic Latent  Semantic 
Indexing. Proc.of the 22nd International conference on 
Research and Development in Information Retrieval 
(SIGIR?99),50-57 
 
James R.Curran and Marc Moens. 2002. Improve-
ments in Automatic Thesaurus Extraction. Proceed-
ings of the Workshop of the ACL Special Interest 
Group on the Lexicon (SIGLEX), pp. 59-66 
 
Jia-Ming You and Keh-Jiann Chen, 2004 Automatic 
Semantic Role Assignment for a Tree Structure, Pro-
 
 
Wai4jie4  
?? 
               Yi3wai4?? 
 
 
 
Omnipotence viewer 
? 
Zhong1shen1
?? 
? 
sheng1ping2
?? 
viewer 
6
ceedings of 3rd ACL SIGHAN Workshop 
 
Jiahua Lin. 1991. Divergence measures based on the 
Shannon Entropy. IEEE transactions on Information 
Theory, 37(1): 145-151 
 
Lillian Lee. 2001. On the effectiveness of the skew 
divergence for statistical language analysis. In Artifi-
cial Intelligence and Statistics 2001, page 65-72. 
 
Lillian Lee. 1999. Measure of distributional similarity. 
In Proceeding of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-1999), 
page 23-32.  
 
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko 
Toyama. 2005. PLSI Utilization for Automatic The-
saurus Construction. IJCNLP 2005, LNAI 651, pp. 
334-345.  
 
Mei,Jiaju,Yiming Lan, Yunqi Gao, Yongxian Ying 
(1983) ?????   [ A Dictionary of Syno-
nyms],Shanghai Cishu Chubanshe.  
 
Mochihashi, D., Matsumoto, Y.2002. Probabilistic 
Representation of Meanings. IPSJ SIG Notes Natural 
Language, 2002-NL-147:77-84. 
 
T.Cover and J.Thomas, 1991. Element of Information 
Theory. Wiley & sons, New York 
 
 
 
 
 
7
Appendix: 
Some feature clustering results 
?? ???  
??? ??? ??? ???  
?? ???  
??? ??? ??  
?? ?? ??? ??  
??? ?? ???  
?? ?? ?? ??  
???? ???? ????  
?? ?? ?? ??? ?? ?? ?? ?? ?? 
?? ??  
??? ?? ??  
???? ??  
???? ?? ?? ?? ??  
?? ?? ?? ?? ??  
??? ???  
??? ???  
?? ?? ??  
?? ?? ??? ???  
?? ??? ??  
???? ?? ?? ?? ??  
?? ???  
?? ??? ?? ??  
?? ?? ?? ?? ??? ??  
?? ?? ?? ????  
?? ??  
??? ?? ??? ?? ??? ?? ???  
?? ?? ?? ?? ?? ??  
?? ??  
?? ??  
?? ?? ?? ??? ??? ?? ??? ?? 
?? ?? ?? ?? ?? ?? ?? ??  
?? ?? ??? ??? ???  
 
??? ??? ??? ?? ??? ??? ?? ?
?? ??? ??? ???  
?? ??? ?? ??? ?? ?? ??  
??? ???  
??? ??? ??  
???? ??????? ?? ??  
?? ?? ?? ???  
?? ?? ??  
?? ?? ??  
?? ??  
?? ?? ?? ??  
?? ???  
???? ?? ??? ?? ???? ?? ??? 
??? ?? ???  
?? ???? ??? ??  
?? ?? ?? ?? ??? ?? ??  
?? ?? ?? ?? ?? ?? ?? ??  
?? ??  
?? ?? ??  
?? ?? ??  
??? ??? ?? ??  
?? ?? ?? ??  
?? ?? ??  
?? ?? ??? ??? ?? ?? ?? ?? ?
? ?? ??? ?? ??  
?? ?? ?? ???? ??? ?? ?? ??  
?? ?? ?? ??  
??? ???  
?? ?? ???  
?? ?? ??  
??? ??? ??  
?? ??  
??? ??  
?? ?? ?? ???  
??? ???? ?? ?? ???  
?? ??? ?? ?? ?? ?? ???  
?? ?? ??  
?? ?? ??? ?? ?? ??  
8
