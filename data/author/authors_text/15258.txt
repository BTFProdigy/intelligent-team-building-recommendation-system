Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 857?866,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Language and Translation Model Adaptation using Comparable Corpora
Matthew Snover and Bonnie Dorr
Laboratory for Computational Linguistics
and Information Processing
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland
College Park, MD 20742
{snover, bonnie}@umiacs.umd.edu
Richard Schwartz
BBN Technologies
10 Moulton Street
Cambridge, MA 02138, USA
schwartz@bbn.com
Abstract
Traditionally, statistical machine translation
systems have relied on parallel bi-lingual data
to train a translation model. While bi-lingual
parallel data are expensive to generate, mono-
lingual data are relatively common. Yet mono-
lingual data have been under-utilized, having
been used primarily for training a language
model in the target language. This paper de-
scribes a novel method for utilizing monolin-
gual target data to improve the performance
of a statistical machine translation system on
news stories. The method exploits the exis-
tence of comparable text?multiple texts in
the target language that discuss the same or
similar stories as found in the source language
document. For every source document that is
to be translated, a large monolingual data set
in the target language is searched for docu-
ments that might be comparable to the source
documents. These documents are then used
to adapt the MT system to increase the prob-
ability of generating texts that resemble the
comparable document. Experimental results
obtained by adapting both the language and
translation models show substantial gains over
the baseline system.
1 Introduction
While the amount of parallel data available to train a
statistical machine translation system is sharply lim-
ited, vast amounts of monolingual data are generally
available, especially when translating to languages
such as English. Yet monolingual data are generally
only used to train the language model of the trans-
lation system. Previous work (Fung and Yee, 1998;
Rapp, 1999) has sought to learn new translations for
words by looking at comparable, but not parallel,
corpora in multiple languages and analyzing the co-
occurrence of words, resulting in the generation of
new word-to-word translations.
More recently, Resnik and Smith (2003)
and Munteanu and Marcu (2005) have exploited
monolingual data in both the source and target
languages to find document or sentence pairs that
appear to be parallel. This newly discovered bilin-
gual data can then be used as additional training data
for the translation system. Such methods generally
have a very low yield leaving vast amounts of data
that is only used for language modeling.
These methods rely upon comparable corpora,
that is, multiple corpora that are of the same gen-
eral genre. In addition to this, documents can be
comparable?two documents that are both on the
same event or topic. Comparable documents occur
because of the repetition of information across lan-
guages, and in the case of news data, on the fact that
stories reported in one language are often reported
in another language. In cases where no direct trans-
lation can be found for a source document, it is of-
ten possible to find documents in the target language
that are on the same story, or even on a related story,
either in subject matter or historically. Such docu-
ments can be classified as comparable to the origi-
nal source document. Phrases within this compara-
ble document are likely to be translations of phrases
in the source document, even if the documents them-
selves are not parallel.
Figure 1 shows an excerpt of the reference trans-
lation of an Arabic document, and figure 2 shows a
857
Cameras are flashing and reporters are following up, for
Hollywood star Angelina Jolie is finally talking to the pub-
lic after a one-month stay in India, but not as a movie star.
The Hollywood actress, goodwill ambassador of the United
Nations high commissioner for refugees, met with the In-
dian minister of state for external affairs, Anand Sharma,
here today, Sunday, to discuss issues of refugees and chil-
dren. ... Jolie, accompanied by her five-year-old son, Mad-
dox, visited the refugee camps that are run by the Khalsa
Diwan Society for social services and the high commis-
sioner for refugees Saturday afternoon after she arrived in
Delhi. Jolie has been in India since October 5th shooting
the movie ?A Mighty Heart,? which is based on the life of
Wall Street Journal correspondent Daniel Pearl, who was
kidnapped and killed in Pakistan. Jolie plays the role of
Pearl?s wife, Mariane.
Figure 1: Excerpt of Example Reference Translation of
an Arabic Source Document
comparable passage.1 In this case, the two new sto-
ries are not translations of each other and were not
reported at the same time?the comparable passage
being an older news story?but both discuss actress
Angelina Jolie?s visit to India. Many phrases and
words are shared between the two, including: the
name of the movie, the name and relationship of the
actress? character, the name and age of her son and
many others. Such a pairing is extremely compara-
ble, although even less related document pairs could
easily be considered comparable.
We seek to take advantage of these comparable
documents to inform the translation of the source
document. This can be done by augmenting the ma-
jor components of the statistical translation system:
the Language Model and the Translation Model.
This work is in the same tradition as Kim and
Khudanpur (2003), Zhao et al (2004), and Kim
(2005). Kim (2005) used large amounts of compa-
rable data to adapt language models on a document-
by-document basis, while Zhao et al (2004) used
comparable data to perform sentence level adapta-
tion of the language model. These adapted lan-
guage models were shown to improve performance
1This is an actual source document from the tuning set used
in our experiments, and the first of a number of similar passages
found by the comparable text selection system described in sec-
tion 2.
Actress Angelina Jolie hopped onto a crowded Mumbai
commuter train Monday to film a scene for a movie about
slain journalist Daniel Pearl, who lived and worked in In-
dia?s financial and entertainment capital. Hollywood actor
Dan Futterman portrays Pearl and Jolie plays his wife Mar-
iane in the ?A Mighty Heart? co-produced by Plan B, a pro-
duction company founded by Brad Pitt and his ex-wife, ac-
tress Jennifer Aniston. Jolie and Pitt, accompanied by their
three children ? Maddox, 5, 18-month-old Zahara and 5-
month-old Shiloh Nouvel ? arrived in Mumbai on Saturday
from the western Indian city Pune where they were shooting
the movie for nearly a month. ...
Figure 2: Excerpt of Example Comparable Document
for both automatic speech recognition as well as ma-
chine translation.
In addition to language model adaptation we
also modify the translation model, adding additional
translation rules that enable the translation of new
words and phrases in both the source and target lan-
guages, as well as increasing the probability of ex-
isting translation rules. Translation adaptation us-
ing the translation system?s own output, known as
Self-Training (Ueffing, 2006) has previously shown
gains by augmenting the translation model with ad-
ditional translation rules. In that approach however,
the translation model was augmented using parallel
data, rather than comparable data, by interpolating
a translation model trained using the system output
with the original translation model.
Translation model adaptation using comparable
out-of-domain parallel data, rather than monolingual
data was shown by Hildebrand et al (2005) to yield
significant gains over a baseline system. The trans-
lation model was adapted by selecting comparable
sentences from parallel corpora for each of the sen-
tences to be translated. In addition to selecting out-
of-domain data to adapt the translation model, com-
parable data selection techniques have been used to
select and weight portions of the existing training
data for the translation model to improve translation
performance (Lu et al, 2007).
The research presented in this paper utilizes a dif-
ferent approach to translation model adaptation us-
ing comparable monolingual text rather than parallel
text, exploiting data that would otherwise be unused
858
for estimating the translation model. In addition,
this data also informs the translation system by in-
terpolating the original language model with a new
language model trained from the same comparable
documents.
We discuss the selection of comparable text for
model adaptation in section 2. In sections 3.1
and 3.2, we describe the model adaptation for the
language model and translation model, respectively.
Experimental results describing the application of
model adaptation to a hierarchical Arabic-to-English
MT system are presented in section 4. Finally we
draw conclusions in sections 5.
2 Comparable Text Selection
Comparable text is selected for every source doc-
ument from a large monolingual corpus in the tar-
get language. In practice, one could search the
World Wide Web for documents that are compara-
ble to a set of source documents, but this approach
presents problems for ensuring the quality of the re-
trieved documents. The experiments in this paper
use comparable text selected from a collection of
English news texts. Because these texts are all flu-
ent English, and of comparable genre to the test set,
they are also used for training the standard language
model training.
The problem of selecting comparable text has
been widely studied in the information retrieval
community and cross-lingual information retrieval
(CLIR) (Oard and Dorr, 1998; Levow et al, 2005)
has been largely successful at the task of selecting
comparable or relevant documents in one language
given a query in another language. We use CLIR to
select a ranked list of documents in our target lan-
guage, English in the experiments described in this
paper, for each source document, designated as the
query in the CLIR framework, that we wish to trans-
late.
The CLIR problem can be framed probabilisti-
cally as: Given a query Q, find a document D that
maximizes the equation Pr(D is rel|Q). This equa-
tion can be expanded using Bayes? Law as shown
in equation 1. The prior probability of a document
being relevant can be viewed as uniform, and thus
in this work, we assume Pr(D is rel) is a constant.2
2In fact, it can be beneficial to use features of the document
The Pr(Q) is constant across all documents. There-
fore finding a document to maximize Pr(D is rel|Q)
is equivalent to finding a document that maximizes
Pr(Q|D is rel).
Pr(D is rel|Q) =
Pr(D is rel) Pr(Q|D is rel)
Pr(Q)
(1)
A method of calculating the probability of a query
given a document was proposed by (Xu et al, 2001)3
and is shown in Equation 2. In this formulation, each
foreign word, f , in the query is generated from the
foreign vocabulary with probability ? and from the
English document with probability 1 ? ?, where ?
is a constant.4 The probability of f being generated
by the general foreign vocabulary, F , is Pr(f |F ) =
freq(f, F )/|F |, the frequency of the word f in the
vocabulary divided by the size of the vocabulary.
The probability of the word being generated by the
English document is the sum of the probabilities of it
being generated by each English word, e, in the doc-
ument which is the frequency of the English word in
the document, (Pr(e|D) = freq(e,D)/|D|) multi-
plied by the probability of the translation of the En-
glish word to the foreign word, Pr(f |e).
Pr(Q|D) =
?
f?Q
(?Pr(f |F )+ (2)
(1? ?)
?
e
Pr(e|D) Pr(f |e))
This formulation favors longer English docu-
ments over shorter English documents. In addition,
many documents cover multiple stories and topics.
For the purposes of adaptation, shorter, fully com-
parable documents are preferred to longer, only par-
tially comparable documents. We modify the CLIR
system by taking the 1000 highest ranked target lan-
guage documents found by the CLIR system for
each source document, and dividing them into over-
lapping passages of approximately 300 words.5 Sen-
to estimate Pr(D is rel) (Miller and Schwartz, 1998) but we
have not explored that here.
3Xu et al (2001) formulated this for the selection of foreign
documents given an English query. We reverse this to select
English documents given a foreign query.
4As in Xu et al (2001), a value of 0.3 was used for ?.
5The length of 300 was chosen as this was approximately
the same length as the source documents.
859
tence boundaries are preserved when creating pas-
sages, insuring that the text is fluent within each pas-
sage. These passages are then scored again by the
CLIR system, resulting in a list of passages of about
300 words each for each source document. Finally,
we select the top N passages to be used for adapta-
tion.
The N passages selected by this method are not
guaranteed to be comparable and are often largely
unrelated to the story or topic in the source docu-
ment. We shall refer to the set of passages selected
by the CLIR system as the bias text to differentiate
it from comparable text, as the adaptation methods
will use this text to bias the MT system so that its
output will be more similar to the bias text.
While we have not conducted experiments using
other CLIR systems, the adaptation methods pre-
sented in this paper could be applied without modifi-
cation using another CLIR system, as the adaptation
method treats the CLIR system as a black box. With
the exception of running a second pass of CLIR, we
use the algorithm of Xu et al (2001) without any
significant modification, including the use of a stop
word list for both the English and foreign texts. The
parameters for Pr(f |F ) and Pr(f |e) were estimated
using the same parallel data that our translation sys-
tem was trained on.
The bias text selected for a source document is
used to adapt the language model (described in sec-
tion 3.1) and the translation model (described in sec-
tion 3.2) when translating that source document.
3 Model Adaptation
We use the same bias text to adapt both the lan-
guage model and the translation model. For lan-
guage model adaptation, we increase the probability
of the word sequences in the bias text, and for trans-
lation model adaptation we use additional phrasal
translation rules. The adaptations can be done in-
dependently and while they can augment each other
when used together, this is not required. It is not
necessary to use the same number of passages for
both forms of adaptation, although doing so makes
it more likely both that the English side of the new
translation rule will be assigned a high probability
by the adapted language model, and that the transla-
tion model produces the English text to which the
language model has been adapted. Bias text that
is used by one adaptation but not the other will re-
ceive no special treatment by the other model. This
could result in new translation rules that produce text
to which the language assigns low probability, or it
could result in the language model being able to as-
sign a high probability to a good English translation
that cannot be produced by the translation model due
to a lack of necessary translation rules.
While both adaptation methods are integrated into
a hierarchical translation model (Chiang, 2005),
they are largely implementation independent. Lan-
guage model adaptation could be integrated into any
statistical machine translation that uses a language
model over words, while translation model adapta-
tion could be added to any statistical machine trans-
lation that can utilize phrasal translation rules.
3.1 Language Model Adaptation
For every source document, we estimate a new lan-
guage model, the bias language model, from the cor-
responding bias text. Since this bias text is short, the
corresponding bias language model is small and spe-
cific, giving high probabilities to those phrases that
occur in the bias text. The bias language model is
interpolated with the generic language model that
would otherwise be used for translation if no LM
adaptation was used. The new bias language model
is of the same order as the generic language model,
so that if a trigram language model is used for the
MT decoding, then the biased language model will
also be a trigram language model. The bias lan-
guage model is created using the same settings as
the generic language model. In our particular im-
plementation however, the generic language model
uses Kneser-Ney smoothing, while the biased lan-
guage model uses Witten-Bell smoothing due to im-
plementation limitations. In principle the biased lan-
guage model can be smoothed in the same manner as
the generic language model.
We interpolate the bias language model and
the generic language model as shown in equa-
tion 3, where Prg and Prb are the probabilities
from the generic language model and the bias lan-
guage model, respectively. A constant interpolation
weight, ? is used to weight the two probabilities for
all documents. While a value for ? could be cho-
sen that minimizes perplexity on a tuning set, in a
860
similar fashion to Kim (2005), it is unclear that such
a weight would be ideal when the interpolated lan-
guage model is used as part of a statistical translation
system. In practice we have observed that weights
other than one that minimizes perplexity, typically a
lower weight, can yield better translation results on
the tuning set.
Pr(e) = (1? ?) Pr
g
(e) + ?Pr
b
(e) (3)
The resulting interpolated language model is then
used in place of the generic language model in the
translation process, increasing the probability that
the translation output will resemble the bias text. It
is important to note that, unlike the translation model
adaptation described in section 3.2, no new infor-
mation is added to the system with language model
adaptation. Because the bias text is extracted from
the same monolingual corpus that the generic lan-
guage model was estimated from, all of the word se-
quences used for training the bias language model
were also used for training the generic language
model. Language model adaptation only increases
the weight of the portion of the language model data
that was selected as comparable.
3.2 Translation Model Adaptation
It is frequently the case in machine translation that
unknown words or phrases are present in the source
document, or that the known translations of source
words are based on a very small number of oc-
currences in the training data. In other cases,
translations may be known for individual words in
the source document, but not for longer phrases.
Translation model adaptation seeks to generate new
phrasal translation rules for these source words and
phrases. The bias text for a source document may,
if comparable, contain a number of English words
and phrases that are the English side of these desired
rules.
Because the source data and the bias text are
not translations of each other and are not sen-
tence aligned, conventional alignment tools, such as
GIZA++ (Och and Ney, 2000), cannot be used to
align the source and bias text. Because the passages
in the bias text are not translations of the source doc-
ument, it will always be the case that portions of the
source document have no translation in the bias text,
and portions of the bias text have no translation in
the source document. In addition a phrase in one
of these texts might have multiple, differing transla-
tions in the other text.
Unlike language model adaptation, the entirety of
the bias text is not used for translation adaptation.
We extract those phrases that occur in at least M
of the passages in the bias texts. A phrase is only
counted once for every passage in which it occurs,
so that repeated use of a phrase within a passage
does not affect whether it used to generate new rules.
Typically, passages selected by the CLIR tend to be
very similar to each other if they are comparable
to the source document and are very different from
each other if they are not comparable to the source
document. Phrases that are identical across passages
are the ones that are most likely to be comparable,
whereas a phrase or word that occurs in only one
passage is likely to be present only by chance or if
the passage it is in is not comparable. Filtering the
target phrases to those that occur in multiple pas-
sages therefore serves not only to reduce the total
number of rules, but also to filter out phrases from
passages that are not comparable.
For each phrase in the source document we gener-
ate a new translation to each of the phrases selected
from the bias text, and assign it a low uniform prob-
ability.6 For each translation rule we also have a
lexical translation probability that we estimate cor-
rectly from the trained word model. These new rules
are then added to the phrase table of the existing
translation model when translating the source doc-
ument. Rather than adding probability to the ex-
isting generic rules, the new rules are marked as
bias rules by the system and given their own fea-
ture weight. While the vast majority of these rules
are incorrect translations, these incorrect rules will
be naturally biased against by the translation sys-
tem. If the source side of a translation already has a
number of observed translations, then the low prob-
ability of the new bias rule will cause it to not be
selected by the translation system. If the new trans-
lation rules would produce garbled English, then it
will be biased against by the language model. When
this is combined with the language model adapta-
6A probability of 1/700 is arbitrarily used for the bias rules
although it is then weighted by the bias translation rule weight.
861
tion, a natural pressure is exerted to use the bias rules
for source phrases primarily when it would cause the
output to look more like the bias text.
4 Experimental Results
We evaluated the performance of language and
translation model adaptation with our translation
system on two conditions, the details of which are
presented in section 4.1. One condition involved a
small amount of parallel training, such as one might
find when translating a less commonly taught lan-
guage (LCTL). The other condition involved the full
amount of training available for Arabic-to-English
translation. In the case of LCTLs we expect our
translation model to have the most deficiencies and
be most in need of additional translation rules. So,
it is under such a condition we would expect the
translation model adaptation to be the most bene-
ficial. We evaluate the system?s performance under
this condition in section 4.2. The effectiveness of
this technique on state-of-the-art systems, and its ef-
ficiency when used with a well trained generic trans-
lation model is presented in section 4.3.
4.1 Implementation Details
Both language-model and translation-model adap-
tation are implemented on top of a hierarchical
Arabic-to-English translation system with string-to-
dependency rules as described in Shen et al (2008).
While generalized rules are generated from the par-
allel data, rules generated by the translation model
adaptation are not generalized and are used only as
phrasal rules. A trigram language model was used
during decoding, and a 5-gram language model was
used to re-score the n-best list after decoding. In ad-
dition to the features described in Shen et al (2008),
a new feature is added to the model for the bias
rule weight, allowing the translation system to ef-
fectively tune the probability of the rules added by
translation model adaptation in order to improve per-
formance on the tuning set.
Bias texts were selected from three mono-
lingual corpora: the English Gigaword cor-
pus (2,793,350,201 words), the FBIS corpus
(28,465,936 words), and a collection of news archive
data collected from the websites of various on-
line, public news sites (828,435,409 words). All
three corpora were also part of the generic language
model training data. Language model adaptation
on both the trigram and 5-gram language models
used 10 comparable passages with an interpolation
weight of 0.1. Translation model adaptation used 10
comparable passages for the bias text and a value of
2 for M .
Each selected passage contains approximately
300 words, so in the case where 10 comparable pas-
sages are used to create a bias text, the resulting text
will be 3000 words long on average. The language
models created using these bias texts are very spe-
cific giving large probability to n-gram sequences
seen in those texts.
The construction of the bias texts increases the
overall run-time of the translation system, although
in practice this is a small expenditure. The most in-
tensive portion is the initial indexing of the monolin-
gual corpus, but this is only required once and can be
reused for any subsequent test set that is evaluated.
This index can then be quickly searched for com-
parable passages. When considering research envi-
ronments, test sets are used repeatedly and bias texts
only need to be built once per set, making the build-
ing cost negligible. Otherwise, the time required to
build the bias text is still small compared to the ac-
tual translation time.
All conditions were optimized using BLEU (Pap-
ineni et al, 2002) and evaluated using both BLEU
and Translation Edit Rate (TER) (Snover et al,
2006). BLEU is an accuracy measure, so higher
values indicate better performance, while TER is an
error metric, so lower values indicate better perfor-
mance. Optimization was performed on a tuning set
of newswire data, comprised of portions of MTEval
2004, MTEval 2005, and GALE 2007 newswire de-
velopment data, a total of 48921 words of English
in 1385 segments and 173 documents. Results were
measured on the NIST MTEval 2006 Arabic Evalu-
ation set, which was 55578 words of English in 1797
segments and 104 documents. Four reference trans-
lations were used for scoring each translation.
Parameter optimization method was done using n-
best optimization, although the adaptation process
is not tied to this method. The MT decoder is run
on the tuning set generating an n-best list (where
n = 300), on which all of the translation features
(including bias rule weights) are optimized using
862
Powell?s method. These new weights are then used
to decode again, repeating the whole process, using
a cumulative n-best list. This continues for several
iterations until performance on the tuning set stabi-
lizes. The resulting feature weights are used when
decoding the test set. A similar, but simpler, method
is used to determine the feature weights after 5-gram
rescoring. This n-best optimization method has sub-
tle implications for translation model adaptation. In
the first iteration, few bias rules are used in decoding
the 300-best, and those that are used frequently help,
although the overall gain is small due to the small
number of bias rules used. This causes the opti-
mizer to greatly increase the weight of the bias rules,
causing the decoder to overuse the bias rules in the
next iteration causing a sharp decrease in translation
quality. Several iterations are needed for the cumu-
lative n-best to achieve sufficient diversity and size
to assign a weight for the bias translation rules that
results in an increase in performance over the base-
line. Alternative optimization methods could likely
circumvent this process. Language model adapta-
tion does not suffer from this phenomenon.
4.2 Less Commonly Taught Language
Simulation
In order to better examine the nature of translation
model adaptation, we elected to work with a transla-
tion model that was trained on only 5 million words
of parallel Arabic-English text. Limiting the trans-
lation model training in this way simulates the prob-
lem of translating less commonly taught languages
(LCTL) where less parallel text is available, a situa-
tion that is not the case for Arabic. Since the model
is trained on less parallel data, it is lacking a large
number of translation rules, which is expected to be
addressed by the translation model adaptation. By
working in an environment with a more deprived
baseline translation model, we are giving the trans-
lation model adaptation more room to assist.
The experiments described below use a 5 million
word Arabic parallel text corpus constructed from
the LDC2004T18 and LDC2006E25 corpora. The
full monolingual English data were used for the lan-
guage model and for selection of comparable doc-
uments. Unless otherwise specified no language
model adaptation was used.
We first establish an upper limit on the gain us-
ing translation model adaptation, using the reference
data to adapt the translation system. These reference
data can be considered to be extremely comparable,
better than could ever be hoped to gain by compara-
ble document selection. We first aligned this data
using GIZA++ to the source data, simulating the
ideal case where we can perfectly determine which
source words translate to which comparable words.
Because our translation model adaptation system as-
signs uniform probability to all bias rules, we ignore
the correct rule probabilities that we could extract
from word alignment and assign uniform probabil-
ity to all of the bias translation rules. As expected,
this gives a large gain over the baseline.
We also examine limiting these new translation
rules to those rules whose target side occurs in the
top 100 passages selected by CLIR, thus minimiz-
ing the adaption to those rules that it theoretically
could learn from the bias text. On average, 50% of
the rules were removed by this filtering, resulting in
a corresponding 50% decrease in the gain over the
baseline. The results of these experiments and an
unadapted baseline are shown in table 1.
Test Set TM Adaptation TER BLEU
Tune None 0.4984 0.4080
Aligned Reference 0.3692 0.5841
Overlapping Only 0.4179 0.5138
MT06 None 0.5516 0.3468
Aligned Reference 0.4517 0.5216
Overlapping Only 0.4899 0.4335
Table 1: LCTL Aligned Reference Adaptation Results
The fair translation model adaptation system,
however, does not align source phrases to the cor-
rect bias text phrases in such a fashion, and instead
aligns all source words to all target words. To in-
vestigate the effect of this over production of rules,
we again used the reference translations as if they
were comparable data, but we ignored the align-
ments learned by GIZA++, and instead allowed all
source phrases to translate to all English phrases in
the reference text, with uniform probability. This
still shows large gains in translation quality over the
baseline, as measured by TER and BLEU. Again,
we also examined limiting the text used for transla-
tion model adaptation to those phrases that occur in
863
both the reference text and the top 100 comparable
passages selected the CLIR system. While this de-
creased performance, the system still performs sig-
nificantly better than the baseline, as shown in the
following table 2.
Test Set TM Adaptation TER BLEU
Tune None 0.4984 0.4080
Unaligned Ref. 0.4492 0.4566
Overlapping Only 0.4808 0.4313
MT06 None 0.5516 0.3468
Unaligned Ref. 0.5254 0.3990
Overlapping Only 0.5390 0.3695
Table 2: LCTL Unaligned Reference Adaptation Results
Applying translation model and language model
adaptation fairly, using only bias text from the com-
parable data selection, yields smaller gains on both
the tuning and MT06 sets, as shown in table 3.
The combination of language-model and translation-
model adaptation exceeds the gains that would be
achieved over the baseline by either method sepa-
rately.
Test Set Adaptation TER BLEU
Tune None 0.4984 0.4080
LM 0.4922 0.4140
TM 0.4916 0.4169
LM & TM 0.4888 0.4244
MT06 None 0.5516 0.3468
LM 0.5559 0.3490
TM 0.5545 0.3478
LM & TM 0.5509 0.3536
Table 3: LCTL Fair Adaptation Results
4.3 Full Parallel Training Results
While the simulation described in section 4.2 used
only 5 million words of parallel training, 230 mil-
lion words of parallel data from 18.5 million seg-
ments were used for training the full Arabic-to-
English translation system. This parallel data in-
cludes the LDC2007T08 ?ISI Arabic-English Auto-
matically Extracted Parallel Text? corpus (Munteanu
and Marcu, 2007), which was created from monolin-
gual corpora in English and Arabic using the algo-
rithm described in Munteanu and Marcu (2005), as
the techniques used in that work are separate and
independent from the adaptation methods we de-
scribe in this paper.7 Language model adaptation
and translation model adaptation were applied both
independently and jointly to the translation system,
and the results were evaluated against an unadapted
baseline, as shown in table 4.
While gains from language model adaptation
were substantial on the tuning set, on the MT06 test
set they are reduced to a 0.65% gain on BLEU and
a negligible improvement in TER. The translation
model adaptation performs better with 1.37% im-
provement in BLEU and a 0.26% improvement in
TER. This gain increases to a 2.07% improvement
in BLEU and a 0.64% improvement in TER when
language adaptation is used in conjunction with the
translation model adaptation, showing the impor-
tance of using both adaptation methods. While it
could be expected that a more heavily trained trans-
lation model might not require the benefit of lan-
guage and translation model adaptation, a more sub-
stantial gain over the baseline can be seen when both
forms of adaptation are used than in the case with
less parallel training?a difference of 2.07% BLEU
versus 0.68% BLEU.
Test Set Adaptation TER BLEU
Tune None 0.4339 0.4661
LM 0.4227 0.4857
TM 0.4351 0.4657
LM & TM 0.4245 0.4882
MT06 None 0.5146 0.3852
LM 0.5140 0.3917
TM 0.5120 0.3989
LM & TM 0.5082 0.4059
Table 4: Full Training Adaptation Results
Of the comparable passages selected by the CLIR
system for the MT06 test set in the full training
experiment, 16.3% were selected from the News
7The two methods are not directly comparable, and so we
do not make any attempt to do so. Munteanu and Marcu (2005)
creates new parallel corpora from two monolingual corpora.
This new parallel data is generally applicable for training a
translation model but does not target any particular test set. Our
adaptation method does not generate new parallel data, but cre-
ates a new specific translation model for a test document that is
being translated.
864
Archive corpus, 81.2% were selected from the En-
glish GigaWord corpus and 2.5% were selected from
the FBIS corpus. A slightly different distribution
was found for the Tuning set, where 17.8% of the
passages were selected from the News Archive cor-
pus, 77.1% were selected from the English Giga-
Word corpus, and 5.1% were selected from the FBIS
corpus.
5 Discussion
The reuse of a monolingual corpus that was already
used by a translation system for language model
training to perform both language and translation
model adaptation shows large gains over an un-
adapted baseline. By leveraging off of a CLIR sys-
tem, which itself contains no information not al-
ready given to the translation system,8 potentially
comparable passages can be found which allow im-
proved translation. Surprisingly, these gains are
largest when the baseline model is better trained, in-
dicating that a strong reliance of the adaptation on
the existing models.
One explantation for these counter-intuitive
results?larger gains in the full training scenario ver-
sus the LCTL scenario?is that the lexical probabili-
ties are better estimated in the former case. The bias
rules all have equal translation probability and only
vary in probability according to the lexical proba-
bility of the rules. Better estimates of these lexical
probabilities may enable the translation system to
more clearly distinguish between helpful and harm-
ful bias rules.
There are many clear directions for the improve-
ment of these methods. The current adaptation
method does not utilize the probabilities from the
CLIR system and treats the top-ranked passages all
as equally comparable regardless of the probabil-
ity assigned. Variable weighting of passages could
prove beneficial to both language model adaptation,
where the passages could be weighted proportion-
ally to the probability of the passage being relevant,
and translation model adaptation, where the require-
ment on repetition of phrases across passages could
be weighted, as could the probability of the new
8The probabilistic parameters of the CLIR system are esti-
mated from the same parallel corpora that is used to train the
generic translation model.
rules produced by the translation system. In ad-
dition, the CLIR score, among other possible fea-
tures such as phrase overlap, could be used to de-
termine those documents where no comparable pas-
sage could be detected and where it would be bene-
ficial to not adapt the models.
A clear limitation of using comparable documents
to adapt the language and translation model is that
comparable documents must be found. For many
source documents, none of the top passages found
by the CLIR system were comparable. We suspect
that while this will always occur to some extent, this
becomes more common as the monolingual data be-
comes less like the source data, such as when there is
a large time gap between the two. The full extent of
this and the effect of the level of document compa-
rability on translation remains an open question. In
addition, while newswire is an excellent source of
comparable text, it is unclear how well this method
can be used on newsgroups or spoken data, where
the fluency of the source text is diminished. When
translating news stories, this technique is not lim-
ited to major news events. While many of the events
discussed in the source data receive world-wide at-
tention, many are local events that are unreported
in the English comparable data used in our experi-
ments. Events of a similar nature or events involving
many of the same people often do occur in the En-
glish comparable data, allowing improvement even
when the stories are quite different.
The adaptation methods described in this paper
are not limited to a particular framework of statis-
tical machine translation, but have applicability to
any statistical machine translation system that uses
a language model or translation rules.
Acknowledgments
This work was supported, in part, by BBN Tech-
nologies under the GALE Program, DARPA/IPTO
Contract No. HR0011-06-C-0022. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the sponsor.
We are very grateful to all three reviewers for their
careful and thoughtful reviews.
865
References
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Pro-
ceedings of ACL, pages 263?270.
Pascale Fung and Lo Yuen Yee. 1998. An IR Approach
for Translating New Words from Nonparallel, Compa-
rable Texts. In Proceedings of COLING-ACL98, pages
414?420, August.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the Translation
Model for Statistical Machine Translation based on In-
formation Retrieval. In Proceedings of EAMT 2005,
Budapest, Hungary, May.
Woosung Kim and Sanjeev Khudanpur. 2003. Cross-
Lingual Lexical Triggers in Statistical Language Mod-
eling. In 2003 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2003), pages
17?24, July.
Woosung Kim. 2005. Language Model Adaptation for
Automatic Speech Recognition and Statistical Machine
Translation. Ph.D. thesis, The Johns Hopkins Univer-
sity, Baltimore, MD.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based cross-language retrieval. In-
formation Processing and Management, 41:523?547.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
Statistical Machine Translation Performance by Train-
ing Data Selection and Optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
343?350.
T. Leek Miller and Richard Schwartz. 1998. BBN at
TREC7: Using Hidden Markov Models for Informa-
tion Retrieval. In TREC 1998, pages 80?89, Gaithers-
burg, MD.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Exploit-
ing Non-Parallel Corpora. Computational Linguistics,
31:477?504.
Dragos Stefan Munteanu and Daniel Marcu. 2007. Isi
arabic-english automatically extracted parallel text.
Linguistic Data Consortium, Philadelphia.
Douglas W. Oard and Bonnie J. Dorr. 1998. Evaluat-
ing Cross-Language Text Retrieval Effectiveness. In
Gregory Grefenstette, editor, Cross-Language Infor-
mation Retrieval, pages 151?161. Kluwer Academic
Publishers, Boston, MA.
F. J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the 38th Annual Con-
ference of the Association for Computational Linguis-
tics, pages 440?447, Hongkong, China.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Traslation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, pages
519?526.
Philip Resnik and Noah Smith. 2003. The Web as a
Parallel Corpus. Computational Linguistics, 29:349?
380.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
New String-to-Dependency Machine Translation Al-
gorithm with a Target Dependency Language Model.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas.
Nicola Ueffing. 2006. Using Monolingual Source-
Language to Improve MT Performance. In Proceed-
ings of IWSLT 2006.
Jinxi Xu, Ralpha Weischedel, and Chanh Nguyen. 2001.
Evaluating a Probabilistic Model for Cross-lingual In-
formation Retrieval. In Proceedings of SIGIR 2001
Conference, pages 105?110.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language Model Adaptation for Statistical Machine
Translation via Structured Query Models. In Proceed-
ings of Coling 2004, pages 411?417, Geneva, Switzer-
land, Aug 23?Aug 27. COLING.
866
A Lexically-Driven Algorithm for Disfluency Detection
Matthew Snover, Bonnie Dorr
Institute for Advanced Computer Studies1
University of Maryland
College Park, MD 20740
snover,bonnie@umiacs.umd.edu
Richard Schwartz
BBN
9861 Broken Land Parkway
Columbia, MD 21046
schwartz@bbn.com
Abstract
This paper describes a transformation-based learn-
ing approach to disfluency detection in speech tran-
scripts using primarily lexical features. Our method
produces comparable results to two other systems
that make heavy use of prosodic features, thus
demonstrating that reasonable performance can be
achieved without extensive prosodic cues. In addi-
tion, we show that it is possible to facilitate the iden-
tification of less frequently disfluent discourse mark-
ers by taking speaker style into account.
1 Introduction
Disfluencies in human speech are widespread and cause
problems for both downstream processing and human
readability of speech transcripts. Recent human studies
(Jones et al, 2003) have examined the effect of disfluen-
cies on the readability of speech transcripts. These results
suggest that the ?cleaning? of text by removing disfluent
words can increase the speed at which readers can process
text. Recent work on detecting edits for use in parsing
of speech transcripts (Core and Schubert, 1999), (Char-
niak and Johnson, 2001) has shown an improvement in
the parser error rate by modeling disfluencies.
Many researchers investigating disfluency detection
have focused on the use of prosodic cues, as opposed to
lexical features (Nakatani and Hirschberg, 1994). There
are different approaches to detecting disfluencies. In one
approach, one can first try to locate evidence of a gen-
eral disfluency, e.g., using prosodic features or language
model discontinuations. These locations are called inter-
ruption points (IPs). Following this, it is generally suffi-
cient to look in the nearby vicinity of the IP to find the dis-
1This work is supported in part by BBNT Con-
tract 9500006806 and an NSF CISE Infrastructure Award
EIA0130422.
fluent words. The most successful approaches so far com-
bine the detection of IPs using prosodic features and lan-
guage modeling techniques (Liu et al, 2003), (Shriberg
et al, 2001), (Stolcke et al, 1998).
Our work is based on the premise that the vast ma-
jority of disfluencies can be detected using primarily
lexical features?specifically the words themselves and
part-of-speech (POS) labels?without the use of exten-
sive prosodic cues. Lexical modeling of disfluencies with
only minimal acoustic cues has been shown to be suc-
cessful in the past using strongly statistical techniques
(Heeman and Allen, 1999). We shall discuss our algo-
rithm and compare it to two other algorithms that make
extensive use of acoustic features. Our algorithm per-
forms comparably on most of the tasks assigned and in
some cases outperforms systems that used both prosodic
and lexical features.
We discuss the task definition in Section 2. In Section
3 we describe our Transformation-Based Learning (TBL)
algorithm and its associated features. Section 4 presents
results for our system and two other systems that make
heavy use of prosodic features to detect disfluencies. We
then discuss the errors made by our system, in Section 5,
and discuss our conclusions and future work in Section 6.
2 EARS Disfluency Annotation
One of the major goals of the DARPA program for
Effective, Affordable, Reusable Speech-to-Text (EARS)
(Wayne, 2003) is to provide a rich transcription of speech
recognition output, including speaker identification, sen-
tence boundary detection and the annotation of disfluen-
cies in the transcript (This collection of additional fea-
tures is also known as Metadata). One result of this pro-
gram has been production of an annotation specification
for disfluencies in speech transcripts and the transcription
of sizable amounts of speech data, both from conversa-
tional telephone speech and broadcast news, according to
this specification (Strassel, 2003).
The task of disfluency detection is to distinguish flu-
ent from disfluent words. The EARS MDE (MetaData
Extraction) program addresses two types of disfluencies:
(i) edits?words that were not intended to be said and
that are normally replaced with the intended words, such
as repeats, restarts, and revisions; and (ii) fillers?words
with no meaning that are used as discourse markers and
pauses, such as ?you know? and ?um?.
3 The Algorithm
We set out to solve the task of disfluency detection using
primarily lexical features in a system we call System A.
This section describes our algorithm, including the set of
features we use to identify disfluencies.
The training data for the system are time aligned refer-
ence speech transcripts, with speaker identification, sen-
tence boundaries, edits, fillers and interruption points an-
notated. The input for evaluation is a transcript, either
a reference transcript or a speech recognizer output tran-
script. Some of the evaluation data may be marked with
sentence boundaries and speaker identification. The task
is to identify which words in the transcript are fillers, ed-
its, or fluent. The evaluation data was held out, and not
available for tuning system parameters.
The input to System A is a transcript of either con-
versational telephone speech (CTS) or broadcast news
speech (BNEWS). In all experiments, the system was
trained on reference transcripts, but was tested on both
reference and speech output transcripts.
We use a Transformation-Based Learning (TBL)
(Brill, 1995) algorithm to induce rules from the training
data. TBL is a technique for learning a set of rules that
transform an initial hypothesis for the purpose of reduc-
ing the error rate of the hypothesis. The set of possi-
ble rules is found by expanding rule templates, which are
given as an input. The algorithm greedily selects the rule
that reduces the error rate the most, applies it to the data,
and then searches for the next rule. The algorithm halts
when there are no more rules that can reduce the error
rate by more than the threshold. The output of the system
is an ordered set of rules, which can then be applied to
the test data to annotate it for disfluencies.
We allow one of three tags to be assigned to each word:
edit, filler or fluent. Since only 15% of the words in con-
versational speech are disfluent, we begin with the initial
hypothesis that all the words in the corpus are fluent. The
system then learns rules to relabel words as edits or fillers
in order to reduce the number of errors. The rules are it-
eratively applied to the data from left to right.
3.1 Feature Set
The rules learned by the system are conditioned on sev-
eral features of each of the words including the lexeme
(the word itself), a POS tag for the word, whether the
word is followed by a silence and whether the word is a
high frequency word. That is, whether the word is more
frequent for this speaker than in the rest of the corpus.
The last feature (high frequency of the word) is useful for
identifying when words that are usually fluent?but are
sometimes disfluent (such as ?like?)?are more likely to
be disfluencies, with the intuition being that if a speaker
is using the word ?like? very frequently, then it is likely
that the word is being used as a filler. The word ?like?
for example was only a disfluency 22% of the time it oc-
curred. So a rule that always tags ?like? as a disfluency
would hurt rather than help the system.2
3.2 Rule Templates
The system was given a set of 33 rule templates, which
were used to generate the set of possible rules. Not all
rule templates generated rules that were chosen by the
system. Below is a representative subset of rule templates
chosen by the system. Change the label of:
1. word X from L1 to L2.
2. word sequence X Y to L1.
3. left side of simple repeat to L1.
4. word with POS X from L1 to L2 if followed by word with
POS Y.
5. word from L1 to L2 if followed by words X Y.
6. word X with POS Y from L1 to L2.
7. A to L1 in the pattern A POS X B A, where A and B can
be any words.
8. left side of repeat with POS X in the middle to L1.
9. word with POS X from L1 to L2 if followed by silence
and followed by word with POS Y.
10. word X that is a high frequency word for the speaker from
L1 to L2.
4 Results
All of the results in this section are from training and eval-
uation on data produced by the Linguistic Data Consor-
tium (LDC) for the EARS Metadata community. There
were 491,543 tokens in the CTS training set and 189,766
tokens in the BNEWS training set. The CTS evaluation
set contained 33,670 tokens and the BNEWS evaluation
set contained 14,544 tokens.
We compare our System A to two other systems that
were designed for the same task, System B and System
C. System C was only applied to conversational speech,
so there are no results for it on broadcast news transcripts.
Our system was also given the same speech recognition
output as System C for the conversational speech condi-
tion, whereas System B used transcripts produced by a
different speech recognition system.
2We use a POS tagger (Ratnaparkhi, 1996) trained on
switchboard data with the additional tags of FP (filled pause)
and FRAG (word fragment).
System B used both prosodic cues and lexical informa-
tion to detect disfluencies. The prosodic cues were mod-
eled by a decision tree classifier, whereas the lexical in-
formation was modeled using a 4-gram language model,
separately trained for both CTS and BNEWS.
System C first inserts IPs into the text using a decision-
tree classifier based on both prosodic and lexical features
and then uses TBL. In addition to POS, System C?s fea-
ture set alo includes whether the word is commonly used
as a filler, edit, back-channel word, or is part of a short re-
peat. Turn and segment boundary flags were also used by
the system. Whereas System A only attempted to learn
three labels (filler, edit and fluent), System C attempted
to learn many subtypes of disfluencies, which were not
distinguished in the evaluation.
4.1 Lexeme Error Rate
We use Lexeme Error Rate (LER) as a measure of recog-
nition effectiveness. This measure is the same as the tra-
ditional word-error rate used in speech recognition, ex-
cept that filled pauses and fragments are not optionally
deletable. The LERs of the speech transcripts used by the
three systems were all fairly similar (about 25% for CTS
and 12% for BNEWS).
4.2 Top Rules Learned
A total of 106 rules were learned by the system for CTS?
the top 10 rules learned are:
1. Label all fluent filled pauses as fillers.
2. Label the left side of a simple repeat as an edit.
3. Label ?you know? as a filler.
4. Label fluent ?well?s with a UH part-of-speech as a filler.
5. Label fluent fragments as edits.
6. Label ?I mean? as a filler.
7. Label the left side of a simple repeat separated by a filled
pause as an edit.
8. Label the left side of a simple repeat separated by a frag-
ment as an edit.
9. Label edit filled pauses as fillers.
10. Label edit fragments at end of sentence as fluent.
Of the errors that system was able to fix in the CTS train-
ing data, the top 5 rules were responsible for correcting
86%, the top ten rules, for 94% and the top twenty, for
96%.
All systems were evaluated using rteval (Rich Tran-
scription Evaluation) version 2.3 (Kubala and Srivastava,
2003). Rteval aligns the system output to the annotated
reference transcripts in such a way as to minimize the lex-
eme error rate. The error rate is the number of disfluency
errors (insertions and deletions) divided by the number of
disfluent tokens in the reference transcript. Edit and filler
errors are calculated separately. The results of the evalu-
ation are shown in Table 1. Most of the small differences
in the CTS results were not found to be significantly dif-
ferent.
Data System Edit Err Filler Err
CTS Reference A 68.0% 18.1%
B 59.0% 18.2%
C 75.1% 23.2%
CTS Speech A 87.9% 48.8%
B 87.5% 46.9%
C 88.5% 51.0%
BNews Reference A 45.3% 6.5%
B 44.2% 7.9%
BNews Speech A 93.9% 57.2%
B 96.1% 50.4%
Table 1: Disfluency Detection Results
5 Error Analysis
It is clear from the discrepancies between the reference
and speech condition that a large portion of the errors (a
majority except in the case of edit detection for CTS) are
due to errors in the STT (Speech-To-Text). This is most
notable for fillers in broadcast news where the error rate
for our system increases from 6.5% to 57.2%. Such a
trend can be seen for the other systems, indicating that?
even with prosodic models?the other systems were not
more robust to the lexical errors.
All three systems produced comparable results on all
of the conditions, with the only large exception being edit
detection for CTS Reference, where System B had an er-
ror rate of 59% compared to our system?s error rate of
68%.3
The speech output condition suffers from several types
of errors due to errors in the transcript produced by the
speech transcription system. First, the system can output
the wrong word causing it to be misannotated. 27% of our
edit errors in CTS and 19% of our filler errors occurred
when the STT system misrecognized the word. If a filled
pause is hallucinated, the disfluency detection system will
always annotate it as a filler. Errors also occur (19% of
our edit and 12% of our filler error) when the recognizer
deletes a word that was an edit or a filler. Finally, errors
in the context words surrounding disfluencies can affect
disfluency detection as well.
One possible method to correct for the STT errors
would be to train our system on speech output from the
recognizer rather than on reference transcripts. Another
option would be to use a word recognition confidence
score from the recognizer as a feature in the TBL sys-
tem; these were not used. A more systematic analysis
of the errors caused by the recognizer and their effect on
disfluencies also needs to be performed.
System A has a much higher error for edits than fillers,
due, in large part, to the presence of long, difficult to
3This is possibly due to the prosodic model employed by
System B, though no significant gain was shown for the other
conditions.
detect edits. Consider the following word sequence: ?[
and whenever they come out with a warning ] you know
they were coming out with a warning about trains ?. The
portion within square brackets is the edit to be detected.
The difficulty in finding such regions is that the edit itself
appears very fluent. One can identify these regions by
examining what comes after the edit and finding that is
highly similar in content to the edit region. Prosodic fea-
tures can be useful in identifying the interruption point
at which the edit ends, but the degree to which the edit
extends backwards from this point still needs to be iden-
tified. Long distance dependencies should reveal the edit
region, and it is possible that parsing or semantic analysis
of the text would be a useful technique to employ. In ad-
dition there are other cues such as the filler ?you know?
after the edit which can be used to locate these edit re-
gions. Long edit regions (of length four or more) are re-
sponsible for 48% of the edit errors in the CTS reference
condition for our system.
6 Conclusions and Future Work
We have presented a TBL approach to detecting disfluen-
cies that uses primarily lexical features. Our system per-
formed comparably with other systems that relied on both
prosodic and lexical features. Our speaker style (high fre-
quency word) feature enabled us to detect rarer disfluen-
cies, although this was not a large factor in our perfor-
mance. It does appear to be a promising technique for
future research however.
The technique described here shows promise for ex-
tension to disfluency detection in other languages. Since
TBL is a weakly statistical technique, it does not require
a large training corpus and could be more rapidly applied
to new languages. Assuming the basic forms of disflu-
encies in other languages are similar to those in English,
very few modifications would be required.
The longer edits that the system currently misses may
be detectable using parsing, with the intuition that a
parser trained on fluent speech may perform poorly in the
presence of longer edits. Techniques using parse trees to
identify disfluencies have shown success in the past (Hin-
dle, 1983). The system could use portions of the parse
structure as features and could relabel entire subtrees of
the parse tree. Repeated words are another feature of the
longer edits, which we might leverage off of by perform-
ing a weighted alignment of the edit and the repair. Even-
tually it may prove that more elaborate acoustic cues will
be needed to identify these edits, at which point a model
of interruption points could be included as a feature in the
rules learned by the system.
References
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?565.
Eugene Charniak and Mark Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings of the
NAACL.
Mark G. Core and Lenhart K. Schubert. 1999. A model of
speech repairs and other disruptions. In Susan E. Brennan,
Alain Giboin, and David Traum, editors, Working Papers of
the AAAI Fall Symposium on Psychological Models of Com-
munication in Collaborative Systems, pages 48?53, Menlo
Park, California. AAAI.
Peter Heeman and James Allen. 1999. Speech repairs, into-
national phrases, and discourse marker: Modeling speakers?
utterances in spoken dialogue. Computational Linguistics,
25(4).
Donald Hindle. 1983. Deterministic parsing of syntactic non-
fluencies. In Proceedings of ACL, pages 123?128.
Douglas Jones, Florian Wolf, Edward Gibson, Elliott Williams,
Evelina Fedorenko, Douglas Reynolds, and Marc Zissman.
2003. Measuring the readability of automatic speech-to-text
transcripts. In Proceedings of Eurospeech, Geneva.
Francis Kubala and Amit Srivastava. 2003. A Framework for
Evaluating Rich Transcription Technology. BBN Ears Web-
site. http://www.speech.bbn.com/ears.
Yang Liu, Elizabeth Shriberg, and Andreas Stolcke. 2003.
Automatic disfluency identification in coversational speech
using multiple knowledge sources. In Proceedings of Eu-
rospeech, Geneva.
Christine Nakatani and Julia Hirschberg. 1994. A corpus-based
study of repair cue in spontaneous speech. Journal of the
Acoustical Society of America, 95(3):160?1616.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In ACL-SIGDAT Proceedings of the
Conference on Empirical Methods in Natural Language Pro-
cessing, pages 133?142, Philadelphia, PA.
Elizabeth Shriberg, Andreas Stolcke, and Dan Baron. 2001.
Can prosody aid the automatic processing of multi-party
meetings? evidence from predicting punctuation, disfluen-
cies, and overlapping speech. In Proceedings of ISCA Tuto-
rial and Research Workshop on Prosody in Speech Recogni-
tion and Understanding, pages 139?146, Red Bank, NJ.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates, Mari Os-
tendorf, Dilek Hakkani, Madelaine Plauche, Gokhan Tur,
and Yu Lu. 1998. Automatic detection of sentence bound-
aries and disfluencies based on recognized words. In Pro-
ceedings of the ICSLP, volume 5, pages 2247?2250, Sydney,
Australia.
Stephanie Strassel. 2003. Guidelines for RT-03 Transcription
? Version 2.2. Linguistic Data Consortium, Universitry of
Pennsylvannia.
Charles Wayne. 2003. Effective, Affordable, Reusable Speech-
to-Text (EARS). Official web site for DARPA/EARS Pro-
gram. http://www.darpa.muk/iao/EARS.htm.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 161?168,
Sydney, July 2006. c?2006 Association for Computational Linguistics
PCFGs with Syntactic and Prosodic Indicators of Speech Repairs
John Halea Izhak Shafranb Lisa Yungc
Bonnie Dorrd Mary Harperde Anna Krasnyanskayaf Matthew Leaseg
Yang Liuh Brian Roarki Matthew Snoverd Robin Stewartj
a Michigan State University; b,c Johns Hopkins University; d University of Maryland, College Park; e Purdue University
f UCLA; g Brown University; h University of Texas at Dallas; i Oregon Health & Sciences University; j Williams College
Abstract
A grammatical method of combining two
kinds of speech repair cues is presented.
One cue, prosodic disjuncture, is detected
by a decision tree-based ensemble clas-
sifier that uses acoustic cues to identify
where normal prosody seems to be inter-
rupted (Lickley, 1996). The other cue,
syntactic parallelism, codifies the expec-
tation that repairs continue a syntactic
category that was left unfinished in the
reparandum (Levelt, 1983). The two cues
are combined in a Treebank PCFG whose
states are split using a few simple tree
transformations. Parsing performance on
the Switchboard and Fisher corpora sug-
gests that these two cues help to locate
speech repairs in a synergistic way.
1 Introduction
Speech repairs, as in example (1), are one kind
of disfluent element that complicates any sort
of syntax-sensitive processing of conversational
speech.
(1) and [ the first kind of invasion of ] the first
type of privacy seemed invaded to me
The problem is that the bracketed reparan-
dum region (following the terminology of Shriberg
(1994)) is approximately repeated as the speaker
The authors are very grateful for Eugene Charniak?s help
adapting his parser. We also thank the Center for Language
and Speech processing at Johns Hopkins for hosting the sum-
mer workshop where much of this work was done. This
material is based upon work supported by the National Sci-
ence Foundation (NSF) under Grant No. 0121285. Any opin-
ions, findings and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the views of the NSF.
?repairs? what he or she has already uttered.
This extra material renders the entire utterance
ungrammatical?the string would not be gener-
ated by a correct grammar of fluent English. In
particular, attractive tools for natural language
understanding systems, such as Treebank gram-
mars for written corpora, naturally lack appropri-
ate rules for analyzing these constructions.
One possible response to this mismatch be-
tween grammatical resources and the brute facts
of disfluent speech is to make one look more
like the other, for the purpose of parsing. In
this separate-processing approach, reparanda are
located through a variety of acoustic, lexical or
string-based techniques, then excised before sub-
mission to a parser (Stolcke and Shriberg, 1996;
Heeman and Allen, 1999; Spilker et al, 2000;
Johnson and Charniak, 2004). The resulting
parse tree then has the reparandum re-attached in
a standardized way (Charniak and Johnson, 2001).
An alternative strategy, adopted in this paper, is
to use the same grammar to model fluent speech,
disfluent speech, and their interleaving.
Such an integrated approach can use syntac-
tic properties of the reparandum itself. For in-
stance, in example (1) the reparandum is an
unfinished noun phrase, the repair a finished
noun phrase. This sort of phrasal correspon-
dence, while not absolute, is strong in conver-
sational speech, and cannot be exploited on the
separate-processing approach. Section 3 applies
metarules (Weischedel and Sondheimer, 1983;
McKelvie, 1998a; Core and Schubert, 1999) in
recognizing these correspondences using standard
context-free grammars.
At the same time as it defies parsing, con-
versational speech offers the possibility of lever-
aging prosodic cues to speech repairs. Sec-
161
Figure 1: The pause between two or s and the glottalization at the end of the first makes it easy for a
listener to identify the repair.
tion 2 describes a classifier that learns to label
prosodic breaks suggesting upcoming disfluency.
These marks can be propagated up into parse
trees and used in a probabilistic context-free gram-
mar (PCFG) whose states are systematically split
to encode the additional information.
Section 4 reports results on Switchboard (God-
frey et al, 1992) and Fisher EARS RT04F data,
suggesting these two features can bring about in-
dependent improvements in speech repair detec-
tion. Section 5 suggests underlying linguistic and
statistical reasons for these improvements. Sec-
tion 6 compares the proposed grammatical method
to other related work, including state of the art
separate-processing approaches. Section 7 con-
cludes by indicating a way that string- and tree-
based approaches to reparandum identification
could be combined.
2 Prosodic disjuncture
Everyday experience as well as acoustic anal-
ysis suggests that the syntactic interruption in
speech repairs is typically accompanied by a
change in prosody (Nakatani and Hirschberg,
1994; Shriberg, 1994). For instance, the spectro-
gram corresponding to example (2), shown in Fig-
ure 1,
(2) the jehovah?s witness or [ or ] mormons or
someone
reveals a noticeable pause between the occurrence
of the two ors, and an unexpected glottalization at
the end of the first one. Both kinds of cues have
been advanced as explanations for human listen-
ers? ability to identify the reparandum even before
the repair occurs.
Retaining only the second explanation, Lickley
(1996) proposes that there is no ?edit signal? per se
but that repair is cued by the absence of smooth
formant transitions and lack of normal juncture
phenomena.
One way to capture this notion in the syntax
is to enhance the input with a special disjunc-
ture symbol. This symbol can then be propa-
gated in the grammar, as illustrated in Figure 2.
This work uses a suffix ?+ to encode the percep-
tion of abnormal prosody after a word, along with
phrasal -BRK tags to decorate the path upwards to
reparandum constituents labeled EDITED. Such
NP
NP EDITED CC NP
NP NNP CC?BRK or NNPS
DT NNP POS witness
the jehovah ?s
or~+ mormons
Figure 2: Propagating BRK, the evidence of dis-
fluent juncture, from acoustics to syntax.
disjuncture symbols are identified in the ToBI la-
beling scheme as break indices (Price et al, 1991;
Silverman et al, 1992).
The availability of a corpus annotated with
ToBI labels makes it possible to design a break
index classifier via supervised training. The cor-
pus is a subset of the Switchboard corpus, con-
sisting of sixty-four telephone conversations man-
ually annotated by an experienced linguist accord-
ing to a simplified ToBI labeling scheme (Osten-
dorf et al, 2001). In ToBI, degree of disjuncture
is indicated by integer values from 0 to 4, where
a value of 0 corresponds to clitic and 4 to a major
phrase break. In addition, a suffix p denotes per-
ceptually disfluent events reflecting, for example,
162
hesitation or planning. In conversational speech
the intermediate levels occur infrequently and the
break indices can be broadly categorized into three
groups, namely, 1, 4 and p as in Wong et al
(2005).
A classifier was developed to predict three
break indices at each word boundary based on
variations in pitch, duration and energy asso-
ciated with word, syllable or sub-syllabic con-
stituents (Shriberg et al, 2005; Sonmez et al,
1998). To compute these features, phone-level
time-alignments were obtained from an automatic
speech recognition system. The duration of these
phonological constituents were derived from the
ASR alignment, while energy and pitch were com-
puted every 10ms with snack, a public-domain
sound toolkit (Sjlander, 2001). The duration, en-
ergy, and pitch were post-processed according to
stylization procedures outlined in Sonmez et al
(1998) and normalized to account for variability
across speakers.
Since the input vector can have missing val-
ues such as the absence of pitch during unvoiced
sound, only decision tree based classifiers were
investigated. Decision trees can handle missing
features gracefully. By choosing different com-
binations of splitting and stopping criteria, an
ensemble of decision trees was built using the
publicly-available IND package (Buntine, 1992).
These individual classifiers were then combined
into ensemble-based classifiers.
Several classifiers were investigated for detect-
ing break indices. On ten-fold cross-validation,
a bagging-based classifier (Breiman, 1996) pre-
dicted prosodic breaks with an accuracy of 83.12%
while chance was 67.66%. This compares favor-
ably with the performance of the supervised classi-
fiers on a similar task in Wong et al (2005). Ran-
dom forests and hidden Markov models provide
marginal improvements at considerable computa-
tional cost (Harper et al, 2005).
For speech repair, the focus is on detecting dis-
fluent breaks. The precision and recall trade-off
on its detection can be adjusted using a thresh-
old on the posterior probability of predicting ?p?,
as shown in Figure 3.
In essence, the large number of acoustic and
prosodic features related to disfluency are encoded
via the ToBI label ?p?, and provided as additional
observations to the PCFG. This is unlike previous
work on incorporating prosodic information (Gre-
00.10.20.30.40.50.6 0
0.1
0.2
0.3
0.4
0.5
0.6
Probability of Miss
Probab
ility of 
False 
Alarm
Figure 3: DET curve for detecting disfluent breaks
from acoustics.
gory et al, 2004; Lease et al, 2005; Kahn et al,
2005) as described further in Section 6.
3 Syntactic parallelism
The other striking property of speech repairs is
their parallel character: subsequent repair regions
?line up? with preceding reparandum regions. This
property can be harnessed to better estimate the
length of the reparandum by considering paral-
lelism from the perspective of syntax. For in-
stance, in Figure 4(a) the unfinished reparandum
noun phrase is repaired by another noun phrase ?
the syntactic categories are parallel.
3.1 Levelt?s WFR and Conjunction
The idea that the reparandum is syntactically par-
allel to the repair can be traced back to Levelt
(1983). Examining a corpus of Dutch picture de-
scriptions, Levelt proposes a bi-conditional well-
formedness rule for repairs (WFR) that relates the
structure of repairs to the structure of conjunc-
tions. The WFR conceptualizes repairs as the con-
junction of an unfinished reparandum string (?)
with a properly finished repair (?). Its original
formulation, repeated here, ignores optional inter-
regna like ?er? or ?I mean.?
Well-formedness rule for repairs (WFR) A re-
pair ???? is well-formed if and only if there
is a string ? such that the string ??? and? ??
is well-formed, where ? is a completion of
the constituent directly dominating the last
element of ?. (and is to be deleted if that
last element is itself a sentence connective)
In other words, the string ? is a prefix of a phrase
whose completion, ??if it were present?would
163
render the whole phrase ?? grammatically con-
joinable with the repair ?. In example (1) ? is the
string ?the first kind of invasion of?, ? is ?the first
type of privacy? and ? is probably the single word
?privacy.?
This kind of conjoinability typically requires
the syntactic categories of the conjuncts to be the
same (Chomsky, 1957, 36). That is, a rule schema
such as (2) where X is a syntactic category, is pre-
ferred over one where X is not constrained to be
the same on either side of the conjunction.
X ? X Conj X (2)
If, as schema (2) suggests, conjunction does fa-
vor like-categories, and, as Levelt suggests, well-
formed repairs are conjoinable with finished ver-
sions of their reparanda, then the syntactic cate-
gories of repairs ought to match the syntactic cat-
egories of (finished versions of) reparanda.
3.2 A WFR for grammars
Levelt?s WFR imposes two requirements on a
grammar
? distinguishing a separate category of ?unfin-
ished? phrases
? identifying a syntactic category for reparanda
Both requirements can be met by adapting Tree-
bank grammars to mirror the analysis of McK-
elvie1 (1998a; 1998b). McKelvie derives phrase
structure rules for speech repairs from fluent rules
by adding a new feature called abort that can
take values true and false. For a given gram-
mar rule of the form
A ? B C
a metarule creates other rules of the form
A [abort = Q] ?
B [abort = false] C [abort = Q]
where Q is a propositional variable. These rules
say, in effect, that the constituent A is aborted just
in case the last daughter C is aborted. Rules that
don?t involve a constant value for Q ensure that the
same value appears on parents and children. The
1McKelvie?s metarule approach declaratively expresses
Hindle?s (1983) Stack Editor and Category Copy Editor rules.
This classic work effectively states the WFR as a program for
the Fidditch deterministic parser.
WFR is then implemented by rule schemas such
as (3)
X ? X [abort = true] (AFF) X (3)
that permit the optional interregnum AFF to con-
join an unfinished X-phrase (the reparandum) with
a finished X-phrase (the repair) that comes after it.
3.3 A WFR for Treebanks
McKelvie?s formulation of Levelt?s WFR can be
applied to Treebanks by systematically recoding
the annotations to indicate which phrases are un-
finished and to distinguish matching from non-
matching repairs.
3.3.1 Unfinished phrases
Some Treebanks already mark unfinished
phrases. For instance, the Penn Treebank pol-
icy (Marcus et al, 1993; Marcus et al, 1994) is
to annotate the lowest node that is unfinished with
an -UNF tag as in Figure 4(a).
It is straightforward to propagate this mark up-
wards in the tree from wherever it is annotated to
the nearest enclosing EDITED node, just as -BRK
is propagated upwards from disjuncture marks on
individual words. This percolation simulates the
action of McKelvie?s [abort = true]. The re-
sulting PCFG is one in which distributions on
phrase structure rules with ?missing? daughters are
segregated from distributions on ?complete? rules.
3.4 Reparanda categories
The other key element of Levelt?s WFR is the
idea of conjunction of elements that are in some
sense the same. In the Penn Treebank annota-
tion scheme, reparanda always receive the label
EDITED. This means that the syntactic category
of the reparandum is hidden from any rule which
could favor matching it with that of the repair.
Adding an additional mark on this EDITED node
(a kind of daughter annotation) rectifies the situ-
ation, as depicted in Figure 4(b), which adds the
notation -childNP to a tree in which the unfin-
ished tags have been propagated upwards. This
allows a Treebank PCFG to represent the general-
ization that speech repairs tend to respect syntactic
category.
4 Results
Three kinds of experiments examined the effec-
tiveness of syntactic and prosodic indicators of
164
SCC EDITED NP
and NP NP
NP PP
DT JJ NN IN NP
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(a) The lowest unfinished node is given.
S
CC EDITED?childNP NP
and NP?UNF NP
NP PP?UNF
DT JJ NN IN NP?UNF
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(b) -UNF propagated, daughter-annotated Switchboard tree
Figure 4: Input (a) and output (b) of tree transformations.
speech repairs. The first two use the CYK algo-
rithm to find the most likely parse tree on a gram-
mar read-off from example trees annotated as in
Figures 2 and 4. The third experiment measures
the benefit from syntactic indicators alone in Char-
niak?s lexicalized parser (Charniak, 2000). The ta-
bles in subsections 4.1, 4.2, and 4.3 summarize
the accuracy of output parse trees on two mea-
sures. One is the standard Parseval F-measure,
which tracks the precision and recall for all labeled
constituents as compared to a gold-standard parse.
The other measure, EDIT-finding F, restricts con-
sideration to just constituents that are reparanda. It
measures the per-word performance identifying a
word as dominated by EDITED or not. As in pre-
vious studies, reference transcripts were used in all
cases. A check (
?
) indicates an experiment where
prosodic breaks where automatically inferred by
the classifier described in section 2, whereas in the
(?) rows no prosodic information was used.
4.1 CYK on Fisher
Table 1 summarizes the accuracy of a stan-
dard CYK parser on the newly-treebanked
Fisher corpus (LDC2005E15) of phone conver-
sations, collected as part of the DARPA EARS
program. The parser was trained on the entire
Switchboard corpus (ca. 107K utterances) then
tested on the 5368-utterance ?dev2? subset of the
Fisher data. This test set was tagged using MX-
POST (Ratnaparkhi, 1996) which was itself trained
on Switchboard. Finally, as described in section 2
these tags were augmented with a special prosodic
break symbol if the decision tree rated the proba-
bility a ToBI ?p? symbol higher than the threshold
value of 0.75.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 66.54 22.9?
66.08 26.1
daughter annotation ? 66.41 29.4? 65.81 31.6
-UNF propagation ? 67.06 31.5? 66.45 34.8
both ? 69.21 40.2? 67.02 40.6
Table 1: Improvement on Fisher, MXPOSTed tags.
The Fisher results in Table 1 show that syntac-
tic and prosodic indicators provide different kinds
of benefits that combine in an additive way. Pre-
sumably because of state-splitting, improvement
in EDIT-finding comes at the cost of a small decre-
ment in overall parsing performance.
4.2 CYK on Switchboard
Table 2 presents the results of similar experi-
ments on the Switchboard corpus following the
165
train/dev/test partition of Charniak and Johnson
(2001). In these experiments, the parser was given
correct part-of-speech tags as input.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 70.92 18.2?
69.98 22.5
daughter annotation ? 71.13 25.0? 70.06 25.5
-UNF propagation ? 71.71 31.1? 70.36 30.0
both ? 71.16 41.7? 71.05 36.2
Table 2: Improvement on Switchboard, gold tags.
The Switchboard results demonstrate independent
improvement from the syntactic annotations. The
prosodic annotation helps on its own and in com-
bination with the daughter annotation that imple-
ments Levelt?s WFR.
4.3 Lexicalized parser
Finally, Table 3 reports the performance of Char-
niak?s non-reranking, lexicalized parser on the
Switchboard corpus, using the same test/dev/train
partition.
Annotation Parseval F EDIT F
baseline 83.86 57.6
daughter annotation 80.85 67.2
-UNF propagation 81.68 64.7
both 80.16 70.0
flattened EDITED 82.13 64.4
Table 3: Charniak as an improved EDIT-finder.
Since Charniak?s parser does its own tagging,
this experiment did not examine the utility of
prosodic disjuncture marks. However, the com-
bination of daughter annotation and -UNF prop-
agation does lead to a better grammar-based
reparandum-finder than parsers trained on flat-
tened EDITED regions. More broadly, the re-
sults suggest that Levelt?s WFR is synergistic with
the kind of head-to-head lexical dependencies that
Charniak?s parser uses.
5 Discussion
The pattern of improvement in tables 1, 2, and
3 from none or baseline rows where no syntac-
tic parallelism or break index information is used,
to subsequent rows where it is used, suggest why
these techniques work. Unfinished-category an-
notation improves performance by preventing the
grammar of unfinished constituents from being
polluted by the grammar of finished constituents.
Such purification is independent of the fact that
rules with daughters labeled EDITED-childXP
tend to also mention categories labeled XP fur-
ther to the right (or NP and VP, when XP starts
with S). This preference for syntactic parallelism
can be triggered either by externally-suggested
ToBI break indices or grammar rules annotated
with -UNF. The prediction of a disfluent break
could be further improved by POS features and N-
gram language model scores (Spilker et al, 2001;
Liu, 2004).
6 Related Work
There have been relatively few attempts to harness
prosodic cues in parsing. In a spoken language
system for VERBMOBIL task, Batliner and col-
leagues (2001) utilize prosodic cues to dramati-
cally reduce lexical analyses of disfluencies in a
end-to-end real-time system. They tackle speech
repair by a cascade of two stages ? identification of
potential interruption points using prosodic cues
with 90% recall and many false alarms, and the
lexical analyses of their neighborhood. Their ap-
proach, however, does not exploit the synergy be-
tween prosodic and syntactic features in speech re-
pair. In Gregory et al (2004), over 100 real-valued
acoustic and prosodic features were quantized into
a heuristically selected set of discrete symbols,
which were then treated as pseudo-punctuation in
a PCFG, assuming that prosodic cues function like
punctuation. The resulting grammar suffered from
data sparsity and failed to provide any benefits.
Maximum entropy based models have been more
successful in utilizing prosodic cues. For instance,
in Lease et al (2005), interruption point probabil-
ities, predicted by prosodic classifiers, were quan-
tized and introduced as features into a speech re-
pair model along with a variety of TAG and PCFG
features. Towards a clearer picture of the inter-
action with syntax and prosody, this work uses
ToBI to capture prosodic cues. Such a method is
analogous to Kahn et al (2005) but in a genera-
tive framework.
The TAG-based model of Johnson and Charniak
(2004) is a separate-processing approach that rep-
166
resents the state of the art in reparandum-finding.
Johnson and Charniak explicitly model the
crossed dependencies between individual words
in the reparandum and repair regions, intersect-
ing this sequence model with a parser-derived lan-
guage model for fluent speech. This second step
improves on Stolcke and Shriberg (1996) and Hee-
man and Allen (1999) and outperforms the specific
grammar-based reparandum-finders tested in sec-
tion 4. However, because of separate-processing
the TAG channel model?s analyses do not reflect
the syntactic structure of the sentence being ana-
lyzed, and thus that particular TAG-based model
cannot make use of properties that depend on the
phrase structure of the reparandum region. This
includes the syntactic category parallelism dis-
cussed in section 3 but also predicate-argument
structure. If edit hypotheses were augmented to
mention particular tree nodes where the reparan-
dum should be attached, such syntactic paral-
lelism constraints could be exploited in the rerank-
ing framework of Johnson et al (2004).
The approach in section 3 is more closely re-
lated to that of Core and Schubert (1999) who
also use metarules to allow a parser to switch from
speaker to speaker as users interrupt one another.
They describe their metarule facility as a modi-
fication of chart parsing that involves copying of
specific arcs just in case specific conditions arise.
That approach uses a combination of longest-first
heuristics and thresholds rather than a complete
probabilistic model such as a PCFG.
Section 3?s PCFG approach can also be viewed
as a declarative generalization of Roark?s (2004)
EDIT-CHILD function. This function helps an
incremental parser decide upon particular tree-
drawing actions in syntactically-parallel contexts
like speech repairs. Whereas Roark conditions the
expansion of the first constituent of the repair upon
the corresponding first constituent of the reparan-
dum, in the PCFG approach there exists a separate
rule (and thus a separate probability) for each al-
ternative sequence of reparandum constituents.
7 Conclusion
Conventional PCFGs can improve their detection
of speech repairs by incorporating Lickley?s hy-
pothesis about interrupted prosody and by im-
plementing Levelt?s well-formedness rule. These
benefits are additive.
The strengths of these simple tree-based tech-
niques should be combinable with sophisticated
string-based (Johnson and Charniak, 2004; Liu,
2004; Zhang and Weng, 2005) approaches by
applying the methods of Wieling et al (2005)
for constraining parses by externally-suggested
brackets.
References
L. Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
W. Buntine. 1992. Tree classication software. In Tech-
nology 2002: The Third National Technology Trans-
fer Conference and Exposition, Baltimore.
E. Charniak and M. Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings
of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-00, pages 132?
139.
N. Chomsky. 1957. Syntactic Structures. Anua Lin-
guarum Series Minor 4, Series Volume 4. Mouton
de Gruyter, The Hague.
M. G. Core and L. K. Schubert. 1999. A syntactic
framework for speech repairs and other disruptions.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 413?
420.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proceedings of ICASSP,
volume I, pages 517?520, San Francisco.
M. Gregory, M. Johnson, and E. Charniak. 2004.
Sentence-internal prosody does not help parsing the
way punctuation does. In Proceedings of North
American Association for Computational Linguis-
tics.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, and L. Yung. 2005.
Parsing and spoken structural event detection. In
2005 Johns Hopkins Summer Workshop Final Re-
port.
P. A. Heeman and J. F. Allen. 1999. Speech repairs,
intonational phrases and discourse markers: model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?571.
D. Hindle. 1983. Deterministic parsing of syntactic
non-fluencies. In Proceedings of the ACL.
M. Johnson and E. Charniak. 2004. A TAG-based
noisy channel model of speech repairs. In Proceed-
ings of ACL, pages 33?39.
167
M. Johnson, E. Charniak, and M. Lease. 2004. An im-
proved model for recognizing disfluencies in conver-
sational speech. In Proceedings of Rich Transcrip-
tion Workshop.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in
parsing conversational speech. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 233?240.
M. Lease, E. Charniak, and M. Johnson. 2005. Pars-
ing and its applications for conversational speech. In
Proceedings of ICASSP.
W. J. M. Levelt. 1983. Monitoring and self-repair in
speech. Cognitive Science, 14:41?104.
R. J. Lickley. 1996. Juncture cues to disfluency. In
Proceedings the International Conference on Speech
and Language Processing.
Y. Liu. 2004. Structural Event Detection for Rich
Transcription of Speech. Ph.D. thesis, Purdue Uni-
versity.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger. 1994. The Penn Treebank: Annotating
Predicate Argument Structure. In Proceedings of
the 1994 ARPA Human Language Technology Work-
shop.
D. McKelvie. 1998a. SDP ? Spoken Dialog Parser.
ESRC project on Robust Parsing and Part-of-speech
Tagging of Transcribed Speech Corpora, May.
D. McKelvie. 1998b. The syntax of disfluency in spon-
taneous spoken language. ESRC project on Robust
Parsing and Part-of-speech Tagging of Transcribed
Speech Corpora, May.
C. Nakatani and J. Hirschberg. 1994. A corpus-based
study of repair cues in spontaneous speech. Journal
of the Acoustical Society of America, 95(3):1603?
1616, March.
M. Ostendorf, I. Shafran, S. Shattuck-Hufnagel,
L. Carmichael, and W. Byrne. 2001. A prosodically
labelled database of spontaneous speech. In Proc.
ISCA Tutorial and Research Workshop on Prosody
in Speech Recognition and Understanding, pages
119?121.
P. Price, M. Ostendorf, S. Shattuck-Hufnagel, and
C. Fong. 1991. The use of prosody in syntactic
disambiguation. Journal of the Acoustic Society of
America, 90:2956?2970.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of Empirical Methods
in Natural Language Processing Conference, pages
133?141.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
E. Shriberg, L. Ferrer, S. Kajarekar, A. Venkataraman,
and A. Stolcke. 2005. Modeling prosodic feature
sequences for speaker recognition. Speech Commu-
nication, 46(3-4):455?472.
E. Shriberg. 1994. Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, UC Berkeley.
H. F. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,
C. Wightman, P. Price, J. Pierrehumbert, and J. Hir-
shberg. 1992. ToBI: A standard for labeling English
prosody. In Proceedings of ICSLP, volume 2, pages
867?870.
K. Sjlander, 2001. The Snack sound visualization mod-
ule. Royal Institute of Technology in Stockholm.
http://www.speech.kth.se/SNACK.
K. Sonmez, E. Shriberg, L. Heck, and M. Weintraub.
1998. Modeling dynamic prosodic variation for
speaker verification. In Proceedings of ICSLP, vol-
ume 7, pages 3189?3192.
Jo?rg Spilker, Martin Klarner, and Gu?nther Go?rz. 2000.
Processing self-corrections in a speech-to-speech
system. In Wolfgang Wahlster, editor, Verbmobil:
Foundations of speech-to-speech translation, pages
131?140. Springer-Verlag, Berlin.
J. Spilker, A. Batliner, and E. No?th. 2001. How to
repair speech repairs in an end-to-end system. In
R. Lickley and L. Shriberg, editors, Proc. of ISCA
Workshop on Disfluency in Spontaneous Speech,
pages 73?76.
A. Stolcke and E. Shriberg. 1996. Statistical language
modeling for speech disfluencies. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 405?408, At-
lanta, GA.
R. M. Weischedel and N. K. Sondheimer. 1983.
Meta-rules as a basis for processing ill-formed in-
put. American Journal of Computational Linguis-
tics, 9(3-4):161?177.
M. Wieling, M-J. Nederhof, and G. van Noord. 2005.
Parsing partially bracketed input. Talk presented at
Computational Linguistics in the Netherlands.
D. Wong, M. Ostendorf, and J. G. Kahn. 2005. Us-
ing weakly supervised learning to improve prosody
labeling. Technical Report UWEETR-2005-0003,
University of Washington Electrical Engineering
Dept.
Q. Zhang and F. Weng. 2005. Exploring features for
identifying edited regions in disfluent sentences. In
Proceedings of the Nineth International Workshop
on Parsing Technologies, pages 179?185.
168
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 259?268,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Fluency, Adequacy, or HTER?
Exploring Different Human Judgments with a Tunable MT Metric
Matthew Snover?, Nitin Madnani?, Bonnie J. Dorr? ? & Richard Schwartz? ?
?Laboratory for Computational Linguistics and Information Processing
?Institute for Advanced Computer Studies
?University of Maryland, College Park
?Human Language Technology Center of Excellence
?BBN Technologies
{snover,nmadnani,bonnie}@umiacs.umd.edu schwartz@bbn.com
Abstract
Automatic Machine Translation (MT)
evaluation metrics have traditionally been
evaluated by the correlation of the scores
they assign to MT output with human
judgments of translation performance.
Different types of human judgments, such
as Fluency, Adequacy, and HTER, mea-
sure varying aspects of MT performance
that can be captured by automatic MT
metrics. We explore these differences
through the use of a new tunable MT met-
ric: TER-Plus, which extends the Transla-
tion Edit Rate evaluation metric with tun-
able parameters and the incorporation of
morphology, synonymy and paraphrases.
TER-Plus was shown to be one of the
top metrics in NIST?s Metrics MATR
2008 Challenge, having the highest aver-
age rank in terms of Pearson and Spear-
man correlation. Optimizing TER-Plus
to different types of human judgments
yields significantly improved correlations
and meaningful changes in the weight of
different types of edits, demonstrating sig-
nificant differences between the types of
human judgments.
1 Introduction
Since the introduction of the BLEU metric (Pa-
pineni et al, 2002), statistical MT systems have
moved away from human evaluation of their per-
formance and towards rapid evaluation using au-
tomatic metrics. These automatic metrics are
themselves evaluated by their ability to generate
scores for MT output that correlate well with hu-
man judgments of translation quality. Numer-
ous methods of judging MT output by humans
have been used, including Fluency, Adequacy,
and, more recently, Human-mediated Translation
Edit Rate (HTER) (Snover et al, 2006). Fluency
measures whether a translation is fluent, regard-
less of the correct meaning, while Adequacy mea-
sures whether the translation conveys the correct
meaning, even if the translation is not fully flu-
ent. Fluency and Adequacy are frequently mea-
sured together on a discrete 5 or 7 point scale,
with their average being used as a single score
of translation quality. HTER is a more complex
and semi-automatic measure in which humans do
not score translations directly, but rather generate
a new reference translation that is closer to the
MT output but retains the fluency and meaning
of the original reference. This new targeted refer-
ence is then used as the reference translation when
scoring the MT output using Translation Edit Rate
(TER) (Snover et al, 2006) or when used with
other automatic metrics such as BLEU or ME-
TEOR (Banerjee and Lavie, 2005). One of the
difficulties in the creation of targeted references
is a further requirement that the annotator attempt
to minimize the number of edits, as measured by
TER, between the MT output and the targeted ref-
erence, creating the reference that is as close as
possible to the MT output while still being ade-
quate and fluent. In this way, only true errors in
the MT output are counted. While HTER has been
shown to be more consistent and finer grained than
individual human annotators of Fluency and Ade-
quacy, it is much more time consuming and tax-
ing on human annotators than other types of hu-
man judgments, making it difficult and expensive
to use. In addition, because HTER treats all edits
equally, no distinction is made between serious er-
rors (errors in names or missing subjects) and mi-
nor edits (such as a difference in verb agreement
259
or a missing determinator).
Different types of translation errors vary in im-
portance depending on the type of human judg-
ment being used to evaluate the translation. For
example, errors in tense might barely affect the ad-
equacy of a translation but might cause the trans-
lation be scored as less fluent. On the other hand,
deletion of content words might not lower the flu-
ency of a translation but the adequacy would suf-
fer. In this paper, we examine these differences
by taking an automatic evaluation metric and tun-
ing it to these these human judgments and exam-
ining the resulting differences in the parameteri-
zation of the metric. To study this we introduce
a new evaluation metric, TER-Plus (TERp)1 that
improves over the existing Translation Edit Rate
(TER) metric (Snover et al, 2006), incorporating
morphology, synonymy and paraphrases, as well
as tunable costs for different types of errors that
allow for easy interpretation of the differences be-
tween human judgments.
Section 2 summarizes the TER metric and dis-
cusses how TERp improves on it. Correlation re-
sults with human judgments, including indepen-
dent results from the 2008 NIST Metrics MATR
evaluation, where TERp was consistently one of
the top metrics, are presented in Section 3 to show
the utility of TERp as an evaluation metric. The
generation of paraphrases, as well as the effect of
varying the source of paraphrases, is discussed in
Section 4. Section 5 discusses the results of tuning
TERp to Fluency, Adequacy and HTER, and how
this affects the weights of various edit types.
2 TER and TERp
Both TER and TERp are automatic evaluation
metrics for machine translation that score a trans-
lation, the hypothesis, of a foreign language text,
the source, against a translation of the source text
that was created by a human translator, called a
reference translation. The set of possible cor-
rect translations is very large?possibly infinite?
and any single reference translation is just a sin-
gle point in that space. Usually multiple refer-
ence translations, typically 4, are provided to give
broader sampling of the space of correct transla-
tions. Automatic MT evaluation metrics compare
the hypothesis against this set of reference trans-
lations and assign a score to the similarity; higher
1Named after the nickname??terp??of the University of
Maryland, College Park, mascot: the diamondback terrapin.
scores are given to hypotheses that are more simi-
lar to the references.
In addition to assigning a score to a hypothe-
sis, the TER metric also provides an alignment be-
tween the hypothesis and the reference, enabling it
to be useful beyond general translation evaluation.
While TER has been shown to correlate well with
human judgments of translation quality, it has sev-
eral flaws, including the use of only a single ref-
erence translation and the measuring of similarity
only by exact word matches between the hypoth-
esis and the reference. The handicap of using a
single reference can be addressed by the construc-
tion of a lattice of reference translations. Such a
technique has been used with TER to combine the
output of multiple translation systems (Rosti et al,
2007). TERp does not utilize this methodology2
and instead focuses on addressing the exact match-
ing flaw of TER. A brief description of TER is pre-
sented in Section 2.1, followed by a discussion of
how TERp differs from TER in Section 2.2.
2.1 TER
One of the first automatic metrics used to evaluate
automatic machine translation (MT) systems was
Word Error Rate (WER) (Niessen et al, 2000),
which is the standard evaluation metric for Au-
tomatic Speech Recognition. WER is computed
as the Levenshtein (Levenshtein, 1966) distance
between the words of the system output and the
words of the reference translation divided by the
length of the reference translation. Unlike speech
recognition, there are many correct translations for
any given foreign sentence. These correct transla-
tions differ not only in their word choice but also
in the order in which the words occur. WER is
generally seen as inadequate for evaluation for ma-
chine translation as it fails to combine knowledge
from multiple reference translations and also fails
to model the reordering of words and phrases in
translation.
TER addresses the latter failing of WER by al-
lowing block movement of words, called shifts.
within the hypothesis. Shifting a phrase has the
same edit cost as inserting, deleting or substitut-
ing a word, regardless of the number of words
being shifted. While a general solution to WER
with block movement is NP-Complete (Lopresti
2The technique of combining references in this fashion
has not been evaluated in terms of its benefit when correlating
with human judgments. The authors hope to examine and
incorporate such a technique in future versions of TERp.
260
and Tomkins, 1997), TER addresses this by using
a greedy search to select the words to be shifted,
as well as further constraints on the words to be
shifted. These constraints are intended to simu-
late the way in which a human editor might choose
the words to shift. For exact details on these con-
straints, see Snover et al (2006). There are other
automatic metrics that follow the general formu-
lation as TER but address the complexity of shift-
ing in different ways, such as the CDER evaluation
metric (Leusch et al, 2006).
When TER is used with multiple references, it
does not combine the references. Instead, it scores
the hypothesis against each reference individually.
The reference against which the hypothesis has the
fewest number of edits is deemed the closet refer-
ence, and that number of edits is used as the nu-
merator for calculating the TER score. For the de-
nominator, TER uses the average number of words
across all the references.
2.2 TER-Plus
TER-Plus (TERp) is an extension of TER that
aligns words in the hypothesis and reference not
only when they are exact matches but also when
the words share a stem or are synonyms. In ad-
dition, it uses probabilistic phrasal substitutions
to align phrases in the hypothesis and reference.
These phrases are generated by considering possi-
ble paraphrases of the reference words. Matching
using stems and synonyms (Banerjee and Lavie,
2005) and using paraphrases (Zhou et al, 2006;
Kauchak and Barzilay, 2006) have previously been
shown to be beneficial for automatic MT evalu-
ation. Paraphrases have also been shown to be
useful in expanding the number of references used
for parameter tuning (Madnani et al, 2007; Mad-
nani et al, 2008) although they are not used di-
rectly in this fashion within TERp. While all edit
costs in TER are constant, all edit costs in TERp
are optimized to maximize correlation with human
judgments. This is because while a set of constant
weights might prove adequate for the purpose of
measuring translation quality?as evidenced by
correlation with human judgments both for TER
and HTER?they may not be ideal for maximiz-
ing correlation.
TERp uses all the edit operations of TER?
Matches, Insertions, Deletions, Substitutions and
Shifts?as well as three new edit operations: Stem
Matches, Synonym Matches and Phrase Substitu-
tions. TERp identifies words in the hypothesis and
reference that share the same stem using the Porter
stemming algorithm (Porter, 1980). Two words
are determined to be synonyms if they share the
same synonym set according to WordNet (Fell-
baum, 1998). Sequences of words in the reference
are considered to be paraphrases of a sequence of
words in the hypothesis if that phrase pair occurs
in the TERp phrase table. The TERp phrase table
is discussed in more detail in Section 4.
With the exception of the phrase substitutions,
the cost for all other edit operations is the same re-
gardless of what the words in question are. That
is, once the edit cost of an operation is determined
via optimization, that operation costs the same no
matter what words are under consideration. The
cost of a phrase substitution, on the other hand,
is a function of the probability of the paraphrase
and the number of edits needed to align the two
phrases according to TERp. In effect, the proba-
bility of the paraphrase is used to determine how
much to discount the alignment of the two phrases.
Specifically, the cost of a phrase substitution be-
tween the reference phrase, p1 and the hypothesis
phrase p2 is:
cost(p1, p2) =w1+
edit(p1, p2)?
(w2 log(Pr(p1, p2))
+ w3 Pr(p1, p2) + w4)
where w1, w2, w3, and w4 are the 4 free param-
eters of the edit cost, edit(p1, p2) is the edit cost
according to TERp of aligning p1 to p2 (excluding
phrase substitutions) and Pr(p1, p2) is the prob-
ability of paraphrasing p1 as p2, obtained from
the TERp phrase table. The w parameters of the
phrase substitution cost may be negative while still
resulting in a positive phrase substitution cost, as
w2 is multiplied by the log probability, which is al-
ways a negative number. In practice this term will
dominate the phrase substitution edit cost.
This edit cost for phrasal substitutions is, there-
fore, specified by four parameters, w1, w2, w3
and w4. Only paraphrases specified in the TERp
phrase table are considered for phrase substitu-
tions. In addition, the cost for a phrasal substi-
tution is limited to values greater than or equal to
0, i.e., the substitution cost cannot be negative. In
addition, the shifting constraints of TERp are also
relaxed to allow shifting of paraphrases, stems,
and synonyms.
261
In total TERp uses 11 parameters out of which
four represent the cost of phrasal substitutions.
The match cost is held fixed at 0, so that only the
10 other parameters can vary during optimization.
All edit costs, except for the phrasal substitution
parameters, are also restricted to be positive. A
simple hill-climbing search is used to optimize the
edit costs by maximizing the correlation of human
judgments with the TERp score. These correla-
tions are measured at the sentence, or segment,
level. Although it was done for the experiments
described in this paper, optimization could also
be performed to maximize document level correla-
tion ? such an optimization would give decreased
weight to shorter segments as compared to the seg-
ment level optimization.
3 Correlation Results
The optimization of the TERp edit costs, and com-
parisons against several standard automatic eval-
uation metrics, using human judgments of Ade-
quacy is first described in Section 3.1. We then
summarize, in Section 3.2, results of the NIST
Metrics MATR workshop where TERp was eval-
uated as one of 39 automatic metrics using many
test conditions and types of human judgments.
3.1 Optimization of Edit Costs and
Correlation Results
As part of the 2008 NIST Metrics MATR work-
shop (Przybocki et al, 2008), a development sub-
set of translations from eight Arabic-to-English
MT systems submitted to NIST?s MTEval 2006
was released that had been annotated for Ade-
quacy. We divided this development set into an
optimization set and a test set, which we then used
to optimize the edit costs of TERp and compare it
against other evaluation metrics. TERp was op-
timized to maximize the segment level Pearson
correlation with adequacy on the optimization set.
The edit costs determined by this optimization are
shown in Table 1.
We can compare TERp with other metrics by
comparing their Pearson and Spearman corre-
lations with Adequacy, at the segment, docu-
ment and system level. Document level Ade-
quacy scores are determined by taking the length
weighted average of the segment level scores. Sys-
tem level scores are determined by taking the
weighted average of the document level scores in
the same manner.
We compare TERp with BLEU (Papineni et al,
2002), METEOR (Banerjee and Lavie, 2005), and
TER (Snover et al, 2006). The IBM version of
BLEU was used in case insensitive mode with
an ngram-size of 4 to calculate the BLEU scores.
Case insensitivity was used with BLEU as it was
found to have much higher correlation with Ade-
quacy. In addition, we also examined BLEU using
an ngram-size of 2 (labeled as BLEU-2), instead
of the default ngram-size of 4, as it often has a
higher correlation with human judgments. When
using METEOR, the exact matching, porter stem-
ming matching, and WordNet synonym matching
modules were used. TER was also used in case
insensitive mode.
We show the Pearson and Spearman correlation
numbers of TERp and the other automatic metrics
on the optimization set and the test set in Tables 2
and 3. Correlation numbers that are statistically
indistinguishable from the highest correlation, us-
ing a 95% confidence interval, are shown in bold
and numbers that are actually not statistically sig-
nificant correlations are marked with a ?. TERp
has the highest Pearson correlation in all condi-
tions, although not all differences are statistically
significant. When examining the Spearman cor-
relation, TERp has the highest correlation on the
segment and system levels, but performs worse
than METEOR on the document level Spearman
correlatons.
3.2 NIST Metrics MATR 2008 Results
TERp was one of 39 automatic metrics evaluated
in the 2008 NIST Metrics MATR Challenge. In
order to evaluate the state of automatic MT eval-
uation, NIST tested metrics across a number of
conditions across 8 test sets. These conditions in-
cluded segment, document and system level corre-
lations with human judgments of preference, flu-
ency, adequacy and HTER. The test sets included
translations from Arabic-to-English, Chinese-to-
English, Farsi-to-English, Arabic-to-French, and
English-to-French MT systems involved in NIST?s
MTEval 2008, the GALE (Olive, 2005) Phase 2
and Phrase 2.5 program, Transtac January and July
2007, and CESTA run 1 and run 2, covering mul-
tiple genres. The version of TERp submitted to
this workshop was optimized as described in Sec-
tion 3.1. The development data upon which TERp
was optimized was not part of the test sets evalu-
ated in the Challenge.
262
Phrase Substitution
Match Insert Deletion Subst. Stem Syn. Shift w1 w2 w3 w4
0.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18
Table 1: Optimized TERp Edit Costs
Optimization Set Test Set Optimization+Test
Metric Seg Doc Sys Seg Doc Sys Seg Doc Sys
BLEU 0.623 0.867 0.952 0.563 0.852 0.948 0.603 0.861 0.954
BLEU-2 0.661 0.888 0.946 0.591 0.876 0.953 0.637 0.883 0.952
METEOR 0.731 0.894 0.952 0.751 0.904 0.957 0.739 0.898 0.958
TER -0.609 -0.864 -0.957 -0.607 -0.860 -0.959 -0.609 -0.863 -0.961
TERp -0.782 -0.912 -0.996 -0.787 -0.918 -0.985 -0.784 -0.914 -0.994
Table 2: Optimization & Test Set Pearson Correlation Results
Due to the wealth of testing conditions, a sim-
ple overall view of the official MATR08 results re-
leased by NIST is difficult. To facilitate this anal-
ysis, we examined the average rank of each metric
across all conditions, where the rank was deter-
mined by their Pearson and Spearman correlation
with human judgments. To incorporate statistical
significance, we calculated the 95% confidence in-
terval for each correlation coefficient and found
the highest and lowest rank from which the cor-
relation coefficient was statistically indistinguish-
able, resulting in lower and upper bounds of the
rank for each metric in each condition. The aver-
age lower bound, actual, and upper bound ranks
(where a rank of 1 indicates the highest correla-
tion) of the top metrics, as well as BLEU and TER,
are shown in Table 4, sorted by the average upper
bound Pearson correlation. Full descriptions of the
other metrics3, the evaluation results, and the test
set composition are available from NIST (Przy-
bocki et al, 2008).
This analysis shows that TERp was consistently
one of the top metrics across test conditions and
had the highest average rank both in terms of Pear-
son and Spearman correlations. While this anal-
ysis is not comprehensive, it does give a general
idea of the performance of all metrics by syn-
thesizing the results into a single table. There
are striking differences between the Spearman and
Pearson correlations for other metrics, in particu-
lar the CDER metric (Leusch et al, 2006) had the
second highest rank in Spearman correlations (af-
3System description of metrics are also distributed
by AMTA: http://www.amtaweb.org/AMTA2008.
html
ter TERp), but was the sixth ranked metric accord-
ing to the Pearson correlation. In several cases,
TERp was not the best metric (if a metric was the
best in all conditions, its average rank would be 1),
although it performed well on average. In partic-
ular, TERp did significantly better than the TER
metric, indicating the benefit of the enhancements
made to TER.
4 Paraphrases
TERp uses probabilistic phrasal substitutions to
align phrases in the hypothesis with phrases in the
reference. It does so by looking up?in a pre-
computed phrase table?paraphrases of phrases in
the reference and using its associated edit cost as
the cost of performing a match against the hy-
pothesis. The paraphrases used in TERp were ex-
tracted using the pivot-based method as described
in (Bannard and Callison-Burch, 2005) with sev-
eral additional filtering mechanisms to increase
the precision. The pivot-based method utilizes the
inherent monolingual semantic knowledge from
bilingual corpora: we first identify English-to-F
phrasal correspondences, then map from English
to English by following translation units from En-
glish to F and back. For example, if the two En-
glish phrases e1 and e2 both correspond to the
same foreign phrase f, then they may be consid-
ered to be paraphrases of each other with the fol-
lowing probability:
p(e1|e2) ? p(e1|f) ? p(f |e2)
If there are several pivot phrases that link the two
English phrases, then they are all used in comput-
263
Optimization Set Test Set Optimization+Test
Metric Seg Doc Sys Seg Doc Sys Seg Doc Sys
BLEU 0.635 0.816 0.714? 0.550 0.740 0.690? 0.606 0.794 0.738?
BLEU-2 0.643 0.823 0.786? 0.558 0.747 0.690? 0.614 0.799 0.738?
METEOR 0.729 0.886 0.881 0.727 0.853 0.738? 0.730 0.876 0.922
TER -0.630 -0.794 -0.810? -0.630 -0.797 -0.667? -0.631 -0.801 -0.786?
TERp -0.760 -0.834 -0.976 -0.737 -0.818 -0.881 -0.754 -0.834 -0.929
Table 3: MT06 Dev. Optimization & Test Set Spearman Correlation Results
Metric Average Rank by Pearson Average Rank by Spearman
TERp 1.49 6.07 17.31 1.60 6.44 17.76
METEOR v0.7 1.82 7.64 18.70 1.73 8.21 19.33
METEOR ranking 2.39 9.45 19.91 2.18 10.18 19.67
METEOR v0.6 2.42 10.67 19.11 2.47 11.27 19.60
EDPM 2.45 8.21 20.97 2.79 7.61 20.52
CDER 2.93 8.53 19.67 1.69 8.00 18.80
BleuSP 3.67 9.93 21.40 3.16 8.29 20.80
NIST-v11b 3.82 11.13 21.96 4.64 12.29 23.38
BLEU-1 (IBM) 4.42 12.47 22.18 4.98 14.87 24.00
BLEU-4 (IBM) 6.93 15.40 24.69 6.98 14.38 25.11
TER v0.7.25 8.87 16.27 25.29 6.93 17.33 24.80
BLEU-4 v12 (NIST) 10.16 18.02 27.64 10.96 17.82 28.16
Table 4: Average Metric Rank in NIST Metrics MATR 2008 Official Results
ing the probability:
p(e1|e2) ?
?
f ?
p(e1|f ?) ? p(f ?|e2)
The corpus used for extraction was an Arabic-
English newswire bitext containing a million sen-
tences. A few examples of the extracted para-
phrase pairs that were actually used in a run of
TERp on the Metrics MATR 2008 development
set are shown below:
(brief ? short)
(controversy over? polemic about)
(by using power? by force)
(response? reaction)
A discussion of paraphrase quality is presented
in Section 4.1, followed by a brief analysis of the
effect of varying the pivot corpus used by the auto-
matic paraphrase generation upon the correlation
performance of the TERp metric in Section 4.2.
4.1 Analysis of Paraphrase Quality
We analyzed the utility of the paraphrase probabil-
ity and found that it was not always a very reliable
estimate of the degree to which the pair was se-
mantically related. For example, we looked at all
paraphrase pairs that had probabilities greater than
0.9, a set that should ideally contain pairs that are
paraphrastic to a large degree. In our analysis, we
found the following five kinds of paraphrases in
this set:
(a) Lexical Paraphrases. These paraphrase
pairs are not phrasal paraphrases but instead
differ in at most one word and may be con-
sidered as lexical paraphrases for all practical
purposes. While these pairs may not be very
valuable for TERp due to the obvious overlap
with WordNet, they may help in increasing
the coverage of the paraphrastic phenomena
that TERp can handle. Here are some exam-
ples:
(2500 polish troops? 2500 polish soldiers)
(accounting firms? auditing firms)
(armed source? military source)
(b) Morphological Variants. These phrasal
pairs only differ in the morphological form
264
for one of the words. As the examples show,
any knowledge that these pairs may provide
is already available to TERp via stemming.
(50 ton? 50 tons)
(caused clouds? causing clouds)
(syria deny? syria denies)
(c) Approximate Phrasal Paraphrases. This
set included pairs that only shared partial se-
mantic content. Most paraphrases extracted
by the pivot method are expected to be of this
nature. These pairs are not directly beneficial
to TERp since they cannot be substituted for
each other in all contexts. However, the fact
that they share at least some semantic content
does suggest that they may not be entirely
useless either. Examples include:
(mutual proposal? suggest)
(them were exiled? them abroad)
(my parents? my father)
(d) Phrasal Paraphrases. We did indeed find
a large number of pairs in this set that were
truly paraphrastic and proved the most useful
for TERp. For example:
(agence presse? news agency)
(army roadblock? military barrier)
(staff walked out? team withdrew)
(e) Noisy Co-occurrences. There are also pairs
that are completely unrelated and happen
to be extracted as paraphrases based on the
noise inherent in the pivoting process. These
pairs are much smaller in number than the
four sets described above and are not signif-
icantly detrimental to TERp since they are
rarely chosen for phrasal substitution. Exam-
ples:
(counterpart salam? peace)
(regulation dealing? list)
(recall one? deported)
Given this distribution of the pivot-based para-
phrases, we experimented with a variant of TERp
that did not use the paraphrase probability at all
but instead only used the actual edit distance be-
tween the two phrases to determine the final cost
of a phrase substitution. The results for this exper-
iment are shown in the second row of Table 5. We
can see that this variant works as well as the full
version of TERp that utilizes paraphrase probabil-
ities. This confirms our intuition that the proba-
bility computed via the pivot-method is not a very
useful predictor of semantic equivalence for use in
TERp.
4.2 Varying Paraphrase Pivot Corpora
To determine the effect that the pivot language
might have on the quality and utility of the ex-
tracted paraphrases in TERp, we used paraphrase
pairsmade available by Callison-Burch (2008).
These paraphrase pairs were extracted from Eu-
roparl data using each of 10 European languages
(German, Italian, French etc.) as a pivot language
separately and then combining the extracted para-
phrase pairs. Callison-Burch (2008) also extracted
and made available syntactically constrained para-
phrase pairs from the same data that are more
likely to be semantically related.
We used both sets of paraphrases in TERp as al-
ternatives to the paraphrase pairs that we extracted
from the Arabic newswire bitext. The results are
shown in the last four rows of Table 5 and show
that using a pivot language other than the one that
the MT system is actually translating yields results
that are almost as good. It also shows that the
syntactic constraints imposed by Callison-Burch
(2008) on the pivot-based paraphrase extraction
process are useful and yield improved results over
the baseline pivot-method. The results further sup-
port our claim that the pivot paraphrase probability
is not a very useful indicator of semantic related-
ness.
5 Varying Human Judgments
To evaluate the differences between human judg-
ment types we first align the hypothesis to the ref-
erences using a fixed set of edit costs, identical to
the weights in Table 1, and then optimize the edit
costs to maximize the correlation, without realign-
ing. The separation of the edit costs used for align-
ment from those used for scoring allows us to re-
move the confusion of edit costs selected for align-
ment purposes from those selected to increase cor-
relation.
For Adequacy and Fluency judgments, the
MTEval 2002 human judgement set4 was used.
This set consists of the output of ten MT sys-
tems, 3 Arabic-to-English systems and 7 Chinese-
4Distributed to the authors by request from NIST.
265
Pearson Spearman
Paraphrase Setup Seg Doc Sys Seg Doc Sys
Arabic pivot -0.787 -0.918 -0.985 -0.737 -0.818 -0.881
Arabic pivot and no prob -0.787 -0.933 -0.986 -0.737 -0.841 -0.881
Europarl pivot -0.775 -0.940 -0.983 -0.738 -0.865 -0.905
Europarl pivot and no prob -0.775 -0.940 -0.983 -0.737 -0.860 -0.905
Europarl pivot and syntactic constraints -0.781 -0.941 -0.985 -0.739 -0.859 -0.881
Europarl pivot, syntactic constraints and no prob -0.779 -0.946 -0.985 -0.737 -0.866 -0.976
Table 5: Results on the NIST MATR 2008 test set for several variations of paraphrase usage.
Human Phrase Substitution
Judgment Match Insert Deletion Subst. Stem Syn. Shift w1 w2 w3 w4
Alignment 0.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18
Adequacy 0.0 0.18 1.42 1.71 0.0 0.0 0.19 -0.38 -0.03 0.22 0.47
Fluency 0.0 0.12 1.37 1.81 0.0 0.0 0.43 -0.63 -0.07 0.12 0.46
HTER 0.0 0.84 0.76 1.55 0.90 0.75 1.07 -0.03 -0.17 -0.08 -0.09
Table 6: Optimized Edit Costs
to-English systems, consisting of a total, across
all systems and both language pairs, of 7,452 seg-
ments across 900 documents. To evaluate HTER,
the GALE (Olive, 2005) 2007 (Phase 2.0) HTER
scores were used. This set consists of the out-
put of 6 MT systems, 3 Arabic-to-English systems
and 3 Chinese-to-English systems, although each
of the systems in question is the product of system
combination. The HTER data consisted of a total,
across all systems and language pairs, of 16,267
segments across a total of 1,568 documents. Be-
cause HTER annotation is especially expensive
and difficult, it is rarely performed, and the only
source, to the authors? knowledge, of available
HTER annotations is on GALE evaluation data for
which no Fluency and Adequacy judgments have
been made publicly available.
The edit costs learned for each of these human
judgments, along with the alignment edit costs are
shown in Table 6. While all three types of human
judgements differ from the alignment costs used
in alignment, the HTER edit costs differ most sig-
nificantly. Unlike Adequacy and Fluency which
have a low edit cost for insertions and a very high
cost for deletions, HTER has a balanced cost for
the two edit types. Inserted words are strongly pe-
nalized against in HTER, as opposed to in Ade-
quacy and Fluency, where such errors are largely
forgiven. Stem and synonym edits are also penal-
ized against while these are considered equivalent
to a match for both Adequacy and Fluency. This
penalty against stem matches can be attributed to
Fluency requirements in HTER that specifically
penalize against incorrect morphology. The cost
of shifts is also increased in HTER, strongly penal-
izing the movement of phrases within the hypoth-
esis, while Adequacy and Fluency give a much
lower cost to such errors. Some of the differences
between HTER and both fluency and adequacy
can be attributed to the different systems used. The
MT systems evaluated with HTER are all highly
performing state of the art systems, while the sys-
tems used for adequacy and fluency are older MT
systems.
The differences between Adequacy and Fluency
are smaller, but there are still significant differ-
ences. In particular, the cost of shifts is over twice
as high for the fluency optimized system than the
adequacy optimized system, indicating that the
movement of phrases, as expected, is only slightly
penalized when judging meaning, but can be much
more harmful to the fluency of a translation. Flu-
ency however favors paraphrases more strongly
than the edit costs optimized for adequacy. This
might indicate that paraphrases are used to gener-
ate a more fluent translation although at the poten-
tial loss of meaning.
266
6 Discussion
We introduced a new evaluation metric, TER-Plus,
and showed that it is competitive with state-of-the-
art evaluation metrics when its predictions are cor-
related with human judgments. The inclusion of
stem, synonym and paraphrase edits allows TERp
to overcome some of the weaknesses of the TER
metric and better align hypothesized translations
with reference translations. These new edit costs
can then be optimized to allow better correlation
with human judgments. In addition, we have ex-
amined the use of other paraphrasing techniques,
and shown that the paraphrase probabilities esti-
mated by the pivot-method may not be fully ad-
equate for judgments of whether a paraphrase in
a translation indicates a correct translation. This
line of research holds promise as an external eval-
uation method of various paraphrasing methods.
However promising correlation results for an
evaluation metric may be, the evaluation of the
final output of an MT system is only a portion
of the utility of an automatic translation metric.
Optimization of the parameters of an MT system
is now done using automatic metrics, primarily
BLEU. It is likely that some features that make an
evaluation metric good for evaluating the final out-
put of a system would make it a poor metric for use
in system tuning. In particular, a metric may have
difficulty distinguishing between outputs of an MT
system that been optimized for that same metric.
BLEU, the metric most frequently used to opti-
mize systems, might therefore perform poorly in
evaluation tasks compared to recall oriented met-
rics such as METEOR and TERp (whose tuning
in Table 1 indicates a preference towards recall).
Future research into the use of TERp and other
metrics as optimization metrics is needed to better
understand these metrics and the interaction with
parameter optimization.
Finally, we explored the difference between
three types of human judgments that are often
used to evaluate both MT systems and automatic
metrics, by optimizing TERp to these human
judgments and examining the resulting edit costs.
While this can make no judgement as to the pref-
erence of one type of human judgment over an-
other, it indicates differences between these hu-
man judgment types, and in particular the differ-
ence between HTER and Adequacy and Fluency.
This exploration is limited by the the lack of a
large amount of diverse data annotated for all hu-
man judgment types, as well as the small num-
ber of edit types used by TERp. The inclusion
of additional more specific edit types could lead
to a more detailed understanding of which trans-
lation phenomenon and translation errors are most
emphasized or ignored by which types of human
judgments.
Acknowledgments
This work was supported, in part, by BBN Tech-
nologies under the GALE Program, DARPA/IPTO
Contract No. HR0011-06-C-0022 and in part by
the Human Language Technology Center of Ex-
cellence.. TERp is available on the web for down-
load at: http://www.umiacs.umd.edu/?snover/terp/.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL 2005 Workshop on Intrinsic and
Extrinsic Evaulation Measures for MT and/or Sum-
marization.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005),
pages 597?604, Ann Arbor, Michigan, June.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 196?
205, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
http://www.cogsci.princeton.edu/?wn [2000,
September 7].
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the ACL, pages 455?
462.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using Block
Movements. In Proceedings of the 11th Confer-
enceof the European Chapter of the Association for
Computational Linguistics (EACL 2006).
V. I. Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions, and Reversals. Soviet
Physics Doklady, 10:707?710.
267
Daniel Lopresti and Andrew Tomkins. 1997. Block
edit models for approximate string matching. Theo-
retical Computer Science, 181(1):159?179, July.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Workshop on Statistical Machine
Translation, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are Multiple Reference
Translations Necessary? Investigating the Value
of Paraphrased Reference Translations in Parameter
Optimization. In Proceedings of the Eighth Confer-
ence of the Association for Machine Translation in
the Americas, October.
S. Niessen, F.J. Och, G. Leusch, and H. Ney. 2000. An
evaluation tool for machine translation: Fast evalua-
tion for MT research. In Proceedings of the 2nd In-
ternational Conference on Language Resources and
Evaluation (LREC-2000), pages 39?45.
Joseph Olive. 2005. Global Autonomous Language
Exploitation (GALE). DARPA/IPTO Proposer In-
formation Pamphlet.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Traslation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Martin F. Porter. 1980. An algorithm for suffic strip-
ping. Program, 14(3):130?137.
Mark Przybocki, Kay Peterson, and Sebas-
tian Bronsart. 2008. Official results
of the NIST 2008 ?Metrics for MAchine
TRanslation? Challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/,
October.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings
of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 312?319, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas.
Liang Zhou, Chon-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating Machine Translation Results with
Paraphrase Support. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2006), pages 77?84.
268
  
	 Unsupervised Learning of Morphology Using a Novel Directed Search
Algorithm: Taking the First Step
Matthew G. Snover and Gaja E. Jarosz and Michael R. Brent
Department of Computer Science
Washington University
St Louis, MO, USA, 63130-4809
 
ms9, gaja, brent  @cs.wustl.edu
Abstract
This paper describes a system for the un-
supervised learning of morphological suf-
fixes and stems from word lists. The sys-
tem is composed of a generative probabil-
ity model and a novel search algorithm.
By examining morphologically rich sub-
sets of an input lexicon, the search identi-
fies highly productive paradigms. Quanti-
tative results are shown by measuring the
accuracy of the morphological relations
identified. Experiments in English and
Polish, as well as comparisons with other
recent unsupervised morphology learning
algorithms demonstrate the effectiveness
of this technique.
1 Introduction
There are numerous languages for which no anno-
tated corpora exist but for which there exists an
abundance of unannotated orthographic text. It is
extremely time-consuming and expensive to cre-
ate a corpus annotated for morphological structure
by hand. Furthermore, a preliminary, conservative
analysis of a language?s morphology would be use-
ful in discovering linguistic structure beyond the
word level. For instance, morphology may provide
information about the syntactic categories to which
words belong, knowledge which could be used by
parsing algorithms. From a cognitive perspective, it
is crucial to determine whether the amount of infor-
mation found in pure speech is sufficient for discov-
ering the level of morphological structure that chil-
dren are able to find without any direct supervision.
Thus, we believe the task of automatically discover-
ing a conservative estimate of the orthographically-
based morphological structure in a language inde-
pendent manner is a useful one.
Additionally, an initial description of a lan-
guage?s morphology could provide a starting
point for supervised morphological mod-
els, such as the memory-based algorithm of
Van den Bosch and Daelemans (1999), which can-
not be used on languages for which annotated data
is unavailable.
During the last decade several minimally super-
vised and unsupervised algorithms that address the
problem have been developed. Gaussier (1999) de-
scribes an explicitly probabilistic system that is
based primarily on spellings. It is an unsupervised
algorithm, but requires the tweaking of parameters
to tune it to the target language. Brent (1993) and
Brent et al (1995), described Minimum Description
Length, (MDL), systems. One approach used only
the spellings of the words; another attempted to find
the set of suffixes in the language used the syntactic
categories from a tagged corpus as well. While both
are unsupervised, the latter is not knowledge free
and requires data that is tagged for part of speech,
making it less suitable for analyzing under examined
languages.
A similar MDL approach is described by
Goldsmith (2001). It is ideal in being both knowl-
edge free and unsupervised. The difficulty lies in
Goldsmith?s liberal definition of morphology which
he uses to evaluate with; a more conservative ap-
proach would seem to be a better hypothesis to boot-
strap from.
We previously, Snover and Brent (2001), pre-
sented a very conservative unsupervised system,
                     July 2002, pp. 11-20.  Association for Computational Linguistics.
        ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,
       Morphological and Phonological Learning: Proceedings of the 6th Workshop of the
which uses a generative probability model and a hill
climbing search. No quantitative studies had been
conducted on it, and it appears that the hill-climbing
search used limits that system?s usefulness. We have
developed a system based on a novel search and
an extension of the previous probability model of
Snover and Brent.
The use of probabilistic models is equivalent to
minimum description length models. Searching for
the most probable hypothesis is just as compelling
as searching for the smallest hypothesis and a model
formulated in one framework can, through some
mathematical manipulation, be reformulated into the
other framework. By taking the negative log of a
probability distribution, one can find the number of
bits required to encode a value according to that dis-
tribution. Our system does not use the minimum de-
scription length principle but could easily be refor-
mulated to do so.
Our goal in designing this system, is to be able to
detect the final stem and suffix break of each word
given a list of the most common words in a language.
We do not distinguish between derivational and in-
flectional suffixation or between the notion of a stem
and a base. Our probability model differs slightly
from that of Snover and Brent (2001), but the main
difference is in the search technique. We find and
analyze subsets of the lexicon to find good solutions
for a small set of words. We then combine these sub-
hypotheses to form a morphological analysis of the
entire input lexicon. We do not attempt to learn pre-
fixes, infixes, or other more complex morphological
systems, such as template-based morphology: we
are attempting to discover the component of many
morphological systems that is strictly concatenative.
Finally, our model does not currently have a mecha-
nism to deal with multiple interpretations of a word,
or to deal with morphological ambiguity.
2 Probability Model
This section introduces a prior probability distribu-
tion over the space of all hypotheses, where a hy-
pothesis is a set of words, each with morphological
split separating the stem and suffix. The distribution
is based on a seven-model model for the generation
of hypothesis, which is heavily based upon the prob-
ability model presented in Snover and Brent (2001),
with steps 1-3 of the generative procedure being the
same. The two models diverge at step 4 with the
pairing of stems and suffixes. Whereas the previ-
ous model paired individual stems with suffixes, our
new model uses the abstract structure of paradigms.
A paradigm is a set of suffixes and the stems that
attach to those suffixes and no others. Each stem is
in exactly one paradigm, and each paradigm has at
least one stem. This is an important improvement
to the model as it takes into account the patterns in
which stems and suffixes attach.
The seven steps are presented below, along with
their probability distributions and a running exam-
ple of how a hypothesis could be generated by this
process. By taking the product over the distributions
from all of the steps of the generative process, one
can calculate the prior probability for any given hy-
pothesis. What is described in this section is a math-
ematical model and not an algorithm intended to be
run.
1. Choose the number of stems,   , according to
the distribution:

 
	



 

(1)
The 	 
  term normalizes the inverse-squared
distribution on the positive integers. The num-
ber of suffixes,  is chosen according to the
same probability distribution. The symbols M
for steMs and X for suffiXes are used through-
out this paper.
Example:   = 5.  = 3.
2. For each stem  , choose its length in letters  ,
according to the inverse squared distribution.
Assuming that the lengths are chosen indepen-
dently and multiplying together their probabil-
ities we have:
Cross-lingual Slot Filling from Comparable Corpora
Matthew Snover, Xiang Li, Wen-Pin Lin, Zheng Chen, Suzanne Tamang,
Mingmin Ge, Adam Lee, Qi Li, Hao Li, Sam Anzaroot, Heng Ji
Computer Science Department
Queens College and Graduate Center
City University of New York
New York, NY 11367, USA
msnover@qc.cuny.edu, hengji@cs.qc.cuny.edu
Abstract
This paper introduces a new task of
crosslingual slot filling which aims to dis-
cover attributes for entity queries from
crosslingual comparable corpora and then
present answers in a desired language. It is
a very challenging task which suffers from
both information extraction and machine
translation errors. In this paper we ana-
lyze the types of errors produced by five
different baseline approaches, and present
a novel supervised rescoring based valida-
tion approach to incorporate global evi-
dence from very large bilingual compara-
ble corpora. Without using any additional
labeled data this new approach obtained
38.5% relative improvement in Precision
and 86.7% relative improvement in Recall
over several state-of-the-art approaches.
The ultimate system outperformed mono-
lingual slot filling pipelines built on much
larger monolingual corpora.
1 Introduction
The slot filling task at NIST TAC Knowledge
Base Population (KBP) track (Ji et al, 2010)
is a relatively new and popular task with the
goal of automatically building profiles of enti-
ties from large amounts of unstructured data,
and using these profiles to populate an existing
knowledge base. These profiles consist of nu-
merous slots such as ?title?, ?parents? for per-
sons and ?top-employees? for organizations. A
variety of approaches have been proposed to ad-
dress both tasks with considerable success; nev-
ertheless, all of the KBP tasks so far have been
limited to monolingual processing. However, as
the shrinking fraction of the world?s Web pages
are written in English, many slot fills can only
be discovered from comparable documents in
foreign languages. By comparable corpora we
mean texts that are about similar topics, but
are not in general translations of each other.
These corpora are naturally available, for ex-
ample, many news agencies release multi-lingual
news articles on the same day. In this paper we
propose a new and more challenging crosslin-
gual slot filling task, to find information for any
English query from crosslingual comparable cor-
pora, and then present its profile in English.
We developed complementary baseline ap-
proaches which combine two difficult problems:
information extraction (IE) and machine trans-
lation (MT). In this paper we conduct detailed
error analysis to understand how we can exploit
comparable corpora to construct more complete
and accurate profiles.
Many correct answers extracted from our
baselines will be reported multiple times in any
external large collection of comparable docu-
ments. We can thus take advantage of such in-
formation redundancy to rescore candidate an-
swers. To choose the best answers we consult
large comparable corpora and corresponding IE
results. We prefer those answers which fre-
quently appear together with the query in cer-
tain IE contexts, including co-occurring names,
coreference links, relations and events. For ex-
ample, we prefer ?South Korea? instead of ?New
York Stock Exchange? as the ?per:employee of ?
answer for ?Roh Moo-hyun? using global ev-
idence from employment relation extraction.
Such global knowledge from comparable corpora
110
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 110?119,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
provides substantial improvement over each in-
dividual baseline system and even state-of-the-
art monolingual slot filling systems. Compared
to previous methods of exploiting comparable
corpora, our approach is novel in multiple as-
pects because it exploits knowledge from: (1)
both local and global statistics; (2) both lan-
guages; and (3) both shallow and deep analysis.
2 Related Work
Sudo et al (2004) found that for a crosslin-
gual single-document IE task, source language
extraction and fact translation performed no-
tably better than machine translation and tar-
get language extraction. We observed the same
results. In addition we also demonstrate that
these two approaches are complementary and
can be used to boost each other?s results in a
statistical rescoring model with global evidence
from large comparable corpora.
Hakkani-Tur et al (2007) described a filtering
mechanism using two crosslingual IE systems
for improving crosslingual document retrieval.
Many previous validation methods for crosslin-
gual QA, such as those organized by Cross Lan-
guage Evaluation Forum (Vallin et al, 2005), fo-
cused on local information which involves only
the query and answer (e.g. (Kwork and Deng,
2006)), keyword translation (e.g. (Mitamura et
al., 2006)) and surface patterns (e.g. (Soubbotin
and Soubbotin, 2001)). Some global valida-
tion approaches considered information redun-
dancy based on shallow statistics including co-
occurrence, density score and mutual informa-
tion (Clarke et al, 2001; Magnini et al, 2001;
Lee et al, 2008), deeper knowledge from depen-
dency parsing (e.g. (Shen et al, 2006)) or logic
reasoning (e.g. (Harabagiu et al, 2005)). How-
ever, all of these approaches made limited efforts
at disambiguating entities in queries and limited
use of fact extraction in answer search and vali-
dation.
Several recent IE studies have stressed the
benefits of using information redundancy on
estimating the correctness of the IE out-
put (Downey et al, 2005; Yangarber, 2006;
Patwardhan and Riloff, 2009; Ji and Grish-
man, 2008). Some recent research used com-
parable corpora to re-score name translitera-
tions (Sproat et al, 2006; Klementiev and Roth,
2006) or mine new word translations (Fung and
Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao
and Zhai, 2005; Hassan et al, 2007; Udupa et
al., 2009; Ji, 2009). To the best of our knowl-
edge, this is the first work on mining facts from
comparable corpora for answer validation in a
new crosslingual entity profiling task.
3 Experimental Setup
3.1 Task Definition
The goal of the KBP slot filling task is to extract
facts from a large source corpus regarding cer-
tain attributes (?slots?) of an entity, which may
be a person or organization, and use these facts
to augment an existing knowledge base (KB).
Along with each slot answer, the system must
provide the ID of a document which supports
the correctness of this answer. KBP 2010 (Ji et
al., 2010) defines 26 types of attributes for per-
sons (such as the age, birthplace, spouse, chil-
dren, job title, and employing organization) and
16 types of attributes for organizations (such
as the top employees, the founder, the year
founded, the headquarters location, and the sub-
sidiaries).
The new problem we define in this paper is an
extension of this task to a crosslingual paradigm.
Given a query in a target language t and a col-
lection of documents in a source language s,
a system must extract slot answers about the
query and present the answers in t. In this pa-
per we examine a specific setting of s=Chinese
and t=English.
To score crosslingual slot filling, we pool all
the system responses and group equivalent an-
swers into equivalence classes. Each system re-
sponse is rated as correct, wrong, inexact or re-
dundant. Given these judgments, we calculate
the precision, recall and F-measure of each sys-
tem, crediting only correct answers.
3.2 Data and Query Selection
We use the comparable corpora of English
TDT5 (278,358 documents) and Chinese TDT5
111
(56,424 documents) as our source collection.
For query selection, we collected all the en-
tities from the entire source collection and
counted their frequencies. We then selected 50
informative entities (25 persons and 25 organiza-
tions) which were located in the middle range of
frequency counts. Among the 25 person queries,
half are Chinese-specific names, and half are
non-Chinese names. The 25 organizations fol-
low a representative distribution according to
the entity subtypes defined in NIST Automatic
Content Extraction (ACE) program1.
3.3 Baseline Pipelines
3.3.1 Overview
We employ the following two types of base-
line crosslingual slot filling pipelines to process
Chinese documents. Figure 1 and Table 1 shows
the five system pipelines we have used to con-
duct our experiments.
Type A Translate Chinese texts into English,
and apply English slot filling systems to the
translations.
Type B Translate English queries into Chinese,
apply Chinese slot filling systems to Chinese
texts, and translate answers back to English. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Machine 
Translation 
English 
Texts 
Chinese 
Texts 
English Candidate Answers 
English
Query
English Slot Filling 
 Answer 
Translation  Pattern Matching 
 Supervised
Classification
Chinese Slot Filling 
 Supervised 
Classification 
Chinese 
Query 
 Query 
Translation
Figure 1: Overview of Baseline Crosslingual Slot Fill-
ing Pipelines
1http://www.itl.nist.gov/iad/mig/tests/ace/
 
Pipeline Label Components Data 
(1) English Supervised Classification Mono-
lingual (2) English Pattern Matching 
English 
TDT5 
 
(3) 
MT+English 
Supervised 
Classification Type A  
(4) 
MT+English 
Pattern Matching Cross-lingual 
Type 
B 
 
 
(5) 
Query Translation 
+Chinese Supervised 
Classification 
+Answer Translation 
Chinese 
TDT5 
 
 
 
Table 1: Monolingual and Crosslingual Baseline Slot
Filling Pipelines
3.3.2 Monolingual Slot Filling
We applied a state-of-the-art bilingual slot
filling system (Chen et al, 2010) to process
bilingual comparable corpora. This baseline
system includes a supervised ACE IE pipeline
and a bottom-up pattern matching pipeline.
The IE pipeline includes relation extraction and
event extraction based on maximum entropy
models that incorporate diverse lexical, syntac-
tic, semantic and ontological knowledge. The
extracted ACE relations and events are then
mapped to KBP slot fills. In pattern matching,
we extract and rank patterns based on a dis-
tant supervision approach (Mintz et al, 2009)
that uses entity-attribute pairs from Wikipedia
Infoboxes and Freebase (Bollacker et al, 2008).
We set a low threshold to include more answer
candidates, and then a series of filtering steps
to refine and improve the overall pipeline re-
sults. The filtering steps include removing an-
swers which have inappropriate entity types or
have inappropriate dependency paths to the en-
tities.
3.3.3 Document and Name Translation
We use a statistical, phrase-based MT sys-
tem (Zens and Ney, 2004) to translate Chinese
documents into English for Type A Approaches.
The best translation is computed by using a
weighted log-linear combination of various sta-
tistical models: an n-gram language model, a
phrase translation model and a word-based lex-
112
icon model. The latter two models are used in
source-to-target and target-to-source directions.
The model scaling factors are optimized with re-
spect to the BLEU score similar to (Och, 2003).
The training data includes 200 million running
words in each language. The total language
model training data consists of about 600 mil-
lion running words.
We applied various name mining approaches
from comparable corpora and parallel corpora,
as described in (Ji et al, 2009) to extract and
translate names in queries and answers in Type
B approaches. The accuracy of name translation
is about 88%. For those names not covered by
these pairs, we relied on Google Translate 2 to
obtain results.
4 Analysis of Baseline Pipelines
In this section we analyze the coverage (Sec-
tion 4.1) and precision (Section 4.2) results of
the baseline pipelines. We then illustrate the
potential for global validation from comparable
corpora through a series of examples.
4.1 Coverage Analysis: Toward
Information Fusion
Table 2 summarizes the Precision (P), Recall
(R) and F-measure (F) of baseline pipelines and
the union of their individual results.
Table 2: Baseline Pipeline Results 
System P R F 
(1) 0.08 0.54 0.15 
(2) 0.02 0.35 0.03 Mono- 
lingual Union of 
(1)+(2) 
0.03 0.69 0.05 
(3) 0.04 0.04 0.04 
(4) 0.03 0.25 0.05 
Union of 
(3)+(4) 0.03 0.26 0.05 
(5) 0.04 0.46 0.08 
Cross- 
lingual 
Union of 
(3)+(4)+(5) 0.03 0.56 0.05 
Compara
ble 
Corpora 
Union of 
(1)+(2)+(3)+
(4)+(5) 
0.02 1 0.04 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
2http://translate.google.com/
Although crosslingual pipelines used a much
smaller corpus than monolingual pipelines, they
extracted comparable number of correct answers
(66 vs. 81) with a slightly better precision.
In fact, the crosslingual pipeline (5) performs
even better than monolingual pipeline (2), es-
pecially on the employment slots. In particu-
lar, 96.35% of the correct answers for Chinese-
specific person queries (e.g. ?Tang Jiaxuan?)
were extracted from Chinese data. Even for
those facts discovered from English data, they
are about quite general slots such as ?title? and
?employee of ?. In contrast, Chinese data covers
more diverse biographical slots such as ?family
members? and ?schools attended?.
Compared to the union of Type A approaches
(pipelines (3)+(4)), Pipeline (5) returned many
more correct answers with higher precision. The
main reason is that Type A approaches suffer
from MT errors. For example, MT mistakenly
translated the query name ?Celine Dion? into
?Clinton? and thus English slot filling compo-
nents failed to identify any answers. One can
hypothesize that slot filling on MT output can
be improved by re-training extraction compo-
nents directly from MT output. However, our
experiments of learning patterns from MT out-
put showed negative impact, mainly because
MT errors were too diverse to generalize. In
other cases even though slot filling produced cor-
rect results, MT still failed to translate the an-
swer names correctly. For example, English slot
filling successfully found a potential answer for
?org:founded by? of the query ?Microsoft? from
the following MT output: ?The third largest of
the Microsoft common founder Alan Doss , aged
50, and net assets of US 22 billion.?; however,
the answer string ?Paul Allen? was mistakenly
translated into ?Alan Doss?. MT is not so cru-
cial for ?per:title? slot because it does not require
translation of contexts.
To summarize, 59% of the missing errors were
due to text, query or answer translation errors
and 20% were due to slot filling errors. Never-
theless, the union of (3)+(4)+(5) still contain
more correct answers. These baseline pipelines
were developed from a diverse set of algorithms,
and typically showed strengths in specific slots.
113
In general we can conclude that monolin-
gual and crosslingual pipelines are complemen-
tary. Combining the responses from all baseline
pipelines, we can get similar number of correct
answers compared to one single human annota-
tor.
4.2 Precision Analysis: Toward Global
Validation
The spurious errors from baseline crosslingual
slot filling pipelines reveal both the shortcom-
ings of the MT system and extraction across
languages. Table 3 shows the distribution of
spurious errors.
Pipeline Spurious Errors Distribution
Content Translation 
+ Extraction 
85% 
Query Translation 13% 
Type A 
Answer Translation 2% 
Word Segmentation 34% 
Relation Extraction 33% 
Coreference 17% 
Semantic Type 13% 
Type B 
Slot Type 3% 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 3: Distribution of Spurious Errors
Table 3 indicates a majority (85%) of spurious
errors from Type A pipelines were due to ap-
plying monolingual slot filling methods to MT
output which preserves Chinese structure.
As demonstrated in previous work (e.g. (Par-
ton and McKeown, 2010; Ji et al, 2009)),
we also found that many (14.6%) errors were
caused by the low quality of name translation
for queries and answers.
For example, ?????/McGinty? was mis-
takenly translated into the query name ?Kim
Jong-il?, which led to many incorrect answers
such as ?The British Royal joint military re-
search institute? for ?per:employee of ?.
In contrast, the spurious errors from Type B
pipelines were more diverse. Chinese IE com-
ponents severely suffered from word segmen-
tation errors (34%), which were then directly
propagated into Chinese document retrieval and
slot filling. Many segmentation errors occurred
with out-of-vocabulary names, especially per-
son names and nested organization names. For
example, the name ????/Yao Mingbao? was
mistakenly segmented into two words ???/Yao
Ming? and ??/bao?, and thus the document was
mistakenly retrieved for the query ?Yao Ming?.
In many cases (33%) Chinese relation and
event extraction components failed to cap-
ture Chinese-specific structures due to the lim-
ited size of training corpora. For example,
from the context ???????????????
?/Xiao Wan-chang, who were invited to be-
come the economics consultant for Chen Shui-
bian?, Chinese slot filling system mistakenly ex-
tracted ?consultant? as a ?per:title? answer for
the query ?Chen Shui-bian? using a common
pattern ?<query><title>?.
13% of errors were caused due to invalid se-
mantic types for certain slots. For example,
many metaphoric titles such as ?tough guy?
don?t match the definition of ?per:title? in the
annotation guideline ?employment or member-
ship position?.
5 Global Validation
Based on the above motivations we propose to
incorporate global evidence from a very large
collection of comparable documents to refine
local decisions. The central idea is to over-
generate candidate answers from multiple weak
baselines to ensure high upper-bound of recall,
and then conduct effective global validation to
filter spurious errors while keeping good answers
in order to enhance precision.
5.1 Supervised Rescoring
Ideally, we want to choose a validation model
which can pick out important features in a con-
text wider than that used by baseline pipelines.
Merging individual systems to form the union of
answers can be effective, but Table 2 shows that
simple union of all pipelines produced worse F-
measure than the best pipeline.
In this paper we exploit the reranking
paradigm, commonly used in information re-
trieval, to conduct global validation. By model-
ing the empirical distribution of labeled training
data, statistical models are used to identify the
114
strengths and weaknesses (e.g. high and low pre-
cision slots) of individual systems, and rescore
answers accordingly. Specially, we develop a
supervised Maximum Entropy (MaxEnt) based
model to rescore the answers from the pipelines,
selecting only the highest-scoring answers.
The rescorer was trained (using cross-
validation) on varying subsets of the features.
The threshold at which an answer is deemed to
be true is chosen to maximize the F-Measure on
the training set.
5.2 Validation Features
Table 4 describes the validation features used for
rescoring, where q is the query, q? the Chinese
translation of q, t the slot type, a the candidate
answer, a? the Chinese form of a, s the context
sentence and d is the context document support-
ing a.
The feature set benefits from multiple dimen-
sions of crosslingual slot filling. These features
were applied to both languages wherever anno-
tation resources were available.
In the KBP slot filling task, slots are of-
ten dependent on each other, so we can im-
prove the results by improving the ?coherence?
of the story (i.e. consistency among all gener-
ated answers - query profiles). We use feature
f2 to check whether the same answer was gen-
erated for conflicting slots, such as per:parents
and per:children.
Compared to traditional QA tasks, slot fill-
ing is a more fine-grained task in which differ-
ent slots are expected to obtain semantically
different answers. Therefore, we explored se-
mantic constraints in both local and global con-
texts. For example, we utilized bilingual name
gazetteers from ACE training corpora, Google
n-grams (Ji and Lin, 2009) and the geonames
website 3 to encode features f6, f8 and f9; The
org:top members/employees slot requires a sys-
tem to distinguish whether a person member/
employee is in the top position, thus we encoded
f10 for this purpose.
The knowledge used in our baseline pipelines
is relatively static ? it is not updated during the
3http://www.geonames.org/statistics/
extraction process. Achieving high performance
for cross-lingual slot filling requires that we take
a broader view, one that looks outside a sin-
gle document or a single language in order to
exploit global knowledge. Fortunately, as more
and more large crosslingual comparable corpora
are available, we can take advantage of informa-
tion redundancy to validate answers. The basic
intuition is that if a candidate answer a is cor-
rect, it should appear together with the query
q repeatedly, in different documents, or even in
certain coreference links, relations and events.
For example, ?David Kelly - scientist?, and
??????/Shintaro Ishihara - ??/governor?
pairs appear frequently in ?title? coreference
links in both English and Chinese corpora;
?Elizabeth II? is very often involved in an ?em-
ployment? relation with ?United Kingdom? in
English corpora. On the other hand, some in-
correct answers with high global statistics can be
filtered out using these constraints. For exam-
ple, although the query ????/Tang Jiaxuan?
appears frequently together with the candidate
per:title answer ???/personnel?, it is linked by
few coreference links; in contrast, it?s coreferen-
tial with the correct title answer ?????/State
Council member? much more frequently.
We processed cross-lingual comparable cor-
pora to extract coreference links, relations and
events among mentions (names, nominals and
time expressions etc.) and stored them in an
external knowledge base. Any pair of <q, a>
is then compared to the entries in this knowl-
edge base. We used 157,708 documents from
Chinese TDT5 and Gigaword to count Chinese
global statistics, and 7,148,446 documents from
DARPA GALE MT training corpora to count
English global statistics, as shown in features
f12 and f13. Fact based global features f14, f15,
f16 and f17, were calculated from 49,359 Chi-
nese and 280,513 English documents (annotated
by the bilingual IE system in Section 3.3.2.
6 Experiments
In this section, we examine the overall perfor-
mance of this method. We then discuss the
usefulness of the individual sets of features. In
115
Characteristics 
Scope Depth Language 
Description 
f1: frequency of <q, a, t> that appears in all baseline outputs Global 
(Cross-
system) 
Shallow 
 English f2: number of conflicting slot types in which answer a appears in all baseline 
outputs 
f3: conjunction of t and whether a is a year answer Shallow English 
f4: conjunction of t and whether a includes numbers or letters 
f5: conjunction of place t and whether a is a country name 
f6: conjunction of per:origin t and whether a is a nationality 
f7: if t=per:title, whether a is an acceptable title 
f8: if t requires a name answer, whether a is a name 
Local 
Deep 
 
English 
 
f9: whether a has appropriate semantic type 
f10: conjunction of org:top_members/employees and whether there is a high-level 
title in s 
Global 
(Within-
Document) 
Deep English 
f11: conjunction of alternative name and whether a is an acronym of q 
Chinese f12: conditional probability of q/q' and a/a' appear in the same document Shallow 
(Statistics) English f13: conditional probability  of q/q' and a/a' appear in the same sentence 
Both f14:  co-occurrence of q/q' and a/a'  appear in coreference links 
English f15: co-occurrence of q/q' and a/a'  appear in relation/event links 
English f16: conditional probability of q/q' and a/a' appear in relation/event links 
Global 
(Cross-
document 
in 
comparable 
corpora) 
Deep 
(Fact-
based) 
English f17: mutual information of q/q' and a/a' appear in relation/event links 
 
Table 4: Validation Features for Crosslingual Slot Filling
the following results, the baseline features are
always used in addition to any other features.
6.1 Overall Performance
Because of the data scarcity, ten-fold cross-
validation, across queries, was used to train
and test the system. Quantitative results after
combining answers from multiple pipelines are
shown in Table 5. We used two basic features,
one is the slot type and the other is the entity
type of the query (i.e. person or organization).
This basic feature set is already successful in im-
proving the precision of the pipelines, although
this results in a number of correct answers be-
ing discarded as well. By adding the additional
validation features described previously, both
the f-score and precision of the models are im-
proved. In the case of the cross-lingual pipelines
(3+4+5) the number of correct answers chosen
is almost doubled while increasing the precision
of the output.
6.2 Impact of Global Validation
A comparison of the benefits of global versus lo-
cal features are shown in Table 6, both of which
dramatically improve scores over the baseline
features. The global features are universally
Pipelines F P R
Basic Features
1+2 0.31 0.31 0.30
3+4+5 0.26 0.39 0.20
1+2+3+4+5 0.27 0.29 0.25
Full Features
1+2 0.37 0.30 0.46
3+4+5 0.36 0.35 0.37
1+2+3+4+5 0.31 0.28 0.35
Table 5: Using Basic Features to Filter Answers
more beneficial than the local features, although
the local features generate results with higher
precision at the expense of the number of correct
answers returned. The global features are espe-
cially useful for pipelines 3+4+5, where the per-
formance using just these features reaches those
of using all other features ? this does not hold
true for the monolingual pipelines however.
6.3 Impact of Fact-driven Deep
Knowledge
The varying benefit of fact-driven cross-
document features and statistical cross-
document features are shown in Table 7.
116
Pipelines F P R
Local Features
1+2 0.34 0.35 0.33
3+4+5 0.29 0.40 0.22
1+2+3+4+5 0.27 0.32 0.24
Global Features
1+2 0.35 0.30 0.42
3+4+5 0.37 0.36 0.38
1+2+3+4+5 0.33 0.29 0.38
Table 6: The Benefit of Global versus Local Features
While both feature sets are beneficial, the
monolingual pipelines (1+2) benefit more
from statistical features while the cross-lingual
pipelines (3+4+7) benefit slightly more from
the fact-based features. Despite this bias, the
overall results when the features are used in
all pipelines are very close with the fact-based
features being slightly more useful overall.
Pipelines F P R
Fact-Based Features
1+2 0.33 0.27 0.42
3+4+5 0.35 0.43 0.29
1+2+3+4+5 0.30 0.27 0.34
Statistical Features
1+2 0.37 0.34 0.40
3+4+5 0.34 0.35 0.33
1+2+3+4+5 0.29 0.25 0.34
Table 7: Fact vs. Statistical Cross-Doc Features
Translation features were only beneficial to
pipelines 3, 4, and 5, and provided a slight in-
crease in precision from 0.39 to 0.42, but pro-
vided no noticeable benefit when used in con-
junction with results from pipelines 1 and 2.
This is because the answers where translation
features would be most useful were already be-
ing selected by pipelines 1 and 2 using the base-
line features.
6.4 Discussion
The use of any re-scoring, even with baseline
features, provides large gains over the union of
the baseline pipelines, removing large number
of incorrect answers. The use of more sophis-
ticated features provided substantial gains over
the baseline features. In particular, global fea-
tures proved very effective. Further feature en-
gineering to address the remaining errors and
the dropped correct answer would likely provide
increasing gains in performance.
In addition, two human annotators, indepen-
dently, conducted the same task on the same
data, with a second pass of adjudication. The F-
scores of inter-annotator agreement were 52.0%
for the first pass and 73.2% for the second pass.
This indicates that slot filling remains a chal-
lenging task for both systems and human anno-
tators?only one monolingual system exceeded
30% F-score in the KBP2010 evaluation.
7 Conclusion and Future Work
Crosslingual slot filling is a challenging task
due to limited performance in two separate ar-
eas: information extraction and machine trans-
lation. Various methods of combining tech-
niques from these two areas provided weak yet
complementary baseline pipelines. We proposed
an effective approach to integrate these base-
lines and enhance their performance using wider
and deeper knowledge from comparable cor-
pora. The final system based on cross-lingual
comparable corpora outperformed monolingual
pipelines on much larger monolingual corpora.
The intuition behind our approach is that
over-generation of candidate answers from weak
baselines provides a potentially strong recall
upper-bound. The remaining enhancement be-
comes simpler: filtering errors. Our experiments
also suggest that our rescoring models tend to
over-fit due to small amount of training data.
Manual annotation and assessment are quite
costly, motivating future work in active learning
and semi-supervised learning methods. In addi-
tion, we plan to apply our results as feedback to
improve MT performance on facts using query
and answer-driven language model adaptation.
We have demonstrated our approach on English-
Chinese pair, but the framework is language-
independent; ultimately we would like to extend
the task to extracting information from more
languages.
117
Acknowledgments
This work was supported by the U.S. NSF CAREER
Award under Grant IIS-0953149 and PSC-CUNY
Research Program. Any opinions, findings, and con-
clusions or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily
reflect the views of the National Science Foundation.
References
K. Bollacker, R. Cook, and P. Tufts. 2008. Free-
base: A shared database of structured general hu-
man knowledge. In Proc. National Conference on
Artificial Intelligence.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,
Marissa Passantino, and Heng Ji. 2010. Top-
down and bottom-up: A combined approach to
slot filling. Lecture Notes in Computer Science,
6458:300?309, December.
C. L. A. Clarke, G. V. Cormack, and T.R. Lynam.
2001. Exploiting redundancy in question answer-
ing. In Proc. SIGIR2001.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A Probabilistic Model of Redundancy in
Information Extraction. In Proc. IJCAI 2005.
Pascale Fung and Lo Yuen Yee. 1998. An ir ap-
proach for translating new words from nonparallel
and comparable texts. In COLING-ACL.
Dilek Hakkani-Tur, Heng Ji, and Ralph Grishman.
2007. Using information extraction to improve
cross-lingual document retrieval. In Proc. RANLP
workshop on Multi-source, Multilingual Informa-
tion Extraction and Summarization.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden,
A. Hickl, and P. Wang. 2005. Employing two
question answering systems in trec 2005. In Proc.
TREC2005.
Ahmed Hassan, Haytham Fahmy, and Hany Has-
san. 2007. Improving named entity translation
by exploiting comparable and parallel corpora. In
RANLP.
Heng Ji and Ralph Grishman. 2008. Refining Event
Extraction through Cross-Document Inference. In
Proc. of ACL-08: HLT, pages 254?262.
Heng Ji and Dekang Lin. 2009. Gender and animacy
knowledge discovery from web-scale n-grams for
unsupervised person mention detection. In Proc.
PACLIC2009.
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias
Blume, John Wang, Shahram Khadivi, Richard
Zens, and Hermann Ney. 2009. Name translation
for distillation. Handbook of Natural Language
Processing and Machine Translation: DARPA
Global Autonomous Language Exploitation.
Heng Ji, Ralph Grishman, Hoa Trang Dang, and
Kira Griffitt. 2010. An overview of the tac2010
knowledge base population track. In Proc.
TAC2010.
Heng Ji. 2009. Mining name translations from com-
parable corpora by creating bilingual information
networks. In ACL-IJCNLP 2009 workshop on
Building and Using Comparable Corpora (BUCC
2009): from Parallel to Non-parallel Corpora.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In HLT-NAACL 2006.
K.-L. Kwork and P. P. Deng. 2006. Chinese
question-answering: Comparing monolingual with
english-chinese cross-lingual results. In Asia In-
formation Retrieval Symposium.
Cheng-Wei Lee, Yi-Hsun Lee, and Wen-Lian Hsu.
2008. Exploring shallow answer ranking features
in cross-lingual and monolingual factoid question
answering. Computational Linguistics and Chi-
nese Language Processing, 13:1?26, March.
B. Magnini, M. Negri, R. Prevete, and H. Tanev.
2001. Is it the right answer?: Exploiting web
redundancy for answer validation. In Proc.
ACL2001.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In ACL-IJCNLP
2009.
Teruko Mitamura, Mengqiu Wang, Hideki Shima,
and Frank Lin. 2006. Keyword translation accu-
racy and cross-lingual question answering in chi-
nese and japanese. In EACL 2006 Workshop on
MLQA.
F. J. Och. 2003. Minimum error rate training in
statistical machine translaton. In Proc.ACL2003.
Kristen Parton and Kathleen McKeown. 2010. Mt
error detection for cross-lingual question answer-
ing. Proc. COLING2010.
Siddharth Patwardhan and Ellen Riloff. 2009. A
Unified Model of Phrasal and Sentential Evidence
for Information Extraction. In Proc. EMNLP
2009.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and ger-
man corpora. In ACL 1999.
Li Shao and Hwee Tou Ng. 2004. Mining new word
translations from comparable corpora. In COL-
ING2004.
D. Shen, G. Saarbruechen, and D. Klakow. 2006.
Exploring correlation of dependency relation
paths for answer extraction. In Proc. ACL2006.
118
M. M. Soubbotin and S. M. Soubbotin. 2001. Pat-
terns of potential answer expressions as clues to
the right answers. In Proc. TREC2001.
Richard Sproat, Tao Tao, and ChengXiang Zhai.
2006. Named entity transliteration with compa-
rable corpora. In ACL 2006.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2004. Cross-lingual information extraction evalu-
ation. In Proc. COLING2004.
Tao Tao and Chengxiang Zhai. 2005. Mining com-
parable bilingual text corpora for cross-language
information integration. In Proc. KDD2005.
Raghavendra Udupa, K. Saravanan, A. Kumaran,
and Jagadeesh Jagarlamudi. 2009. Mint: A
method for effective and scalable mining of named
entity transliterations from large comparable cor-
pora. In EACL2009.
Alessandro Vallin, Bernardo Magnini, Danilo Gi-
ampiccolo, Lili Aunimo, Christelle Ayache, Petya
Osenova, Anselmo Peas, Maaren de Rijke, Bogdan
Sacaleanu, Diana Santos, and Richard Sutcliffe.
2005. Overview of the clef 2005 multilingual ques-
tion answer track. In Proc. CLEF2005.
Roman Yangarber. 2006. Verification of Facts across
Document Boundaries. In Proc. International
Workshop on Intelligent Information Access.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In Proc. HLT/NAACL 2004.
119
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 43?52,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Language-Independent Name Translation Mining from
Wikipedia Infoboxes
Wen-Pin Lin, Matthew Snover, Heng Ji
Computer Science Department
Queens College and Graduate Center
City University of New York
New York, NY 11367, USA
danniellin@gmail.com, msnover@qc.cuny.edu, hengji@cs.qc.cuny.edu
Abstract
The automatic generation of entity profiles
from unstructured text, such as Knowledge
Base Population, if applied in a multi-lingual
setting, generates the need to align such pro-
files from multiple languages in an unsuper-
vised manner. This paper describes an unsu-
pervised and language-independent approach
to mine name translation pairs from entity pro-
files, using Wikipedia Infoboxes as a stand-in
for high quality entity profile extraction. Pairs
are initially found using expressions that are
written in language-independent forms (such
as dates and numbers), and new translations
are then mined from these pairs. The algo-
rithm then iteratively bootstraps from these
translations to learn more pairs and more
translations. The algorithm maintains a high
precision, over 95%, for the majority of its
iterations, with a slightly lower precision of
85.9% and an f-score of 76%. A side effect
of the name mining algorithm is the unsuper-
vised creation of a translation lexicon between
the two languages, with an accuracy of 64%.
We also duplicate three state-of-the-art name
translation mining methods and use two ex-
isting name translation gazetteers to compare
with our approach. Comparisons show our
approach can effectively augment the results
from each of these alternative methods and re-
sources.
1 Introduction
A shrinking fraction of the world?s web pages are
written in English, while about 3,000 languages are
endangered (Krauss, 2007). Therefore the ability
to access information across a range of languages,
especially low-density languages, is becoming in-
creasingly important for many applications. In this
paper we hypothesize that in order to extend cross-
lingual information access to all the language pairs
on the earth, or at least to some low-density lan-
guages which are lacking fundamental linguistic re-
sources, we can start from the much more scalable
task of ?information? translation, or more specifi-
cally, new name translation.
Wikipedia, as a remarkable and rich online ency-
clopedia with a wealth of general knowledge about
varied concepts, entities, events and facts in the
world, may be utilized to address this need. As
of March 2011 Wikipedia contains pages from 275
languages1, but statistical machine translation (MT)
techniques can only process a small portion of them
(e.g. Google translate can only translate between
59 languages). Wikipedia infoboxes are a highly
structured form of data and are composed of a set
of subject-attribute-value triples that summarize or
highlight the key features of the concept or sub-
ject of each article. A large number of instance-
centered knowledge-bases that have harvested this
structured data are available. The most well-known
are probably DBpedia (Auer et al, 2007), Free-
base (Bollacker et al, 2007) and YAGO (Suchanek
et al, 2007). However, almost all of these ex-
isting knowledge bases contain only one language.
Even for high-density languages, more than 70% of
Wikipedia pages and their infobox entries do not
contain cross-lingual links.
1http://meta.wikimedia.org/wiki/List_of_
Wikipedias
43
Recent research into Knowledge Base Population,
the automatic generation of profiles for named enti-
ties from unstructured text has raised the possibility
of automatic infobox generation in many languages.
Cross-lingual links between entities in this setting
would require either expensive multilingual human
annotation or automatic name pairing. We hypoth-
esize that overlaps in information across languages
might allow automatic pairing of profiles, without
any preexisting translational capabilities. Wikipedia
infoboxes provide a proxy for these high quality
cross lingual automatically generated profiles upon
which we can explore this hypothesis.
In this paper we propose a simple and general un-
supervised approach to discover name translations
from knowledge bases in any language pair, using
Wikipedia infoboxes as a case study. Although dif-
ferent languages have different writing systems, a
vast majority of the world?s countries and languages
use similar forms for representing information such
as time/calendar date, number, website URL and
currency (IBM, 2010). In fact most languages com-
monly follow the ISO 8601 standard2 so the formats
of time/date are the same or very similar. Therefore,
we take advantage of this language-independent for-
matting to design a new and simple bootstrapping
based name pair mining approach. We start from
language-independent expressions in any two lan-
guages, and then extract those infobox entries which
share the same slot values. The algorithm itera-
tively mines more name pairs by utilizing these pairs
and comparing other slot values. In this unsuper-
vised manner we don?t need to start from any name
transliteration module or document-wise temporal
distributions as in previous work.
We conduct experiments on English and Chinese
as we have bi-lingual annotators available for eval-
uating results. However, our approach does not re-
quire any language-specific knowledge so it?s gen-
erally applicable to any other language pairs. We
also compare our approach to state-of-the-art name
translation mining approaches.
1.1 Wikipedia Statistics
A standard Wikipedia entry includes a title, a docu-
ment describing the entry, and an ?infobox? which
2http://en.wikipedia.org/wiki/ISO_8601
is a fixed-format table designed to be added to
the top right-hand corner of the article to con-
sistently present a summary of some unifying at-
tributes (or ?slots?) about the entry. For example,
in the Wikipedia entry about the singer ?Beyonce
Knowles?, the infobox includes information about
her birth date, origin, song genres, occupation, etc.
As of November 2010, there were 10,355,225 En-
glish Wikipedia entries, and 772,826 entries. Only
27.2% of English Wikipedia entries have cross-
lingual hyperlinks referring to their corresponding
Chinese entries.
Wikipedia entries are created and updated expo-
nentially (Almeida et al, 2007) because of the in-
creasing number of contributors, many of whom are
not multi-lingual speakers. Therefore it is valuable
to align the cross-lingual entries by effective name
mining.
1.2 Motivating Example
Figure 1: A Motivating Example
Figure 1 depicts a motivating example for our ap-
proach. Based on the assumption that if two per-
son entries had the same birth date and death date,
44
they are likely to be the same person, we can find
the entity pair of (Michael Jackson /???.???).
We can get many name pairs using similar language-
independent clues. Then starting from these name
pairs, we can iteratively get new pairs with a large
portion of overlapped slots. For example, since
??????? and ?The Jackson 5? share many slot
values such as ?member? and ?years active?, they
are likely to be a translation pair. Next we can use
the new pair of (The Jackson 5 / ?????) to
mine more pairs such as ?????? and ?Steeltown
Records.?
2 Data and Pre-Processing
Because not all Wikipedia contributors follow the
standard naming conventions and date/number for-
mats for all languages, infoboxes include some
noisy instances. Fortunately the NIST TAC Knowl-
edge Base Population (KBP) task (Ji et al, 2010) de-
fined mapping tables which can be directly used to
normalize different forms of slot types3. For exam-
ple, we can group ?birthdate?, ?date of birth?, ?date-
birth? and ?born? to ?birth date.? In addition, we also
normalized all date slot values into one standard for-
mat as ?YYYY MM DD.? For example, both ?1461-
8-5? and ?5 August, 1461? are normalized as ?1461
08 05.? Only those Wikipedia entries that have at
least one slot corresponding to the Knowledge Base
Population task are used for name mining. Entries
with multiple infoboxes are also discarded as these
are typically ?List of ? entries and do not corre-
spond to a particular named entity. The number of
entries in the resulting data set are shown in Table 1.
The set of slots were finally augmented to include
the entry?s name as a new slot. The cross-lingual
links between Chinese and English Wikipedia pages
were used as the gold standard that the unsupervised
algorithm attempted to learn.
Language Entries Slot Values E-Z Pairs
English (E) 634,340 2,783,882 11,109Chinese (Z) 21,152 110,466
Table 1: Processed Data Statistics
3It is important to note that the vast majority of Chinese
Wikipedia pages store slot types in English in the underlying
wiki source, removing the problem of aligning slot types be-
tween languages.
3 Unsupervised Name Pair Mining
The name pair mining algorithm takes as input a set
of English infoboxes E and Chinese infoboxes Z.
Each infobox consists of a set of slot-value pairs,
where each slot or value may occur multiple times in
a single infobox. The output of the algorithm is a set
of pairs of English and Chinese infoboxes, match-
ing an infobox in one language to the corresponding
infobox in the other language. There is nothing in-
herently designed in the algorithm for English and
Chinese, and this method could be applied to any
language pair.
Because the algorithm is unsupervised, it begins
with no initial pairs, nor is there any initial trans-
lation lexicon between the two languages. As the
new pairs are learned, both the entries titles and the
values of their infoboxes are used to generate new
translations which can be used to learn more cross-
lingual name pairs.
3.1 Search Algorithm
The name pair mining algorithm considers all pairs
of English and Chinese infoboxes4, assigns a score,
described in Section 3.2, to each pair and then greed-
ily selects the highest scoring pairs, with the follow-
ing constraints:
1. Each infobox can only be paired to a single in-
fobox in the other language, with the highest
scoring infobox being selected. While there are
some instances of two entries in one language
for one entity which both have translation links
to the same page in another language, these are
rare occurrences and did not occur for the KBP
mapped data used in these experiments.
2. An pair (e, z) can only be added if the score
for the pair is at least 95%5 percent higher than
the score for the second best pair for both e and
z. This eliminates the problem of ties in the
data, and follows the intuition that if there are
4The algorithm does not need to compare all pairs of in-
foboxes as the vast majority will have a score of 0. Only those
pairs with some equivalent slot-value pairs need to be scored.
The set of non-zero scoring pairs can thus be quickly found by
indexing the slot-value pairs.
5The value of 95% was arbitrarily chosen; variations in this
threshold produce only small changes in performance.
45
multiple pairs with very similar scores it is ben-
eficial to postpone the decision until more evi-
dence becomes available.
To improve the speed of the algorithm, the top 500
scoring pairs, that do not violate these constraints,
are added at each iteration. The translation lexicon
is then updated. The translation lexicon is updated
each iteration from the total set of pairs learned us-
ing the following procedure. For each pair (e, z) in
the learned pairs, new translations are added for each
of the following conditions:
1. A translation of the name of e to the name z is
added.
2. If a slot s in e has one value, ve, and that slot
in z has one value, vz , a translation ve ? vz is
added.
3. If a slot s has multiple values in e and z, but all
but one of these values, for both e and z, have
translations to values in the other entry, then a
translation is learned for the resulting untrans-
lated value.
These new translations are all given equal weight
and are added to the translation lexicon even if the
evidence for this translation occurs in only a sin-
gle name pair6. These translations can be used to
align more name pairs in subsequent iterations by
providing more evidence that a given pair should be
aligned. After a translation is learned, we consider
the English side to be equivalent to the Chinese side
when scoring future infobox pairs.
The algorithm halts when there are no longer any
new name pairs with non-zero score which also sat-
isfy the search constraints described above.
3.2 Scoring Function
A score can be calculated for the pairing of an En-
glish infobox, e and a Chinese infobox, z according
to the following formula:
?
s?slots
{
IZ(s) + IE(s) ?v1, v2 : z.s.v1 ? e.s.v2
0 otherwise
(1)
6Assigning a probability to each translation learned based
upon the number of entries providing evidence for the transla-
tion could be used to further refine the predictions of the model,
but was not explored in this work.
A slot-value pair in Chinese, z.s.v1, is considered
equivalent to a slot-value pair in English, e.s.v2, if
the values are the same (typically only the case with
numerical values) or if there is a known translation
from v1 to v2. These translations are automatically
learned during the name-mining process. Initially
there are no known translations between the two lan-
guages.
The term IL(s) in equation 1 reflects how infor-
mative the slot s is in either English (E) or Chinese
(Z), and is calculated as the number of unique val-
ues for that slot for that language divided by the to-
tal number of slot-value pairs for that language, as
shown in equation 2.
IL(slot s) =
|{v|i ? L ? ?i.s.v}|
|{i.s.v|i ? L}|
(2)
If a slot s contains unique values such that a slot
and value pair is never repeated then IL(s) is 1.0
and indicates that the slot distinguishes entities very
well. Slots such as ?date of birth? are less infor-
mative since many individuals share the same birth-
date, and slots such as ?origin? are the least informa-
tive since so many people are from the same coun-
tries. A sampling of the IL(s) scores is shown in
Table 2. The slots ?origin? and ?religion? are the two
lowest scoring slots in both languages, while ?in-
fobox name? (the name of wikipedia page in ques-
tion), ?website?, ?founded? are the highest scoring
slot types.
Slot IZ IE
origin 0.21 0.03
religion 0.24 0.08
parents 0.57 0.60
date of birth 0.84 0.33
spouse 0.97 0.86
founded by 0.97 0.94
website 0.99 0.96
infobox name 1.00 1.00
Table 2: Sample I(s) Values
4 Evaluation
In this section we present the evaluation results of
our approach.
46
4.1 Evaluation Method
Human evaluation of mined name pairs can be dif-
ficult as a human assessor may frequently need to
consult the infoboxes of the entries along with con-
textual documents to determine if a Chinese entry
and an English entry correspond to the same en-
tity. This is especially true when the translations are
based on meanings instead of pronunciations. An al-
ternative way of mining name pairs from Wikipedia
is to extract titles from a Chinese Wikipedia page
and its corresponding linked English page if the link
exists (Ji et al, 2009). This method results in a
very high precision but can miss pairs if no such
link between the pages exists. We utilized these
cross-lingual page links as an answer key and then
only performed manual evaluation, using a bilingual
speaker, on those pairs generated by our algorithm
that were not in the answer key.
4.2 Results
Figure 2 shows the precision, recall and f-score of
the algorithm as it learns more pairs. The final
output of the mining learned 8799 name pairs, of
which 7562 were correct according to the cross-
lingual Wikipedia links. This results in a precision
of 85.94%, a recall of 68.07% and a F1 score of
75.9%. The precision remains above 95% for the
first 7,000 name pairs learned. If highly precise an-
swers are desired, at the expense of recall, the algo-
rithm could be halted earlier. The translation lexicon
contained 18,941 entries, not including translations
learned from the entry names themselves.
Assessment Number
Link Missing From Wikipedia 35 2.8%
Same Name, Different Entity 17 1.4%
Partially Correct 98 7.9%
Incorrect 1,087 87.9%
Table 3: Human Assessment of Errors
Because the answer key for name mining is au-
tomatically extracted from the cross-lingual links
in Wikipedia, it is possible that correct name pairs
could be missing from the answer key if no cross-
lingual link exists. To examine if any such pairs
were learned, a manual assessment of the name pairs
that were not in the answer key was performed, as
shown in Table 4.2. This assessment was performed
by bilingual speakers with an inter-annotator agree-
ment rate of 93.75%.
The vast majority, 87.9%, of the presumably er-
roneous name pairs assessed that were missing from
the answer-key were actually incorrect pairs. How-
ever, 35, or 2.8%, of the name pairs were actually
correct with their corresponding Wikipedia pages
lacking cross-lingual links (these corrections are
not reflected in the previous results reported above,
which were based solely on the pairs in the an-
swer key). For a small portion, 1.4%, of the errors,
the name translation is correct but the entries actu-
ally refer to different entities with the same name.
One such example is (Martin Rowlands / ???).
The English entity, ?Martin Rowlands? is an ath-
lete (an English football player), while the Chinese
entity is a former Hong Kong government official,
whose name translates to English as ?Martin Row-
lands?, as revealed on his Wikipedia page. Neither
entity has an entry in the other language. The fi-
nal category are partially correct answers, such as
the pair (Harrow, London / ???), where the En-
glish entry refers to an area within the London Bor-
ough of Harrow, while the Chinese entry refers to
the London Borough of Harrow as a whole. The
English entry ?Harrow, London? does not have a
corresponding entry in Chinese, although there is
an entry in both language for the larger Borough it-
self. All of these cases represent less 15% of the
learned name pairs though as 85.94% of the name
pairs were already determined to be correct based
on cross-lingual Wikipedia links.
Judgement Percent
Correct 64.4%
Partial 18.4%
Incorrect 15.1%
Not Translations 2.1%
Table 4: Slot Value Translation Assessment from Ran-
dom Sample of 1000
The name mining algorithm bootstraps many
name pairs by using possible translations between
the slot values in previously learned pairs. The fi-
nal translation lexicon learned had 18,941 entries.
A random sample of 1,000 entries from the trans-
47
Figure 2: Performance of Unsupervised Name Mining
lation lexicon was assessed by a human annotator,
and judged as correct, partial, incorrect or not trans-
lations, as shown in Table 4.2. Partial translations
were usually cases where a city was written with
its country name in language and as just the city
name in the other languages, such as ?Taipei Taiwan
Republic of China? and ????? (Taipei). Cases
are marked as ?not translations? if both sides are in
the same language, typically English, such as ?Eric
Heiden? in English being considered a translation of
?Eric Arthur Heiden? from a Chinese entry (not in
Chinese characters though). This normally occurs if
the Chinese page contained English words that were
not translated or transliterated.
An example7 of the name mining is shown in Fig-
ure 3, where the correct name pair for (George W.
Bush / ????????) is learned in iteration i,
is mined for additional translations and then pro-
vides evidence in iteration i+1 for the correct name
pair (Laura Bush / ?????????). When
learning the name pair for ?George W. Bush?, ev-
idence is first found from the slots marked as equiv-
alent (approx). Translations for ?Harvard Busi-
ness School? and ? Republican Party? were learned
in previous iterations from other name pairs and
now provide evidence, along with the identical val-
ues in the ?date of birth? slot for the pair (George
W. Bush / ????????). After learning this
7Many slot value pairs that were not relevant for the calcu-
lation are not shown to save space. Otherwise, this example is
as learned in the unsupervised name mining.
pair, new translations are extracted from the pair
for ?George W. Bush?, ?George Walker Bush?,
?President of the United States?, ?Laura Bush?,
and ?Yale University?. The translations for ?Laura
Bush? and ?George W. Bush? provide crucial in-
formation in the next iteration that the pair (Laura
Bush / ?????????) is correct. From this,
more translations are learned, although not all of
these translations are fully correct, such as ?Author
Teacher Librarian First Lady? which is now pos-
tulated to be a translation of ????? (Librar-
ian), which is only partially true, as the other pro-
fessions are not represented in the translation. While
such translations may not be fully correct, they still
could prove useful for learning future name pairs (al-
though this is unlikely in this case since there are
very few entries with ?first lady? as part of their ti-
tle.
5 Discussion
Besides retaining high accuracy, the final list of
name pairs revealed several advantages of our ap-
proach.
Most previous name translation methods are lim-
ited to names which are phonetically transliterated
(e.g. translate Chinese name ???? (You shen
ke)? to ?Yushchenko? in English). But many other
types of names such as organizations are often ren-
dered semantically, for example, the Chinese name
????? (jie fang zhi hu)? is translated into ?Lib-
eration Tiger? in English. Some other names in-
48
Iteration i
George W. Bush ???????? (George Walker Bush)
alt names George Walker Bush alt names ?????? (George Bush)
title President of the United States title ???? (President of the
USA)
date of birth 1946-7-6 ? date of birth 1946-7-6
member of Republican Party ? member of ??? (Republican Party)
spouse Laura Bush spouse ????????? (Laura
Welch Bush)
schools attended Yale University schools attended ???? (Yale University)
schools attended Harvard Business School ? schools attended ????? (Harvard Business
School)
Iteration i + 1
Laura Bush ????????? (Laura Welch Bush)
alt names Laura Bush ? alt names ????????? (Laura
Welch Bush)
alt names ????????? (Laura
Lane Welch)
date of birth 1946-11-4 ? date of birth 1946-11-4
place of birth Midland Texas place of birth ???????? (Texas
Midland)
title Author Teacher Librarian First
Lady
title ????? (Librarian)
title First Lady of the United States ? title ??????(First Lady of
USA)
spouse George W. Bush ? spouse ???????? (George
Walker Bush)
Figure 3: Example of Learned Name Pairs with Gloss Translations in Parentheses
volve both semantic and phonetic translations, or
none of them. Our approach is able to discover all
these different types, regardless of their translation
sources. For example, our approach successfully
mined a pair (Tarrytown / ???) where ?Tarry-
town? is translated into ????? neither by its pro-
nunciation ?bai you cun? nor its meaning ?tar vil-
lage.?
Name abbreviations are very challenging to trans-
late because they need expansions based on con-
texts. However our approach mined many abbrevia-
tions using slot value comparison. For example, the
pair of (Yctc /????) was successfully mined al-
though its English full name ?Yeh-Chiang Technol-
ogy Corp.? did not appear in the infoboxes.
Huang (2005) also pointed out that name transla-
tion benefited from origin-specific features. In con-
trast, our approach is able to discover name pairs
from any origins. For example, we discovered the
person name pair (Seishi Yokomizo / ????) in
which ?Seishi Yokomizo? was transliterated based
on Japanese pronunciation.
Furthermore, many name translations are context
dependent. For example, a person name in Chinese
?????????? could be translated into ?Yasser
Arafat? (PLO Chairman) or ?Yasir Arafat? (Crick-
eter) based on different contexts. Our method can
naturally disambiguate such entities based on slot
comparison at the same time as translation mining.
More importantly, our final list includes a large
portion of uncommon names, which can be valu-
able to address the out-of-vocabulary problem in
both MT and cross-lingual information processing.
Especially we found many of them are not in the
name pairs mined from the cross-lingual Wikipedia
title links, such as (Axis Communications / ???),
(Rowan Atkinson / ??????), (ELSA Technol-
ogy /?????) and (Nelson Ikon Wu /???).
49
6 Comparison with Previous Methods and
Resources
There have been some previous methods focusing on
mining name translations using weakly-supervised
learning. In addition there are some existing name
translation gazetteers which were manually con-
structed. We duplicated a variety of alternative
state-of-the-art name translation mining methods
and mined some corresponding name pair sets for
comparison. In fact we were able to implement the
techniques in previous approaches but could not du-
plicate the same number of results because we could
not access the same data sets. Therefore the main
purpose of this experiment is not to claim our ap-
proach outperforms these existing methods, rather
to investigate whether we can mine any new infor-
mation on top of these methods from reasonable
amounts of data.
1. Name Pair Mining from Bitexts
Within each sentence pair in a parallel cor-
pus, we ran an HMM based bilingual name
tagger (references omitted for anonymous re-
view). If the types of the name tags on both
sides are identical, we extract the name pairs
from this sentence. Then at the corpus-wide
level, we count the frequency for each name
pair, and only keep the name pairs that are fre-
quent enough. The corpora used for this ap-
proach were all DARPA GALE MT training
corpora.
2. Comparable Corpora
We implemented an information extraction
driven approach as described in Ji (2009) to
extract name pairs from comparable corpora.
This approach is based on extracting infor-
mation graphs from each language and align
names by a graph traverse algorithm. The cor-
pora used for this approach were 2000 English
documents and 2000 Chinese documents from
the Gigaword corpora.
3. Using patterns for Web mining
We constructed heuristic patterns such as par-
enthetical structure ?Chinese name (English
name)? (Lin et al, 2008) to extract name pairs
from web data with mixed Chinese and En-
glish. We used about 1,000 web pages for this
experiment.
4. Bilingual Gazetteer
We exploited an LDC bilingual name dictio-
nary (LDC2005T34) and a Japanese-English
person name dictionary including 20126
Japanese names written in Chinese charac-
ters (Kurohashi et al, 1994).
5. ACE2007 Entity Translation Training Data
We also used ACE 2007 entity translation train-
ing corpus which includes 119 Chinese-English
document pairs.
Table 5 shows the number of correct and unique
pairs mined pairs from each of the above ap-
proaches, as well as how these name mining meth-
ods can be augmented using the infobox name min-
ing described in this paper. The names mined from
our approach greatly extend the total number of cor-
rect translations with only a small number of con-
flicting name translations.
7 Related Work
Most of the previous name translation work com-
bined supervised transliteration approaches with
Language Model based re-scoring (Al-Onaizan and
Knight, 2002; Huang et al, 2004; Huang, 2005).
Our goal of addressing name translation for a large
number of languages is similar to the panlingual lex-
ical translation project (Etzioni et al, 2007). Some
recent research used comparable corpora to re-score
name transliterations (Sproat et al, 2006; Klemen-
tiev and Roth, 2006) or mine new word transla-
tions (Udupa et al, 2009; Ji, 2009; Fung and Yee,
1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al,
2007). However, most of these approaches needed
large amount of seeds and suffered from informa-
tion extraction errors, and thus relied on phonetic
similarity or document similarity to re-score candi-
date name translation pairs.
Some recent cross-lingual information access
work explored attribute mining from Wikipedia
pages. For example, Bouma et al (2009) aligned at-
tributes in Wikipedia infoboxes based on cross-page
links. Navigli and Ponzetto (2010) built a multi-
lingual semantic network by integrating the cross-
lingual Wikipedia page links and WordNet. Ji et
50
# Name Infobox Mining
Method Pairs # New # Conflicting
Automatic
(1) Bitexts 2,451 8,673 78
(2) Comparable Corpora 288 8,780 13
(3) Patterns for Web Mining 194 8799 0
Manual (4) Bilingual Gazetteer 59,886 8,689 74(5) ACE2007 Training Data 1,541 8,718 52
Table 5: Name Pairs Mined Using Previous Methods
al. (2009) described various approaches to auto-
matically mine name translation pairs from aligned
phrases (e.g. cross-lingual Wikipedia title links)
or aligned sentences (bi-texts). G et al (2009)
mined candidate words from Wikipedia and vali-
dated translations based on parallecl corpora. Some
other work mined name translations from mono-
lingual documents that include foreign language
texts. For example, Lin et al (2008) described a
parenthesis translation mining method; You et al
(2010) applied graph alignment algorithm to ob-
tain name translation pairs based on co-occurrence
statistics. This kind of data does not commonly exist
for low-density languages. Sorg and Cimiano (2008)
discovered cross-lingual links between English and
German using supervised classification based on
support vector machines. Adar et al (2009) aligned
cross-lingual infoboxes using a boolean classifier
based on self-supervised training with various lin-
guistic features. In contrast, our approach described
in this paper is entirely based on unsupervised learn-
ing without using any linguistic features. de Melo
and Weikum (2010) described an approach to detect
imprecise or wrong cross-lingual Wikipedia links
based on graph repair operations. Our algorithm can
help recover those missing cross-lingual links.
8 Conclusion and Future Work
In this paper we described a simple, cheap and ef-
fective self-boosting approach to mine name trans-
lation pairs from Wikipedia infoboxes. This method
is implemented in a completely unsupervised fash-
ion, without using any manually created seed set,
training data, transliteration or pre-knowledge about
the language pair. The underlying motivation is
that some certain expressions, such as numbers and
dates, are written in language-independent forms
among a large majority of languages. Therefore our
approach can be applied to any language pairs in-
cluding low-density languages as long as they share
a small set of such expressions. Experiments on
English-Chinese pair showed that this approach is
able to mine thousands of name pairs with more
than 85% accuracy. In addition the resulting name
pairs can be used to significantly augment the results
from existing approaches. The mined name pairs are
made publicly available.
In the future we will apply our method to mine
other entity types from more language pairs. We
will also extend our name discovery method to all
infobox pairs, not just those that can be mapped
into KBP-like slots. As a bi-product, our method
can be used for automatic cross-lingual Wikipedia
page linking, as well as unsupervised translation lex-
icon extraction, although this might require confi-
dence estimates on the translations learned. Once
our approach is applied to a panlingual setting (most
languages on the Wikipedia), we can also utilize
the voting results across multiple languages to au-
tomatically validate information or correct poten-
tial errors in Wikipedia infoboxes. Finally, as au-
tomatic name profile generation systems are gener-
ated cross-lingually, our method could be attempted
to automatic cross-lingual mappings between enti-
ties.
Acknowledgement
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
Number W911NF-09-2-0053, the U.S. NSF CA-
REER Award under Grant IIS-0953149 and PSC-
CUNY Research Program. The views and con-
clusions contained in this document are those of
the authors and should not be interpreted as repre-
51
senting the official policies, either expressed or im-
plied, of the Army Research Laboratory or the U.S.
Government. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
ment purposes notwithstanding any copyright nota-
tion hereon.
References
Eytan Adar, Michael Skinner, and Daniel S. Weld. 2009.
Information arbitrage across multi-lingual wikipedia.
In Second ACM International Conference on Web
Search and Data Mining (WSDM?09), Barcelona,
Spain, February 2009, February.
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In ACL 2002.
Rodrigo B. Almeida, BarzanMosafari, and Junghoo Cho.
2007. On the evolution of wikipedia. In Int. Conf. on
Weblogs and Social Media.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. 2007.
Dbpedia: A nucleus for a web of open data. In The 6th
International Semantic Web Conference.
Kurt Bollacker, Robert Cook, and Patrick Tufts. 2007.
Freebase: A shared database of structured general hu-
man knowledge. In The National Conference on Arti-
ficial Intelligence (Volume 2).
Gosse Bouma, Sergio Duarte, and Zahurul Islam. 2009.
Cross-lingual alignment and complettion of wikipedia
templates. In The Third International Workshop on
Cross Lingual Information Access: Addressing the In-
formation Need of Multilingual Societies.
Gerard de Melo and Gerhard Weikum. 2010. Untangling
the cross-lingual link structure of wikipedia. In 48th
Annual Meeting of the Association for Computational
Linguistics (ACL 2010), Uppsala, Sweden.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach for
translating new words from nonparallel and compara-
ble texts. In COLING-ACL.
Rohit Bharadwaj G, Niket Tandon, and Vasudeva Varma.
2009. An iterative approach to extract dictionaries
from wikipedia for under-resourced languages. In
Proc. ICON2010, February.
Ahmed Hassan, Haytham Fahmy, and Hany Hassan.
2007. Improving named entity translation by exploit-
ing comparable and parallel corpora. In RANLP.
Fei Huang, Stephan Vogel, and Alex Waibel. 2004. Im-
proving named entity translation combining phonetic
and semantic similarities. In HLT/NAACL2004.
Fei Huang. 2005. Cluster-specific name transliteration.
In HLT-EMNLP 2005.
IBM. 2010. Ibm globalization library.
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias
Blume, John Wang, Shahram Khadivi, Richard Zens,
and Hermann Ney. 2009. Name translation for distil-
lation. Handbook of Natural Language Processing and
Machine Translation: DARPA Global Autonomous
Language Exploitation.
Heng Ji, Ralph Grishman, Hoa Trang Dang, and Kira
Griffitt. 2010. An overview of the tac2010 knowledge
base population track. In Text Analytics Conference
(TAC2010).
Heng Ji. 2009. Mining name translations from com-
parable corpora by creating bilingual information net-
works. In ACL-IJCNLP 2009 workshop on Building
and Using Comparable Corpora (BUCC 2009): from
Parallel to Non-parallel Corpora.
Michael E. Krauss. 2007. Keynote-mass Language Ex-
tinction and Documentation: The Race Over Time. The
Vanishing Languages of the Pacific Rim. Oxford Uni-
versity Press.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of japanese
morphological analyzer juman. In The International
Workshop on Sharable Natural Language Resources
and pp.22-28.
Dekang Lin, Shaojun Zhao, Benjamin Van Durme, and
Marius Pasca. 2008. Mining parenthetical translations
from the web by word alignment. In ACL2008.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belnet: Building a very large multilingual semantic
network. In 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), Uppsala,
Sweden.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In ACL 1999.
Li Shao and Hwee Tou Ng. 2004. Mining new
word translations from comparable corpora. In COL-
ING2004.
Philipp Sorg and Philipp Cimiano. 2008. Enrich-
ing the crosslingual link structure of wikipedia - a
classification-based approach. In AAAI 2008 Work-
shop on Wikipedia and Artifical Intelligence, June.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowledge.
In The 16th International World Wide Web conference.
Raghavendra Udupa, K. Saravanan, A. Kumaran, and Ja-
gadeesh Jagarlamudi. 2009. Mint: A method for ef-
fective and scalable mining of named entity transliter-
ations from large comparable corpora. In EACL2009.
Gae-won You, Seung won Hwang, Young-In Song, Long
Jiang, and Zaiqing Nie. 2010. Mining name transla-
tions from entity graph mapping. In EMNLP2010.
52
