Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 566?571,
Dublin, Ireland, August 23-24, 2014.
SINAI: Voting System for Aspect Based Sentiment Analysis
Salud Mar??a Jim
?
enez-Zafra, Eugenio Mart??nez-C
?
amara,
M. Teresa Mart??n-Valdivia, L. Alfonso Ure
?
na-L
?
opez
SINAI Research Group
University of Ja?en
E-23071, Ja?en (Spain)
{sjzafra, emcamara, maite, laurena}@ujaen.es
Abstract
This paper describes the participation of
the SINAI research group in Task 4 of the
2014 edition of the International Work-
shop SemEval. This task is concerned
with Aspect Based Sentiment Analysis
and its goal is to identify the aspects of
given target entities and the sentiment ex-
pressed towards each aspect.
1 Introduction
The web has evolved progressively since its be-
ginning in 1990. At first, the user was almost a
passive subject who received the information or
published it, without many possibilities to gener-
ate an interaction. The emergence of the Web 2.0
was a social revolution, because it offered users
the possibility of producing and sharing contents,
opinions, experiences, etc.
Some years ago it was common to ask family
and friends to know their opinion about a particu-
lar topic, but after the emergence of the Web 2.0,
the number of Internet users has been greatly in-
creased. The exponential growth of the subjective
information in the last years has created a great in-
terest in the treatment of this information.
Opinion Mining (OM), also known as Senti-
ment Analysis (SA) is the discipline that focuses
on the computational treatment of opinion, sen-
timent and subjectivity in texts (Pang and Lee,
2008). Currently, OM is a trendy task in the field
of Natural Language Processing due mainly to the
fact of the growing interest in the knowledge of
the opinion of people from different sectors of the
society. However, the study on Opinion Mining
goes back to 2002 when two of the most cited arti-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
cles in this task were published (Pang et al., 2002)
(Turney, 2002).
OM or SA can be divided into two subtasks
that are known as subjectivity classification and
polarity classification. Subjectivity classification
is the task concentrated on the identification of
subjectivity in texts, that is, these systems are bi-
nary classifiers that separate the documents in two
classes, objective and subjective ones. On the
other hand, polarity classification is the task of de-
termining the semantic orientation of a subjective
text. The ideal OM system has to be composed
by a subjectivity classifier and a polarity classifier.
However, most of the works in the field of OM are
carried out considering the documents as subjec-
tive, so polarity classification systems have been
more studied than subjectivity classification ones.
The reader can find a complete overview about the
research in OM in (Pang and Lee, 2008) and (Liu,
2012).
As Liu asserts in (Liu, 2012), the polarity clas-
sification systems can be divided into three levels:
? Document level polarity classification:
This kind of systems assumes that each doc-
ument expresses an opinion on a single entity
(Pang et al., 2002) (Turney, 2002).
? Sentence level polarity classification: In
this case the polarity classification systems
are focused on the identification of the level
of polarity of each sentence of the docu-
ment (Wilson et al., 2005) (Yu and Hatzivas-
siloglou, 2003).
? Entity and Aspect level polarity classifi-
cation: These systems accomplish a finer-
grained sentiment classification. Whereas the
document-level and sentiment-level only dis-
cover the overall sentiment expressed by the
author, the goal of the entity and aspect po-
larity classification is the identification of the
566
sentiment of the author towards each entity or
aspect.
An entity usually is composed by several as-
pects, for example a telephone is formed by a
headset, which also consists of a speaker and an
earphone. An entity can be regarded as a hierarchy
of all the aspects whose head is the entity, so the
entity can also be considered as an aspect or gen-
eral aspect. Therefore, the task ?entity and aspect
level polarity classification? can be called ?aspect
polarity classification?.
The main objective of OM at aspect level is to
discover every quintuple (e
i
, a
ij
, s
ijkl
, h
k
, t
l
) in
a given document, where e
i
is the entity, a
ij
is
one of the aspects of the entity or the entity and
s
ijkl
is the orientation of the opinion expressed
by the opinion holder h
k
in a certain moment t
l
.
To achieve the objective of populate the quintu-
ple is needed the splitting of the task into several
subtasks that correspond with the identification of
the aspect, the author or the holder of the opinion
and the moment when the opinion is expressed or
posted. But in a real scenario, OM at aspect level
is also limited like OM at sentence and document
level, and most of the research works are only fo-
cused on the identification of the aspect and in the
calculation of the level of intensity of the senti-
ment stated about the aspect. However, there are
some papers that are closely to the goal of finding
out each of the components of the quintuple (Kim
and Hovy, 2004) (Kim and Hovy, 2006).
The task four of the 2014 edition of SemEval
workshop aims to promote the research polarity
classification systems at aspect level. The task is
divided into four subtasks, two of them related to
the aspect identification and the other with the po-
larity classification. Due to the fact that OM is a
domain-dependent task, the organization proposes
the four subtasks in two different domains, Restau-
rants and Laptops. Task one and three are the
ones linked to the aspect identification. Subtask
one is focused on the identification of the aspects
in each review of the two given corpus. Subtask
three goes one step further, in which the main ob-
jective is for a given predefined set of aspect cate-
gories, identify the aspect categories discussed in
the given sentence. Subtask two proposes the clas-
sification of the sentiment expressed by the author
about each of the aspects extracted, and subtask
four has as challenge the classification of the po-
larity of each of the categories of the aspects. A
wider description of the task and the datasets used
can be found in the task description paper (Pontiki
et al., 2014).
The rest of the paper is organized as follows.
Section two outlines the two main parts of our pro-
posed system, firstly the strategy to solve the sub-
task 1 and 2 and then the method used to resolve
the subtask 3 and 4. To sum up the paper, an anal-
ysis of the results and the conclusion of this work
are shown in section three and four respectively.
2 System description
The guidelines of this task indicate that each team
may submit two runs: constrained (using only the
provided training data and other resources, such as
lexicons) and unconstrained (using additional data
for training). We decided to follow an unsuper-
vised approach that we present below.
Our system is divided into two subsystems (Fig-
ure 1). The aim of the first subsystem is to extract
the aspect terms related to a given target entity
(subtask 1) and calculate the sentiment expressed
towards each aspect in the opinion (subtask 2).
The goal of the second is, for a given set of cat-
egories, to identify the categories discussed in the
review (subtask 3) and determine its polarity (sub-
task 4).
2.1 Subsystem 1: Aspects Identification and
Polarity Classification
To identify the aspects related with the target en-
tity (laptops or restaurants) we decided to use
a bag of words built from all the aspect terms
present in the training data. But this method
only detects previously tagged aspect in the train-
ing data, so, we enriched the list of words with
data automatically extracted from the collabora-
tive knowledge base Freebase
1
, in order to im-
prove the identification. For this, we obtained
all categories in restaurants domain and in com-
puters domain
2
(types in a domain) using MQL
3
(Metaweb Query Language) (Figure 2).
Then, for each domain category we extracted all
terms (instances of a type) to enrich the bag. In
Figure 3 we can see an example to get all terms of
1
http://www.freebase.com/
2
Nowadays, Freebase has more than 70 different domains.
But, for this task, we are only interested in these two.
3
MQL is a language which is used to express Metaweb
queries. This allows you to incorporate knowledge from the
Freebase database into your own applications and websites.
567
Figure 1: Arquitecture of the system.
Figure 2: Query for list all categories in food do-
main.
a category, in particular cheese category of food
domain.
Figure 3: Query for list all term in cheese category.
In this way, given a review of the test data, the
first step is to tokenize it to get a vector of uni-
grams with all single words in the text (we do not
divide the reviews into sentences because there is
only one sentence per review). The second step
is to represent each review as a list of n lists of
unigrams, bigrams, . . . , n-grams where n is the
number of tokens in the sentence. This is because
an aspect term can be a nominal phrase, a word
formed from a verb but functioning as a different
part of speech (e.g. gerunds and participles) or a
simple term. For example, the review ?The salad
was excellent as was the lamb chettinad? is repre-
sented as shown in Figure 4.
After obtaining the possible terms of a review,
the next step is to go over the list of lists to ex-
tract the aspects. Each list is traversed backwards
matching each term with each aspect from the bag.
When an aspect is found or the top of the list is
reached the search begins in the next list. In the
review showed in Figure 4, the system will iden-
tify two aspects: salad and lamb chettinad. The
search in this example begins in the list 1 with
?The salad was excellent as was the lamb chetti-
nad?, ends with ?The? and continues with the next
list, because the top of the list is reached. The
search in the list 2 begins with ?salad was excel-
lent as was the lamb chettinad?, ends with ?salad?
because it is an aspect and continues with the list
3 and so on. At last, the search in the list 8 be-
gins with the term ?lamb chettinad?, ends with it
because it is an aspect presents in the bag of words
and continues with the list 9.
Once extracted the aspects related with the tar-
get entity, the next step is to determine the words
that modify each aspect. For this, we have used
the Stanford Dependencies Parser
4
. This parser
4
http://nlp.stanford.edu/software/lex-parser.shtml
568
Figure 4: Possible terms of the sentence ?The salad was excellent as was the lamb chettinad?.
was designed to provide a simple description of
the grammatical relationships in a sentence that
can easily be understood and effectively used by
people without linguistic expertise who want to
extract textual relations (De Marneffe and Man-
ning, 2008). It represents all sentence relation-
ships uniformly as typed dependency relations. In
this work, we have considered the main relation-
ships for expressing opinion about an aspect: us-
ing a verb (?nsubj? or ?nsubjpass?), an adjectival
modifier (?amod?) or a dependency relation with
another word (?dep?). In the review ?The salad
was excellent as was the lamb chettinad?, the sys-
tem will identify two modifiers words: the ad-
jective excellent that expresses how is the salad
through the relationship ?nsubj? and the adjective
excellent that also modified the aspect lamb chet-
tinad through the relationship ?dep? Figure 5.
To determine the sentiment expressed over an
aspect we have calculated the polarity of each
word that modifies it through a voting system
based on three classifiers: Bing Liu Lexicon (Hu
and Liu, 2004), SentiWordNet (Baccianella et al.,
2010) and MPQA (Wilson et al., 2005). The Bing
Liu Lexicon is a list of 2006 positive words and
another with 4783 negative ones. MPQA is also
a subjectivity lexicon with positive and negative
words and has extra information about each one:
the part-of-speech, the strength, etc. Finally, Sen-
tiWordNet is a lexical resource that assigns to each
synset of WordNet three sentiment scores: positiv-
ity, negativity and objectivity. Therefore, an aspect
is positive/negative if there are at least two clas-
sifiers that tag it as positive/negative and neutral
in another case. It may happen that a word is af-
fected by negation, to treat this problem we have
used a straightforward method, the fixed window
size method. We have considered the negative par-
ticles: ?not?, ?n?t?, ?no?, ?never?. So if any of the
preceding or following 3 words to one aspect is
one of these negative particles, the aspect polarity
is reversed (positive ?> negative, negative ?>
positive, neutral ?> neutral).
In the example showed in Figure 5, the aspect
salad is modified by the word excellent that also
modified the aspect lamb chettinad. This adjective
is part of the Bing Liu positive list, MPQA classi-
fies it as positive and SentiWordNet assigns it the
scores: 1 (positivity), 0(objectivity), 0 (objectiv-
ity). Then, the aspects salad and lamb chettinad
are classified as positive by the voting system.
Figure 5: Dependency analysis of the sentence:
?The salad was excellent as was the lamb chetti-
nad?.
569
2.2 Subsystem 2: Categories Identification
and Polarity Classification
As we have mentioned above, this subsystem fo-
cuses on the treatment of the categories and has
been used only with the dataset of restaurants.
On the one hand, we have built a bag of words
for each of the given categories related to the tar-
get entity (restaurants). We have tagged manu-
ally each aspect of the bag of words, built for the
first subsystem, in one of the categories of the
given set (food, service, price, ambience, anec-
dotes/miscellaneous). Thus, to determine the cat-
egories that are referenced in a review we have
searched each aspect identified with the first sub-
system in each bag, if the aspect belongs to any
category then this category is identified. If any as-
pect belongs to a category, then the category allo-
cated is ?anecdotes/miscellaneous?.
On the other hand, the sentiment expressed
about each category has been calculated as the
most frequent polarity of the aspects that belongs
to this category. In case of a tie between positive
and negative values, the polarity value conflict is
assigned to the category. If any aspect belongs to
the category, then the polarity value of the review
is assigned to the category.
In the above example, the aspects salad and
lamb chettinad belong to food?s bag of words, so
that the system will identify that the category food
is discussed in this review and will assign it the
polarity value positive, because the sentiment ex-
pressed about the two aspects that belongs to this
category is positive.
3 Analysis of the results
The aim of this section is to provide a meaningful
report of the results obtained after participation in
the task related to Aspect Based Sentiment Anal-
ysis (ABSA). Table 1 shows the evaluation results
for the aspect extraction subtask. As we can see,
the recall overcomes the mean value of results of
participants in both domains (laptops and restau-
rants), that is, the system identifies quite aspects
of the corpus. However, the precision is lower
because the system identifies aspects that are not
considered by the organization, due to the fact that
our bag of words contains more aspects than the
tagged by the organization.
The results reached in the aspect term extraction
subtask are similar (Table 2). It should be taken
into account that the system is a general-domain
Laptops Restaurants
SINAI Average SINAI Average
Precision 0.3729 0.6890 0.5961 0.7674
Recall 0.5765 0.5045 0.72487 0.6726
F-score 0.4529 0.5620 0.6542 0.7078
Table 1: Aspect Term Extraction results.
sentiment classifier, so it does not use specific
knowledge for each of the domains. This fact can
be shown in the results reached in the task of po-
larity classification for the two domains, which are
similar. Therefore, this subtask could be improved
by taking into account the domain and other rela-
tionships for expressing opinion about an aspect
apart from that we have treated (?nsubj?, ?nsubj-
pass?, ?amod?, ?dep?).
Laptops Restaurants
SINAI Average SINAI Average
Accuracy 0.5872 0.5925 0.5873 0.6910
Table 2: Aspect Term Polarity results.
On the other hand, the results in the identifica-
tion of the categories discussed in a review have
been high (Table 3) and even overcome the aver-
age recall of the participating systems. At last, Ta-
ble 4 shows the result evaluation of the aspect cat-
egory polarity subtask that are slightly lower than
the average. These tables show that is possible to
reach good results using a simple approach as de-
scribed in subsection 2.2.
Restaurants
SINAI Average
Precision 0.6659 0.76
Recall 0.8244 0.7226
F-score 0.7367 0.7379
Table 3: Aspect Category Detection results.
4 Conclusion and future works
In SA can be differentiated three levels of study of
a text: document level, sentence level and aspect
level. The document level analysis determines the
overall sentiment expressed in a review, while the
sentence level analysis specifies for each sentence
of a text, whether express a positive, negative or
neutral opinion. However, these two types of anal-
570
Restaurants
SINAI Average
Accuracy 0.6030 0.6951
Table 4: Aspect Category Polarity results.
ysis do not reach the level of detail that an user
wants when searches for information about a prod-
uct. The fact that the overall sentiment of a prod-
uct is positive does not mean that the author has a
positive opinion about all aspects of that product,
or the fact that is negative does not involve that
everything about the product is bad.
In addition, the large amount of sources and
the high volume of texts with reviews, make dif-
ficult for the user to select information of interest.
Therefore, it is necessary to develop classification
systems at aspect level that help users to make de-
cisions and, on the other hand, that show compa-
nies the opinion that consumers have about their
products, in order to help them to decide what to
keep, what to delete and what to improve.
In this paper we have presented our first ap-
proach for the Aspect Based Sentiment Analysis
that has been developed for the task four of the
2014 edition of SemEval workshop. After analyz-
ing the evaluation results we consider that is pos-
sible to introduce some improvements we are cur-
rently working: domain adaptation in the polarity
calculation, consideration of other relationships
to determine which words modify an aspect and
treatment of negation (in the system proposed we
have used the fixed window size method). Also, in
a near future we will try to extrapolate it to Span-
ish reviews.
Acknowledgments
This work has been partially supported by a grant
from the Fondo Europeo de Desarrollo Regional
(FEDER), ATTOS project (TIN2012-38536-C03-
0) from the Spanish Government, AORESCU
project (P11-TIC-7684 MO) from the regional
government of Junta de Andaluc??a and CEATIC-
2013-01 project from the University of Ja?en.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC, volume 10, pages 2200?2204.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. Stanford typed dependencies manual.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th International Conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, SST ?06,
pages 1?8, Stroudsburg, PA, USA.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical Methods in Natu-
ral Language Processing - Volume 10, EMNLP ?02,
pages 79?86, Stroudsburg, PA, USA.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
Peter D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 417?424, Stroudsburg, PA,
USA.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?03, pages 129?136, Strouds-
burg, PA, USA.
571
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 572?577,
Dublin, Ireland, August 23-24, 2014.
SINAI: Voting System for Twitter Sentiment Analysis
Eugenio Mart
?
?nez-C
?
amara, Salud Mar
?
?a Jim
?
enez-Zafra,
M. Teresa Mart
?
?n-Valdivia, L. Alfonso Ure
?
na-L
?
opez
SINAI Research Group
University of Jae?n
E-23071, Jae?n (Spain)
{emcamara, sjzafra, maite, laurena}@ujaen.es
Abstract
This article presents the participation of
the SINAI research group in the task Sen-
timent Analysis in Twitter of the SemEval
Workshop. Our proposal consists of a
voting system of three polarity classifiers
which follow a lexicon-based approach.
1 Introduction
Opinion Mining (OM) or Sentiment Analysis (SA)
is the task focuses on the computational treatment
of opinion, sentiment and subjectivity in texts
(Pang and Lee, 2008). Currently, OM is a trendy
task in the field of Natural Language Processing
due mainly to the fact of the growing interest in
the knowledge of the opinion of people from dif-
ferent sectors of the society.
The interest in the research community for the
extraction of the sentiment in Twitter posts is re-
flected in the organization of several workshops
with the aim of promoting the research in this task.
Two are the most relevant, the first is the task
Sentiment Analysis in Twitter celebrated within
the SemEval workshop whose first edition was in
2013 (Nakov et al., 2013). The second is the work-
shop TASS
1
, which is a workshop for promot-
ing the research in sentiment analysis in Spanish
in Twitter. The first edition of the workshop took
place in 2012 (Villena-Roma?n et al., 2013).
The 2014 edition of the task Sentiment Analy-
sis in Twitter proposes a first subtask, which has
as challenge the sentiment classification at entity
level, and a second subtask that consists of the
polarity classification at document or tweet level.
The training corpus is the same than the former
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://www.daedalus.es/TASS
edition, but this year the test corpus is consider-
ably bigger than the prior one. A wider description
of the task and the corpus can be read in (Rosen-
thal et al., 2014).
We present an unsupervised polarity classifica-
tion system for the subtask B of the task Senti-
ment Analysis in Twitter. The system is based
on a voting strategy of three lexicon-based senti-
ment classifiers. The sentiment analysis research
community broadly knows the lexicons selected.
They are, SentiWordNet (Baccianella et al., 2010),
the lexicon developed by Hu and Liu (Hu and
Liu, 2004) and the MPQA lexicon (Wilson et al.,
2005).
The rest of the paper is organized as follows.
The following section focuses on the description
of the different sentiment resources used for de-
veloping the sentiment classifiers. The subsequent
section outlines the system proposed for the 2014
edition of the task. The last section exposes the
analysis of the results reached this year.
2 Sentiment lexical resources
Sentiment lexicons are lexical resources com-
posed of opinion-bearing words and some of them
also of sentiment phrases of idioms. Most of the
sentiment lexicons are formed by a list of words
without any additional information.
A sentiment classifier based on list of opinion-
bearing words usually consists of finding out the
words of the list in a given document. This method
can be considered very simple for the complexity
of OM, but it has reached acceptable results in dif-
ferent domains and also is applied in real systems
like Trayt.com
2
.
Our experience in the field of SA allows us to
assert that sentiment lexicons can be divided de-
pending on the information linked to each word,
2
Trayt.com is a search engine of reviews of restaurants.
The polarity classifier of Tryt.com is a lexicon-based system
which uses the opinion list compiled by Bing Liu.
572
so three groups can be found:
? List of opinion-bearing words: These lexi-
cons are usually two lists of polar words, one
of them of positive words and another one
of negative terms. Some examples of this
kind of sentiment lexicons are for English the
one compiled by (Hu and Liu, 2004), and for
Spanish, the iSOL lexicon (Molina-Gonza?lez
et al., 2013).
? List of opinion-bearing words with syntactic
information: As it is wider known, OM is a
domain-dependent task and can be also said
that a context-dependent task. Thus, some
lexicons add syntactic information with the
aim of offering some information for disam-
biguating the term, and also provide a differ-
ent orientation of the word depending on its
POS-tags. One example of this kind of lexi-
con is MPQA subjectivity lexicon (Wilson et
al., 2005).
? Knowledge base sentiment lexicons: These
lexicons usually indicate the semantic orien-
tation of the different senses of each word,
whereas the previous lexicons only indicate
the polarity of each word. Also, it is very
common that in the knowledge base senti-
ment lexicons each sense is linked to the like-
lihood of being positive, negative and neutral.
One example of this kind of polar lexicon is
SentiWordNet (Baccianella et al., 2010).
In the polarity classifier developed for the work-
shop a lexicon of each type has been utilised. The
sentiment linguistic resources used has been:
? Sentiment lexicon compiled by Bing Liu:
The lexicon was used the first time in (Hu
and Liu, 2004). Since then, the authors have
been updating the list, and currently the list is
formed by 2006 positive words and 4783 neg-
ative words. Also, the lexicon includes some
misspellings with the aim of better represent-
ing the language used in the Internet.
? MPQA Subjectivity lexicon (Wilson et al.,
2005): The lexicon is formed by over 8000
subjectivity clues. Subjectivity clues are
words and phrases that have subjective us-
ages. The lexicon was developed joining
words compiled by the authors and with
words taken from General Inquirer. Each
Figure 1: Architecture of the system.
word is linked with its grade of subjectivity,
with its part of speech tag and with its seman-
tic orientation. Due to the fact that each word
has its POS-tag there are some words that de-
pending on its POS have a different semantic
orientation.
? SentiWordNet 3.0 (Baccianella et al., 2010):
is a lexical resource which assigns three sen-
timent scores to each synset of WordNet:
positivity, negativity and objectivity.
3 Polarity classification
We wanted to take advantage from our experi-
ence in meta-classification in OM for the 2014
edition of the task, Sentiment Analysis in Twit-
ter. We have reached good results in OM us-
ing meta-classifiers in different domains (Perea-
Ortega et al., 2013) and (Mart??n-Valdivia et al.,
2013). Therefore, we propose a voting system that
combines three polarity classifiers. The general ar-
chitecture of the system is shown in Figure 1.
Tokenization is a common step of the three clas-
sifiers. Due to the specific characteristics of the
language used in Twitter, a specific tokenizer for
Twitter was preferred to use. The tokenizer pub-
lished by Christopher Potts
3
was selected and up-
dated, with the aim of recognizing a wider range
of tokens.
When the tweet is tokenized, the following step
is discover its polarity. Each of the three polarity
classifiers follows the same strategy for the clas-
sification, but they perform different operations
on each tweet. The classifier based on the lexi-
con compiled by Bing Liu (C BingL) consists of
seeking each token in the opinion-bearing words
3
http://sentiment.christopherpotts.net/tokenizing.html
573
list. Therefore, after the tokenization, any linguis-
tic operation has to be performed on the tweet.
This classifier classifies a tweet as positive if the
number of positive tokens is greater or equal than
the number of negative tokens. If there are not po-
lar tokens, the polarity of the tweet is neutral.
The second polarity classifier is the based on
MPQA lexicon (C MPQA). Some of the words
that are in the MPQA lexicon are lemmatized,
and also the sentiment depends on their POS-tag.
Thus, to take advantage of all the information of-
fered by MPQA is needed to perform a morpho-
logical analysis to each tweet. The morphological
analysis firstly identifies the POS-tag of each to-
ken of the tweet, and then the lemmatizer extracts
the lemma of the token.
Recently, some linguistic tools have been pub-
lished to carry out linguistic analysis in tweets.
Currently, two POS-taggers for Twitter are avail-
able. One of them, is the described in (Gimpel et
al., 2011) and the second one in (Derczynski et al.,
2013). Although the authors of the two systems
are competing for which of the two taggers are bet-
ter, our selection was based on the usability of the
two systems. To use the tagger developed by Gim-
pel et al. is needed to download their software,
meanwhile the one developed by Derczynski et al.
can be integrated in other taggers. On our point of
view, the tagger of Derczynski et al. has the ad-
vantage of offering the training model of the tag-
ger
4
, which allows us to integrate it in other POS-
tagging tools. The training model of the tagger
was integrated in the Stanford Part-of-Speech Tag-
ger
5
. When each token of the tweet is associated
with its corresponding POS-tag, the lemmatizer is
run over the tweet. The lemmatizer used is the of-
fered by the toolkit for Natural Language Process-
ing, NLTK (Bird et al., 2009). When each token
is accompanied by its corresponding POS-tag and
lemma, the polarity classifier can seek each token
in the MPQA subjective lexicon.
Besides the label of the polar class (positive or
negative), each entry in the MPQA corpus has a
field called type, which indicates whether the term
is considered strongly subjective or the term is
considered weakly subjective. Thus, in the calcu-
lation of the polarity score these two levels of sub-
jectivity are considered, so when the term is strong
subjective it is considered to have a score of 1, and
4
https://gate.ac.uk/wiki/twitter-postagger.html
5
http://nlp.stanford.edu/software/tagger.shtml
when the term is weak subjective the system con-
siders the term as less important and its score is
0.75.
The polarity classifier based on the use of Sen-
tiWordNet (C SWN) needs that each word of the
tweet is linked with its POS-tag and its lemma,
so the same pipeline that the classifier based on
MPQA follows is also followed by the classifier
based on SentiWordNet.
In the bibliography about OM can be found dif-
ferent ways to calculate the polarity class when
SentiWordNet is used as a sentiment knowledge
base. Some works perform a disambiguation
method with the aim of selecting only the synset
that corresponds with the sense of the word in
the context of the given document. But there are
other works that do not perform any disambigua-
tion method, and also reach good results. De-
necke in (Denecke, 2008) describes a very sim-
ple method to calculate the polarity of each of the
words of a document without the need of a dis-
ambiguation process. The method consists of cal-
culating per each word in the document, which is
in SentiWordNet, the arithmetic mean of the pos-
itive, negative and neutral score of each of the
synsets that the word has in SentiWordNet. When
the scores of each word are calculated, the score
of the document is determined as the arithmetic
mean of each score of the words. The class of the
document is corresponded with the greatest polar
score (positive, negative, neutral). Due to the ac-
ceptable results that the Denecke formula reaches,
we have introduced a soft disambiguation process
with the aim of improving the classification ac-
curacy. This soft disambiguation process consist
of only taking those synsets corresponding with
POS-tag of the word whose polarity are being cal-
culated. For example, the word ?good? can do the
function of an adverb, a noun or an adjective. In
SentiWordNet, there are two synsets of ?good? as
an adverb, four synset of ?good? as a noun, and
twenty-one synsets as an adjective. If the polar-
ity score is calculated with the Denecke formula,
the twenty-seven synsets are used. Meanwhile, if
it used our proposal, and the word ?good? in the
given sentence is acting as an adverb, then only
the two synsets of the word ?good? when it is ad-
verb are considered to calculate the polarity score.
During the development of the system, we no-
ticed that synsets have a lower probability to be
positive or negative, and most of them in Senti-
574
WordNet are neutral. With the aim of boosting the
likelihood to be positive or negative, the polarity
classifier does not consider the neutral score of the
synset. If the positive score is greater than the neg-
ative score and greater than 0.15 then the term is
positive. If the negative score is greater than the
positive score and greater than 0.15 then the word
is negative, in other case the word is neutral.
Each of the polarity classifiers take into consid-
eration the presence of emoticons, the expressions
of laughing and negation. The emoticons are pro-
cessed as words, so for determining their polarity
a sentiment lexicon of emoticons was built. The
polar lexicon of emoticons consists of fifty-eight
positive emoticons and forty-four negative ones.
Laughing expressions usually express a positive
sentiment, so when a laughing expression is de-
tected the counter of positive words is increased
by one. The strategy for negation identification
is a bit straightforward but effective. Due to the
specific linguistic characteristics of tweets, a strat-
egy based on windows of words has been imple-
mented. When a polar word is identified, it is
sought in the previous three words whether there
is a negative particle. In those cases that a nega-
tive particle is found, the polarity of the sentiment
word is reversed, that it to say if a positive (neg-
ative) word is negated the system considers it as
negative (positive).
The last step of the polarity classifier is the run-
ning of a voting system among the three polarity
classifiers. Three are the possible output values of
the three base classifiers {negative, neutral, pos-
itive}. When the majority class is positive, the
tweet is classified as positive, when the majority
class is negative then negative is the class assigned
to the tweet and when majority class is neutral or
there is not a majority class then the tweet is clas-
sified as neutral.
4 Analysis of the results
Before showing the results reached in the evalu-
ation of the task, the results accomplished in the
development phase of the system will be shown.
Three main systems were assessed during the de-
velopment phase:
? Baseline (BL): The three base classifiers
compose the baseline system, but the three
polarity scores of SentiWordNet are consid-
ered and negation is not taken into account.
? Neutral scores are not considered (NN): It is
the same than the Baseline system but the
neutral scores of SentiWordNet are not con-
sidered.
? Negation identification (NI): The neutral
scores of SentiWordNet are not taken into ac-
count and the negation is identified.
The results are show in Table 1.
Precision Recall F1 Accuracy Improve (Acc.)
BL 55.85% 52.02% 53.87% 60.32% -
NN 56.03% 52.27% 54.09% 60.46% 0.23%
NI 57.22% 53.41% 55.25% 61.12% 1.33%
Table 1: Results achieved during the developing
phase.
As can be seen in Table 1 the systems (NN) and
(NI) reach better results than the baseline, so all
the modifications to the baseline are good for the
polarity classification process. The results confirm
our hypothesis that the neutral score of the synsets
in SentiWordNet are not contributing positively
to the sentiment classification. Also, a straight-
forward strategy for identifying the scope of the
negation improves the accuracy of the classifica-
tion. The results help us to choose the final con-
figuration of the system. As is described in the
former section the final polarity classification sys-
tem follows a voting scheme of three base lexicon-
based polarity classifiers. The three base classi-
fiers take into consideration the presence of emoti-
cons, laughing expressions, identifies the scope of
negation, and the classifier based on SentiWord-
Net does not take into consideration the neutral
score of the synsets.
The edition 2014 of the task Sentiment Analysis
in Twitter has assessed the systems with five dif-
ferent corpus tests: LiveJournal2014, SMS2013,
Twitter2013, Twitter2014, Twitter2014Sarcasm.
The results reached with each of the test corpus
are shown in Table 2.
Some of the results shown in Table 2 are much
closed to the results reached during the develop-
ment phase, because all of the F1 scores are closed
to 55%. The lower results have been reached with
the corpus Twitter2014 and Twitter2014Sarcasm.
The poor results in Twitter2014Sarcasm are due to
the lack of a module in the system for the detection
of sarcasm. A sarcastic sentence is usually a sen-
tence with a sentiment that expresses the opposite
575
Precision Recall F1
LiveJournal2014
Positive 60.19% 76.95% 67.54%
Negative 36,51% 75,00% 49.12%
Neutral 82.48% 51.36% 63.31%
Overall ? ? 58.33%
SMS2013
Positive 63.01% 60.19% 61.57%
Negative 42.13% 71.86% 53.12%
Neutral 82.27% 73.72% 77,76%
Overall ? ? 57.34%
Twitter2013
Positive 60.56% 70.15% 65.01%
Negative 28.29% 50.15% 36.17%
Neutral 73.66% 57.06% 64.31%
Overall ? ? 50.59%
Twitter2014
Positive 57.13% 77.49% 65.77%
Negative 27.23% 42.64% 33.23%
Neutral 73.54% 49.20% 58.96%
Overall ? ? 49.50%
Twitter2014Sarcasm
Positive 57.58% 48.72% 52.78%
Negative 5.00% 100.00% 9.52%
Neutral 84.62% 24.44% 37.93%
Overall ? ? 31.15%
Table 2: Results reached with the test corpus.
sentiment, so a polarity classifier without a spe-
cific module to treat this linguistic phenomenon
will be probably misclassified the sarcastic sen-
tences. The results for Twitter2014Sarcasm for
the negative class indicate this problem. The low
value of the precision and the high value of the re-
call in the negative class mean that a high number
of negative sentences have been classified as posi-
tive.
The analysis of the results is completed with
the assessment of our method. We proceed from
the hypothesis that a combination of several clas-
sifiers will improve the final classification. Our
hypothesis is based on own previous publications,
(Perea-Ortega et al., 2013) and (Mart??n-Valdivia et
al., 2013). We have classified the test corpus with
each of the three base classifiers, with the aim of
knowing the performance of each one. The results
are shown in Table 3.
Table 3 shows that the classifier C BingL
reaches better results than the combination of
the three classifiers. The first conclusion we
draw from this fact is that the good perfor-
mance of meta-classifiers with large opinions is
not achieved with the short texts of Twitter. But,
this conclusion is preliminary, because the lower
results of the voting system may be due to a not
good combination of the three classifiers. So we
have to continue working in the analysis on how
to build a meta-classifier for OM in Twitter. The
rest of the classifiers reached lower results than the
voting system. Another reason that the voting sys-
tem achieved lower results than C BingL may be-
cause the three classifiers are not heterogeneous,
F1
C BingL C SWN C MPQA
LiveJournal2014
Positive 68.11% 42.62% 65.20%
Negative 55.43% 39.81% 49.60%
Neutral 64.03% 58.07% 58.43%
Overall 61.77% 41.21% 57.40%
SMS2013
Positive 61.67% 43.53% 53.56%
Negative 54.19% 28.79% 52.678%
Neutral 76.00% 75.85% 68.38%
Overall 57.93% 36.16% 53.12%
Twitter2013
Positive 68.30% 23.40% 62.37%
Negative 46.20% 11.60% 37.75%
Neutral 61.17% 62.11% 57.39%
Overall 57.25% 17.50% 50.06%
Twitter2014
Positive 69.33% 22.17% 66.74%
Negative 41.55% 9.79% 33.00%
Neutral 53.25% 55.63% 52.76%
Overall 55.44% 15.98% 49.87%
Twitter2014Sarcasm
Positive 56.10% 27.27% 52.06%
Negative 17.78% 9.52% 8.51%
Neutral 44.44% 30.24% 30.77%
Overall 36.94% 18.40% 30.28%
Table 3: Results reached by each base classifier
with the test corpus.
in other words, when one of the systems misclas-
sified a document the other ones classify it cor-
rectly, so the base classifiers help each other, and
the combination of systems reaches better results
than the individual systems. But, in our case may
be that the systems are not heterogeneous, so our
ongoing work is the study of the heterogeneity be-
tween the three classifiers.
If we focus only in the results achieved by
C BingL, it is remarkable that the higher differ-
ence is in the negative class. C BingL reaches
greater results than the voting system in negative
class, and it has the same negation treatment mod-
ule that the voting system. This fact allow us to say
that the low results in the negative class reached by
the voting system is not due to the negation treat-
ment module, and may because by the own com-
bination method.
To sum up, after analysing the results, we have
noticed that the same meta-classifier methodology
that we usually apply to large reviews cannot be
directly apply to tweets. Therefore, our ongoing
work is focused firstly on conducting a deep anal-
ysis of the results presented in this work, and sec-
ondly in the study on how to improve of polarity
classification in Twitter following a unsupervised
methodology, and thirdly on how to build a good
meta-classifier for OM in Twitter.
Acknowledgements
This work has been partially supported by a grant
from the Fondo Europeo de Desarrollo Regional
576
(FEDER), ATTOS project (TIN2012-38536-C03-
0) from the Spanish Government, AORESCU
project (P11-TIC-7684 MO) from the regional
government of Junta de Andaluc??a and CEATIC-
2013-01 project from the University of Jae?n.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python. O?Reilly
Media, Inc.
Kerstin Denecke. 2008. Using SentiWordnet for mul-
tilingual sentiment analysis. In Data Engineering
Workshop, 2008. ICDEW 2008. IEEE 24th Interna-
tional Conference on, pages 507?512, April.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: Overcoming sparse and noisy data. In
Proceedings of the International Conference Recent
Advances in Natural Language Processing RANLP
2013, pages 198?206, Hissar, Bulgaria, September.
INCOMA Ltd. Shoumen, BULGARIA.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
ACL: Human Language Technologies: Short Papers
- Volume 2, HLT ?11, pages 42?47, Stroudsburg, PA,
USA. ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Mar??a-Teresa Mart??n-Valdivia, Eugenio Mart??nez-
Ca?mara, Jose-M. Perea-Ortega, and L. Alfonso
Uren?a Lo?pez. 2013. Sentiment polarity detection in
Spanish reviews combining supervised and unsuper-
vised approaches. Expert Syst. Appl., 40(10):3934?
3942, August.
M. Dolores Molina-Gonza?lez, Eugenio Mart??nez-
Ca?mara, Maria Teresa Mart??n-Valdivia, and Jose? M.
Perea-Ortega. 2013. Semantic orientation for po-
larity classification in spanish reviews. Expert Syst.
Appl., 40(18):7250?7257.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA, June. ACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135, January.
Jose? M. Perea-Ortega, M. Teresa Mart??n-Valdivia,
L. Alfonso Uren?a Lo?pez, and Eugenio Mart??nez-
Ca?mara. 2013. Improving polarity classification of
bilingual parallel corpora combining machine learn-
ing and semantic orientation approaches. Journal of
the American Society for Information Science and
Technology, 64(9):1864?1877.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ?14, Dublin, Ireland.
Julio Villena-Roma?n, Sara Lana-Serrano, Euge-
nio Mart??nez-Ca?mara, and Jose? Carlos Gonza?lez-
Cristo?bal. 2013. TASS - Workshop on sentiment
analysis at SEPLN. Procesamiento del Lenguaje
Natural, 50.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
ACL.
577
