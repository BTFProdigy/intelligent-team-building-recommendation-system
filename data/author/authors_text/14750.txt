Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 59?70, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Detecting Subgroups in Online Discussions by Modeling Positive and
Negative Relations among Participants
Ahmed Hassan
Microsoft Research
Redmond, WA
hassanam@microsoft.com
Amjad Abu-Jbara
University of Michigan
Ann Arbor, MI
amjbara@umich.edu
Dragomir Radev
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
A mixture of positive (friendly) and nega-
tive (antagonistic) relations exist among users
in most social media applications. However,
many such applications do not allow users to
explicitly express the polarity of their interac-
tions. As a result most research has either ig-
nored negative links or was limited to the few
domains where such relations are explicitly
expressed (e.g. Epinions trust/distrust). We
study text exchanged between users in online
communities. We find that the polarity of the
links between users can be predicted with high
accuracy given the text they exchange. This
allows us to build a signed network represen-
tation of discussions; where every edge has
a sign: positive to denote a friendly relation,
or negative to denote an antagonistic relation.
We also connect our analysis to social psy-
chology theories of balance. We show that the
automatically predicted networks are consis-
tent with those theories. Inspired by that, we
present a technique for identifying subgroups
in discussions by partitioning singed networks
representing them.
1 Introduction
Most online communities involve a mixture of pos-
itive and negative relations between users. Positive
relations may indicate friendship, agreement, or ap-
proval. Negative relations usually indicate antago-
nism, opposition, or disagreement.
Most of the research on relations in social media
applications has almost exclusively focused on pos-
itive links between individuals (e.g. friends, fans,
followers, etc.). We think that one of the main rea-
sons, of why the interplay of positive and negative
links did not receive enough attention, is the lack of
a notion for explicitly expressing negative interac-
tions. Recently, this problem has received increas-
ing attention. However, all studies have been limited
to a handful of datasets from applications that allow
users to explicitly label relations as either positive or
negative (e.g. trust/distrust on Epinion (Leskovec et
al., 2010b) and friends/foes on Slashdot (Kunegis et
al., 2009)).
Predicting positive/negative relations between
discussants is related to another well studied prob-
lem, namely debate stance recognition. The ob-
jective of this problem is to identify which partic-
ipants are supporting and which are opposing the
topic being discussed. This line of work does not
pay enough attention to the relations between par-
ticipants, rather it focuses on participant?s stance to-
ward the topic. It also assumes that every partici-
pant either supports or opposes the topic being dis-
cussed. This is a simplistic view that ignore the
nature of complex topics that has many aspects in-
volved which may result in more than two subgroups
with different opinions.
In this work, we apply Natural Language Pro-
cessing techniques to text correspondences ex-
changed between individuals to identify the under-
lying signed social structure in online communities.
We present a method for identifying user attitude
and for automatically constructing a signed social
network representation of discussions. We apply
the proposed methods to a large set of discussion
posts. We evaluate the performance using a manu-
ally labeled dataset. We also conduct a large scale
evaluation by showing that predicted links are con-
sistent with the principals of social psychology the-
ories, namely the Structural Balance Theory (Hei-
der, 1946). The balance theory has been shown to
hold both theoretically (Heider, 1946) and empiri-
cally (Leskovec et al2010c) for a variety of social
community settings. Finally, we present a method
for identifying subgroups in online discussions by
identifying groups with high density of intra-group
positive relations and high density of inter-group
negative relations. This method is capable of identi-
fying subgroups even if the community splits into
more than two subgroups which is more general
than stance recognition which assumes that only two
groups exist.
59
B 
C 
D 
A E 
F G 
I H 
Positive Negative 
Source Target Sign Evidence from Text A E - I have to disagree with what you are saying.  G A - You are missing the entire point, he is putting lives at risk. D I - and you manufacture lies for what reason? E G + you have explained your position very well. C H + I am neutral on this, but I agree with your assessment! 
Figure 1: An example showing a signed social network
along with evidence from text that justifies edge signs.
The input to our algorithm is a set of text corre-
spondences exchanged between users (e.g. posts or
comments). The output is a signed network where
edges signify the existence of an interaction between
two users. The resulting network has polarity asso-
ciated with every edge. Edge polarity is a means for
indicating positive or negative affinity between two
individuals.
Figure 1 shows a signed network representation
for a subset of posts from a long discussion thread.
The thread discussed the November 2010 Wikileaks
cable release. We notice that participants split into
two groups, one supporting and one opposing the
leak. We also notice that most negative edges are
between groups, and most positive edges are within
groups. It is worth mentioning that networks gen-
erated from larger datasets (i.e. with thousands of
posts) have much more noise compared to this ex-
ample.
The rest of the paper is structured as follows. In
section 2, we review some of the related prior work
on mining sentiment from text, mining online dis-
cussions, extracting social networks from text, and
analyzing signed social networks. We define our
problem and explain our approach in Section 3. Sec-
tion 4 describes our dataset. Results and discussion
are presented in Section 5. We present a method for
identifying subgroups in online discussions in Sec-
tion 3.3. We conclude in Section 6.
2 Related Work
In this section, we survey several lines of research
that are related to our work.
2.1 Mining Sentiment from Text
Our general goal of mining attitude from one indi-
vidual toward another makes our work related to a
huge body of work on sentiment analysis. One such
line of research is the well-studied problem of iden-
tifying the polarity of individual words (Hatzivas-
siloglou and McKeown, 1997; Turney and Littman,
2003; Kim and Hovy, 2004; Takamura et al2005).
Subjectivity analysis is yet another research line that
is closely related to our general goal of mining at-
titude. The objective of subjectivity analysis is to
identify text that presents opinion as opposed to ob-
jective text that presents factual information (Wiebe,
2000; Hatzivassiloglou and Wiebe, 2000; Banea et
al., 2008; Riloff and Wiebe, 2003). Our work is dif-
ferent from subjectivity analysis because we are not
only interested in discriminating between opinions
and facts. Rather, we are interested in identifying
the polarity of interactions between individuals. Our
method is not restricted to phrases or words, rather it
generalizes this to identifying the polarity of an in-
teraction between two individuals based on several
posts they exchange.
2.2 Stance Classification
Perhaps the closest work to this paper is the work on
stance classification. We notice that most of these
methods focus on the polarity of the written text as-
suming that anyone using positive text belongs to
one group and anyone using negative text belongs
to another. This works well for single-aspect topics
or entities like the ones used in (Tan et al2011)
(e.g. Obama, Sara Palin, Lakers, etc.). In this sim-
ple notion of topics, it is safe to assume that text
polarity is a good enough discriminator. This unfor-
tunately is not the case in online discussions about
complex topics having many aspects (e.g. abortion,
health care, etc.). In such complex topics, people use
positive and negative text targeting different aspects
of the topic, for example in the health care bill topic,
discussants expressed their opinion regarding many
aspects including: the enlarged coverage, the insur-
ance premiums, Obama, socialism, etc. This shows
that simply looking at text polarity is not enough to
identify groups.
Tan et al2011) studied how twitter following re-
lations can be used to improve stance classification.
Their main hypothesis is that connected users are
more likely to hold similar opinions. This may be
correct for the twitter following relations, but it is
not necessarily correct for open discussions where
60
no such relations exist. The only criterion that can be
used to connect discussants is how often they reply
to each other?s posts. We will show later that while
many people reply to people with similar opinions,
many others reply to people with different opinions
as well.
Thomas et al2006) address the same problem
of determining support and opposition as applied to
congressional floor-debates. They assess the agree-
ment/disagreement between different speakers by
training a text classifier and applying it to a win-
dow surrounding the names of other speakers. They
construct their training data by assuming that if two
speaker have the same vote, then every reference
connecting them is an agreement and vice versa.
We believe this will result in a very noisy train-
ing/testing set and hence we decided to recruit hu-
man annotators to create a training set. We found
out that many instances with references to other
discussants were labeled as neither agreement nor
disagreement regardless of whether the discussants
have similar or opposing positions. We will use this
system as a baseline and will show that the exis-
tence of positive/negative words close to a person
name does not necessarily show agreement or dis-
agreement with that person.
Hassan et al2010) use a language model based
approach for identifying agreement and disagree-
ment sentences in discussions. This work is limited
to sentences. It does not consider the overall rela-
tion between participants. It also does not consider
subgroup detection. We will use this method as a
baseline for one of our components and will show
that the proposed method outperforms it.
Murakami and Raymond (2010) present another
method for stance recognition. They use a small
number of hand crafted rules to identify agreement
and disagreement interactions. Hand crafted rules
usually result in systems with very low recall caus-
ing them to miss many agreement/disagreement in-
stances (they report 0.26 recall at the 0.56 preci-
sion level). We present a machine learning system
to solve this problem and achieve much better per-
formance. Park et al2011) propose a method for
finding news articles with different views on con-
tentious issues. Mohit et al2008) present a set
of heuristics for including disagreement informa-
tion in a minimum cut stance classification frame-
work. Galley et al2004) show the value of us-
ing durational and structural features for identify-
ing agreement and disagreement in spoken conver-
sational speech. They use features like duration of
spurts, speech rate, speaker overlap, etc. which are
not applicable to written language.
Our approach is different from agree-
ment/disagreement identification because we
not only study sentiment at the local sentiment
level but also at the global level that takes into
consideration many posts exchanged between
participants to build a signed network representation
of the discussion. Research on debate stance
recognition attempts to perform classification under
the ?supporting vs. opposing? paradigm. However
such simple view might not always be accurate
for discussions on more complex topics with
many aspects. After building the signed network
representation of discussions, we present a method
that can detect how the large group could split into
many subgroups (not necessarily two) with coherent
opinions.
2.3 Extracting Social Networks from Text
Little work has been done on the front of extracting
social relations between individuals from text. El-
son et al2010) present a method for extracting so-
cial networks from nineteenth-century British nov-
els and serials. They link two characters based on
whether they are in conversation or not. McCal-
lum et al2007) explored the use of structured data
such as email headers for social network construc-
tion. Gruzd and Hyrthonthwaite (2008) explored the
use of post text in discussions to study interaction
patterns in e-learning communities. Extracting so-
cial power relations from natural language (i.e. who
influences whom) has been studied in (Bramsen et
al., 2011; Danescu-Niculescu-Mizil et al2011).
Our work is related to this line of research because
we employ natural language processing techniques
to reveal embedded social structures. Despite sim-
ilarities, our work is uniquely characterized by the
fact that we extract signed social networks with both
positive and negative links from text.
2.4 Signed Social Networks
Most of the work on social networks analysis has
only focused on positive interactions. A few recent
papers have taken the signs of edges into account.
Brzozowski et al2008) study the positive and
negative relationships between users of Essembly.
Essembly is an ideological social network that dis-
tinguishes between ideological allies and nemeses.
Kunegis et al2009) analyze user relationships in
61
the Slashdot technology news site. Slashdot allows
users of the website to tag other users as friends or
foes, providing positive and negative endorsements.
Leskovec et al2010b) study signed social networks
generated from Slashdot, Epinions, and Wikipedia.
They also connect their analysis to theories of signed
networks from social psychology. A similar study
used the same datasets for predicting positive and
negative links given their context (Leskovec et al
2010a).
All this work has been limited to analyzing a
handful of datasets for which an explicit notion of
both positive and negative relations exists. Our work
goes beyond this limitation by leveraging the power
of natural language processing to automate the dis-
covery of signed social networks using the text em-
bedded in the network.
The research presented in this paper extends this
previous work in a number of ways: (i) we present
a method based on linguistic analysis that finds in-
stances of showing positive or negative attitude be-
tween participants (ii) we propose a technique for
representing discussions as signed networks where a
sign is associated with every edge to denote whether
the relation is friendly or antagonistic (iii) we eval-
uate the proposed methods using human annotated
data and also conduct a large scale evaluation based
on social psychology theories; (iv) finally we present
a method for identifying subgroups that globally
splits the community involved in the discussion by
utilizing the dynamics of the local interactions be-
tween participants.
3 Approach
3.1 Identifying Attitude from Text
To build a signed network representation of discus-
sants, we start by trying to identify sentences that
show positive or negative attitude from the writer to
the addressee. The first step toward identifying at-
titude is to identify words with positive/negative se-
mantic orientation. The semantic orientation or po-
larity of a word indicates the direction the word devi-
ates from the norm (Lehrer, 1974). We use Opinion-
Finder (Wilson et al2005a) to identify words with
positive or negative semantic orientation. The polar-
ity of a word is also affected by the context where
the word appears. For example, a positive word that
appears in a negated context should have a negative
polarity. Other polarized words sometimes appear as
neutral words in some contexts. To identify contex-
tual polarity of words, a large set of features is used
including words, sentences, structure, and other fea-
tures similar to the method described in (Wilson et
al., 2005b).
Our overall objective is to find the direct attitude
between participants. Hence after identifying the se-
mantic orientation of individual words, we move on
to predicting which polarized expressions target the
addressee and which do not.
Text polarity alone cannot be used to identify at-
titude between participants. Sentences that show
an attitude are different from subjective sentences.
Subjective sentences are sentences used to express
opinions, evaluations, and speculations (Riloff and
Wiebe, 2003). While every sentence that shows an
attitude is a subjective sentence, not every subjective
sentence shows an attitude toward the recipient.
In this method, we address the problem of iden-
tifying sentences with attitude as a relation detec-
tion problem in a supervised learning setting. We
study sentences that has mentions to the addressee
and polarized expressions (negative/positive words
or phrases). Mentions could either be names of other
participants or second person pronouns (you, your,
yours) used in text posted as a reply to another par-
ticipant. Reply structure (i.e. who replies to whom)
is readily available in many discussion forums. In
cases where reply structure is not available, we can
use a method like the one in (Lin et al2009) to re-
cover it.
We predict whether the mention is related to the
polarized expression or not. We regard the mention
and the polarized expression as two entities and try
to learn a classifier that predicts whether the two en-
tities are related or not.
The text connecting the two entities offers a very
condensed representation of the information needed
to assess whether they are related or not. For ex-
ample the two sentences ?you are completely un-
qualified? and ?you know what, he is unqualified ...?
show two different ways the words ?you?, and ?un-
qualified? could appear in a sentence. In the first
case the polarized word ?unqualified? refers to the
word ?you?. In the second case, the two words are
not related. The information in the shortest path
between two entities in a dependency tree can be
used to assert whether a relationship exists between
them (Bunescu and Mooney, 2005).
The sequence of words connecting the two enti-
ties is a very good predictor of whether they are re-
lated or not. However, these paths are completely
62
lexicalized and consequently their performance will
be limited by data sparseness. To alleviate this prob-
lem, we use higher levels of generalization to rep-
resent the path connecting the two tokens. These
representations are the part-of-speech tags, and the
shortest path in a dependency graph connecting the
two tokens. We represent every sentence with sev-
eral representations at different levels of generaliza-
tion. For example, the sentence ?your ideas are very
inspiring? will be represented using lexical, polar-
ity, part-of-speech, and dependency information as
follows:
LEX: ?YOUR ideas are very POS?
POS: ?YOUR NNS VBP RB JJ POS?
DEP: ?YOUR poss nsubj POS?
The set of features we use are the set of unigrams,
and bigrams representing the words, part-of-speech
tags, and dependency relations connecting the two
entities. For example the following features will be
set for the previous example:
YOUR ideas, YOUR NNS, YOUR poss,
poss nsubj, ...., etc.
We use Support Vector Machines (SVM) as a
learning system because it is good with handling
high dimensional feature spaces.
3.2 Extracting the Signed Network
In this subsection, we describe the procedure we
used to build the signed network given the compo-
nent we described in the previous subsection. This
procedure consists of two main steps. The first is
building the network without signs, and the second
is assigning signs to different edges.
To build the network, we parse our data to identify
different threads, posts and senders. Every sender is
represented with a node in the network. An edge
connects two nodes if there exists an interaction be-
tween the corresponding participants. We add a di-
rected edgeA? B, ifA replies toB?s posts at least
n times in m different threads. We set m, and n to
2 in all of our experiments. The interaction infor-
mation (i.e. who replies to whom) can be extracted
directly from the thread structure. Alternatively, as
mentioned earlier, we can use a method similar to
the one presented in (Lin et al2009) to recover the
reply structure if it is not readily available.
Once we build the network, we move to the more
challenging task in which we associate a sign with
Participant Features
Number of posts per month for A (B)
Percentage of positive posts per month for A (B)
Percentage of negative posts per month for A (B)
gender
Interaction Features
Percentage/number of positive (negative) sentences per post
Percentage/number of positive (negative) posts per thread
Discussion Domain (e.g. politics, science, etc.)
Table 1: Features used by the Interaction Sign Classifier.
every edge. We have shown in the previous section
how sentences with positive and negative attitude
can be extracted from text. Unfortunately the sign
of an interaction cannot be trivially inferred from the
polarity of sentences. For example, a single negative
sentence written by A and directed to B does not
mean that the interaction between A and B is neg-
ative. One way to solve this problem would be to
compare the number of negative sentences to posi-
tive sentences in all posts betweenA andB and clas-
sify the interaction according to the plurality value.
We will show later, in our experiments section, that
such a simplistic method does not perform well in
predicting the sign of an interaction.
As a result, we decided to pose the problem as a
classical supervised learning problem. We came up
with a set of features that we think are good predic-
tors of the interaction sign, and we trained a classi-
fier using those features on a labeled dataset. Our
features include numbers and percentages of pos-
itive/negative sentences per post, posts per thread,
and so on. A sentence is labeled as positive/negative
if a relation has been detected in this sentence be-
tween a mention referring to the addressee and a
positive/negative expression. A post is considered
positive/negative based on the majority of relations
detected in it. We use two sets of features. The first
set is related to A only or B only. The second set
is related to the interactions between A and B. The
features are summarized in Table 1.
3.3 Sub-Group Detection
In any discussion, different subgroups may emerge.
Members of every subgroup usually have a common
focus (positive or negative) toward the topic being
discussed. Each member of a group is more likely
to show positive attitude to members of the same
group, and negative attitude to members of opposing
groups. The signed network representation could
prove to be very useful for identifying those sub-
groups. To detect subgroups in a discussion thread,
63
we would like to partition the corresponding signed
network such that positive intra-group links and neg-
ative inter-group links are dense.
This problem is related to the constrained cluster-
ing (Wagstaff et al2001) and the correlation clus-
tering problem (Bansal et al2004). In constrained
clustering, a pairwise similarity metric (which is
not available in our domain), and a set of must-
link/cannot-link constraints are used with a standard
data clustering algorithm. Correlation clustering op-
erates in a scenario where given a signed graph
G = (V,E) where the edge label indicates whether
two nodes are similar (+) or different (-), the task
is to cluster the vertices so that similar objects are
grouped together. Bansal et. al (2004) proved NP-
hardness and gave constant-factor approximation al-
gorithms for the special case in which the graph
is complete (full information) and every edge has
weight +1 or -1 which is not the case in our network.
Alternatively, we can use a greedy optimization al-
gorithm to find partitions. A criterion function for
a local optimization partitioning procedure is con-
structed such that positive links are dense within
groups and negative links are dense between groups.
For any potential partition C, we seek to optimize
the following function: P (C) = ?
?
n +(1??)
?
p
where
?
n is the number of negative links between
nodes in the same subgroup,
?
p is the number of
positive links between nodes in different subgroups,
and ? is a trade factor that represents the importance
of the two terms. We set ? to 0.5 in all our experi-
ments.
Clusters are selected such that: C? =
argminP (C). A greedy optimization framework
is used to minimize P (C). Initially, nodes are ran-
domly partitioned into t different clusters and the
criterion function P is evaluated for that cluster. Ev-
ery cluster has a set of neighbors in the cluster space.
A neighbor cluster is obtained by moving one node
from one cluster to another, or by exchanging two
nodes in two different clusters. Neighbor partitions
are evaluated, and if one with a lower value for the
criterion function is found, it is set as the current
partition. This greedy procedure is repeated with
random restarts until a minimal solution is found.
To determine the number of subgroups t, we select
t that minimizes the optimization function P (C). In
all experiments we used an upper limit of t = 5.
This technique was able to identify the correct num-
ber of subgroups in 77% of the times. In the rest of
the cases, the number was different from the correct
number by at most 1 except for a single case where
it was 2.
4 Data
4.1 Signed Network Extraction
Our data consists of a large amount of discussion
threads collected from online discussion forums. We
collected around 41, 000 topics (threads) and 1.2M
posts from the period between the end of 2008 and
the end of 2010. All threads were in English and had
5 posts or more. They covered 11 different domains
including: politics, religion, science, etc. The aver-
age number of participants per domain is 1320 and
per topic is 52. The data was tokenized, sentence-
split, and part-of-speech tagged with the OpenNLP
toolkit. It was parsed with the Stanford parser (Klein
and Manning, 2003).
We randomly selected around 5300 posts (1000
interactions), and asked human annotators to label
them. Our annotators were instructed to read all the
posts exchanged between two participants and de-
cide whether the interaction between them is posi-
tive or negative. We used Amazon Mechanical Turk
for annotations. Following previous work (Callison-
Burch, 2009; Akkaya et al2010), we took sev-
eral precautions to maintain data integrity. We re-
stricted annotators to those based in the US to main-
tain an acceptable level of English fluency. We also
restricted annotators to those who have more than
95% approval rate for all previous work. Moreover,
we asked three different annotators to label every in-
teraction. The label was computed by taking the ma-
jority vote among the three annotators. We refer to
this data as the Interactions Dataset.
We ran a different annotation task where we se-
lected sentences including mentions referring to dis-
cussants (names or pronouns) and polarized expres-
sions. Annotators were asked to select sentences
where the polarized attribute is referring to the men-
tion and hence show a positive or negative attitude
toward other discussion participants. This resulted
in a set of 5000 manually annotated sentences. We
refer to this data as the Sentences Dataset.
We asked three different annotators to label ev-
ery instance. The kappa measure between the three
groups of annotations was 0.62 for the Interactions
Dataset and 0.64 for the Sentences Dataset. To bet-
ter assess the quality of the annotations, we asked a
trained annotator to label 10% of the data. We mea-
sured the agreement between the expert annotator
64
Logistic Reg.
Class Pos. Neg. Weigh. Avg.
Precision 0.848 0.724 0.809
Recall 0.884 0.657 0.812
F-Measure 0.866 0.689 0.81
Accuracy - - 0.812
SVM
Precision 0.906 0.71 0.844
Recall 0.847 0.809 0.835
F-Measure 0.875 0.756 0.838
Accuracy - - 0.835
Table 2: Interaction sign classifier performance.
Classifier Random Thresh-Num Thresh-Perc. SVM
Accuracy 65% 69% 71% 83.5%
Table 3: A comparison of different sign interaction clas-
sifiers.
and the majority label from Mechanical Turk. The
kappa measure was 0.69 for the Interactions Dataset
and 0.67 for the Sentences Dataset.
4.2 Sub-group Detection
We used a dataset of more than 42 topics and ap-
proximately 9000 posts collected from two political
forums (Createdebate1 and Politicalforum2). The fo-
rum administrators ran a poll asking participants to
select their stance from a set of possible answers
and hence the dataset was self-labeled with respect
to groups. We also used a set of discussions from
the Wikipedia discussion section. When a topic on
Wikipedia is disputed, the editors of that topic start a
discussion about it. We collected 117 Wikipedia dis-
cussion threads. The threads contain a total of 1,867
posts. The discussions were annotated by an expert
annotator (a professor in sociolinguistics, not an au-
thor of the paper) who was instructed to read each
of the Wikipedia discussion threads in its entirety
and determine whether the discussants split into sub-
groups, in which case he was asked to identify the
subgroup membership for each discussant. In to-
tal, we had 159 topics with an average of approxi-
mately 500 posts, 60 participants and 2.7 subgroups
per topic. Examples of the topics include: Arizona
immigration law, airport security, oil spill, evolution,
Ireland partitions, abortion and many others.
5 Results and Discussion
We performed experiments on the data described
in the previous section. We trained and tested the
sentence with the attitude detection classifiers de-
scribed in Section 3.1 using the Sentences Dataset.
1www.createdebate.com
2www.politicalforum.com
We also trained and tested the interaction sign clas-
sifier described in Section 3.2 using the Interactions
Dataset. We built one signed social network for ev-
ery domain (e.g. politics, economics, etc.). We de-
cided to build a network for every domain as op-
posed to one single network because the relation be-
tween any two individuals may vary across domains
(e.g. politics vs. science). In the rest of this section,
we will describe the experiments we did to assess the
performance of the sentences with attitude detection
and interaction sign prediction steps.
In addition to classical evaluation, we evaluate
our results using the structural balance theory which
has been shown to hold both theoretically (Heider,
1946) and empirically (Leskovec et al2010c). We
validate our results by showing that the automati-
cally extracted networks mostly agree with the the-
ory. We evaluated the approach using the structural
balance theory because it presents a global (pertain-
ing to relations between multiple edges) and large-
scale (used millions of posts and thousands of users)
evaluation of the results as opposed to traditional
evaluation which is local in nature (only considers
one edge at a time) and smaller in scale (used thou-
sands of posts).
5.1 Identifying Sentences with Attitude
We compare the proposed methods to two baselines.
The first baseline is based on the work of (Thomas
et al2006). We used the speaker agreement com-
ponent presented in (Thomas et al2006) as a base-
line. The speaker agreement component is one step
in their approach. In this component, they used
an SVM classifier trained using a window of text
surrounding references to other speakers to predict
agreement/disagreement between speakers.
We build an SVM text classifier trained on the
sentence at which the mention referring to the other
participant occurred. We refer to this baseline as
the Text Classification approach. The second base-
lines adopts the language model approach presented
in (Hassan et al2010). Two language models
are trained using a stream of words, part-of-speech
tags, and dependency relations, one for sentences
that show an attitude and one for sentences that do
not. New sentences are classified based on gener-
ation likelihoods. We refer to this baseline as the
Language Models approach.
We tested this component using the Sentences
Dataset described in Section 4. We compared the
performance of the proposed method and the two
65
Extracted Networks Random Networks
Domain (+++) (++?) (+??) (???) (+++) (++?) (+??) (???)
abortion 51.67 26.31 18.92 0.48 35.39 43.92 18.16 2.52
current-events 67.36 22.26 8.76 0.23 54.08 36.90 8.39 0.64
off-topic-chat 65.28 23.54 9.45 0.25 58.07 34.59 6.88 0.46
economics 72.68 18.30 7.77 0.00 66.50 29.09 4.22 0.20
political opinions 60.60 24.24 12.81 0.43 45.97 40.79 12.06 1.19
environment 47.46 32.54 17.26 0.30 37.38 43.61 16.89 2.12
latest world news 58.29 22.41 16.33 0.62 42.26 42.20 13.98 1.56
religion 47.17 25.89 22.56 1.42 39.68 42.94 15.51 1.87
science-technology 57.53 26.03 14.33 0.00 50.14 38.93 10.05 0.87
terrorism 64.96 23.36 9.46 0.73 41.54 42.42 14.36 1.68
Table 4: Percentage of different types of triangles in the extracted networks vs. the random networks.
Method Accuracy Precision Recall F1
Text Classification 60.4 61.1 60.2 60.6
Language Models 80.3 81.0 79.4 80.2
Relation Extraction 82.3 82.3 82.3 82.3
Table 5: Comparison of attitude identification methods.
baselines. Table 5 compares the precision, recall,
F1, and accuracy for the three methods. The text
classification based approach does much worse than
others. The reasons is that it ignores the structure
and uses much less information (part-of-speech tags
and dependency trees are not used) compared to the
other methods. Additionally, the short length of the
sentences compared to what is typical in text clas-
sification may have had a bad effect on the perfor-
mance. Both other models try to learn the char-
acteristics of the path connecting the mention and
the polarized expression. We notice that optimizing
the weights for unigram and bigrams features using
SVM results in a better performance compared to
language models because it does not have the con-
straints imposed by the former model on the learned
weights.
We evaluated the importance of the feature types
(i.e. dependency vs. pos tags vs words) by measur-
ing the chi-squared statistic for every feature with
respect to the class. Dependency features were most
helpful, but other types of features helped improve
the performance as well.
5.2 Interaction Sign Classifier
We used the relation detection classifier described in
Section 3.1 to find sentences with positive and nega-
tive attitude. The output of this classifier was used to
compute the features described in Section 3.2, which
were used to train a classifier that predicts the sign
of an interaction between any two individuals.
We used both Support Vector Machines (SVM)
and logistic regression to train the sign interaction
classifier. We report several performance metrics for
them in Table 2. We notice that the SVM classifier
performs better with an accuracy of 83.5% and an
F-measure of 81%. All results were computed using
10 fold cross validation on the labeled data. To bet-
ter assess the performance of the proposed classifier,
we compare it to a baseline that labels the relation as
negative if the percentage of negative sentences ex-
ceeds a particular threshold, otherwise it is labeled
as positive. The thresholds were empirically esti-
mated using a separate development set. The accu-
racy of this baseline is only 71%.
To better assess the performance of the proposed
classifier, we compare it to three baselines. The first
is a random baseline that predicts an interaction as
positive with probability p that equals the proportion
of positive instances to all instances in the training
set. The second classifier (Thresh-Num) labels the
edge as negative if the number of negative instances
exceeds a threshold Tn. The third classifier (Thresh-
Perc) labels the edge as negative if the percentage of
negative instances to all instances exceeds a thresh-
old Tp. The cutoff thresholds were estimated using
a separate development set.
The 3 baselines were tested using the entire la-
beled dataset. The SVM classifier was tested using
10 fold cross validation. The accuracy of the ran-
dom classifier, the two based on a cut off number
and percentage , and the SVM classifier are shown
in Table 3. We notice that the random classifier per-
forms worst, and the classifier based on percentage
cutoff outperforms the one based on number cut-
off. The SVM classifier significantly outperforms all
other classifiers. We tried to train a classifier using
both the number and percentage of negative and pos-
itive posts. The improvement over using the baseline
using the percentage of negative posts was not sta-
tistically significant.
We evaluated the importance of the features listed
66
in Table 1 by measuring the chi-squared statistic for
every feature with respect to the class. We found
out that the features describing the interaction be-
tween the two participants are more informative than
the ones describing individuals characteristics. The
later features are still helpful though and they im-
prove the performance by a statistically significant
amount. We also noticed that all features based on
percentages are more informative than those based
on counts. The most informative features are: per-
centage of negative posts per tread, percentage of
negative sentences per post, percentage of positive
posts per thread, number of negative posts, and dis-
cussion domain.
5.3 Structural Balance Theory
The structural balance theory is a psychological the-
ory that tries to explain the dynamics of signed so-
cial interactions. It has been shown to hold both the-
oretically (Heider, 1946) and empirically (Leskovec
et al2010c). In this section, we study the agree-
ment between the theory and our automatically ex-
tracted networks. The theory has its origins in the
work of Heider (1946). It was then formalized in
a graph theoretic form by (Cartwright and Harary,
1956). The theory is based on the principles that ?the
friend of my friend is my friend?, ?the enemy of my
friend is my enemy?, ?the friend of my enemy is
my enemy?, and variations on these. The structural
balance theory states that triangles that have an odd
number of positive signs (+ + + and + - -) are bal-
anced, while triangles that have an even number of
positive signs (- - - and + + -) are not.
In this section, we compare the predictions of
edge signs made by our system to the structural bal-
ance theory by counting the frequencies of differ-
ent types of triangles in the predicted network. Ta-
ble 4 shows the frequency of every type of trian-
gle for 10 different domains. To better understand
these numbers, we compare them to the frequencies
of triangles in a set of random networks. We shuf-
fle the signs for all edges on every network keeping
the fractions of positive and negative edges constant.
We repeat shuffling for 1000 times and report the av-
erage.
We find that the all-positive triangle (+ + +) is
overrepresented in the generated network compared
to chance across all domains. We also see that the
triangle with two positive edges (+ + ?), and the
all-negative triangle (? ? ?) are underrepresented
compared to chance across all domains. The tri-
angle with a single positive edge is slightly over-
represented in most but not all of the topics com-
pared to chance. This shows that the predicted net-
works mostly agree with the structural balance the-
ory. The slightly non standard behavior of the tri-
angle with one positive edge could be explained in
light of the weak balance theory. In this theory,
Davis (1967) states that this triangle, which corre-
sponds to the ?enemy of enemy is my friend? propo-
sition, holds only if the network can be partitioned
into exactly two subsets, but not when there are more
than two. In general, the percentage of balanced tri-
angles in the predicted networks is higher than in
the shuffled networks, and hence the balanced trian-
gles are significantly overrepresented compared to
chance showing that our automatically constructed
network is similar to explicit signed networks in that
they both mostly agree with the balance theory.
5.4 Sub-Group Detection
We compare the performance of the sub-group de-
tection method to three baselines. The first base-
line uses graph clustering (GC) to partition a net-
work based on the frequency of interaction between
participants. We build a graph where each node
represents a participant. Edges link participants if
they exchange posts, and edge weights are based on
the number of posts exchanged. The second base-
line (TC) is based on the premise that participants
with similar text are more likely to belong to the
same subgroup. We measure text similarity by com-
puting the cosine similarity between the tf-idf rep-
resentations of the text in a high dimensional vec-
tor space. We tried two methods for partitioning
those graphs: spectral partitioning (Luxburg, 2007)
and a hierarchical agglomeration algorithm which
works by greedily optimizing the modularity for
graphs (Clauset et al2004). The third baseline is
based on stance classification approaches (e.g. (Tan
et al2011)). In this baseline we put all the partic-
ipants who use more positive text in one subgroup
and the participants who use more negative text in
another subgroup. Text polarity is identified using
the method described in Section 3.1.
Table 6 shows the average purity (Purity), entropy
(Entropy), Normalizes Mutual Information (NMI),
and Rand Index (RandIndex) values of the method
based on signed networks and the baselines using
different partitioning algorithms. The differences in
the results shown in the table are statistically sig-
nificant at the 0.05 level (as indicated by a 2-tailed
67
Figure 2: A signed network representing participants in a discussion about the ?Health Care Reform Bill?. Blue (dark)
nodes represent participants with the bill, Yellow (light) nodes represent participants against the bill, red (solid) edges
represent negative attitude, while green (dashed) edges represent positive attitude.
Createdebate Politicalforum Wikipedia
Method Purity Entropy NMI RandIndex Purity Entropy NMI RandIndex Purity Entropy NMI RandIndex
GC - Spectral 0.50 0.85 0.28 0.40 0.50 0.88 0.27 0.39 0.49 0.89 0.33 0.35
GC - Hierarchical 0.48 0.86 0.30 0.41 0.47 0.89 0.31 0.40 0.49 0.87 0.38 0.39
TC - Spectral 0.50 0.85 0.31 0.43 0.48 0.90 0.30 0.45 0.51 0.87 0.40 0.46
TC - Hierarchical 0.49 0.90 0.35 0.46 0.48 0.91 0.33 0.49 0.53 0.80 0.40 0.49
Text Polarity 0.55 0.80 0.38 0.49 0.54 0.91 0.31 0.38 0.34 0.95 0.30 0.40
Signed Networks 0.64 0.74 0.46 0.59 0.58 0.80 0.43 0.55 0.65 0.54 0.51 0.60
Table 6: Comparison of the sub-group detection method to baseline systems
paired t-test).
We notice that partitioning the signed network
that was automatically extracted from text results in
significantly better partitions on the three datasets as
indicated by the higher Purity, NMI, and RandIndex
and the lower Entropy values it achieves. We believe
that the first two baselines performed poorly because
the interaction frequency and the text similarity are
not key factors in identifying subgroup structures.
Many people would respond to people they disagree
with more, while others would mainly respond to
people they agree with most of the time. Also, peo-
ple in opposing subgroups tend to use very similar
text when discussing the same topic and hence text
clustering does not work as well. The baseline that
classifies the stance of discussants based on the po-
larity of their text performed bad too because it over-
looks the fact that most of the discussed topics in our
datasets have multiple aspects and a discussant may
use both positive and negative text targeting differ-
ent aspects of the topic. An example of a signed net-
work and the corresponding subgtoups as extracted
from real data is showm in Figure 2.
6 Conclusions
In this paper, we have shown that natural language
processing techniques can be reliably used to extract
signed social networks from text correspondences.
We believe that this work brings us closer to un-
derstanding the relation between language use and
social interactions and opens the door to further re-
search efforts that go beyond standard social net-
work analysis by studying the interplay of positive
and negative connections. We rigorously evaluated
the proposed methods on labeled data and connected
our analysis to social psychology theories to show
that our predictions mostly agree with them. Finally,
we presented potential applications that benefit from
the automatically extracted signed network.
Acknowledgments
This research was funded in part by the Office of the
Director of National Intelligence, Intelligence Ad-
vanced Research Projects Activity. All statements
of fact, opinion or conclusions contained herein are
those of the authors and should not be construed as
representing the official views or policies of IARPA,
the ODNI or the U.S. Government
68
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk for
subjectivity word sense disambiguation. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechani-
cal Turk, CSLDAMT ?10, pages 195?203.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation Clustering. Machine Learning, 56(1):89?
113.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label dis-
agreement in the min-cut classification framework. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters.
Philip Bramsen, Martha Escobar-Molano, Ami Patel, and
Rafael Alonso. 2011. Extracting social power rela-
tionships from natural language. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies -
Volume 1, pages 773?782.
Michael J. Brzozowski, Tad Hogg, and Gabor Szabo.
2008. Friends and foes: ideological social network-
ing. In Proceeding of the twenty-sixth annual SIGCHI
conference on Human factors in computing systems,
pages 817?820, New York, NY, USA.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing, HLT ?05, pages 724?731,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s mechan-
ical turk. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 286?295.
Dorwin Cartwright and Frank Harary. 1956. Structure
balance: A generalization of heiders theory. Psych.
Rev., 63.
Aaron Clauset, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70:066111.
Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang,
and Jon M. Kleinberg. 2011. Echoes of power: Lan-
guage effects and power differences in social interac-
tion. CoRR.
J. A. Davis. 1967. Clustering and structural balance in
graphs. Human Relations, 20:181?187.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 138?147,
Uppsala, Sweden, July.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech: use
of bayesian networks to model pragmatic dependen-
cies. In Proceedings of the 42nd Annual Meeting on
Association for Computational Linguistics, ACL ?04,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Anatoliy Gruzd and Caroline Haythornthwaite. 2008.
Automated discovery and analysis of social networks
from threaded discussions. In Proceedings of the In-
ternational Network of Social Network Analysis (IN-
SNA), St. Pete Beach, Florida.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Fritz Heider. 1946. Attitudes and cognitive organization.
Journal of Psychology, 21:107?112.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In COLING, pages 1367?1373.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL?03, pages 423?430.
Je?ro?me Kunegis, Andreas Lommatzsch, and Christian
Bauckhage. 2009. The slashdot zoo: mining a so-
cial network with negative edges. In Proceedings of
the 18th international conference on World wide web,
pages 741?750, New York, NY, USA.
Adrienne Lehrer. 1974. Semantic fields and lezical struc-
ture. North Holland, Amsterdam and New York.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010a. Predicting positive and negative links in online
social networks. In Proceedings of the 19th interna-
tional conference on World wide web, pages 641?650,
New York, NY, USA.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010b. Signed networks in social media. In Proceed-
ings of the 28th international conference on Human
factors in computing systems, pages 1361?1370, New
York, NY, USA.
69
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010c. Signed networks in social media. In CHI 2010,
pages 1361?1370, New York, NY, USA. ACM.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09,
pages 131?138.
Ulrike Luxburg. 2007. A tutorial on spectral clustering.
Statistics and Computing, 17:395?416, December.
Andrew McCallum, Xuerui Wang, and Andre?s Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. J. Artif. Int. Res., 30:249?272, October.
Akiko Murakami and Rudy Raymond. 2010. Support or
oppose?: classifying positions in online debates from
reply activities and opinion expressions. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Posters, pages 869?875.
Souneil Park, KyungSoon Lee, and Junehwa Song. 2011.
Contrasting opposing views of news articles on con-
tentious issues. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, pages
340?349.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings
of the 17th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?11,
pages 1397?1405.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In In Proceedings
of EMNLP, pages 327?335.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Kiri Wagstaff, Claire Cardie, Seth Rogers, and Stefan
Schro?dl. 2001. Constrained k-means clustering with
background knowledge. In Proceedings of the Eigh-
teenth International Conference on Machine Learning,
pages 577?584.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: a system for subjectiv-
ity analysis. In Proceedings of HLT/EMNLP on Inter-
active Demonstrations, HLT-Demo ?05, pages 34?35,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05, Vancou-
ver, Canada.
Bo Yang, William Cheung, and Jiming Liu. 2007. Com-
munity mining from signed social networks. IEEE
Trans. on Knowl. and Data Eng., 19(10):1333?1348.
70
A Random Walk?Based Model for
Identifying Semantic Orientation
Ahmed Hassan?
Microsoft Research
Amjad Abu-Jbara??
University of Michigan
Wanchen Lu?
University of Michigan
Dragomir Radev?
University of Michigan
Automatically identifying the sentiment polarity of words is a very important task that has
been used as the essential building block of many natural language processing systems such as
text classification, text filtering, product review analysis, survey response analysis, and on-line
discussion mining. We propose a method for identifying the sentiment polarity of words that
applies a Markov random walk model to a large word relatedness graph, and produces a polarity
estimate for any given word. The model can accurately and quickly assign a polarity sign and
magnitude to any word. It can be used both in a semi-supervised setting where a training set of
labeled words is used, and in a weakly supervised setting where only a handful of seed words is
used to define the two polarity classes. The method is experimentally tested using a gold standard
set of positive and negative words from the General Inquirer lexicon. We also show how our
method can be used for three-way classification which identifies neutral words in addition to
positive and negative words. Our experiments show that the proposed method outperforms the
state-of-the-art methods in the semi-supervised setting and is comparable to the best reported
values in the weakly supervised setting. In addition, the proposed method is faster and does not
need a large corpus. We also present extensions of our methods for identifying the polarity of
foreign words and out-of-vocabulary words.
? Microsoft Research, Redmond, WA, USA. E-mail: hassanam@microsoft.com. This research was
performed while at the University of Michigan.
?? Department of Electrical Engineering & Computer Science, University of Michigan, Ann Arbor, MI, USA.
E-mail: amjbara@umich.edu.
? Department of Electrical Engineering & Computer Science, University of Michigan, Ann Arbor, MI, USA.
E-mail: wanchlu@umich.edu.
? Department of Electrical Engineering & Computer Science and School of Information, University of
Michigan, Ann Arbor, MI, USA. E-mail: radev@umich.edu.
Submission received: 15 November 2011; revised submission received: 10 May 2013; accepted for publication:
14 July 2013.
doi:10.1162/COLI a 00192
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
1. Introduction
Identifying emotions and attitudes from unstructured text has a variety of possible
applications. For example, there has been a large body of work for mining product
reputation on the Web (Morinaga et al. 2002; Turney 2002). Morinaga et al. (2002)
have shown how product reputation mining helps with marketing and customer re-
lation management. The Google products catalog and many on-line shopping sites
like Amazon.com provide customers not only with comprehensive information and
reviews about a product, but also with faceted sentiment summaries. Such systems are
all supported by a sentiment lexicon, some even in multiple languages.
Another interesting application is mining on-line discussions. An enormous num-
ber of discussion groups exist on the Web. Millions of users post content to these groups
covering pretty much every possible topic. Tracking a participant attitude toward differ-
ent topics and toward other participants is a very important task that makes use of sen-
timent lexicons. For example, Tong (2001) presented the concept of sentiment timelines.
His system classifies discussion posts about movies as either positive or negative. This
is used to produce a plot of the number of positive and negative sentiment messages
over time. All these applications would benefit from an automatic way of identifying
semantic orientation of words.
In this article, we study the task of automatically identifying the semantic orienta-
tion of any word by analyzing its relations to other words, Automatically classifying
words as positive, negative, or neutral enables us to automatically identify the polarity
of larger pieces of text. This could be a very useful building block for systems that
mine surveys, product reviews, and on-line discussions. We apply a Markov random
walk model to a large semantic relatedness graph, producing a polarity estimate for
any given word. Previous work on identifying the semantic orientation of words has
addressed the problem as both a semi-supervised (Takamura, Inui, and Okumura 2005)
and a weakly supervised (Turney and Littman 2003) learning problem. In the semi-
supervised setting, a training set of labeled words is used to train the model. In the
weakly supervised setting, only a handful of seeds are used to define the two polarity
classes.
Our proposed method can be used both in a semi-supervised and in a weakly
supervised setting. Empirical experiments on a labeled set of positive and negative
words show that the proposed method outperforms the state-of-the-art methods in the
semi-supervised setting. The results in the weakly supervised setting are comparable to
the best reported values. The proposed method has the advantages that it is faster and
does not need a large training corpus.
The rest of the article is structured as follows. In Section 2, we review related work
on word polarity and subjectivity classification and note applications of the random
walk and hitting times framework. Section 3 presents our method for identifying word
polarity. We describe how the proposed method can be extended to cover foreign
languages in Section 4, and out-of-vocabulary words in Section 5. Section 6 describes
our experimental set-up. We present our conclusions in Section 7.
2. Related Work
2.1 Identifying Word Polarity
Hatzivassiloglou and McKeown (1997) proposed a method for identifying the word
polarity of adjectives. They extract all conjunctions of adjectives from a given corpus
540
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
and then they classify each conjunctive expression as either the same orientation such
as ?simple and well-received? or different orientation such as ?simplistic but well-
received.? The result is a graph that they cluster into two subsets of adjectives. They
classify the cluster with the higher average frequency as positive. They created and
labeled their own data set for experiments. Their approach works only with adjectives
because there is nothing wrong with conjunctions of nouns or verbs with opposite
polarities (?war and peace?, ?rise and fall?, etc.).
Turney and Littman (2003) identify word polarity by looking at its statistical asso-
ciation with a set of positive/negative seed words. They use two statistical measures
for estimating association: Pointwise Mutual Information (PMI) and Latent Semantic
Analysis (LSA). To get co-occurrence statistics, they submit several queries to a search
engine. Each query consists of the given word and one of the seed words. They use the
search engine NEAR operator to look for instances where the given word is physically
close to the seed word in the returned document. They present their method as an un-
supervised method where a very small number of seed words are used to define
semantic orientation rather than train the model. One of the limitations of their method
is that it requires a large corpus of text to achieve good performance. They use sev-
eral corpora; the size of the best performing data set is roughly one hundred billion
words (Turney and Littman 2003).
Takamura et al. (2005) propose using spin models for extracting semantic orienta-
tion of words. They construct a network of words using gloss definitions, thesaurus, and
co-occurrence statistics. They regard each word as an electron. Each electron has a spin
and each spin has a direction taking one of two values: up or down. Two neighboring
spins tend to have the same orientation from an energy point of view. Their hypothesis
is that as neighboring electrons tend to have the same spin direction, neighboring words
tend to have similar polarity. They pose the problem as an optimization problem and
use the mean field method to find the best solution. The analogy with electrons leads
them to assume that each word should be either positive or negative. This assumption
is not accurate because most of the words in the language do not have any semantic ori-
entation. They report that their method could get misled by noise in the gloss definition
and their computations sometimes get trapped in a local optimum because of its greedy
optimization flavor.
Kamps et al. (2004) construct a network based on WordNet (Miller 1995) synonyms
and then use the shortest paths between any given word and the words ?good? and
?bad? to determine word polarity. They report that using shortest paths could be very
noisy. For example, ?good? and ?bad? themselves are closely related in WordNet with
a 5-long sequence ?good, sound, heavy, big, bad.? A given word w may be more
connected to one set of words (e.g., positive words); yet have a shorter path connecting
it to one word in the other set. Restricting seed words to only two words affects their
accuracy. Adding more seed words could help but it will make their method extremely
costly from the computation point of view. They evaluate their method using only
adjectives.
Hu and Liu (2004) propose another method that uses WordNet. They use WordNet
synonyms and antonyms to predict the polarity of words. For any word whose polarity
is unknown, they search WordNet and a list of seed labeled words to predict its polarity.
They check if any of the synonyms of the given word has known polarity. If so, they
label it with the label of its synonym. Otherwise, they check if any of the antonyms
of the given word has known polarity. If so, they label it with the opposite label of
the antonym. They continue in a bootstrapping manner until they label all possible
words.
541
Computational Linguistics Volume 40, Number 3
2.2 Building Sentiment Lexicons
A number of other methods try to build lexicons of polarized words. Esuli and
Sebastiani (2005, 2006) use a textual representation of words by collating all the glosses
of the word as found in some dictionary. Then, a binary text classifier is trained using
the textual representation and applied to new words.
Kim and Hovy (2004) start with two lists of positive and negative seed words. Word-
Net is used to expand these lists. Synonyms of positive words and antonyms of negative
words are considered positive, and synonyms of negative words and antonyms of posi-
tive words are considered negative. A similar method is presented in Andreevskaia and
Bergler (2006), where WordNet synonyms, antonyms, and glosses are used to iteratively
expand a list of seeds. The sentiment classes are treated as fuzzy categories where some
words are very central to one category, whereas others may be interpreted differently.
Mohammad, Dunne, and Dorr (2009) utilize the marking theory, which states that
overtly marked words such as dishonest, unhappy, and impure tend to have negative
semantic orientations whereas their unmarked counterparts (honest, happy, and pure)
tend to have positive semantic orientation. They use a set of 11 antonym-generating affix
patterns to generate overtly marked words and their counterparts from the Macquarie
Thesaurus. After obtaining a set of 2,600 seeds by the affix patterns, they expand the
sentiment lexicon using a Roget-like thesaurus. Their method does not require seed
sentiment words or WordNet, but still needs a comprehensive thesaurus. The idea of
the marking theory is language-dependent and cannot be applied from one language to
another.
Contrasting the dictionary based approaches that rely on resources such as Word-
Net, Velikovich et al. (2010) investigated the viability of learning sentiment lexicons
semi-automatically from the Web. Kanayama and Nasukawa (2006) use syntactic fea-
tures and context coherency (i.e., the tendency for same polarities to appear succes-
sively) to detect polar clauses.
2.3 Random Walk?Based Methods
Closest to our work in its methodology is probably the line of research on semi-
supervised graphical methods for sentiment classification. Rao and Ravichandran
(2009) build a lexical graph similar to ours. The graph is constructed of both unlabeled
and labeled nodes, each node representing a word that can be either positive or neg-
ative, and each edge representing some semantic relatedness that can be constructed
using resources like WordNet or other thesaurus. They evaluate two semi-supervised
methods: Mincut (including its variant, Randomized Mincut) and label propagation.
The general idea of label propagation is defining a probability distribution over the
positive and negative classes for each node in the graph. A Markov random walk is
performed on the graph to recover this distribution for the unlabeled nodes.
Additionally, Rao and Ravichandra (2009) and Blair-Goldensohn et al. (2008) use a
similar label propagation method on a lexical graph built from WordNet, where a small
set of words with known polarities are used as seeds. Brody and Elhadad (2010) use
label propagation over a graph constructed of adjectives only.
Velikovich et al. (2010) compare label propagation with a Web-based method and
conclude that label propagation is not suitable when the whole Web is used as a
background corpus, because the constructed graph is very noisy and contains many
dense subgraphs, unlike the lexical graph constructed from WordNet.
542
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Random walk?based methods have been studied in the context of many other NLP
tasks. For example, Kok and Brockett (2010) construct a graph from bilingual parallel
corpora, where each node represents a phrase and two nodes are connected by an edge
if they are aligned in a phrase table. Then they compute hitting time of random walks
to learn paraphrases.
Our work is different from previous random walk methods in that it uses the mean
hitting time as the criterion for assigning polarity labels. Our experiments showed that
this achieves better results than methods that use label propagation.
2.4 Subjectivity Analysis
Subjectivity analysis is another research line that is closely related to our work. The
main task in subjectivity analysis is to identify text that presents opinion as opposed to
objective text that present factual information (Wiebe 2000). Text could be either words,
phrases, sentences, or other chunks. Wiebe et al. (2001) list a number of applications of
subjectivity analysis such as classifying e-mails and mining reviews. For example, to
analyze movie reviews, Pang and Lee (2004) apply Mincut to a graph constructed from
individual sentences as nodes to determine whether a sentence is subjective or objective.
Each node (sentence) has an individual subjectivity score obtained from a first-pass
classifier using sentence features and linguistic knowledge. Edges are weighted by a
similarity metric of how likely it is that the two sentences will be in the same subjectivity
class. All sentences to be classified are represented as unlabeled nodes and the only two
labeled nodes represent the subjective and objective classes. A Mincut algorithm is then
performed on the constructed graph to obtain the subjectivity classes for individual
sentences. The authors also integrate the subjectivity classification of isolated sentences
to document level sentiment analysis.
There are two main categories of work on subjectivity analysis. In the first cate-
gory, subjective words and phrases are identified without considering their context
(Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In
the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff
and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and
Mihalcea (2006a) studied the association of word subjectivity and word sense. They
showed that different subjectivity labels can be assigned to different senses of the same
word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles
from a wide variety of news sources manually annotated for opinions and other private
states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions
and emotions in language.
In addition, there has been a large body of work on labeling subjectivity of WordNet
words. Wiebe and Mihalcea (2006b) label word senses in WordNet as subjective or ob-
jective, utilizing the MPQA corpus. They show that subjectivity information for Word-
Net senses can improve word sense disambiguation tasks for subjectivity ambiguous
words.
Su and Markert (2009) propose a semi-supervised minimum cut framework to label
word sense entries in WordNet with subjectivity information. Their method requires
less training data other than the sense definitions and relational structure of WordNet.
2.5 Word Polarity Classification for Foreign Languages
Word sentiment and subjectivity has also been studied for languages other than English.
Jijkoun and Hofmann (2009) describe a method for creating a non-English subjectivity
543
Computational Linguistics Volume 40, Number 3
lexicon based on an English lexicon, an on-line translation service, and Wordnet.
Mihalcea and Banea (2007) use bilingual resources such as a bilingual dictionary or a
parallel corpus to generate subjectivity analysis resources for foreign languages. Rao
and Ravichandran (2009) adapt their label propagation model to Hindi using Hindi
WordNet and French using a French thesaurus.
3. Approach
We use a Markov random walk model to identify the polarity of words. Assume that
we have a network of words, some of which are labeled as either positive or nega-
tive. In this network, two words are connected if they are related. Different sources
of information are used to decide whether two words are related. For example, the
synonyms of a word are all semantically related to it. The intuition behind connect-
ing semantically related words is that those words tend to have similar polarities.
Now imagine a random surfer walking along the network starting from an unlabeled
word w.
The random walk continues until the surfer hits a labeled word. If the word w is
positive then the probability that the random walk hits a positive word is higher, and if
w is negative then the probability that the random walk hits a negative word is higher.
Thus, if the word w is positive then the average time it takes a random walk starting at
w to hit a positive node should be much less than the average time it takes a random
walk starting at w to hit a negative node. If w doesn?t have a clear polarity and we would
like to say that it is neutral, we expect that the positive hitting time and negative hitting
time to not have a significant difference.
We describe how we construct a word relatedness graph in Section 3.1. The random
walk model is described in Section 3.2. Hitting time is defined in Section 3.3. Finally,
an algorithm for computing a sign and magnitude for the polarity of any given word
is described in Section 3.4.
3.1 Network Construction
We construct a network where two nodes are linked if they are semantically related.
Several sources of information are used as indicators of the relatedness of words. One
such source is WordNet (Miller 1995). WordNet is a large lexical database of English.
Nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms
(synsets), each expressing a distinct concept (Miller 1995). Synsets are interlinked by
means of conceptual-semantic and lexical relations.
The simplest approach is to connect words that occur in the same WordNet synset.
We can collect all words in WordNet, and add links between any two words that
occur in the same synset. The resulting graph is a graph G(W, E) where W is a set
of word/part-of-speech (POS) pairs for all the words in WordNet. E is the set of
edges connecting each pair of synonymous words. Nodes represent word/POS pairs
rather than words because the part of speech tags are helpful in disambiguating
the different senses for a given word. For example, the word ?fine? has two dif-
ferent meanings, with two opposite polarities when used as an adjective and as a
noun.
Several other methods can be used to link words. For example, we can use other
WordNet relations: hypernyms, similar to, and so forth. Another source of links be-
tween words is co-occurrence statistics from a corpus. Following the method presented
544
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
in Hatzivassiloglou and McKeown (1997), we can connect words if they appear together
in a conjunction in the corpus. This method is only applicable to adjectives. If two
adjectives are connected by ?and,? it is highly likely that they have the same semantic
orientation. In all our experiments, we restricted the network to only WordNet relations.
We study the effect of using co-occurrence statistics to connect words later at the end of
our experiments. If more than one relation exists between any two words, the strength
of the corresponding edge is adjusted accordingly.
3.2 Random Walk Model
Imagine a random surfer walking along the word relatedness graph G. Starting from a
word with unknown polarity i, it moves to a node j with probability Pij after the first
step. The walk continues until the surfer hits a word with known polarity. Seed words
with known polarity act as an absorbing boundary for the random walk. If we repeat
the number of random walks N times, the percentage of times in which the walk ends at
a positive/negative word could be used as an indicator of its positive/negative polarity.
The average time a random walk starting at w takes to hit the set of positive/negative
nodes is also an indicator of its polarity. This view is closely related to the partially la-
beled classification with random walks approach in Szummer and Jaakkola (2002) and
the semi-supervised learning using harmonic functions approach in Zhu, Ghahramani,
and Lafferty (2003).
Let W be the set of words in our lexicon. We construct a graph whose nodes V are
all words in W. Edges E correspond to the relatedness between words. We define the
transition probability Pt+1|t( j|i) from i to j by normalizing the weights of the edges out
of node i, so:
Pt+1|t( j|i) = Wij/
?
k
Wik (1)
where k represents all nodes in the neighborhood of i. Pt+1|t( j|i) denotes the transition
probability from node i at step t to node j at time step t + 1. We note that the matrix of
weights Wij is symmetric whereas the matrix of transition probabilities Pt+1|t( j|i) is not
necessarily symmetric because of the node outdegree normalization.
3.3 First-Passage Time
The mean first-passage (hitting) time h(i|k) is defined as the average number of steps a
random walker, starting in state i 6= k, will take to enter state k for the first time (Norris
1997). Let G = (V, E) be a graph with a set of vertices V and a set of edges E. Consider
a subset of vertices S ? V. Consider a random walk on G starting at node i 6? S. Let Nt
denote the position of the random surfer at time t. Let h(i|S) be the average number of
steps a random walker, starting in state i 6? S, will take to enter a state k ? S for the first
time. Let TS be the first-passage for any vertex in S.
P(TS = t|N0 = i) =
?
j?V
pij ? P(TS = t? 1|N0 = j) (2)
545
Computational Linguistics Volume 40, Number 3
h(i|S) is the expectation of TS. Hence:
h(i|S) = E(TS|N0 = i)
=
??
t=1
t? P(TS = t|N0 = i)
=
??
t=1
t
?
j?V
pijP(TS = t? 1|N0 = j)
=
?
j?V
??
t=1
(t? 1)pijP(TS = t? 1|N0 = j)
+
?
j?V
??
t=1
pijP(TS = t? 1|N0 = j)
=
?
j?V
pij
??
t=1
tP(TS = t|N0 = j) + 1
=
?
j?V
pij ? h( j|S) + 1 (3)
Hence the first-passage (hitting) time can be formally defined as:
h(i|S) =
{
0 i ? S
?
j?V pij ? h( j|S) + 1 otherwise
(4)
3.4 Word Polarity Calculation
Based on the description of the random walk model and the first-passage (hitting)
time above, we now propose our word polarity identification algorithm. We begin by
constructing a word relatedness graph and defining a random walk on that graph as
described above. Let S+ and S? be two sets of vertices representing seed words that are
already labeled as either positive or negative, respectively.
For any given word w, we compute the hitting time h(w|S+) and h(w|S?) for the
two sets iteratively as described earlier. The ratio between the two hitting times is then
used as an indication of how positive/negative the given word is. This is useful in
case we need to provide a confidence measure for the prediction. This could be used
to allow the model to abstain from classifying words when the confidence level is low.
It also means that our method can be easily extended from two-way classification (i.e.,
positive or negative) to three-way classification (positive, negative, or neutral). This can
be done by setting a threshold ? on the ratio of positive and negative hitting time,
and classifying a word to positive or negative only when the two hitting times have
a significant difference; otherwise we classify it to neutral.
When the relatedness graph is very large, computing hitting time as described
earlier may be very time consuming. The graph constructed from the English WordNet
546
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Algorithm 1 3-class word polarity using random walks (parameter ? : 0 < ? < 1)
Require: A word relatedness graph G
1: Given a word w in V
2: Define a random walk on the graph. The transition probability between any two
nodes i, and j is defined as: Pt+1|t( j|i) = Wij/
?
k Wik
3: Start k independent random walks from w with a maximum number of steps m
4: Stop when a positive word is reached
5: Let h?(w|S+) be the estimated value for h(w|S+)
6: Repeat for negative words computing h?(w|S?)
7: if h?(w|S+) ? ?h?(w|S?) then
8: Classify w as positive
9: else if h?(w|S?) ? ?h?(w|S+) then
10: Classify w as negative
11: else
12: Classify w as neutral
13: end if
and synsets contains 155,000 nodes and 117,000 edges. To overcome this problem, we
propose a Monte Carlo?based algorithm (Algorithm 1) for estimating it.
In the case of binary classification, where each word must be either positive or
negative, if h(w|S+) is greater than h(w|S?), the word is classified as negative and
positive otherwise. This can be achieved by setting parameter ? = 1 in Algorithm 1.
4. Foreign Word Polarity
As we mentioned earlier, a large body of research has focused on identifying the
semantic orientation of words. This work has almost exclusively dealt with English and
uses several language-dependent resources. When we try to apply these methods to
other languages, we run into the problem of the lack of resources in other languages
when compared with English. For example, the General Inquirer lexicon (Stone et al.
1966) has thousands of English words labeled with semantic orientation. Most of the
literature has used it as a source of labeled seeds or for evaluation. Such lexicons are
not readily available in other languages.
As we showed earlier, WordNet (Miller 1995) has been used for this task. How-
ever, even though W have been built for other languages, their coverage is relatively
limited when compared to the English WordNet. The current release of English Word-
Net (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the
resources for other languages, the Arabic WordNet (Black et al. 2006; Elkateb et al.
2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al. 2001; Narayan
et al. 2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets in
Spanish, 15K in German, and 22K in French, among other European languages. In some
cases, accuracy was traded for coverage. For example, the current release of the Japanese
WordNet has 57K synsets but contains errors in as many as 5% of the entries.1
In this section, we show how we can extend the methods presented earlier to predict
the semantic orientation of foreign words. The proposed method is based on creating
1 http://nlpwww.nict.go.jp/wn-ja/index.en.html.
547
Computational Linguistics Volume 40, Number 3
a multilingual network of words that represents both English and foreign words. The
network has English?English connections, as well as Foreign?Foreign connections and
English?Foreign connections. This allows us to benefit from the richness of the resources
built for the English language and at the same time utilize resources specific to foreign
languages. We define a random walk model over the multilingual network and pre-
dict the semantic orientation of any given word by comparing the mean hitting time
of a random walk starting from it to a positive and a negative set of seed English
words.
We use Arabic and Hindi in our experiments. We compare the performance of sev-
eral methods using the foreign language resources only, and the multilingual network
that has both English and foreign words. We show that bootstrapping from languages
with dense resources such as English is useful for improving the performance on other
languages with limited resources.
4.1 Multilingual Word Network
We build a network G(V, E) where V = Ven ? Vfr is the union of the sets of English and
Foreign words. E is a set of edges connecting nodes in V. There are three types of connec-
tions: English?English connections, Foreign?Foreign connections, and English?Foreign
connections. For the English?English connections, we use the same methodology as in
Section 3.
Foreign?Foreign connections are created in a similar way to the English con-
nections. Some foreign languages have lexical resources based on the design of the
Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic Word-
Net (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al.
2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of
Hatzivassiloglou and McKeown (1997).
Finally, to connect foreign words to English words, we use a Foreign to English dic-
tionary. For every word in a list of foreign words, we look up its meaning in a dictionary
and add an edge between the foreign word and every other English word that appeared
as a possible meaning for it. If there is no comprehensive enough dictionary available,
constructing a multilingual word network like a translation graph (Etzioni et al. 2007)
may be a resolution.
4.2 Foreign Word Semantic Orientation Prediction
We use the multilingual network described previously to predict the semantic orien-
tation of words based on the mean hitting time to two sets of positive and negative
seeds. Given two lists of seed English words with known polarity, we define two sets
of nodes S+ and S? representing those seeds. For any given word w, we calculate the
mean hitting time between w and the two seed sets h(w|S+) and h(w|S?). If h(w|S+)
is greater than h(w|S?), the word is classified as negative; otherwise it is classified as
positive. We used the list of labeled seeds from Hatzivassiloglou and McKeown (1997)
and Stone et al. (1966).
5. Out-of-Vocabulary Words
We observed that a significant portion of the text used on-line in discussions, comments,
product reviews, and so on, contains words that are not defined in WordNet or in
548
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
standard dictionaries. We call these words Out-of-Vocabulary (OOV) words. Table 6
later in this article shows some OOV word examples. To show the importance of OOV
word polarity identification, we calculated the proportion of OOV words in three
corpora used for sentiment studies: a set of movie reviews, a set of on-line discussions
from a political forum, and a set of randomly sampled tweets. For each word in the
data, we look it up in two standard English dictionaries, together containing 160K
unique words. Table 1 shows the statistics.
OOV words have a high chance of being polarized because people tend to use
informal language or special acronyms to emphasize their attitudes or impress the
audience. Therefore, being able to automatically identify the polarity of OOV words
will essentially benefit real-world applications.
Consider the graph G(W, E) described in Section 3.1. So far, the only resource we use
to construct the graph is WordNet synsets. The first step in our approach to OOV word
polarity identification is to find the words in WordNet that are related to an OOV
word. Next, we add the OOV words to our graph by creating a new node for each OOV
word and adding an edge between each OOV word and each of its related words. Once
we have constructed the extended network, we use the random walk model described
in Section 3.2 to predict the polarity of each OOV word.
5.1 Mining OOV Word Relatedness from the Web
There are several alternative methods of linking words in the graph. Agirre et al. (2009)
studied the strengths and weaknesses of different approaches to term similarity and
relatedness. They noticed that lexicographical methods such as the WordNet suffer from
the limitation of lexicon coverage, which is the case here with OOV words. To overcome
this limitation, we use a Web-based distributional approach to find the set of related
words to each OOV word. We perform a Web search using the OOV word as a search
query and retrieve the top S search results. We extract the textual content of the retrieved
results and tokenize it. After removing all the stop words, we compute the number of
times each word co-occurs with the OOV word in the same document. We rank the
words based on their co-occurrence frequency and return the top R words as the set of
related words to the given OOV word.
We experimented with three different variants of this approach. In the first variant,
the frequency values of the co-occurring words are normalized by the lengths of the
Table 1
Proportion of OOV words in some corpora used for real world applications. (Numbers in
parentheses exclude words whose first letters are capitalized because they are likely to refer to
named entities.)
corpus source # of words Percentage
of OOV
Movie reviews 3, 411 customer reviews from IMDb for the
movie The Dark Knight (2008)
10.7 M (9.5 M) 5.3 (2.7)
Political forum 23K sentences from www.politicalforum.com
on various topics
381 K (348 K) 8 (6)
tweets 0.6M random English tweets from twitter.com.
(We count a tweet as in English if at least half
of the words are English dictionary words.
Tags and symbols were removed.)
7.1 M (5.9 M) 30 (27)
549
Computational Linguistics Volume 40, Number 3
documents that contributed to the count of each word. The intuition here is that longer
documents contain more words and hence the probability that a word in the that
document is related to the search query (i.e., the OOV word) is lower than when the
document is shorter.
In the second variant, we only consider the words that appear in the proximity of
the OOV word (i.e., within d words around the OOV word) when we compute the co-
occurrence frequency. The intuition here is that words that appear near the OOV word
are more likely to be semantically related than the words that appear far away.
In the third variant, instead of searching the entire Web, we limit the search to
social text. In the experiments described subsequently, we search for the OOV words
in tweets posted on Twitter.2 The intuition here is that searching the entire Web is likely
to return results that do not necessarily contain opinionated text?particularly because
many words have different senses. In contrast, the text written in a social context is more
likely to carry sentiment and express emotions. This helps us find better related words
that suit our task.
5.2 Word Network Extension with OOV Words
To extend the graph to include OOV words, we start with the graph G(W, E) constructed
from WordNet synsets. For each OOV word that does not exist in G, we create a new
node w. We set the part of speech of w to unspecified. Then we use the Web-based method
described in the previous section to find a set of words that are most related to w. Finally,
we create a link between each OOV word and each of its related words. To predict the
polarity of an OOV word, we use the same random walk model described earlier.
6. Experiments
We performed experiments on the gold-standard data set for positive/negative words
from the General Inquirer lexicon (Stone et al. 1966). The data set contains 4, 206 words,
1, 915 of which are positive and 2, 291 of which are negative. Some of the ambiguous
words were removed, as in Turney (2002) and Takamura, Inui, and Okumura (2005).
Some examples of positive/negative words are listed in Table 2.
We use WordNet (Miller 1995) as a source of synonyms and hypernyms for the
word relatedness graph. We used the Reuters Corpus, Volume 1 (Lewis et al. 2004) to
generate co-occurrence statistics in the experiments that used them. We used 10-fold
cross-validation for all tests. We evaluate our results in terms of accuracy. Statistical
significance was tested using a two-tailed paired t-test. All reported results are statisti-
cally significant at the 0.05 level. We perform experiments varying the parameters and
the network. We also look at the performance of the proposed method for different
parts of speech, and for different confidence levels. We compare our method to the
Semantic Orientation from PMI (SO-PMI) method described in Turney (2002), the Spin
model described in Takamura, Inui, and Okumura (2005), the shortest path method
described in Kamps et al. (2004), a re-implementation of the label propagation and
Mincut methods described in Rao and Ravichandran (2009), and the bootstrapping
method described in Hu and Liu (2004).
2 http://www.twitter.com.
550
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Table 2
Examples of positive and negative words.
Positive Negative
able adjective abandon verb
acceptable adjective abuse verb
admire verb burglar noun
amazing adjective chaos noun
careful adjective contagious adjective
ease noun corruption noun
guide verb lie verb
inspire verb reluctant adjective
truthful adjective wrong adjective
6.1 Comparison with Other Methods
This method could be used in a semi-supervised setting where a set of labeled words are
used and the system learns from these labeled nodes and from other unlabeled nodes.
Under this setting, we compare our method to the spin model described in Takamura,
Inui, and Okumura (2005). Table 3 compares the performance using 10-fold cross val-
idation. The table shows that the proposed method outperforms the spin model. The
spin model approach uses word glosses, WordNet synonym, hypernym, and antonym
relations, in addition to co-occurrence statistics extracted from corpus. The proposed
method achieves better performance by only using WordNet synonym, hypernym, and
similar to relations. Adding co-occurrence statistics slightly improved performance, and
using glosses did not help at all.
We also compare our method to a re-implementation of the label propagation (LP)
method. Our method outperforms the LP method in both the 10-fold cross-validation
set-up and when only 14 seeds are used.
We also compare our method to the SO-PMI method. Turney and Littman (2002)
propose two methods for predicting the semantic orientation of words. They use
Latent Semantic Analysis (SO-LSA) and Pointwise Mutual Information (SO-PMI) for
measuring the statistical association between any given word and a set of 14 seed
words. They describe this method as unsupervised because they only use 14 seeds
as paradigm words that define the semantic orientation rather than train the model
(Turney 2002).
Table 3
Accuracy for SO-PMI with different data set sizes, the spin model, the label propagation model,
and the random walks model for 10-fold cross-validation and 14 seeds.
? CV 14 seeds
SO-PMI (1? 107) ? 61.3
SO-PMI (2? 109) ? 76.1
SO-PMI (1? 1011) ? 82.8
Spin Model 91.5 81.9
Label Propagation 88.40 74.83
Random Walks 93.1 82.1
551
Computational Linguistics Volume 40, Number 3
The SO-PMI value can be calculated as follows:
SO-PMI(w) = log
hitsw,pos ? hitsneg
hitsw,neg ? hitspos
(5)
where w is a word with unknown polarity, hitsw,pos is the number of hits returned by a
commercial search engine when the search query is the given word and the disjunction
of all positive seed words. hitspos is the number of hits when we search for the disjunction
of all positive seed words. hitsw,neg, and hitsneg are defined similarly.
After Turney (2002), we use our method to predict semantic orientation of words in
the General Inquirer lexicon (Stone et al. 1966) using only 14 seed words. The network
we used contains only WordNet relations. No glosses or co-occurrence statistics are
used. The results comparing the SO-PMI method with different data set sizes, the spin
model, and the proposed method using only 14 seeds is shown in Table 3. We observe
that the random walk method outperforms SO-PMI when SO-PMI uses data sets of
sizes 1? 107 and 2? 109 words. The performance of SO-PMI and the random walk
methods are comparable when SO-PMI uses a very large data set (1? 1011 words). The
performance of the spin model approach is also comparable to the other two methods.
The advantages of the random walk method over SO-PMI is that it is faster and it does
not need a very large corpus. Another advantage is that the random walk method can
be used along with the labeled data from the General Inquirer lexicon (Stone et al. 1966)
to get much better performance. This is costly for the SO-PMI method because that will
require the submission of almost 4,000 queries to a commercial search engine.
We also compare our method with the bootstrapping method described in Hu and
Liu (2004), and the shortest path method described in Kamps et al. (2004). We build a
network using only WordNet synonyms and hypernyms. We restrict the test set to the
set of adjectives in the General Inquirer lexicon because our method is mainly interested
in classifying adjectives.
The performance of the spin model, the bootstrapping method, the shortest path
method, the LP method, the Mincut method, and the random walk method for only
adjectives is shown in Table 4. We notice from the table that the random walk method
outperforms the spin model, the bootstrapping method, the shortest path method,
the LP method, and the Mincut method for adjectives. The reported accuracy for the
shortest path method only considers the words it could assign a non-zero orientation
value. If we consider all words, its accuracy will drop to around 61%.
6.1.1 Varying Parameters. As we mentioned in Section 3.4, we use a parameter m to put
an upper bound on the length of random walks. In this section, we explore the impact
of this parameter on our method?s performance.
Figure 1 shows the accuracy of the random walk method as a function of the
maximum number of steps m as it varies from 5 to 50. We use a network built from
WordNet synonyms and hypernyms only. The number of samples k was set to 1, 000.
Table 4
Accuracy for adjectives only for the spin model, the bootstrap method, and the random walk
model.
Method Spin Model Bootstrap Shortest Path LP Mincut Random Walks
Accuracy 83.6 72.8 68.8 84.8 73.8 88.8
552
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Figure 1
The effect of varying the maximum number of steps (m) on accuracy (k = 1,000).
We perform 10-fold cross-validation using the General Inquirer lexicon. We observe
that the maximum number of steps m has very little impact on performance until it
rises above 30. At that point, the performance drops by no more than 1%, and then it no
longer changes as m increases. An interesting observation is that the proposed method
performs quite well with a very small number of steps (around 10). We looked at the
data set to understand why increasing the number of steps beyond 30 negatively affects
performance. We found out that when the number of steps is very large compared with
the diameter of the graph, the random walk that starts at ambiguous words (which are
hard to classify) have the chance of moving until it hits a node in the opposite class.
That does not happen when the limit on the number of steps is smaller because those
walks are then terminated without hitting any labeled nodes and are hence ignored.
Next, we study the effect of the number of samples k on our method?s performance.
As explained in Section 3.4, k is the number of samples used by the Monte Carlo
algorithm to find an estimate for the hitting time. Figure 2 shows the accuracy of the
random walks method as a function of the number of samples k. We use the same
Figure 2
The effect of varying the number of samples (k) on accuracy.
553
Computational Linguistics Volume 40, Number 3
settings as in the previous experiment. The only difference is that we fix m at 15 and
vary k from 10 to 20, 000 (note the logarithmic scale). We notice that the performance
is badly affected when the value of k is very small (less than 100). We also notice that
after 1, 000, varying k has very little, if any, effect on performance. This shows that the
Monte Carlo algorithm for computing the random walks hitting time performs quite
well with values of the number of samples as small as 1, 000.
The preceding experiments suggest that the parameter m has very little impact
on the performance. This suggests that the approach is fairly robust (i.e., it is quite
insensitive to different parameter settings).
6.1.2 Other Experiments. We now measure the performance of the random walk method
when the system is allowed to abstain from classifying the words for which it has low
confidence. We regard the ratio between the hitting time to positive words and hitting
time to negative words as a confidence measure and evaluate the top words with the
highest confidence level at different values of threshold. Figure 3 shows the accuracy for
10-fold cross validation and for using only 14 seeds at different thresholds. We notice
that the accuracy improves by abstaining from classifying the difficult words. The figure
shows that the top 60% words are classified with accuracy greater than 99% for 10-fold
cross validation and 92% with 14 seed words. This may be compared with the work
described in Takamura, Inui, and Okumura (2005), where they achieve the 92% level
when they only consider the top 1,000 words (28%).
Figure 4 shows a learning curve displaying how the performance of both the pro-
posed method and the LP method is affected with varying the labeled set size (i.e., the
number of seeds). We notice that the accuracy exceeds 90% when the training set size
rises above 20%. The accuracy steadily increases as the size of labeled data increases.
We also looked at the classification accuracy for different parts of speech in Figure 5.
We notice that, in the case of 10-fold cross-validation, the performance is consistent
across parts of speech. However, when we only use 14 seeds?all of which are ad-
jectives, similar to Turney and Littman (2003)?we notice that the performance on
adjectives is much better than other parts of speech. When we use 14 seeds but replace
some of the adjectives with verbs and nouns such as love, harm, friend, enemy, the per-
formance for nouns and verbs improves considerably at the cost of a small drop in the
Figure 3
Accuracy for words with high confidence measure.
554
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Figure 4
The effect of varying the number of seeds on accuracy.
50
55
60
65
70
75
80
85
90
95
100
Adj Adv Noun Verb
CV 14 Adj Seeds 14 Seeds
Figure 5
Accuracy for different parts of speech.
performance on adjectives. Finally, we tried adding edges to the network from glosses
and co-occurrence statistics but we did not get any statistically significant improvement.
Some of the words that were very weakly linked benefited from adding new types
of links and they were correctly predicted. Others were misled by the noise and were
incorrectly classified. We had a closer look at the results to find out what are the reasons
behind incorrect predictions. We found two main reasons. First, some words have more
than one sense, possibly with different semantic orientations. Disambiguating the sense
of words given their context before trying to predict their polarity should solve this
problem. The second reason is that some words have very few connections in the
thesaurus. A possible solution to this might be to identify those words and add more
links to them from glosses of co-occurrence statistics in the corpus.
6.1.3 General Purpose Three-Way Classification. The experiments described so far all use
the General Inquirer lexicon, which contains a well-established gold standard data set
of positive and negative words. However, in realistic applications, a general purpose
555
Computational Linguistics Volume 40, Number 3
Table 5
Accuracy for three classes on a general purpose list of 2,000 words.
Class Positive Negative Neutral Overall
Accuracy 68.0 82.1 80.6 77.9
list of words will frequently have neutral words that don?t express sentiment polarity.
To evaluate the effectiveness of the random walk method in distinguishing polarized
words from neutral words, we constructed a data set of 2, 000 words randomly picked
from a standard English dictionary3 and hand labeled them with three classes: posi-
tive, negative, and neutral. Among the 2, 000 words, 494 were labeled positive,
491 negative, and 1, 015 neutral. The distribution among different parts of speech is
532 adjectives, 335 verbs, 1, 051 nouns, and 82 others.
We used the semi-supervised setting with the General Inquirer lexicon polarized
word list as the training set. Because the 2, 000 test set has some portion of polarized
words overlapping with the training set, we excluded the words that appear in the test
set from the training set. We performed Algorithm 2 in Section 3.4 with parameters
? = 0.8, m = 15, k = 1, 000. The overall accuracy as well as the precision for each class is
shown in Table 5. We can see that the accuracy of the positive class is much lower than
the negative class, due to the many positive words classified as neutral. This means
that the average confidence of negative words is higher than positive words. One factor
that could have caused this is the bias originating from the training set. Because there
are more negative seeds than positive ones, the constructed graph has an overall bias
towards the negative class.
6.2 Foreign Words
In addition to the English data we described earlier, we constructed a labeled set of 300
Arabic and 300 Hindi words for evaluation. For every language, we asked two native
speakers to examine a large amount of text and identify a set of positive and negative
words. We also used an Arabic?English and a Hindi?English dictionary to generate
Foreign?English links.
We compare our results with two baselines. The first is the SO-PMI method de-
scribed in Turney and Littman (2003). We used the same seven positive and seven
negative seeds as Turney and Littman (2003).
The second baseline constructs a network of only foreign words as described earlier.
It uses mean hitting time to find the semantic association of any given word. We used
10-fold cross-validation for this experiment. We will refer to this system as HT-FR.
Finally, we build a multilingual network and use the hitting time as before to predict
semantic orientation. We used the English words from Stone et al. (1966) as seeds and
the labeled foreign words for evaluation. We will refer to this system as HT-FR-EN.
Figure 6 compares the accuracy of the three methods for Arabic and Hindi. We
notice that the SO-PMI and the hitting time?based methods perform poorly on both
Arabic and Hindi. This is clearly evident when we consider that the accuracy of the two
systems on English was 83%, and 93%, respectively (Turney and Littman 2003; Hassan
3 Very infrequent words were filtered out by setting a threshold on the inverse document frequency of the
words in a corpus.
556
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
0
10
20
30
40
50
60
70
80
90
100
Arabic Hindi
SO-PMI HT - FR HT - FR+ EN
Figure 6
Accuracy of foreign word polarity identification.
and Radev 2010). This supports our hypothesis that state-of-the-art methods, designed
for English, perform poorly on foreign languages due to the limited amount of resources
in them. The figure also shows that the proposed method, which combines resources
from both English and foreign languages, performs significantly better. Finally, we
studied how much improvement is achieved by including links between foreign words
from global WordNets. We found out that it improves the performance by 2.5% and 4%
for Arabic and Hindi, respectively.
6.3 OOV Words
We created a labeled set of 300 positive and negative OOV words. We asked a native
English speaker to examine a large number of threads posted on several on-line forums
and identify OOV words and label them with their polarities. Some examples of posi-
tive/negative OOV words are listed in Table 6.
The baseline we use for OOV words is the SO-PMI method with the same 14 seeds
as in Turney and Littman (2003). The calculation of SO-PMI is given in Equation (5).
We used the approach described in Section 5 to automatically label the words. We
used the words of the General Inquirer lexicon as labeled seeds. We set the maximum
number of steps m to 15 and the number of samples k to 1, 000. We experimented with
Table 6
Examples of positive and negative OOV words.
Positive Negative
Word Meaning Word Meaning
beautimous beautiful and fabulous disastrophy a catastrophy and a disaster
gr8 great banjaxed ruined
buffting attractive ijit idiot
557
Computational Linguistics Volume 40, Number 3
Figure 7
Accuracy of different methods in predicting OOV words polarity.
the three variants we proposed for extracting the related words as described in Section 5.
We give the experimental set-up for each variant here:
1. Search the entire Web (WS): We used Yahoo search4 to execute the search
queries. For each OOV word, we retrieve the top 500 results and use them
to extract the related words.
2. Search the entire Web and limit the extraction of related words to the
proximity of the OOV word (WSP): We fix the proximity of a given
OOV word to 15 words before and 15 words after the OOV word (we
experimented with different ranges but no significant changes were
observed).
3. Limit the search to social content (SOC): We limit the search for OOV
words to tweets posted on Twitter. We use the Twitter search API
to submit the search queries. For each OOV word, we retrieve
10,000 tweets. Each tweet is maximum of 140 characters long.
Figure 7 shows the results of the three methods compared with the baseline SO-PMI.
The results show that extracting related words from tweets gives the best accuracy. This
corroborated our intuition that using social content is more likely to provide sentiment-
related words. The baseline SO-PMI and WS obtain very similar accuracy. This agrees
with the comparable performance of the two methods in the earlier experiment on the
General Inquirer lexicon.
The three variant methods for obtaining related words have a tunable parameter
R, the number of related words extracted for each OOV word. We observe that R
has a non-negligible effect on the prediction accuracy. The results shown in Figure 8
correspond to R = 90. To better understand the impact of varying this parameter, we ran
the experiment that uses Twitter to extract related words several times using different
values for R. Figure 8 shows how the accuracy of polarity prediction changes as R
changes.
4 http://www.yahoo.com.
558
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
40%
45%
50%
55%
60%
65%
70%
0 20 40 60 80 100 120 140 160 180
Accu
racy
 
Number of related words  
Figure 8
The effect of varying the number of extracted related words on accuracy.
7. Conclusions
Predicting the semantic orientation of words is a very interesting task in natural lan-
guage processing and it has a wide variety of applications. We proposed a method for
automatically predicting the semantic orientation of words using random walks and
hitting time. The proposed method is based on the observation that a random walk
starting at a given word is more likely to hit another word with the same semantic
orientation before hitting a word with a different semantic orientation. The proposed
method can be used in a semi-supervised setting, where a training set of labeled words
is used, and in a weakly supervised setting, where only a handful of seeds is used to
define the two polarity classes. We predict semantic orientation with high accuracy.
The proposed method is fast, simple to implement, and does not need any corpus. We
also extended the proposed method to cover the problem of predicting the semantic
orientation of foreign words. All previous work on this task has almost exclusively
focused on English. Applying off-the-shelf methods developed for English to other
languages does not work well because of the limited amount of resources available
in foreign languages compared with English. We show that the proposed method can
predict the semantic orientation of foreign words with high accuracy and outperforms
state-of-the-art methods limited to using language specific resources. Finally, we further
extended the method to cover out-of-vocabulary words. These words do not exist in
WordNet and are not defined in the standard dictionaries of the language. We proposed
using a Web-based approach to add the OOV words to our words network based on
co-occurrence statistics, then use the same random walk model to predict the polar-
ity. We showed that this method can predict the polarity of OOV words with good
accuracy.
Acknowledgments
This research was funded by the Office of the
Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects
Activity (IARPA), through the U.S. Army
Research Lab. All statements of fact, opinion,
or conclusions contained herein are those of
the authors and should not be construed as
representing the official views or policies of
IARPA, the ODNI, or the U.S. Government.
559
Computational Linguistics Volume 40, Number 3
References
Agirre, Eneko, Enrique Alfonseca, Keith
Hall, Jana Kravalova, Marius Pas?ca, and
Aitor Soroa. 2009. A study on similarity
and relatedness using distributional and
wordnet-based approaches. In Proceedings
of Human Language Technologies: The 2009
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, NAACL ?09, pages 19?27,
Stroudsburg, PA.
Andreevskaia, Alina and Sabine Bergler.
2006. Mining WordNet for fuzzy
sentiment: Sentiment tag extraction
from WordNet glosses. In EACL?06,
pages 209?216.
Banea, Carmen, Rada Mihalcea, and
Janyce Wiebe. 2008. A bootstrapping
method for building subjectivity lexicons
for languages with scarce resources.
In LREC?08, pages 2,764?2,767.
Black, W., S. Elkateb, H. Rodriguez,
M. Alkhalifa, P. Vossen, A. Pease, and
C. Fellbaum. 2006. Introducing the
Arabic WordNet project. In Third
International WordNet Conference,
pages 295?299.
Blair-Goldensohn, Sasha, Tyler Neylon,
Kerry Hannan, George A. Reis, Ryan
McDonald, and Jeff Reynar. 2008. Building
a sentiment summarizer for local service
reviews. In NLP in the Information
Explosion Era.
Brody, Samuel and Noemie Elhadad. 2010.
An unsupervised aspect-sentiment model
for online reviews. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 804?812, Los Angeles, CA.
Elkateb, S., W. Black, H. Rodriguez,
M. Alkhalifa, P. Vossen, A. Pease, and
C. Fellbaum. 2006a. Building a WordNet
for Arabic. In Fifth International Conference
on Language Resources and Evaluation,
pages 29?34.
Elkateb, S., W. Black, P. Vossen, D. Farwell,
H. Rodriguez, A. Pease, and M. Alkhalifa.
2006b. Arabic WordNet and the challenges
of Arabic. In Arabic NLP/MT Conference,
pages 15?24.
Esuli, Andrea and Fabrizio Sebastiani. 2005.
Determining the semantic orientation
of terms through gloss classification.
In CIKM?05, pages 617?624.
Esuli, Andrea and Fabrizio Sebastiani. 2006.
Sentiwordnet: A publicly available lexical
resource for opinion mining. In LREC?06,
pages 417?422.
Etzioni, Oren, Kobi Reiter, Stephen Soderl,
and Marcus Sammer. 2007. Lexical
translation with application to image
search on the Web. In Proceedings of
Machine Translation Summit XI.
Hassan, Ahmed and Dragomir R. Radev.
2010. Identifying text polarity using
random walks. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, pages 395?403,
Uppsala.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1997. Predicting the semantic
orientation of adjectives. In EACL?97,
pages 174?181.
Hatzivassiloglou, Vasileios and Janyce
Wiebe. 2000. Effects of adjective
orientation and gradability on sentence
subjectivity. In COLING, pages 299?305.
Hu, Minqing and Bing Liu. 2004. Mining
and summarizing customer reviews.
In KDD?04, pages 168?177.
Jha, S., D. Narayan, P. Pande, and
P. Bhattacharyya. 2001. A WordNet for
Hindi. In International Workshop on Lexical
Resources in Natural Language Processing.
Jijkoun, Valentin and Katja Hofmann. 2009.
Generating a non-English subjectivity
lexicon: Relations that matter. In
Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009),
pages 398?405, Athens.
Kamps, Jaap, Maarten Marx, Robert J.
Mokken, and Maarten De Rijke. 2004.
Using WordNet to measure semantic
orientations of adjectives. In Proceedings
of the 4th International Conference on
Language Resources and Evaluation
(LREC 2004), pages 1115?1118.
Kanayama, Hiroshi and Tetsuya Nasukawa.
2006. Fully automatic lexicon expansion
for domain-oriented sentiment analysis.
In EMNLP?06, pages 355?363.
Kim, Soo-Min and Eduard Hovy. 2004.
Determining the sentiment of opinions.
In COLING, pages 1,367?1,373.
Kok, Stanley and Chris Brockett. 2010.
Hitting the right paraphrases in good
time. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 145?153,
Los Angeles, CA.
Lewis, D. D., Y. Yang, T. Rose, and F. Li.
2004. Rcv1: A new benchmark collection
for text categorization research. Journal of
Machine Learning Research, 5:361?397.
Mihalcea, Rada and Carmen Banea. 2007.
Learning multilingual subjective language
560
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
via cross-lingual projections. In Proceedings
of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 976?983.
Miller, George A. 1995. Wordnet: A lexical
database for English. Communications of
ACM, 38(11):39?41.
Mohammad, Saif, Cody Dunne, and Bonnie
Dorr. 2009. Generating high-coverage
semantic orientation lexicons from overtly
marked words and a thesaurus. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing: Volume 2, EMNLP ?09,
pages 599?608, Stroudsburg, PA.
Morinaga, Satoshi, Kenji Yamanishi, Kenji
Tateishi, and Toshikazu Fukushima. 2002.
Mining product reputations on the Web.
In KDD?02, pages 341?349.
Narayan, Dipak, Debasri Chakrabarti,
Prabhakar Pande, and P. Bhattacharyya.
2002. An experience in building the Indo
WordNet?a WordNet for Hindi. In First
International Conference on Global WordNet.
Nasukawa, Tetsuya and Jeonghee Yi. 2003.
Sentiment analysis: Capturing favorability
using natural language processing.
In K-CAP ?03: Proceedings of the 2nd
International Conference on Knowledge
Capture, pages 70?77.
Norris, J. 1997. Markov Chains. Cambridge
University Press.
Pang, Bo and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using
subjectivity summarization based on
minimum cuts. In Proceedings of the
42nd Annual Meeting of the Association
for Computational Linguistics, ACL ?04,
Stroudsburg, PA.
Popescu, Ana-Maria and Oren Etzioni. 2005.
Extracting product features and opinions
from reviews. In HLT-EMNLP?05,
pages 339?346.
Rao, Delip and Deepak Ravichandran.
2009. Semi-supervised polarity lexicon
induction. In Proceedings of the 12th
Conference of the European Chapter of the
ACL (EACL 2009), pages 675?682, Athens.
Riloff, Ellen and Janyce Wiebe. 2003.
Learning extraction patterns for
subjective expressions. In EMNLP?03,
pages 105?112.
Stone, Philip, Dexter Dunphy, Marchall
Smith, and Daniel Ogilvie. 1966. The
General Inquirer: A Computer Approach
to Content Analysis. The MIT Press.
Su, Fangzhong and Katja Markert. 2009.
Subjectivity recognition on word
senses via semi-supervised mincuts.
In Proceedings of Human Language
Technologies: The 2009 Annual Conference of
the North American Chapter of the Association
for Computational Linguistics, NAACL ?09,
pages 1?9, Stroudsburg, PA.
Szummer, Martin and Tommi Jaakkola.
2002. Partially labeled classification with
Markov random walks. In NIPS?02,
pages 945?952.
Takamura, Hiroya, Takashi Inui, and
Manabu Okumura. 2005. Extracting
semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Tong, Richard M. 2001. An operational
system for detecting and tracking opinions
in on-line discussion. Workshop note,
SIGIR 2001 Workshop on Operational Text
Classification.
Turney, Peter and Michael Littman. 2003.
Measuring praise and criticism: Inference
of semantic orientation from association.
ACM Transactions on Information Systems,
21:315?346.
Turney, Peter D. 2002. Thumbs up or thumbs
down?: Semantic orientation applied to
unsupervised classification of reviews.
In ACL?02, pages 417?424.
Velikovich, Leonid, Sasha Blair-Goldensohn,
Kerry Hannan, and Ryan McDonald. 2010.
The viability of Web-derived polarity
lexicons. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 777?785,
Los Angeles, CA.
Vossen, P. 1997. Eurowordnet: A multilingual
database for information retrieval. In
DELOS Workshop on Cross-Language
Information Retrieval, pages 5?7.
Wiebe, Janyce. 2000. Learning subjective
adjectives from corpora. In Proceedings of
the Seventeenth National Conference on
Artificial Intelligence and the Twelfth
Conference on Innovative Applications of
Artificial Intelligence, pages 735?740.
Wiebe, Janyce, Rebecca Bruce, Matthew Bell,
Melanie Martin, and Theresa Wilson.
2001. A corpus study of evaluative and
speculative language. In Proceedings of the
Second SIGdial Workshop on Discourse and
Dialogue, pages 1?10.
Wiebe, Janyce and Rada Mihalcea.
2006a. Word sense and subjectivity.
In Proceedings of the 21st International
Conference on Computational Linguistics
and the 44th Annual Meeting of the
Association for Computational Linguistics,
pages 1,065?1,072, Sydney.
Wiebe, Janyce and Rada Mihalcea. 2006b.
Word sense and subjectivity. In Proceedings
561
Computational Linguistics Volume 40, Number 3
of the 21st International Conference on
Computational Linguistics and the 44th
Annual Meeting of the Association for
Computational Linguistics, ACL-44,
pages 1,065?1,072, Stroudsburg, PA.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions of
opinions and emotions in language.
Language Resources and Evaluation,
39(2-3):165?210.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In EMNLP?03, pages 129?136.
Zhu, Xiaojin, Zoubin Ghahramani, and
John Lafferty. 2003. Semi-supervised
learning using Gaussian fields and
harmonic functions. In ICML?03,
pages 912?919.
562
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 80?90,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Reference Scope Identification in Citing Sentences
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
A citing sentence is one that appears in a sci-
entific article and cites previous work. Cit-
ing sentences have been studied and used in
many applications. For example, they have
been used in scientific paper summarization,
automatic survey generation, paraphrase iden-
tification, and citation function classification.
Citing sentences that cite multiple papers are
common in scientific writing. This observa-
tion should be taken into consideration when
using citing sentences in applications. For in-
stance, when a citing sentence is used in a
summary of a scientific paper, only the frag-
ments of the sentence that are relevant to the
summarized paper should be included in the
summary. In this paper, we present and com-
pare three different approaches for identifying
the fragments of a citing sentence that are re-
lated to a given target reference. Our methods
are: word classification, sequence labeling,
and segment classification. Our experiments
show that segment classification achieves the
best results.
1 Introduction
Citation plays an important role in science. It makes
the accumulation of knowledge possible. When a
reference appears in a scientific article, it is usually
accompanied by a span of text that highlights the
important contributions of the cited article. We
call a sentence that contains an explicit reference
to previous work a citation sentence. For example,
sentence (1) below is a citing sentence that cites a
paper by Philip Resnik and describes the problem
Resnik addressed in his paper.
(1) Resnik (1999) addressed the issue of language identification
for finding Web pages in the languages of interest.
Previous work has studied and used citation sen-
tences in various applications such as: scientific pa-
per summarization (Elkiss et al, 2008; Qazvinian
and Radev, 2008; Mei and Zhai, 2008; Qazvinian
et al, 2010; Qazvinian and Radev, 2010; Abu-
Jbara and Radev, 2011), automatic survey genera-
tion (Nanba et al, 2000; Mohammad et al, 2009),
citation function classification (Nanba et al, 2000;
Teufel et al, 2006; Siddharthan and Teufel, 2007;
Teufel, 2007), and paraphrase recognition (Nakov et
al., 2004; Schwartz et al, 2007).
Sentence (1) above contains one reference, and
the whole sentence is talking about that reference.
This is not always the case in scientific writing.
Sentences that contain references to multiple papers
are very common. For example, sentence (2) below
contains three references.
(2) Grefenstette and Nioche (2000) and Jones and Ghani (2000)
use the web to generate corpora for languages where electronic
resources are scarce, while Resnik (1999) describes a method
for mining the web for bilingual texts.
80
The first fragment describes the contribution of
Grefenstette and Nioche (2000) and Jones and Ghani
(2000). The second fragment describes the contribu-
tion of Resnik (1999).
This observation should be taken into considera-
tion when using citing sentences in the aforemen-
tioned applications. For example, in citation-based
summarization of scientific papers, a subset of cit-
ing sentences that cite a given target paper is se-
lected and used to form a summary of that paper.
It is very likely that one or more of the selected sen-
tences cite multiple papers besides the target. This
means that some of the text included in the sum-
mary might be irrelevant to the summarized paper.
Including irrelevant text in the summary introduces
several problems. First, the summarization task aims
at summarizing the contributions of the target paper
using minimal text. Extraneous text takes space in
the summary while being irrelevant and less impor-
tant. Second, including irrelevant text in the sum-
mary breaks the context and confuses the reader.
Therefore, if sentence (2) above is to be added to
a citation-based summary of Resniks? (1999) paper,
only the underlined fragment should be added to the
summary and the rest of the sentence should be ex-
cluded.
For another example, consider the task of citation
function classification. The goal of this task is to
determine the reason for citing paper B by paper A
based on linguistic and structural features extracted
from citing sentences that appear in A and cite B. If
a citing sentence in A cites multiple papers besides
B, classification features should be extracted only
from the fragments of the sentence that are relevant
to B. Sentence (3) below shows an examples of this
case.
(3) Cohn and Lapata (2008) used the GHKM extraction method (Galley
et al, 2004), which is limited to constituent phrases and thus produces
a reasonably small set of syntactic rules.
If the target reference is Cohn and Lapata (2008),
only the underlined segment should be used for fea-
ture extraction. The limitation stated in the sec-
ond segment of sentence is referring to Galley et al,
(2004).
In this paper, we address the problem of identi-
fying the fragments of a citing sentence that are re-
lated to a given target reference. Henceforth, we use
the term Reference Scope to refer to those fragments.
We present and compare three different approaches
to this problem.
In the first approach, we define the problem as a
word classification task. We classify each word in
the sentence as inside or outside the scope of the tar-
get reference.
In the second approach, we define the problem as
a sequence labeling problem. This is different from
the first approach in that the label assigned to each
word is dependent on the labels of nearby words. In
the third approach, instead of classifying individual
words, we split the sentence into segments and clas-
sify each segment as inside or outside the scope of
the target reference.
Applying any of the three approaches is pre-
ceded by a preprocessing stage. In this stage, cit-
ing sentences are analyzed to tag references, iden-
tify groups of references, and distinguish between
syntactic and non-syntactic references.
The rest of this paper is organized as follows. Sec-
tion 2 examines the related work. We define the
problem in Section3. Section 4 presents our ap-
proaches. Experiments, results and analysis are pre-
sented in Section 5. We conclude and provide direc-
tions to future work in Section 6
2 Related Work
Our work is related to a large body of research on
citations (Hodges, 1972; Garfield et al, 1984). The
interest in studying citations stems from the fact that
bibliometric measures are commonly used to esti-
mate the impact of a researcher?s work (Borgman
and Furner, 2002; Luukkonen, 1992). White (2004)
provides a good recent survey of the different re-
search lines that use citations. In this section we re-
view the research lines that are relevant to our work
81
and show how our work is different.
One line of research that is related to our work
has to do with identifying what Nanba and Oku-
mura (1999) call the citing area They define the cit-
ing area as the succession of sentences that appear
around the location of a given reference in a sci-
entific paper and have connection to it. Their al-
gorithm starts by adding the sentence that contains
the target reference as the first member sentence in
the citing area. Then, they use a set of cue words
and hand-crafted rules to determine whether the sur-
rounding sentences should be added to the citing
area or not. In (Nanba et al, 2000) they use their cit-
ing area identification algorithm to improve citation
type classification and automatic survey generation.
Qazvinian and Radev (2010) addressed a simi-
lar problem. They proposed a method based on
probabilistic inference to extract non-explicit cit-
ing sentences; i.e., sentences that appear around
the sentence that contains the target reference and
are related to it. They showed experimentally that
citation-based survey generation produces better re-
sults when using both explicit and non-explicit cit-
ing sentences rather than using the explicit ones
alone.
Although this work shares the same general goal
with ours (i.e identifying the pieces of text that are
relevant to a given target reference), our work is dif-
ferent in two ways. First, previous work mostly ig-
nored the fact that the citing sentence itself might
be citing multiple references. Second, it defined the
citing area (Nanba and Okumura, 1999) or the ci-
tation context (Qazvinian and Radev, 2010) as a set
of whole contiguous sentences. In our work, we ad-
dress the case where one citing sentence cites mul-
tiple papers, and define what we call the reference
scope to be the fragments (not necessarily contigu-
ous) of the citing sentence that are related to the tar-
get reference.
In a recent work on citation-based summarization
by Abu-Jbara and Radev (2011), the authors noticed
the issue of having multiple references in one sen-
tence. They raised this issue when they discussed
the factors that impede the coherence and the read-
ability of citation-based summaries. They suggested
that removing the fragments of a citing sentence that
are not relevant to the summarized paper will sig-
nificantly improve the quality of the produced sum-
maries. In their work, they defined the scope of a
given reference as the shortest fragment of the citing
sentence that contains the reference and could form
a grammatical sentence if the rest of the sentence
was removed. They identify the scope by generating
the syntactic parse tree of the sentence and then find-
ing the text that corresponds to the smallest subtree
rooted at an S node and contains the target reference
node as one of its leaf nodes. They admitted that
their method was very basic and works only when
the scope forms one grammatical fragment, which
is not true in many cases.
Athar (2011) noticed the same issue with cit-
ing sentences that cite multiple references, but this
time in the context of sentiment analysis in ci-
tations. He showed experimentally that identify-
ing what he termed the scope of citation influ-
ence improves sentiment classification accuracy. He
adapted the same basic method proposed by Abu-
Jbara and Radev (2011). We use this method as a
baseline in our evaluation below.
In addition to this related work, there is a large
body of research that used citing sentences in differ-
ent applications. For example, citing sentences have
been used to summarize the contributions of a scien-
tific paper (Qazvinian and Radev, 2008; Qazvinian
et al, 2010; Qazvinian and Radev, 2010; Abu-Jbara
and Radev, 2011). They have been also used to
generate surveys of scientific paradigms (Nanba and
Okumura, 1999; Mohammad et al, 2009). Several
other papers analyzed citing sentences to recognize
the citation function; i.e., the author?s reason for cit-
ing a given paper (Nanba et al, 2000; Teufel et al,
2006; Teufel, 2007). Schwartz et al (2007) pro-
posed a method for aligning the words within citing
sentences that cite the same paper. The goal of his
work was to aid named entity recognition and para-
phrase identification in scientific papers.
82
We believe that all the these applications will ben-
efit from the output of our work.
3 Problem Definition
The problem that we are trying to solve is to iden-
tify which fragments of a given citing sentence that
cites multiple references are semantically related
to a given target reference. As stated above, we
call these fragments the reference scope. Formally,
given a citing sentence S = {w1, w2, ..., wn} where
w1, w2, ..., wn are the tokens of the sentence and
given that S contains a set of two or more references
R, we want to assign the label 1 to the word wi if it
falls in the scope of a given target reference r ? R,
and 0 otherwise.
For example, sentences (4) and (5) below are
labeled for the target references Tetreault and
Chodorow (2008), and Cutting et al(1992) respec-
tively. The underlined words are labeled 1 (i.e.,
inside the target reference scope), while all others
are labeled 0.
(4) For example, Tetreault and Chodorow (2008) use a maximum
entropy classifier to build a model of correct preposition usage, with 7
million instances in their training set, and Lee and Knutsson (2008)
use memory-based learning, with 10 million sentences in their training
set.
(5) There are many POS taggers developed using different techniques
for many major languages such as transformation-based error-driven
learning (Brill, 1995), decision trees (Black et al, 1992), Markov
model (Cutting et al, 1992), maximum entropy methods (Ratnaparkhi,
1996) etc for English.
4 Approach
In this section, we present our approach for address-
ing the problem defined in the previous section. Our
approach involves two stages: 1) preprocessing and
2) reference scope identification. We present three
alternative methods for the second stage. The fol-
lowing two subsections describe the two stages.
4.1 Stage 1: Preprocessing
The goal of the preprocessing stage is to clean and
prepare the citing sentence for the next processing
steps. The second stage involves higher level text
processing such as part-of-speech tagging, syntac-
tic parsing, and dependency parsing. The available
tools for these tasks are not trained on citing sen-
tences which contain references written in a special
format. For example, it is very common in scien-
tific writing to have references (usually written be-
tween parentheses) that are not a syntactic part of the
sentence. It is also common to cite a group of ref-
erences who share the same contribution by listing
them between parentheses separated by a comma or
a semi-colon. We address these issues to improve
the accuracy of the processing done in the second
stage. The preprocessing stage involves three tasks:
4.1.1 Reference Tagging
The first preprocessing task is to find and tag all
the references that appear in the citing sentence.
Authors of scientific articles use standard patterns
to include references in text. We apply a regular
expression to find all the references that appear
in a sentence. We replace each reference with a
placeholder. The target reference is replaced by
TREF. Each other reference is replaced by REF.
We keep track of the original text of each replaced
reference. Sentence (6) below shows an example of
a citing sentence with the references replaced.
(6) These constraints can be lexicalized (REF.1; REF.2), un-
lexicalized (REF.3; TREF.4) or automatically learned (REF.5;
REF.6).
4.1.2 Reference Grouping
It is common in scientific writing to attribute one
contribution to a group of references. Sentence (6)
above contains three groups of references. Each
group constitutes one entity. Therefore, we replace
each group with a placeholder. We use GTREF
to replace a group of references that contains the
target reference, and GREF to replace a group of
references that does not contain the target reference.
83
Sentence (7) below is the same as sentence (6) but
with the three groups of references replaced.
(7) These constraints can be lexicalized (GREF.1), unlexicalized
(GTREF.2) or automatically learned (GREF.3).
4.1.3 Non-syntactic Reference Removal
A reference (REF or TREF) or a group of refer-
ences (GREF or GTREF) could either be a syntactic
constituent and has a semantic role in the sentence
(e.g. GTREF.1 in sentence (8) below) or not (e.g.
REF.2 in sentence (8)).
(8) (GTREF.1) apply fuzzy techniques for integrating source
syntax into hierarchical phrase-based systems (REF.2).
The task in this step is to determine whether a ref-
erence is a syntactic component in the sentence or
not. If yes, we keep it as is. If not, we remove it
from the sentence and keep track of its position. Ac-
cordingly, after this step, REF.2 in sentence (8) will
be removed. We use a rule-based algorithm to deter-
mine whether a reference should be removed from
the sentence or kept. Our algorithm (Algorithm 1)
uses stylistic and linguistic features such as the style
of the reference, the position of the reference, and
the surrounding words to make the decision.
When a reference is removed, we pick a word
from the sentence to represent it. This is needed for
feature extraction in the next stage. We use as a rep-
resentative the head of the closest noun phrase (NP)
that comes before the position of the removed refer-
ence. For example, in sentence (8) above, the closest
NP before REF.2 is hierarchical phrase-based sys-
tems and the head is the noun systems.
4.2 Stage 2: Reference Scope Identification
In this section we present three different methods
for identifying the scope of a given reference within
a citing sentence. We compare the performance of
these methods in Section 5. The following three sub-
sections describe the methods.
Algorithm 1 Remove Non-syntactic References
Require: A citing sentence S
1: for all Reference R (REF, TREF, GREF, or GTREF)
in S do
2: if R style matches ?Authors (year)? then
3: Keep R // syntactic
4: else if R is the first word in the sentence or in a
clause then
5: Keep R // syntactic
6: else if R is preceded by a preposition (in, of, by,
etc.) then
7: Keep R // syntactic
8: else
9: Remove R // non-syntactic
10: end if
11: end for
4.2.1 Word Classification
In this method we define reference scope identifi-
cation as a classification task of the individual words
of the citing sentence. Each word is classified as
inside or outside the scope of a given target refer-
ence. We use a number of linguistic and structural
features to train a classification model on a set of
labeled sentences. The trained model is then used
to label new sentences. The features that we use to
train the model are listed in Table 1. We use the
Stanford parser (Klein and Manning, 2003) for syn-
tactic and dependency parsing. We experiment with
two classification algorithms: Support Vector Ma-
chines (SVM) and logistic regression.
4.2.2 Sequence Labeling
In the method described in Section 4.2.1 above,
we classify each word independently from the la-
bels of the nearby words. The nature of our task,
however, suggests that the accuracy of word classifi-
cation can be improved by considering the labels of
the words surrounding the word being classified. It
is very likely that the word takes the same label as
the word before and after it if they all belong to the
same clause in the sentence. In this method we de-
fine the problem as a sequence labeling task. Now,
instead of looking for the best label for each word
individually, we look for the globally best sequence
84
Feature Description
Distance The distance (in words) between the word and the target reference.
Position This feature takes the value 1 if the word comes before the target reference, and 0 otherwise.
Segment After splitting the sentence into segments by punctuation and coordination conjunctions, this feature takes
the value 1 if the word occurs in the same segment with the target reference, and 0 otherwise.
Part of speech tag The part of speech tag of the word, the word before, and the word after.
Dependency Distance Length of the shortest dependency path (in the dependency parse tree) that connects the word to the tar-
get reference or its representative. It has been shown in previous work on relation extraction that the
shortest path between any two entities captures the information required to assert a relationship between
them (Bunescu and Mooney, 2005)
Dependency Relations This item includes a set of features. Each features corresponds to a dependency relation type. If the relation
appears in the dependency path that connects the word to the target reference or its representative, its
corresponding feature takes the value 1, and 0 otherwise.
Common Ancestor Node The type of the node in the syntactic parse tree that is the least common ancestor of the word and the target
reference.
Syntactic Distance The number of edges in the shortest path that connects the word and the target reference in the syntactic
parse tree.
Table 1: The features used for word classification and sequence labeling
of labels for all the words in the sentence at once.
We use Conditional Random Fields (CRF) as our
sequence labeling algorithm. In particular, we use
first-order chain-structured CRF. The chain consists
of two sets of nodes: a set of hidden nodes Y which
represent the scope labels (0 or 1) in our case, and
a set of observed nodes X which represent the ob-
served features. The task is to estimate the probabil-
ity of a sequence of labels Y given the sequence of
observed features X: P (Y|X)
Lafferty et al (2001) define this probability to be
a normalized product of potential functions ?:
P (y|x) =
?
t
?k(yt, yt?1, x) (1)
Where ?k(yt, yt?1, x) is defined as
?k(yt, yt?1, x) = exp(
?
k
?kf(yt, yt?1, x)) (2)
where f(yt, yt?1, x) is a transition feature func-
tion of the label at positions i ? 1 and i and the
observation sequence x; and ?j is parameter to be
estimated from training data. We use, as the obser-
vations at each position, the same features that we
used in Section 4.2.1 above (Table 1).
4.2.3 Segment Classification
We noticed that the scope of a given reference
often consists of units of higher granularity than
words. Therefore, in this method, we split the
sentence into segments of contiguous words and,
instead of labeling individual words, we label
the whole segment as inside or outside the scope
of the target reference. We experimented with
two different segmentation methods. In the first
method (method-1), we segment the sentence at
punctuation marks, coordination conjunctions, and
a set of special expressions such as ?for example?,
?for instance?, ?including?, ?includes?, ?such as?,
?like?, etc. Sentence (8) below shows an example of
this segmentation method (Segments are enclosed
in square brackets).
(8) [Rerankers have been successfully applied to numerous NLP
tasks such as] [parse selection (GTREF)], [parse reranking (GREF)],
[question-answering (REF)].
In the second segmentation method (method-2),
we split the sentence into segments of finer gran-
ularity. We use a chunking tool to identify noun
groups, verb groups, preposition groups, adjective
85
groups, and adverb groups. Each such group (or
chunk) forms a segment. If a word does not belong
to any chunk, it forms a singleton segment by
itself. Sentence (9) below shows an example of this
segmentation method (Segments are enclosed in
square brackets).
(9) [To] [score] [the output] [of] [the coreference models],
[we] [employ] [the commonly-used MUC scoring program (REF)]
[and] [the recently-developed CEAF scoring program (TREF)].
We assign a label to each segment in two steps. In
the first step, we use the sequence labeling method
described in Section 4.2.2 to assign labels to all the
individual words in the sentence. In the second step,
we aggregate the labels of all the words contained in
a segment to assign a label to the whole segment. We
experimented with three different label aggregation
rules: 1) rule-1: assign to the segment the majority
label of the words it contains, and 2) rule-2: assign
to the segment the label 1 (i.e., inside) if at least one
of the words contained in the segment is labeled 1,
and assign the label 0 to the segment otherwise, and
3) rule-3: assign the label 0 to the segment if at least
of the words it contains is labeled 0, and assign 1
otherwise.
5 Evaluation
5.1 Data
We use the ACL Anthology Network corpus
(AAN) (Radev et al, 2009) in our evaluation. AAN
is a publicly available collection of more than 19,000
NLP papers. AAN provides a manually curated cita-
tion network of its papers and the citing sentence(s)
associated with each edge. The current release of
AAN contains about 76,000 unique citing sentences
56% of which contain 2 or more references and 44%
contain 1 reference only. From this set, we ran-
domly selected 3500 citing sentences, each contain-
ing at least two references (3.75 references on aver-
age with a standard deviation of 2.5). The total num-
ber of references in this set of sentences is 19,591.
We split the data set into two random subsets:
a development set (200 sentences) and a train-
ing/testing set (3300 sentences). We used the devel-
opment set to study the data and develop our strate-
gies of addressing the problem. The second set was
used to train and test the system in a cross-validation
mode.
5.2 Annotation
We asked graduate students with good background
in NLP (the area of the annotated sentences) to pro-
vide three annotations for each sentence in the data
set described above. First, we asked them to de-
termine whether each of the references in the sen-
tence was correctly tagged or not. Second, we asked
them to determine for each reference whether it is a
syntactic constituent in the sentence or not. Third,
we asked them to determine and label the scope of
one reference in each sentence which was marked
as a target reference (TREF). We designed a user-
friendly tool to collect the annotations from the stu-
dents.
To estimate the inter-annotator agreement, we
picked 500 random sentences from our data set and
assigned them to two different annotators. The inter-
annotator agreement was perfect on both the refer-
ence tagging annotation and the reference syntacti-
cality annotation. This is expected since both are ob-
jective, clear, and easy tasks. To measure the inter-
annotator agreement on the scope annotation task,
we deal with it as a word classification task. This
allows us to use the popular classification agreement
measure, the Kappa coefficient (Cohen, 1968). The
Kappa coefficient is defined as follows:
K =
P (A)? P (E)
1? P (E)
(3)
where P(A) is the relative observed agreement
among raters and P(E) is the hypothetical probabil-
ity of chance agreement. The agreement between
the two annotators on the scope identification task
was K = 0.61. On Landis and Kochs (Landis and
Koch, 1977) scale, this value indicates substantial
agreement.
86
5.3 Experimental Setup
We use the Edinburgh Language Technology Text
Tokenization Toolkit (LT-TTT) (Grover et al, 2000)
for text tokenization, part-of-speech tagging, chunk-
ing, and noun phrase head identification. We use
the Stanford parser (Klein and Manning, 2003) for
syntactic and dependency parsing. We use Lib-
SVM (Chang and Lin, 2011) for Support Vector Ma-
chines (SVM) classification. Our SVM model uses a
linear kernel. We use Weka (Hall et al, 2009) for lo-
gistic regression classification. We use the Machine
Learning for Language Toolkit (MALLET) (McCal-
lum, 2002) for CRF-based sequence labeling. In
all the scope identification experiments and results
below, we use 10-fold cross validation for train-
ing/testing.
5.4 Preprocessing Component Evaluation
We ran our three rule-based preprocessing modules
on the testing data set and compared the output to
the human annotations. The test set was not used
in the tuning of the system but was done using the
development data set as described above. We report
the results for each of the preprocessing modules.
Our reference tagging module achieved 98.3% pre-
cision and 93.1% recall. Most of the errors were
due to issues with text extraction from PDF or due
to bad references practices by some authors (i.e., not
following scientific referencing standards). Our ref-
erence grouping module achieved perfect accuracy
for all the correctly tagged references. This was
expected since this is a straightforward task. The
non-syntactic reference removal module achieved
90.08% precision and 90.1% recall. Again, most of
the errors were the result of bad referencing prac-
tices by the authors.
5.5 Reference Scope Identification
Experiments
We conducted several experiments to compare the
methods proposed in Section 4 and their variants.
We ran all the experiments on the training/testing
set (the 3300 sentences) described in Section 5.1.
Method Accuracy Precision Recall F-measure
AR-2011 54.0% 63.3% 33.1% 41.5%
WC-SVM 74.9% 74.5% 93.4% 82.9%
WC-LR 74.3% 76.8% 88.0% 82.0%
SL-CRF 78.2% 80.1% 94.2% 86.6%
SC-S1-R1 73.7% 72.1% 97.8% 83.0%
SC-S1-R2 69.3% 68.4% 98.9% 80.8%
SC-S1-R3 60.0% 61.8% 73.3% 60.9%
SC-S2-R1 81.8% 81.2% 93.8% 87.0%
SC-S2-R2 78.2% 77.3% 94.9% 85.2%
SC-S2-R3 66.1% 67.1% 71.2% 69.1%
Table 3: Results of scope identification using the different
algorithms described in the paper
The experiments that we ran are as follows: 1) word
classification using a SVM classifier (WC-SVM);
2) word classification using a logistic regression
classifier(WC-LR); 3) CRF-based sequence labeling
(SL-CRF); 4) segment classification using segmen-
tation method-1 and label aggregation rule-1 (SC-
S1-R1); 5,6,7,8,9) same as (4) but using different
combinations of segmentation methods 1 and 2, and
label aggregation rules 1,2 and 3: SC-S1-R2, SC-
S1-R3, SC-S2-R1, SC-S2-R2, SC-S2-R3 (where Sx
refers to segmentation method x and Ry refers to
label aggregation rule y all as explained in Sec-
tion 4.2.3). Finally, 10) we compare our meth-
ods to the baseline method proposed by Abu-Jbara
and Radev (2011) which was described in Section 4
(AR-2011).
To better understand which of the features listed
in Table 1 are more important for the task, we use
Guyon et al?s (2002) method for feature selection
using SVM to rank the features based on their im-
portance. The results of the experiments and the
feature analysis are presented and discussed in the
following subsection.
5.6 Results and Discussion
5.6.1 Experimental Results
We ran the experiments described in the previ-
ous subsection on the testing data described in Sec-
87
Method Output
E
xa
m
pl
e
1 Word Classification
(WC-SVM)
A wide range of contextual information, such as surrounding words (GREF ), dependency or case structure
(GTREF ), and dependency path (GREF ), has been utilized for similarity calculation, and achieved consid-
erable success.
Sequence Labeling (SL-
CRF)
A wide range of contextual information, such as surrounding words (GREF), dependency or case structure
(GTREF), and dependency path (GREF ), has been utilized for similarity calculation, and achieved consid-
erable success.
Segment Classification
(SC-S2-R1)
A wide range of contextual information, such as surrounding words (GREF ), dependency or case structure
(GTREF ), and dependency path (GREF ), has been utilized for similarity calculation, and achieved
considerable success.
E
xa
m
pl
e
2 Word Classification
(WC-SVM)
Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering
(REF).
Sequence Labeling (SL-
CRF)
Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering
(REF).
Segment Classification
(SC-S2-R1)
Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering
(REF).
Table 2: Two example outputs produced by the three methods
tion 5.1. Table 3 compares the precision, recall, F1,
and accuracy for the three methods described in Sec-
tion 4 and their variations. All the metrics were com-
puted at the word level. The results show that all our
methods outperform the baseline method AR-2011
that was proposed by Abu-Jbara and Radev (2011).
In the word classification method, we notice no sig-
nificant difference between the performance of the
SVM vs Logistic Regression classifier. We also no-
tice that the CRF-based sequence labeling method
performs significantly better than the word classi-
fication method. This result corroborates our intu-
ition that the labels of neighboring words are de-
pendent. The results also show that segment la-
beling generally performs better than word label-
ing. More specifically, the results indicate that seg-
mentation based on chunking and the label aggre-
gation based on plurality when used together (i.e.,
SC-S2-R1) achieve higher precision, accuracy, and
F-measure than the punctuation-based segmentation
and the other label aggregation rules.
Table 2 shows the output of the three methods on
two example sentences. The underlined words are
labeled by the system as scope words.
5.6.2 Feature Analysis
We performed an analysis of our classification
features using Guyon et al (2002) method. The
analysis revealed that both structural and syntactic
features are important. Among the syntactic fea-
tures, the dependency path is the most important.
Among the structural features, the segment feature
(as described in Table 1) is the most important.
6 Conclusions
We presented and compared three different meth-
ods for reference scope identification: word classi-
fication, sequence labeling, and segment classifica-
tion. Our results indicate that segment classification
achieves the best performance. The next direction in
this research is to extract the scope of a given refer-
ence as a standalone grammatical sentence. In many
cases, the scope identified by our method can form
a grammatical sentence with no or minimal postpro-
cessing. In other cases, more advanced text regener-
ation techniques are needed for scope extraction.
References
Amjad Abu-Jbara and Dragomir Radev. 2011. Coherent
citation-based summarization of scientific papers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
88
guage Technologies, pages 500?509, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Awais Athar. 2011. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceedings
of the ACL 2011 Student Session, pages 81?87, Port-
land, OR, USA, June. Association for Computational
Linguistics.
Christine L. Borgman and Jonathan Furner. 2002. Schol-
arly communication and bibliometrics. ANNUAL RE-
VIEW OF INFORMATION SCIENCE AND TECH-
NOLOGY, 36(1):2?72.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 724?731, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
J. Cohen. 1968. Weighted kappa: Nominal scale agree-
ment with provision for scaled disagreement or partial
credit. Psychological Bulletin, 70:213?220.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir Radev. 2008. Blind men
and elephants: What do citation summaries tell us
about a research article? J. Am. Soc. Inf. Sci. Tech-
nol., 59(1):51?62.
E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The
Use of Citation Data in Writing the History of Science.
Institute for Scientific Information Inc., Philadelphia,
Pennsylvania, USA.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. Lt ttt - a flexible tokenisation
tool. In In Proceedings of Second International Con-
ference on Language Resources and Evaluation, pages
1147?1154.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Mach.
Learn., 46:389?422, March.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
T. L. Hodges. 1972. Citation indexing-its theory
and application in science, technology, and humani-
ties. Ph.D. thesis, University of California at Berke-
ley.Ph.D. thesis, University of California at Berkeley.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In IN PROCEEDINGS OF
THE 41ST ANNUAL MEETING OF THE ASSOCIA-
TION FOR COMPUTATIONAL LINGUISTICS, pages
423?430.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
J. Richard Landis and Gary G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159?174, March.
Terttu Luukkonen. 1992. Is scientists? publishing be-
haviour rewardseeking? Scientometrics, 24:297?319.
10.1007/BF02017913.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL-08: HLT, pages 816?824, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 584?592, Boulder, Colorado, June. Association
for Computational Linguistics.
Preslav I. Nakov, Ariel S. Schwartz, and Marti A. Hearst.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In In Proceedings of the SI-
GIR04 workshop on Search and Discovery in Bioin-
formatics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI ?99: Proceedings of the Six-
teenth International Joint Conference on Artificial In-
telligence, pages 926?931, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and
Of Information Science. 2000. Classification of re-
search papers using citation links and citation types:
Towards automatic review article generation.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 689?696, Manchester, UK, August. Coling 2008
Organizing Committee.
89
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555?564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Ozgur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 895?903, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network corpus.
In NLPIR4DL ?09: Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital Li-
braries, pages 54?61, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Ariel Schwartz, Anna Divoli, and Marti Hearst. 2007.
Multiple alignment of citation sentences with con-
ditional random fields and posterior decoding. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 847?857.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attributing
scientific work to citations. In In Proceedings of
NAACL/HLT-07.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
In Proc. of EMNLP-06.
Simone Teufel. 2007. Argumentative zoning for im-
proved citation indexing. computing attitude and affect
in text. In Theory and Applications, pages 159170.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89?116.
90
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 33?36,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
AttitudeMiner: Mining Attitude from Online Discussions
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Ahmed Hassan
Microsoft Research
Redmond, WA, USA
hassanam@microsoft.com
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
This demonstration presents AttitudeMiner, a
system for mining attitude from online dis-
cussions. AttitudeMiner uses linguistic tech-
niques to analyze the text exchanged between
participants of online discussion threads at dif-
ferent levels of granularity: the word level, the
sentence level, the post level, and the thread
level. The goal of this analysis is to iden-
tify the polarity of the attitude the discussants
carry towards one another. Attitude predic-
tions are used to construct a signed network
representation of the discussion thread. In this
network, each discussant is represented by a
node. An edge connects two discussants if
they exchanged posts. The sign (positive or
negative) of the edge is set based on the po-
larity of the attitude identified in the text asso-
ciated with the edge. The system can be used
in different applications such as: word polar-
ity identification, identifying attitudinal sen-
tences and their signs, signed social network
extraction from text, subgroup detect in dis-
cussion. The system is publicly available for
download and has an online demonstration at
http://clair.eecs.umich.edu/AttitudeMiner/.
1 Introduction
The rapid growth of social media has encouraged
people to interact with each other and get involved
in discussions more than anytime before. The most
common form of interaction on the web uses text
as the main communication medium. When people
discuss a topic, especially when it is a controversial
one, it is normal to see situations of both agreement
and disagreement among the discussants. It is even
not uncommon that the big group of discussants split
into two or more smaller subgroups. The members
of each subgroup mostly agree and show positive
attitude toward each other, while they mostly dis-
agree with the members of opposing subgroups and
possibly show negative attitude toward them. These
forms of sentiment are expressed in text by using
certain language constructs (e.g. use insult or nega-
tive slang to express negative attitude).
In this demonstration, we present a system that
applies linguistic analysis techniques to the text of
online discussions to predict the polarity of relations
that develop between discussants. This analysis is
done on words to identify their polarities, then on
sentences to identify attitudinal sentences and the
sign of attitude, then on the post level to identify the
sign of an interaction, and finally on the entire thread
level to identify the overall polarity of the relation.
Once the polarity of the pairwise relations that de-
velop between interacting discussants is identified,
this information is then used to construct a signed
network representation of the discussion thread.
The system also implements two signed network
partitioning techniques that can be used to detect
how the discussants split into subgroups regarding
the discussion topic.
The functionality of the system is based on
our previous research on word polarity identifica-
tion (Hassan and Radev, 2010) and attitude identifi-
cation (Hassan et al, 2010). The system is publicly
available for download and has a web interface to try
online1.
This work is related to previous work in the areas
of sentiment analysis and online discussion mining.
Many previous systems studied the problem of iden-
tifying the polarity of individual words (Hatzivas-
siloglou and McKeown, 1997; Turney and Littman,
2003). Opinionfinder (Wilson et al, 2005a) is a sys-
tem for mining opinions from text. Another research
line focused on analyzing online discussions. For
example, Lin et al (2009) proposed a sparse coding-
based model that simultaneously models the seman-
tics and the structure of threaded discussions and
Shen et al (2006) proposed a method for exploit-
ing the temporal information in discussion streams
to identify the reply structure of the dialog. Many
systems addressed the problem of extracting social
networks from data (Elson et al, 2010; McCallum
et al, 2007), but none of them considered both pos-
itive and negative relations.
In the rest of the paper, we describe the system
architecture, implementation, usage, and its perfor-
1http://clair.eecs.umich.edu/AttitudeMiner/
33
Discussion 
Thread 
?.??. 
?.??. 
?.??. 
Text Polarity 
Identification 
? Identify polarized words 
? Identify the contextual 
polarity of each word 
 
 
Attitude Identification 
? Identify Attitudinal 
Sentences 
? Predict the sign on 
attitude 
 
 
Post sign identification 
? Aggregate the signs of 
attitudinal sentences to 
assign a sign to the 
post. 
 
Relation Sign 
? Aggregate the signs of all 
the posts exchanged by 
interacting participants 
to assign a sign for their 
relation. 
Signed Network 
 
 
 
 
 
Subgroups 
 
 
 
 
 
Thread Parsing 
? Identify posts 
? Identify discussants 
? Identify the reply 
structure 
? Tokenize text 
? Split posts into sentences 
 
+ _ 
Figure 1: Overview of the system processing pipeline
mance evaluation.
2 System Overview
Figure 1 shows a block diagram of the system com-
ponents and the processing pipeline. The first com-
ponent in the system is the thread parsing com-
ponent which takes as input a discussion thread
and parses it to identify the posts, the participants,
and the reply structure of the thread. This compo-
nent uses a module from CLAIRLib (Abu-Jbara and
Radev, 2011) to tokenize the posts and split them
into sentences.
The second component in the pipeline processes
the text of the posts to identify polarized words and
tag them with their polarity. This component uses
the publicly available tool, opinionfinder (Wilson et
al., 2005a), as a framework for polarity identifica-
tion. This component uses an extended polarity lex-
icon created by applying a random walk model to
WordNet (Miller, 1995) and a set of seed polarized
words. This approach is described in detail in our
previous work (Hassan and Radev, 2010). The con-
text of words is taken into consideration by running
a contextual word classifier that determines whether
the word is used in a polarized sense given the con-
text (Wilson et al, 2005b). For example, a positive
word appearing in a negated scope is used in a neg-
ative, rather than a positive sense.
The next component is the attitude identification
component. Given a sentence, our model predicts
whether it carries an attitude from the text writer to-
ward the text recipient or not. As we are only in-
terested in attitudes between participants, we limit
our analysis to sentences that use mentions of a dis-
cussion participants (i.e. names or second person
pronouns). We also discard all sentences that do
not contain polarized expressions as detected by the
previous component. We extract several patterns at
different levels of generalization representing any
given sentence. We use words, part-of-speech tags,
and dependency relations. We use those patterns to
build two Markov models for every kind of patterns.
The first model characterizes the relation between
different tokens for all patterns that correspond to
sentences that have an attitude. The second model
is similar to the first one, but it uses all patterns that
correspond to sentences that do not have an attitude.
Given a new sentence, we extract the corresponding
patterns and estimate the likelihood of every pattern
being generated from the two corresponding mod-
els. We then compute the likelihood ratio of the sen-
tence under every pair of models. Notice that we
have a pair of models corresponding to every type of
patterns. The likelihood ratios are combined using a
linear model, the parameters of which are estimated
using a development dataset. Please refer to (Hassan
et al, 2010) for more details about this component.
The next component works on the post level. It
assigns a sign to each post based on the signs of the
sentences it contains. A post is classified as negative
if it has at leastNs negative sentences, otherwise it is
classified as positive. The value ofNs can be chosen
by the user or set to default which was estimated
using a small labeled development set. The default
value forNs is 1 (i.e. if the post contains at least one
negative sentence, the whole post is considered to be
negative).
The next component in the pipeline uses the atti-
tude predictions from posts to construct a signed net-
work representation of the discussion thread. Each
participant is represented by a node. An edge is
created between two participants if they interacted
with each other. A sign (positive or negative) is as-
signed to an edge based on the signs of the posts
the two participants connected by the edge have ex-
changed. This is done by comparing the number of
positive and negative posts. A negative sign is given
if the two participants exchanged at least Np nega-
tive posts. The value of Np can be set using a devel-
opment set. The default value is 1.
The last component is the subgroup identifica-
34
Figure 2: The web interface for detecting subgroups in discussions
tion component. This component provides imple-
mentations for two signed network partitioning algo-
rithms. The first one is a greedy optimization algo-
rithm that is based on the principals of the structural
balance theory. The algorithm uses a criterion func-
tion for a local optimization partitioning such that
positive links are dense within groups and negative
links are dense between groups. The algorithm is de-
scribed in detail in (Doreian and Mrvar, 1996). The
second algorithm is FEC (Yang et al, 2007). FEC
is based on an agent-based random walk model. It
starts by finding a sink community, and then extract-
ing it from the entire network based on a graph cut
criteria that Yang et al (2007) proposed. The same
process is then applied recursively to the extracted
community and the rest of the network.
3 Implementation Details
The system is implemented in Perl. Some of the
components in the processing pipeline use external
tools that are implemented in either Perl, Java, or
Python. All the external tools come bundled with the
system. The system is compatible with all the ma-
jor platforms including windows, Mac OS, and all
Linux distributions. The installation process is very
straightforward. There is a single installation script
that will install the system, install all the dependen-
cies, and do all the required configurations. The in-
stallation requires that Java JRE, Perl, and Python be
installed on the machine.
The system has a command-line interface that
provides full access to the system functionality. The
command-line interface can be used to run the whole
pipeline or any portion of it. It can also be used to ac-
cess any component directly. Each component has a
corresponding script that can be run separately. The
input and output specifications of each component
are described in the accompanying documentation.
All the parameters that control the performance of
the system can also be passed through the command-
line interface.
The system can process any discussion thread that
is input to it in a specific XML format. The fi-
nal output of the system is also in XML format.
The XML schema of the input/output is described
in the documentation. It is the user responsibil-
ity to write a parser that converts an online discus-
sion thread to the expected XML format. The sys-
tem package comes with three such parsers for three
different discussion sites: www.politicalforum.com,
groups.google.com, and www.createdebate.com.
The distribution also comes with three datasets
(from three different sources) comprising a total of
300 discussion threads. The datasets are annotated
with the subgroup labels of discussants. Included in
the distribution as well, a script for generating a vi-
sualization of the extracted signed network and the
identified subgroups.
AttitudeMiner also has a web interface that
demonstrates most of its functionality. The web in-
35
Figure 3: The web interface for identifying attitudinal
sentences and their polarity
terface is intended for demonstration purposes only.
No webservice is provided. Figure 2 and Figrue 3
show two screenshots for the web interface.
4 System Performance
In this section, we give a brief summary of the sys-
tem performance. The method that generates the
extended polarity lexicon that is used for word po-
larity identification achieves 88.8% accuracy as re-
ported in (Hassan and Radev, 2010). The attitude
identification component distinguishes between at-
titudinal and non-attitudinal sentences with 80.3%
accuracy, and predicts the signs of attitudinal sen-
tences with 97% accuracy as reported in (Hassan et
al., 2010). Our evaluation for the signed network
extraction component on a large annotated dataset
showed that it achieves 83.5% accuracy. Finally, our
experiments on an annotated discussion showed that
the system can detect subgroups with 77.8% purity.
The system was evaluated using a dataset with thou-
sands of posts labeled by human annotators.
5 Conclusion
We presented of a demonstration of a social me-
dia mining system that used linguistic analysis tech-
niques to understand the relations that develop be-
tween users in online communities. The system is
capable of analyzing the text exchanged during dis-
cussions and identifying positive and negative atti-
tudes. Positive attitude reflects a friendly relation
while negative attitude is a sign of an antagonistic
relation. The system can also use the attitude infor-
mation to identify subgroups with a homogeneous
and common focus among the discussants. The sys-
tem predicts attitudes and identifies subgroups with
high accuracy.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
References
Amjad Abu-Jbara and Dragomir Radev. 2011. Clairlib:
A toolkit for natural language processing, information
retrieval, and network analysis. In ACL-HLT 2011-
Demo, June.
Patrick Doreian and Andrej Mrvar. 1996. A partitioning
approach to structural balance. Social Networks.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In ACL 2010, pages 138?147, Uppsala, Sweden, July.
Ahmed Hassan and Dragomir R. Radev. 2010. Identify-
ing text polarity using random walks. In ACL 2010.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude? identifying sentences
with attitude in online discussions.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09.
Andrew McCallum, Xuerui Wang, and Andre?s Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. J. Artif. Int. Res.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: a system for subjectivity anal-
ysis. In HLT/EMNLP - Demo.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05.
Bo Yang, William Cheung, and Jiming Liu. 2007. Com-
munity mining from signed social networks. IEEE
Trans. on Knowl. and Data Eng.
36
Proceedings of NAACL-HLT 2013, pages 596?606,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Purpose and Polarity of Citation: Towards NLP-based Bibliometrics
Amjad Abu-Jbara
Department of EECS
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Jefferson Ezra
Department of EECS
University of Michigan
Ann Arbor, MI, USA
jezra@umich.edu
Dragomir Radev
Department of EECS
and School of Information
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
Bibliometric measures are commonly used to
estimate the popularity and the impact of pub-
lished research. Existing bibliometric mea-
sures provide ?quantitative? indicators of how
good a published paper is. This does not nec-
essarily reflect the ?quality? of the work pre-
sented in the paper. For example, when h-
index is computed for a researcher, all incom-
ing citations are treated equally, ignoring the
fact that some of these citations might be neg-
ative. In this paper, we propose using NLP
to add a ?qualitative? aspect to biblometrics.
We analyze the text that accompanies citations
in scientific articles (which we term citation
context). We propose supervised methods for
identifying citation text and analyzing it to de-
termine the purpose (i.e. author intention) and
the polarity (i.e. author sentiment) of citation.
1 Introduction
An objective and fair evaluation of the impact
of published research requires both quantitative
and qualitative assessment. Existing bibliometric
measures such as H-Index (Hirsch, 2005; Hirsch,
2010), G-index (Egghe, 2006), and Impact Fac-
tor (Garfield, 1994) focus on the quantitative aspect
of this evaluation which dose not always correlate
with the qualitative aspect.
For example, the number of papers published by
a researcher only tells how productive she or he is.
It does not say anything about the quality or the im-
pact of the work. Similarly, the number of citations
that a paper receives should not be used to gauge
the quality of the work as it really only measures
the popularity of the work and the interest of other
researchers in it (Garfield, 1979). Controversial pa-
pers or those based on fabricated data or experiments
may receive a large number of citations. A popular
example of fraudulent research that deceived many
researchers and caught media attention was the case
of a South Korean research scientist, Hwang Woo-
suk, who was found to have faked his research re-
sults in the area of human stem cell cloning. His re-
search was published in Science and received close
to 200 citations after the fraud was discovered. The
vast majority of those citations were negative.
This suggests that the purpose of citation should
be taken into consideration when biblometric mea-
sures are computed. Negative citations should be
weighted less than positive or neutral citations. This
motivates the need to automatically distinguish be-
tween positive, negative, and neutral citations and to
identify the purpose of a citation; i.e. the author?s in-
tention behind choosing a published article and cit-
ing it.
This analysis of citation purpose and polarity can
be useful for many applications. For example, it can
be used to build systems that help funding agencies
and hiring committees at universities and research
institutions evaluate researchers? work more accu-
rately. It can also be used as a preprocessing step in
systems that process scholarly data. For example,
citation-based summarization systems (Qazvinian
and Radev, 2008; Qazvinian et al, 2010; Abu-
Jbara and Radev, 2011) and survey generation sys-
tems (Mohammad et al, 2009; Qazvinian et al,
2013) can benefit from citation purpose and polar-
ity analysis to improve paper and content selection.
In this paper, we investigate the use of linguis-
tic analysis techniques to automatically identify the
purpose of citing a paper and the polarity of this cita-
tion. We first present a sequence labeling method for
extracting the text that cites a given target reference;
i.e. the text that appears in a scientific article and
refers to another article and comments on it. We use
the term citation context to refer to this text. Next,
596
we use supervised classification techniques to ana-
lyze this text and identify the purpose and polarity
of citation.
The rest of this paper is organized as follows. Sec-
tion 2 reviews the related work. We present our ap-
proach in Section 3. We then describe the data and
experiments in Section 4. Finally, Section 5 con-
cludes the paper and suggests directions for future
work.
2 Related Work
Our work is related to a large body of research
on citations. Studying citation patterns and ref-
erencing practices has interested researchers for
many years (Hodges, 1972; Garfield et al, 1984).
White (2004) provides a good survey of the differ-
ent research directions that study or use citations. In
the following subsections, we review three lines of
research that are closely related to our work.
2.1 Citation Context Identification
The first line of related research addresses the prob-
lem of identifying citation context. The context of a
citation that cites a given target paper can be a set of
sentences, one sentence, or a fragment of a sentence.
Nanba and Okumura (1999) use the term citing
area to refer to the same concept. They define the
citing area as the succession of sentences that ap-
pear around the location of a given reference in a
scientific paper and have connection to it. Their al-
gorithm starts by adding the sentence that contains
the target reference as the first member sentence in
the citing area. Then, they use a set of cue words
and hand-crafted rules to determine whether the sur-
rounding sentences should be added to the citing
area or not. In (Nanba et al, 2000), they use their
algorithm to improve citation type classification and
automatic survey generation.
Qazvinian and Radev (2010) addressed a simi-
lar problem. They proposed a method based on
probabilistic inference to extract non-explicit cit-
ing sentences; i.e., sentences that appear around
the sentence that contains the target reference and
are related to it. They showed experimentally that
citation-based survey generation produces better re-
sults when using both explicit and non-explicit cit-
ing sentences rather than using the explicit ones
alone.
In previous work, we addressed the issue of iden-
tifying the scope of a given target reference in citing
sentences that contain multiple references (2012).
Our definition of reference scope was limited to
fragments of the explicit citing sentence (i.e. the
sentence in which actual citation appears). That
method does not identify related text in surrounding
sentences.
In this work, we propose a supervised sequence
labeling method for identifying the citation context
of given reference which includes the explicit citing
sentence and the related surrounding sentences.
2.2 Citation Purpose Classification
Several research efforts have focused on studying
the different purposes for citing a paper (Garfield,
1964; Weinstock, 1971; Moravcsik and Muruge-
san, 1975; and Moitra, 1975; Bonzi, 1982).
Bonzi (1982) studied the characteristics of citing
and cited works that may aid in determining the re-
latedness between them. Garfield (1964) enumer-
ated several reasons why authors cite other publi-
cations, including ?alerting researchers to forthcom-
ing work?, paying homage to the leading scholars
in the area, and citations which provide pointers to
background readings. Weinstock (1971) adopted the
same scheme that Garfield proposed in her study of
citations.
Spiegel-Rosing (1977) proposed 13 categories for
citation purpose based on her analysis of the first
four volumes of Science Studies. Some of them are:
Cited source is the specific point of departure for
the research question investigated, Cited source con-
tains the concepts, definitions, interpretations used,
Cited source contains the data used by the citing pa-
per. Nanba and Okumura (1999) came up with a
simple schema composed of only three categories:
Basis, Comparison, and other Other. They pro-
posed a rule-based method that uses a set of statis-
tically selected cue words to determine the category
of a citation. They used this classification as a first
step for scientific paper summarization. Teufel et
al. (2006), in their work on citation function classifi-
cation, adopted 12 categories from Spiegel-Rosing?s
taxonomy. They trained an SVM classifier and used
it to label each citing sentence with exactly one cat-
egory. Further, they mapped the twelve categories to
four top level categories namely: weakness, contrast
597
(4 categories), positive (6 categories) and neutral.
The taxonomy that we use in this work is based
on previous work. We adopt a scheme that contains
six categories. We selected the six categories after
studying all the previously used citation taxonomies.
We included the ones we believed are important for
improving bibliometric measures and for the appli-
cations that we are planning to pursue in the future
(Section 5).
2.3 Citation Polarity Classification
The polarity (or sentiment) of a citation has also
been studied previously. Previous work showed
that positive and negative citations are common, al-
though negative citations might be expressed indi-
rectly or in an implicit way (Ziman, 1968; Mac-
Roberts and MacRoberts, 1984; THOMPSON and
YIYUN, 1991). Athar (2011) addressed the prob-
lem of identifying sentiment in citing sentences. He
used a set of structure-based features to train a ma-
chine learning classifier using annotated data. This
work uses the citing sentence only to predict senti-
ment. Context sentences were ignored. Athar and
Teufel (2012a) observed that taking the context into
consideration when judging sentiment in citations
increases the number of negative citations by a fac-
tor of 3. They proposed two methods for utilizing
the context. In the first method, they treat the citing
sentence and a fixed context (a window of four sen-
tences around the citing sentence) as if they were
a single sentence. They extract features from the
merged text and train a classifier similar to what they
did in their 2011 paper. In the second method, they
use a four-class annotation scheme. Each sentence
in a window of four sentences around the citation
is labeled as positive, negative, neutral, or excluded
(unrelated to the cited work). There experiments
surprisingly gave negative results and showed that
classifying sentiment without considering the con-
text achieves better results. They attributed this to
the small size of their training data and to the noise
that including the context text introduces to the data.
In (Athar and Teufel, 2012b), the authors present a
method for automatically identifying all the men-
tions of the cited paper in the citing paper. They
show that considering all the mentions improves the
performance of detecting sentiment in citations.
In our work, we propose a sequence labeling
method for identifying the citation context first, and
then use a supervised approach to determine the po-
larity of a given citation.
3 Approach
In this section, we describe our approach to three
tasks: citation context identification, citation pur-
pose classification, and citation polarity identifica-
tion. We also describe a preprocessing stage that is
applied to the citation text before performing any of
the three tasks.
3.1 Preprocessing
The goal of the preprocessing stage is to clean and
prepare the citation text for part-of-speech tagging
and parsing. The available POS taggers and parsers
are not trained on citation text. Citation text is dif-
ferent from normal text in that it contains references
written in a special format (e.g., author names and
publication year written in parentheses; or reference
indices written in square brackets). Many citing sen-
tences contain multiple references, some of which
might be grouped together in a pair of parentheses
and separated by a comma or a semi-colons. These
references are usually not syntactic nor semantic
constituents of the sentences they appear in. This
results in many POS tagging and parsing errors. We
address this issue in the pre-processing stage to im-
prove the performance of the feature extraction com-
ponent. We perform three pre-processing steps:
a. Reference Tagging: In the first step, we find
and tag all the references that appear in the text. We
use a regular expression to find references and re-
place each reference with a placeholder. The ref-
erence to the target paper is replaced by the place-
holder TREF. Each other reference is replaced by
REF.
b. Reference Grouping: In this step, we identify
grouped references (i.e. multiple references listed
between one pair of parentheses separated by semi-
colons). Each such group is replaced by a place-
holder, GREF. If the target reference is a member of
the group, we use a different placeholder: GTREF.
c. Non-syntactic Reference Removal: A refer-
ence or a group of references could either be a syn-
tactic constituent and has a semantic role in the sen-
tence or not (Whidby, 2012; Abu Jbara and Radev,
2012). If the reference is not a syntactic compo-
598
Feature Description
Demonstrative determiners Takes a value of 1 if the current sentence contains contains a demonstrative determiner (this, these,
etc.), and 0 otherwise.
Conjunctive adverbs Takes a value of 1 if the current sentence starts with a conjunctive adverb (However, Furthermore,
Accordingly, etc.), and 0 otherwise.
Position Position of the current sentence with respect to the citing sentence. This feature takes one of four
values: -1, 0, 1, and 2.
Contains Closest Noun Phrase Takes a value of 1 if the current sentence contains closest noun phrase (if any) immediately before
the reference position in the citing sentence, and 0 otherwise. This noun phrase often is the name of
a method, a tool, or corpus originating from the cited reference.
2-3 grams The first bigram and trigram in the sentence (This approach, One problem with, etc.).
Contains Other references Takes a value of 1 if the current sentence contains references other than the target, and 0 otherwise.
Contains a Mention of target reference Takes a value of 1 if the current sentence contains a mention (explicit or anaphoric) of the target
reference, and 0 otherwise.
Multiple references Takes a value of 1 if the citing sentence contains multiple references, and 0 otherwise. If the cit-
ing sentence contains multiple references, it becomes less likely that the surrounding sentences are
related.
Table 1: Features used for citation context identification
nent in the sentence, we remove it to reduce pars-
ing errors. Following our previous work (Abu Jbara
and Radev, 2012), we use a rule-based algorithm to
determine whether a reference should be removed
from the sentence or kept. The algorithm uses stylis-
tic and linguistic features such as the style of the
reference, the position of the reference, and the sur-
rounding words to make the decision. When a ref-
erence is removed, the head of the closest noun
phrase (NP) immediately before the position of the
removed reference is used as a representative of the
reference. This is needed for feature extraction as
shown later in the paper.
3.2 Citation Context Identification
The task of identifying the citation context of a given
target reference can be formally defined as follows.
Given a scientific article A that cites another article
B, find a set of sentences in A that talk about the
work done in B such that at least one of these sen-
tences contains an explicit reference to B.
We treat this problem as a sequence labeling prob-
lem. The goal is to find the globally best sequence
of labels for all the sentences that appear within a
window around the citing sentence. The citing sen-
tence is the one that contains an explicit reference
to the cited paper. Each sentence within the window
is labeled as INCLUDED or EXCLUDED from the
citation context of the given target paper. To deter-
mine the size of the window, we examined a devel-
opment set of 300 sentences. We noticed that the re-
lated context almost always falls within a window of
four sentences. The window includes the citing sen-
tence, one sentence before the citing sentence, and
two sentences after the citing sentence.
We use Conditional Random Fields (CRFs) for
sequence labeling. In particular, we use a first-order
chain-structured CRF. The chain consists of two sets
of nodes: 1) a set of hidden nodes Y which represent
the context labels of sentences (INCLUDED or EX-
CLUDED), and 2) a set of observed nodes X which
represent the features extracted from the sentences.
The task is to estimate the probability of a sequence
of labels Y given the sequence of observed features
X: P (Y|X)
Lafferty et al (2001) define this probability to be
a normalized product of potential functions ?:
P (y|x) =
?
t
?k(yt, yt?1, x) (1)
Where ?k(yt, yt?1, x) is defined as
?k(yt, yt?1, x) = exp(
?
k
?kf(yt, yt?1, x)) (2)
where f(yt, yt?1, x) is a transition feature func-
tion of the label at positions i ? 1 and i and the ob-
servation sequence x; and ?j is a parameter that the
algorithm estimates from training data.
The features we use to train the CRF model in-
clude structural and lexical features that attempt to
capture indicators of relatedness to the given target
reference. The features that we used and their de-
scriptions are listed in table 1.
599
Category Description Example
Criticizing Criticism can be positive or negative. A citing sentence is classi-
fied as ?criticizing? when it mentions the weakness/strengths of
the cited approach, negatively/positively criticizes the cited ap-
proach, negatively/positively evaluates the cited source.
Chiang (2005) introduced a constituent feature to reward
phrases that match a syntactic tree but did not yield signif-
icant improvement.
Comparison A citing sentence is classified as ?comparison? when it compares
or contrasts the work in the cited paper to the author?s work. It
overlaps with the first category when the citing sentence says one
approach is not as good as the other approach. In this case we use
the first category.
Our approach permits an alternative to minimum error-rate
training (MERT; Och, 2003);
Use A citing sentence is classified as ?use? when the citing paper uses
the method, idea or tool of the cited paper.
We perform the MERT training (Och, 2003) to tune the
optimal feature weights on the development set.
Substantiating A citing sentence is classified as ?substantiating? when the re-
sults, claims of the citing work substantiate, verify the cited paper
and support each other.
It was found to produce automated scores, which strongly
correlate with human judgements about translation flu-
ency (Papineni et al , 2002).
Basis A citing sentence is classified as ?basis? when the author uses the
cited work as starting point or motivation and extends on the cited
work.
Our model is derived from the hidden-markov model for
word alignment (Vogel et al, 1996; Och and Ney, 2000).
Neutral (Other) A citing sentence is classified as ?neutral? when it is a neutral
description of the cited work or if it doesn?t come under any of
the above categories.
The solutions of these problems depend heavily on the
quality of the word alignment (Och and Ney, 2000).
Table 2: Annotation scheme for citation purpose. Motivated by the work of (Spiegel-Ro?sing, 1977) and (Teufel et al,
2006)
3.3 Citation Purpose Classification
In this section, we describe the citation purpose clas-
sification task. Given a target paper B and its cita-
tion context (extracted using the method described
above) in a given article A, we want to determine
the purpose of citing B by A. The purpose is de-
fined as intention behind selecting B and citing it by
the author of A (Garfield, 1964).
We use a taxonomy that consists of six categories.
We designed this taxonomy based on our study of
similar taxonomies proposed in previous work. We
selected the categories that we believe are more im-
portant and useful from a bibliometric point of view,
and the ones that can be detected through citation
text analysis. We also tried to limit the number of
categories by grouping similar categories proposed
in previous work under one category. The six cate-
gories, their descriptions, and an example for each
category are listed in Table 2.
We use a supervised approach whereby a classifi-
cation model is trained on a number of lexical and
structural features extracted from a set of labeled ci-
tation contexts. Some of the features that we use to
train the classifier are listed in table 3.
3.4 Citation Polarity Identification
In this section, we describe the citation polarity iden-
tification task. Given a target paper B and its citation
context in a given article A, we want to determine
the polarity of the citation text with respect to B.
The polarity can be: positive, negative, or neutral
(objective). Positive, negative, and neutral in this
context are defined in a slightly different way than
their usual sense. A citation is marked positive if it
either explicitly states a strength of the target paper
or indicates that the work done in the target paper
has been used either by the author or a third-party. It
is also marked as positive if it is compared to another
paper (possibly by the same authors) and deemed
better in some way. A citation is marked negative
if it explicitly points to a weakness of the target pa-
per. It is also marked as negative if it is compared
to another paper and deemed worse in some way. A
citation is marked as neutral if it is only descriptive.
Similar to citation purpose classification, we use
a supervised approach for this problem. We train a
classification model using the same features listed in
Table 3. Due to the high skewness in the data (more
than half of the citations are neutral), we use two
setups for binary classification. In the first setup,
the citation is classified as Polarized (Subjective) or
(Neutral) Objective. In the second one, Subjective
citations are classified as Positive or Negative. We
find that this method gives more intuitive results than
using a 3-way classifier.
600
Feature Description
Reference count The number of references that appear in the citation context.
Is Separate Whether the target reference appears within a group of references or separate (i.e. single reference).
Closest Verb / Adjective / Adverb The lemmatized form of the closest verb/adjective/adverb to the target reference or its representative or any mention
of it. Distance is measure based on the shortest path in the dependency tree.
Self Citation Whether the citation from the source paper to the target reference is a self citation.
Contains 1st/3rd PP Whether the citation context contains a first/third person pronoun.
Negation Whether the citation context contains a negation cue. The list of negation cues is taken from the training data of
the *SEM 2012 negation detection shared task (Morante and Blanco, 2012).
Speculation Whether the citation context contains a speculation cue. The list is taken from Quirk et al (1985)
Closest Subjectivity Cue The closest subjectivity cue to the target reference or its representative or any anaphoric mention of it. The list of
cues is taken from OpinionFinder (Wilson et al, 2005)
Contrary Expressions Whether the citation context contains a contrary expression. The list is taken from Biber (1988)
Section The headline of the section in which the citation appears. We identify five title categorizes: 1) Introduction,
Motivation, etc. 2) Background, Prior Work, Previous Work, etc. 3) Experiments, Data, Results, Evaluation, etc.
4) Discussion, Conclusion, Future work, etc.. 5) All other section headlines. Headlines are identified using regular
expressions.
Dependency Relations All the dependency relations that appear in the citation context. For example, nsubj(outperform, algorithm)
is one of the relations extracted from ?This algorithm outperforms the one proposed by...?. The arguments of the
dependency relation are replaced by their lemmatized forms. This type of features has been shown to give good
results in similar tasks (Athar and Teufel, 2012a).
Table 3: The features used for citation purpose and polarity classification
4 Evaluation
In this section, we describe the data that we used for
evaluation and the experiments that we conducted.
4.1 Data
We use the ACL Anthology Network corpus
(AAN) (Radev et al, 2009; Radev et al, 2013) in
our evaluation. AAN is a publicly available collec-
tion of more than 19,000 NLP papers. It includes
a manually curated citation network of its papers
as well as the full text of the papers and the cit-
ing sentences associated with each edge in the ci-
tation network. From this set, we selected 30 pa-
pers that have different numbers of incoming cita-
tions and that were consistently cited since they were
published. These 30 papers received a total of about
3,500 citations from within AAN (average = 115 ci-
tation/paper, Min = 30, and Max = 338). These ci-
tations come from 1,493 unique papers. For each
of these citations, we extracted a window of 4 sen-
tences around the reference position. This brings
the number of sentences in our dataset to a total of
roughly 14,000 sentences. We refer to this dataset as
training/testing dataset.
In addition to this dataset, we created another
dataset that contains 300 citations that cite 5 papers
from AAN. We refer to this dataset as the develop-
ment dataset. This dataset was used to determine the
size of the citation context window, and to develop
the feature sets used in the three tasks described in
Section 3 above.
4.2 Annotation
In this section, we describe the annotation process.
We asked graduate students with good background
in NLP (the topic of the annotated sentences) to pro-
vide three annotations for each citation example (a
window of 4 sentences around the reference anchor)
in the training/testing dataset. We asked them to
mark the sentences that are related to a given tar-
get reference. In addition, we asked them to deter-
mine the purpose of citing the target reference by
choosing from the six purpose categories that we
described earlier. We also asked them to determine
whether the citation is negative, positive, or neutral.
To estimate the inter-annotator agreement, we
picked 400 sentences from the training/testing
dataset and assigned them to two different annota-
tors. We use the Kappa coefficient (Cohen, 1968)
to measure the agreement. The Kappa coefficient is
defined as follows:
K =
P (A)? P (E)
1? P (E)
(3)
where P(A) is the relative observed agreement
among annotators and P(E) is the hypothetical prob-
601
ability of chance agreement. The agreement be-
tween the two annotators on the context identifica-
tion task wasK = 0.89. On Landis and Kochs (Lan-
dis and Koch, 1977) scale, this value indicates al-
most perfect agreement. The agreement on the pur-
pose and the polarity classification task were K =
0.61 and K = 0.66, respectively; which indicates
substantial agreement on the same scale.
The annotation shows that in 22% of the citation
examples, the citation context consists of 2 or more
sentences. The distribution of the purpose categories
in the data was: 14.7% criticism, 8.5% comparison,
17.7% use, 7% substantiation, 5% basis, and 47%
other. The distribution of the polarity categories
was: 30% positive, 12% negative, and 58% neutral.
4.3 Experimental Setup
We use the CRF++1 toolkit for CRF training and
testing. We use the Stanford parser to parse the ci-
tation text and generate the dependency parse trees
of sentences. We use Weka for classification experi-
ments. We experimented with several classifiers in-
cluding: SVM, Logistic Regression (LR), and Naive
Bayes. All the experiments that we conducted used
the training/testing dataset in a 10-fold cross vali-
dation mode. All the results have been tested for
statistical significance using a 2-tailed paired t-test.
4.4 Evaluation of Citation Context
Identification
We compare the CRF approach to three baselines.
The first baseline (ALL) labels all the sentences in
the citation window of size 4 as INCLUDED in the
citation context. The second baseline (CS-ONLY)
labels the citing sentence only as INCLUDED in the
citation context. In the third baseline, we use a su-
pervised classification method instead of sequence
labeling. We use Support Vector Machines (SVM)
to train a model using the same set of features as in
the CRF approach.
Table 4 shows the precision, recall, and F1 score
of the CRF approach and the baselines. The re-
sults show that our CRF approach outperforms all
the baselines. It also asserts our expectation that ad-
dressing this problem as a sequence labeling prob-
lem leads to better performance than individual sen-
1http://crfpp.googlecode.com/svn/trunk/doc/index.html
Precision Recall F1
CRFs 98.5% 82.0% 89.5%
ALL 30.7% 100.0% 46.9%
CS-ONLY 88.0% 74.0% 80.4%
SVM 92.0% 76.4% 83.5%
Table 4: Results of citation context identification
tence classification, which is also clear from the na-
ture of the task.
Feature Analysis: We evaluated the importance
of the features listed in Table 1 by computing the
chi-squared statistic for every feature with respect to
the class. We found that the lexical features (such as
determiners and conjunction adverbs) are generally
more important than the structural features (such as
position and reference count). The features shown
in Table 1 are listed in the order of their importance
based on this analysis.
4.5 Evaluation of Citation Purpose
Classification
Our experiments with several classification algo-
rithms showed that the SVM classifier outperforms
Logistic Regression and Naive Bayes classifiers.
Due to space limitations, we only show the results
for SVM. Table 5 shows the precision, recall, and
F1 for each of the six categories. It also shows the
overall accuracy and the Macro-F measure.
Feature Analysis: The chi-squared evaluation of
the features listed in Table 3 shows that both lexical
and structural features are important. It also shows
that among lexical features, the ones that are limited
to the existence of a direct relation to the target ref-
erence (such as closest verb, adjective, adverb, sub-
jective cue, etc.) are most useful. This can be ex-
plained by the fact that the restricting the features to
having direct dependency relation introduces much
less noise than other features (such as Dependency
Triplets). Among the structural features, the num-
ber of references in the citation context showed to
be more useful.
4.6 Evaluation of Citation Polarity
Identification
Similar to the case of citation purpose classification,
our experiments showed that the SVM classifier out-
performs the other classifiers that we experimented
with. Table 6 shows the precision, recall, and F1 for
602
Criticism Comparison Use Substantiating Basis Other
Precision 53.0% 55.2% 60.0% 50.1% 47.3% 64.0%
Recall 77.4% 43.1% 73.0% 57.3% 39.1% 85.1%
F1 63.0% 48.4% 66.0% 53.5% 42.1% 73.1%
Accuracy: 70.5%
Macro-F: 58.0%
Table 5: Summary of Citation Purpose Classification Results (10-fold cross validation, SVM: Linear Kernel, c = 1.0)
each of the three categories. It also shows the over-
all accuracy and the Macro-F measure. The analysis
of the features used to train this classifier using chi-
squared analysis leads to the same conclusions about
the relative importance of the features as described
in the previous subsection. However, we noticed that
features that are related to subjectivity (Subjectiv-
ity Cues, Negation, Speculation) are ranked higher
which makes sense in the case of polarity classifica-
tion.
4.7 Impact of Context on Classification
Accuracy
To study the impact of using citation context in ad-
dition to the citing sentence on classification per-
formance, we ran two polarity classification exper-
iments. In the first experiment, we used the citing
sentence only to extract the features that are used
to train the classifiers. In the second experiment,
we used the gold context sentences (the ones la-
beled INCLUDED by human annotators). Table 6
shows the results of the first experiment between
rounded parentheses and the results of the second
experiments in square brackets. The results show
that adding citation context improves the classifica-
tion accuracy especially in the subjective categories,
specially in the negative category if we want to be
more specific. This supports our intuition about po-
larized citations that authors start their review of the
cited work with an objective (neutral) sentence and
then follow it with their criticism if they have any.
We also reached to similar conclusions with purpose
classification, but we are not showing the numbers
due to space limitations.
4.8 Other Experiments
4.8.1 Can We Do Better?
In this section, we investigate whether it is possi-
ble to improve the performance in the two classifica-
tion tasks. One factor that we believe could have an
Negative % Positive % Neutral %
Precision 68.7 (66.4) [69.8] 54.9 (52.1) [55.4] 83.6 (82.8) [84.2]
Recall 79.2 (71.1) [81.1] 48.1 (45.6) [46.3] 95.5 (95.1) [95.3]
F1 73.6 (68.7) [75.0] 51.3 (48.6) [50.4] 89.1 (88.5) [89.4]
Accuracy: 81.4 (74.2) [84.2] %
Macro-F: 71.3 (62.1) [74.2] %
Table 6: Summary of Citation Polarity Classification Re-
sults (10-fold cross validation, SVM: Linear Kernel, c =
1.0). Numbers between rounded parentheses are when
only the explicit citing sentence is used (i.e. no context).
Numbers in square brackets are when the gold standard
context is used.
impact on the result is the size of the training data.
To examine this hypothesis, we ran the experiment
on different sizes of data. Figure 1 shows the learn-
ing curve of the two classifiers for different sizes of
training data. The accuracy increases as more train-
ing data is available so we can expect that with even
more data, we can do even better.
4.8.2 Relation Between Citation
Purpose/Polarity and Citation Count
The main motivation of this work is our hypothet-
ical assumption that using NLP for analyzing cita-
tions gives a clearer picture of the impact of the cited
work. As a way to check the validity of this assump-
tion, we study the correlation between the counts of
the different purpose and polarity categories. We
also study the correlation between these categories
and the total number of citations that a paper re-
ceived since it was published. We use the train-
ing/testing dataset and the gold annotations for this
study.
We compute the Pearson correlation coefficient
between the counts of citations from the different
categories that a paper received per year since its
publication. We found that, on average, the correla-
tion between positive and negative citations is neg-
ative (AVG P = -0.194) and that the correlation be-
603
25
35
45
55
65
75
85
0 500 1000 1500 2000 2500 3000
Accu
racy
 
Dataset Size 
Purpose Accuracy
Polarity Accuracy
Figure 1: The effect of size of the data set size on the
classifiers accuracy.
tween the count of positive citations and the total
number of citations is higher than the correlation be-
tween negative citations and total citations (AVG P =
0.531 for positive vs. AVG P = 0.054 for negative).
Similarly, we noticed that there is a higher posi-
tive correlation between Use citations and total ci-
tations than in the case of both Substantiation and
Basis. This can be explained by the intuition that
publications that present new algorithms, tools, or
corpora that are used by the research community be-
come more and more popular with time and thus re-
ceive more and more citations.
Figure 2 shows the result of running our pur-
pose classifier on all the citations to Papineni et
al.?s (2002) paper about Bleu, an automatic metric
for evaluating Machine Translation (MT) systems.
The figure shows that this paper receives a high
number of Use citations. This makes sense for a pa-
per that describes an evaluation metric that has been
widely used in the MT area. The figure also shows
that in the recent years, this metric started to receive
some Criticizing citations that resulted in a slight de-
crease in the number of Use citations. Such a tempo-
ral analysis of citation purpose and polarity is useful
for studying the dynamics of research. It can also
be used to detect the emergence or de-emergence of
research techniques.
0
10
20
30
40
50
60
70
80
2001 2003 2005 2007 2009 2011
CriticizingComparisonUseSubstantiatingBasisOther
Figure 2: Change in the purpose of the citations to Pap-
ineni et al (2002)
5 Conclusion
In this paper, we presented methods for three tasks:
citation context identification, citation purpose clas-
sification, and citation polarity classification. This
work is motivated by the need for more accurate
bibliometric measures that evaluates the impact of
research both qualitatively and quantitatively. Our
experiments showed that we can classify the pur-
pose and polarity of citation with a good accuracy. It
also showed that using the citation context improves
the classification accuracy and increases the num-
ber of polarized citations detected. For future work,
we plan to use the output of this research in several
applications such as predicting future prominence of
publications, studying the dynamics of research, and
designing more accurate bibliometric measures.
Acknowledgement
This research is supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Center
(DoI/NBC) contract number D11PC20153. The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. Dis-
claimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Government.
604
References
Daryl E. and Soumyo D. Moitra. 1975. Content analysis
of references: Adjunct or alternative to citation count-
ing? Social Studies of Science, 5(4):pp. 423?441.
Amjad Abu-Jbara and Dragomir Radev. 2011. Coherent
citation-based summarization of scientific papers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 500?509, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Amjad Abu Jbara and Dragomir Radev. 2012. Refer-
ence scope identification in citing sentences. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 80?
90, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Awais Athar and Simone Teufel. 2012a. Context-
enhanced citation sentiment detection. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 597?
601, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Awais Athar and Simone Teufel. 2012b. Detection of
implicit citations for sentiment detection. In Proceed-
ings of the Workshop on Detecting Structure in Schol-
arly Discourse, pages 18?26, Jeju Island, Korea, July.
Association for Computational Linguistics.
Awais Athar. 2011. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceedings
of the ACL 2011 Student Session, pages 81?87, Port-
land, OR, USA, June. Association for Computational
Linguistics.
Douglas Biber. 1988. Variation across speech and writ-
ing. Cambridge University Press, Cambridge.
Susan Bonzi. 1982. Characteristics of a literature as pre-
dictors of relatedness between cited and citing works.
Journal of the American Society for Information Sci-
ence, 33(4):208?216.
J. Cohen. 1968. Weighted kappa: Nominal scale agree-
ment with provision for scaled disagreement or partial
credit. Psychological Bulletin, 70:213?220.
Leo Egghe. 2006. Theory and practise of the g-index.
Scientometrics, 69:131?152.
E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The
Use of Citation Data in Writing the History of Science.
Institute for Scientific Information Inc., Philadelphia,
Pennsylvania, USA.
Eugene Garfield. 1964. Can citation indexing be auto-
mated?
E. Garfield. 1979. Is citation analysis a legitimate evalu-
ation tool? Scientometrics, 1(4):359?375.
Eugene Garfield. 1994. The thomson reuters impact fac-
tor.
J. E. Hirsch. 2005. An index to quantify an individual?s
scientific research output. Proceedings of the National
Academy of Sciences, 102(46):16569?16572, Novem-
ber.
J. E. Hirsch. 2010. An index to quantify an individ-
ual?s scientific research output that takes into account
the effect of multiple coauthorship. Scientometrics,
85(3):741?754, December.
T. L. Hodges. 1972. Citation indexing-its theory
and application in science, technology, and humani-
ties. Ph.D. thesis, University of California at Berke-
ley.Ph.D. thesis, University of California at Berkeley.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
J. Richard Landis and Gary G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159?174, March.
Michael H. MacRoberts and Barbara R. MacRoberts.
1984. The negational reference: Or the art of dissem-
bling. Social Studies of Science, 14(1):pp. 91?94.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 584?592, Boulder, Colorado, June. Association
for Computational Linguistics.
Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics - Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation, SemEval
?12, pages 265?274, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
M. J. Moravcsik and P. Murugesan. 1975. Some results
on the function and quality of citations. Social Studies
of Science, 5:86?92.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI ?99: Proceedings of the Six-
605
teenth International Joint Conference on Artificial In-
telligence, pages 926?931, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and
Of Information Science. 2000. Classification of re-
search papers using citation links and citation types:
Towards automatic review article generation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 689?696, Manchester, UK, August. Coling 2008
Organizing Committee.
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555?564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Ozgur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 895?903, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Vahed Qazvinian, Dragomir R. Radev, Saif Mohammad,
Bonnie Dorr, David Zajic, Michael Whidby, and Tae-
sun Moon. 2013. Generating extractive summaries of
scientific paradigms. Journal of Artificial Intelligence
Research.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammar
of the English Language. Longman, London.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network corpus.
In NLPIR4DL ?09: Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital Li-
braries, pages 54?61, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Dragomir R. Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The acl
anthology network corpus. Language Resources and
Evaluation, pages 1?26.
Ina Spiegel-Ro?sing. 1977. Science Studies: Bibliomet-
ric and Content Analysis. Social Studies of Science,
7(1):97?113, February.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
In Proc. of EMNLP-06.
GEOFF THOMPSON and YE YIYUN. 1991. Evalu-
ation in the reporting verbs used in academic papers.
Applied Linguistics, 12(4):365?382.
Melvin Weinstock. 1971. Citation Indexes. Encyclope-
dia of Library and Information Science.
Michael Alan Whidby. 2012. Citation handling: Pro-
cessing citation text in scientific documents. In Master
Thesis.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89?116.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
Proceedings of HLT/EMNLP on Interactive Demon-
strations, HLT-Demo ?05, pages 34?35, Stroudsburg,
PA, USA. Association for Computational Linguistics.
J. M. Ziman. 1968. Public knowledge: An essay con-
cerning the social dimension of science. Cambridge
U.P., London.
606
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 500?509,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Coherent Citation-Based Summarization of Scientific Papers
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department and
School of Information
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In citation-based summarization, text written
by several researchers is leveraged to identify
the important aspects of a target paper. Previ-
ous work on this problem focused almost ex-
clusively on its extraction aspect (i.e. selecting
a representative set of citation sentences that
highlight the contribution of the target paper).
Meanwhile, the fluency of the produced sum-
maries has been mostly ignored. For exam-
ple, diversity, readability, cohesion, and order-
ing of the sentences included in the summary
have not been thoroughly considered. This re-
sulted in noisy and confusing summaries. In
this work, we present an approach for produc-
ing readable and cohesive citation-based sum-
maries. Our experiments show that the pro-
posed approach outperforms several baselines
in terms of both extraction quality and fluency.
1 Introduction
Scientific research is a cumulative activity. The
work of downstream researchers depends on access
to upstream discoveries. The footnotes, end notes,
or reference lists within research articles make this
accumulation possible. When a reference appears in
a scientific paper, it is often accompanied by a span
of text describing the work being cited.
We name the sentence that contains an explicit
reference to another paper citation sentence. Cita-
tion sentences usually highlight the most important
aspects of the cited paper such as the research prob-
lem it addresses, the method it proposes, the good
results it reports, and even its drawbacks and limita-
tions.
By aggregating all the citation sentences that cite
a paper, we have a rich source of information about
it. This information is valuable because human ex-
perts have put their efforts to read the paper and sum-
marize its important contributions.
One way to make use of these sentences is creat-
ing a summary of the target paper. This summary
is different from the abstract or a summary gener-
ated from the paper itself. While the abstract rep-
resents the author?s point of view, the citation sum-
mary is the summation of multiple scholars? view-
points. The task of summarizing a scientific paper
using its set of citation sentences is called citation-
based summarization.
There has been previous work done on citation-
based summarization (Nanba et al, 2000; Elkiss et
al., 2008; Qazvinian and Radev, 2008; Mei and Zhai,
2008; Mohammad et al, 2009). Previous work fo-
cused on the extraction aspect; i.e. analyzing the
collection of citation sentences and selecting a rep-
resentative subset that covers the main aspects of the
paper. The cohesion and the readability of the pro-
duced summaries have been mostly ignored. This
resulted in noisy and confusing summaries.
In this work, we focus on the coherence and read-
ability aspects of the problem. Our approach pro-
duces citation-based summaries in three stages: pre-
processing, extraction, and postprocessing. Our ex-
periments show that our approach produces better
summaries than several baseline summarization sys-
tems.
The rest of this paper is organized as follows. Af-
ter we examine previous work in Section 2, we out-
line the motivation of our approach in Section 3.
Section 4 describes the three stages of our summa-
rization system. The evaluation and the results are
presented in Section 5. Section 6 concludes the pa-
per.
500
2 Related Work
The idea of analyzing and utilizing citation informa-
tion is far from new. The motivation for using in-
formation latent in citations has been explored tens
of years back (Garfield et al, 1984; Hodges, 1972).
Since then, there has been a large body of research
done on citations.
Nanba and Okumura (2000) analyzed citation
sentences and automatically categorized citations
into three groups using 160 pre-defined phrase-
based rules. They also used citation categoriza-
tion to support a system for writing surveys (Nanba
and Okumura, 1999). Newman (2001) analyzed
the structure of the citation networks. Teufel et
al. (2006) addressed the problem of classifying ci-
tations based on their function.
Siddharthan and Teufel (2007) proposed a method
for determining the scientific attribution of an arti-
cle by analyzing citation sentences. Teufel (2007)
described a rhetorical classification task, in which
sentences are labeled as one of Own, Other, Back-
ground, Textual, Aim, Basis, or Contrast according
to their role in the authors argument. In parts of our
approach, we were inspired by this work.
Elkiss et al (2008) performed a study on citation
summaries and their importance. They concluded
that citation summaries are more focused and con-
tain more information than abstracts. Mohammad
et al (2009) suggested using citation information to
generate surveys of scientific paradigms.
Qazvinian and Radev (2008) proposed a method
for summarizing scientific articles by building a sim-
ilarity network of the citation sentences that cite
the target paper, and then applying network analy-
sis techniques to find a set of sentences that covers
as much of the summarized paper facts as possible.
We use this method as one of the baselines when we
evaluate our approach. Qazvinian et al (2010) pro-
posed a citation-based summarization method that
first extracts a number of important keyphrases from
the set of citation sentences, and then finds the best
subset of sentences that covers as many keyphrases
as possible. Qazvinian and Radev (2010) addressed
the problem of identifying the non-explicit citing
sentences to aid citation-based summarization.
3 Motivation
The coherence and readability of citation-based
summaries are impeded by several factors. First,
many citation sentences cite multiple papers besides
the target. For example, the following is a citation
sentence that appeared in the NLP literature and
talked about Resnik?s (1999) work.
(1) Grefenstette and Nioche (2000) and Jones
and Ghani (2000) use the web to generate cor-
pora for languages where electronic resources are
scarce, while Resnik (1999) describes a method
for mining the web for bilingual texts.
The first fragment of this sentence describes dif-
ferent work other than Resnik?s. The contribution
of Resnik is mentioned in the underlined fragment.
Including the irrelevant fragments in the summary
causes several problems. First, the aim of the sum-
marization task is to summarize the contribution of
the target paper using minimal text. These frag-
ments take space in the summary while being irrel-
evant and less important. Second, including these
fragments in the summary breaks the context and,
hence, degrades the readability and confuses the
reader. Third, the existence of irrelevant fragments
in a sentence makes the ranking algorithm assign a
low weight to it although the relevant fragment may
cover an aspect of the paper that no other sentence
covers.
A second factor has to do with the ordering of the
sentences included in the summary. For example,
the following are two other citation sentences for
Resnik (1999).
(2) Mining the Web for bilingual text (Resnik, 1999) is
not likely to provide sufficient quantities of high quality
data.
(3) Resnik (1999) addressed the issue of language
identification for finding Web pages in the languages of
interest.
If these two sentences are to be included in the
summary, the reasonable ordering would be to put
the second sentence first.
Thirdly, in some instances of citation sentences,
the reference is not a syntactic constituent in the sen-
501
tence. It is added just to indicate the existence of
citation. For example, in sentence (2) above, the ref-
erence could be safely removed from the sentence
without hurting its grammaticality.
In other instances (e.g. sentence (3) above), the
reference is a syntactic constituent of the sentence
and removing it makes the sentence ungrammatical.
However, in certain cases, the reference could be re-
placed with a suitable pronoun (i.e. he, she or they).
This helps avoid the redundancy that results from re-
peating the author name(s) in every sentence.
Finally, a significant number of citation sentences
are not suitable for summarization (Teufel et al,
2006) and should be filtered out. The following
sentences are two examples.
(4) The two algorithms we employed in our depen-
dency parsing model are the Eisner parsing (Eisner,
1996) and Chu-Lius algorithm (Chu and Liu, 1965).
(5) This type of model has been used by, among others,
Eisner (1996).
Sentence (4) appeared in a paper by Nguyen et al
(2007). It does not describe any aspect of Eisner?s
work, rather it informs the reader that Nguyen et al
used Eisner?s algorithm in their model. There is no
value in adding this sentence to the summary of Eis-
ner?s paper. Teufel (2007) reported that a significant
number of citation sentences (67% of the sentences
in her dataset) were of this type.
Likewise, the comprehension of sentence (5) de-
pends on knowing its context (i.e. its surrounding
sentences). This sentence alone does not provide
any valuable information about Eisner?s paper and
should not be added to the summary unless its con-
text is extracted and included in the summary as
well.
In our approach, we address these issues to
achieve the goal of improving the coherence and the
readability of citation-based summaries.
4 Approach
In this section we describe a system that takes a sci-
entific paper and a set of citation sentences that cite
it as input, and outputs a citation summary of the
paper. Our system produces the summaries in three
stages. In the first stage, the citation sentences are
preprocessed to rule out the unsuitable sentences and
the irrelevant fragments of sentences. In the sec-
ond stage, a number of citation sentences that cover
the various aspects of the paper are selected. In the
last stage, the selected sentences are post-processed
to enhance the readability of the summary. We de-
scribe the stages in the following three subsections.
4.1 Preprocessing
The aim of this stage is to determine which pieces of
text (sentences or fragments of sentences) should be
considered for selection in the next stage and which
ones should be excluded. This stage involves three
tasks: reference tagging, reference scope identifica-
tion, and sentence filtering.
4.1.1 Reference Tagging
A citation sentence contains one or more references.
At least one of these references corresponds to the
target paper. When writing scientific articles, au-
thors usually use standard patterns to include point-
ers to their references within the text. We use pattern
matching to tag such references. The reference to
the target is given a different tag than the references
to other papers.
The following example shows a citation sentence
with all the references tagged and the target refer-
ence given a different tag.
In <TREF>Resnik (1999)</TREF>, <REF>Nie,
Simard, and Foster (2001)</REF>, <REF>Ma and
Liberman (1999)</REF>, and <REF>Resnik and
Smith (2002)</REF>, the Web is harvested in search of
pages that are available in two languages.
4.1.2 Identifying the Reference Scope
In the previous section, we showed the importance
of identifying the scope of the target reference; i.e.
the fragment of the citation sentence that corre-
sponds to the target paper. We define the scope of
a reference as the shortest fragment of the citation
sentence that contains the reference and could form
a grammatical sentence if the rest of the sentence
was removed.
To find such a fragment, we use a simple yet ade-
quate heuristic. We start by parsing the sentence us-
ing the link grammar parser (Sleator and Temperley,
502
1991). Since the parser is not trained on citation sen-
tences, we replace the references with placeholders
before passing the sentence to the parser. Figure 1
shows a portion of the parse tree for Sentence (1)
(from Section 1).
Figure 1: An example showing the scope of a target ref-
erence
We extract the scope of the reference from the
parse tree as follows. We find the smallest subtree
rooted at an S node (sentence clause node) and con-
tains the target reference node. We extract the text
that corresponds to this subtree if it is grammati-
cal. Otherwise, we find the second smallest subtree
rooted at an S node and so on. For example, the
parse tree shown in Figure 1 suggests that the scope
of the reference is:
Resnik (1999) describes a method for mining the web for
bilingual texts.
4.1.3 Sentence Filtering
The task in this step is to detect and filter out unsuit-
able sentences; i.e., sentences that depend on their
context (e.g. Sentence (5) above) or describe the
own work of their authors, not the contribution of
the target paper (e.g Sentence (4) above). Formally,
we classify the citation sentences into two classes:
suitable and unsuitable sentences. We use a ma-
chine learning technique for this purpose. We ex-
tract a number of features from each sentence and
train a classification model using these features. The
trained model is then used to classify the sentences.
We use Support Vector Machines (SVM) with linear
kernel as our classifier. The features that we use in
this step and their descriptions are shown in Table 1.
4.2 Extraction
In the first stage, the sentences and sentence frag-
ments that are not useful for our summarization task
are ruled out. The input to this stage is a set of cita-
tion sentences that are believed to be suitable for the
summary. From these sentences, we need to select
a representative subset. The sentences are selected
based on these three main properties:
First, they should cover diverse aspects of the pa-
per. Second, the sentences that cover the same as-
pect should not contain redundant information. For
example, if two sentences talk about the drawbacks
of the target paper, one sentence can mention the
computation inefficiency, while the other criticize
the assumptions the paper makes. Third, the sen-
tences should cover as many important facts about
the target paper as possible using minimal text.
In this stage, the summary sentences are selected
in three steps. In the first step, the sentences are clas-
sified into five functional categories: Background,
Problem Statement, Method, Results, and Limita-
tions. In the second step, we cluster the sen-
tences within each category into clusters of simi-
lar sentences. In the third step, we compute the
LexRank (Erkan and Radev, 2004) values for the
sentences within each cluster. The summary sen-
tences are selected based on the classification, the
clustering, and the LexRank values.
4.2.1 Functional Category Classification
We classify the citation sentences into the five cat-
egories mentioned above using a machine learning
technique. A classification model is trained on a
number of features (Table 2) extracted from a la-
beled set of citation sentences. We use SVM with
linear kernel as our classifier.
4.2.2 Sentence Clustering
In the previous step we determined the category
of each citation sentence. It is very likely that
sentences from the same category contain similar or
overlapping information. For example, Sentences
(6), (7), and (8) below appear in the set of citation
503
Feature Description
Similarity to the target paper The value of the cosine similarity (using TF-IDF vectors) between the citation sentence and the target paper.
Headlines The section in which the citation sentence appeared in the citing paper. We recognize 10 section types such
as Introduction, Related Work, Approach, etc.
Relative position The relative position of the sentence in the section and the paragraph in which it appears
First person pronouns This feature takes a value of 1 if the sentence contains a first person pronoun (I, we, our, us, etc.), and 0
otherwise.
Tense of the first verb A sentence that contains a past tense verb near its beginning is more likely to be describing previous work.
Determiners Demonstrative Determiners (this, that, these, those, and which) and Alternative Determiners (another, other).
The value of this feature is the relative position of the first determiner (if one exists) in the sentence.
Table 1: The features used for sentence filtering
Feature Description
Similarity to the sections of the target paper The sections of the target paper are categorized into five categories: 1) Introduction, Moti-
vation, Problem Statement. 2) Background, Prior Work, Previous Work, and Related Work.
3) Experiments, Results, and Evaluation. 4) Discussion, Conclusion, and Future work. 5)
All other headlines. The value of this feature is the cosine similarity (using TF-IDF vectors)
between the sentence and the text of the sections of each of the five section categories.
Headlines This is the same feature that we used for sentence filtering in Section 4.1.3.
Number of references in the sentence Sentences that contain multiple references are more likely to be Background sentences.
Verbs We use all the verbs that their lemmatized form appears in at least three sentences that belong
to the same category in the training set. Auxiliary verbs are excluded. In our annotated dataset,
for example, the verb propose appeared in 67 sentences from the Methodology category, while
the verbs outperform and achieve appeared in 33 Result sentences.
Table 2: The features used for sentence classification
sentences that cite Goldwater and Griffiths? (2007).
These sentences belong to the same category (i.e
Method). Both Sentences (6) and (7) convey the
same information about Goldwater and Griffiths
(2007) contribution. Sentence (8), however, de-
scribes a different aspect of the paper methodology.
(6) Goldwater and Griffiths (2007) proposed an
information-theoretic measure known as the Variation of
Information (VI)
(7) Goldwater and Griffiths (2007) propose using the
Variation of Information (VI) metric
(8) A fully-Bayesian approach to unsupervised POS
tagging has been developed by Goldwater and Griffiths
(2007) as a viable alternative to the traditional maximum
likelihood-based HMM approach.
Clustering divides the sentences of each cate-
gory into groups of similar sentences. Following
Qazvinian and Radev (2008), we build a cosine sim-
ilarity graph out of the sentences of each category.
This is an undirected graph in which nodes are sen-
tences and edges represent similarity relations. Each
edge is weighted by the value of the cosine similarity
(using TF-IDF vectors) between the two sentences
the edge connects. Once we have the similarity net-
work constructed, we partition it into clusters using
a community finding technique. We use the Clauset
algorithm (Clauset et al, 2004), a hierarchical ag-
glomerative community finding algorithm that runs
in linear time.
4.2.3 Ranking
Although the sentences that belong to the same clus-
ter are similar, they are not necessarily equally im-
portant. We rank the sentences within each clus-
ter by computing their LexRank (Erkan and Radev,
2004). Sentences with higher rank are more impor-
tant.
4.2.4 Sentence Selection
At this point we have determined (Figure 2), for each
sentence, its category, its cluster, and its relative im-
portance. Sentences are added to the summary in
order based on their category, the size of their clus-
ters, then their LexRank values. The categories are
504
Figure 2: Example illustrating sentence selection
ordered as Background, Problem, Method, Results,
then Limitations. Clusters within each category are
ordered by the number of sentences in them whereas
the sentences of each cluster are ordered by their
LexRank values.
In the example shown in Figure 2, we have three
categories. Each category contains several clusters.
Each cluster contains several sentences with differ-
ent LexRank values (illustrated by the sizes of the
dots in the figure.) If the desired length of the sum-
mary is 3 sentences, the selected sentences will be
in order S1, S12, then S18. If the desired length is 5,
the selected sentences will be S1, S5, S12, S15, then
S18.
4.3 Postprocessing
In this stage, we refine the sentences that we ex-
tracted in the previous stage. Each citation sentence
will have the target reference (the author?s names
and the publication year) mentioned at least once.
The reference could be either syntactically and se-
mantically part of the sentence (e.g. Sentence (3)
above) or not (e.g. Sentence (2)). The aim of this
refinement step is to avoid repeating the author?s
names and the publication year in every sentence.
We keep the author?s names and the publication year
only in the first sentence of the summary. In the
following sentences, we either replace the reference
with a suitable personal pronoun or remove it. The
reference is replaced with a pronoun if it is part of
the sentence and this replacement does not make the
sentence ungrammatical. The reference is removed
if it is not part of the sentence. If the sentence con-
tains references for other papers, they are removed if
this doesn?t hurt the grammaticality of the sentence.
To determine whether a reference is part of the
sentence or not, we again use a machine learning
approach. We train a model on a set of labeled sen-
tences. The features used in this step are listed in
Table 3. The trained model is then used to classify
the references that appear in a sentence into three
classes: keep, remove, replace. If a reference is to
be replaced, and the paper has one author, we use
?he/she? (we do not know if the author is male or
female). If the paper has two or more authors, we
use ?they?.
5 Evaluation
We provide three levels of evaluation. First, we eval-
uate each of the components in our system sepa-
rately. Then we evaluate the summaries that our
system generate in terms of extraction quality. Fi-
nally, we evaluate the coherence and readability of
the summaries.
5.1 Data
We use the ACL Anthology Network (AAN) (Radev
et al, 2009) in our evaluation. AAN is a collection
of more than 16000 papers from the Computational
Linguistics journal, and the proceedings of the ACL
conferences and workshops. AAN provides all cita-
tion information from within the network including
the citation network, the citation sentences, and the
citation context for each paper.
We used 55 papers from AAN as our data. The
papers have a variable number of citation sentences,
ranging from 15 to 348. The total number of cita-
tion sentences in the dataset is 4,335. We split the
data randomly into two different sets; one for evalu-
ating the components of the system, and the other for
evaluating the extraction quality and the readability
of the generated summaries. The first set (dataset1,
henceforth) contained 2,284 sentences coming from
25 papers. We asked humans with good background
in NLP (the area of the annotated papers) to provide
two annotations for each sentence in this set: 1) label
the sentence as Background, Problem, Method, Re-
sult, Limitation, or Unsuitable, 2) for each reference
in the sentence, determine whether it could be re-
placed with a pronoun, removed, or should be kept.
505
Feature Description
Part-of-speech (POS) tag We consider the POS tags of the reference, the word before, and the word after. Before passing the
sentence to the POS tagger, all the references in the sentence are replaced by placeholders.
Style of the reference It is common practice in writing scientific papers to put the whole citation between parenthesis
when the authors are not a constitutive part of the enclosing sentence, and to enclose just the year
between parenthesis when the author?s name is a syntactic constituent in the sentence.
Relative position of the reference This feature takes one of three values: first, last, and inside.
Grammaticality Grammaticality of the sentence if the reference is removed/replaced. Again, we use the Link
Grammar parser (Sleator and Temperley, 1991) to check the grammaticality
Table 3: The features used for author name replacement
Each sentence was given to 3 different annotators.
We used the majority vote labels.
We use Kappa coefficient (Krippendorff, 2003) to
measure the inter-annotator agreement. Kappa coef-
ficient is defined as:
Kappa =
P (A)? P (E)
1? P (E)
(1)
where P (A) is the relative observed agreement
among raters and P (E) is the hypothetical proba-
bility of chance agreement.
The agreement among the three annotators on dis-
tinguishing the unsuitable sentences from the other
five categories is 0.85. On Landis and Kochs(1977)
scale, this value indicates an almost perfect agree-
ment. The agreement on classifying the sentences
into the five functional categories is 0.68. On the
same scale this value indicates substantial agree-
ment.
The second set (dataset2, henceforth) contained
30 papers (2051 sentences). We asked humans with
a good background in NLP (the papers topic) to gen-
erate a readable, coherent summary for each paper in
the set using its citation sentences as the source text.
We asked them to fix the length of the summaries
to 5 sentences. Each paper was assigned to two hu-
mans to summarize.
5.2 Component Evaluation
Reference Tagging and Reference Scope Iden-
tification Evaluation: We ran our reference tag-
ging and scope identification components on the
2,284 sentences in dataset1. Then, we went through
the tagged sentences and the extracted scopes, and
counted the number of correctly/incorrectly tagged
(extracted)/missed references (scopes). Our tagging
- Bkgrnd Prob Method Results Limit.
Precision 64.62% 60.01% 88.66% 76.05% 33.53%
Recall 72.47% 59.30% 75.03% 82.29% 59.36%
F1 68.32% 59.65% 81.27% 79.04% 42.85%
Table 4: Precision and recall results achieved by our cita-
tion sentence classifier
component achieved 98.2% precision and 94.4% re-
call. The reference to the target paper was tagged
correctly in all the sentences.
Our scope identification component extracted the
scope of target references with good precision
(86.4%) but low recall (35.2%). In fact, extracting
a useful scope for a reference requires more than
just finding a grammatical substring. In future work,
we plan to employ text regeneration techniques to
improve the recall by generating grammatical sen-
tences from ungrammatical fragments.
Sentence Filtering Evaluation: We used Sup-
port Vector Machines (SVM) with linear kernel as
our classifier. We performed 10-fold cross validation
on the labeled sentences (unsuitable vs all other cat-
egories) in dataset1. Our classifier achieved 80.3%
accuracy.
Sentence Classification Evaluation: We used
SVM in this step as well. We also performed 10-
fold cross validation on the labeled sentences (the
five functional categories). This classifier achieved
70.1% accuracy. The precision and recall for each
category are given in Table 4
Author Name Replacement Evaluation: The
classifier used in this task is also SVM. We per-
formed 10-fold cross validation on the labeled sen-
tences of dataset1. Our classifier achieved 77.41%
accuracy.
506
Produced using our system
There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical tech-
niques, e.g. constraint-based techniques and transformation-based techniques. A thorough removal of ambiguity requires a syntactic
process. A rule-based tagger described in Voutilainen (1995) was equipped with a set of guessing rules that had been hand-crafted using
knowledge of English morphology and intuitions. The precision of rule-based taggers may exceed that of the probabilistic ones. The
construction of a linguistic rule-based tagger, however, has been considered a difficult and time-consuming task.
Produced using Qazvinian and Radev (2008) system
Another approach is the rule-based or constraint-based approach, recently most prominently exemplified by the Constraint Grammar work
(Karlsson et al , 1995; Voutilainen, 1995b; Voutilainen et al , 1992; Voutilainen and Tapanainen, 1993), where a large number of
hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context.
Some systems even perform the POS tagging as part of a syntactic analysis process (Voutilainen, 1995). A rule-based tagger described
in (Voutilainen, 1995) is equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphology
and intuition. Older versions of EngCG (using about 1,150 constraints) are reported ( butilainen et al 1992; Voutilainen and HeikkiUi
1994; Tapanainen and Voutilainen 1994; Voutilainen 1995) to assign a correct analysis to about 99.7% of all words while each word in
the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remait unresolved. We evaluate the resulting
disambiguated text by a number of metrics defined as follows (Voutilainen, 1995a).
Table 5: Sample Output
5.3 Extraction Evaluation
To evaluate the extraction quality, we use dataset2
(that has never been used for training or tuning any
of the system components). We use our system to
generate summaries for each of the 30 papers in
dataset2. We also generate summaries for the pa-
pers using a number of baseline systems (described
in Section 5.3.1). All the generated summaries were
5 sentences long. We use the Recall-Oriented Un-
derstudy for Gisting Evaluation (ROUGE) based on
the longest common substrings (ROUGE-L) as our
evaluation metric.
5.3.1 Baselines
We evaluate the extraction quality of our system
(FL) against 7 different baselines. In the first base-
line, the sentences are selected randomly from the
set of citation sentences and added to the sum-
mary. The second baseline is the MEAD summa-
rizer (Radev et al, 2004) with all its settings set
to default. The third baseline is LexRank (Erkan
and Radev, 2004) run on the entire set of citation
sentences of the target paper. The forth baseline is
Qazvinian and Radev (2008) citation-based summa-
rizer (QR08) in which the citation sentences are first
clustered then the sentences within each cluster are
ranked using LexRank. The remaining baselines are
variations of our system produced by removing one
component from the pipeline at a time. In one vari-
ation (FL-1), we remove the sentence filtering com-
ponent. In another variation (FL-2), we remove the
sentence classification component; so, all the sen-
tences are assumed to come from one category in the
subsequent components. In a third variation (FL-3),
the clustering component is removed. To make the
comparison of the extraction quality to those base-
lines fair, we remove the author name replacement
component from our system and all its variations.
5.3.2 Results
Table 6 shows the average ROUGE-L scores (with
95% confidence interval) for the summaries of the
30 papers in dataset2 generated using our system
and the different baselines. The two human sum-
maries were used as models for comparison. The
Human score reported in the table is the result of
comparing the two human summaries to each others.
Statistical significance was tested using a 2-tailed
paired t-test. The results are statistically significant
at the 0.05 level.
The results show that our approach outperforms
all the baseline techniques. It achieves higher
ROUGE-L score for most of the papers in our test-
ing set. Comparing the score of FL-1 to the score
of FL shows that sentence filtering has a significant
impact on the results. It also shows that the classifi-
cation and clustering components both improve the
extraction quality.
5.4 Coherence and Readability Evaluation
We asked human judges (not including the authors)
to rate the coherence and readability of a number
of summaries for each of dataset2 papers. For
each paper we evaluated 3 summaries. The sum-
507
- Human Random MEAD LexRank QR08
ROUGE-L 0.733 0.398 0.410 0.408 0.435
- FL-1 FL-2 FL-3 FL -
ROUGE-L 0.475 0.511 0.525 0.539 -
Table 6: Extraction Evaluation
Average Coherence Rating
Number of summaries
Human FL QV08
1? coherence <2 0 9 17
2? coherence <3 3 11 12
3? coherence <4 16 9 1
4? coherence ?5 11 1 0
Table 7: Coherence Evaluation
mary that our system produced, the human sum-
mary, and a summary produced by Qazvinian and
Radev (2008) summarizer (the best baseline - after
our system and its variations - in terms of extrac-
tion quality as shown in the previous subsection.)
The summaries were randomized and given to the
judges without telling them how each summary was
produced. The judges were not given access to the
source text. They were asked to use a five point-
scale to rate how coherent and readable the sum-
maries are, where 1 means that the summary is to-
tally incoherent and needs significant modifications
to improve its readability, and 5 means that the sum-
mary is coherent and no modifications are needed to
improve its readability. We gave each summary to 5
different judges and took the average of their ratings
for each summary. We used Weighted Kappa with
linear weights (Cohen, 1968) to measure the inter-
rater agreement. The Weighted Kappa measure be-
tween the five groups of ratings was 0.72.
Table 7 shows the number of summaries in each
rating range. The results show that our approach sig-
nificantly improves the coherence of citation-based
summarization. Table 5 shows two sample sum-
maries (each 5 sentences long) for the Voutilainen
(1995) paper. One summary was produced using our
system and the other was produced using Qazvinian
and Radev (2008) system.
6 Conclusions
In this paper, we presented a new approach for
citation-based summarization of scientific papers
that produces readable summaries. Our approach in-
volves three stages. The first stage preprocesses the
set of citation sentences to filter out the irrelevant
sentences or fragments of sentences. In the second
stage, a representative set of sentences are extracted
and added to the summary in a reasonable order. In
the last stage, the summary sentences are refined to
improve their readability. The results of our exper-
iments confirmed that our system outperforms sev-
eral baseline systems.
Acknowledgments
This work is in part supported by the National
Science Foundation grant ?iOPENER: A Flexible
Framework to Support Rapid Learning in Unfamil-
iar Research Domains?, jointly awarded to Univer-
sity of Michigan and University of Maryland as
IIS 0705832, and in part by the NIH Grant U54
DA021519 to the National Center for Integrative
Biomedical Informatics.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this paper are those of
the authors and do not necessarily reflect the views
of the supporters.
References
Aaron Clauset, M. E. J. Newman, and Cristopher Moore.
2004. Finding community structure in very large net-
works. Phys. Rev. E, 70(6):066111, Dec.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement provision for scaled disagreement or partial
credit. Psychological Bulletin, 70(4):213 ? 220.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir Radev. 2008. Blind men
and elephants: What do citation summaries tell us
about a research article? J. Am. Soc. Inf. Sci. Tech-
nol., 59(1):51?62.
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text sum-
marization. J. Artif. Int. Res., 22(1):457?479.
E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The
Use of Citation Data in Writing the History of Science.
Institute for Scientific Information Inc., Philadelphia,
Pennsylvania, USA.
T. L. Hodges. 1972. Citation indexing-its theory
and application in science, technology, and humani-
ties. Ph.D. thesis, University of California at Berke-
ley.Ph.D. thesis, University of California at Berkeley.
508
Klaus H. Krippendorff. 2003. Content Analysis: An In-
troduction to Its Methodology. Sage Publications, Inc,
2nd edition, December.
J. Richard Landis and Gary G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159?174, March.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL-08: HLT, pages 816?824, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 584?592, Boulder, Colorado, June. Association
for Computational Linguistics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI ?99: Proceedings of the Six-
teenth International Joint Conference on Artificial In-
telligence, pages 926?931, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and
Of Information Science. 2000. Classification of re-
search papers using citation links and citation types:
Towards automatic review article generation.
M. E. J. Newman. 2001. The structure of scientific
collaboration networks. Proceedings of the National
Academy of Sciences of the United States of America,
98(2):404?409, January.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 689?696, Manchester, UK, August.
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555?564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Ozgur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 895?903, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
Winkel, and Zhu Zhang. 2004. MEAD - a platform
for multidocument multilingual text summarization.
In LREC 2004, Lisbon, Portugal, May.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network corpus.
In NLPIR4DL ?09: Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital Li-
braries, pages 54?61, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attributing
scientific work to citations. In In Proceedings of
NAACL/HLT-07.
Daniel D. K. Sleator and Davy Temperley. 1991. Parsing
english with a link grammar. In In Third International
Workshop on Parsing Technologies.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
In Proc. of EMNLP-06.
Simone Teufel. 2007. Argumentative zoning for im-
proved citation indexing. computing attitude and affect
in text. In Theory and Applications, pages 159170.
509
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 248?253,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Towards Style Transformation from Written-Style to Audio-Style
Amjad Abu-Jbara?
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Barbara Rosario
Intel Labs
Santa Clara, CA, USA
barbara.rosario@intel.com
Kent Lyons
Intel Labs
Santa Clara, CA, USA
kent.lyons@intel.com
Abstract
In this paper, we address the problem of op-
timizing the style of textual content to make
it more suitable to being listened to by a user
as opposed to being read. We study the dif-
ferences between the written style and the au-
dio style by consulting the linguistics and jour-
nalism literatures. Guided by this study, we
suggest a number of linguistic features to dis-
tinguish between the two styles. We show
the correctness of our features and the impact
of style transformation on the user experience
through statistical analysis, a style classifica-
tion task, and a user study.
1 Introduction
We live in a world with an ever increasing amount
and variety of information. A great deal of that con-
tent is in a textual format. Mobile technologies have
increased our expectations as to when, where, and
how we can access such content. As such, it is not
uncommon to want to gain access to this information
when a visual display is not convenient or available
(while driving or walking for example). One way of
addressing this issue is to use audio displays and, in
particular, have users listen to content read to them
by a speech synthesizer instead of reading it them-
selves on a display.
While listening to speech opens many opportu-
nities, it also has issues which must be considered
when using it as a replacement for reading. One im-
portant consideration is that the text that was origi-
nally written to be read might not be suitable to be
listened to. Journalists, for example, write differ-
ently for audio (i.e. radio news broadcast) compared
?Work conducted while interning at Intel Labs
to writing content meant to be read (i.e. newspaper
articles) (Fang, 1991).
One key reason for the difference is that under-
standing is more important than grammar to a radio
news writer. Furthermore, audio has different per-
ceptual and information qualities compared to read-
ing. For example, the use of the negations not and
no should be limited since it is easy for listeners to
miss that single utterance. Listener cannot relisten to
a word; and, missing it has a huge impact on mean-
ing.
In this paper, we address the problem of changing
the writing-style of text to make it suitable to being
listened to instead of being read.
We start by researching the writing-style differ-
ences across text and audio in the linguistics and
journalism literatures. Based on this study, we sug-
gest a number of linguistic features that set the two
styles apart. We validate these features statistically
by analyzing their distributions in a corpus of paral-
lel text- and audio-style documents; and experimen-
tally through a style classification task. Moreover,
we evaluate the impact of style transformation on
the user experience by conducting a user study.
The rest of this paper is organized as follows. In
the next section, we examine the related work. In
Section 3, we summarize the main style differences
as they appear in the journalism and linguistics lit-
eratures. In Section 4, we describe the data that we
collected and used in this work. The features that we
propose and their validation are discussed in Section
5. In Section 6, we describe the user study and dis-
cuss the results. We conclude in Section 7.
2 Related Work
There has been a considerable amount of research
on the language variations for different registers and
248
genres in the linguistics community, including re-
search that focused on the variations between writ-
ten and spoken language (Biber, 1988; Halliday,
1985; Esser, 1993; Whittaker et al, 1998; Esser,
2000). For example, Biber (1988) provides an ex-
haustive study of such variations. He uses compu-
tational techniques to analyze the linguistic charac-
teristics of twenty-three spoken and written genres,
enabling identification of the basic, underlying di-
mensions of variation in English.
Halliday (1985) performs a comparative study
of spoken and written language, contrasting the
prosodic features and grammatical intricacy of
speech with the high lexical density and grammat-
ical metaphor or writing. Esser (2000) proposes
a general framework for the different presentation
structures of medium-dependent linguistic units.
Most of these studies focus on the variations be-
tween the written and the spontaneous spoken lan-
guage. Our focus is on the written language for
audio, i.e. on a style that we hypothesize being
somewhere between the formally written and spon-
taneous speech styles. Fang (1991) provides a prag-
matic analysis and a side-by-side comparisons of the
?writing style differences in newspaper, radio, and
television news? as part of the instructions for jour-
nalist students learning to write for the three differ-
ent mediums.
Paraphrase generation (Barzilay and McKeown,
2001; Shinyama et al, 2002; Quirk et al, 2004;
Power and Scot, 2005; Zhao et al, 2009; Madnani
and Dorr, 2010) is related to our work, but usually
the focus has been on the semantics, with the goal
of generating relevant content, and on the syntax to
generate well formed text. In this work the goal is to
optimize the style, and generation is one approach to
that end (we plan addressing it for future work)
Authorship attribution (Mosteller and Wallace,
1964; Stamatatos et al, 2000; Argamon et al, 2003;
Argamon et al, 2007; Schler and Argamon, 2009)
is also related to our work since arguably differ-
ent authors write in different styles. For exam-
ple, Argamon et al (2003) explored differences
between male and female writing in a large sub-
set of the British National Corpus covering a range
of genres. Argamon el al. (2007) addressed the
problem of classifying texts by authors, author per-
sonality, gender of literary characters, sentiment
(positive/negative feeling), and scientific rhetorical
styles. They used lexical features based on tax-
onomies of various semantic functions of different
lexical items (words or phrases). These studies fo-
cused on the correlation between style of the text
and the personal characteristics of its author. In our
work, we focus on the change in writing style ac-
cording to the change of the medium.
3 Writing Style Differences Across Text
and Audio
In this section, we summarize the literature on writ-
ing style differences across text and audio. Style dif-
ferences are not due to happenstance. Writing styles
for different media have evolved due to the unique
nature of each medium and to the manner in which
its audience consumes it. For example, in audio, the
information must be consumed sequentially and the
listener does not have the option to skip the informa-
tion that she finds less interesting.
Also, the listener, unlike the reader, cannot stop
to review the meaning of a word or a sentence. The
eye skip around in text but there is not that option
with listening. Moreover, unlike attentive readers of
text, audio listeners may be engaged in some task
(e.g. driving, working, etc.) other than absorbing the
information they listen to, and therefore are paying
less attention.
All these differences of the audio medium affect
the length of sentences, the choice of words, the
structure of phrases of attribution, the use of pro-
nouns, etc.
Some general guidelines of audio style (Biber,
1988; Fang, 1991) include 1) the choice of sim-
ple words and short, declarative sentences with ac-
tive voice preferred. 2) Attribution precedes state-
ments as it does in normal conversations. 3) The
subject should be as close to the predicate as feasi-
ble. 4) Pronouns should be used with a lot of wari-
ness. It is better to repeat a name, so that the lis-
tener will not have to pause or replay to recall. 5)
Direct quotations are uncommon and the person be-
ing quoted is identified before the quotation. 6) De-
pendent clauses should be avoided, especially at the
start of a sentence. It is usually better to make a sep-
arate sentence of a dependent clause. 7) Numbers
should be approximated so that they can be under-
249
0100
200
300
400
500
600
700
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57
Mo
re
0
50
100
150
200
250
300
350
400
450
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46
Mo
re
0
200
400
600
800
1000
1200
1400
1600
0
0.0
1
0.0
2
0.0
3
0.0
4
0.0
5
0.0
6
0.0
7
0.0
8
0.0
9 0.1 0.1
2
0.1
3
Average Sentence  Length Percentage of Complex Words Ratio of Adverbs
Transcripts
Articles
Transcripts
Articles
Transcripts
Articles
# D
ocu
me
nts
# D
ocu
me
nts
# D
ocu
me
nts
Figure 1: The distributions of three features for both articles and transcripts
stood. For example, the sum of $52,392 could be
stated as more than fifty thousand dollars. 8) Adjec-
tives and adverbs should be used only when neces-
sary for the meaning.
4 Data
In order to determine the differences between the
text and audio styles, we needed textual data that
ideally covered the same semantic content but was
produced for the two different media. National
Public Radio (NPR) has exactly this type of data.
Through their APIs we obtained the same semantic
content in the two different styles: written text style
(articles, henceforth) and in audio style (transcripts,
henceforth). The NPR Story API output contains
links to the Transcript API when a transcript is avail-
able. With the Transcript API, we were able to get
full transcripts of stories heard on air1. To the best of
our knowledge, this is the first use of this collection
for NLP research.
We collected 3855 news articles and their corre-
sponding transcripts. The data cover a varied set of
topics from four months of broadcast (from March 6
to June 3, 2010). Table 2 shows an example of such
article-transcript pairs.
5 Features
Based on the study of style differences outlined in
section 3, we propose a number of document-level,
linguistic features that we hypothesized distinguish
the two writing styles. We extracted these fea-
1http://www.npr.org/api/index
tures for each article and transcript. The analysis
of these features (will be discussed later in the sec-
tion) showed that they are of different importance to
style identification. Table 1 shows a list of the top
features and their descriptions.
5.1 Statistical Analysis
The goal of this analysis is to show that the values
of the features that we extracted are really different
across the two styles and that the difference is sig-
nificant. We compute the distribution of the values
of each feature in articles and its distribution in tran-
scripts. For example, Figure 1 shows the distribu-
tions of 3 features for both articles and transcripts.
The figure clearly shows how the distributions are
different. A two-tailed paired Student?s T-test (with
alpha set to 0.05) reveals statistically significant dif-
ference for all of the features (p < 0.0001).
This analysis corroborated our linguistic hypothe-
ses, such as the average sentence length is longer for
articles than for transcripts, complex words (more
than 3 syllables) are more common in articles, arti-
cles contain more adverbs, etc.
5.2 Classification
To further verify that our features really distinguish
between the two writing styles, we conducted a clas-
sification experiment. We used the features de-
scribed in Table 1 (excluding the Direct Quotation
feature) and the dataset described in section 4 to
train a classifier. We used Libsvm (Chang and Lin,
2001) with a linear kernel as our classifier. We per-
formed 10-fold cross validation on the entire dataset.
250
Feature Description Rank
Direct quotations We use a pattern matching rule to find all the instances of direct speech (e.g. ?I love English?, says
Peter).
1
Average sentence length The length of a sentence is the number of words it contains. 2
Ratio of complex words A complex word consists of three or more syllables (Gunning, 1952). Complex words are more
difficult to pronounce and harder to understand when being listened to than simpler words.
3
Ratio of pronouns We count the different types of pronouns; first person pronouns, second person pronouns, third
person pronoun, demonstrative pronouns (this, these, those), and the pronoun it.
4
Average distance between
each verb and its subject
We associate each verb with its subject by parsing the sentence using a dependency parser and
finding nsubj link. The distance is the word count between the verb and its subject.
5
Ratio of adjectives We count attributive adjectives (e.g. the big house) and predictive adjectives (e.g. the house is big)
separately.
6
Dependent clauses We identify dependent clauses by parsing the sentence and finding a SBAR node in the parse tree. 7
Average noun phrase mod-
ification degree
The average number of modifiers for all the noun phrases in the document. 8
Average number of sylla-
bles
The total number of syllables in the document divided by the number of words. To get an accurate
count of syllables in a word, we look up the word in a dictionary. All the numbers are converted
to words (e.g. 25 becomes twenty five). We also change all the contractions to their normal form
(e.g. I?ll becomes I will).
9
Ratio of passive sentences We find passive sentences using a pattern match rule against the part-of-speech tags of the sentence.
We compute the ratios of agentless passive sentences and by-passive sentences separately.
10
Ratio of adverbs In addition to counting all the adverbs, we also count special types of adverbs separately includ-
ing: amplifiers (e.g. absolutely, completely, enormously, etc), downtoners (e.g. almost, barely,
hardly, etc), place adverbials (e.g. abroad, above, across, etc), and time adverbials (e.g. after-
wards, eventually, initially, etc). The list of special adverbs and their types is taken from Quirk et.
al (1985).
11
Size of vocabulary The number of unique words in a document divided by the total number of words. 12
Ratio of verb tenses We count the three main types of verbs, present, past, and perfect aspect. 13
Ratio of approximated
numbers
We count the instance of approximated numbers in text. In particular, we count the pattern more
than/less than/about/almost ?integer number?.
14
Table 1: Style Features
Written article
The mammoth oil spill in the Gulf of Mexico, sparked by the explo-
sion and sinking of a deep-water oil rig, now surrounds the Missis-
sippi River Delta, all but shutting down fisheries. But the oil industry
still has a lot of friends on the delta. As Louisianans fight the crude
invading their coast, many also want to repel efforts to limit offshore
drilling. ?We need the oil industry, and down here, there are only
two industries ? fishing and oil,? says charter boat captain Devlin
Roussel. Like most charter captains on the delta, Roussel has just
been sitting on the dock lately. But if he did have paying customers
to take out fishing, he?d most likely take them to an oil rig. [..]
Transcript
It?s MORNING EDITION from NPR News. I?m Steve Inskeep.
And I?m Renee Montagne. President Obama?s administration is
promising action on that catastrophic oil spill. The president?s en-
vironmental adviser says the BP oil leak will be plugged. More on
that in a moment. President Obama yesterday said the nation is too
dependent on fossil fuels. But you dont realize just how dependent
until you travel to the Mississippi River Delta. The fishing industry
there is all but shut down. Yet some residents do not want to stop
or slow offshore drilling despite the disaster. NPR?s Frank Morris
visited Buras, Louisiana [..]
Table 2: An example of an article?transcript pair.
Our classifier achieved 87.4% accuracy which is
high enough to feel confident about the features.
We excluded the Direct Quotation feature from
this experiment because it is a very distinguishing
feature for articles. The vast majority of the articles
in our dataset contained direct quotations and none
of the transcripts did. When this feature is included,
the accuracy rises to 97%.
To better understand which features are more im-
portant indicators of the style, we use Guyon et
al.?s (2002) method for feature selection using SVM
to rank the features based on their importance. The
ranks are shown in the last column in Table 1.
251
6 User Study
Up to this point, we know that there are differences
in style between articles and transcripts, and we for-
malized these differences in the form of linguistic
features that are easy to extract using computational
techniques. However, we still do not know the im-
pact of changing the style on the user experience. To
address this issue, we did manual transformation of
style for 50 article paragraphs. The transformation
was done in light of the features described in the pre-
vious section. For example, if a sentence is longer
than 25 words, we simplify it; and, if it is in passive
voice we change it to active voice whenever possi-
ble, etc. We used a speech synthesizer to convert the
original paragraphs and their transformed versions
into audio clips. We used these audio clips to con-
duct a user study.
We gave human participants the audio clips to lis-
ten to and transcribe. Each audio clip was divided
into segments 15 seconds long. Each segment can
be played only once and pauses automatically when
it is finished to allow the user to transcribe the seg-
ment. The user was not allowed to replay any seg-
ment of the clip. Our hypothesis for this study is
that audio clips of the transformed paragraphs (audio
style) are easier to comprehend, and hence, easier to
transcribe than the original paragraphs (text style).
We use the edit distance between the transcripts and
the text of each audio clip to measure the transcrip-
tion accuracy. We assume that the transcription ac-
curacy is an indicator for the comprehension level,
i.e. the higher the accuracy of the transcription the
higher the comprehension.
We used Amazon Mechanical Turk to run the user
study. We took several precautions to guarantee the
quality of the data (burch, 2009). We restricted the
workers to those who have more than 95% approval
rate for all their previous work and who live in the
United States (since we are targeting English speak-
ers). We also assigned the same audio clip to 10
different workers and took the average edit distance
of the 10 transcripts for each audio clip.
The differences in the transcription accuracy for
the original and the transformed paragraphs were
statically significant at the 0.05 level according to
a 2-tailed paired t-test. The overall average edit dis-
tance was 0.69 for the 50 transformed paragraphs
and 0.56 for the original article paragraphs. This re-
sult indicates that the change in style has an impact
on the comprehension of the delivered information
as measured by the accuracy of the transcriptions.
7 Conclusions and Future Work
In this paper, we presented the progress on an on-
going research on writing style transformation from
text style to audio style. We motivated the topic and
emphasized its importance. We surveyed the lin-
guistics and journalism literatures for the differences
in writing style for different media. We formalized
the problem by suggesting a number of linguistic
features and showing their validity in distinguishing
between the two styles of interest, text vs audio. We
also conducted a user study to show the impact of
style transformation on comprehension and the over-
all user experience.
The next step in this work would be to build a
style transformation system that uses the features
discussed in this paper as the bases for determining
when, where, and how to do the style transforma-
tion.
References
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and
Anat Rachel Shimoni. 2003. Gender, genre, and writ-
ing style in formal written texts. TEXT, 23:321?346.
Shlomo Argamon, Paul Chase, Sushant Dhawle, Sobhan
Raj, Hota Navendu, and Garg Shlomo Levitan. 2007.
Stylistic text classification using functional lexical fea-
tures. Journal of the American Society of Information
Science. ((In press)) Baayen, 7:91?109.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, ACL ?01, pages 50?57,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Douglas Biber. 1988. Variation across Speech and Writ-
ing. Cambridge University Press.
Chris Callison burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazons mechan-
ical turk.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Jrgen Esser. 1993. English linguistic stylistics.
Niemeyer.
252
Jrgen Esser. 2000. Medium-transferability and presenta-
tion structure in speech and writing. Journal of Prag-
matics, 32.
Irving E. Fang. 1991. Writing Style Differences in News-
paper, Radio, and Television News. Monograph Ser
Vol, 1. University of Minnesota. Center for Interdisci-
plinary Studies of Writing.
Robert Gunning. 1952. The technique of clear writing.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Machine
Learning, 46:389?422. 10.1023/A:1012487302797.
Michael Alexander Kirkwood Halliday. 1985. Spoken
and Written Language. Deakin University Press.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Comput. Linguist., 36:341?387.
Frederick Mosteller and David L. Wallace. 1964. In-
ference and disputed authorship : the Federalist / [by]
Frederick Mosteller [and] David L. Wallace. Addison-
Wesley, Reading, Mass. :.
Richard Power and Donia Scot. 2005. Automatic gener-
ation of large-scale paraphrase.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 142?149.
Jonathan Schler and Shlomo Argamon. 2009. Computa-
tional methods in authorship attribution.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of the second international con-
ference on Human Language Technology Research,
HLT ?02, pages 313?318, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Efstathios Stamatatos, George Kokkinakis, and
Nikos Fakotakis T. 2000. Automatic text categoriza-
tion in terms of genre and author. Computational
Linguistics, 26:471?495.
Steve Whittaker, Julia Hirschberg, and Christine H.
Nakatani. 1998. Play it again: a study of the fac-
tors underlying speech browsing behavior. In CHI
?98: CHI 98 conference summary on Human factors
in computing systems, pages 247?248, New York, NY,
USA. ACM.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2 - Volume 2, ACL ?09,
pages 834?842, Stroudsburg, PA, USA. Association
for Computational Linguistics.
253
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 592?597,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Identifying the Semantic Orientation of Foreign Words
Ahmed Hassan
EECS Department
University of Michigan
Ann Arbor, MI
hassanam@umich.edu
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI
amjbara@umich.edu
Rahul Jha
EECS Department
University of Michigan
Ann Arbor, MI
rahuljha@umich.edu
Dragomir Radev
EECS Department and School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
We present a method for identifying the pos-
itive or negative semantic orientation of for-
eign words. Identifying the semantic orienta-
tion of words has numerous applications in the
areas of text classification, analysis of prod-
uct review, analysis of responses to surveys,
and mining online discussions. Identifying
the semantic orientation of English words has
been extensively studied in literature. Most of
this work assumes the existence of resources
(e.g. Wordnet, seeds, etc) that do not exist
in foreign languages. In this work, we de-
scribe a method based on constructing a mul-
tilingual network connecting English and for-
eign words. We use this network to iden-
tify the semantic orientation of foreign words
based on connection between words in the
same language as well as multilingual connec-
tions. The method is experimentally tested us-
ing a manually labeled set of positive and neg-
ative words and has shown very promising re-
sults.
1 Introduction
A great body of research work has focused on iden-
tifying the semantic orientation of words. Word po-
larity is a very important feature that has been used
in several applications. For example, the problem
of mining product reputation from Web reviews has
been extensively studied (Turney, 2002; Morinaga
et al, 2002; Nasukawa and Yi, 2003; Popescu and
Etzioni, 2005; Banea et al, 2008). This is a very
important task given the huge amount of product re-
views written on the Web and the difficulty of man-
ually handling them. Another interesting applica-
tion is mining attitude in discussions (Hassan et al,
2010), where the attitude of participants in a discus-
sion is inferred using the text they exchange.
Due to its importance, several researchers have
addressed the problem of identifying the semantic
orientation of individual words. This work has al-
most exclusively focused on English. Most of this
work used several language dependent resources.
For example Turney and Littman (2003) use the en-
tire English Web corpus by submitting queries con-
sisting of the given word and a set of seeds to a
search engine. In addition, several other methods
have used Wordnet (Miller, 1995) for connecting se-
mantically related words (Kamps et al, 2004; Taka-
mura et al, 2005; Hassan and Radev, 2010).
When we try to apply those methods to other lan-
guages, we run into the problem of the lack of re-
sources in other languages when compared to En-
glish. For example, the General Inquirer lexicon
(Stone et al, 1966) has thousands of English words
labeled with semantic orientation. Most of the lit-
erature has used it as a source of labeled seeds or
for evaluation. Such lexicons are not readily avail-
able in other languages. Another source that has
been widely used for this task is Wordnet (Miller,
1995). Even though other Wordnets have been built
for other languages, their coverage is very limited
when compared to the English Wordnet.
In this work, we present a method for predicting
the semantic orientation of foreign words. The pro-
592
Figure 1: Sparse Foreign Networks are connected to
Dense English Networks. Dashed nodes represent la-
beled positive and negative seeds.
posed method is based on creating a multilingual
network of words that represents both English and
foreign words. The network has English-English
connections, as well as foreign-foreign connections
and English-foreign connections. This allows us to
benefit from the richness of the resources built for
the English language and in the meantime utilize
resources specific to foreign languages. Figure 1
shows a multilingual network where a sparse foreign
network and a dense English network are connected.
We then define a random walk model over the multi-
lingual network and predict the semantic orientation
of any given word by comparing the mean hitting
time of a random walk starting from it to a positive
and a negative set of seed English words.
We use both Arabic and Hindi for experiments.
We compare the performance of several methods us-
ing the foreign language resources only and the mul-
tilingual network that has both English and foreign
words. We show that bootstrapping from languages
with dense resources such as English is useful for
improving the performance on other languages with
limited resources.
The rest of the paper is structured as follows. In
section 2, we review some of the related prior work.
We define our problem and explain our approach in
Section 3. Results and discussion are presented in
Section 4. We conclude in Section 5.
2 Related Work
The problem of identifying the polarity of individual
words is a well-studied problem that attracted sev-
eral research efforts in the past few years. In this
section, we survey several methods that addressed
this problem.
The work of Hatzivassiloglou and McKeown
(1997) is among the earliest efforts that addressed
this problem. They proposed a method for identify-
ing the polarity of adjectives. Their method is based
on extracting all conjunctions of adjectives from a
given corpus and then they classify each conjunc-
tive expression as either the same orientation such
as ?simple and well-received? or different orienta-
tion such as ?simplistic but well-received?. Words
are clustered into two sets and the cluster with the
higher average word frequency is classified as posi-
tive.
Turney and Littman (2003) identify word polar-
ity by looking at its statistical association with a set
of positive/negative seed words. They use two sta-
tistical measures for estimating association: Point-
wise Mutual Information (PMI) and Latent Seman-
tic Analysis (LSA). Co-occurrence statistics are col-
lected by submitting queries to a search engine. The
number of hits for positive seeds, negative seeds,
positives seeds near the given word, and negative
seeds near the given word are used to estimate the
association of the given word to the positive/negative
seeds.
Wordnet (Miller, 1995), thesaurus and co-
occurrence statistics have been widely used to mea-
sure word relatedness by several semantic orienta-
tion prediction methods. Kamps et al (2004) use the
length of the shortest-path in Wordnet connecting
any given word to positive/negative seeds to iden-
tify word polarity. Hu and Liu (2004) use Word-
net synonyms and antonyms to bootstrap from words
with known polarity to words with unknown polar-
ity. They assign any given word the label of its syn-
onyms or the opposite label of its antonyms if any of
them are known.
Kanayama and Nasukawa (2006) used syntactic
features and context coherency, defined as the ten-
dency for same polarities to appear successively,
to acquire polar atoms. Takamura et al (2005)
proposed using spin models for extracting seman-
tic orientation of words. They construct a network
of words using gloss definitions, thesaurus and co-
occurrence statistics. They regard each word as an
electron. Each electron has a spin and each spin has
a direction taking one of two values: up or down.
593
Two neighboring spins tend to have the same orien-
tation from an energetic point of view. Their hypoth-
esis is that as neighboring electrons tend to have the
same spin direction, neighboring words tend to have
similar polarity. Hassan and Radev (2010) use a ran-
dom walk model defined over a word relatedness
graph to classify words as either positive or negative.
Words are connected based on Wordnet relations as
well as co-occurrence statistics. They measure the
random walk mean hitting time of the given word to
the positive set and the negative set. They show that
their method outperforms other related methods and
that it is more immune to noisy word connections.
Identifying the semantic orientation of individ-
ual words is closely related to subjectivity analy-
sis. Subjectivity analysis focused on identifying
text that presents opinion as opposed to objective
text that presents factual information (Wiebe, 2000).
Some approaches to subjectivity analysis disregard
the context phrases and words appear in (Wiebe,
2000; Hatzivassiloglou and Wiebe, 2000; Banea
et al, 2008), while others take it into considera-
tion (Riloff and Wiebe, 2003; Yu and Hatzivas-
siloglou, 2003; Nasukawa and Yi, 2003; Popescu
and Etzioni, 2005).
3 Approach
The general goal of this work is to mine the seman-
tic orientation of foreign words. We do this by cre-
ating a multilingual network of words. In this net-
work two words are connected if we believe that they
are semantically related. The network has English-
English, English-Foreign and Foreign-Foreign con-
nections. Some of the English words will be used as
seeds for which we know the semantic orientation.
Given such a network, we will measure the mean
hitting time in a random walk starting at any given
word to the positive set of seeds and the negative set
of seeds. Positive words will be more likely to hit the
positive set faster than hitting the negative set and
vice versa. In the rest of this section, we define how
the multilingual word network is built and describe
an algorithm for predicting the semantic orientation
of any given word.
3.1 Multilingual Word Network
We build a network G(V,E) where V = Ven ? Vfr
is the union of a set of English and foreign words.
E is a set of edges connecting nodes in V . There
are three types of connections: English-English con-
nections, Foreign-Foreign connections and English-
Foreign connections.
For the English-English connections, we use
Wordnet (Miller, 1995). Wordnet is a large lexical
database of English. Words are grouped in synsets
to express distinct concepts. We add a link between
two words if they occur in the same Wordnet synset.
We also add a link between two words if they have a
hypernym or a similar-to relation.
Foreign-Foreign connections are created in a sim-
ilar way to the English connections. Some other lan-
guages have lexical resources based on the design of
the Princeton English Wordnet. For example: Euro
Wordnet (EWN) (Vossen, 1997), Arabic Wordnet
(AWN) (Elkateb, 2006; Black and Fellbaum, 2006;
Elkateb and Fellbaum, 2006) and the Hindi Word-
net (Narayan et al, 2002; S. Jha, 2001). We also use
co-occurrence statistics similar to the work of Hatzi-
vassiloglou and McKeown (1997).
Finally, to connect foreign words to English
words, we use a foreign to English dictionary. For
every word in a list of foreign words, we look up
its meaning in a dictionary and add an edge between
the foreign word and every other English word that
appeared as a possible meaning for it.
3.2 Semantic Orientation Prediction
We use the multilingual network we described above
to predict the semantic orientation of words based
on the mean hitting time to two sets of positive and
negative seeds. Given the graph G(V,E), we de-
scribed in the previous section, we define the transi-
tion probability from node i to node j by normaliz-
ing the weights of the edges out from i:
P (j|i) = Wij/
?
k
Wik (1)
The mean hitting time h(i|j) is the average num-
ber of steps a random walker, starting at i, will take
to enter state j for the first time (Norris, 1997). Let
the average number of steps that a random walker
starting at some node i will need to enter a state
594
k ? S be h(i|S). It can be formally defined as:
h(i|S) =
{
0 i ? S
?
j?V pij ? h(j|S) + 1 otherwise
(2)
where pij is the transition probability between
node i and node j.
Given two lists of seed English words with known
polarity, we define two sets of nodes S+ and S?
representing those seeds. For any given word w, we
calculate the mean hitting time between w and the
two seed sets h(w|S+) and h(w|S?). If h(w|S+)
is greater than h(w|S?), the word is classified as
negative, otherwise it is classified as positive. We
used the list of labeled seeds from (Hatzivassiloglou
and McKeown, 1997) and (Stone et al, 1966). Sev-
eral other similarity measures may be used to predict
whether a given word is closer to the positive seeds
list or the negative seeds list (e.g. average shortest
path length (Kamps et al, 2004)). However hit-
ting time has been shown to be more efficient and
more accurate (Hassan and Radev, 2010) because it
measures connectivity rather than distance. For ex-
ample, the length of the shortest path between the
words ?good? and ?bad? is only 5 (Kamps et al,
2004).
4 Experiments
4.1 Data
We used Wordnet (Miller, 1995) as a source of syn-
onyms and hypernyms for linking English words in
the word relatedness graph. We used two foreign
languages for our experiments Arabic and Hindi.
Both languages have a Wordnet that was constructed
based on the design the Princeton English Wordnet.
Arabic Wordnet (AWN) (Elkateb, 2006; Black and
Fellbaum, 2006; Elkateb and Fellbaum, 2006) has
17561 unique words and 7822 synsets. The Hindi
Wordnet (Narayan et al, 2002; S. Jha, 2001) has
56,928 unique words and 26,208 synsets.
In addition, we used three lexicons with words la-
beled as either positive or negative. For English, we
used the General Inquirer lexicon (Stone et al, 1966)
as a source of seed labeled words. The lexicon con-
tains 4206 words, 1915 of which are positive and
2291 are negative. For Arabic and Hindi we con-
structed a labeled set of 300 words for each language
0
10
20
30
40
50
60
70
80
90
100
Arabic Hindi
SO-PMI HT-FR HT-FR+EN
Figure 2: Accuracy of the proposed method and baselines
for both Arabic and Hindi.
for use in evaluation. Those sets were labeled by two
native speakers of each language. We also used an
Arabic-English and a Hindi-English dictionaries to
generate Foreign-English links.
4.2 Results and Discussion
We performed experiments on the data described in
the previous section. We compare our results to
two baselines. The first is the SO-PMI method de-
scribed in (Turney and Littman, 2003). This method
is based on finding the semantic association of any
given word to a set of positive and a set of negative
words. It can be calculated as follows:
SO-PMI(w) = log
hitsw,pos ? hitsneg
hitsw,neg ? hitspos
(3)
where w is a word with unknown polarity,
hitsw,pos is the number of hits returned by a com-
mercial search engine when the search query is the
given word and the disjunction of all positive seed
words. hitspos is the number of hits when we
search for the disjunction of all positive seed words.
hitsw,neg and hitsneg are defined similarly. We used
7 positive and 7 negative seeds as described in (Tur-
ney and Littman, 2003).
The second baseline constructs a network of for-
eign words only as described earlier. It uses mean
hitting time to find the semantic association of any
given word. We used 10 fold cross validation for this
experiment. We will refer to this system as HT-FR.
Finally, we build a multilingual network and use
the hitting time as before to predict semantic orien-
595
tation. We used the English words from (Stone et
al., 1966) as seeds and the labeled foreign words
for evaluation. We will refer to this system as
HT-FR + EN.
Figure 2 compares the accuracy of the three meth-
ods for Arabic and Hindi. We notice that the
SO-PMI and the hitting time based methods per-
form poorly on both Arabic and Hindi. This is
clearly evident when we consider that the accuracy
of the two systems on English was 83% and 93% re-
spectively (Turney and Littman, 2003; Hassan and
Radev, 2010). This supports our hypothesis that
state of the art methods, designed for English, per-
form poorly on foreign languages due to the limited
amount of resources available in foreign languages
compared to English. The figure also shows that the
proposed method, which combines resources from
both English and foreign languages, performs sig-
nificantly better. Finally, we studied how much im-
provement is achieved by including links between
foreign words from global Wordnets. We found out
that it improves the performance by 2.5% and 4%
for Arabic and Hindi respectively.
5 Conclusions
We addressed the problem of predicting the seman-
tic orientation of foreign words. All previous work
on this task has almost exclusively focused on En-
glish. Applying off-the-shelf methods developed for
English to other languages does not work well be-
cause of the limited amount of resources available
in foreign languages compared to English. We pro-
posed a method based on the construction of a multi-
lingual network that uses both language specific re-
sources as well as the rich semantic relations avail-
able in English. We then use a model that computes
the mean hitting time to a set of positive and neg-
ative seed words to predict whether a given word
has a positive or a negative semantic orientation.
We showed that the proposed method can predict
semantic orientation with high accuracy. We also
showed that it outperforms state of the art methods
limited to using language specific resources.
Acknowledgments
This research was funded in part by the Office
of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab. All
statements of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the ofcial views or poli-
cies of IARPA, the ODNI or the U.S. Government.
References
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Elkateb S. Rodriguez H Alkhalifa M. Vossen P. Pease A.
Black, W. and C. Fellbaum. 2006. Introducing the
arabic wordnet project. In Third International Word-
Net Conference.
Black. W. Rodriguez H Alkhalifa M. Vossen P. Pease A.
Elkateb, S. and C. Fellbaum. 2006. Building a word-
net for arabic. In Fifth International Conference on
Language Resources and Evaluation.
Black W. Vossen P. Farwell D. Rodrguez H. Pease A.
Alkhalifa M. Elkateb, S. 2006. Arabic wordnet and
the challenges of arabic. In Arabic NLP/MT Confer-
ence.
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD?04, pages 168?177.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In National Insti-
tute for, pages 1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP?06, pages 355?363.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39?41.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and
Toshikazu Fukushima. 2002. Mining product reputa-
tions on the web. In KDD?02, pages 341?349.
596
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande,
and P. Bhattacharyya. 2002. An experience in build-
ing the indo wordnet - a wordnet for hindi. In First
International Conference on Global WordNet.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03: Proceedings of the 2nd
international conference on Knowledge capture, pages
70?77.
J. Norris. 1997. Markov chains. Cambridge University
Press.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In HLT-
EMNLP?05, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
P. Pande P. Bhattacharyya S. Jha, D. Narayan. 2001. A
wordnet for hindi. In International Workshop on Lexi-
cal Resources in Natural Language Processing.
Philip Stone, Dexter Dunphy, Marchall Smith, and Daniel
Ogilvie. 1966. The general inquirer: A computer ap-
proach to content analysis. The MIT Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In ACL?02, pages 417?424.
P. Vossen. 1997. Eurowordnet: a multilingual database
for information retrieval. In DELOS workshop on
Cross-language Information Retrieval.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03, pages 129?136.
597
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 121?126,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Clairlib: A Toolkit for Natural Language Processing, Information Retrieval,
and Network Analysis
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department and
School of Information
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In this paper we present Clairlib, an open-
source toolkit for Natural Language Process-
ing, Information Retrieval, and Network Anal-
ysis. Clairlib provides an integrated frame-
work intended to simplify a number of generic
tasks within and across those three areas. It
has a command-line interface, a graphical in-
terface, and a documented API. Clairlib is
compatible with all the common platforms and
operating systems. In addition to its own func-
tionality, it provides interfaces to external soft-
ware and corpora. Clairlib comes with a com-
prehensive documentation and a rich set of tu-
torials and visual demos.
1 Introduction
The development of software packages and code li-
braries that implement algorithms and perform tasks
in scientific areas is of great advantage for both re-
searchers and educators. The availability of these
tools saves the researchers a lot of the time and the
effort needed to implement the new approaches they
propose and conduct experiments to verify their hy-
potheses. Educators also find these tools useful in
class demonstrations and for setting up practical pro-
gramming assignments and projects for their stu-
dents.
A large number of systems have been developed
over the years to solve problems and perform tasks
in Natural Language Processing, Information Re-
trieval, or Network Analysis. Many of these sys-
tems perform specific tasks such as parsing, Graph
Partitioning, co-reference resolution, web crawling
etc. Some other systems are frameworks for per-
forming generic tasks in one area of focus such as
NLTK (Bird and Loper, 2004) and GATE (Cun-
ningham et al, 2002) for Natural Language Pro-
cessing; Pajek (Batagelj and Mrvar, 2003) and
GUESS (Adar, 2006) for Network Analysis and Vi-
sualization; and Lemur1 for Language Modeling and
Information Retrieval.
This paper presents Clairlib, an open-source
toolkit that contains a suit of modules for generic
tasks in Natural Language Processing (NLP), Infor-
mation Retrieval (IR), and Network Analysis (NA).
While many systems have been developed to address
tasks or subtasks in one of these areas as we have
just mentioned, Clairlib provides one integrated en-
vironment that addresses tasks in the three areas.
This makes it useful for a wide range of applications
within and across the three domains.
Clairlib is designed to meet the needs of re-
searchers and educators with varying purposes and
backgrounds. For this purpose, Clairlib provides
three different interfaces to its functionality: a
graphical interface, a command-line interface, and
an application programming interface (API).
Clairlib is developed and maintained by the Com-
putational Linguistics and Information Retrieval
(CLAIR) group at the University of Michigan. The
first version of Clairlib was released in the year
2007. It has been heavily developed since then until
it witnessed a qualitative leap by adding the Graphi-
cal Interface and many new features to the latest ver-
sion that we are presenting here.
Clairlib core modules are written in Perl. The
GUI was written in Java. The Perl back-end and the
Java front-end are efficiently tied together through a
communication module. Clairlib is compatible with
1http://www.lemurproject.org/
121
all the common platforms and operating systems.
The only requirements are a Perl interpreter and Java
Runtime Environment (JRE).
Clairlib has been used in several research projects
to implement systems and conduct experiments. It
also has been used in several academic courses.
The rest of this paper is organized as follows. In
Section 2, we describe the structure of Clairlib. In
Section 3, we present its functionality. Section 4
presents some usage examples. We conclude in Sec-
tion 5.
2 System Overview
Clairlib consists of three main components: the core
library, the command-line interface, and the graph-
ical user interface. The three components were de-
signed and connected together in a manner that aims
to achieve simplicity, integration, and ease of use. In
the following subsections, we briefly describe each
of the three components.
2.1 Modules
The core of Clairlib is a collection of more than 100
modules organized in a shallow hierarchy, each of
which performs a specific task or implements a cer-
tain algorithm. A set of core modules define the data
structures and perform the basic processing tasks.
For example, Clair::Document defines a data struc-
ture for holding textual data in various formats, and
performs the basic text processing tasks such as tok-
enization, stemming, tag stripping, etc.
Another set of modules perform more specific
tasks in the three areas of focus (NLP, IR, and NA).
For example, Clair::Bio::GIN::Interaction is de-
voted to protein-protein interaction extraction from
biomedical text.
A third set contains modules that interface Clair-
lib to external tools. For example, Clair::Utils::Parse
provides an interface to Charniak parser (Charniak,
2000), Stanford parser (Klein and Manning, 2003),
and Chunklink2.
Each module has a well-defined API. The API is
oriented to developers to help them write applica-
tions and build systems on top of Clairlib modules;
and to researchers to help them write applications
and setup custom experiments for their research.
2http://ilk.uvt.nl/team/sabine/chunklink/README.html
2.2 Command-line Interface
The command-line interface provides an easy access
to many of the tasks that Clairlib modules imple-
ment. It provides more than 50 different commands.
Each command is documented and demonstrated in
one or more tutorials. The function of each com-
mand can be customized by passing arguments with
the command. For example, the command
partition.pl -graph graph.net -method GirvanNewman -n 4
uses the GrivanNewman algorithm to divide a
given graph into 4 partitions.
2.3 Graphical User Interface
The graphical user interface (GUI) is an impor-
tant feature that has been recently added to Clairlib
and constituted a quantum leap in its development.
The main purpose of the GUI is to make the rich
set of Clairlib functionalities easier to access by a
larger number of users from various levels and back-
grounds especially students and users with limited or
no programming experience.
It is also intended to help students do their assign-
ments, projects, and research experiments in an in-
teractive environment. We believe that visual tools
facilitate understanding and make learning a more
enjoyable experience for many students. Focusing
on this purpose, the GUI is tuned for simplicity and
ease of use more than high computational efficiency.
Therefore, while it is suitable for small and medium
scale projects, it is not guaranteed to work efficiently
for large projects that involve large datasets and re-
quire heavy processing. The command-line inter-
face is a better choice for large projects.
The GUI consists of three components: the Net-
work Editor/Visualizer/Analyzer, the Text Proces-
sor, and the Corpus Processor. The Network com-
ponent allows the user to 1) build a new network
using a set of drawing and editing tools, 2) open
existing networks stored in files in several different
formats, 3) visualize a network and interact with it,
4) compute different statistics for a network such as
diameter, clustering coefficient, degree distribution,
etc., and 5) perform several operations on a network
such as random walk, label propagation, partition-
ing, etc. This component uses the open source li-
brary, JUNG3 to visualize networks. Figure 1 shows
3http://jung.sourceforge.net/
122
Figure 1: A screenshot for the network visualization component of Clairlib
a screenshot for the Network Visualizer.
The Text Processing component allows users to
process textual data published on the internet or im-
ported from a file stored on the disk. It can process
data in plain, html, or PDF format. Most of the text
processing capabilities implemented in Clairlib core
library are available through this component. Fig-
ure 2 shows a screenshot of the text processing com-
ponent.
The Corpus Processing component allows users
to build a corpus of textual data out of a collection
of files in plain, HTML, or PDF format; or by crawl-
ing a website. Several tasks could be performed on
a corpus such as indexing, querying, summarization,
information extraction, hyperlink network construc-
tion, etc.
Although these components can be run indepen-
dently, they are very integrated and designed to eas-
ily interact with each other. For example, a user can
crawl a website using the Corpus component, then
switch to the Text Processing component to extract
the text from the web documents and stem all the
words, then switch back to the Corpus component
to build a document similarity graph. The graph can
then be taken to the Network component to be visu-
alized and analyzed.
2.4 Documentation
Clairlib comes with an extensive documentation.
The documentation contains the installation infor-
mation for different platforms, a description of all
Clairlib components and modules, and a lot of usage
examples. In addition to this documentation, Clair-
lib provides three other resources:
API Reference
The API Reference provides a complete descrip-
tion of each module in the library. It describes each
subroutine, the task it performs, the arguments it
takes, the value it returns, etc. This reference is use-
ful for developers who want to use Clairlib modules
in their own applications and systems. The API Ref-
erence is published on the internet.
Tutorials
Tutorials teach users how to use Clairlib by ex-
amples. Each tutorial addresses a specific task and
provides a set of instructions to complete the task
using Clairlib command-line tools or its API.
Visual Demos
Visual demos target the users of the graphical in-
terface. The demos visually show how to start the
GUI and how to use its components to perform sev-
eral tasks.
123
Figure 2: A screenshot for the text processing component of Clairlib
3 Functionality
Clairlib provides modules and tools for a broad spec-
trum of tasks. Most of the functionalities are native
to Clairlib. Some functionalities, however, are im-
ported from other open-source packages or external
software. This section lists the main functionalities
categorized by their areas.
3.1 Natural Language Processing
NLP functionalities include Tokenization, Sen-
tence Segmentation, Stemming, HTML Tags Strip-
ping, Syntactic Parsing, Dependency Parsing,
Part-of-Speech Tagging, Document Classification,
LexRank, Summarization, Synthetic Corpus Gen-
eration, N-grams Extraction, XML Parsing, XML
Tree Building, Text Similarity, Political Text Analy-
sis, and Protein Name Tagging.
3.2 Information Retrieval
IR functionalities include Web Crawling, Indexing,
TF-IDF, PageRank, Phrase Based Retrieval, Fuzzy
OR Queries, Latent Semantic Indexing, Web Search,
Automatic Link Extraction, and Protein-Protein In-
teraction Extraction.
3.3 Network Analysis
Network Analysis functionalities include Network
Statistics, Random Network Generation, Network
Visualization, Network Partitioning, Community
Finding, Random Walks, Flow Networks, Signed
Networks, and Semi-supervised Graph-based Clas-
sification. Network Statistics include Centralities,
Clustering Coefficient, Shortest Paths, Diameter,
Triangles, Triplets, etc.
Some of these functionalities are implemented us-
ing several approaches. For example, Clairlib have
implementations for 5 graph partitioning algorithms.
This makes Clairlib a useful tool for conducting ex-
periments for comparative studies.
4 Uses of Clairlib
The diverse set of domains that Clairlib covers and
the different types of interfaces it provides make it
suitable for use in many contexts. In this section, we
highlight some of its uses.
Education
Clairlib contains visual tools that instructors can use
to do class demonstrations to help their students un-
derstand the basic concepts and the algorithms they
face during their study. For example, the random
walk simulator can be used to teach the students how
random walk works by showing a sample network
and then walk randomly step-by-step through it and
show the students how the probabilities change after
each step.
It can also be used to create assignments of vary-
ing levels of difficulty and different scopes. Instruc-
124
tors may ask their students to do experiments with a
dataset using Clairlib, write applications that use the
API, extend an existing module, or contribute new
modules to Clairlib. One example could be to ask
the students to a build a simple information retrieval
system that indexes a collection of documents and
executes search queries on it.
Clairlib has been used to create assignments and
projects in NLP and IR classes at the University of
Michigan and Columbia University. The experience
was positive for both the instructors and the stu-
dents. The instructors were able to design assign-
ments that cover several aspects of the course and
can be done in a reasonable amount of time. The stu-
dents used the API to accomplish their assignments
and projects. This helped them focus on the impor-
tant concepts rather than diving into fine program-
ming details.
Research
Clairlib contains implementations for many algo-
rithms and approaches that solve common problems.
It also comes with a number of corpora and anno-
tated datasets. This makes it a good resource for re-
searchers to build systems and conduct experiments.
Clairlib was successfully used in several research
projects. Examples include Political Text Analy-
sis (Hassan et al, 2008), Scientific Paper Summa-
rization (Qazvinian and Radev, 2009), Blog Net-
works Analysis (Hassan et al, 2009), Protein In-
teraction Extraction (Ozgur and Radev, 2009),
and Citation-Based Summarization (Abu-Jbara and
Radev, 2011).
4.1 Examples
In this subsection, we present some examples where
Clairlib has been used.
Example: Protein-Protein Interaction
Extraction
This is an example of a project that builds an
information extraction system and uses Clairlib as
its main processing component (Ozgur and Radev,
2009). This system is now part of a larger bioinfor-
matics project, NCIBI.
The system uses Clairlib to process a biomedical
article: 1) splits it into sentences using the segmen-
tation module, 2) parses each sentence using the in-
terface to the Stanford Dependency Parser, 3) tags
the protein names, 4) extracts protein-protein inter-
actions using a specific Clairlib module devoted to
this task, and then 5) it builds a protein interaction
network in which nodes are proteins and edges rep-
resent interaction relations. Figure 3 shows an ex-
ample protein interaction network extracted from the
abstracts of a collection of biomedical articles from
PubMed. This network is then analyzed to compute
node centralities and the basic network statistics.
Example: Scientific Paper Summarization Using
Citation Networks
This is an example of a research work that
used Clairlib to implement an approach and con-
duct experiments to support the research hypothe-
sis. Qazvinian and Radev (2009) used Clairlib to
implement their method for citation-based summa-
rization. Given a set of sentences that cite a paper,
they use Clairlib to 1) construct a cosine similarity
network out of these sentences, 2) find communities
of similar sentences using Clairlib community find-
ing module, 3) run Clairlib LexRank module to rank
the sentences, 4) extract the sentence with the high-
est rank from each community, and finally 5) return
the set of extracted sentences as a summary para-
graph.
Example: Text Classification
This is an example of a teaching assignment that
was used in an introductory course on information
retrieval at the University of Michigan. Students
were given the 20-newsgroups corpus (a large set
of news articles labeled by their topic and split into
training and testing sets) and were asked to use
Clairlib API to: 1) stem the text of the documents,
2) convert each document into a feature vector based
on word frequencies, 2) train a multi-class Percep-
tron or Naive Bayes classifier on the documents in
the training set, and finally 3) classify the documents
in the testing set using the trained classifier.
5 Conclusions
Clairlib is a broad-coverage toolkit for Natural Lan-
guage Processing, Information Retrieval, and Net-
work Analysis. It provides a simple, integrated, in-
teractive, and extensible framework for education
and research uses. It provides an API, a command-
125
Figure 3: Clairlib used to construct and analyze a protein network extracted from biomedical articles
line interface, and graphical user interface for the
convenience of users with varying purposes and
backgrounds. Clairlib is well-documented, easy to
learn, and simple to use. It has been tested for vari-
ous types of tasks in various environments.
Clairlib is an open source project and we welcome
all the contributions. Readers who are interested in
contributing to Clairlib are encouraged to contact the
authors.
Acknowledgements
We would like to thank Mark Hodges, Anthony
Fader, Mark Joseph, Joshua Gerrish, Mark Schaller,
Jonathan dePeri, Bryan Gibson, Chen Huang, Arzu-
can Ozgur, and Prem Ganeshkumar who contributed
to the development of Clairlib.
This work was supported in part by grants
R01-LM008106 and U54-DA021519 from the US
National Institutes of Health, U54 DA021519,
IDM 0329043, DHB 0527513, 0534323, and
0527513 from the National Science Foundation, and
W911NF-09-C-0141 from IARPA.
References
R. Gaizauskas, P. J. Rodgers and K. Humphreys 2001.
Visual Tools for Natural Language Processing. Jour-
nal of Visual Languages and Computing, Volume 12,
Issue 4, Pages 375-412.
Arzucan Ozgor and Dragomir Radev 2009. Supervised
classification for extracting biomedical events. Pro-
ceedings of the BioNLP?09 Workshop Shared Task on
Event Extraction at NAACL-HLT, Boulder, Colorado,
USA, pages 111-114
Ahmed Hassan, Dragomir R. Radev, Junghoo Cho, Am-
ruta Joshi. 2009. Content Based Recommendation
and Summarization in the Blogosphere. ICWSM-
2009.
Vahed Qazvinian, Dragomir Radev. 2008. Scientific
Paper Summarization Using Citation Summary Net-
works. COLING 2008.
Ahmed Hassan, Anthony Fader, Michael Crespin, Kevin
Quinn, Burt Monroe, Michael Colaresi and Dragomir
Radev. 2008. Tracking the Dynamic Evolution of Par-
ticipants Salience in a Discussion. COLING 2008.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. Proceedings of NAACL-2000.
Dan Klein and Christopher Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings of ACL-2003.
Amjad Abu-Jbara and Dragomir Radev 2011. Coher-
ent Citation-based Summarization of Scientific Papers
Proceedings of ACL-2011.
H. Cunningham and D. Maynard and K. Bontcheva and
V. Tablan 2002. GATE: A Framework and Graphical
Development Environment for Robust NLP Tools and
Applications Proceedings of ACL-2002, Philadelphia.
Steven Bird and Edward Loper. 2004. NLTK: The Natu-
ral Language Toolkit Proceedings of ACL-2004.
V. Batagelj and A. Mrvar 2003. Pajek - Analysis and
Visualization of Large Networks Springer, Berlin.
Eytan Adar. 2006. GUESS: A Language and Interface
for Graph Exploration CHI 2006.
126
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 399?409,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Subgroup Detection in Ideological Discussions
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Mona Diab
Center for Computational Learning Systems
Columbia University
New York, NY, USA
mdiab@ccls.columbia.edu
Pradeep Dasigi
Department of Computer Science
Columbia University
New York, NY, USA
pd2359@columbia.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
The rapid and continuous growth of social
networking sites has led to the emergence of
many communities of communicating groups.
Many of these groups discuss ideological and
political topics. It is not uncommon that the
participants in such discussions split into two
or more subgroups. The members of each sub-
group share the same opinion toward the dis-
cussion topic and are more likely to agree with
members of the same subgroup and disagree
with members from opposing subgroups. In
this paper, we propose an unsupervised ap-
proach for automatically detecting discussant
subgroups in online communities. We analyze
the text exchanged between the participants of
a discussion to identify the attitude they carry
toward each other and towards the various as-
pects of the discussion topic. We use attitude
predictions to construct an attitude vector for
each discussant. We use clustering techniques
to cluster these vectors and, hence, determine
the subgroup membership of each participant.
We compare our methods to text clustering
and other baselines, and show that our method
achieves promising results.
1 Introduction
Online forums discussing ideological and political
topics are common1. When people discuss a dis-
puted topic they usually split into subgroups. The
members of each subgroup carry the same opinion
1www.politicalforum.com, www.createdebate.com,
www.forandagainst.com, etc
toward the discission topic. The member of a sub-
group is more likely to show positive attitude to the
members of the same subgroup, and negative atti-
tude to the members of opposing subgroups.
For example, let us consider the following two
snippets from a debate about the enforcement of a
new immigration law in Arizona state in the United
States:
(1) Discussant 1: Arizona immigration law is good.
Illegal immigration is bad.
(2) Discussant 2: I totally disagree with you. Ari-
zona immigration law is blatant racism, and quite
unconstitutional.
In (1), the writer is expressing positive attitude
regarding the immigration law and negative attitude
regarding illegal immigration. The writer of (2) is
expressing negative attitude towards the writer of
(1) and negative attitude regarding the immigration
law. It is clear from this short dialog that the writer
of (1) and the writer of (2) are members of two
opposing subgroups. Discussant 1 is supporting the
new law, while Discussant 2 is against it.
In this paper, we present an unsupervised ap-
proach for determining the subgroup membership of
each participant in a discussion. We use linguistic
techniques to identify attitude expressions, their po-
larities, and their targets. The target of attitude could
be another discussant or an entity mentioned in the
discussion. We use sentiment analysis techniques
to identify opinion expressions. We use named en-
399
tity recognition and noun phrase chunking to iden-
tify the entities mentioned in the discussion. The
opinion-target pairs are identified using a number of
syntactic and semantic rules.
For each participant in the discussion, we con-
struct a vector of attitude features. We call this vec-
tor the discussant attitude profile. The attitude pro-
file of a discussant contains an entry for every other
discussant and an entry for every entity mentioned
in the discission. We use clustering techniques to
cluster the attitude vector space. We use the clus-
tering results to determine the subgroup structure of
the discussion group and the subgroup membership
of each participant.
The rest of this paper is organized as follows. Sec-
tion 2 examines the previous work. We describe the
data used in the paper in Section 2.4. Section 3
presents our approach. Experiments, results and
analysis are presented in Section 4. We conclude
in Section 5
2 Related Work
2.1 Sentiment Analysis
Our work is related to a huge body of work on sen-
timent analysis. Previous work has studied senti-
ment in text at different levels of granularity. The
first level is identifying the polarity of individual
words. Hatzivassiloglou and McKeown (1997) pro-
posed a method to identify the polarity of adjec-
tives based on conjunctions linking them. Turney
and Littman (2003) used pointwise mutual infor-
mation (PMI) and latent semantic analysis (LSA)
to compute the association between a given word
and a set of positive/negative seed words. Taka-
mura et al (2005) proposed using a spin model to
predict word polarity. Other studies used Word-
Net to improve word polarity prediction (Hu and
Liu, 2004a; Kamps et al, 2004; Kim and Hovy,
2004; Andreevskaia and Bergler, 2006). Hassan
and Radev (2010) used a random walk model built
on top of a word relatedness network to predict the
semantic orientation of English words. Hassan et
al. (2011) proposed a method to extend their random
walk model to assist word polarity identification in
other languages including Arabic and Hindi.
Other work focused on identifying the subjectiv-
ity of words. The goal of this work is to deter-
mine whether a given word is factual or subjective.
We use previous work on subjectivity and polar-
ity prediction to identify opinion words in discus-
sions. Some of the work on this problem classi-
fies words as factual or subjective regardless of their
context (Wiebe, 2000; Hatzivassiloglou and Wiebe,
2000; Banea et al, 2008). Some other work no-
ticed that the subjectivity of a given word depends
on its context. Therefor, several studies proposed
using contextual features to determine the subjec-
tivity of a given word within its context (Riloff and
Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Na-
sukawa and Yi, 2003; Popescu and Etzioni, 2005).
The second level of granularity is the sentence
level. Hassan et al (2010) presents a method for
identifying sentences that display an attitude from
the text writer toward the text recipient. They de-
fine attitude as the mental position of one partici-
pant with regard to another participant. A very de-
tailed survey that covers techniques and approaches
in sentiment analysis and opinion mining could be
found in (Pang and Lee, 2008).
2.2 Opinion Target Extraction
Several methods have been proposed to identify
the target of an opinion expression. Most of the
work have been done in the context of product re-
views mining (Hu and Liu, 2004b; Kobayashi et
al., 2007; Mei et al, 2007; Stoyanov and Cardie,
2008). In this context, opinion targets usually refer
to product features (i.e. product components or at-
tributes, as defined by Liu (2009)). In the work of
Hu and Liu (2004b), they treat frequent nouns and
noun phrases as product feature candidates. In our
work, we extract as targets frequent noun phrases
and named entities that are used by two or more dif-
ferent discussants. Scaffidi et al (2007) propose a
language model approach to product feature extrac-
tion. They assume that product features are men-
tioned more often in product reviews than they ap-
pear in general English text. However, such statistics
may not be reliable when the corpus size is small.
In another related work, Jakob and
Gurevych (2010) showed that resolving the
anaphoric links in the text significantly improves
opinion target extraction. In our work, we use
anaphora resolution to improve opinion-target
400
Participant A posted: I support Arizona because they have every right to do so. They are just upholding well-established
federal law. All states should enact such a law.
Participant B commented on A?s
post:
I support the law because the federal government is either afraid or indifferent to the issue. Arizona
has the right and the responsibility to protect the people of the State of Arizona. If this requires a
possible slight inconvenience to any citizen so be it.
Participant C commented on B?s
post:
That is such a sad thing to say. You do realize that under the 14th Amendment, the very interaction
of a police officer asking you to prove your citizenship is Unconstitutional? As soon as you start
trading Constitutional rights for ?security?, then you?ve lost.
Table 1: Example posts from the Arizona Immigration Law thread
pairing as shown in Section 3 below.
2.3 Community Mining
Previous work also studied community mining in so-
cial media sites. Somasundaran and Wiebe (2009)
presents an unsupervised opinion analysis method
for debate-side classification. They mine the web
to learn associations that are indicative of opinion
stances in debates and combine this knowledge with
discourse information. Anand et al (2011) present
a supervised method for stance classification. They
use a number of linguistic and structural features
such as unigrams, bigrams, cue words, repeated
punctuation, and opinion dependencies to build a
stance classification model. This work is limited to
dual sided debates and defines the problem as a clas-
sification task where the two debate sides are know
beforehand. Our work is characterized by handling
multi-side debates and by regarding the problem as
a clustering problem where the number of sides is
not known by the algorithm. This work also uti-
lizes only discussant-to-topic attitude predictions for
debate-side classification. Out work utilizes both
discussant-to-topic and discussant-to-discussant at-
titude predictions.
In another work, Kim and Hovy (2007) predict
the results of an election by analyzing discussion
threads in online forums that discuss the elections.
They use a supervised approach that uses unigrams,
bigrams, and trigrams as features. In contrast, our
work is unsupervised and uses different types infor-
mation. Moreover, although this work is related to
ours at the goal level, it does not involve any opinion
analysis.
Another related work classifies the speakers side
in a corpus of congressional floor debates, using
the speakers final vote on the bill as a labeling
for side (Thomas et al, 2006; Bansal et al, 2008;
Yessenalina et al, 2010). This work infers agree-
ment between speakers based on cases where one
speaker mentions another by name, and a simple al-
gorithm for determining the polarity of the sentence
in which the mention occurs. This work shows that
even with the resulting sparsely connected agree-
ment structure, the MinCut algorithm can improve
over stance classification based on textual informa-
tion alone. This work also requires that the de-
bate sides be known by the algorithm and it only
identifies discussant-to-discussant attitude. In our
experiments below we show that identifying both
discussant-to-discussant and discussant-to-topic at-
titudes achieves better results.
2.4 Data
In this section, we describe the datasets used in
this paper. We use three different datasets. The
first dataset (politicalforum, henceforth) consists of
5,743 posts collected from a political forum2. All
the posts are in English. The posts cover 12 dis-
puted political and ideological topics. The discus-
sants of each topic were asked to participate in a
poll. The poll asked them to determine their stance
on the discussion topic by choosing one item from a
list of possible arguments. The list of participants
who voted for each argument was published with
the poll results. Each poll was accompanied by a
discussion thread. The people who participated in
the poll were allowed to post text to that thread to
justify their choices and to argue with other partic-
ipants. We collected the votes and the discussion
thread of each poll. We used the votes to identify
the subgroup membership of each participant.
The second dataset (createdebate, henceforth)
comes from an online debating site 3. It consists of
2http://www.politicalforum.com
3http://www.createdebate.com
401
Source Topic Question #Sides #Posts #Participants
Politicalforum
Arizona Immigration Law Do you support Arizona in its decision to enact their
Immigration Enforcement law?
2 738 59
Airport Security Should we pick muslims out of the line and give ad-
ditional scrutiny/screening?
4 735 69
Vote for Obama Will you vote for Obama in the 2012 Presidential
elections?
2 2599 197
Createdebate
Evolution Has evolution been scientifically proved? 2 194 98
Social networking sites It is easier to maintain good relationships in social
networking sites such as Facebook.
2 70 31
Abortion Should abortion be banned 3 477 70
Wikipedia
Ireland Misleading description of Irland island partition 3 40 10
South Africa Goverment Was the current form of South African government
born in May 1910?
3 23 5
Oil Spill Obama?s response to gulf oil spill 3 30 12
Table 2: Example threads from our three datasets
30 debates containing a total of 2,712 posts. Each
debate is about one topic. The description of each
debate states two or more positions regarding the de-
bate topic. When a new participant enters the discus-
sion, she explicitly picks a position and posts text to
support it, support a post written by another partici-
pant who took the same position, or to dispute a post
written by another participant who took an opposing
position. We collected the discussion thread and the
participant positions for each debate.
The third dataset (wikipedia, henceforth) comes
from the Wikipedia4 discussion section. When a
topic on Wikipedia is disputed, the editors of that
topic start a discussion about it. We collected 117
Wikipeida discussion threads. The threads contains
a total of 1,867 posts.
The politicalforum and createdebate datasets are
self labeled as described above. To annotate the
Wikipedia data, we asked an expert annotator (a
professor in sociolinguistics who is not one of the
authors) to read each of the Wikipedia discussion
threads and determine whether the discussants split
into subgroups in which case he was asked to deter-
mine the subgroup membership of each discussant.
Table 2 lists few example threads from our three
datasets. Table 1 shows a portion of discussion
thread between three participants about enforcing a
new immigration law in Arizona. This thread ap-
peared in the polictalforum dataset. The text posted
by the three participants indicates that A?s position
4http://www.wikipedia.com
is with enforcing the law, that B agrees with A, and
that C disagrees with both. This means that A and B
belong to the same opinion subgroup, while belongs
to an opposing subgroup.
We randomly selected 6 threads from our datasets
(2 from politicalforum, 2 from createdebate, and 2
from Wikipedia) and used them as development set.
This set was used to develop our approach.
3 Approach
In this section, we describe a system that takes a
discussion thread as input and outputs the subgroup
membership of each discussant. Figure 1 illustrates
the processing steps performed by our system to de-
tect subgroups. In the following subsections we de-
scribe the different stages in the system pipeline.
3.1 Thread Parsing
We start by parsing the thread to identify posts, par-
ticipants, and the reply structure of the thread (i.e.
who replies to whom). In the datasets described in
Section 2.4, all this information was explicitly avail-
able in the thread. We tokenize the text of each post
and split it into sentences using CLAIRLib (Abu-
Jbara and Radev, 2011).
3.2 Opinion Word Identification
The next step is to identify the words that express
opinion and determine their polarity (positive or
negative). Lehrer (1974) defines word polarity as
the direction the word deviates to from the norm. We
402
use OpinionFinder (Wilson et al, 2005a) to identify
polarized words and their polarities.
The polarity of a word is usally affected by
the context in which it appears. For example, the
word fine is positive when used as an adjective and
negative when used as a noun. For another example,
a positive word that appears in a negated context
becomes negative. OpinionFinder uses a large set of
features to identify the contextual polarity of a given
polarized word given its isolated polarity and the
sentence in which it appears (Wilson et al, 2005b).
Snippet (3) below shows the result of applying this
step to snippet (1) above (O means neutral; POS
means positive; NEG means negative).
(3) Arizona/O Immigration/O law/O good/POS ./O
Illegal/O immigration/O bad/NEG ./O
3.3 Target Identification
The goal of this step is to identify the possible tar-
gets of opinion. A target could be another discus-
sant or an entity mentioned in the discussion. When
the target of opinion is another discussant, either the
discussant name is mentioned explicitly or a second
person pronoun is used to indicate that the opinion
is targeting the recipient of the post. For example,
in snippet (2) above the second person pronoun you
indicates that the opinion word disagree is targeting
Discussant 1, the recipient of the post.
The target of opinion can also be an entity
mentioned in the discussion. We use two methods to
identify such entities. The first method uses shallow
parsing to identify noun groups (NG). We use the
Edinburgh Language Technology Text Tokenization
Toolkit (LT-TTT) (Grover et al, 2000) for this pur-
pose. We consider as an entity any noun group that
is mentioned by at least two different discussants.
We replace each identified entity with a unique
placeholder (ENTITYID). For example, the noun
group Arizona immigration law is mentioned by
Discussant 1 and Discussant 2 in snippets 1 and 2
above respectively. Therefore, we replace it with a
placehold as illustrated in snippets (4) and (5) below.
(4) Discussant 1: ENTITY1 is good. Illegal im-
NER NP Chunking
Barack Obama the Republican nominee
Middle East the maverick economists
Bush conservative ideologues
Bob McDonell the Nobel Prize
Iraq Federal Government
Table 3: Some of the entities identified using NER and
NP Chunking in a discussion thread about the US 2012
elections
migration is bad.
(5) Discussant 2: I totally disagree with you. ENTITY1
is blatant racism, and quite unconstitutional.
We only consider as entities noun groups that
contain two words or more. We impose this require-
ment because individual nouns are very common
and regarding all of them as entities will introduce
significant noise.
In addition to this shallow parsing method, we
also use named entity recognition (NER) to identify
more entities. We use the Stanford Named Entity
Recognizer (Finkel et al, 2005) for this purpose. It
recognizes three types of entities: person, location,
and organization. We impose no restrictions on the
entities identified using this method. Again, we re-
place each distinct entity with a unique placeholder.
The final set of entities identified in a thread is the
union of the entities identified by the two aforemen-
tioned methods. Table 3
Finally, a challenge that always arises when
performing text mining tasks at this level of gran-
ularity is that entities are usually expressed by
anaphorical pronouns. Previous work has shown
that For example, the following snippet contains
an explicit mention of the entity Obama in the first
sentence, and then uses a pronoun to refer to the
same entity in the second sentence. The opinion
word unbeatable appears in the second sentence
and is syntactically related to the pronoun He.
In the next subsection, it will become clear why
knowing which entity does the pronoun He refers to
is essential for opinion-target pairing.
(6) It doesn?t matter whether you vote for Obama.
403
Discussion 
Thread 
?.??. 
?.??. 
?.??. 
Opinion Identification 
? Identify polarized words 
? Identify the contextual 
polarity of each word 
 
 
Target Identification 
? Anaphora resolution 
? Identify named entities 
? Identify Frequent noun 
phrases. 
? Identify mentions of 
other discussants 
Opinion-Target Pairing 
? Dependency Rules 
 
 
 
Discussant Attitude 
Profiles (DAPs)  
 
 
 
Clustering 
Subgroups 
 
 
 
 
 
Thread Parsing 
? Identify posts 
? Identify discussants 
? Identify the reply 
structure 
? Tokenize text. 
? Split posts into sentences 
 
Figure 1: An overview of the subgroups detection system
He is unbeatable.
Jakob and Gurevych (2010) showed experi-
mentally that resolving the anaphoric links in the
text significantly improves opinion target extraction.
We use the Beautiful Anaphora Resolution Toolkit
(BART) (Versley et al, 2008) to resolve all the
anaphoric links within the text of each post sepa-
rately. The result of applying this step to snippet (6)
is:
(6) It doesn?t matter whether you vote for Obama.
Obama is unbeatable.
Now, both mentions of Obama will be recog-
nized by the Stanford NER system and will be
identified as one entity.
3.4 Opinion-Target Pairing
At this point, we have all the opinion words and
the potential targets identified separately. The next
step is to determine which opinion word is target-
ing which target. We propose a rule based approach
for opinion-target pairing. Our rules are based on
the dependency relations that connect the words in
a sentence. We use the Stanford Parser (Klein and
Manning, 2003) to generate the dependency parse
tree of each sentence in the thread. An opinion word
and a target form a pair if they stratify at least one
of our dependency rules. Table 4 illustrates some
of these rules 5. The rules basically examine the
types of the dependencies on the shortest path that
connect the opinion word and the target in the de-
pendency parse tree. It has been shown in previous
work on relation extraction that the shortest depen-
dency path between any two entities captures the in-
formation required to assert a relationship between
them (Bunescu and Mooney, 2005).
If a sentence S in a post written by participant
Pi contains an opinion word OPj and a target TRk,
and if the opinion-target pair satisfies one of our de-
pendency rules, we say that Pi expresses an attitude
towards TRk. The polarity of the attitude is deter-
mined by the polarity of OPj . We represent this as
Pi
+
? TRk if OPj is positive and Pi
?
? TRk if OPj
is negative.
It is likely that the same participant Pi express
sentiment toward the same target TRk multiple
times in different sentences in different posts. We
keep track of the counts of all the instances of posi-
tive/negative attitude Pi expresses toward TRk. We
represent this as Pi
m+
???
n?
TRk where m (n) is the
number of times Pi expressed positive (negative) at-
titude toward TRk.
3.5 Discussant Attitude Profile
We propose a representation of discussantsa?ttitudes
towards the identified targets in the discussion
thread. As stated above, a target could be another
discussant or an entity mentioned in the discussion.
5The code will be made publicly available at the time of
publication
404
ID Rule In Words Example
R1 OP ? nsubj ? TR The target TR is the nominal subject of the opinion
word OP
ENTITY1TR is goodOP .
R2 OP ? dobj ? TR The target T is a direct object of the opinion OP I hateOP ENTITY2TR
R3 OP ? prep ? ? TR The target TR is the object of a preposition that
modifies the opinion word OP
I totally disagreeOP with youTR.
R4 TR? amod? OP The opinion is an adjectival modifier of the target The badOP ENTITY3TR is spreading lies
R5 OP ? nsubjpass? TR The target TR is the nominal subject of the passive
opinion word OP
ENTITY4TR is hatedOP by everybody.
R6 OP ? prep ? ? poss? TR The opinion word OP connected through a prep ?
relation as in R2 to something possessed by the
target TR
The main flawOP in yourTR analysis is
that it?s based on wrong assumptions.
R7 OP ? dobj ? poss? TR The target TR possesses something that is the direct
object of the opinion word OP
I likeOP ENTITY5TR?s brilliant ideas.
R8 OP ? csubj ? nsubj ? TR The opinon word OP is a causal subject of a phrase
that has the target TR as its nominal subject
What ENTITY6TR announced was
misleadingOP .
Table 4: Examples of the dependency rules used for opinion-target pairing.
Our representation is a vector containing numeri-
cal values. The values correspond to the counts of
positive/negative attitudes expressed by the discus-
sant toward each of the targets. We call this vector
the discussant attitude profile (DAP). We construct a
DAP for every discussant. Given a discussion thread
with d discussants and e entity targets, each attitude
profile vector has n = (d + e) ? 3 dimensions. In
other words, each target (discussant or entity) has
three corresponding values in the DAP: 1) the num-
ber of times the discussant expressed positive atti-
tude toward the target, 2) the number of times the
discussant expressed a negative attitude towards the
target, and 3) the number of times the the discussant
interacted with or mentioned the target. It has to be
noted that these values are not symmetric since the
discussions explicitly denote the source and the tar-
get of each post.
3.6 Clustering
At this point, we have an attitude profile (or vec-
tor) constructed for each discussant. Our goal is to
use these attitude profiles to determine the subgroup
membership of each discussant. We can achieve this
goal by noticing that the attitude profiles of discus-
sants who share the same opinion are more likely to
be similar to each other than to the attitude profiles
of discussants with opposing opinions. This sug-
gests that clustering the attitude vector space will
achieve the goal and split the discussants into sub-
groups according to their opinion.
4 Evaluation
In this section, we present several levels of evalu-
ation of our system. First, we compare our sys-
tem to baseline systems. Second, we study how the
choice of the clustering algorithm impacts the re-
sults. Third, we study the impact of each component
in our system on the performance. All the results
reported in this section that show difference in the
performance are statistically significant at the 0.05
level (as indicated by a 2-tailed paired t-test). Be-
fore describing the experiments and presenting the
results, we first describe the evaluation metrics we
use.
4.0.1 Evaluation Metrics
We use two evaluation metrics to evaluate sub-
groups detection accuracy: Purity and Entropy. To
compute Purity (Manning et al, 2008), each clus-
ter is assigned the class of the majority vote within
the cluster, and then the accuracy of this assignment
is measured by dividing the number of correctly as-
signed members by the total number of instances. It
can be formally defined as:
purity(?, C) =
1
N
?
k
max
j
|?k ? cj | (1)
where ? = {?1, ?2, ..., ?k} is the set of clusters
and C = {c1, c2, ..., cJ} is the set of classes. ?k is
interpreted as the set of documents in ?k and cj as
405
the set of documents in cj . The purity increases as
the quality of clustering improves.
The second metric is Entropy. The Entropy of a
cluster reflects how the members of the k distinct
subgroups are distributed within each resulting clus-
ter; the global quality measure is computed by aver-
aging the entropy of all clusters:
Entropy = ?
j? nj
n
i?
P (i, j)? log2P (i, j)
(2)
where P (i, j) is the probability of finding an ele-
ment from the category i in the cluster j, nj is the
number of items in cluster j, and n the total num-
ber of items in the distribution. In contrast to purity,
the entropy decreases as the quality of clustering im-
proves.
4.1 Comparison to Baseline Systems
We compare our system (DAPC) that was described
in Section 3 to two baseline methods. The first base-
line (GC) uses graph clustering to partition a net-
work based on the interaction frequency between
participants. We build a graph where each node
represents a participant. Edges link participants if
they exchange posts, and edge weights are based on
the number of interactions. We tried two methods
for clustering the resulting graph: spectral partition-
ing (Luxburg, 2007) and a hierarchical agglomera-
tion algorithm which works by greedily optimizing
the modularity for graphs (Clauset et al, 2004).
The second baseline (TC) is based on the premise
that the member of the same subgroup are more
likely to use vocabulary drawn from the same lan-
guage model. We collect all the text posted by each
participant and create a tf-idf representations of the
text in a high dimensional vector space. We then
cluster the vector space to identify subgroups. We
use k-means (MacQueen, 1967) as our clustering
algorithm in this experiment (comparison of vari-
ous clustering algorithms is presented in the next
subsection). The distances between vectors are
Eculidean distances. Table 5 shows that our sys-
tem performs significantly better the baselines on the
three datasets in terms of both the purity (P ) and the
entropy (E) (notice that lower entropy values indi-
cate better clustering). The values reported are the
Method Createdebate Politicalforum Wikipedia
P E P E P E
GC - Spectral 0.50 0.85 0.50 0.88 0.49 0.89
GC - Hierarchical 0.48 0.86 0.47 0.89 0.49 0.87
TC - kmeans 0.51 0.84 0.49 0.88 0.52 0.85
DAPC - kmeans 0.64 0.68 0.61 0.80 0.66 0.55
Table 5: Comparison to baseline systems
Method Createdebate Politicalforum Wikipedia
P E P E P E
DAPC - EM 0.63 0.71 0.61 0.82 0.63 0.61
DAPC - FF 0.63 0.70 0.60 0.83 0.64 0.59
DAPC - kmeans 0.64 0.68 0.61 0.80 0.66 0.55
Table 6: Comparison of different clustering algorithms
average results of the threads of each dataset. We
believe that the baselines performed poorly because
the interaction frequency and the text similarity are
not key factors in identifying subgroup structures.
Many people would respond to people they disagree
with more, while others would mainly respond to
people they agree with most of the time. Also, peo-
ple in opposing subgroups tend to use very similar
text when discussing the same topic and hence text
clustering does not work as well.
4.2 Choice of the clustering algorithm
We experimented with three different clustering al-
gorithms: expectation maximization (EM), and k-
means (MacQueen, 1967), and FarthestFirst (FF)
(Hochbaum and Shmoys, 1985; Dasgupta, 2002).
As we did in the previous subsection, we use
Eculidean distance to measure the distance between
vectors All the system (DAP) components are in-
cluded as described in Section 3. The purity and
entropy values using each algorithm are shown in
Table 6. Although k-means seems to be performing
slightly better than other algorithms, the differences
in the results are not significant. This indicates that
the choice of the clustering algorithm does not have
a noticeable impact on the results. We also exper-
imented with using Manhattan distance and cosine
similarity instead of Euclidean distance to measure
the distance between attitude vectors. We noticed
that the choice of the distance does not have signifi-
cant impact on the results as well.
406
4.3 Component Evaluation
In this subsection, we evaluate the impact of the dif-
ferent components in the pipeline on the system per-
formance. We do that by removing each component
from the pipeline and measuring the change in per-
formance. We perform the following experiments:
1) We run the full system with all its components
included (DAPC). 2) We run the system and in-
clude only discussant-to-discussant attitude features
in the attitude vectors (DAPC-DD). 3) We include
only discussant-to-entity attitude features in the atti-
tude vectors (DAPC-DE). 4) We include only senti-
ment features in the attitude vector; i.e. we exclude
the interaction count features (DAPC-SE). 5) We in-
clude only interaction count features to the attitude
vector; i.e. we exclude sentiment features (DAPC-
INT). 6) We skip the anaphora resolution step in the
entity identification component (DAPC-NO AR). 7)
We only use named entity recognition to identify en-
tity targets; i.e. we exclude the entities identified
through noun phrasing chunking (DAPC-NER). 8)
Finally, we only noun phrase chunking to identify
entity targets (DAPC-NP). In all these experiments
k-means is used for clustering and the number of
clusters is set as explained in the previous subsec-
tion.
The results show that all the components in the
system contribute to better performance of the sys-
tem. We notice from the results that the performance
of the system drops significantly if sentiment fea-
tures are not included. This is result corroborates
our hypothesis that interaction features are not suffi-
cient factors for detecting rift in discussion groups.
Including interaction features improve the perfor-
mance (although not by a big difference) because
they help differentiate between the case where par-
ticipants A and B never interacted with each other
and the case where they interact several time but
never posted text that indicate difference in opin-
ion between them. We also notice that the perfor-
mance drops significantly in DAPC-DD and DAPC-
DD which also supports our hypotheses that both
the sentiment discussants show toward one another
and the sentiment they show toward the aspects of
the discussed topic are important for the task. Al-
though using both named entity recognition (NER)
and noun phrase chunking achieves better results, it
Method Createdebate Politicalforum Wikipedia
P E P E P E
DAPC 0.64 0.68 0.61 0.80 0.66 0.55
DAPC-DD 0.59 0.77 0.57 0.86 0.62 0.61
DAPC-DE 0.60 0.69 0.58 0.84 0.58 0.78
DAPC-SE 0.62 0.70 0.60 0.83 0.61 0.62
DAPC-INT 0.54 0.88 0.52 0.91 0.57 0.85
DAPC-NO AR 0.62 0.72 0.60 0.84 0.64 0.60
DAPC-NER 0.61 0.71 0.58 0.86 0.63 0.59
DAPC-NP 0.63 0.75 0.59 0.84 0.65 0.62
Table 7: Impact of system components on the perfor-
mance
can also be noted from the results that NER con-
tributes more to the system performance. Finally,
the results support Jakob and Gurevych (2010) find-
ings that anaphora resolution aids opinion mining
systems.
5 Conclusions
In this paper, we presented an approach for subgroup
detection in ideological discussions. Our system
uses linguistic analysis techniques to identify the at-
titude the participants of online discussions carry to-
ward each other and toward the aspects of the discus-
sion topic. Attitude prediction as well as interaction
frequency to construct an attitude vector for each
participant. The attitude vectors of discussants are
then clustered to form subgroups. Our experiments
showed that our system outperforms text clustering
and interaction graph clustering. We also studied the
contribution of each component in our system to the
overall performance.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
407
References
Amjad Abu-Jbara and Dragomir Radev. 2011. Clairlib:
A toolkit for natural language processing, information
retrieval, and network analysis. In Proceedings of the
ACL-HLT 2011 System Demonstrations, pages 121?
126, Portland, Oregon, June. Association for Compu-
tational Linguistics.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Workshop
on Computational Approaches to Subjectivity and Sen-
timent Analysis (WASSA 2.011), pages 1?9, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
wordnet for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In EACL?06.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The
power of negative thinking: Exploiting label disagree-
ment in the min-cut classification framework.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 724?731, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Aaron Clauset, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70:066111.
Sanjoy Dasgupta. 2002. Performance guarantees for
hierarchical clustering. In 15th Annual Conference
on Computational Learning Theory, pages 351?363.
Springer.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. Lt ttt - a flexible tokenisation
tool. In In Proceedings of Second International Con-
ference on Language Resources and Evaluation, pages
1147?1154.
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255.
Ahmed Hassan, Amjad AbuJbara, Rahul Jha, and
Dragomir Radev. 2011. Identifying the semantic
orientation of foreign words. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 592?597, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Hochbaum and Shmoys. 1985. A best possible heuristic
for the k-center problem. Mathematics of Operations
Research, 10(2):180?184.
Minqing Hu and Bing Liu. 2004a. Mining and summa-
rizing customer reviews. In KDD?04, pages 168?177.
Minqing Hu and Bing Liu. 2004b. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages 168?
177, New York, NY, USA. ACM.
Niklas Jakob and Iryna Gurevych. 2010. Using anaphora
resolution to improve opinion target identification in
movie reviews. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 263?268, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In National Insti-
tute for, pages 1115?1118.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In COLING, pages 1367?1373.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In IN PROCEEDINGS OF
THE 41ST ANNUAL MEETING OF THE ASSOCIA-
TION FOR COMPUTATIONAL LINGUISTICS, pages
423?430.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL.
Adrienne Lehrer. 1974. Semantic fields and lezical struc-
ture. North Holland, Amsterdam and New York.
408
Bing Liu. 2009. Web Data Mining: Exploring Hyper-
links, Contents, and Usage Data (Data-Centric Sys-
tems and Applications). Springer, 1st ed. 2007. corr.
2nd printing edition, January.
Ulrike Luxburg. 2007. A tutorial on spectral clustering.
Statistics and Computing, 17:395?416, December.
J. B. MacQueen. 1967. Some methods for classification
and analysis of multivariate observations. In L. M. Le
Cam and J. Neyman, editors, Proc. of the fifth Berkeley
Symposium on Mathematical Statistics and Probabil-
ity, volume 1, pages 281?297. University of California
Press.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Pro-
ceedings of the 16th international conference on World
Wide Web, WWW ?07, pages 171?180, New York, NY,
USA. ACM.
Soo min Kim and Eduard Hovy. 2007. Crystal: Ana-
lyzing predictive opinions on the web. In In EMNLP-
CoNLL 2007.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03: Proceedings of the 2nd
international conference on Knowledge capture, pages
70?77.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In HLT-
EMNLP?05, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
226?234, Suntec, Singapore, August. Association for
Computational Linguistics.
Veselin Stoyanov and Claire Cardie. 2008. Topic iden-
tification for fine-grained opinion analysis. In In Col-
ing.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In In Proceedings
of EMNLP, pages 327?335.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-
aofeng Yang, and Alessandro Moschitti. 2008. Bart:
A modular toolkit for coreference resolution. In Pro-
ceedings of the ACL-08: HLT Demo Session, pages
9?12, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: a system for subjectiv-
ity analysis. In Proceedings of HLT/EMNLP on Inter-
active Demonstrations, HLT-Demo ?05, pages 34?35,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05, Vancou-
ver, Canada.
Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010.
Multi-level structured models for document-level sen-
timent classification. In In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03, pages 129?136.
409
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 133?138,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Subgroup Detector: A System for Detecting Subgroups in Online
Discussions
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
We present Subgroup Detector, a system
for analyzing threaded discussions and
identifying the attitude of discussants towards
one another and towards the discussion
topic. The system uses attitude predictions to
detect the split of discussants into subgroups
of opposing views. The system uses an
unsupervised approach based on rule-based
opinion target detecting and unsupervised
clustering techniques. The system is open
source and is freely available for download.
An online demo of the system is available at:
http://clair.eecs.umich.edu/SubgroupDetector/
1 Introduction
Online forums discussing ideological and political
topics are common1. When people discuss a con-
troversial topic, it is normal to see situations of both
agreement and disagreement among the discussants.
It is even not uncommon that the big group of dis-
cussants split into two or more smaller subgroups.
The members of each subgroup have the same opin-
ion toward the discission topic. The member of a
subgroup is more likely to show positive attitude to
the members of the same subgroup, and negative at-
titude to the members of opposing subgroups. For
example, consider the following snippet taken from
a debate about school uniform
1www.politicalforum.com, www.createdebate.com,
www.forandagainst.com, etc
(1) Discussant 1: I believe that school uniform is a
good idea because it improves student attendance.
(2) Discussant 2: I disagree with you. School uniform
is a bad idea because people cannot show their person-
ality.
In (1), the writer is expressing positive attitude
regarding school uniform. The writer of (2) is ex-
pressing negative attitude (disagreement) towards
the writer of (1) and negative attitude with respect
to the idea of school uniform. It is clear from this
short dialog that the writer of (1) and the writer of
(2) are members of two opposing subgroups. Dis-
cussant 1 supports school uniform, while Discussant
2 is against it.
In this demo, we present an unsupervised system
for determining the subgroup membership of each
participant in a discussion. We use linguistic tech-
niques to identify attitude expressions, their polar-
ities, and their targets. We use sentiment analy-
sis techniques to identify opinion expressions. We
use named entity recognition, noun phrase chunk-
ing and coreference resolution to identify opinion
targets. Opinion targets could be other discussants
or subtopics of the discussion topic. Opinion-target
pairs are identified using a number of hand-crafted
rules. The functionality of this system is based on
our previous work on attitude mining and subgroup
detection in online discussions.
This work is related to previous work in the areas
of sentiment analysis and online discussion mining.
Many previous systems studied the problem of iden-
133
tifying the polarity of individual words (Hatzivas-
siloglou and McKeown, 1997; Turney and Littman,
2003). Opinionfinder (Wilson et al, 2005) is a sys-
tem for mining opinions from text. SENTIWORD-
NET (Esuli and Sebastiani, 2006) is a lexical re-
source in which each WordNet synset is associated
to three numerical scores Obj(s), Pos(s) and Neg(s),
describing how objective, positive, and negative the
terms contained in the synset are. Dr Sentiment (Das
and Bandyopadhyay, 2011) is an online interactive
gaming technology used to crowd source human
knowledge to build an extension of SentiWordNet.
Another research line focused on analyzing on-
line discussions. For example, Lin et al (2009)
proposed a sparse coding-based model that simul-
taneously models the semantics and the structure
of threaded discussions. Shen et al (2006) pro-
posed a method for exploiting the temporal and lex-
ical similarity information in discussion streams to
identify the reply structure of the dialog. Many sys-
tems addressed the problem of extracting social net-
works from discussions (Elson et al, 2010; McCal-
lum et al, 2007). Other related sentiment analy-
sis systems include MemeTube (Li et al, 2011), a
sentiment-based system for analyzing and display-
ing microblog messages; and C-Feel-It (Joshi et al,
2011), a sentiment analyzer for micro-blogs.
In the rest of this paper, we describe the system
architecture, implementation, usage, and its evalua-
tion.
2 System Overview
Figure 1 shows a block diagram of the system com-
ponents and the processing pipeline. The first com-
ponent is the thread parsing component which takes
as input a discussion thread and parses it to iden-
tify posts, participants, and the reply structure of the
thread. The second component in the pipeline pro-
cesses the text of posts to identify polarized words
and tag them with their polarity. The list of polar-
ity words that we use in this component has been
taken from the OpinionFinder system (Wilson et al,
2005).
The polarity of a word is usually affected by the
context in which it appears. For example, the word
fine is positive when used as an adjective and neg-
ative when used as a noun. For another example, a
positive word that appears in a negated context be-
comes negative. To address this, we take the part-
of-speech (POS) tag of the word into consideration
when we assign word polarities. We require that the
POS tag of a word matches the POS tag provided in
the list of polarized words that we use. The negation
issue is handled in the opinion-target pairing step as
we will explain later.
The next step in the pipeline is to identify the can-
didate targets of opinion in the discussion. The tar-
get of attitude could be another discussant, an entity
mentioned in the discussion, or an aspect of the dis-
cussion topic. When the target of opinion is another
discussant, either the discussant name is mentioned
explicitly or a second person pronoun (e.g you, your,
yourself) is used to indicate that the opinion is tar-
geting the recipient of the post.
The target of opinion could also be a subtopic or
an entity mentioned in the discussion. We use two
methods to identify such targets. The first method
depends on identifying noun groups (NG). We con-
sider as an entity any noun group that is mentioned
by at least two different discussants. We only con-
sider as entities noun groups that contain two words
or more. We impose this requirement because in-
dividual nouns are very common and considering
all of them as candidate targets will introduce sig-
nificant noise. In addition to this shallow pars-
ing method, we also use named entity recognition
(NER) to identify more targets. The named en-
tity tool that we use recognizes three types of en-
tities: person, location, and organization. We im-
pose no restrictions on the entities identified using
this method.
A challenge that always arises when perform-
ing text mining tasks at this level of granularity
is that entities are usually expressed by anaphori-
cal pronouns. Jakob and Gurevych (2010) showed
experimentally that resolving the anaphoric links
134
Discussion 
Thread 
?.??. 
?.??. 
?.??. 
Opinion Identification 
? Identify polarized words 
? Identify the contextual 
polarity of each word 
 
 
Target Identification 
? Anaphora resolution 
? Identify named entities 
? Identify Frequent noun 
phrases. 
? Identify mentions of 
other discussants 
Opinion-Target Pairing 
? Dependency Rules 
 
 
 
Discussant Attitude 
Profiles (DAPs)  
 
 
 
Clustering 
Subgroups 
 
 
 
 
 
Thread Parsing 
? Identify posts 
? Identify discussants 
? Identify the reply 
structure 
? Tokenize text. 
? Split posts into sentences 
 
Figure 1: A block diagram illustrating the processing pipeline of the subgroup detection system
in text significantly improves opinion target extrac-
tion. Therefore, we use co-reference resolution tech-
niques to resolve all the anaphoric links in the dis-
cussion thread.
At this point, we have all the opinion words and
the potential targets identified separately. The next
step is to determine which opinion word is target-
ing which target. We propose a rule based approach
for opinion-target pairing. Our rules are based on
the dependency relations that connect the words in
a sentence. An opinion word and a target form a
pair if the dependency path between them satisfies
at least one of our dependency rules. Table 1 illus-
trates some of these rules. The rules basically exam-
ine the types of dependency relations on the shortest
path that connect the opinion word and the target in
the dependency parse tree. It has been shown in pre-
vious work on relation extraction that the shortest
dependency path between any two entities captures
the information required to assert a relationship be-
tween them (Bunescu and Mooney, 2005). If a sen-
tence S in a post written by participant Pi contains
an opinion word OPj and a target TRk, and if the
opinion-target pair satisfies one of our dependency
rules, we say that Pi expresses an attitude towards
TRk. The polarity of the attitude is determined by
the polarity of OPj . We represent this as Pi
+
? TRk
if OPj is positive and Pi
?
? TRk if OPj is nega-
tive. Negation is handled in this step by reversing
the polarity if the polarized expression is part of a
neg dependency relation.
It is likely that the same participant Pi expresses
sentiment towards the same target TRk multiple
times in different sentences in different posts. We
keep track of the counts of all the instances of posi-
tive/negative attitude Pi expresses toward TRk. We
represent this as Pi
m+
???
n?
TRk where m (n) is the
number of times Pi expressed positive (negative) at-
titude toward TRk.
Now, we have information about each discussant
attitude. We propose a representation of discus-
santsa?ttitudes towards the identified targets in the
discussion thread. As stated above, a target could
be another discussant or an entity mentioned in the
discussion. Our representation is a vector contain-
ing numerical values. The values correspond to the
counts of positive/negative attitudes expressed by
the discussant toward each of the targets. We call
this vector the discussant attitude profile (DAP). We
construct a DAP for every discussant. Given a dis-
cussion thread with d discussants and e entity tar-
gets, each attitude profile vector has n = (d+ e) ? 3
dimensions. In other words, each target (discussant
or entity) has three corresponding values in the DAP:
1) the number of times the discussant expressed pos-
itive attitude toward the target, 2) the number of
times the discussant expressed a negative attitude to-
wards the target, and 3) the number of times the the
discussant interacted with or mentioned the target.
It has to be noted that these values are not symmet-
135
ID Rule In Words
R1 OP ? nsubj ? TR The target TR is the nominal subject of the opinion word OP
R2 OP ? dobj ? TR The target T is a direct object of the opinion OP
R3 OP ? prep ? ? TR The target TR is the object of a preposition that modifies the opinion word OP
R4 TR? amod? OP The opinion is an adjectival modifier of the target
R5 OP ? nsubjpass? TR The target TR is the nominal subject of the passive opinion word OP
R6 OP ? prep ? ? poss? TR The opinion word OP connected through a prep ? relation as in R2 to something pos-
sessed by the target TR
R7 OP ? dobj ? poss? TR The target TR possesses something that is the direct object of the opinion word OP
R8 OP ? csubj ? nsubj ? TR The opinon word OP is a causal subject of a phrase that has the target TR as its nominal
subject.
Table 1: Examples of the dependency rules used for opinion-target pairing.
ric since the discussions explicitly denote the source
and the target of each post.
At this point, we have an attitude profile (or vec-
tor) constructed for each discussant. Our goal is to
use these attitude profiles to determine the subgroup
membership of each discussant. We can achieve this
goal by noticing that the attitude profiles of discus-
sants who share the same opinion are more likely to
be similar to each other than to the attitude profiles
of discussants with opposing opinions. This sug-
gests that clustering the attitude vector space will
achieve the goal and split the discussants into sub-
groups based on their opinion.
3 Implementation
The system is fully implemented in Java. Part-of-
speech tagging, noun group identification, named
entity recognition, co-reference resolution, and de-
pendency parsing are all computed using the Stan-
ford Core NLP API.2 The clustering component
uses the JavaML library3 which provides implemen-
tations to several clustering algorithms such as k-
means, EM, FarthestFirst, and OPTICS.
The system requires no installation. It, however,
requires that the Java Runtime Environment (JRE)
be installed. All the dependencies of the system
come bundled with the system in the same package.
The system works on all the standard platforms.
The system has a command-line interface that
2http://nlp.stanford.edu/software/corenlp.shtml
3http://java-ml.sourceforge.net/
provides full access to the system functionality. It
can be used to run the whole pipeline to detect sub-
groups or any portion of the pipeline. For example,
it can be used to tag an input text with polarity or to
identify candidate targets of opinion in a given in-
put. The system behavior can be controlled by pass-
ing arguments through the command line interface.
For example, the user can specify which clustering
algorithm should be used.
To facilitate using the system for research pur-
poses, the system comes with a clustering evaluation
component that uses the ClusterEvaluator package.4.
If the input to the system contains subgroup labels,
it can be run in the evaluation mode in which case
the system will output the scores of several different
clustering evaluation metrics such as purity, entropy,
f-measure, Jaccard, and RandIndex. The system also
has a Java API that can be used by researchers to de-
velop other systems using our code.
The system can process any discussion thread that
is input to it in a specific format. The format of
the input and output is described in the accompa-
nying documentation. It is the user responsibility
to write a parser that converts an online discussion
thread to the expected format. However, the sys-
tem package comes with two such parsers for two
different discussion sites: www.politicalforum.com
and www.createdebate.com.
The distribution also comes with three datasets
4http://eniac.cs.qc.cuny.edu/andrew/v-
measure/javadoc/index.html
136
Figure 2: A screenshot of the online demo
(from three different sources) comprising a total of
300 discussion threads. The datasets are annotated
with the subgroup labels of discussants.
Finally, we created a web interface to demonstrate
the system functionality. The web interface is in-
tended for demonstration purposes only. No web-
service is provided. Figure 2 shows a screenshots of
the web interface. The online demo can be accessed
at http://clair.eecs.umich.edu/SubgroupDetector/
4 Evaluation
In this section, we give a brief summary of the sys-
tem evaluation. We evaluated the system on discus-
sions comprising more than 10,000 posts in more
than 300 different topics. Our experiments show that
the system detects subgroups with promising accu-
racy. The average clustering purity of the detected
subgroups in the dataset is 0.65. The system signif-
icantly outperforms baseline systems based on text
clustering and discussant interaction frequency. Our
experiments also show that all the components in the
system (such as co-reference resolution, noun phrase
chunking, etc) contribute positively to the accuracy.
5 Conclusion
We presented a demonstration of a discussion min-
ing system that uses linguistic analysis techniques to
predict the attitude the participants in online discus-
sions forums towards one another and towards the
different aspects of the discussion topic. The system
is capable of analyzing the text exchanged in dis-
cussions and identifying positive and negative atti-
tudes towards different targets. Attitude predictions
are used to assign a subgroup membership to each
participant using clustering techniques. The sys-
tem predicts attitudes and identifies subgroups with
promising accuracy.
References
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 724?731, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Amitava Das and Sivaji Bandyopadhyay. 2011. Dr sen-
timent knows everything! In Proceedings of the ACL-
HLT 2011 System Demonstrations, pages 50?55, Port-
land, Oregon, June. Association for Computational
Linguistics.
137
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 138?147,
Uppsala, Sweden, July.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC06, pages
417?422.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Niklas Jakob and Iryna Gurevych. 2010. Using anaphora
resolution to improve opinion target identification in
movie reviews. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 263?268, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Aditya Joshi, Balamurali AR, Pushpak Bhattacharyya,
and Rajat Mohanty. 2011. C-feel-it: A sentiment ana-
lyzer for micro-blogs. In Proceedings of the ACL-HLT
2011 System Demonstrations, pages 127?132, Port-
land, Oregon, June. Association for Computational
Linguistics.
Cheng-Te Li, Chien-Yuan Wang, Chien-Lin Tseng, and
Shou-De Lin. 2011. Memetube: A sentiment-based
audiovisual system for analyzing and displaying mi-
croblog messages. In Proceedings of the ACL-HLT
2011 System Demonstrations, pages 32?37, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09,
pages 131?138.
Andrew McCallum, Xuerui Wang, and Andre?s Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. J. Artif. Int. Res., 30:249?272, October.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
HLT/EMNLP - Demo.
138
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572?577,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A System for Summarizing Scientific Topics Starting from Keywords
Rahul Jha
Department of EECS
University of Michigan
Ann Arbor, MI, USA
rahuljha@umich.edu
Amjad Abu-Jbara
Department of EECS
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
Department of EECS and
School of Information
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In this paper, we investigate the problem
of automatic generation of scientific sur-
veys starting from keywords provided by
a user. We present a system that can take
a topic query as input and generate a sur-
vey of the topic by first selecting a set
of relevant documents, and then selecting
relevant sentences from those documents.
We discuss the issues of robust evalua-
tion of such systems and describe an eval-
uation corpus we generated by manually
extracting factoids, or information units,
from 47 gold standard documents (surveys
and tutorials) on seven topics in Natural
Language Processing. We have manually
annotated 2,625 sentences with these fac-
toids (around 375 sentences per topic) to
build an evaluation corpus for this task.
We present evaluation results for the per-
formance of our system using this anno-
tated data.
1 Introduction
The rise of the number of publications in all sci-
entific fields is making it more and more difficult
to get quickly acquainted with the new develop-
ments in a new area. One way to wade through this
huge amount of scholarly information is to consult
topical surveys written by experts in an area. For
example, for machine translation, one might read
(Lopez, 2008)1. Such surveys can be very help-
ful when available, but unfortunately, may not be
available for all areas. Additionally, the manual
surveys quickly go out of date within a few years
of publication as additional papers are published
in the field.
1Adam Lopez. 2008. Statistical machine translation.
ACM Comput. Surv. 40, 3, Article 8
Thus, a system that can generate such surveys
automatically would be a useful tool. Short sum-
maries in the form of abstracts are available for
individual papers, but no such information is avail-
able for scientific topics. In this paper, we ex-
plore strategies for generating and evaluating such
surveys of scientific topics automatically starting
from a phrase representing a topic area. We evalu-
ate our system on a set of topics in the field of Nat-
ural Language Processing. In earlier work, (Teufel
and Moens, 2002) have examined the problem
of summarizing scientific articles using rhetorical
analysis of sentences. Nanba and Okumura (1999)
have also discussed the problem of generating sur-
veys of multiple papers. Mohammad et al (2009)
presented experiments on generating surveys of
scientific topics starting from papers to be summa-
rized. More recently, Hoang and Kan (2010) have
presented initial results on automatically generat-
ing related work section for a target paper by tak-
ing a hierarchical topic tree as an input.
In this paper, we tackle the more challenging
problem of summarizing a topic starting from a
topic query. Our system takes as an input a string
describing the topic area, selects the relevant pa-
pers from a corpus of papers, and then selects sen-
tences from the citing sentences to these papers to
generate a survey of the topic. A sample output of
our system for the topic of ?Word Sense Disam-
biguation? is shown in Figure 1.
2 Candidate Document Selection
Given a query representing the topic to be sum-
marized, our first task is to find the set of rele-
vant documents from the corpus. The simplest
way to do this for a corpus of scientific publica-
tions is to do a query search using exact match or
a standard TF*IDF system such Lucene, rank the
documents using either citation counts or pager-
ank in the bibliometric citation network, and se-
lect the top n documents. However, comparing
572
Many corpus based methods have been proposed to deal with the sense disambiguation problem when given de nition for each possible sense of a target word or a
tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacock et al , 1998), and semi-supervised sense disambiguation
(Yarowsky, 1995).
Most researchers working on word sense disambiguation (WSD) use manually sense tagged data such as SemCor (Miller et al , 1993) to train statistical
classifiers, but also use the information in SemCor on the overall sense distribution for each word as a backoff model.
Yarowsky (1995) has proposed a bootstrapping method for word sense disambiguation.
Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002;
Mihalcea and Moldovan, 2001; Yarowsky et al , 2001).
For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al , 1991; Dagan, 1991; Dagan and Itai, 1994;
Ide, 2000; Resnik and Yarowsky, 1999).
Figure 1: A sample output survey of our system on the topic of ?Word Sense Disambiguation? produced
by paper selection using Restricted Expansion and sentence selection using Lexrank. In our evaluations,
this survey achieved a pyramid score of 0.82 and Unnormalized RU score of 0.31.
Document selection algorithm CG5 CG10 CG20
Title match sorted with citation count 1.82 2.75 3.29
Title match sorted with pagerank 1.77 2.55 3.34
Citation expansion sorted with citation
count 0.53 1.20 2.29
Citation expansion sorted with pagerank 0.20 0.78 1.99
TF*IDF ranked 0.14 0.14 0.56
TF*IDF sorted with citation count 0.44 2.25 3.18
TF*IDF sorted with pagerank 1.54 2.22 2.85
Restricted Expansion 2.52 3.91 6.01
Table 1: Comparison of different methods for
document selection by measuring the Cumulative
Gain (CG) of top 5, 10 and 20 results.
the results of these techniques with the papers cov-
ered by gold standard surveys on a few topics, we
found that some important papers are missed by
these simple approaches. One reason for this is
that early papers in a field might use non-standard
terms in the absence of a stable, accepted termi-
nology. Some early Word Sense Disambiguation
papers, for example, refer to the problem as Lex-
ical Ambiguity Resolution. Additionally, papers
might use alternative forms or abbreviations of
topics in their titles and abstracts, e.g. for input
query ?Semantic Role Labelling?, papers such as
(Dahlmeier et al, 2009) titled ?Joint Learning of
Preposition Senses and Semantic Roles of Prepo-
sitional Phrases? and (Che and Liu, 2010) titled
?Jointly Modeling WSD and SRL with Markov
Logic? might be missed.
To find these papers, we add a simple heuristic
called Restricted Expansion. In this method, we
first create a base set B, by finding papers with an
exact match to the query. This is a high precision
set since a paper with a title that contains the ex-
act query phrase is very likely to be relevant to the
topic. We then find additional papers by expand-
ing in the citation network around B, that is, by
finding all the papers that are cited by or cite the
papers in B, to create an extended set E. From
this combined set (B ?E), we create a new set F
by filtering out the set of papers that are not cited
by or cite a minimum threshold tinit of papers in
B. If the total number of papers is lower than fmin
or higher than fmax, we iteratively increase or de-
crease t till fmin ? |F | ? fmax. This method
allows us to increase our recall without losing pre-
cision. The values for our current experiments are:
tinit = 5, fmin = 150, fmax = 250.
Authors Year Size
Surveys
ACL Wiki 2012 4
Roberto Navigli 2009 68
Eneko Agirre; Philip Edmonds 2006 28
Xiaohua Zhou; Hyoil Han 2005 6
Nancy Ide; Jean Vronis 1998 41
Tutorials
Sanda Harabagiu 2011 45
Diana McCarthy 2011 120
Philipp Koehn 2008 17
Rada Mihalcea 2005 186
Table 2: The set of surveys and tutorials col-
lected for the topic of ?Word Sense Disambigua-
tion?. Sizes for surveys are expressed in number
of pages, sizes for tutorials are expressed in num-
ber of slides.
To evaluate different methods of candidate doc-
ument selection, we use Cumulative Gain (CG),
where the weight for each paper is estimated by
the fraction of surveys it appears in. Table 1
shows the average Cumulative Gain of top 5, 10
and 20 documents for each of eight methods we
tried. Restricted Expansion outperformed every
other method. Once we obtain a set of papers to
be summarized, we select the top n most cited pa-
pers in the document set as the papers to be sum-
marized, and extract the set of citing sentences S
from all the papers in the document set to these n
papers. S is the input for our sentence selection
algorithms, described in Section 4.
573
Factoid S1 S2 S3 S4 S5 T1 T2 T3 T4 Factoid Weight
definition of wsd X X X X X X X X X 9
wordnet X X X X X X X X 8
knowledge based wsd X X X X X X X 7
supervised wsd X X X X X X X 7
senseval X X X X X X X 7
definition of word senses X X X X X X 7
knowledge based wsd using machine readable dictionaries X X X X X X 6
unsupervised wsd X X X X X X 6
bootstrapping algorithms X X X X X X 6
supervised wsd using decision lists X X X X X X 6
Table 3: Top 10 factoids for the topic of ?Word Sense Disambiguation? and their distribution across
various data sources.
3 Evaluation Data for Survey Generation
We use the ACL Anthology Network (AAN) as the
corpus for our experiments (Radev et al, 2013).
We built a factoid inventory for seven topics in
NLP based on manual written surveys in the fol-
lowing way. For each topic, we found at least 3
recent tutorials and 3 recent surveys on the topic
and extracted the factoids that are covered in each
of them. Table 2 shows the complete list of ma-
terial collected for the topic of ?Word Sense Dis-
ambiguation?. We found around 80 factoids per
topic on an average. Once the factoids were ex-
tracted, each factoid was assigned a weight based
on the number of documents it appears in, and any
factoids with weight one were removed. Table 3
shows the top ten factoids in the topic of Word
Sense Disambiguation along with their distribu-
tion across the different surveys and tutorials and
final weight.
For each of the topics, we used the method de-
scribed in Section 2 to create a candidate docu-
ment set and extracted the candidate citing sen-
tences to be used as the input for the content se-
lection component. Each sentence in each topic
was then annotated by a human judge against the
factoid list for that topic. A sentence is allowed
to have zero or more than one factoid. The human
assessors were graduate students in Computer Sci-
ence who have taken a basic ?Natural Language
Processing? course or an equivalent course. On an
average, 375 citing sentences were annotated for
each topic, with 2,625 sentences being annotated
in total. We present all our experimental results on
this large annotated corpora which is also available
for download 2.
4 Content Models
Once we have the set of input sentences, our sys-
tem must select the sentences that should be part
2http://clair.si.umich.edu/corpora/survey data/
of the survey. For this task, we experimented with
three content models, described below.
4.1 Centroid
The centroid of a set of documents is a set of words
that are statistically important to the cluster of doc-
uments. Centroid based summarization of a docu-
ment set involves first creating the centroid of the
documents, and then judging the salience of each
document based on its similarity to the centroid
of the document set. In our case, the input citing
sentences represent the documents from which we
extract the centroid. We use the centroid imple-
mentation from the publicly available summariza-
tion toolkit, MEAD (Radev et al, 2004).
4.2 Lexrank
LexRank (Erkan and Radev, 2004) is a network
based content selection algorithm that works by
first building a graph of all the documents in a
cluster. The edges between corresponding nodes
represent the cosine similarity between them.
Once the network is built, the algorithm computes
the salience of sentences in this graph based on
their eigenvector centrality in the network.
4.3 C-Lexrank
C-Lexrank is another network based content selec-
tion algorithm that focuses on diversity (Qazvinian
and Radev, 2008). Given a set of sentences, it first
creates a network using these sentences and then
runs a clustering algorithm to partition the net-
work into smaller clusters that represent different
aspects of the paper. The motivation behind the
clustering is to include more diverse facts in the
summary.
5 Experiments and Results
To do an evaluation of our different content selec-
tion methods, we first select the documents using
our Restricted Expansion method, and then pick
574
Topic Rand Cent LR C-LR
Summarization 0.68 0.61 0.91 0.82
Question Answering 0.52 0.50 0.65 0.56
Word Sense Disambiguation 0.78 0.73 0.82 0.76
Named Entity Recognition 0.90 0.90 0.94 0.94
Sentiment Analysis 0.75 0.78 0.77 0.78
Semantic Role Labeling 0.78 0.79 0.88 0.94
Dependency Parsing 0.67 0.38 0.71 0.53
Average 0.72 0.68 0.81? 0.76
Table 4: Results of pyramid evaluation for each
of the three methods and the random baseline on
each topic.
the citing sentences to be used as the input to the
summarization module as described in Section 2.
Given this input, we generate 500 word summaries
for each of the seven topics using the four meth-
ods: Centroid, Lexrank, C-Lexrank and a random
baseline.
For each summary, we compute two evaluation
metrics. The first is the Pyramid score (Nenkova
and Passonneau, 2004) computed by treating the
factoids as Summary Content Units (SCU?s). The
Pyramid scores for each summary is shown in Ta-
ble 4. The second metric is an Unnormalized Rel-
ative Utility score (Radev and Tam, 2003), com-
puted using the factoid scores of sentences based
on the method presented in (Qazvinian, 2012). We
call this Unnormalized RU since we are not able to
normalize the scores with human generated gold
summaries. The results for Unnormalized RU are
shown in Table 5. The parameter ? is the RU
penalty for including a redundant sentence sub-
sumed by an earlier sentence. If the summary
chooses a sentence si with score worig that is sub-
sumed by an earlier summary sentence, the score
is reduced as wsubsumed = (? ? worig). We ap-
proximate subsumption by marking a sentence sj
as being subsumed by si if Fj ? Fi, where Fi and
Fj are sets of factoids covered in each sentence.
Topic Rand Cent LR C-LR
Summarization 0.16 0.57 0.29 0.17
Question Answering 0.32 0.39 0.48 0.30
Word Sense Disambiguation 0.28 0.33 0.31 0.30
Named Entity Recognition 0.36 0.38 0.34 0.31
Sentiment Analysis 0.23 0.34 0.48 0.33
Semantic Role Labeling 0.11 0.17 0.16 0.21
Dependency Parsing 0.16 0.05 0.30 0.15
Average 0.23 0.32 0.34? 0.25
Table 5: Results of Unnormalized Relative Utility
evaluation for the three methods and random base-
line using ? = 0.5.
The reason for the relatively high scores for the
random baseline is that our process to select the
initial set of sentences eliminates many bad sen-
tences. For example, for a subset of 5 topics,
the total input set contains 1508 sentences, out of
which 922 of the sentences (60%) have at least one
factoid. This makes it highly likely to pick good
content sentences even when we are picking sen-
tences at random.
We find that the Lexrank method outperforms
other sentence selection methods on both evalua-
tion metrics. The higher performance of Lexrank
compared to Centroid is consistent with earlier
published results (Erkan and Radev, 2004). The
reason for the low performance of C-Lexrank as
compared to Lexrank on this data set can be at-
tributed to the fact that the input sentence set is
derived from a much more diverse set of papers
which can have a high diversity in lexical choice
when describing the same factoid. Thus simple
lexical similarity is not enough to find good clus-
ters in this sentence set.
The lower Unnormalized RU scores compared
to Pyramid scores indicate that we are selecting
sentences containing highly weighted factoids, but
we do not select the most informative sentences
that contain a large number of factoids. This
also shows that we select some redundant factoids,
since Unnormalized RU contains a penalty for re-
dundancy. This is again, explained by the fact
that the simple lexical diversity based model in C-
Lexrank is not able to detect the same factoids be-
ing present in two sentences. Despite these short-
comings, our system works quite well in terms
of content selection for unseen topics, Figure 2
shows the top 5 sentences for the query ?Condi-
tional Random Fields?.
6 Conclusion and Future Work
In this paper, we described a pipeline for the gen-
eration of scientific surveys starting from a topic
query. Our system is divided into two components.
The first component finds the set of papers from
the corpus relevant to the query using a simple
heuristic called Restricted Expansion. The second
component selects sentences from these papers to
generate a survey of the topic. One of the main
contributions of this work is a manually annotated
data set for evaluating both the tasks. We collected
47 gold standard documents (surveys and tutori-
als) on seven topics in Natural Language Process-
ing and extracted factoids for each topic. Each
factoid is given an importance score based on the
number of gold standard documents it appears in.
575
In recent years, conditional random fields (CRFs) (Lafferty et al , 2001)
have shown success on a number of natural language processing (NLP)
tasks, including shallow parsing (Sha and Pereira, 2003), named entity
recognition (McCallum and Li, 2003) and information extraction from
research papers (Peng and McCallum, 2004).
In natural language processing, two aspects of CRFs have been
investigated sufficiently: one is to apply it to new tasks, such as named
entity recognition (McCallum and Li, 2003; Li and McCallum, 2003;
Settles, 2004), part-of-speech tagging (Lafferty et al, 2001), shallow
parsing (Sha and Pereira, 2003), and language modeling (Roark et al,
2004); the other is to exploit new training methods for CRFs, such as
improved iterative scaling (Lafferty et al, 2001), L-BFGS (McCallum,
2003) and gradient tree boosting (Dietterich et al, 2004)
NP chunks are very similar to the ones of Ramshaw and Marcus (1995).
CRFs have shown empirical successes recently in POS tagging (Lafferty
et al , 2001), noun phrase segmentation (Sha and Pereira, 2003) and
Chinese word segmentation (McCallum and Feng, 2003)
CRFs have been successfully applied to a number of real-world tasks,
including NP chunking (Sha and Pereira, 2003), Chinese word
segmentation (Peng et al, 2004), information extraction (Pinto et al,
2003; Peng and McCallum, 2004), named entity identification (McCallum
and Li, 2003; Settles, 2004), and many others.
Figure 2: A sample output survey produced by
our system on the topic of ?Conditional Random
Fields? using Restricted Expansion and Lexrank.
Additionally, we manually annotated 2,625 input
sentences, about 375 sentences per topic, with the
factoids extracted from the gold standard docu-
ments for each topic. Using this corpus, we pre-
sented experimental results for the performance of
our document selection component and three sen-
tence selection strategies.
Our results indicate three main directions for
future work. We plan to look at better models
of diversity in sentence selection, since methods
based on simple lexical similarity do not seem to
work well. The low factoid recall shown by low
unnormalized RU scores suggests integrating the
full text of papers with citation based summaries
which might help us find factoids such as topic
definitions that are unlikely to be present in citing
sentences. A final goal would be to improve the
readability and coherence of our system output.
Acknowledgments
We thank Vahed Qazvinian, Wanchen Lu, Ben
King, and Shiwali Mohan for extremely useful
discussions and help with the data annotation.
This research is supported by the Intelligence
Advanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D11PC20153.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Govern-
ment.
References
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Cong Duy Vu Hoang and Min-Yen Kan. 2010. To-
wards automated related work summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING ?10,
pages 427?435, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 584?592, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In Proceedings of the 16th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI-99), pages 926?931.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics - Human Language Technologies (HLT-
NAACL ?04).
Vahed Qazvinian and Dragomir R. Radev. 2008.
Scientific paper summarization using citation sum-
mary networks. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING-08), Manchester, UK.
Vahed Qazvinian. 2012. Using Collective Discourse
to Generate Surveys of Scientific Paradigms. Ph.D.
thesis.
Dragomir R. Radev and Daniel Tam. 2003. Sum-
marization evaluation using relative utility. In
CIKM2003, pages 508?511.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
576
Winkel, and Zhu Zhang. 2004. MEAD - a platform
for multidocument multilingual text summarization.
In LREC 2004, Lisbon, Portugal, May.
Dragomir R. Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The acl
anthology network corpus. Language Resources
and Evaluation, pages 1?26.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
577
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 829?835,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Identifying Opinion Subgroups in Arabic Online Discussions
Amjad Abu-Jbara
Department of EECS
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Mona Diab
Department of Computer Science
George Washington University
Washington DC, USA
mtdiab@gwu.edu
Ben King
Department of EECS
University of Michigan
Ann Arbor, MI, USA
benking@umich.edu
Dragomir Radev
Department of EECS
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In this paper, we use Arabic natural lan-
guage processing techniques to analyze
Arabic debates. The goal is to identify
how the participants in a discussion split
into subgroups with contrasting opinions.
The members of each subgroup share the
same opinion with respect to the discus-
sion topic and an opposing opinion to
the members of other subgroups. We
use opinion mining techniques to identify
opinion expressions and determine their
polarities and their targets. We opinion
predictions to represent the discussion in
one of two formal representations: signed
attitude network or a space of attitude vec-
tors. We identify opinion subgroups by
partitioning the signed network represen-
tation or by clustering the vector space
representation. We evaluate the system us-
ing a data set of labeled discussions and
show that it achieves good results.
1 Introduction
Arabic is one of the fastest growing languages
on the internet. The number of internet users in
the Arab region grew by 2500% over the past 10
years. As of January 2012, the number of Arabic-
speaking internet users was 86 millions. The re-
cent political and civic movements in the Arab
World resulted in a revolutionary growth in the
number of Arabic users on social networking sites.
For example, Arabic is the fastest growing lan-
guage in Twitter history 1.
This growth in the presence of Arab users on
social networks and all the interactions and dis-
cussions that happen among them led to a huge
amount of opinion-rich Arabic text being avail-
able. Analyzing this text could reveal the different
viewpoints of Arab users with respect to the topics
that they discuss online.
When a controversial topic is discussed, it is
normal for the discussants to adopt different view-
points towards it. This usually causes rifts in dis-
cussion groups and leads to the split of the dis-
cussants into subgroups with contrasting opinions.
Our goal in this paper is to use natural language
processing techniques to detect opinion subgroups
in Arabic discussions. Our approach starts by
identifying opinionated (subjective) text and deter-
mining its polarity (positive, negative, or neutral).
Next, we determine the target of each opinion ex-
pression. The target of opinion can be a named
entity mentioned in the discussion or an aspect of
the discussed topic. We use the identified opinion-
target relations to represent the discussion in one
of two formal representations. In the first repre-
sentation, each discussant is represented by a vec-
tor that encodes all his or her opinion information
towards the discussion topic. In the second repre-
sentation, each discussant is represented by a node
in a signed graph. A positive edge connects two
discussants if they have similar opinion towards
the topic, otherwise the sign of the edge is nega-
1http://semiocast.com/publications/
2011_11_24_Arabic_highest_growth_on_
Twitter
829
tive. To identify opinion subgroups, we cluster the
vector space (the first representation) or partition
the signed network (the second representation).
We evaluate this system using a data set of Ara-
bic discussions collected from an Arabic debating
site. We experiment with several variations of the
system. The results show that the clustering the
vector space representation achieves better results
than partitioning the signed network representa-
tion.
2 Previous Work
Our work is related to a large body of research on
opinion mining and sentiment analysis. Pang &
Lee (2008) and Liu & Zhang (2012) wrote two re-
cent comprehensive surveys about sentiment anal-
ysis and opinion mining techniques and applica-
tions.
Previous work has proposed methods for iden-
tifying subjective text that expresses opinion
and distinguishing it from objective text that
presents factual information (Wiebe, 2000; Hatzi-
vassiloglou and Wiebe, 2000a; Banea et al, 2008;
Riloff and Wiebe, 2003).
Subjective text may express positive, negative,
or neutral opinion. Previous work addressed the
problem of identifying the polarity of subjective
text (Hatzivassiloglou and Wiebe, 2000b; Hassan
et al, 2010; Riloff et al, 2006). Many of the pro-
posed methods for text polarity identification de-
pend on the availability of polarity lexicons (i.e.
lists of positive and negative words). Several ap-
proaches have been devised for building such lex-
icons (Turney and Littman, 2003; Kanayama and
Nasukawa, 2006; Takamura et al, 2005; Hassan
and Radev, 2010). Other research efforts focused
on identifying the holders and the targets of opin-
ion (Zhai et al, 2010; Popescu and Etzioni, 2007;
Bethard et al, 2004).
Opinion mining and sentiment analysis tech-
niques have been used in various applications.
One example of such applications is identifying
perspectives (Grefenstette et al, 2004; Lin et al,
2006) which is most similar to the topic of this
paper. For example, in (Lin et al, 2006), the au-
thors experiment with several supervised and sta-
tistical models to capture how perspectives are ex-
pressed at the document and the sentence levels.
Laver et al (2003) proposed a method for extract-
ing perspectives from political texts. They used
their method to estimate the policy positions of po-
litical parties in Britain and Ireland, on both eco-
nomic and social policy dimensions.
Somasundaran and Wiebe (2009) present an un-
supervised opinion analysis method for debate-
side classification. They mine the web to learn
associations that are indicative of opinion stances
in debates and combine this knowledge with dis-
course information. Anand et al (2011) present a
supervised method for stance classification. They
use a number of linguistic and structural fea-
tures such as unigrams, bigrams, cue words, re-
peated punctuation, and opinion dependencies to
build a stance classification model. In previous
work, we proposed a method that uses participant-
to-participant and participant-to-topic attitudes to
identify subgroups in ideological discussions us-
ing attitude vector space clustering (Abu-Jbara and
Radev, 2012). In this paper, we extend this method
by adding latent similarity features to the attitude
vectors and applying it to Arabic discussions. In
another previous work, our group proposed a su-
pervised method for extracting signed social net-
works from text (Hassan et al, 2012a). The
signed networks constructed using this method
were based only on participant-to-participant at-
titudes that are expressed explicitly in discussions.
We used this method to extract signed networks
from discussions and used a partitioning algo-
rithm to detect opinion subgroups (Hassan et al,
2012b). In this paper, we extend this method by
using participant-to-topic attitudes to construct the
signed network.
Unfortunately, not much work has been done
on Arabic sentiment analysis and opinion min-
ing. Abbasi et al (2008) applies sentiment anal-
ysis techniques to identify and classify document-
level opinions in text crawled from English and
Arabic web forums. Hassan et al (2011) pro-
posed a method for identifying the polarity of non-
English words using multilingual semantic graphs.
They applied their method to Arabic and Hindi.
Abdul-Mageed and Diab (2011) annotated a cor-
pus of Modern Standard Arabic (MSA) news text
for subjectivity at the sentence level. In a later
work (2012a), they expanded their corpus by la-
830
beling data from more genres using Amazon Me-
chanical Turk. Abdul-Mageed et al (2012a) de-
veloped SAMAR, a system for subjectivity and
Sentiment Analysis for Arabic social media gen-
res. We use this system as a component in our
approach.
3 Approach
In this section, we present our approach to de-
tecting opinion subgroups in Arabic discussions.
We propose a pipeline that consists of five com-
ponents. The input to the pipeline is a discussion
thread in Arabic language crawled from a discus-
sion forum. The output is the list of participants
in the discussion and the subgroup membership of
each discussant. We describe the components of
the pipeline in the following subsections.
3.1 Preprocessing
The input to this component is a discussion thread
in HTML format. We parse the HTML file to iden-
tify the posts, the discussants, and the thread struc-
ture. We transform the Arabic content of the posts
and the discussant names that are written in Arabic
to the Buckwalter encoding (Buckwalter, 2004).
We use AMIRAN (Diab, 2009), a system for pro-
cessing Arabic text, to tokenize the text and iden-
tify noun phrases.
3.2 Identifying Opinionated Text
To identify opinion-bearing text, we start from the
word level. We identify the polarized words that
appear in text by looking each word up in a lexicon
of Arabic polarized words. In our experiments, we
use Sifat (Abdul-Mageed and Diab, 2012b), a lex-
icon of 3982 Arabic adjectives labeled as positive,
negative, or neutral.
The polarity of a word may be dependant on
its context (Wilson et al, 2005). For example,
a positive word that appears in a negated context
should be treated as expressing negative opinion
rather than positive. To identify the polarity of
a word given the sentence it appears in, we use
SAMAR (Abdul-Mageed et al, 2012b), a system
for Subjectivity and Sentiment Analysis for Ara-
bic social media genres. SAMAR labels a sen-
tence that contains an opinion expression as pos-
itive, negative, or neutral taking into account the
context of the opinion expression. The reported
accuracy of SAMAR on different data sets ranges
between 84% and 95% for subjectivity classifica-
tion and 65% and 81% for polarity classification.
3.3 Identifying Opinion Targets
In this step, we determine the targets that the opin-
ion is expressed towards. We treat as an opin-
ion target any noun phrase (NP) that appears in
a sentence that SAMAR labeled as polarized (pos-
itive or negative) in the previous step. To avoid
the noise that may result from including all noun
phrases, we limit what we consider as an opinion
target, to the ones that appear in at least two posts
written by two different participants. Since, the
sentence may contain multiple possible targets for
every opinion expression, we associate each opin-
ion expression with the target that is closest to it in
the sentence. For each discussant, we keep track
of the targets mentioned in his/her posts and the
number of times each target was mentioned in a
positive/negative context.
3.4 Latent Textual Similarity
If two participants share the same opinion, they
tend to focus on similar aspects of the discus-
sion topic and emphasize similar points that sup-
port their opinion. To capture this, we follow
previous work (Guo and Diab, 2012; Dasigi et
al., 2012) and apply Latent Dirichelet Allocation
(LDA) topic models to the text written by the dif-
ferent participants. We use an LDA model with
100 topics. So, we represent all the text written
in the discussion by each participant as a vector
of 100 dimensions. The vector of each participant
contains the topic distribution of the participant, as
produced by the LDA model.
3.5 Subgroup Detection
At this point, we have for every discussant the tar-
gets towards which he/she expressed explicit opin-
ion and a 100-dimensions vector representing the
LDA distribution of the text written by him/her.
We use this information to represent the discussion
in two representations. In the first representation,
each discussant is represented by a vector. For ev-
ery target identified in steps 3 of the pipeline, we
add three entries in the vector. The first entry holds
the total number of times the target was mentioned
by the discussant. The second entry holds the
831
 ??? ???? ???? ?? ?????? ????? ???? ?????? ???? ???? ?????
?????? ??????? ?????? ?????? ?? ????? ???? ??? ???? ?? ??????? 
 ?????? ???? ??? ???? ?????? ??????? ????? ??? ???? ?????? ???????
 ?? ?? ?????? ???? ???? ???? ???? ??????? ???? ??????? ??????
 ????? ??? ?????? ????? ?? ????? ???? ??  ??????? ?? ?????
???? ?? ????? ?????? ?????? ??? ??? ?? ???? ?? ???? ??? ???? 
 ???? ??? ??? ???  ?????????? ???? ???? ??? ??? ?? ????
 ???????? ?????? ?? ?????? ??????? ?? ?? ???? ???????
 ?????? ???????? ???????? ??? ????? ?????? ?? ?????
????? ??? ???? ??? ???? ???????? ??? 
(a) 
(c) (b) 
Figure 1: An example debate taken from our dataset. (a) is the discussion topic. (b) and (c) are two posts
expressing contrasting viewpoints with respect to the topic.
number of times the target was mentioned in a pos-
itive context. The third entry holds the number of
target mentions in a negative context. We also add
to this vector the 100 topic entries from the LDA
vector of that discussant. So, if the number of tar-
gets identified in step 3 of the pipeline is t then
the number of entries in the discussant vector is
3 ? t+ 100.
To identify opinion subgroups, we cluster
the vector space. We experiment with several
clustering algorithms including K-means (Mac-
Queen, 1967), Farthest First (FF) (Hochbaum and
Shmoys, 1985; Dasgupta, 2002), and Expectation
Maximization (EM) (Dempster et al, 1977).
The second representation is a signed network
representation. In this representation, each dis-
cussant is represented by a node in a graph. Two
discussants are connected by an edge if they both
mention at least one common target in their posts.
If a discussant mentions a target multiple times in
different contexts with different polarities, the ma-
jority polarity is assumed as the opinion of this
discussant with respect to this target. A positive
sign is assigned to the edge connecting two discus-
sants if the number of targets that they have simi-
lar opinion towards is greater than the targets that
they have opposing opinion towards, otherwise a
negative sign is assigned to the edge.
To identify subgroups, we use a signed net-
work partitioning algorithm to partition the net-
work. Each resulting partition constitutes a sub-
group. Following (Hassan et al, 2012b), we use
the Dorian-Mrvar (1996) algorithm to partition the
signed network. The optimization criterion aims
to have dense positive links within groups and
dense negative links between groups.
The optimization function is as follows:
F (C) = ?? |NEG|+ (1? ?)? |POS| (1)
where C is the clustering under evaluation,
|NEG| is the number of negative links between
nodes in the same subgroup, |POS| is the number
of positive links between nodes in different sub-
groups, and ? is a parameter that specifies impor-
tance of the two terms. We set ? to 0.5 in all our
experiments.
Clusters are selected such that P (C) is mini-
mum. The best clustering that minimizes P (C) is
found by moving nodes around clusters in a greedy
way starting with a random clustering. To han-
dle the possibility of finding a local minima, the
whole process is repeated k times with random
restarts and the clustering with the minimum value
of P (C) is returned. We set k to 3 in all our ex-
periments.
4 Data
We use data from an Arabic discussion forum
called Naqeshny.2. Naqeshny is a platform for
two-sided debates. The debate starts when a per-
son asks a question (e.g. which political party do
you support?) and gives two possible answers or
positions. The registered members of the web-
site who are interested in the topic participate in
the debate by selecting a position and then post-
ing text to support that position and dispute the
2www.Naqeshny.com
832
opposing position. This means that the data set is
self-labeled for subgroup membership. Since the
tools used in our system are trained on Modern
Standard Arabic (MSA) text, we selected debates
that are mostly MSA. The data set consists of 36
debates comprising a total of 711 posts written by
326 users. The average number of posts per dis-
cussion is 19.75 and the average number of partic-
ipants per discussion is 13.08. Figure 1 shows an
example from the data.
5 Experiments and Results
We use three metrics to evaluate the resulting sub-
groups: Purity (Manning et al, 2008), Entropy,
and F-measure. We ran several variations of the
system on the data set described in the previous
section. In one variation, we use the signed net-
work partitioning approach to detect subgroups.
In the other variations, we use the vector space
clustering approach. We experiment with differ-
ent clustering algorithms. We also run two experi-
ments to evaluate the contribution of both opinion-
target counts and latent similarity features on the
clustering accuracy. In one run, we use target-
opinion counts only. In the other run, we use latent
similarity features only. EM was used as the clus-
tering algorithm in these two runs. Table 1 shows
the results. All the results have been tested for sta-
tistical significance using a 2-tailed paired t-test.
The differences between the results of the different
methods shown in the table are statistically signif-
icant at the 0.05 level. The results show that the
clustering approach achieves better results than the
signed network partitioning approach. This can be
explained by the fact that the vector representa-
tion is a richer representation and encodes all the
discussants? opinion information explicitly. The
results also show that Expectation Maximization
achieves significantly better results than the other
clustering algorithms that we experimented with.
The results also show that both latent text similar-
ity and opinion-target features are important and
contribute to the performance.
6 Conclusion
In this paper, we presented a system for iden-
tifying opinion subgroups in Arabic online dis-
cussions. The system uses opinion and text sim-
System Purity F-Measure Entropy
Signed Network 0.71 0.67 0.68
Clustering - K-means 0.72 0.70 0.67
Clustering - EM 0.77 0.76 0.50
Clustering - FF 0.72 0.69 0.70
Opinion-Target Only 0.67 0.65 0.72
Text Similarity Only 0.64 0.65 0.74
Table 1: Comparison of the different variations of
the proposed approach
ilarity features to encode discussants? opinions.
Two approaches were explored for detecting sub-
groups. The first approach clusters a space of dis-
cussant opinion vectors. The second approach par-
titions a signed network representation of the dis-
cussion. Our experiments showed that the former
approach achieves better results. Our experiments
also showed that both opinion and similarity fea-
tures are important.
Acknowledgements
This research was funded in part by the Office of
the Director of National Intelligence, Intelligence
Advanced Research Projects Activity. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the of?cial views or poli-
cies of IARPA, the ODNI or the U.S. Government.
The authors would like to thank Basma Siam for
her help with collecting the data.
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem.
2008. Sentiment analysis in multiple languages:
Feature selection for opinion classification in web
forums. ACM Trans. Inf. Syst., 26(3):12:1?12:34,
June.
Muhammad Abdul-Mageed and Mona Diab. 2011.
Subjectivity and sentiment annotation of modern
standard arabic newswire. In Proceedings of the
5th Linguistic Annotation Workshop, pages 110?
118, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Muhammad Abdul-Mageed and Mona Diab. 2012a.
Awatif: A multi-genre corpus for modern standard
arabic subjectivity and sentiment analysis. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet Ugur Dogan, Bente
Maegaard, Joseph Mariani, Jan Odijk, and Stelios
833
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).
Muhammad Abdul-Mageed and Mona Diab. 2012b.
Toward building a large-scale arabic sentiment lexi-
con. In Proceedings of the 6th International Global
Word-Net Conference, Matsue, Japan.
Muhammad Abdul-Mageed, Sandra Ku?bler, and Mona
Diab. 2012a. Samar: a system for subjectivity
and sentiment analysis of arabic social media. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
WASSA ?12, pages 19?28, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Muhammad Abdul-Mageed, Sandra Kuebler, and
Mona Diab. 2012b. Samar: A system for subjec-
tivity and sentiment analysis of arabic social me-
dia. In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 19?28, Jeju, Korea, July. Associa-
tion for Computational Linguistics.
Amjad Abu-Jbara and Dragomir Radev. 2012. Sub-
group detection in ideological discussions. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Jeju, Korea, July. The Associa-
tion for Computational Linguistics.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis (WASSA 2.011), pages 1?9,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In LREC?08.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Auto-
matic extraction of opinion propositions and their
holders. In 2004 AAAI Spring Symposium on Ex-
ploring Attitude and Affect in Text, page 2224.
Tim Buckwalter. 2004. Issues in arabic orthography
and morphology analysis. In Proceedings of the
Workshop on Computational Approaches to Arabic
Script-based Languages, Semitic ?04, pages 31?34,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sanjoy Dasgupta. 2002. Performance guarantees for
hierarchical clustering. In 15th Annual Conference
on Computational Learning Theory, pages 351?363.
Springer.
Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: A study of implicit attitude us-
ing textual latent semantics. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 65?69, Jeju Island, Korea, July. Association
for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. JOURNAL OF THE ROYAL STATIS-
TICAL SOCIETY, SERIES B, 39(1):1?38.
Mona Diab. 2009. Second generation tools (amira
2.0): Fast and robust tokenization, pos tagging, and
base phrase chunking. In Khalid Choukri and Bente
Maegaard, editors, Proceedings of the Second Inter-
national Conference on Arabic Language Resources
and Tools, Cairo, Egypt, April. The MEDAR Con-
sortium.
Patrick Doreian and Andrej Mrvar. 1996. A partition-
ing approach to structural balance. Social Networks,
18(2):149?168.
Gregory Grefenstette, Yan Qu, James G Shanahan, and
David A Evans. 2004. Coupling niche browsers
and affect analysis for an opinion mining applica-
tion. In Proceedings of RIAO, volume 4, pages 186?
194. Citeseer.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
864?872, Jeju Island, Korea, July. Association for
Computational Linguistics.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir
Radev. 2010. What?s with the attitude?: identi-
fying sentences with attitude in online discussions.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1245?1255.
Ahmed Hassan, Amjad Abu-Jbara, Rahul Jha, and
Dragomir Radev. 2011. Identifying the semantic
orientation of foreign words. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: short papers - Volume 2, HLT ?11, pages 592?
597, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir
Radev. 2012a. Extracting signed social networks
from text. In Workshop Proceedings of TextGraphs-
7: Graph-based Methods for Natural Language Pro-
cessing, pages 6?14, Jeju, Republic of Korea, July.
Association for Computational Linguistics.
834
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir
Radev. 2012b. Signed attitude networks: Predict-
ing positive and negative links using linguistic anal-
ysis. In Submitted to the Conference on Emprical
Methods in Natural Language Processing, Jeju, Ko-
rea, July. The Association for Computational Lin-
guistics.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000a.
Effects of adjective orientation and gradability on
sentence subjectivity. In COLING, pages 299?305.
Vasileios Hatzivassiloglou and Janyce M Wiebe.
2000b. Effects of adjective orientation and grad-
ability on sentence subjectivity. In Proceedings of
the 18th conference on Computational linguistics-
Volume 1, pages 299?305. Association for Compu-
tational Linguistics.
Hochbaum and Shmoys. 1985. A best possible heuris-
tic for the k-center problem. Mathematics of Oper-
ations Research, 10(2):180?184.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In EMNLP?06, pages
355?363.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
97(02):311?331.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: identifying perspectives at the document and
sentence levels. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 109?116. Association for Computational Lin-
guistics.
Bing Liu and Lei Zhang. 2012. A survey of opin-
ion mining and sentiment analysis. In Charu C. Ag-
garwal and ChengXiang Zhai, editors, Mining Text
Data, pages 415?463. Springer US.
J. B. MacQueen. 1967. Some methods for classifi-
cation and analysis of multivariate observations. In
L. M. Le Cam and J. Neyman, editors, Proc. of the
fifth Berkeley Symposium on Mathematical Statistics
and Probability, volume 1, pages 281?297. Univer-
sity of California Press.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schtze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Orena Etzioni. 2007. Extract-
ing product features and opinions from reviews. In
Natural language processing and text mining, pages
9?28. Springer.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.
2006. Feature subsumption for opinion analysis.
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
440?448. Association for Computational Linguis-
tics.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 226?234, Suntec, Singapore, August.
Association for Computational Linguistics.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In ACL?05, pages 133?140.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21:315?346.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of
Artificial Intelligence, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05, Van-
couver, Canada.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010.
Grouping product features using semi-supervised
learning with soft-constraints. In Proceedings of the
23rd International Conference on Computational
Linguistics, pages 1272?1280. Association for Com-
putational Linguistics.
835
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 328?334,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UMichigan: A Conditional Random Field Model for Resolving the Scope of
Negation
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In this paper, we present a system for de-
tecting negation in English text. We address
three tasks: negation cue detection, negation
scope resolution and negated event identifi-
cation. We pose these tasks as sequence la-
beling problems. For each task, we train a
Conditional Random Field (CRF) model on
lexical, structural, and syntactic features ex-
tracted from labeled data. The models are
trained and tested using the dataset distributed
with the *sem Shared Task 2012 on resolving
the scope and focus of negation. The system
detects negation cues with 90.98% F1 mea-
sure (94.3% and 87.88% recall). It identifies
negation scope with 82.70% F1 on token-by-
token level and 64.78% F1 on full scope level.
Negated events are detected with 51.10% F1
measure.
1 Introduction
Negation is a linguistic phenomenon present in all
languages (Tottie, 1993; Horn, 1989). The seman-
tic function of negation is to transform an affirma-
tive statement into its opposite meaning. The auto-
matic detection of negation and its scope is a prob-
lem encountered in a wide range of natural language
processing applications including, but not limited to,
data mining, relation extraction, question answering,
and sentiment analysis. For example, failing to ac-
count for negation may result in giving wrong an-
swers in question answering systems or in the pre-
diction of opposite sentiment in sentiment analysis
systems.
The occurrence of negation in a sentence is deter-
mined by the presence of a negation cue. A nega-
tion cue is a word, a phrase, a prefix, or a postfix
that triggers negation. Scope of negation is the part
of the meaning that is negated (Huddleston and Pul-
lum, 2002). The negated event is the event or the en-
tity that the negation indicates its absence or denies
its occurrence. For example, in the sentence below
never is the negation cue. The scope is enclosed in
square brackets. The negated event is underlined.
[Andrew had] never [liked smart phones],
but he received one as a gift last week and
started to use it.
Negation cues and scopes may be discontinuous.
For example, the negation cue neither ... nor is dis-
continuous.
In this chapter, we present a system for automat-
ically detecting negation cues, negated events, and
negation scopes in English text. The system uses
conditional random field (CRF) models trained on
labeled sentences extracted from two classical En-
glish novels. The CRF models are trained using lex-
ical, structural, and syntactic features. The experi-
ments show promising results.
This paper is organized as follows. Section 2 re-
views previous work. Section 3 describes the data.
Section 4 describes the CRFs models. Section 5
presents evaluation, results, and discussion.
2 Previous Work
Most research on negation has been done in the
biomedical domain (Chapman et al, 2001; Mutalik
et al, 2001; Kim and Park, 2006; Morante et al,
328
Token Lemma POS Syntax Cue 1 Scope 1 Event 1 Cue 2 Scope 2 Event 2
She She PRP (S(NP*) - She - - - -
would would MD (VP* - would - - - -
not not RB * not - - - - -
have have VB (VP* - have - - - -
said say VBD (VP* - said - - - -
? ? ? (SBAR(S(NP* - ? - - - -
Godspeed Godspeed NNP * - Godspeed - - - -
? ? ? *) - ? - - - -
had have VBD (VP* - had - - had -
it it PRP (ADVP* - it - - it -
not not RB *) - not - not - -
been be VBN (VP* - been - - been -
so so RB (ADVP*)))))))) - so - - so -
. . . *) - - - - - -
Table 1: Example sentence annotated for negation following sem shared task 2012 format
2008a; Morante and Daelemans, 2009; Agarwal and
Yu, 2010; Morante, 2010; Read et al, 2011), mostly
on clinical reports. The reason is that most NLP re-
search in the biomedical domain is interested in au-
tomatically extracting factual relations and pieces of
information from unstructured data. Negation detec-
tion is important here because information that falls
in the scope of a negation cue cannot be treated as
facts.
Chapman et al (2001) proposed a rule-based al-
gorithm called NegEx for determining whether a
finding or disease mentioned within narrative med-
ical reports is present or absent. The algorithm
uses regular-expression-based rules. Mutalik et
al. (2001) developed another rule based system
called Negfinder that recognizes negation patterns
in biomedical text. It consists of two components:
a lexical scanner, lexer that uses regular expres-
sion rules to generate a finite state machine, and a
parser. Morante (2008b) proposed a supervised ap-
proach for detecting negation cues and their scopes
in biomedical text. Their system consists of two
memory-based engines, one that decides if the to-
kens in a sentence are negation signals, and another
one that finds the full scope of these negation sig-
nals.
Negation has been also studied in the context of
sentiment analysis (Wilson et al, 2005; Jia et al,
2009; Councill et al, 2010; Heerschop et al, 2011;
Hogenboom et al, 2011). Wiegand et al (2010) sur-
veyed the recent work on negation scope detection
for sentiment analysis. Wilson et al (2005) studied
the contextual features that affect text polarity. They
used a machine learning approach in which nega-
tion is encoded using several features. One feature
checks whether a negation expression occurs in a
fixed window of four words preceding the polar ex-
pression. Another feature accounts for a polar pred-
icate having a negated subject. They also have dis-
ambiguation features to handle negation words that
do not function as negation cues in certain contexts,
e.g. not to mention and not just.
Jia et al (2009) proposed a rule based method to
determine the polarity of sentiments when one or
more occurrences of a negation term such as not ap-
pear in a sentence. The hand-crafted rules are ap-
plied to syntactic and dependency parse tree repre-
sentations of the sentence.
Hogenboom et al (2011) found that applying a
simple rule that considers two words, following a
negation keyword, to be negated by that keyword,
to be effective in improving the accuracy of senti-
ment analysis in movie reviews. This simple method
yields a significant increase in overall sentiment
classification accuracy and macro-level F1 of 5.5%
and 6.2%, respectively, compared to not accounting
for negation.
This work is characterized by addressing three
tasks at once: negation cue detection, negated
event identification, and negation scope resolution.
Our proposed approach uses a supervised graphical
probabilistic model trained using labeled data.
329
3 Data
We use the dataset distributed by the organizers of
the *sem Shared Task 2012 on resolving the scope
and focus of negation. This dataset includes two sto-
ries by Conan Doyle, The Hound of the Baskervilles,
The Adventures of Wisteria Lodge. All occur-
rences of negation are annotated accounting for
negation expressed by nouns, pronouns, verbs, ad-
verbs, determiners, conjunctions and prepositions.
For each negation cue, the negation cue and scope
are marked, as well as the negated event (if any ex-
ists). The annotation guidelines follow the proposal
of Morante et al (2011)1. The data is split into three
sets: a training set containing 3,644 sentences, a de-
velopment set containing 787 sentences, and a test-
ing set containing 1,089 sentences. The data is pro-
vided in CoNLL format. Each line corresponds to a
token and each annotation is provided in a column;
empty lines indicate end of sentences. The provided
annotations are:
? Column 1: chapter name
? Column 2: sentence number within chapter
? Column 3: token number within sentence
? Column 4: word
? Column 5: lemma
? Column 6: part-of-speech
? Column 7: syntax
? Columns 8 to last:
? If the sentence has no negations, column
8 has a ?***? value and there are no more
columns.
? If the sentence has negations, the annota-
tion for each negation is provided in three
columns. The first column contains the
word or part of the word (e.g., morpheme
?un?), that belongs to the negation cue.
The second contains the word or part of
the word that belongs to the scope of the
negation cue. The third column contains
the word or part of the word that is the
1http://www.clips.ua.ac.be/sites/default/files/ctrs-n3.pdf
Token Lemma Punc. Cat. POS Label
Since Since 0 OTH IN O
we we 0 PRO PRP O
have have 0 VB VBP O
been be 0 VB VBN O
so so 0 ADVB RB O
unfortunate unfortunate 0 ADJ JJ PRE
as as 0 ADVB RB O
to to 0 OTH TO O
miss miss 0 VB VB O
him him 0 PRO PRP O
and and 0 OTH CC O
have have 0 VB VBP O
no no 0 OTH DT NEG
notion notion 0 NOUN NN O
of of 0 OTH IN O
his his 0 PRO PRP$ O
errand errand 0 NOUN NN O
, , 1 OTH , O
this this 0 OTH DT O
accidental accidental 0 ADJ JJ O
souvenir souvenir 0 NOUN NN O
becomes become 0 VB VBZ O
of of 0 OTH IN O
importance importance 0 NOUN NN O
. . 1 OTH . O
Table 2: Example sentence labeled for negation cue de-
tection
negated event or property. It can be the
case that no negated event or property are
marked as negated.
Table 1 shows an example of an annotated sen-
tence that contains two negation cues.
4 Approach
The problem that we are trying to solve can be split
into three tasks. The first task is to detect negation
cues. The second task is to identify the scope of each
detected negation cue. The third task is to identify
the negated event. We use a machine learning ap-
proach to address these tasks. We train a Condi-
tional Random Field (CRF) (Lafferty et al, 2001)
model on lexical, structural, and syntactic features
extracted from the training dataset. In the following
subsections, we describe the CRF model that we use
for each task.
4.1 Negation Cue Detection
Negation cues are lexical elements that indicate the
existence of negation in a sentence. From lexical
330
point of view, negation cues can be divided into four
categories:
1. Prefix (i.e. in-, un-, im-, il-, dis-). For example,
un- in unsuitable) is a prefix negation cue.
2. Postfix (i.e. -less). for example, -less in
careless.
3. Multi-word negation cues such as neither...nor,
rather than, by no means, etc.
4. Single word negation cues such as not, no,
none, nobody, etc.
The goal of this task is to detect negation cues.
We pose this problem as a sequence labeling task.
The reason for this choice is that some negation cues
may not indicate negation in some contexts. For
example, the negation cue not in the phrase not to
mention does not indicate negation. Also, as we saw
above, some negation cues may consist of multiple
words, some of them are continuous and others are
discontinuous. Treating the task as a sequence label-
ing problem help model the contextual factors that
affect the function of negation cues. We train a CRF
model using features extracted from the sentences of
the training dataset. The token level features that we
train the model on are:
? Token: The word or the punctuation mark as it
appears in the sentence.
? Lemma: The lemmatized form of the token.
? Part-Of-Speech tag: The part of speech tag of
the token.
? Part-Of-Speech tag category: Part-of-speech
tags reduced into 5 categories: Adjec-
tive (ADJ), Verb (VB), Noun (NN), Adverb
(ADVB), Pronoun (PRO), and other (OTH).
? Is punctuation mark: This feature takes the
value 1 if the token is a punctuation mark and 0
otherwise.
? Starts with negation prefix: This feature takes
the value 1 if the token is a word that starts with
un-, in-, im-, il-, or dis- and 0 otherwise.
? Ends with negation postfix: This feature takes
the value 1 if the token is a word that ends with
-less and 0 otherwise.
The CRF model that we use considers at each to-
ken the features of the current token, the two pre-
ceding tokens, and the two proceeding tokens. The
model also uses token bigrams and trigrams, and
part-of-speech tag bigrams and trigrams as features.
The labels are 5 types: ?O? for tokens that are
not part of any negation cue; ?NEG? for single
word negation cues; ?PRE? for prefix negation cue;
?POST? for postfix negation cue; and ?MULTI-
NEG? for multi-word negation cues. Table 2 shows
an example labeled sentence.
At testing time, if a token is labeled ?NEG? or
?MULTI-NEG? the whole token is treated as a nega-
tion cue or part of a negation cue respectively. If a
token is labeled as ?PRE? or ?POST?, a regular ex-
pression is used to determine the prefix/postfix that
trigged the negation.
4.2 Negation Scope Detection
Scope of negation is the sequence of tokens (can
be discontinuous) that expresses the meaning that
is meant to be negated by a negation cue. A sen-
tence may contain zero or more negation cues. Each
negation cue has its own scope. It is possible that
the scope of two negation cues overlap. We use
each negation instance (i.e. each negation cue and
its scope) as one training example. Therefore, a
sentence that contains two negation cues provides
two training examples. We train a CRF model on
features extracted from all negation instances in the
training dataset. The features that we use are:
? Token: The word or the punctuation mark as it
appears in the sentence.
? Lemma: The lemmatized form of the token.
? Part-Of-Speech tag: The part of speech tag of
the token.
? Part-Of-Speech tag category: Part-of-speech
tags reduced into 5 categories: Adjec-
tive (ADJ), Verb (VB), Noun (NN), Adverb
(ADVB), Pronoun (PRO), and other (OTH).
331
? Is punctuation mark: This feature takes the
value 1 if the token is a punctuation mark and 0
otherwise.
? Type of negation cue: Possible types are:
?NEG? for single word negation cues; ?PRE?
for prefix negation cue; ?POST? for postfix
negation cue; and ?MULTI? for multi-word
negation cues.
? Relative position: This feature takes the value
1 if the token position in the sentence is be-
fore the position of the negation cue, 2 if the
token position is after the position of the nega-
tion cue, and 3 if the token is the negation cue
itself.
? Distance: The number of tokens between the
current token and the negation cue.
? Same segment: This feature takes the value 1
if this token and the negation cue fall in the
segment in the sentence. The sentence is seg-
mented by punctuation marks.
? Chunk: This feature takes the value NP-B (VP-
B) if this token is the first token of a noun (verb)
phrase, NP-I (VP-I) if it is inside a noun (verb)
phrase, NP-E (VP-E) if it is the last token of a
noun (verb) phrase.
? Same chunk: This feature takes the value 1 if
this token and the negation cue fall in the same
chunk (noun phrase or verb phrase).
? Is negation: This feature takes the value 1 if
this token is a negation cue, and 0 otherwise.
? Syntactic distance: The number of edges in the
shortest path that connects the token and the
negation in the syntactic parse tree.
? Common ancestor node: The type of the node
in the syntactic parse tree that is the least com-
mon ancestor of this token and the negation cue
token.
The CRF model considers the features of 4 tokens
to the left and to the right at each position. It also
uses bigram and trigram combinations of some of
the features.
At testing time a few postprocessing rules are
used to fix sure labels if they were labeled incor-
rectly. For example, if a word starts with a prefix
negation cue, the word itself (without the prefix) is
always part of the scope and it is also the negated
event.
4.3 Negated Event Identification
It is possible that a negation cue comes associated
with an event. A negation has an event if it oc-
curs in a factual context. The dataset that we use
was labeled for negated events whenever one exists.
We used the same features described in the previous
subsection to train a CRF model for negated event
identification. We have also tried to use one CRF
model for both scope resolution and negated event
identification, but we noticed that using two sepa-
rate models results in significantly better results for
both tasks.
5 Evaluation
We use the testing set described in Section 3 to eval-
uate the system. The testing set contains 1089 sen-
tences 235 of which contains at least one negation.
We use the standard precision, recall, and f-
measure metrics to evaluate the system. We perform
the evaluation on different levels:
1. Cues: the metrics are computed only for cue
detection.
2. Scope (tokens): the metrics are calculated at to-
ken level. If a sentence has 2 scopes, one with
5 tokens and another with 4, the total number
of scope tokens is 9.
3. Scope (full): the metrics are calculated at the
full scope level. Both the negation cue and
the whole scope should be correctly identified.
If a sentence contains 2 negation cues, then 2
scopes are checked. We report two values here
one the requires the cue match correctly and
one that does not.
4. Negated Events: the metrics are computed only
for negated events identification (apart from
negation cue and scope).
332
Variant A
gold system tp fp fn precision recall F1
Cues 264 250 232 14 32 94.31 87.88 90.98
Scope (cue match) 249 227 126 14 123 90.00 50.60 64.78
Scope (no cue match) 249 227 126 14 123 90.00 50.60 64.78
Scope (tokens - no cue match) 1805 1716 1456 260 349 84.85 80.66 82.70
Negated (no cue match) 173 183 70 70 64 50.00 52.24 51.10
Full negation: 264 250 75 14 189 84.27 28.41 42.49
Variant B
gold system tp fp fn precision recall F1
Cues : 264 250 232 14 32 92.80 87.88 90.27
Scope (cue match): 249 227 126 14 123 55.51 50.60 52.94
Scope (no cue match): 249 227 126 14 123 55.51 50.60 52.94
Negated (no cue match): 173 183 70 70 64 38.25 52.24 44.16
Full negation : 264 250 75 14 189 30.00 28.41 29.18
# Sentences 1089
# Negation sentences 235
# Negation sentences with errors 171
% Correct sentences 83.47
% Correct negation sentences 27.23
Table 3: Results of negation cue, negated event, and negation scope detection
5. Full negation: the metrics are computed for all
the three tasks at once and requiring everything
to match correctly.
For cue, scope and negated event to be correct,
both the tokens and the words or parts of words have
to be correctly identified. The final periods in abbre-
viations are disregarded. If gold has value ?Mr.? and
system ?Mr?, system is counted as correct. Also,
punctuation tokens are *not* taken into account for
evaluation.
Two variants of the metrics are computed. In the
first variant (A), precision is calculated as tp / (tp +
fp) and recall is calculated as tp / (tp + fn) where tp
is the count of true positive labels, fp is the count
of false positive labels, and fn is the count of false
negative labels. In variant B, the precision is calcu-
lated differently, using the formula precision = tp /
system.
Table 3 shows the results of our system.
6 Error Analysis
The system used no external resources outside the
training data. This means that the system recognizes
only negation cues that appeared in the training set.
This was the first source of error. For example, the
word unacquainted that starts with the negation pre-
fix un has never been seen in the training data. In-
tuitively, if no negation cue is detected, the system
does not attempt to produce scope levels. This prob-
lem can be overcome by using a lexicon of negation
words and those words that can be negated by adding
a negation prefix to them.
We noticed in several occasions that scope detec-
tion accuracy can be improved if some simple rules
can be imposed after doing the initial labeling us-
ing the CRF model (but we have not actually imple-
mented any such rules in the system). For example,
the system can require all the tokens that belong to
the same chunk (noun group, verb group, etc.) all
have the same label (e.g. the majority vote label).
The same thing could be also applied on the segment
rather than the chunk level where the boundaries of
segments are determined by punctuation marks.
7 Conclusion
We presented a supervised system for identifying
negation in English sentences. The system uses
three CRF trained models. One model is trained for
negation cue detection. Another model is trained
for negated event identification. A third one is
trained for negation scope identification. The mod-
els are trained using features extracted from a la-
beled dataset. Our experiments show that the system
achieves promising results.
333
References
Shashank Agarwal and Hong Yu. 2010. Biomedi-
cal negation scope detection with conditional random
fields. Journal of the American Medical Informatics
Association, 17(6):696?701.
Wendy Webber Chapman, Will Bridewell, Paul Hanbury,
Gregory F. Cooper, and Bruce G. Buchanan. 2001. A
simple algorithm for identifying negated findings and
diseases in discharge summaries. Journal of Biomedi-
cal Informatics, pages 301?310.
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing, NeSp-NLP ?10, pages 51?59, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Bas Heerschop, Paul van Iterson, Alexander Hogenboom,
Flavius Frasincar, and Uzay Kaymak. 2011. Analyz-
ing sentiment in a large set of web data while account-
ing for negation. In AWIC, pages 195?205.
Alexander Hogenboom, Paul van Iterson, Bas Heerschop,
Flavius Frasincar, and Uzay Kaymak. 2011. Deter-
mining negation scope and strength in sentiment anal-
ysis. In SMC, pages 2589?2594.
Laurence R. Horn. 1989. A natural history of nega-
tion / Laurence R. Horn. University of Chicago Press,
Chicago :.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.
Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The
effect of negation on sentiment analysis and retrieval
effectiveness. In Proceedings of the 18th ACM con-
ference on Information and knowledge management,
CIKM ?09, pages 1827?1830, New York, NY, USA.
ACM.
Jung-Jae Kim and Jong C. Park. 2006. Extracting con-
trastive information from negation patterns in biomed-
ical literature. 5(1):44?60, March.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Roser Morante and Walter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. Pro-
ceedings of the Workshop on BioNLP BioNLP 09,
(June):28.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008a. Learning the scope of negation in
biomedical texts. Proceedings of the Conference on
Empirical Methods in Natural Language Processing
EMNLP 08, (October):715?724.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008b. Learning the scope of negation in
biomedical texts. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 715?724, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope. Technical report.
Roser Morante. 2010. Descriptive analysis of negation
cues in biomedical texts. Language Resources And
Evaluation, pages 1?8.
P. G. Mutalik, A. Deshpande, and P. M. Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents: a quantitative
study using the UMLS. Journal of the American Med-
ical Informatics Association : JAMIA, 8(6):598?609.
Jonathon Read, Erik Velldal, Stephan Oepen, and Lilja
vrelid. 2011. Resolving speculation and negation
scope in biomedical articles with a syntactic con-
stituent ranker. In Proceedings of the Fourth Inter-
national Symposium on Languages in Biology and
Medicine, Singapore.
Gunnel Tottie. 1993. Negation in English Speech and
Writing: A Study in Variation. Language, 69(3):590?
593.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing, NeSp-NLP
?10, pages 60?68, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
334
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 1?12,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Rediscovering ACL Discoveries Through the Lens of ACL Anthology
Network Citing Sentences
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Abstract
The ACL Anthology Network (AAN)1 is a
comprehensive manually curated networked
database of citations and collaborations in the
field of Computational Linguistics. Each cita-
tion edge in AAN is associated with one or
more citing sentences. A citing sentence is
one that appears in a scientific article and con-
tains an explicit reference to another article. In
this paper, we shed the light on the usefulness
of AAN citing sentences for understanding re-
search trends and summarizing previous dis-
coveries and contributions. We also propose
and motivate several different uses and appli-
cations of citing sentences.
1 Introduction
The ACL Anthology2 is one of the most success-
ful initiatives of the Association for Computational
Linguistics (ACL). It was initiated by Steven Bird
in 2001 and is now maintained by Min-Yen Kan. It
includes all papers published by ACL and related or-
ganizations as well as the Computational Linguistics
journal over a period of four decades.
The ACL Anthology Network (AAN) is another
successful initiative built on top of the ACL Anthol-
ogy. It was started in 2007 by our group (Radev
et al, 2009) at the University of Michigan. AAN
provides citation and collaboration networks of the
articles included in the ACL Anthology (excluding
book reviews). AAN also includes rankings of pa-
pers and authors based on their centrality statistics
1http://clair.si.umich.edu/anthology/
2http://www.aclweb.org/anthology-new/
in the citation and collaboration networks. It also
includes the citing sentences associated with each
citation link. These sentences were extracted auto-
matically using pattern matching and then cleaned
manually. Table 1 shows some statistics of the cur-
rent release of AAN.
The text surrounding citations in scientific publi-
cations has been studied and used in previous work.
Nanba and Okumura (1999) used the term citing
area to refer to citing sentences. They define the cit-
ing area as the succession of sentences that appear
around the location of a given reference in a scien-
tific paper and has connection to it. They proposed
a rule-based algorithm to identify the citing area of
a given reference. In (Nanba et al, 2000) they use
their citing area identification algorithm to identify
the purpose of citation (i.e. the author?s reason for
citing a given paper.)
Nakov et al (2004) use the term citances to refer
to citing sentences. They explored several different
uses of citances including the creation of training
and testing data for semantic analysis, synonym set
creation, database curation, summarization, and in-
formation retrieval.
Other previous studies have used citing sentences
in various applications such as: scientific paper
summarization (Elkiss et al, 2008; Qazvinian and
Radev, 2008; Mei and Zhai, 2008; Qazvinian et al,
2010; Qazvinian and Radev, 2010; Abu-Jbara and
Radev, 2011a), automatic survey generation (Nanba
et al, 2000; Mohammad et al, 2009), and citation
function classification (Nanba et al, 2000; Teufel
et al, 2006; Siddharthan and Teufel, 2007; Teufel,
2007).
1
Number of papers 18,290
Number of authors 14,799
Number of venues 341
Number of paper citations 84,237
Citation network diameter 22
Collaboration network diameter 15
Number of citing sentences 77,753
Table 1: Statistics of AAN 2011 release
In this paper, we focus on the usefulness of the
citing sentences included in AAN. We propose sev-
eral uses of citing sentences such as analyzing the
trends of research, understanding the impact of re-
search and how this impact changes over time, sum-
marizing the contributions of a researcher, summa-
rizing the discoveries in a certain research field,
and providing high quality data for Natural Lan-
guage Processing tasks. In the rest of this paper
we present some of these ideas and provide exam-
ples from AAN to demonstrate their applicability.
Some of these ideas have been explored in previous
work, but we believe that they still need further ex-
ploration. However, most of the ideas are novel to
our knowledge. We present our ideas in the follow-
ing sections.
2 Temporal Analysis of Citations
The interest in studying citations stems from the fact
that bibliometric measures are commonly used to es-
timate the impact of a researcher?s work (Borgman
and Furner, 2002; Luukkonen, 1992). Several pre-
vious studies have performed temporal analysis of
citation links (Amblard et al, 2011; Mazloumian et
al., 2011; Redner, 2005) to see how the impact of
research and the relations between research topics
evolve overtime. These studies focused on observ-
ing how the number of incoming citations to a given
article or a set of related articles change over time.
However, the number of incoming citations is often
not the only factor that changes with time. We be-
lieve that analyzing the text of citing sentences al-
lows researchers to observe the change in other di-
mensions such as the purpose of citation, the polarity
of citations, and the research trends. The following
subsections discuss some of these dimensions.
Comparison Contrast/Comparison in Results, Method, or
Goals
Basis Author uses cited work as basis or starting point
Use Author uses tools, algorithms, data, or defini-
tions
Description Neutral description of cited work
Weakness Limitation or weakness of cited work
Table 2: Annotation scheme for citation purpose
2.1 Temporal Analysis of Citation Purpose
Teufel et al (2006) has shown that the purpose of
citation can be determined by analyzing the text of
citing sentences. We hypothesize that performing
a temporal analysis of the purpose for citing a pa-
per gives a better picture about its impact. As a
proof of concept, we annotated all the citing sen-
tences in AAN that cite the top 10 cited papers from
the 1980?s with citation purpose labels. The labels
we used for annotation are based on Teufel et al?s
annotation scheme and are described in Table 2. We
counted the number of times the paper was cited
for each purpose in each year since its publication
date. This analysis revealed interesting observations
about the paper impacts. We will discuss these ob-
servations in Section 2.3. Figure 1 shows the change
in the ratio of each purpose with time for Shieber?s
(1985) work on parsing.
2.2 Temporal Analysis of Citation Polarity
The bibliometric measures that are used to estimate
the impact of research are often computed based on
the number of citations it received. This number is
taken as a proxy for the relevance and the quality of
the published work. It, however, ignores the fact that
citations do not necessarily always represent posi-
tive feedback. Many of the citations that a publica-
tion receives are neutral citations, and citations that
represent negative criticism are not uncommon. To
validate this intuition, we annotated about 2000 cit-
ing sentences from AAN for citation polarity. We
found that only 30% of citations are positive, 4.3%
are negative, and the rest are neutral. In another pub-
lished study, Athar (2011) annotated 8736 citations
from AAN with their polarity and found that only
10% of citations are positive, 3% are negative and
the rest were all neutral. We believe that consider-
ing the polarity of citations when conducting tem-
poral analysis of citations gives more insight about
2
-10
0
10
20
30
40
50
60
70
80
1985-1987 1987-1989 1989-1991 1991-1993 1993-1995 1995-1997 1997-1999 1999-2001 2001-2003 2007-2009
comparison
basis
using
weakness
descriptive
Figure 1: Change in the citation purpose of Shieber (1985) paper
0
20
40
60
80
100
120 NeutralPosivtiveNegative
Figure 2: Change in the polarity of the sentences citing
Church (1988) paper
how the way a published work is perceived by the re-
search community over time. As a proof of concept,
we annotated the polarity of citing sentences for the
top 10 cited papers in AAN that were published in
the 1980?s. We split the year range of citations into
two-year slots and counted the number of positive,
negative, and neutral citations that each paper re-
ceived during that time slot. We observed how the
ratios of each category changed overtime. Figure 2
shows the result of this analysis when applied to the
work of Kenneth Church (1988) on part-of-speech
tagging.
2.3 Predict Emergence of New Techniques or
Decline of Impact of Old Techniques.
The ideas discussed in Sections 2.1 and 2.2 and the
results illustrated in Figures 1 and 2 suggest that
studying the change in citation purpose and cita-
tion polarity allow us to predict the emergence of
new techniques or the decline in impact of old tech-
niques. For example, the analysis illustrated in Fig-
ure 2 shows that the work of Ken Church (1988)
on part-of-speech tagging received significant posi-
tive feedback during the 1990s and until early 2000s
before it started to receive more negative feedback.
This probably can be explained by the emergence
of better statistical models for part-of-speech (POS)
tagging (e.g. Conditional Random Fields (Lafferty
et al, 2001)) that outperformed Church?s approach.
However, as indicated by the neutral citation curve,
Church?s work continued to be cited as a classical
pioneering research on the POS tagging task, but
not as the state-of-the-art approach. Similar anal-
ysis can be applied to the change in citation purpose
of Shieber (1985) as illustrated in Figure 1
2.4 Study the Dynamics of Research
In recent research, Gupta and Manning (2011) con-
ducted a study that tries to understand the dynamics
of research in computational linguistics (CL). They
analyzed the abstracts of CL papers included in the
ACL Anthology Reference Corpus. They extracted
the contributions, the domain of application, and the
3
apply propose extend system
Abstracts 1368 2856 425 5065
Citing Sentences 2534 3902 917 6633
Table 3: Comparison of trigger word occurrences in ab-
stracts vs citing sentences.
techniques and tools used in each paper. They com-
bined this information with pre-calculated article-to-
community assignments to study the influence of a
community on others in terms of techniques bor-
rowed and the maturing of some communities to
solve problems from other domains. We hypothe-
size that conducting such an analysis using the cit-
ing sentences of papers instead of (or in combination
with) abstracts leads to a more accurate picture of
research dynamics and the interaction between dif-
ferent research communities. There are several intu-
itions that support this hypothesis.
First, previous research (Elkiss et al, 2008) has
shown that the citing sentences that cite a paper are
more focused and more concise than the paper ab-
stract, and that they consistently contain additional
information that does not appear in abstracts. This
means that additional characteristics of a paper can
be extracted from citing sentences that cannot be
extracted from abstracts. To verify this, we com-
pared abstracts vs citing sentences (within AAN)
in terms of the number of occurrences of the trig-
ger words that Gupta and Manning (2011) deemed
to be indicative of paper characteristics (Table 3).
All the abstracts and citing sentences included in
the 2011 release of AAN were used to get these
numbers. The numbers clearly show that the trig-
ger words appear more frequently in the set of cit-
ing sentences of papers than they do in the paper
abstracts. We also found many papers that none of
the trigger words appeared in their abstracts, while
they do appear in their citing sentences. This sug-
gests that more paper properties (contributions, tech-
niques used, etc.) could be extracted from citations
than from abstracts.
Second, while the contributions included in an ab-
stract are the claims of the paper author(s), the con-
tributions highlighted in citing sentences are collec-
tively deemed to be important by peer researchers.
This means that the contributions extracted from ci-
Rank
word 1980s 1990s 2000s
grammar 22 71 123
model 75 72 26
rules 77 89 148
statistical - 69 74
syntax 257 1018 683
summarization - 880 359
Table 4: Ranks of selected keywords in citing sentences
to papers published in 80s, 90s and 2000s
tations are more important from the viewpoint of the
community and are likely to reflect research trends
more accurately.
We performed another simple experiment that
demonstrates the use of citing sentences to track the
changes in the focus of research. We split the set of
citing sentences in AAN into three subsets: the set
of citing sentences that cite papers from 1980s, the
set of citing sentences that cite papers from 1990s,
and the set of citing sentences that cite papers from
2000s. We counted the frequencies of words in each
of the three sets. Then, we ranked the words in each
set by the decreasing order of their frequencies. We
selected a number of keywords and compared their
ranks in the three year ranges. Some of these key-
words are listed in Table 4. This analysis shows, for
example, that there was more focus on ?grammar? in
the computational linguistics research in the 1980s
then this focus declined with time as indicated by the
lower rank of the keyword ?grammar? in the 1990s
and 2000s. Similarly, rule based methods were pop-
ular in the 1980s and 1990s but their popularity de-
clined significantly in the 2000s.
3 Scientific Literature Summarization
Using Citing Sentences
The fact that citing sentences cover different aspects
of the cited paper and highlight its most important
contributions motivates the idea of using citing sen-
tences to summarize research. The comparison that
Elkiss et al (2008) performed between abstracts and
citing sentences suggests that a summary generated
from citing sentences will be different and proba-
bly more concise and informative than the paper
abstract or a summary generated from the full text
of the paper. For example, Table 5 shows the ab-
stract of Resnik (1999) and 5 selected sentences that
cite it in AAN. We notice that citing sentences con-
4
tain additional facts that are not in the abstract, not
only ones that summarize the paper contributions,
but also those that criticize it (e.g., the last citing
sentence in the Table).
Previous work has explored this research direc-
tion. Qazvinian and Radev (2008) proposed a
method for summarizing scientific articles by build-
ing a similarity network of the sentences that cite
it, and then applying network analysis techniques to
find a set of sentences that covers as much of the
paper facts as possible. Qazvinian et al (2010) pro-
posed another summarization method that first ex-
tracts a number of important key phrases from the
set of citing sentences, and then finds the best sub-
set of sentences that covers as many key phrases as
possible.
These works focused on analyzing the citing sen-
tences and selecting a representative subset that cov-
ers the different aspects of the summarized article.
In recent work, Abu-Jbara and Radev (2011b) raised
the issue of coherence and readability in summaries
generated from citing sentences. They added a pre-
processing and postprocessing steps to the summa-
rization pipeline. In the preprocessing step, they use
a supervised classification approach to rule out ir-
relevant sentences or fragments of sentences. In the
postprocessing step, they improve the summary co-
herence and readability by reordering the sentences,
removing extraneous text (e.g. redundant mentions
of author names and publication year).
Mohammed et al (2009) went beyond single pa-
per summarization. They investigated the useful-
ness of directly summarizing citation texts in the
automatic creation of technical surveys. They gen-
erated surveys from a set of Question Answering
(QA) and Dependency Parsing (DP) papers, their ab-
stracts, and their citation texts. The evaluation of the
generated surveys shows that both citation texts and
abstracts have unique survey-worthy information. It
is worth noting that all the aforementioned research
on citation-based summarization used the ACL An-
thology Network (AAN) for evaluation.
4 Controversy Identification
Some arguments and claims made by researchers
may get disputed by other researchers (Teufel,
1999). The following are examples of citing
sentences that dispute previous work.
(1) Even though prior work (Teufel et al, 2006) argues that citation
text is unsuitable for summarization, we show that in the framework
of multi-document survey creation, citation texts can play a crucial role.
(2) Mining the Web for bilingual text (Resnik, 1999) is not
likely to provide sufficient quantities of high quality data.
In many cases, it is useful to know which ar-
guments were confirmed and accepted by the
research community and which ones where dis-
puted or even rejected. We believe that analyzing
citation text helps identify these contrasting views
automatically.
5 Comparison of Different Techniques
Citing sentences that compare different tech-
niques or compare the techniques proposed by
the author to previous work are common. The fol-
lowing sentences are examples of such comparisons.
(3) In (Zollmann et al, 2008), an interesting comparison be-
tween phrase-based, hierarchical and syntax-augmented models is
carried out, concluding that hierarchical and syntax-based models
slightly outperform phrase-based models under large data conditions
and for sufficiently non-monotonic language pairs.
(4) Brill?s results demonstrate that this approach can outper-
form the Hidden Markov Model approaches that are frequently used
for part-of-speech tagging (Jelinek, 1985; Church, 1988; DeRose,
1988; Cutting et al, 1992; Weischedel et al, 1993), as well as showing
promise for other applications.
(5) Our highest scores of 90.8% LP and 90.5% LR outperform
the scores of the best previously published parser by Charniak (2000)
who obtains 90.1% for both LP and LR.
Extracting such comparisons from citations can be
of great benefit to researchers. It will allow them
to quickly determine which technique works better
for their tasks. To verify that citation text could
be a good source for extracting comparisons, we
created a list of words and phrases that are usually
used to express comparisons and counted their
frequency in AAN citing sentences. We found, for
example, that the word compare (at its variations)
5
Abstract STRAND (Resnik, 1998) is a language-independent system for automatic discovery of text in parallel translation on the World
Wide Web. This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders
of magnitude, and formally evaluating performance. The most recent end-product is an automatically acquired parallel corpus
comprising 2491 English-French document pairs, approximately 1.5 million words per language.
Selected
Citing
Sentences
Many research ideas have exploited the Web in unsupervised or weakly supervised algorithms for natural language processing
(e.g. , Resnik (1999))
Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest.
In Resnik (1999), the Web is harvested in search of pages that are available in two languages, with the aim of building parallel
corpora for any pair of target languages.
The STRAND system of (Resnik, 1999), uses structural markup information from the pages, without looking at their content, to
attempt to align them.
Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data.
Table 5: Comparison of the abstract and a selected set of sentences that cite Resnik (1999) work
appears in about 4000 sentences, and that the words
outperform and contrast each appears in about 1000
citing sentences.
6 Ontology Creation
It is useful for researchers to know which tasks
and research problems are important, and what
techniques and tools are usually used with them.
Citation text is a good source of such information.
For example, sentence (6) below shows three
different techniques (underlined) that were used to
extend tools and resources that were created for
English so that they work for other languages. For
another example, sentence (7) shows different tasks
in which re-ranking has been successfully applied.
These relations can be easily extracted from citing
sentences and can be possibly used to build an
ontology of tasks, methods, tools, and the relations
between them.
(6) Another strain of research has sought to exploit resources and tools
in some languages (especially English) to construct similar resources
and tools for other languages, through heuristic projection (Yarowsky
and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett
and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011;
McDonald et al, 2011) or inference (Smith and Smith, 2004).
(7) (Re)rankers have been successfully applied to numerous
NLP tasks, such as parse selection (Osborne and Baldridge, 2004;
Toutanova et al, 2004), parse reranking (Collins and Duffy, 2002;
Charniak and Johnson, 2005), question-answering (Ravichandran et
al., 2003).
7 Paraphrase Extraction
It is common that multiple citing sentences high-
light the same facts about a cited paper. Since these
sentences were written by different authors, they
often use different wording to describe the cited
paper facts. This motivates the idea of using citing
sentences to create data sets for paraphrase extrac-
tion. For example, sentences (8) and (9) below both
cite (Turney, 2002) and highlight the same aspect
of Turney?s work using slightly different wordings.
Therefore, sentences (8) and (9) can be considered
paraphrases of each other.
(8) In (Turney, 2002), an unsupervised learning algorithm was
proposed to classify reviews as recommended or not recommended
by averaging sentiment annotation of phrases in reviews that contain
adjectives or adverbs.
(9) For example, Turney (2002) proposes a method to classify
reviews as recommended/not recommended, based on the average
semantic orientation of the review.
The paraphrase annotation of citing sentences
consists of manually labeling which sentence
consists of what facts. Then, if two citing sentences
consist of the same set of facts, they are labeled
as paraphrases of each other. For example, if a
paper has 50 sentences citing it, this gives us a
paraphrasing data set that consists of 50*49 = 2450
pairs. As a proof of concept, we annotated 25 papers
from AAN using the annotation method described
above. This data set consisted of 33,683 sentence
pairs of which 8,704 are paraphrases.
The idea of using citing sentences to create data
sets for paraphrase extraction was initially suggested
6
by Nakov et al (2004) who proposed an algorithm
that extracts paraphrases from citing sentences us-
ing rules based on automatic named entity annota-
tion and the dependency paths between them.
8 Scientific Article Classification
Automatic classification of scientific articles is one
of the important tasks for creating publication
databases. A variety of machine learning algorithms
have been proposed for this task. Many of these
methods perform the classification based on the title,
the abstract, or the full text of the article. Some other
methods used citation links in addition to content to
make classification decisions. Cao and Gao (2005)
proposed a two-phase classification system. The
system first applies a content-based statistical clas-
sification method which is similar to general text
classification. In the second phase, the system uses
an iterative method to update the labels of classified
instances using citation links. A similar approach
is also proposed by Zhang et al (2006). These ap-
proaches use citation links only to improve classifi-
cation decisions that were made based on content.
We hypothesize that using the text of citing sen-
tences in addition to citation structure and content
leads to more accurate classification than using the
content and citation links only.
9 Terminology Translation
Citing sentences can also be used to improve
machine translation systems by using citing sen-
tences from different languages to build parallel
corpus of terms and their translations. This can
be done by identifying articles written in different
languages that cite a common target paper, then
extracting the citing sentences from each paper.
Word alignment techniques can then be applied to
the text surrounding the reference to the common
target paper. The aligned words from each source
can then be extracted and used as translations of the
same term. Sentences (10) and (11) below illustrate
how the application of this proposed method can
identify that the underlined terms in sentence 10
(Spanish) and sentence 11 (English) are translations
of each other.
(10) Spanish: Se comprobo? que la agrupacio?n por bloques
ofrec??a mejores resultados que, la introduccio?n de vocabulario (Hearst,
1997) o las cadenas le?xicas (Hearst, 1994) y, por tanto, es la que se ha
utilizado en la segunda fase del algoritmo.
(11) English: This can be done either by analyzing the number
of overlapping lexical chains (Hearst, 1994) or by building a
short-range and long-range language model (Beeferman et al, 1999).
10 Other Uses of Citing Sentences
Nakov et al (2004) proposed several other uses of
citing sentences. First, they suggested using them as
a source for unannotated comparable corpora. Such
comparable corpora can be used in several applica-
tions such as paraphrase extraction as we showed
earlier. They also noticed that the scientific liter-
ature is rife with abbreviations and synonyms, and
hence, citing sentences referring to the same article
may allow synonyms to be identified and recorded.
They also proposed using citing sentences to build
a model of the different ways used to express a re-
lationship between two entities. They hypothesized
that this model can help improve both relation ex-
traction and named entity recognition systems. Fi-
nally, they proposed improving the indexing and
ranking of publications by considering, in addition
to the content of the publication, the text of citing
sentences that cite it and their contexts.
11 Summarizing 30 years of ACL
Discoveries Using Citing Sentences
The ACL Anthology Corpus contains all the pro-
ceedings of the Annual Meeting of the Association
of Computational Linguistics (ACL) since 1979. All
the ACL papers and their citation links and citing
sentences are included in the ACL Anthology Net-
work (ACL). In this section, we show how citing
sentences can be used to summarize the most im-
portant contributions that have been published in the
ACL conference since 1979. We selected the most
cited papers in each year and then manually picked a
citing sentence that cites a top cited and describes it
contribution. It should be noted here that the citation
counts we used for ranking papers reflect the number
of incoming citations the paper received only from
the venues included in AAN. To create the summary,
we used citing sentences that has the reference to the
cited paper in the beginning of the sentence. This is
7
1979 Carbonell (1979) discusses inferring the meaning of new words.
1980 Weischedel and Black (1980) discuss techniques for interacting with the linguist/developer to identify insufficiencies in the gram-
mar.
1981 Moore (1981) observed that determiners rarely have a direct correlation with the existential and universal quantifiers of first-order
logic.
1982 Heidorn (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to
attachment decisions based on syntactic considerations only.
1983 Grosz et al (1983) proposed the centering model which is concerned with the interactions between the local coherence of discourse
and the choices of referring expressions.
1984 Karttunen (1984) provides examples of feature structures in which a negation operator might be useful.
1985 Shieber (1985) proposes a more efficient approach to gaps in the PATR-II formalism, extending Earley?s algorithm by using
restriction to do top-down filtering.
1986 Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of
the neighborhood.
1987 Brennan et al (1987) propose a default ordering on transitions which correlates with discourse coherence.
1988 Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals,
and questions show initiative, while answers and acknowledgements do not.
1989 Church and Hanks (1989) explored tile use of mutual information statistics in ranking co-occurrences within five-word windows.
1990 Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs.
1991 Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and
monetary information.
1992 Pereira and Schabes (1992) establish that evaluation according to the bracketing accuracy and evaluation according to perplexity
or crossentropy are very different.
1993 Pereira et al (1993) proposed a soft clustering scheme, in which membership of a word in a class is probabilistic.
1994 Hearst (1994) presented two implemented segmentation algorithms based on term repetition, and compared the boundaries pro-
duced to the boundaries marked by at least 3 of 7 subjects, using information retrieval metrics.
1995 Yarowsky (1995) describes a ?semi-unsupervised? approach to the problem of sense disambiguation of words, also using a set of
initial seeds, in this case a few high quality sense annotations.
1996 Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.
1997 Collins (1997)?s parser and its re-implementation and extension by Bikel (2002) have by now been applied to a variety of lan-
guages: English (Collins, 1999), Czech (Collins et al , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins,
2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic.
1998 Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus
using a parsed corpus.
1999 Rapp (1999) proposed that in any language there is a correlation between the cooccurrences of words which are translations of
each other.
2000 Och and Ney (2000) introduce a NULL-alignment capability to HMM alignment models.
2001 Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and
proposed a tree to string model for alignment.
2002 BLEU (Papineni et al, 2002) was devised to provide automatic evaluation of MT output.
2003 Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear
MT models.
2004 Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier
separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the
subjective texts into positive and negative.
2005 Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation.
2006 Liu et al (2006) experimented with tree-to-string translation models that utilize source side parse trees.
2007 Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model
size.
2008 Huang (2008) improves the re-ranking work of Charniak and Johnson (2005) by re-ranking on packed forest, which could poten-
tially incorporate exponential number of k-best list.
2009 Mintz et al (2009) uses Freebase to provide distant supervision for relation extraction.
2010 Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical
phrase-based system.
Table 6: A citation-based summary of the important contributions published in ACL conference proceedings since
1979. The top cited paper in each year is found and one citation sentence is manually picked to represent it in the
summary.
8
because such citing sentences are often high-quality,
concise summaries of the cited work. Table 6 shows
the summary of the ACL conference contributions
that we created using citing sentences.
12 Conclusion
We motivated and discussed several different uses
of citing sentences, the text surrounding citations.
We showed that citing sentences can be used to an-
alyze the dynamics of research and observe how it
trends. We also gave examples on how analyzing
the text of citing sentences can give a better under-
standing of the impact of a researcher?s work and
how this impact changes over time. In addition, we
presented several different applications that can ben-
efit from citing sentences such as scientific literature
summarization, identifying controversial arguments,
and identifying relations between techniques, tools
and tasks. We also showed how citing sentences can
provide high-quality for NLP tasks such as informa-
tion extraction, paraphrase extraction, and machine
translation. Finally, we used AAN citing sentences
to create a citation-based summary of the important
contributions included in the ACL conference publi-
cation in the past 30 years.
References
Amjad Abu-Jbara and Dragomir Radev. 2011a. Coher-
ent citation-based summarization of scientific papers.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 500?509, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Amjad Abu-Jbara and Dragomir Radev. 2011b. Coher-
ent citation-based summarization of scientific papers.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 500?509, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
F. Amblard, A. Casteigts, P. Flocchini, W. Quattrocioc-
chi, and N. Santoro. 2011. On the temporal analysis
of scientific network evolution. In Computational As-
pects of Social Networks (CASoN), 2011 International
Conference on, pages 169 ?174, oct.
Awais Athar. 2011. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceedings
of the ACL 2011 Student Session, pages 81?87, Port-
land, OR, USA, June. Association for Computational
Linguistics.
Christine L. Borgman and Jonathan Furner. 2002. Schol-
arly communication and bibliometrics. ANNUAL RE-
VIEW OF INFORMATION SCIENCE AND TECH-
NOLOGY, 36(1):2?72.
Susan E. Brennan, Marilyn W. Friedman, and Carl J. Pol-
lard. 1987. A centering approach to pronouns. In
Proceedings of the 25th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 155?162,
Stanford, California, USA, July. Association for Com-
putational Linguistics.
Minh Duc Cao and Xiaoying Gao. 2005. Combin-
ing contents and citations for scientific document clas-
sification. In Proceedings of the 18th Australian
Joint conference on Advances in Artificial Intelligence,
AI?05, pages 143?152, Berlin, Heidelberg. Springer-
Verlag.
Jaime G. Carbonell. 1979. Towards a self-extending
parser. In Proceedings of the 17th Annual Meeting of
the Association for Computational Linguistics, pages
3?7, La Jolla, California, USA, June. Association for
Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicogra-
phy. In Proceedings of the 27th Annual Meeting of the
Association for Computational Linguistics, pages 76?
83, Vancouver, British Columbia, Canada, June. Asso-
ciation for Computational Linguistics.
Kenneth Ward Church. 1988. A stochastic parts program
and noun phrase parser for unrestricted text. In Pro-
ceedings of the Second Conference on Applied Natural
Language Processing, pages 136?143, Austin, Texas,
USA, February. Association for Computational Lin-
guistics.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 184?191, Santa
Cruz, California, USA, June. Association for Compu-
tational Linguistics.
9
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain, July.
Association for Computational Linguistics.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir Radev. 2008. Blind men
and elephants: What do citation summaries tell us
about a research article? J. Am. Soc. Inf. Sci. Tech-
nol., 59(1):51?62.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of the 29th Annual Meeting of the As-
sociation for Computational Linguistics, pages 177?
184, Berkeley, California, USA, June. Association for
Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744?751, Prague, Czech Republic, June. Association
for Computational Linguistics.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1983. Providing a unified account of definite
noun phrases in discourse. In Proceedings of the 21st
Annual Meeting of the Association for Computational
Linguistics, pages 44?50, Cambridge, Massachusetts,
USA, June. Association for Computational Linguis-
tics.
Sonal Gupta and Christopher Manning. 2011. Analyz-
ing the dynamics of research by extracting key as-
pects of scientific papers. In Proceedings of 5th Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 1?9, Chiang Mai, Thailand, November.
Asian Federation of Natural Language Processing.
Marti A. Hearst. 1994. Multi-paragraph segmentation
expository text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 9?16, Las Cruces, New Mexico, USA,
June. Association for Computational Linguistics.
George E. Heidorn. 1982. Experience with an easily
computed metric for ranking alternative parses. In
Proceedings of the 20th Annual Meeting of the As-
sociation for Computational Linguistics, pages 82?84,
Toronto, Ontario, Canada, June. Association for Com-
putational Linguistics.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proceedings of the
28th Annual Meeting of the Association for Compu-
tational Linguistics, pages 268?275, Pittsburgh, Penn-
sylvania, USA, June. Association for Computational
Linguistics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, Ohio, June.
Association for Computational Linguistics.
Megumi Kameyama. 1986. A property-sharing con-
straint in centering. In Proceedings of the 24th Annual
Meeting of the Association for Computational Linguis-
tics, pages 200?206, New York, New York, USA, July.
Association for Computational Linguistics.
Lauri Karttunen. 1984. Features and values. In Proceed-
ings of the 10th International Conference on Compu-
tational Linguistics and 22nd Annual Meeting of the
Association for Computational Linguistics, pages 28?
33, Stanford, California, USA, July. Association for
Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics, Volume 2, pages 768?774, Mon-
treal, Quebec, Canada, August. Association for Com-
putational Linguistics.
Yang (1) Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 609?616, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Terttu Luukkonen. 1992. Is scientists? publishing be-
haviour rewardseeking? Scientometrics, 24:297?319.
10.1007/BF02017913.
Amin Mazloumian, Young-Ho Eom, Dirk Helbing, Sergi
Lozano, and Santo Fortunato. 2011. How citation
boosts promote scientific paradigm shifts and nobel
prizes. PLoS ONE, 6(5):e18975, 05.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL-08: HLT, pages 816?824, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 1003?1011,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
10
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 584?592, Boulder, Colorado, June. Association
for Computational Linguistics.
Robert C. Moore. 1981. Problems in logical form. In
Proceedings of the 19th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 117?124,
Stanford, California, USA, June. Association for Com-
putational Linguistics.
Preslav I. Nakov, Ariel S. Schwartz, and Marti A. Hearst.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In In Proceedings of the SI-
GIR04 workshop on Search and Discovery in Bioin-
formatics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI ?99: Proceedings of the Six-
teenth International Joint Conference on Artificial In-
telligence, pages 926?931, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and
Of Information Science. 2000. Classification of re-
search papers using citation links and citation types:
Towards automatic review article generation.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447, Hong Kong, October. As-
sociation for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July. Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 271?278,
Barcelona, Spain, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the 30th Annual Meeting of the As-
sociation for Computational Linguistics, pages 128?
135, Newark, Delaware, USA, June. Association for
Computational Linguistics.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In Pro-
ceedings of the 31st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 183?190,
Columbus, Ohio, USA, June. Association for Compu-
tational Linguistics.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 689?696, Manchester, UK, August. Coling 2008
Organizing Committee.
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555?564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Ozgur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 895?903, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network corpus.
In NLPIR4DL ?09: Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital Li-
braries, pages 54?61, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, pages
519?526, College Park, Maryland, USA, June. Asso-
ciation for Computational Linguistics.
Sidney Redner. 2005. Citation statistics from 110 years
of physical review. Physics Today, 58(6):49?54.
Philip Resnik. 1999. Mining the web for bilingual text.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 527?
534, College Park, Maryland, USA, June. Association
for Computational Linguistics.
Stuart M. Shieber. 1985. Using restriction to ex-
tend parsing algorithms for complex-feature-based
formalisms. In Proceedings of the 23rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 145?152, Chicago, Illinois, USA, July. Associ-
ation for Computational Linguistics.
11
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attributing
scientific work to citations. In In Proceedings of
NAACL/HLT-07.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
In Proc. of EMNLP-06.
Simone Teufel. 1999. Argumentative zoning: Informa-
tion extraction from scientific text. Technical report.
Simone Teufel. 2007. Argumentative zoning for im-
proved citation indexing. computing attitude and affect
in text. In Theory and Applications, pages 159170.
Ralph M. Weischedel and John E. Black. 1980. If the
parser fails. In Proceedings of the 18th Annual Meet-
ing of the Association for Computational Linguistics,
pages 95?95, Philadelphia, Pennsylvania, USA, June.
Association for Computational Linguistics.
Steve Whittaker and Phil Stenton. 1988. Cues and con-
trol in expert-client dialogues. In Proceedings of the
26th Annual Meeting of the Association for Computa-
tional Linguistics, pages 123?130, Buffalo, New York,
USA, June. Association for Computational Linguis-
tics.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523?530, Toulouse, France, July.
Association for Computational Linguistics.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 189?196,
Cambridge, Massachusetts, USA, June. Association
for Computational Linguistics.
M. Zhang, X. Gao, M.D. Cao, and Yuejin Ma. 2006.
Neural networks for scientific paper classification.
In Innovative Computing, Information and Control,
2006. ICICIC ?06. First International Conference on,
volume 2, pages 51 ?54, 30 2006-sept. 1.
12
Proceedings of the TextGraphs-7 Workshop at ACL, pages 6?14,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Extracting Signed Social Networks From Text
Ahmed Hassan
Microsoft Research
Redmond, WA, USA
hassanam@microsoft.com
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
Most of the research on social networks has al-
most exclusively focused on positive links be-
tween entities. There are much more insights
that we may gain by generalizing social net-
works to the signed case where both positive
and negative edges are considered. One of the
reasons why signed social networks have re-
ceived less attention that networks based on
positive links only is the lack of an explicit
notion of negative relations in most social net-
work applications. However, most such appli-
cations have text embedded in the social net-
work. Applying linguistic analysis techniques
to this text enables us to identify both positive
and negative interactions. In this work, we
propose a new method to automatically con-
struct a signed social network from text. The
resulting networks have a polarity associated
with every edge. Edge polarity is a means for
indicating a positive or negative affinity be-
tween two individuals. We apply the proposed
method to a larger amount of online discus-
sion posts. Experiments show that the pro-
posed method is capable of constructing net-
works from text with high accuracy. We also
connect out analysis to social psychology the-
ories of signed network, namely the structural
balance theory.
1 Introduction
A great body of research work has focused on so-
cial network analysis. Social network analysis plays
a huge role in understanding and improving so-
cial computing applications. Most of this research
has almost exclusively focused on positive links be-
tween individuals (e.g. friends, fans, followers,
etc.). However, if we carefully examine the relation-
ships between individuals in online communities,
we will find out that limiting links to positive inter-
actions is a very simplistic assumption. It is true that
people show positive attitude by labeling others as
friends, and showing agreement, but they also show
disagreement, and antagonism toward other mem-
bers of the online community. Discussion forums
are one example that makes it clear that considering
both positive and negative interactions is essential
for understanding the rich relationships that develop
between individuals in online communities.
If considering both negative and positive interac-
tions will provide much more insight toward under-
standing the social network, why did most of pre-
vious work only focus on positive interactions? We
think that one of the main reasons behind this is the
lack of a notion for explicitly labeling negative re-
lations. For example, most social web applications
allow people to mark others as friends, like them,
follow them, etc. However, they do not allow people
to explicitly label negative relations with others.
Previous work has built networks from discus-
sions by linking people who reply to one another.
Even though, the mere fact that X replied to Y ?s
post does show an interaction, it does not tell us any-
thing about the type of that interaction. In this case,
the type of interaction is not readily available; how-
ever it may be mined from the text that underlies
the social network. Hence, if we examine the text
exchanged between individuals, we may be able to
come up with conclusions about, not only the exis-
tence of an interaction, but also its type.
In this work, we apply Natural Language Pro-
cessing techniques to text correspondences ex-
changed between individuals to identify the under-
6
lying signed social structure in online communities.
We present and compare several algorithms for iden-
tifying user attitude and for automatically construct-
ing a signed social network representation. We ap-
ply the proposed methods to a large set of discussion
posts. We evaluate the performance using a manu-
ally labeled dataset.
The input to our algorithm is a set of text corre-
spondences exchanged between users (e.g. posts or
comments). The output is a signed network where
edges signify the existence of an interaction between
two users. The resulting network has polarity asso-
ciated with every edge. Edge polarity is a means for
indicating a positive or negative affinity between two
individuals.
The proposed method was applied to a very large
dataset of online discussions. To evaluate our auto-
mated procedure, we asked human annotators to ex-
amine text correspondences exchanged between in-
dividuals and judge whether their interaction is pos-
itive or negative. We compared the edge signs that
had been automatically identified to edges manually
created by human annotators.
We also connected our analysis to social psychol-
ogy theories, namely the Structural Balance The-
ory (Heider, 1946). The balance theory has been
shown to hold both theoretically (Heider, 1946) and
empirically (Leskovec et al, 2010b) for a variety
of social community settings. Showing that it also
holds for our automatically constructed network fur-
ther validates our results.
The rest of the paper is structured as follows. In
section 2, we review some of the related prior work
on mining sentiment from text, mining online dis-
cussions, extracting social networks from text, and
analyzing signed social networks. We define our
problem and explain our approach in Section 3. Sec-
tion 4 describes our dataset. Results and discussion
are presented in Section 5. We present a possible
application for the proposed approach in Section 6.
We conclude in Section 7.
2 Related Work
In this section, we survey several lines of research
that are related to our work.
2.1 Mining Sentiment from Text
Our general goal of mining attitude from one in-
dividual toward another makes our work related to
a huge body of work on sentiment analysis. One
such line of research is the well-studied problem
of identifying the of individual words. In previ-
ous work, Hatzivassiloglou and McKeown (1997)
proposed a method to identify the polarity of ad-
jectives based on conjunctions linking them in a
large corpus. Turney and Littman (2003) used sta-
tistical measures to find the association between a
given word and a set of positive/negative seed words.
Takamura et al (2005) used the spin model to ex-
tract word semantic orientation. Finally, Hassan and
Radev (2010) use a random walk model defined over
a word relatedness graph to classify words as either
positive or negative.
Subjectivity analysis is yet another research line
that is closely related to our general goal of mining
attitude. The objective of subjectivity analysis is to
identify text that presents opinion as opposed to ob-
jective text that presents factual information (Wiebe,
2000). Prior work on subjectivity analysis mainly
consists of two main categories: subjectivity of a
phrase or word is analyzed regardless of the context
(Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000;
Banea et al, 2008), or within its context (Riloff and
Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Na-
sukawa and Yi, 2003; Popescu and Etzioni, 2005).
Hassan et al (2010) presents a method for identify-
ing sentences that display an attitude from the text
writer toward the text recipient. Our work is dif-
ferent from subjectivity analysis because we are not
only interested in discriminating between opinions
and facts. Rather, we are interested in identifying
the polarity of interactions between individuals. Our
method is not restricted to phrases or words, rather it
generalizes this to identifying the polarity of an in-
teraction between two individuals based on several
posts they exchange.
2.2 Mining Online Discussions
Our use of discussion threads as a source of data
connects us to some previous work on mining
online discussions. Lin et al (2009) proposed
a sparse coding-based model that simultaneously
models semantics and structure of threaded discus-
7
sions. Huang et al (2007) learn SVM classifiers
from data to extract (thread-title, reply) pairs. Their
objective was to build a chatbot for a certain do-
main using knowledge from online discussion fo-
rums. Shen et al (2006) proposed three clustering
methods for exploiting the temporal information in
discussion streams, as well as an algorithm based on
linguistic features to analyze discourse structure in-
formation.
2.3 Extracting Social Networks from Text
Little work has been done on the front of extracting
social relations between individuals from text. El-
son et al (2010) present a method for extracting so-
cial networks from nineteenth-century British nov-
els and serials. They link two characters based on
whether they are in conversation or not. McCal-
lum et al (2007) explored the use of structured data
such as email headers for social network construc-
tion. Gruzd and Hyrthonthwaite (2008) explored the
use of post text in discussions to study interaction
patterns in e-learning communities.
Our work is related to this line of research because
we employ natural language processing techniques
to reveal embedded social structures. Despite sim-
ilarities, our work is uniquely characterized by the
fact that we extract signed social networks from text.
2.4 Signed Social Networks
Most of the work on social networks analysis has
only focused on positive interactions. A few recent
papers have taken the signs of edges into account.
Brzozowski et al (2008) study the positive and
negative relationships between users of Essembly.
Essembly is an ideological social network that dis-
tinguishes between ideological allies and nemeses.
Kunegis et al (2009) analyze user relationships in
the Slashdot technology news site. Slashdot allows
users of the website to tag other users as friends or
foes, providing positive and negative endorsements.
Leskovec et al (2010c) study signed social networks
generated from Slashdot, Epinions, and Wikipedia.
They also connect their analysis to theories of signed
networks from social psychology. A similar study
used the same datasets for predicting positive and
negative links given their context (Leskovec et al,
2010a). Other work addressed the problem of clus-
tering signed networks by taking both positive and
negative edges into consideration (Yang et al, 2007;
Doreian and Mrvar, 2009).
All this work has been limited to analyzing a
handful of datasets for which an explicit notion of
both positive and negative relations exists. Our work
goes beyond this limitation by leveraging the power
of natural language processing to automate the dis-
covery of signed social networks using the text em-
bedded in the network.
3 Approach
The general goal of this work is to mine attitude be-
tween individuals engaged in an online discussion.
We use that to extract a signed social network rep-
resenting the interactions between different partici-
pants. Our approach consists of several steps. In
this section, we will explain how we identify senti-
ment at the word level (i.e. polarity), at the sentence
level (i.e. attitude), and finally generalize over this
to find positive/negative interactions between indi-
viduals based on their text correspondences.
The first step toward identifying attitude is to
identify polarized words. Polarized words are very
good indicators of subjective sentences and hence
we their existence will be highly correlated with the
existence of attitude. The method we use for identi-
fying word polarity is a Random Walk based method
over a word relatedness graph (Hassan and Radev,
2010).
The following step is to move to the sentence level
by examining different sentences to find out which
sentences display an attitude from the text writer to
the recipient. We train a classifier based on several
sources of information to make this prediction (Has-
san et al, 2010). We use lexical items, polarity tags,
part-of-speech tags, and dependency parse trees to
train a classifier that identifies sentences with atti-
tude.
Finally, we build a network connecting partici-
pants based on their interactions. We use the predic-
tions we made both at the word and sentence levels
to associate a sign to every edge.
3.1 Identified Positive/Negative Words
The first step toward identifying attitude is to iden-
tify words with positive/negative semantic orienta-
tion. The semantic orientation or polarity of a word
8
indicates the direction the word deviates from the
norm (Lehrer, 1974). Past work has demonstrated
that polarized words are very good indicators of
subjective sentences (Hatzivassiloglou and Wiebe,
2000; Wiebe et al, 2001). We use a Random Walk
based method to identify the semantic orientation
of words (Hassan and Radev, 2010). We construct
a graph where each node represents a word/part-
of-speech pair. We connect nodes based on syn-
onyms, hypernyms, and similar-to relations from
WordNet (Miller, 1995). For words that do not
appear in WordNet, we use distributional similar-
ity (Lee, 1999) as a proxy for word relatedness.
We use a list of words with known polarity (Stone
et al, 1966) to label some of the nodes in the graph.
We then define a random walk model where the set
of nodes correspond to the state space, and transi-
tion probabilities are estimated by normalizing edge
weights. We assume that a random surfer walks
along the word relatedness graph starting from a
word with unknown polarity. The walk continues
until the surfer hits a word with a known polarity.
Seed words with known polarity act as an absorb-
ing boundary for the random walk. We calculate the
mean hitting time (Norris, 1997) from any word with
unknown polarity to the set of positive seeds and the
set of negative seeds. If the absolute difference of
the two mean hitting times is below a certain thresh-
old, the word is classified as neutral. Otherwise, it
is labeled with the class that has the smallest mean
hitting time.
3.2 Identifying Attitude from Text
The first step toward identifying attitude is to iden-
tify words with positive/negative semantic orienta-
tion. The semantic orientation or polarity of a word
indicates the direction the word deviates from the
norm (Lehrer, 1974). We use OpinionFinder (Wil-
son et al, 2005a) to identify words with positive
or negative semantic orientation. The polarity of a
word is also affected by the context where the word
appears. For example, a positive word that appears
in a negated context should have a negative polarity.
Other polarized words sometimes appear as neutral
words in some contexts. Hence, we use the method
described in (Wilson et al, 2005b) to identify the
contextual polarity of words given their isolated po-
larity. A large set of features is used for that purpose
including words, sentences, structure, and other fea-
tures.
Our overall objective is to find the direct attitude
between participants. Hence after identifying the se-
mantic orientation of individual words, we move on
to predicting which polarized expressions target the
addressee and which are not.
Sentences that show an attitude are different from
subjective sentences. Subjective sentences are sen-
tences used to express opinions, evaluations, and
speculations (Riloff and Wiebe, 2003). While ev-
ery sentence that shows an attitude is a subjective
sentence, not every subjective sentence shows an at-
titude toward the recipient. A discussion sentence
may display an opinion about any topic yet no atti-
tude.
We address the problem of identifying sentences
with attitude as a relation detection problem in a su-
pervised learning setting (Hassan et al, 2010). We
study sentences that use second person pronouns and
polarized expressions. We predict whether the sec-
ond person pronoun is related to the polarized ex-
pression or not. We regard the second person pro-
noun and the polarized expression as two entities
and try to learn a classifier that predicts whether the
two entities are related or not. The text connecting
the two entities offers a very condensed represen-
tation of the information needed to assess whether
they are related or not. For example the two sen-
tences ?you are completely unqualified? and ?you
know what, he is unqualified ...? show two differ-
ent ways the words ?you?, and ?unqualified? could
appear in a sentence. In the first case the polarized
word unqualified refers to the word you. In the sec-
ond case, the two words are not related. The se-
quence of words connecting the two entities is a
very good predictor for whether they are related or
not. However, these paths are completely lexicalized
and consequently their performance will be limited
by data sparseness. To alleviate this problem, we
use higher levels of generalization to represent the
path connecting the two tokens. These representa-
tions are the part-of-speech tags, and the shortest
path in a dependency graph connecting the two to-
kens. We represent every sentence with several rep-
resentations at different levels of generalization. For
example, the sentence your ideas are very inspiring
will be represented using lexical, polarity, part-of-
9
speech, and dependency information as follows:
LEX: ?YOUR ideas are very POS?
POS: ?YOUR NNS VBP RB JJ POS?
DEP: ?YOUR poss nsubj POS?
3.2.1 A Text Classification Approach
In this method, we treat the problem as a topic
classification problem with two topics: having pos-
itive attitude and having negative attitude. As we
are only interested in attitude between participants
rather than sentiment in general, we restrict the text
we analyze to sentences that contain mentions of the
addressee (e.g. name or second person pronouns).
A similar approach for sentiment classification has
been presented in (Pang et al, ).
We represent text using the popular bag-of-words
approach. Every piece of text is represented using
a high dimensional feature space. Every word is
considered a feature. The tf-idf weighting schema
is used to calculate feature weights. tf, or term fre-
quency, is the number of time a term t occurred in
a document d. idf, or inverse document frequency,
is a measure of the general importance of the term.
It is obtained by dividing the total number of doc-
uments by the number of documents containing the
term. The logarithm of this value is often used in-
stead of the original value.
We used Support Vector Machines (SVMs) for
classification. SVM has been shown to be highly
effective for traditional text classification. We used
the SVM Light implementation with default param-
eters (Joachims, 1999). All stop words were re-
moved and all documents were length normalized
before training.
The set of features we use are the set of unigrams,
and bigrams representing the words, part-of-speech
tags, and dependency relations connecting the two
entities. For example the following features will be
set for the previous example:
YOUR ideas, YOUR NNS, YOUR poss,
poss nsubj, ...., etc.
We use Support Vector Machines (SVM) as a
learning system because it is good with handling
high dimensional feature spaces.
3.3 Extracting the Signed Network
In this subsection, we describe the procedure we
used to build the signed network given the compo-
nents we described in the previous subsections. This
procedure consists of two main steps. The first is
building the network without signs, and the second
is assigning signs to different edges.
To build the network, we parse our data to identify
different threads, posts and senders. Every sender is
represented with a node in the network. An edge
connects two nodes if there exists an interaction be-
tween the corresponding participants. We add a di-
rected edge A? B, if A replies to B?s posts at least
n times in m different threads. We set m, and n to
2 in most of our experiments. The interaction infor-
mation (i.e. who replies to whom) can be extracted
directly from the thread structure.
Once we build the network, we move to the more
challenging task in which we associate a sign with
every edge. We have shown in the previous section
how sentences with positive and negative attitude
can be extracted from text. Unfortunately the sign
of an interaction cannot be trivially inferred from the
polarity of sentences. For example, a single negative
sentence written by A and directed to B does not
mean that the interaction between A and B is neg-
ative. One way to solve this problem would be to
compare the number of negative sentences to posi-
tive sentences in all posts between A and B and clas-
sify the interaction according to the plurality value.
We will show later, in our experiment section, that
such a simplistic method does not perform well in
predicting the sign of an interaction.
As a result, we decided to pose the problem
as a classical supervised learning problem. We
came up with a set of features that we think are
good predictors of the interaction sign, and we
train a classifier using those features on a labeled
dataset. Our features include numbers and percent-
ages of positive/negative sentences per post, posts
per thread, and so on. A sentence is labeled as posi-
tive/negative if a relation has been detected in this
sentence between a second person pronoun and a
positive/negative expression. A post is considered
positive/negative based on the majority of relations
detected in it. We use two sets of features. The first
set is related to A only or B only. The second set
10
Participant Features
Number of posts per month for A (B)
Percentage of positive posts per month for A (B)
Percentage of negative posts per month for A (B)
gender
Interaction Features
Percentage/number of positive (negative) sentences per post
Percentage/number of positive (negative) posts per thread
Discussion Topic
Table 1: Features used by the Interaction Sign Classifier.
is related to the interactions between A and B. The
features are outlined in Table 1.
4 Data
Our data consists of a large amount of discussion
threads collected from online discussion forums. We
collected around 41, 000 threads and 1.2M posts
from the period between the end of 2008 and the end
of 2010. All threads were in English and had 5 posts
or more. They covered a wide range of topics in-
cluding: politics, religion, science, etc. The data was
tokenized, sentence-split, and part-of-speech tagged
with the OpenNLP toolkit. It was parsed with the
Stanford parser (Klein and Manning, 2003).
We randomly selected 5300 posts (having approx-
imately 1000 interactions), and asked human anno-
tators to label them. Our annotators were instructed
to read all the posts exchanged between two partic-
ipants and decide whether the interaction between
them is positive or negative. We used Amazon Me-
chanical Turk for annotations. Following previous
work (Callison-Burch, 2009; Akkaya et al, 2010),
we took several precautions to maintain data in-
tegrity. We restricted annotators to those based in
the US to maintain an acceptable level of English
fluency. We also restricted annotators to those who
have more than 95% approval rate for all previous
work. Moreover, we asked three different annota-
tors to label every interaction. The label was com-
puted by taking the majority vote among the three
annotators. We refer to this data as the Interactions
Dataset.
The kappa measure between the three groups of
annotations was 0.62. To better assess the quality
of the annotations, we asked a trained annotator to
label 10% of the data. We measured the agreement
between the expert annotator and the majority label
from the Mechanical Turk. The kappa measure was
Class Pos. Neg. Weigh. Avg.
TP Rate 0.847 0.809 0.835
FP Rate 0.191 0.153 0.179
Precision 0.906 0.71 0.844
Recall 0.847 0.809 0.835
F-Measure 0.875 0.756 0.838
Accuracy - - 0.835
Table 2: Interaction sign classifier evaluation.
0.69.
We trained the classifier that detects sentences
with attitude (Section 3.1) on a set of 4000 manu-
ally annotated sentences. None of this data overlaps
with the dataset described earlier. A similar annota-
tion procedure was used to label this data. We refer
to this data as the Sentences Dataset.
5 Results and Discussion
We performed experiments on the data described
in the previous section. We trained and tested the
sentence with attitude detection classifiers described
in Section 3.1 using the Sentences Dataset. We
also trained and tested the interaction sign classi-
fier described in Section 3.3 using the Interactions
Dataset. We build one unsigned network from ev-
ery topic in the data set. This results in a signed
social network for every topic (e.g. politics, eco-
nomics,etc.). We decided to build a network for ev-
ery topic as opposed to one single network because
the relation between any two individuals may vary
across topics. In the rest of this section, we will de-
scribe the experiments we did to assess the perfor-
mance of the sentences with attitude detection and
interaction sign prediction steps.
In addition to classical evaluation, we evaluate our
results using the structural balance theory which has
been shown to hold both theoretically (Heider, 1946)
and empirically (Leskovec et al, 2010b). We val-
idate our results by showing that the automatically
extracted networks mostly agree with the theory.
5.1 Identifying Sentences with Attitude
We tested this component using the Sentences
Dataset described in Section 4. In a 10-fold cross
validation mode, the classifier achieves 80.3% accu-
racy, 81.0% precision, %79.4 recall, and 80.2% F1.
11
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 Balanced Triangles Balanced Triangles (Random)
Figure 1: Percentage of balanced triangles in extracted
network vs. random network.
5.2 Interaction Sign Classifier
We used the relation detection classifier described in
Section 3.2 to find sentences with positive and neg-
ative attitude. The output of this classifier was used
to compute the the features described in Section 3.3,
which were used to train a classifier that predicts the
sign of an interaction between any two individuals.
We used Support Vector Machines (SVM) to train
the sign interaction classifier. We report several per-
formance metrics for them in Table 2. All results
were computed using 10 fold cross validation on the
labeled data. To better assess the performance of
the proposed classifier, we compare it to a baseline
that labels the relation as negative if the percentage
of negative sentences exceeds a particular threshold,
otherwise it is labeled as positive. The thresholds
was empirically evaluated using a separate develop-
ment set. The accuracy of this baseline is only 71%.
We evaluated the importance of the features listed
in Table 1 by measuring the chi-squared statistic for
every feature with respect to the class. We found
out that the features describing the interaction be-
tween the two participants are more informative than
the ones describing individuals characteristics. The
later features are still helpful though and they im-
prove the performance by a statistically significant
amount. We also noticed that all features based on
percentages are more informative than those based
on count. The most informative features are: per-
centage of negative posts per tread, percentage of
negative sentences per post, percentage of positive
posts per thread, number of negative posts, and dis-
cussion topic.
5.3 Structural Balance Theory
The structural balance theory is a psychological the-
ory that tries to explain the dynamics of signed so-
cial interactions. It has been shown to hold both the-
oretically (Heider, 1946) and empirically (Leskovec
et al, 2010b). In this section, we study the agree-
ment between the theory and the automatically ex-
tracted networks. The theory has its origins in the
work of Heider (1946). It was then formalized in a
graph theoretic form in (Cartwright and Harary, ).
The theory is based on the principles that ?the friend
of my friend is my friend?, ?the enemy of my friend
is my enemy?, ?the friend of my enemy is my en-
emy?, and variations on these.
There are several possible ways in which trian-
gles representing the relation of three people can be
signed. The structural balance theory states that tri-
angles that have an odd number of positive signs (+
+ + and + - -) are balanced, while triangles that have
an even number of positive signs (- - - and + + -) are
not.
Even though the structural balance theory posits
some triangles as unbalanced, that does not elimi-
nate the chance of their existence. Actually, for most
observed signed structures for social groups, exact
structural balance does not hold (Doreian and Mr-
var, 1996). Davis (1967) developed the theory fur-
ther into the weak structural balance theory. In this
theory, he extended the structural balance theory to
cases where there can be more than two such mu-
tually antagonistic subgroups. Hence, he suggested
that only triangles with exactly two positive edges
are implausible in real networks, and that all other
kinds of triangles should be permissible.
In this section, we connect our analysis to the
structural balance theory. We compare the predic-
tions of edge signs made by our system to the struc-
tural balance theory by counting the frequencies of
different types of triangles in the predicted network.
Showing that our automatically constructed network
agrees with the structural balance theory further val-
idates our results.
We compute the frequency of every type of trian-
gle for ten different topics. We compare these fre-
quencies to the frequencies of triangles in a set of
random networks. We shuffle signs for all edges on
every network keeping the fractions of positive and
12
0.1
0.15
0.2
0.25
0.3
0.35
Figure 2: Percentage of negative edges across topics.
negative edges constant.
We repeat shuffling for 1000 times. Every time,
we compute the frequencies of different types of tri-
angles. We find that the all-positive triangle (+++)
is overrepresented in the generated network com-
pared to chance across all topics. We also see that
the triangle with two positive edges (++?), and the
all-negative triangle (? ? ?) are underrepresented
compared to chance across all topics. The trian-
gle with a single positive edge is slightly overrep-
resented in most but not all of the topics compared
to chance. This shows that the predicted networks
mostly agree with the structural balance theory. In
general, the percentage of balanced triangles in the
predicted networks is higher than in the shuffled net-
works, and hence the balanced triangles are signif-
icantly overrepresented compared to chance. Fig-
ure 1 compares the percentage of balanced triangles
in the predicted networks and the shuffled networks.
This proves that our automatically constructed net-
work is similar to explicit signed networks in that
they both mostly agree with the balance theory.
6 Application: Dispute Level Prediction
There are many applications that could benefit from
the signed network representation of discussions
such as community finding, stance recognition, rec-
ommendation systems, and disputed topics identifi-
cation. In this section, we will describe one such
application.
Discussion forums usually respond quickly to
new topics and events. Some of those topics usu-
ally receive more attention and more dispute than
others. We can identify such topics and in general
measure the amount of dispute every topic receives
using the extracted signed network. We computed
the percentage of negative edges to all edges for ev-
ery topic. We believe that this would act as a mea-
sure for how disputed a particular topic is. We see,
from Figure 2, that ?environment?, ?science?, and
?technology? topics are among the least disputed
topics, whereas ?terrorism?, ?abortion? and ?eco-
nomics? are among the most disputed topics. These
findings are another way of validating our predic-
tions. They also suggest another application for this
work that focuses on measuring the amount of dis-
pute different topics receive. This can be done for
more specific topics, rather than high level topics as
shown here, to identify hot topics that receive a lot
of dispute.
7 Conclusions
In this paper, we have shown that natural language
processing techniques can be reliably used to extract
signed social networks from text correspondences.
We believe that this work brings us closer to un-
derstanding the relation between language use and
social interactions and opens the door to further re-
search efforts that go beyond standard social net-
work analysis by studying the interplay of positive
and negative connections. We rigorously evaluated
the proposed methods on labeled data and connected
our analysis to social psychology theories to show
that our predictions mostly agree with them. Finally,
we presented potential applications that benefit from
the automatically extracted signed network.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk for
subjectivity word sense disambiguation. In CSLDAMT
?10.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Michael J. Brzozowski, Tad Hogg, and Gabor Szabo.
2008. Friends and foes: ideological social network-
ing. In SIGCHI.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s mechan-
ical turk. In EMNLP ?9, EMNLP ?09.
Dorwin Cartwright and Frank Harary. Structure balance:
A generalization of heiders theory. Psych. Rev.
J. A. Davis. 1967. Clustering and structural balance in
graphs. Human Relations.
Patrick Doreian and Andrej Mrvar. 1996. A partitioning
approach to structural balance. Social Networks.
13
Patrick Doreian and Andrej Mrvar. 2009. Partitioning
signed social networks. Social Networks.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In ACL 2010, Uppsala, Sweden.
Anatoliy Gruzd and Caroline Haythornthwaite. 2008.
Automated discovery and analysis of social networks
from threaded discussions. In (INSNA).
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In EACL?97.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING.
Fritz Heider. 1946. Attitudes and cognitive organization.
Journal of Psychology.
J. Huang, M. Zhou, and D. Yang. 2007. Extracting chat-
bot knowledge from online discussion forums. In IJ-
CAI?07.
Thorsten Joachims, 1999. Making large-scale support
vector machine learning practical. MIT Press.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL?03.
Je?ro?me Kunegis, Andreas Lommatzsch, and Christian
Bauckhage. 2009. The slashdot zoo: mining a social
network with negative edges. In WWW ?09.
Lillian Lee. 1999. Measures of distributional similarity.
In ACL-1999.
A. Lehrer. 1974. Semantic fields and lezical structure.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010a. Predicting positive and negative links in online
social networks. In WWW ?10.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010b. Signed networks in social media. In CHI 2010.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010c. Signed networks in social media. In Proceed-
ings of the 28th international conference on Human
factors in computing systems, pages 1361?1370, New
York, NY, USA.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09.
Andrew McCallum, Xuerui Wang, and Andre?s Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. J. Artif. Int. Res., 30:249?272, October.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03.
J. Norris. 1997. Markov chains. Cambridge University
Press.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
Thumbs up?: sentiment classification using machine
learning techniques. In EMNLP. Association for Com-
putational Linguistics.
A. Popescu and O. Etzioni. 2005. Extracting product fea-
tures and opinions from reviews. In HLT-EMNLP?05.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In EMNLP?03.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42.
Philip Stone, Dexter Dunphy, Marchall Smith, and Daniel
Ogilvie. 1966. The general inquirer: A computer ap-
proach to content analysis. The MIT Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05.
P. Turney and M. Littman. 2003. Measuring praise and
criticism: Inference of semantic orientation from asso-
ciation. Transactions on Information Systems.
Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie
Martin, and Theresa Wilson. 2001. A corpus study of
evaluative and speculative language. In Proceedings
of the Second SIGdial Workshop on Discourse and Di-
alogue, pages 1?10.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In AAAI-IAAI.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: a system for subjectivity anal-
ysis. In HLT/EMNLP.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05.
Bo Yang, William Cheung, and Jiming Liu. 2007. Com-
munity mining from signed social networks. IEEE
Trans. on Knowl. and Data Eng., 19(10).
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03.
14
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 82?88,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Experimental Results on the Native Language Identification Shared Task
Amjad Abu-Jbara, Rahul Jha, Eric Morley, Dragomir Radev
Department of EECS
University of Michigan
Ann Arbor, MI, USA
[amjbara, rahuljha, eamorley, radev]@umich.edu
Abstract
We present a system for automatically iden-
tifying the native language of a writer. We
experiment with a large set of features and
train them on a corpus of 9,900 essays writ-
ten in English by speakers of 11 different lan-
guages. our system achieved an accuracy of
43% on the test data, improved to 63% with
improved feature normalization. In this paper,
we present the features used in our system, de-
scribe our experiments and provide an analysis
of our results.
1 Introduction
The task of Native Language Identification (NLI)
is the task of identifying the native language of a
writer or a speaker by analyzing their writing in
English. Previous work in this area shows that
there are several linguistic cues that can be used
to do such identification. Based on their native
language, different speakers tend to make different
kinds of errors pertaining to spelling, punctuation,
and grammar (Garfield, 1964; Wong and Dras, 2009;
Kochmar, 2011). We describe the complete set of
features we considered in Section 4. We evaluate
different combinations of these features, and differ-
ent ways of normalizing them in Section 5.
There are many possible applications for an NLI
system, as noted by Kochmar (2011): finding the
origins of anonymous text; error correction in var-
ious tasks including speech recognition, part-of-
speech tagging, and parsing; and in the field of sec-
ond language acquisition for identifying learner dif-
ficulties. We are most interested in statistical ap-
proaches to this problem because it may point to-
wards fruitful avenues of research in language and
sound transfer, which are how people apply knowl-
edge of their native language, and its phonology
and orthography, respectively, to a second language.
For example, Tsur and Rappoport (2007) found that
character bigrams are quite useful for NLI, which
led them to suggest that second language learners?
word choice may in part be driven by their native
languages. Analysis of such language and sound
translation patterns might be useful in understand-
ing the process of language acquisition in humans.
2 Previous Work
The work presented in this paper was done as part
of the NLI shared task (Tetreault et al, 2013), which
is the first time this problem has been the subject
of a shared task. However, several researchers have
investigated NLI and similar problems. Authorship
attribution, a related problem, has been well stud-
ied in the literature, starting from the seminal work
on disputed Federalist Papers by Mosteller and Wal-
lace (1964). The goal of authorship attribution is
to assign a text to one author from a candidate set
82
of authors. This technique has many applications,
and has recently been used to investigate terrorist
communication (Abbasi and Chen, 2005) and dig-
ital crime (Chaski, 2005). The goal of NLI some-
what similar to authorship attribution, in that NLI
attempts to distinguish between candidate commu-
nities of people who share a common cultural and
linguistic background, while authorship attribution
distinguishes between candidate individuals.
In the earliest treatment of this problem, Koppel
et al (2005) used stylistic text features to identify
the native language of an author. They used features
based on function words, character n-grams and er-
rors and idiosyncrasies such as spelling errors and
non-standard syntactic constructions. They exper-
imented on a dataset with essays written by non-
native English speakers from five countries, Russia,
Czech Republic, Bulgaria, France and Spain, with
258 instances from each dataset. They trained a
multi-class SVM model using the above features and
reported 10-fold cross validation accuracy of 80.2%.
Tsur and Rappoport (2007) studied the problem
of NLI with a focus on language transfer, i.e. how
a seaker?s native language affects the way in which
they acquire a second language, an important area in
Second Language Acquisition research. Their fea-
ture analysis showed that character bigrams alone
can lead to a classification accuracy of about 66%
in a 5-class task. They concluded that the choice of
words people make when writing in a second lan-
guage is highly influenced by the phonology of their
native language.
Wong and Dras (2009) studied syntactic errors de-
rived from contrastive analysis as features for NLI.
They used the five languages from Koppel et al
(2008) along with Chinese and Japanese, but did not
find an improvement in classification accuracy by
adding error features based on contrastive analysis.
Later, Wong and Dras (2011) studied a more gen-
eral set of syntactic features and showed that adding
these features improved the accuracy significantly.
They also investigated classification models based
on LDA (Wong et al, 2011), but did not find them
to be useful overall. They did, however, notice that
some of the topics were capturing information that
would be useful for identifying particular native lan-
guages. They also proposed the use of adaptor gram-
mars (Johnson et al, 2007), which are a generaliza-
tion of probabilistic context-free grammars, to cap-
ture collocational pairings. In a later paper, Wong
et al explored the use of adapter grammars in de-
tail (Wong et al, 2012) and showed that an exten-
sion of adaptor grammars to discover collocations
beyond lexical words can produce features useful for
the NLI task.
3 Dataset
The experiments for this paper were performed us-
ing the TOEFL11 dataset (Blanchard et al, 2013)
provided as part of the shared task. The dataset con-
tains essays written in English from native speakers
of 11 languages (Arabic, Chinese, French, German,
Hindi, Italian, Japanese, Korean, Spanish, Telugu,
and Turkish). The corpus contains 12,099 essays per
language sampled evenly from 8 prompts or topics.
This dataset was designed specifically to support the
task of NLI and addresses some of the shortcom-
ings of earlier datasets used for research in this area.
Specifically, the dataset has been carefully selected
in order to maintain consistency in topic distribu-
tions, character encodings and annotations across
the essays from different native languages. The data
was split into three data sets: a training set com-
prising 9,900 essays, a development set comprising
1,100 essays, and a test set comprising 1,100 essays.
4 Approach
We addressed the problem as a supervised, multi-
class classification task. We trained a Support Vector
Machine (SVM) classifier on a set of lexical, syntac-
tic and dependency features extracted from the train-
ing data. We computed the minimum and maximum
values for each of the features and normalized the
values by the range (max - min). Here we describe
the features in turn.
83
Character and Word N-grams Tsur and Rap-
poport (2007) found that character bigrams were
useful for NLI, and they suggested that this may be
due to the writer?s native language influencing their
choice of words. To reflect this, we compute features
using both characters and word N-grams. For char-
acters, we consider 2,3 and 4-grams, with padding
characters at the beginning and end of each sentence.
The features are generated over the entire training
data, i.e., every n-gram occurring in the training data
is used as a feature. Similarly, we create features
with 1,2 and 3-grams of words. Each word n-gram
is used as a separate feature. We explore both binary
features for each character or word n-gram, as well
as normalized count features.
Part-Of-Speech N-grams Several investigations,
for example those conducted by Kochmar (2011)
and Wong and Dras (2011), have found that part-of-
speech tags can be useful for NLI. Therefore we in-
clude part-of-speech (POS) n-grams as features. We
parse the sentences with the Stanford Parser (Klein
and Manning, 2003) and extract the POS tags. We
use binary features describing the presence or ab-
sence of POS bigrams in a document, as well as nu-
merical features describing their relative frequency
in a document.
Function Words Koppel et al (2005) found that
function words can help identify someone?s native
language. To this end, we include a categorical fea-
ture for the presence of function words that are in-
cluded in list of 321 function words.
Use of punctuation Based on our experience
with speakers of native languages, as well as
Kochmar?s (2011) observations of written English
produced by Germanic and Romance language
speakers, we suspect that speakers of different native
languages use punctuation in different ways, pre-
sumably based on the punctuation patterns in their
native language. For example, comma placement
differs between German and English, and neither
Chinese nor Japanese requires a full stop at the end
of every sentence. To capture these kinds of patterns,
we create two features for each essay: the number of
punctuation marks used per sentence, and the num-
ber of punctuation marks used per word.
Number of Unique Stems Speakers of different
native languages might differ in the amount of vo-
cabulary they use when communicating in English.
We capture this by counting the number of unique
stems in each essay and using this as an additional
feature. The hypothesis here is that depending on the
similarity of the native language with English, the
presence of common words, and other cultural cues,
people with different native language might have ac-
cess to different amounts of vocabulary.
Misuse of Articles We count instances in which
the number of an article is inconsistent with the as-
sociated noun. To do so, we fist identify all the det
dependency relations in the essay. We then com-
pute the ratio of det relations between ?a? or ?an?
and a plural noun (NNS), to all det relations. We
also count the ratio of det relations between ?a? or
?an? and an uncountable noun, to all det relations.
We do this using a list of 288 uncountable nouns.1
Capitalization The writing systems of some lan-
guages in the data set, for example Telugu, do not
include capitalization. Furthermore, other languages
may use capitalization quite differently from En-
glish, for example German, in which all nouns are
capitalized, and French, in which nationalities are
not. Character capitalization mistakes may be com-
mon in the text written by the speakers of such lan-
guages. We compute the ratio of words with at least
two letters that are written in all caps to identify ex-
cessive capitalization. We also count the relative fre-
quency of capitalized words that appear in the mid-
dle of a sentence that are not tagged as proper nouns
by the part of speech tagger.
Tense and Aspect Frequency Verbal tense and
aspect systems vary widely between languages. En-
glish has obligatory tense (past, present, future) and
1http://www.englishclub.com/vocabulary/nouns-
uncountable-list.htm
84
aspect (imperfect, perfect, progressive) marking on
verbs. Other languages, for example French, may
require verbs to be marked for tense, but not as-
pect. Still other languages, for example Chinese,
may use adverbials and temporal phrases to com-
municate temporal and aspectual information. To
attempt to capture some of the ways learners of En-
glish may be influenced by their native language?s
system of tense and aspect, we compute two fea-
tures. First, we compute the relative frequency of
each tense and aspect in the article from the counts
of each verb POS tags (ex. VB, VBD, VBG). We
also compute the percentage of sentences that con-
tain verbs of different tenses or aspect, again using
the verb POS tags.
Missing Punctuation We compute the relative
frequency of sentences that include an introductory
phrase (e.g. however, furthermore, moreover) that is
not followed by a comma. We also count the relative
frequency of sentences that start with a subordinat-
ing conjunction (e.g. sentences starting with if, after,
before, when, even though, etc.), but do not contain
a comma.
Average Number of Syllables We count the num-
ber of syllables per word and the ratio of words with
three or more syllables. To count the number of syl-
lables in a word, we used a perl module that esti-
mates the number of syllables by applying a set of
hand-crafted rules.2.
Arc Length We calculate several features pertain-
ing to dependency arc length and direction. We
parse each sentence separately, using the Stanford
Dependency Parser, and then compute a single value
for each of these features for each document. First,
we simply compute the percentage of arcs that point
left or right (PCTARCL, PCTARCR). We also com-
pute the minumum, maximum, and mean depen-
dency arc length, ignoring arc direction. We also
compute similar features for typed dependencies:
the minimum, maximum, and mean dependency arc
2http://search.cpan.org/dist/Lingua-EN-
Syllable/Syllable.pm
length for each typed dependency; and the percent-
age of arcs for each typed dependency that go to the
left or right.
Downtoners and Intensifiers We compute three
features to describe the use of downtoners, and three
for intensifiers in each document. First, we count the
number of downtoners or intensifiers in a given doc-
ument.3 We normalize this count by the number of
tokens, types, and sentences in the document to yield
the three features capturing the use of downtoners or
intensifiers.
Production Rules We compute a set of features to
describe the relative frequency of production rules
in a given document. First, we parse each sentence
using the Stanford Parser, using the default English
PCFG (Klein and Manning, 2003). We then count
all non-terminal production rules in a given docu-
ment, and report the relative frequency of each pro-
duction rule in that document.
Subject Agreement We count the number of sen-
tences in which there appears to be a mistake in sub-
ject agreement. To do this, we first identify nsubj
and nsubjpass dependency relationships. Of these
dependencies, we count ones meeting the following
criteria as mistakes: a third person singular present
tense verb with a nominal that is not third person
singular, and a third person singular subject with a
present tense verb not marked as third person sin-
gular. We then normalize the count of errors by the
total number of nsubj and nsubj pass dependencies
in the document, and the number of sentences in the
document to produce two features.
Words per Sentence We compute both the num-
ber of tokens per line and the number of types per
3The words we count as downtoners are: ?almost?, ?alot?,
?a lot?, ?barely?, ?a bit?, ?fairly?, ?hardly?, ?just?, ?kind of?,
?least?, ?less?, ?merely?, ?mildly?, ?nearly?, ?only?, ?partially?,
?partly?, ?practically?, ?rather?, ?scarcely?, ?sort of?, ?slightly?,
and ?somewhat?. The intensifiers are: ?a good deal?, ?a great
deal?, ?absolutely?, ?altogether?, ?completely?,?enormously?,
?entirely?, ?extremely?, ?fully?, ?greatly?, ?highly?, ?intensely?,
?more?, ?most?, ?perfectly?, ?quite?, ?really?, ?so?, ?strongly?,
?super?, ?thoroughly?, ?too?, ?totally?, ?utterly?, and ?very?.
85
line.
Topic Scores We construct an unsupervised topic
model for all of the documents using Mallet (Mc-
Callum, 2002) with 100 topics, dirichlet hyperpa-
rameter reestimation every 10 rounds, and all other
options set to default values. We then use the topic
weights as features.
Passive Constructions We count the number of
times an author uses passive constructions by count-
ing the number of nsubjpass dependencies in each
document. We normalize this count in two ways to
produce two different features: dividing by the num-
ber of sentences, and dividing by the total number of
nsubj and nsubjpass dependencies.
5 Experiments and Results
We used weka (Hall et al, 2009) and libsvm (Chang
and Lin, 2011) to run the experiments. The classi-
fication was done using an SVM classifier. We ex-
perimented with different SVM kernels and different
values for the cost parameter. The best performance
was achieved with a linear kernel and cost = 0.001.
We trained the model using the combination of the
training and the development sets. We submitted the
output of the system to the NLI shared task work-
shop. Our system achieved 43.3% accuracy. Table 1
shows the confusion matrix and the precision, recall,
and F-measure for each language. After the NLI
submission deadline, we noticed that we our system
was not handling the normalization of the features
properly which resulted in the poor performance.
After fixing the problem, our system achieved 63%
accuracy on both test data and 10-fold cross valida-
tion on the entire data.
6 Analysis
We did feature analysis on the training and devel-
opment data sets using the Chi-squared test. Our
feature analysis shows that the most important fea-
tures for the classifier were topic models, charac-
ter n-grams of all orders, word unigrams and bi-
grams, POS bigrams, capitalization features, func-
tion words, production rules, and arc length. These
results are consistent with those presented in previ-
ous work done on this task.
Looking at the confusion matrix in Figure 1, we
see that Korean and Japanese were the most com-
monly confused pair of languages. Hindi and Tel-
ugu, two languages from the Indian subcontinent,
were also often confused. To analyze this further,
we did another experiment by training just a binary
classifier on Korean and Japanese using the exact
same feature set as earlier. We achieved a 10-fold
cross validation accuracy of 83.3% on this classifi-
cation task. Thus, given just these two languages,
we were able to obtain high classification accuracy.
This suggests that a potentially fruitful strategy for
NLI systems might be to fuse often-confused pairs,
such as Korean/Japanese and Hindi/Telugu, into sin-
gleton classes for the initial run, and then run a sec-
ond classifier to do a more fine grained classification
within these higher level classes.
When doing feature analysis for these two lan-
guages, we found that the character bigrams rep-
resenting the country names were some of the top
features used for classification. For example ?Kor?
occurred as a trigram frequently in essays from na-
tive language speakers of Korean. Based on this, we
designed a small experiment where we created fea-
tures corresponding to the country name associated
with each native language, e.g., ?Korea?, ?China?,
?India?, ?France?, etc. For Arabic, we used a list of
22 countries where Arabic is spoken. Just using this
feature, we obtained a 10-fold cross validation accu-
racy of 21.3% on the development set. This suggests
that in certain genres, one may be able to leverage in-
formation conveying geographical and demographic
attributes for NLI.
7 Conclusion
In this paper, we presented a supervised system for
the task of Native Language Identification. We de-
scribe and motivate several features for this task
and report results of supervised classification using
these features on a test data set consisting of 11 lan-
86
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measure
ARA 41 7 8 3 6 2 3 5 10 7 8 44.6% 41.0% 42.7%
CHI 6 38 5 2 2 8 15 8 3 3 10 40.0% 38.0% 39.0%
FRE 8 6 43 8 1 14 2 4 6 1 7 39.1% 43.0% 41.0%
GER 3 3 10 49 4 9 1 7 6 0 8 54.4% 49.0% 51.6%
HIN 5 2 6 9 34 0 3 1 3 32 5 47.9% 34.0% 39.8%
ITA 5 3 10 5 1 52 2 1 17 0 4 46.0% 52.0% 48.8%
JPN 3 11 0 1 1 3 49 26 1 1 4 37.4% 49.0% 42.4%
KOR 2 6 6 1 1 2 35 40 1 1 5 38.1% 40.0% 39.0%
SPA 4 6 14 1 1 17 6 2 38 0 11 40.9% 38.0% 39.4%
TEL 9 7 3 4 18 2 2 2 2 48 3 51.1% 48.0% 49.5%
TUR 6 6 5 7 2 4 13 9 6 1 41 38.7% 41.0% 39.8%
Accuracy = 43.0%
Table 1: The results of our original submission to the NLI shared task on the test set. These results reflect the
performance of the system that does not normalize the features properly
guages provided as part of the NLI shared task. We
found that our classifier often confused two pairs
of languages that are spoken near one another, but
are linguistically unrelated: Hindi/Telugu and Ko-
rean/Japanese. We found that we could obtain high
classification accuracy on these two pairs of lan-
guages using a binary classifier trained on just these
pairs. During our feature analysis, we also found
that certain features that happened to convey geo-
graphical and demographic information were also
informative for this task.
References
Ahmed Abbasi and Hsinchun Chen. 2005. Apply-
ing authorship analysis to extremist-group web fo-
rum messages. IEEE Intelligent Systems, 20(5):67?75,
September.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Carole E. Chaski. 2005. Who?s at the keyboard: Au-
thorship attribution in digital evidence investigations.
International Journal of Digital Evidence, 4:2005.
Eugene Garfield. 1964. Can citation indexing be auto-
mated?
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for speci-
fying compositional nonparametric bayesian models.
Advances in neural information processing systems,
19:641.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Associ-
ation for Computational Linguistics.
Ekaterina Kochmar. 2011. Identification of a Writer?s
Native Langauge by Error Analysis. Ph.D. thesis.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2008. Computational methods in authorship attribu-
tion. Journal of the American Society for information
Science and Technology, 60(1):9?26.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Frederick Mosteller and David L. Wallace. 1964. Infer-
ence and Disputed Authorship: The Federalist Papers.
Addison-Wesley, Reading, Mass.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
87
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Com-
putational Language Acquisition, CACLA ?07, pages
9?16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
Analysis and Native Language Identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop 2009, pages 53?61, Sydney, Aus-
tralia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic Modeling for Native Language Identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124, Canberra, Australia, December.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
88
