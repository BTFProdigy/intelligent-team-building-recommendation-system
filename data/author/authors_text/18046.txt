Workshop on Computational Linguistics for Literature, pages 26?35,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Stylistic Segmentation of Poetry
with Change Curves and Extrinsic Features
Julian Brooke
Dept of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Adam Hammond
Dept of English
University of Toronto
adam.hammond@utoronto.ca
Graeme Hirst
Dept of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
The identification of stylistic inconsistency is a
challenging task relevant to a number of gen-
res, including literature. In this work, we
carry out stylistic segmentation of a well-known
poem, The Waste Land by T.S. Eliot, which
is traditionally analyzed in terms of numerous
voices which appear throughout the text. Our
method, adapted from work in topic segmen-
tation and plagiarism detection, predicts breaks
based on a curve of stylistic change which com-
bines information from a diverse set of features,
most notably co-occurrence in larger corpora via
reduced-dimensionality vectors. We show that
this extrinsic information is more useful than
(within-text) distributional features. We achieve
well above baseline performance on both artifi-
cial mixed-style texts and The Waste Land itself.
1 Introduction
Most work in automated stylistic analysis operates
at the level of a text, assuming that a text is stylis-
tically homogeneous. However, there are a number
of instances where that assumption is unwarranted.
One example is documents collaboratively created
by multiple authors, in which contributors may, ei-
ther inadvertently or deliberately (e.g. Wikipedia
vandalism), create text which fails to form a stylis-
tically coherent whole. Similarly, stylistic incon-
sistency might also arise when one of the ?contrib-
utors? is actually not one of the purported authors
of the work at all ? that is, in cases of plagia-
rism. More-deliberate forms of stylistic dissonance
include satire, which may first follow and then flout
the stylistic norms of a genre, and much narrative lit-
erature, in which the author may give the speech or
thought patterns of a particular character their own
style distinct from that of the narrator. In this paper,
we address this last source of heterogeneity in the
context of the well-known poem The Waste Land by
T.S. Eliot, which is often analyzed in terms of the
distinct voices that appear throughout the text.
T.S. Eliot (1888?1965), recipient of the 1948 No-
bel Prize for Literature, is among the most important
twentieth-century writers in the English language.
Though he worked in a variety of forms ? he was
a celebrated critic as well as a dramatist, receiving
a Tony Award in 1950 ? he is best remembered to-
day for his poems, of which The Waste Land (1922)
is among the most famous. The poem deals with
themes of spiritual death and rebirth. It is notable
for its disjunctive structure, its syncopated rhythms,
its wide range of literary allusions, and its incorpo-
ration of numerous other languages. The poem is di-
vided into five parts; in total it is 433 lines long, and
contains 3533 tokens, not including the headings.
A prominent debate among scholars of The Waste
Land concerns whether a single speaker?s voice pre-
dominates in the poem (Bedient, 1986), or whether
the poem should be regarded instead as dramatic
or operatic in structure, composed of about twelve
different voices independent of a single speaker
(Cooper, 1987). Eliot himself, in his notes to The
Waste Land, supports the latter view by referring to
?characters? and ?personage[s]? in the poem.
One of the poem?s most distinctive voices is that
of the woman who speaks at the end of its second
section:
26
I can?t help it, she said, pulling a long face,
It?s them pills I took, to bring it off, she said
[158?159]
Her chatty tone and colloquial grammar and lexis
distinguish her voice from many others in the poem,
such as the formal and traditionally poetic voice of a
narrator that recurs many times in the poem:
Above the antique mantel was displayed
As though a window gave upon the sylvan scene
The change of Philomel
[97?99]
While the stylistic contrasts between these and
other voices are apparent to many readers, Eliot
does not explicitly mark the transitions between
them. The goal of the present work is to investigate
whether computational stylistic analysis can identify
the transition between one voice and the next.
Our unsupervised approach, informed by research
in topic segmentation (Hearst, 1994) and intrinsic
plagiarism detection (Stamatatos, 2009), is based
on deriving a curve representing stylistic change,
where the local maxima represent likely transition
points. Notably, our curve represents an amalga-
mation of different stylistic metrics, including those
that incorporate external (extrinsic) knowledge, e.g.
vector representations based on larger corpus co-
occurrence, which we show to be extremely use-
ful. For development and initial testing we follow
other work on stylistic inconsistency by using arti-
ficial (mixed) poems, but the our main evaluation is
on The Waste Land itself. We believe that even when
our segmentation disagrees with expert human judg-
ment, it has the potential to inform future study of
this literary work.
2 Related work
Poetry has been the subject of extensive computa-
tional analysis since the early days of literary and
linguistic computing (e.g., Beatie 1967). Most of the
research concerned either authorship attribution or
analysis of metre, rhyme, and phonetic properties of
the texts, but some work has studied the style, struc-
ture, and content of poems with the aim of better un-
derstanding their qualities as literary texts. Among
research that, like the present paper, looks at varia-
tion with a single text, Simonton (1990) found quan-
titative changes in lexical diversity and semantic
classes of imagery across the components of Shake-
speare?s sonnets, and demonstrated correlations be-
tween some of these measures and judgments of the
?aesthetic success? of individual sonnets. Duggan
(1973) developed statistical measures of formulaic
style to determine whether the eleventh-century epic
poem Chanson de Ronald manifests primarily an
oral or a written style. Also related to our work,
although it concerned a novel rather than a poem,
is that of McKenna and Antonia (2001), who used
principal component analysis of lexical frequency
to discriminate different voices (dialogue, interior
monologue, and narrative) and different narrative
styles in sections of Ulysses by James Joyce.
More general work on identifying stylistic incon-
sistency includes that of Graham et al (2005), who
built artificial examples of style shift by concate-
nating Usenet postings by different authors. Fea-
ture sets for their neural network classifiers included
standard textual features, frequencies of function
words, punctuation and parts of speech, lexical en-
tropy, and vocabulary richness. Guthrie (2008) pre-
sented some general methods for identifying stylis-
tically anomalous segments using feature vector dis-
tance, and tested the effectiveness of his unsuper-
vised method with a number of possible stylistic
variations. He used features such as simple textual
metrics (e.g. word and sentence length), readability
measures, obscure vocabulary features, frequency
rankings of function words (which were not found
to be useful), and context analysis features from
the General Inquirer dictionary. The most effective
method ranked each segment according to the city-
block distance of its feature vector to the feature vec-
tor of the textual complement (the union of all other
segments in the text). Koppel et al (2011) used a
semi-supervised method to identify segments from
two different books of the Bible artificially mixed
into a single text. They first demonstrated that, in
this context, preferred synonym use is a key stylis-
tic feature that can serve as high-precision boot-
strap for building a supervised SVM classifier on
more general features (common words); they then
used this classifier to provide an initial prediction
for each verse and smooth the results over adjacent
segments. The method crucially relied on properties
of the King James Version translation of the text in
27
order to identify synonym preferences.
The identification of stylistic inconsistency or het-
erogeneity has received particular attention as a
component of intrinsic plagiarism detection ? the
task of ?identify[ing] potential plagiarism by analyz-
ing a document with respect to undeclared changes
in writing style? (Stein et al, 2011). A typical ap-
proach is to move a sliding window over the text
looking for areas that are outliers with respect to the
style of the rest of the text, or which differ markedly
from other regions in word or character-trigram fre-
quencies (Oberreuter et al, 2011; Kestemont et al,
2011). In particular, Stamatatos (2009) used a win-
dow that compares, using a special distance func-
tion, a character trigram feature vector at various
steps throughout the text, creating a style change
function whose maxima indicate points of interest
(potential plagarism).
Topic segmentation is a similar problem that has
been quite well-explored. A common thread in this
work is the importance of lexical cohesion, though
a large number of competing models based on this
concept have been proposed. One popular unsu-
pervised approach is to identify the points in the
text where a metric of lexical coherence is at a (lo-
cal) minimum (Hearst, 1994; Galley et al, 2003).
Malioutov and Barzilay (2006) also used a lexi-
cal coherence metric, but applied a graphical model
where segmentations are graph cuts chosen to max-
imize coherence of sentences within a segment, and
minimize coherence among sentences in different
segments. Another class of approaches is based
on a generative model of text, for instance HMMs
(Blei and Moreno, 2001) and Bayesian topic mod-
eling (Utiyama and Isahara, 2001; Eisenstein and
Barzilay, 2008); in such approaches, the goal is to
choose segment breaks that maximize the probabil-
ity of generating the text, under the assumption that
each segment has a different language model.
3 Stylistic change curves
Many popular text segmentation methods depend
crucially on a reliable textual unit (often a sentence)
which can be reliably classified or compared to oth-
ers. But, for our purposes here, a sentence is both
too small a unit ? our stylistic metrics will be more
accurate over larger spans ? and not small enough
? we do not want to limit our breaks to sentence
boundaries. Generative models, which use a bag-of-
words assumption, have a very different problem: in
their standard form, they can capture only lexical co-
hesion, which is not the (primary) focus of stylistic
analysis. In particular, we wish to segment using in-
formation that goes beyond the distribution of words
in the text being segmented. The model for stylis-
tic segmentation we propose here is related to the
TextTiling technique of Hearst (1994) and the style
change function of Stamatatos (2009), but our model
is generalized so that it applies to any numeric met-
ric (feature) that is defined over a span; importantly,
style change curves represent the change of a set of
very diverse features.
Our goal is to find the precise points in the text
where a stylistic change (a voice switch) occurs. To
do this, we calculate, for each token in the text, a
measure of stylistic change which corresponds to
the distance of feature vectors derived from a fixed-
length span on either side of that point. That is, if vi j
represents a feature vector derived from the tokens
between (inclusive) indices i and j, then the stylistic
change at point ci for a span (window) of size w is:
ci = Dist(v(i?w)(i?1),vi(i+w?1))
This function is not defined within w of the edge of
the text, and we generally ignore the possibility of
breaks within these (unreliable) spans. Possible dis-
tance metrics include cosine distance, euclidean dis-
tance, and city-block distance. In his study, Guthrie
(2008) found best results with city-block distance,
and that is what we will primarily use here. The fea-
ture vector can consist of any features that are de-
fined over a span; one important step, however, is to
normalize each feature (here, to a mean of 0 and a
standard deviation of 1), so that different scaling of
features does not result in particular features having
an undue influence on the stylistic change metric.
That is, if some feature is originally measured to be
fi in the span i to i+w?1, then its normalized ver-
sion f ?i (included in vi(i+w?1)) is:
f ?i =
fi? f
? f
The local maxima of c represent our best predic-
tions for the stylistic breaks within a text. However,
28
stylistic change curves are not well behaved; they
may contain numerous spurious local maxima if a
local maximum is defined simply as a higher value
between two lower ones. We can narrow our def-
inition, however, by requiring that the local max-
imum be maximal within some window w?. That
is, our breakpoints are those points i where, for all
points j in the span x?w?, x+w?, it is the case that
gi > g j. As it happens, w? = w/2 is a fairly good
choice for our purposes, creating spans no smaller
than the smoothed window, though w? can be low-
ered to increase breaks, or increased to limit them.
The absolute height of the curve at each local min-
imum offers a secondary way of ranking (and elim-
inating) potential breakpoints, if more precision is
required; however, in our task here the breaks are
fairly regular but often subtle, so focusing only on
the largest stylistic shifts is not necessarily desirable.
4 Features
The set of features we explore for this task falls
roughly into two categories: surface and extrinsic.
The distinction is not entirely clear cut, but we wish
to distinguish features that use the basic properties
of the words or their PoS, which have traditionally
been the focus of automated stylistic analysis, from
features which rely heavily on external lexical infor-
mation, for instance word sentiment and, in partic-
ular, vector space representations, which are more
novel for this task.
4.1 Surface Features
Word length A common textual statistic in reg-
ister and readability studies. Readability, in turn,
has been used for plagiarism detection (Stein et al,
2011), and related metrics were consistently among
the best for Guthrie (2008).
Syllable count Syllable count is reasonably good
predictor of the difficulty of a vocabulary, and is
used in some readability metrics.
Punctuation frequency The presence or absence
of punctuation such as commas, colons, semicolons
can be very good indicator of style. We also include
periods, which offer a measure of sentence length.
Line breaks Our only poetry-specific feature; we
count the number of times the end of a line appears
in the span. More or fewer line breaks (that is, longer
or shorter lines) can vary the rhythm of the text, and
thus its overall feel.
Parts of speech Lexical categories can indicate,
for instance, the degree of nominalization, which is
a key stylistic variable (Biber, 1988). We collect
statistics for the four main lexical categories (noun,
verb, adjective, adverb) as well as prepositions, de-
terminers, and proper nouns.
Pronouns We count the frequency of first-,
second-, and third-person pronouns, which can in-
dicate the interactiveness and narrative character of
a text (Biber, 1988).
Verb tense Past tense is often preferred in narra-
tives, whereas present tense can give a sense of im-
mediacy.
Type-token ratio A standard measure of lexical
diversity.
Lexical density Lexical density is the ratio of the
count of tokens of the four substantive parts of
speech to the count of all tokens.
Contextuality measure The contextuality mea-
sure of Heylighen and Dewaele (2002) is based on
PoS tags (e.g. nouns decrease contextuality, while
verbs increase it), and has been used to distin-
guish formality in collaboratively built encyclope-
dias (Emigh and Herring, 2005).
Dynamic In addition to the hand-picked features
above, we test dynamically including words and
character trigrams that are common in the text being
analyzed, particularly those not evenly distributed
throughout the text (we exclude punctuation). To
measure the latter, we define clumpiness as the
square root of the index of dispersion or variance-
to-mean ratio (Cox and Lewis, 1966) of the (text-
length) normalized differences between successive
occurrences of a feature, including (importantly) the
difference between the first index of the text and the
first occurrence of the feature as well as the last oc-
currence and the last index; the measure varies be-
tween 0 and 1, with 0 indicating perfectly even dis-
tribution. We test with the top n features based on
the ranking of the product of the feature?s frequency
29
in the text (tf ) or product of the frequency and its
clumpiness (tf-cl); this is similar to a tf-idf weight.
4.2 Extrinsic features
For those lexicons which include only lemmatized
forms, the words are lemmatized before their values
are retrieved.
Percent of words in Dale-Chall Word List A list
of 3000 basic words that is used in the Dale-Chall
Readability metric (Dale and Chall, 1995).
Average unigram count in 1T Corpus Another
metric of whether a word is commonly used. We use
the unigram counts in the 1T 5-gram Corpus (Brants
and Franz, 2006). Here and below, if a word is not
included it is given a zero.
Sentiment polarity The positive or negative
stance of a span could be viewed as a stylistic vari-
able. We test two lexicons, a hand-built lexicon for
the SO-CAL sentiment analysis system which has
shown superior performance in lexicon-based sen-
timent analysis (Taboada et al, 2011), and Senti-
WordNet (SWN), a high-coverage automatic lexicon
built from WordNet (Baccianella et al, 2010). The
polarity of each word over the span is averaged.
Sentiment extremity Both lexicons provide a
measure of the degree to which a word is positive or
negative. Instead of summing the sentiment scores,
we sum their absolute values, to get a measure of
how extreme (subjective) the span is.
Formality Average formality score, using a lex-
icon of formality (Brooke et al, 2010) built using
latent semantic analysis (LSA) (Landauer and Du-
mais, 1997).
Dynamic General Inquirer The General Inquirer
dictionary (Stone et al, 1966), which was used for
stylistic inconsistency detection by Guthrie (2008),
includes 182 content analysis tags, many of which
are relevant to style; we remove the two polarity tags
already part of the SO-CAL dictionary, and select
others dynamically using our tf-cl metric.
LSA vector features Brooke et al (2010) have
posited that, in highly diverse register/genre corpora,
the lowest dimensions of word vectors derived us-
ing LSA (or other dimensionality reduction tech-
niques) often reflect stylistic concerns; they found
that using the first 20 dimensions to build their for-
mality lexicon provided the best results in a near-
synonym evaluation. Early work by Biber (1988)
in the Brown Corpus using a related technique (fac-
tor analysis) resulted in discovery of several identi-
fiable dimensions of register. Here, we investigate
using these LSA-derived vectors directly, with each
of the first 20 dimensions corresponding to a sepa-
rate feature. We test with vectors derived from the
word-document matrix of the ICWSM 2009 blog
dataset (Burton et al, 2009) which includes 1.3 bil-
lion tokens, and also from the BNC (Burnard, 2000),
which is 100 million tokens. The length of the vector
depends greatly on the frequency of the word; since
this is being accounted for elsewhere, we normalize
each vector to the unit circle.
5 Evaluation method
5.1 Metrics
To evaluate our method we apply standard topic
segmentation metrics, comparing the segmentation
boundaries to a gold standard reference. The mea-
sure Pk, proposed by Beeferman et al (1997), uses a
probe window equal to half the average length of a
segment; the window slides over the text, and counts
the number of instances where a unit (in our case,
a token) at one edge of the window was predicted
to be in the same segment (according to the refer-
ence) as a unit at the other edge, but in fact is not; or
was predicted not to be in the same segment, but in
fact is. This count is normalized by the total number
of tests to get a score between 0 and 1, with 0 be-
ing a perfect score (the lower, the better). Pevzner
and Hearst (2002) criticize this metric because it
penalizes false positives and false negatives differ-
ently and sometimes fails to penalize false positives
altogether; their metric, WindowDiff (WD), solves
these problems by counting an error whenever there
is a difference between the number of segments in
the prediction as compared to the reference. Recent
work in topic segmentation (Eisenstein and Barzi-
lay, 2008) continues to use both metrics, so we also
present both here.
During initial testing, we noted a fairly serious
shortcoming with both these metrics: all else be-
ing equal, they will usually prefer a system which
30
predicts fewer breaks; in fact, a system that predicts
no breaks at all can score under 0.3 (a very com-
petitive result both here and in topic segmentation),
if the variation of the true segment size is reason-
ably high. This is problematic because we do not
want to be trivially ?improving? simply by moving
towards a model that is too cautious to guess any-
thing at all. We therefore use a third metric, which
we call BD (break difference), which sums all the
distances, calculated as fractions of the entire text,
between each true break and the nearest predicted
break. This metric is also flawed, because it can be
trivially made 0 (the best score) by guessing a break
everywhere. However, the relative motion of the two
kinds of metric provides insight into whether we are
simply moving along a precision/recall curve, or ac-
tually improving overall segmentation.
5.2 Baselines
We compare our method to the following baselines:
Random selection We randomly select bound-
aries, using the same number of boundaries in the
reference. We use the average over 50 runs.
Evenly spaced We put boundaries at equally
spaced points in the text, using the same number of
boundaries as the reference.
Random feature We use our stylistic change
curve method with a single feature which is created
by assigning a uniform random value to each token
and averaging across the span. Again, we use the
average score over 50 runs.
6 Experiments
6.1 Artificial poems
Our main interest is The Waste Land. It is, however,
prudent to develop our method, i.e. conduct an initial
investigation of our method, including parameters
and features, using a separate corpus. We do this by
building artificial mixed-style poems by combining
stylistically distinct poems from different authors, as
others have done with prose.
6.1.1 Setup
Our set of twelve poems used for this evaluation was
selected by one of the authors (an English literature
expert) to reflect the stylistic range and influences
of poetry at the beginning of the twentieth century,
and The Waste Land in particular. The titles were
removed, and each poem was tagged by an auto-
matic PoS tagger (Schmid, 1995). Koppel et al built
their composite version of two books of the Bible by
choosing, at each step, a random span length (from a
uniform distribution) to include from one of the two
books being mixed, and then a span from the other,
until all the text in both books had been included.
Our method is similar, except that we first randomly
select six poems to include in the particular mixed
text, and at each step we randomly select one of po-
ems, reselecting if the poem has been used up or the
remaining length is below our lower bound. For our
first experiment, we set a lower bound of 100 tokens
and an upper bound of 200 tokens for each span; al-
though this gives a higher average span length than
that of The Waste Land, our first goal is to test
whether our method works in the (ideal) condition
where the feature vectors at the breakpoint gener-
ally represent spans which are purely one poem or
another for a reasonably high w (100). We create 50
texts using this method. In addition to testing each
individual feature, we test several combinations of
features (all features, all surface features, all extrin-
sic features), and present the best results for greedy
feature removal, starting with all features (exclud-
ing dynamic ones) and choosing features to remove
which minimize the sum of the three metrics.
6.1.2 Results
The Feature Sets section of Table 1 gives the in-
dividual feature results for segmentation of the
artificially-combined poems. Using any of the fea-
tures alone is better than our baselines, though some
of the metrics (in particular type-token ratio) are
only a slight improvement. Line breaks are obvi-
ously quite useful in the context of poetry (though
the WD score is high, suggesting a precision/recall
trade-off), but so are more typical stylistic features
such as the distribution of basic lexical categories
and punctuation. The unigram count and formal-
ity score are otherwise the best two individual fea-
tures. The sentiment-based features did more mod-
estly, though the extremeness of polarity was use-
ful when paired with the coverage of SentiWord-
Net. Among the larger feature sets, the GI was the
least useful, though more effective than any of the
31
Table 1: Segmentation accuracy in artificial poems
Configuration Metrics
WD Pk BD
Baselines
Random breaks 0.532 0.465 0.465
Even spread 0.498 0.490 0.238
Random feature 0.507 0.494 0.212
Feature sets
Word length 0.418 0.405 0.185
Syllable length 0.431 0.419 0.194
Punctuation 0.412 0.401 0.183
Line breaks 0.390 0.377 0.200
Lexical category 0.414 0.402 0.177
Pronouns 0.444 0.432 0.213
Verb tense 0.444 0.433 0.202
Lexical density 0.445 0.433 0.192
Contextuality 0.462 0.450 0.202
Type-Token ratio 0.494 0.481 0.204
Dynamic (tf, n=50) 0.399 0.386 0.161
Dynamic (tf-cl, 50) 0.385 0.373 0.168
Dynamic (tf-cl, 500) 0.337 0.323 0.165
Dynamic (tf-cl, 1000) 0.344 0.333 0.199
Dale-Chall 0.483 0.471 0.202
Count in 1T 0.424 0.414 0.193
Polarity (SO-CAL) 0.466 0.487 0.209
Polarity (SWN) 0.490 0.478 0.221
Extremity (SO-CAL) 0.450 0.438 0.199
Extremity (SWN) 0.426 0.415 0.182
Formality 0.409 0.397 0.184
All LSA (ICWSM) 0.319 0.307 0.134
All LSA (BNC) 0.364 0.352 0.159
GI (tf, n=5) 0.486 0.472 0.201
GI (tf-cl, 5) 0.449 0.438 0.196
GI (tf-cl, 50) 0.384 0.373 0.164
GI (tf-cl, 100) 0.388 0.376 0.163
Combinations
Surface 0.316 0.304 0.150
Extrinsic 0.314 0.301 0.124
All 0.285 0.274 0.128
All w/o GI, dynamic 0.272 0.259 0.102
All greedy (Best) 0.253 0.242 0.099
Best, w=150 0.289 0.289 0.158
Best, w=50 0.338 0.321 0.109
Best, Diff=euclidean 0.258 0.247 0.102
Best, Diff=cosine 0.274 0.263 0.145
individual features, while dynamic word and char-
acter trigrams did better, and the ICWSM LSA vec-
tors better still; the difference in size between the
ICWSM and BNC is obviously key to the perfor-
mance difference here. In general using our tf-cl
metric was better than tf alone.
When we combine the different feature types, we
see that extrinsic features have a slight edge over the
surface features, but the two do complement each
other to some degree. Although the GI and dynamic
feature sets do well individually, they do not com-
bine well with other features in this unsupervised
setting, and our best results do not include them.
The greedy feature selector removed 4 LSA dimen-
sions, type-token ratio, prepositions, second-person
pronouns, adverbs, and verbs to get our best result.
Our choice of w to be the largest fully-reliable size
(100) seems to be a good one, as is our use of city-
block distance rather than the alternatives. Overall,
the metrics we are using for evaluation suggest that
we are roughly halfway to perfect segmentation.
6.2 The Waste Land
6.2.1 Setup
In order to evaluate our method on The Waste Land,
we first created a gold standard voice switch seg-
mentation. Our gold standard represents an amal-
gamation, by one of the authors, of several sources
of information. First, we enlisted a class of 140 un-
dergraduates in an English literature course to seg-
ment the poem into voices based on their own intu-
itions, and we created a combined student version
based on majority judgment. Second, our English
literature expert listened to the 6 readings of the
poem included on The Waste Land app (Touch Press
LLP, 2011), including two readings by T.S. Eliot,
and noted places where the reader?s voice seemed
to change; these were combined to create a reader
version. Finally, our expert amalgamated these two
versions and incorporated insights from independent
literary analysis to create a final gold standard.
We created two versions of the poem for evalua-
tion: for both versions, we removed everything but
the main body of the text (i.e. the prologue, dedi-
cation, title, and section titles), since these are not
produced by voices in the poem. The ?full? ver-
sion contains all the other text (a total of 68 voice
32
switches), but our ?abridged? version involves re-
moving all segments (and the corresponding voice
switches, when appropriate) which are 20 or fewer
tokens in length and/or which are in a language
other than English, which reduces the number of
voice switches to 28 (the token count is 3179). This
version allows us to focus on the segmentation for
which our method has a reasonable chance of suc-
ceeding and ignore the segmentation of non-English
spans, which is relatively trivial but yet potentially
confounding. We use w = 50 for the full version,
since there are almost twice as many breaks as in
the abridged version (and our artificially generated
texts).
6.2.2 Results
Our results for The Waste Land are presented in Ta-
ble 2. Notably, in this evaluation, we do not investi-
gate the usefulness of individual features or attempt
to fully optimize our solution using this text. Our
goal is to see if a general stylistic segmentation sys-
tem, developed on artificial texts, can be applied suc-
cessfully to the task of segmenting an actual stylis-
tically diverse poem. The answer is yes. Although
the task is clearly more difficult, the results for the
system are well above the baseline, particularly for
the abridged version. One thing to note is that using
the features greedily selected for the artificial sys-
tem (instead of just all features) appears to hinder,
rather than help; this suggests a supervised approach
might not be effective. The GI is too unreliable to
be useful here, whereas the dynamic word and tri-
gram features continue to do fairly well, but they do
not improve the performance of the rest of the fea-
tures combined. Once again the LSA features seem
to play a central role in this success. We manually
compared predicted with real switches and found
that there were several instances (corresponding to
very clear voices switches in the text) which were
nearly perfect. Moreover, the model did tend to pre-
dict more switches in sections with numerous real
switches, though these predictions were often fewer
than the gold standard and out of sync (because the
sampling windows never consisted of a pure style).
7 Conclusion
In this paper we have presented a system for auto-
matically segmenting stylistically inconsistent text
Table 2: Segmentation accuracy in The Waste Land
Configuration Metrics
WD Pk BD
Full text
Baselines
Random breaks 0.517 0.459 0.480
Even spread 0.559 0.498 0.245
Random feature 0.529 0.478 0.314
System (w=50)
Table 1 Best 0.458 0.401 0.264
GI 0.508 0.462 0.339
Dynamic 0.467 0.397 0.257
LSA (ICWSM) 0.462 0.399 0.280
All w/o GI 0.448 0.395 0.305
All w/o dynamic, GI 0.456 0.394 0.228
Abridged text
Baselines
Random breaks 0.524 0.478 0.448
Even spread 0.573 0.549 0.266
Random feature 0.525 0.505 0.298
System (w=100)
Table 1 Best 0.370 0.341 0.250
GI 0.510 0.492 0.353
Dynamic 0.415 0.393 0.274
LSA (ICWSM) 0.411 0.390 0.272
All w/o GI 0.379 0.354 0.241
All w/o dynamic, GI 0.345 0.311 0.208
and applied it to The Waste Land, a well-known
poem in which stylistic variation, in the form of dif-
ferent ?voices?, provides an interesting challenge to
both human and computer readers. Our unsuper-
vised model is based on a stylistic change curve de-
rived from feature vectors. Perhaps our most inter-
esting result is the usefulness of low-dimension LSA
vectors over surface features such as words and tri-
gram characters as well as other extrinsic features
such as the GI dictionary. In both The Waste Land
and our development set of artificially combined po-
ems, our method performs well above baseline. Our
system could probably benefit from the inclusion of
machine learning, but our main interest going for-
ward is the inclusion of additional features ? in par-
ticular, poetry-specific elements such as alliteration
and other more complex lexicogrammatical features.
33
Acknowledgments
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta, May.
Bruce A. Beatie. 1967. Computer study of medieval Ger-
man poetry: A conference report. Computers and the
Humanities, 2(2):65?70.
Calvin Bedient. 1986. He Do the Police in Different
Voices: The Waste Land and its protagonist. Univer-
sity of Chicago Press.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. Text segmentation using exponential models. In
In Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing (EMNLP
?97), pages 35?46.
Douglas Biber. 1988. Variation Across Speech and Writ-
ing. Cambridge University Press.
David M. Blei and Pedro J. Moreno. 2001. Topic seg-
mentation with an aspect hidden Markov model. In
Proceedings of the 24th annual international ACM SI-
GIR conference on Research and Development in In-
formation Retrieval, SIGIR ?01, pages 343?348.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus Version 1.1. Google Inc.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10).
Lou Burnard. 2000. User reference guide for British
National Corpus. Technical report, Oxford University.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
John Xiros Cooper. 1987. T.S. Eliot and the politics of
voice: The argument of The Waste Land. UMI Re-
search Press, Ann Arbor, Mich.
David R. Cox and Peter A.W. Lewis. 1966. The Sta-
tistical Analysis of Series of Events. Monographs on
Statistics and Applied Probability. Chapman and Hall.
Edgar Dale and Jeanne Chall. 1995. Readability Re-
visited: The New Dale-Chall Readability Formula.
Brookline Books, Cambridge, MA.
Joseph J. Duggan. 1973. The Song of Roland: Formulaic
style and poetic craft. University of California Press.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?08, EMNLP ?08, pages
334?343.
William Emigh and Susan C. Herring. 2005. Collabo-
rative authoring on the web: A genre analysis of on-
line encyclopedias. In Proceedings of the 38th Annual
Hawaii International Conference on System Sciences
(HICSS ?05), Washington, DC.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics (ACL ?03), ACL ?03, pages 562?569.
Neil Graham, Graeme Hirst, and Bhaskara Marthi. 2005.
Segmenting documents by stylistic character. Natural
Language Engineering, 11(4):397?415.
David Guthrie. 2008. Unsupervised Detection of
Anomalous Text. Ph.D. thesis, University of Sheffield.
Marti A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?94), ACL ?94, pages 9?16.
Francis Heylighen and Jean-Marc Dewaele. 2002. Vari-
ation in the contextuality of language: An empirical
measure. Foundations of Science, 7(3):293?340.
Mike Kestemont, Kim Luyckx, and Walter Daelemans.
2011. Intrinsic plagiarism detection using character
trigram distance scores. In Proceedings of the PAN
2011 Lab: Uncovering Plagiarism, Authorship, and
Social Software Misuse.
Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011. Unsupervised decompo-
sition of a document into authorial components. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?11).
Thomas K. Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of the 44th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL ?06), pages
25?32.
C. W. F. McKenna and A. Antonia. 2001. The statistical
analysis of style: Reflections on form, meaning, and
ideology in the ?Nausicaa? episode of Ulysses. Liter-
ary and Linguistic Computing, 16(4):353?373.
34
Gabriel Oberreuter, Gaston L?Huillier, Sebastia?n A. R??os,
and Juan D. Vela?squez. 2011. Approaches for intrin-
sic and external plagiarism detection. In Proceedings
of the PAN 2011 Lab: Uncovering Plagiarism, Author-
ship, and Social Software Misuse.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36, March.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the ACL SIGDAT Workshop, pages 47?50.
Dean Keith Simonton. 1990. Lexical choices and aes-
thetic success: A computer content analysis of 154
Shakespeare sonnets. Computers and the Humanities,
24(4):251?264.
Efstathios Stamatatos. 2009. Intrinsic plagiarism detec-
tion using character n-gram profiles. In Proceedings
of the SEPLN?09 Workshop on Uncovering Plagia-
rism, Authorship and, Social Software Misuse (PAN-
09), pages 38?46. CEUR Workshop Proceedings, vol-
ume 502.
Benno Stein, Nedim Lipka, and Peter Prettenhofer. 2011.
Intrinsic plagiarism analysis. Language Resources
and Evaluation, 45(1):63?82.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilivie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manifred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Touch Press LLP. 2011. The Waste Land
app. http://itunes.apple.com/ca/app/the-waste-land/
id427434046?mt=8 .
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of the 39th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?01), pages
499?506.
35
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 1?8,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
A Tale of Two Cultures:
Bringing Literary Analysis and Computational Linguistics Together
Adam Hammond
Dept of English
University of Toronto
adam.hammond@utoronto.ca
Julian Brooke
Dept of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Dept of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
There are cultural barriers to collaborative effort
between literary scholars and computational lin-
guists. In this work, we discuss some of these
problems in the context of our ongoing research
project, an exploration of free indirect discourse
in Virginia Woolf?s To The Lighthouse, ulti-
mately arguing that the advantages of taking
each field out of its ?comfort zone? justifies the
inherent difficulties.
1 Introduction
Within the field of English literature, there is a grow-
ing interest in applying computational techniques, as
evidenced by the growth of the Digital Humanities
(Siemens et al, 2004). At the same time, a subfield
in Computational Linguistics that addresses a range
of problems in the genre of literature is gaining mo-
mentum (Mani, 2013). Nevertheless, there are sig-
nificant barriers to true collaborative work between
literary and computational researchers. In this pa-
per, we discuss this divide, starting from the classic
rift between the two cultures of the humanities and
the sciences (Snow, 1959) and then focusing in on a
single aspect, the attitude of the two fields towards
ambiguity. Next, we introduce our ongoing collab-
orative project which is an effort to bridge this gap;
in particular, our annotation of Virginia Woolf?s To
the Lighthouse for free indirect discourse, i.e. mix-
tures of objective narration and subjective speech,
requires a careful eye to literary detail, and, while
novel, interacts in interesting ways with established
areas of Computational Linguistics.
2 Background
2.1 The ?Two Cultures? Problem
Since the publication of C. P. Snow?s influential
The Two Cultures and the Scientific Revolution
(Snow, 1959), the phrase ?the two cultures? been
used to signify the rift?perceived and generally
lamented?between scientific and humanities intel-
lectual cultures. The problem, of course, is the igno-
rance of each culture with regard to the methods and
assumptions of the other, and the resulting impos-
sibility of genuine dialogue between them, prevent-
ing them from working together to solve important
problems. Many scholars describing the recent rise
of the Digital Humanities?the area of research and
teaching concerned with the intersection of comput-
ing and humanities disciplines?have argued that it
effects a reconciliation of the two alienated spheres,
bringing scientific methodology to bear on problems
within the humanities, many of which had previ-
ously been addressed in a less-than-rigorous manner
(Hockey, 2004).
From within the discipline of English literature,
however, the application of computational meth-
ods to literary analysis has frequently been?and
continues to be?a matter of considerable contro-
versy (Hoover, 2007; Flanders, 2009). This con-
troversy arises from the perception of many tradi-
tional humanists that computational analysis, which
aims to resolve dilemmas, seeking singular truth
and hard-and-fast answers, is incompatible with the
aims of humanistic research, which is often focused
on opening up questions for debate rather than re-
solving them decisively, and often premised on the
1
idea that there are no right answers, only well- and
poorly-supported arguments. Critics have responded
to these views by arguing that the best computational
literary analysis participates in this project of open-
ing up meaning, arguing that it is not a rejection of
literary reading but rather a method for carrying it
out more efficiently and extending it to more texts
(Ramsay, 2007), and that computational modelling,
even when unsuccessful, allows for the application
of the scientific method and thus carries the poten-
tial for intellectual advancement not possible with
purely anecdotal evidence (McCarty, 2005). Despite
such counter-arguments, however, the fear remains
widespread among traditional literary scholars that
the rise of computational analysis will entail the loss
of certain sacred assumptions of humanistic inquiry.
2.2 Ambiguity Across the ?Cultures?
We argue, though, that these fears are not without
basis, particularly when one considers the very dif-
ferent approaches to the question of ambiguity in
the two specific disciplines involved in our project:
English Literature and Computational Linguistics.
Here, the rift of the two cultures remains evident.
A major focus of literary scholarship since the
early twentieth century has been the semantic mul-
tiplicity of literary language. Such scholarship has
argued that literature, distinct from other forms of
discourse, may be deliberately ambiguous or poly-
semous and that literary analysis, distinct from other
analytic schools, should thus aim not to resolve am-
biguity but to describe and explore it. This was a
central insight of the early twentieth-century school,
the New Criticism, advanced in such works as
William Empson?s Seven Types of Ambiguity (Emp-
son, 1930) and Cleanth Brooks?s The Well Wrought
Urn (Brooks, 1947), which presented ambiguity and
paradox not as faults of style but as important po-
etic devices. New Criticism laid out a method of
literary analysis centred on the explication of the
complex tensions created by ambiguity and para-
dox, without any effort to resolve them. Also in
the first half of the twentieth century, but indepen-
dently, the Russian critic Mikhail Bakhtin developed
his theory of dialogism, which valorized ?double-
voiced? or polyphonic works that introduce multi-
ple, competing perspectives?particularly voices?
that present conflicting ideologies (Bakhtin, 1981).
Bakhtin, who wrote his seminal work ?Discourse in
the Novel? under a Stalinist sentence of exile, par-
ticularly valued works that enacted the free compe-
tition of ideologically opposed voices. In a simi-
lar spirit, but independently of Bakhtin, the German
critic Erich Auerbach described the ?multi-personal
representation of consciousness?, a narrative tech-
nique in which the writer, typically the narrator of
objective facts, is pushed entirely into the back-
ground and the story proceeds by reflecting the in-
dividual consciousnesses of the characters; Auer-
bach argued that this was a defining quality of mod-
ernist (early twentieth-century) literature (Auerbach,
1953). In the second half of the twentieth century,
this critical emphasis on ambiguity and paradox de-
veloped in an extreme form into the school of de-
constructive criticism, which held a theory of the
linguistic sign according to which determinate lin-
guistic meaning is considered logically impossible.
Deconstructive literary analysis proceeds by seeking
out internal contradictions in literary texts to support
its theory of infinitely ambiguous signification.
In Computational Linguistics, by contrast, ambi-
guity is almost uniformly treated as a problem to be
solved; the focus is on disambiguation, with the as-
sumption that one true, correct interpretation exists.
In the sphere of annotation, for instance, there is
an expectation that agreement between annotators,
as measured by statistics such as kappa (Di Euge-
nio and Glass, 2004), reach levels (generally 0.67 or
higher) where disagreements can be reasonably dis-
missed as noise; the implicit assumption here is that
subjectivity is something to be minimized. The chal-
lenge of dealing with subjectivity in CL has been
noted (Alm, 2011), and indeed there are rare exam-
ples in the field where multiple interpretations have
been considered during evaluations?for instance,
work in lexical cohesion (Morris and Hirst, 2005)
and in using annotator disagreements as an indicator
that two words are of similar orientation (Taboada
et al, 2011)?but they are the exception. Work in
CL focused on literary texts tends towards aspects
of the texts which readers would not find particu-
larly ambiguous, for example identifying major nar-
rative threads (Wallace, 2012) or distinguishing au-
thor gender (Luyckx et al, 2006).
2
3 A Collaborative Research Agenda
The obvious solution to the problem of the ?two
cultures??and one that has often been proposed
(Friedlander, 2009)?is interdisciplinary collabora-
tion. But while there are many computational lin-
guists working in literary topics such as genre,
and many literary scholars performing computa-
tional analysis of literature, genuine collaboration
between the disciplines remains quite rare. Over the
past two years, we have undertaken two collabora-
tive projects?one mostly complete, one ongoing?
which aim at such genuine collaboration, and in so
doing seek to bridge the real rift between scientific
and humanities cultures.1 Each of these projects
is multi-faceted, seeking (a) to produce meaningful
research within both disciplines of Computational
Linguistics and English Literature; (b) to provide
educational experience which broadens the disci-
plinary horizons of the undergraduate students in-
volved in the projects; and (c) to provide a model
of collaborative research that will spur further such
?culture-spanning? projects.
Each of our projects was launched in the context
of a course entitled ?The Digital Text? offered by the
Department of English at the University of Toronto.
The first author, whose background is in English Lit-
erature, is instructor of the course, while the sec-
ond author, a graduate student in Computer Science,
was assigned as a teaching assistant. Working to-
gether with the third author, we have designed these
projects collaboratively.
The first project, which we call ?He Do the
Police in Different Voices?,2 was carried out in
2011?12 (Hammond, 2013). Focused on a ?multi-
personal? poem, The Waste Land (1922) by T.S.
Eliot, it encompassed each of the three aspects of
our projects outlined above; in particular, it was mo-
tivated by a research question of interest to both dis-
ciplines: could we identify the points in The Waste
Land where the style changes, where one ?voice?
gives way to another? A computational approach
1In addition, the third author was part of a separate collabo-
rative project between our departments (Le et al, 2011), though
the aim of that project was not literary analysis.
2This is a reference to Eliot?s working title for The Waste
Land, which in itself is a reference to a talented storyteller in
Our Mutual Friend by Charles Dickens; another Dickens novel
is alluded to in the title of this paper.
promised to bring added rigor as well as a degree
of objectivity to this question, which humanities
methods had proven unable to resolve in almost a
century of debate. Both because poetry is dense
in signification, and because the multiple voices in
The Waste Land are a deliberate effect achieved by
a single author rather than a disguised piecing to-
gether of the works of multiple authors, the ques-
tion provided a meaningful challenge to the com-
putational approach, an unsupervised vector-space
model which first segments by identifying points
of stylistic change (Brooke et al, 2012) and then
clusters the resulting segments together into voices
(Brooke et al, 2013).
This research project was tightly integrated into
the curriculum of ?The Digital Text?. Students were
instructed in the use of the Text Encoding Initiative
(TEI) XML guidelines,3 and each of the students
provided one annotation related to voice as part of
a marked assignment. Students also participated in
an online poll in which they indicated every instance
in which they perceived a vocal switch in the poem,
and their responses were used in the construction of
a gold standard for the evaluation of our computa-
tional approach.
Once they were complete, we developed our re-
sults into a publicly accessible website.4 This web-
site promises to encourage collaboration between
literary scholars and computational linguists by ex-
plaining the project and our results in language ac-
cessible to both, and by producing a new digital edi-
tion of the poem based on our findings. Human and
computer readings of the poem are presented side-
by-side on the website, to demonstrate that each in-
terprets the poem in different ways, but that neither
of these methods is absolutely valid. Rather, we en-
courage website visitors to decide for themselves
where they believe that the vocal switches occur,
and we provide an interactive interface for divid-
ing the poem up according to their own interpreta-
tion. In addition to serving as a model of collabora-
tion between English Literature and Computational
Linguistics?and also serving as a teaching tool for
instructors of The Waste Land at any level?the site
is thus useful to us as a source of further data.
3http://www.tei-c.org/Guidelines/
4http://www.hedothepolice.org
3
4 The ?Brown Stocking? Project
4.1 Free Indirect Discourse in To the
Lighthouse
Our second, ongoing project, ?The Brown Stock-
ing?, focuses on a literary text deliberately chosen
for its deeply ambiguous, polysemous, dialogic na-
ture: Virginia Woolf?s (1927) To the Lighthouse
(TTL). Woolf?s novel was produced at the same time
that critical theories of ambiguity and polyvocality
were being developed, and indeed was taken as a
central example by many critics. Our project takes
its title from the final chapter of Erich Auerbach?s
Mimesis, in which Auerbach presents TTL as the
representative text of modernist literature?s ?mul-
tipersonal representation of consciousness? (Auer-
bach, 1953). For Auerbach, there are two prin-
cipal distinguishing features in Woolf?s narrative
style. The first is the tendency, already noted, to ?re-
flect? incidents through the subjective perspectives
of characters rather than presenting them from the
objective viewpoint of the author; thus TTL becomes
a work in which there is more than one order and in-
terpretation. Woolf?s technique not only introduces
multiple interpretations, however, but also blurs the
transitions between individual perspectives, making
it difficult to know in many instances who is speak-
ing or thinking.
Woolf achieves this double effect?multiple sub-
jective impressions combined with obscuring of the
lines separating them from the narrator and from one
another?chiefly through the narrative technique of
free indirect discourse (also known as free indirect
style). Whereas direct discourse reports the actual
words or thoughts of a character, and indirect dis-
course summarizes the thoughts or words of a char-
acter in the words of the entity reporting them, free
indirect discourse (FID) is a mixture of narrative and
direct discourse (Abrams, 1999). As in indirect dis-
course, the narrator employs third-person pronouns,
but unlike indirect discourse, the narrator includes
words and expressions that indicate subjective or
personalized aspects clearly distinct from the narra-
tor?s style. For example, in the opening sentences of
TTL:
?Yes, of course, if it?s fine tomorrow,? said Mrs.
Ramsay. ?But you?ll have to be up with the
lark,? she added. To her son these words con-
veyed an extraordinary joy, as if it were settled,
the expedition were bound to take place, and
the wonder to which he had looked forward, for
years and years it seemed, was, after a night?s
darkness and a day?s sail, within touch.
we are presented with two spans of objective nar-
ration (said Mrs. Ramsay and she added) and two
passages of direct discourse, in which the narrator
introduces the actual words of Mrs. Ramsay (?Yes,
of course, if it?s fine tomorrow? and ?But you?ll have
to be up with the lark?). The rest of the passage is
presented in FID, mixing together the voices of the
narrator, Mrs. Ramsay, and her son James: while the
use of third-person pronouns and the past tense and
clearly indicates the voice of the narrator, phrases
such as for years and years it seemed clearly present
a subjective perspective.
In FID?s mixing of voices, an element of uncer-
tainty is inevitably present. While we can be con-
fident of the identity of the voice speaking certain
words, it remains unclear whether other words be-
long to the narrator or a character; in this case, it
is not clear whether for years and years it seemed
presents James?s actual thoughts, Mrs. Ramsay?s
summary of her son?s thoughts, the narrator?s sum-
mary of James?s thoughts, the narrator?s summary
of Mrs. Ramsay?s summary of James?s thoughts, etc.
Abrams (1999) emphasizes uncertainty as a defining
trait of FID: the term ?refers to the way, in many nar-
ratives, that the reports of what a character says and
thinks shift in pronouns, adverbs, and grammatical
mode, as we move?or sometimes hover?between
the direct narrated reproductions of these events as
they occur to the character and the indirect repre-
sentation of such events by the narrator?. FID, with
its uncertain ?hovering?, is used throughout TTL;
it is the principal technical means by which Woolf
produces ambiguity, dialogism, and polysemy in the
text. It is thus the central focus of our project.
In Literary Studies, Toolan (2008) was perhaps
the first to discuss the possibility of automatic recog-
nition of FID, but his work was limited to a very
small, very informal experiment using a few a pri-
ori features, with no implementation or quantita-
tive analysis of the results. Though we are not
aware of work in Computational Linguistics that
deals with this kind of subjectivity in literature?
FID is included in the narrative annotation schema
4
of Mani (2013), but it is not given any particular
attention within that framework?there are obvious
connections with sentence-level subjectivity analy-
sis (Wilson et al, 2005) and various other stylis-
tic tasks, including authorship profiling (Argamon
et al, 2007). Since the subjective nature of these
passages is often expressed through specific lexical
choice, it would be interesting to see if sentiment
dictionaries (Taboada et al, 2011) or other stylistic
lexical resources such as dictionaries of lexical for-
mality (Brooke et al, 2010) could be useful.
4.2 Our Approach
Our project is proceeding in four stages: an initial
round of student annotation, a second round of stu-
dent annotation, computational analysis of these an-
notations, and the development of a project website.
In the first stage, we had 160 students mark up a pas-
sage of between 100?150 words in accordance with
TEI guidelines. Students were instructed to use the
TEI said element to enclose any instance of char-
acter speech, to identify the character whose speech
is being introduced, and to classify each of these in-
stances as either direct, indirect, or free indirect dis-
course and as either spoken aloud or thought silently.
Because there are often several valid ways of inter-
preting a given passage, and because we are inter-
ested in how different students respond to the same
passage, each 100?150 word span was assigned to
three or four students. This first round of annotation
focused only on the first four chapters of TTL. Raw
average agreement of the various annotations at the
level of the word was slightly less than 70%,5 and
though we hope to do better in our second round,
levels of agreement typically required are likely to
be beyond our reach due to the nature of the task.
For example, all four sudents responsible for the
passage cited above agreed on the tagging of the first
two sentences; however, two students read the third
sentence as FID mixing the voices of the narrator
and Mrs. Ramsay, and two read it as FID mixing
the voice of the narrator and James. Though they
disagree, these are both valid interpretations of the
5Since each passage was tagged by a different set of stu-
dents, we cannot apply traditional kappa measures. Raw agree-
ment overestimates success, since unlike kappa it does not
discount random agreement, which in this case varies widely
across the different kinds of annotation.
passage.
In the second round of annotation, with 160 dif-
ferent student annotators assigned slightly longer
spans of 200?300 words, we are focusing on the
final seven chapters of TTL. We have made sev-
eral minor changes to our annotation guidelines, and
two significant changes. First, we now ask that in
every span of text which students identify as FID,
they explicitly identify the words that they regard
as clearly coming from the subjective perspective
of the character. We believe this will help students
make a valid, defensible annotation, and it may also
help with the computational analysis to follow. Sec-
ond, we are also allowing embedded tags, for in-
stances of direct or indirect discourse within spans
of FID, which were confusing to students in the ini-
tial round. For instance, students would now be able
to tag the above-cited passage of as a span of FID
mixing the narrator?s and Mrs. Ramsay?s words, in-
side of which Mrs. Ramsay introduces an indirect-
discourse rendering of her son?s thoughts. Moving
from a flat to a recursive representation will natu-
rally result in additional complexity, but we believe
it is necessary to capture what is happening in the
text.
Once this second round of tagging is complete, we
will begin our computational analysis. The aim is to
see whether we can use supervised machine learn-
ing to replicate the way that second-year students
enrolled in a rigorous English literature program re-
spond to a highly complex text such as TTL. We
are interested to see whether the subjective, messy
data of the students can be used to train a useful
model, even if it is inadequate as a gold standard.
If successful, this algorithm could be deployed on
the remaining, untagged sections of TTL (i.e. ev-
erything between the first four and last seven chap-
ters) and produce meaningful readings of the text.
It would proceed by (a) identifying passages of FID
(that is, passages in which it is unclear whether a
particular word belongs to the narrator or a char-
acter); (b) making an interpretation of that passage
(hypothesizing as to which particular voices are be-
ing mixed); and (c) judging the likely validity of
this interpretation. It would seek not only to identify
spans of vocal ambiguity, but also to describe them,
as far as possible. It would thus not aim strictly
at disambiguation?at producing a right-or-wrong
5
reading of the text?but rather at producing the best
possible interpretation. The readings thus generated
could then be reviewed by an independent expert as
a form of evaluation.
Finally, we will develop an interactive website for
the project. It will describe the background and aims
of the project, present the results from the first three
stages of the project, and also include an interface
allowing visitors to the site to annotate the text for
the same features as the students (via a Javascript in-
terface, i.e. without having to manipulate the XML
markup directly). This will provide further annota-
tion data for our project, as well as giving instruc-
tors in English Literature and Digital Humanities a
resource to use in their teaching.
5 Discussion
We believe our approach has numerous benefits on
both sides of the divide. From a research perspec-
tive, the inter-disciplinary approach forces partici-
pants from both English Literature and Computa-
tional Linguistics to reconsider some of their funda-
mental disciplinary assumptions. The project takes
humanities literary scholarship out of its ?comfort
zone? by introducing alien and unfamiliar method-
ologies such as machine learning, as well as by its
basic premise that FID?by definition, a moment
of uncertainty where the question of who is speak-
ing is unresolved?can be detected automatically.
Even though many of these problems can be linked
with classic Computational Linguistics research ar-
eas, the project likewise takes Computational Lin-
guistics out of its comfort zone by seeking not to
resolve ambiguity but rather to identify it and, as far
as possible, describe it. It presents an opportunity
for a computational approach to take into account a
primary insight of twentieth-century literary schol-
arship: that ambiguity and subjectivity are often de-
sirable, intentional qualities of literary language, not
problems to be solved. It promises literary scholar-
ship a method for extending time-consuming, labo-
rious human literary readings very rapidly to a vast
number of literary texts, the possible applications of
which are unclear at this early stage, but are surely
great.
While many current major projects in computer-
assisted literary analysis operate on a ?big-data?
model, drawing conclusions from analysis of vast
numbers of lightly annotated texts, we see advan-
tages in our own method of beginning with a few
heavily-annotated texts and working outward. Tra-
ditional literary scholars often object that ?big-data?
readings take little or no account of subjective, hu-
man responses to literary texts; likewise, they find
the broad conclusions of such projects (that the nine-
teenth century novel moves from telling to show-
ing (Heuser and Le-Khac, 2012); that Austen is
more influential than Dickens (Jockers, 2012)) dif-
ficult to test (or reconcile) with traditional literary
scholarship. The specific method we are pursuing?
taking a great number of individual human read-
ings of a complex literary text and using them as
the basis for developing a general understanding of
how FID works?promises to move literary analysis
beyond merely ?subjective? readings without, how-
ever, denying the basis of all literary reading in indi-
vidual, subjective responses. Our method indeed ap-
proaches the condition of a multi-voiced modernist
literary work like TTL, in which, as Erich Auerbach
perceived, ?overlapping, complementing, and con-
tradiction yield something that we might call a syn-
thesized cosmic view?. We too are building our syn-
thetic understanding out of the diverse, often contra-
dictory, responses of individual human readers.
Developing this project in an educational
context?basing our project on readings developed
by students as part of marked assignments for
?The Digital Text??is likewise beneficial to both
cultures. It forces humanities undergraduates
out of their comfort zone by asking them to turn
their individual close readings of the text into an
explicit, machine-readable representation (in this
case, XML). Recognizing the importance of a
sharable language for expressing literary features
in machine-readable way, we have employed the
standard TEI guidelines mark-up with as few
customizations as possible, rather than developing
our own annotation language from the ground up.
The assignment asks students, however, to reflect
critically on whether such explicit languages can
ever adequately capture the polyvalent structures
of meaning in literary texts; that is, whether there
will always necessarily be possibilities that can?t
be captured in the tag set, and whether, as such, an
algorithmic process can ever really ?read? literature
6
in a useful way. At the same time, this method
has potentially great benefits to the development
of such algorithmic readings, precisely by making
available machine-readable approximations of how
readers belonging to another ?culture??humanities
undergraduates?respond to a challenging literary
text. Such annotations would not be possible from
a pool of annotators trained in the sciences, but
could only come from students of the humanities
with a basic understanding of XML. We do not
believe, for example, workers on Amazon Mechan-
ical Turk could reliably be used for this purpose,
though it might be interesting to compare our
?studentsourcing? with traditional crowdsourcing
techniques.
Our approach also faces several important chal-
lenges. Certainly the largest is whether an algo-
rithmic criticism can be developed that could come
to terms with ambiguity. The discipline of literary
studies has long taught its students to accept what
the poet John Keats called ?negative capability, that
is, when a man is capable of being in uncertainties,
mysteries, doubts, without any irritable searching af-
ter fact and reason? (Keats, 2002). Computational
analysis may simply be too fundamentally premised
on ?irritable searching after fact and reason? to be
capable of ?existing in uncertainty? in the manner of
many human literary readers. Even if we are able to
develop a successful algorithmic method of detect-
ing FID in Woolf, this method may not prove appli-
cable to other literary texts, which may employ the
device in highly individual manners; TTL may prove
simply too complex?and employ too much FID?
to serve as a representative sample text. At a more
practical level, even trained literature students do not
produce perfect annotations: they make errors both
in XML syntax and in their literary interpretation of
TTL, a text that proves elusive even for some spe-
cialists. Since we do not want our algorithm to base
its readings on invalid student readings (for instance,
readings that attribute speech to a character clearly
not involved in the scene), we face the challenge of
weeding out bad student readings?and we will face
the same challenge once readings begin to be sub-
mitted by visitors to the website. These diverse read-
ings do, however, also present an interesting possi-
bility, which we did not originally foresee: the de-
velopment of a reader-response ?map? showing how
human readers actually interpret (and in many cases
misinterpret) complex modernist texts like TTL.
6 Conclusion
Despite the philosophical and technical chal-
lenges that face researchers in this growing multi-
disciplinary area, we are increasingly optimistic that
collaboration between computational and literary re-
searchers is not only possible, but highly desirable.
Interesting phenomena such as FID, this surprising
melding of objective and personal perspective that
is the subject of the current project, requires experts
in both fields working together to identify, annotate,
and ultimately model. Though fully resolving the
rift between our two cultures is not, perhaps, a feasi-
ble goal, we argue that even this early and tentative
collaboration has demonstrated the potential benefits
on both sides.
Acknowledgements
This work was financially supported by the So-
cial Sciences and Humanities Research Council of
Canada and the Natural Sciences and Engineering
Research Council of Canada.
References
M. H. Abrams. 1999. A Glossary of Literary Terms.
Harcourt Brace, Toronto, 7th edition.
Cecilia Ovesdotter Alm. 2011. Subjective natural lan-
guage problems: Motivations, applications, charac-
terizations, and implications. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 107?112.
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional lex-
ical features. Journal of the American Society for In-
formation Science and Technology, 7:91?109.
Erich Auerbach. 1953. Mimesis: The Representation
of Reality in Western Literature. Princeton University
Press, Princeton, NJ.
Mikhail Mikhailovich Bakhtin. 1981. Discourse in
the novel. In Michael Holquist, editor, The Dialogic
Imagination: Four Essays, pages 259?422. Austin:
Univeristy of Texas Press.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
7
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10), Beijing.
Julian Brooke, Adam Hammond, and Graeme Hirst.
2012. Unsupervised stylistic segmentation of poetry
with change curves and extrinsic features. In Proceed-
ings of the 1st Workshop on Computational Literature
for Literature (CLFL ?12), Montreal.
Julian Brooke, Graeme Hirst, and Adam Hammond.
2013. Clustering voices in the Waste Land. In Pro-
ceedings of the 2nd Workshop on Computational Lit-
erature for Literature (CLFL ?13), Atlanta.
Cleanth Brooks. 1947. The Well Wrought Urn. Harcourt
Brace, New York.
Barbara Di Eugenio and Michael Glass. 2004. The kappa
statistic: a second look. Computational Linguistics,
30(1):95?101, March.
T.S. Eliot. 1971. The Waste Land. In The Complete
Poems and Plays, 1909?1950, pages 37?55. Harcourt
Brace Jovanovich, New York.
William Empson. 1930. Seven Types of Ambiguity.
Chatto and Windus, London.
Julia Flanders. 2009. Data and wisdom: Electronic edit-
ing and the quantification of knowledge. Literary and
Linguistic Computing, 24(1):53?62.
Amy Friedlander. 2009. Asking questions and build-
ing a research agenda for digital scholarship. Work-
ing Together or Apart: Promoting the Next Generation
of Digital Scholarship. Report of a Workshop Cospon-
sored by the Council on Library and Information Re-
sources and The National Endowment for the Human-
ities, March.
Adam Hammond. 2013. He do the police in different
voices: Looking for voices in The Waste Land. Sem-
inar: ?Mapping the Fictional Voice? American Com-
parative Literature Association (ACLA).
Ryan Heuser and Long Le-Khac. 2012. A quantita-
tive literary history of 2,958 nineteenth-century British
novels: The semantic cohort method. Stanford Lit-
erary Lab Pamphlet No. 4. http://litlab.stanford.edu/
LiteraryLabPamphlet4.pdf .
Susan Hockey. 2004. The history of humanities com-
puting. In Ray Siemens, Susan Schreibman, and John
Unsworth, editors, A Companion to Digital Humani-
ties. Blackwell, Oxford.
David L. Hoover. 2007. Quantitative analysis and lit-
erary studies. In Ray Siemens and Susan Schreib-
man, editors, A Companion to Digital Literary Studies.
Blackwell, Oxford.
Matthew L. Jockers. 2012. Computing and visualiz-
ing the 19th-century literary genome. Presented at the
Digital Humanities Conference. Hamburg.
John Keats. 2002. Selected Letters. Oxford University
Press, Oxford.
Xuan Le, Ian Lancashire, Graeme Hirst, and Regina
Jokel. 2011. Longitudinal detection of dementia
through lexical and syntactic changes in writing: A
case study of three British novelists. Literary and Lin-
guistic Computing, 26(4):435?461.
Kim Luyckx, Walter Daelemans, and Edward Vanhoutte.
2006. Stylogenetics: Clustering-based stylistic analy-
sis of literary corpora. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC ?06), Genoa, Italy.
Inderjeet Mani. 2013. Computational Modeling of Nar-
rative. Morgan & Claypool.
Willard McCarty. 2005. Humanities Computing. Pal-
grave Macmillan, New York.
Jane Morris and Graeme Hirst. 2005. The subjectivity
of lexical cohesion in text. In James G. Shanahan, Yan
Qu, and Janyce M. Wiebe, editors, Computing Attitude
and Affect in Text. Springer, Dordrecht, The Nether-
lands.
Stephen Ramsay. 2007. Algorithmic criticism. In Ray
Siemens and Susan Schreibman, editors, A Companion
to Digital Literary Studies. Blackwell, Oxford.
Ray Siemens, Susan Schreibman, and John Unsworth,
editors. 2004. A Companion to Digital Humanities.
Blackwell, Oxford.
C. P. Snow. 1959. The Two Cultures and the Scientific
Revolution. Cambridge University Press, New York.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Michael Toolan. 2008. Narrative progression in the short
story: First steps in a corpus stylistic approach. Nar-
rative, 16(2):105?120.
Byron Wallace. 2012. Multiple narrative disentangle-
ment: Unraveling Infinite Jest. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1?10, Montre?al,
Canada, June. Association for Computational Linguis-
tics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT/EMNLP
?05, pages 347?354.
Virginia Woolf. 1927. To the Lighthouse. Hogarth, Lon-
don.
8
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 41?46,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
Clustering voices in The Waste Land
Julian Brooke
Dept of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Dept of Computer Science
University of Toronto
gh@cs.toronto.edu
Adam Hammond
Dept of English
University of Toronto
adam.hammond@utoronto.ca
Abstract
T.S. Eliot?s modernist poem The Waste Land is
often interpreted as collection of voices which
appear multiple times throughout the text. Here,
we investigate whether we can automatically
cluster existing segmentations of the text into
coherent, expert-identified characters. We show
that clustering The Waste Land is a fairly dif-
ficult task, though we can do much better than
random baselines, particularly if we begin with
a good initial segmentation.
1 Introduction
Although literary texts are typically written by a sin-
gle author, the style of a work of literature is not nec-
essarily uniform. When a certain character speaks,
for instance, an author may shift styles to give the
character a distinct voice. Typically, voice switches
in literature are explicitly marked, either by the use
of quotation marks with or without a said quota-
tive, or, in cases of narrator switches, by a major
textual boundary (e.g. the novel Ulysses by James
Joyce). However, implicit marking is the norm in
some modernist literature: a well-known example is
the poem The Waste Land by T.S. Eliot, which is
usually analyzed in terms of voices that each appear
multiple times throughout the text. Our interest is
distinguishing these voices automatically.
One of the poem?s most distinctive voices is that
of the woman who speaks at the end of its second
section:
I can?t help it, she said, pulling a long face,
It?s them pills I took, to bring it off, she said
[158?159]
Her chatty tone and colloquial grammar and lexis
distinguish her voice from many others in the poem,
such as the formal and traditionally poetic voice of a
narrator that recurs many times in the poem:
Above the antique mantel was displayed
As though a window gave upon the sylvan scene
The change of Philomel
[97?99]
Although the stylistic contrasts between these and
other voices are clear to many readers, Eliot does
not explicitly mark the transitions, nor is it obvi-
ous when a voice has reappeared. Our previous
work focused on only the segmentation part of the
voice identification task (Brooke et al, 2012). Here,
we instead assume an initial segmentation and then
try to create clusters corresponding to segments of
the The Waste Land which are spoken by the same
voice. Of particular interest is the influence of the
initial segmentation on the success of this down-
stream task.
2 Related Work
There is a small body of work applying quantita-
tive methods to poetry: Simonton (1990) looked
at lexical and semantic diversity in Shakespearean
sonnets and correlated this with aesthetic success,
whereas Dugan (1973) developed statistics of for-
mulaic style and applied them to the Chanson de
Roland to determine whether it represents an oral
or written style. Kao and Jurafsky (2012) quantify
various aspects of poety, including style and senti-
ment, and use these features to distinguish profes-
sional and amateur writers of contemporary poetry.
41
With respect to novels, the work of McKenna and
Antonia (2001) is very relevant; they used principal
components analysis of lexical frequency to discrim-
inate different voices and narrative styles in sections
of Ulysses by James Joyce.
Clustering techniques have been applied to liter-
ature in general; for instance, Luyckx (2006) clus-
tered novels according to style, and recent work in
distinguishing two authors of sections of the Bible
(Koppel et al, 2011) relies crucially on an initial
clustering which is bootstrapped into a supervised
classifier which is applied to segments. Beyond lit-
erature, the tasks of stylistic inconsistency detec-
tion (Graham et al, 2005; Guthrie, 2008) and intrin-
sic (unsupervised) plagiarism detection (Stein et al,
2011) are very closely related to our interests here,
though in such tasks usually only two authors are
posited; more general kinds of authorship identifi-
cation (Stamatatos, 2009) may include many more
authors, though some form of supervision (i.e. train-
ing data) is usually assumed.
Our work here is built on our earlier work (Brooke
et al, 2012). Our segmentation model for The Waste
Land was based on a stylistic change curve whose
values are the distance between stylistic feature vec-
tors derived from 50 token spans on either side of
each point (spaces between tokens) in the text; the
local maxima of this curve represent likely voice
switches. Performance on The Waste Land was far
from perfect, but evaluation using standard text seg-
mentation metrics (Pevzner and Hearst, 2002) indi-
cated that it was well above various baselines.
3 Method
Our approach to voice identification in The Waste
Land consists first of identifying the boundaries of
voice spans (Brooke et al, 2012). Given a segmenta-
tion of the text, we consider each span as a data point
in a clustering problem. The elements of the vector
correspond to the best feature set from the segmen-
tation task, with the rationale that features which
were useful for detecting changes in style should
also be useful for identifying stylistic similarities.
Our features therefore include: a collection of read-
ability metrics (including word length), frequency
of punctuation, line breaks, and various parts-of-
speech, lexical density, average frequency in a large
external corpus (Brants and Franz, 2006), lexicon-
based sentiment metrics using SentiWordNet (Bac-
cianella et al, 2010), formality score (Brooke et al,
2010), and, perhaps most notably, the centroid of 20-
dimensional distributional vectors built using latent
semantic analysis (Landauer and Dumais, 1997), re-
flecting the use of words in a large web corpus (Bur-
ton et al, 2009); in previous work (Brooke et al,
2010), we established that such vectors contain use-
ful stylistic information about the English lexicon
(including rare words that appear only occasionally
in such a corpus), and indeed LSA vectors were the
single most promising feature type for segmentation.
For a more detailed discussion of the feature set, see
Brooke et al (2012). All the features are normalized
to a mean of zero and a standard deviation of 1.
For clustering, we use a slightly modified ver-
sion of the popular k-means algorithm (MacQueen,
1967). Briefly, k-means assigns points to a cluster
based on their proximity to the k cluster centroids,
which are initialized to randomly chosen points from
the data and then iteratively refined until conver-
gence, which in our case was defined as a change of
less than 0.0001 in the position of each centroid dur-
ing one iteration.1 Our version of k-means is distinct
in two ways: first, it uses a weighted centroid where
the influence of each point is based on the token
length of the underlying span, i.e. short (unreliable)
spans which fall into the range of some centroid will
have less effect on the location of the centroid than
larger spans. Second, we use a city-block (L1) dis-
tance function rather than standard Euclidean (L2)
distance function; in the segmentation task, Brooke
et al found that city-block (L1) distance was pre-
ferred, a result which is in line with other work
in stylistic inconsistency detection (Guthrie, 2008).
Though it would be interesting to see if a good k
could be estimated independently, for our purposes
here we set k to be the known number of speakers in
our gold standard.
4 Evaluation
We evaluate our clusters by comparing them to a
gold standard annotation. There are various met-
rics for extrinsic cluster evaluation; Amigo? et al
1Occasionally, there was no convergence, at which point we
halted the process arbitrarily after 100 iterations.
42
(2009) review various options and select the BCubed
precision and recall metrics (Bagga and Baldwin,
1998) as having all of a set of key desirable prop-
erties. BCubed precision is a calculation of the frac-
tion of item pairs in the same cluster which are also
in the same category, whereas BCubed recall is the
fraction of item pairs in the same category which
are also in the same cluster. The harmonic mean
of these two metrics is BCubed F-score. Typically,
the ?items? are exactly what has been clustered, but
this is problematic in our case, because we wish to
compare methods which have different segmenta-
tions and thus the vectors that are being clustered
are not directly comparable. Instead, we calculate
the BCubed measures at the level of the token; that
is, for the purposes of measuring performance we
act as if we had clustered each token individually,
instead of the spans of tokens actually used.
Our first evaluation is against a set of 20
artificially-generated ?poems? which are actually
randomly generated combinations of parts of 12 po-
ems which were chosen (by an English literature ex-
pert, one of the authors) to represent the time period
and influences of The Waste Land. The longest of
these poems is 1291 tokens and the shortest is just
90 tokens (though 10 of the 12 have at least 300 to-
kens); the average length is 501 tokens. Our method
for creating these poems is similar to that of Kop-
pel et al (2011), though generalized for multiple
authors. For each of the artificial poems, we ran-
domly selected 6 poems from the 12 source poems,
and then we concatenated 100-200 tokens (or all the
remaining tokens, if less than the number selected)
from each of these 6 poems to the new combined
poem until all the poems were exhausted or below
our minimum span length (20 tokens). This allows
us to evaluate our method in ideal circumstances, i.e.
when there are very distinct voices corresponding to
different poets, and the voice spans tend to be fairly
long.
Our gold standard annotation of The Waste Land
speakers is far more tentative. It is based on a
number of sources: our own English literature ex-
pert, relevant literary analysis (Cooper, 1987), and
also The Waste Land app (Touch Press LLP, 2011),
which includes readings of the poem by various ex-
perts, including T.S. Eliot himself. However, there
is inherently a great deal of subjectivity involved in
literary annotation and, indeed, one of the potential
benefits of our work is to find independent justifi-
cation for a particular voice annotation. Our gold
standard thus represents just one potential interpre-
tation of the poem, rather than a true, unique gold
standard. The average size of the 69 segments in
the gold standard is 50 tokens; the range, however,
is fairly wide: the longest is 373 tokens, while the
shortest consists of a single token. Our annotation
has 13 voices altogether.
We consider three segmentations: the segmen-
tation of our gold standard (Gold), the segmenta-
tion predicted by our segmentation model (Auto-
matic), and a segmentation which consists of equal-
length spans (Even), with the same number of spans
as in the gold standard. The Even segmentation
should be viewed as the baseline for segmentation,
and the Gold segmentation an ?oracle? represent-
ing an upper bound on segmentation performance.
For the automatic segmentation model, we use the
settings from Brooke et al (2012). We also com-
pare three possible clusterings for each segmenta-
tion: no clustering at all (Initial), that is, we assume
that each segment is a new voice; k-means clustering
(k-means), as outlined above; and random clustering
(Random), in which we randomly assign each voice
to a cluster. For these latter two methods, which both
have a random component, we averaged our metrics
over 50 runs. Random and Initial are here, of course,
to provide baselines for judging the effectiveness of
k-means clustering model. Finally, when using the
gold standard segmentation and k-means clustering,
we included another oracle option (Seeded): instead
of the standard k-means method of randomly choos-
ing them from the available datapoints, each cen-
troid is initialized to the longest instance of a dif-
ferent voice, essentially seeding each cluster.
5 Results
Table 1 contains the results for our first evaluation
of voice clustering, the automatically-generated po-
ems. In all the conditions, using the gold segmen-
tation far outstrips the other two options. The au-
tomatic segmentation is consistently better than the
evenly-spaced baseline, but the performance is actu-
ally worse than expected; the segmentation metrics
we used in our earlier work
43
Table 1: Clustering results for artificial poems
Configuration BCubed metrics
Prec. Rec. F-score
Initial Even 0.703 0.154 0.249
Initial Automatic 0.827 0.177 0.286
Initial Gold 1.000 0.319 0.465
Random Even 0.331 0.293 0.307
Random Automatic 0.352 0.311 0.327
Random Gold 0.436 0.430 0.436
k-means Even 0.462 0.409 0.430
k-means Automatic 0.532 0.479 0.499
k-means Gold 0.716 0.720 0.710
k-means Gold Seeded 0.869 0.848 0.855
Table 2: Clustering results for The Waste Land
Configuration BCubed metrics
Prec. Rec. F-score
Initial Even 0.792 0.069 0.128
Initial Automatic 0.798 0.084 0.152
Initial Gold 1.000 0.262 0.415
Random Even 0.243 0.146 0.183
Random Automatic 0.258 0.160 0.198
Random Gold 0.408 0.313 0.352
k-means Even 0.288 0.238 0.260
k-means Automatic 0.316 0.264 0.296
k-means Gold 0.430 0.502 0.461
k-means Gold Seeded 0.491 0.624 0.550
The results for The Waste Land are in Table 2.
Many of the basic patterns are the same, including
the consistent ranking of the methods; overall, how-
ever, the clustering is far less effective. This is par-
ticularly true for the gold-standard condition, which
only increases modestly between the initial and clus-
tered state; the marked increase in recall is balanced
by a major loss of precision. In fact, unlike with
the artificial text, the most promising aspect of the
clustering seems to be the fairly sizable boost to the
quality of clusters in automatic segmenting perfor-
mance. The effect of seeding is also very consistent,
nearly as effective as in the automatic case.
We also looked at the results for individual speak-
ers in The Waste Land; many of the speakers (some
of which appear only in a few lines) are very poorly
distinguished, even with the gold-standard segmen-
tation and seeding, but there are a few that cluster
quite well; the best two are in fact our examples from
Section 1,2 that is, the narrator (F-score 0.869), and
the chatty woman (F-score 0.605). The former re-
sult is particularly important, from the perspective
of literary analysis, since there are several passages
which seem to be the main narrator (and our ex-
pert annotated them as such) but which are definitely
open to interpretation.
6 Conclusion
Literature, by its very nature, involves combin-
ing existing means of expression in surprising new
ways, resisting supervised analysis methods that de-
pend on assumptions of conformity. Our unsuper-
vised approach to distinguishing voices in poetry of-
fers this necessary flexibility, and indeed seems to
work reasonably well in cases when the stylistic dif-
ferences are clear. The Waste Land, however, is a
very subtle text, and our results suggest that we are
a long way from something that would be a consid-
ered a possible human interpretation. Nevertheless,
applying quantitative methods to these kinds of texts
can, for literary scholars, bridge the gab between
abstract interpretations and the details of form and
function (McKenna and Antonia, 2001). In our own
case, this computational work is just one aspect of
a larger project in literary analysis where the ulti-
mate goal is not to mimic human behavior per se,
but rather to better understand literary phenomena
by annotation and modelling of these phenomena
(Hammond, 2013; Hammond et al, 2013).
With respect to future enhancements, improving
segmentation is obviously important; the best au-
tomated efforts so far provide only a small boost
over a baseline approach to segmentation. However,
independently of this, our experiments with gold-
standard seeding suggest that refining our approach
to clustering, e.g. a method that identifies good ini-
tial points for our centroids, may also pay dividends
in the long run. A more radical idea for future work
would be to remove the somewhat artificial delim-
2These passages are the original examples from our earlier
work (Brooke et al, 2012), selected by our expert for their dis-
tinctness, so the fact that they turned out to be the most easily
clustered is actually a result of sorts (albeit an anecdotal one),
suggesting that our clustering behavior does correspond some-
what to a human judgment of distinctness.
44
itation of the task into segmentation and clustering
phases, building a model which works iteratively
to produce segments that are sensitive to points of
stylistic change but that, at a higher level, also form
good clusters (as measured by intrinsic measures of
cluster quality).
Acknowledgements
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
References
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12:461?486, August.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th Conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta, May.
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics (ACL-COLING ?98), pages 79?85, Montreal,
Quebec, Canada.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus Version 1.1. Google Inc.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10), Beijing.
Julian Brooke, Adam Hammond, and Graeme Hirst.
2012. Unsupervised stylistic segmentation of poetry
with change curves and extrinsic features. In Proceed-
ings of the 1st Workshop on Computational Literature
for Literature (CLFL ?12), Montreal.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
John Xiros Cooper. 1987. T.S. Eliot and the politics of
voice: The argument of The Waste Land. UMI Re-
search Press, Ann Arbor, Mich.
Joseph J. Duggan. 1973. The Song of Roland: Formulaic
style and poetic craft. University of California Press.
Neil Graham, Graeme Hirst, and Bhaskara Marthi. 2005.
Segmenting documents by stylistic character. Natural
Language Engineering, 11(4):397?415.
David Guthrie. 2008. Unsupervised Detection of
Anomalous Text. Ph.D. thesis, University of Sheffield.
Adam Hammond, Julian Brooke, and Graeme Hirst.
2013. A tale of two cultures: Bringing literary analy-
sis and computational linguistics together. In Proceed-
ings of the 2nd Workshop on Computational Literature
for Literature (CLFL ?13), Atlanta.
Adam Hammond. 2013. He do the police in different
voices: Looking for voices in The Waste Land. Sem-
inar: ?Mapping the Fictional Voice? American Com-
parative Literature Association (ACLA).
Justine Kao and Dan Jurafsky. 2012. A computational
analysis of style, sentiment, and imagery in contem-
porary poetry. In Proceedings of the 1st Workshop on
Computational Literature for Literature (CLFL ?12),
Montreal.
Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011. Unsupervised decompo-
sition of a document into authorial components. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?11), Port-
land, Oregon.
Thomas K. Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Kim Luyckx, Walter Daelemans, and Edward Vanhoutte.
2006. Stylogenetics: Clustering-based stylistic analy-
sis of literary corpora. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC ?06), Genoa, Italy.
J. B. MacQueen. 1967. Some methods for classification
and analysis of multivariate observations. In Proceed-
ings of the Fifth Berkeley Symposium on Mathematical
Statistics and Probability, pages 281?297.
C. W. F. McKenna and A. Antonia. 2001. The statistical
analysis of style: Reflections on form, meaning, and
ideology in the ?Nausicaa? episode of Ulysses. Liter-
ary and Linguistic Computing, 16(4):353?373.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36, March.
Dean Keith Simonton. 1990. Lexical choices and aes-
thetic success: A computer content analysis of 154
Shakespeare sonnets. Computers and the Humanities,
24(4):251?264.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538?556.
45
Benno Stein, Nedim Lipka, and Peter Prettenhofer. 2011.
Intrinsic plagiarism analysis. Language Resources
and Evaluation, 45(1):63?82.
Touch Press LLP. 2011. The Waste Land
app. http://itunes.apple.com/ca/app/the-waste-land/
id427434046?mt=8 .
46
