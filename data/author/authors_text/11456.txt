Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 326?332,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
A Comparison between Dialog Corpora Acquired
with Real and Simulated Users
David Griol
Departamento de Informa?tica
Universidad Carlos III de Madrid
dgriol@inf.uc3m.es
Zoraida Callejas, Ramo?n Lo?pez-Co?zar
Dpto. Lenguajes y Sistemas Informa?ticos
Universidad de Granada
{zoraida, rlopezc}@ugr.es
Abstract
In this paper, we test the applicability
of a stochastic user simulation technique
to generate dialogs which are similar to
real human-machine spoken interactions.
To do so, we present a comparison be-
tween two corpora employing a compre-
hensive set of evaluation measures. The
first corpus was acquired from real inter-
actions of users with a spoken dialog sys-
tem, whereas the second was generated by
means of the simulation technique, which
decides the next user answer taking into
account the previous user turns, the last
system answer and the objective of the di-
alog.
1 Introduction
During the last decade, there has been a grow-
ing interest in learning corpus-based approaches
for the different components of spoken dialog sys-
tems (Minker, 1999), (Young, 2002), (Esteve et al,
2003), (He and Young, 2003), (Torres et al, 2005),
(Georgila et al, 2006), (Williams and Young,
2007). One of the most relevant areas of study
has been the automatic generation of dialogs be-
tween the dialog manager and an additional mod-
ule, called the user simulator, which generates au-
tomatic interactions with the dialog system.
A considerable effort is necessary to acquire
and label a corpus with the data necessary to train
good models. User simulators make it possible to
generate a large number of dialogs in a very simple
way, reducing the time and effort needed for the
evaluation of a dialog system each time the sys-
tem is modified.
The construction of user models based on sta-
tistical methods has provided interesting and well-
founded results in recent years and is currently a
growing research area. A probabilistic user model
can be trained from a corpus of human-computer
dialogs to simulate user answers. Therefore, it can
be used to learn a dialog strategy by means of its
interaction with the dialog manager. In the liter-
ature, there are several corpus-based approaches
for developing user simulators, learning optimal
management strategies, and evaluating the dialog
system (Scheffler and Young, 2001) (Pietquin and
Dutoit, 2005) (Georgila et al, 2006) (Cuaya?huitl
et al, 2006) (Lo?pez-Co?zar et al, 2006). A sum-
mary of user simulation techniques for reinforce-
ment learning of the dialog strategy can be found
in (Schatzmann et al, 2006). In this paper, we
propose a statistical approach to acquire a labeled
dialog corpus from the interaction of a user simu-
lator and a dialog manager. In our methodology,
the new user turn is selected using the probabil-
ity distribution provided by a neural network. By
means of the interaction of the dialog manager and
the user simulator, an initial dialog corpus can be
extended by increasing its variability and detect-
ing dialog situations in which the dialog manager
does not provide an appropriate answer. We pro-
pose the use of this corpus for evaluating both our
user simulation technique and our dialog system
performance.
Different studies have been carried out to com-
pare corpora acquired by means of different tech-
niques and to define the most suitable measures to
carry out this evaluation (Schatzmann et al, 2005),
(Turunen et al, 2006), (Ai et al, 2007b), (Ai and
Litman, 2006), (Ai and Litman, 2007), (Ai et al,
2007a). In this work, we have applied our dia-
log simulation technique to acquire a corpus in the
academic domain, and compared it with a corpus
recorded from real users interactions with a spo-
326
ken dialog system
The results of this comparison show that the
simulated corpus obtained is very similar to the
corpus recorded from real user interactions in
terms of number of turns, confirmations and dia-
log acts among other evaluation measures.
The rest of the paper is organized as follows.
Section 2 summarizes the main characteristics of
the UAH system. Section 3 describes our statis-
tical methodology for user simulation. Section 4
describes the set of measures used to compare the
corpus acquired with real users and the simulated
corpus. Section 5 presents the results of this eval-
uation, and finally, the conclusions are presented
in Section 6.
2 The UAH System
Universidad Al Habla (UAH - University on the
Line) is a spoken dialog system developed to pro-
vide spoken access to academic information about
our department (Callejas and Lo?pez-Co?zar, 2005)
(Callejas and Lo?pez-Co?zar, 2008). The system is
comprised of the five typical modules of current
spoken dialog systems, concerned with automatic
speech recognition (ASR), dialog management
(DM), database access (DB Access), data storage
(DB) and oral response generation (RG). In ad-
dition, we implemented a module called GAG to
automatically create ASR grammars (Callejas and
Lo?pez-Co?zar, 2007).
The dialog manager of the UAH system was im-
plemented using VoiceXML documents that are
dynamically created using PHP. This way, it can
adapt the system responses to the context of the
conversation and the dialog state, which improves
the naturalness of the interaction. For example, the
help messages provided by the system take into
account the topic that the user and the system are
addressing at a particular moment. The context is
used as well to decide the confirmation strategy to
use.
The information that the system provides can be
classified in four main groups: subjects, profes-
sors, doctoral studies and registration, as shown in
Table 1. As can be observed, the system must ask
the user for different pieces of information before
producing a response.
A set of 100 dialogs was acquired with the UAH
system by means of its interaction with real users.
The acquisition process resulted in a spontaneous
Spanish speech dialog corpus with 60 different
speakers. The total number of user turns was 422
and the recorded material has a duration of 150
minutes. Figure 1 shows an example of a dialog
extracted from this corpus. Turns tagged with an
?S? refer to system turns, and turns tagged with ?U?
refer to user turns.
S1: Welcome to the UAH system. How can I
help you?
U1: I want to know information about sub-
jects in Computer Science
S2: Which subject do you want to consult?
U2: Language Processors I
S3: What would you like to know?
U3: Can you inform me about the schedules
of Group D?
S4: The schedule of Group D is: Mondays
from 9 to 10 and Wednesdays from 9 to 11
and from 11 to 13. Anything else?
U4: Can you tell me the name of the lecturer?
S5: The name of the lecturer of the Group
D of Language Processors I is David Griol.
Anything else?
U5: No, thank you.
S6: Thank you for using the UAH system.
Figure 1: An example of a dialog acquired with
the interaction of the UAH system with real users
3 The Statistical User Simulation
Technique
In the UAH spoken dialogue system, the user sim-
ulator developed replaces the functions performed
by the ASR and the NLU modules.
The methodology that we have developed for
user simulation extends our work for developing
a statistical methodology for dialog management
(Griol et al, 2008). The user answers are gener-
ated taking into account the information provided
by the simulator throughout the history of the dia-
log, the last system turn, and the objective(s) pre-
defined for the dialog. A labeled corpus of dialogs
is used to estimate the user model. The formal de-
scription of the proposed model is as follows:
Let Ai be the output of the dialog system (the
system answer) at time i, expressed in terms of di-
alog acts. Let Ui be the semantic representation of
the user turn. We represent a dialog as a sequence
of pairs (system-turn, user-turn):
327
Category Information provided by the user (including examples) Information provided by the
system
Subject
Name Compilers Degree, lecturers, responsible
lecturer, semester, credits, web
page
Degree, in case that there are
several subjects with the same
name
Computer science
Group name and optionally type,
in case he asks for information
about a specific group
A
Theory A
Timetable, lecturer
Lecturers Any combination of name andsurnames
Zoraida
Zoraida Callejas
Ms. Callejas
Office location, contact infor-
mation (phone, fax, email),
groups and subjects, doctoral
courses
Optionally semester, in case he
asks for the tutoring hours
First semester
Second semester
Tutoring timetable
Doctoral studies Name of a doctoral program Software development Department, responsibleName of a course if he asks
for information about a specific
course
Object-oriented program-
ming
Type, credits
Registration Name of the deadline Provisional registration
confirmation
Initial time, final time, de-
scription
Table 1: Information provided by the UAH system
(A1, U1), ? ? ? , (Ai, Ui), ? ? ? , (An, Un)
where A1 is the greeting turn of the system (the
first turn of the dialog), and Un is the last user turn.
We refer to a pair (Ai, Ui) as Si, the state of the
dialog sequence at time i.
Given this representation, the objective of the
user simulator at time i is to find an appropriate
user answer Ui. This selection, which is a local
process for each time i, takes into account the se-
quence of dialog states that precede time i, the sys-
tem answer at time i, and the objective of the di-
alog O. If the most probable user answer Ui is
selected at each time i, the selection is made using
the following maximization:
U?i = argmax
Ui?U
P (Ui|S1, ? ? ? , Si?1, Ai,O)
where set U contains all the possible user answers.
As the number of possible sequences of states
is very large, we establish a partition in this space
(i.e., in the history of the dialog preceding time i).
Let URi be the user register at time i. The user
register is defined as a data structure that contains
the information provided by the user throughout
the previous history of the dialog.The partition
that we establish in this space is based on the as-
sumption that two different sequences of states are
equivalent if they lead to the same UR. After ap-
plying the above considerations and establishing
the equivalence relations in the histories of the di-
alogs, the selection of the best Ui is given by:
U?i = argmax
Ui?U
P (Ui|URi?1, Ai,O) (1)
We propose the use of a multilayer percep-
tron (MLP) to make the assignation of a user
turn. The input layer receives the current situa-
tion of the dialog, which is represented by the term
(URi?1, Ai,O) in Equation 1. The values of the
output layer can be viewed as the a posteriori prob-
ability of selecting the different user answers de-
fined for the simulator given the current situation
of the dialog. The choice of the most probable
user answer of this probability distribution leads
to Equation 1. In this case, the user simulator will
always generate the same answer for the same sit-
uation of the dialog. Since we want to provide the
user simulator with a richer variability of behav-
iors, we base our choice on the probability distri-
bution supplied by the MLP on all the feasible user
answers.
For the UAH task, the variable O is modeled
taking into account the different types of scenarios
defined for the acquisition of the original corpus
with real users (33).
The corpus acquired with real users includes in-
formation about the errors that were introduced by
328
the ASR and the NLU modules during this acqui-
sition. This information also includes confidence
measures, which are used by the DM to evaluate
the reliability of the concepts and attributes gener-
ated by the NLU module.
An error simulator module has been designed
to perform error generation. The error simulator
modifies the frames generated by the user simula-
tor once the UR is updated. In addition, the error
simulator adds a confidence score to each concept
and attribute in the frames. Experimentally, we
have detected 2.3 errors per dialog in our initial
corpus. This value can be modified to adapt the er-
ror simulator module to the operation of any ASR
and NLU modules.
A maximum number of twelve user turns per di-
alog was defined for acquiring a corpus using our
user simulator. A user request for closing the di-
alog is selected once the system has provided the
information defined in the objective(s) of the dia-
log. The dialogs that fulfill this condition before
the maximum number of turns are considered suc-
cessful. The dialog manager considers that the di-
alog is unsuccessful and decides to abort it when
the following conditions take place:
? The dialog exceeds the maximum number of
user turns.
? The answer selected by the dialog manager
corresponds with a query not required by the
user simulator.
? The database query module provides an error
warning because the user simulator has not
provided the mandatory information needed
to carry out the query.
? The answer generator provides a error warn-
ing when the selected answer involves the use
of a data not provided by the user simulator.
The semantics selected for the dialog manager
is represented through the 27 possible system an-
swers defined for the UAH task. The selection of
the possible user answers is carried out using the
semantics defined for the user in the NLU module.
A dialog corpus of 3000 successful dialogs was
acquired using the user simulation technique fol-
lowing a set of scenarios covering all the possible
types of queries regarding the information sought.
Figure 2 shows an example of a dialog cor-
responding to a scenario with two objectives (to
know timetables of a given lecturer and subject,
and information about the registration in its spe-
cific degree). The confidence scores generated by
the error simulator are shown between brackets.
Sentences in natural language, which are equiv-
alent to the labeled system and user turns, have
been incorporated to clarify the explanation.
S1: Opening
Welcome to the UAH system. How may I help
you?
U1: Lecturers [0.1]
I want to know information about lecturers.
S2: (Confirmation:Lecturers)
Do you want to know information about lec-
turers?
U2: Affirmation [0.9]
Yes.
S3: (Question:Lecturer Name)
Tell me the name of the lecturer.
U3: Lecturer Name: Ramo?n Lo?pez-Co?zar
[0.8]
Ramo?n Lo?pez-Co?zar.
S4: (Question:Lecturer Information)
What would you like to know about Mr.
Lo?pez-Co?zar?
U4: (Question:Office) [0.9]
The location of his office please.
S5: (Answer:Office) (New-Query)
{The office of Ramo?n Lo?pez-Co?zar is located
in the third floor of ...} Anything else?
U5: (Question:Registration) [0.9]
Degree: Computer Science[0.9]
How to register in Computer Science.
S6: (Answer:Registration) (New-Query)
{Registration information} Anything else?
U6: Negation [0.9] No.
S7: (Closing)
Thank you for using the UAH system.
Figure 2: An example of a dialog acquired by
means of the simulation technique
4 Evaluation of the Corpora
We used a set of measures to carry out the evalu-
ation of the acquired corpora based on prior work
in the dialog literature. (Schatzmann et al, 2005)
proposed a comprehensive set of quantitative eval-
uation measures to compare two dialog corpora.
These measures were adapted for our purpose and
can be divided into three types:
329
High-level dialog features
Average number of turns per dialog
Percentage of different dialogs
Number of repetitions of the most seen dialog
Number of turns of the most seen dialog
Number of turns of the shortest dialog
Number of turns of the longest dialog
Dialog style/cooperativeness measures
System dialog acts: Confirmation of concepts and attributes, Questions to require information, and
Answers generated after a database query.
User dialog acts: Request to the system, Provide information, Confirmation, Yes/No answers, and
Other answers.
Figure 3: Evaluation measures used to compare the acquired corpora
? High-level dialog features: These features
evaluate the duration of the dialogs, the
amount of information transmitted in the in-
dividual turns, and how active the dialog par-
ticipants are.
? Dialog style/cooperativeness measures:
These measures analyze the frequency of
the different speech acts and study, for
example, the proportion of actions which are
goal-directed vs. dialog formalities.
? Task success/efficiency measures: These are
computations of the goal achievement rates
and goal completion times.
We have defined six high-level dialog features
for the evaluation of the dialogs: the average num-
ber of turns per dialog, the percentage of differ-
ent dialogs without considering the attribute val-
ues, the number of repetitions of the most seen di-
alog, the number of turns of the most seen dialog,
the number of turns of the shortest dialog, and the
number of turns of the longest dialog. Using these
measures, we tried to evaluate the success of the
simulated dialogs as well as their efficiency and
variability with regard to the different objectives.
For dialog style features, we have defined a set
of system/user dialog acts. On the system side,
we have measured the frequency of confirmations,
questions that require information, and system an-
swers generated after a database query. We have
not taken into account the opening and closing sys-
tem turns. On the user side, we have measured the
percentage of turns in which the user carries out
a request to the system, provide information, con-
firms a concept or attribute, Yes/No answers, and
other answers not included in the previous cate-
gories.
We have not considered task success/efficiency
measures in our evaluation, since only the dialogs
that fulfill the objectives predefined in the scenar-
ios have been incorporated into our corpora. We
have considered successful dialogs those that ful-
fill the complete list of objectives defined in the
corresponding scenario. Figure 3 summarizes the
complete set of measures used in the evaluation.
5 Evaluation Results
To compare the two corpora, we have computed
the mean value for each corpus with respect to
each of the evaluation measures shown in the pre-
vious section. Then two-tailed t-tests have been
employed to compare the means across the two
corpora as described in (Ai et al, 2007a). All dif-
ferences reported as statistically significant have
p-values less than 0.05 after Bonferroni correc-
tions.
5.1 High-level Dialog Features
As stated in the previous section, the first group of
experiments covers the following statistical prop-
erties: i) Dialog length in terms of the average
number of turns per dialog, number of turns of the
shortest dialog, number of turns of the longest di-
alog, and number of turns of the most seen dialog;
ii) Number of different dialogs in each corpus in
terms of the percentage of different dialogs and the
number of repetitions of the most seen dialog; iii)
Turn length in terms of actions per turn; iv) Partic-
ipant activity as a ratio of system and user actions
per dialog.
330
Initial Corpus Simulated Corpus
Average number of user turns per dialog 4.99 3.75
Percentage of different dialogs 85.71% 77.42%
Number of repetitions of the most seen dialog 5 27
Number of turns of the most seen dialog 2 2
Number of turns of the shortest dialog 2 2
Number of turns of the longest dialog 14 12
Table 2: Results of the high-level dialog features defined for the comparison of the three corpora
Table 2 shows the results of the comparison of
the high-level dialog features. It can be observed
that all measures have similar values in both cor-
pora. The more significant difference is the aver-
age number of user turns. In the four types of sce-
narios, the dialogs acquired using the simulation
technique were shorter than the dialogs acquired
with real users. This can be explained by the fact
that there was a number of dialogs acquired with
real users in which the user asked for additional
information not included in the definition of the
corresponding scenario once the dialog objectives
had been achieved.
5.2 Dialog Style and Cooperativeness
Tables 3 and 4 respectively show the frequency of
the most dominant user and system dialog acts.
Table 3 shows the results of this comparison for
the system dialog acts. It can be observed that
there are also only slight differences between the
values obtained for both corpora. There is a higher
percentage of confirmations and questions in the
corpus acquired with real users due to its higher
average number of turns per dialog.
Table 4 shows the results of this comparison for
the user dialog acts. The most significant differ-
ence between both corpora is the percentage of
turns in which the user makes a request to the sys-
tem, which is lower in the corpus acquired with
real users. This is possibly because it is less prob-
able that simulated users provide useless informa-
tion, as it is shown in the lower percentage of the
users turns classified as Other answers.
6 Conclusions
In this paper, we have presented a comparison be-
tween two corpora acquired using two different
techniques. Firstly, we gathered an initial dialog
corpus from real user-system interactions. Sec-
ondly, we have employed a statistical user simu-
lation technique based on a classification process
to automatically obtain a corpus of simulated di-
alogs. Our results show that it is feasible to acquire
a realistic corpus by means of the simulation tech-
nique. The experimental results reported indicate
that the simulated and real interactions corpora are
very similar in terms of number of user turns, user
and system dialog style and cooperativeness, and
most frequent dialogs statistics. As future work,
we plan to employ the simulated dialogs for eval-
uation purposes and for extracting valuable infor-
mation to optimize the current dialog strategy.
References
H. Ai and D. Litman. 2006. Comparing Real-Real,
Simulated-Simulated, and Simulated-Real Spoken
Dialogue Corpora. In Procs. of AAAI Workshop Sta-
tistical and Empirical Approaches for Spoken Dia-
logue Systems, Boston, USA.
H. Ai and D.J. Litman. 2007. Knowledge Consistent
User Simulations for Dialog Systems. In Proc. of In-
terspeech?07, pages 2697?2700, Antwerp, Belgium.
H. Ai, A. Raux, D. Bohus, M. Eskenazi, and D. Litman.
2007a. Comparing Spoken Dialog Corpora Col-
lected with Recruited Subjects versus Real Users. In
Proc. of the SIGdial?07, pages 124?131, Antwerp,
Belgium.
H. Ai, J.R. Tetreault, and D.J. Litman. 2007b. Com-
paring User Simulation Models For Dialog Strategy
Learning. In Proc. of NAACL HLT?07, pages 1?4,
Rochester, NY, USA.
Z. Callejas and R. Lo?pez-Co?zar. 2005. Implementing
modular dialogue systems: a case study. In Proc. of
Applied Spoken Language Interaction in Distributed
Environments (ASIDE?05), Aalborg, Denmark.
Z. Callejas and R. Lo?pez-Co?zar. 2007. Automatic
creation of ASR grammar rules for unknown vo-
cabulary applications. In Proc. of the 8th Interna-
tional workshop on Electronics, Control, Modelling,
Measurement and Signals (ECMS?07), pages 50?55,
Liberec, Czech Republic.
Z. Callejas and R. Lo?pez-Co?zar. 2008. Relations be-
tween de-facto criteria in the evaluation of a spoken
331
Initial Corpus Simulated Corpus
Confirmation of concepts and attributes 13.51% 12.23%
Questions to require information 18.44% 16.57%
Answers generated after a database query 68.05% 71.20%
Table 3: Percentages of the different types of system dialog acts in both corpora
Initial Corpus Simulated Corpus
Request to the system 31.74% 35.43%
Provide information 21.72% 20.98%
Confirmation 10.81% 9.34%
Yes/No answers 33.47% 32.77%
Other answers 2.26% 1.48%
Table 4: Percentages of the different types of user dialog acts in both corpora
dialogue system. Speech Communication, 50(8?
9):646?665.
H. Cuaya?huitl, S. Renals, O. Lemon, and H. Shi-
modaira. 2006. Learning Multi-Goal Dialogue
Strategies Using Reinforcement Learning with Re-
duced State-Action Spaces. In Proc. of the 9th Inter-
national Conference on Spoken Language Process-
ing (Interspeech/ICSLP), pages 469?472, Pittsburgh
(USA).
Y. Esteve, C. Raymond, F. Bechet, and R. De Mori.
2003. Conceptual Decoding for Spoken Dialog sys-
tems. In Proc. of European Conference on Speech
Communications and Technology (Eurospeech?03),
volume 1, pages 617?620, Geneva (Switzerland).
K. Georgila, J. Henderson, and O. Lemon. 2006. User
Simulation for Spoken Dialogue Systems: Learn-
ing and Evaluation. In Proc. of the 9th Interna-
tional Conference on Spoken Language Processing
(Interspeech/ICSLP), pages 1065?1068, Pittsburgh
(USA).
D. Griol, L.F. Hurtado, E. Segarra, and E. Sanchis.
2008. A Statistical Approach to Spoken Dialog Sys-
tems Design and Evaluation. Speech Communica-
tion, 50(8?9):666?682.
Y. He and S. Young. 2003. A data-driven spoken lan-
guage understanding system. In Proc. of IEEE Auto-
matic Speech Recognition and Understanding Work-
shop (ASRU?03), pages 583?588, St. Thomas (U.S.
Virgin Islands).
R. Lo?pez-Co?zar, Z. Callejas, and M. McTear. 2006.
Testing the performance of spoken dialogue systems
by means of an artificially simulated user. Artificial
Intelligence Review, 26:291?323.
W. Minker. 1999. Stocastically-based semantic analy-
sis. In Kluwer Academic Publishers, Boston (USA).
O. Pietquin and T. Dutoit. 2005. A probabilistic
framework for dialog simulation and optimal strat-
egy learning. In IEEE Transactions on Speech and
Audio Processing, Special Issue on Data Mining of
Speech, Audio and Dialog, volume 14, pages 589?
599.
J. Schatzmann, K. Georgila, and S. Young. 2005.
Quantitative Evaluation of User Simulation Tech-
niques for Spoken Dialogue Systems. In Proc. of
SIGdial?05, pages 45?54, Lisbon (Portugal).
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. In Knowledge
Engineering Review, volume 21(2), pages 97?126.
K. Scheffler and S. Young. 2001. Automatic learning
of dialogue strategy using dialogue simulation and
reinforcement learning. In Proc. of HLT?02, pages
12?18, San Diego (USA).
F. Torres, L.F. Hurtado, F. Garc??a, E. Sanchis, and
E. Segarra. 2005. Error handling in a stochastic dia-
log system through confidence measures. In Speech
Communication, pages (45):211?229.
M. Turunen, J. Hakulinen, and A. Kainulainen. 2006.
Evaluation of a Spoken Dialogue System with Us-
ability Tests and Long-term Pilot Studies: Similar-
ities and Differences. In Proc. of the 9th Interna-
tional Conference on Spoken Language Processing
(Interspeech/ICSLP), pages 1057?1060, Pittsburgh,
USA.
J. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. In Computer Speech and Language, volume
21(2), pages 393?422.
S. Young. 2002. The Statistical Approach to the De-
sign of Spoken Dialogue Systems. Technical re-
port, CUED/F-INFENG/TR.433, Cambridge Uni-
versity Engineering Department, Cambridge (UK).
332
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 269?272,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Statistical Dialog Management Methodologies for Real Applications
David Griol
Dept. of Computer Science
Carlos III University of Madrid
Av. Universidad, 30, 28911, Legane?s
dgriol@inf.uc3m.es
Zoraida Callejas, Ramo?n Lo?pez-Co?zar
Dept. of Languages and Computer Systems, CITIC-UGR
University of Granada
C/ Pdta. Daniel Saucedo Aranda, 18071, Granada
{zoraida, rlopezc}@ugr.es
Abstract
In this paper we present a proposal for the
development of dialog systems that, on the
one hand, takes into account the benefits of
using standards like VoiceXML, whilst on
the other, includes a statistical dialog mod-
ule to avoid the effort of manually defin-
ing the dialog strategy. This module is
trained using a labeled dialog corpus, and
selects the next system response consider-
ing a classification process that takes into
account the dialog history. Thus, system
developers only need to define a set of
VoiceXML files, each including a system
prompt and the associated grammar to rec-
ognize the users responses to the prompt.
We have applied this technique to develop
a dialog system in VoiceXML that pro-
vides railway information in Spanish.
1 Introduction
When designing a spoken dialog system, develop-
ers need to specify the system actions in response
to user utterances and environmental states that,
for example, can be based on observed or inferred
events or beliefs. In addition, the dialog manager
needs a dialog strategy that defines the conversa-
tional behavior of the system. This is the funda-
mental task of dialog management (Paek and Pier-
accini, 2008), as the performance of the system is
highly dependent on the quality of this strategy.
Thus, a great effort is employed to empirically de-
sign dialog strategies for commercial systems. In
fact, the design of a good strategy is far from be-
ing a trivial task since there is no clear definition
of what constitutes a good strategy (Schatzmann
et al, 2006). Once the strategy has been designed,
the implementation of the system is leveraged by
programming languages such as VoiceXML, for
which different programming environments and
tools have been created to help developers.
As an alternative of the previously described
rule-based approaches, the application of statis-
tical approaches to dialog management makes it
possible to consider a wider space of dialog strate-
gies (Georgila et al, 2006; Williams and Young,
2007; Griol et al, 2009). The main reason is that
statistical models can be trained from real dialogs,
modeling the variability in user behaviors. The fi-
nal objective is to develop dialog systems that have
a more robust behavior and are easier to adapt to
different user profiles or tasks.
(Pieraccini et al, 2009) highlights the imprac-
ticality of applying statistical learning approaches
to develop commercial applications, in the sense
that it is difficult to consider the expert knowl-
edge of human designers. From his perspective,
a hybrid approach, combining statistical and rule-
based approaches, could be a good solution. The
reason is that statistical approaches can offer a
wider range of alternatives at each dialog state,
whereas rule based approaches may offer knowl-
edge on best practices.
For example, (Williams, 2008) proposes taking
advantage of POMDPs and rule-based approaches
by using POMDPs to foster robustness and at the
same time being able to incorporate handcrafted
constraints which cover expert knowledge in the
application domain. Also (Lee et al, 2010) have
recently proposed a different hybrid approach to
dialog modeling in which n-best recognition hy-
potheses are weighted using a mixture of expert
knowledge and data-driven measures by using an
agenda and an example-based machine translation
approach respectively. In both approaches, the hy-
brid method achieved significant improvements.
Additionally, speech recognition grammars for
commercial systems have been usually built on
the basis of handcrafted rules that are tested re-
cursively, which in complex applications is very
costly (McTear, 2004). However, as stated by
(Pieraccini et al, 2009), many sophisticated com-
269
mercial systems already available receive a large
volume of interactions. Therefore, industry is be-
coming more interested in substituting rule based
grammars with statistical approaches based on the
large amounts of data available.
As an attempt to improve the current technol-
ogy, we propose to merge statistical approaches
with VoiceXML. Our goal is to combine the flex-
ibility of statistical dialog management with the
facilities that VoiceXML offers, which would help
to introduce statistical approaches for the develop-
ment of commercial (and not strictly academic) di-
alog systems. To this end, our technique employs
a statistical dialog manager that takes into account
the history of the dialog up to the current dialog
state in order to decide the next system prompt.
In addition, the system prompts and the gram-
mars for ASR are implemented in VoiceXML-
compliant formats, for example, JSGF or SRGS.
As it is often difficult to find or gather a human-
machine corpus which cover an identical domain
as the system which is to be implemented, our ap-
proach is also based on the compilation of cor-
pora of interactions of simulated users, which is
a common practice when using machine learning
approaches for system development.
In contrast with other hybrid approaches, our
main aim is not to incorporate knowledge about
best strategies in statistical dialog management,
but rather to take advantage of an implementa-
tion language which has been traditionally used
to build rule-based systems (such as VoiceXML),
for the development of statistical dialog strate-
gies. Expert knowledge about deployment of
VoiceXML applications, development environ-
ments and tools can still be exploited using our
technique. The only change is in the transition be-
tween states, which is carried out on a data-driven
basis (i.e., is not deterministic). We have applied
our technique to develop a dialog system that pro-
vides railway information, for which we have de-
veloped a statistical dialog management technique
in a previous study.
2 Our Proposal to Introduce Statistical
Methodologies in Commercial
Applications
As stated in the introduction, our approach to inte-
grate statistical methodologies in commercial ap-
plications is based on the automatic learning of the
dialog strategy using a statistical dialog manage-
ment methodology. In most dialog systems, the
dialog manager makes decisions based only on the
information provided by the user in the previous
turns and its own dialog model. For example, this
is the case with most dialog systems for slot-filling
tasks. The methodology that we propose for the
selection of the next system response for this kind
of task is detailed in (Griol et al, 2008). It is based
on the definition of a data structure that we call
Dialog Register (DR), which contains the infor-
mation provided by the user throughout the dialog
history. In brief, it is as follows: for each time i,
the selection of the next system prompt Ai is car-
ried out by means of the following maximization:
A?i = argmax
Ai?A
P (Ai|DRi?1, Si?1)
where the set A contains all the possible system
responses and Si?1 is the state of the dialog se-
quence (system-turn, user-turn) at time i.
Each user turn supplies the system with infor-
mation about the task; that is, he/she asks for a
specific concept and/or provides specific values
for certain attributes. However, a user turn could
also provide other kinds of information, such as
task-independent information. This is the case of
turns corresponding to Affirmation, Negation and
Not-Understood dialog acts. This kind of infor-
mation implies some decisions which are different
from simply updating the DRi?1. Hence, for the
selection of the best system response Ai, we take
into account the DR that results from turn 1 to
turn i? 1, and we explicitly consider the last state
Si?1. Our model can be extended by incorporating
additional information to the DR, such as some
chronological information (e.g. number of turns
up to the current turn) or user profiles (e.g. user
experience or preferences).
The selection of the system response is car-
ried out through a classification process, for which
a multilayer perceptron (MLP) is used. The in-
put layer receives the codification of the pair
(DRi?1, Si?1). The output generated by the MLP
can be seen as the probability of selecting each of
the different system answers defined for a specific
task.
To learn the dialog model we use dialog sim-
ulation techniques. Our approach for acquiring a
dialog corpus is based on the interaction of a user
simulator and a dialog manager simulator (Griol et
al., 2007). The user simulation replaces the user
intention level, that is, it provides concepts and
270
attributes that represent the intention of the user.
This way, the user simulator carries out the func-
tions of the ASR and NLU modules. Errors and
confidence scores are simulated by a specific mod-
ule in the simulator. The acquired dialogs are em-
ployed to automatically generate VoiceXML code
for each system prompt and create the grammar
needed to recognize the possible user utterances
after each one of the system prompts.
3 Development of a railway information
system using the proposed technique
To test our proposal, we have used the defini-
tions taken to develop the DIHANA dialog system,
which was developed in a previous study to pro-
vide information about train services, schedules
and fares in Spanish (Griol et al, 2009; Griol et
al., 2008). The DR defined for the our railway in-
formation system is a sequence of 15 fields, corre-
sponding to the five concepts (Hour, Price, Train-
Type, Trip-Time, Services) and ten attributes (Ori-
gin, Destination, Departure-Date, Arrival-Date,
Departure-Hour, Arrival-Hour, Class, Train-Type,
Order-Number, Services). The system generates a
total of 51 different prompts.
Three levels of labeling are defined for the la-
beling of the system dialog acts. The first level
describes general acts which are task independent.
The second level is used to represent concepts and
attributes involved in dialog turns that are task-
dependent. The third level represents values of at-
tributes given in the turns. The following labels
are defined for the first level: Opening, Closing,
Undefined, Not-Understood, Waiting, New-Query,
Acceptance, Rejection, Question, Confirmation,
and Answer. The labels defined for the second and
third level were the following: Departure-Hour,
Arrival-Hour, Price, Train-Type, Origin, Destina-
tion, Date, Order-Number, Number-Trains, Ser-
vices, Class, Trip-Type, Trip-Time, and Nil. There
are dialog turns which are labeled with several di-
alog acts.
Having this kind of labeling and the values of
attributes obtained during a dialog, it is straightfor-
ward to construct a sentence in natural language.
Some examples of the dialog act labeling of the
system turns are shown in Figure 1.
Two million dialogs were simulated using a set
of two types of scenarios. Type S1 defines one
objective for the dialog, whereas Type S2 defines
two. Table 1 summarizes the statistics of the ac-
[SPANISH] Bienvenido al servicio de informacio?n de
trenes. ?En que? puedo ayudarle?
[ENGLISH] Welcome to the railway information sys-
tem. How can I help you?
(Opening:Nil:Nil)
[SPANISH] El u?nico tren es un Euromed que sale a las
0:27. ?Desea algo ma?s?
[ENGLISH] There is only one train, which is a Eu-
romed, that leaves at 0:27. Anything else?
(Answer:Departure-Hour:Departure-Hour:Departure-
Hour[0.27],Number-Trains[1],Train-Type[Euromed])
(New-Query:Nil:Nil)
Figure 1: Labeling examples of system turns from
the DIHANA corpus
quisition for the two types of scenarios.
Type S1 Type S2
Simulated dialogs 106 106
Successful dialogs 15,383 1,010
Different dialogs 14,921 998
Number of user turns per dialog 4.9 6.2
Table 1: Statistics of the new corpus acquisition
The 51 different system prompts have been au-
tomatically generated in VoiceXML using the pro-
posed technique. For example, Figure 2 shows the
VXML document to prompt the user for the origin
city, whereas Figure 3 shows the obtained gram-
mar for ASR.
<?xml version="1.0" encoding="UTF-8"?>
<vxml xmlns="http://www.w3.org/2001/vxml"
xmlns:xsi="http://www.w3.org/2001/
XMLSchema-instance"
xsi:schemaLocation="http://www.w3.org/2001/vxml
http://www.w3.org/TR/voicexml20/vxml.xsd"
version="2.0" application="app-dihana.vxml">
<form id="origin_form">
<field name="origin">
<grammar type="application/srgs+xml"
src="/grammars/origin.grxml"/>
<prompt>Tell me the origin city.</prompt>
<filled>
<return namelist="origin"/>
</filled>
</field>
</form>
</vxml>
Figure 2: VXML document to require the origin
city
4 Conclusions
In this paper, we have described a technique for
developing dialog systems using a well known
271
#JSGF V1.0;
grammar origin;
public <origin> = [<desire>]
[<travel> <city> {this.destination=$city}]
[<proceed> <city> {this.origin=$city}];
<desire> = I want [to know] | I would like
[to know] | I would like | I want | I need
| I have to;
<travel> = go to | travel to | to go to
| to travel to;
<city> = Jae?n | Co?rdoba | Sevilla | Huelva |
Ca?diz | Ma?laga | Granada | Almer??a |
Valencia | Alicante | Castello?n | Barcelona
| Madrid;
<proceed> = from | going from | go from;
Figure 3: Grammar defined to capture the origin
city
standard like VoiceXML, and considering a statis-
tical dialog model that is automatically learnt from
a dialog corpus.
The main objective of our work is to reduce the
gap between academic and commercial systems
by reducing the effort required to define optimal
dialog strategies and implement the system. Our
proposal works on the benefits of statistical meth-
ods for dialog management and VoiceXML, re-
spectively. The former provide an efficient means
to exploring a wider range of dialog strategies,
whereas the latter makes it possible to benefit from
the advantages of using the different tools and
platforms that are already available to simplify
system development. We have applied our tech-
nique to develop a dialog system that provides rail-
way information, and have shown that it enables
creating automatically VoiceXML documents to
prompt the user for data, as well as the necessary
grammars for ASR. As a future work, we plan to
study ways for adapting the proposed dialog man-
agement technique to more complex domains.
Additionally, we are interested in investigating
possible ways for easing the adoption of our tech-
nique in industry, and the main challenges that
might arise in using it to develop commercial sys-
tems.
Acknowledgments
This research has been funded by the Spanish
Ministry of Science and Technology, under project
HADA TIN2007-64718.
References
K. Georgila, J. Henderson, and O. Lemon. 2006. User
Simulation for Spoken Dialogue Systems: Learn-
ing and Evaluation. In Proc. of the 9th Inter-
speech/ICSLP, pages 1065?1068, Pittsburgh (USA).
D. Griol, L.F. Hurtado, E. Sanchis, and E. Segarra.
2007. Acquiring and Evaluating a Dialog Corpus
through a Dialog Simulation Technique. In Proc.
of the 8th SIGdial Workshop on Discourse and Dia-
logue, pages 39?42, Antwerp (Belgium).
D. Griol, L.F. Hurtado, E. Segarra, and E. Sanchis.
2008. A Statistical Approach to Spoken Dialog Sys-
tems Design and Evaluation. Speech Communica-
tion, 50(8?9):666?682.
D. Griol, G. Riccardi, and Emilio Sanchis. 2009. A
Statistical Dialog Manager for the LUNA project.
In Proc. of Interspeech/ICSLP?09, pages 272?275,
Brighton (UK).
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, and
Gary Geunbae Lee. 2010. Hybrid approach to
robust dialog management using agenda and dia-
log examples. Computer Speech and Language,
24(4):609?631.
Michael F. McTear, 2004. Spoken Dialogue Technol-
ogy: Towards the Conversational User Interface.
Springer.
T. Paek and R. Pieraccini. 2008. Automating spoken
dialogue management design using machine learn-
ing: An industry perspective . Speech Communica-
tion, 50(8?9):716?729.
Roberto Pieraccini, David Suendermann, Krishna
Dayanidhi, and Jackson Liscombe. 2009. Are We
There Yet? Research in Commercial Spoken Dia-
log Systems. Lecture Notes in Computer Science,
5729:3?13.
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. In Knowledge
Engineering Review, volume 21(2), pages 97?126.
J. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. In Computer Speech and Language, volume
21(2), pages 393?422.
Jason D. Williams. 2008. The best of both
worlds: Unifying conventional dialog systems and
POMDPs. In Proceedings of the International Con-
ference on Spoken Language Processing.
272
