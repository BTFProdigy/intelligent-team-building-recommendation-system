Proceedings of NAACL HLT 2009: Demonstrations, pages 17?20,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
WordNet::SenseRelate::AllWords -
A Broad Coverage Word Sense Tagger
that Maximizes Semantic Relatedness
Ted Pedersen and Varada Kolhatkar
Department of Computer Science
University of Minnesota
Duluth, MN 55812 USA
{tpederse,kolha002}@d.umn.edu
http://senserelate.sourceforge.net
Abstract
WordNet::SenseRelate::AllWords is a freely
available open source Perl package that as-
signs a sense to every content word (known
to WordNet) in a text. It finds the sense of
each word that is most related to the senses
of surrounding words, based on measures
found in WordNet::Similarity. This method is
shown to be competitive with results from re-
cent evaluations including SENSEVAL-2 and
SENSEVAL-3.
1 Introduction
Word sense disambiguation is the task of assigning
a sense to a word based on the context in which it
occurs. This is one of the central problems in Nat-
ural Language Processing, and has a long history of
research. A great deal of progress has been made in
using supervised learning to build models of disam-
biguation that assign a sense to a single target word
in context. This is sometimes referred to as the lexi-
cal sample or target word formulation of the task.
However, to be effective, supervised learning re-
quires many manually disambiguated examples of
a single target word in different contexts to serve
as training data to learn a classifier for that word.
While the resulting models are often quite accurate,
manually creating training data in sufficient volume
to cover even a few words is very time consuming
and error prone. Worse yet, creating sufficient train-
ing data to cover all the different words in a text is
essentially impossible, and has never even been at-
tempted.
Despite these difficulties, word sense disambigua-
tion is often a necessary step in NLP and can?t sim-
ply be ignored. The question arises as to how to de-
velop broad coverage sense disambiguation modules
that can be deployed in a practical setting without in-
vesting huge sums in manual annotation efforts. Our
answer is WordNet::SenseRelate::AllWords (SR-
AW), a method that uses knowledge already avail-
able in the lexical databaseWordNet to assign senses
to every content word in text, and as such offers
broad coverage and requires no manual annotation
of training data.
SR-AW finds the sense of each word that is most
related or most similar to those of its neighbors in the
sentence, according to any of the ten measures avail-
able in WordNet::Similarity (Pedersen et al, 2004).
It extends WordNet::SenseRelate::TargetWord, a
lexical sample word sense disambiguation algorithm
that finds the maximum semantic relatedness be-
tween a target word and its neighbors (Patward-
han et al, 2003). SR-AW was originally developed
by (Michelizzi, 2005) (through version 0.06) and is
now being significantly enhanced.
2 Methodology
SR-AW processes a text sentence by sentence. It
proceeds through each sentence word by word from
left to right, centering each content word in a bal-
anced window of context whose size is determined
by the user. Note that content words at the start
or end of a sentence will have unbalanced windows
associated with them, since the algorithm does not
cross sentence boundaries and treats each sentence
independently.
17
All of the possible senses of the word in the center
of the window are measured for similarity relative to
the possible senses of each of the surrounding words
in the window in a pairwise fashion. The sense of
the center word that has the highest total when those
pairwise scores are summed is considered to be the
sense of that word. SR-AW then moves the center
of the window to the next content word to the right.
The user has the option of fixing the senses of the
words that precede it to those that were discovered
by SR-AW, or allowing all their senses to be consid-
ered in subsequent steps.
WordNet::Similarity1 offers six similarity mea-
sures and four measures of relatedness. Measures
of similarity are limited to making noun to noun and
verb to verb comparisons, and are based on using
the hierarchical information available for nouns and
verbs in WordNet. These measures may be based
on path lengths (path, wup, lch) or on path lengths
augmented with Information Content derived from
corpora (res, lin, jcn). The measures of relatedness
may make comparisons between words in any part
of speech, and are based on finding paths between
concepts that are not limited to hierarchical relations
(hso), or on using gloss overlaps either for string
matching (lesk) or for creating a vector space model
(vector and vector-pairs) that are used for measuring
relatedness.
The availability of ten different measures that can
be used with SR-AW leads to an incredible richness
and variety in this approach. In general word sense
disambiguation is based on the presumption that
words that occur together will have similar or related
meanings, so SR-AW allows for a wide range of op-
tions in deciding how to assess similarity and relat-
edness. SR-AW can be viewed as a graph based ap-
proach when using the path based measures, where
words are assigned the senses that are located most
closely together in WordNet. These path based
methods can be easily augmented with Information
Content in order to allow for finer grained distinc-
tions to be made. It is also possible to lessen the
impact of the physical structure of WordNet by us-
ing the content of the glosses as the primary source
of information.
1http://wn-similarity.sourceforge.net
3 WordNet::SenseRelate::AllWords Usage
Input : The input to SR-AW can either be plain
untagged text (raw), or it may be tagged with Penn
Treebank part of speech tags (tagged : 47 tags; e.g.,
run/VBD), or with WordNet part of speech tags (wn-
tagged: 4 tags for noun, verb, adjective, adverb;
e.g., run#v). Penn Treebank tags are mapped to
WordNet POS tags prior to SR-AW processing, so
even though this tag set is very rich, it is used sim-
ply to distinguish between the four parts of speech
WordNet knows, and identify function words (which
are ignored as WordNet only includes open class
words). In all cases simple morphological process-
ing as provided by WordNet is utilized to identify
the root form of a word in the input text.
Examples of each input format are shown below:
? (raw) : The astronomer married a movie star.
? (tagged) : The/DT astronomer/NN mar-
ried/VBD a/DT movie star/NN
? (wntagged) : The astronomer#n married#v a
movie star#n
If the format is raw, SR-AW will identify Word-
Net compounds before processing. These are multi-
word terms that are usually nouns with just one
sense, so their successful identification can signif-
icantly improve overall accuracy. If a compound
is not identified, then it often becomes impossible
to disambiguate. For example, if White House is
treated as two separate words, there is no combina-
tion of senses that will equal the residence of the
US president, where that is the only sense of the
compound White House. To illustrate the scope of
compounds, of the 155,287 unique strings in Word-
Net 3.0, more than 40% (64,331) of them are com-
pounds. If the input is tagged or wntagged, it is
assumed that the user has identified compounds by
connecting the words that make up a compound with
(e.g., white house, movie star).
In the tagged and wntagged formats, the user must
identify compounds and also remove punctuation.
In the raw format SR-AW will simply ignore punc-
tuation unless it happens to be part of a compound
(e.g., adam?s apple, john f. kennedy). In all formats
the upper/lower case distinction is ignored, and it is
18
assumed that the input is already formatted one line
per sentence, one sentence per line.
SR-AW will then check to see if a stoplist has
been provided by the user, or if the user would like to
use the default stoplist. In general a stoplist is highly
recommended, since there are quite a few words in
WordNet that have unexpected senses and might be
problematic unless they are excluded. For example,
who has a noun sense of World Health Organization.
A has seven senses, including angstrom, vitamin A,
a nucleotide, a purine, an ampere, the letter, and the
blood type. Many numbers have noun senses that
define them as cardinal numbers, and some have ad-
jective senses as well.
In the raw format, the stoplist check is done after
compounding, because certain compounds include
stop words (e.g., us house of representatives). In
the wntagged and tagged formats the stoplist check
is still performed, but the stoplist must take into ac-
count the form of the part of speech tags. How-
ever, stoplists are expressed using regular expres-
sions, making it quite convenient to deal with part
of speech tags, and also to specify entire classes of
terms to be ignored, such as numbers or single char-
acter words.
Disambiguation Options : The user has a number
of options to control the direction of the SR-AW al-
gorithm. These include the very powerful choices
regarding the measure of similarity or relatedness
that is to be used. There are ten such measures as
has been described previously. As was also already
mentioned, the user also can choose to fix the senses
of words that have already been processed.
In addition to these options, the user can con-
trol the size of the window used to determine which
words are involved in measuring relatedness or simi-
larity. A window size ofN includes the center word,
and then extends out to the left and right of the cen-
ter for N/2 content words, unless it encounters the
sentence boundaries. IfN is odd then the number of
words to the left and right (N ? 1)/2, and if N is
even there are N/2 words to the left, and (N/2)? 1
words to the right.
When using a measure of similarity and tagged or
wntagged text, it may be desirable to coerce the part
of speech of surrounding words to that of the word
in the center of the window of context. If this is
not done, then any word with a part of speech other
than that of the center word will not be included in
the calculation of semantic similarity. Coercion is
performed by first checking for forms of the word in
a different part of speech, and then checking if there
are any derivational relations from the word to the
part of speech of the center word. Note that in the
raw format part of speech coercion is not necessary,
since the algorithm will consider all possible parts of
speech for each word. If the sense of previous words
has already been fixed, then part of speech coercion
does not override those fixed assignments.
Finally, the user is able to control several scoring
thresholds in the algorithm. The user may specify a
context score which indicates a minimum threshold
that a sense of the center word should achieve with
all the words in the context in order to be selected.
If this threshold is not met, no sense is assigned and
it may be that the window should be increased.
The pair score is a finer grained threshold that in-
dicates the minimum values that a relatedness score
between a sense of the center word and a sense of
one of the neighbors must achieve in order to be
counted in the overall score of the center word. If
this threshold is not met then the pair will contribute
0 to that score. This can be useful for filtering out
noise from the scores when set to modest values.
Output : The output of SR-AW is the original text
with WordNet sense tags assigned. WordNet sense
tags are given in WPS form, which means word, part
of speech, and sense number. In addition, glosses are
displayed for each of the selected senses.
There are also numerous trace options available,
which can be combined in order to provide more de-
tailed diagnostic output. This includes displaying
the window of context with the center word desig-
nated (1), the winning score for each context win-
dow (2), the non-zero scores for each sense of the
center word (4), the non-zero pairwise scores (8),
the zero values for any of the previous trace levels
(16), and the traces from the semantic relatedness
measures from WordNet::Similarity (32).
4 Experimental Results
We have evaluated SR-AW using three corpora that
have been manually annotated with senses from
WordNet. These include the SemCor corpus, and
19
Table 1: SR-AW Results (%)
2 5 15
SC P R F P R F P R F
lch 56 13 21 54 29 36 52 35 42
jcn 65 15 24 64 31 42 62 41 49
lesk 58 49 53 62 60 61 62 61 61
S2 P R F P R F P R F
lch 48 10 16 50 24 32 48 31 38
jcn 55 9 15 55 21 31 55 31 39
lesk 54 44 48 58 56 57 59 59 59
S3 P R F P R F P R F
lch 48 13 20 49 29 37 48 35 41
jcn 55 14 22 55 31 40 53 38 46
lesk 51 43 47 54 52 53 54 53 54
the SENSEVAL-2 and SENSEVAL-3 corpora. Sem-
Cor is made up of more than 200,000 words of run-
ning text from news articles found in the Brown Cor-
pus. The SENSEVAL data sets are each approxi-
mately 4,000 words of running text from Wall Street
Journal news articles from the Penn Treebank. Note
that only the words known to WordNet in these cor-
pora have been sense tagged. As a result, there are
185,273 sense tagged words in SemCor, 2,260 in
SENSEVAL-2, and 1,937 in SENSEVAL-3. We have
used versions of these corpora where the WordNet
senses have been mapped to WordNet 3.02.
In Table 4 we report results using Precision (P),
Recall (R), and F-Measure (F). We use three window
sizes in these experiments (2, 5, and 15), threeWord-
Net::Similarity measures (lch, jcn, and lesk),and
three different corpora : SemCor (SC), SENSEVAL-
2 (S2), SENSEVAL-3 (S3). These experiments were
carried out with version 0.17 of SR-AW.
For all corpora we observe the same patterns.
The lesk measure tends to result in much higher re-
call with smaller window sizes, since it is able to
measure similarity between words with any parts of
speech, whereas lch and jcn are limited to making
noun-noun and verb-verb measurements. But, as the
window size increases so does recall. Precision con-
tinues to increase for lesk as the window size in-
creases. Our best results come from using the lesk
measure with a window size of 15. For SemCor this
results in an F-measure of 61%. For SENSEVAL-2 it
2http://www.cse.unt.edu/?rada/downloads.html
results in an F-measure of 59%, and for SENSEVAL-
3 it results in an F-measure of 54%. These results
would have ranked 4th of 22 teams and 15th of 26 in
the respective SENSEVAL events.
A well known baseline for all words disambigua-
tion is to assign the first WordNet sense to each am-
biguous word. This results in an F-measure of 76%
for SemCor, 69% for SENSEVAL-2, and 68% for
SENSEVAL-3. A lower bound can be established
by randomly assigning senses to words. This re-
sults in an F-Measure of 41% for SemCor, 41% for
SENSEVAL-2, and 37% for SENSEVAL-3. This is
relatively high due to the large number of words that
have just one possible sense (so randomly selecting
will result in a correct assignment). For example,
in SemCor approximately 20% of the ambiguous
words have just one sense. From these results we
can see that SR-AW lags behind the sense one base-
line (which is common among all words systems),
but significantly outperforms the random baseline.
5 Conclusions
WordNet::SenseRelate::AllWords is a highly flexi-
ble method of word sense disambiguation that of-
fers broad coverage and does not require training of
any kind. It uses WordNet and measures of seman-
tic similarity and relatedness to identify the senses
of words that are most related to each other in a sen-
tence. It is implemented in Perl and is freely avail-
able from the URL on the title page both as source
code and via a Web interface.
References
J. Michelizzi. 2005. Semantic relatedness applied to all
words sense disambiguation. Master?s thesis, Univer-
sity of Minnesota, Duluth, July.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing measures of semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics, pages 241?257, Mexico
City, February.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::Similarity - Measuring the relatedness of
concepts. In Proceedings of Fifth Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics, pages 38?41, Boston, MA.
20
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1255?1265, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Resolving ?This-issue? Anaphora
Varada Kolhatkar
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
varada@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gh@cs.toronto.edu
Abstract
We annotate and resolve a particular
case of abstract anaphora, namely, this-
issue anaphora. We propose a candidate
ranking model for this-issue anaphora
resolution that explores different issue-
specific and general abstract-anaphora
features. The model is not restricted
to nominal or verbal antecedents; rather,
it is able to identify antecedents that
are arbitrary spans of text. Our re-
sults show that (a) the model outperforms
the strong adjacent-sentence baseline;
(b) general abstract-anaphora features,
as distinguished from issue-specific fea-
tures, play a crucial role in this-issue
anaphora resolution, suggesting that our
approach can be generalized for other
NPs such as this problem and this debate;
and (c) it is possible to reduce the search
space in order to improve performance.
1 Introduction
Anaphora in which the anaphoric expression refers
to an abstract object such as a proposition, a prop-
erty, or a fact is known as abstract object anaphora.
This is seen in the following examples.
(1) [Be careful what you wish... because wishes
sometimes come true.]i [That]i is what the
Semiconductor Industry Association, which rep-
resents U.S. manufacturers, has been learning.
(from Asher (1993))
(2) This prospective study suggested [that oral
carvedilol is more effective than oral meto-
prolol in the prevention of AF after on-pump
CABG]i. It is well tolerated when started before
and continued after the surgery. However, further
prospective studies are needed to clarify [this is-
sue]i.
(3) In principle, he said, airlines should be allowed
[to sell standing-room-only tickets for adults]i
? as long as [this decision]i was approved by
their marketing departments.
These examples highlight a difficulty not found with
nominal anaphora. First, the anaphors refer to ab-
stract concepts that can be expressed with differ-
ent syntactic shapes which are usually not nominals.
The anaphor That in (1) refers to the proposition in
the previous utterance, whereas the anaphor this is-
sue in (2) refers to a clause from the previous text.
In (3), the anaphoric expression this decision refers
to a verb phrase from the same sentence. Second,
the antecedents do not always have precisely defined
boundaries. In (2), for example, the whole sentence
containing the marked clause could also be thought
to be the correct antecedent. Third, the actual refer-
ents are not always the precise textual antecedents.
The actual referent in (2), the issue to be clarified,
is whether oral carvedilol is more effective than oral
metoprolol in the prevention of AF after on-pump
CABG or not, a variant of the antecedent text.
Generally, abstract anaphora, as distinguished
from nominal anaphora, is signalled in English by
pronouns this, that, and it (Mu?ller, 2008). But in
abstract anaphora, English prefers demonstratives
to personal pronouns and definite articles (Pas-
sonneau, 1989; Navarretta, 2011).1 Demonstra-
1This is not to say that personal pronouns and definite arti-
cles do not occur in abstract anaphora, but they are not common.
1255
tives can be used in isolation (That in (1)) or with
nouns (e.g., this issue in (2)). The latter follows
the pattern demonstrative {modifier}* noun. The
demonstrative acts as a determiner and the noun fol-
lowing the demonstrative imposes selectional con-
straints for the antecedent, as in examples (2) and
(3). Francis (1994) calls such nouns label nouns,
which ?serve to encapsulate or package a stretch
of discourse?. Schmid (2000) refers to them as
shell nouns, a metaphoric term which reflects differ-
ent functions of these nouns such as encapsulation,
pointing, and signalling.
Demonstrative nouns, along with pronouns like
both and either, are referred to as sortal anaphors
(Castan?o et al 2002; Lin and Liang, 2004; Torii
and Vijay-Shanker, 2007). Castan?o et alobserved
that sortal anaphors are prevalent in the biomedi-
cal literature. They noted that among 100 distinct
anaphors derived from a corpus of 70 Medline ab-
stracts, 60% were sortal anaphors. But how often
do demonstrative nouns refer to abstract objects?
We observed that from a corpus of 74,000 randomly
chosen Medline2 abstracts, of the first 150 most fre-
quently occurring distinct demonstrative nouns (fre-
quency > 30), 51.3% were abstract, 41.3% were
concrete, and 7.3% were discourse deictic. This
shows that abstract anaphora resolution is an impor-
tant component of general anaphora resolution in the
biomedical domain. However, automatic resolution
of this type of anaphora has not attracted much atten-
tion and the previous work for this task is limited.
The present work is a step towards resolving ab-
stract anaphora in written text. In particular, we
choose the interesting abstract concept issue and
demonstrate the complexities of resolving this-issue
anaphora manually as well as automatically in the
Medline domain. We present our algorithm, results,
and error analysis for this-issue anaphora resolution.
The abstract concept issue was chosen for the fol-
lowing reasons. First, it occurs frequently in all
kinds of text from newspaper articles to novels to
scientific articles. There are 13,489 issue anaphora
instances in the New York Times corpus and 1,116
instances in 65,000 Medline abstracts. Second, it is
abstract enough that it can take several syntactic and
2http://www.nlm.nih.gov/bsd/pmresources.
html
semantic forms, which makes the problem interest-
ing and non-trivial. Third, issue referents in scien-
tific literature generally lie in the previous sentence
or two, which makes the problem tractable. Fourth,
issues in Medline abstracts are generally associated
with clinical problems in the medical domain and
spell out the motivation of the research presented in
the article. So extraction of this information would
be useful in any biomedical information retrieval
system.
2 Related Work
Anaphora resolution has been extensively studied
in computational linguistics (Hirst, 1981; Mitkov,
2002; Poesio et al 2011). But CL research has
mostly focused on nominal anaphora resolution
(e.g., resolving multiple ambiguous mentions of a
single entity representing a person, a location, or an
organization) mainly for two reasons. First, nominal
anaphora is the most frequently occurring anaphora
in most domains, and second, there is a substantial
amount of annotated data available for this kind of
anaphora.
Besides pronominal anaphora, some work has
been done on complement anaphora (Modjeska,
2003) (e.g., British and other European steelmak-
ers). There is also some research on resolving sor-
tal anaphora in the medical domain using domain
knowledge (Castan?o et al 2002; Lin and Liang,
2004; Torii and Vijay-Shanker, 2007). But all these
approaches focus only on the anaphors with nominal
antecedents.
By contrast, the area of abstract object anaphora
remains relatively unexplored mainly because the
standard anaphora resolution features such as agree-
ment and apposition cannot be applied to abstract
anaphora resolution. Asher (1993) built a theoreti-
cal framework to resolve abstract anaphora. He di-
vided discourse abstract anaphora into three broad
categories: event anaphora, proposition anaphora,
and fact anaphora, and discussed how abstract en-
tities can be resolved using discourse representa-
tion theory. Chen et al(2011) focused on a sub-
set of event anaphora and resolved event corefer-
ence chains in terms of the representative verbs of
the events from the OntoNotes corpus. Our task dif-
fers from their work as follows. Chen et almainly
1256
focus on events and actions and use verbs as a proxy
for the non-nominal antecedents. But this-issue an-
tecedents cannot usually be represented by a verb.
Our work is not restricted to a particular syntactic
type of the antecedent; rather we provide the flexibil-
ity of marking arbitrary spans of text as antecedents.
There are also some prominent approaches to ab-
stract anaphora resolution in the spoken dialogue
domain (Eckert and Strube, 2000; Byron, 2004;
Mu?ller, 2008). These approaches go beyond nom-
inal antecedents; however, they are restricted to spo-
ken dialogues in specific domains and need serious
adaptation if one wants to apply them to arbitrary
text.
In addition to research on resolution, there is
also some work on effective annotation of abstract
anaphora (Strube and Mu?ller, 2003; Botley, 2006;
Poesio and Artstein, 2008; Dipper and Zinsmeister,
2011). However, to the best of our knowledge, there
is currently no English corpus annotated for issue
anaphora antecedents.
3 Data and Annotation
To create an initial annotated dataset, we collected
188 this {modifier}* issue instances along with the
surrounding context from Medline abstracts.3 Five
instances were discarded as they had an unrelated
(publication related) sense. Among the remaining
183 instances, 132 instances were independently an-
notated by two annotators, a domain expert and a
non-expert, and the remaining 51 instances were an-
notated only by the domain expert. We use the for-
mer instances for training and the latter instances
(unseen by the developer) for testing. The anno-
tator?s task was to mark arbitrary text segments
as antecedents (without concern for their linguistic
types). To make the task tractable, we assumed that
an antecedent does not span multiple sentences but
lies in a single sentence (since we are dealing with
singular this-issue anaphors) and that it is a continu-
ous span of text.
3Although our dataset is rather small, its size is similar to
other available abstract anaphora corpora in English: 154 in-
stances in Eckert and Strube (2000), 69 instances in Byron
(2003), 462 instances annotated by only one annotator in Botley
(2006), and 455 instances restricted to those which have only
nominal or clausal antecedents in Poesio and Artstein (2008).
r11 r12 r13 r14 r15
r21 r22 r23 r24 r25
Annotator 1
Annotator 2
r16 r17 r18 r19
r26 r27 r28 r29 r2,10
id2
Intersections 1 2 3 4 5 6 7 8 9 10 11 12 13 14
id3 id4 id5
id1 id2 id3 id4 id5
Figure 1: Example of annotated data. Bold segments
denote the marked antecedents for the corresponding
anaphor ids. rh j is the jth section identified by the an-
notator h.
3.1 Inter-annotator Agreement
This kind of annotation ? identifying and marking
arbitrary units of text that are not necessarily con-
stituents ? requires a non-trivial variant of the usual
inter-annotator agreement measures. We use Krip-
pendorff?s reliability coefficient for unitizing (?u)
(Krippendorff, 1995) which has not often been used
or described in CL. In our context, unitizing means
marking the spans of the text that serve as the an-
tecedent for the given anaphors within the given text.
The coefficient ?u assumes that the annotated sec-
tions do not overlap in a single annotator?s output
and our data satisfies this criterion.4 The general
form of coefficient ?u is:
?u = 1? u
Do
uDe
(1)
where uDo and uDe are observed and expected dis-
agreements respectively. Both disagreement quanti-
ties express the average squared differences between
the mismatching pairs of values assigned by anno-
tators to given units of analysis. ?u = 1 indicates
perfect reliability and ?u = 0 indicates the absence
of reliability. When ?u < 0, the disagreement is sys-
tematic. Annotated data with reliability of ?u? 0.80
is considered reliable (Krippendorff, 2004).
Krippendorff?s ?u is non-trivial, and explaining it
in detail would take too much space, but the general
idea, in our context, is as follows. The annotators
mark the antecedents corresponding to each anaphor
in their respective copies of the text, as shown in Fig-
ure 1. The marked antecedents are mutually exclu-
sive sections r; we denote the jth section identified
4If antecedents overlap with each other in a single annota-
tor?s output (which is a rare event) we construct data that satis-
fies the non-overlap criterion by creating different copies of the
same text corresponding to each anaphor instance.
1257
Antecedent type Distribution Example
clause 37.9% There is a controversial debate (SBAR whether back school program might improve
quality of life in back pain patients). This study aimed to address this issue.
sentence 26.5% (S Reduced serotonin function and abnormalities in the hypothalamic-pituitary-adrenal
axis are thought to play a role in the aetiology of major depression.) We sought to
examine this issue in the elderly ...
mixed 18.2% (S (PP Given these data) (, ,) (NP decreasing HTD to < or = 5 years) (VP may have
a detrimental effect on patients with locally advanced prostate cancer) (. .)) Only a
randomized trial will conclusively clarify this issue.
nominalization 17.4% As (NP the influence of estrogen alone on breast cancer detection) is not established,
we examined this issue in the Women?s Health Initiative trial...
Table 1: Antecedent types. In examples, the antecedent type is in bold and the marked antecedent is in italics.
by the annotator h by rh j. In Figure 1, annotators 1
and 2 have reached different conclusions by identi-
fying 9 and 10 sections respectively in their copies
of the text. Annotator 1 has not marked any an-
tecedent for the anaphor with id = 1, while annotator
2 has marked r21 for the same anaphor. Both anno-
tators have marked exactly the same antecedent for
the anaphor with id = 4. The difference between two
annotated sections is defined in terms of the square
of the distance between the non-overlapping parts of
the sections. The distance is 0 when the sections are
unmarked by both annotators or are marked and ex-
actly same, and is the summation of the squares of
the unmatched parts if they are different. The coeffi-
cient is computed using intersections of the marked
sections. In Figure 1, annotators 1 and 2 have a to-
tal of 14 intersections. The observed disagreement
uDo is the weighted sum of the differences between
all mismatching intersections of sections marked by
the annotators, and the expected disagreement is the
summation of all possible differences of pairwise
combinations of all sections of all annotators nor-
malized by the length of the text (in terms of the
number of tokens) and the number of pairwise com-
binations of annotators.
For our data, the inter-annotator agreement was
?u = 0.86 (uDo = 0.81 and uDe = 5.81) despite the
fact that the annotators differed in their domain ex-
pertise, which suggests that abstract concepts such
as issue can be annotated reliably.
3.2 Corpus Statistics
A gold standard corpus was created by resolving the
cases where the annotators disagreed. Among 132
training instances, the annotators could not resolve
6 instances and we broke the tie by writing to the
authors of the articles and using their response to
resolve the disagreement. In the gold standard cor-
pus, 95.5% of the antecedents were in the current or
previous sentence and 99.2% were in the current or
previous two sentences. Only one antecedent was
found more than two sentences back and it was six
sentences back. One instance was a cataphor, but
the antecedent occurred in the same sentence as the
anaphor. This suggests that for an automatic this-
issue resolution system, it would be reasonable to
consider only the previous two sentences along with
the sentence containing the anaphor.
The distribution of the different linguistic forms
that an antecedent of this-issue can take in our data
set is shown in Table 1. The majority of antecedents
are clauses or whole sentences. A number of an-
tecedents are noun phrases, but these are gener-
ally nominalizations that refer to abstract concepts
(e.g., the influence of estrogen alone on breast can-
cer detection). Some antecedents are not even well-
defined syntactic constituents5 but are combinations
of several well-defined constituents. We denote the
type of such antecedents as mixed. In the corpus,
18.2% of the antecedents are of this type, suggest-
ing that it is not sufficient to restrict the antecedent
search space to well-defined syntactic constituents.6
In our data, we did not find anaphoric chains for
any of the this-issue anaphor instances, which indi-
cates that the antecedents of this-issue anaphors are
5We refer to every syntactic constituent identified by the
parser as a well-defined syntactic constituent.
6Indeed, many of mixed type antecedents (nearly three-
quarters of them) are the result of parser attachment errors, but
many are not.
1258
in the reader?s local memory and not in the global
memory. This observation supports the THIS-NPs
hypothesis (Gundel et al 1993; Poesio and Mod-
jeska, 2002) that this-NPs are used to refer to enti-
ties which are active albeit not in focus, i.e., they are
not the center of the previous utterance.
4 Resolution Algorithm
4.1 Candidate Extraction
For correct resolution, the set of extracted candidates
must contain the correct antecedent in the first place.
The problem of candidate extraction is non-trivial in
abstract anaphora resolution because the antecedents
are of many different types of syntactic constituents
such as clauses, sentences, and nominalizations.
Drawing on our observation that the mixed type an-
tecedents are generally a combination of different
well-defined syntactic constituents, we extract the
set of candidate antecedents as follows. First, we
create a set of candidate sentences which contains
the sentence containing the this-issue anaphor and
the two preceding sentences. Then, we parse every
candidate sentence with the Stanford Parser7. Ini-
tially, the set of candidate constituents contains a
list of well-defined syntactic constituents. We re-
quire that the node type of these constituents be in
the set {S, SBAR, NP, SQ, SBARQ, S+V}. This
set was empirically derived from our data. To each
constituent, there is associated a set of mixed type
constituents. These are created by concatenating the
original constituent with its sister constituents. For
example, in (4), the set of well-defined eligible can-
didate constituents is {NP, NP1} and so NP1 PP1 is
a mixed type candidate.
(4) NP
NP1 PP1 PP2
The set of candidate constituents is updated with
the extracted mixed type constituents. Extracting
mixed type candidate constituents not only deals
with mixed type instances as shown in Table 1, but
as a side effect it also corrects some attachment er-
rors made by the parser. Finally, the constituents
7http://nlp.stanford.edu/software/
lex-parser.shtml
having a number of leaves (words) less than a thresh-
old8 are discarded to give the final set of candidate
constituents.
4.2 Features
We explored the effect of including 43 automati-
cally extracted features (12 feature classes), which
are summarized in Table 2. The features can also be
broadly divided into two groups: issue-specific fea-
tures and general abstract-anaphora features. Issue-
specific features are based on our common-sense
knowledge of the concept of issue and the different
semantic forms it can take; e.g., controversy (X is
controversial), hypothesis (It has been hypothesized
X), or lack of knowledge (X is unknown), where X
is the issue. In our data, we observed certain syn-
tactic patterns of issues such as whether X or not
and that X and the IP feature class encodes this in-
formation. Other issue-specific features are IVERB
and IHEAD. The feature IVERB checks whether
the governing verb of the candidate is an issue
verb (e.g., speculate, hypothesize, argue, debate),
whereas IHEAD checks whether the candidate head
in the dependency tree is an issue word (e.g., contro-
versy, uncertain, unknown). The general abstract-
anaphora resolution features do not make use of
the semantic properties of the word issue. Some
of these features are derived empirically from the
training data (e.g., ST, L, D). The EL feature is bor-
rowed from Mu?ller (2008) and encodes the embed-
ding level of the candidate within the candidate sen-
tence. The MC feature tries to capture the idea of the
THIS-NPs hypothesis (Gundel et al 1993; Poesio
and Modjeska, 2002) that the antecedents of this-
NP anaphors are not the center of the previous utter-
ance. The general abstract-anaphora features in the
SR feature class capture the semantic role of the can-
didate in the candidate sentence. We used the Illinois
Semantic Role Labeler9 for SR features. The gen-
eral abstract-anaphora features also contain a few
lexical features (e.g., M, SC). But these features are
independent of the semantic properties of the word
issue. The general abstract-anaphora resolution fea-
tures also contain dependency-tree features, lexical-
8The threshold 5 was empirically derived. Antecedents in
our training data had on average 17 words.
9http://cogcomp.cs.illinois.edu/page/
software_view/SRL
1259
ISSUE PATTERN (IP)
ISWHETHER 1 iff the candidate follows the pattern SBAR? (IN whether) (S ...)
ISTHAT 1 iff the candidate follows the pattern SBAR? (IN that) (S ...)
ISIF 1 iff the candidate follows the pattern SBAR? (IN iff) (S ...)
ISQUESTION 1 iff the candidate node is SBARQ or SQ
SYNTACTIC TYPE (ST)
ISNP 1 iff the candidate node is of type NP
ISS 1 iff the candidate node is a sentence node
ISSBAR 1 iff the candidate node is an SBAR node
ISSQ 1 iff the candidate node is an SQ or SBARQ node
MIXED 1 iff the candidate node is of type mixed
EMBEDDING LEVEL (EL) (Mu?ller, 2008)
TLEMBEDDING level of embedding of the given candidate in its top clause (the root node of the syntactic tree)
ILEMBEDDING level of embedding of the given candidate in its immediate clause (the closest parent of type S or SBAR)
MAIN CLAUSE (MC)
MCLAUSE 1 iff the candidate is in the main clause
DISTANCE (D)
ISSAME 1 iff the candidate is in the same sentence as anaphor
SADJA 1 iff the candidate is in the adjacent sentence
ISREM 1 iff the candidate occurs 2 or more sentences before the anaphor
POSITION 1 iff the antecedent occurs before anaphor
SEMANTIC ROLE LABELLING (SR)
IVERB 1 iff the governing verb of the given candidate is an issue verb
ISA0 1 iff the candidate is the agent of the governing verb
ISA1 1 iff the candidate is the patient of the governing verb
ISA2 1 iff the candidate is the instrument of the governing verb
ISAM 1 iff the candidate plays the role of modiffication
ISNOR 1 iff the candidate plays no well-defined semantic role in the sentence
DEPENDENCY TREE (DT)
IHEAD 1 iff the candidate head in the dependency tree is an issue word (e.g., controversial, unknown)
ISSUBJ 1 iff the dependency relation of the candidate to its head is of type nominal, controlling or clausal subject
ISOBJ 1 iff the dependency relation of the candidate to its head is of type direct object or preposition obj
ISDEP 1 iff the dependency relation of the candidate to its head is of type dependent
ISROOT 1 iff the candidate is the root of the dependency tree
ISPREP 1 iff the dependency relation of the candidate to its head is of type preposition
ISCONT 1 iff the dependency relation of the candidate to its head is of type continuation
ISCOMP 1 iff the dependency relation of the candidate to its head is of type clausal or adjectival complement
ISSENT 1 iff candidate?s head is the root node
PRESENCE OF MODALS (M)
MODAL 1 iff the given candidate contains a modal verb
PRESENCE OF SUBORDINATING CONJUNCTION (SC)
ISCONT 1 iff the candidate starts with a contrastive subordinating conjunction (e.g., however, but, yet)
ISCAUSE 1 iff the candidate starts with a causal subordinating conjunction (e.g., because, as, since)
ISCOND 1 iff the candidate starts with a conditional subordinating conjunction (e.g., if, that, whether or not)
LEXICAL OVERLAP (LO)
TOS normalized ratio of the overlapping words in candidate and the title of the article
AOS normalized ratio of the overlapping words in candidate and the anaphor sentence
DWS proportion of domain-specific words in the candidate
CONTEXT (C)
ISPPREP 1 iff the preceding word of the candidate is a preposition
ISFPREP 1 iff the following word of the candidate is a preposition
ISPPUNCT 1 iff the preceding word of the candidate is a punctuation
ISFPUNCT 1 iff the following word of the candidate is a punctuation
LENGTH (L)
LEN length of the candidate in words
Table 2: Feature sets for this-issue resolution. All features are extracted automatically.
1260
overlap features, and context features.
4.3 Candidate Ranking Model
Given an anaphor ai and a set of candidate
antecedents C = {C1,C2, ...,Ck}, the problem of
anaphora resolution is to choose the best candidate
antecedent for ai. We follow the candidate-ranking
model proposed by Denis and Baldridge (2008).
The advantage of the candidate-ranking model over
the mention-pair model is that it overcomes the
strong independence assumption made in mention-
pair models and evaluates how good a candidate is
relative to all other candidates.
We train our model as follows. If the anaphor
is a this-issue anaphor, the set C is extracted us-
ing the candidate extraction algorithm from Section
4.1. Then a corresponding set of feature vectors,
C f = {C f 1,C f 2, ...,C f k}, is created using the features
in Table 2. The training instances are created as de-
scribed by Soon et al(2001). Note that the instance
creation is simpler than for general coreference res-
olution because of the absence of anaphoric chains
in our data. For every anaphor ai and eligible can-
didates C f = {C f 1,C f 2, ...,C f k}, we create training
examples (ai,C f i, label),?C f i ? C f . The label is 1
if Ci is the true antecedent of the anaphor ai, oth-
erwise the label is ?1. The examples with label 1
get the rank of 1, while other examples get the rank
of 2. We use SVMrank (Joachims, 2002) for train-
ing the candidate-ranking model. During testing, the
trained model is used to rank the candidates of each
test instance of this-issue anaphor.
5 Evaluation
In this section we present the evaluation of each
component of our resolution system.
5.1 Evaluation of Candidate Extraction
The set of candidate antecedents extracted by the
method from Section 4.1 contained the correct an-
tecedent 92% of the time. Each anaphor had, on
average, 23.80 candidates, of which only 5.19 can-
didates were nominal type. The accuracy dropped
to 84% when we did not extract mixed type candi-
dates. The error analysis of the 8% of the instances
where we failed to extract the correct antecedent re-
vealed that most of these errors were parsing errors
which could not be corrected by our candidate ex-
traction method.10 In these cases, the parts of the
antecedent had been placed in completely different
branches of the parse tree. For example, in (5), the
correct antecedent is a combination of the NP from
the S? V P? NP? PP? NP branch and the PP
from S?V P? PP branch. In such a case, concate-
nating sister constituents does not help.
(5) The data from this pilot study (VP (VBP provide)
(NP (NP no evidence) (PP (IN for) (NP a dif-
ference in hemodynamic effects between pulse
HVHF and CPFA))) (PP in patients with sep-
tic shock already receiving CRRT)). A larger
sample size is needed to adequately explore this
issue.
5.2 Evaluation of this-issue Resolution
We propose two metrics for abstract anaphora eval-
uation. The simplest metric is the percentage of an-
tecedents on which the system and the annotated
gold data agree. We denote this metric as EXACT-
M (Exact Match) and compute it as the ratio of
number of correctly identified antecedents to the to-
tal number of marked antecedents. This metric is
a good indicator of a system?s performance; how-
ever, it is a rather strict evaluation because, as we
noted in section 1, issues generally have no precise
boundaries in the text. So we propose another met-
ric called RLL, which is similar to the ROUGE-L
metric (Lin, 2004) used for the evaluation of auto-
matic summarization. Let the marked antecedents
of the gold corpus for k anaphor instances be G =
?g1,g2, ...,gk? and the system-annotated antecedents
be A = ?a1,a2, ...,ak?. Let the number of words in
G and A be m and n respectively. Let LCS(gi,ai)
be the the number of words in the longest common
subsequence of gi and ai. Then the precision (PRLL)
and recall (RRLL) over the whole data set are com-
puted as shown in equations (2) and (3). PRLL is
the total number of word overlaps between the gold
and system-annotated antecedents normalized by the
number of words in system-annotated antecedents
and RRLL is the total number of such word overlaps
normalized by the number of words in the gold an-
tecedents. If the system picks too much text for an-
tecedents, RRLL is high but PRLL is low. The F-score,
10Extracting candidate constituents from the dependency
trees did not add any new candidates to the set of candidates.
1261
5-fold Cross-Validation Test
PRLL RRLL FRLL EX-M PRLL RRLL FRLL EX-M
1 Adjacent sentence 66.47 86.16 74.93 22.93 61.73 87.69 72.46 24.00
2 Random 50.71 32.84 39.63 8.40 43.75 35.00 38.89 15.69
3 {IP, D, C, LO, EL, M, MC, L, SC, SR, DT} 79.37 83.66 81.11 59.80 71.89 85.74 78.20 58.82
4 {IP, D, C, LO, M, MC, L, SC, DT} 78.71 83.86 81.14 59.89 70.64 88.09 78.40 54.90
5 {IP, D, C, EL, L, SC, SR, DT} 77.95 83.06 80.33 57.41 72.03 84.85 77.92 60.78
6 {IP, D, EL, MC, L, SR, DT} 80.00 84.75 82.24 59.91 68.88 85.29 76.22 56.86
7 {IP, D, M, L, SR} 73.42 83.16 77.90 52.31 70.74 91.03 79.61 50.98
8 {D, C, LO, L, SC, SR, DT} 79.15 85.28 82.04 56.07 67.39 86.32 75.69 52.94
9 issue-specific features 74.66 45.70 56.57 41.42 64.20 45.88 53.52 41.38
10 non-issue features 76.39 79.39 77.82 51.48 71.19 83.24 76.75 58.82
11 All 78.22 82.92 80.41 56.75 71.28 83.24 76.80 56.86
12 Oracle candidate extractor + row 3 79.63 82.26 80.70 58.32 74.65 87.06 80.38 64.71
13 Oracle candidate sentence extractor + row 3 86.67 92.12 89.25 63.72 79.71 91.49 85.20 62.00
Table 3: this-issue resolution results with SVMrank. All means evaluation using all features. Issue-specific features =
{IP, IVERB, IHEAD}. EX-M is EXACT-M.
FRLL, combines these two scores.
PRLL =
1
n
k
?
i=1
LCS(gi,ai) (2)
RRLL =
1
m
k
?
i=1
LCS(gi,ai) (3)
FRLL =
2?PRLL?RRLL
PRLL +RRLL
(4)
The lower bound of FRLL is 0, where no true an-
tecedent has any common substring with the pre-
dicted antecedents and the upper bound is 1, where
all the predicted and true antecedents are exactly the
same. In our results we represent these scores in
terms of percentage.
There are no implemented systems that resolve is-
sue anaphora or abstract anaphora signalled by label
nouns in arbitrary text to use as a comparison. So
we compare our results against two baselines: ad-
jacent sentence and random. The adjacent sentence
baseline chooses the previous sentence as the correct
antecedent. This is a high baseline because in our
data 84.1% of the antecedents lie within the adjacent
sentence. The random baseline chooses a candidate
drawn from a uniform random distribution over the
set of candidates.11
11Note that our FRLL scores for both baselines are rather high
because candidates often have considerable overlap with one
another; hence a wrong choice may still have a high FRLL score.
We carried out two sets of systematic experi-
ments in which we considered all combinations of
our twelve feature classes. The first set consists of
5-fold cross-validation experiments on our training
data. The second set evaluates how well the model
built on the training data works on the unseen test
data.
Table 3 gives results of our system. The first two
rows are the baseline results. Rows 3 to 8 give re-
sults for some of the best performing feature sets.
All systems based on our features beat both base-
lines on F-scores and EXACT-M. The empirically
derived feature sets IP (issue patterns) and D (dis-
tance) appeared in almost all best feature set com-
binations. Removing D resulted in a 6 percentage
points drop in FRLL and a 4 percentage points drop
in EXACT-M scores. Surprisingly, feature set ST
(syntactic type) was not included in most of the best
performing set of feature sets. The combination of
syntactic and semantic feature sets {IP, D, EL, MC,
L, SR, DT} gave the best FRLL and EXACT-M scores
for the cross-validation experiments. For the test-
data experiments, the combination of semantic and
lexical features {D, C, LO, L, SC, SR, DT} gave
the best FRLL results, whereas syntactic, discourse,
and semantic features {IP, D, C, EL, L, SC, SR,
DT} gave the best EXACT-M results. Overall, row
3 of the table gives reasonable results for both cross-
validation and test-data experiments with no statisti-
cally significant difference to the corresponding best
1262
EXACT-M scores in rows 6 and 5 respectively.12
To pinpoint the errors made by our system, we
carried out three experiments. In the first experi-
ment, we examined the contribution of issue-specific
features versus non-issue features (rows 9 and 10).
Interestingly, when we used only non-issue features,
the performance dropped only slightly. The FRLL re-
sults from using only issue-specific features were
below baseline, suggesting that the more general
features associated with abstract anaphora play a
crucial role in resolving this-issue anaphora.
In the second experiment, we determined the er-
ror caused by the candidate extractor component of
our system. Row 12 of the table gives the result
when an oracle candidate extractor was used to add
the correct antecedent in the set of candidates when-
ever our candidate extractor failed. This did not
affect cross-validation results by much because of
the rarity of such instances. However, in the test-
data experiment, the EXACT-M improvements that
resulted were statistically significant. This shows
that our resolution algorithm was able to identify an-
tecedents that were arbitrary spans of text.
In the last experiment, we examined the effect of
the reduction of the candidate search space. We as-
sumed an oracle candidate sentence extractor (Row
13) which knows the exact candidate sentence in
which the antecedent lies. We can see that both
RLL and EXACT-M scores markedly improved in
this setting. In response to these results, we trained
a decision-tree classifier to identify the correct an-
tecedent sentence with simple location and length
features and achieved 95% accuracy in identifying
the correct candidate sentence.
6 Discussion and Conclusions
We have demonstrated the possibility of resolv-
ing complex abstract anaphora, namely, this-issue
anaphora having arbitrary antecedents. The work
takes the annotation work of Botley (2006) and Dip-
per and Zinsmeister (2011) to the next level by re-
solving this-issue anaphora automatically. We pro-
posed a set of 43 automatically extracted features
that can be used for resolving abstract anaphora.
12We performed a simple one-tailed, k-fold cross-validated
paired t-test at significance level p = 0.05 to determine whether
the difference between the EXACT-M scores of two feature
classes is statistically significant.
Our results show that general abstract-anaphora
resolution features (i.e., other than issue-specific
features) play a crucial role in resolving this-issue
anaphora. This is encouraging, as it suggests that
the approach could be generalized for other NPs ?
especially NPs having similar semantic constraints
such as this problem, this decision, and this conflict.
The results also show that reduction of search
space markedly improves the resolution perfor-
mance, suggesting that a two-stage process that first
identifies the broad region of the antecedent and then
pinpoints the exact antecedent might work better
than the current single-stage approach. The rationale
behind this two-stage process is twofold. First, the
search space of abstract anaphora is large and noisy
compared to nominal anaphora.13 And second, it is
possible to reduce the search space and accurately
identify the broad region of the antecedents using
simple features such as the location of the anaphor
in the anaphor sentence (e.g., if the anaphor occurs
at the beginning of the sentence, the antecedent is
most likely present in the previous sentence).
We chose scientific articles over general text be-
cause in the former domain the actual referents are
seldom discourse deictic (i.e., not present in the
text). In the news domain, for instance, which we
have also examined and are presently annotating, a
large percentage of this-issue antecedents lie out-
side the text. For example, newspaper articles often
quote sentences of others who talk about the issues
in their own world, as shown in example (6).
(6) As surprising and encouraging to organizers of
the movement are the Wall Street names added
to their roster. Prominent among them is Paul
Singer, a hedge fund manager who is straight
and chairman of the conservative Manhattan
Institute. He has donated more than $8 million
to various same-sex marriage efforts, in states
including California, Maine, New Hampshire,
New Jersey, New York and Oregon, much of it
since 2007.
?It?s become something that gradually peo-
13If we consider all well-defined syntactic constituents of a
sentence as issue candidates, in our data, a sentence has on av-
erage 43.61 candidates. Combinations of several well-defined
syntactic constituents only add to this number. Hence if we
consider the antecedent candidates from the previous 2 or 3 sen-
tences, the search space can become quite large and noisy.
1263
ple like myself weren?t afraid to fund, weren?t
afraid to speak out on,? Mr. Singer said in an in-
terview. ?I?m somebody who is philosophically
very conservative, and on this issue I thought
that this really was important on the basis of
liberty and actual family stability.?
In such a case, the antecedent of this issue is not
always in the text of the newspaper article itself, but
must be inferred from the context of the quotation
and the world of the speaker quoted. That said, we
do not use any domain-specific information in our
this-issue resolution model. Our features are solely
based on distance, syntactic structure, and semantic
and lexical properties of the candidate antecedents
which could be extracted for text in any domain.
Issue anaphora can also be signalled by demon-
stratives other than this. However, for our initial
study, we chose this issue for two reasons. First, in
our corpus as well as in other general corpora such
as the New York Times corpus, issue occurs much
more frequently with this than other demonstratives.
Second, we did not want to increase the complexity
of the problem by including the plural issues.
Our approach needs further development to make
it useful. Our broad goal is to resolve abstract
anaphora signalled by label nouns in all kinds of
text. At present, the major obstacle is that there
is very little annotated data available that could be
used to train an abstract anaphora resolution sys-
tem. And the understanding of abstract anaphora
itself is still at an early stage; it would be prema-
ture to think about unsupervised approaches. In this
work, we studied the narrow problem of resolution
of this-issue anaphora in the medical domain to get
a good grasp of the general abstract-anaphora reso-
lution problem.
A number of extensions are planned for this work.
First, we will extend the work to resolve other ab-
stract anaphors (e.g., this decision, this problem).
Second, we will experiment with a two-stage reso-
lution approach. Third, we would like to explore the
effect of including serious discourse structure fea-
tures in our model. (The feature sets SC and C en-
code only shallow discourse information.) Finally,
during annotation, we noted a number of issue pat-
terns (e.g., An open question is X, X is under debate);
a possible extension is extracting issues and prob-
lems from text using these patterns as seed patterns.
7 Acknowledgements
We thank Dr. Brian Budgell from the Canadian
Memorial Chiropractic College for annotating our
data and for helpful discussions. We also thank
the anonymous reviewers for their detailed and con-
structive comments. This research was financially
supported by the Natural Sciences and Engineering
Research Council of Canada and by the University
of Toronto.
References
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers, Dordrecht,
Netherlands.
Philip Simon Botley. 2006. Indirect anaphora: Testing
the limits of corpus-based linguistics. International
Journal of Corpus Linguistics, 11(1):73?112.
Donna K. Byron. 2003. Annotation of pronouns and
their antecedents: A comparison of two domains.
Technical Report, University of Rochester.
Donna K. Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, Rochester, New
York: University of Rochester.
Jose? Castan?o, Jason Zhang, and James Pustejovsky.
2002. Anaphora resolution in biomedical literature. In
Proceedings of the International Symposium on Refer-
ence Resolution for NLP, Alicante, Spain, June.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.
2011. A unified event coreference resolution by inte-
grating multiple resolvers. In Proceedings of 5th Inter-
national Joint Conference on Natural Language Pro-
cessing, Chiang Mai, Thailand, November.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669,
Honolulu, Hawaii, October. Association for Computa-
tional Linguistics.
Stefanie Dipper and Heike Zinsmeister. 2011. Annotat-
ing abstract anaphora. Language Resources and Eval-
uation, 69:1?16.
Miriam Eckert and Michael Strube. 2000. Dialogue acts,
synchronizing units, and anaphora resolution. Journal
of Semantics, 17:51?89.
Gill Francis. 1994. Labelling discourse: an aspect
of nominal group lexical cohesion. In Malcolm
Coulthard, editor, Advances in written text analysis,
pages 83?101, London. Routledge.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
1264
pressions in discourse. Language, 69(2):274?307,
June.
Graeme Hirst. 1981. Anaphora in Natural Language Un-
derstanding: A Survey, volume 119 of Lecture Notes
in Computer Science. Springer.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD),
pages 133?142.
Klaus Krippendorff. 1995. On the reliability of unitizing
contiguous data. Sociological Methodology, 25:47?
76.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to Its Methodology. Sage, Thousand Oaks,
CA, second edition.
Yu-Hsiang Lin and Tyne Liang. 2004. Pronominal and
sortal anaphora resolution for biomedical literature. In
Proceedings of ROCLING XVI: Conference on Com-
putational Linguistics and Speech Processing, Taiwan,
September.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81, Barcelona, Spain, July. Association for
Computational Linguistics.
Ruslan Mitkov. 2002. Anaphora Resolution. Longman.
Natalia N. Modjeska. 2003. Resolving Other-Anaphora.
Ph.D. thesis, School of Informatics, University of Ed-
inburgh.
Christoph Mu?ller. 2008. Fully Automatic Resolution of
It, This and That in Unrestricted Multi-Party Dialog.
Ph.D. thesis, Universita?t Tu?bingen.
Costanza Navarretta. 2011. Antecedent and referent
types of abstract pronominal anaphora. In Proceed-
ings of the Workshop Beyond Semantics: Corpus-
based investigations of pragmatic and discourse phe-
nomena, Go?ttingen, Germany, February.
Rebecca Passonneau. 1989. Getting at discourse refer-
ents. In Proceedings of the 27th Annual Meeting of
the Association for Computational Linguistics, pages
51?59, Vancouver, British Columbia, Canada, June.
Association for Computational Linguistics.
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proceedings of
the Sixth International Conference on Language Re-
sources and Evaluation (LREC?08), Marrakech, Mo-
rocco, May.
Massimo Poesio and Natalia N. Modjeska. 2002.
The THIS-NPs hypothesis: A corpus-based investiga-
tion. In Proceedings of the 4th Discourse Anaphora
and Anaphor Resolution Conference (DAARC 2002),
pages 157?162, Lisbon, Portugal, September.
Massimo Poesio, Simone Ponzetto, and Yannick Versley.
2011. Computational models of anaphora resolution:
A survey. Unpublished.
Hans-Jo?rg Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Topics
in English Linguistics. De Gruyter Mouton, Berlin.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages
168?175, Sapporo, Japan, July. Association for Com-
putational Linguistics.
Manabu Torii and K. Vijay-Shanker. 2007. Sortal
anaphora resolution in Medline abstracts. Computa-
tional Intelligence, 23(1):15?27.
1265
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 300?310,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Interpreting Anaphoric Shell Nouns using Antecedents of
Cataphoric Shell Nouns as Training Data
Varada Kolhatkar
Department of Computer Science
University of Toronto
varada@cs.toronto.edu
Heike Zinsmeister
Institut fu?r Germanistik
Universita?t Hamburg
heike.zinsmeister@uni-hamburg.de
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
Interpreting anaphoric shell nouns
(ASNs) such as this issue and this fact
is essential to understanding virtually
any substantial natural language text.
One obstacle in developing methods
for automatically interpreting ASNs is
the lack of annotated data. We tackle
this challenge by exploiting cataphoric
shell nouns (CSNs) whose construction
makes them particularly easy to interpret
(e.g., the fact that X). We propose an ap-
proach that uses automatically extracted
antecedents of CSNs as training data to
interpret ASNs. We achieve precisions
in the range of 0.35 (baseline = 0.21) to
0.72 (baseline = 0.44), depending upon
the shell noun.
1 Introduction
Anaphors such as this fact and this issue encapsulate
complex abstract entities such as propositions, facts,
and events. An example is shown below.
(1) Here is another bit of advice: Environmental
Defense, a national advocacy group, notes that
?Mowing the lawn with a gas mower produces
as much pollution in half an hour as driving a
car 172 miles.? This fact may help to explain
the recent surge in the sales of the good old-
fashioned push mowers or the battery-powered
mowers.
Here, the anaphor this fact is interpreted with the
help of the clausal antecedent marked in bold. The
antecedent here is complex because it involves a
number of entities and events (e.g., mowing the
lawn, a gas mower) and relationships between them,
and is abstract because the antecedent itself is not a
purely physical entity.
The distinguishing property of these anaphors is
that they contain semantically rich abstract nouns
(e.g., fact in (1)) which characterize and label their
corresponding antecedents. Linguists and philoso-
phers have studied such abstract nouns for decades
(Vendler, 1968; Halliday and Hasan, 1976; Francis,
1986; Ivanic, 1991; Asher, 1993). Our work is in-
spired by one such study, namely that of Schmid
(2000). Following Schmid, we refer to these abstract
nouns as shell nouns, as they serve as conceptual
shells for complex chunks of information. Accord-
ingly, we refer to the anaphoric occurrences of shell
nouns (e.g., this fact in (1)) as anaphoric shell nouns
(ASNs).
An important reason for studying ASNs is their
ubiquity in all kinds of text. Schmid (2000) ob-
served that shell nouns such as fact, idea, point, and
problem were among the 100 most frequently oc-
curring nouns in a corpus of 225 million words of
British English. Moreover, ASNs can play several
roles in organizing a discourse such as encapsulation
of complex information, cohesion, and topic bound-
ary marking. So correct interpretation of ASNs can
be an important step for correct interpretation of a
discourse, and in a number of NLP applications such
as text summarization, information extraction, and
non-factoid question answering.
Despite their importance, ASNs have not received
much attention in Computational Linguistics, and
research in this field remains in its earliest stages. At
300
present, the major obstacle is that there is very little
annotated data available that could be used to train a
supervised machine learning system for robustly in-
terpreting these anaphors, and manual annotation is
an expensive and time-consuming task.
We tackle this challenge by exploiting a category
of examples, as shown in (2), whose construction is
particularly easy to interpret.
(2) Congress has focused almost solely on the fact
that special education is expensive ? and that
it takes away money from regular education.
Here, in contrast with (1), the fact is not anaphoric
in the traditional sense, but is an easy case of a
forward-looking anaphor ? a cataphor. While the
resolution process of this fact in (1) is quite chal-
lenging as it requires the use of semantics and world
knowledge, it is fairly easy to interpret the fact in
(2) based on the syntactic structure alone. We refer
to these easy-to-interpret cataphoric occurrences of
shell nouns as cataphoric shell nouns (CSNs). The
interpretation of both ASNs and CSNs will be re-
ferred to as antecedent.1 The antecedent of the fact
in (2) is given in the post-nominal that clause. We
use the term shell concept to refer to the general no-
tion of a shell noun, i.e., the semantic type of the
antecedent. For example, the notion of an issue is an
important problem which requires a solution.
In this work, we propose an approach to interpret
ASNs that exploits unlabelled but easy-to-interpret
CSN examples to extract characteristic features as-
sociated with the antecedent of different shell con-
cepts. We evaluate our approach using crowdsourc-
ing. Our results show that these unlabelled CSN ex-
amples provide useful linguistic properties that help
in interpreting ASNs.
2 Related work
The resolution of anaphors to non-nominal an-
tecedents has been well analyzed taking discourse
structure and semantic types into account (Web-
ber, 1991; Passonneau, 1989; Asher, 1993). Most
work in machine anaphora resolution, however,
is restricted to anaphora that involve nominal an-
tecedents only (Poesio et al, 2011).
1Sadly, the more-logical term for the interpretation of a
CSN, succedent, does not actually exist.
There are some notable exceptions which have
tackled the challenge of interpreting non-nominal
antecedents (Eckert and Strube, 2000; Strube and
Mu?ller, 2003; Byron, 2004; Mu?ller, 2008). These
approaches are limited as they either rely heavily
on domain-specific syntactic and semantic annota-
tion or prepossessing, or mark only verbal proxies
for non-nominal antecedents.
Recently, Kolhatkar and Hirst (2012) presented
a machine-learning based resolution system for this
issue anaphora, identifying full syntactic phrases as
antecedents. Although they achieved promising re-
sults, their approach was limited in two respects.
First, it focused on only one type of shell noun
anaphora (issue anaphora). Second, their training
data was restricted to MEDLINE abstracts in which
this issue is used in a rather systematic way. Further-
more, their work is based on manually labelled ASN
antecedents, whereas we use automatically identi-
fied CSN antecedents, which we interpret as explic-
itly expressed antecedents in comparison to the more
implicitly expressed ASN antecedents.
Using explicitly expressed structure in the text
to identify implicit structure is not new. The same
idea has been applied before in computational lin-
guistics. Marcu and Echihabi (2002) identified im-
plicit discourse relations using explicit ones. Mark-
ert and Nissim (2005) used Hearst?s (1992) explicit
patterns to learn lexical semantic relations for NP-
coreference and other-anaphora resolution from the
web. Although our work focuses on a different topic,
the methodology is in the same vein.
3 Hypothesis of this work
The hypothesis of this work is that CSN antecedents
and ASN antecedents share some linguistic proper-
ties and hence linguistic knowledge encoded in CSN
antecedents will help in interpreting ASNs. Accord-
ingly, we examine which features present in CSN
antecedents are relevant in interpreting ASNs.
The motivation and intuition behind this hypoth-
esis is as follows. The antecedents of both ASNs
and CSNs represent the corresponding shell con-
cept. So are there any characteristic features asso-
ciated with this shell concept? Do speakers of En-
glish follow certain patterns of syntactic shape or
words, for instance, when they state facts, decisions,
301
The CSN corpus  (211,722 instances) The ASN corpus  (2,323 instances) 
Training 
Training data 
CSN antecedent models  
CSN antecedent extractor Ranked ASN antecedent candidates 
Crowdsourcing evaluation 
Testing 
Figure 1: Overview of our approach
or issues? There is an abundance of data for CSN
antecedents and if we are able to capture particu-
lar linguistic characteristic features associated with
a shell concept using this data, we can use this in-
formation to interpret ASNs. For instance, exam-
ple (2) demonstrates characteristic properties of an-
tecedents of the shell noun fact including that (a)
they are propositions and are generally expressed
with clauses or sentences rather than noun phrases,
and (b) they are generally expressed in the present
tense. Observe that these properties also hold for
the antecedent of this fact in example (1).
We test our hypothesis by building machine learn-
ing models that are trained on automatically ex-
tracted CSN antecedents and then applying these
models to recover ASN antecedents. Figure 1 shows
an overview of our methodology.
4 Background
Formal definition Shell-nounhood is a functional
notion; it is defined by the use of an abstract noun
rather than the inherent properties of the noun itself
(Schmid, 2000). An abstract noun is a shell noun
when the speaker decides to use it as a shell noun.
Shell noun categorization Schmid (2000) gives
a list of 670 English nouns which are frequently
used as shell nouns. He divides them into six
broad semantic classes: factual, linguistic, mental,
modal, eventive, and circumstantial. Table 1 shows
this classification, along with example shell nouns
for each category. For this work, we selected six
frequently occurring shell nouns covering four of
Schmid?s six classes: fact and reason from factual,
issue and decision from mental, question from lin-
guistic, and possibility from modal. These shell
nouns tend to have antecedents that lie within a sin-
gle sentence. We excluded eventive and circumstan-
Class Description Examples
factual states of affairs fact, reason
linguistic linguistic acts question
mental ideas issue, decision
modal judgements possibility
eventive events act, reaction
circumstantial situations situation, way
Table 1: Schmid?s classification of shell nouns. The
nouns given in the Example column tend to occur fre-
quently with the respective class. The shell nouns used in
this work are shown in boldface.
Pattern Example
N-to Several people at the group said the deci-
sion to write the letters was not controver-
sial internally.
N-be-to The principal reason is to create a repre-
sentative government rather than to select
the most talented person.
N-that Mr. Shoval left open the possibility that
Israel would move into other West Bank
cities.
N-be-that The simple and reassuring fact is that a
future generation of leaders is seeking new
challenges during challenging times.
N-wh There is now some question whether the
country was ever really in a recession.
N-be-wh Of course, the central, and probably in-
soluble, issue is whether animal testing is
cruel.
Table 2: Easy-to-interpret CSN patterns given by Schmid
(2000). In the Example column, the patterns are marked
in boldface and the antecedents are marked in italics.
tial classes because the shell nouns in these classes
tend to have rather unclear and long antecedents.2
Shell noun patterns Schmid (2000) also provides
a number of lexico-grammatical patterns for shell
nouns. In Section 1, we noted two such patterns:
this-N (this fact in example (1)) and N-that (fact that
in example (2)). We also noted that CSNs with pat-
tern N-that are fairly easy to interpret compared to
the ASN pattern this-N. Table 2 shows some other
easy-to-interpret CSN patterns given by Schmid.
Generally, for all these patterns, the antecedent is
2These observations are based on an exploratory pilot anno-
tation we carried out on sample data of 150 ASN instances.
302
quite easy to extract with a few predefined rules.
Shell antecedent properties Antecedents of
CSNs and ASNs share some properties while they
are distinguished by others. The distinguishing
property is that CSNs, by their construction, have
their antecedents in the same sentence, as shown
in example (2). On the other hand, ASNs can
have long-distance as well as short-distance an-
tecedents.3 The common properties are as follows.
First, antecedents of both ASNs and CSNs represent
the corresponding shell concept, e.g., the notion
of a fact or an issue. Second, in both cases, the
antecedents are complex abstract entities, which
involve a number of entities and relationships
between them. Finally, in both cases, there is no
one-to-one correspondence between the syntactic
type of an antecedent and semantic type of its
referent (Webber, 1991). For instance, a semantic
type such as fact can be expressed with different
syntactic shapes such as a clause, a verb phrase, or
a complex sentence. Conversely, a syntactic shape,
such as a clause, can function as several semantic
types, including fact, proposition, and event.
5 Training phase
As shown in Figure 1, the goal of the training phase
is to build training data from CSNs and their an-
tecedents and train models which can be used for
resolving ASNs.
5.1 The CSN corpus
We automatically constructed a corpus, a subset of
the New York times (NYT) corpus4, which contains
211,722 sentences following CSN patterns from Ta-
ble 2. We considered part-of-speech information5
while looking for the patterns. For instance, in-
stead of the pattern N-that, we actually looked for
{shell noun NN that IN}.
3In our annotated sample data, we observed ASN an-
tecedents as close as the same sentence and as far as 7 sentences
away from the anaphor.
4http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2008T19
5http://nlp.stanford.edu/software/tagger.
shtml
5.2 Antecedent extractor for CSNs
The goal of the antecedent extractor is to create auto-
matically labelled CSN antecedent data. Recall that
antecedents of CSNs can be extracted using simple
predefined rules that are based on the syntactic struc-
ture alone. For instance, the antecedent extraction
rule for example (2) would be: if the example fol-
lows the pattern fact-that, extract the post-nominal
that clause as the antecedent. To come up with a list
of such extraction rules, we systematically analyzed
a sample of examples (about 20 examples) of each
pattern for each shell noun. Table 3 summarizes the
resulting antecedent extraction rules.
The actual antecedent extraction works as fol-
lows. First, we parsed the examples from the CSN
corpus using the Stanford parser.6 Then for each ex-
ample, we applied rules from Table 3 depending on
the shell noun and the pattern it follows to extract
an appropriate syntactic constituent as the CSN an-
tecedent. For instance, for the noun fact following
the N-that pattern, as in example (2), we first looked
for the NP constituent containing the shell noun fact,
and then extracted the sentential constituent follow-
ing the NP constituent as the CSN antecedent. Al-
though, in most of the cases, the antecedent is given
in the post-nominal wh, that, or infinitive clauses,
sometimes it is not present in the immediately fol-
lowing clause but is given only as a predicate, as
shown in (3).
(3) The primary reason that the archdiocese cannot
pay teachers more is that its students cannot af-
ford higher tuition.
In such cases, we looked for the pattern (VP (VB
be verb) X) in the right sibling of the NP contain-
ing the pattern shell noun-that and extracted X as
the CSN antecedent.
Two contradictory goals need to be achieved
while extracting antecedents of CSNs. The first re-
quires only considering CSNs with high-confidence
patterns, whereas the second requires considering as
many patterns as possible to allow a wide variety of
antecedent examples with different linguistic prop-
erties (e.g., syntactic shape). Our antecedent extrac-
tor tries to find a balance between the two goals.
6http://nlp.stanford.edu/software/lex-parser.
shtml
303
fact reason issue decision question possibility
N-to ? ? ? inf clause predicate inf clause
N-be-to ? inf clause inf clause inf clause inf /wh clause inf clause
N-that that clause predicate predicate ? predicate that clause
N-be-that that clause that clause that clause that clause that clause that clause
N-wh ? predicate wh clause wh clause wh clause ?
N-be-wh wh clause wh clause wh clause wh clause wh clause wh clause
Table 3: Content extraction patterns for CSNs. Patterns in boldface are the prominent patterns for the respective shell
noun. inf clause = infinitive clause. Discarded patterns are denoted by ?.
To address the first goal, we filter examples fol-
lowing noisy patterns, i.e., the patterns that do not
unambiguously encode antecedents of that CSN. For
instance, the pattern N-to is a highly preferred pat-
tern for decision, as shown in (4). The antecedent
extraction rule here is relatively simple: if the exam-
ple follows the pattern decision-to, extract the post-
nominal infinitive clause as the correct antecedent.
(4) President Jacques Chirac?s arrogant decision to
defy the world and go ahead with two nuclear
bomb tests in Polynesia deserves contempt.
But the same pattern is noisy for reason. In (5), for
example, the actual reason is not given anywhere in
the sentence. So we discard the examples following
the pattern N-to for reason.
(5) Investors have had reason to worry about stocks.
We also discard examples with negative determiners,
as in (6), because in such cases, the extraction rules
do not precisely give the antecedent of the given
CSN.
(6) He was careful to repeat anew that he had made
no decision to go to war.
For the N-wh pattern, we exclude certain wh
words for certain nouns. For example, we exclude
the wh word which for question as the Penn Tree-
bank tagset7 does not distinguish between which as a
relative pronoun and as a question. We are interested
in the latter but not the former. Other discarded wh
words include which and when for fact; all wh words
except when and why for reason, all wh words except
how and whether for issue; which, whom, when, and
why for decision; which and when for question; and
all wh words for possibility.
7The Stanford tagger we employ uses the Penn Treebank
tagset (Marcus et al, 1993).
To address the second goal of allowing a wide
variety of antecedent examples, we try to include
as many patterns as possible for each shell noun,
as shown in Table 3. For instance, the patterns
question-to and question-be-to will have infinitive
clauses as antecedents (marked as VP or S+VP
by the parser), whereas for the examples follow-
ing patterns question-wh and question-be-wh the
antecedent will be in the wh clauses (marked as
SBAR). For the pattern question-that, the antecedent
will be in the predicate (similar to example (3)),
which can be a prepositional phrase, a noun phrase
or a clause.
5.3 Models for CSN antecedents
The antecedent extractor gives labels for each in-
stance in the CSN corpus. Using this labelled data,
we train machine learning ranking models for dif-
ferent shell concepts that capture the characteristic
features associated with that shell concept. The fol-
lowing sections describe each step of our ranking
models in detail.
5.3.1 Candidate extraction
The first step is to extract the set of eligible an-
tecedent candidates C = {C1,C2, ...,Ck} for the CSN
instance ai. To train a machine learning model we
need positive and negative examples. We already
have positive examples for antecedent candidates ?
the true antecedents given by the antecedent extrac-
tor. But we also need negative examples of an-
tecedent candidates. By their construction, CSNs
have their antecedents in the same sentence. So
we extract all syntactic constituents of this sentence,
given by the Stanford parser. All the syntactic con-
stituents, except the true antecedent, are considered
as negative examples. With this candidate extraction
method, we end up with many more negative exam-
304
ples than positive examples, but that is exactly what
we expect with ASN antecedent candidates, i.e., the
test data on which we will be applying our models.
5.3.2 Features
Although our problem is similar to anaphora res-
olution, we cannot make use of the usual anaphora
or coreference resolution features such as agreement
or string matching (Soon et al, 2001) because of the
nature of ASN and CSN antecedents. We came up
with a set of features based on the properties that
were common in both ASN and CSN antecedents,
according to our judgement.
Syntactic type of the candidate (S) We observed
that each shell noun prefers specific CSN patterns
and each pattern involves a particular syntactic type.
For instance, decision prefers the pattern N-to and
consequently realizes as its antecedents more verb
phrases than, for example, noun phrases. We employ
two versions of syntactic type: fine-grained syntac-
tic type given by the Stanford parser (e.g., NP-TMP,
RRC) and coarse-grained syntactic type (e.g., NP,
VP, S, PP) in which we consider ten basic syntactic
categories and map all fine-grained syntactic types
to these categories.
Context features (C) Context features allow our
models to learn about the contextual clues that signal
the antecedent. This class contains two features: (a)
coarse-grained syntactic type of left and right sib-
lings of the candidate, and (b) part-of-speech tag of
the preceding and following words of the candidate.
Embedding level features (E) These features
(Mu?ller, 2008) encode the embedding level of the
candidate within its sentence. We consider two em-
bedding level features: top embedding level and im-
mediate embedding level. Top embedding level is
the level of embedding of the given candidate with
respect to its top clause (the root node), and immedi-
ate embedding level is the level of embedding with
respect to its immediate clause (the closest ancestor
of type S or SBAR). The intuition behind this fea-
ture is that if the candidate is deep in the parse tree,
it is possibly not salient enough to be an antecedent.
As we consider all syntactic constituents as poten-
tial candidates, there are many that clearly cannot be
antecedents. This feature will allow us to get rid of
this noise.
Subordinating conjunctions (SC) As we can see
in Table 2, subordinating conjunctions are common
with CSN and ASN antecedents. Vendler (1968)
points out that the shell noun fact prefers a that-
clause, and question and issue prefer a wh-question
clause. We observed that the pattern because X is
common with reason. The subordinating conjunc-
tion feature encodes these preferences for different
shell nouns. The feature checks whether the candi-
date follows the pattern SBAR ? (IN sconj) (S ...),
where sconj is a subordinating conjunction.
Verb features (V) A prominent property of CSN
and ASN antecedents is that they tend to contain
verbs. All examples from Table 2, for example, con-
tain verbs. Moreover, certain shell nouns have tense
and aspect preferences. For instance, for shell noun
fact, lexical verbs in past and present tenses predom-
inate (Schmid, 2000), whereas modal forms are ex-
tremely common for possibility. We use three verb
features that capture this idea: (a) presence of verbs
in general, (b) whether the main verb is finite or non-
finite, and (c) presence of modals.
Length features (L) The intuition behind these
features is that CSN and ASN antecedents tend to be
long, especially for nouns such as fact. We consider
two length features: (a) length of the candidate in
words, and (b) relative length of the candidate with
respect to the sentence containing the antecedent.
Lexical features (LX) Our extractor gives us a
large number of antecedent examples for each shell
noun. A natural question is whether certain words
tend to occur more frequently in the antecedent than
non-antecedent parts of the sentence. To deal with
this question, we extracted all antecedent unigrams
(i.e., unigrams occurring in antecedent part of the
sentence) and non-antecedent unigrams (i.e., uni-
grams occurring in non-antecedent parts of the sen-
tence) for each shell noun. Then for all antecedent
unigrams for a particular shell noun, we computed
term goodness in terms of information gain (Yang
and Pedersen, 1997) and considered the first 50
highly ranked unigrams as the lexical features for
that noun. Note that, in contrast with the other fea-
tures, these lexical features are tailored for each shell
noun and are extracted a priori.
305
5.3.3 Candidate ranking models
Now that we have the set of candidate antecedents
and a set of features, we are ready to train CSN an-
tecedent models. We follow the candidate-ranking
models proposed by Denis and Baldridge (2008) be-
cause they allow us to evaluate how good an an-
tecedent candidate is relative to all other candidates.
For every shell noun, we gather automatically ex-
tracted antecedent data given by the extractor for all
instances of that shell noun. Then for each instance
in this data, we extract the set C as explained in
Section 5.3.1. For each candidate Ci ? C, we ex-
tract a feature vector to create a corresponding set of
feature vectors, C f = {C f 1,C f 2, ...,C f k}. For every
CSN ai and a set of feature vectors corresponding
to its eligible candidates C f = {C f 1,C f 2, ...,C f k},
we create training examples (ai,C f i,rank),?C f i ?
C f . The rank is 1 if Ci is same as the true an-
tecedent, i.e., the automatically extracted antecedent
for that CSN, otherwise the rank is 2. We use the
svm rank learn call of SVMrank (Joachims, 2002)
for training the candidate-ranking models.
6 Testing phase
In this phase, we use the learned candidate ranking
models to identify the antecedents of ASNs.
6.1 The ASN corpus
We started with about 450 instances for each of the
six selected shell nouns (2,700 total instances), con-
taining the pattern {this shell noun}. The instances
were extracted from the NYT. Each instance con-
tains three paragraphs from the corresponding NYT
article: the paragraph containing the ASN and two
preceding paragraphs as context. After automati-
cally removing duplicates and ASNs with a non-
abstract sense (e.g., this issue with a publication-
related sense), we were left with 2,323 instances.
6.2 Antecedent identification
Candidate extraction The search space of ASN
antecedents is quite large for two reasons: ASNs
tend to have long-distance as well as short-distance
antecedents, and there is no clear restriction on the
syntactic type of the antecedents. In the ASN cor-
pus, each sentence on average had 49.5 distinct syn-
tactic constituents given by the Stanford parser. If
we consider n preceding sentences, the sentence
containing the anaphor, and one following sentence8
as sources for antecedents, then the average num-
ber of antecedent candidates will be 49.5? (n+ 2).
This is large compared to the search space of ordi-
nary nominal anaphora. In our previous work (Kol-
hatkar et al, 2013), we have developed methods that
identify the sentence containing the antecedent of
the ASN before identifying the precise antecedent.
In brief, given a set of a fixed number of sentences
around the sentence containing an ASN, these meth-
ods reliably identify the sentence containing the an-
tecedent. In this paper, we treat these methods as a
black box.
Given the sentence containing the antecedent,
we extract all syntactic constituents given by the
Stanford parser from that sentence as potential an-
tecedent candidates as for the training phase. In
the training phase, the antecedent is always con-
tained in the set of syntactic constituents given by
the Stanford parser because the extractor obtains the
appropriate antecedent using the syntactic informa-
tion. But in the testing phase, we cannot guarantee
that the true antecedent occurs in the extracted syn-
tactic constituents due to the parser?s errors. So for
robust candidate extraction, we extract all distinct
constituents from the 30-best parses instead of only
considering the best parse, which increases the aver-
age number of candidates from 49.5 to 55.2.
Feature extraction and candidate ranking
Given the antecedent candidates, feature extraction
and candidate ranking are essentially the same as
for the training phase, except of course we do not
know the true antecedent. Once we have the feature
vectors for each antecedent candidate, the appro-
priate trained model, i.e., the model trained for the
corresponding shell noun, is invoked and the can-
didates are ranked using the svm rank classify
call of SVMrank.
7 Evaluation
We evaluate the ranked candidates of ASN instances
using crowdsourcing.
8The ASN corpus contains a few cataphoric examples that
do not follow the standard patterns of the CSNs shown in Table
2, but actually refer to an antecedent in the following sentence
(e.g., Mr. Dukakis put this question to him: X).
306
Interface We chose to use CrowdFlower9 as our
crowdsourcing interface because of its integrated
quality-control mechanism. For instance, it throws
gold questions randomly at the workers and the
workers who do not answer them correctly are not
allowed to continue.
We presented to the crowd evaluators the ASN
instances from the ASN corpus. Recall that each
ASN instance is made up of the paragraph contain-
ing the ASN and two preceding paragraphs as con-
text. We displayed the first 10 highly-ranked candi-
dates (ordered randomly) given by our testing phase
and asked the evaluators to choose the best answer
that represents the ASN antecedent. We encouraged
the evaluators to select None when they did not agree
with any of the displayed answers. We also asked
them how satisfied they were with the displayed an-
swers. We provided them with three options: unsat-
isfied, satisfied, and partially satisfied.
Our job contained 2,323 evaluation units. We
asked for 8 judgements per instance and paid 6
cents per evaluation unit. As we were interested
in the verdict of native speakers of English, we
limited the allowed demographic region to English-
speaking countries.
Results Among the 2,323 ASN instances, 96% of
them were labelled as satisfied, 3% as partially satis-
fied and 1% as unsatisfied. Only 2% of the instances
were labelled as None. As expected, evaluators were
unsatisfied or partially satisfied with the options of
these instances. These results suggest that our res-
olution models trained on automatically extracted
antecedents of CSNs bring the relevant candidates
of ASN antecedents to the top, i.e., within first 10
highly-ranked candidates. This itself is a positive re-
sult given the large search space of ASN antecedent
candidates (more than 55 candidates on average).
Among the evaluation units, more than half of the
evaluators agreed on an answer for 1,810 units. We
used these instances for further analysis.
To examine which CSN antecedent features are
relevant in identifying ASN antecedents, we carried
out ablation experiments with all feature class com-
binations. We compared the rankings given by our
ranker to the crowd?s answer using precision at n
9http://crowdflower.com/
(P@n).10 More specifically, we count the number of
instances where the crowd?s answers occur within
our ranker?s first n choices. P@n then is this count
divided by the total number of instances. Note that
P@1 is equivalent to the standard precision.
We compared our results against two baselines:
preceding sentence and chance. The preceding sen-
tence baseline chooses the previous sentence as the
correct antecedent. The chance baseline chooses a
candidate from a uniform random distribution over
the set of 10 top-ranked candidates.
The results are shown in Table 4. Although dif-
ferent feature combinations gave the best results for
different shell nouns, the features that occur fre-
quently in many best-performing combinations were
embedding level (E), lexical (LX), and subordinat-
ing conjunction (SC) features. The SC features were
particularly effective for issue and question, where
we expected patterns such as whether X.
Surprisingly, the syntactic type features (S) did
not show up very often in the best-performing fea-
ture combinations, suggesting that the ASN an-
tecedents had a greater variety of syntactic types
than what was available in our CSN training data.
The context features (C) did not appear in any of
the best-performing feature combinations. In fact,
they resulted in a sharp decline in the precision. For
instance, for question, adding the context features
to the best-performing combination {E,SC,V,L,LX}
resulted in a drop of 16 percentage points. This
result was not surprising because although the an-
tecedents of ASNs and CSNs share similar proper-
ties such as common words, we know that their con-
text is generally different.
We did not observe specific features associated
with Schmid?s semantic categories. An exception
was the E features which were particularly effective
for the factual nouns fact and reason: the results
with them alone gave high precision (0.68 for fact
and 0.72 for reason). That said, the E features were
present in most of the best-performing combinations
even for the shell nouns in other semantic categories.
10CrowdFlower gives us a unique answer for each instance,
which we take to be the crowd?s answer. During annotation, ev-
ery annotator is presented with a few gold questions randomly
and each annotator is assigned a trust score based on her per-
formance on these gold questions. The unique answer for an
instance is the answer with the highest sum of trusts.
307
fact (43,000 train and 472 test instances)
Features P@1 P@2 P@3 P@4
{E,L,LX} .70? .85 .91 .94
{E,V,L,LX} .68? .86 .92 .95
{E,SC,L,LX} .66? .83 .92 .95
PSbaseline .40 ? ? ?
reason (4,520 train and 443 test instances)
Features P@1 P@2 P@3 P@4
{E,V,L} .72? .86 .90 .93
{E,V} .72? .85 .90 .92
{E,SC,LX} .69? .84 .90 .94
PSbaseline .44 ? ? ?
issue (3,000 train and 303 test instances)
Features P@1 P@2 P@3 P@4
{SC,L} .47? .59 .71 .78
{SC,L,LX} .46? .60 .70 .81
{S,E,SC,L,LX}.40? .61 .72 .81
PSbaseline .30 ? ? ?
decision (42,332 train and 390 test instances)
Features P@1 P@2 P@3 P@4
{E,LX} .35? .53 .67 .76
{E,SC,LX} .30? .48 .65 .75
{E,SC,V,L,LX}.27 .44 .57 .69
PSbaseline .21 ? ? ?
question (9,336 train and 440 test instances)
Features P@1 P@2 P@3 P@4
{E,SC,V,L,LX} .70? .82 .87 .90
{E,SC,LX} .68? .83 .88 .91
{E,SC,V,LX} .69? .80 .87 .91
PSbaseline .25 ? ? ?
possibility (11,735 train and 278 test instances)
Features P@1 P@2 P@3 P@4
{SC,L,LX} .56? .75 .87 .92
{E,SC} .56? .76 .87 .91
{E,L,LX} .54? .76 .86 .91
PSbaseline .34 ? ? ?
Table 4: Evaluation of our ranker for antecedents of six ASNs. For each noun we show the three best-performing
feature combinations. P@n is the precision at rank n (P@1 = standard precision). Boldface indicates the best in the
column. PSbaseline = preceding sentence baseline. The P@1 results significantly higher than PSbaseline are marked
with ?(two-sample ?2 test: p < 0.05). The chance baseline results were 0.1, 0.2, 0.3, and 0.4 for P@1, P@2, P@3,
and P@4, respectively.
The only previous work with which our results
could be compared is that of Kolhatkar and Hirst
(2012). The work reports precision in the range
of 0.41 to 0.61 in resolving this issue anaphora in
the Medline domain. In our case, for this issue in-
stances from the NYT corpus, we achieved precision
in the range of 0.40 to 0.47. Furthermore, we ap-
plied our models to resolve this issue instances from
Kolhatkar and Hirst?s (2012) work.11 Even with
models trained on automatically labelled CSN an-
tecedents, we achieved similar results to Kolhatkar
and Hirst?s results: P@1 of 0.45, P@2 of 0.59, P@3
of 0.65, and P@4 of 0.67. These results show the
domain robustness of our methods with respect to
the shell noun issue. Recall that Kolhatkar and Hirst
(2012) looked at only very specific cases of this is-
sue and used manually annotated data (Section 2),
as opposed to the automatically extracted CSN an-
tecedent data we use.
11We thank an anonymous reviewer for suggesting this to us.
8 Discussion and conclusion
The goal of this paper was to examine to what ex-
tent CSNs help in interpreting ASNs. Based on the
evaluators? satisfaction level and very few None re-
sponses, we conclude that our models trained on
CSN antecedents were able to bring the relevant
ASN antecedent candidates into the top 10 candi-
dates.
When we applied the models trained on CSN an-
tecedents to interpret ASNs, we achieved precision
in the range of 0.35 to 0.72. The precision results as
high as 0.72 for reason and 0.70 for fact and ques-
tion support our hypothesis that the linguistic knowl-
edge provided by CSN antecedents helps in identify-
ing the antecedents of ASNs. We observed different
behaviour for different nouns. The mental nouns is-
sue and decision in general were harder to interpret
than other shell nouns. The models trained on CSNs
achieved precisions of 0.35 for decision and 0.47 for
issue. So there is still much room for improvement.
That said, for the same nouns, the antecedents were
in the first four ranks about 76% to 81% of the times,
308
suggesting that in future research, our models can be
used as base models to reduce the large search space
of ASN antecedent candidates.
We observed a wide range of performance for dif-
ferent shell nouns. One reason is that the size of the
training data was different for different shell nouns.
After excluding the noisy examples (Section 5.2),
there were about 43,000 training examples for fact,
but only about 3,000 for issue. In addition, a par-
ticular shell concept itself can be difficult, e.g., the
very idea of what counts as an issue is more fuzzy
than what counts as a fact.
One limitation of our approach is that it only
learns the properties that are present in CSN an-
tecedents. However, ASN antecedents have addi-
tional properties which are not always captured by
CSN antecedents. For instance, for the shell noun
decision, most of the training examples were infini-
tive phrases of the form to X. But antecedents of the
ASN decision were mostly court decisions and were
expressed with full sentences.
Moreover, although the models trained on CSN
antecedents are able to encode characteristic fea-
tures associated with the general shell concept, they
are unable to distinguish between two competing
candidates both containing the characteristic fea-
tures of that shell concept. For instance, our ap-
proach will not be able to handle the constructed ex-
amples in (7).
(7) The teacher erased the solutions before John had
time to copy them out, as he had momentarily
been distracted by a band playing outside.
a) This fact infuriated him, as the teacher al-
ways erased the board quickly and John sus-
pected it was just to punish anyone who was
lost in thought, even for a moment.
b) This fact infuriated the teacher, who had al-
ready told John several times to focus on
class work.
Here, both propositions possess properties of the
shell concept fact. Understanding the context of the
anaphor itself is crucial in correctly identifying the
fact in each case, which cannot be learnt from CSN
antecedents due to their specific context patterns.
A number of extensions are planned for this work.
First, we plan to use both kinds of data, CSN and
ASN antecedent data, which will give us a basis
for developing a better performing ASN resolver.
We also plan to incorporate contextual features (e.g.,
right-frontier rule (Webber, 1991) and context rank-
ing (Eckert and Strube, 2000)). Finally, we will ex-
amine whether a model trained for one shell noun
can be generalized to other shell nouns from the
same semantic category.
Acknowledgements
We thank the anonymous reviewers for their
constructive comments. This material is based
upon work supported by the United States Air
Force and the Defense Advanced Research Projects
Agency under Contract No. FA8650-09-C-0179,
Ontario/Baden-Wu?rttemberg Faculty Research Ex-
change, and the University of Toronto.
References
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers, Dordrecht,
Netherlands.
Donna K. Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, Rochester, New
York: University of Rochester.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669,
Honolulu, Hawaii, October. Association for Computa-
tional Linguistics.
Miriam Eckert and Michael Strube. 2000. Dialogue acts,
synchronizing units, and anaphora resolution. Journal
of Semantics, 17:51?89.
Gill Francis. 1986. Anaphoric Nouns. Discourse Anal-
ysis Monographs 11, Birmingham: English Language
Research, University of Birmingham.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman Pub Group.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In In Proceedings of
the 14th International Conference on Computational
Linguistics, pages 539?545, Nantes, France. Associa-
tion for Computational Linguistics.
Roz Ivanic. 1991. Nouns in search of a context: A study
of nouns with both open- and closed-system character-
istics. International Review of Applied Linguistics in
Language Teaching, 29:93?114.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In ACM SIGKDD Conference
309
on Knowledge Discovery and Data Mining (KDD),
pages 133?142.
Varada Kolhatkar and Graeme Hirst. 2012. Resolv-
ing ?this-issue? anaphora. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1255?1265, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Varada Kolhatkar, Heike Zinsmeister, and Graeme Hirst.
2013. Annotating anaphoric shell nouns with their an-
tecedents. In Proceedings of the 7th Linguistic Anno-
tation Workshop and Interoperability with Discourse,
pages 112?121, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 368?375, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Christoph Mu?ller. 2008. Fully Automatic Resolution of
It, This and That in Unrestricted Multi-Party Dialog.
Ph.D. thesis, Universita?t Tu?bingen.
Rebecca J. Passonneau. 1989. Getting at discourse refer-
ents. In Proceedings of the 27th Annual Meeting of the
Association for Computational Linguistics, pages 51?
59, Vancouver, British Columbia, Canada. Association
for Computational Linguistics.
Massimo Poesio, Simone Ponzetto, and Yannick Versley.
2011. Computational models of anaphora resolution:
A survey. Unpublished.
Hans-Jo?rg Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Topics
in English Linguistics 34. Mouton de Gruyter, Berlin.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages
168?175, Sapporo, Japan, July. Association for Com-
putational Linguistics.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton de Gruyter, The Hague.
Bonnie Lynn Webber. 1991. Structure and ostension
in the interpretation of discourse deixis. In Language
and Cognitive Processes, pages 107?135.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of the 14th International Conference on
Machine Learning, pages 412?420, Nashville, TN.
310
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 499?510,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Resolving Shell Nouns
Varada Kolhatkar
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
varada@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gh@cs.toronto.edu
Abstract
Shell nouns, such as fact and problem, oc-
cur frequently in all kinds of texts. These
nouns themselves are unspecific, and can
only be interpreted together with the shell
content. We propose a general approach
to automatically identify shell content of
shell nouns. Our approach exploits lexico-
syntactic knowledge derived from the lin-
guistics literature. We evaluate the ap-
proach on a variety of shell nouns with a
variety of syntactic expectations, achiev-
ing accuracies in the range of 62% (base-
line = 33%) to 83% (baseline = 74%) on
crowd-annotated data.
1 Introduction
Shell nouns are abstract nouns, such as fact, issue,
idea, and problem, which facilitate efficiency by
avoiding repetition of long stretches of text. The
shell metaphor comes from Schmid (2000), and it
captures the various functions of these nouns in a
discourse: containment, signalling, pointing, and
encapsulating. Shell nouns themselves are unspe-
cific, and can only be interpreted together with
their shell content, i.e., the propositional content
they encapsulate in the given context. The process
of identifying this content in the given context is
referred to as shell noun resolution or interpreta-
tion. Examples (1), (2), and (3) show usages of the
shell nouns fact and issue. The shell noun phrases
are resolved to the postnominal that clause, the
complement wh clause, and the immediately pre-
ceding clause, respectively.
1,2
(1) The fact that a major label hadn?t been
at liberty to exploit and repackage the
material on CD meant that prices on the
vintage LP market were soaring.
(2) The issue that this country and Congress
must address is how to provide optimal
care for all without limiting access for
the many.
(3) Living expenses are much lower in rural
India than in New York, but this fact is
not fully captured if prices are converted
with currency exchange rates.
Observe that the relation between shell noun
phrases and their shell content is similar to
the relation of abstract anaphora (or cataphora)
(Asher, 1993) with backward- or forward-looking
abstract-object antecedents. For anaphoric shell
noun examples, the shell content precedes the
shell noun phrase, and for cataphoric shell noun
examples the shell content follows the shell noun
phrase.
3
Shell nouns as a group occur frequently in argu-
mentative texts (Schmid, 2000; Flowerdew, 2003;
Botley, 2006). They play an important role in or-
ganizing a discourse and maintaining its coher-
ence (Schmid, 2000; Flowerdew, 2003), and re-
solving them is an important component of var-
ious computational linguistics tasks that rely on
1
Note that the postnominal that-clause in (1) is not a rela-
tive clause: the fact in question is not an argument of exploit
and repackage.
2
All examples in this paper are from the New
York Times corpus (https://catalog.ldc.upenn.edu/
LDC2008T19)
3
We use the terms cataphoric shell noun and anaphoric
shell noun for lack of better alternatives.
499
discourse structure. Accordingly, identifying shell
content can be helpful in summarization, informa-
tion retrieval, and ESL learning (Flowerdew, 2003;
Hinkel, 2004).
Despite their importance in discourse, under-
standing of shell nouns from a computational lin-
guistics perspective is only in the preliminary
stage. Recently, we proposed an approach to anno-
tate and resolve anaphoric cases of six typical shell
nouns: fact, reason, issue, decision, question, and
possibility (Kolhatkar et al., 2013b). This work
drew on the observation that shell nouns following
cataphoric constructions are easy to resolve. We
manually developed rules to identify shell content
for such cases. Later, we used these cataphoric ex-
amples and their shell content as training data to
resolve harder anaphoric examples.
In this paper, we propose a general algorithm to
resolve cataphoric shell noun examples. Our long-
term goal is to build an end-to-end shell-noun res-
olution system. If we want to go beyond the six
shell nouns from our previous work, and general-
ize our approach to other shell nouns, first we need
to develop an approach to resolve cataphoric shell
noun examples. A number of challenges are asso-
ciated with this seemingly easy task. The primary
challenges is that this resolution is in many cru-
cial respects a semantic phenomenon. To obtain
the required semantic knowledge, we exploit the
properties of shell nouns and their categorization
described in the linguistics literature. We evalu-
ate our method using crowdsourcing, and demon-
strate how far one can get with simple, determin-
istic shell content extraction.
2 Related work
Shell-nounhood is a well-established concept in
linguistics (Vendler, 1968; Ivanic, 1991; Asher,
1993; Francis, 1994; Schmid, 2000, inter alia).
However, understanding of shell nouns from a
computational linguistics perspective is only in the
preliminary stage.
Shell nouns take a number of semantic argu-
ments. In this respect, they are similar to the gen-
eral class of argument-taking nominals as given
in the NomBank (Meyers et al., 2004). Simi-
larly, there is a small body of literature that ad-
dresses nominal semantic role labelling (Gerber et
al., 2009) and nominal subcategorization frames
(Preiss et al., 2007). That said, the distinguishing
property of shell nouns is that one of their seman-
tic arguments is the shell content, but the literature
in computational linguistics does not provide any
method that is able to identify the shell content.
The focus of our work is to rectify this.
Shell content represents complex and abstract
objects. So traditional linguistic and psycholin-
guistic principles used in pronominal anaphora
resolution (see the survey by Poesio et al. (2011)),
such as gender and number agreement, are not ap-
plicable in resolving shell nouns. That said, there
is a line of literature on annotating and resolving
personal and demonstrative pronouns, which typi-
cally refer to similar kinds of non-nominal abstract
entities (Passonneau, 1989; Eckert and Strube,
2000; Byron, 2003; M?ller, 2008; Hedberg et
al., 2007; Poesio and Artstein, 2008; Navarretta,
2011, inter alia). Also, there have been attempts
at annotating the shell content of anaphoric occur-
rences of shell nouns (e.g., Botley (2006), Kol-
hatkar et al. (2013a)). However, none of these
approaches attempt to annotate and resolve cat-
aphoric examples such (1) and (2).
3 Challenges
A number of challenges are associated with the
task of resolving cataphoric shell noun examples,
especially when it comes to developing a holistic
approach for a variety of shell nouns.
First, each shell noun has idiosyncrasies. Dif-
ferent shell nouns have different semantic and syn-
tactic expectations, and hence they take different
types of one or more semantic arguments: one in-
troducing the shell content, and others expressing
circumstantial information about the shell noun.
For instance, fact typically takes a single factual
clause as an argument, which is its shell content,
as we saw in example (1), whereas reason expects
two arguments: the cause and the effect, with the
content introduced in the cause, as shown in exam-
ple (4).
4
Similarly, decision takes an agent making
the decision and the shell content is represented as
an action or a proposition, as shown in (5).
5
(4) One reason [that 60 percent of New York
City public-school children read below
grade level]
effect
is [that many elementary
schools don?t have libraries]
cause
.
4
Observe that the postnominal that clause in (4) is not a
relative clause, and still it is not the shell content because it is
not the cause argument of the shell noun reason.
5
Observe that this aspect of shell nouns of taking different
numbers and kinds of complement clauses is similar to verbs
having different subcategorization frames.
500
(5) I applaud loudly the decision of
[Greenburgh]
agent
to ban animal per-
formances.
Second, the relation between a shell noun and
its content is in many crucial respects a seman-
tic phenomenon. For instance, resolving the shell
noun reason to its shell content involves identify-
ing a) that reason generally expects two semantic
arguments: cause and effect, b) that the cause ar-
gument (and not the effect argument) represents
the shell content, and c) that a particular con-
stituent in the given context represents the cause
argument.
Third, at the conceptual level, once we know
which semantic argument represents shell content,
resolving examples such as (4) seems straightfor-
ward using syntactic structure, i.e., by extracting
the complement clause. But at the implementa-
tion level, this is a non-trivial problem for two rea-
sons. The first reason is that examples contain-
ing shell nouns often follow syntactically complex
constructions, including embedded clauses, coor-
dination, and sentential complements. An auto-
matic parser is not always accurate for such ex-
amples. So the challenge is whether the avail-
able tools in computational linguistics such as syn-
tactic parsers and discourse parsers are able to
provide us with the information that is necessary
to resolve these difficult cases. The second rea-
son is that the shell content can occur in many
different constructions, such as apposition (e.g.,
parental ownership of children, a concept that
allows . . . ), postnominal and complement clause
constructions, as we saw in examples (1) and (2),
and modifier constructions (e.g., the liberal trade
policy that . . . ). Moreover, in some constructions,
the content is indefinite (e.g., A bad idea does not
harm until someone acts upon it.) or None be-
cause the example is a non-shell noun usage (e.g.,
this week?s issue of Sports Illustrated), and the
challenge is to identify such cases.
Finally, whether the postnominal clause intro-
duces the shell content or not is dependent on
the context of the shell noun phrase. The reso-
lution can be complicated by complex syntactic
constructions. For instance, when the shell noun
follows verbs such as expect, it becomes difficult
for an automatic system to identify whether the
postnominal or the complement clause is of the
verb or of the shell noun (e.g., they did not expect
the decision to reignite tension in Crown Heights
vs. no one expected the decision to call an elec-
tion). Similarly, shell noun phrases can be ob-
jects of prepositions, and whether the postnomi-
nal clause introduces the shell content or not is de-
pendent on this preposition. For instance, for the
pattern reason that, the postnominal that clause
does not generally introduce the shell content, as
we saw in (4); however, this does not hold when
the shell noun phrase containing reason follows
the preposition for, as shown in (6).
(6) Low tax rates give people an incentive to
work, for the simple reason that they get
to keep more of what they earn.
4 Linguistic framework
Linguists have studied a variety of shell nouns,
their classification, different patterns they follow,
and their semantic and syntactic properties in de-
tail (Vendler, 1968; Ivanic, 1991; Asher, 1993;
Francis, 1994; Schmid, 2000, inter alia). Schmid
points out that being a shell noun is a property of
a specific usage of the noun rather than an inher-
ent property of the word. He provides a list of 670
English nouns that tend to occur as shell nouns. A
few frequently occurring ones are: problem, no-
tion, concept, issue, fact, belief, decision, point,
idea, event, possibility, reason, trouble, question,
plan, theory, aim, and principle.
4.1 Lexico-syntactic patterns
Precisely defining the notion of shell-nounhood
is tricky. A necessary property of shell nouns is
that they are capable of taking clausal arguments,
primarily with two lexico-syntactic constructions:
Noun + postnominal clause and Noun + be + com-
plement clause (Vendler, 1968; Biber et al., 1999;
Schmid, 2000; Huddleston and Pullum, 2002).
Schmid exploits these lexico-syntactic construc-
tions to identify shell noun usages. In particular,
he provides a number of typical lexico-syntactic
patterns that are indicative of either anaphoric or
cataphoric shell noun occurrences. Table 1 shows
these patterns with examples.
Cataphoric These patterns primarily follow two
constructions.
N-be-clause In this construction, the shell
noun phrase occurs as the subject in a subject-
verb-clause construction, with the linking verb be,
and the shell content embedded as a wh clause,
that clause, or to-infinitive clause. The linking
501
Cataphoric
1 N-be-to Our plan is to hire and retain the best managers we can.
2 N-be-that The major reason is that doctors are uncomfortable with uncertainty.
3 N-be-wh Of course, the central, and probably insoluble, issue is whether animal testing is cruel.
4 N-to The decision to disconnect the ventilator came after doctors found no brain activity.
5 N-that Mr. Shoval left open the possibility that Israel would move into other West Bank cities.
6 N-wh If there ever is any doubt whether a plant is a poppy or not, break off a stem and squeeze it.
7 N-of The concept of having an outsider as Prime Minister is outdated.
Anaphoric
8 th-N Living expenses are much lower in rural India than in New York, but this fact is not fully
captured if prices are converted with currency exchange rates.
9 th-be-N People change. This is a fact.
10 Sub-be-N If the money is available, however, cutting the sales tax is a good idea.
Table 1: Lexico-grammatical patterns of shell nouns (Schmid, 2000). Shell noun phrases are underlined,
the pattern is marked in boldface, and the shell content is marked in italics.
Proportion
Noun N-be-to N-be-that N-be-wh N-to N-that N-wh N-of total
idea 7 2 - 5 23 10 53 91,277
issue - 1 5 7 14 2 71 55,088
concept 1 - - 6 12 - 79 14,301
decision - - - 80 12 1 5 55,088
plan 5 - - 72 17 - 4 67,344
policy 4 1 - 16 25 2 51 24,025
Table 2: Distribution of cataphoric patterns for six shell nouns in the New York Times corpus. Each
column shows the percentage of instances following that pattern. The last column shows the total number
of cataphoric instances of each noun in the corpus.
verb be indicates the semantic identity between the
shell noun and its content in the given context. The
construction follows the patterns in rows 1, 2, and
3 of Table 1.
N-clause This construction includes the cat-
aphoric patterns 4?7 in Table 1. For these patterns
the link between the shell noun and the content
is much less straightforward: whether the post-
nominal clause expresses the shell content or not
is dependent on the shell noun and the syntac-
tic structure under consideration. For instance,
for the shell noun fact, the shell content is em-
bedded in the postnominal that clause, as shown
in (1), but this does not hold for the shell noun
reason in example (4). The N-of pattern is dif-
ferent from other patterns: it follows the con-
struction N-prepositional phrase rather than N-
clause, and since a prepositional phrase can take
different kinds of embedded constituents such as a
noun phrase, a sentential complement, and a verb
phrase, the pattern offers flexibility in the syntactic
type of the shell content.
Anaphoric For these patterns, the link between
the shell noun and the content is created using
linguistic elements such as the, this, that, other,
same, and such. For the patterns 8 and 9 the shell
content does not typically occur in the sentence
containing the shell noun phrase. For the pattern
10, the shell content is the subject in a subject-
verb-N construction.
Pattern preferences Different shell nouns have
different pattern preferences. Table 2 shows the
distribution of cataphoric patterns for six shell
nouns in the New York Times corpus. The shell
nouns idea, issue, and concept prefer N-of pattern,
whereas plan and decision prefer the pattern N-to.
Among all instances of the shell noun decision fol-
502
Idea family
Semantic features: [mental], [conceptual]
Frame: mental; focus on propositional content of IDEA
Nouns: idea, issue, concept, point, notion, theory, . . .
Patterns: N-be-that/of, N-that/of
Plan family
Semantic features: [mental], [volitional], [manner]
Frame: mental; focus on IDEA
Nouns: decision, plan, policy, idea, . . .
Patterns: N-be-to/that, N-to/that
Trouble family
Semantic features: [eventive], [attitudinal], [manner],
[deontic]
Frame: general eventive
Nouns: problem, trouble, difficulty, dilemma, snag
Patterns: N-be-to
Problem family
Semantic features: [factual], [attitudinal], [impeding]
Frame: general factual
Nouns: problem, trouble, difficulty, point, thing, snag,
dilemma , . . .
Patterns: N-be-that/of
Thing family
Semantic features: [factual]
Frame: general factual
Nouns: fact, phenomenon, point, case, thing, business
Patterns: N-that, N-be-that
Reason family
Semantic features: [factual], [causal]
Frame: causal; attentional focus on CAUSE
Nouns: reason, cause, ground, thing
Patterns: N-be-that/why, N-that/why
Table 3: Example families from Schmid (2000). The nouns in boldface are used to evaluate this work.
lowing Schmid?s cataphoric patterns, 80% of the
instances follow the pattern N-to.
6
4.2 Categorization of shell nouns
Schmid classifies shell nouns at three levels. At
the most abstract level, he classifies shell nouns
into six semantic classes: factual, linguistic, men-
tal, modal, eventive, and circumstantial. Each se-
mantic class indicates the type of experience the
shell noun is intended to describe. For instance,
the mental class describes ideas and cognitive
states, whereas the linguistic class describes utter-
ances, linguistic acts, and products thereof.
The next level of classification includes more-
detailed semantic features. Each broad semantic
class is sub-categorized into a number of groups.
A group of an abstract class tries to capture
the semantic features associated with the fine-
grained differences between different usages of
shell nouns in that class. For instance, groups
associated with the mental class are: conceptual,
creditive, dubiative, volitional, and emotive.
The third level of classification consists of fam-
ilies. A family groups together shell nouns with
similar semantic features. Schmid provides 79 dis-
tinct families of 670 shell nouns. Each family is
named after the primary noun in that family. Table
3 shows six families: Idea, Plan, Trouble, Prob-
lem, Thing, and Reason. A shell noun can be
6
Table 2 does not include anaphoric patterns, as this pa-
per is focused on cataphoric shell noun examples. Anaphoric
patterns are common for all shell nouns: among all instances
of a shell noun, approximately 50 to 80% are anaphoric.
a member of multiple families. The nouns sub-
sumed in a family share semantic features. For
instance, all nouns in the Idea family are mental
and conceptual. They are mental because ideas
are only accessible through thoughts, and concep-
tual because they represent reflection or an appli-
cation of a concept. Each family activates a se-
mantic frame. The idea of these semantic frames is
similar to that of frames in Frame semantics (Fill-
more, 1985) and in semantics of grammar (Talmy,
2000). In particular, Schmid follows Talmy?s con-
ception of frames. A semantic frame describes
conceptual structures, its elements, and their in-
terrelationships. For instance, the Reason family
invokes the causal frame, which has cause and ef-
fect as its elements with the attentional focus on
the cause. According to Schmid, the nouns in a
family also share a number of lexico-syntactic fea-
tures. The patterns attribute in Table 3 shows pro-
totypical lexico-syntactic patterns, which attract
the members of the family. Schmid defines attrac-
tion as the degree to which a lexico-grammatical
pattern attracts a certain noun. For instance, the
patterns N-to and N-that attract the shell nouns in
the Plan family, whereas the N-that pattern attracts
the nouns in the Thing family. The pattern N-of is
restricted to a smaller group of nouns such as con-
cept, problem, and issue.
7,8
7
Schmid used the British section of COBUILD?S Bank of
English for his classification.
8
Schmid?s families could help enrich resources such as
FrameNet (Baker et al., 1998) with the shell content informa-
tion.
503
5 Resolution algorithm
With this exposition, the problem of shell noun
resolution is identifying the appropriate seman-
tic argument of the shell noun representing its
shell content. This section describes our algorithm
to resolve shell nouns following cataphoric pat-
terns. The algorithm addresses the primary chal-
lenge of idiosyncrasies of shell nouns by exploit-
ing Schmid?s semantic families (see Section 4.2).
The input of the algorithm is a shell noun instance
following a cataphoric pattern, and the output is
its shell content or None if the shell content is not
present in the given sentence. The algorithm fol-
lows three steps. First, we parse the given sentence
using the Stanford parser.
9
Second, we look for
the noun phrase (NP), where the head of the NP is
the shell noun to be resolved.
10
Finally, we extract
the appropriate shell content, if it is present in the
given sentence.
5.1 Identifying potentially anaphoric
shell-noun constructions
Before starting the actual resolution, first we iden-
tify whether the shell content occurs in the given
sentence or not. According to Schmid, the lexico-
syntactic patterns signal the position of the shell
content. For instance, if the pattern is of the form
N-be-clause, the shell content is more likely to
occur in the complement clause in the same sen-
tence. That said, although on the surface level, the
shell noun seems to follow a cataphoric pattern, it
is possible that the shell content is not given in a
postnominal or a complement clause, as shown in
(7).
(7) Just as weekend hackers flock to the golf
ball most used by PGA Tour players,
recreational skiers, and a legion of youth
league racers, gravitate to the skis worn
by Olympic champions. It is the reason
that top racers are so quick flash their skis
for the cameras in the finish area.
Here, the shell noun and its content are linked via
the pronoun it. For such constructions, the shell
noun phrase and shell content do not occur in the
same sentence. Shell content occurs in the preced-
ing discourse, typically in the preceding sentence.
9http://nlp.stanford.edu/software/
lex-parser.shtml
10
We extract the head of an NP following the heuristics
proposed by Collins (1999, p. 238).
We identify such cases, and other cases where the
shell content is not likely to occur in the postnom-
inal or complements clauses, by looking for the
patterns below in the given order, returning the
shell content when it occurs in the given sentence.
Sub-be-N This pattern corresponds to the
lexico-grammatical pattern in Figure 1(a). If this
pattern is found, there are three main possibilities
for the subject. First, if an existential there occurs
at the subject position, we move to the next pat-
tern. Second, if the subject is it (example (7)), this
or that, we return None, assuming that the con-
tent is not present in the given sentence. Finally,
if the first two conditions are not satisfied, i.e., if
the subject is neither a pronoun not an existential
there, we assume that subject contains a valid shell
content, and return it. An example is shown in (8).
Note that in such cases, unlike other patterns, the
shell content is expressed as a noun phrase.
(8) Strict liability is the biggest issue when
considering what athletes put in their bod-
ies.
Apposition Another case where shell content
does not typically occur in the postnominal or
complement clause is the case of apposition. In-
definite shell noun phrases often occur in apposi-
tion constructions, as shown in (9).
(9) The LH lineup, according to Gale, will
feature ?cab-forward? design, a concept
that particularly pleases him.
In this step, we check for this construction and re-
turn the sentential, verbal, or nominal left sibling
of the shell noun phrase.
Modifier For shell nouns such as issue, phe-
nomenon, and policy, often the shell content is
given in the modifier of the shell noun, as shown
in (10).
(10) But in the 18th century, Leipzig?s central
location in German-speaking Europe and
the liberal trade policy of the Saxon court
fostered publishing.
We deal with such cases as follows. First, we
extract the modifier phrases by concatenating the
modifier words having noun, verb, or adjective
part-of-speech tags. To exclude unlikely modi-
fier phrases as shell content (e.g., good idea, big
504
Parent
NP
Subject
VP
VB*
form of be
NP/NN
head = shell
(a) Sub-be-N pattern
Parent
NP/NN
head = shell
VP
VB*
form of be
SBAR/S
IN
that/wh
S
clause
(b) N-be-clause pattern
Parent
NP/NN
head = shell
SBAR/S
IN
that/wh
S
clause
(c) N-that/wh pattern
Figure 1: Lexico-syntactic patterns for shell nouns
issue), we extract a list of modifiers for a num-
ber of shell nouns and create a stoplist of modi-
fiers. If any of the words in the modifier phrases
is a pronoun or occurs in the stoplist, we move to
the next pattern. If the modifier phrase passes the
stoplist test, to distinguish between non-shell con-
tent and shell content modifiers, we examine the
hypernym paths of the words in the modifier
phrase in WordNet (Fellbaum, 1998). If the synset
abstraction.n.06 occurs in the path, we consider
the modifier phrase to be valid shell content, as-
suming that the shell content of shell nouns most
typically represents an abstract entity.
5.2 Resolving remaining instances
At this stage we are assuming that the shell con-
tent occurs either in the postnominal clause or the
complement clause. So we look for the patterns
below, returning the shell content when found.
N-be-clause The lexico-grammatical pattern
corresponding to the pattern N-be-clause is shown
in Figure 1(b). This is one of the more reliable
patterns for shell content extraction, as the be verb
suggests the semantic identity between the shell
noun and the complement clause. The be-verb
does not necessarily have to immediately follow
the shell noun. For instance, in example (2), the
head of the NP The issue that this country and
Congress must address is the shell noun issue, and
hence it satisfies the construction in Figure 1(b).
N-clause Finally, we look for this pattern. An
example of this pattern is shown in Figure 1(c).
This is the most common (see Table 2) and tricki-
est pattern in terms of resolution, and whether the
shell content is given in the postnominal clause or
not is dependent on the properties of the shell noun
under consideration and the syntactic construction
of the example. For instance, for the shell noun
decision, the postnominal to-infinitive clause typi-
cally represents shell content. But this did not hold
for the shell noun reason, as shown in (11).
(11) The reason to resist becoming a partici-
pant is obvious.
Here, Schmid?s semantic families come in the
picture. We wanted to examine a) the extent to
which the previous steps help in resolution, and b)
whether knowledge extracted from Schmid?s fam-
ilies add value to the resolution. So we employ
two versions of this step.
Include Schmid?s cues (+SC) This version
exploits the knowledge encoded in Schmid?s se-
mantic families (Section 4.2), and extracts post-
nominal clauses only if Schmid?s pattern cues are
satisfied. In particular, given a shell noun, we de-
termine the families in which it occurs and list all
possible patterns of these families as shell content
cues. The postnominal clause is a valid shell con-
tent only if it satisfies these cues. For instance,
the shell noun reason occurs in only one family:
Reason, with the allowed shell content patterns N-
that and N-why. Schmid?s patterns suggest that the
postnominal to-infinitive clauses are not allowed
as shell content for this shell noun, and thus this
step will return None. This version helps correctly
resolving examples such as (11) to None.
Exclude Schmid?s cues (?SC) This version
does not enforce Schmid?s cues in extracting the
postnominal clauses. For instance, the Problem
family does not include N-that/wh/to/of patterns,
but in this condition, we nonetheless allow these
patterns in extracting the shell content of the nouns
from this family.
6 Evaluation data
We claim that our algorithm is able to resolve a
variety of shell nouns. That said, creating eval-
uation data for all of Schmid?s 670 English shell
505
nouns is extremely time-consuming, and is there-
fore not pursued further in the current study. In-
stead we create a sample of representative evalua-
tion data to examine how well the algorithm works
a) on a variety of shell nouns, b) for shell nouns
within a family, c) for shell nouns across families
with completely different semantic and syntactic
expectations, and d) for a variety of shell patterns
from Table 1.
6.1 Selection of nouns
Recall that each shell noun has its idiosyncrasies.
So in order to evaluate whether our algorithm is
able to address these idiosyncrasies, the evalua-
tion data must contain a variety of shell nouns with
different semantic and syntactic expectations. To
examine a), we consider the six families shown in
Table 3. These families span three abstract cat-
egories: mental, eventive, and factual, and five
distinct groups: conceptual, volitional, factual,
causal, and attitudinal. Also, the families have
considerably different syntactic expectations. For
instance, the nouns in the Idea family can have
their content in that or of clauses occurring in N-
clause or N-be-clause constructions, whereas the
Trouble and Problem families do not allow N-
clause pattern. The shell content of the nouns in
the Plan family is generally represented with to-
infinitive clauses. To examine b) and c), we choose
three nouns from each of the first four families
from Table 3. To add diversity, we also include
two shell nouns from the Thing family and a shell
noun from the Reason family. So we selected a
total of 12 shell nouns for evaluation: idea, issue,
concept, decision, plan, policy, problem, trouble,
difficulty, reason, fact, and phenomenon.
6.2 Selection of instances
Recall that the shell content varies based on the
shell noun and the pattern it follows. Moreover,
shell nouns have pattern preferences, as shown in
Table 2. To examine d), we need shell noun exam-
ples following different patterns from Table 1. We
consider the New York Times corpus as our base
corpus, and from this corpus extract all sentences
following the lexico-grammatical patterns in Ta-
ble 1 for the twelve selected shell nouns. Then we
arbitrarily pick 100 examples for each shell noun,
making sure that the selection contains examples
of each cataphoric pattern from Table 1. These
examples consist of 70% examples of each of the
seven cataphoric patterns, and the remaining 30%
of the examples are picked randomly from the dis-
tribution of patterns for that shell noun.
6.3 Crowdsourcing annotation
We designed a crowdsourcing experiment to ob-
tain the annotated data for evaluation. We parse
each sentence using the Stanford parser, and ex-
tract all possible candidates, i.e., arguments of the
shell noun from the parser?s output. Since our ex-
amples include embedding clauses and sentential
complements, the parser is often inaccurate. For
instance, in example (12), the parser attaches only
the first clause of the coordination (that people
were misled) to the shell noun fact.
(12) The fact that people were misled and in-
formation was denied, that?s the reason
that you?d wind up suing.
To deal with such parsing errors, we consider the
30-best parses given by the parser. From these
parses, we extract a list of eligible candidates. This
list includes the arguments of the shell noun given
in the appositional clauses, modifier phrases, post-
nominal that, wh, or to-infinitive clauses, comple-
ment clauses, objects of postnominal prepositions
of the shell noun, and subject if the shell noun fol-
lows subject-be-N construction. On average, there
were three candidates per instance.
After extracting the candidates, we present the
annotators with the sentence, with the shell noun
highlighted, and the extracted candidates. We ask
the annotators to choose the option that provides
the correct interpretation of the highlighted shell
noun. We also provide them the option None of
the above, and ask them to select it if the shell con-
tent is not present in the given sentence or the shell
content is not listed in the list of candidates.
CrowdFlower We used CrowdFlower
11
as our
crowdsourcing platform, which in turn uses vari-
ous worker channels such as Amazon Mechanical
Turk
12
. CrowdFlower offers a number of features.
First, it provides a quiz mode which facilitates
filtering out spammers by requiring an annotator
to pass a certain number of test questions before
starting the real annotation. Second, during an-
notation, it randomly presents test questions with
known answers to the annotators to keep them on
their toes. Based on annotators? responses to the
test questions, each annotator is assigned a trust
11http://crowdflower.com/
12https://www.mturk.com/mturk/welcome
506
? 5 ? 4 ? 3 < 3
idea 53 67 95 5
issue 44 65 95 5
concept 40 56 96 4
decision 50 72 98 2
plan 41 55 95 5
policy 42 61 94 6
problem 52 70 100 0
trouble 44 69 99 1
difficulty 45 61 96 4
reason 48 60 93 7
fact 52 68 98 2
phenomenon 39 56 95 5
all 46 63 96 4
Table 4: Annotator agreement on shell content.
Each column shows the percentage of instances on
which at least n or fewer than n annotators agree
on a single answer.
score: an annotator performing well on the test
questions gets a high trust score. Finally, Crowd-
Flower allows the user to select the permitted de-
mographic areas and skills required.
Settings We asked for at least 5 annotations per
instance by annotators from the English-speaking
countries. The evaluation task contained a total
of 1200 instances, 100 instances per shell noun.
To maintain the annotation quality, we included
105 test questions, distributed among different an-
swers. We paid 2.5 cents per instance and the an-
notation task was completed in less than 24 hours.
Results Table 4 shows the agreement of the
crowd. In most cases, at least 3 out of 5 anno-
tators agreed on a single answer. We took this an-
swer as the gold standard in our evaluation, and
discard the instances where fewer than three anno-
tators agreed. The option None of the above was
annotated for about 30% of the cases. We include
these cases in the evaluation. In total we had 1,257
instances (1,152 instances where at least 3 annota-
tors agreed + 105 test questions).
7 Evaluation results
Baseline We evaluate our algorithm against
crowd-annotated data using a lexico-syntactic
clause (LSC) baseline. Given a sentence con-
taining a shell instance and its parse tree, this
baseline extracts the postnominal or complement
clause from the parse tree depending only upon
the lexico-syntactic pattern of the shell noun. For
instance, for the N-that and N-be-to patterns, it ex-
Nouns LSC A?SC A+SC
1 idea 74 82 83
2 issue 60 75 77
3 concept 51 67 68
4 decision 70 71 73
5 plan 51 63 62
6 policy 58 70 52
7 problem 66 69 59
8 trouble 63 68 50
9 difficulty 68 75 49
10 reason 43 53 77
11 fact 43 55 68
12 phenomenon 33 62 50
13 all 57 69 64
Table 5: Shell noun resolution results. Each col-
umn shows the percent accuracy of resolution with
the respective method. Boldface is best in row.
tracts the postnominal that clause and the comple-
ment to-infinitive clause, respectively.
13
Results Table 5 shows the evaluation results for
the LSC baseline, the algorithm without Schmid?s
cues (A?SC), and the algorithm with Schmid?s
cues (A+SC). The A?SC condition in all cases and
the A+SC condition in some cases outperform the
LSC baseline, which proves to be rather low, espe-
cially for the shell nouns with strict syntactic ex-
pectations (e.g., fact and reason). Thus we see that
our algorithm is adding value.
That said, we observe a wide range of per-
formance for different shell nouns. On the up
side, adding Schmid?s cues helps resolving the
shell nouns with strict syntactic expectations. The
A+SC results for the shell nouns idea, issue, con-
cept, decision, reason, and fact outperform the
baseline and the A?SC results. In particular, the
A+SC results for the shell nouns fact and rea-
son are markedly better than the baseline results.
These nouns have strict syntactic expectations for
the shell content clauses they take: the families
Thing and Certainty of the shell noun fact allow
only a that clause, and the Reason family of the
shell noun reason allows only that and because
clauses for the shell content. These cues help
in correctly resolving examples such as (11) to
None, where the postnominal to-infinitive clause
13
Note that we only extract subordinating clauses (e.g.,
(SBAR (IN that) (clause))) and to-infinitive clauses, and not
relative clauses.
507
describes the purpose or the goal for the reason,
but not the shell content itself.
On the down side, adding Schmid?s cues hurts
the performance of more versatile nouns, which
can take a variety of clauses. Although the A?SC
results for the shell nouns plan, policy, problem,
trouble, difficulty, and phenomenon are well above
the baseline, the A+SC results are markedly be-
low it. That is, Schmid?s cues were deleterious.
Our error analysis revealed that these nouns are
versatile in terms of the clauses they take as shell
content, and Schmid?s cues restrict these clauses
to be selected as shell content. For instance, the
shell noun problem occurs in two semantic fami-
lies with N-be-that/of and N-be-to as pattern cues
(Table 3), and postnominal clauses are not allowed
for this noun. Although these cues help in filtering
some unwanted cases, we observed a large number
of cases where the shell content is given in post-
nominal clauses, as shown in (13).
(13) I was trying to address the problem of un-
reliable testimony by experts in capital
cases.
Similarly, the Plan family does not allow the N-
of pattern. This cue works well for the shell noun
decision from the same family because often the
postnominal of clause is the agent for this shell
noun and not the shell content. However, it hurts
the performance of the shell noun policy, as N-
of is a common pattern for this shell noun (e.g.,
. . . officials in Rwanda have established a policy of
refusing to protect refugees. . . ). Other failures of
the algorithm are due to parsing errors and lack of
inclusion of context information.
8 Discussion and conclusion
In this paper, we proposed a general method to re-
solve shell nouns following cataphoric construc-
tions. This is a first step towards end-to-end shell
noun resolution. In particular, this method can
be used to create training data for any given shell
noun, which can later be used to resolve harder
anaphoric cases of that noun using the method that
we proposed earlier (Kolhatkar et al., 2013b).
The first goal of this work was to point out the
difficulties associated with the resolution of cat-
aphoric cases of shell nouns. The low resolution
results of the LSC baseline demonstrate the diffi-
culties of resolving such cases using syntax alone,
suggesting the need for incorporating more lin-
guistic knowledge in the resolution.
The second goal of this work was to examine to
what extent knowledge derived from the linguis-
tics literature helps in resolving shell nouns. We
conclude that Schmid?s pattern and clausal cues
are useful for resolving nouns with strict syntac-
tic expectations (e.g., fact, reason); however, these
cues are defeasible: they miss a number of cases in
our corpus. It is possible to improve on Schmid?s
cues using crowdsourcing annotation and by ex-
ploiting lexico-syntactic patterns associated with
different shell nouns from a variety of corpora.
One limitation of our approach is that in our res-
olution framework, we do not consider the prob-
lem of ambiguity of nouns that might not be used
as shell nouns. The occurrence of nouns with the
lexical patterns in Table 1 does not always guaran-
tee shell noun usage. For instance, in our data, we
observed a number of instances of the noun issue
with the publication sense (e.g., this week?s issue
of Sports Illustrated).
Our algorithm is able to deal with only a re-
stricted number of shell noun usage constructions,
but the shell content can be expressed in a variety
of other constructions. A robust machine learning
approach that incorporates context and deeper se-
mantics of the sentence, along with Schmid?s cues,
could mitigate this limitation.
This work opens a number of new research di-
rections. Our next planned task is clustering dif-
ferent shell nouns based on the kind of comple-
ments they take in different usages similar to verb
clustering (Merlo and Stevenson, 2000; Schulte im
Walde and Brew, 2002).
Acknowledgements
We thank the anonymous reviewers for their com-
ments. We also thank Suzanne Stevenson, Gerald
Penn, Heike Zinsmeister, Kathleen Fraser, Aida
Nematzadeh, and Ryan Kiros for their feedback.
This research was financially supported by the
Natural Sciences and Engineering Research Coun-
cil of Canada and by the University of Toronto.
References
Nicholas Asher. 1993. Reference to Abstract Objects
in Discourse. Kluwer Academic Publishers, Dor-
drecht, Netherlands.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
508
ings of the 17th International Conference on Com-
putational Linguistics, volume 1 of COLING ?98,
pages 86?90, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Douglas Biber, Stig Johansson, Geoffrey Leech, Su-
san Conrad, and Edward Finegan. 1999. Longman
Grammar of Spoken and Written English. Pearson
ESL, November.
Simon Philip Botley. 2006. Indirect anaphora: Testing
the limits of corpus-based linguistics. International
Journal of Corpus Linguistics, 11(1):73?112.
Donna K. Byron. 2003. Annotation of pronouns and
their antecedents: A comparison of two domains.
Technical report, University of Rochester. Computer
Science Department.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Miriam Eckert and Michael Strube. 2000. Dialogue
acts, synchronizing units, and anaphora resolution.
Journal of Semantics, 17:51?89.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 6(2):222?
254.
John Flowerdew. 2003. Signalling nouns in discourse.
English for Specific Purposes, 22(4):329?346.
Gill Francis. 1994. Labelling discourse: An aspect of
nominal group lexical cohesion. In M. Coulthard,
editor, Advances in written text analysis, pages 83?
101. Routledge, London.
Matthew Gerber, Joyce Chai, and Adam Meyers. 2009.
The role of implicit argumentation in nominal srl.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 146?154, Boulder, Colorado, June.
Association for Computational Linguistics.
Nancy Hedberg, Jeanette K. Gundel, and Ron
Zacharski. 2007. Directly and indirectly anaphoric
demonstrative and personal pronouns in newspaper
articles. In Proceedings of DAARC-2007 8th Dis-
course Anaphora and Anaphora Resolution Collo-
quium, pages 31?36.
Eli Hinkel. 2004. Teaching Academic ESL Writ-
ing: Practical Techniques in Vocabulary and Gram-
mar (ESL and Applied Linguistics Professional).
Lawrence Erlbaum, Mahwah, NJ, London.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.
Roz Ivanic. 1991. Nouns in search of a context: A
study of nouns with both open- and closed-system
characteristics. International Review of Applied Lin-
guistics in Language Teaching, 29:93?114.
Varada Kolhatkar, Heike Zinsmeister, and Graeme
Hirst. 2013a. Annotating anaphoric shell nouns
with their antecedents. In Proceedings of the 7th
Linguistic Annotation Workshop and Interoperabil-
ity with Discourse, pages 112?121, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Varada Kolhatkar, Heike Zinsmeister, and Graeme
Hirst. 2013b. Interpreting anaphoric shell nouns us-
ing antecedents of cataphoric shell nouns as training
data. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 300?310, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Paola Merlo and Suzanne Stevenson. 2000. Automatic
verb classification based on statistical distributions
of argument structure. Computational Linguistics,
27(3):373?408.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In In Proceedings of the
NAACL/HLT Workshop on Frontiers in Corpus An-
notation.
Christoph M?ller. 2008. Fully Automatic Resolution of
It, This and That in Unrestricted Multi-Party Dialog.
Ph.D. thesis, Universit?t T?bingen.
Costanza Navarretta. 2011. Antecedent and referent
types of abstract pronominal anaphora. In Proceed-
ings of the Workshop Beyond Semantics: Corpus-
based investigations of pragmatic and discourse
phenomena, G?ttingen, Germany, Feb.
Rebecca J. Passonneau. 1989. Getting at discourse ref-
erents. In Proceedings of the 27th Annual Meeting
of the Association for Computational Linguistics,
pages 51?59, Vancouver, British Columbia, Canada.
Association for Computational Linguistics.
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco, May. European Language Resources As-
sociation (ELRA).
Massimo Poesio, Simone Ponzetto, and Yannick Vers-
ley. 2011. Computational models of anaphora reso-
lution: A survey. Unpublished.
Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 912?919, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
509
Hans-J?rg Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Top-
ics in English Linguistics 34. Mouton de Gruyter,
Berlin.
Sabine Schulte im Walde and Chris Brew. 2002. In-
ducing German semantic verb classes from purely
syntactic subcategorisation information. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 223?230,
Philadelphia, PA.
Leonard Talmy. 2000. The windowing of attention.
In Toward a Cognitive Semantics, volume 1, pages
257?309. The MIT Press.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton and Co., The Netherlands.
510
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 112?121,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Annotating Anaphoric Shell Nouns with their Antecedents
Varada Kolhatkar
Department of Computer Science
University of Toronto
varada@cs.toronto.edu
Heike Zinsmeister
Institut fu?r Maschinelle Sprachverarbeitung
Universita?t Stuttgart
zinsmeis@ims.stuttgart-uni.de
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
Anaphoric shell nouns such as this is-
sue and this fact conceptually encapsulate
complex pieces of information (Schmid,
2000). We examine the feasibility of anno-
tating such anaphoric nouns using crowd-
sourcing. In particular, we present our
methodology for reliably annotating an-
tecedents of such anaphoric nouns and the
challenges we faced in doing so. We also
evaluated the quality of crowd annotation
using experts. The results suggest that
most of the crowd annotations were good
enough to use as training data for resolv-
ing such anaphoric nouns.
1 Introduction
Anaphoric shell nouns (ASNs) such as this fact,
this possibility, and this issue are common in all
kinds of text. They are called shell nouns be-
cause they provide nominal conceptual shells for
complex chunks of information representing ab-
stract concepts such as fact, proposition, and event
(Schmid, 2000). An example is shown in (1).
(1) Despite decades of education and widespread course
offerings, the survival rate for out-of-hospital car-
diac arrest remains a dismal 6 percent or less
worldwide.
This fact prompted the American Heart Association
last November to simplify the steps of CPR to make
it easier for lay people to remember and to encour-
age even those who have not been formally trained
to try it when needed.
Here, the ASN this fact encapsulates the clause
marked in bold from the preceding paragraph.
ASNs play an important role in organizing a dis-
course. First, they are used metadiscursively to
talk about the current discourse. In (1), the au-
thor characterizes the information presented in the
context by referring to it as a fact ? a thing that
is indisputably the case. Second, they are used as
cohesive devices in a discourse. In (1), for exam-
ple, this fact on the one hand refers to the propo-
sition marked in bold, and on the other, faces for-
ward and serves as the starting point of the follow-
ing paragraph. Finally, as Schmid (2000) points
out, like conjunctions so and however, ASNs
may function as topic boundary markers and topic
change markers.
Despite their importance, ASNs have not re-
ceived much attention in Computational Linguis-
tics. Although there has been some effort to anno-
tate certain anaphors with similar properties, i.e.,
demonstratives and the pronoun it (Byron, 2003;
Artstein and Poesio, 2006), in contrast to ordi-
nary nominal anaphora, there are not many anno-
tated corpora available that could be used to study
ASNs. Indeed, many questions of annotation of
ASNs must still be answered. For example, the
extent to which native speakers themselves agree
on the resolution of such anaphors, i.e., on the pre-
cise antecedents, remains unclear.
An essential first step in this field of research
is therefore to clearly establish the extent of inter-
annotator agreement on antecedents of ASNs as
a measure of feasibility of the task. In this pa-
per, we describe our methodology for annotating
ASNs using crowdsourcing, a cheap and fast way
of obtaining annotation. We also describe how we
evaluated the feasibility of the task and the quality
of the annotation, and the challenges we faced in
doing so, both with regard to the task itself and the
crowdsourcing platform we use. The results sug-
gest that most of the crowd-annotations were good
enough to use as training data for ASN resolution.
112
2 Related work
There exist only few annotated corpora of
anaphora with non-nominal antecedents (Dipper
and Zinsmeister, 2011). The largest one of these,
the ARRAU corpus (Poesio and Artstein, 2008),
contains 455 anaphors pointing to non-nominal
antecedents, but only a few instances are ASNs.
Kolhatkar and Hirst (2012) annotated antecedents
of the same type as we do, but restricted their ef-
forts to the ASN this issue.1 In addition, there are
corpora annotated with event anaphora in which
verbal instances are identified as proxies for non-
nominal antecedents (Pradhan et al, 2007; Chen
et al, 2011; Lee et al, 2012).
For the task of identifying non-nominal an-
tecedents as free spans of text, there is no standard
way of reporting inter-annotator agreement. Some
studies report only observed percentage agree-
ment with results in the range of about 0.40?
0.55 (Vieira et al, 2002; Dipper and Zinsmeis-
ter, 2011). The studies differed with respect to
number of annotators, types of anaphors, and lan-
guage of the corpora. Artstein and Poesio (2006)
discuss Krippendorff?s alpha for chance-corrected
agreement. They considered antecedent strings as
bags of words and computed the degree of differ-
ence between them by different distance measures
(e.g. Jaccard, Dice). The bag-of-words approach
is rather optimistic in the sense that even two non-
overlapping strings are very likely to share at least
a few words. Kolhatkar and Hirst (2012) followed
a different approach by using Krippendorff?s uni-
tizing alpha (u?) which considers the longest com-
mon subsequence of different antecedent options
(Krippendorff, 2013). They reported high chance-
corrected u? of 0.86 for two annotators but in a
very restricted domain.
There has been some prior effort to annotate
anaphora and coreference using Games with a
Purpose as a method of crowdsourcing (Chamber-
lain et al, 2009; Hladka? et al, 2009). Another, less
time-consuming approach of crowdsourcing is us-
ing platforms such as Amazon Mechanical Turk2.
It has been shown that crowdsourced data can suc-
cessfully be used as training data for NLP tasks
(Hsueh et al, 2009).
1Another data set reported in the literature could have
been relevant for us: Botley?s (2006) corpus contained about
462 ASN instances signaled by shell nouns; but this data is
no longer available (S. Botley, p.c.).
2https://mturk.com/mturk/
Class Description Examples
factual states of affairs fact, reason
linguistic linguistic acts question, report
mental thoughts and ideas issue, decision
modal subjective judgements possibility, truth
eventive events act, reaction
circumstantial situations situation, way
Table 1: Schmids categorization of shell nouns.
The nouns in boldface are used in this research.
3 The Anaphoric Shell Noun Corpus
Our goal is to obtain annotated data for ASN an-
tecedents that could be used to train a supervised
machine learning system to resolve ASNs. For
that, we created the Anaphoric Shell Noun (ASN)
corpus.
Schmid (2000) provides a list of 670 English
nouns which are frequently used as shell nouns.
He divides them into six broad semantic classes:
factual, mental, linguistic, modal, circumstantial,
and eventive. Table 1 shows this classification,
along with example shell nouns for each category.
To begin with, we considered articles contain-
ing occurrences of these 670 shell nouns from the
New York Times (NYT) corpus (about 711,046
occurrences).3 To create a corpus of a manage-
able size for annotation, we considered first 10
highly frequent shell nouns distributed across each
of Schmid?s shell noun categories from Table 1
and extracted ASN instances by searching for the
pattern {this shell noun} in these articles.4
To examine the feasibility of the annotation, we
systematically annotated sample data ourselves,
which contained about 15 examples of each of
these 10 highly frequent shell nouns. The anno-
tation process revealed that not all ASN instances
are easy to resolve. The instances with shell nouns
from the circumstantial and eventive categories, in
particular, had very long and unclear antecedents.
So we excluded these categories in this research
and work with six shell nouns from the other four
categories: fact, reason, issue, decision, question,
and possibility. To create the ASN corpus, we
extracted about 500 instances for each of these
six shell nouns. After removing duplicates and
instances with a non-abstract sense (e.g., this is-
3http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2008T19
4Schmid (2000) provides patterns for anaphoric shell
nouns, and this-NP is the most prominent pattern among
them.
113
sue with a publication-related sense), we were left
with 2,822 ASN instances.
4 ASN Annotation Challenges
ASN antecedent annotation is a complex task, as
it involves deeply understanding the discourse and
interpreting it. Here we point out two main chal-
lenges associated with the task.
What to annotate? The question of ?what to an-
notate? as mentioned by Fort et al (2012) is not
straightforward for ASN antecedents, as the no-
tion of markables is complex compared to ordi-
nary nominal anaphora: the units on which the
annotation work should focus are heterogeneous.5
Moreover, due to this heterogeneous nature of an-
notation units, there is a huge number of mark-
ables (e.g., all syntactic constituents given by a
syntactic parse tree). So there are many options
to choose from, while only a few units are actu-
ally to be annotated. Moreover, there is no one-
to-one correspondence between the syntactic type
of an antecedent and the semantic type of its refer-
ent (Webber, 1991). For instance, a semantic type
such as fact can be expressed with different syn-
tactic shapes such as a clause, a verb phrase, or a
complex sentence. Conversely, a syntactic shape,
such as a clause, can function as several semantic
types, including fact, proposition, and event.
Lack of the notion of the right answer It is not
obvious how to define clear and detailed annota-
tion guidelines to create a gold-standard corpus
for ASN antecedent annotation due to our limited
understanding of the nature and interpretation of
such antecedents. The notion of the right answer
is not well-defined for ASN antecedents. Indeed
most people will be hard-pressed to say whether
or not to include the clause Despite decades of
education and widespread course offerings in the
antecedent of this fact in example (1). The main
challenge is to identify the conditions when two
different candidates for annotation should be con-
sidered as representing essentially the same con-
cept, which raises deep philosophical issues that
we do not propose to solve in this paper. For our
purposes, we believe, this challenge could only
be possibly tackled by the requirements of down-
stream applications of ASN resolution.
5Occasionally, ASN antecedents are non-contiguous
spans of text, but in this work, we ignore them for simplicity.
5 Annotation Methodology
Considering the difficulties of ASN annotation
discussed above, there were two main challenges
involved in the annotation process: first, to find an-
notators who can annotate data reliably with min-
imal guidelines, and second, to design simple an-
notation tasks that will elicit data useful for our
purposes. Now we discuss how we dealt with
these challenges.
Crowdsourcing We wanted to examine to what
extent non-expert native speakers of English with
minimal annotation guidelines would agree on
ASN antecedents. We explored the possibility of
using crowdsourcing, which is an effective way to
obtain annotations for natural language research
(Snow et al, 2008). In particular, we explored the
use of CrowdFlower6, a crowdsourcing platform
that in turn uses various worker channels such as
Amazon Mechanical Turk. CrowdFlower offers a
number of features.
First, it offers a number of integrated quality-
control mechanisms. For instance, it throws gold
questions randomly at the annotators, and anno-
tators who do not answer them correctly are not
allowed to continue. To further minimize spam-
mers, it also offers a training phase before the ac-
tual annotation. In this phase, every annotator is
presented with a few gold questions. Only those
annotators who get the gold questions right get ad-
mittance to do the actual annotation.
Second, CrowdFlower chooses a unique answer
for each annotation unit based on the majority vote
of the trusted annotators. For each annotator, it
assigns a trust level based on how she performs
on the gold examples. The unique answer is com-
puted by adding together the trust scores of an-
notators, and then picking the answer with the
highest sum of trusts (CrowdFlower team, p.c.).
It also assigns a confidence score (denoted as c
henceforth) for each answer, which is a normal-
ized score of the summation of the trusts. For ex-
ample, suppose annotators A, B, and C with trust
levels 0.75, 0.75, and 1.0 give answers no, yes, yes
respectively for a particular instance. Then the an-
swer yes will score 1.75 and answer no will score
0.75 and yes will be chosen as the crowd?s answer
with c = 0.7 (i.e., 1.75/(1.75 + 0.75)). We use
these confidence scores in our analysis of inter-
annotator agreement below.
6http://crowdflower.com/
114
Finally, CrowdFlower also provides detailed an-
notation results including demographic informa-
tion and trustworthiness of each annotator.
Design of the annotation tasks With the help of
well-designed gold examples, CrowdFlower can
get rid of spammers and ensures that only reliable
annotators perform the annotation task. But the
annotation task must be well-designed in the first
place to get a good quality annotation. Following
the claim in the literature that with crowdsourc-
ing platforms simple tasks do best (Madnani et al,
2010; Wang et al, 2012), we split our annotation
task into two relatively simple sequential annota-
tion tasks. First, identifying the broad region of the
antecedent, i.e., not the precise antecedent but the
region where the antecedent lies, and second, iden-
tifying the precise antecedent, given the broad re-
gion of the antecedent. Now we will discuss each
of our annotation tasks in detail.
5.1 CrowdFlower experiment 1
The first annotation task was about identifying the
broad region of ASN antecedents without actu-
ally pinpointing the precise antecedents. We de-
fined the broad region as the sentence containing
the ASN antecedent, as the shell nouns we have
chosen tend to have antecedents that lie within
a single sentence. We designed a CrowdFlower
experiment where we presented to the annotators
ASNs from the ASN corpus with three preceding
paragraphs as context. Sentences in the vicinity
of ASNs were each labelled: four sentences pre-
ceding the anaphor, the sentence containing the
anaphor, and two sentences following the anaphor.
This choice was based on our pilot annotation:
the antecedents very rarely occur more than four
sentences away from the anaphor. The annota-
tion task was to pinpoint the sentence in the pre-
sented text that contained the antecedent for the
ASN and selecting the appropriate sentence label
as the correct answer. If no labelled sentence in the
presented text contained the antecedent, we sug-
gested to the annotators to select None. If the an-
tecedent spanned more than one sentence, then we
suggested to them to select Combination. We also
provided a link to the complete article from which
the text was drawn in case the annotators wanted
to have a look at it.
Settings We asked for 8 judgements per instance
and paid 8 cents per annotation unit. Our job
contained in total 2,822 annotation units with 168
gold units. As we were interested in the ver-
dict of native speakers of English, we limited the
allowed demographic region to English-speaking
countries.
5.2 CrowdFlower experiment 2
This annotation task was about pinpointing the
exact antecedent text of the ASN instances. We
designed a CrowdFlower experiment, where we
presented to the annotators ASN instances from
the ASN corpus with highlighted ASNs and the
sentences containing the antecedents, the output
of experiment 1. One way to pinpoint the ex-
act antecedent string is to ask the annotators to
mark free spans of text within the antecedent sen-
tence, similar to Byron (2003) and Artstein and
Poesio (2006). However, CrowdFlower quality-
control mechanisms require multiple-choice an-
notation labels. So we decided to display a set
of labelled candidates to the annotators and ask
them to choose the answer that best represents
the ASN antecedent. A practical requirement of
this approach is that the number of options to be
displayed be only a handful in order to make it
a feasible task for online annotation. But as we
noted in Section 4, the number of markables for
ASN antecedents is large. If, for example, we de-
fine markables as all syntactic constituents given
by the Stanford parser7, there are on average 49.5
such candidates per sentence in the ASN corpus. It
is not practical to display all these candidates and
to ask CrowdFlower annotators to choose one an-
swer from this many options. Also, some potential
candidates are clearly not appropriate candidates
for a particular shell noun. For instance, the NP
constituent the survival rate in example (1) is not
an appropriate candidate for the shell noun fact as
generally facts are propositions. So the question is
whether it is possible to restrict this set of candi-
dates by discarding unlikely ones.
To deal with this question, we used super-
vised machine learning methods trained on easy,
non-anaphoric unlabelled examples of shell nouns
(e.g., the fact that X). In this paper, we will focus
on the annotation and will treat these methods as a
black box. In brief, the methods reduce the large
search space of ASN antecedent candidates to a
size that is manageable for crowdsourcing anno-
tation, without eliminating the most likely candi-
7http://nlp.stanford.edu/software/
lex-parser.shtml
115
dates. We displayed the 10 most-likely candidates
given by these methods. In addition, we made sure
not to display two candidates with only a negli-
gible difference. For example, given two candi-
dates, X and that X, which differ only with respect
to the introductory that, we chose to display only
the longer candidate that X.
In a controlled annotation, with detailed guide-
lines, such difficulties of selecting between minor
variations could be avoided. However, such de-
tailed annotation guidelines still have to be devel-
oped.
Settings As in experiment 1, we asked for 8
judgements per instance and paid 6 cents per anno-
tation unit. But for this experiment we considered
only 2,323 annotation units with 151 gold units,
only high-confidence units (c ? 0.5) from experi-
ment 1. This task turned out to be a suitable task
for crowdsourcing as it offered a limited number
of options to choose from, instead of asking the
annotators to mark arbitrary spans of text.
6 Agreement
Our annotation tasks pose difficulties in measur-
ing inter-annotator agreement both in terms of the
task itself and the platform used for annotation. In
this section, we describe our attempt to compute
agreement for each of our annotation tasks and the
challenges we faced in doing so.
6.1 CrowdFlower experiment 1
Recall that in this experiment, annotators identify
the sentence containing the antecedent and select
the appropriate sentence label as their answer. We
know from our pilot annotation that the distribu-
tion of such labels is skewed: most of the ASN an-
tecedents lie in the sentence preceding the anaphor
sentence. We observed the same trend in the re-
sults of this experiment. In the ASN corpus, the
crowd chose the preceding sentence 64% of the
time, the same sentence 13% of the time, and long-
distance sentences 23% of the time.8 Consider-
ing the skewed distribution of labels, if we use tra-
ditional agreement coefficients, such as Cohen?s
? (1960) or Krippendorff?s ? (2013), expected
agreement is very high, which in turn results in a
low reliability coefficient (in our case ? = 0.61)
that does not necessarily reflect the true reliability
of the annotation (Artstein and Poesio, 2008).
8This confirms Passonneau?s (1989) observation that non-
nominal antecedents tend to be close to the anaphors.
F R I D Q P all
c < .5 8 8 36 21 13 7 16
.5? c < .6 6 6 13 8 7 5 8
.6? c < .8 24 25 31 31 22 27 27
.8? c < 1. 22 23 11 14 19 25 18
c = 1. 40 38 9 26 39 36 31
Average c .83 .82 .61 .72 .80 .83 .76
Table 2: CrowdFlower confidence distribution for
CrowdFlower experiment 1. Each column shows
the distribution in percentages for confidence of
annotating antecedents of that shell noun. The fi-
nal row shows the average confidence of the dis-
tribution. Number of ASN instances = 2,822. F
= fact, R = reason, I = issue, D = decision, Q =
question, P = possibility.
One way to measure the reliability of the data,
without taking chance correction into account, is
to consider the distribution of the ASN instances
with different levels of CrowdFlower confidence.
Table 2 shows the percentages of instances in dif-
ferent confidence level bands for each shell noun
as well as for all instances. For example, for the
shell noun fact, 8% of the total number of this fact
instances were annotated with c < 0.5. As we
can see, most of the instances of the shell nouns
fact, reason, question, and possibility were anno-
tated with high confidence. In addition, most of
them occurred in the band 0.8 ? c ? 1. There
are relatively few instances with low confidence
for these nouns, suggesting the feasibility of re-
liable antecedent annotation for these nouns. By
contrast, the mental nouns issue and decision had
a large number of low-confidence (c < 0.5) in-
stances, bringing in the question of reliability of
antecedent annotation of these nouns.
Given these results with different confidence
levels, the primary question is what confidence
level should be considered acceptable? For our
task, we required that at least four trusted anno-
tators out of eight annotators should agree on an
answer for it to be acceptable.9 We will talk about
acceptability later in Section 7.
6.2 CrowdFlower experiment 2
Recall that this experiment was about identifying
the precise antecedent text segment given the sen-
tence containing the antecedent. It is not clear
what the best way to measure the amount of such
9We chose this threshold after systematically examining
instances with different confidence levels.
116
Jaccard Dice
Do De ? Do De ?
A&P .53 .95 .45 .43 .94 .55
Our results .47 .96 .51 .36 .92 .61
Table 3: Agreement using Krippendorff?s ? for
CrowdFlower experiment 2. A&P = Artstein and
Poesio (2006).
agreement is. Agreement coefficients such as Co-
hen?s ? underestimate the degree of agreement for
such annotation, suggesting disagreement even be-
tween two very similar annotated units (e.g., two
text segments that differ in just a word or two).
We present the agreement results in three different
ways: Krippendorff?s ? with distance metrics Jac-
card and Dice (Artstein and Poesio, 2006), Krip-
pendorff?s unitizing alpha (Krippendorff, 2013),
and CrowdFlower confidence values.
Krippendorff?s ? using Jaccard and Dice To
compare our agreement results with previous ef-
forts to annotate such antecedents, following Art-
stein and Poesio (2006), we computed Krippen-
dorff?s ? using distance metrics Jaccard and Dice.
The general form of coefficient ? is:
? = 1? Do
De
(1)
where Do and De are observed and expected dis-
agreements respectively. ? = 1 indicates perfect
reliability and u? = 0 indicates the absence of re-
liability. When u? < 0, either the sample size
is very small or the disagreement is systematic.
Table 3 shows the agreement results. Our agree-
ment results are comparable to Artstein and Poe-
sio?s agreement results. They had 20 annotators
annotating 16 anaphor instances with segment an-
tecedents, whereas we had 8 annotators annotat-
ing 2,323 ASN instances. As Artstein and Poesio
point out, expected disagreement in case of such
antecedent annotation is close to maximal, as there
is little overlap between segment antecedents of
different anaphors and therefore ? pretty much re-
flects the observed agreement.
Krippendorff?s unitizing ? (u?) Following
Kolhatkar and Hirst (2012), we use u? for measur-
ing reliability of the ASN antecedent annotation
task. This coefficient is appropriate when the an-
notators work on the same text, identify the units
in the text that are relevant to the given research
F R I D Q P all
c < .5 11 17 32 31 14 28 21
.5? c < .6 12 12 19 23 9 19 15
.6? c < .8 36 33 34 32 30 36 33
.8? c < 1. 24 22 10 10 21 13 18
c = 1. 17 16 5 3 26 4 13
Average c .74 .71 .60 .59 .77 .62 .68
Table 4: CrowdFlower confidence distribution for
CrowdFlower experiment 2. Each column shows
the distribution in percentages for confidence of
annotating antecedents of that shell noun. The fi-
nal row shows the average confidence of the dis-
tribution. Number of ASN instances = 2,323. F
= fact, R = reason, I = issue, D = decision, Q =
question, P = possibility.
question, and then label the identified units (Krip-
pendorff, p.c.). The general form of coefficient
u? is the same as in equation 1. In our context,
the annotators work on the same text, the ASN in-
stances. We define an elementary annotation unit
(the smallest separately judged unit) to be a word
token. The annotators identify and locate ASN
antecedents for the given anaphor in terms of se-
quences of elementary annotation units.
u? incorporates the notion of distance between
strings by using a distance function which is de-
fined as the square of the distance between the
non-overlapping tokens in our case. The distance
is 0 when the annotated units are exactly the same,
and is the summation of the squares of the un-
matched parts if they are different. We compute
observed and expected disagreement as explained
by Krippendorff (2013, Section 12.4). For our
data, u? was 0.54.10 u? was lower for the men-
tal nouns issue and decision and the modal noun
possibility compared to other shell nouns.
CrowdFlower confidence results We also ex-
amined different confidence levels for ASN an-
tecedent annotation. Table 4 gives confidence re-
sults for all instances and for each noun. In con-
trast with Table 2, the instances are more evenly
distributed here. As in experiment 1, the men-
tal nouns issue and decision had many low con-
fidence instances. For the modal noun possibility,
it was easy to identify the sentence containing the
antecedent, but pinpointing the precise antecedent
10Note that u? reported here is just an approximation of
the actual agreement as in our case the annotators chose an
option from a set of predefined options instead of marking
free spans of text.
117
turned out to be difficult.
Now we discuss the nature of disagreement in
ASN annotation.
Disagreement in experiment 1 There were two
primary sources of disagreement in experiment 1.
First, the annotators had problems agreeing on the
answer None. We instructed them to choose None
when the sentence containing the antecedent was
not labelled. Nonetheless, some annotators chose
sentences that did not precisely contain the actual
antecedent but just hinted at it. Second, sometimes
it was hard to identify the precise antecedent sen-
tence as the antecedent was either present in the
blend of all labelled sentences or there were multi-
ple possible answers, as shown in example (2).
(2) Any biography of Thomas More has to answer one
fundamental question. Why? Why, out of all the
many ambitious politicians of early Tudor England,
did only one refuse to acquiesce to a simple piece
of religious and political opportunism? What was it
about More that set him apart and doomed him to a
spectacularly avoidable execution?
The innovation of Peter Ackroyd?s new biography of
More is that he places the answer to this question
outside of More himself.
Here, the author formulates the question in a num-
ber of ways and any question mentioned in the
preceding text can serve as the antecedent of the
anaphor this question.
Hard instances Low agreement can indicate
different problems: unclear guidelines, poor-
quality annotators, or difficult instances (e.g., not
well understood linguistic phenomena) (Artstein
and Poesio, 2006). We can rule out the pos-
sibility of poor-quality annotators for two rea-
sons. First, we consider 8 diverse annotators
who work independently. Second, we use Crowd-
Flower?s quality-control mechanisms and hence
allow only trustworthy annotators to annotate our
texts. Regarding instructions, we take inter-
annotator agreement as a measure for feasibility of
the task, and hence we keep the annotation instruc-
tion as simple as possible. This could be a source
of low agreement. The third possibility is hard in-
stances. Our results show that the mental nouns
issue and decision had many low-confidence in-
stances, suggesting the difficulty associated with
the interpretation of these nouns (e.g., the very
idea of what counts as an issue is fuzzy). The shell
noun decision was harder because most of its in-
stances were court-decision related articles, which
were in general hard to understand.
Different strings representing similar concepts
As noted in Section 4, the main challenge with the
ASN annotation task is that different antecedent
candidates might represent the same concept and
it is not trivial to incorporate this idea in the anno-
tation process. When five trusted annotators iden-
tify the antecedent as but X and three trusted anno-
tators identify it as merely X, since CrowdFlower
will consider these two answers to be two com-
pletely different answers, it will give the answer
but X a confidence of only about 0.6. u? or ? with
Jaccard and Dice will not consider this as a com-
plete disagreement; however, the coefficients will
register it as a difference. In other words, the dif-
ference functions used with these coefficients do
not respond to semantics, paraphrases, and other
similarities that humans might judge as inconse-
quential. One way to deal with this problem would
be clustering the options that reflect essentially the
same concepts before measuring the agreement.
Some of these problems could also be avoided by
formulating instructions for marking antecedents
so that these differences do not occur in the iden-
tified antecedents. However, crowdsourcing plat-
forms require annotation guidelines to be clear and
minimal, which makes it difficult to control the an-
notation variations.
7 Evaluation of Crowd Annotation
CrowdFlower experiment 2 resulted in 1,810 ASN
instances with c > 0.5. The question is how good
are these annotations from experts? point of view.
To examine the quality of the crowd annotation
we asked two judges A and B to evaluate the ac-
ceptability of the crowd?s answers. The judges
were highly-qualified academic editors: A, a re-
searcher in Linguistics and B, a translator with a
Ph.D. in History and Philosophy of Science. From
the crowd-annotated ASN antecedent data, we
randomly selected 300 instances, 50 instances per
shell noun. We made sure to choose instances with
borderline confidence (0.5 ? c < 0.6), medium
confidence (0.6 ? c < 0.8), and high confidence
(0.8 ? c ? 1.0). We asked the judges to rate
the acceptability of the crowd-answers based on
the extent to which they provided interpretation of
the corresponding anaphor. We gave them four
options: perfectly (the crowd?s answer is perfect
and the judge would have chosen the same an-
tecedent), reasonably (the crowd?s answer is ac-
ceptable and is close to their answer),
118
Judge B
P R I N Total
Judge A
P 171 44 11 7 233
R 12 27 7 4 50
I 2 4 6 1 13
N 1 2 0 1 4
Total 186 77 24 13 300
Table 5: Evaluation of ASN antecedent annota-
tion. P = perfectly, R = reasonably, I = implicitly,
N = not at all
implicitly (the crowd?s answer only implicitly
contains the actual antecedent), and not at all (the
crowd?s answer is not in any way related to the
actual antecedent).11 Moreover, if they did not
mark perfectly, we asked them to provide their an-
tecedent string. The two judges worked on the task
independently and they were completely unaware
of how the annotation data was collected.
Table 5 shows the confusion matrix of the rat-
ings of the two judges. Judge B was stricter than
Judge A. Given the nature of the task, it was
encouraging that most of the crowd-antecedents
were rated as perfectly by both judges (72% by
A and 62% by B). Note that perfectly is rather a
strong evaluation for ASN antecedent annotation,
considering the nature of ASN antecedents them-
selves. If we weaken the acceptability criteria and
consider the antecedents rated as reasonably to be
also acceptable antecedents, 84.6% of the total in-
stances were acceptable according to both judges.
Regarding the instances marked implicitly, most
of the times the crowd?s answer was the closest
textual string of the judges? answer. So we again
might consider instances marked implicitly as ac-
ceptable answers.
For a very few instances (only about 5%) either
of the judges marked not at all. This was a posi-
tive result and suggests success of different steps
of our annotation procedure: identifying broad re-
gion, identifying the set of most likely candidates,
and identifying precise antecedent. As we can see
in Table 5, there were 7 instances where the judge
A rated perfectly while the judge B rated not at all,
i.e., completely contradictory judgements. When
we looked at these examples, they were rather hard
and ambiguous cases. An example is shown in (3).
The whether clause marked in the preceding sen-
11Before starting the actual annotation, we carried out a
training phase with 30 instances, which gave an opportunity
to the judges to ask questions about the task.
tence is the crowd?s answer. One of our judges
rated this answer as perfectly, while the other rated
it as not at all. According to her the correct an-
tecedent is whether Catholics who vote for Mr.
Kerry would have to go to confession.
(3) Several Vatican officials said, however, that any such
talk has little meaning because the church does not
take sides in elections. But the statements by several
American bishops that Catholics who vote for Mr.
Kerry would have to go to confession have raised
the question in many corners about whether this is
an official church position.
The church has not addressed this question publicly
and, in fact, seems reluctant to be dragged into the
fight...?
There was no notable relation between the an-
notator?s rating and the confidence level: many in-
stances with borderline confidence were marked
perfectly or reasonably, suggesting that instances
with c ? 0.5 were reasonably annotated instances,
to be used as training data for ASN resolution.
8 Conclusion
In this paper, we addressed the fundamental ques-
tion about feasibility of ASN antecedent annota-
tion, which is a necessary step before developing
computational approaches to resolve ASNs. We
carried out crowdsourcing experiments to get na-
tive speaker judgements on ASN antecedents. Our
results show that among 8 diverse annotators who
worked independently with a minimal set of an-
notation instructions, usually at least 4 annotators
converged on a single ASN antecedent. The re-
sult is quite encouraging considering the nature of
such antecedents.
We asked two highly-qualified judges to in-
dependently examine the quality of a sample of
crowd-annotated ASN antecedents. According to
both judges, about 95% of the crowd-annotations
were acceptable. We plan to use this crowd-
annotated data (1,810 instances) as training data
for an ASN resolver. We also plan to distribute the
annotations at a later date.
Acknowledgements
We thank the CrowdFlower team for their respon-
siveness and Hans-Jo?rg Schmid for helpful dis-
cussions. This material is based upon work sup-
ported by the United States Air Force and the De-
fense Advanced Research Projects Agency under
Contract No. FA8650-09-C-0179, Ontario/Baden-
Wu?rttemberg Faculty Research Exchange, and the
University of Toronto.
119
References
Ron Artstein and Massimo Poesio. 2006. Identify-
ing reference to abstract objects in dialogue. In
Proceedings of the 10th Workshop on the Semantics
and Pragmatics of Dialogue, pages 56?63, Potsdam,
Germany.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596, December.
Simon Philip Botley. 2006. Indirect anaphora: Testing
the limits of corpus-based linguistics. International
Journal of Corpus Linguistics, 11(1):73?112.
Donna K. Byron. 2003. Annotation of pronouns and
their antecedents: A comparison of two domains.
Technical report, University of Rochester. Computer
Science Department.
Jon Chamberlain, Udo Kruschwitz, and Massimo Poe-
sio. 2009. Constructing an anaphorically anno-
tated corpus with non-experts: Assessing the quality
of collaborative annotations. In Proceedings of the
2009 Workshop on The People?s Web Meets NLP:
Collaboratively Constructed Semantic Resources,
pages 57?62, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim
Tan. 2011. A unified event coreference resolu-
tion by integrating multiple resolvers. In Proceed-
ings of 5th International Joint Conference on Nat-
ural Language Processing, pages 102?110, Chiang
Mai, Thailand, November.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37.
Stefanie Dipper and Heike Zinsmeister. 2011. Anno-
tating abstract anaphora. Language Resources and
Evaluation, 69:1?16.
Kare?n Fort, Adeline Nazarenko, and Sophie Rosset.
2012. Modeling the complexity of manual anno-
tation tasks: a grid of analysis. In 24th Inter-
national Conference on Computational Linguistics,
pages 895?910.
Barbora Hladka?, Jir??? M??rovsky?, and Pavel Schlesinger.
2009. Play the language: Play coreference. In Pro-
ceedings of the Association of Computational Lin-
guistics and International Joint Conference on Nat-
ural Language Processing 2009 Conference Short
Papers, pages 209?212, Suntec, Singapore, August.
Association for Computational Linguistics.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: A study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 27?35, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Varada Kolhatkar and Graeme Hirst. 2012. Resolving
?this-issue? anaphora. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 1255?1265, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Klaus Krippendorff. 2013. Content Analysis: An
Introduction to Its Methodology. Sage, Thousand
Oaks, CA, third edition.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489?500, Jeju Island, Korea, July. Association for
Computational Linguistics.
Nitin Madnani, Jordan Boyd-Graber, and Philip
Resnik. 2010. Measuring transitivity using un-
trained annotators. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk, pages
188?194, Los Angeles, June. Association for Com-
putational Linguistics.
Rebecca J. Passonneau. 1989. Getting at discourse ref-
erents. In Proceedings of the 27th Annual Meeting
of the Association for Computational Linguistics,
pages 51?59, Vancouver, British Columbia, Canada,
June. Association for Computational Linguistics.
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proeedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco, May. European Language Resources As-
sociation (ELRA).
Sameer S. Pradhan, Lance A. Ramshaw, Ralph M.
Weischedel, Jessica MacBride, and Linnea Micci-
ulla. 2007. Unrestricted coreference: Identifying
entities and events in OntoNotes. In Proceedings of
the International Conference on Semantic Comput-
ing, pages 446?453, September.
Hans-Jo?rg Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Top-
ics in English Linguistics 34. De Gruyter Mouton,
Berlin.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it
good? evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 254?263, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Renata Vieira, Susanne Salmon-Alt, Caroline
Gasperin, Emmanuel Schang, and Gabriel Othero.
2002. Coreference and anaphoric relations of
120
demonstrative noun phrases in multilingual corpus.
In Proceedings of the 4th Discourse Anaphora and
Anaphor Resolution Conference (DAARC 2002),
pages 385?427, Lisbon, Portugal, September.
Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan.
2012. Perspectives on crowdsourcing annotations
for natural language processing. In Language Re-
sources and Evaluation, volume in press, pages 1?
23. Springer.
Bonnie Lynn Webber. 1991. Structure and ostension in
the interpretation of discourse deixis. In Language
and Cognitive Processes, pages 107?135.
121
