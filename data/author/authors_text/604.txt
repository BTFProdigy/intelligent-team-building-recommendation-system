Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 109?117,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Knowing the Unseen: Estimating Vocabulary Size over Unseen Samples
Suma Bhat
Department of ECE
University of Illinois
spbhat2@illinois.edu
Richard Sproat
Center for Spoken Language Understanding
Oregon Health & Science University
rws@xoba.com
Abstract
Empirical studies on corpora involve mak-
ing measurements of several quantities for
the purpose of comparing corpora, creat-
ing language models or to make general-
izations about specific linguistic phenom-
ena in a language. Quantities such as av-
erage word length are stable across sam-
ple sizes and hence can be reliably esti-
mated from large enough samples. How-
ever, quantities such as vocabulary size
change with sample size. Thus measure-
ments based on a given sample will need
to be extrapolated to obtain their estimates
over larger unseen samples. In this work,
we propose a novel nonparametric estima-
tor of vocabulary size. Our main result is
to show the statistical consistency of the
estimator ? the first of its kind in the lit-
erature. Finally, we compare our proposal
with the state of the art estimators (both
parametric and nonparametric) on large
standard corpora; apart from showing the
favorable performance of our estimator,
we also see that the classical Good-Turing
estimator consistently underestimates the
vocabulary size.
1 Introduction
Empirical studies on corpora involve making mea-
surements of several quantities for the purpose of
comparing corpora, creating language models or
to make generalizations about specific linguistic
phenomena in a language. Quantities such as av-
erage word length or average sentence length are
stable across sample sizes. Hence empirical mea-
surements from large enough samples tend to be
reliable for even larger sample sizes. On the other
hand, quantities associated with word frequencies,
such as the number of hapax legomena or the num-
ber of distinct word types changes are strictly sam-
ple size dependent. Given a sample we can ob-
tain the seen vocabulary and the seen number of
hapax legomena. However, for the purpose of
comparison of corpora of different sizes or lin-
guistic phenomena based on samples of different
sizes it is imperative that these quantities be com-
pared based on similar sample sizes. We thus need
methods to extrapolate empirical measurements of
these quantities to arbitrary sample sizes.
Our focus in this study will be estimators of
vocabulary size for samples larger than the sam-
ple available. There is an abundance of estima-
tors of population size (in our case, vocabulary
size) in existing literature. Excellent survey arti-
cles that summarize the state-of-the-art are avail-
able in (Bunge and Fitzpatrick, 1993) and (Gan-
dolfi and Sastri, 2004). Of particular interest to
us is the set of estimators that have been shown
to model word frequency distributions well. This
study proposes a nonparametric estimator of vo-
cabulary size and evaluates its theoretical and em-
pirical performance. For comparison we consider
some state-of-the-art parametric and nonparamet-
ric estimators of vocabulary size.
The proposed non-parametric estimator for the
number of unseen elements assumes a regime
characterizing word frequency distributions. This
work is motivated by a scaling formulation to ad-
dress the problem of unlikely events proposed in
(Baayen, 2001; Khmaladze, 1987; Khmaladze and
Chitashvili, 1989; Wagner et al, 2006). We also
demonstrate that the estimator is strongly consis-
tent under the natural scaling formulation. While
compared with other vocabulary size estimates,
we see that our estimator performs at least as well
as some of the state of the art estimators.
2 Previous Work
Many estimators of vocabulary size are available
in the literature and a comparison of several non
109
parametric estimators of population size occurs in
(Gandolfi and Sastri, 2004). While a definite com-
parison including parametric estimators is lacking,
there is also no known work comparing methods
of extrapolation of vocabulary size. Baroni and
Evert, in (Baroni and Evert, 2005), evaluate the
performance of some estimators in extrapolating
vocabulary size for arbitrary sample sizes but limit
the study to parametric estimators. Since we con-
sider both parametric and nonparametric estima-
tors here, we consider this to be the first study
comparing a set of estimators for extrapolating vo-
cabulary size.
Estimators of vocabulary size that we compare
can be broadly classified into two types:
1. Nonparametric estimators- here word fre-
quency information from the given sample
alone is used to estimate the vocabulary size.
A good survey of the state of the art is avail-
able in (Gandolfi and Sastri, 2004). In this
paper, we compare our proposed estimator
with the canonical estimators available in
(Gandolfi and Sastri, 2004).
2. Parametric estimators- here a probabilistic
model capturing the relation between ex-
pected vocabulary size and sample size is the
estimator. Given a sample of size n, the
sample serves to calculate the parameters of
the model. The expected vocabulary for a
given sample size is then determined using
the explicit relation. The parametric esti-
mators considered in this study are (Baayen,
2001; Baroni and Evert, 2005),
(a) Zipf-Mandelbrot estimator (ZM);
(b) finite Zipf-Mandelbrot estimator (fZM).
In addition to the above estimators we consider
a novel non parametric estimator. It is the nonpara-
metric estimator that we propose, taking into ac-
count the characteristic feature of word frequency
distributions, to which we will turn next.
3 Novel Estimator of Vocabulary size
We observe (X1, . . . ,Xn), an i.i.d. sequence
drawn according to a probability distribution P
from a large, but finite, vocabulary ?. Our goal
is in estimating the ?essential? size of the vocabu-
lary ? using only the observations. In other words,
having seen a sample of size n we wish to know,
given another sample from the same population,
how many unseen elements we would expect to
see. Our nonparametric estimator for the number
of unseen elements is motivated by the character-
istic property of word frequency distributions, the
Large Number of Rare Events (LNRE) (Baayen,
2001). We also demonstrate that the estimator is
strongly consistent under a natural scaling formu-
lation described in (Khmaladze, 1987).
3.1 A Scaling Formulation
Our main interest is in probability distributions P
with the property that a large number of words in
the vocabulary ? are unlikely, i.e., the chance any
word appears eventually in an arbitrarily long ob-
servation is strictly between 0 and 1. The authors
in (Baayen, 2001; Khmaladze and Chitashvili,
1989; Wagner et al, 2006) propose a natural scal-
ing formulation to study this problem; specifically,
(Baayen, 2001) has a tutorial-like summary of the
theoretical work in (Khmaladze, 1987; Khmaladze
and Chitashvili, 1989). In particular, the authors
consider a sequence of vocabulary sets and prob-
ability distributions, indexed by the observation
size n. Specifically, the observation (X1, . . . ,Xn)
is drawn i.i.d. from a vocabulary ?n according to
probability Pn. If the probability of a word, say
? ? ?n is p, then the probability that this specific
word ? does not occur in an observation of size n
is
(1 ? p)n .
For ? to be an unlikely word, we would like this
probability for large n to remain strictly between
0 and 1. This implies that
c?
n ? p ?
c?
n , (1)
for some strictly positive constants 0 < c? < c? <
?. We will assume throughout this paper that c?
and c? are the same for every word ? ? ?n. This
implies that the vocabulary size is growing lin-
early with the observation size:
n
c? ? |?n| ?
n
c? .
This model is called the LNRE zone and its appli-
cability in natural language corpora is studied in
detail in (Baayen, 2001).
3.2 Shadows
Consider the observation string (X1, . . . ,Xn) and
let us denote the quantity of interest ? the number
110
of word types in the vocabulary ?n that are not
observed ? by On. This quantity is random since
the observation string itself is. However, we note
that the distribution of On is unaffected if one re-
labels the words in ?n. This motivates studying
of the probabilities assigned by Pn without refer-
ence to the labeling of the word; this is done in
(Khmaladze and Chitashvili, 1989) via the struc-
tural distribution function and in (Wagner et al,
2006) via the shadow. Here we focus on the latter
description:
Definition 1 Let Xn be a random variable on ?n
with distribution Pn. The shadow of Pn is de-
fined to be the distribution of the random variable
Pn({Xn}).
For the finite vocabulary situation we are con-
sidering, specifying the shadow is exactly equiv-
alent to specifying the unordered components of
Pn, viewed as a probability vector.
3.3 Scaled Shadows Converge
We will follow (Wagner et al, 2006) and sup-
pose that the scaled shadows, the distribution of
n ?Pn(Xn), denoted by Qn converge to a distribu-
tion Q. As an example, if Pn is a uniform distribu-
tion over a vocabulary of size cn, then n ? Pn(Xn)
equals 1c almost surely for each n (and hence it
converges in distribution). From this convergence
assumption we can, further, infer the following:
1. Since the probability of each word ? is lower
and upper bounded as in Equation (1), we
know that the distribution Qn is non-zero
only in the range [c?, c?].
2. The ?essential? size of the vocabulary, i.e.,
the number of words of ?n on which Pn
puts non-zero probability can be evaluated di-
rectly from the scaled shadow, scaled by 1n as
? c?
c?
1
y dQn(y). (2)
Using the dominated convergence theorem,
we can conclude that the convergence of the
scaled shadows guarantees that the size of the
vocabulary, scaled by 1/n, converges as well:
|?n|
n ?
? c?
c?
1
y dQ(y). (3)
3.4 Profiles and their Limits
Our goal in this paper is to estimate the size of the
underlying vocabulary, i.e., the expression in (2),
? c?
c?
n
y dQn(y), (4)
from the observations (X1, . . . ,Xn). We observe
that since the scaled shadow Qn does not de-
pend on the labeling of the words in ?n, a suf-
ficient statistic to estimate (4) from the observa-
tion (X1, . . . ,Xn) is the profile of the observation:
(?n1 , . . . , ?nn), defined as follows. ?nk is the num-
ber of word types that appear exactly k times in
the observation, for k = 1, . . . , n. Observe that
n
?
k=1
k?nk = n,
and that
V def=
n
?
k=1
?nk (5)
is the number of observed words. Thus, the object
of our interest is,
On = |?n| ? V. (6)
3.5 Convergence of Scaled Profiles
One of the main results of (Wagner et al, 2006) is
that the scaled profiles converge to a deterministic
probability vector under the scaling model intro-
duced in Section 3.3. Specifically, we have from
Proposition 1 of (Wagner et al, 2006):
n
?
k=1
?
?
?
?
k?k
n ? ?k?1
?
?
?
?
?? 0, almost surely, (7)
where
?k :=
? c?
c?
yk exp(?y)
k! dQ(y) k = 0, 1, 2, . . . .
(8)
This convergence result suggests a natural estima-
tor for On, expressed in Equation (6).
3.6 A Consistent Estimator of On
We start with the limiting expression for scaled
profiles in Equation (7) and come up with a natu-
ral estimator for On. Our development leading to
the estimator is somewhat heuristic and is aimed
at motivating the structure of the estimator for the
number of unseen words, On. We formally state
and prove its consistency at the end of this section.
111
3.6.1 A Heuristic Derivation
Starting from (7), let us first make the approxima-
tion that
k?k
n ? ?k?1, k = 1, . . . , n. (9)
We now have the formal calculation
n
?
k=1
?nk
n ?
n
?
k=1
?k?1
k (10)
=
n
?
k=1
? c?
c?
e?yyk?1
k! dQ(y)
?
? c?
c?
e?y
y
( n
?
k=1
yk
k!
)
dQ(y) (11)
?
? c?
c?
e?y
y (e
y ? 1) dQ(y) (12)
? |?n|n ?
? c?
c?
e?y
y dQ(y). (13)
Here the approximation in Equation (10) follows
from the approximation in Equation (9), the ap-
proximation in Equation (11) involves swapping
the outer discrete summation with integration and
is justified formally later in the section, the ap-
proximation in Equation (12) follows because
n
?
k=1
yk
k! ? e
y ? 1,
as n ? ?, and the approximation in Equa-
tion (13) is justified from the convergence in Equa-
tion (3). Now, comparing Equation (13) with
Equation (6), we arrive at an approximation for
our quantity of interest:
On
n ?
? c?
c?
e?y
y dQ(y). (14)
The geometric series allows us to write
1
y =
1
c?
?
?
?=0
(
1 ? yc?
)?
, ?y ? (0, c?) . (15)
Approximating this infinite series by a finite sum-
mation, we have for all y ? (c?, c?),
1
y ?
1
c?
M
?
?=0
(
1 ? yc?
)?
=
(
1 ? yc?
)M
y
?
(
1 ? c?c?
)M
c? . (16)
It helps to write the truncated geometric series as
a power series in y:
1
c?
M
?
?=0
(
1 ? yc?
)?
= 1c?
M
?
?=0
?
?
k=0
(
?
k
)
(?1)k
(y
c?
)k
= 1c?
M
?
k=0
( M
?
?=k
(
?
k
)
)
(?1)k
(y
c?
)k
=
M
?
k=0
(?1)k aMk yk, (17)
where we have written
aMk :=
1
c?k+1
( M
?
?=k
(
?
k
)
)
.
Substituting the finite summation approximation
in Equation 16 and its power series expression in
Equation (17) into Equation (14) and swapping the
discrete summation with the integral, we can con-
tinue
On
n ?
M
?
k=0
(?1)k aMk
? c?
c?
e?yyk dQ(y)
=
M
?
k=0
(?1)k aMk k!?k. (18)
Here, in Equation (18), we used the definition of
?k from Equation (8). From the convergence in
Equation (7), we finally arrive at our estimate:
On ?
M
?
k=0
(?1)k aMk (k + 1)! ?k+1. (19)
3.6.2 Consistency
Our main result is the demonstration of the consis-
tency of the estimator in Equation (19).
Theorem 1 For any ? > 0,
lim
n??
?
?
?
On ?
?M
k=0 (?1)
k aMk (k + 1)! ?k+1
?
?
?
n ? ?
almost surely, as long as
M ? c? log2 e + log2 (?c?)log2 (c?? c?) ? 1 ? log2 (c?)
. (20)
112
Proof: From Equation (6), we have
On
n =
|?n|
n ?
n
?
k=1
?k
n
= |?n|n ?
n
?
k=1
?k?1
k ?
n
?
k=1
1
k
(k?k
n ? ?k?1
)
. (21)
The first term in the right hand side (RHS) of
Equation (21) converges as seen in Equation (3).
The third term in the RHS of Equation (21) con-
verges to zero, almost surely, as seen from Equa-
tion (7). The second term in the RHS of Equa-
tion (21), on the other hand,
n
?
k=1
?k?1
k =
? c?
c?
e?y
y
( n
?
k=1
yk
k!
)
dQ(y)
?
? c?
c?
e?y
y (e
y ? 1) dQ(y), n ? ?,
=
? c?
c?
1
y dQ(y) ?
? c?
c?
e?y
y dQ(y).
The monotone convergence theorem justifies the
convergence in the second step above. Thus we
conclude that
lim
n??
On
n =
? c?
c?
e?y
y dQ(y) (22)
almost surely. Coming to the estimator, we can
write it as the sum of two terms:
M
?
k=0
(?1)k aMk k!?k (23)
+
M
?
k=0
(?1)k aMk k!
((k + 1)?k+1
n ? ?k
)
.
The second term in Equation (23) above is seen to
converge to zero almost surely as n ? ?, using
Equation (7) and noting that M is a constant not
depending on n. The first term in Equation (23)
can be written as, using the definition of ?k from
Equation (8),
? c?
c?
e?y
( M
?
k=0
(?1)k aMk yk
)
dQ(y). (24)
Combining Equations (22) and (24), we have that,
almost surely,
lim
n??
On ?
?M
k=0 (?1)k aMk (k + 1)! ?k+1
n =
? c?
c?
e?y
(
1
y ?
M
?
k=0
(?1)k aMk yk
)
dQ(y). (25)
Combining Equation (16) with Equation (17), we
have
0 < 1y ?
M
?
k=0
(?1)k aMk yk ?
(
1 ? c?c?
)M
c? . (26)
The quantity in Equation (25) can now be upper
bounded by, using Equation (26),
e?c?
(
1 ? c?c?
)M
c? .
For M that satisfy Equation (20) this term is less
than ?. The proof concludes.
3.7 Uniform Consistent Estimation
One of the main issues with actually employing
the estimator for the number of unseen elements
(cf. Equation (19)) is that it involves knowing the
parameter c?. In practice, there is no natural way to
obtain any estimate on this parameter c?. It would
be most useful if there were a way to modify the
estimator in a way that it does not depend on the
unobservable quantity c?. In this section we see that
such a modification is possible, while still retain-
ing the main theoretical performance result of con-
sistency (cf. Theorem 1).
The first step to see the modification is in ob-
serving where the need for c? arises: it is in writing
the geometric series for the function 1y (cf. Equa-
tions (15) and (16)). If we could let c? along with
the number of elements M itself depend on the
sample size n, then we could still have the geo-
metric series formula. More precisely, we have
1
y ?
1
c?n
Mn
?
?=0
(
1? yc?n
)?
= 1y
(
1 ? yc?n
)Mn
? 0, n ? ?,
as long as
c?n
Mn
? 0, n ? ?. (27)
This simple calculation suggests that we can re-
place c? and M in the formula for the estimator (cf.
Equation (19)) by terms that depend on n and sat-
isfy the condition expressed by Equation (27).
113
4 Experiments
4.1 Corpora
In our experiments we used the following corpora:
1. The British National Corpus (BNC): A cor-
pus of about 100 million words of written and
spoken British English from the years 1975-
1994.
2. The New York Times Corpus (NYT): A cor-
pus of about 5 million words.
3. The Malayalam Corpus (MAL): A collection
of about 2.5 million words from varied ar-
ticles in the Malayalam language from the
Central Institute of Indian Languages.
4. The Hindi Corpus (HIN): A collection of
about 3 million words from varied articles in
the Hindi language also from the Central In-
stitute of Indian Languages.
4.2 Methodology
We would like to see how well our estimator per-
forms in terms of estimating the number of unseen
elements. A natural way to study this is to ex-
pose only half of an existing corpus to be observed
and estimate the number of unseen elements (as-
suming the the actual corpus is twice the observed
size). We can then check numerically how well
our estimator performs with respect to the ?true?
value. We use a subset (the first 10%, 20%, 30%,
40% and 50%) of the corpus as the observed sam-
ple to estimate the vocabulary over twice the sam-
ple size. The following estimators have been com-
pared.
Nonparametric: Along with our proposed esti-
mator (in Section 3), the following canonical es-
timators available in (Gandolfi and Sastri, 2004)
and (Baayen, 2001) are studied.
1. Our proposed estimator On (cf. Section 3):
since the estimator is rather involved we con-
sider only small values of M (we see empir-
ically that the estimator converges for very
small values of M itself) and choose c? = M.
This allows our estimator for the number of
unseen elements to be of the following form,
for different values of M :
M On
1 2 (?1 ? ?2)
2 32 (?1 ? ?2) + 34?3
3 43 (?1 ? ?2) + 89
(
?3 ? ?43
)
Using this, the estimator of the true vocabu-
lary size is simply,
On + V. (28)
Here (cf. Equation (5))
V =
n
?
k=1
?nk . (29)
In the simulations below, we have considered
M large enough until we see numerical con-
vergence of the estimators: in all the cases,
no more than a value of 4 is needed for M .
For the English corpora, very small values of
M suffice ? in particular, we have considered
the average of the first three different estima-
tors (corresponding to the first three values
of M ). For the non-English corpora, we have
needed to consider M = 4.
2. Gandolfi-Sastri estimator,
VGS def=
n
n? ?1
(
V + ?1?2
)
, (30)
where
?2 = ?1 ? n? V2n +
?
5n2 + 2n(V ? 3?1) + (V ? ?1)2
2n ;
3. Chao estimator,
VChao def= V +
?21
2?2
; (31)
4. Good-Turing estimator,
VGT def=
V
(
1? ?1n
) ; (32)
5. ?Simplistic? estimator,
VSmpl def= V
(nnew
n
)
; (33)
here the supposition is that the vocabulary
size scales linearly with the sample size (here
nnew is the new sample size);
6. Baayen estimator,
VByn def= V +
(?1
n
)
nnew; (34)
here the supposition is that the vocabulary
growth rate at the observed sample size is
given by the ratio of the number of hapax
legomena to the sample size (cf. (Baayen,
2001) pp. 50).
114
% error of top 2 and Good?Turing estimates compared
%
 e
rro
r
?
40
?
30
?
20
?
10
0
10
Our GT ZM Our GT ZM Our GT ZM Our GT ZM
BNC NYT Malayalam Hindi
Figure 1: Comparison of error estimates of the 2
best estimators-ours and the ZM, with the Good-
Turing estimator using 10% sample size of all the
corpora. A bar with a positive height indicates
and overestimate and that with a negative height
indicates and underestimate. Our estimator out-
performs ZM. Good-Turing estimator widely un-
derestimates vocabulary size.
Parametric: Parametric estimators use the ob-
servations to first estimate the parameters. Then
the corresponding models are used to estimate the
vocabulary size over the larger sample. Thus the
frequency spectra of the observations are only in-
directly used in extrapolating the vocabulary size.
In this study we consider state of the art paramet-
ric estimators, as surveyed by (Baroni and Evert,
2005). We are aided in this study by the availabil-
ity of the implementations provided by the ZipfR
package and their default settings.
5 Results and Discussion
The performance of the different estimators as per-
centage errors of the true vocabulary size using
different corpora are tabulated in tables 1-4. We
now summarize some important observations.
? From the Figure 1, we see that our estima-
tor compares quite favorably with the best of
the state of the art estimators. The best of the
state of the art estimator is a parametric one
(ZM), while ours is a nonparametric estima-
tor.
? In table 1 and table 2 we see that our esti-
mate is quite close to the true vocabulary, at
all sample sizes. Further, it compares very fa-
vorably to the state of the art estimators (both
parametric and nonparametric).
? Again, on the two non-English corpora (ta-
bles 3 and 4) we see that our estimator com-
pares favorably with the best estimator of vo-
cabulary size and at some sample sizes even
surpasses it.
? Our estimator has theoretical performance
guarantees and its empirical performance is
comparable to that of the state of the art es-
timators. However, this performance comes
at a very small fraction of the computational
cost of the parametric estimators.
? The state of the art nonparametric Good-
Turing estimator wildly underestimates the
vocabulary; this is true in each of the four
corpora studied and at all sample sizes.
6 Conclusion
In this paper, we have proposed a new nonpara-
metric estimator of vocabulary size that takes into
account the LNRE property of word frequency
distributions and have shown that it is statistically
consistent. We then compared the performance of
the proposed estimator with that of the state of the
art estimators on large corpora. While the perfor-
mance of our estimator seems favorable, we also
see that the widely used classical Good-Turing
estimator consistently underestimates the vocabu-
lary size. Although as yet untested, with its com-
putational simplicity and favorable performance,
our estimator may serve as a more reliable alter-
native to the Good-Turing estimator for estimating
vocabulary sizes.
Acknowledgments
This research was partially supported by Award
IIS-0623805 from the National Science Founda-
tion.
References
R. H. Baayen. 2001. Word Frequency Distributions,
Kluwer Academic Publishers.
Marco Baroni and Stefan Evert. 2001. ?Testing the ex-
trapolation quality of word frequency models?, Pro-
ceedings of Corpus Linguistics , volume 1 of The
Corpus Linguistics Conference Series, P. Danielsson
and M. Wagenmakers (eds.).
J. Bunge and M. Fitzpatrick. 1993. ?Estimating the
number of species: a review?, Journal of the Amer-
ican Statistical Association, Vol. 88(421), pp. 364-
373.
115
Sample True % error w.r.t the true value
(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS
10 153912 1 -27 -4 -8 46 23 8 -11
20 220847 -3 -30 -9 -12 39 19 4 -15
30 265813 -2 -30 -9 -11 39 20 6 -15
40 310351 1 -29 -7 -9 42 23 9 -13
50 340890 2 -28 -6 -8 43 24 10 -12
Table 1: Comparison of estimates of vocabulary size for the BNC corpus as percentage errors w.r.t the
true value. A negative value indicates an underestimate. Our estimator outperforms the other estimators
at all sample sizes.
Sample True % error w.r.t the true value
(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS
10 37346 1 -24 5 -8 48 28 4 -8
20 51200 -3 -26 0 -11 46 22 -1 -11
30 60829 -2 -25 1 -10 48 23 1 -10
40 68774 -3 -25 0 -10 49 21 -1 -11
50 75526 -2 -25 0 -10 50 21 0 -10
Table 2: Comparison of estimates of vocabulary size for the NYT corpus as percentage errors w.r.t the
true value. A negative value indicates an underestimate. Our estimator compares favorably with ZM and
Chao.
Sample True % error w.r.t the true value
(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS
10 146547 -2 -27 -5 -10 9 34 82 -2
20 246723 8 -23 4 -2 19 47 105 5
30 339196 4 -27 0 -5 16 42 93 -1
40 422010 5 -28 1 -4 17 43 95 -1
50 500166 5 -28 1 -4 18 44 94 -2
Table 3: Comparison of estimates of vocabulary size for the Malayalam corpus as percentage errors
w.r.t the true value. A negative value indicates an underestimate. Our estimator compares favorably with
ZM and GS.
Sample True % error w.r.t the true value
(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS
10 47639 -2 -34 -4 -9 25 32 31 -12
20 71320 7 -30 2 -1 34 43 51 -7
30 93259 2 -33 -1 -5 30 38 42 -10
40 113186 0 -35 -5 -7 26 34 39 -13
50 131715 -1 -36 -6 -8 24 33 40 -14
Table 4: Comparison of estimates of vocabulary size for the Hindi corpus as percentage errors w.r.t the
true value. A negative value indicates an underestimate. Our estimator outperforms the other estimators
at certain sample sizes.
116
A. Gandolfi and C. C. A. Sastri. 2004. ?Nonparamet-
ric Estimations about Species not Observed in a
Random Sample?, Milan Journal of Mathematics,
Vol. 72, pp. 81-105.
E. V. Khmaladze. 1987. ?The statistical analysis of
large number of rare events?, Technical Report, De-
partment of Mathematics and Statistics., CWI, Am-
sterdam, MS-R8804.
E. V. Khmaladze and R. J. Chitashvili. 1989. ?Statis-
tical analysis of large number of rate events and re-
lated problems?, Probability theory and mathemati-
cal statistics (Russian), Vol. 92, pp. 196-245.
. P. Santhanam, A. Orlitsky, and K. Viswanathan, ?New
tricks for old dogs: Large alphabet probability es-
timation?, in Proc. 2007 IEEE Information Theory
Workshop, Sept. 2007, pp. 638?643.
A. B. Wagner, P. Viswanath and S. R. Kulkarni. 2006.
?Strong Consistency of the Good-Turing estimator?,
IEEE Symposium on Information Theory, 2006.
117
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 386?389,
Prague, June 2007. c?2007 Association for Computational Linguistics
UIUC: A Knowledge-rich Approach to Identifying Semantic Relations
between Nominals
Brandon Beamer,1,4 Suma Bhat,2,4 Brant Chee,3,4 Andrew Fister,1,4 Alla Rozovskaya,1,4
Roxana Girju1,4
Department of Linguistics1,
Department of Electrical and Computer Engineering2,
Department of Library and Information Science3,
Beckman Institute4,
University of Illinois at Urbana-Champaign
{bbeamer, spbhat2, chee, afister2, rozovska, girju}@uiuc.edu
Abstract
This paper describes a supervised,
knowledge-intensive approach to the auto-
matic identification of semantic relations
between nominals in English sentences.
The system employs different sets of new
and previously used lexical, syntactic, and
semantic features extracted from various
knowledge sources. At SemEval 2007 the
system achieved an F-measure of 72.4% and
an accuracy of 76.3%.
1 Introduction
The SemEval 2007 task on Semantic Relations be-
tween Nominals is to identify the underlying se-
mantic relation between two nouns in the context
of a sentence. The dataset provided consists of a
definition file and 140 training and about 70 test
sentences for each of the seven relations consid-
ered: Cause-Effect, Instrument-Agency, Product-
Producer, Origin-Entity, Theme-Tool, Part-Whole,
and Content-Container. The task is defined as a
binary classification problem. Thus, given a pair
of nouns and their sentential context, the classifier
decides whether the nouns are linked by the target
semantic relation. In each training and test exam-
ple sentence, the nouns are identified and manu-
ally labeled with their corresponding WordNet 3.0
senses. Moreover, each example is accompanied by
the heuristic pattern (query) the annotators used to
extract the sentence from the web and the position
of the arguments in the relation.
(1) 041 ?He derives great joy and <e1>happiness</e1>
from <e2>cycling</e2>.? WordNet(e1) =
?happiness%1:12:00::?, WordNet(e2) = ?cy-
cling%1:04:00::?, Cause-Effect(e2,e1) = ?true?,
Query = ?happiness from *?
Based on the information employed, systems can
be classified in four types of classes: (A) systems
that use neither the given WordNet synsets nor the
queries, (B) systems that use only WordNet senses,
(C) systems that use only the queries, and (D) sys-
tems that use both.
In this paper we present a type-B system that re-
lies on various sets of new and previously used lin-
guistic features employed in a supervised learning
model.
2 Classification of Semantic Relations
Semantic relations between nominals can be en-
coded by different syntactic constructions. We
extend here over previous work that has focused
mainly on noun compounds and other noun phrases,
and noun?verb?noun constructions.
We selected a list of 18 lexico-syntactic and se-
mantic features split here into three sets: feature set
#1 (core features), feature set #2 (context features),
and the feature set #3 (special features). Table 1
shows all three sets of features along with their defi-
nitions; a detailed description is presented next. For
some features, we list previous works where they
proved useful. While features F1 ? F4 were selected
from our previous experiments, all the other features
are entirely the contribution of this research.
Feature set #1: Core features
This set contains six features that were employed
in all seven relation classifiers. The features take
into consideration only lexico-semantic information
386
No. Feature Definition
Feature Set #1: Core features
F1 Argument position indicates the position of the arguments in the semantic relation
(Girju et al, 2005; Girju et al, 2006) (e.g., Part-Whole(e1, e2), where e1 is the part and e2 is the whole).
F2 Semantic specialization this is the prediction returned by the automatic WordNet IS-A semantic
(Girju et al, 2005; Girju et al, 2006) specialization procedure.
F3, F4 Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations
(Girju et al, 2004) or not. Specifically, we distinguish here between agential nouns,
other nominalizations, and neither.
F5, F6 Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location.
Feature Set #2: Context features
F7, F8 Grammatical role describes the grammatical role of e1 (F7) and e2 (F8). There are three
possible values: subject, direct object, or neither.
F9 PP Attachment applies to NP PP constructions and indicates if the prepositional phrase
containing e2 attaches to the NP containing e1.
F10, F11 Semantic Role is concerned with the semantic role of the phrase containing
either e1 (F10) or e2 (F11). In particular, we focused on three semantic
roles: Time, Location, Manner. The feature is set to 1 if the target noun
is part of a phrase of that type and to 0 otherwise.
F12, F13, Inter-noun context sequence is a set of three features. F12 captures the sequence of stemmed
F14 words between e1 and e2, while F13 lists the part of speech sequence in
between the target nouns. F14 is a scoring weight (with possible values
1, 0.5, 0.25, and 0.125) which measures the similarity of an unseen
sequence to the set of sequence patterns associated with a relation.
Feature Set #3: Special features
F15, F16 Psychological feature is used in the Theme-Tool classifier; indicates if e1 (F15) or e2 (F16)
belong or not to a predefined set of psychological features.
F17 Instrument semantic role is used for the Instrument-Agency relation and indicates whether
the phrase containing e1 is labeled as em Instrument or not.
F18 Syntactic attachment is used for the Instrument-Agent relation and indicates whether the phrase
containing the Instrument role attaches to a noun or a verb
Table 1: The three sets of features used for the automatic semantic relation classification.
about the two target nouns.
Argument position (F1) indicates the position of
the semantic arguments in the relation. This infor-
mation is very valuable, since some relations have a
particular argument arrangement depending on the
lexico-syntactic construction in which they occur.
For example, most of the noun compounds encod-
ing Stuff-Object / Part-Whole relations have e1 as
the part and e2 as the whole (e.g., silk dress).
Semantic specialization (F2) is a binary feature
representing the prediction of a semantic specializa-
tion learning model. The method consists of a set
of iterative procedures of specialization of the train-
ing examples on the WordNet IS-A hierarchy. Thus,
after all the initial noun?noun pairs are mapped
through generalization to entity ? entity pairs in
WordNet, a set of necessary specialization iterations
is applied until it finds a boundary that separates pos-
itive and negative examples. This boundary is tested
on new examples for relation prediction.
The nominalization features (F3, F4) indicate if
the target noun is a nominalization and, if yes, of
what type. We distinguish here between agential
nouns, other nominalizations, and neither. The
features were identified based on WordNet and
NomLex-Plus1 and were introduced to filter some
of negative examples, such as car owner/THEME.
Spatio?Temporal features (F5, F6) were also in-
troduced to recognize some near miss examples,
such as Temporal and Location relations. For in-
stance, activation by summer (near-miss for Cause-
Effect) and mouse in the field (near-miss for Content-
Container). Similarly, for Theme-Tool, a word act-
ing as a Theme should not indicate a period of time,
as in <e1>the appointment</e1> was for more
than one <e2>year</e2>. For this we used the in-
formation provided by WordNet and special classes
generated from the works of (Herskovits, 1987),
(Linstromberg, 1997), and (Tyler and Evans, 2003).
1NomLex-Plus is a hand-coded database of 5,000 verb nom-
inalizations, de-adjectival, and de-adverbial nouns.
http://nlp.cs.nyu.edu/nomlex/index.html
387
Feature set #2: Context features
This set takes advantage of the sentence context to
identify features at different linguistic levels.
The grammatical role features (F7, F8) determine
if e1 or e2 is the subject, direct object, or neither.
This feature helps filter out some instances with poor
context, such as noun compounds and identify some
near-miss examples. For example, a restriction im-
posed by the definition of Theme-Tool indicates that
in constructions such as Y/Tool is used for V-ing
X/Theme, neither X nor Y can be the subject of
the sentence, and hence Theme-Tool(X, Y) would be
false. This restriction is also captured by the nomi-
nalization feature in case X or Y is an agential noun.
PP attachment (F9) is defined for NP PP construc-
tions, where the prepositional phrase containing the
noun e2 attaches or not to the NP (containing e1).
The rationale is to identify negative instances where
the PP attaches to any other word before NP in the
sentence. For example, eat <e1>pizza</e1> with
<e2>a fork</e2>, where with a fork attaches to
the verb to eat (cf. (Charniak, 2000)).
Furthermore, we implemented and used two se-
mantic role features which identify the semantic role
of the phrase in a verb?argument structure, phrase
containing either e1 (F10) or e2 (F11). In particular,
we focus on three semantic roles: Time, Location,
Manner. The feature is set to 1 if the target noun
is part of a semantic role phrase and to 0 otherwise.
The idea is to filter out near-miss examples, expe-
cially for the Instrument-Agency relation. For this,
we used ASSERT, a semantic role labeler developed
at the University of Colorado at Boulder2 which was
queried through a web interface.
Inter-noun context sequence features (F12, F13)
encode the sequence of lexical and part of speech
information between the two target nouns. Feature
F14 is a weight feature on the values of F12 and
F13 and indicates how similar a new sequence is to
the already observed inter-noun context associated
with the relation. If there is a direct match, then the
weight is set to 1. If the part-of-speech pattern of the
new substring matches that of an already seen sub-
string, then the weight is set to 0.5. Weights 0.25
and 0.125 are given to those sequences that overlap
entirely or partially with patterns encoding other se-
2http://oak.colorado.edu/assert/
mantic relations in the same contingency set (e.g.,
semantic relations that share syntactic pattern se-
quences). The value of the feature is the summation
of the weights thus obtained. The rationale is that
the greater the weight, the more representative is the
context sequence for that relation.
Feature set #3: Special features
This set includes features that help identify specific
information about some semantic relations.
Psychological feature was defined for the Theme-
Tool relation and indicates if the target noun (F15,
F16) belongs to a list of special concepts. This fea-
ture was obtained from the restrictions listed in the
definition of Theme-Tool. In the example need for
money, the noun need is a psychological feature, and
thus the instance cannot encode a Theme-Tool rela-
tion. A list of synsets from WordNet subhierarchy
of motivation and cognition constituted the psycho-
logical factors. This was augmented with precondi-
tions such as foundation and requirement since they
would not be allowed as tools for the theme.
The Instrument semantic role is used for the
Instrument-Agency relation as a boolean feature
(F17) indicating whether the argument identified as
Instrument in the relation (e.g., e1 if Instrument-
Agency(e1, e2)) belongs to an instrument phrase as
identified by a semantic role tool, such as ASSERT.
The syntactic attachment feature (F18) is a fea-
ture that indicates whether the argument identified
as Instrument in the relation attaches to a verb or to
a noun in the syntactically parsed sentence.
3 Learning Model and Experimental
Setting
For our experiments we chose libSVM, an open
source SVM package3. Since some of our features
are nominal, we followed the standard practice of
representing a nominal feature with n discrete val-
ues as n binary features. We used the RBF kernel.
We built a binary classifier for each of the seven
relations. Since the size of the task training data per
relation is small, we expanded it with new examples
from various sources. We added a new corpus of
3,000 sentences of news articles from the TREC-9
text collection (Girju, 2003) encoding Cause-Effect
(1,320) and Product-Producer (721). Another col-
3http://www.csie.ntu.edu.tw/?cjlin/libsvm/
388
Relation P R F Acc Total Base-F Base-Acc Best features
Cause-Effect 69.5 100.0 82.0 77.5 80 67.8 51.2 F1, F2, F5, F6, F12?F14
Instrument-Agency 68.2 78.9 73.2 71.8 78 65.5 51.3 F7, F8, F10, F11, F15?F18
Product-Producer 84.5 79.0 81.7 76.3 93 80.0 66.7 F1?F4, F12?F14
Origin-Entity 86.4 52.8 65.5 75.3 81 61.5 55.6 F1, F2, F5, F6, F12?F14
Theme-Tool 85.7 41.4 55.8 73.2 71 58.0 59.2 F1?F6, F15, F16
Part-Whole 70.8 65.4 68.0 77.8 72 53.1 63.9 F1?F4
Content-Container 93.1 71.1 80.6 82.4 74 67.9 51.4 F1?F6, F12?F14
Average 79.7 69.8 72.4 76.3 78.4
Table 2: Performance obtained per relation. Precision, Recall, F-measure, Accuracy, and Total (number of examples) are macro-
averaged for system?s performance on all 7 relations. Base-F shows the baseline F measure (all true), while Base-Acc shows the
baseline accuracy score (majority).
lection of 3,129 sentences from Wall Street Journal
(Moldovan et al, 2004; Girju et al, 2004) was con-
sidered for Part-Whole (1,003), Origin-Entity (167),
Product-Producer (112), and Theme-Tool (91). We
also extracted 552 Product-Producer instances from
eXtended WordNet4 (noun entries and their gloss
definition). Moreover, for Theme-Tool and Content-
Container we used special lists of constraints5. Be-
sides the selectional restrictions imposed on the
nouns by special features such as F15 and F16 (psy-
chological feature), we created lists of containers
from various thesauri6 and identified selectional re-
strictions that differentiate between containers and
locations relying on taxonomies of spatial entities
discussed in detail in (Herskovits, 1987) and (Tyler
and Evans, 2003).
Each instance in this text collection had the tar-
get nouns identified and annotated with WordNet
senses. Since the annotations used different Word-
Net versions, senses were mapped to sense keys.
4 Experimental Results
Table 2 shows the performance of our system for
each semantic relation. Base-F indicates the base-
line F-measure (all true), while Base-Acc shows the
baseline accuracy score (majority). The Average
score of precision, recall, F-measure, and accuracy
is macroaveraged over all seven relations. Overall,
all features contributed to the performance, with a
different contribution per relation (cf. Table 2).
5 Conclusions
This paper describes a method for the automatic
identification of a set of seven semantic relations
4http://xwn.hlt.utdallas.edu/
5The Instrument-Agency classifier was trained only on the
task dataset.
6Thesauri such as TheFreeDictionary.com.
based on support vector machines (SVMs). The ap-
proach benefits from an extended dataset on which
binary classifiers were trained for each relation. The
feature sets fed into the SVMs produced very good
results.
Acknowledgments
We would like to thank Brian Drexler for his valu-
able suggestions on the set of semantic relations.
References
E. Charniak. 2000. A Maximum-entropy-inspired Parser. In
the Proceedings of the 1st NAACL Conference.
R. Girju, A. Giuglea, M. Olteanu, O. Fortu, O. Bolohan, and
D. Moldovan. 2004. Support vector machines applied to
the classification of semantic relations in nominalized noun
phrases. In the Proceedings of the HLT/NAACL Workshop
on Computational Lexical Semantics.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On
the semantics of noun compounds. Computer Speech and
Language, 19(4):479?496.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic
discovery of part-whole relations. Computational Linguis-
tics, 32(1).
R. Girju. 2003. Automatic detection of causal relations for
question answering. In the Proceedings of the ACL Work-
shop on ?Multilingual Summarization and Question Answer-
ing - Machine Learning and Beyond?.
A. Herskovits. 1987. Language and spatial cognition: An in-
terdisciplinary study of the prepositions in English. Cam-
bridge University Press.
S. Linstromberg. 1997. English Prepositions Explained. John
Benjamins Publishing Co., Amsterdam/Philaderphia.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.
2004. Models for the semantic classification of noun
phrases. In the Proceedings of the HLT/NAACL Workshop
on Computational Lexical Semantics.
A. Tyler and V. Evans. 2003. The Semantics of English Prepo-
sitions: Spatial Sciences, Embodied Meaning, and Cogni-
tion. Cambridge University Press.
389
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 600?608, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Assessment of ESL Learners? Syntactic Competence Based on Similarity
Measures
Su-Youn Yoon
Educational Testing Service
Princeton, NJ 08541
syoon@ets.org
Suma Bhat
Beckman Institute,
Urbana, IL 61801
spbhat2@illinois.edu
Abstract
This study presents a novel method that
measures English language learners? syntac-
tic competence towards improving automated
speech scoring systems. In contrast to most
previous studies which focus on the length of
production units such as the mean length of
clauses, we focused on capturing the differ-
ences in the distribution of morpho-syntactic
features or grammatical expressions across
proficiency. We estimated the syntactic com-
petence through the use of corpus-based NLP
techniques. Assuming that the range and so-
phistication of grammatical expressions can
be captured by the distribution of Part-of-
Speech (POS) tags, vector space models of
POS tags were constructed. We use a large
corpus of English learners? responses that are
classified into four proficiency levels by hu-
man raters. Our proposed feature measures
the similarity of a given response with the
most proficient group and is then estimates the
learner?s syntactic competence level.
Widely outperforming the state-of-the-art
measures of syntactic complexity, our method
attained a significant correlation with human-
rated scores. The correlation between human-
rated scores and features based on manual
transcription was 0.43 and the same based on
ASR-hypothesis was slightly lower, 0.42. An
important advantage of our method is its ro-
bustness against speech recognition errors not
to mention the simplicity of feature genera-
tion that captures a reasonable set of learner-
specific syntactic errors.
1 Introduction
This study provides a novel method that measures
ESL (English as a second language) learners? com-
petence in grammar usage (syntactic competence).
Being interdisciplinary in nature, it shows how to
combine the core findings in the ESL literature with
various empirical NLP techniques for the purpose of
automated scoring.
Grammar usage is one of the dimensions of lan-
guage ability that is assessed during non-native pro-
ficiency level testing in a foreign language. Overall
proficiency in the target language can be assessed
by testing the abilities in various areas including flu-
ency, pronunciation, and intonation; grammar and
vocabulary; and discourse structure. Testing rubrics
for human raters contain descriptors used for the
subjective assessment of several of these features.
With the recent move towards the objective assess-
ment of language ability (spoken and written), it is
imperative that we develop methods for quantifying
these abilities and measuring them automatically.
Ortega (2003) indicated that ?the range of forms
that surface in language production and the degree
of sophistication of such forms? were two impor-
tant areas in grammar usage and called the combina-
tion of these two areas ?syntactic complexity.? Fea-
tures that measure syntactic complexity have been
frequently studied in ESL literature and have been
found to be highly correlated with students? profi-
ciency levels in writing.
Studies in automated speech scoring have focused
on fluency (Cucchiarini et al2000; Cucchiarini et
al., 2002), pronunciation (Witt and Young, 1997;
600
Witt, 1999; Franco et al1997; Neumeyer et al
2000), and intonation (Zechner et al2009), and rel-
atively fewer studies have been conducted on gram-
mar usage. More recently, Lu (2010), Chen and
Yoon (2011) and Chen and Zechner (2011) have
measured syntactic competence in speech scoring.
Chen and Yoon (2011) estimated the complexity of
sentences based on the average length of the clauses
or sentences. In addition to these length measures,
Lu (2010) and Chen and Zechner (2011) measured
the parse-tree based features such as the mean depth
of parsing tree levels. However, these studies found
that these measures did not show satisfactory empir-
ical performance in automatic speech scoring (Chen
and Yoon, 2011; Chen and Zechner, 2011) when the
features were calculated from the output of a speech
recognition engine.
This study considers new features that measure
syntactic complexity and is novel in two important
ways. First, in contrast to most features that in-
fer syntactic complexity based upon the length of
the unit, we directly measure students? sophistica-
tion and range in grammar usage. Second, instead
of rating a student?s response using a scale based on
native speech production, our experiments compare
it with a similar body of learners? speech. Elicit-
ing native speakers? data and rating it for grammar
usage (supervised approach) can be arbitrary, since
there can be a very wide range of possible grammat-
ical structures that native speakers utilize. Instead,
we proceed in a semi-supervised fashion. A large
amount of learners? spoken responses were collected
and classified into four groups according to their
proficiency level. We then sought to find how dis-
tinct the proficiency classes were based on the distri-
bution of POS tags. Given a student?s response, we
calculated the similarity with a sample of responses
for each score level based on the proportion and dis-
tribution of Part-of-Speech using NLP techniques.
POS tag distribution has been used in various
tasks such as text genre classification (Feldman et
al., 2009); in a language testing context, it has been
used in grammatical error detection (Chodorow and
Leacock, 2000; Tetreault and Chodorow, 2008) and
essay scoring. Recently, Roark et al2011) ex-
plored POS tag distribution to capture the differ-
ences in syntactic complexity between healthy sub-
jects and subjects with mild cognitive impairment,
but no other research has used POS tag distribution
in measuring syntactic complexity, to the best of au-
thors? knowledge.
An assessment of ESL learners? syntactic compe-
tence should consider the structure of sentences as a
whole - a task which may not be captured by the sim-
plistic POS tag distribution. However, studies of Lu
(2010) and Chen and Zechner (2011) showed that
more complex syntactic features are unreliable in
ASR-based scoring system. Furthermore, we show
that POS unigrams or bigrams indeed capture a rea-
sonable portion of learners? range and sophistication
of grammar usage in our discussion in Section 7.
This paper will proceed as follows: we will re-
view related work in Section 2 and present the
method to calculate syntactic complexity in Section
3. Data and experiment setup will be explained in
Section 4 and Section 5. The results will be pre-
sented in Section 6. Finally, in Section 7, we discuss
the levels of syntactic competence that are captured
using our proposed measure.
2 Related Work
Second Language Acquisition (SLA) researchers
have developed many quantitative measures to es-
timate the level of acquisition of syntactic compe-
tence. Bardovi-Harlig and Bofman (1989) classi-
fied these measures into two groups. The first group
is related to the acquisition of specific morphosyn-
tactic features or grammatical expressions. Tests of
negations or relative clauses - whether these expres-
sions occurred in the test responses without errors -
fell into this group (hereafter, the expression-based
group). The second group is related to the length of
the clause or the relationship between clauses and
hence not tied to particular structures (hereafter, the
length-based group). Examples of the second group
measures include the average length of clause unit
and dependent clauses per sentence unit.
These syntactic measures have been extensively
studied in ESL writing. Ortega (2003) synthesized
25 research studies which employed syntactic mea-
sures on ESL writing and reported a significant re-
lationship between the proposed features and writ-
ing proficiency. He reported that a subset of features
such as the mean length of the clause unit increased
with students? proficiency. More recently, Lu (2010)
601
has conducted a more systematic study using an au-
tomated system. He applied 14 syntactic measures
to a large database of Chinese learners? writing sam-
ples and found that syntactic measures were strong
predictors of students? writing proficiency.
Studies in the area of automated speech scor-
ing have only recently begun to actively investi-
gate the usefulness of syntactic measures for scoring
spontaneous speech (Chen et al2010; Bernstein
et al2010). These have identified clause bound-
aries (identified from manual annotations and au-
tomatically) and obtained length-based features. In
addition to these conventional syntactic complexity
features, Lu (2009) implemented an automated sys-
tem that calculates the revised Developmental Level
(D-Level) Scale (Covington et al2006) using nat-
ural language processing (NLP) techniques. The
original D-Level Scale was proposed by Rosenberg
and Abbeduto (1987) based primarily on observa-
tions of child language acquisition. They classified
children?s grammatical acquisition into 7 different
groups according to the presence of certain types of
complex sentences. The revised D-Level Scale clas-
sified sentences into the eight levels according to the
presence of particular grammatical expressions. For
instance, level 0 is comprised of simple sentences,
while level 5 is comprised of sentences joined by
subordinating conjunction or nonfinite clauses in an
adjunct position. The D-Level Scale has been less
studied in the speech scoring. To our knowledge,
Chen and Zechner (2011) is the only study that ap-
plied the D-Level analyzer to ESL learners? spoken
responses.
In contrast to ESL writing, applying syntactic
complexity features, both conventional length-based
features and D-Level features, presents serious ob-
stacles for speaking. First, the length of the spo-
ken responses are typically shorter than written re-
sponses. Most measures are based on sentence or
sentence-like units, and in speaking tests that elicit
only a few sentences the measures are less reli-
able. Chen and Yoon (2011) observed a marked
decrease in correlation between syntactic measures
and proficiency as response length decreased. In
addition, speech recognition errors only worsen the
situation. Chen and Zechner (2011) showed that
the significant correlation between syntactic mea-
sures and speech proficiency (correlation coefficient
= 0.49) became insignificant when they were applied
to the speech recognition word hypotheses. Errors
in speech recognition seriously influenced the mea-
sures and decreased the performance. Due to these
problems, the existing syntactic measures do not
seem reliable enough for being used in automated
speech proficiency scoring.
In this study, we propose novel syntactic measures
which are relatively robust against speech recogni-
tion errors and are reliable in short responses. In
contrast to recent studies focusing on length-based
features, we focus on capturing differences in the
distribution of morphosyntactic features or gram-
matical expressions across proficiency levels. We in-
vestigate the distribution of a broader class of gram-
matical forms through the use of corpus-based NLP
techniques.
3 Method
Many previous studies, that assess syntactic com-
plexity based on the distribution of morpho-
syntactic features and grammatical expressions, lim-
ited their experiments to a few grammatical expres-
sions. Covington et al2006) and Lu (2009) cov-
ered all sentence types, but their approaches were
based on expert observation (supervised rubrics),
and descriptions of each level were brief and ab-
stract. It is important to develop a more detailed and
refined scale, but developing scales in a supervised
way is difficult due to the subjectivity and the com-
plexity of structures involved.
In order to overcome this problem, we employed
NLP technology and a corpus-based approach. We
hypothesize that the level of acquired grammatical
forms is signaled by the distribution of the POS tags,
and the differences in grammatical proficiency re-
sult in differences in POS tag distribution. Based on
this assumption, we collected large amount of ESL
learners? spoken responses and classified them into
four groups according to their proficiency levels.
The syntactic competence was estimated based on
the similarity between the test responses and learn-
ers? corpus.
A POS-based vector space model (VSM), in
which the response belonging to separate profi-
ciency levels were converted to vectors and the sim-
ilarity between vectors were calculated using cosine
602
similarity measure and tf-idf weighting, was em-
ployed. Such a score-category-based VSM has been
used in automated essay scoring. Attali and Burstein
(2006) to assess the lexical content of an essay by
comparing the words in the test essay with the words
in a sample essays from each score category. We
extend this to assessment of grammar usage using
vectors of POS tags.
Proficient speakers use complicated grammati-
cal expressions, while beginners use simple expres-
sions and sentences with frequent grammatical er-
rors. POS tags (or sequences) capturing these ex-
pressions may be seen in corresponding proportions
in each score group. These distributional differences
are captured by inverse-document frequency.
In addition, we identify frequent POS tag se-
quences as those having high mutual information
and include them in our experiments. Temple (2000)
pointed out that the proficient learners are charac-
terized by increased automaticity in speech produc-
tion. These speakers tend to memorize frequently
used multi-word sequences as a chunk and retrieve
the whole chunk as a single unit. The degree of auto-
maticity can be captured by the frequent occurrence
of POS sequences with high mutual information.
We quantify the usefulness of the generated fea-
tures for the purpose of automatic scoring by first
considering its correlation with the human scores.
We then compare the performance of our features
with those in Lu (2011), where the features are a
collection of measures of syntactic complexity that
have shown promising directions in previous stud-
ies.
4 Data
Two different sets of data were used in this study:
the AEST 48K dataset and AEST balanced dataset.
Both were collections of responses from the AEST,
a high-stakes test of English proficiency and had
no overlaps. The AEST assessment consists of 6
items in which speakers are prompted to provide re-
sponses lasting between 45 and 60 seconds per item.
In summary, approximately 3 minutes of speech is
collected per speaker.
Among the 6 items, two items are tasks that ask
examinees to provide information or opinions on fa-
miliar topics based on their personal experience or
background knowledge. The four remaining items
are integrated tasks that include other language skills
such as listening and reading. All items extract
spontaneous, unconstrained natural speech. The
size, purpose, and speakers? native language infor-
mation for each dataset is summarized in Table 1.
Each response was rated by trained human raters
using a 4-point scoring scale, where 1 indicates
a low speaking proficiency and 4 indicates a high
speaking proficiency. In order to evaluate the relia-
bility of the human ratings, the data should be scored
by two raters. Since none of the AEST balanced
data was double scored the inter-rater agreement ra-
tio was estimated using a large (41K) double-scored
dataset using the same scoring guidelines and scor-
ing process. The Pearson correlation coefficient was
0.63 suggesting a reasonable inter-rater agreement.
The distribution of the scores for this data can be
found in Table 2.
We used the AEST 48K dataset as the training
data and the AEST balanced dataset as the evalua-
tion data.
5 Experiments
5.1 Overview
Our experimental procedure is as follows. All tran-
scriptions were tagged using the POS tagger de-
scribed in Section 5.3 and POS tag sequences were
extracted. Next, the POS-based VSMs (one for
each score class) were created using the AEST 48K
dataset. Finally, for a given test response in the
AEST balanced dataset, similarity features were
generated.
A score-class-specific POS-based VSM was cre-
ated using POS tags generated from the manual tran-
scriptions. For evaluation, two different types of
transcriptions (manual transcription and word hy-
potheses from the speech recognizer described in
Section 5.2) were used in order to investigate the in-
fluence of speech recognition errors in the feature
performance.
5.2 Speech recognition
An HMM recognizer was trained on AEST 48K
dataset - approximately 733 hours of non-native
speech collected from 7872 speakers. A gender in-
dependent triphone acoustic model and combination
603
Corpus name Purpose Number of
speakers
Number of
responses
Native languages Size
(Hrs)
AEST 48K
data
ASR training and
POS model train-
ing
7872 47227 China (20%), Korea (19%),
Japanese (7%), India (7%), oth-
ers (46%)
733
AEST bal-
anced data
Feature develop-
ment and evalua-
tion
480 2880 Korean (15%), Chinese (14%),
Japanese (7%), Spanish (9%),
Others (55%)
44
Table 1: Data size and speakers native languages
Corpus name Size Score1 Score2 Score3 Score4
AEST 48K data Number of files 1953 16834 23106 5334
(%) 4 36 49 11
AEST balanced data Number of files 141 1133 1266 340
(%) 5 40 45 12
Table 2: Proficiency scores and data sizes
of bigram, trigram, and four-gram language models
were used. The word error rate (WER) on the held-
out test dataset was 27%.
5.3 POS tagger
POS tags were generated using the POS tagger im-
plemented in the OpenNLP toolkit. It was trained
on the Switchboard (SWBD) corpus. This POS tag-
ger was trained on about 528K word/tag pairs and
achieved a tagging accuracy of 96.3% on a test set
of 379K words. The Penn POS tag set was used in
the tagger.
5.4 Unit generation using mutual information
POS bigrams with high mutual information were se-
lected and used as a single unit. First, all POS bi-
grams which occurred less than 50 times were fil-
tered out. Next, the remaining POS tag bigrams
were sorted by their mutual information scores, and
two different sets (top50 and top110) were selected.
The selected POS pairs were transformed into com-
pound tags. As a result, we generated three sets
of POS units by this process: the original POS set
without the compound unit (Base), the original set
and an additional 50 compound units (Base+mi50),
and the original set and an additional 110 units
(Base+mi110).
Finally, unigram, bigram and trigram were gener-
ated for each set separately. The size of total terms
in each condition was presented in table 3.
Base Base+mi50 Base+mi110
Unigram 42 93 151
Bigram 1366 4284 9691
Trigram 21918 54856 135430
Table 3: Number of terms used in VSMs
5.5 Building VSMs
For each ngram, three sets of VSMs were built us-
ing three sets of tags as terms, yielding a total of
nine VSMs. The results were based on the individ-
ual model and we did not combine any models.
5.6 Cosine similarity-based features
The cosine similarity has been frequently used in
the information retrieval field to identify the relevant
documents for the given query. This measures the
similarity between a given query and a document by
measuring the cosine of the angle between vectors in
a high-dimensional space, whereby each term in the
query and documents corresponding to a unique di-
mension. If a document is relevant to the query, then
it shares many terms resulting in a small angle. In
this study, the term was a single or compound POS
tag (unigram,bigram or trigram) weighted by its tf-
idf, and the document was the response.
First, the inverse document frequency was calcu-
lated from the training data, and each response was
treated as a document. Next, responses in the same
604
Unigram Bigram Trigram
Base Base
+mi50
Base
+mi110
Base Base
+mi50
Base
+mi110
Base Base
+mi50
Base
+mi110
Trans-
cription
0.301** 0.297** 0.329** 0.427** 0.361** 0.366** 0.402** 0.322** 0.295**
ASR 0.246** 0.272** 0.304** 0.415** 0.348** 0.347** 0.373** 0.311** 0.282**
Table 4: Pearson correlation coefficients between ngram-based features and expert proficiency scores
** Correlation is significant at the 0.01 level
score group were concatenated, and a single vector
was generated for each score group. A total of 4
vectors were generated using training data. For each
test response, a similarity score was calculated as
follows:
cos(~q, ~dj) =
nP
i=1
qidji
nP
i=1
qi2
nP
i=1
di2
qi ? tf(ti, ~q)? log
(
N
df(ti)
)
dji ? tf(ti, ~dj)? log
(
N
df(ti)
)
where ~q is a vector of the test response,
~dj is a vector of the scoreGroupj ,
n is the total number of POS tags,
tf(ti, ~q) is the term frequency of POS tag ti in the
test response,
tf(ti, ~dj) is the term frequency of POS tag ti in the
scoreGroupj ,
N is the total number of training responses,
df(ti) is the document frequency of POS tag ti in
the total training responses
Finally, a total of 4 cos scores (one per score
group) were generated. Among these four values,
the cos4, the similarity score to the responses in the
score group 4, was selected as a feature with the fol-
lowing intuition. cos4 measures the similarity of a
given test response to the representative vector of
score class 4; the larger the value, the closer it would
be to score class 4.
6 Results
6.1 Correlation
Table 4 shows correlations between cosine similarity
features and proficiency scores rated by experts.
The bigram-based features outperformed both
unigram-based and trigram-based features. In par-
ticular, the similarities using the base tag set with
bigrams achieved the best performance. By adding
the mutual information-based compound units to the
original POS tag sets, the performance of features
improved in the unigram models. However, there
was no performance gain in either bigram or tri-
gram models; on the contrary, there was a large
drop in performance. Unigrams have good coverage
but limited power in distinguishing different score
levels. On the other hand, trigrams have opposite
characteristics. Bigrams seem to strike a balance
in both coverage and complexity (from among the
three considered here) and may thus have resulted in
the best performace.
The performance of ASR-based features were
comparable to that of transcription-based features.
The best performing feature among ASR-based-
features were from the bigram and base set, with
correlations nearly the same as the best performing
one among the transcription-based-features. See-
ing how close the correlations were in the case of
transcription-based and ASR-hypothesis based fea-
ture extraction, we conclude that the proposed mea-
sure is robust to ASR errors.
6.2 Comparison with other Measures of
Syntactic Complexity
We compared the performance of our features with
the features of syntactic complexity proposed in (Lu,
2011). Towards this, the clause boundaries of the
ASR hypotheses, were automatically detected using
the automated clause boundary detection method1.
1The automated clause boundary detection method in this
study was a Maximum Entropy Model based on word bigrams,
POS tag bigrams, and pause features. The method achieved an
605
The utterances were then parsed using the Stanford
Parser, and a total of 22 features including both
length-related features and parse-tree based features
were generated using (Lu, 2011). Finally, we calcu-
lated Pearson correlation coefficients between these
features and human proficiency scores.
Study Feature Correlation
Current study bigram based cos4 0.41**
(Lu, 2011) DCC 0.14**
Table 5: Comparison between (Lu, 2011) and this study
** Correlation is significant at the 0.01 level
As indicated in Table 5, the best performing fea-
ture was mean number of dependent clauses per
clause (DCC) and the correlation r was 0.14. No
features other than DCC achieved statistically sig-
nificant correlation. Our best performing feature (bi-
gram based cos4) widely outperformed the best of
Lu (2011)?s features (correlations approximately 0.3
apart).
A logical explanation for the poor performance of
Lu (2011)?s features is that the features are gener-
ated using multi-stage automated process, and the
errors in each process contributes the low feature
performance. For instance, the errors in the auto-
mated clause boundary detection may result in a se-
rious drop in the performance. With the spoken re-
sponses being particularly short (a typical response
in the data set had 10 clauses on average), even one
error in clause boundary detection can seriously af-
fect the reliability of features.
7 Discussion
While the measure of syntactic competence that we
study here is an abstraction of the overall syntactic
competence, without consideration of specific con-
structions, we analyzed the results further with the
intention of casting light on the level of details of
syntactic competence that can be explained using
our measure. Furthermore, this section will show
that bigram POS sequences can yield significant in-
formation on the range and sophistication of gram-
mar usage in the specific assessment context (spon-
F-score of 0.60 on the non-native speakers? ASR hypotheses.
A detailed description of the method is presented in (Chen and
Zechner, 2011)
taneous speech comprised of only declarative sen-
tences).
ESL speakers with high proficiency scores are ex-
pected to use more complicated grammatical expres-
sions that result in a high proportion of POS tags
related to these expressions in that score group. The
distribution of POS tags was analyzed in detail in or-
der to investigate whether there were systematic dis-
tributional changes according to proficiency levels.
Owing to space constraints, we restrict our discus-
sion to the analysis using unigrams (base and com-
pund). For each score group, the POS tags were
sorted based on the frequencies in training data, and
the rank orders were calculated. The more frequent
the POS tag, the higher its rank.
A total of 150 POS tags, including the original
POS tag set and top 110 compound tags, were clas-
sified into 5 classes:
? Absence-of-low-proficiency (ABS): Group of
POS tags that appear in all score groups except
the lowest proficiency group;
? Increase (INC): Group of POS tags whose
ranks increase consistently as proficiency in-
creases;
? Decrease (DEC): Group of POS tags whose
ranks decrease consistently as proficiency in-
creases;
? Constant (CON): Group of POS tags whose
ranks remain same despite change in profi-
ciency;
? Mix: Group of POS tags of with no consistent
pattern in the ranks.
Table 6 presents the number of POS tags in each
class.
ABS INC DEC CON Mix
14 37 33 18 48
Table 6: Tag distribution and proficiency scores
The ?ABS? class mostly consists of ?WP? and
?WDT?; more than 50% of tags in this class are re-
lated to these two tags. ?WP? is a Wh-pronoun while
?WDT? is a Wh-determiner. Since most sentences in
606
our data are declarative sentences, ?Wh? phrase sig-
nals the use of relative clause. Therefore, the lack
of these tags strongly support the hypothesis that the
speakers in score group 1 showed incompetence in
the use of relative clauses or their use in limited sit-
uations.
The ?INC? class can be sub-classified into three
groups: verb, comparative, and relative clause. Verb
group is includes the infinitive (TO VB), passive
(VB VBN, VBD VBN, VBN, VBN IN, VBN RP),
and gerund forms (VBG, VBG RP, VBG TO). Next,
the comparative group encompasses comparative
constructions. Finally, the relative clause group sig-
nals the presence of relative clauses. The increased
proportion of these tags reflects the use of more
complicated tense forms and modal forms as well
as more frequent use of relative clauses. It supports
the hypothesis that speakers with higher proficiency
scores tend to use more complicated grammatical
expressions.
The ?DEC? class can be sub-classified into five
groups: noun, simple tense verb, GW and UH,
non-compound, and comparative. The noun group
is comprised of many noun or proper noun-related
expressions, and their high proportions are consis-
tent with the tendency that less proficient speakers
use nouns more frequently. Secondly, the simple
tense verb group is comprised of the base form (VB)
and simple present and past forms(PRP VBD, VB,
VBD TO, VBP TO, VBZ). The expressions in these
groups are simpler than those in ?Increase? group.
The ?UH? tag is for interjection and filler words
such as ?uh? and ?um?, while the ?GW? tag is for
word-fragments. These two spontaneous speech
phenomena are strongly related to fluency, and it
signals problems in speech production. Frequent
occurrences of these two tags are evidence of fre-
quent planning problems and their inclusion in the
?DEC? class suggests that instances of speech plan-
ning problems decrease with increased proficiency.
Tags in the non-compound group, such as ?DT?,
?MD?, ?RBS?, and ?TO?, have related compound
tags. The non-compound tags are associated with
the expressions that do not co-occur with strongly
related words, and they tend to be related to errors.
For instance, the non-compound ?MD? tag signals
that there is an expression that a modal verb is not
followed by ?VB? (base form) and as seen in the ex-
amples, ?the project may can change? and ?the others
must can not be good?, they are related to grammat-
ical errors.
Finally, the comparative group includes
?RBR JJR?. The decrease of ?RBR JJR? is re-
lated to the correct acquisition of the comparative
form. ?RBR? is for comparative adverbs and ?JJR? is
for comparative adjectives, and the combination of
two tags is strongly related to double-marked errors
such as ?more easier?. In the intermediate stage in
the acquisition of comparative form, learners tend
to use the double-marked form. The compound tags
correctly capture this erroneous stage.
The ?Decrease? class also includes three Wh-
related tags (WDT NN, WDT VBP, WRB), but the
proportion is much smaller than the ?Increase? class.
The above analysis shows that the combination of
original and compound POS tags correctly capture
systematic changes in the grammatical expressions
according to changes in proficiency levels.
The robust performance of our proposed mea-
sure to speech recognition errors may be better ap-
preciated in the context of similar studies. Com-
pared with the state-of-the art measures of syntac-
tic complexity proposed in Lu (2011) our features
achieve significantly better performance especially
when generated from ASR hypotheses. It is to
be noted that the performance drop between the
transcription-based feature and the ASR hypothesis-
based feature was marginal.
8 Conclusions
In this paper, we presented features that measure
syntactic competence for the automated speech scor-
ing. The features measured the range and sophisti-
cation of grammatical expressions based on POS tag
distributions. A corpus with a large number of learn-
ers? responses was collected and classified into four
groups according to proficiency levels. The syntac-
tic competence of the test response was estimated by
identifying the most similar group from the learners?
corpus. Furthermore, speech recognition errors only
resulted in a minor performance drop. The robust-
ness against speech recognition errors is an impor-
tant advantage of our method.
607
Acknowledgments
The authors would like to thank Shasha Xie, Klaus
Zechner, and Keelan Evanini for their valuable com-
ments, help with data preparation and experiments.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e?rater R v.2. The Journal of Technology,
Learning, and Assessment, 4(3).
Kathleen Bardovi-Harlig and Theodora Bofman. 1989.
Attainment of syntactic and morphological accuracy
by advanced language learners. Studies in Second
Language Acquisition, 11:17?34.
Jared Bernstein, Jian Cheng, and Masanori Suzuki. 2010.
Fluency and structural complexity as predictors of L2
oral proficiency. In Proceedings of InterSpeech 2010,
Tokyo, Japan, September.
Lei Chen and Su-Youn Yoon. 2011. Detecting structural
events for assessing non-native speech. In Proceed-
ings of the 6th Workshop on Innovative Use of NLP for
Building Educational Applications, pages 38?45.
Miao Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics 2011, pages 722?731.
Lei Chen, Joel Tetreault, and Xiaoming Xi. 2010.
Towards using structural events to assess non-native
speech. In Proceedings of the NAACL HLT 2010 Fifth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 74?79.
Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
In Proceedings of NAACL00, pages 140?147.
Michael A. Covington, Congzhou He, Cati Brown, Lo-
rina Naci, and John Brown. 2006. How complex
is that sentence? A proposed revision of the Rosen-
berg and Abbeduto D-Level Scale. Technical report,
CASPR Research Report 2006-01, Athens, GA: The
University of Georgia, Artificial Intelligence Center.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learners?
fluency: Comparisons between read and spontaneous
speech. The Journal of the Acoustical Society of Amer-
ica, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learners?
fluency: Comparisons between read and spontaneous
speech. The Journal of the Acoustical Society of Amer-
ica, 111(6):2862?2873.
Sergey Feldman, M.A. Marin, Maria Ostendorf, and
Maya R. Gupta. 2009. Part-of-speech histograms for
genre classification of text. In Acoustics, Speech and
Signal Processing, 2009. ICASSP 2009. IEEE Interna-
tional Conference on, pages 4781 ?4784, april.
Horacio Franco, Leonardo Neumeyer, Yoon Kim, and
Orith Ronen. 1997. Automatic pronunciation scoring
for language instruction. In Proceedings of ICASSP
97, pages 1471?1474.
Xiaofei Lu. 2009. Automatic measurement of syntac-
tic complexity in child language acquisition. Interna-
tional Journal of Corpus Linguistics, 14(1):3?28.
Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. International
Journal of Corpus Linguistics, 15(4):474?496.
Xiaofei Lu. 2011. L2 syntactic complex-
ity analyze. Retrieved March 17, 2012 from
http://www.personal.psu.edu/xxl13/
downloads/l2sca.html/.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, pages 88?93.
Lourdes Ortega. 2003. Syntactic complexity measures
and their relationship to L2 proficiency: A research
synthesis of college?level L2 writing. Applied Lin-
guistics, 24(4):492?518.
Brian. Roark, Margaret Mitchell, John-Paul. Hosom,
Kristy Hollingshead, and Jeffrey Kaye. 2011. Spo-
ken language derived measures for detecting mild cog-
nitive impairment. Audio, Speech, and Language
Processing, IEEE Transactions on, 19(7):2081 ?2090,
sept.
Sheldon Rosenberg and Leonard Abbeduto. 1987. Indi-
cators of linguistic competence in the peer group con-
versational behavior of mildly retarded adults. Applied
Psycholinguistics, 8:19?32.
Liz Temple. 2000. Second language learner speech pro-
duction. Studia Linguistica, pages 288?297.
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in esl writing.
In In Proceedings of COLING.
Silke Witt and Steve Young. 1997. Performance mea-
sures for phone-level pronunciation teaching in CALL.
In Proceedings of the Workshop on Speech Technology
in Language Learning, pages 99?102.
Silke Witt. 1999. Use of the speech recognition in
computer-assisted language learning. Unpublished
dissertation, Cambridge University Engineering de-
partment, Cambridge, U.K.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51:883?895, October.
608
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1305?1315,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Shallow Analysis Based Assessment of Syntactic Complexity for
Automated Speech Scoring
Suma Bhat
Beckman Institute,
University of Illinois,
Urbana, IL
spbhat2@illinois.edu
Huichao Xue
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA
hux10@cs.pitt.edu
Su-Youn Yoon
Educational Testing Service
Princeton, NJ
syoon@ets.org
Abstract
Designing measures that capture various
aspects of language ability is a central
task in the design of systems for auto-
matic scoring of spontaneous speech. In
this study, we address a key aspect of lan-
guage proficiency assessment ? syntactic
complexity. We propose a novel measure
of syntactic complexity for spontaneous
speech that shows optimum empirical per-
formance on real world data in multiple
ways. First, it is both robust and reliable,
producing automatic scores that agree well
with human rating compared to the state-
of-the-art. Second, the measure makes
sense theoretically, both from algorithmic
and native language acquisition points of
view.
1 Introduction
Assessment of a speaker?s proficiency in a second
language is the main task in the domain of au-
tomatic evaluation of spontaneous speech (Zech-
ner et al, 2009). Prior studies in language ac-
quisition and second language research have con-
clusively shown that proficiency in a second lan-
guage is characterized by several factors, some of
which are, fluency in language production, pro-
nunciation accuracy, choice of vocabulary, gram-
matical sophistication and accuracy. The design of
automated scoring systems for non-native speaker
speaking proficiency is guided by these studies in
the choice of pertinent objective measures of these
key aspects of language proficiency.
The focus of this study is the design and per-
formance analysis of a measure of the syntactic
complexity of non-native English responses for
use in automatic scoring systems. The state-of-
the art automated scoring system for spontaneous
speech (Zechner et al, 2009; Higgins et al, 2011)
currently uses measures of fluency and pronuncia-
tion (acoustic aspects) to produce scores that are in
reasonable agreement with human-rated scores of
proficiency. Despite its good performance, there
is a need to extend its coverage to higher order as-
pects of language ability. Fluency and pronunci-
ation may, by themselves, already be good indi-
cators of proficiency in non-native speakers, but
from a construct validity perspective
1
, it is neces-
sary that an automatic assessment model measure
higher-order aspects of language proficiency. Syn-
tactic complexity is one such aspect of proficiency.
By ?syntactic complexity?, we mean a learner?s
ability to use a wide range of sophisticated gram-
matical structures.
This study is different from studies that fo-
cus on capturing grammatical errors in non-native
speakers (Foster and Skehan, 1996; Iwashita et al,
2008). Instead of focusing on grammatical errors
that are found to be highly representative of lan-
guage proficiency, our interest is in capturing the
range of forms that surface in language production
and the degree of sophistication of such forms,
collectively referred to as syntactic complexity in
(Ortega, 2003).
The choice and design of objective measures of
language proficiency is governed by two crucial
constraints:
1. Validity: a measure should show high dis-
criminative ability between various levels of
language proficiency, and the scores pro-
duced by the use of this measure should show
high agreement with human-assigned scores.
2. Robustness: a measures should be derived
automatically and should be robust to errors
in the measure generation process.
A critical impediment to the robustness con-
straint in the state-of-the-art is the multi-stage au-
1
Construct validity is the degree to which a test measures
what it claims, or purports, to be measuring and an important
criterion in the development and use of assessments or tests.
1305
tomated process, where errors in the speech recog-
nition stage (the very first stage) affect subsequent
stages. Guided by studies in second language de-
velopment, we design a measure of syntactic com-
plexity that captures patterns indicative of profi-
cient and non-proficient grammatical structures by
a shallow-analysis of spoken language, as opposed
to a deep syntactic analysis, and analyze the per-
formance of the automatic scoring model with its
inclusion. We compare and contrast the proposed
measure with that found to be optimum in Yoon
and Bhat (2012).
Our primary contributions in this study are:
? We show that the measure of syntactic com-
plexity derived from a shallow-analysis of
spoken utterances satisfies the design con-
straint of high discriminative ability between
proficiency levels. In addition, including our
proposed measure of syntactic complexity in
an automatic scoring model results in a statis-
tically significant performance gain over the
state-of-the-art.
? The proposed measure, derived through a
completely automated process, satisfies the
robustness criterion reasonably well.
? In the domain of native language acquisition,
the presence or absence of a grammatical
structure indicates grammatical development.
We observe that the proposed approach ele-
gantly and effectively captures this presence-
based criterion of grammatical development,
since the feature indicative of presence or ab-
sence of a grammatical structure is optimal
from an algorithmic point of view.
2 Related Work
Speaking in a non-native language requires diverse
abilities, including fluency, pronunciation, into-
nation, grammar, vocabulary, and discourse. In-
formed by studies in second language acquisition
and language testing that regard these factors as
key determiners of spoken language proficiency,
some researchers have focused on the objective
measurement of these aspects of spoken language
in the context of automatic assessment of language
ability. Notable are studies that have focused on
assessment of fluency (Cucchiarini et al, 2000;
Cucchiarini et al, 2002), pronunciation (Witt and
Young, 1997; Witt, 1999; Franco et al, 1997;
Neumeyer et al, 2000), and intonation (Zechner
et al, 2009). The relative success of these studies
has yielded objective measures of acoustic aspects
of speaking ability, resulting in a shift in focus
to more complex aspects of assessment of gram-
mar (Bernstein et al, 2010; Chen and Yoon, 2011;
Chen and Zechner, 2011), topic development (Xie
et al, 2012), and coherence (Wang et al, 2013).
In an effort to assess grammar and usage in a
second language learning environment, numerous
studies have focused on identifying relevant quan-
titative measures. These measures have been used
to estimate proficiency levels in English as a sec-
ond language (ESL) writing with reasonable suc-
cess. Wolf-Quintero et al (1998), Ortega (2003),
and Lu (2010) found that measures such as mean
length of T-unit
2
and dependent clauses per clause
(henceforth termed as length-based measures) are
well correlated with holistic proficiency scores
suggesting that these quantitative measures can be
used as objective indices of grammatical develop-
ment.
In the context of spoken ESL, these measures
have been studied as well but the results have been
inconclusive. The measures could only broadly
discriminate between students? proficiency levels,
rated on a scale with moderate to weak correla-
tions, and strong data dependencies on the par-
ticipant groups were observed (Halleck, 1995;
Iwashita et al, 2008; Iwashita, 2010).
With the recent interest in the area of auto-
matic assessment of speech, there is a concur-
rent need to assess the grammatical development
of ESL students automatically. Studies that ex-
plored the applicability of length-based measures
in an automated scoring system (Chen and Zech-
ner, 2011; Chen and Yoon, 2011) observed another
important drawback of these measures in that set-
ting. Length-based measures do not meet the con-
straints of the design, that, in order for measures
to be effectively incorporated in the automated
speech scoring system, they must be generated in
a fully automated manner, via a multi-stage au-
tomated process that includes speech recognition,
part of speech (POS) tagging, and parsing.
A major bottleneck in the multi-stage process
of an automated speech scoring system for second
language is the stage of automated speech recog-
nition (ASR). Automatic recognition of non-native
speakers? spontaneous speech is a challenging task
as evidenced by the error rate of the state-of-the-
2
T-units are defined as ?the shortest grammatically allow-
able sentences into which writing can be split.? (Hunt, 1965)
1306
art speech recognizer. For instance, Chen and
Zechner (2011) reported a 50.5% word error rate
(WER) and Yoon and Bhat (2012) reported a 30%
WER in the recognition of ESL students? spoken
responses. These high error rates at the recogni-
tion stage negatively affect the subsequent stages
of the speech scoring system in general, and in
particular, during a deep syntactic analysis, which
operates on a long sequence of words as its con-
text. As a result, measures of grammatical com-
plexity that are closely tied to a correct syntac-
tic analysis are rendered unreliable. Not surpris-
ingly, Chen and Zechner (2011) studied measures
of grammatical complexity via syntactic parsing
and found that a Pearson?s correlation coefficient
of 0.49 between syntactic complexity measures
(derived from manual transcriptions) and profi-
ciency scores, was drastically reduced to near non-
existence when the measures were applied to ASR
word hypotheses. This suggests that measures
that rely on deep syntactic analysis are unreliable
in current ASR-based scoring systems for sponta-
neous speech.
In order to avoid the problems encountered
with deep analysis-based measures, Yoon and
Bhat (2012) explored a shallow analysis-based ap-
proach, based on the assumption that the level of
grammar sophistication at each proficiency level
is reflected in the distribution of part-of-speech
(POS) tag bigrams. The idea of capturing dif-
ferences in POS tag distributions for classification
has been explored in several previous studies. In
the area of text-genre classification, POS tag dis-
tributions have been found to capture genre differ-
ences in text (Feldman et al, 2009; Marin et al,
2009); in a language testing context, it has been
used in grammatical error detection and essay
scoring (Chodorow and Leacock, 2000; Tetreault
and Chodorow, 2008). We will see next what as-
pects of syntactic complexity are captured by such
a shallow-analysis.
3 Shallow-analysis approach to
measuring syntactic complexity
The measures of syntactic complexity in this ap-
proach are POS bigrams and are not obtained by a
deep analysis (syntactic parsing) of the structure of
the sentence. Hence we will refer to this approach
as ?shallow analysis?. In a shallow-analysis ap-
proach to measuring syntactic complexity, we rely
on the distribution of POS bigrams at every profi-
ciency level to be representative of the range and
sophistication of grammatical constructions at that
level. At the outset, POS-bigrams may seem too
simplistic to represent any aspect of true syntactic
complexity. We illustrate to the contrary, that they
are indeed able to capture certain grammatical er-
rors and sophisticated constructions by means of
the following instances. Consider the two sentence
fragments below taken from actual responses (the
bigrams of interest and their associated POS tags
are bold-faced).
1. They can/MD to/TO survive . . .
2. They created the culture/NN that/WDT
now/RB is common in the US.
We notice that Example 1 is not only less gram-
matically sophisticated than Example 2 but also
has a grammatical error. The error stems from the
fact that it has a modal verb followed by the word
?to?. On the other hand, Example 2 contains a
relative clause composed of a noun introduced by
?that?. Notice how these grammatical expressions
(one erroneous and the other sophisticated) can be
detected by the POS bigrams ?MD-TO? and ?NN-
WDT?, respectively.
The idea that the level of syntactic complex-
ity (in terms of its range and sophistication) can
be assessed based on the distribution of POS-tags
is informed by prior studies in second language
acquisition. It has been shown that the usage of
certain grammatical constructions (such as that of
the embedded relative clause in the second sen-
tence above) are indicators of specific milestones
in grammar development (Covington et al, 2006).
In addition, studies such as Foster and Skehan
(1996) have successfully explored the utility of
frequency of grammatical errors as objective mea-
sures of grammatical development.
Based on this idea, Yoon and Bhat (2012) de-
veloped a set of features of syntactic complex-
ity based on POS sequences extracted from a
large corpus of ESL learners? spoken responses,
grouped by human-assigned scores of proficiency
level. Unlike previous studies, it did not rely
on the occurrence of normative grammatical con-
structions. The main assumption was that each
score level is characterized by different types of
prominent grammatical structures. These repre-
sentative constructions are gathered from a collec-
tion of ESL learners? spoken responses rated for
overall proficiency. The syntactic complexity of
a test spoken response was estimated based on its
1307
similarity to the proficiency groups in the refer-
ence corpus with respect to the score-specific con-
structions. A score was assigned to the response
based on how similar it was to the high score
group. In Section 4.1, we go over the approach
in further detail.
Our current work is inspired by the shallow
analysis-based approach of Yoon and Bhat (2012)
and operates under the same assumptions of cap-
turing the range and sophistication of grammati-
cal constructions at each score level. However,
the approaches differ in the way in which a spo-
ken response is assigned to a score group. We
first analyze the limitations of the model studied in
(Yoon and Bhat, 2012) and then describe how our
model can address those limitations. The result is
a new measure based on POS bigrams to assess
ESL learners? mastery of syntactic complexity.
4 Models for Measuring Grammatical
Competence
We mentioned that the measure proposed in this
study is derived from assumptions similar to those
studied in (Yoon and Bhat, 2012). Accordingly,
we will summarize the previously studied model,
outline its limitations, show how our proposed
measure addresses those limitations and compare
the two measures for the task of automatic scoring
of speech.
4.1 Vector-Space Model based approach
Yoon and Bhat (2012) explored an approach in-
spired by information retrieval. They treat the con-
catenated collection of responses from a particular
score-class as a ?super? document. Then, regard-
ing POS bigrams as terms, they construct POS-
based vector space models for each score-class
(there are four score classes denoting levels of pro-
ficiency as will be explained in Section 5.2), thus
yielding four score-specific vector-space models
(VSMs). The terms of the VSM are weighted by
the term frequency-inverse document frequency
(tf -idf ) weighting scheme (Salton et al, 1975).
The intuition behind the approach is that responses
in the same proficiency level often share similar
grammar and usage patterns. The similarity be-
tween a test response and a score-specific vector is
then calculated by a cosine similarity metric. Al-
though a total of 4 cosine similarity scores (one
per score group) were generated, only cos
4
from
among the four similarity scores, and cosmax,
were selected as features.
? cos
4
: the cosine similarity score between the
test response and the vector of POS bigrams
for the highest score class (level 4); and,
? cosmax: the score level of the VSM with
which the given response shows maximum
similarity.
Of these, cos
4
was selected based on its empir-
ical performance (it showed the strongest corre-
lation with human-assigned scores of proficiency
among the distance-based measures). In addition,
an intuitive justification for the choice is that the
score-4 vector is a grammatical ?norm? represent-
ing the average grammar usage distribution of the
most proficient ESL students. The measure of syn-
tactic complexity of a response, cos
4
, is its simi-
larity to the highest score class.
The study found that the measures showed rea-
sonable discriminative ability across proficiency
levels. Despite its encouraging empirical perfor-
mance, the VSM method of capturing grammati-
cal sophistication has the following limitations.
First, the VSM-based method is likely to over-
estimate the contribution of the POS bigrams
when highly correlated bigrams occur as terms in
the VSM. Consider the presence of a grammar pat-
tern represented by more than one POS bigram.
For example, both ?NN-WDT? and ?WDT-RB? in
Sentence 2 reflect the learner?s usage of a relative
clause. However, we note that the two bigrams are
correlated and including them both results in an
over-estimation of their contribution. The VSM
set-up has no mechanism to handle correlated fea-
tures.
Second, the tf -idf weighting scheme for rela-
tively rare POS bigrams does not adequately cap-
ture their underlying distribution with respect to
score groups. Grammatical expressions that occur
frequently in one score level but rarely in other
levels can be assumed to be characteristic of a
specific score level. Therefore, the more uneven
the distribution of a grammatical expression across
score classes, the more important that grammatical
expression should be as an indicator of a particular
score class. However, the simple idf scheme can-
not capture this uneven distribution. A pattern that
occurs rarely but uniformly across different score
groups can get the same weight as a pattern which
is unevenly distributed to one score group. Mar-
tineau and Finin (2009) observed this weakness of
the tf -idf weighting in the domain of sentiment
1308
analysis. When using tf -idf weighting to extract
words that were strongly associated with positive
sentiment in a movie review corpus (they consid-
ered each review as a document and a word as a
term), it was found that a substantial proportion
of words with the highest tf -idf were rare words
(e.g., proper nouns) which were not directly asso-
ciated with the sentiment.
We propose to address these important limita-
tions of the VSM approach by the use of a method
that accounts for each of the deficiencies. This is
done by resorting to a maximum entropy model
based approach, to which we turn next.
4.2 Maximum Entropy-Based model
In order to address the limitations discussed in 4.1,
we propose a classification-based approach. Tak-
ing an approach different from previous studies,
we formulate the task of assigning a score of syn-
tactic complexity to a spoken response as a classi-
fication problem: given a spoken response, assign
the response to a proficiency class. A classifier is
trained in an inductive fashion, using a large cor-
pus of learner responses that is divided into pro-
ficiency scores as the training data and then used
to test data that is similar to the training data. A
distinguishing feature of the current study is that
the measure is based on a comparison of charac-
teristics of the test response to models trained on
large amounts of data from each score point, as op-
posed to measures that are simply characteristics
of the responses themselves (which is how mea-
sures have been considered in prior studies).
The inductive classifier we use here is the
maximum-entropy model (MaxEnt) which has
been used to solve several statistical natural lan-
guage processing problems with much success
(Berger et al, 1996; Borthwick et al, 1998; Borth-
wick, 1999; Pang et al, 2002; Klein et al, 2003;
Rosenfeld, 2005). The productive feature en-
gineering aspects of incorporating features into
the discriminative MaxEnt classifier motivate the
model choice for the problem at hand. In partic-
ular, the ability of the MaxEnt model?s estimation
routine to handle overlapping (correlated) features
makes it directly applicable to address the first lim-
itation of the VSM model. The second limitation,
related to the ineffective weighting of terms via
the the tf -idf scheme, seems to be addressed by
the fact that the MaxEnt model assigns a weight
to each feature (in our case, POS bigrams) on a
per-class basis (in our case, score group), by tak-
ing every instance into consideration. Therefore,
a MaxEnt model has an advantage over the model
described in 4.1 in that it uses four different weight
schemes (one per score level) and each scheme is
optimized for each score level. This is beneficial
in situations where the features are not evenly im-
portant across all score levels.
5 Experimental Setup
Our experiments seek answers to the following
questions.
1. To what extent does a MaxEnt-score of syn-
tactic complexity discriminate between levels
of proficiency?
2. What is the effect of including the proposed
measure of syntactic complexity in the state-
of-the-art automatic scoring model?
3. How robust is the measure to errors in the var-
ious stages of automatic generation?
5.1 Tasks
In order to answer the motivating questions of the
study, we set-up two tasks. In the first task, we
compare the extent to which the VSM-based mea-
sure and the MaxEnt-based measure (outlined in
4.1 and 4.2 above) discriminate between levels of
syntactic complexity. Additionally, we compare
the performance of an automatic scoring model of
overall proficiency that includes the measures of
syntactic complexity from each of the two mod-
els being compared and analyze the gains with re-
spect to the state-of-the-art. In the second task, we
study the measures? robustness to errors incurred
by ASR.
5.2 Data
In this study, we used a collection of responses
from an international English language assess-
ment. The assessment consisted of questions to
which speakers were prompted to provide sponta-
neous spoken responses lasting approximately 45-
60 seconds per question. Test takers read and/or
listened to stimulus materials and then responded
to questions based on the stimuli. All questions so-
licited spontaneous, unconstrained natural speech.
A small portion of the available data with inad-
equate audio quality and lack of student response
was excluded from the study. The remaining re-
sponses were partitioned into two datasets: the
ASR set and the scoring model training/test (SM)
1309
set. The ASR set, with 47,227 responses, was
used for ASR training and POS similarity model
training. The SM set, with 2,950 responses, was
used for feature evaluation and automated scoring
model evaluation. There was no overlap in speak-
ers between the ASR set and the SM set.
Each response was rated for overall proficiency
by trained human scorers using a 4-point scoring
scale, where 1 indicates low speaking proficiency
and 4 indicated high speaking proficiency. The
distribution of proficiency scores, along with other
details of the data sets, are presented in Table 1.
As seen in Table 1, there is a strong bias towards
the middle scores (score 2 and 3) with approxi-
mately 84-85% of the responses belonging to these
two score levels. Although the skewed distribution
limits the number of score-specific instances for
the highest and lowest scores available for model
training, we used the data without modifying the
distribution since it is representative of responses
in a large-scale language assessment scenario.
Human raters? extent of agreement in the sub-
jective task of rating responses for language pro-
ficiency constrains the extent to which we can ex-
pect a machine?s score to agree with that of hu-
mans. An estimate of the extent to which human
raters agree on the subjective task of proficiency
assessment, is obtained by two raters scoring ap-
proximately 5% of data (2,388 responses from
ASR set and 140 responses from SM set). Pear-
son correlation r between the scores assigned by
the two raters was 0.62 in ASR set and 0.58 in SM
set. This level of agreement will guide the evalua-
tion of the human-machine agreement on scores.
5.3 Stages of Automatic Grammatical
Competence Assessment
Here we outline the multiple stages involved in the
automatic syntactic complexity assessment. The
first stage, ASR, yields an automatic transcription,
which is followed by the POS tagging stage. Sub-
sequently, the feature extraction stage (a VSM or
a MaxEnt model as the case may be) generates the
syntactic complexity feature which is then incor-
porated in a multiple linear regression model to
generate a score.
The steps for automatic assessment of overall
proficiency follow an analogous process (either in-
cluding the POS tagger or not), depending on the
objective measure being evaluated. The various
objective measures are then combined in the mul-
tiple regression scoring model to generate an over-
all score of proficiency.
5.3.1 Automatic Speech Recognizer
An HMM recognizer was trained using ASR set
(approximately 733 hours of non-native speech
collected from 7,872 speakers). A gender inde-
pendent triphone acoustic model and combination
of bigram, trigram, and four-gram language mod-
els were used. A word error rate (WER) of 31%
on the SM dataset was observed.
5.3.2 POS tagger
POS tags were generated using the POS tagger
implemented in the Open-NLP toolkit
3
. It was
trained on the Switchboard (SWBD) corpus. This
POS tagger was trained on about 528K word/tag
pairs. A combination of 36 tags from the Penn
Treebank tag set and 6 tags generated for spoken
languages were used in the tagger.
The tagger achieved a tagging accuracy of
96.3% on a Switchboard evaluation set composed
of 379K words, suggesting high accuracy of the
tagger. However, due to substantial amount of
speech recognition errors in our data, the POS
error rate (resulting from the combined errors of
ASR and automated POS tagger) is expected to be
higher.
5.3.3 VSM-based Model
We used the ASR data set to train a POS-bigram
VSM for the highest score class and generated
cos
4
and cosmax reported in Yoon and Bhat
(2012), for the SM data set as outlined in Sec-
tion 4.1.
5.3.4 Maximum Entropy Model Classifier
The input to the classifier is a set of POS bi-
grams (1366 bigrams in all) obtained from the
POS-tagged output of the data. We considered
binary-valued features (whether a POS bigram oc-
curred or not), occurrence frequency, and relative
frequency as input for the purpose of experimen-
tation. We used the maximum entropy classifier
implementation in the MaxEnt toolkit
4
. The clas-
sifier was trained using the LBFGS algorithm for
parameter estimation and used equal-scale gaus-
sian priors for smoothing. The results that fol-
low are based on MaxEnt classifier?s parameter
settings initialized to zero. Since a preliminary
3
http://opennlp.apache.org
4
http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html.
1310
Data set No. of No. of Score Score distribution
responses speakers Mean SD 1 2 3 4
ASR 47,227 7,872 2.67 0.73 1,953 16,834 23,106 5,334
4% 36% 49% 11%
SM 2,950 500 2.61 0.74 166 1,103 1,385 296
6% 37% 47% 10%
Table 1: Data size and score distribution
analysis of the effect of varying the feature (bi-
nary or frequency) revealed that the binary-valued
feature was optimal (in terms of yielding the best
agreement between human and machine scores),
we only report our results for this case. The ASR
data set was used to train the MaxEnt classifier and
the features generated from the SM data set were
used for evaluation.
One straightforward way of using the maximum
entropy classifier?s prediction for our case is to
directly use its predicted score-level ? 1, 2, 3 or
4. However, this forces the classifier to make a
coarse-grained choice and may over-penalize the
classifier?s scoring errors. To illustrate this, con-
sider a scenario where the classifier assigns two
responses A and B to score level 2 (based on the
maximum a posteriori condition). Suppose that,
for response A, the score class with the second
highest probability corresponds to score level 1
and that, for response B, it corresponds to score
level 3. It is apparent that the classifier has an
overall tendency to assign a higher score to B, but
looking at its top preference alone (2 for both re-
sponses), masks this tendency.
We thus capture the classifier?s finer-grained
scoring tendency by calculating the expected value
of the classifier output. For a given response, the
MaxEnt classifier calculates the conditional prob-
ability of a score-class given the response, in turn
yielding conditional probabilities of each score
group given the observation ? p
i
for score group
i ? {1, 2, 3, 4}. In our case, we consider the pre-
dicted score of syntactic complexity to be the ex-
pected value of the class label given the observa-
tion as, mescore = 1?p
1
+2?p
2
+3?p
3
+4?p
4
.
This permits us to better represent the score as-
signed by the MaxEnt classifier as a relative pref-
erence over score assignments.
5.3.5 Automatic Scoring System
We consider a multiple regression automatic scor-
ing model as studied in Zechner et al (2009; Chen
and Zechner (2011; Higgins et al (2011). In its
state-of-the-art set-up, the following model uses
the features ? HMM acoustic model score (global
normalized), speaking rate, word types per sec-
ond, average chunk length in words and language
model score (global normalized). We use these
features by themselves (Base), and also in con-
junction with the VSM-based feature (cva4) and
the MaxEnt-based feature (mescore).
5.4 Evaluation Metric
We evaluate the measures using the metrics cho-
sen in previous studies (Zechner et al, 2009; Chen
and Zechner, 2011; Yoon and Bhat, 2012). A
measure?s utility has been evaluated according to
its ability to discriminate between levels of pro-
ficiency assigned by human raters. This is done
by considering the Pearson correlation coefficient
between the feature and the human scores. In an
ideal situation, we would have compared machine
score with scores of grammatical skill assigned by
human raters. In our case, however, with only
access to the overall proficiency scores, we use
scores of language proficiency as those of gram-
matical skill.
A criterion for evaluating the performance of
the scoring model is the extent to which the au-
tomatic scores of overall proficiency agree with
the human scores. As in prior studies, here too
the level of agreement is evaluated by means of
the weighted kappa measure as well as unrounded
and rounded Pearson?s correlations between ma-
chine and human scores (since the output of the re-
gression model can either be rounded or regarded
as is). The feature that maximizes this degree of
agreement will be preferred.
6 Experimental Results
First, we compare the discriminative ability of
measures of syntactic complexity (VSM-model
based measure with that of the MaxEnt-based
measure) across proficiency levels. Table 2 sum-
marizes our experimental results for this task. We
1311
Features Manual Transcriptions ASR
mescore 0.57 0.52
cos
4
0.48 0.43
cosmax - 0.31
Table 2: Pearson correlation coefficients between measures and holistic proficiency scores. All values
are significant at level 0.01. Only the measures cos
4
and mescore were compared for robustness using
manual and ASR transcriptions.
notice that of the measures compared, mescore
shows the highest correlation with scores of syn-
tactic complexity. The correlation was approxi-
mately 0.1 higher in absolute value than that of
cos
4
, which was the best performing feature in the
VSM-based model and the difference is statisti-
cally significant.
Seeking to study the robustness of the mea-
sures derived using a shallow analysis, we next
compare the two measures studied here, with re-
spect to the impact of speech recognition errors on
their correlation with scores of syntactic complex-
ity. Towards this end, we compare mescore and
cos
4
when POS bigrams are extracted from man-
ual transcriptions (ideal ASR) and ASR transcrip-
tions.
In Table 2, noticing that the correlations de-
crease going along a row, we can say that the er-
rors in the ASR system caused both mescore and
cos
4
to under-perform. However, the performance
drop (around 0.05) resulting from a shallow anal-
ysis is relatively small compared to the drop ob-
served while employing a deep syntactic analysis.
Chen and Zechner (2011) found that while using
measures of syntactic complexity obtained from
transcriptions, errors in ASR transcripts caused
over 0.40 drop in correlation from that found with
manual transcriptions
5
. This comparison suggests
that the current POS-based shallow analysis ap-
proach is more robust to ASR errors compared to
a syntactic analysis-based approach.
The effect of the measure of syntactic complex-
ity is best studied by including it in an automatic
scoring model of overall proficiency. We com-
pare the performance gains over the state-of-the-
art with the inclusion of additional features (VSM-
based and MaxEnt-based, in turn). Table 3 shows
the system performance with different grammar
sophistication measures. The results reported are
averaged over a 5-fold cross validation of the mul-
tiple regression model, where 80% of the SM data
5
Due to differences in the dataset and ASR system, a di-
rect comparison between the current study and the cited prior
study was not possible.
set is used to train the model and the evaluation is
done using 20% of the data in every fold.
As seen in Table 3, using the proposed measure,
mescore, leads to an improved agreement be-
tween human and machine scores of proficiency.
Comparing the unrounded correlation results in
Table 3 we notice that the model Base+mescore
shows the highest correlation of predicted scores
with human scores. In addition, we test the sig-
nificance of the difference between two depen-
dent correlations using Steiger?s Z-test (via the
paired.r function in the R statistical package
(Revelle, 2012)). We note that the performance
gain of Base+mescore over Base as well as over
Base + cos4 is statistically significant at level =
0.01. The performance gain of Base+cos4 over
Base, however, is not statistically significant at
level = 0.01. Thus, the inclusion of the MaxEnt-
based measure of syntactic complexity results in
improved agreement between machine and hu-
man scores compared to the state-of-the-art model
(here, Base).
7 Discussions
We now discuss some of the observations and re-
sults of our study with respect to the following
items.
Improved performance: We sought to verify
empirically that the MaxEnt model really outper-
forms the VSM in the case of correlated POS
bigrams. To see this, we separate the test set
into three subsets A,B,C. Set A contains re-
sponses where MaxEnt outperforms VSM; set B
contains responses where VSM outperforms Max-
Ent; set C contains responses where their predic-
tions are comparable. For each group of responses
s ? {A,B,C}, we calculate the percentage of re-
sponses P
s
where two highly correlated POS bi-
grams occur
6
. We found that the percentages fol-
low the order: P
A
= 12.93% > P
C
= 7.29% >
6
We consider two POS bigrams to be highly correlated,
when the their pointwise-mutual information is higher than
4.
1312
Evaluation method Base Base+cos4 Base+mescore
Weighted kappa 0.503 0.524 0.546
Correlation (unrounded) 0.548 0.562 0.592
Correlation (rounded) 0.482 0.492 0.519
Table 3: Comparison of scoring model performances using features of syntactic complexity studied in
this paper along with those available in the state-of-the-art. Here, Base is the scoring model without the
measures of syntactic complexity. All correlations are significant at level 0.01.
P
B
= 4.41%. This suggests that when correlated
POS bigrams occur, MaxEnt is more likely to pro-
vide better score predictions than VSM does.
Feature design: In the case of MaxEnt,
the observation that binary-valued features (pres-
ence/absence of POS bigrams) yield better perfor-
mance than features indicative of the occurrence
frequency of the bigram has interesting implica-
tions. This was also observed in Pang et al (2002)
where it was interpreted to mean that overall senti-
ment is indicated by the presence/absence of key-
words, as opposed to topic of a text, which is in-
dicated by the repeated use of the same or simi-
lar terms. An analogous explanation is applicable
here.
At first glance, the use of the presence/absence
of grammatical structures may raise concerns
about a potential loss of information (e.g. the dis-
tinction between an expression that is used once
and another that is used multiple times is lost).
However, when considered in the context of lan-
guage acquisition studies, this approach seems to
be justified. Studies in native language acquisi-
tion, have considered multiple grammatical devel-
opmental indices that represent the grammatical
levels reached at various stages of language acqui-
sition. For instance, Covington et al (2006) pro-
posed the revised D-level scale which was origi-
nally studied by Rosenberg and Abbeduto (1987).
The D-Level Scale categorizes grammatical de-
velopment into 8 levels according to the pres-
ence of a set of diverse grammatical expressions
varying in difficulty (for example, level 0 con-
sists of simple sentences, while level 5 consists
of sentences joined by a subordinating conjunc-
tion). Similarly, Scarborough (1990) proposed
the Index of Productive Syntax (IPSyn), accord-
ing to which, the presence of particular grammati-
cal structures, from a list of 60 structures (ranging
from simple ones such as including only subjects
and verbs, to more complex constructions such as
conjoined sentences) is evidence of language ac-
quisition milestones.
Despite the functional differences between the
indices, there is a fundamental operational simi-
larity - that they both use the presence or absence
of grammatical structures, rather than their oc-
currence count, as evidence of acquisition of cer-
tain grammatical levels. The assumption that a
presence-based view of grammatical level acquisi-
tion is also applicable to second language assess-
ment helps validate our observation that binary-
valued features yield a better performance when
compared with frequency-valued features.
Generalizability: The training and test sets
used in this study had similar underlying distribu-
tions ? they both sought unconstrained responses
to a set of items with some minor differences in
item type. Looking ahead, an important question
is the extent to which our measure is sensitive to a
mismatch between training and test data.
8 Conclusions
Seeking alternatives to measuring syntactic com-
plexity of spoken responses via syntactic parsers,
we study a shallow-analysis based approach for
use in automatic scoring.
Empirically, we show that the proposed mea-
sure, based on a maximum entropy classification,
satisfied the constraints of the design of an objec-
tive measure to a high degree. In addition, the pro-
posed measure was found to be relatively robust to
ASR errors. The measure outperformed a related
measure of syntactic complexity (also based on
shallow-analysis of spoken response) previously
found to be well-suited for automatic scoring. In-
cluding the measure of syntactic complexity in
an automatic scoring model resulted in statisti-
cally significant performance gains over the state-
of-the-art. We also make an interesting observa-
tion that the impressionistic evaluation of syntactic
complexity is better approximated by the presence
or absence of grammar and usage patterns (and
not by their frequency of occurrence), an idea sup-
ported by studies in native language acquisition.
1313
References
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39?71.
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010. Fluency and structural complexity as predic-
tors of L2 oral proficiency. In Proceedings of Inter-
Speech, pages 1241?1244.
Andrew Borthwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998. Exploiting diverse
knowledge sources via maximum entropy in named
entity recognition. In Proc. of the Sixth Workshop
on Very Large Corpora.
Andrew Borthwick. 1999. A maximum entropy ap-
proach to named entity recognition. Ph.D. thesis,
New York University.
Lei Chen and Su-Youn Yoon. 2011. Detecting
structural events for assessing non-native speech.
In Proceedings of the 6th Workshop on Innovative
Use of NLP for Building Educational Applications,
IUNLPBEA ?11, pages 38?45, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722?
731.
Martin Chodorow and Claudia Leacock. 2000. An un-
supervised method for detecting grammatical errors.
In Proceedings of NAACL, pages 140?147.
Michael A Covington, Congzhou He, Cati Brown, Lo-
rina Naci, and John Brown. 2006. How complex
is that sentence? a proposed revision of the rosen-
berg and abbeduto d-level scale. ReVision. Wash-
ington, DC http://www. ai. uga. edu/caspr/2006-01-
Covington. pdf.(Accessed May 10, 2010.).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers? fluency by means of automatic speech recogni-
tion technology. The Journal of the Acoustical Soci-
ety of America, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers? fluency: comparisons between read and sponta-
neous speech. The Journal of the Acoustical Society
of America, 111(6):2862?2873.
Sergey Feldman, M.A. Marin, Mari Ostendorf, and
Maya R. Gupta. 2009. Part-of-speech histograms
for genre classification of text. In Proceedings of
ICASSP, pages 4781 ?4784.
Pauline Foster and Peter Skehan. 1996. The influence
of planning and task type on second language per-
formance. Studies in Second Language Acquisition,
18:299?324.
Horacio Franco, Leonardo Neumeyer, Yoon Kim, and
Orith Ronen. 1997. Automatic pronunciation scor-
ing for language instruction. In Proceedings of
ICASSP, pages 1471?1474.
Gene B Halleck. 1995. Assessing oral proficiency: a
comparison of holistic and objective measures. The
Modern Language Journal, 79(2):223?234.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David Williamson. 2011. A three-stage approach
to the automated scoring of spontaneous spoken re-
sponses. Computer Speech & Language, 25(2):282?
306.
Kellogg W Hunt. 1965. Grammatical structures writ-
ten at three grade levels. ncte research report no. 3.
Noriko Iwashita, Annie Brown, Tim McNamara, and
Sally O?Hagan. 2008. Assessed levels of second
language speaking proficiency: How distinct? Ap-
plied Linguistics, 29(1):24?49.
Noriko Iwashita. 2010. Features of oral proficiency in
task performance by efl and jfl learners. In Selected
proceedings of the Second Language Research Fo-
rum, pages 32?47.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003-Volume 4, pages 180?183. Asso-
ciation for Computational Linguistics.
Xiaofei Lu. 2010. Automatic analysis of syntac-
tic complexity in second language writing. Inter-
national Journal of Corpus Linguistics, 15(4):474?
496.
M.A Marin, Sergey Feldman, Mari Ostendorf, and
Maya R. Gupta. 2009. Filtering web text to match
target genres. In Proceedings of ICASSP, pages
3705?3708.
Justin Martineau and Tim Finin. 2009. Delta tfidf: An
improved feature space for sentiment analysis. In
ICWSM.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, pages 88?93.
Lourdes Ortega. 2003. Syntactic complexity measures
and their relationship to L2 proficiency: A research
synthesis of college?level L2 writing. Applied Lin-
guistics, 24(4):492?518.
1314
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79?86. As-
sociation for Computational Linguistics.
William Revelle, 2012. psych: Procedures for Psycho-
logical, Psychometric, and Personality Research.
Northwestern University, Evanston, Illinois. R
package version 1.2.1.
Sheldon Rosenberg and Leonard Abbeduto. 1987. In-
dicators of linguistic competence in the peer group
conversational behavior of mildly retarded adults.
Applied Psycholinguistics, 8:19?32.
Ronald Rosenfeld. 2005. Adaptive statistical language
modeling: a maximum entropy approach. Ph.D. the-
sis, IBM.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
Hollis S Scarborough. 1990. Index of productive syn-
tax. Applied Psycholinguistics, 11(1):1?22.
Joel R. Tetreault and Martin Chodorow. 2008. The
ups and downs of preposition error detection in ESL
writing. In Proceedings of COLING, pages 865?
872.
Xinhao Wang, Keelan Evanini, and Klaus Zechner.
2013. Coherence modeling for the automated as-
sessment of spontaneous spoken responses. In Pro-
ceedings of NAACL-HLT, pages 814?819.
Silke Witt and Steve Young. 1997. Performance
measures for phone-level pronunciation teaching in
CALL. In Proceedings of STiLL, pages 99?102.
Silke Witt. 1999. Use of the speech recognition in
computer-assisted language learning. Unpublished
dissertation, Cambridge University Engineering de-
partment, Cambridge, U.K.
Kate Wolf-Quintero, Shunji Inagaki, and Hae-Young
Kim. 1998. Second language development in writ-
ing: Measures of fluency, accuracy, and complexity.
Technical Report 17, Second Language Teaching
and curriculum Center, The University of Hawai?i,
Honolulu, HI.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the NAACL-HLT, pages
103?111.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
esl learners? syntactic competence based on similar-
ity measures. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 600?608. Association for Compu-
tational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51(10):883?895.
1315
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 180?189,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Vocabulary Profile as a Measure of Vocabulary Sophistication
Su-Youn Yoon, Suma Bhat*, Klaus Zechner
Educational Testing Service, 660 Rosedale Road, Princeton, NJ, USA
{syoon,kzechner}@ets.org
* University of Illinois, Urbana-Champaign, IL, USA
sumapramod@gmail.com
Abstract
This study presents a method that assesses
ESL learners? vocabulary usage to improve
an automated scoring system of sponta-
neous speech responses by non-native English
speakers. Focusing on vocabulary sophistica-
tion, we estimate the difficulty of each word
in the vocabulary based on its frequency in
a reference corpus and assess the mean diffi-
culty level of the vocabulary usage across the
responses (vocabulary profile).
Three different classes of features were gen-
erated based on the words in a spoken re-
sponse: coverage-related, average word rank
and the average word frequency and the extent
to which they influence human-assigned lan-
guage proficiency scores was studied. Among
these three types of features, the average word
frequency showed the most predictive power.
We then explored the impact of vocabulary
profile features in an automated speech scor-
ing context, with particular focus on the im-
pact of two factors: genre of reference corpora
and the characteristics of item-types.
The contribution of the current study lies in
the use of vocabulary profile as a measure of
lexical sophistication for spoken language as-
sessment, an aspect heretofore unexplored in
the context of automated speech scoring.
1 Introduction
This study provides a method that measures ESL
(English as a second language) learners? compe-
tence in vocabulary usage.
Spoken language assessments typically measure
multiple dimensions of language ability. Overall
proficiency in the target language can be assessed
by testing the abilities in various areas including flu-
ency, pronunciation, and intonation; grammar and
vocabulary; and discourse structure. With the recent
move toward the objective assessment of language
ability (spoken and written), it is imperative that we
develop methods for quantifying these abilities and
measuring them automatically.
A majority of the studies in automated speech
scoring have focused on fluency (Cucchiarini et al,
2000; Cucchiarini et al, 2002), pronunciation (Witt
and Young, 1997; Witt, 1999; Franco et al, 1997;
Neumeyer et al, 2000), and intonation (Zechner et
al., 2011). More recently, Chen and Yoon (2011)
and Chen and Zechner (2011) have measured syn-
tactic competence in speech scoring. However, only
a few have explored features related to vocabulary
usage and they have been limited to type-token ratio
(TTR) related features (e.g., Lu (2011)). In addi-
tion, Bernstein et al (2010) developed vocabulary
features that measure the similarity between the vo-
cabulary in the test responses and the vocabulary in
the pre-collected texts in the same topic. However,
their features assessed content and topicality, not vo-
cabulary usage.
The speaking construct of vocabulary usage com-
prises two sub-constructs: sophistication and preci-
sion. The aspect of vocabulary that we intend to
measure in this paper is that of lexical sophistication,
also termed lexical diversity and lexical richness in
second language studies. Measures of lexical so-
phistication attempt to quantify the degree to which
a varied and large vocabulary is used (Laufer and
Nation, 1995). In order to assess the degree of lex-
180
ical sophistication, we employ a vocabulary profile-
based approach (partly motivated from the results of
a previous study, as will be explained in Section 2).
By a vocabulary profile, it is meant that the fre-
quency of each vocabulary item is calculated from
a reference corpus covering the language variety of
the target situation. The degree of lexical sophisti-
cation is captured by the word frequency - low fre-
quency words are considered to be more difficult,
and therefore more sophisticated. We then design
features that capture the difficulty level of vocabu-
lary items in test takers? responses. Finally, we per-
form correlation analyses between these new fea-
tures and human proficiency scores and assess the
feature?s importance with respect to the other fea-
tures in an automatic scoring module. The novelty
of this study lies in the use of vocabulary profile in
an automatic scoring set-up to assess lexical sophis-
tication.
This paper will proceed as follows: we will re-
view related work in Section 2. Data and experiment
setup will be explained in Section 3 and Section 4.
Next, we will present the results in Section 5, discuss
them in Section 6, and conclude with a summary of
the importance of our findings in Section 7.
2 Related Work
Measures of lexical richness have been the focus of
several studies involving assessment of L1 and L2
language abilities (Laufer and Nation, 1995; Ver-
meer, 2000; Daller et al, 2003; Kormos and Denes,
2004). The types of measures considered in these
studies can be grouped into quantitative and qualita-
tive measures.
The quantitative measures give insight into the
number of words known, but do not distinguish them
from one another based on their category or fre-
quency in language use. They have evolved to make
up for the widely applied measure type-token-ratio
(TTR). However, owing to its sensitivity to the num-
ber of tokens, TTR has been considered as an un-
stable measure in differing proficiency levels of lan-
guage learners. The Guiraud index, Uber index, and
Herdan index (Vermeer, 2000; Daller et al, 2003;
Lu, 2011) are some measures in this category mostly
derived from TTR as either simpler transformations
of the TTR or its scaled versions to ameliorate the
effect of differing token cardinalities.
Qualitative measures, on the other hand, dis-
tinguish themselves from those derived from TTR
since they take into account distinctions between
words such as their parts of speech or difficulty lev-
els. Adding a qualitative dimension gives more in-
sight into lexical aspects of language ability than
the purely quantitative measures such as TTR-based
measures. Some measures in this category in-
clude a derived form of the limiting relative diver-
sity (LRD) given by
?
D(verbs)/D(nouns) using
the D-measure proposed in (Malvern and Richards,
1997), Lexical frequency profile (LFP) (Laufer and
Nation, 1995) and P-Lex (Meara and Bell, 2003).
LFP uses a vocabulary profile (VP) for a given
body of written text or spoken utterance and gives
the percentage of words used at different frequency
levels (such as from the one-thousand most com-
mon words, the next thousand most common words)
where the words themselves come from a pre-
compiled vocabulary list, such as the Academic
Word List (AWL) with its associated frequency dis-
tribution on words by Coxhead(1998). Frequency
level refers to a class of words (or appropriately cho-
sen word units) that are grouped based on their fre-
quencies of actual usage in corpora. P-Lex is an-
other approach that uses the frequency level of the
words to assess lexical richness. These measures are
based on the differing frequencies of lexical items
and hence rely on the availability of frequency lists
for the language being considered.
These two different types of measures have been
used in the analysis of essays written by second lan-
guage learners of English (ESL). Laufer and Nation
(1995) have shown that LFP correlates well with an
independent measure of vocabulary knowledge and
that it is possible to categorize learners according to
different proficiency levels using this measure. In
another study seeking to understand the extent to
which VP based on students? essays predicted their
academic performance (Morris and Cobb, 2004), it
was observed that students? vocabulary profile re-
sults correlated significantly with their grades. Ad-
ditionally, VP was found to be indicative of finer dis-
tinctions in the language skills of high proficiency
nonnative speakers than oral interviews can cover.
Furthermore, these measures have been employed
in automated essay scoring. Attali and Burstein
181
(2006) used average word frequency and average
word length in characters across the words in the
essay. In addition to the average word frequency
measure, the average word length measure was im-
plemented to assess the average difficulty of the
word used in the essay under the assumption that
the words with more characters were more difficult
than the words with fewer characters. These fea-
tures showed promising performance in estimating
test takers? proficiency levels.
In contrast to qualitative measures, quantitative
measures did not achieve promising performance.
Vermeer (2000) showed that quantitative measures
achieve neither the validity nor the reliability of the
measures, regardless of the transformations and cor-
rections.
More recently, the relationship of lexical rich-
ness to ESL learners? speaking task performance
has been studied by Lu (2011). The comprehensive
study was aimed at measuring lexical richness along
the three dimensions of lexical density, sophistica-
tion, and variation, using 25 different metrics (be-
longing to both the qualitative and quantitative cate-
gories above) available in the language acquisition
literature. His results, based on the manual tran-
scription of a spoken corpus of English learners, in-
dicate that a) lexical variation (the number of word
types) correlated most strongly with the raters? judg-
ments of the quality of ESL learners? oral narratives,
b) lexical sophistication only had a very small ef-
fect, and c) lexical density (indicative of proportion
of lexical words) in an oral narrative did not appear
to relate to its quality.
In this study, we seek to quantify vocabulary us-
age in terms of measures of lexical sophistication:
VP based on a set of reference word lists. The nov-
elty of the current study lies in the use of VP as
a measure of lexical sophistication for spoken lan-
guage assessment. It derives support from other
studies (Morris and Cobb, 2004; Laufer and Nation,
1995) but is carried out in a completely different
context, that of automatic scoring of proficiency lev-
els in spontaneous speech, an area not explored thus
far in existing literature.
Furthermore, we investigate the impact of the
genre of the reference corpus on the performance of
these lexical measures. For this purpose, three dif-
ferent corpora will be used to generate reference fre-
quency levels. Finally, we will investigate how the
characteristics of the item types influence the perfor-
mance of these measures.
3 Data
The AEST balanced data set, a collection of re-
sponses from the AEST, is used in this study.
AEST is a high-stakes test of English proficiency,
and it consists of 6 items in which speakers are
prompted to provide responses lasting between 45
and 60 seconds per item, yielding approximately 5
minutes of spoken content per speaker.
Among the 6 items, two items elicit information
or opinions on familiar topics based on the exam-
inees? personal experience or background knowl-
edge. These constitute the independent (IND) items.
The four remaining items are integrated tasks that
include other language skills such as listening and
reading. These constitute the integrated (INT)
items. Both sets of items extract spontaneous and
unconstrained natural speech. The primary dif-
ference between the two elicitation types is that
IND items only provide a prompt whereas INT items
provide a prompt, a reading passage, and a listening
stimulus. The size, purpose, and speakers? native
language information for each dataset are summa-
rized in Table 1. All items extract spontaneous, un-
constrained natural speech.
Each response was rated by a trained human rater
using a 4-point scoring scale, where 1 indicates
a low speaking proficiency and 4 indicates a high
speaking proficiency. The scoring guideline is sum-
marized in the AEST rubrics.
Since none of the AEST balanced data was
double-scored, we estimate the inter-rater agreement
ratio of the corpus by using a large double-scored
dataset which used the same scoring guidelines and
scoring process; using the 41K double-scored re-
sponses collected from AEST, we calculate the Pear-
son correlation coefficient to be 0.63, suggesting a
reasonable agreement. The distribution of scores for
this data can be found in Table 2.
4 Experiments
4.1 Overview
In this study, we developed vocabulary profile fea-
tures. From a reference corpus, we pre-compiled
182
Corpus
name
Purpose # of
speakers
# of re-
sponses
Native languages Size
(Hrs)
AEST bal-
anced data
Feature evaluation, Scor-
ing model training and
evaluation
480 2880 Korean (15%), Chinese (14%),
Japanese (7%), Spanish (9%),
Others (55%)
44
Table 1: Data size and speakers? native languages
Size Score1 Score2 Score3 Score4
Number
of files
141 1133 1266 340
(%) 5 40 45 12
Table 2: Distribution of proficiency scores in the dataset
multiple sets of vocabulary lists (e.g., a list of the
100 most frequent words in a reference corpus).
Next, for each test response, a transcription was gen-
erated using the speech recognizer. For each re-
sponse with respect to each reference word list, vo-
cabulary profile features were calculated. In addi-
tion to vocabulary profile features, type-token ratio
(TTR) was calculated as a baseline feature. Despite
its instability, TTR has been employed in the auto-
mated speech scoring systems such as (Zechner et
al., 2009), and its use here allows a direct compar-
ison of the performance of the features with the re-
sults of previous studies.
4.2 Vocabulary list generation
The three reference corpora we used in this study
are presented in Table 3: The General Service
List (GSL), the TOEFL 2000 Spoken and Written
Academic Language Corpus (T2K-SWAL) and the
AEST data.
Corpus Genre Tokens Types
GSL Written - 2,284
T2K-SWAL Spoken 1,869,346 28,855
AEST data Spoken 5,520,375 23,165
Table 3: Three reference corpora used in this study
GSL (West, 1953) comprises 2,284 words se-
lected to be of ?general service? to learners of En-
glish. In this study, we used the version with fre-
quency information from (Bauman, 1995). The orig-
inal version did not include word frequency and
was ?enhanced? by John Bauman and Brent Culli-
gan with the frequency information obtained from
the Brown Corpus, a collection of written texts.
T2K-SWAL (Biber et al, 2002) is a collection of
spoken and written texts covering a broad language
variety and use in the academic setting. In this study,
only its spoken texts were used. The spoken corpus
included manual transcriptions of discussions, con-
versations, and lectures that occurred in class ses-
sions, study-group meetings, office hours, and ser-
vice encounters.
Finally, AEST data is a collection of manual tran-
scriptions of spoken responses from the AEST for
non-native English speakers. Although there was no
overlap between AEST data and the evaluation data
(AEST balanced data), the vocabulary lists in AEST
data might be a closer match to the vocabulary lists
in the evaluation data since both of them come from
the same test products. From a content perspective,
this dataset is likely to better reflect characteristics
of non-native English speakers than the other two
reference corpora.
For T2K-SWAL and AEST, all transcriptions
were normalized; all the tokens were further de-
capitalized and removed of all non-alphanumeric
characters except for dash and quote. The morpho-
logical variants were considered as different words.
All words were sorted by the word occurrences in
the corpus, and a set of 6 lists were generated:
top-100 words (TOP1), word frequency ranks 101-
300 (TOP2), ranks 301-700 (TOP3), ranks 701-1500
(TOP4), ranks 1501-3000 (TOP5), and all other
words with ranks of 3001 and above (TOP6). For
GSL, a set of 5 lists was generated; TOP6 was
not generated since GSL only included about 2200
words.
Compared to written texts, speakers tended to use
a much smaller vocabulary in speech. For instance,
the percentage of words within the top-1000 words
on the total word types of AEST data responses was
over 90% on average, and they were similar across
183
proficiency levels. This is the reason why we sub-
classified the top 1000 words into three lists, unlike
the vocabulary profile features using top-1000 words
as one list like (Morris and Cobb, 2004), which did
not have any power to differentiate between profi-
ciency levels.
4.3 Transcription generation for evaluation
data
A Hidden Markov Model (HMM) speech recognizer
was trained on the AEST dataset, approximately
733 hours of non-native speech collected from 7872
speakers. A gender independent triphone acoustic
model and a combination of bigram, trigram, and
four-gram language models was used. The word
error rate (WER) on the held-out test dataset was
27%. For each response in the evaluation partition,
an ASR-based transcription was generated using the
speech recognizer.
4.4 Feature generation
Each response comprised less than 60 seconds of
speech with an average of 113 word tokens. Due
to the short response length, there was wide varia-
tion in the proportion of low-frequency word types
for the same speaker. In order to address this issue,
for each speaker, two responses from the same item-
type (IND/INT) were concatenated and used as one
large response. As a result, three concatenated re-
sponses (one IND response and two INT responses)
were generated for each speaker, yielding a total of
480 concatenated responses for IND items and 960
concatenated responses for INT items for our exper-
iment.
First, a list of word types was generated from
the ASR hypothesis of each concatenated response.
IND items provide only a one-sentence prompt,
while INT items provide stimuli including a prompt,
a reading passage, and a listening stimulus. In order
to minimize the influence of the vocabulary in the
stimuli on that of the speakers, we excluded the con-
tent words that occurred in the prompts or stimuli
from the word type list1.
1This process prevents to measure the content relevance;
whether the response is off-topic or not. However, this is not
problematic since the features in this study will be used in the
conjunction with the features that measure the accuracy of the
aspects of content and topicality such as (Xie et al, 2012)?s fea-
Table 4: List of features.
Feature # of Feature Description
features type
TTR 1 Ratio Type-token ratio
TOPn 5 or 6a Listrel Proportion of types
that occurred both
the response and
TOPn list in the to-
tal types of the re-
sponse.
aRank 1 Rank Avg. word rankb
aFreq 1 Freq Avg. word freq.c
lFreq 1 Freq Avg. log(word
freq)d
a For GSL, five different features were created using
TOP1-TOP5 lists, but TOP6 was not created. For
T2K-SWAL and AEST data, six different features were
created using TOP1-TOP6 lists separately.
b ?rank? is the ordinal number of words in a list that is sorted in
descending order of word frequency; words not present in the
reference corpus get the default rank of RefMaxRank+1.
c Avg. word frequency is the sum of the word-frequencies of
word types in the reference corpus divided by the total
number of words in the reference corpus; words not in the
reference corpus get assigned a default frequency of 1.
d Same as feature aFreq, but the logarithm of the word
frequency is taken here
Next, we generated five types of features using
three reference vocabulary lists. A maximum of 10
features were generated for each reference list. The
feature-types are tabulated in Table 4.
All features above were generated from word
types, not word tokens, i.e., multiple occurrences of
the same word in a response were only counted once.
Below we delineate the step-by-step process with
a sample response that leads to the feature genera-
tion outlined in Table 5.
? Step 1: Generate ASR hypothesis for the given
speech response. e.g: Every student has dif-
ferent perspective about how to relax. Playing
xbox.
? Step 2: Generate type list from ASR hypoth-
esis. For the response above we get the list
- about, how, different, xbox, to, relax, every,
perspective, student, has, playing.
tures.
184
word
freq. in
reference
corpus
word rank in
the reference
corpus
TOPn
about 25672 30 TOP1
how 8944 96 TOP1
has 18105 53 TOP1
to 218976 2 TOP1
different 5088 153 TOP2
every 2961 236 TOP2
playing 798 735 TOP4
perspec-
tive
139 1886 TOP5
xbox 1 20000 No
Table 5: An example of feature calculation.
? Step 3: Generate type list excluding words that
occurred in the prompt - about, how, different,
xbox, to, every, perspective, has, playing.
From the ASR hypotheses (result of Step 1), the
corresponding type list was generated (Step 2) and
two words (?student?, ?relax?) were excluded from
the final list due to overlap with the prompt. The
final word list used in the feature generation has 9
types (Step 3).
Next, for each word in the above type list, if it oc-
curs in the reference corpus (a list of words sorted
by frequency), its word frequency, word rank and
the TOPn information (whether the word belonged
to the TOPn list or not) are obtained. If it did not oc-
cur in the reference corpus, the default frequency (1)
and the default word rank (20000) were assigned. In
5, the default values were assigned for ?xbox? since
it was not in the reference corpus.
Finally, the average of the word frequencies and
the average of the the word ranks were calculated
(aFreq and aRank). For lFreq, the log value of each
frequency was calculated and then averaged. For
TOPn features, we obtain the proportion of the word
types that belong to the TOPn category. For the
above sample, the TOP1 feature value was 0.444
since 4 words belong to TOP1 and the total number
of word types was 9 (4/9=0.444).
5 Results
5.1 Correlation
We analyzed the relationship between the proposed
features and human proficiency scores to assess their
influence on predicting the proficiency score. The
reference proficiency score for a concatenated re-
sponse was estimated by summing up the two scores
of the constituent responses. Thus, the new score
scale was 2-8. Table 6 presents Pearson correlation
coefficients (r).
The best performing feature was aFreq followed
by TOP1. Both features showed statistically signif-
icant negative correlations with human proficiency
scores. TOP6 also showed statistically significant
correlation with human scores, but it was 10-20%
lower than TOP1. This suggests that a human rater
more likely assigned high scores when the vocabu-
lary of the response was not limited to a few most
frequent words. However, the use of difficult words
(low-frequency) shows a weaker relationship with
the proficiency scores.
Features based on AEST data outperformed fea-
tures based on T2K-SWAL or GSL. The correlation
of the AEST data-based aFreq feature was ?0.61
for the IND items and?0.51 for the INT items; they
were approximately 0.1 higher than the correlations
of T2K-SWAL or GSL-based features. A similar
tendency was found for the TOP1-TOP6 features,
although differences between AEST data-based fea-
tures and other reference-based features were less
salient overall.
For top-performing vocabulary profile features
including aFreq and TOP1, the correlations of
INT items were weaker than those of the IND items.
In general, the correlations of INT items were 10-
20% lower than those of the IND items in absolute
value.
aFreq and TOP1 consistently achieved better
performance than TTR across all item-types.
5.2 Scoring model building
To arrive at an automatic scoring model, we included
the new vocabulary profile features with other fea-
tures previously found to be useful in a multiple lin-
ear regression (MLR) framework. A total of 80 fea-
tures were generated by the automated speech pro-
ficiency scoring system from Zechner et al (2009),
185
Reference TTR TOP1 TOP2 TOP3 TOP4 TOP5 TOP6 aRank aFreq lFreq
IND GSL -.147 -.347 .027 .078 .000 .053 - .266 -.501 -.260
T2K-SWAL -.147 -.338 .085 .207 .055 .020 .168 .142 -.509 -.159
ATEST -.147 -.470 .014 .275 .172 .187 .218 .236 -.613 -.232
INT GSL -.245 -.255 -.086 -.019 -.068 -.031 - .316 -.404 -.318
T2K-SWAL -.245 -.225 .010 .094 .047 .079 .124 .087 -.405 -.198
ATEST -.245 -.345 -.092 .156 .135 .188 .194 .214 -.507 -.251
Table 6: Correlations between features and human proficiency scores
and they were classified into 5 sub-groups: fluency,
pronunciation, prosody, vocabulary complexity, and
grammar usage. For each sub-group, at least one
feature that correlated well with human scores but
had a low inter-correlation with other features was
selected. A total of following 6 features were se-
lected and used in the base model (base):
? wdpchk (fluency): Average chunk length in words;
a chunk is a segment whose boundaries are set by
long silences
? tpsecutt (fluency): Number of types per sec.
? normAM (pronunciation): Average acoustic model
score normalized by the speaking rate
? phn shift (pronunciation): Average absolute dis-
tance of the normalized vowel durations compared
to standard normalized vowel durations estimated
on a native speech corpus
? stretimdev (prosody): Mean deviation of distance
between stressed syllables in sec.
? lmscore (grammar): Average language model score
normalized by number of words
We first calculated correlations between these fea-
tures and human proficiency scores and compared
them with the most predictive vocabulary profile
features. Table 7 presents Pearson correlation co-
efficients (r) of these features.
In both item-types, the most correlated features
represented the aspect of fluency in production.
While tpsecutt was the best feature in IND items
and the correlation with human scores was approx-
imately 0.66, in INT items, wdpchk was the best
feature and the correlation was even higher, 0.73.
The performance of aFreq was particularly high
in IND items; it was the second best feature and only
marginally lower than the best feature (by 0.04).
aFreq also achieved promising performance in INT;
Features IND INT
wdpchk .538 .612
tpsecutt .659 .729
normAM .467 .429
phn shift -.503 -.535
stretimemdev -.442 -.397
lmscore .257 .312
aFreq -.613 -.507
TOP1 -.470 -.345
TTR -.147 -.245
Table 7: Comparison of feature-correlations with human-
assigned proficiency scores.
it was the fourth best feature. However, the perfor-
mance was considerably lower than the the best fea-
ture, and the difference between the best feature and
aFreq was approximately 22%.
We compared the performances of this base
model with an augmented model (base + TTR + all
vocabulary profile features) whose feature set was
the base augmented with our proposed measures of
vocabulary sophistication. Item-type specific multi-
ple linear regression models were trained using five-
fold cross validation. The 480 IND responses 960
INT responses were partitioned into five sets, sepa-
rately. In each fold, an item-type specific regression
model was trained using four of these partitions and
tested on the remaining one.
The averages of the five-fold models are sum-
marized in Table 8, showing weighted kappa to
indicate agreement between automatic scores and
human-assigned scores and also the Pearson?s cor-
relation (r) of the unrounded (un-rnd) and rounded
(rnd) scores with the human-assigned scores. We
used the correlation and weighted kappa as perfor-
mance evaluation measures to maintain the consis-
tency with the previous studies such as (Zechner
et al, 2009). In addition, the correlation metric
186
matches better with our goal to investigate the rela-
tionship between the predicted scores and the actual
scores rather than the difference between the pre-
dicted scores and the actual scores.
Features un-
rnd
corr.
rnd
corr.
weighted
kappa
IND base 0.66 0.62 0.55
base + TTR 0.66 0.63 0.56
base + TTR +
all
0.66 0.64 0.57
INT base 0.76 0.73 0.69
base + TTR 0.76 0.74 0.70
base + TTR +
all
0.77 0.74 0.70
Table 8: Performance of item-type specific multiple lin-
ear regression based scoring models.
The new scores show slightly better agreement
with human-assigned scores, but the improvement
was small in both item-types, approximately 1%.
6 Discussion
In general, we found that the test takers used a fairly
small number of vocabulary items in the spoken re-
sponses. On average, the total types used in the
responses was 87.21 for IND items and 98.52 for
INT items. Furthermore, the proportions of high
frequency words on test takers? spoken responses
were markedly high. The proportion of top-100
words was almost 50% and the proportion of top-
1500 words (summation of TOP1-TOP4) was over
89% on average. This means that only 1500 words
represent almost 90% of the active vocabulary of
the test takers in their spontaneous speech. Figure
1 presents the average TOP1-TOP6 features across
all proficiency levels.
The values of INT items were similar to IND
items, but the TOP3-TOP6 values were slightly
higher than IND items; INT items tended to include
more low frequency words. In order to investigate
the impact of the higher proportion of low frequency
words in INT items, we selected two features (aFreq
and TOP1) and further analyzed them.
Table 9 provides the mean of aFreq and TOP1 for
each score level. The features were generated using
AEST as a reference.
Figure 1: Proportion of top-N frequent words on average
Score aFreq TOP1
IND INT IND INT
2 43623 36175 .60 .52
3 38165 32493 .55 .49
4 33861 28884 .51 .48
5 30599 27118 .49 .46
6 28485 26327 .46 .45
7 27358 25093 .45 .43
8 26065 24711 .43 .43
Table 9: Mean of vocabulary profile features for each
score level
On average, the differences between adjacent
score levels in INT items were smaller than those
in IND items. The weaker distinction between score
levels may result in the lower performance of vo-
cabulary profile features in INT items. Particularly,
the differences were smaller in lower score levels (2-
4) than in higher score levels (5-8). The relatively
high proportion of low frequency words in the low
score level reduced the predictive power of vocabu-
lary profile features.
This difference between the item-types strongly
supports item-type-specific modeling. We combined
the IND and INT item responses and computed
a correlation between the features and the profi-
ciency scores over the entirety of data sets. De-
spite increase in sample sizes, the correlations were
lower than both the corresponding correlations of
the IND items and the INT items. For instance, the
correlation of the T2K-SWAL-based aFreq feature
was?0.393, and that of the AEST data-based aFreq
was?0.50, which was approximately 3% lower than
the INT items and 10% lower than the IND items.
The difference in the vocabulary distributions be-
tween the two item-types decreased the performance
187
of the features.
In this study, AEST data-based features outper-
formed T2K-SWAL-based features. Although no
items in the evaluation data overlapped with items
in AEST data, the similarity in the speakers? profi-
ciency levels and task types might have resulted in
a better match between the vocabulary and its dis-
tributions of AEST data with AEST balanced data,
finally the AEST data-based features achieved the
best performance.
In order to explore the degree to which AEST bal-
anced data (test responses) and the reference cor-
pora matched, we calculated the proportion of word
types that occurred in test responses and reference
corpora (the coverage of reference list). The ASR
hypotheses of AEST balanced data comprised 6,024
word types. GSL covered 73%, T2K-SWAL cov-
ered 99%, and AEST data covered close to 100%.
Considering the fact that, a) despite high coverage
of both T2K-SWAL and AEST data, T2K-SWAL-
based features achieved much lower performance
than AEST data, and, b) despite huge differences
in the coverage between T2K-SWAL and GSL, the
performance of features based on these reference
corpora were comparable, coverage was not likely
to have been a factor having a strong impact on the
performance. The large differences in the perfor-
mance of TOP1 across reference lists support the
possibility of the strong influence of high frequency
word types on proficiency; the kinds of word types
that were in the TOP1 bins were an important factor
that influenced the performance of vocabulary pro-
file features. Finally, genre differences (spoken texts
vs. written texts) in reference corpora did not have
strong impact on the predictive ability of the fea-
tures; the performance of features based on written
reference corpus (GSL) were comparable to those
based on a spoken reference corpus (T2K-SWAL).
Despite the high correlation shown by the indi-
vidual features (such as aFreq), we do not see a cor-
responding increase in the performance of the scor-
ing model with all the best performing features. The
most likely explanation to this is the small training
data size; in each fold, only about 380 responses for
IND and about 760 responses for INT were used
in the scoring model training. Another possibility
is overlap with the existing features; the aspect that
vocabulary profile features are modeling may be al-
ready covered to some extent in existing feature set.
In future research, we will further investigate this as-
pect in details.
7 Conclusions
In this study, we presented features that measure
ESL learners? vocabulary usage. In particular, we
focused on vocabulary sophistication, and explored
the suitability of vocabulary profile features to cap-
ture sophistication. From three different reference
corpora, the frequency of vocabulary items was cal-
culated which was then used to estimate the sophis-
tication of test takers? vocabulary. Among the three
different reference corpora, features based on AEST
data, a collections of responses similar to that of the
test set, showed the best performance. A total of 29
features were generated, and the average word fre-
quency (aFreq) achieved the best correlation with
human proficiency scores. In general, vocabulary
profile features showed strong correlations with hu-
man proficiency scores, but when used in an auto-
matic scoring model in combination with an existing
set of predictors of language proficiency, the aug-
mented feature set showed marginal improvement in
predicting human-assigned scores of proficiency.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e?rater R v.2. The Journal of Technology,
Learning, and Assessment, 4(3).
John Bauman. 1995. About the GSL. Retrieved
March 17, 2012 from http://jbauman.com/
gsl.html.
Jared Bernstein, Jian Cheng, and Masanori Suzuki. 2010.
Fluency and structural complexity as predictors of L2
oral proficiency. In Proceedings of InterSpeech 2010,
Tokyo, Japan, September.
Douglas Biber, Susan Conrad, Randi Reppen, Pat Byrd,
and Marie Helt. 2002. Speaking and writing in the
university: A multidimensional comparison. TESOL
Quarterly, 36:9?48.
Lei Chen and Su-Youn Yoon. 2011. Detecting structural
events for assessing non-native speech. In Proceed-
ings of the 6th Workshop on Innovative Use of NLP for
Building Educational Applications, pages 38?45.
Miao Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
188
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics 2011, pages 722?731.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learners?
fluency: Comparisons between read and spontaneous
speech. The Journal of the Acoustical Society of Amer-
ica, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learners?
fluency: Comparisons between read and spontaneous
speech. The Journal of the Acoustical Society of Amer-
ica, 111(6):2862?2873.
Helmut Daller, Roeland van Hout, and Jeanine Treffers-
Daller. 2003. Lexical richness in the spontaneous
speech of bilinguals. Applied Linguistics, 24(2):197?
222.
Horacio Franco, Leonardo Neumeyer, Yoon Kim, and
Orith Ronen. 1997. Automatic pronunciation scoring
for language instruction. In Proceedings of ICASSP
97, pages 1471?1474.
Judit Kormos and Mariann Denes. 2004. Exploring mea-
sures and perceptions of fluency in the speech of sec-
ond language learners. System, 32:145?164.
Batia Laufer and Paul Nation. 1995. Vocabulary size and
use: lexical richness in L2 written production. Applied
Linguistics, 16:307?322.
Xiaofei Lu. 2011. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Language Journal.
David D. Malvern and Brian J. Richards. 1997. A
new measure of lexical diversity. In Evolving mod-
els of language: Papers from the Annual Meeting of
the British Association of Applied Linguists held at the
University of Wales, Swansea, September, pages 58?
71.
Paul Meara and Huw Bell. 2003. P lex: A simple and
effective way of describing the lexical characteristics
of short L2 texts. Applied Linguistics, 24(2):197?222.
Lori Morris and Tom Cobb. 2004. Vocabulary profiles
as predictors of the academic performance of teaching
english as a second language trainees. System, 32:75?
87.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, pages 88?93.
Anne Vermeer. 2000. Coming to grips with lexical rich-
ness in spontaneous speech data. Language Testing,
17(1):65?83.
Michael West. 1953. A General Service List of English
Words. Longman, London.
Silke Witt and Steve Young. 1997. Performance mea-
sures for phone-level pronunciation teaching in CALL.
In Proceedings of the Workshop on Speech Technology
in Language Learning, pages 99?102.
Silke Witt. 1999. Use of the speech recognition in
computer-assisted language learning. Unpublished
dissertation, Cambridge University Engineering de-
partment, Cambridge, U.K.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech scor-
ing. In Proceedings of the NAACL-HLT, Montreal,
July.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51:883?895, October.
Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011. Eval-
uating prosodic features for automated scoring of non-
native read speech. In IEEE Workshop on Automatic
Speech Recognition and Understanding 2011, Hawaii,
December.
189
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 55?59,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Predicting Attrition Along the Way: The UIUC Model
Bussaba Amnueypornsakul, Suma Bhat and Phakpoom Chinprutthiwong
University of Illinois,
Urbana-Champaign, USA
{amnueyp1,spbhat2,chinpru2}@illinois.edu
Abstract
Discussion forum and clickstream are two primary
data streams that enable mining of student behav-
ior in a massively open online course. A student?s
participation in the discussion forum gives direct
access to the opinions and concerns of the student.
However, the low participation (5-10%) in discus-
sion forums, prompts the modeling of user behav-
ior based on clickstream information. Here we
study a predictive model for learner attrition on a
given week using information mined just from the
clickstream. Features that are related to the quiz
attempt/submission and those that capture inter-
action with various course components are found
to be reasonable predictors of attrition in a given
week.
1 Introduction
As an emerging area that promises new hori-
zons in the landscape resulting from the merger
of technology and pedagogy massively open on-
line courses (MOOCs) offer unprecedented av-
enues for analyzing many aspects of learning at
scales not imagine before. The concept though in
its incipient stages offers a fertile ground for an-
alyzing learner characteristics that span demogra-
phies, learning styles, and motivating factors. At
the same time, their asynchronous and impersonal
approach to learning and teaching, gives rise to
several challenges, one of which is student reten-
tion.
In the absence of a personal communication be-
tween the teacher and the student in such a sce-
nario, it becomes imperative to be able to under-
stand class dynamics based on the course logs that
are available. This serves the efforts of the in-
structor to better attend to the needs of the class
at large. One such analysis is to be able to predict
if a student will drop out or continue his/her par-
ticipation in the course which is the shared task of
the EMNLP 2014 Workshop on Modeling Large
Scale Social Interaction in Massively Open Online
Courses (Rose and Siemens, 2014).
Our approach is to model student attrition as be-
ing a function of interaction with various course
components.
2 Related Works
The task of predicting student behavior has been
the topic of several recent studies. In this context
course logs have been analyzed with an effort to
predict students? behavior. The available studies
can be classified based on the type of course data
that has been used for the analysis as those us-
ing discussion forum data and those using click-
stream data.
Studies using only discussion forum to under-
stand user-behavior rely only on available discus-
sion forum posts as their source of information. In
this context, in (Ros?e et al., 2014) it was observed
that students? forum activity in the first week can
reasonably predict the likelihood of users drop-
ping out. Taking a sentiment analysis approach,
Wen et al. (Wen et al., 2014b) observed a corre-
lation between user sentiments expressed via fo-
rum posts and their chance of dropping out. Mo-
tivation being a crucial aspect for a successful on-
line learning experience, (Wen et al., 2014a) em-
ploys computational linguistic models to measure
learner motivation and cognitive engagement from
the text of forum posts and observe that participa-
tion in discussion forums is a strong indicator of
student commitment.
Even though discussion forum serves as a rich
source of information that offers insights into
many aspects of student behavior, it has been ob-
served that a very small percentage of students
(5-10%) actually participate in the discussion fo-
rum. As an alternate data trace of student inter-
action with the course material, the clickstream
55
data of users contains a wider range of informa-
tion affording other perspectives of student behav-
ior. This is the theme of studies such as (Guo
and Reinecke, 2014), which is focused on the nav-
igation behavior of various demographic groups,
(Kizilcec et al., 2013) which seeks to understand
how students engage with the course, (Ramesh
et al., 2014), that attempts to understand student
disengagement and their learning patterns towards
minimizing dropout rate and (Stephens-Martinez
et al., 2014) which seeks to model motivations of
users by mining clickstream data.
In this study, the task is to predict if a user
will stay in the course or drop out using infor-
mation from forum posts and clickstream infor-
mation. Our approach is to use only clickstream
information and is motivated by key insights such
as interaction with the various course components
and quiz attempt/submission.
3 Data
Data from one MOOC with approximately 30K
students was distributed as training data. This
included discussion post information and click-
stream information of the students with com-
pletely anonymized user ids. Of this a subset of
6583 users was considered the held-out dataset on
which we report the performance of the model.
3.1 Preprocessing Stage
Since participants (posters) in the discussion fo-
rum constitute a very small minority of the users
in a course (between 5-10% as observed in prior
studies), we mine the clickstream information for
course-interaction. From the clickstream we ex-
tract the following information to indicate involve-
ment in the course.
? Total watch time: From the video view infor-
mation the amount of time watched is calcu-
lated by taking the summation of the differ-
ence between the time of the last event a user
interacts with a video and the initial time a
user starts the same video. If a user is idle
for longer than 50 minutes, we add the differ-
ence between the current time before the user
goes idle and the time the user initially inter-
acts with the video to the total time. The new
initial time will be after the user goes active
again. Then we repeat the process until there
is no more viewing action in the clickstream
for that user.
? Number of quiz attempts;
? Number of quiz submissions;
? Number of times a user visits the discussion
forum;
? Number of times a user posts: The number
of times a user posts in a forum is counted.
This count includes whether the user starts a
thread, posts, or comments.
? Action sequence: We define an action se-
quence of a given user as being the sequence
of course-related activity in a given week for
a given user. It captures the user?s interac-
tion with the various components of a course
in chronological order, such as seeking infor-
mation on the course-wiki, watching a lecture
video, posting in the discussion forum. The
activities are, p = forum post, a = quiz at-
tempt, s = quiz submit, l = lecture page view,
d = lecture download, f = forum view, w =
wiki page visited, t = learning tool page vis-
ited, o = play video. As an example, the ac-
tion sequence of a user wwaaws in a given
week indicates that the user began the course-
activity with a visit to the course wiki, fol-
lowed by another visit to the wiki, then at-
tempted the quiz two successive times and fi-
nally submitted the quiz.
Each of the items listed above, captures impor-
tant aspects of interaction with the course serv-
ing as an index of attrition; the more a user inter-
acts with the course in a given week, the less the
chances are of dropping out in that week.
21%	 ?
32%	 ?
47%	 ? Drop	 ?Inac?ve	 ?
Ac?ve	 ?
Figure 1: Percentage of each type of users
56
An exploratory analysis of the data reveals that
there are three classes of users based on their in-
teraction with the course components as revealed
by the clickstream activity. More specifically, with
respect to the length of their action sequence, the
3 classes are:
1. Active: This class is the majority class rep-
resented by 47% of the users in the course.
The users actively interact with more than
one component of the course and their enroll-
ment status shows that they did not drop.
2. Drop: This is the class represented by a rel-
ative minority of the users (21%). The users
hardly interact with the course and from their
enrollment status they have dropped.
3. Inactive: This class of students, represented
by 32% of the course, shares commonalities
with the first two classes. Whereas their en-
rollment status indicates that they have not
dropped (similar to the Active group), their
clickstream information shows that their level
of course activity is similar to that of the
Drop class (as evidenced by the length of
their action sequence. We define a user to be
inactive if the action sequence is less than 2
and the user is still enrolled in the course.
The distribution of the three classes of users in the
training data is shown in Figure 1. This key ob-
servation of the presence of three classes of users
prompts us to consider three models to predict user
attrition on any given week since we only predict
whether a user dropped or not.
1. Mode 1 (Mod1): Inactive users are modeled
to be users that dropped because of their sim-
ilar activity pattern;
2. Mode 2 (Mod2): Inactive users are modeled
as Active users because they did not formally
drop out;
3. Mode 3 (Mod3): Inactive users are modeled
as Drop with a probability of 0.5 and Active
with a probability of 0.5. This is because they
share status attributes with Active and inter-
action attributes with Drop.
4 Features
We use two classes of features to represent user-
behavior in a course and summarize them as fol-
lows.
? Quiz related: The features in this class are:
whether a user submitted the quiz (binary),
whether a user attempted the quiz (binary),
whether a user attempted but did not submit
the quiz (binary). The intuition behind this
set of features is that in general quiz-related
activity denotes a more committed student
with a higher level of involvement with the
course. This set is also intended to capture
three levels of commitment, ranging from
only an attempt at the lowest level, attempt-
ing but not submitting at a medium level, to
submitting the quiz being the highest level.
? Activity related: The features in this category
are derived from the action sequence of the
user during that week and they are:
1. Length of the action sequence (nu-
meric);
2. The number of times each activity (p, a,
s, l, d, f, w, o, or t) occurred (numeric);
3. The number of wiki page visits/length of
the action sequence (numeric).
The features essentially capture the degree of
involvement as a whole and the extent of in-
teraction with each component.
5 Experiments
5.1 Models
We consider two input data distributions of the
training data: a) a specific case, where the inac-
tive users are excluded. In this case, the model is
trained only on users that are either active or those
that have dropped. b) a general case, where the
inactive users are included as is. In both cases, the
testing data has the inactive users included, but are
either modeled as Mode 1, 2 or 3. This results in
6 models {specific, general} x {Mode1, Mode2,
Mode3}.
We train an SVM for each model and ob-
serve that an rbf kernel achieves the best accuracy
among the kernel choices. We use the scikit imple-
mentation of SVM (Pedregosa et al., 2011). The
parameter ? was tuned to maximize accuracy via 5
fold cross validation on the entire training set. We
observe that the performance of Mode 3 was much
lower than that of Modes 1 and 2 and thus exclude
it from the results.
The tuned models were finally evaluated for ac-
curacy, precision, recall, F-measure and Cohen?s
57
? on the held-out dataset.
5.2 Experimental Results
Mode 1 Mode 2
Specific General Specific General
Baseline 46.42% 46.42% 78.66% 78.66%
Accuracy 91.31% 85.34% 78.48% 78.56%
Table 1: Accuracy of the models after parameter
tuning.
We compare the accuracy of the tuned mod-
els with a simple baseline which classifies a user,
who, during a given week, submits the quiz and
has an action sequence length more than 1 as
one who will not drop. The baseline accuracy is
46.42% for Mode 1 and 78.66% for Mode 2. We
observe that modeling the inactive user as one who
drops performs significantly better than the base-
line, whereas modeling the inactive user as one
who stays, does not improve the baseline. This
is summarized in Table 1.
Of these models we chose two of the best per-
forming models and evaluate them on the held-
out data. The chosen models were: Model 1 =
(specific,Mode1) and Model 2 = (general,Mode2).
The resulting tuned Model 1 (inactive = drop) had
? = 0.1 and Model 2 (inactive = stay) had a
? = 0.3 and C as the default value.
Model 1 Model 2
Accuracy 50.98% 80.40%
Cohen?s Kappa -0.06 0.065
P 0.167 0.482
R 0.371 0.058
F 0.228 0.104
Table 2: Accuracy, Cohen?s kappa, Precision (P),
Recall (R) and F-measure (F) scores for the mod-
els on the held-out data.
The performance (accuracy, Cohen?s ?, preci-
sion, recall and F-measure scores of the two mod-
els on the held-out data are shown in Table 2. The
final model submitted for evaluation on the test set
is Model 2. It was more general since its training
data included the inactive users as well. However,
the skew in the data distribution is even larger for
this model.
We highlight some important observations
based on the result.
? Model 2, which is trained to be more gen-
eral and has the inactive users included, but
operates in Mode 2 (regards inactive users
as active) has a better accuracy compared to
Model 1, which is trained by excluding the
inactive users, but operates in Mode 1 (re-
gards inactive users as drop).
? In terms of the ? score, Model 2 shows some
agreement, but Model 1 shows no agreement.
? The increased accuracy of Model 2 comes at
the expense of reduced recall. This suggests
that Model 2 has more false negatives com-
pared to Model 1 on the held-out set.
? Even with reduced recall, Model 2 is more
precise than Model 1. This implies that
Model 1 tends to infer a larger fraction of
false positives compared to Model 2.
6 Discussion
6.1 Data Imbalance
The impact of class imbalance on the SVM clas-
sifier is well-known to result in the majority class
being well represented compared to the minority
class (Longadge and Dongre, 2013). In our mod-
eling with different input data distributions as in
the specific case (Model 1), where we exclude in-
active users, the data imbalance could have signif-
icantly affected the performance. This is because,
the class of active users is more than double the
size of the class of users who dropped.
Our attempt to counter the effect of the minor-
ity class by oversampling, resulted in no improve-
ment in performance. In future explorations, other
efforts to counter the data imbalance may be help-
ful.
6.2 Parameter tuning
The models studied here were tuned to maximize
accuracy. In the future, models that are tuned to
maximize Cohen?s ? may be worth exploring.
6.3 Ablation Analysis
Quiz Related Activity Related
Model 1 80.48% 50.95%
Model 2 80.48% 80.41%
Table 3: Accuracy and kappa scores for the mod-
els by removing the corresponding set of features.
Table 3 summarizes the results of the ablation
study conducted for each model by removing each
class of features. For Model 1, the activity-related
features constitute the most important set of fea-
tures as seen by the drop in accuracy resulting
from its omission. For Model 2, however, both
sets of features have nearly the same effect.
58
References
Philip J. Guo and Katharina Reinecke. 2014. Demo-
graphic differences in how students navigate through
moocs. In Proceedings of the First ACM Conference
on Learning @ Scale Conference, L@S ?14, pages
21?30, New York, NY, USA. ACM.
Ren?e F. Kizilcec, Chris Piech, and Emily Schnei-
der. 2013. Deconstructing disengagement: Analyz-
ing learner subpopulations in massive open online
courses. In Proceedings of the Third International
Conference on Learning Analytics and Knowledge,
LAK ?13, pages 170?179, New York, NY, USA.
ACM.
R. Longadge and S. Dongre. 2013. Class Imbalance
Problem in Data Mining Review. ArXiv e-prints,
May.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Arti Ramesh, Dan Goldwasser, Bert Huang, Hal
Daume III, and Lise Getoor. 2014. Uncovering hid-
den engagement patterns for predicting learner per-
formance in moocs. In ACM Conference on Learn-
ing at Scale, Annual Conference Series. ACM, ACM
Press.
Carolyn Rose and George Siemens. 2014. Shared task
on prediction of dropout over time in massively open
online courses. In Proceedings of the 2014 Empiri-
cal Methods in Natural Language Processing Work-
shop on Modeling Large Scale Social Interaction in
Massively Open Online Courses.
Carolyn Penstein Ros?e, Ryan Carlson, Diyi Yang,
Miaomiao Wen, Lauren Resnick, Pam Goldman,
and Jennifer Sherer. 2014. Social factors that con-
tribute to attrition in moocs. In Proceedings of the
First ACM Conference on Learning @ Scale Con-
ference, L@S ?14, pages 197?198, New York, NY,
USA. ACM.
Kristin Stephens-Martinez, Marti A. Hearst, and Ar-
mando Fox. 2014. Monitoring moocs: Which infor-
mation sources do instructors value? In Proceedings
of the First ACM Conference on Learning @ Scale
Conference, L@S ?14, pages 79?88, New York, NY,
USA. ACM.
Miaomiao Wen, Diyi Yang, and Carolyn Penstein
Ros?e. 2014a. Linguistic reflections of student
engagement in massive open online courses. In
ICWSM.
Miaomiao Wen, Diyi Yang, and Carolyn Penstein
Ros?e. 2014b. Sentiment analysis in mooc discus-
sion forums: What does it tell us? In the 7th Inter-
national Conference on Educational Data Mining.
59
