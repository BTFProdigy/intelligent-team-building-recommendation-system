Automated Generalization of Phrasal Paraphrases from the Web*
Weigang Li 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
lee@ir.hit.edu
.cn
Ting Liu 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
tliu@ir.hit.ed
u.cn
Yu Zhang
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
zhangyu@ir.hit
.edu.cn
Sheng Li 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
lis@ir.hit.edu
.cn
Wei He 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
truman@ir.hit.
edu.cn
Abstract
Rather than creating and storing thou-
sands of paraphrase examples, para-
phrase templates have strong 
representation capacity and can be used
to generate many paraphrase examples.
This paper describes a new template
representation and generalization
method. Combing a semantic diction-
ary, it uses multiple semantic codes to
represent a paraphrase template. Using
an existing search engine to extend the
word clusters and generalize the exam-
ples.  We also design three metrics to
measure our generalized templates. The 
experimental results show that the rep-
resentation method is reasonable and 
the generalized templates have a higher 
precision and coverage.
1 Introduction
Paraphrases are alternative ways to convey the 
same information (Barzilay and McKeown,
2001) and they have been applied in many fields
of natural language processing. There are many
previous work on paraphrase examples extrac-
tion or combining them with some applications
such as information retrieval and question an-
swering (Agichtein et al, 2001; Florence et al, 
2003; Rinaldi et al, 2003; Tomuro, 2003; Lin
and Pantel, 2001;), information extraction 
(Shinyama et al, 2002; Shinyama and Sekine, 
2003), machine translation (Hiroshi et al, 2003;
Zhang and Yamamoto, 2003), multi-document
(Barzilay et al, 2003).
There is also some other research about 
paraphrase. (Wu and Zhou, 2003) just extract 
the synonymy collocation, such as <turn on, 
OBJ, light> and <switch on, OBJ, light> using
both monolingual corpora and bilingual corpora 
to get an optimal result, but do not generalize
them. (Glickman and Dagan, 2003) detects verb
paraphrases instances within a single corpus
without relying on any priori structure and in-
formation. Generation of paraphrase examples
was also investigated (Barzilay and Lee, 2003;
Quirk et al, 2004).
Rather than creating and storing thousands of 
paraphrases, paraphrase templates have strong 
representation capacity and can be used to gen-
erate many paraphrase examples. As (Hirst, 
2003) said, for each aspect of paraphrase there 
are two main challenges: representation of 
knowledge and acquisition of knowledge. Cor-
responding to the problem of generalization of 
paraphrase templates, there are also two prob-
lems: the first is the representation of paraphrase
templates and the second is acquisition of para-
phrase templates.
There are several methods about paraphrase
templates representation. The first method is 
using the Part-of-Speech (Barzilay and McKe-
own, 2001; Daum? and Marcu, 2003; Zhang and 
Yamamoto, 2003), the second uses name entity 
as the variable (Shinyama et al, 2002; Shinyama
and Sekine, 2003), the third method is similar to 
the second method which is called the inference 
rules extraction (Lin and Pantel, 2001).
A paraphrases template is a pair of natural
language phrases with variables standing in for
certain grammatical constructs in (Daum? and 
*: Supported by the Key Project of National Natural Sci-
ence Foundation of China under Grant No. 60435020
49
Marcu, 2003). He used Part-of-Speech to repre-
sent templates. But for some cases, the POS will 
be very limited and for some other cases will be 
over generalized. For example:
?????????
(In my view/mind ----I feel)
The above pair of phrases is a paraphrase, it 
can be generalized using POS information: 
? [pronoun]??
(In [pronoun] view/mind)
[pronoun]??
( [pronoun] feel)
But for this template many noun words will
be excluded. From this point of view, the tem-
plate representation capacity is limited. But for 
other examples, the POS information will be 
over generally. For example:
?????????
(What's the price for the apples?)
????????
(How much is the apples per Jin?)
Here, we just generalize one variable ????.
Then, the template becomes:
[noun]???????
(What's the price for the [noun]?)
[noun]??????
(How much is the [noun] per Jin?)
If there is a sentence ??????????
(What's the price for the notebook?)?, its? para-
phrase will be ?????????(How much 
is the notebook per Jin?)? according to this tem-
plate. Obviously, the result is unreasonable.
(Shinyama et al, 2002) tried to find para-
phrases assuming that two sentences sharing
many Named Entities and a similar structure are 
likely to be paraphrases of each other. But just 
name entities are limited, too. And (Lin and 
Pantel, 2001) present an unsupervised algorithm 
for discovering inference rules from text such as 
?X writes Y? and ?X is the author of Y?. This 
generalized method has good ability. But it also
has some limited aspect. For example:
[Jack] writes [his homework].
According to the paraphrase template, the
target sentence will be transformed into ?[Jack]
is the author of [his homework]?. It?s obviously
that the generated sentence is not standard.
So how to represent paraphrase templates
and generalize the paraphrase examples is a very 
interesting task. In this paper, we present a novel
approach to represent paraphrase template with 
semantic code of words and using an existing
search engine to get the paraphrase template.
The remainder of this paper is organized as 
follows. In the next section, we give the over-
view of our method. In section 3, we define the 
representation method in details. Section 4 pre-
sents the generalization method. Some experi-
ments and discussions are shown in Section 5. 
Finally, we draw a conclusion of this method
and give some suggestions about future work. 
2 Overview of Generalization Method
The origin input of our system is a seed phrasal
paraphrase example. And the output is the gen-
eralized paraphrase templates from the given 
examples. The overall architecture of our para-
phrase generalization is represented on figure 1. 
A seed phrasal
paraphrase examples
Getting the slot word
Extend the slot word
using Search Engine
on every example
Mapping two word
sets to their semantic
code sets
Intersection operation
on the two semantic
code sets
Generalizing a
template
Figure 1: Sketch Map of Paraphrase example
Generalization
We also use the example (1) to illustrate the 
representation. Here a semantic dictionary called 
?TongYiCiCiLin? (Extension Version)1 is used. 
The pair of phrases is a phrasal paraphrase. At
first, after preprocessing which includes word
segment, POS tagging and word sense disam-
biguation, we get the slot word in the paraphrase.
In this example, the slot word is ??(I)?. Then
we search the web using the context of the slot 
word. Every phrase in the phrasal pair derives a
set of sentences which include the original 
phrase context. A dependency parser on these 
sentences is used to extract the corresponding
word with the slot word. Two word sets can be 
obtained through the two sentence sets. Then,
we map word sets to their semantic code sets
1 TongYiCiCiLin (Extended Version) can be downloaded
from the website of HIT-IRLab (Http://ir.hit.edu.cn). In the 
past section, we abbreviate the TongYiCiCiLin (Extended
Version) to Cilin (EV) 
50
according to Cilin(EV). Then an intersection 
operation is conducted on the two sets. We use 
the intersection set to replace the slot word and 
generate the final paraphrase template. 
In order to verify the validation of the gener-
alized paraphrase template, we also design an 
automatic algorithm to confirm whether the 
template is reasonable using the existing search 
engine.
3 Representation of Template 
In the section of introduction, some representa-
tion methods of paraphrase template have been 
introduced. And we proposed a new method us-
ing word semantic codes to represent the vari-
able in a template. Before we introduce the 
representation method, Firstly, we give some 
general introduction about the semantic diction-
ary of Cilin(EV). 
3.1 TongYiCiCiLin (Extended Version) 
Cilin (EV) is derived from original TongY-
iCiCilin in which word senses are decomposed 
to 12 large categories, 94 middle categories, 
1,428 small categories. Cilin (EV) removes 
some outdated words and updates many new 
words. More fine-grained categories are added 
on the base of original classification system to 
satisfy the more complex natural language ap-
plications. The encoding criterion is shown in 
the table 1:
Table 1 Encoding table of dictionary
Encoding
bit 1 2 3 4 5 6 7 8
Example D a 1 5 B 0 2 =
Attribute Big Middle Small groups Atom groups
Layer 1 2 3 4 5
The encoding bits are arranged from left to 
right. The first three layers are same with Cilin. 
The fourth layer is represented by capital letters 
and the fifth layer is two-bit decimal digit. The 
last bit is some more detailed information about 
the atom groups. 
3.2 An Example of a Paraphrase Template 
For simplicity, we just select one slot word in 
every paraphrase. And we stipulated that only 
content word can be slot word. We also use the 
above paraphrase example (1). 
?????????
(In my view/mind ----I feel)
Here, we get the slot word ??(I)?. Through 
the Word Sense Disambiguation processing, we 
get its semantic code ?Aa02A01=? according to 
the fifth layer in Cilin(EV). If we just use the 
semantic code of the slot word, we can get a 
simple paraphrase template as follows:  
? [Aa02A01=] ??
(In [Aa02A01=]  view/mind)
[Aa02A01=] ??
([Aa02A01=]  feel)
But it is obviously that the template is very 
limited. Its? representation ability is also limited. 
So how to extend the ability of a paraphrase 
template is a challenging work.  
3.3 Extending the Template Abstract Ability 
According to the feature of Cilin(EV) architec-
ture, we can use the higher layer?s semantic 
code instead of the slot word to generalize the 
paraphrase template naturally. Of course it?s a 
very simple method to extend the template abil-
ity, but it also brings more redundancy of a 
paraphrase template and it will be proven in the 
later section. 
So we use multiple semantic codes of the dif-
ferent layer instead of only one semantic code of 
slot word in Cilin (EV). The later experimental 
results prove this representation has a good per-
formance with a good precision and coverage. 
4 Generalizing to Templates 
As mentioned above, we can use multiple se-
mantic codes to generalize paraphrase examples. 
So the problem of how to generalize paraphrase 
examples is transformed into the problem of 
how to get the multiple semantic codes set. We 
proposed a new method which uses the existing 
search engine to reach the target.  
4.1 Getting the Candidate Sentences 
After we removed the slot word in the para-
phrase examples, two phrasal contexts of the 
original paraphrase phrases were obtained. Each 
phrase without slot word is used as a search 
query for an existing search engine and achiev-
ing many sentences which include the query 
word. For this example, the two queries are ??
??(in?view)? and ???(feel)?. Each query 
gets one sentence set respectively. Part of the 
two result sentence sets are shown in figure 2 
and figure 3: 
51
Figure 2. Sentence Set 1 
Figure 3. Sentence Set 2 
From the above two sentence sets, we can
find that there is some noisy information in the 
sentences. In order to extend the correspondent
words of the slot word, it is not enough that we 
just use the position information or POS tagging
information of the slot word. Even if we extract
these words, many of them can?t be found in the
dictionary because they are not simple words.
Benefiting from the idea of (Lin and Pantel, 
2001), we use a dependency parser to determine
the correspondent extended words. 
4.2 Dependency Parser 
In this paper, we use a dependency parser (Ma et 
al., 2004) to extract the candidate slot word. For 
example, the dependency parsing result of the 
phrase of ?????? is shown in figure 4. 
Figure 4. Dependency parsing result 
The arcs in the figure represent dependency
relationships. The direction of an arc is from the 
head to the modifier in the relationship. Labels 
associated with the arcs represent types of de-
pendency relations. Table 2 lists a subset of the
dependency relations in the HIT-IRLab depend-
ency parser2.
Table 2. A subset of the dependency relations 
Relation Description
ATT ????(attribute)
HED ??(head)
SBJ ??(subject)
ADV ????(adverbial)
VOB ????(verb-object)
???????????????????
??????????????????
??????????????
,7 ????????????"
??????????????????
2 More information about the dependency parser can be got
from http://ir.hit.edu.cn/cuphelp.htm
4.3 Extracting the extended words 
We just use a very simple method to get the ex-
tended words from the parsed sentences. At first, 
we record the relations of the original parsed 
phrasal examples. And then we use these rela-
tions to matched similar part in the candidate 
parsed sentence except slot word. And we omit
these unseen relations and content words which
don?t appear in the original parsed phrasal ex-
amples. Then we can get the extended words. 
????????????
?????????
???????????????
??????????
???????????B720 ????
Figure 5. Dependency parsing result 
Figure 5 shows the dependency parsing result 
of the phrase of ???????????(In for-
eign capital fund manager view). We can easily
find that the extended word of the slot word
?? ?(I) is ??? ?(manager). Two extended
word sets can be extracted from two sentence
sets. Then we map each word to their semantic
code to get two semantic code sets. Intersection
operation is conducted on these two semantic
code sets to obtain their intersection set. Finally, 
we use the semantic code set instead of the slot 
word to generate the paraphrase template.
4.4 Some tricks 
Because the precision of the current dependency
parser on Chinese is not very high, we just ex-
tract a part of the candidate sentences to parse. 
There are three patterns to segment the long 
candidate sentences according to position of slot 
word in paraphrase examples. They are called
FRONT, MIDDLE and BACK. Here we use an
example to illustrate it as shown in table 3: 
Table 3 Examples of sentence segmentation
Pattern Origin Phrase Segment examples
FRONT (SW)?? ????????
?????
MIDDLE ?(SW)?? ????????
????????
????
The bold section in the sentence will be ex-
tracted to parse. Pattern type can be decided by 
52
the position relation between slot word and con-
text words. And these patterns can reduce the 
relative error rate of the dependency parser. That 
is to say, if the original phrase is parsed wrongly, 
the extracted segments may be parsed wrongly 
with the similar error. But according to our 
method, this kind of parser error has little influ-
ence on the final extracting result. 
5 Experiments and Discussions 
5.1 Setting 
We extract about 510 valid paraphrase examples 
from a Chinese paraphrase corpus (Li et al, 
2004). For simplicity, we just select those 
phrasal paraphrase examples which own same 
word. And we stipulate only content word can 
be as slot word. We just use four seed phrasal 
paraphrases as the original paraphrases in this 
paper. And the generalized paraphrase templates 
represented by semantic codes of the fifth layer 
in Cinlin (EV) are also shown in the Table 4: 
Table 4: Examples of the generalized template 
Origin 
Phrases 
Generalized Paraphrase  
templates 
??? [Aa01A01=,Aa01A05=,   
Aa01C03=,Aa02A01=,  ?]??
1 ? ? ?
?
?[Aa01A01=,Aa01A05=, 
  Aa01C03=,Aa02A01=,...  ]??
??? ? [Ac03A01=,Ah04A01=, 
Ah05A01=,Am03D01@,?]?2 ??? [Ac03A01=,Ah04A01=, 
Ah05A01=,Am03D01@,?]??
? ? ?
?
?[Fb01A01=,Gb07B01=, 
Hb06A01=,He15B01=,? ]??
3 ? ? ?
?
?[Fb01A01=,Gb07B01=, 
Hb06A01=,He15B01=,?  ]??
? ? ?
? ? ?
??
[Aa03A01=,Ac03A01=, 
Ba05A10#,Bb02A01=,?]???
???4
? ? ?
???
[Aa03A01=,Ac03A01=,Ba05A10#,
Bb02A01=,?]?????
5.2 Evaluation on Templates 
The goal of the evaluations is to confirm how 
reasonable this kind of representation method of 
paraphrase templates is and how well the tem-
plate is. We evaluated the generalized para-
phrase template in three ways. They are listed in 
the following three categories: 1) Reasonability; 
2) Precision; 3) Coverage. 
1) Reasonability 
The reasonability of a paraphrase template aims 
to measure the reasonable extent of the presenta-
tion method with multiple semantic codes. For 
example, if we use POS to generalize a para-
phrase template, its reasonability is very lower; 
that is to say, POS is not suitable to represent 
paraphrase template in some extent.  
We use an existing search engine to calcu-
late the reasonability of every paraphrase tem-
plate. Firstly, we instantiate all paraphrase 
examples from a template. Then all these exam-
ples are as the queries of the search engine. If 
two phrases in one paraphrase can be matched 
completely from the search engine, it also means 
that one or more examples are found on the Web 
via search engine, we then consider this para-
phrase is reasonable. Using this method we can 
get the approximate evaluation of all the exam-
ples. We define two metrics: 
Strict_Reasonability = S / N 
Loose_Reasonability = (L + S) / N 
Where N is the total number of the instanti-
ated examples; S is the number of the para-
phrase examples which two phrases in it can be 
matched all; L is the number of paraphrase ex-
amples only one phrase in a paraphrase can be 
matched.
2) Precision 
Every template is correspondent to the examples 
number with the semantic code of different layer 
in Cilin (EV) as shown in table 5.  
Table 5 Templates and their correspond exam-
ples number 
Instantiated examples 
number
Number of 
Paraphrase
templates Cilin3 Cilin4 Cilin5
1 2696 1815 478
2 13032 6354 3011
3 1057 587 177
4 3004 2229 429
From the above table, we can find that every 
template can instantiate many examples. If 
manually judging all of these examples will 
spend plenty of time. So we just sample part of 
all instantiate examples, 200 paraphrase exam-
ples for each template in this paper. For each 
53
phrase in a sample paraphrase example, it is as 
search query to get the first two matched sen-
tences. Evaluators would be asked whether it is
semantically okay to replace the query in the
sentence by the correspondent phrase in a para-
phrase. They were given only two options: Yes
or No. If search query have no matched results, 
we consider that this phrase cannot be replace 
with its correspondent paraphrase. According to 
the above regulations, we know that every para-
phrase examples correspondent to 4 sentences. If 
we sample n examples from a template, the pre-
cision of a paraphrase template can be calculated
by:
Precision = R / (4 * n) 
Where, R is the number of sentences which
is considered to be correct by the evaluator.
3) Coverage 
Evaluating directly the coverage of a paraphrase 
template is difficult because humans can?t enu-
merate all the words to be suitable to the tem-
plate. We use an approximate method to get the
coverage of a template. At first we use another 
search engine to get candidate sentences with 
similar method for generalization of a para-
phrase template. From these retrieved sentences
we can get many different words with the 
known generalized words because more than
85% of search results from different search en-
gine are different. Evaluators extract every sen-
tence which can be replaced with the 
correspondent phrase in a paraphrase and the
new sentences retain the origin meaning. We 
know each sentence is correspondent to a word. 
Then we define two metrics: 
Surface_Coverage = M / NS
Semantic_Coverage =
Map(K) / (Map(NS-M) + Map(K)) 
Where, NS is the number of all manually
tagged right words, M is the number of words 
which can be instantiated from a paraphrase
template, K is the number of all the words that 
generalized the template at the front. Map(X) is 
the total word number of the word clusters 
which derived from X word in the semantic dic-
tionary of Cilin(EV).
5.3 Result 
In order to exhibit the merit of our method, we 
conduct four groups of experiment. They are
POS-Tag, Cilin3, Cilin4 and Ciln5, respectively.
Especially, we just randomly select 400 words 
to satisfy the POS information.
Table 6: Experiment Results 
Reasonability
(%)
Coverage
(%)
St_R Lo_R Su_C Se_C
Preci-
sion
(%)
POS 10.50 17.00 90.00 ---- 11.75
Cilin3 45.57 84.50 27.55 38.71 45.75
Cilin4 46.89 84.54 23.87 44.48 64.13
Cilin5 46.24 83.12 20.39 39.47 69.88
Every value in table 6 is a average value of 
four values correspondent to four templates.
From the table we can find that the reasonability
of the Cilin-based representation template
changes little, and that of POS-based representa-
tion is very lower. We find that the longer origi-
nal phrases are, the lower the coverage of the
generalized template is. Although the average 
coverage of generalized template is relatively
low, we can draw a conclusion that using multi-
ple semantic codes to generalize phrasal para-
phrase examples is reasonable.
The column of the coverage shows that the 
coverage rates of Cilin-based templates are all
not more than 50%. And the POS-based tem-
plate has a very high coverage rate. And we 
know that the extended information is not
enough only depending on one search engine. 
We will combine several different search en-
gines with together to solve this problem in the 
future work. 
1.0 1.5 2.0 2.5 3.0 3.5 4.0
0
10
20
30
40
50
60
70
80
90
100
 strict_Reasonability  loose_Reasonability
 surface_Coverage  semantic_Coverage
 Precision
Va
lu
es
 o
f P
er
ce
nt
Different Template Representation Method
Figure 6. Experimental Results
The numbers from one to four on the X-axis
are correspondent to POS, Cilin3, Cilin4 and 
Cilin5 in figure 6. We can see the features
clearly of different representation methods of 
template from the figure 6. We can find that
54
Cilin5-based template has the highest precision, 
but its coverage is lower. And Cilin3-based 
template has opposite feature. This is because 
that one semantic code of Cilin3 includes more 
words than that of Cilin5. At the same time, 
more words bring more redundant information. 
And Cilin4-based template has a good tradeoff 
between coverage and precision. So we con-
clude that the semantic code of fourth layer in 
Cilin (EV) is more suitable to represent para-
phrase template.  
Some additional information can be extracted 
from the generalized template. Such as, the col-
location information between the slot word and 
the context words can be extract. For example, 
in the fourth template, we can get the informa-
tion about which words can be collocated with 
??(Jin)?.
Although this kind of representation of para-
phrase template has a good performance, it is 
weak for those words or structures that don?t 
exist in dictionary. Also, this method is not suit-
able to the named entities representation. 
6 Conclusion
In this paper, a novel method for automated 
generalization of paraphrase examples is pro-
posed. This method is not dependent on the tra-
ditional limited texts instead it is based on the 
richness of the Web. It uses the multiple seman-
tic codes to generalize a paraphrase example 
combing a semantic dictionary (Cilin (EV)). The 
experimental results proved that this representa-
tion method is reasonable and the generalized 
templates have a good precision and coverage.  
But this is just the beginning of the para-
phrase examples generalization. And we sim-
plify the problem in some aspects, such as we 
limited the number of the slot word in a para-
phrase example, and we stipulate only the same 
word can be slot word. Also, we find that our 
templates are weak for those words or structures 
that don?t exist in dictionary. Some methods in 
information extraction about named entities 
generalization can be used for reference in the 
future. Moreover, how to combine the semantic 
code with other representation forms together is 
also an interesting work. 
References
[1] Chris Quirk, Chris Brockett, and William Dolan. 
Monolingual Machine Translation for Para-
phrase Generation. editors, Dekang Lin and 
Dekai Wu, In Proceedings of EMNLP 2004, 
Barcelona, pages 142-149  
[2] Dekang Lin and Patrick Pantel. 2001. Discovery 
of Inference Rules for Question Answering. 
Natural Language Engineering 7(4):343-360 
[3] Dekang Lin and Patrick Pantel. Discovery of 
inference rules for question answering. Natural 
Language Engineering, 1, 2001.  
[4] E. Agichtein, S. Lawrence, and L. Gravano. 
Learning search engine specific query transfor-
mations for question answering. In Proceedings 
of the 10th International World-Wide Web Con-
ference (WWW10), 2001 
[5] Fabio Rinaldi, James Dowdall, Kaarel Kalju-
rand, Michael Hess, Diego Molla. 2003. Ex-
ploiting Paraphrases in a Question Answering 
System. The Second International Workshop on 
Paraphrasing: Paraphrase Acquisition and Ap-
plications 
[6] Florence Duclaye France. Learning paraphrases 
to improve a question-answering system. In 
EACL Natural Language Processing for Ques-
tion Answering, 2003 
[7] Graeme Hirst. Paraphrasing Paraphrased. In 
Proceedings of the Second International Work-
shop on Paraphrasing, 2003 
[8] Hal Daum? III and Daniel Marcu. Acquiring 
paraphrase templates from document/abstract 
pairs. In NL Seminar in ISI, 2003 
[9] Hua Wu, Ming Zhou. Optimizing Synonym 
Extraction Using Monolingual and Bilingual 
Resources. In Proceedings of the Second Inter-
national Workshop on Paraphrasing, 2003 
[10] Hua Wu, Ming Zhou. Synonymous Collocation 
Extraction Using Translation Information. In 
Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics, 
2003 
[11] Jinshan Ma, Yu Zhang, Ting Liu, and Sheng Li. 
A Statistical Dependency Parser of Chinese un-
der Small Training Data. Workshop: Beyond 
shallow analyses - Formalisms and statistical 
modeling for deep analyses, IJCNLP-04, 4 2004. 
[12] Noriko Tomuro. 2003. Interrogative Reformula-
tion Patterns and Acquisition of Question Para-
phrases. The Second International Workshop on 
Paraphrasing: Paraphrase Acquisition and Ap-
plications 
[13] Oren Glickman and Ido Dagan. Identifying 
lexical paraphrases from a single corpus: A case 
study for verbs. In Proceedings of Recent Ad-
vantages in Natural Language Processing, Sep-
tember 2003 
55
[14] Regina Barzilay and Kathleen McKeown. Ex-
tracting paraphrases from a parallel corpus. In 
Proceedings of the ACL/EACL, Toulouse, 2001 
[15] Regina Barzilay and Lillian Lee. Learning to 
Paraphrase: An Unsupervised Approach Using 
Multiple-Sequence Alignment. In Proceedings 
of HLT-NAACL 2003, pages 16-23  
[16] Regina Barzilay, Noemie Elhadad, Kathleen R. 
McKeown. 2003. Inferring Strategies for Sen-
tence Ordering in Multidocument News Sum-
marization. The Second International Workshop 
on Paraphrasing: Paraphrase Acquisition and 
Applications 
[17] Weigang Li, Ting Liu, Sheng Li. Combining 
Sentence Length with Location Information to 
Align Monolingual Parallel Texts. AIRS, 2004, 
pages 71-77 
[18] Yusuke Shinyama and Satoshi Sekine. Para-
phrase acquisition for information extraction. 
editors, Kentaro Inui and Ulf Hermjakob, In 
Proceedings of the Second International Work-
shop on Paraphrasing, 2003, pages 65-71 
[19] Yusuke Shinyama, Satoshi Sekine, Kiyoshi 
Sudo, and Ralph Grishman. Automatic para-
phrase acquisition from news articles, In Pro-
ceedings of Human Language Technology 
Conference (HLT2002), San Diego, USA, Mar. 
15, 2002 
[20] Zhang Yujie, Kazuhide Yamamoto. Automatic 
Paraphrasing of Chinese Utterances. Journal of 
Chinese Information Processing. Vol. 117 No. 
16: 31-38(Chinese) 
56
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 173?176,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT: Web based Scoring Method for English Lexical Substitution 
Shiqi Zhao, Lin Zhao, Yu Zhang, Ting Liu, Sheng Li 
Information Retrieval Laboratory, School of Computer Science and Technology, 
Box 321, Harbin Institute of Technology 
Harbin, P.R. China, 150001 
{ zhaosq, lzhao, zhangyu, tliu, lisheng }@ir.hit.edu.cn 
 
 
Abstract 
This paper describes the HIT system and its 
participation in SemEval-2007 English 
Lexical Substitution Task. Two main steps 
are included in our method: candidate sub-
stitute extraction and candidate scoring. In 
the first step, candidate substitutes for each 
target word in a given sentence are ex-
tracted from WordNet. In the second step, 
the extracted candidates are scored and 
ranked using a web-based scoring method. 
The substitute ranked first is selected as the 
best substitute. For the multiword subtask, 
a simple WordNet-based approach is em-
ployed. 
1 Introduction 
Lexical substitution aims to find alternative words 
that can occur in given contexts. It is important in 
many applications, such as query reformulation in 
question answering, sentence generation, and 
paraphrasing. There are two key problems in the 
lexical substitution task, the first of which is 
candidate substitute extraction. Generally speaking, 
synonyms can be regarded as candidate substitutes 
of words. However, some looser lexical 
relationships can also be considered, such as 
Hypernyms and Hyponyms defined in WordNet 
(Fellbaum, 1998). In addition, since lexical 
substitution is context dependent, some words 
which do not have similar meanings in general 
may also be substituted in some certain contexts 
(Zhao et al, 2007). As a result, finding a lexical 
knowledge base for substitute extraction is a 
challenging task. 
The other problem is candidate scoring and 
ranking according to given contexts. In the lexical 
substitution task of SemEval-2007, context is con-
strained as a sentence. The system therefore has to 
score the candidate substitutes of each target word 
using the given sentence. The following questions 
should be considered here: (1) What words in the 
given sentence are ?useful? context? (2) How to 
combine the context words and use them in rank-
ing candidate substitutes? For the first question, we 
can use all words of the sentence, words in a win-
dow, or words having syntactic relations with the 
target word. For the second question, we can re-
gard the context words as ?bag of words?, n-grams, 
or syntactic structures. 
In HIT, we extract candidate substitutes from 
WordNet, in which both synonyms and hypernyms 
are investigated (Section 3.1). After that, we score 
the candidates using a web-based scoring method 
(Section 3.2). In this method, we first select frag-
ments containing the target word from the given 
sentence. Then we construct queries by replacing 
the target word in the fragments with the candidate 
substitute. Finally, we search Google using the 
constructed queries and score each candidate based 
on the counts of retrieved snippets. 
The rest of this paper is organized as follows: 
Section 2 reviews some related work on lexical 
substitution. Section 3 describes our system, espe-
cially the web-based scoring method. Section 4 
presents the results and analysis. 
2 Related Work 
Synonyms defined in WordNet have been widely 
used in lexical substitution and expansion (Smea-
ton et al, 1994; Langkilde and Knight, 1998; Bol-
173
shakov and Gelbukh, 2004). In addition, a lot of 
methods have been proposed to automatically con-
struct thesauri of synonyms. For example, Lin 
(1998) clustered words with similar meanings by 
calculating the dependency similarity. Barzilay and 
McKeown (2001) extracted paraphrases using mul-
tiple translations of literature works. Wu and Zhou 
(2003) extracted synonyms with multiple resources, 
including a monolingual dictionary, a bilingual 
corpus, and a monolingual corpus. Besides the 
handcrafted and automatic synonym resources, the 
web has been exploited as a resource for lexical 
substitute extraction (Zhao et al, 2007). 
As for substitute scoring, various methods have 
been investigated, among which the classification 
method is the most widely used (Dagan et al, 2006; 
Kauchak and Barzilay, 2006). In detail, a binary 
classifier is trained for each candidate substitute, 
using the contexts of the substitute as features. 
Then a new contextual sentence containing the tar-
get word can be classified as 1 (the candidate is a 
correct substitute in the given sentence) or 0 (oth-
erwise). The features used in the classification are 
usually similar with that in word sense disam-
biguation (WSD), including bag of word lemmas 
in the sentence, n-grams and parts of speech (POS) 
in a window, etc. There are other models presented 
for candidate substitute scoring. Glickman et al 
(2006) proposed a Bayesian model and a Neural 
Network model, which estimate the probability of 
a word may occur in a given context. 
3 HIT System 
3.1 Candidate Substitute Extraction 
In HIT, candidate substitutes are extracted from 
WordNet. Both synonyms and hypernyms defined 
in WordNet are investigated. Let w be a target 
word, pos the specified POS of w. n the number of 
w?s synsets defined in WordNet. Then the system 
extracts w?s candidate substitutes as follows: 
z Extracts all the synonyms in each synset 
under pos1 as candidate substitutes. 
z If w has no synonym for the i-th synset 
(1?i?n), then extracts the synonyms of its 
nearest hypernym. 
z If pos is r (or a), and no candidate substi-
tute can be extracted as described above, 
                                                 
1 In this task, four kinds of POS are specified: n - noun, v - 
verb, a - adjective, r - adverb.  
then extracts candidate substitutes under the 
POS a (or r). 
3.2 Candidate Substitute Scoring 
As mentioned above, all words in the given sen-
tence can be used as contextual information in the 
scoring of candidate substitutes. However, it is ob-
vious that not all context words are really useful 
when determining a word?s substitutes. An exam-
ple can be seen from Figure 1. 
 
 
She turns eyes <head>bright</head> with 
excitement towards Fiona , still tugging on the 
string of the minitiature airship-cum-dance 
card she has just received at the door . 
Figure 1. An example of a context sentence. 
 
In the example above, words turns, eyes, with, 
and excitement are useful context words, while the 
others are not. The useless contexts may even be 
noise if they are used in the scoring. As a result, it 
is important to select context words carefully. 
In HIT, we select context words based on the 
following assumption: useful context words for 
lexical substitute are those near the target word in 
the given sentence. In other words, the words that 
are far from the target word are not taken into con-
sideration. Obviously, this assumption is not al-
ways true. However, considering only the 
neighboring words can reduce the risk of bringing 
in noise. Besides, Edmonds (1997) has also dem-
onstrated in his paper that short-distance colloca-
tions with neighboring words are more useful in 
lexical choice than long ones. 
Let w be the target word, t a candidate substitute, 
S the context sentence. Our basic idea is that: One 
can substitute w in S with t, which generates a new 
sentence S?. If S? can be found on the web, then the 
substitute is admissible. The more times S? occurs 
on the web, the more probable the substitute is. In 
practice, however, it is difficult to find a whole 
sentence S? on the web due to sparseness. Instead, 
we use fragments of S? which contains t and sev-
eral neighboring context words (based on the as-
sumption above). Then the question is how to ob-
tain one (or more) fragment of S?. 
A window with fixed size can be used here. Su-
ppose p is the position of t in S?, for instance, we 
can construct a fragment using words from posi-
tion p-r to p+r, where r is the radius of window. 
174
However, a fixed r is difficult to set, since it may 
be too large for some sentences, which makes the 
fragments too specific, while too small for some 
other sentences, which makes the fragments too 
loose. An example can be seen in Table 1. 
 
1(a) But when Daniel turned <head>blue</head> 
one time and he totally stopped breathing. 
1(b) Daniel turned t one time 
2(a) We recommend that you <head>check</head> 
with us beforehand. 
2(b) that you t with us 
Table 1. Examples of fragments with fixed size. 
 
In Table1, 1(a) and 2(a) are two sentences from 
the test data of SemEval-2007Task10. 1(b) and 2(b) 
are fragments constructed according to 1(a) and 
2(a), where the window radius is 2 and t denotes 
any candidate substitute of the target word. It is 
obvious that 1(b) is a rather strict fragment, which 
makes it difficult to find sentences containing it on 
the web, while 2(b) is quite loose, which can 
hardly constrain the semantics of t. 
Having considered the problem above, we pro-
pose a rule-based method that constructs fragments 
with varied lengths. Let Ft be a fragment contain-
ing t, the construction rules are as follows: 
Rule-1: Ft must contain at least two words be-
sides t, at least one of which is non-stop word. 
Rule-2: Ft does not cross sub-sentence boundary 
(?,?). 
Rule-3: Ft should be the shortest fragment that 
satisfies Rule-1 and Rule-2. 
According to the rules above, we construct at 
most three fragments for each S?: (1) t occurs at the 
beginning of Ft, (2) t occurs in the middle of Ft, 
and (3) t occurs at the end of Ft. Here we have an-
other constraint: if one constructed fragment F1 is 
the substring of F2, then F2 is removed. Please 
note that the morphology is not taken into account 
when we construct queries. 
For the sentence 1(a) and 2(a) in Table 1, the 
constructed fragments are as follows: 
 
For 1(a): Daniel turned t; t one time; turned t 
one 
For 2(a): recommend that you t; t with us be-
forehand 
Table 2. Examples of the constructed fragments 
 
To score a candidate substitute, we replace ?t? in 
the fragments with each candidate substitute and 
use them as queries, which are then fed to Google. 
The score of t is computed according to the counts 
of retrieved snippets: 
?
=
=
n
i
tWebMining iFSnippetcountn
tScore
1
))((
1
)(     (1) 
where n is the number of constructed fragments, 
Fti is the i-th fragment (query) corresponding to t, 
and count(Snippet(Fti)) is the count of snippets 
retrieved by Fti. 
All candidate substitutes with scores larger than 
0 are ranked and the first 10 substitutes are re-
tained for the oot subtask. If the number of candi-
dates whose scores are larger than 0 is less than 10, 
the system ranks the rest of the candidates by their 
frequencies using a word frequency list. The spare 
capacity is filled with those candidates with largest 
frequencies. For the best subtask, we simply output 
the substitute that ranks first in oot. 
3.3 Detection of Multiwords 
The method used to detect multiword in the HIT 
system is quite similar to that employed in the 
baseline system. We also use WordNet to detect if 
a multiword that includes the target word occurs 
within a window of 2 words before and 2 words 
after the target word.  
A difference from the baseline system lies in 
that our system looks up WordNet using longer 
multiword candidates first. If a longer one is found 
in WordNet, then its substrings will be ignored. 
For example, if we find ?get alng with? in Word-
Net, we will output it as a multiword and will not 
check ?get alng? any more. 
4 Results 
Our system is the only one that participates all the 
three subtasks of Task10, i.e., best, oot, and mw. 
The evaluation results of our system can be found 
in Table 3 to Table 5. Our system ranks the fourth 
in the best subtask and seventh in the oot subtask. 
We have analyzed the results from two aspects, 
i.e., the ability of the system to extract candidate 
substitutes and the ability to rank the correct sub-
stitutes in front. There are a total of 6,873 manual 
substitutes for all the 1,710 items in the gold stan-
dard, only 2,168 (31.54%) of which have been ex-
tracted as candidate substitutes by our system. This 
result suggests that WordNet is not an appropriate 
175
source for lexical substitute extraction. In the fu-
ture work, we will try some other lexical resources, 
such as the Oxford American Writer Thesaurus 
and Encarta. In addition, we will also try the 
method that automatically constructs lexical re-
sources, such as the automatic clustering method. 
Further analysis shows that, 1,388 (64.02%) out 
of the 2,168 extracted correct candidates are 
ranked in the first 10 in the oot output of our sys-
tem. This suggests that there is a big space for our 
system to improve the candidate scoring method. 
In the future work, we will consider more and 
richer features, such as the syntactic features, in 
candidate substitute scoring. Furthermore, A dis-
advantage of this method is that the web mining 
process is quite inefficient. Therefore, we will try 
to use the Web 1T 5-gram Version 1 from Google 
(LDC2006T13) in the future. 
 
 P R ModeP ModeR
OVERALL 11.35 11.35 18.86 18.86 
Further Analysis 
NMWT 11.97 11.97 19.81 19.81 
NMWS 12.55 12.38 19.93 19.65 
RAND 11.81 11.81 20.03 20.03 
MAN 10.81 10.81 17.53 17.53 
Baselines 
WORDNET 9.95 9.95 15.58 15.58 
LIN 8.84 8.53 14.69 14.23 
Table 3. best results. 
 
 P R ModeP ModeR
OVERALL 33.88 33.88 46.91 46.91 
Further Analysis 
NMWT 35.60 35.60 48.48 48.48 
NMWS 36.63 36.63 49.33 49.33 
RAND 33.95 33.95 47.25 47.25 
MAN 33.81 33.81 46.53 46.53 
Baselines 
WORDNET 29.70 29.35 40.57 40.57 
LIN 27.70 26.72 40.47 39.19 
Table 4. oot results. 
 
 Our System WordNet BL 
 P R P R 
detection 45.34 56.15 43.64 36.92
identification 41.61 51.54 40.00 33.85
Table 5. mw results. 
 
Acknowledgements 
This research was supported by National Natural 
Science Foundation of China (60575042, 
60503072, 60675034). 
References 
Barzilay Regina and McKeown Kathleen R. 2001. Ex-
tracting paraphrases from a Parallel Corpus. In Pro-
ceedings of ACL/EACL. 
Bolshakov Igor A. and Gelbukh Alexander. 2004. Syn-
onymous Paraphrasing Using WordNet and Internet. 
In Proceedings of NLDB. 
Dagan Ido, Glickman Oren, Gliozzo Alfio, Marmor-
shtein Efrat, Strapparava Carlo. 2006. Direct Word 
Sense Matching for Lexical Substitution. In Proceed-
ings of ACL. 
Edmonds Philip. 1997. Choosing the Word Most Typi-
cal in Context Using a Lexical Co-occurrence Net-
work. In Proceedings of ACL. 
Fellbaum Christiane. 1998. WordNet: An Electronic 
Lexical Database. MIT Press, Cambridge, MA. 
Glickman Oren, Dagan Ido, Keller Mikaela, Bengio 
Samy. 2006. Investigating Lexical Substitution Scor-
ing for Subtitle Generation. In Proceedings of 
CoNLL. 
Kauchak David and Barzilay Regina. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of 
HLT-NAACL. 
Langkilde I. and Knight K. 1998. Generation that Ex-
ploits Corpus-based Statistical Knowledge. In Pro-
ceedings of the COLING-ACL. 
Lin Dekang. 1998. Automatic Retrieval and Clustering 
of Similar Words. In Proceedings of COLING-ACL. 
Smeaton Alan F., Kelledy Fergus, and O?Donell Ruari. 
1994. TREC-4 Experiments at Dublin City Univer-
sity: Thresholding Posting Lists, Query Expansion 
with WordNet and POS Tagging of Spanish. In Pro-
ceedings of TREC-4. 
Wu Hua and Zhou Ming. 2003. Optimizing Synonym 
Extraction Using Monolingual and Bilingual Re-
sources. In Proceedings of IWP. 
Zhao Shiqi, Liu Ting, Yuan Xincheng, Li Sheng, and 
Zhang Yu. 2007. Automatic Acquisition of Context-
Specific Lexical Paraphrases. In Proceedings of 
IJCAI-07. 
176
Coling 2010: Poster Volume, pages 1167?1175,
Beijing, August 2010
Bridging Topic Modeling and Personalized Search
Wei Song Yu Zhang Ting Liu Sheng Li
School of Computer Science
Harbin Institute of Technology
{wsong, yzhang, tliu, lisheng}@ir.hit.edu.cn
Abstract
This work presents a study to bridge topic
modeling and personalized search. A
probabilistic topic model is used to extract
topics from user search history. These
topics can be seen as a roughly summary
of user preferences and further treated as
feedback within the KL-Divergence re-
trieval model to estimate a more accurate
query model. The topics more relevant
to current query contribute more in updat-
ing the query model which helps to dis-
tinguish between relevant and irrelevant
parts and filter out noise in user search
history. We designed task oriented user
study and the results show that: (1) The
extracted topics can be used to cluster
queries according to topics. (2) The pro-
posed approach improves ranking qual-
ity consistently for queries matching user
past interests and is robust for queries not
matching past interests.
1 Introduction
The majority of queries submitted to search en-
gines are short and ambiguous and the users of
search engines often have different search intents
even when they submit the same query (Janse and
Saracevic, 2000)(Silverstein and Moricz, 1999).
The ?one size fits all? approach fails to optimize
each individual?s specific information need. Per-
sonalized search has be viewed as a promising
direction to solve the ?data overload? problem,
and aims to provide different search results ac-
cording to the specific preference of an individ-
ual(Pitkow and Breuel, 2002). Information re-
trieval (IR) communities have developed models
for context sensitive search and related applica-
tions (Shen and Zhai, 2005a)(White and Chen,
2009).
The search context includes a broad range of in-
formation types such as a user?s background, his
personal desktop index, browser history and even
the context information of a group of similar users
(Teevan, 2009). In this paper, we exploit the user
search history of an individual which contains the
past submitted queries, results returned and the
click through information. As described in (Tan
and Zhai, 2006), search history is one of the most
important forms of search context. When dealing
with search history, distinguishing between rele-
vant and irrelevant parts is important. The search
history may contain a lot of noisy information
which can harm the performance of personaliza-
tion (Dou and Wen, 2007). Hence, we need to
sort out relevant and irrelevant parts to optimize
search personalization.
In this paper, we propose a topic model based
approach to study users? preferences. The main
contribution of this work is modeling user search
history with topics for personalized search. Our
approach mainly consists of two steps: topic ex-
traction and relevance feedback. We assume that
a user?s search history is governed by the underly-
ing hidden properties and apply probabilistic La-
tent Semantic Indexing (pLSI) (Hofmann, 1999)
to extract topics from user search history. Each
topic indexes a unigram language model. We
model these extracted topics as feedback in the
KL-Divergence retrieval framework. The task is
to estimate a more accurate query model based
on the evidence from user feedback. We distin-
1167
guish relevant parts from irrelevant parts in search
history by focusing on the relevance between top-
ics and query. The closer a topic is to the cur-
rent query, the more it contributes in updating the
query model, which in turn is used to rerank the
documents in results set.
2 Related Work
2.1 Personalized IR
Personalized search is an active ongoing research
direction. Based on different representations of
user profile, we classify approaches as follows:
Taxonomy based methods: this approach
maps user interests to an existing taxonomy.
ODP1 is widely used for this purpose. For
example, by exploiting the user search history,
(Speretta and Gauch, 2005) modeled user interest
as a weighted concept hierarchy created from the
top 3 level of ODP. (Havelivala, 2002) proposed
the ?topic sensitive pagerank? algorithm by cal-
culating a set of PageRanks for each web page on
the top 16 ODP categories. (Qiu and Cho, 2006)
further improved this approach by building user
models from user click history. In recent stud-
ies, (Xu S. and Yu, 2008) used ODP categories
for exploring folksonomy for personalized search.
(Dou and Wen, 2007) proposed a method that rep-
resent user profile as a weighting vector of 67 pre-
defined topic categories provided by KDD Cup-
2005. Taxonomy based methods rely on a pre-
defined taxonomy and may suffer from the granu-
larity problem.
Content based methods: this category of
methods use traditional text presentation model
such as vector space model and language model
to express user preference. Rich content infor-
mation such as user search history, browser his-
tory and indexes of desktop documents are ex-
plored. The user profiles are built in the forms of
term vectors or term probability distributions. For
example, (Sugiyama and M., 2004) represented
user profiles as vectors of distinct terms and ac-
cumulated past preferences. (Teevan and Horvitz,
2005) constructed a rich user model based on both
search-related information, such as previously is-
sued queries, and other information such as doc-
1Open Directory Project, http://dmoz.org/
uments and emails a user had read and created.
(Shen and Zhai, 2005b) used browsing histories
and query sessions to construct short term indi-
vidual models for personalized search.
Learning to rank methods: (Eugene and Su-
san, 2005) and (Eugene and Zheng, 2006) incor-
porated user feedback into the ranking process in a
learning to rank framework. They leveraged mil-
lions of past user interaction with web search en-
gine to construct implicit feedback features. How-
ever, this approach aims to satisfy majority of
users rather than individuals.
2.2 Probabilistic Topic Models
Probabilistic topic models have become popular
tools for unsupervised analysis of document col-
lection. Topic models are based upon the idea
that documents are mixtures of topics, where
a topic is a probability distribution over words
(Steyvers and Griffiths, 2007). These topics are
interpretable to a certain degree. In fact, one of
the most important applications of topic models
is to find out semantic lexicons from a corpus.
One of the most popular topic models, the prob-
abilistic Latent Semantic Indexing Model (pLSI),
was introduced by Hofmann (Hofmann, 1999)
and quickly gained acceptance in a number of text
modeling applications. In this study, pLSI is used
to discover the underlying topics in user search
history. Though pLSI is argued that it is not a
complete generative model, we used it because it
does not need to generate unseen documents in
our case and the model is much easier to be es-
timated compared with sophisticated models such
as LDA(David M. Blei and Jordan, 2003).
2.3 Model based Relevance Feedback
Our work is also related to language model based
(pseudo) relevance feedback (Zhai and Lafferty,
2001b) and shares the similar idea with (Tan B.
and Zhai, 2007). The differences are: (1) The
feedback source is user search history rather than
top ranked documents for a query. (2) We make
use of user implicit feedback rather than explicit
feedback. (3) The topics in search history could
be extracted offline and updated periodically. Ad-
ditionally, these topics provide an informative pic-
ture of user search history.
1168
Table 1: An illustration of topics extracted from a
user?s search history. Terms with highest proba-
bilities are listed below each topic.
Topic 2 Topic 3 Topic 9 Topic 16
climb movie swim cup
0.032 0.091 0.044 0.027
setup download ticket world
0.022 0.078 0.032 0.022
equipment dvd notice team
0.020 0.061 0.019 0.016
practice watch travel brazil
0.009 0.060 0.016 0.011
player cinema hotel storm
0.006 0.038 0.008 0.007
3 Proposed Approach
3.1 Main Idea
A user?s search history usually covers multiple
topics. It is crucial to distinguish between rele-
vant and irrelevant parts for optimizing personal-
ization. We propose a topic model based method
to achieve that goal. First, we construct a doc-
ument collection revealing user intents according
to the user?s past activities. A probabilistic topic
model is applied on this collection to extract la-
tent topics. Then the extracted topics are used as
feedback. The query model is updated by high-
lighting the topics highly relevant to current query.
Finally, the search results are reranked according
to the relevance to the updated query model. Ta-
ble 1 shows 4 topics extracted from a user?s search
history. Each topic is a unigram language model.
The terms with higher probabilities belonging to
each topic are listed. We can predict that the user
has interests in both movie and football. However,
when the user submits a query about world cup,
the topic 16 is given higher preference for esti-
mating a more accurate query model.
3.2 Topic Extraction from Search History
Individual?s search history consists of all the past
query units. Each query unit includes query text,
returned search results (with title, snippets and
URLs) and click through information. Here, we
concatenated the title and snippet of each search
result to form a document being considered as a
whole. The whole search history can be seen as a
collection of documents. Obviously, many doc-
uments in the collection may fail to satisfy the
user?s information need and are uncertain for dis-
covering the user?s preferences. Therefore, the
first task is to select proper documents in search
history as the preference collection for topic dis-
covery.
3.2.1 Preference Collection
An intuitive solution is to use the documents
that are clicked by the user. The assumption is
that a user clicks on a result only if he is interested
in the document. However, user click is sparse in
real search environments and the documents not
clicked by the user may also be relevant to the
user?s information need. We assumed that the user
had only one search intent for a submitted query.
To enhance this coherence within a query unit, we
created only one super-document for a query unit
as follows: if a query unit had clicked documents,
then we concatenated these document to form a
preferred document. Otherwise, we selected the
top n documents from the search results and con-
catenated them as a preferred document. That is
motivated by the idea of pseudo relevance feed-
back (Lavrenko and Croft, 2001) and used here for
alleviating data sparsity. Pseudo relevance feed-
back is sensitive to the number of feedback docu-
ments. In this work, n is set to 3, because the aver-
age clicks for a query is not more than 3. By this
way, we got a preference collection whose size is
the same as the number of past queries.
3.2.2 Topic Extraction
Given the collection of preferred documents,
we applied pLSI on this collection to extract
underlying topics. We define the collection as
C={d1,d2,. . . ,dM}, where di corresponds to the
ith query unit, and M is the size of the collection.
Each query unit is viewed as a mixture of differ-
ent topics. It is reasonable in reality. For exam-
ple, a news document about ?play basketball with
obama? might be seen as a mixture of topics ?pol-
itics? and ?sports?.
Modeling: The basic idea of pLSI is to treat
the words in each document as being generated
from a mixture model where the component mod-
els are topic word distributions. Let k be the num-
1169
ber of topics which is assumed known and fixed.
?j is the word distribution for topic j. We extract
topics from collection C using a simple proba-
bilistic mixture model as described in (Zhai and
Yu, 2004). A word w within document d can be
viewed as generated from a mixture model:
pd(w) = ?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)
(1)
where ?B is the background model for all the doc-
uments. The background model is used to draw
common words across all the documents and lead
to more discriminative and informative topic mod-
els, since ?B gives high weights to non-topical
words. ?B is the probability that a term is gen-
erated from the background model which is set to
be a constant. To draw more discriminative topic
models, we set ?B to 0.95. Parameter pid,j indi-
cates the probability that topic j is assigned to the
specific document d, where?kj=1 pid,j=1.Parameter estimation: The parameters we
have to estimate including the background model
?B , {?j} and {pid,j}. ?B is maximum likelihood
estimated (MLE) using all available text in our
data set so that it is a fixed distribution. The other
parameters to be estimated are {?j} and {pid,j}.
The log-likelihood of document d is:
log p(d) =
?
w?V
c(w, d) log[?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)]
(2)
The log-likelihood of the whole collection C is:
log(C) =
?
d?C
?
w?V
c(w, d) log[?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)]
(3)
The Expectation-Maximization (EM) algorithm
(Dempster and Rubin, 1977) is used to find a
group of parameters maximizing equation (3).
The updating formulas are:
,
,
1
( ) ( )
,
,
( ) ( )
, ' '
' 1
, ,( 1)
,
, ,
' 1
(
E-Step:
( | )
( )
( | ) (1 ) ( | )
( | )
( )
( | )
M-Step:
( , )(1 ( )) ( )
( , )(1 ( )) ( ')
B B
d w k
B B B d j j
j
m m
d j j
d w k
m m
d j j
j
d w d wm w V
d j k
d w d ww V
j
p w
p z B
p w p w
p w
p z j
p w
c w d p z B p z j
c w d p z B p z j
p
? ?
? ? ? ? ?
? ?
? ?
?
=
=
+ ?
?=
= =
+ ?
= =
? = ==
? = =
?
?
?
??
, ,
1)
, ' , '
'
( , )(1 ( )) ( )
( | )
( ', )(1 ( )) ( )
d w d w
m d C
j
d w d w
d C w V
c w d p z B p z j
w
c w d p z B p z j
?+ ?
? ?
? = =
= ? = =
?
? ?
 
where c(w, d) denotes the number of times w
occurs in d. A hidden variable zd,w is introduced
for the identity of each word. p(zd,w = B) is
the probability that the word w in document d is
generated by the background model. p(zd,w = j)
denotes the probability that the word w in docu-
ment d is generated using topic j given that w is
not generated from the background model. Infor-
mally, the EM algorithm starts with randomly as-
signing values to the parameters to be estimated
and then alternates between E-Step and M-Step
iteratively until it yields a local maximum of the
log likelihood.
Interpretation: As shown in equation (1), a
word can be viewed as a mixture of topics. From
the updating formulas, we can see that the domi-
nant topic of a word depends on both itself and the
context. The word tends to have the same topic
with the document containing it. While the prob-
ability of assigning topic j to document d is es-
timated by aggregating all the fractions of words
generated by topic j in document d. We can ex-
plain it in a more intuitive way with in our applica-
tion. As we know, the queries are usually ambigu-
ous. A classic example is ?apple? which may re-
fer to a kind of fruit, apple Inc, apple electric prod-
ucts, etc. Therefore, it is reasonable to assume
that each word belongs to multiple latent seman-
tic properties. If a returned result contains ?ap-
ple? and other words like ?computer?, ?ipod? ,
etc. The word ?apple? in this result tends to have
the same topic distributions with ?computer? and
1170
?ipod?. If the user clicks the result, we can predict
that the user?s real preference about query ?ap-
ple? is related to electric products having a high
probability. Further, if ?apple? occurs frequently
in many documents related to electric products,
it obtains a higher probability in this topic. As
a result, we not only know user?s interest in elec-
tric products, but also find a preference to ?apple?
brand.
Since a document?s topic depends on the words
it contains, two documents with similar word dis-
tributions have similar topic distributions. In other
words, each topic is like a bridge connecting
queries with similar intents. In summary, the topic
extraction process plays a role in our application
for finding user preference, highlighting discrimi-
native words and connecting queries with similar
intents.
3.3 Topics as Feedback
The topics extracted from search history are con-
sidered as a kind of feedback. Since topic mod-
els actually are extensions of language models,
we use such feedback within the KL-Divergence
retrieval model (Xu and Croft, 1999)(Zhai and
Lafferty, 2001b) that is a principled framework
to model feedback in the language modeling ap-
proach. In this framework, feedback is treated as
updating the query language model based on extra
evidence obtained from the feedback sources. The
information retrieval task is to rank documents ac-
cording to the KL divergence D(?q||?d) between
a query language model ?q and a document lan-
guage model ?d. The KL divergence is defined as:
D(?q||?d) =
?
w?V
p(w|?q) log
p(w|?q)
p(w|?d)
(4)
where V denotes the vocabulary. We estimate
the document model ?d using Dirichlet estimation
(Zhai and Lafferty, 2001a):
p(w|?d) =
c(w, d) + ?p(w|?C)
|d| + ? (5)
where |d| is document length, p(w|?C) is collec-
tion language model which is estimated using the
whole data collection. ? is the Dirichlet prior that
is set to 20 in this work. The updated query model
is defined as:
p(w|?q) = ?pml(w|?q)
+(1 ? ?)
k?
j=1
p(w|?j)p(z = j|q)
(6)
where pml(w|?q) is the MLE query model. {?j}
represents a set of extracted topics each of which
is a unigram language model. ? is used to bal-
ance the two components. z is a hidden variable
over topics. The task is to estimate the multino-
mial topic distribution p(z|q) for query q. Since
pLSI does not properly provide a prior, we esti-
mate p(z = j|q) as:
p(z = j|q) = p(q, z = j)?k
j?=1 p(q, z = j?)
? sim(?q, ?j)?k
j?=1 sim(?q, ?j?)
(7)
Since the query text is usually very short, it is
not easy to make a decision based on query text
alone. Instead, we concatenate all the available
documents in returned result set to form a super-
document. A language model is estimated for it.
We convert both the document language model
and topic models into weighted term vectors and
use cosine similarity as the sim function. p(z|q)
plays an import role here as it determines the con-
tribution of topics. The topics with higher similar-
ity with current query contributes more in updat-
ing query model. This scheme helps to filter out
noisy information in search history.
4 Evaluation and Discussion
4.1 Data Collection
To the best of our knowledge, there is no public
collection with enough content information and
user implicit feedback. We decided to carry out
a data collection. Due to the difficulty to de-
scribe and evaluate user interests implicitly, we
predefined some user interests and implemented
a search system to collect user interactions.
The predefined interests belong to 5 big cate-
gories namely Entertainment, Computer & Inter-
net, Sports, Health and Social life. Each inter-
est is a kind of user preference such as ?movies?
1171
Table 2: An example of predefined user interests
and tasks
category Enterntainment
interest movies
task1 search for a brief introductionof your favorite movie
task2 search for an introduction ofan actor or actress you like
task3 search for movies about?artificial intelligence?
Table 3: Statistics of the data collection
user 1 2 3 4 5
#queries 218 256 177 206 311
#big category 5 5 5 5 5
#interest 25 25 25 25 25
#tasks 100 100 100 100 100
avg.#relevant 4.17 4.22 3.89 4.12 3.24results
avg.#clicked 2.37 2.21 2.71 1.98 2.42results
and ?outdoor sports?. For each interest, we de-
signed several tasks each of which had a goal. Ta-
ble 2 illustrates an example of a predefined user
interest and related tasks. The volunteers were
asked to find out the information need according
to the tasks. Though we defined these interests
and tasks, we did not impose any constraint on
the queries. The volunteers could choose and re-
formulate any query they thought good for find-
ing the desired information. But we did try to in-
crease the possibility that a user might issue am-
biguous queries by designing tasks like ?search
for movies about artificial intelligence? which was
categorized to interest ?movies?, but also related
to computer science.
To collect the user interaction with search en-
gine, we implemented a Lucene based search sys-
tem on Tianwang terabyte corpus(Yan and Peng,
2005). Five volunteers were asked to submit
queries to this system to find information satisfy-
ing the tasks of each interest. The system recorded
users? activities including submitted queries, re-
turned search results (with title, snippet and URL)
and users? click through information. When the
user finished a task, he clicked a button to tell the
system termination of the session containing all
the queries and activities related to this task. After
finishing all the tasks, the volunteers were asked to
judge the top 20 results? relevance (relevant or not
relevant) for each query according to the search
target. Each volunteer submitted 233 queries on
average. Table 3 presents some statistics of this
collection.
4.2 Evaluating Topic Extraction
It is not easy to assess the quality of topics, be-
cause topic extraction is an unsupervised process
and difficult to give a standard answer. Therefore,
we view the topic extraction as a clustering prob-
lem that is to organize queries into clusters. To
group queries into clusters through extracted top-
ics, we use j? = argmax
j
pid,j to assign a query to
the j?th topic. Each topic corresponds to a cluster.
All the queries are divided into k clusters. Based
on the data collection, we setup the golden an-
swers according to the predefined interests. We
view all the queries belonging to a predefined in-
terest(which includes multiple tasks) form a clus-
ter which helps us to build a golden answer with
25 clusters in tatal.
One purpose of making use of topics in search
history is to find more relevant parts and reduce
the noise. We hope that the extracted topics are
coherent. That is, a cluster should contain as many
queries as possible belonging to a single inter-
est. To evaluate coherence, we adopt purity (Zhao
and Karypis, 2001), a commonly used metric for
evaluating clustering. The higher the purity is,
the better the system performs. We compare our
method (denoted as PLSI) against the k-means al-
gorithm(denoted as K-Means) on the preference
collection.
Figure 1 shows the overall purity with differ-
ent number of topics. Our method gained better
performance than k-means algorithm consistently.
It is effective to discover and organize user inter-
ests. Besides, as illustrated in Table 1, our method
is able to give higher probability to discriminative
words of each topic that provides a clear picture
of user search history. This leads to an emergence
of novel approaches for personalized browsing.
1172
0.2
0.3
0.4
0.5
0.6
0.7
0.8
10 20 30 40 50 60 70 80 90 100
Number of topics
Pu
ri
ty
PLSI K-Means
 
Figure 1: Average purity over 5 users gained by
both PLSI and K-Means with different number of
topics(clusters).
4.3 Evaluating Result Reranking
4.3.1 Metric
To quantify the ranking quality, the Dis-
counted Cumulative Gain (DCG) (Jarvelin and
Kekakainen, 2000) is used. DCG is a metric that
gives higher weights to highly ranked documents
and incorporates different relevance levels by giv-
ing them different gain values.
DCG(i) =
{
G(1), if i = 1
DCG(i? 1) + G(i)log(i) , otherwise
In our work, we use G(i) = 1 for the results la-
beled as relevant by a user and G(i) = 0 for the
results that are not relevant. The average normal-
ized DCG (NDCG)over all the test queries is se-
lected to show the performance.
4.3.2 Systems
We evaluated the performance of following sys-
tems:
PLSI: The proposed method. The history model
was a weighted interpolation over topics extracted
from the preference collection described in ses-
sion 3.2.1.
PSEUDO: From each query unit, we selected
top n documents as pseudo feedback. The lan-
guage history model was estimated on all these
documents.
PLSI-PSEUDO: Top n documents from each
query unit were concatenated to form a preferred
document. The history model was constructed
based on topics extracted from these preferred
documents.
HISTORY: The history language model was es-
timated based on all the documents in search his-
tory.
TB: It was based on(Tan and Zhai, 2006)which
built a unit language model for every past query
and the history model was a weighted interpola-
tion of past unit language models.
ORIGINAL: The default search system.
The first 5 systems provided schemes to smooth
the query model. They estimated the query mod-
els by utilizing different types of feedback (im-
plicit feedback or pseudo feedback) and weight-
ing methods (topic modeling or simple language
modeling). The updated query model was an in-
terpolation between MLE query model and his-
tory language model. The interpolation parameter
was set to 0.5, and n was set to 3.
4.3.3 Performance Comparison
To evaluate the performance on a test query, we
focus on two conditions:
1. the test query matches some past interests.
We want to check the ability of systems to
find relevant information from noisy data.
2. the test query does not match any of past in-
terests. We are interested in the robustness of
the systems.
For the first case, the users were asked to se-
lect at most 2 queries they submitted for each
task. These queries were used as test queries.
The other queries were used to simulate the users?
search history. In total we got 400 queries for
testing. Figure 2 demonstrates the performance
of these systems over all test queries. PLSI
outperformed all other systems consistently that
shows topic model based methods help to esti-
mate a more accurate query model and the user
implicit feedback is better evidence. The PLSI-
PSEUDO also performed well that indicates the
top documents is useful for revealing the topic
of queries, even though they do not satisfy user
need on occasion. TB also gained better perfor-
mance than PSEUDO and HISTORY. It indicates
1173
0.49
0.5
0.51
0.52
0.53
0.54
0.55
0.56
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
0.51
0.514
0.518
0.522
0.526
0.53
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
 
Figure 2: The overall average performance of sys-
tems, when each test query matches some user
past interests
highlighting relevant parts in search history helps
to improve the retrieval performance, when the
query matches some of user past interests. Com-
pared with default system, both HISTORY and
PSEUDO improved a lot which proves that the
context in search history is reliable feedback.
For the second case, each user was asked to
hold out 5 interests from his collection for test-
ing and the other interests were used as search
history. The users selected queries from the held
out interests as test queries. These queries did
not match each user?s past interests. We got 244
test queries. As figure 3 shows, though systems
still performed better against ORIGINAL, the im-
provements were not significant. PLSI still gained
the best performance. It has better ability to al-
leviate the effect of noise. HISTORY and PLSI
are more robust than PLSI-PSEUDOwhich seems
sensitive to the number of topics in this case.
In both cases, HISTORY gained moderate per-
formance but quite robust. It is still a very strong
baseline, though noisy information is not filtered
out. PLSI performed best in both cases. PLSI-
PSEUDO outperformed PSEUDO when the test
queries matched user past interests and gained
comparable results in second case. It shows that
modeling user search history as a mixture of top-
ics and weighting topics according to relevance
between topics and query help to update a better
query model. However, it is necessary to deter-
mine if a query matches past interests that helps
to optimize personalized search strategies.
0.49
0.5
0.51
0.52
0.53
0.54
0.55
0.56
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
0.51
0.514
0.518
0.522
0.526
0.53
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
 
Figure 3: The overall average performance of sys-
tems, when each test query does not match any
user past interest.
5 Conclusion and Future Work
In this paper, we have proposed a topic model
based method for personalized search. This ap-
proach has some advantages: first, it provides a
principled way to combine topic modeling and
personalized search; second, it is able to find user
preferences in an unsupervised way and gives an
informative summary of user search history; third,
it explores the underlying relationship between
different query units via topics that helps to filter
out the noise and improve ranking quality.
In future, we plan to do a large scale study by
leveraging the already built search system or busi-
ness search engines. Also, we will try to add more
information to extend the existing model. Besides,
it is necessary to design methods for determin-
ing whether a submitted query matches the user
past interests that is crucial to apply our algorithm
adaptively and selectively.
Acknowledgements
This research is supported by the National Nat-
ural Science Foundation of China under Grant
No. 60736044, by the National High Technol-
ogy Research and Development Program of China
No. 2008AA01Z144, by Key Laboratory Opening
Funding of MOE-Microsoft Key Laboratory of
Natural Language Processing and Speech, Harbin
Institute of Technology, HIT.KLOF.2009020. We
thank the anonymous reviewers and Fikadu
Gemechu for their useful comments and help.
1174
References
David M. Blei, Andrew Y. Ng and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Dempster, A.P., Laird N.M. and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of Royal Statist. Soc. B,
39:1?38.
Dou, Z., Su R. and J. Wen. 2007. A large-scale evalu-
ation and analysis of personalized search strategies.
Proc. WWW, pages 581?590.
Eugene, A., Eric B. and D. Susan. 2005. Improving
web search ranking by incorporating user behavior
information. Proc.SIGIR, pages 19?26.
Eugene, A. and Zijian Zheng. 2006. Identifying best
bet web search results by mining past user behavior.
Proc.SIGKDD, pages 902?908.
Havelivala, T.H. 2002. Topic-sensitive pagerank.
Proc. WWW, pages 517?526.
Hofmann, T. 1999. Probabilistic latent semantic in-
dexing. Proc.SIGIR, pages 50?57.
Janse, B.J., Spink A. Bateman J. and T. Saracevic.
2000. Real life, real users, and real needs: a study
and analysis of user queries on the web. Information
Processing and Management, 26(2):207?222.
Jarvelin, K. and J. Kekakainen. 2000. Ir evaluation
methods for retrieving highly relevant documents.
Proc.SIGIR, pages 41?48.
Lavrenko, V. and W. Croft. 2001. Relevance based
language models. Proc.SIGIR, pages 120?127.
Pitkow, J., Schutze H. Cass T. Cooley R. Turnbull D.
Edmonds A. Adar E. and T. Breuel. 2002. Person-
alized search. Commun,ACM, 45(9):50?55.
Qiu, F. and J. Cho. 2006. Automatic identification of
user interest for personalized search. Proc.WWW,
pages 727?736.
Shen, X., Tan B. and C. Zhai. 2005a. Context-
sensitive information retrieval using implicit feed-
back. Proc. SIGIR, pages 43?50.
Shen, X., Tan B. and C. Zhai. 2005b. Implicit user
modeling for personalized search. Proc. CIKM,
pages 824?831.
Silverstein, C., Marais H. Henzinger M. and
M. Moricz. 1999. Analysis of a very large web
search engine query log. SIGIR Forum, 33(1):6?12.
Speretta, M. and S. Gauch. 2005. Personalized search
based on user search histories. Proc. WI?05, pages
622?628.
Steyvers, M. and T. Griffiths. 2007. Probabilistic topic
models. Handbook of Latent Semantic Analysis.
Erlbaum, Hillsdale, NJ.
Sugiyama, K., Hatano K. and Yoshkawa. M. 2004.
Personalized search based on user search histories.
Proc. WWW, pages 675?684.
Tan, B., Shen X. and C. Zhai. 2006. Mining long-
term search history to improve search accuracy.
Proc.SIGKDD, pages 718?723.
Tan B., Atulya Velivelli, Fang H. and C. Zhai. 2007.
Term feedback for information retrieval with lan-
guage models. Proc.SIGIR, pages 263?270.
Teevan, J., Dumais S.T. and E. Horvitz. 2005. Per-
sonalizing search via automated analysis of interests
and activities. Proc.SIGKDD, pages 449?456.
Teevan, J., Morris M.R. Bush S. 2009. Discover-
ing and using groups to improve personalization.
Proc.WSDM, pages 15?24.
White, R.W., Bailey P. and L. Chen. 2009. Pre-
dicting user interest from contextual information.
Proc.SIGIR, pages 363?370.
Xu, Jinxi and W. Croft. 1999. Cluster-based language
models for distributed retrieval. Proc.SIGIR, pages
254?261.
Xu S., Bao, S. Fei B. Su Z. and Y. Yu. 2008. Exploring
folksonomy for personalized search. Proc.SIGIR,
pages 155?162.
Yan, H., Li J. Zhu j. and B. Peng. 2005. Tian-
wang search engine at trec 2005: Terabyte track.
Proc.TREC.
Zhai, C. and J. Lafferty. 2001a. A study of smooth-
ing methods for language models applied to ad hoc
information retrieval. Proc.SIGIR, pages 334?342.
Zhai, Chengxiang and John Lafferty. 2001b. Model-
based feedback in the language modeling approach
to information retrieval. Proc.CIKM, pages 403?
410.
Zhai, C., Velivelli A. and B. Yu. 2004. A cross-
collection mixture model for comparative text min-
ing. Proc.SIGKDD, pages 743?748.
Zhao, Y. and G. Karypis. 2001. Criterion functions
for document clustering: Experiments and analysis.
Technical Report TR #01?40, Department of Com-
puter Science, University of Minnesota, Minneapo-
lis, MN.
1175
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 182?192,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Learning of Phonetic Units and Word Pronunciations for ASR
Chia-ying Lee, Yu Zhang, James Glass
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{chiaying,yzhang87,jrg}@csail.mit.edu
Abstract
The creation of a pronunciation lexicon re-
mains the most inefficient process in develop-
ing an Automatic Speech Recognizer (ASR).
In this paper, we propose an unsupervised
alternative ? requiring no language-specific
knowledge ? to the conventional manual ap-
proach for creating pronunciation dictionar-
ies. We present a hierarchical Bayesian model,
which jointly discovers the phonetic inven-
tory and the Letter-to-Sound (L2S) mapping
rules in a language using only transcribed
data. When tested on a corpus of spontaneous
queries, the results demonstrate the superior-
ity of the proposed joint learning scheme over
its sequential counterpart, in which the la-
tent phonetic inventory and L2S mappings are
learned separately. Furthermore, the recogniz-
ers built with the automatically induced lexi-
con consistently outperform grapheme-based
recognizers and even approach the perfor-
mance of recognition systems trained using
conventional supervised procedures.
1 Introduction
Modern automatic speech recognizers require a few
essential ingredients such as a signal representation
of the speech signal, a search component, and typ-
ically a set of stochastic models that capture 1) the
acoustic realizations of the basic sounds of a lan-
guage, for example, phonemes, 2) the realization of
words in terms of these sounds, and 3) how words
are combined in spoken language. When creating
a speech recognizer for a new language the usual
requirements are: first, a large speech corpus with
word-level annotations; second, a pronunciation dic-
tionary that essentially defines a phonetic inventory
for the language as well as word-level pronuncia-
tions, and third, optional additional text data that
can be used to train the language model. Given
these data and some decision about the signal rep-
resentation, e.g., centi-second Mel-Frequency Cep-
stral Coefficients (MFCCs) (Davis and Mermelstein,
1980) with various derivatives, as well as the nature
of the acoustic and language model such as 3-state
HMMs and n-grams, iterative training methods can
be used to effectively learn the model parameters for
the acoustic and language models. Although the de-
tails of the components have changed through the
years, this basic ASR formulation was well estab-
lished by the late 1980?s, and has not really changed
much since then.
One of the interesting aspects of this formulation
is the inherent dependence on the dictionary, which
defines both the phonetic inventory of a language,
and the pronunciations of all the words in the vo-
cabulary. The dictionary is arguably the cornerstone
of a speech recognizer as it provides the essential
transduction from sounds to words. Unfortunately,
the dependency on this resource is a significant im-
pediment to the creation of speech recognizers for
new languages, since they are typically created by
experts, whereas annotated corpora can be relatively
more easily created by native speakers of a language.
The existence of an expert-derived dictionary in
the midst of stochastic speech recognition models is
somewhat ironic, and it is natural to ask why it con-
tinues to receive special status after all these years.
Why can we not learn the inventory of sounds of a
language and associated word pronunciations auto-
matically, much as we learn our acoustic model pa-
rameters? If successful, we would move one step
forward towards breaking the language barrier that
182
limits us from having speech recognizers for all lan-
guages of the world, instead of the less than 2% that
currently exist.
In this paper, we investigate the problem of infer-
ring a pronunciation lexicon from an annotated cor-
pus without exploiting any language-specific knowl-
edge. We formulate our approach as a hierarchi-
cal Bayesian model, which jointly discovers the
acoustic inventory and the latent encoding scheme
between the letters and the sounds of a language.
We evaluate the quality of the induced lexicon and
acoustic model through a series of speech recogni-
tion experiments on a conversational weather query
corpus (Zue et al, 2000). The results demonstrate
that our model consistently generates close perfor-
mance to recognizers that are trained with expert-
defined phonetic inventory and lexicon. Compared
to grapheme-based recognizers, our model is capa-
ble of improving the Word Error Rates (WERs) by
at least 15.3%. Finally, the joint learning framework
proposed in this paper is proven to be much more
effective than modeling the acoustic units and the
letter-to-sound mappings separately, as shown in a
45% WER deduction our model achieves compared
to a sequential approach.
2 Related Work
Various algorithms for learning sub-word based pro-
nunciations were proposed in (Lee et al, 1988;
Fukada et al, 1996; Bacchiani and Ostendorf, 1999;
Paliwal, 1990). In these previous approaches, spo-
ken samples of a word are gathered, and usually
only one single pronunciation for the word is de-
rived based on the acoustic evidence observed in the
spoken samples. The major difference between our
work and these previous works is that our model
learns word pronunciations in the context of letter
sequences. More specifically, our model learns letter
pronunciations first and then concatenates the pro-
nunciation of each letter in a word to form the word
pronunciation. The advantage of our approach is
that pronunciation knowledge learned for a particu-
lar letter in some arbitrary word can subsequently be
used to help learn the letter?s pronunciation in other
words. This property allows our model to potentially
learn better pronunciations for less frequent words.
The more recent work by Garcia and Gish (2006)
and Siu et al (2013) has made extensive use
of self-organizing units for keyword spotting and
other tasks for languages with limited linguistic
resources. Others who have more recently ex-
plored the unsupervised space include (Varadarajan
et al, 2008; Jansen and Church, 2011; Lee and
Glass, 2012). The latter work introduced a non-
parametric Bayesian inference procedure for auto-
matically learning acoustic units that is most similar
to our current work except that our model also infers
word pronunciations simultaneously.
The concept of creating a speech recognizer for
a language with only orthographically annotated
speech data has also been explored previously by
means of graphemes. This approach has been shown
to be effective for alphabetic languages with rela-
tively straightforward grapheme to phoneme trans-
formations and does not require any unsupervised
learning of units or pronunciations (Killer et al,
2003; Stu?ker and Schultz, 2004). As we explain in
later sections, grapheme-based systems can actually
be regarded as a special case of our model; therefore,
we expect our model to have greater flexibilities for
capturing pronunciation rules of graphemes.
3 Model
The goal of our model is to induce a word pronunci-
ation lexicon from spoken utterances and their cor-
responding word transcriptions. No other language-
specific knowledge is assumed to be available, in-
cluding the phonetic inventory of the language. To
achieve the goal, our model needs to solve the fol-
lowing two tasks:
? Discover the phonetic inventory.
? Reveal the latent mapping between the letters
and the discovered phonetic units.
We propose a hierarchical Bayesian model for
jointly discovering the two latent structures from
an annotated speech corpus. Before presenting our
model, we first describe the key latent and observed
variables of the problem.
Letter (lmi ) We use l
m
i to denote the i
th let-
ter observed in the word transcription of the
mth training sample. To be sure, a train-
ing sample involves a speech utterance and its
183
corresponding text transcription. The letter se-
quence composed of lmi and its context, namely
lmi??, ? ? ? , l
m
i?1, l
m
i , l
m
i+1, ? ? ? , l
m
i+?, is denoted as ~l
m
i,?.
Although lmi is referred to as a letter in this paper,
it can represent any character observed in the text
data, including space and symbols indicating sen-
tence boundaries. The set of unique characters ob-
served in the data set is denoted as G. For notation
simplicity, we use L? to denote the set of letter se-
quences of length 2? + 1 that appear in the dataset
and use ~l? to denote the elements in L?. Finally,
P(~l?) is used to represent the parent of ~l?, which is
a substring of ~l? with the first and the last characters
truncated.
Number of Mapped Acoustic Units (nmi ) Each
letter lmi in the transcriptions is assumed to be
mapped to a certain number of phonetic units. For
example, the letter x in the word fox is mapped to
2 phonetic units /k/ and /s/, while the letter e in the
word lake is mapped to 0 phonetic units. We denote
this number as nmi and limit its value to be 0, 1 or 2
in our model. The value of nmi is always unobserved
and needs to be inferred by the our model.
Identity of the Acoustic Unit (cmi,p) For each pho-
netic unit that lmi maps to, we use c
m
i,p, for 1 ? p ?
nmi , to denote the identity of the phonetic unit. Note
that the phonetic inventory that describes the data
set is unknown to our model, and the identities of
the phonetic units are associated with the acoustic
units discovered automatically by our model.
Speech Feature xmt The observed speech data in
our problem are converted to a series of 25 ms 13-
dimensional MFCCs (Davis and Mermelstein, 1980)
and their first- and second-order time derivatives at
a 10 ms analysis rate. We use xmt ? R
39 to denote
the tth feature frame of the mth utterance.
3.1 Generative Process
We present the generative process for a single train-
ing sample (i.e., a speech utterance and its corre-
sponding text transcription); to keep notation sim-
ple, we discard the index variable m in this section.
For each li in the transcription, the model gener-
ates ni, given ~li,?, from the 3-dimensional categori-
cal distribution ?~li,?(ni). Note that for every unique
~li,? letter sequence, there is an associated ?~li,?(ni)
lj 
  1?  p ? ni 
?0 
ci, p 
?0 
K
?c 
di,p  
? 
1 ? i ? Lm 
ni 
xt 
1 ? m ? M 
?l2,n,p 
? ? 
?l,n,p 
G ?{(n,p) | 0 ? n ? 2, 1 ? p ? n} 
?l1,n,p 
G ?G 
G ?G 
?1 
?2 
i-2 ? j ? i+2 
Figure 1: The graphical representation of the pro-
posed hierarchical Bayesian model. The shaded cir-
cle denotes the observed text and speech data, and
the squares denote the hyperparameters of the priors
in our model. See Sec. 3 for a detailed explanation
of the generative process of our model.
distribution, which captures the fact that the number
of phonetic units a letter maps to may depend on its
context. In our model, we impose a Dirichlet distri-
bution prior Dir(?) on ?~li,?(ni).
If ni = 0, li is not mapped to any acoustic units
and the generative process stops for li; otherwise,
for 1 ? p ? ni, the model generates ci,p from:
ci,p ? pi~li,?,ni,p (1)
where pi~li,?,ni,p is a K-dimensional categorical dis-
tribution, whose outcomes correspond to the pho-
netic units discovered by the model from the given
speech data. Eq. 1 shows that for each combination
of~li,?, ni and p, there is an unique categorical distri-
bution. An important property of these categorical
distributions is that they are coupled together such
that their outcomes point to a consistent set of pho-
netic units. In order to enforce the coupling, we con-
struct pi~li,?,ni,p through a hierarchical process.
? ? Dir(?) (2)
pi~li,?,ni,p ? Dir(???) for ? = 0 (3)
pi~li,?,ni,p ? Dir(??pi~li,??1,ni,p) for ? ? 1 (4)
184
To interpret Eq. 2 to Eq. 4, we envision that
the observed speech data are generated by a K-
component mixture model, of which the components
correspond to the phonetic units in the language. As
a result, ? in Eq. 2 can be viewed as the mixture
weight over the components, which indicates how
likely we are to observe each acoustic unit in the
data overall. By adopting this point of view, we
can also regard the mapping between li and the pho-
netic units as a mixture model, and pili,ni,p
1 repre-
sents how probable li is mapped to each phonetic
unit given ni and p. We apply a Dirichlet distribu-
tion prior parametrized by ?0? to pili,ni,p as shown
in Eq. 3. With this parameterization, the mean of
pili,ni,p is the global mixture weight ?, and ?0 con-
trols how similar pili,ni,p is to the mean. More specif-
ically, for large ?0  K, the Dirichlet distribution
is highly peaked around the mean; on the contrary,
for ?0  K, the mean lies in a valley. The parame-
ters of a Dirichlet distribution can also be viewed as
pseudo-counts for each category. Eq. 4 shows that
the prior for pi~li,?,ni,p is seeded by pseudo-counts
that are proportional to the mapping weights over
the phonetic units of li in a shorter context. In other
words, the mapping distribution of li in a shorter
context can be thought of as a back-off distribution
of li?s mapping weights in a longer context.
Each component of the K-dimensional mixture
model is linked to a 3-state Hidden Markov Model
(HMM). These K HMMs are used to model the
phonetic units in the language (Jelinek, 1976). The
emission probability of each HMM state is modeled
by a diagonal Gaussian Mixture Model (GMM). We
use ?c to represent the set of parameters that define
the cth HMM, which includes the state transition
probability and the GMM parameters of each state
emission distribution. The conjugate prior of ?c is
denoted as H(?0)2.
Finally, to finish the generative process, for each
ci,p we use the corresponding HMM ?ci,p to gen-
erate the observed speech data xt, and the genera-
tive process of the HMM determines the duration,
1An abbreviation of pi~li,0,ni,p
2H(?0) includes a Dirichlet prior for the transition probabil-
ity of each state, and a Dirichlet prior for each mixture weight
of the three GMMs, and a normal-Gamma distribution for the
mean and precision of each Gaussian mixture in the 3-state
HMM.
di,p, of the speech segment. The complete genera-
tive model, with ? set to 2, is depicted in Fig. 1; M
is the total number of transcribed utterances in the
corpus, and Lm is the number of letters in utterance
m. The shaded circles denote the observed data, and
the squares denote the hyperparameters of the priors
used in our model. Lastly, the unshaded circles de-
note the latent variables of our model, for which we
derive inference algorithms in the next section.
4 Inference
We employ Gibbs sampling (Gelman et al, 2004) to
approximate the posterior distribution of the latent
variables in our model. In the following sections, we
first present a message-passing algorithm for block-
sampling ni and ci,p, and then describe how we
leverage acoustic cues to accelerate the computa-
tion of the message-passing algorithm. Note that the
block-sampling algorithm for ni and ci,p can be par-
allelized across utterances. Finally, we briefly dis-
cuss the inference procedures for ?~l? , pi~l?,n,p, ?, ?c.
4.1 Block-sampling ni and ci,p
To understand the message-passing algorithm in this
study, it is helpful to think of our model as a sim-
plified Hidden Semi-Markov Model (HSMM), in
which the letters represent the states and the speech
features are the observations. However, unlike in
a regular HSMM, where the state sequence is hid-
den, in our case, the state sequence is fixed to be the
given letter sequence. With this point of view, we
can modify the message-passing algorithms of Mur-
phy (2002) and Johnson and Willsky (2013) to com-
pute the posterior information required for block-
sampling ni and ci,p.
Let L(xt) be a function that returns the index
of the letter from which xt is generated; also, let
Ft = 1 be a tag indicating that a new phone segment
starts at t+ 1. Given the constraint that 0 ? ni ? 2,
for 0 ? i ? Lm and 0 ? t ? Tm, the backwards
messages Bt(i) and B?t (i) for the m
th training sam-
ple can be defined and computed as in Eq. 5 and
Eq. 7. Note that for clarity we discard the index vari-
able m in the derivation of the algorithm.
185
Bt(i) , p(xt+1:T |L(xt) = i, Ft = 1)
=
min{L,i+1+U}?
j=i+1
B?t (j)
j?1?
k=i+1
p(nk = 0|~li,?)
=
min{L,i+1+U}?
j=i+1
B?t (j)
j?1?
k=i+1
?~li,?(0) (5)
B?t (i) , p(xt+1:T |L(xt+1) = i, Ft = 1)
=
T?t?
d=1
p(xt+1:t+d|~li,?)Bt+d(i) (6)
=
T?t?
d=1
{
K?
ci,1=1
?~li,?(1)pi~li,?,1,1(ci,1)p(xt+1:t+d|?ci,1)
+
d?1?
v=1
K?
ci,1
K?
ci,2
?~li,?(2)pi~li,?,2,1(ci,1)pi~li,?,2,2(ci,2)
? p(xt+1:t+v|?ci,1)p(xt+v+1:t+d|?ci,2)}Bt+d(i)
(7)
We use xt1:t2 to denote the segment consisting of
xt1 , ? ? ? , xt2 . Our inference algorithm only allows
up to U letters to emit 0 acoustic units in a row. The
value of U is set to 2 for our experiments. Bt(i)
represents the total probability of all possible align-
ments between xt+1:T and li+1:L. B?t (i) contains
the probability of all the alignments between xt+1:T
and li+1:L that map xt+1 to li particularly. This
alignment constraint between xt+1 and li is explic-
itly shown in the first term of Eq. 6, which represents
how likely the speech segment xt+1:t+d is generated
by li given li?s context. This likelihood is simply
the marginal probability of p(xt+1:t+d, ni, ci,p|~li,?)
with ni and ci,p integrated out, which can be ex-
panded and computed as shown in the last three rows
of Eq. 7. The index v specifies where the phone
boundary is between the two acoustic units that li
is aligned with when ni = 2. Eq. 8 to Eq. 10 are
the boundary conditions of the message passing al-
gorithm. B0(0) carries the total probably of all pos-
sible alignments between l1:L and x1:T . Eq. 9 spec-
ifies that at most U letters at the end of an sentence
can be left unaligned with any speech features, while
Eq. 10 indicates that all of the speech features in an
utterance must be assigned to a letter.
Algorithm 1 Block-sample ni and ci,p from Bt(i)
and B?t (i)
1: i? 0
2: t? 0
3: while i < L ? t < T do
4: nexti ? SampleFromBt(i)
5: if nexti > i+ 1 then
6: for k = i+ 1 to k = nexti ? 1 do
7: nk ? 0
8: end for
9: end if
10: d, ni, ?ci,p?, v ? SampleFromB?t (nexti)
11: t? t+ d
12: i? nexti
13: end while
B0(0) =
min{L,U+1}?
j=1
B?0(j)
j?1?
k=1
?~li,?(0) (8)
BT (i) ,
?
??
??
1 if i = L
?L
j=i+1 ?~li,?(0) if L? U ? i < L
0 if i < L? U
(9)
Bt(L) ,
{
1 if t = T
0 otherwise
(10)
Given Bt(i) and B?t (i), ni and ci,p for each letter
in the utterance can be sampled using Alg. 1. The
SampleFromBt(i) function in line 4 returns a ran-
dom sample from the relative probability distribu-
tion composed by entries of the summation in Eq. 5.
Line 5 to line 9 check whether li (and maybe li+1)
is mapped to zero phonetic units. nexti points to
the letter that needs to be aligned with 1 or 2 phone
segments starting from xt. The number of phonetic
units that lnexti maps to and the identities of the
units are sampled in SampleFromB?t (i). This sub-
routine generates a tuple of d, ni, ?ci,p? as well as
v (if ni = 2) from all the entries of the summation
shown in Eq. 73.
3We use ?ci,p? to denote that ?ci,p?may consist of two num-
bers, ci,1 and ci,2, when ni = 2.
186
4.2 Heuristic Phone Boundary Elimination
The variables d and v in Eq. 7 enumerate through
every frame index in a sentence, treating each fea-
ture frame as a potential boundary between acous-
tic units. However, it is possible to exploit acoustic
cues to avoid checking feature frames that are un-
likely to be phonetic boundaries. We follow the pre-
segmentation method described in Glass (2003) to
skip roughly 80% of the feature frames and greatly
speed up the computation of B?t (i).
Another heuristic applied to our algorithm to re-
duce the search space for d and v is based on the
observation that the average duration of phonetic
units is usually no longer than 300 ms. Therefore,
when computing B?t (i), we only consider speech
segments that are shorter than 300 ms to avoid align-
ing letters to speech segments that are too long to be
phonetic units.
4.3 Sampling ?~l? , pi~l?,ni,p, ? and ?c
Sampling ?~l? To compute the posterior distribu-
tion of ?~l? , we count how many times
~l? is mapped
to 0, 1 and 2 phonetic units from nmi . More specifi-
cally, we define N~l?(j) for 0 ? j ? 2 as follows:
N~l?(j) =
M?
m=1
Lm?
i=1
?(nmi , j)?(~l
m
i,?,~l?)
where we use ?(?) to denote the discrete Kronecker
delta. With N~l? , we can simply sample a new value
for ?~l? from the following distribution:
?~l? ? Dir(? +N~l?)
Sampling pi~l?,n,p and ? The posterior distribu-
tions of pi~l?,n,p and ? are constructed recursively due
to the hierarchical structure imposed on pi~l?,n,p and
?. We start with gathering counts for updating the
pi variables at the lowest level, i.e., pi~l2,n,p given that
? is set to 2 in our model implementation, and then
sample pseudo-counts for the pi variables at higher
hierarchies as well as ?. With the pseudo-counts, a
new ? can be generated, which allows pi~l?,n,p to be
re-sampled sequentially.
More specifically, we define C~l2,n,p(k) to be the
number of times that ~l2 is mapped to n units and
the unit in position p is the kth phonetic unit. This
value can be counted from the current values of cmi,p
as follows.
C~l2,n,p(k) =
M?
m=1
Lm?
i=1
?(~li,2,~l2)?(n
m
i , n)?(c
m
i,p, k)
To derive the posterior distribution of pi~l1,n,p an-
alytically, we need to sample pseudo-counts C~l1,n,p,
which is defined as follows.
C~l1,n,p(k) =
?
~l2?U~l1
C~l2,n,p
(k)
?
i=1
I[?i <
?2pi~l1,n,p(k)
i+ ?2pi~l1,n,p(k)
]
(11)
We use U~l1 = {
~l2|P(~l2) = ~l1} to denote the set of
~l2 whose parent is~l1 and ?i to represent random vari-
ables sampled from a uniform distribution between
0 and 1. Eq. 11 can be applied recursively to com-
pute C~l0,n,p(k) and C ,n,p(k), the pseudo-counts that
are applied to the conjugate priors of pi~l0,n,p and ?.
With the pseudo-count variables computed, new val-
ues for ? and pi~l?,n,p can be sampled sequentially as
shown in Eq. 12 to Eq. 14.
? ? Dir(? + C ,n,p) (12)
pi~l?,n,p ? Dir(??? + C~l?,n,p) for ? = 0 (13)
pi~l?,n,p ? Dir(??pi~l??1,n,p + C~l?,n,p) for ? ? 1
(14)
5 Experimental Setup
To test the effectiveness of our model for joint learn-
ing phonetic units and word pronunciations from an
annotated speech corpus, we construct speech rec-
ognizers out of the training results of our model.
The performance of the recognizers is evaluated and
compared against three baselines: first, a grapheme-
based speech recognizer; second, a recognizer built
by using an expert-crafted lexicon, which is referred
to as an expert lexicon in the rest of the paper for
simplicity; and third, a recognizer built by discover-
ing the phonetic units and L2S pronunciation rules
sequentially without using a lexicon. In this section,
we provide a detailed description of the experimen-
tal setup.
187
? ? ?0 ?1 ?2 ?0 ? K
?0.1?3 ?10?100 1 0.1 0.2 * 2 100
Table 1: The values of the hyperparameters of our
model. We use ?a?D to denote aD-dimensional vec-
tor with all entries being a. *We follow the proce-
dure reported in (Lee and Glass, 2012) to set up the
HMM prior ?0.
5.1 Dataset
All the speech recognition experiments reported in
this paper are performed on a weather query dataset,
which consists of narrow-band, conversational tele-
phone speech (Zue et al, 2000). We follow the ex-
perimental setup of McGraw et al (2013) and split
the corpus into a training set of 87,351 utterances, a
dev set of 1,179 utterances and a test set of 3,497 ut-
terances. A subset of 10,000 utterances is randomly
selected from the training set. We use this subset of
data for training our model to demonstrate that our
model is able to discover the phonetic composition
and the pronunciation rules of a language even from
just a few hours of data.
5.2 Building a Recognizer from Our Model
The values of the hyperparameters of our model are
listed in Table 1. We run the inference procedure de-
scribed in Sec. 4 for 10,000 times on the randomly
selected 10,000 utterances. The samples of ?~l? and
pi~l?n,p from the last iteration are used to decode n
m
i
and cmi,p for each sentence in the entire training set by
following the block-sampling algorithm described
in Sec. 4.1. Since cmi,p is the phonetic mapping of
lmi , by concatenating the phonetic mapping of ev-
ery letter in a word, we can obtain a pronunciation
of the word represented in the labels of discovered
phonetic units. For example, assume that word w
appears in sentence m and consists of l3l4l5 (the
sentence index m is ignored for simplicity). Also,
assume that after decoding, n3 = 1, n4 = 2 and
n5 = 1. A pronunciation ofw is then encoded by the
sequence of phonetic labels c3,1c4,1c4,2c5,1. By re-
peating this process for each word in every sentence
for the training set, a list of word pronunciations can
be compiled and used as a stochastic lexicon to build
a speech recognizer.
In theory, the HMMs inferred by our model can be
directly used as the acoustic model of a monophone
speech recognizer. However, if we regard the ci,p
labels of each utterance as the phone transcription
of the sentence, then a new acoustic model can be
easily re-trained on the entire data set. More conve-
niently, the phone boundaries corresponding to the
ci,p labels are the by-products of the block-sampling
algorithm, which are indicated by the values of d and
v in line 10 of Alg. 1 and can be easily saved during
the sampling procedure. Since these data are readily
available, we re-build a context-independent model
on the entire data set. In this new acoustic model,
a 3-state HMM is used to model each phonetic unit,
and the emission probability of each state is modeled
by a 32-mixture GMM.
Finally, a trigram language model is built by using
the word transcriptions in the full training set. This
language model is utilized in all speech recogni-
tion experiments reported in this paper. Finite State
Transducers (FSTs) are used to build all the recog-
nizers used in this study. With the language model,
the lexicon and the context-independent acoustic
model constructed by the methods described in this
section, we can build a speech recognizer from
the learning output of the proposed model without
the need of a pre-defined phone inventory and any
expert-crafted lexicons.
5.2.1 Pronunciation Mixture Model Retraining
McGraw et al (2013) presented the Pronuncia-
tion Mixture Model (PMM) for composing stochas-
tic lexicons that outperform pronunciation dictionar-
ies created by experts. Although the PMM frame-
work was designed to incorporate and augment ex-
pert lexicons, we found that it can be adapted to pol-
ish the pronunciation list generated by our model.
In particular, the training procedure for PMMs in-
cludes three steps. First, train a L2S model from
a manually specified expert-pronunciation lexicon;
second, generate a list of pronunciations for each
word in the dataset using the L2S model; and finally,
use an acoustic model to re-weight the pronuncia-
tions based on the acoustic scores of the spoken ex-
amples of each word.
To adapt this procedure for our purposes, we sim-
ply plug in the word pronunciations and the acous-
tic model generated by our model. Once we ob-
tain the re-weighted lexicon, we re-generate forced
188
phone alignments and retrain the acoustic model,
which can be utilized to repeat the PMM lexicon re-
weighting procedure. For our experiments, we it-
erate through this model refining process until the
recognition performance converges.
5.2.2 Triphone Model
Conventionally, to train a context-dependent
acoustic model, a list of questions based on the
linguistic properties of phonetic units is required
for growing decision tree classifiers (Young et al,
1994). However, such language-specific knowledge
is not available for our training framework; there-
fore, our strategy is to compile a question list that
treats each phonetic unit as a unique linguistic class.
In other words, our approach to training a context-
dependent acoustic model for the automatically dis-
covered units is to let the decision trees grow fully
based on acoustic evidence.
5.3 Baselines
We compare the recognizers trained by following
the procedures described in Sec. 5.2 against three
baselines. The first baseline is a grapheme-based
speech recognizer. We follow the procedure de-
scribed in Killer et al (2003) and train a 3-state
HMM for each grapheme, which we refer to as the
monophone grapheme model. Furthermore, we cre-
ate a singleton question set (Killer et al, 2003), in
which each grapheme is listed as a question, to train
a triphone grapheme model. Note that to enforce
better initial alignments between the graphemes and
the speech data, we use a pre-trained acoustic model
to identify the non-speech segments at the beginning
and the end of each utterance before starting training
the monophone grapheme model.
Our model jointly discovers the phonetic inven-
tory and the L2S mapping rules from a set of tran-
scribed data. An alternative of our approach is to
learn the two latent structures sequentially. We fol-
low the training procedure of Lee and Glass (2012)
to learn a set of acoustic models from the speech
data and use these acoustic models to generate a
phone transcription for each utterance. The phone
transcriptions along with the corresponding word
transcriptions are fed as inputs to the L2S model
proposed in Bisani and Ney (2008). A stochastic
lexicon can be learned by applying the L2S model
unit(%) Monophone
Our model 17.0
Oracle 13.8
Grapheme 32.7
Sequential model 31.4
Table 2: Word error rates generated by the four
monophone recognizers described in Sec. 5.2 and
Sec. 5.3 on the weather query corpus.
and the discovered acoustic models to PMM. This
two-stage approach for training a speech recognizer
without an expert lexicon is referred to as the se-
quential model in this paper.
Finally, we compare our system against a rec-
ognizer trained from an oracle recognition system.
We build the oracle recognizer on the same weather
query corpus by following the procedure presented
in McGraw et al (2013). This oracle recognizer is
then applied to generate forced-aligned phone tran-
scriptions for the training utterances, from which
we can build both monophone and triphone acous-
tic models. The expert-crafted lexicon used in the
oracle recognizer is also used in this baseline. Note
that for training the triphone model, we compose a
singleton question list (Killer et al, 2003) that has
every expert-defined phonetic unit as a question. We
use this singleton question list instead of a more so-
phisticated one to ensure that this baseline and our
system differ only in the acoustic model and the lex-
icon used to generate the initial phone transcriptions.
We call this baseline the oracle baseline.
6 Results and Analysis
6.1 Monophone Systems
Table 2 shows the WERs produced by the four
monophone recognizers described in Sec. 5.2 and
Sec. 5.3. It can be seen that our model outper-
forms the grapheme and the sequential model base-
lines significantly while approaching the perfor-
mance of the supervised oracle baseline. The im-
provement over the sequential baseline demonstrates
the strength of the proposed joint learning frame-
work. More specifically, unlike the sequential base-
line, in which the acoustic units are discovered in-
dependently from the text data, our model is able to
exploit the L2S mapping constraints provided by the
word transcriptions to cluster speech segments.
189
By comparing our model to the grapheme base-
line, we can see the advantage of modeling the
pronunciations of a letter using a mixture model,
especially for a language like English which has
many pronunciation irregularities. However, even
for languages with straightforward pronunciation
rules, the concept of modeling letter pronunciations
using mixture models still applies. The main dif-
ference is that the mixture weights for letters of
languages with simple pronunciation rules will be
sparser and spikier. In other words, in theory, our
model should always perform comparable to, if not
better than, grapheme recognizers.
Last but not least, the recognizer trained with the
automatically induced lexicon performs similarly to
the recognizer initialized by an oracle recognition
system, which demonstrates the effectiveness of the
proposed model for discovering the phonetic inven-
tory and a pronunciation lexicon from an annotated
corpus. In the next section, we provide some in-
sights into the quality of the learned lexicon and
into what could have caused the performance gap
between our model and the conventionally trained
recognizer.
6.2 Pronunciation Entropy
The major difference between the recognizer that is
trained by using our model and the recognizer that
is seeded by an oracle recognition system is that
the former uses an automatically discovered lexicon,
while the latter exploits an expert-defined pronun-
ciation dictionary. In order to quantify, as well as
to gain insights into, the difference between these
two lexicons, we define the average pronunciation
entropy, H? , of a lexicon as follows.
H? ?
?1
|V |
?
w?V
?
b?B(w)
p(b) log p(b) (15)
where V denotes the vocabulary of a lexicon, B(w)
represents the set of pronunciations of a word w
and p(b) stands for the weight of a certain pronun-
ciation b. Intuitively, we can regard H? as an in-
dicator of how much pronunciation variation that
each word in a lexicon has on average. Table 3
shows that the H? values of the lexicon induced by
our model and the expert-defined lexicon as well as
Our model PMM iterations
(Discovered lexicon) 0 1 2
H? (bit) 4.58 3.47 3.03
WER (%) 17.0 16.6 15.9
Oracle PMM iterations
(Expert lexicon) 0 1 2
H? (bit) 0.69 0.90 0.92
WER (%) 13.8 12.8 12.4
Table 3: The upper-half of the table shows the aver-
age pronunciation entropies, H? , of the lexicons in-
duced by our model and refined by PMM as well
as the WERs of the monophone recognizers built
with the corresponding lexicons for the weather
query corpus. The definition of H? can be found in
Sec. 6.2. The first row of the lower-half of the ta-
ble lists the average pronunciation entropies, H? , of
the expert-defined lexicon and the lexicons gener-
ated and weighted by the L2P-PMM framework de-
scribed in McGraw et al (2013). The second row of
the lower-half of the table shows the WERs of the
recognizers that are trained with the expert-lexicon
and its PMM-refined versions.
their respective PMM-refined versions4. In Table 3,
we can see that the automatically-discovered lexi-
con and its PMM-reweighted versions have much
higher H? values than their expert-defined counter-
parts. These higher H? values imply that the lexicon
induced by our model contains more pronunciation
variation than the expert-defined lexicon. Therefore,
the lattices constructed during the decoding process
for our recognizer tend to be larger than those con-
structed for the oracle baseline, which explains the
performance gap between the two systems in Table 2
and Table 3.
As shown in Table 3, even though the lexicon
induced by our model is noisier than the expert-
defined dictionary, the PMM retraining framework
consistently refines the induced lexicon and im-
proves the performance of the recognizers5. To the
best of our knowledge, we are the first to apply
PMM to lexicons that are created by a fully unsu-
4We build the PMM-refined version of the expert-defined
lexicon by following the L2P-PMM framework described
in McGraw et al (2013).
5The recognition results all converge in 2 ? 3 PMM retrain-
ing iterations.
190
pronunciations
pronunciation probabilities
Our model 1 PMM 2 PMM
93 56 87 39 19 0.125 - -
93 56 61 87 73 99 0.125 - -
11 56 61 87 73 99 0.125 0.400 0.419
93 20 75 87 17 27 52 0.125 0.125 0.124
55 93 56 61 87 73 84 19 0.125 0.220 0.210
93 26 61 87 49 0.125 0.128 0.140
63 83 86 87 73 53 19 0.125 - -
93 26 61 87 61 0.125 0.127 0.107
Table 4: Pronunciation lists of the word Burma pro-
duced by our model and refined by PMM after 1 and
2 iterations.
pervised method. Therefore, in this paper, we pro-
vide further analysis on how PMM helps enhance
the performance of our model.
We compare the pronunciation lists for the word
Burma generated by our model and refined itera-
tively by PMM in Table 4. The first column of Ta-
ble 4 shows all the pronunciations of Burma dis-
covered by our model, to which our model assigns
equal probabilities to create a stochastic list6. As
demonstrated in the third and the fourth columns of
Table 4, the PMM framework is able to iteratively
re-distribute the pronunciation weights and filter out
less-likely pronunciations, which effectively reduces
both the size and the entropy of the stochastic lexi-
con generated by our model. The benefits of using
the PMM to refine the induced lexicon are twofold.
First, the search space constructed during the recog-
nition decoding process with the refined lexicon is
more constrained, which is the main reason why the
PMM is capable of improving the performance of
the monophone recognizer that is trained with the
output of our model. Secondly, and more impor-
tantly, the refined lexicon can greatly reduce the size
of the FST built for the triphone recognizer of our
model. These two observations illustrate why the
PMM framework can be an useful tool for enhancing
the lexicon discovered automatically by our model.
6.3 Triphone Systems
The best monophone systems of the grapheme base-
line, the oracle baseline and our model are used to
6It is also possible to assign probabilities proportional to the
decoding scores of the word tokens.
Unit(%) Triphone
Our model 13.4
Oracle 10.0
Grapheme 15.7
Table 5: Word error rates of the triphone recogniz-
ers. The triphone recognizers are all built by us-
ing the phone transcriptions generated by their best
monohpone system. For the oracle initialized base-
line and for our model, the PMM-refined lexicons
are used to build the triphone recognizers.
generate forced-aligned phone transcriptions, which
are used to train the triphone models described in
Sec. 5.2.2 and Sec. 5.3. Table 5 shows the WERs
of the triphone recognition systems. Note that if a
more conventional question list, for example, a list
that contains rules to classify phones into different
broad classes, is used to build the oracle triphone
system, the WER can be reduced to 6.5%. However,
as mentioned earlier, in order to gain insights into
the quality of the induced lexicon and the discovered
phonetic set, we compare our model against an ora-
cle triphone system that is built by using a singleton
question set.
By comparing Table 2 and Table 5, we can see
that the grapheme triphone improves by a large mar-
gin compared to its monophone counterpart, which
is consistent with the results reported in (Killer et
al., 2003). However, even though the grapheme
baseline achieves a great performance gain with
context-dependent acoustic models, the recognizer
trained using the lexicon learned by our model and
subsequently refined by PMM still outperforms the
grapheme baseline. The consistently better perfor-
mance our model achieves over the grapheme base-
line demonstrates the strength of modeling the pro-
nunciation of each letter with a mixture model that
is presented in this paper.
Last but not least, by comparing Table 2 and
Table 5, it can be seen that the relative perfor-
mance gain achieved by our model is similar to
that obtained by the oracle baseline. Both Table 2
and Table 5 show that even without exploiting any
language-specific knowledge during training, our
recognizer is able to perform comparably with the
recognizer trained using an expert lexicon. The abil-
ity of our model to obtain such similar performance
191
further supports the effectiveness of the joint learn-
ing framework proposed in this paper for discover-
ing the phonetic inventory and the word pronuncia-
tions from simply an annotated speech corpus.
7 Conclusion
We present a hierarchical Bayesian model for si-
multaneously discovering acoustic units and learn-
ing word pronunciations from transcribed spoken ut-
terances. Both monophone and triphone recogniz-
ers can be built on the discovered acoustic units and
the inferred lexicon. The recognizers trained with
the proposed unsupervised method consistently out-
performs grapheme-based recognizers and approach
the performance of recognizers trained with expert-
defined lexicons. In the future, we plan to apply this
technology to develop ASRs for more languages.
Acknowledgements
The authors would like to thank Ian McGraw and
Ekapol Chuangsuwanich for their advice on the
PMM and recognition experiments presented in this
paper. Thanks to the anonymous reviewers for help-
ful comments. Finally, the authors would like to
thank Stephen Shum for proofreading and editing
the early drafts of this paper.
References
Michiel Bacchiani and Mari Ostendorf. 1999. Joint lexi-
con, acoustic unit inventory and model design. Speech
Communication, 29:99 ? 114.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451, May.
Steven B. Davis and Paul Mermelstein. 1980. Com-
parison of parametric representations for monosyllabic
word recognition in continuously spoken sentences.
IEEE Trans. on Acoustics, Speech, and Signal Pro-
cessing, 28(4):357?366.
Toshiaki Fukada, Michiel Bacchiani, Kuldip Paliwal, and
Yoshinori Sagisaka. 1996. Speech recognition based
on acoustically derived segment units. In Proceedings
of ICSLP, pages 1077 ? 1080.
Alvin Garcia and Herbert Gish. 2006. Keyword spotting
of arbitrary words using minimal speech resources. In
Proceedings of ICASSP, pages 949?952.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-
ald B. Rubin. 2004. Bayesian Data Analysis. Texts
in Statistical Science. Chapman & Hall/CRC, second
edition.
James Glass. 2003. A probabilistic framework for
segment-based speech recognition. Computer Speech
and Language, 17:137 ? 152.
Aren Jansen and Kenneth Church. 2011. Towards un-
supervised training of speaker independent acoustic
models. In Proceedings of INTERSPEECH, pages
1693 ? 1696.
Frederick Jelinek. 1976. Continuous speech recogni-
tion by statistical methods. Proceedings of the IEEE,
64:532 ? 556.
Matthew J. Johnson and Alan S. Willsky. 2013. Bayesian
nonparametric hidden semi-markov models. Journal
of Machine Learning Research, 14:673?701, February.
Mirjam Killer, Sebastian Stu?ker, and Tanja Schultz.
2003. Grapheme based speech recognition. In Pro-
ceeding of the Eurospeech, pages 3141?3144.
Chia-ying Lee and James Glass. 2012. A nonparamet-
ric Bayesian approach to acoustic model discovery. In
Proceedings of ACL, pages 40?49.
Chin-Hui Lee, Frank Soong, and Biing-Hwang Juang.
1988. A segment model based approach to speech
recognition. In Proceedings of ICASSP, pages 501?
504.
Ian McGraw, Ibrahim Badr, and James Glass. 2013.
Learning lexicons from speech using a pronunciation
mixture model. IEEE Trans. on Speech and Audio
Processing, 21(2):357?366.
Kevin P. Murphy. 2002. Hidden semi-Markov mod-
els (hsmms). Technical report, University of British
Columbia.
Kuldip Paliwal. 1990. Lexicon-building methods for an
acoustic sub-word based speech recognizer. In Pro-
ceedings of ICASSP, pages 729?732.
Man-hung Siu, Herbert Gish, Arthur Chan, William
Belfield, and Steve Lowe. 2013. Unsupervised train-
ing of an HMM-based self-organizing unit recgonizer
with applications to topic classification and keyword
discovery. Computer, Speech, and Language.
Sebastian Stu?ker and Tanja Schultz. 2004. A grapheme
based speech recognition system for Russian. In Pro-
ceedings of the 9th Conference Speech and Computer.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-
manuel Dupoux. 2008. Unsupervised learning of
acoustic sub-word units. In Proceedings of ACL-08:
HLT, Short Papers, pages 165?168.
Steve J. Young, J.J. Odell, and Philip C. Woodland. 1994.
Tree-based state tying for high accuracy acoustic mod-
elling. In Proceedings of HLT, pages 307?312.
Victor Zue, Stephanie Seneff, James Glass, Joseph Po-
lifroni, Christine Pao, Timothy J. Hazen, and Lee Het-
herington. 2000. Jupiter: A telephone-based con-
versational interface for weather information. IEEE
Trans. on Speech and Audio Processing, 8:85?96.
192
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1?40,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
CoNLL-2012 Shared Task:
Modeling Multilingual Unrestricted Coreference in OntoNotes
Sameer Pradhan
Raytheon BBN Technologies,
Cambridge, MA 02138
USA
pradhan@bbn.com
Alessandro Moschitti
University of Trento,
38123 Povo (TN)
Italy
moschitti@disi.unitn.it
Nianwen Xue
Brandeis University,
Waltham, MA 02453
USA
xuen@cs.brandeis.edu
Olga Uryupina
University of Trento,
38123 Povo (TN)
Italy
uryupina@gmail.com
Yuchen Zhang
Brandeis University,
Waltham, MA 02453
USA
yuchenz@brandeis.edu
Abstract
The CoNLL-2012 shared task involved pre-
dicting coreference in English, Chinese, and
Arabic, using the final version, v5.0, of the
OntoNotes corpus. It was a follow-on to the
English-only task organized in 2011. Un-
til the creation of the OntoNotes corpus, re-
sources in this sub-field of language process-
ing were limited to noun phrase coreference,
often on a restricted set of entities, such as
the ACE entities. OntoNotes provides a large-
scale corpus of general anaphoric coreference
not restricted to noun phrases or to a spec-
ified set of entity types, and covers multi-
ple languages. OntoNotes also provides ad-
ditional layers of integrated annotation, cap-
turing additional shallow semantic structure.
This paper describes the OntoNotes annota-
tion (coreference and other layers) and then
describes the parameters of the shared task in-
cluding the format, pre-processing informa-
tion, evaluation criteria, and presents and dis-
cusses the results achieved by the participat-
ing systems. The task of coreference has
had a complex evaluation history. Potentially
many evaluation conditions, have, in the past,
made it difficult to judge the improvement in
new algorithms over previously reported re-
sults. Having a standard test set and stan-
dard evaluation parameters, all based on a re-
source that provides multiple integrated anno-
tation layers (syntactic parses, semantic roles,
word senses, named entities and coreference)
and in multiple languages could support joint
modeling and help ground and energize on-
going research in the task of entity and event
coreference.
1 Introduction
The importance of coreference resolution for the
entity/event detection task, namely identifying all
mentions of entities and events in text and clustering
them into equivalence classes, has been well recog-
nized in the natural language processing community.
Early work on corpus-based coreference resolu-
tion dates back to the mid-90s by McCarthy and
Lenhert (1995) where they experimented with deci-
sion trees and hand-written rules. Corpora to support
supervised learning of this task date back to the Mes-
sage Understanding Conferences (MUC) (Hirschman
and Chinchor, 1997; Chinchor, 2001; Chinchor and
Sundheim, 2003). The de facto standard datasets
for current coreference studies are the MUC and the
ACE1 (Doddington et al, 2004) corpora. These cor-
pora were tagged with coreferring entities in the
form of noun phrases in the text. The MUC corpora
cover all noun phrases in text but are relatively small
in size. The ACE corpora, on the other hand, cover
much more data, but the annotation is restricted to a
small subset of entities.
Automatic identification of coreferring entities
and events in text has been an uphill battle for sev-
eral decades, partly because it is a problem that re-
quires world knowledge to solve and word knowl-
edge is hard to define, and partly owing to the lack
of substantial annotated data. Aside from the fact
that resolving coreference in text is simply a very
hard problem, there have been other hindrances that
further contributed to the slow progress in this area:
(i) Smaller sized corpora such as MUC which cov-
ered coreference across all noun phrases. Cor-
pora such as ACE which are larger in size, but
cover a smaller set of entities; and
(ii) low consistency in existing corpora annotated
with coreference ? in terms of inter-annotator
agreement (ITA) (Hirschman et al, 1998) ?
owing to attempts at covering multiple coref-
erence phenomena that are not equally anno-
tatable with high agreement which likely less-
ened the reliability of statistical evidence in the
form of lexical coverage and semantic related-
ness that could be derived from the data and
1http://projects.ldc.upenn.edu/ace/data/
1
used by a classifier to generate better predic-
tive models. The importance of a well-defined
tagging scheme and consistent ITA has been
well recognized and studied in the past (Poe-
sio, 2004; Poesio and Artstein, 2005; Passon-
neau, 2004). There is a growing consensus that
in order to take language understanding appli-
cations such as question answering or distilla-
tion to the next level, we need more consistent
annotation for larger amounts of broad cover-
age data to train better automatic models for
entity and event detection.
(iii) Complex evaluation with multiple evaluation
metrics and multiple evaluation scenarios,
complicated with varying training and test
partitions, led to situations where many re-
searchers report results with only one or a few
of the available metrics and under a subset of
evaluation scenarios. This has made it hard to
gauge the improvement in algorithms over the
years (Stoyanov et al, 2009), or to determine
which particular areas require further attention.
Looking at various numbers reported in litera-
ture can greatly affect the perceived difficulty
of the task. It can seem to be a very hard prob-
lem (Soon et al, 2001) or one that is relatively
easy (Culotta et al, 2007).
(iv) the knowledge bottleneck which has been a
well-accepted ceiling that has kept the progress
in this task at bay.
These issues suggest that the following steps
might take the community in the right direction to-
wards improving the state of the art in coreference
resolution:
(i) Create a large corpus with high inter-
annotator agreement possibly by restricting
the coreference annotating to phenomena that
can be annotated with high consistency, and
covering an unrestricted set of entities and
events; and
(ii) Create a standard evaluation scenario with an
official evaluation setup, and possibly several
ablation settings to capture the range of perfor-
mance. This can then be used as a standard
benchmark by the research community.
(iii) Continue to improve learning algorithms that
better incorporate world knowledge and jointly
incorporate information from other layers of
syntactic and semantic annotation to improve
the state of the art.
One of the many goals of the OntoNotes
project2 (Hovy et al, 2006; Weischedel et al, 2011)
2http://www.bbn.com/nlp/ontonotes
was to explore whether it could fill this void and help
push the progress further ? not only in coreference,
but with the various layers of semantics that it tries
to capture. As one of its layers, it has created a
corpus for general anaphoric coreference that cov-
ers entities and events not limited to noun phrases
or a subset of entity types. The coreference layer
in OntoNotes constitutes just one part of a multi-
layered, integrated annotation of shallow semantic
structures in text with high inter-annotator agree-
ment. This addresses the first issue.
In the language processing community, the field
of speech recognition probably has the longest his-
tory of shared evaluations held primary by NIST3
(Pallett, 2002). In the past decade machine trans-
lation has been a topic of shared evaluations also
by NIST4. There are many syntactic and semantic
processing tasks that are not quite amenable to such
continued evaluation efforts. The CoNLL shared
tasks over the past 15 years have filled that gap, help-
ing establish benchmarks and advance the state of
the art in various sub-fields within NLP. The impor-
tance of shared tasks is now in full display in the
domain of clinical NLP (Chapman et al, 2011) and
recently a coreference task was organized as part
of the i2b2 workshop (Uzuner et al, 2012). The
computational learning community is also witness-
ing a shift towards joint inference based evaluations,
with the two previous CoNLL tasks (Surdeanu et al,
2008; Hajic? et al, 2009) devoted to joint learning of
syntactic and semantic dependencies. A SemEval-
2010 coreference task (Recasens et al, 2010) was
the first attempt to address the second issue. It
included six different Indo-European languages ?
Catalan, Dutch, English, German, Italian, and Span-
ish. Among other corpora, a small subset (?120K)
of English portion of OntoNotes was used for this
purpose. However, the lack of a strong participa-
tion prevented the organizers from reaching any firm
conclusions. The CoNLL-2011 shared task was an-
other attempt to address the second issue. It was well
received, but the shared task was only limited to the
English portion of OntoNotes. In addition, the coref-
erence portion of OntoNotes did not have a concrete
baseline prior to the 2011 evaluation, thereby mak-
ing it challenging for participants to gauge the per-
formance of their algorithms in the absence of es-
tablished state of the art on this flavor of annotation.
The closest comparison was to the results reported
by Pradhan et al (2007b) on the newswire portion of
OntoNotes. Since the corpus also covers two other
languages from completely different language fami-
lies, Chinese and Arabic, it provided a great oppor-
tunity to have a follow-on task in 2012 covering all
3http://www.itl.nist.gov/iad/mig/publications/ASRhistory/index.html
4http://www.itl.nist.gov/iad/mig/tests/mt/
2
three languages. As we will see later, peculiarities
of each of these languages had to be considered in
creating the evaluation framework.
The first systematic learning-based study in coref-
erence resolution was conducted on the MUC cor-
pora, using a decision tree learner, by Soon et al
(2001). Significant improvements have been made
in the field of language processing in general, and
improved learning techniques have pushed the state
of the art in coreference resolution forward (Mor-
ton, 2000; Harabagiu et al, 2001; McCallum and
Wellner, 2004; Culotta et al, 2007; Denis and
Baldridge, 2007; Rahman and Ng, 2009; Haghighi
and Klein, 2010). Researchers have continued to
find novel ways of exploiting ontologies such as
WordNet. Various knowledge sources from shallow
semantics to encyclopedic knowledge have been ex-
ploited (Ponzetto and Strube, 2005; Ponzetto and
Strube, 2006; Versley, 2007; Ng, 2007). Given
that WordNet is a static ontology and as such has
limitation on coverage, more recently, there have
been successful attempts to utilize information from
much larger, collaboratively built resources such as
Wikipedia (Ponzetto and Strube, 2006). More re-
cently researchers have used graph based algorithms
(Cai et al, 2011a) rather than pair-wise classifica-
tions. For a detailed survey of the progress in this
field, we refer the reader to a recent article (Ng,
2010) and a tutorial (Ponzetto and Poesio, 2009)
dedicated to this subject. In spite of all the progress,
current techniques still rely primarily on surface
level features such as string match, proximity, and
edit distance; syntactic features such as apposition;
and shallow semantic features such as number, gen-
der, named entities, semantic class, Hobbs? distance,
etc. Further research to reduce the knowledge gap is
essential to take coreference resolution techniques to
the next level.
The rest of the paper is organized as follows: Sec-
tion 2 presents an overview of the OntoNotes cor-
pus. Section 3 describes the range of phenomena
annotated in OntoNotes, and language-specific is-
sues. Section 4 describes the shared task data and
the evaluation parameters, with Section 4.4.2 exam-
ining the performance of the state-of-the-art tools
on all/most intermediate layers of annotation. Sec-
tion 5 describes the participants in the task. Sec-
tion 6 briefly compares the approaches taken by var-
ious participating systems. Section 7 presents the
system results with some analysis. Section 8 com-
pares the performance of the systems on the a subset
of the Engish test set that corresponds with the test
set used for the CoNLL-2011 evaluation. Section 9
draws some conclusions.
2 The OntoNotes Corpus
The OntoNotes project has created a large-scale
corpus of accurate and integrated annotation of mul-
tiple levels of the shallow semantic structure in text.
The English and Chinese language portion com-
prises roughly one million words per language of
newswire, magazine articles, broadcast news, broad-
cast conversations, web data and conversational
speech data. The English subcorpus also contains
an additional 200K words of the English translation
of the New Testament as Pivot Text. The Arabic por-
tion is smaller, comprising 300K words of newswire
articles. The hope is that this rich, integrated an-
notation covering many layers will allow for richer,
cross-layer models and enable significantly better
automatic semantic analysis. In addition to coref-
erence, this data is also tagged with syntactic trees,
propositions for most verb and some noun instances,
partial verb and noun word senses, and 18 named en-
tity types. Manual annotation of a large corpus with
multiple layers of syntax and semantic information
is a costly endeavor. Over the years in the devel-
opment of this corpus, there were various priorities
that came into play, and therefore not all the data in
the corpus could be annotated with all the different
layers of annotation. However, such multi-layer an-
notations, with complex, cross-layer dependencies,
demands a robust, efficient, scalable storage mech-
anism while providing efficient, convenient, inte-
grated access to the the underlying structure. To
this effect, it uses a relational database representa-
tion that captures both the inter- and intra-layer de-
pendencies and also provides an object-oriented API
for efficient, multi-tiered access to this data (Prad-
han et al, 2007a). This facilitates the extraction of
cross-layer features in integrated predictive models
that will make use of these annotations.
OntoNotes comprises the following layers of an-
notation:
? Syntax ? A layer of syntactic annotation for
English, Chinese and Arabic based on a revised
guidelines for the Penn Treebank (Marcus et
al., 1993; Babko-Malaya et al, 2006), the Chi-
nese Treebank (Xue et al, 2005) and the Arabic
Treebank (Maamouri and Bies, 2004).
? Propositions ? The proposition structure of
verbs based on revised guidelines for the En-
glish PropBank (Palmer et al, 2005; Babko-
Malaya et al, 2006), the Chinese PropBank
(Xue and Palmer, 2009) and the Arabic Prop-
Bank (Palmer et al, 2008; Zaghouani et al,
2010).
? Word Sense ? Coarse-grained word senses
are tagged for the most frequent polysemous
verbs and nouns, in order to maximize token
3
coverage. The word sense granularity is tai-
lored to achieve 90% inter-annotator agreement
as demonstrated by Palmer et al (2007). These
senses are defined in the sense inventory files.
In case of English and Arabic languages, the
sense-inventories (and frame files) are defined
separately for each part of speech that is real-
ized by the lemma in the text. For Chinese,
however the sense inventories (and frame files)
are defined per lemma ? independent of the
part of speech realized in the text. For the
English portion of OntoNotes, each individual
sense has been connected to multiple WordNet
senses. This provides users direct access to the
WordNet semantic structure. There is also a
mapping from the OntoNotes word senses to
PropBank frames and to VerbNet (Kipper et
al., 2000) and FrameNet (Fillmore et al, 2003).
Unfortunately, owing to lack of comparable re-
sources as comprehensive as WordNet in Chi-
nese or Arabic, neither language has any inter-
resource mappings available.
? Named Entities ? The corpus was tagged
with a set of 18 well-defined proper named en-
tity types that have been tested extensively for
inter-annotator agreement by Weischedel and
Burnstein (2005).
? Coreference ? This layer captures general
anaphoric coreference that covers entities and
events not limited to noun phrases or a lim-
ited set of entity types (Pradhan et al, 2007b).
It considers all pronouns (PRP, PRP$), noun
phrases (NP) and heads of verb phrases (VP)
as potential mentions. Unlike English, Chinese
and Arabic have dropped subjects and objects
which were also considered during coreference
annotation5. We will take a look at this in detail
in the next section.
3 Coreference in OntoNotes
General anaphoric coreference that spans a rich
set of entities and events ? not restricted to a few
types, as has been characteristic of most coreference
data available until now ? has been tagged with a
high degree of consistency in the OntoNotes corpus.
Two different types of coreference are distinguished:
Identity (IDENT), and Appositive (APPOS). Identity
coreference (IDENT) is used for anaphoric corefer-
ence, meaning links between pronominal, nominal,
and named mentions of specific referents. It does not
include mentions of generic, underspecified, or ab-
stract entities. Appositives (APPOS) are treated sep-
arately because they function as attributions, as de-
scribed further below. Coreference is annotated for
all specific entities and events. There is no limit on
5As we will see later these are not used during the task.
the semantic types of NP entities that can be consid-
ered for coreference, and in particular, coreference
is not limited to ACE types. The guidelines are fairly
language independent. We will look at some salient
aspects of the coreference annotation in OntoNotes.
For more details, and examples, we refer the reader
to the release documentation. We will primarily use
English examples to describe various aspects of the
annotation and use Chinese and Arabic examples es-
pecially to illustrate phenomena not observed in En-
glish, or that have some language specific peculiari-
ties.
3.1 Noun Phrases
The mentions over which IDENT coreference ap-
plies are typically pronominal, named, or definite
nominal. The annotation process begins by automat-
ically extracting all of the NP mentions from parse
trees in the syntactic layer of OntoNotes annotation,
though the annotators can also add additional men-
tions when appropriate. In the following two exam-
ples (and later ones), the phrases in bold form the
links of an IDENT chain.
(1) She had a good suggestion and it was unani-
mously accepted by all.
(2) Elco Industries Inc. said it expects net income
in the year ending June 30, 1990, to fall below a
recent analyst?s estimate of $ 1.65 a share. The
Rockford, Ill. maker of fasteners also said it
expects to post sales in the current fiscal year
that are ?slightly above? fiscal 1989 sales of $
155 million.
Noun phrases (NPs) in Chinese can be complex
noun phrases or bare nouns (nouns that lack a de-
terminer such as ?the? or ?this?). Complex noun
phrases contain structures modifying the head noun,
as in the following examples:
(3) (??????????? ? (???
????? (???))).
((His last APEC (summit meeting)) as the
President)
(4) (?? ?? ? (?? ? ?? ?? ?? ?
(????)))
((The first (U.S. president)) who went to visit
Vietnam after its unification)
In these examples, the smallest phrase in paren-
theses is the bare noun. The longer phrase in paran-
theses includes modifying structures. All the expres-
sions in the parantheses, however, share the same
head noun, i.e., ???? (summit meeting)?, and
????? (U.S. president)? respectively. Nested
noun phrases, or nested NPs, are contained within
4
longer noun phrases. In the above example, ?sum-
mit meeting? and ?U.S. president? are nested NPs.
Wherever NPs are nested, the largest logical span is
used in coreference.
3.2 Verbs
Verbs are added as single-word spans if they can
be coreferenced with a noun phrase or with another
verb. The intent is to annotate the VP, but the single-
word verb head is marked for convenience. This
includes morphologically related nominalizations as
in (5) and noun phrases that refer to the same event,
even if they are lexically distinct from the verb as in
(6). In the following two examples, only the chains
related to the growth event are shown in bold. The
Arabic translation of the same example identifies
mentions using parantheses.
(5) The European economy grew rapidly over the
past years, this growth helped raising ....
H@?
	
J??@ ?C
	
g

??Q??. ?

G
.
?P?

B@ XA?

J

?B

@ ( A? 	? ) Y??
. . . ?
	
P? ?


	
? ??A? ( ??	J? @ @ 	Y? ) , ?J

	
?A?? @
(6) Japan?s domestic sales of cars, trucks and buses
in October rose 18% from a year earlier to
500,004 units, a record for the month, the Japan
Automobile Dealers? Association said. The
strong growth followed year-to-year increases
of 21% in August and 12% in September.
3.3 Pronouns
All pronouns and demonstratives are linked to
anything that they refer to, and pronouns in quoted
speech are also marked. Expletive or pleonastic pro-
nouns (it, there) are not considered for tagging, and
generic you is not marked. In the following exam-
ple, the pronoun you and it would not be marked. (In
this and following examples, an asterisk (*) before a
boldface phrase identifies entity/event mentions that
would not be tagged in the coreference annotation.)
(7) Senate majority leader Bill Frist likes to tell
a story from his days as a pioneering heart
surgeon back in Tennessee. A lot of times,
Frist recalls, *you?d have a critical patient ly-
ing there waiting for a new heart, and *you?d
want to cut, but *you couldn?t start unless *you
knew that the replacement heart would make
*it to the operating room.
In Chinese, all the following pronouns ? ??
???, ?, ?? ????????????
?, ?, ?? (you, me, he, she, and so on), and
demonstrative pronouns ?????????,?
? (this, that, these, those) in singular, plural or pos-
sessive forms are linked to anything they refer to.
Pronouns from classical Chinese such as ? ?
(among which),? (he/she/it),? (he/she/it) are also
linked with other mentions to which they refer.
In Arabic, the following pronouns are corefer-
enced ? nominative personal pronouns (subject) and
demonstrative pronouns which are detached. Sub-
ject pronouns are often null in Arabic; overt subject
pronouns are rare, but do occur.
	?
	
K @ / ?

?
	
K @ / A?

J
	
K @ / 	?m