Event-based Information Extraction for the biomedical domain: the Caderige project 
 
Erick Alphonse**, Sophie Aubin*, Philippe Bessi?res**, Gilles Bisson****, Thierry Hamon*, 
Sandrine Lagarrigue***, Adeline Nazarenko*, Alain-Pierre Manine**, Claire N?dellec**, 
Mohamed Ould Abdel Vetah**, Thierry Poibeau*, Davy Weissenbacher* 
 
*Laboratoire d?Informatique de Paris-Nord  
CNRS UMR 7030 
Av. J.B. Cl?ment 93430 F-Villetaneuse 
{firstname.lastname}@lipn.univ-paris13.fr 
**Laboratoire Math?matique, Informatique et G?nome (MIG), 
INRA,  
Domaine de Vilvert, 78352 F-Jouy-en-Josas 
{firstname.lastname}@jouy.inra.fr 
***Laboratoire de G?n?tique Animale,  
INRA-ENSAR 
Route de Saint Brieuc, 35042 Rennes Cedex 
lagarrig@roazhon.inra.fr 
****Laboratoire Leibniz ? UMR CNRS 5522  
46 Avenue F?lix Viallet - 38031 F-Grenoble Cedex 
Gilles.Bisson@imag.fr 
 
 Abstract  
This paper gives an overview of the 
Caderige project. This project involves 
teams from different areas (biology, 
machine learning, natural language 
processing) in order to develop high-
level analysis tools for extracting 
structured information from biological 
bibliographical databases, especially 
Medline. The paper gives an overview 
of the approach and compares it to the 
state of the art.  
1 Introduction 
Developments in biology and biomedicine are 
reported in large bibliographical databases 
either focused on a specific species (e.g. 
Flybase, specialized on Drosophilia 
Menogaster) or not (e.g. Medline). This type 
of  information sources is crucial for biologists 
but there is a lack of tools to explore them and 
extract relevant information. While recent 
named entity recognition tools have gained a 
certain success on these domains, event-based 
Information Extraction (IE) is still a challenge.  
The Caderige project aims at designing and 
integrating Natural Language Processing 
(NLP) and Machine Learning (ML) techniques 
to explore, analyze and extract targeted 
information in biological textual databases. We 
promote a corpus-based approach focusing on 
text pre-analysis and normalization: it is 
intended to drain out the linguistic variation 
dimension, as most as possible. Actually, the 
MUC (1995) conferences have demonstrated 
that extraction is more efficient when 
performed on normalized texts. The extraction 
patterns are thus easier to acquire or learn, 
more abstract and easier to maintain 
Beyond extraction patterns, it is also possible 
to acquire from the corpus, via ML methods, a 
part of the knowledge necessary for text 
normalization as shown here.  
This paper gives an overview of current 
research activities and achievements of the 
Caderige project. The paper first presents our 
approach and compares it with the one 
developed in the framework of a similar 
project called Genia (Collier et al 1999). We 
then propose an account of Caderige 
techniques on various filtering and 
normalization tasks, namely, sentence filtering, 
resolution of named entity synonymy, 
syntactic parsing, and ontology learning. 
Finally, we show how extraction patterns can 
be learned from normalized and annotated 
documents, all applied to biological texts.  
2 Description of our approach 
In this section, we give some details about the 
motivations and choices of implementation. 
We then briefly compare our approach with the 
one of the Genia project. 
43
2.1 Project organization 
The Caderige project is a multi disciplinary 
French research project on the automatic 
mining of textual data from the biomedical 
domain and is mainly exploratory orientated. It 
involved biology teams (INRA), computer 
science teams (LIPN, INRA and Leibniz-
IMAG) and NLP teams (LIPN) as major 
partners, plus LRI and INRIA from 2000 to 
2003. 
2.2 Project motivations 
Biologists can search bibliographic databases 
via the Internet, using keyword queries that 
retrieve a large superset of relevant papers. 
Alternatively, they can navigate through 
hyperlinks between genome databanks and 
referenced papers. To extract the requisite 
knowledge from the retrieved papers, they 
must identify the relevant abstracts or 
paragraphs. Such manual processing is time 
consuming and repetitive, because of the 
bibliography size, the relevant data sparseness, 
and the database continuous updating. From 
the Medline database, the focused query 
?Bacillus subtilis and transcription? which 
returned 2,209 abstracts in 2002, retrieves 
2,693 of them today. We chose this example 
because Bacillus subtilis is a model bacterium 
and transcription is a central phenomenon in 
functional genomics involved in genic 
interaction, a popular IE problem. 
GerE stimulates cotD transcription and 
inhibits cotA transcription in vitro by 
sigma K RNA polymerase, as expected from 
in vivo studies, and, unexpectedly, 
profoundly inhibits in vitro 
transcription of the gene (sigK) that 
encode sigma K. 
Figure 1: A sentence describing a genic interaction 
Once relevant abstracts have been retrieved, 
templates should be filled by hand since there 
is no available IE tool operational in genomics  
Type: positive 
Agent: GerE 
 
Interaction 
Target: transcription of the 
gene sigK 
Figure 2: A template describing a genic 
interaction. 
Still, applying IE ? la MUC to genomics and 
more generally to biology is not an easy task 
because IE systems require deep analysis 
methods to locate relevant fragments. As 
shown in the example in Figures 1 and 2, 
retrieving that GerE is the agent of the 
inhibition of the transcription of the gene sigK 
requires at least syntactic dependency analysis 
and coordination processing. In most of the 
genomics IE tasks (function, localization, 
homology) the methods should then combine 
the semantic-conceptual analysis of text 
understanding methods with IE through pattern 
matching. 
2.3 Comparison with the Genia project 
Our approach is very close to the one of the 
Genia project (Collier et al, 1999). Both 
projects rely on precise high-level linguistic 
analysis to be able to perform IE. The kind of 
information being searched is similar, 
concerning mainly gene and protein interaction 
as most of the research in this domain. The 
Genia corpus (Ohtae et al 2001) is not 
specialized on a specific species whereas ours 
is based on Bacillus Subtilis.  
Both projects develop annotation tools and 
Document Type Definition (DTD), which are, 
for the most part, compatible. The aim here is 
to build training corpus to which various 
techniques of NLP and ML are applied in 
order to acquire efficient event-based 
extraction patterns. The choice of ML and 
NLP methods differs but their aim is similar to 
our: normalizing text with predicate-arguments 
structures for learning better patterns. For 
example, Genia uses a combination of parsers 
to finally perform an HPSG-like analysis. The 
Caderige syntactic analysis is based on the 
specialization of the Link Parser (Sleator and 
Temperley, 1993 see section 4) to the 
biological domain.  
 In the following two sections, we detail our 
text filtering and normalization methods. 
Filtering aims at pruning the irrelevant part of 
the corpus while normalization aims at 
building an abstract representation of the 
relevant text. Section 4 is devoted to the 
acquisition of extraction patterns from the 
filtered and normalized text. 
3 Text filtering 
IR and text filtering are a prerequisite step to 
IE, as IE methods (including normalization and 
learning) cannot be applied to large and 
irrelevant corpora (they are not robust enough 
and they are computationally expensive). IR 
here is done through Medline interface by 
keyword queries for filtering the appropriate 
44
document subset. Then, text filtering, reduces 
the variability of textual data with the 
following assumptions: 
? desired information is local to sentences ; 
? relevant sentences contain at least two gene 
names. 
These hypotheses may lead to miss some genic 
interactions, but we assume that information 
redundancy is such that at least one instance of 
each interaction is contained into a single 
sentence in the corpus. The documents 
retrieved are thus segmented into sentences 
and the sentences with at least two gene names 
are selected. 
To identify the only relevant sentences among 
thoses,  classical supervised ML methods have 
been applied to a Bacillus Subtilis corpus in 
which relevant and irrelevant sentences had 
been annotated by a biological expert. Among 
SVMs, Na?ve Bayes (NB) methods, Neural 
Networks, decision trees (Marcotte et al, 
2001;  Nedellec et al, 2001), (Nedellec et al 
2001) demonstrates that  simple NB methods 
coupled with feature selection seem to perform 
well by yielding around 85 % precision and 
recall. Moreover, our first experiments show 
that the linguistic-based representation changes 
such as the use of lemmatization, terminology 
and named entities, do not lead to significant 
improvements. The relevant sentences filtered 
at this step are then used as input of the next 
tasks, normalization and IE. 
4 Normalization 
This section briefly presents three text 
normalization tasks: normalization of entity 
names, normalization of relations between text 
elements through syntactic dependency parsing 
and semantic labeling. The normalization 
process, by providing an abstract 
representation of the sentences, allows the 
identification of regularities that simplify the 
acquisition or learning of pattern rules. 
4.1 Entity names normalization 
Named Entity recognition is a critical point in 
biological text analysis, and a lot of work was 
previously done to detect gene names in text 
(Proux and al., 1998), (Fukuda and al., 1998). 
So, in Caderige, we do not develop any 
original NE extraction tool. We focus on a less 
studied problem that is synonyms recognition.  
Beyond typographical variations and 
abbreviations, biological entities often have 
several different names. Synonymy of gene 
names is a well-known problem, partly due to 
the huge amount of data manipulated (43.238 
references registered in Flybase for 
Drosophilia Melanogaster for example). Genes 
are often given a temporary name by a 
biologist. This name is then changed according 
to information on the concerned gene: for 
example SYGP-ORF50 is a gene name 
temporarily attributed by a sequencing project 
to the PMD1 yeast gene. We have shown that, 
in addition to available data in genomic 
database (GenBank, SwissProt,?), it is 
possible to acquire many synonymy relations 
with good precision through text analysis. By 
focusing on synonymy trigger phrases such as 
"also called" or "formerly", we can extract text 
fragments of that type :  gene trigger gene. 
However, the triggers themselves are subject to 
variation and the arguments of the synonymy 
relation must be precisely identified. We have 
shown that it is possible to define patterns to 
recognize synonymy expressions. These 
patterns have been trained on a representative 
set of sentences from Medline and then tested 
on a new corpus made of 106 sentences 
containing the keyword formerly. Results on 
the test corpus are the following: 97.5% 
precision, 75% recall. We chose to have a high 
precision since the acquired information must 
be valid for further acquisition steps 
(Weissenbacher, 2004).  
The approach that has been developed is very 
modular since abstract patterns like gene 
trigger gene (the trigger being a linguistic 
marker or a simple punctuation) can be 
instantiated by various linguistic items. A 
score can be computed for each instantiation of 
the pattern, during a learning phase on a large 
representative corpus. The use of a reduced 
tagged corpus and of a large untagged corpus 
justify the use of semi-supervised learning 
techniques.  
4.2  Sentence parsing 
The extraction of structured information from 
texts requires precise sentence parsing tools 
that exhibit relevant relation between domain 
entities. Contrary to (Akane et al 2001), we 
chose a partial parsing approach: the analysis 
is focused on relevant parts of texts and, from 
these chunks, on specific relations. Several 
reasons motivate this choice: among others, the 
fact that relevant information generally appears 
in predefined syntactic patterns and, moreover, 
45
the fact that we want to learn domain 
knowledge ontologies from specific syntactic 
relations (Faure and Nedellec, 2000 ; Bisson et 
al. 2000). 
First experiments have been done on several 
shallow parsers. It appeared that constituent 
based parsers are efficient to segment the text 
in syntactic phrases but fail to extract relevant 
functional relationships betweens phrases. 
Dependency grammars are more adequate 
since they try to establish links between heads 
of syntactic phrases. In addition, as described 
in Schneider (1998), dependency grammars are 
looser on word order, which is an advantage 
when working on  a domain specific language.  
Two dependency-based syntactic parsers have 
been tested (Aubin 2003): a hybrid commercial 
parser (henceforth HCP) that combines 
constituent and dependency analysis, and a 
pure dependency analyzer: the Link Parser.   
Prasad and Sarkar (2000) promote a twofold 
evaluation for parsers: on the one hand the use 
of a representative corpus and, on the other 
hand, the use of specific manually elaborated 
sentences. The idea is to evaluate analyzers on 
real data (corpus evaluation) and then to check 
the performance on specific syntactic 
phenomena. In this experiment, we chose to 
have only one corpus, made of sentences 
selected from the Medline corpus depending 
on their syntactic particularity. This strategy 
ensures representative results on real data. 
A set of syntactic relations was then selected 
and manually evaluated. This led to the results 
presented for major relations only in table 1. 
For each analyzer and relation, we compute a 
recall and precision score (recall = # relevant 
found relations / # relations to be found; 
precision = # relevant found relations / # 
relations found by the system).  
The Link Parser generally obtains better results 
than HCP. One reason is that a major 
particularity of our corpus (Medline abstracts) 
is that sentences are often (very) long (27 
words on average) and contain several clauses. 
The dependency analyzer is more accurate to 
identify relevant relationships between 
headwords whereas the constituent parser is 
lost in the sentence complexity. We finally 
opted for the Link Parser. Another advantage 
of the Link Parser is the possibility to modify 
its set of rules (see next subsection). The Link 
parser is currently used in INRA to extract 
syntactic relationships from texts in order to 
learn domain ontologies on the basis of a 
distributional analysis (Harris 1951, Faure and 
N?dellec, 1999).  
4.3 Recycling a general parser for biology 
During the evaluation tests, we noticed that 
some changes had to be applied either to the 
parser or to the text itself to improve the 
syntactic analysis of our biomedical corpus. 
The corpus needs to be preprocessed: sentence 
segmentation, named entities and terms 
recognition are thus performed using generic 
modules tuned for the biology domain
1
. Term 
recognition allows the removing of numerous 
structure ambiguities, which clearly benefits 
the parsing quality and execution time.  
                                                     
1
 A term analyser is currently being built at LIPN 
using existing term resources like Gene Ontology 
(see Hamon and Aubin, 2004). 
  Link Parser HCP 
Rel nbRel relOK R. RelTot P. RelOK R RelTot P. 
Subject 
18 13 0.72 19 0.68 14 0.78 20 0.65 
Object 
18 16 0.89 17 0.94 9 0.5 13 0.69 
Prep 
48 25 0.52 55 0.45 20 0.42 49 0.41 
V-GP1 
14 13 0.93 15 0.87 9 0.64 23 0.39 
O-GP 
16 7 0.43 12 0.58 12 0.75 28 0.43 
NofN 
16 13 0.81 15 0.87 14 0.87 26 0.54 
VtoV 
10 9 0.9 9 1 7 0.7 7 1 
VcooV 
10 8 0.8 9 0.89 6 0.6 6 1 
NcooN 
10 8 0.7 10 0.8 4 0.4 6 0.67 
nV-Adj 
10 8 0.8 9 0.89 0 0 0 1 
PaSim 
18 17 0.94 18 0.94 17 0.94 22 0.77 
PaRel 
12 11 0.92 11 1 8 0.67 11 0.73 
Table 1: Evaluation of two parsers on various syntactic relations 
Relations meaning: subject = subject-verb, Object = verb-object, Prep = prepositional phrase, V-GP = verb-prep. 
phrase, O-GP = Object- prep. phrase, NofN = Noun of noun, VtoV = Verb to Verb, VcooV = Verb coord. Verb, 
NcooN = Noun coord. Noun, nV-Adj = not + Verb or adjective, PaSim = passive form, PaRel = passive relative 
46
Concerning the Link Parser, we have manually 
introduced new rules and lexicon to allow the 
parsing of syntactic structures specific to the 
domain. For instance, the Latin-derived Noun 
Adjective phrase "Bacillus subtilis" has a 
structure inverse to the canonical English noun 
phrase (Adjective Noun). Another major task 
was to loosen the rules constraints because 
Medline abstracts are written by biologists 
who express themselves in sometimes broken 
English. A typical error is the omission of the 
determinant before some nouns that require 
one. We finally added words unknown to the 
original parser. 
4.4 Semantic labelling 
Asium software is used to semi-automatically 
acquire relevant semantic categories by 
distributional semantic analysis of parsed 
corpus. These categories contribute to text 
normalization at two levels, disambiguating 
syntactic parsing and typing entities and 
actions for IE. Asium is based on an original 
ascendant hierarchical clustering method that 
builds a hierarchy of semantic classes from the 
syntactic dependencies parsed in the training 
corpus. Manual validation is required in order 
to distinguish between different meanings 
expressed by identical syntactic structures. 
5 Extraction pattern learning 
Extraction pattern learning requires a training 
corpus from which the relevant and 
discriminant regularities can be automatically 
identified. This relies on two processes: text 
normalization that is domain-oriented but not 
task-oriented (as described in previous 
sections), and task-oriented annotation by the 
expert of the task.  
5.1 Annotation procedure 
The Caderige annotation language is based on 
XML and a specific DTD (Document Type 
Definition that can be used to annotate both 
prokaryote and eukaryote organisms by 50 
tags with up to 8 attributes. Such a precision is 
required for learning feasibility and extraction 
efficiency. Practically, each annotation aims at 
highlighting the set of words in the sentence 
describing: 
? Agents (A): the entities activating or 
controlling the interaction 
? Targets (T): the entities that are produced 
or controlled 
? Interaction (I): the kind of control 
performed during the interaction 
? Confidence (C): the confidence level in this 
interaction. 
The annotation of ?A low level of GerE 
activated transcription of CotD by GerE RNA 
polymerase in vitro ...? is given below. The 
attributes associated to the tag <GENIC-
INTERACTION> express the fact that the 
interaction is a transcriptional activation and 
that it is certain. The other tags (<IF>, 
<AF1>, ?) mark  the agent (AF1 and AF2), the 
target (TF1) and the interaction (IF). 
 
<GENIC-INTERACTION 
 id=?1?  
 type=?transcriptional?  
 assertion=?exist?  
 regulation=?activate?  
 uncertainty=?certain?  
 self-contained=?yes?  
 text-clarity=?good?> 
  <IF>A<I> low level </I>of</IF>     
  <AF1><A1  
     type=protein  
        role=modulate  
        direct=yes> GerE 
  </A1></AF1>,  
  <IF><I>activated</I> transcription  
      of</IF>    
     <TF1><T1 type=protein> CotD </T1>           
         </TF1> by   
     <AF2><A2  
           type=protein  
         role=required> 
       GerE RNA polymerase 
   </A2></AF2>,  
   <CF>but<C>in vitro</C></CF> 
</GENIC-INTERACTION> 
5.2 The annotation editor2 
Annotations cannot be processed in text form 
by biologists. The annotation framework 
developed by Caderige provide a general XML 
editor with a graphic interface for creating, 
checking and revising annotated documents. 
For instance, it displays the text with graphic 
attributes as defined in the editor XML style 
sheet, it allows to add the tags without strong 
constraint on the insertion order and it 
automatically performs some checking. 
The editor interface is composed of four main 
parts (see Figure 3). The editable text zone for 
annotation, the list of XML tags that can be 
used at a given time, the attributes zone to edit 
the values of the selected tag, and the XML 
                                                     
2
 Contact one of the authors if you are interested to 
use this annotation tool in a research project 
47
code currently generated. In the text zone, the 
above sentence is displayed as follows: 
A low level of GerE activated 
transcription of CotD by GerE RNA 
polymerase but in vitro 
This editor is currently used by some of the 
Caderige project partners and at SIB (Swiss 
Institute of BioInformatics) with another DTD, 
in the framework of the European BioMint 
project. Several corpora on various species 
have been annotated using this tool, mainly by 
biologists from INRA.  
5.3 Learning 
The vast majority of approaches relies on 
hand-written pattern rules that are based on 
shallow representations of the sentences (e.g. 
Ono et al, 2001). In Caderige, the deep 
analysis methods increase the complexity of 
the sentence representation, and thus of the IE 
patterns. ML techniques appear therefore very 
appealing to automate the process of rule 
acquisition (Freitag, 1998; Califf et al, 1998; 
Craven et al, 1999).  
Learning IE rules is seen as a discrimination  
task, where the concept to learn is a n-ary 
relation between arguments which correspond 
to the template fields. For example, the 
template in figure 2 can be filled by learning a 
ternary relation genic-interaction(X,Y,Z), 
where X,Y and Z are the type, the agent and 
the target of the interaction. The learning 
algorithm is provided with a set of positive and 
negative examples built from the sentences 
annotated and normalized. We use the 
relational learning algorithm, Propal (Alphonse 
et al, 2000). The appeal of using a relational 
method for this task is that it can naturally 
represent the relational structure of the 
syntactic dependencies in the normalized 
sentences and the background knowledge if 
needed, such as for instance semantic relations.  
For instance, the IE rules learned by Propal 
extract, from the following sentence :"In this 
mutant, expression of the spoIIG gene, whose 
transcription depends on both sigA and the 
phosphorylated Spo0A protein, Spo0AP, a 
major transcription factor during early stages 
of sporulation, was greatly reduced at 43 
degrees C.", successfully extract the two 
relations genic-interaction(positive, sigA, 
spoIIG) and genic-interaction(positive, 
Spo0AP, spoIIG). As preliminary experiments, 
we selected a subset of sentences as learning 
dataset, similar to this one. The performance of 
the learner evaluated by ten-fold cross-
validation is 69?6.5% of recall and 86?3.2% 
of precision. This result is encouraging, 
showing that the normalization process 
provides a good representation for learning IE 
rules with both high recall and high precision. 
6 Conclusion 
We have presented in this paper some results 
from the Caderige project. Two major issues 
are the development of a specific annotation 
editor for domain specialists and a set of 
machine learning and linguistic processing 
tools tuned for the biomedical domain.  
Current developments focus on the use of 
learning methods in the extraction process. 
These methods are introduced at different 
levels in the system architecture. A first use is 
Figure 3: the Caderige annotation editor 
48
the acquisition of domain knowledge to 
enhance the extraction phase. A second use 
concerns a dynamic adaptation of existing 
modules during the analysis according to 
specific features in a text or to specific text 
genres.  
7 References 
E. Agichtein and H. Yu (2003). Extracting 
synonymous gene and protein terms from 
biological literature. Bioinformatics, vol. 19 
Suppl.1, Oxford Press. 
E. Alphonse and C. Rouveirol (2000). Lazy 
propositionalisation for Relational  
Learning. In 14th European Conference on 
Artificial Intelligence (ECAI?00, W. Horn ed.), 
Berlin, pp. 256-260.  
S. Aubin (2003). ?valuation comparative de deux 
analyseurs produisant des relations syntaxiques. 
In workshop TALN and multilinguism. Batz-sur-
Mer. 
Y. Akane, Y. Tateisi, Y. Miyao and J. Tsujii. 
(2001). Event extraction from biomedical papers 
using a full parser. In Proceedings of the sixth 
Pacific Symposium on Biocomputing (PSB 2001). 
Hawaii, U.S.A.. pp. 408-419.  
G. Bisson, C. Nedellec, L. Ca?amero 2000. 
Designing clustering methods for ontology 
building: The Mo?K workbench. In Proceedings 
of Ontology Learning workshop (ECAI 2000), 
Berlin, 22 ao?t 2000.  
M. E. Califf, 1998. Relational Learning Techniques 
for Natural Language Extraction. Ph.D. 
Disseration, Computer Science Department, 
University of Texas, Austin, TX. AI Technical 
Report 98-276. 
N. Collier, Hyun Seok Park, Norihiro Ogata, Yuka 
Tateisi, Chikashi Nobata, Takeshi Sekimizu, 
Hisao Imai and Jun'ichi Tsujii. (1999). The 
GENIA project: corpus-based knowledge 
acquisition and information extraction from 
genome research papers. In Proceedings of the 
European Association for Computational 
Linguistics (EACL 1999). 
M. Craven et al, 1999. Constructing Biological 
Knowledge Bases by Extracting Information 
from Text Sources. ISMB 1999: 77-86 
D. Faure and C. Nedellec (1999). Knowledge 
acquisition of predicate argument structures from 
technical texts using Machine Learning: the 
system ASIUM. In EKAW'99, pp. 329-334, 
Springer-Verlag.  
D. Freitag, 1998, Multistrategy learning for 
information extraction. In Proceedings of the 
Fifteenth International Conference on Machine 
Learning, 161-169. Madison, WI: Morgan 
Kaufmann 
T. Hamon and S. Aubin (2004). Evaluating 
terminological resource coverage for relevant 
sentence selection and semantic class building. 
LIPN internal report. 
K. Fukuda, T. Tsunoda, A. Tamura, T. Takagi 
(1998). Toward information extraction : 
identifying protein names from biological papers. 
Proceedings of the Pacific Symposium of 
Biocomputing, pp. 707-718. 
Z. Harris (1951). Methods in Structural Linguistics. 
Chicago. University of Chicago Press.  
E.M. Marcotte, I. Xenarios I., and D. Eisenberg 
(2001). Mining litterature for protein-protein 
interactions. In Bioinformatics, vo. 17 n? 4, 
pp. 359-363. 
MUC (1995) Proceeding of the 6
th
 Message 
understanding Conference. Morgan Kaufmann. 
Palo Alto.  
C. N?dellec, M. Ould Abdel Vetah and P. Bessi?res 
(2001). Sentence Filtering for Information 
Extraction in Genomics: A Classification 
Problem. In Proceedings of the International 
Conference on Practical Knowledge Discovery in 
Databases (PKDD?2001), pp. 326?338. Springer 
Verlag, LNAI 2167, Freiburg. 
T. Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Mima 
and Jun'ichi Tsujii. (2001). Ontology Based 
Corpus Annotation and Tools. In Proceedings of 
the 12th Genome Informatics 2001. pp. 469--470. 
T. Ono, H. Hishigaki, A. Tanigami and T. Takagi 
(2001). Automated extraction of information on 
protein-protein interactions from the biological  
literature. Bioinformatics. vol 17, n? 2, pp. 155-
161, Oxford Press. 
B. Prasad and A. Sarkar (2000) Comparing Test-
suite based evaluation and Corpus-based 
evaluation of a wide-coverage grammar for 
English. In Using Evaluation within Human 
Language Technology. LREC. Athens.  
D. Proux, F. Rechenmann, L. Julliard, V. Pillet, B. 
Jacq (1998). Detecting gene symbols and names 
in biological texts : a first step toward pertinent 
information extraction. In Genome Informatics, 
vol. 9, pp. 72-80. 
G. Schneider (1998). A Linguistic Comparison of 
Constituency, Dependency and Link Grammar. 
PhD thesis, Institut f?r Informatik der Universit?t 
Z?rich, Switzerland. 
D. Sleator and D. Temperley (1993). Parsing 
English with a Link Grammar. In Third 
International Workshop on Parsing 
Technologies. Tilburg. Netherlands. 
D. Weissenbacher (2004). La relation de 
synonymie en g?nomique. In Recital conference. 
Fes. 
49
Proceedings of BioNLP Shared Task 2011 Workshop, pages 56?64,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP shared Task 2011 - Bacteria Biotope 
Robert Bossy1, Julien Jourde1, Philippe Bessi?res1, Maarten van de Guchte2,  
Claire N?dellec1 
 
1MIG UR1077 2Micalis UMR 1319  
INRA, Domaine de Vilvert 
78352 Jouy-en-Josas, France 
forename.name@jouy.inra.fr 
 
 
Abstract 
This paper presents the Bacteria Biotope 
task as part of the BioNLP Shared Tasks 
2011. The Bacteria Biotope task aims at 
extracting the location of bacteria from 
scientific Web pages. Bacteria location is a 
crucial knowledge in biology for phenotype 
studies. The paper details the corpus 
specification, the evaluation metrics, 
summarizes and discusses the participant 
results.  
1 Introduction 
The Bacteria Biotope (BB) task is one of the five 
main tasks of the BioNLP Shared Tasks 2011. The 
BB task consists of extracting bacteria location 
events from Web pages, in other words, citations 
of places where a given species lives. Bacteria 
locations range from plant or animal hosts for 
pathogenic or symbiotic bacteria, to natural 
environments like soil or water. Challenges for 
Information Extraction (IE) of relations in Biology 
are mostly devoted to the identification of bio-
molecular events in scientific papers where the 
events are described by relations between named 
entities, e.g. genic interactions (N?dellec, 2005), 
protein-protein interactions (Pyysalo et al, 2008), 
and more complex molecular events (Kim et al, 
2011). However, this far from reflects the diversity 
of the potential applications of text mining to 
biology. The objective of previous challenges has 
mostly been focused on modeling biological 
functions and processes using the information on 
elementary molecular events extracted from text. 
The BB task is the first step towards linking 
information on bacteria at the molecular level to 
ecological information. The information on 
bacterial habitats and properties of these habitats is 
very abundant in literature, in particular in 
Systematics literature (e.g. International Journal of 
Systematic and Evolutionary Microbiology), 
however it is rarely available in a structured way 
(Hirschman et al, 2008; Tamames and de Lorenzo, 
2009). The NCBI GenBank nucleotide isolation 
source field (GenBank) and the JGI Genome 
OnLine Database (GOLD) isolation site field are 
incomplete with respect to the microbial diversity 
and are expressed in natural language. The two 
critical missing steps in terms of biotope 
knowledge modeling are (1) the automatic 
population of databases with organism/location 
pairs that are extracted from text, and (2) the 
normalization of the habitat name with respect to 
biotope ontologies. The BB task mainly aims at 
solving the first information extraction issue. The 
second classification issue is handled through the 
categorization of locations into eight types. 
2 Context 
According to NCBI statistics there are nearly 900 
bacteria with complete genomes, which account 
for more than 87% of total complete genomes. 
Consequently, molecular studies in bacteriology 
are shifting from species-centered to full diversity 
investigation. The current trend in high-throughput 
experiments targets diversity related fields, 
typically phylogeny or ecology. In this context, 
adaptation properties, biotopes and biotope 
properties become critical information. Illustrative 
questions are: 
56
? Is there a phylogenetic correlation between 
species that share the same biotope? 
? What are common metabolic pathways of 
species that live in given conditions, especially 
species that survive in extreme conditions? 
? What are the molecular signaling patterns in 
host relationships or population relationships 
(e.g. in biofilms)? 
Recent metagenomic experiments produce 
molecular data associated with a habitat rather than 
a single species. This raises new challenges in 
computational biology and data integration, such 
as identifying known and new species that belong 
to a metagenome. 
Not only will these studies require 
comprehensive databases that associate bacterial 
species to their habitat, but they also require a 
formal description of habitats for property 
inference. The bacteria biotope description is 
potentially very rich since any physical object, 
from a cell to a continent, can be a bacterial 
habitat. However these relations are much simpler 
to model than with general formal spatial 
ontologies. A given place is a bacterial habitat if 
the bacteria and the habitat are physically in 
contact, while the relative position of the bacteria 
and its dissemination are not part of the BB task 
model.  
The BB Task requires the locations to be 
assigned different types (e.g. soil, water). We view 
location typing as a preliminary step of more fine-
grained modeling in location ontologies. Some 
classifications for bacteria biotopes have been 
proposed by some groups (Floyd et al, 2005; 
Hirschman et al, 2008; Field et al, 2008; 
Pignatelli et al, 2009). The Environment Ontology 
project (EnvO) is developing an ambitious detailed 
environment ontology for supporting standard 
manual annotation of environments of all types of 
organisms and biological samples (Field et al, 
2008). In a similar way, the GOLD group at JGI 
defined a standard classification for bacteria 
population metagenome projects. Developing 
methods for the association of such biotope classes 
to organisms remains an open question. EnvDB 
(Pignatelli et al, 2009) is an attempt to inventory 
isolation sources of bacteria as recorded in 
GenBank and to map them to a three level 
hierarchy of 71 biotope classes. The assignment of 
bacterial samples in one of the EnvDB classes is 
supported by a text-mining tool based on a Na?ve 
Bayes (NB) classifier applied to a bag of words 
representing the associated reference title and 
abstract. Unfortunately, the low number of paper 
references associated with the isolation source field 
(46 %) limits the scope of the method. 
The BB task has a similar goal, but directly 
applies to natural language texts thus avoiding the 
issue of database incompleteness. As opposed to 
database-based approaches, biotope information 
density is higher but the task has to include 
bacteria and location identification, as well as 
information extraction to relate them.  
The eight types of locations in the BB task 
capture high-level information for further ontology 
mappings.  The location types are Host, HostPart, 
Geographical and Environmental. Environmental 
is broadly defined to qualify locations that are not 
associated to hosts, in a similar way to what was 
described by Floyd et al (Floyd et al, 2005). In 
addition, the BB task types exclude artificially 
constructed biotopes (e.g. bacteria growing in labs 
on a specific medium) and laboratory mutant 
bacteria. The Environmental class is divided into 
Food, Medical, Soil and Water. Locations that are 
none of these subtypes are classified as 
Environmental. 
The exact geographical location (e.g. latitude 
and longitude coordinates) has less importance 
here than in eukaryote ecology because most of the 
biotope properties vary along distances smaller 
than the precision of the current positioning 
technologies. Geographical names are only useful 
in bacteria biotope studies when the physico-
chemical properties of the location can be inferred. 
For the sake of simplicity, the locations of bacteria 
host (e.g. the stall of the infected cow) are not 
taken into account despite their richness (Floyd et 
al., 2005). 
The important information conveyed by the 
locations, especially of Environment type, is the 
function of the bacterium in its ecosystem rather 
than the substance of the habitat. Indeed the final 
goal is to extract habitat properties and bacteria 
phenotypes. Beyond the identification of locations, 
their properties (e.g. temperature, pH, salinity, 
oxygen) are of high interest for phenotypes (e.g. 
thermophily, acidophily, halophily) and trophism 
studies. This information is difficult to extract, and 
is often incomplete or even not available in papers 
(Tamames and de Lorenzo., 2009). Hopefully, 
some properties can be automatically retrieved 
57
with the help of specialized databases, which give 
the physico-chemical properties of locations, such 
as hosts (plant, animal, human organs), soils (see 
WebSoilSurvey, Corine Land Cover), water, or 
chemical pollutants. 
From a linguistic point of view, the BB task 
differs from other IE molecular biology tasks while 
it raises some issues common to biomedicine and 
more general IE tasks. The documents are 
scientific Web pages intended for non-experts such 
as encyclopedia notices. The information is dense 
compared to scientific papers. Documents are 
structured as encyclopedia pages, with the main 
focus on a single species or a few species of the 
same genus or family. The frequency of anaphora 
and coreferences is unusually high. The location 
entities are denoted by complex expressions with 
semantic boundaries instead of rigid designators.  
3 Task description 
The goal of the BB task is illustrated in Figure 1.  
 
Bifidobacterium longum . This organism is found in 
adult humans  and formula fed infants  as a normal 
component of gut  flora. 
Figure 1. Example of information to be extracted 
in the BB Task. 
 
The entities to be extracted are of two main 
types: bacteria and locations. They are text-bound 
and their position has to be predicted. Relations are 
of type Localization between bacteria and 
locations, and PartOf between hosts and host parts. 
In the example in Figure 1, Bifidobacterium 
longum is a bacterium. adult humans and formula 
fed infants denote host locations for the bacteria. 
gut is also a bacteria location, part of the two hosts 
and thus of type host part.  
Coreference relations between entities denoting 
the same information represent valid alternatives 
for the relation arguments. For example, the three 
taxon names in Figure 2 are equivalent. 
 
 
 
The green sulfur bacteria  (GSB ; Phylum Chlorobi ) 
are commonly found in aquatic environments . 
Figure 2. Coreference example. 
 
The coreference relation between pairs of 
entities is binary, symmetric and transitive. 
Coreference sets are equivalence sets defined as 
the transitive closure of the binary coreference 
relation. Their annotation is provided in the 
training and development sets, but it does not have 
to be predicted in the test set. 
4 Corpus description 
The corpus sources are the following bacteria 
sequencing project Web pages: 
? Genome Projects referenced at NCBI; 
? Microbial Genomics Program at JGI; 
? Bacteria Genomes at EBI; 
? Microorganisms sequenced at Genoscope; 
? Encyclopedia pages from MicrobeWiki. 
The documents are publicly available and quite 
easy to understand by non-experts compared to 
scientific papers on similar topics. From the 2,086 
downloaded documents, 105 were randomly 
selected for the BB task. A quarter of the corpus 
was retained for test evaluation. The rest was split 
into train and development sets. Table 1 gives the 
distribution of the entities and relations per corpus. 
The distribution of the five document sources in 
the test corpus reflects the distribution of the 
training set and no other criteria. Food is therefore 
underrepresented.  
 
 Training+Dev Test 
Document 78 (65 + 13) 27 (26 %) 
Bacteria 538 121 (18 %) 
Environment 62 16 (21 %) 
Host 486 101 (17 %) 
HostPart 217 84 (28 %) 
Geographical 111 25 (18 %) 
Water 70 21 (23 %) 
Food 46 0 (0 %) 
Medical 24 2 (8 %) 
Soil 26 20 (43 %) 
Coreference 484 100 (17 %) 
Total entities 1,580 390 
58
 Training+Dev Test 
Localization 998 250 (20 %) 
Part of Host 204 78 (28 %) 
Total relations 1,202 328 
Table 1. Corpus Figures. 
5 Annotation methodology 
HTML tags and irrelevant metadata were stripped 
from the corpus. The Alvis pipeline (N?dellec et 
al., 2009) pre-annotated the species names that are 
potential bacteria and host names. A team of 7 
scientists manually annotated the entities, 
coreferences and relations using the Cadixe XML 
editor (Cadixe). Each document was processed by 
two independent annotators in a double-blind 
manner. Conflicts were automatically detected, 
resolved by annotator negotiation and irrelevant 
documents (e.g. without bacterial location) were 
removed. The remaining inconsistencies among 
documents were resolved by the two annotators 
assisted by a third person acting as an arbitrator. 
The annotator group designed the detailed 
annotation guidelines in two phases. First, they 
annotated a set of 10 documents, discussed the 
options and wrote detailed guidelines with 
representative and illustrative examples. During 
the annotation of the rest of the documents, new 
cases were discussed by email and the guidelines 
amended accordingly. 
Location types. The main issues under debate 
were the definition of location types, boundaries of 
annotations and coreferences. Additional 
annotation specifications concerned the exclusion 
of overly general locations (e.g. environment, 
zone), artificially constructed biotopes and indirect 
effects of bacteria on distant places. For instance, a 
disease symptom occurring in a given host part 
does not imply the presence of the bacteria in this 
place, whereas infection does. Boundaries of types 
were also an important point of discussion since 
the definite formalization of habitat categories was 
at stake. For instance we decided to exclude land 
environment citations (fields, deserts, savannah, 
etc.) from the type Soil, and thus enforced a strict 
definition of soil bacteria. The most controversial 
type was host parts. We decided to include fluids, 
secretions and excretions (which are not strictly 
organs). Therefore, the host parts category required 
specifications to determine at which point of 
dissociation from the original host is a habitat not a 
host part anymore (e.g. mother?s milk vs. industrial 
milk, rhizosphere as host part instead of soil). 
Boundaries. The bacteria name boundaries do 
not include any external modifiers (e.g. two A. 
baumannii strains). Irrelevant modifiers of 
locations are considered outside the annotation 
boundaries (e.g. responsible for a hospital 
epidemic). All annotations are contiguous and span 
on a single fragment in the same way as the other 
BioNLP Shared Tasks. This constraint led us to 
consider cases where several annotations occur 
side by side. The preferred approach was to have 
one distinct annotation for each different location 
(e.g. contact with infected animal products or 
through the air). In the case of head or modifier 
factorization, the annotation depends on the 
information conveyed by the factorized part. If the 
head is not relevant to determine the location type, 
then each term is annotated separately (e.g. 
tropical and temperate zones). Conversely, if the 
head is the most informative with regards to the 
location type, a single annotation spans the whole 
fragment (fresh and salt water). 
Coreferences. Two expressions are considered 
as coreferential and thus valid solution alternatives, 
if they convey the same information. For instance, 
complete taxon names and non-ambiguous 
abbreviations are valid alternatives (e.g. Borrelia 
garinii vs. B. garinii), while ambiguous anaphora 
ellipses are not (e.g. as in ?[..] infected with 
Borrelia duttonii. Borrelia then multiplies [..]?). 
The ellipsis of the omitted specific name 
(dutotonii) leaves the ambiguous generic name 
(Borrelia). 
The full guidelines document is available for 
download on the BioNLP Shared Task Bacteria 
Biotope page1. 
6 Evaluation procedure 
6.1 Campaign organization 
The training and development corpora with the 
reference annotations were made available to the 
participants by December 1st 2010 on the BioNLP 
Shared Tasks pages together with the evaluation 
software. The test corpus, which does not contain 
                                                   
1 https://sites.google.com/site/bionlpst/home/bacteria-biotopes/ 
BioNLP-ST_2011_Bacteria_Biotopes_Guidelines.pdf 
59
any annotation, was made available by March, 1st 
2011. The participants sent the predicted 
annotations to the BioNLP Shared Task organizers 
by March 10th. Each participant submitted a single 
final prediction set. The detailed evaluation results 
were computed, provided to the participants and 
published on the BioNLP website by March, 11th.  
6.2 Evaluation metrics 
The evaluation metrics are based on precision, 
recall and the F-measure. In the following section, 
the PartOf and Localization relations will both be 
referred to as events. The metrics measure the 
accuracy of the participant prediction of events 
with respect to the reference annotation of the test 
corpus. Predicted entities that are not event 
arguments are ignored and they do not penalize the 
score. Each event Er in the reference set is matched 
to the predicted event Ep that maximizes the event 
similarity function S. The recall is the sum of the S 
results divided by the number of events in the 
reference set. Each event Ep in the predicted set is 
matched to the reference event Er that maximizes 
S. The precision is the sum of the S results divided 
by the number of events in the predicted set. 
Participants were ranked by the F-score defined as 
the harmonic mean between precision and recall. 
Eab, the event similarity between a reference 
Localization event a and a predicted Localization 
event b, is defined as: 
Eab = Bab . Tab . Jab 
? Bab is the bacteria boundary component defined 
as: if the Bacterium arguments of both the 
predicted and reference events have exactly the 
same boundaries, then Bab = 1, otherwise Bab = 
0. Bacteria name boundary matching is strict 
since boundary mistakes usually yield a 
different taxon. 
? Tab is the location type prediction component 
defined as: if the Location arguments of both 
the predicted and reference events are of the 
same type, then Tab = 1, otherwise Tab = 0.5. 
Thus type errors divide the score by two. 
? Jab is the location boundary component defined 
as: if the Location arguments of the predicted 
and reference events overlap, then 
1?+=
ab
ba
ab OV
LENLENJ  
where LENa and LENb are the length of the 
Localization arguments of predicted and 
reference events, and OVab is the length of the 
overlapping segment between the Localization 
arguments of the predicted and reference 
events. If the arguments do not overlap, then Jab 
is 0. This formula is a Jaccard index applied to 
overlapping segments. Location boundary 
matching is relaxed, though the Jaccard index 
rewards predictions that approach the reference. 
For PartOf events between Hosts and HostParts, 
the matching score Pab is defined as: if the Host 
arguments of the reference and predicted events 
overlap and the Part arguments of the reference 
and predicted events overlap, then Pab = 1, 
otherwise Pab = 0. Boundary matching of PartOf 
arguments is relaxed, since boundary mistakes are 
already penalized in Eab. 
Arguments belonging to the same coreference 
set are strictly equivalent. In other words, the 
argument in the predicted event is correct if it is 
equal to the reference entity or to any item in the 
reference entity coreference set. 
7 Results  
7.1 Participating systems 
Three teams submitted predictions to the BB task. 
The first team is from the University of Turku 
(UTurku); their system is generic and produced 
predictions for every BioNLP Shared Task. This 
system uses ML intensely, especially SVMs, for 
entity recognition, entity typing and event 
extraction. UTurku adapted their system for the BB 
task by using specific NER patterns and external 
resources (Bj?rne and Salakoski, 2011). 
The second team is from the Japan Advanced 
Institute of Science and Technology (JAIST); their 
system was specifically designed for this task. 
They used CRF for entity recognition and typing, 
and classifiers for coreference resolution and event 
extraction (Nguyen and Tsuruoka, 2011). 
The third team is from Bibliome INRA; their 
system was specifically designed for this task 
(Ratkovik et al, 2011). This team has the same 
affiliation as the BB Task authors, however great 
care was taken to prevent communication on the 
subject between task participants and the test set 
annotators. 
60
The results of the three submissions according to 
the official metrics are shown in Table 2. The 
scores are micro-averaged: Localization and 
PartOf relations have the same weight. Given the 
novelty and the complexity of the task, these first 
results are quite encouraging. Almost half of the 
relations are correctly predicted. The Bibliome 
team achieved the highest F-measure with a 
balanced recall and precision (45%). 
 
 Recall Precision F-score 
Bibliome 45 45 45 
JAIST 27 42 33 
UTurku 17 52 26 
 
Table 2. Bacteria Biotope Task results. 
7.2 Systems description and result analysis 
All three systems perform the same distinct sub-
tasks: bacteria name detection, detection and 
typing of locations, coreference resolution and 
event extraction. The following description of the 
approaches used by the three systems in each 
subtask will be supported by intermediate results. 
Bacteria name detection. Interestingly the three 
participants used three different resources for the 
detection of bacteria names: the List of Prokaryotic 
Names with Standing in Nomenclature (LPNSN) 
by UTurku, names in the genomic BLAST page of 
NCBI by JAIST and the NCBI Taxonomy by 
Bibliome. 
 
Bibliome 84 
JAIST 55 
UTurku 16 
Table 3. Bacteria entity recall. 
 
Table 3 shows a disparity in the bacteria entity 
recall of participants. The merits of each resource 
cannot be deduced directly from these figures since 
they have been exploited in different manners. 
UTurku and JAIST systems injected the resource 
as features in a ML algorithm, whereas Bibliome 
directly projected the resource on the corpus with 
additional rule-based abbreviation detection. 
However there is some evidence that the 
resources have a major impact on the result. 
According to Sneath and Brenner (1992) LPNSN 
is necessarily incomplete. NCBI BLAST only 
contains names of species for which a complete 
genome has been published. The NCBI Taxonomy 
used by INRA only contains names of taxa for 
which some sequence was published. It appears 
that all the lists are incomplete. However, the 
bacteria referenced by the sequencing projects, 
which are mentioned in the corpus should all be 
recorded by the NCBI Taxonomy. 
Location detection and typing. As stated before, 
locations are not necessarily denoted by rigid 
designators. This was an interesting challenge that 
called for the use of external resources and 
linguistic analysis with a broad scope. 
UTurku and JAIST both used WordNet, a 
sensible choice since it encompasses a wide 
vocabulary and  is also structured with synsets and 
hyperonymy relations. The WordNet entries were 
injected as features in the participant ML-based 
entity recognition and typing subsystems. 
It is worth noting that JAIST also used word 
clustering based on MEMM for entity detection. 
This method has things in common with 
distributional semantics. JAIST experiments 
demonstrated a slight improvement using word 
clustering, but further exploration of this idea may 
prove to be valuable. 
Alternatively, the Bibliome system extracted 
terms from the corpus using linguistic criteria 
classified them as locations and predicted their 
type, by comparing them to classes in a habitat-
specific ontology. This prediction uses both 
linguistic analysis of terms and the hierarchical 
structure of the ontology. Bibliome also used 
additional resources for specific types: the NCBI 
Taxonomy for type Host and Agrovoc countries 
for type Geographical. 
 Bibliome JAIST UTurku 
Host 82 49 28 
Host part 72 36 28 
Geo. 29 60 53 
Environment 53 10 11 
Water 83 32 2 
Soil 86 37 34 
Table 4. Location entity recall by type. The 
number of entities of type Food and Medical in the 
test set is too low to be significant. The scores are 
computed using Tab and Jab. 
61
 
The location entity recall in Table 4 shows that 
Bibliome consistently outperformed the other 
groups for all types except for Geographical. This 
demonstrates the strength of exploiting a resource 
with strong semantics (ontology vs. lexicon) and 
with mixed semantic and linguistic rules. 
In order to evaluate the impact of Location entity 
boundaries and types, we computed the final score 
by relaxing Tab and Jab measures. We re-defined Tab 
as always equal to 1, in other words the type of the 
localization was not evaluated. We also re-defined 
Jab as: if the Location arguments overlap, then Jab = 
1, otherwise Jab = 0. This means that boundaries 
were relaxed. The relaxed scores are shown in 
Table 5. While the difference is not significant for 
JAIST and UTurku, the Bibliome results exhibit a 
9 point increase. This demonstrates that the 
Bibliome system is efficient at predicting which 
entities are locations, while the other participants 
predict more accurately the boundaries and types. 
 Recall Prec. F-score Diff. 
Bibliome 54 54 54 +9 
JAIST 29 45 35 +2 
UTurku 19 56 28 +2 
Table 5. Participants score using relaxed location 
boundaries and types. 
Coreference resolution. The corpus exhibits an 
unusual number of anaphora, especially bacteria 
coreferences since a single bacterium species is 
generally the central topic of a document. The 
Bibliome submission is the only one that 
performed bacteria coreference resolution. Their 
system is rule-based and dealt with referential ?it?, 
bi-antecedent anaphora and more importantly 
sortal anaphora. The JAIST system has a bacteria 
coreference module based on ML. However the 
submission was done without coreference 
resolution since their experiments did not show 
any performance improvement. 
 
Event extraction. Both UTurku and JAIST 
approached the event extraction as a classification 
task using ML (SVM). Bibliome exploited the co-
occurrence of arguments and the presence of 
trigger words from a predefined list. Both UTurku 
and Bibliome generate events in the scope of a 
sentence, whereas JAIST generates events in the 
scope of a paragraph. 
As shown in Table 6, UTurku achieved the best 
score for PartOf events. For all participants, the 
prediction is often correct (between 60 and 80%) 
while the recall is rather low (20 to 32%). 
 
  Recall Precis. F-score 
 Host 61 48 53 
 Host part 53 42 47 
 Geo. 13 38 19 
B. Env. 29 24 26 
 Water 60 55 57 
 Soil 69 59 63 
 Part-of 23 79 36 
 Host 30 43 36 
 Host part 18 68 28 
 Geo. 52 35 42 
J. Env. 5 0 0 
 Water 19 27 23 
 Soil 21 42 28 
 Part-of 31 61 41 
 Host 15 51 23 
 Host part 9 40 15 
 Geo. 32 40 36 
U. Env. 6 50 11 
 Water 1 7 2 
 Soil 12 21 15 
 Part-of 32 83 46 
Table 6. Event extraction results per type. 
 
Conversely, the score of the Localization relation 
by UTurku has been penalized by its low 
recognition of bacteria names (16%). This strongly 
affects the score of Localizations since the 
bacterium is the only expected agent argument. 
The good results of Bibliome are partly explained 
by its high bacteria name recall of 84%. 
The lack of coreference resolution might penalize 
the event extraction recall. To test this hypothesis, 
we computed the recall by taking only into account 
events where both arguments occur in the same 
sentence. The goal of this selection is to remove 
most events denoted through a coreference. The 
recall difference was not significant for Bibliome 
and JAIST, however UTurku recall raised by 12 
points (29%). That experiment confirms that 
UTurku low recall is explained by coreferences 
62
rather than the quality of event extraction. The 
paragraph scope chosen by JAIST probably 
compensates the lack of coreference resolution. 
As opposed to Bibliome, the precision of the 
Localization relation prediction by JAIST and 
UTurku, is high compared to the recall, with a 
noticeable exception of geographical locations. 
The difference between participants seems to be 
caused by the geographical entity recognition step 
more than the relation itself. This is shown by the 
difference between the entity and the event recall 
(Table 4 and 6 respectively).. The worst predicted 
type is Environment, which includes diverse 
locations, such as agricultural, natural and 
industrial sites and residues. This reveals 
significant room for improvement for Water, Soil 
and Environment entity recognition. 
8 Discussion 
The participant papers describe complementary 
methods for tackling BB Task?s new goals. The 
novelty of the task prevents participants from 
deeply investing in all of the issues together. 
Depending on the participants, the effort was 
focused on different issues with various 
approaches: entity recognition and anaphora 
resolution based on extensive use of background 
knowledge, and relation prediction based on 
linguistic analysis of syntactic dependencies. 
Moreover, these different approaches revealed to 
be complementary with distinct strengths and 
limitations. In the future, one may expect that the 
integration of these promising approaches will 
improve the current score. 
The corpus of BioNLP BB Task 2011 consists 
of a set of Web pages that were selected for their 
readability. However, some corpus traits make the 
IE task more difficult compared to scientific 
papers. For example, the relaxed style of some 
pages tolerates some typographic errors (e.g. 
morrow instead of marrow) and ambiguous 
anaphora. The genome sequencing project 
documents aim at justifying the sequencing of 
bacteria. This results in abundant descriptions of 
potential uses and locations that should not be 
predicted as actual locations. Their correct 
prediction requires complex analysis of modalities 
(possibility, probability, negation). Some pages 
describe the action of hosted bacteria at the 
molecular level, such as cellular infection. Terms 
related to the cell are ambiguous locations because 
they may refer to either bacteria or host cells. 
Scientific papers form a much richer source of 
bacterial location information that is exempt from 
such flaws. However, as opposed to Web pages, 
most of them are not publicly available and they 
are in PDF format. 
The typology of locations was designed 
according to the BB Task corpus with a strong bias 
towards natural environments since bioremediation 
and plant growth factor are important motivations 
for bacteria sequencing. It could be necessary to 
revise it according to a broader view of bacterial 
studies where pathogenicity and more generally 
human and animal health are central issues. 
9 Conclusion 
The Bacteria Biotope Task corpus and objectives 
differ from molecular biology text-mining of 
scientific papers. The annotation strategy and the 
analysis of the participant results contributed to the 
construction of a preliminary review of the nature 
and the richness of its linguistic specificities. The 
participant results are encouraging for the future of 
the Bacteria Biotope issue. The degree of 
sophistication of participating systems shows that 
the community has technologies, which are mature 
enough to address this crucial biology question. 
However, the results leave a large room for 
improvement. 
The Bacteria Biotope Task was an opportunity 
to extend molecular biology text-mining goals 
towards the support of bacteria biodiversity studies 
such as metagenomics, ecology and phylogeny. 
The prediction of bacterial location information is 
the very first step in this direction. The abundance 
of scientific papers dealing with this issue and 
describing location properties form a potentially 
rich source for further extensions. 
Acknowledgments 
The authors thank Valentin Loux for his valuable 
contribution to the definition of the Bacteria 
Biotope task. This work was partially supported by 
the French Quaero project. 
63
References 
Jari Bj?rne and Taio Salakoski. 2011. Generalizing 
Biomedical Event Extraction. Proceedings of the 
BioNLP 2011 Workshop Companion Volume for 
Shared Task. 
Cadixe. http://caderige.imag.fr/Articles/CADIXE-XML-
Annotation.pdf 
Corine Land Cover. 
http://www.eea.europa.eu/themes/landuse/interactive/
clc-download 
EnvDB database. http://metagenomics.uv.es/envDB/ 
EnvO Project. 
http://gensc.org/gc_wiki/index.php/EnvO_Project  
Dawn Field [et al. 2008. Towards a richer description 
of our complete collection of genomes and 
metagenomes: the ?Minimum Information about a 
Genome Sequence? (MIGS) specification. Nature 
Biotechnology. 26: 541-547. 
Melissa M. Floyd, Jane Tang, Matthew Kane and David 
Emerson. 2005. Captured Diversity in a Culture 
Collection: Case Study of the Geographic and 
Habitat Distributions of Environmental Isolates Held 
at the American Type Culture Collection. Applied 
and Environmental Microbiology. 71(6):2813-23. 
GenBank. http://www.ncbi.nlm.nih.gov/  
GOLD. http://www.genomesonline.org/cgi-
bin/GOLD/bin/gold.cgi 
Lynette Hirschman, Cheryl Clark, K. Bretonnel Cohen, 
Scott Mardis, Joanne Luciano, Renzo Kottmann, 
James Cole, Victor Markowitz, Nikos Kyrpides, 
Norman Morrison, Lynn M. Schriml, Dawn Field. 
2008. Habitat-Lite: a GSC case study based on free 
text terms for environmental metadata. Omics. 
12(2):129-136. 
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, 
Yoshinobu Kano, Jun?ichi Tsujii. 2010. Extracting 
bio-molecular events from literature - the BioNLP?09 
shared task. Special issue of the International 
Journal of Computational Intelligence. 
MicrobeWiki. 
http://microbewiki.kenyon.edu/index.php/MicrobeWi
ki  
Microbial Genomics Program at JGI. http://genome.jgi-
psf.org/programs/bacteria-archaea/index.jsf 
Microorganisms sequenced at Genoscope. 
http://www.genoscope.cns.fr/spip/Microorganisms-
sequenced-at.html 
Claire N?dellec. 2005. Learning Language in Logic - 
Genic Interaction Extraction Challenge" in 
Proceedings of the Learning Language in Logic 
(LLL05) workshop joint to ICML'05. Cussens J. and 
N?dellec C. (eds). Bonn. 
Claire N?dellec, Adeline Nazarenko, Robert Bossy. 
2008..Information Extraction. Ontology Handbook. 
S. Staab, R. Studer (eds.), Springer Verlag, 2008. 
Nhung T. H. Nguyen and Yoshimasa Tsuruoka. 2011. 
Extracting Bacteria Biotopes with Semi-supervised 
Named Entity Recognition and Coreference 
Resolution. Proceedings of the BioNLP 2011 
Workshop Companion Volume for Shared Task. 
Miguel Pignatelli, Andr?s Moya, Javier Tamames.  
(2009). EnvDB, a database for describing the 
environmental distribution of prokaryotic taxa. 
Environmental Microbiology Reports. 1:198-207. 
Prokaryote Genome Projects at NCBI. 
http://www.ncbi.nlm.nih.gov/genomes/lproks.cgi  
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari 
Bj?rne, Filip Ginter and Tapio Salakoski. 2008. 
Comparative analysis of five protein-protein 
interaction corpora. BMC Bioinformatics. vol 9. 
Suppl 3. S6. 
Zorana Ratkovic, Wiktoria Golik, Pierre Warnier, 
Philippe Veber, Claire N?dellec. 2011. BioNLP 2011 
Task Bacteria Biotope ? The Alvis System. 
Proceedings of the BioNLP 2011 Workshop 
Companion Volume for Shared Task. 
Peter H. A. Sneath and Don J. Brenner. 1992. ?Official? 
Nomenclature Lists. American Society for 
Microbioloy News. 58, 175. 
Javier Tamames and Victor de Lorenzo. 2010. 
EnvMine: A text-mining system for the automatic 
extraction of contextual information. BMC 
Bioinformatics. 11:294. 
Web Soil Survey. http://websoilsurvey.nrcs.usda.gov/ 
64
Proceedings of BioNLP Shared Task 2011 Workshop, pages 65?73,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP Shared Task 2011 ? Bacteria Gene Interactions and Renaming
Julien Jourde1, Alain-Pierre Manine2, Philippe Veber1, Kare?n Fort3, Robert Bossy1,
Erick Alphonse2, Philippe Bessie`res1
1Mathe?matique, Informatique et 2PredictiveDB 3LIPN ? Universite? Paris-Nord/
Ge?nome ? Institut National de la 16, rue Alexandre Parodi CNRS UMR7030 and
Recherche Agronomique F75010 Paris, France INIST CNRS UPS76 ? F54514
MIG INRA UR1077 {apmanine,alphonse} Vand?uvre-le`s-Nancy, France
F78352 Jouy-en-Josas, France @predictivedb.com karen.fort@inist.fr
forename.lastname@jouy.inra.fr
Abstract
We present two related tasks of the BioNLP
Shared Tasks 2011: Bacteria Gene Renam-
ing (Rename) and Bacteria Gene Interactions
(GI). We detail the objectives, the corpus spec-
ification, the evaluation metrics, and we sum-
marize the participants? results. Both issued
from PubMed scientific literature abstracts,
the Rename task aims at extracting gene name
synonyms, and the GI task aims at extracting
genic interaction events, mainly about gene
transcriptional regulations in bacteria.
1 Introduction
The extraction of biological events from scientific
literature is the most popular task in Information Ex-
traction (IE) challenges applied to molecular biol-
ogy, such as in LLL (Ne?dellec, 2005), BioCreative
Protein-Protein Interaction Task (Krallinger et al,
2008), or BioNLP (Demner-Fushman et al, 2008).
Since the BioNLP 2009 shared task (Kim et al,
2009), this field has evolved from the extraction of a
unique binary interaction relation between proteins
and/or genes towards a broader acceptation of bio-
logical events including localization and transforma-
tion (Kim et al, 2008). In the same way, the tasks
Bacteria Gene Interactions and Bacteria Gene Re-
naming deal with the extraction of various molecu-
lar events capturing the mechanisms relevant to gene
regulation in prokaryotes. The study of bacteria has
numerous applications for health, food and indus-
try, and overall, they are considered as organisms
of choice for the recent integrative approaches in
systems biology, because of their relative simplicity.
Compared to eukaryotes, they allow easier and more
in-depth analysis of biological functions and of their
related molecular mechanisms.
Processing literature on bacteria raises linguis-
tic and semantic specificities that impact text anal-
ysis. First of all, gene renaming is a frequent phe-
nomenon, especially for model bacteria. Hence, the
abundance of gene synonyms that are not morpho-
logical variants is high compared to eukaryotes. The
history of bacterial gene naming has led to drastic
amounts of homonyms and synonyms which are of-
ten missing (or worse, erroneous) in gene databases.
In particular, they often omit old gene names that
are no longer used in new publications, but that are
critical for exhaustive bibliography search. Poly-
semy makes the situation even worse, as old names
frequently happen to be reused to denote different
genes. A correct and complete gene synonym table
is crucial to biology studies, for instance when inte-
grating large scale experimental data using distinct
nomenclatures. Indeed this information can save a
lot of bibliographic research time. The Rename Task
is a new task in text-mining for biology that aims at
extracting explicit mentions of renaming relations.
It is a critical step in gene name normalization that
is needed for further extraction of biological events
such as genic interactions.
Regarding stylistics, gene and protein interactions
are not formulated in the same way for eukary-
otes and prokaryotes. Descriptions of interactions
and regulations in bacteria include more knowledge
about their molecular actors and mechanisms, com-
pared to the literature on eukaryotes. Typically in
bacteria literature, the genic regulations are more
65
likely expressed by direct binding of the protein,
while in eukaryote literature, non-genic agents re-
lated to environmental conditions are much more
frequent. The bacteria GI Task is based on (Manine
et al, 2010) which is a semantic re-annotation of the
LLL challenge corpus (Ne?dellec, 2005), where the
description of the GI events in a fine-grained rep-
resentation includes the distinction between expres-
sion, transcription and other action events, as well as
different transcription controls (e.g. regulon mem-
bership, promoter binding). The entities are not only
protein agent and gene target but extend to families,
complexes and DNA sites (binding sites, promoters)
in order to better capture the complexity of the reg-
ulation at a molecular level. The task consists in re-
lating the entities with the relevant relations.
2 Rename Task Description
The goal of the Rename task is illustrated by Figure
1. It consists in predicting renaming relations be-
tween text-bound gene names given as input. The
only type of event is Renaming where both argu-
ments are of type Gene. The event is directed, the
former and the new names are distinguished. Genes
and proteins were not distinguished because of the
high frequency of metonymy in renaming events.
The relation to predict between genes is a Renam-
ing of a former gene name into a new one. In the
example of Figure 1, YtaA, YvdP and YnzH are the
former names of three proteins renamed CotI, CotQ
and CotU, respectively.
Figure 1: Examples of relations to be extracted.
2.1 Rename Task corpus
The Rename Task corpus is a set of 1,836 PubMed
references of bacterial genetic and genomic studies,
including title and abstract. A first set of 23,000 doc-
uments was retrieved, identifying the presence of the
bacterium Bacillus subtilis in the text and/or in the
MeSH terms. B. subtilis documents are particularly
rich in renaming mentions. Many genes were re-
named in the middle of the nineties, so that the new
names matched those of the Escherichia coli homo-
logues. The 1,843 documents the most susceptible
to mention renaming were automatically filtered ac-
cording to two non exclusive criteria:
1. Either the document mentions at least two gene
synonyms as recorded in the fusion of seven B.
subtilis gene nomenclatures. This led to a set
of 703 documents.
2. Or the document contains a renaming expres-
sion from a list that we manually designed and
tested (e.g. rename, also known as). It is an ex-
tension of a previous work by (Weissenbacher,
2004). A total of 1,140 new documents not in-
cluded in the first set match this criteria.
About 70% of the documents (1,146) were kept in
the training data set. The rest was split into the de-
velopment and test sets, containing 246 and 252 doc-
uments respectively. Table 1 gives the distribution
of genes and renaming relations per corpus. Gene
names were automatically annotated in the docu-
ments with the nomenclature of B. subtilis. Gene
names involved in renaming acts were manually cu-
rated. Among the 21,878 gene mentions in the three
corpus, 680 unique names are involved in renaming
relations which represents 891 occurrences of genes.
Training + Dev. Test
Documents (1,146 + 246) 1,392 252 (15%)
Gene names 18,503 3,375 (15%)
Renamings 373 88 (24%)
Table 1: Rename Task corpus content.
2.2 Rename Task annotation and guidelines
Annotation procedure The corpus was annotated
in a joint effort of MIG/INRA and INIST/CNRS.
The reference annotation of the Rename Task cor-
pus was done in two steps, a first annotation step
by science information professionals of INIST with
MIG initial specifications, a second checking step by
people at MIG. Two annotators and a project man-
ager were in charge of the task at INIST. The docu-
ments were annotated using the Cadixe editor1. We
1http://caderige.imag.fr/Articles/
CADIXEXML-Annotation.pdf
66
provided to them detailed annotation guidelines that
were largely modified in the process. A subset of
100 documents from the first set of 703 was anno-
tated as a training session. This step was used to re-
fine the guidelines according to the methodology de-
scribed in (Bonneau-Maynard et al, 2005). Several
inter-annotator agreements coefficients were com-
puted to measure the discrepancy between annota-
tors (Fort et al, 2009). With a kappa and pi scores
(for more details on those, see (Artstein and Poesio,
2008)), the results can be considered satisfactory.
The manual analysis of the 18 discrepancies led to
enrich the annotation guidelines. The first hundreds
of documents of the second set did not mention any
renaming, leading to concentrate the annotation ef-
forts on the first set. These documents actually con-
tained renamings, but nearly exclusively concerning
other kinds of biological entities (protein domains,
molecules, cellular ultrastructures, etc.).
Guidelines In order to simplify the task, only
short names of gene/protein/groups in B. subtilis
were considered. Naming conventions set short
names of four letters long with an upper case let-
ter at the end for all genes (e.g. gerE) and the same
names with the upper case of the initial letter (e.g.
GerE) and long names for the proteins (e.g. Spore
germination protein gerE). But many irregular gene
names exist (e.g. tuf), which are considered as well.
It also happens that gene or protein name lists are
abbreviated by factorization to form a sequence. For
instance queCDEF is the abbreviation of the list of
gene names queC, queD, queE and queF. Such ag-
gregations are acceptable gene names as well. In any
case, these details were not needed by the task par-
ticipants since the corpus was provided with tagged
gene names.
Most renaming relations involve couples of the
same type, genes, proteins or aggregations. Only
18 relations link mixed couples of genes and pro-
teins. In case of ambiguity, annotators would consult
international gene databases and an internal INRA
database to help them determine whether a given
couple of names were actually synonyms.
Multiple occurrences of the same renaming rela-
tion were annotated independently, and had to be
predicted. The renaming pairs are directed, the for-
mer and the new forms have to be distinguished.
When the renaming order was not explicit in the
document, the rule was to annotate by default the
first member of the couple as the new form, and the
second one as the former form. Figure 2 presents the
most common forms of renaming.
Figure 2: Common types of relations to be extracted.
Revised annotations INIST annotations were
systematically checked by two experts in Bioinfor-
matics from INRA. Mainly, encoding relations (e.g.
the gene encoding sigma K (sigK)) that are not re-
naming cases were purged. Given the number of
ambiguous annotations, we designed a detailed ty-
pology in order to justify acceptance or rejection
decisions in seven different sub-cases hereafter pre-
sented. Three positive relations figure in Table 2,
where the underlined names are the former names
and the framed names are the new ones. Explicit re-
naming relations occur in 261 sentences, synonymy-
like relations in 349 sentences, biological proof-
based relations in 76 sentences.
Explicit renaming relation is the easiest positive
case to identify. In the example, the aggregation of
gene names ykvJKLM is clearly renamed by the au-
thors as queCDEF. Although the four genes are con-
Explicit renaming
PMID 15767583 : Genetic analysis of ykvJKLM mu-
tants in Acinetobacter confirmed that each was essen-
tial for queuosine biosynthesis, and the genes were re-
named queCDEF .
Implicit renaming
PMID 8002615 : Analysis of a suppressor mutation
ssb ( kinC ) of sur0B20 (spo0A) mutation in Bacil-
lus subtilis reveals that kinC encodes a histidine pro-
tein kinase.
Biological proof
PMID 1744050 : DNA sequencing established that
spoIIIF and spoVB are a single monocistronic locus
encoding a 518-amino-acid polypeptide with features
of an integral membrane protein.
Table 2: Positive examples of the Rename Task.
67
catenated, there is no evidence mentioned of them
acting as an operon. Furthermore, despite the con-
text involving mutants of Acinetobacter, the aggre-
gation belongs correctly to B. subtilis.
Implicit renaming is an asymmetric relation
since one of the synonyms is intended to replace the
other one in future uses. The example presents two
renaming relations between former names ssb and
spo0A, and new names kinC and sur0B20, respec-
tively. The renaming relation between ssb and kinC
has a different orientation due to additional informa-
tion in the reference. Like in the preceding example,
the renaming is a consequence of a genetic mutation
experiment. Mutation names represent an important
transversal issue that is discussed below.
Biological proof is a renaming relation induced
by an explicit scientific conclusion while the renam-
ing is not, as in the example where experiments re-
veal that two loci spoIIIF and spoVB are in fact the
same one and then become synonyms. Terms such
as ?allelic to? or ?identical to? usually qualify such
conclusions. Predicting biological proof-based rela-
tions requires some biological modeling.
The next three cases are negative (Table 3). Un-
derlined gene and protein names are involved in a
relation which is not a renaming relation.
Protein encoding relation occurs between a gene
and the protein it codes for. Some mentions may
look like renaming relations. The example presents
the gene yeaC coding for MoxR. No member of the
couple is expected to replace the other one.
Homology measures the similarity between gene
or protein sequences. Most of the homology men-
tions involve genes or proteins from different species
Protein encoding
PMID 8969499: The putative products of ORFs yeaB
(Czd protein), yeaC (MoxR), yebA (CNG-channel and
cGMP-channel proteins from eukaryotes),
Genetic homology
PMID 10619015 : Dynamic movement of the ParA-
like Soj protein of B. subtilis and its dual role in nu-
cleoid organization and developmental regulation.
Operon | Regulon | Family
PMID 3127379 : Three promoters direct transcription
of the sigA (rpoD) operon in Bacillus subtilis.
Table 3: Negative examples of the Rename Task.
(orthologues). The others compare known gene or
protein sequences of the same species (paralogues).
This may be misleading since the similarity men-
tion may look like biological proof-based relations,
as between ParA and Soj in Table 3.
Operon, regulon or family renaming involves
objects that may look like genes, proteins or sim-
ple aggregations of gene or protein names but that
are perceptibly different. The objects represent more
than one gene or protein and the renaming does not
necessarily affect all of them. More problematic,
their name may be the same as one of the genes or
proteins they contain, as in the example where sigA
and rpoD are operons but are also known as gene
names. Here, sigA (and so rpoD) represents at least
two different genes. For the sake of clarity, oper-
ons, regulons and families are rejected, even if all
the genes are clearly named, as in an aggregation.
The last point concerns mutation which are fre-
quent in Microbiology for revealing gene pheno-
types. They carry information about the original
gene names (e.g., rvtA11 is a mutant name created
by adding 11 to rvtA). But partial names cannot be
partially annotated, that is to say, the original part
(rvtA) should not be annotated in the mutation name
(rvtA11). Most of these names are local names, and
should not be annotated because of their restricted
scope. It may happen so that the mutation name
is registered as a synonym in several international
databases. To avoid inconsistencies, all renamings
involving a mutation referenced in a database were
accepted, and only biological proof-based and ex-
plicit renamings involving a strict non-null unrefer-
enced mutation (a null mutation corresponds to a to-
tal suppression of a gene) were accepted.
2.3 Rename Task evaluation procedure
The evaluation of the Rename task is given in terms
of recall, precision and F-score of renaming rela-
tions. Two set of scores are given: the first set is
computed by enforcing strict direction of renaming
relations, the second set is computed with relaxed
direction. Since the relaxed score takes into ac-
count renaming relations even if the arguments are
inverted, it will necessarily be greater or equal than
the strict score. The participant score is the relaxed
score, the strict score is given for information. Re-
laxed scores are informative with respect to the ap-
68
plication goal. The motivation of the Rename task
is to keep bacteria gene synonyms tables up to date.
The choice of the canonical name among synonyms
for denoting a gene is done by the bacteriology com-
munity, and it may be independent of the anteriority
or novelty of the name. The annotation of the ref-
erence corpus showed that the direction was not al-
ways decidable, even for a human reader. Thus, it
would have been unfair to evaluate systems on the
basis of unsure information.
2.4 Results of the Rename Task participants
Final submissions were received from three teams,
the University of Turku (Uturku), the University of
Concordia (Concordia) and the Bibliome team from
MIG/INRA. Their results are summarized in Table
4. The ranking order is given by the overall F-score
for relations with relaxed argument order.
Team Prec. Recall F-score
Univ. of Turku 95.9 79.6 87.0
Concordia Univ. 74.4 65.9 69.9
INRA 57.0 73.9 64.4
Table 4: Participant scores at the Rename Task.
Uturku achieved the best F-score with a very high
precision and a high recall. Concordia achieved the
second F-score with balanced precisions and recalls.
Bibliome is five points behind with a better recall
but much lower precision. Both UTurku and Con-
cordia predictions rely on dependencies (Charniak-
Johnson and Stanford respectively, using McClosky
model), whereas Bibliome predictions rely on bag of
words. This demonstrates the high value of depen-
dency parsing for this task, in particular for the pre-
cision of predictions. We notice that UTurku system
uses machine learning (SVM) and Concordia uses
rules based on trigger words. The good results of
UTurku confirms the hypothesis that gene renam-
ing citations are highly regular in scientific litera-
ture. The most frequently missed renamings belong
to the Biological Proof category (see Table 2). This
is expected because the renaming is formulated as a
reasoning where the conclusion is only implicit.
2.5 Discussion
The very high score of Uturku method leads us to
conclude that the task can be considered as solved
by a linguistic-based approach. Whereas Bib-
liome used an extensive nomenclature considered
as exhaustive and sentence filtering using a SVM,
Uturku used only two nomenclatures in synergy but
with more sophisticated linguistic-based methods,
in particular syntactic analyses. Bibliome methods
showed that a too high dependence to nomenclatures
may decrease scores if they contain compromised
data. However, the use of an extensive nomencla-
ture as done by Bibliome may complement Uturku
approach and improve recall. It is also interesting
that both systems do not manage renamings cross-
ing sentence boundaries.
The good results of the renaming task will be ex-
ploited to keep synonym gene lists up to date with
extensive bibliography mining. In particular this
will contribute to enriching SubtiWiki, a collabora-
tive annotation effort on B. subtilis (Flo?rez et al,
2009; Lammers et al, 2010).
3 Gene Interactions Task description
The goal of the Bacteria GI Task is illustrated by
Figure 3. The genes cotB and cotC are related to
their two promoters, not named here, by the rela-
tion PromoterOf. The protein GerE is related to
these promoters by the relation BindTo. As a con-
sequence, GerE is related to cotB and cotC by an In-
teraction relation. According to (Kim et al, 2008),
the need to define specialized relations replacing one
unique and general interaction relation was raised in
(Manine et al, 2009) for extracting genic interac-
tions from text. An ontology describes relations and
entities (Manine et al, 2008) catching a model of
gene transcription to which biologists implicitly re-
fer in their publications. Therefore, the ontology is
mainly oriented towards the description of a struc-
tural model of genes, with molecular mechanisms
of their transcription and associated regulations.
The corpus roughly contains three kinds of genic
Figure 3: Examples of relations to be extracted.
69
interaction mentions, namely regulations, regulon
membership and binding. The first case corresponds
to interactions the mechanism of which is not explic-
itly given in the text. The mention only tells that the
transcription of a given gene is influenced by a given
protein, either positively (activation), negatively (in-
hibition) or in an unspecified way. The second kind
of genic interaction mention (regulon membership)
basically conveys the same information, using the
regulon term/concept. The regulon of a gene is the
set of genes that it controls. In that case, the interac-
tion is expressed by saying that a gene is a member
of some regulon. The third and last kind of mention
provides with more mechanistic details on a regula-
tion, since it describes the binding of a protein near
the promoter of a target gene. This motivates the in-
troduction of Promoter and Site entities, which cor-
respond to DNA regions. It is thus possible to extract
the architecture of a regulatory DNA region, linking
a protein agent to its gene target (see Figure 3).
The set of entity types is divided into two main
groups, namely 10 genic entities and 3 kinds of ac-
tion (Table 5). Genic entities represent biological
objects like a gene, a group of genes or a gene prod-
uct. In particular, a GeneComplex annotation corre-
sponds to an operon, which is a group of genes that
are contiguous in the genome and under the control
of the same promoter. The annotation GeneFamily
is used to denote either genes involved in the same
biological function or genes with sequence homolo-
gies. More importantly, PolymeraseComplex anno-
tations correspond to the protein complex that is re-
sponsible for the transcription of genes. This com-
plex includes several subunits (components), com-
bined with a sigma factor, that recognizes specific
promoters on the DNA sequence.
The second group of entities are phrases express-
ing either molecular processes (e.g. sequestration,
dephosphorylation, etc.) or the molecular state of
the bacteria (e.g. presence, activity or level of a pro-
tein). They represent some kind of action that can
be performed on a genic entity. Note that transcrip-
tion and expression events were tagged as specific
actions, because they play a specific part in certain
relations (see below).
The annotation of entities and actions was pro-
vided to the participants, and the task consisted in
extracting the relations listed in Table 6.
Name Example
Gene cotA
GeneComplex sigX-ypuN
GeneFamily class III heat shock genes
GeneProduct yvyD gene product
Protein CotA
PolymeraseComplex SigK RNA polymerase
ProteinFamily DNA-binding protein
Site upstream site
Promoter promoter regions
Regulon regulon
Action activity | level | presence
Expression expression
Transcription transcription
Table 5: List of molecular entities and actions in GI.
Name Example
ActionTarget expression of yvyD
Interaction ComK negatively regulates
degR expression
RegulonDependence sigmaB regulon
RegulonMember yvyD is member of sigmaB
regulon
BindTo GerE adheres to the pro-
moter
SiteOf -35 sequence of the pro-
moter
PromoterOf the araE promoter
PromoterDependence GerE-controlled promoter
TranscriptionFrom transcription from the up-
stream site
TranscriptionBy transcription of cotD by
sigmaK RNA polymerase
Table 6: List of relations in GI.
The relations are binary and directed, and rely the
entities defined above. The three kinds of interac-
tions are represented with an Interaction annotation,
linking an agent to its target. The other relations
provide additional details on the regulation, like ele-
mentary components involved in the reaction (sites,
promoters) and contextual information (mainly pro-
vided by the ActionTarget relations). A formal def-
inition of relations and relation argument types can
be found on the Bacteria GI Task Web page.
3.1 Bacteria Gene Interactions corpus
The source of the Bacteria GI Task corpus is a set
of PubMed abstracts mainly dealing with the tran-
70
scription of genes in Bacillus subtilis. The semantic
annotation, derived from the ontology of (Manine et
al., 2008), contains 10 molecular entities, 3 different
actions, and 10 specialized relations. This is applied
to 162 sentences from the LLL set (Ne?dellec, 2005),
which are provided with manually checked linguis-
tic annotations (segmentation, lemmatization, syn-
tactic dependencies). The corpus was split into 105
sentences for training, 15 for development and 42
for test. Table 7 gives the distribution of the entities
and actions per corpus and Table 8 gives the distri-
bution of the relations per corpus.
3.2 Annotation procedures and guidelines
The semantic annotation scheme was developed by
two annotators through a series of independent an-
notations of the corpus, followed by reconciliation
steps, which could involve concerted modifications
(Manine et al, 2010). As a third and final stage, the
Entity or action Train. + Dev. Test
Documents (105+15) 120 42
Protein 219 85
Gene 173 56
Transcription 53 21
Promoter 49 10
Action 45 22
PolymeraseComplex 43 14
Expression 29 6
Site 22 8
GeneComplex 19 4
ProteinFamily 12 3
Regulon 11 2
GeneProduct 10 3
GeneFamily 6 5
Table 7: Distribution of entities and actions in GI.
Relation Train. + Dev. Test
Interaction 208 64
ActionTarget 173 47
PromoterOf 44 8
BindTo 39 4
PromoterDependence 36 4
TranscriptionBy 36 8
SiteOf 23 6
RegulonMember 17 2
TranscriptionFrom 14 2
RegulonDependence 12 1
Table 8: Distribution of relations in GI.
corpus was reviewed and the annotation simplified
to make it more appropriate to the contest. The final
annotation contains 748 relations distributed in nine
categories, 146 of them belonging to the test set.
The annotation scheme was generally well suited
to accurately represent the meaning of the sentences
in the corpus, with one notable exception. In the cor-
pus, there is a common phrasing telling that a pro-
tein P regulates the transcription of a gene G by a
given sigma factor S. In that case, the only anno-
tated interactions are between the couples (P, G) and
(S, G). This representation is not completely satis-
factory, and a ternary relation involving P, S and G
would have been more adequate.
Additional specific rules were needed to cope
with linguistic issues. First, when the argument of a
relation had coreferences, the relation was repeated
for each maximally precise coreference of the argu-
ment. Second, in case of a conjunction like ?sig-
maA and sigmaX holoenzymes?, there should ide-
ally be two entities (namely ?sigmaA holoenzyme?
and ?sigmaX holoenzyme?); however, this is not
easy to represent using the BioNLP format. In this
situation, we grouped the two entities into a single
one. These cases were rare and unlikely affected the
feasibility of the task, since entities were provided
in the test set.
3.3 Gene Interactions evaluation procedure
The training and development corpora with the ref-
erence annotations were made available to partici-
pants by December, 1st on the BioNLP shared Task
pages together with evaluation software. The test
corpus with the entity annotations has been made
available by March, 1st. The participants sent the
predicted annotations to the BioNLP shared Task
organizers by March, 10th. The evaluation results
were computed and provided to the participants and
on the Web site the same day. The participants are
evaluated and ranked according to two scores: F-
score for all event types together, and F-score for
the Interaction event type. In order for a predicted
event to count as a hit, both arguments must be the
same as in the reference in the right order and the
event type must be the same as in the reference.
71
3.4 Results of GI Task participants
There was only one participant, whose results are
shown in Tables 9 and 10. Some relations were
not significantly represented in the test set and thus
the corresponding results should be considered with
caution. This is the case for RegulonMember and
TranscriptionFrom, only represented two times each
in the test. The lowest recall, 17%, obtained for the
SiteOf relation is explained by its low representa-
tion in the corpus: most of the test errors come from
a difficult sentence with coreferences.
The recall of 56% for the Interaction relation cer-
tainly illustrates the heterogeneity of this category,
gathering mentions of interactions at large, as well
as precise descriptions of gene regulations. For in-
stance, Figure 4 shows a complex instance where all
of the interactions were missed. Surprisingly, we
also found false negatives in rather trivial examples
(?ykuD was transcribed by SigK RNA polymerase
from T4 of sporulation.?). Uturku used an SVM-
based approach for extraction, and it is thus delicate
to account for the false negatives in a simple and
concise way.
Event U. Turku scores
Global Precision 85
Global Recall 71
Global F-score 77
Interaction Precision 75
Interaction Recall 56
Interaction F-score 64
Table 9: University of Turku global scores.
Event Prec. Rec. F-score
Global 85 71 77
ActionTarget 94 92 93
BindTo 75 75 75
Interaction 75 56 64
PromoterDependence 100 100 100
PromoterOf 100 100 100
RegulonDependence 100 100 100
RegulonMember 100 50 67
SiteOf 100 17 29
TranscriptionBy 67 50 57
TranscriptionFrom 100 100 100
Table 10: University of Turku scores for each relation.
Figure 4: Examples of three missed interactions.
3.5 Discussion
The GI corpus was previously used in a relation
extraction work (Manine et al 2009) based on In-
ductive Logic Programming (Muggleton and Raedt,
1994). However a direct comparison of the results
is not appropriate here since the annotations were
partially revised, and the evaluation setting was dif-
ferent (leave-one-out in Manine?s work, test set in
the challenge).
Nevertheless, we note similar tendencies if we
compare relative results between relations. In partic-
ular, it was also found in Manine?s paper that SiteOf,
TranscriptionBy and Interaction are the most diffi-
cult relations to extract. It is also worth to mention
that both approaches rely on syntactic dependencies,
and use the curated dependencies provided in the
corpus. Interestingly, the approach by the University
of Turku reports a slightly lower F-measure with de-
pendencies calculated by the Charniak parser (about
1%, personal communication). This information is
especially important in order to consider a produc-
tion setting.
4 Conclusion
The quality of results for both challenges suggests
that current methods are mature enough to be used
in semi-automatic strategies for genome annotation,
where they could efficiently assist biological experts
involved in collaborative annotation efforts (Lam-
mers et al, 2010). However, the false positive rate,
notably for the Interaction relation, is still too high
for the extraction results to be used as a reliable
source of information without a curation step.
Acknowlegments
We thank Franc?oise Tisserand and Bernard Talercio
(INIST) for their work on the Rename corpus, and
the QUAERO Programme funded by OSEO (French
agency for innovation) for its support.
72
References
Artstein R., Poesio M. (2008). Inter-coder agreement
for Computational Linguistics. Computational Lin-
guistics, 34(4):555-96.
Bjo?rne J., Heimonen J., Ginter F., Airola A., Pahikkala
T., Salakoski T. (2009). Extracting complex biological
events with rich graph-based feature sets. BioNLP?09
Proc. Workshop Current Trends in Biomedical Natural
Language Processing: Shared Task, pp. 10-18.
Bonneau-Maynard H., Rosset S., Ayache C., Kuhn A.,
Mostefa D. (2005). Semantic annotation of the French
Media Dialog Corpus. Interspeech-2005, pp. 3457-60.
Demner-Fushman D., Ananiadou S., Cohen K.B., Pestian
J., Tsujii J., Webber B. (2008). Themes in biomedical
natural language processing: BioNLP08. BMC Bioin-
formatics, 9(Suppl. 11):S1.
Flo?rez L.A., Roppel S.F., Schmeisky A.G., Lammers
C.R., Stu?lke J. (2009). A community-curated con-
sensual annotation that is continuously updated: The
Bacillus subtilis centred wiki SubtiWiki. Database,
2009:bap012.
Fort K., Franc?ois C., Ghribi M. (2010). ?Evaluer des an-
notations manuelles disperse?es : les coefficients sont-
ils suffisants pour estimer l?accord inter-annotateurs ?
17e Conf. Traitement Automatique des Langues Na-
turelles (TALN 2010).
Kim J.D., Ohta T., Tsujii J. (2008) Corpus annotation for
mining biomedical events from literature. BMC Bioin-
formatics, 9:10.
Kim J.D., Ohta T., Pyysalo S., Kano Y., Tsujii J. (2009).
Overview of BioNLP?09 shared task on event ex-
traction. BioNLP?09 Proc. Workshop Current Trends
in Biomedical Natural Language Processing: Shared
Task, pp. 1-9.
Krallinger M., Leitner F., Rodriguez-Penagos C., Va-
lencia A. (2008). Overview of the protein-protein in-
teraction annotation extraction task of BioCreative II.
Genome Biology, 9(Suppl. 2):S4.
Lammers C.R., Flo?rez L.A., Schmeisky A.G., Roppel
S.F., Ma?der U., Hamoen L., Stu?lke J. (2010). Con-
necting parts with processes: SubtiWiki and Subti-
Pathways integrate gene and pathway annotation for
Bacillus subtilis. Microbiology, 156(3):849-59.
Manine A.P., Alphonse E., Bessie`res P. (2008). Informa-
tion extraction as an ontology population task and its
application to genic interactions. 20th IEEE Int. Conf.
Tools with Artificial Intelligence (ICTAI?08), pp. 74-
81.
Manine A.P., Alphonse E., Bessie`res P. (2009). Learn-
ing ontological rules to extract multiple relations of
genic interactions from text. Int. J. Medical Informat-
ics, 78(12):e31-8.
Manine A.P., Alphonse E., Bessie`res P. (2010). Extrac-
tion of genic interactions with the recursive logical the-
ory of an ontology. Lecture Notes in Computer Sci-
ences, 6008:549-63.
Muggleton S., Raedt L.D. (1994) Inductive Logic Pro-
gramming: Theory and methods. J. Logic Program-
ming, 19-20:629-79.
Ne?dellec C. (2005). Learning Language in Logic ? Genic
Interaction Extraction Challenge. Proc. 4th Learning
Language in Logic Workshop (LLL?05), pp. 31-7.
Weissenbacher, D. (2004). La relation de synonymie en
Ge?nomique. RECITAL 2004 Conference.
73
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 153?160,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP Shared Task 2013 ? An overview of the Genic Regulation Network Task 
Robert Bossy, Philippe Bessi?res, Claire N?dellec  Unit? Math?matique, Informatique et G?nome Institut National de la Recherche Agronomique UR1077, F78352 Jouy-en-Josas, France forename.name@jouy.inra.fr  Abstract 
The goal of the Genic Regulation Network task (GRN) is to extract a regulation network that links and integrates a variety of molecular interactions between genes and proteins of the well-studied model bacterium Bacillus subtilis. It is an extension of the BI task of BioNLP-ST?11. The corpus is composed of sentences selected from publicly available PubMed scientific abstracts. The paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results. 1 Introduction  The Genic Regulation Network (GRN) task consists of (1) extracting information on molecular interactions between genes and proteins that are described in scientific literature, and (2) using this information to reconstruct a regulation network between molecular partners in a formal way. Several other types of biological networks can be defined at the molecular level, such as metabolisms, gene expressions, protein-protein interactions or signaling pathways.  All these networks are closely interconnected. For example, a gene codes for a protein that catalyzes the transformation of small molecules (metabolites), while the expression of the gene and its related regulation is controlled by other proteins. The concept of biological networks is not new. However, the development of new methods in molecular biology in the past twenty years has made them accessible at the level of an organism as a whole. These new methods allow for the design of large-scale experimental approaches with high throughput rates of data. They are then used to build static and dynamic models that represent the behavior of a cell in the field of Systems Biology (Kitano, 2002; de Jong, 2002). In this context, there has recently been a 
considerable focus on ?biological network inference?, that is to say the process of making inferences and predictions about these networks (D'haeseleer, et al, 2000). Therefore, it is expected that Information Extraction (IE) from scientific literature may play an important role in the domain, contributing to the construction of networks (Blaschke et al, 1999). IE also plays a role in the design and the validation of large-scale experiments, on the basis of detailed knowledge that has already been published. 2 Context Extracting molecular interactions from scientific literature is one of the most popular tasks in IE challenges applied to biology. The GRN task adds a supplementary level that is closer to the biological needs: the participant systems have to extract a regulation network from the text that links and integrates basic molecular interactions.  The GRN task is based on a series of previous challenges in IE that started with the LLL challenge in 2005 (N?dellec, 2005). The LLL corpus is a set of sentences of PubMed abstracts about molecular interactions of the model bacterium Bacillus subtilis. Originally, the LLL task defined a unique binary genic interaction relation between proteins and genes. Since then, it has evolved to include the description of interaction events in a fine-grained representation that includes the distinction between transcription, different types of regulations and binding events, as proposed by (Manine et al, 2009). This new schema better captures the complexity of regulations at the molecular level. Entities other than genes and proteins were introduced, such as DNA sites (e.g. transcription promoter sites, transcriptional regulator binding sites). We proposed the Genic Interaction task (Bossy et al, 2012) in the BioNLP?11 Shared Task with a full re-annotation of the LLL corpus that follows this schema. The GRN task in 
153
BioNLP-ST?13 builds on this corpus and includes annotation improvements and extensions that are detailed below. 3 Task description The BioNLP-ST 2013 GRN task consists of the automatic construction of the regulation network that can be derived from a set of sentences. As usual in relation extraction tasks, the GRN corpus includes text-bound annotations. However the extraction target is the network, which is a structure with a higher level of abstraction. GRN thus also provides an explicit procedure to derive a network from a set of text-bound annotations. The GRN annotation is stacked in four successive levels of annotation: 1. Text-bound entities represent genes, proteins and aggregates (families, complexes). Some entities directly relate to a gene and are given a unique gene identifier corresponding to a node of the network. These entities are hereby called genic named entities. 2. Biochemical events and relations are molecular-level events (e.g. transcription, binding) and detailed knowledge on relationships between entities (e.g. promoter of gene, regulon membership). 3. Interactions denote relations between entities and events and relations. Interactions are the first abstract annotations; they are the key to the construction of the network arcs. 4. Finally, the Genic Regulation Network is derived from the Interactions and from the identifiers of the named genic entities.  GerE is a DNA-binding protein that adheres    to the promoter of cotB and cotC    Figure 1. Example of annotated sentence.  Levels 1, 2 and 3 were obtained by a manual annotation of the GRN corpus sentences by a domain expert. Level 4 was automatically computed from the lower level annotations. The training corpus was provided to the participants with level 1, 2 and 3 annotations. The algorithm 
to compute the next level was described and implemented as a script and made available to the participants during the training stage of the challenge. The test corpus was provided with only  level 1 annotations (entities). The participants submitted their prediction either as a set of Interactions (level 3) or directly as a network (level 4). This setting allows the participants to train systems that work at different levels of abstraction. Submissions in the form of Interactions are translated into a Genic Regulation Network using the algorithm provided during the training stage. The evaluation of each submission is carried out by comparing the predicted network with the reference network. The reference network is itself computed from the gold level 1, 2 and 3 annotations of the test sentences. The following subsections describe the four annotation levels. The full annotation schema that specifies the constraints on event and relation arguments can be found on the task web page1. 3.1 Text-bound entity types Text-bound entities come in three kinds: event trigger words, genic entities and entity aggregates. Trigger words are of type Action, they serve as anchors for events. Genic entities represent mentions of biochemical objects of the bacteria cell. Genic entity types include Gene, mRNA, Promoter, Protein and Site. Finally aggregates denote composite objects of the bacteria cell. Aggregate types are: - GeneFamily: homologous gene families. - Operon: operons sensu prokaryotes. - PolymeraseComplex: RNA polymerase complexes, either the core complex alone, or bound to a sigma factor. - ProteinComplex: protein complexes formed by several proteins that bind together. - ProteinFamily: homologous protein families. - Regulon: regulons, sensu prokaryotes. 3.2 Biochemical events and relation types Biochemical events and relations represent the knowledge of cellular mechanisms at the molecular level. There are three types of events: - Transcription_by represents the transcription event by a specific RNA 
                                                       1 https://sites.google.com/site/bionlpst2013/tasks/gene-regulation-network 
Master of  Promoter Interaction 
Promoter of 
154
polymerase. Its agent is usually a PolymeraseComplex. - Transcription_from represents the transcription from a specific site or promoter. - Action_Target is a generic bio-molecular event. The relation types represent three major genetic regulation patterns in bacteria: promoter activation, regulons and binding to specific DNA sites. Two types of relations specifically denote mechanisms that involve promoters: - Promoter_of is a relation between a gene (or operon) and its promoter. - Master_of_Promoter relation represents the control of the transcription from a specific promoter by a proteic entity (Protein, ProteinComplex or ProteinFamily). Two other relation types represent the function of regulons: - Member_of_Regulon relation denotes the membership of a genic entity to a regulon. - Master_of_Regulon relation represents the control of the activity of an entire regulon by a protein. Finally two types are used to represent relations that are common to different regulation mechanisms: - Bind_to relation represents the binding of a proteic entity to a site on the chromosome. - Site_of relation denotes the belonging of a chromosomal site to a genic entity such as a gene or a promoter. 3.3 Interaction types Interaction relations are labeled with one of six types grouped into a small hierarchy following two axes: mechanism and effect. The hierarchical levels are figured here by the text indentations. Regulation Binding Transcription Activation Requirement Inhibition Figure 2. Types of Interaction relations 
The Binding and Transcription types specify the mechanism through which the agent regulates the target. In a Binding Interaction, the agent binds to the target; this includes Protein-DNA binding and excludes Protein-Protein binding mechanisms. In a Transcription Interaction, the agent affects the transcription of the target. The Activation, Requirement and Inhibition types specify the effect of the agent on the target. In an Activation Interaction, the agent increases the expression of the target. In a Requirement Interaction, the agent is necessary for the expression of the target. In an Inhibition Interaction, the agent reduces the expression of the target. The Regulation type is the default type: in such interactions, neither the mechanism nor the effect is specified. 3.4 Genic Regulation Network inference algorithm The genic regulation network corresponding to a corpus is inferred from the set of Interaction relations. The network presents itself as a directed labeled graph where nodes represent gene identifiers and edges represent gene interactions. The inference is done in two steps: the resolution of Interaction relations and the removal of redundant arcs. Step 1: Resolution of Interaction relations The agent and the target of an Interaction relation are not necessarily genic named entities. They can be secondary events or relations, another Interaction, or auxiliary entities (e.g. Promoter). The resolution of an Interaction aims to look for the genic named entity in order to infer the node concerned by the network edge. The resolution of Interaction arguments is performed using the rules specified below. These rules express well-known molecular mechanisms in a logical manner: 1. If the agent (or target) is a genic named entity, then the agent (or target) node is the gene identifier of the entity. If the entity does not have a gene identifier, then it is not a genic named entity and there is no node (and thus no edge). 2. If the agent (or target) is an event, then the agent (or target) node is the entity referenced by the event. 3. If the agent (or target) is a relation, then the agent (or target) of both arguments of the relation are nodes. 
Mechanism 
Effect 
155
4. If the target is a Promoter and this promoter is the argument of a Promoter_of relation, then the target node is the other argument of the Promoter_of relation. i.e. if A interacts with P, and P is a promoter of B, then A interacts with B. 5. If the agent is a Promoter and this promoter is the argument of a Master_of_Promoter relation, the agent is the other argument of the Master_of_Promoter relation. i.e. if A is the master of promoter P, and P interacts with B, then A interacts with B. The resolution of Interaction arguments consists of a traversal of the graph of annotations where these rules are applied iteratively. Event and relation arguments are walked through. Promoter entities are handled according to rules 4 and 5. If the resolution of the agent or the target yields more than one node, then the Interaction resolves to as many edges as the Cartesian product of the resolved nodes. For instance, if both the agent and the target resolve to two nodes, the Interaction relation resolves into four edges. Edges are labeled with the same set of types as the Interactions. Each edge inherits the type of the Interaction relation from which it has been inferred. Step 2: Removal of redundant arcs In this step, edges with the same agent, target and type are simplified into a single edge. This means that if the same Interaction is annotated several times in the corpus, then it will resolve into a single edge. This means that the prediction of only one of the interactions in the corpus is enough to reconstruct the edge. Moreover, Interaction types are ordered according to the hierarchy defined in the preceding section. Since the sentences are extracted from PubMed abstracts published during different periods, they may mention the same Interaction with different levels of detail, depending on the current state of knowledge. For a given edge, if there is another edge for the same node pair with a more specialized type, then it is removed. For instance, the edges (A, Regulation, B) and (A, Transcription, B) are simplified into (A, Transcription, B). Indeed the former edge conveys no additional information in comparison with the latter.  4 Corpus description The GRN corpus is a set of 201 sentences selected from PubMed abstracts, which are 
mainly about the sporulation phenomenon in Bacillus subtilis. This corpus is an extended version of the LLL and BI (BioNLP-ST?11) corpora. The additional sentences ensure a better coverage of the description of the sporulation. An expert of this phenomenon examined the regulation network derived from the annotation of the original sentences, and then manually listed the important interactions that were missing. We selected sentences from PubMed abstracts that contain occurrences of the missing pairs of genes. In this way, the genic interaction network is more complete with respect to the sporulation. Moreover, the publications from which the sentences are extracted cover a wider period, from 1996 to 2012. They represent a diverse range of writing styles and experimental methods. 42 sentences have been added, but 4 sentences were removed from the BI sentences because they described genic interactions in bacteria other than Bacillus subtilis. The distribution of the sentences among the training, development and test sets has been done in the following way: - Legacy sentences belong to the same set as in previous evaluation campaigns (LLL and BI). - Additional sentences have been randomly distributed to training, development and test sets. The random sampling has been constrained so that the proportion of different types of interactions is as much as possible the same as in the three sets. The GRN task does not include the automatic selection by the participant methods of the relevant sentences, which are provided. With regards to a real-world application, this selection step can be achieved with good performance by sentence filtering, as demonstrated by N?dellec et al (2001), by using a Naive Bayesian classifier. Moreover, the corpus contains sentences with no interaction. Tables 1 to 3 detail the distribution of the entities, relations and events in the corpus. They are balanced between the training and test sets: the test represents between a quarter and a third of the annotations. Table 1 details the entity frequency and their distributions by type. Column 5 contains the contribution of each entity type to the total. Genes and proteins represent two thirds of the entities, since they are the main actors in genic interactions. It is worth noting that the high number of promoters and polymerase complexes is specific to bacteria 
156
where the biological mechanisms are detailed at a molecular level.  Entity # Train+Dev Test Gene 199 70% 30% GeneFamily 2 50% 50% mRNA 1 100%  0% Operon 33 67% 33% PolymeraseComplex 62 71% 29% Promoter 63 73% 27% Protein 486 65% 35% ProteinComplex 7 100%  0% ProteinFamily 18 78% 22% Regulon 14 79% 21% Site 32 78% 22% Total 917 68% 32% 
Table 1. Entity distribution in the GRN corpus. Table 2 details the distribution of the biochemical events and relations (level 2). The most frequent event is Action Target. Action Target links, for instance, Transcription by and Transcription from events to the target gene.  Event/Relation # Train+dev Test Action target 226 68% 32% Bind to 9 78% 22% Master of Promoter 60 80% 20% Master of Regulon 13 85% 15% Member of Regulon 12 92% 8% Promoter of 47 72% 28% Site of 24 75% 25% Transcription by 86 71% 29% Transcription from 18 78% 22% 
Total 495 72% 28% Table 2. Distribution of the biochemical events and relations in the GRN corpus. Finally, Table 3 details the distribution of the Interaction relations (level 3). The distribution 
among Interaction relations is more uniform than among entities and molecular events. The frequency of the Transcription relation is much higher than Binding, which is not surprising since transcription is the major mechanism of regulation in bacteria, while binding is rare. Conversely, the relative frequency of relations among Effect types of relations is balanced.  Interaction  # Train+dev Test Regulation 80 65% 35% Inhibition 50 66% 34% Activation 49 67% 33% Requirement 35 66% 34% Binding 12 75% 25% Transcription 108 74% 26% Total 334 69% 31% Table 3. Distribution of the Interaction relations in the GRN corpus. 5 Annotation methodology A senior biologist, who is a specialist of Bacillus subtilis and a bioinformatician, a specialist of semantic annotation, defined the annotation schema. The biologist annotated the whole corpus, using the BI annotations as a starting point. The bioinformatician carefully checked each annotation. They both used the AlvisAE Annotation Editor (Papazian et al, 2012) that supported their productivity due to its intuitive visualization of dense semantic annotations. Subtiwiki provided the identifiers of genes and proteins (Fl?rez et al, 2009). Subtiwiki is a community effort that has become the reference resource for the gene nomenclature normalization of Bacillus subtilis. Other genic named entities, like operons, families or protein complexes, were given an identifier similar to their surface form. Several annotation iterations and regular cross-validations allowed the annotators to refine and normalize these identifiers. The consistency of the annotations was checked by applying the rules of the network inference procedure that revealed contradictions or dangling events. The biologist double-checked the inferred network against his deep expertise of sporulation in Bacillus subtilis. 
157
6 Evaluation procedure 6.1 Campaign organization The same rules and schedule were applied to GRN as the other BioNLP-ST tasks. The training and development data were provided eleven weeks before the test set. The submissions were gathered through an on-line service, which was active for ten days. We took into account the final run of each participant to compute the official scores. They were published on the BioNLP-ST web site together with the detailed scores. 6.2 Evaluation metrics The predictions of the participating teams were evaluated by comparing the reference network to the predicted network that was either submitted directly, or derived from the predicted Interactions. Since the genic named entity annotations are provided with their identifier, the network nodes are fixed. Therefore, the evaluation consists of comparing the edges of the two networks. Their discrepancy is measured using the Slot Error Rate (SER) defined by (Makhoul et al, 1999) as: SER = (S + D + I) / N where: - S is the number of substitutions (i.e. edges predicted with the wrong type) - D is the number of deletions (false negatives) - I is the number of insertions (false positives) - N is the number of arcs in the reference network. The SER has the advantage over F1, namely it uses an explicit characterization of the substitutions. (Makhoul et al, 1999) demonstrates that the implicit comprehension of substitutions in both recall and precision scores leads to the underestimation of deletions and insertions in the F score. However, we compute the Recall, Precision and F1 in order to make the interpretation of results easier: Recall = M / N Precision = M / P where: - M is the number of matches (true positives). - P is the number of edges in the predicted network. Matches, substitutions, deletions and insertions are counted for each pair of nodes. The genic regulation network is an oriented graph, thus the 
node pairs (A,B) and (B,A) are handled independently. For a given node pair (A,B), the number of exact matches (M) is the number of edges with the same type in the prediction as in the reference. The number of substitutions, deletions and insertions depends on the number of remaining edges. We name q and r, the number of remaining edges between two nodes A and B in the prediction and the reference respectively: - S = min(q, r) - if q > r, then I = q ? r, D = 0 - if q < r, then I = 0, D = r ? q In other words, edges from the prediction and the reference are paired, first by counting matches, then by maximizing substitutions. The remaining edges are counted either as insertions or deletions depending if the extra edges are in the prediction or reference, respectively. The values of S, D, I and M for the whole network are the sum of S, D, I and M on all the node pairs. 7 Results 7.1 Participating systems Five systems participated in GRN: - University of Ljubljana (Slovenia) (?itnik et al, 2013),  - K.U.Leuven (Belgium) (Provoost and Moens, 2013),  - IRISA-TexMex (INRIA, France) (Claveau, 2013), - EVEX (U. of Turku / TUCS, Finland and VIB / U. of Ghent, Belgium) (Hakala et al, 2013),  - TEES-2.1 (TUCS, Finland) (Bj?rne and Salakoski, 2013). 
 Participant SER Recall Precision U. of Ljubljana  0.73 34% 68% K.U.Leuven  0.83 23% 50% TEES-2.1  0.86 23% 54% IRISA-TexMex  0.91 41% 40% EVEX  0.92 13% 44% Table 4. Final evaluation of the GRN task. Teams are ranked by SER. S: Substitutions, D: Deletions, I: Insertions, M: Matches. 
158
Table 4 summarizes the scores by decreasing order. The scores are distributed between the best SER, 0.73 achieved by the University of Ljubljana, 20 points more than the lowest at 0.92. For all systems, the number of insertions is much lower than the number of deletions, except for IRISA-TexMex. The substitutions correspond to the edges that were predicted with the wrong type. In order to reveal the quality of the predictions with regards to the edge types, we calculated two alternate SERs. The results are displayed in Table 5.The SER Network Shape is obtained by erasing the type of all of the edges in the reference and predicted networks, as if all edges were of the Regulation type. The SER Network Shape measures the capacity of the systems to reconstruct the unlabeled shape of the regulation network. The SER Effect is obtained by erasing the mechanism types of all edges only, as if Binding and Transcription edges were of type Regulation. The Effect edges are kept unchanged. The SER Effect measures the quality of the predictions for valued networks that only contain Effect edges.  Participant SER SER Shape SER Effect U. of Ljubljana 0.73 0.60 0.74 
K.U. Leuven 0.83 0.64 0.83 
TEES-2.1 0.86 0.74 0.84 
IRISA-TexMex 0.91 0.51 0.87 
EVEX 0.92 0.79 0.91 
Table 5. Scores obtained by erasing edge types (Network Shape) or mechanism types (Effect). The SER Network Shape is significantly better for all systems, but the impact is dramatic for IRISA-TexMex and K.U. Leuven, showing that the typing of relations may be the major source of error. The SER Effect does not differ significantly from the original score. We deduce from the comparison of the three scores that the types that are the hardest to discriminate are effect types. This result is interesting because Effect labels are in fact the most valuable for systems biology and network inference studies. U. of Ljubljana and TEES-2.1 submissions contained level 2 and 3 predictions (interactions and biochemical events). IRISA provided only 
predictions at level 3 (interactions only). K.U. Leuven and EVEX directly submitted a network. The performance of the systems that use annotations of level 2 confirms our hypothesis that a significant part of the interactions can be deduced from low-level events. 7.2     Systems description and result analysis All systems applied machine-learning algorithms with linguistic features that were stems or lemmas, POS-tags and parses, most of them being provided by the BioNLP supporting resources. With the exception of K.U. Leuven, all systems used dependency paths between candidate arguments. However different ML algorithms were used, as shown in Table 6.  Participant ML algorithm U. Ljubljana Linear-chain CRF K.U.Leuven SVM (Gaussian RBF) TEES-2.1 SVMmulticlass (linear) IRISA-TexMex kNN (language model) EVEX SVM (TEES-2.1) Table 6. ML algorithms used by the participants. Beyond syntactic parses and ML algorithms, the participant systems combined many different sources of information and processing, so that no definitive conclusion on the respective potential of the methods can be drawn here. 8 Conclusion The GRN task has a strong legacy since the corpus is derived from LLL. Moreover, the GRN task has advanced a novel IE setting. We proposed to extract a formal data structure from successive abstract layers. Five different teams participated in the task with distinct strategies. In particular, we received submissions that work on all proposed abstraction levels. This shows that Information Extraction implementations have reached a state of maturity, which allow for new problems to be addressed quickly. The performances are promising, yet some specific problems have to be addressed, like the labeling of edges. Acknowledgments This work was partially supported by the Quaero programme funded by OSEO (the French agency for innovation). 
159
References Jari Bj?rne, Tapio Salakoski. 2013. TEES 2.1: Automated Annotation Scheme Learning in the BioNLP 2013 Shared Task. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. Christian Blaschke, Miguel A. Andrade, Christos Ouzounis, Alfonso Valencia. 1999. Automatic Extraction of Biological Information From Scientific Text: Protein-Protein Interactions. Proceedings of the International Conference on Intelligent Systems for Molecular Biology (ISMB 1999), 60-67.  Robert Bossy, Julien Jourde, Alain-Pierre Manine, Philippe Veber, Erick Alphonse, Marteen van de Guchte, Philippe Bessi?res, Claire N?dellec. 2012. BioNLP Shared Task - The Bacteria Track. BMC Bioinformatics. 13(Suppl 11):S3. Vincent Claveau. 2013. IRISA participation to BioNLP-ST 2013: lazy-learning and information retrieval for information extraction tasks. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. Patrik D'haeseleer, Shoudan Liang, Roland Somogyi. 2000. Genetic network inference: from co-expression clustering to reverse engineering. Bioinformatics. 16(8):707-726. Lope A. Fl?rez, Sebastian F Roppel, Arne G Schmeisky, Christoph R Lammers, J?rg St?lke. 2009. A community-curated consensual annotation that is continuously updated: the Bacillus subtilis centred wiki SubtiWiki. Database (Oxford), 2009:bap012. Kai Hakala, Sofie Van Landeghem, Tapio Salakoski, Yves Van de Peer and Filip Ginter. 2013. EVEX in ST?13: Application of a large-scale text mining resource to event extraction and network construction. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. 
GenBank. http://www.ncbi.nlm.nih.gov/  Hidde de Jong. 2002. Modeling and simulation of genetic regulatory systems: a literature review. J. Computational Biology, 9(1):67-103. Hiroaki Kitano. 2002. Computational systems biology. Nature, 420(6912):206-210. John Makhoul, Francis Kubala, Richard Schwartz and Ralph Weischedel. 1999.  Performance measures for information extraction. In Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February. Alain-Pierre Manine, Erick Alphonse, Philippe Bessi?res. 2009. Learning ontological rules to extract multiple relations of genic interactions from text. Int. J. Medical Informatics, 78(12):31?38. Claire N?dellec, Mohamed Ould Abdel Vetah, Philippe Bessi?res. 2001. Sentence filtering for information extraction in genomics, a classification problem. Practice of Knowledge Discovery in Databases (PKDD 2001), 326-337. Claire N?dellec. 2005. Learning Language in Logic - Genic Interaction Extraction Challenge" in Proceedings of the Learning Language in Logic (LLL05) workshop joint to ICML'05. Cussens J. and N?dellec C. (eds). Bonn. Fr?d?ric Papazian, Robert Bossy and Claire N?dellec. 2012. AlvisAE: a collaborative Web text annotation editor for knowledge acquisition. The 6th Linguistic Annotation Workshop (The LAW VI), Jeju, Korea. Thomas Provoost, Marie-Francine Moens. 2013. Detecting Relations in the Gene Regulation Network. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. Slavko ?itnik, Marinka ?itnik, Bla? Zupan, Marko Bajec. 2013. Extracting Gene Regulation Networks Using Linear-Chain Conditional Random Fields and Rules. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics.  
160
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 161?169,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP shared Task 2013 ? An Overview of the  Bacteria Biotope Task 
Robert Bossy1, Wiktoria Golik1, Zorana Ratkovic1,2, Philippe Bessi?res1, Claire N?dellec1  1Unit? Math?matique, Informatique et G?nome MIG INRA UR1077 ? F-78352 Jouy-en-Josas ? France 2LaTTiCe UMR 8094 CNRS, 1 rue Maurice Arnoux, F-92120 Montrouge ? France forename.name@jouy.inra.fr  Abstract 
This paper presents the Bacteria Biotope task of the BioNLP Shared Task 2013, which follows BioNLP-ST-11. The Bacteria Biotope task aims to extract the location of bacteria from scientific web pages and to characterize these locations with respect to the OntoBiotope ontology. Bacteria locations are crucial knowledge in biology for phenotype studies. The paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results.  1 Introduction The Bacteria Biotope (BB) task extends the BioNLP 2013 Shared Task molecular biology scope. It consists of extracting bacteria and their locations from web pages, and categorizing the locations with respect to the OntoBiotope1 ontology of microbe habitats. The locations denote the places where given species live. The bacteria habitat information is critical for the study of the interaction between the species and their environment, and for a better understanding of the underlying biological mechanisms at a molecular level. The information on bacteria biotopes and their properties is very abundant in scientific literature and in genomic databases and BRC (Biology Resource Center) catalogues. However, the information is highly diverse and expressed in natural language (Bossy et al, 2012). The two critical missing steps for population of biology databases and biotope knowledge modeling are (1) the automatic extraction of organism/location pairs and (2) the normalization of the habitat names with respect to biotope ontologies.                                                         1http://bibliome.jouy.inra.fr/MEM-OntoBiotope/OntoBiotope_BioNLP-ST13.obo 
The aim of the previous edition of the BB task (BioNLP-ST?11) was to solve the first information extraction step. The results obtained by the participant systems reached 45 percent F-measure. These results showed both the feasibility of the task, as well as a large room for improvement (Bossy et al, 2012).  The 2013 edition of the BB task maintains the primary objective of event extraction, and introduces the second issue of biotope normalization. It is handled through the categorization of the locations into a large set of types defined in the OntoBiotope ontology. Bacteria locations range from hosts, plant and animals, to natural environments (e.g. water, soil), including industrial environments.  BB?11 set of categories contained 7 types. This year, entity categorization has been enriched to better answer the biological needs, as well as to contribute to the general problem of automatic semantic annotation by ontologies. BB task is divided into three sub-tasks. Entity detection and event extraction are tackled by two distinct sub-tasks, so that the contribution of each method could be assessed. A third sub-task conjugates the two in order to measure the impact of the method interactions. 2 Context Biological motivation. Today, new sequencing methods allow biologists to study complex environments such as microbial ecosystems. Therefore, the sequence annotation process is facing radical changes with respect to the volume of data and the nature of the annotations to be considered. Not only do biochemical functions still need to be assigned to newly identified genes, but biologists have to take into account the conditions and the properties of the ecosystems in which microorganisms are living and are identified, as well as the interactions and relationships developed with their environment and other 
161
living organisms (Korbel et al, 2005). Metagenomic studies of ecosystems yield important information on the phylogenetic composition of the microbiota. The availability of bacteria biotope information represented in a formal language would then pave the way for many new environment-aware bioinformatic services. The development of methods that are able to extract and normalize natural language information at a large scale would allow us to rapidly obtain and summarize information that the bacterial species or genera are associated with in the literature. In turn, this will allow for the formulation of hypotheses regarding properties of the bacteria, the ecosystem, and the links between them.  The pioneering work on EnvDB (Pignatelli et al, 2009) aimed to link GenBank sequences of microbes to biotope mentions in scientific papers. However, EnvDB was affected by the incompleteness of the GenBank isolation source field, the low number of related bibliographic references, the bag-of-words extraction method and the small size of its habitat classification. Habitat categories. The most developed classifications of habitats are EnvO, the Metagenome classification supported by the Genomics Standards Consortium (GSC), and the OntoBiotope ontology developed by our group. EnvO (Environment Ontology project) targets a Minimum Information about a Genome Sequence (MIGS) specification (Field et al, 2008) of mainly Eukaryotes. This ambitious detailed environment ontology aims to support standard manual annotations  of all types of organism environments and biological samples. However, it suffers from some limitations for bacterial biotope descriptions. A large part of EnvO is devoted to environmental biotopes and extreme habitats, whilst it fails to finely account for the main trends in bacteria studies, such as their technological use for food transformation and bioremediation, and their pathogenic or symbiotic properties. Moreover, EnvO terms are often poorly suited for bacteria literature analysis (Ratkovic et al, 2012). The Metagenome Classification  from JGI of DOE (Joint Genome Institute, US Department Of Energy) is intended to classify metagenome projects and samples according to a mixed typology of habitats (e.g. environmental, host) and their physico-chemical properties (e.g. pH, salinity) (Ivanova et al, 2010). It is a valuable 
source of vocabulary for the analysis of bacteria literature, but its structure and scope are strongly biased by the indexing of metagenome projects. The OntoBiotope ontology is appropriate for the categorization of bacteria biotopes in the BB task because its scope and its organization reflect the scientific subject division and the microbial diversity. Its size (1,756 concepts) and its deep hierarchical structure are suitable for a fine-grained normalization of the habitats. Its vocabulary has been selected after a thorough terminological analysis of relevant scientific documents, papers, GOLD (Chen et al, 2010) and GenBank, which was partly automated by term extraction. Related terms are attached to the OntoBiotope concept labels (i.e. 383 synonyms), improving OntoBiotope coverage of natural language documents.  Its structure and a part of its vocabulary have been inspired by EnvO, the Metagenome classification and the small ATCC (American Type Collection Culture) classification for microbial collections (Floyd et al, 2005). Explicit references to 34 EnvO terms are given in the OntoBiotope file. Its main topics are: - ? Artificial ? environments (industrial and domestic), Agricultural habitats, Aquaculture habitats, Processed food; - Medical environments, Living organisms, Parts of living organisms, Bacteria-associated habitats; - ? Natural ? environment habitats, Habitats wrt physico-chemical property (including extreme ones); - Experimental medium (i.e. experimental biotopes designed for studying bacteria). The structure, the comprehensiveness and the detail of the habitat classification are critical factors for research in biology. Biological investigations involving the habitats of bacteria are very diverse and still unanticipated. Thus, shallow and light classifications are insufficient to tackle the full extent of the biological questions. Indexing genomic data with a hierarchical fine-grained ontology such as OntoBiotope allows us to obtain aggregated and adjusted information by selecting the right level or axis of abstraction. Bacteria Biotope Task.  The corpus is the same as BB?11. The documents are scientific web pages intended for a general audience in the form of encyclopedia notices. They focus on a single organism or a family. The habitat mentions are dense and more diverse than 
162
in PubMed abstracts. These features make the task both useful and feasible with a reduced investment in biology. Its linguistic characteristics, high frequency of anaphora, entities denoted by complex nominal expressions raised interesting question for BioNLP that have been treated for a long time in the general and the biomedical domains. 3 Task description The BB Task is split into two secondary goals: 1. The detection of entities and their categorization(s) (Sub-task 1). 2. The extraction of Localization relations given the entities (sub-task 2) Sub-task 1 involves the prediction of habitat entities and their position in the text. The participant also has to assign each entity to one or more concepts of the OntoBiotope ontology: the categorization task. For instance, in the excerpt Isolated from the water of abalone farm, the entity abalone farm should be assigned the OntoBiotope category fish farm. Sub-task 2 is a relation extraction task. The schema of this task contains three types of entities: - The Habitat type is the same as in sub-task 1. - Geographical entities represent location and organization named entities. - Bacteria entities are bacterial taxa. Additionally, there are two types of relations illustrated by Figure 1. - Localization relations link Bacteria to the place where they live (either a Habitat or a Geographical). - PartOf relations relate couples of Habitat entities, a living organism, which is a host (e.g. adult human), and a part of this living organism (e.g. gut).  Bifidobacterium longum. This organism is 
found in adult humans and formula fed infants 
as a normal component of gut flora. 
Figure 1. Example of a localization event in the BB Task. Sub-task 2 participants are provided with document texts and entities, and should predict the relations between the candidate entities. 
Sub-task 3 is the combination of these two sub-tasks. It consists of predicting both the entity positions and the relations between entities. Compared to sub-task 1, the systems have to predict Habitat entities, but also Geographical and Bacteria entities. It is similar to the BB task of BioNLP-ST?11, except that no categorization of the entities is required. 4 Corpus description The BB corpus document sources are web pages from bacteria sequencing projects, (EBI, NCBI, JGI, Genoscope) and encyclopedia pages from MicrobeWiki. The documents are publicly available. Table 1 gives the distribution of the entities and relations in the corpora per sub-task.   Training + Dev Test 1 & 3 Test 2 Document 78 27 26 Word 25,828 7,670 10,353     Bacteria 1,347 332 541 Geographical 168 38 82 Habitat 1,545 507 623 OntoBiotope cat. 1,575 522 NA Total entities 3,060 877 1,246     Localization 1,030 269 538 Part of Host 235 111 129 Total relations 1,265 328 667 Table 1. BB?13 corpus figures. The categorization of entities by a large ontology (sub-task 1) offers a novel task to the BioNLP-ST community; a close examination of the annotated corpus allowed us to anticipate the challenges for participating teams. A total of 2,052 entities have been manually annotated for sub-task 1 (training, development and test sets together). These entities have 1,036 distinct surface forms, which means that an entity surface form is repeated a little less than twice, on average. However, only a quarter of the surface forms are actually repeated; three quarters are unique in the corpus. Moreover, 60% of habitat entities have a surface form that does not match one of the synonyms of their ontology concept. This configuration suggests that methods that simply propagate surface forms and concept attributions from ontology synonyms and from training entities would be inefficient. We have developed a baseline prediction that projects the ontology synonyms and the training corpus 
Localization Localization 
Part of Part of 
163
habitat surface forms onto the test. This prediction scores a high Slot Error Rate of 0.74. We also note there are a few ambiguous forms (i.e. 112 forms) that are synonyms in several different concepts or that do not always denote a habitat, and a few entities are assigned more than one concept (i.e. 42 of them). These are difficult cases that require prediction methods capable of word sense disambiguation. The low number of ambiguous occurrences has a low impact on the participant scores, although their presence may motivate more sophisticated methods. 5 Annotation methodology The methodology of entity position and relations annotation is similar to BB Task?11. It involved seven scientists who participated in a double-blind annotation (each document was annotated twice), followed by a conflict resolution phase. They used the AlvisAE annotation editor (Papazian et al, 2012). The guidelines included some improvements that are detailed below. Boundaries. Habitat entities may be either names or adjective. In the case of adjectives, the head is included in the entity span if it denotes a location (e.g. intestinal sample) and is excluded otherwise (e.g. hospital epidemic). The entity spans may be discontinuous, which is relevant for overlapping entities like ground water and surface water in ground and surface water. The major change is the inclusion of all modifiers that describe the location in the habitat entity span. This makes the entity more informative and the entity boundaries easier to predict, and less subject to debate. For instance, in the example,  isolated from the water of an abalone farm,  the water entity extends from water to farm. Note that in sub-task 1, all entities have to be predicted, even when not involved in a relation. This led to the annotation of embedded entities as potential habitats for bacteria, such as abalone farm and abalone in the above example.  Equivalent sets of entities.  As in BB?11, there are many equivalent mentions of the same bacteria in the documents that play a similar role with respect to the Localization relation. Selecting only one of them as the gold reference would have been arbitrary. When this is the case, the reference annotation includes equivalent sets of entities that convey the same information (e.g. Borrelia garinii vs. B. garinii, but not Borrelia).  
Category assignment. The assignment of categories to habitat entities has been done in two steps: (i) an automatic pre-annotation by the method of Ratkovic et al, (2012) and (ii) a manual double-blind revision followed by a conflict resolution phase. In the manual annotation phase, the most frequent conflicts between annotators were the same as in the previous edition. They involved the assignment of entities to either the living organism category, organic matter or food. An example is the cane entity in cane cuttings. To handle these cases, the guidelines assert that a dead organism cannot be assigned to a living organism category. The high quality of the pre-annotation and its visualization and revision using the AlvisAE annotation editor notably sped-up the annotation process. Table 2 summarizes the figures of the pre-annotation. For sub-task 1, the pre-annotation consisted of assigning OntoBiotope categories to entities for the whole corpus (train+dev+test). The pre-annotation yielded very high results with an F-measure of almost 90%. The pre-annotation was also useful to assess the relevance of the OntoBiotope ontology for the BB task. For sub-task 2, the pre-annotation consisted of the detection of entities in the test set, where no categorization is needed. The second line in Table 2 shows that the recall of entity detection affects the F-score, but that it still made the prediction helpful for the annotators. Further data analysis revealed that the terminology-based approach of the pre-annotation poorly detected the correct boundaries of embedded entities, thereby decreasing the recall of the entity recognition.   Recall Precision F1 Corpus sub-task1 89.7% 90.1% 89.9% Test sub-task 2 47.3% 95.7% 63.3% Table 2. Pre-annotation scores. 6 Evaluation procedure The evaluation procedure was similar to the previous edition in terms of resources, schedule and metrics except that an original relevant metric was developed for the new problem of entity categorization in a hierarchy.  6.1 Campaign organization The training and development corpora with the reference annotations were made available to the participants eleven weeks before the release of 
164
the test sets. Participating teams then had ten days to submit their predictions. As with all BioNLP-ST tasks, each participant submitted a single final prediction for each BB sub-task. The detailed evaluation results were computed, provided to the participants and published on the BioNLP website two days after the submission deadline.  6.2 Evaluation metrics Sub-task 1. In this sub-task participants were given only the document texts. They had to predict habitat entities along with their categorization with the OntoBiotope ontology. The evaluation of sub-task 1 takes into account the accuracy of the boundaries of the predicted entities as well as of the ontology category. Entity pairing. The evaluation algorithm performs an optimal pairwise matching between the habitat entities in the reference and the predicted entities. We defined a similarity between two entities that takes into account the boundaries and the categorization. Each reference entity is paired with the predicted entity for which the similarity is the highest among non-zero similarities.  If the boundaries of a reference entity do not overlap with any predicted entity, then it is a false negative, or a deletion. Conversely, if the boundaries of a predicted entity do not overlap with any reference entity, then it is a false positive, or an insertion. If the similarity between the entities is 1, then it is a perfect match. But if the similarity is lower than 1, then it is a substitution.  Entity similarity. The similarity M between two entities is defined as: M = J . W J measures the accuracy of the boundaries between the reference and the predicted entities. It is defined as a Jaccard Index adapted to segments (Bossy et al, 2012). For a pair of entities with the exact same boundaries, J equals to 1. W measures the accuracy between the ontology concept assignment of the reference entity and the predicted concept assignment of the predicted entity. We used the semantic similarity proposed by Wang, et al (2007). This similarity compares the set of all ancestors of the concept assigned to the reference entity and the set of all ancestors of 
the concept assigned to the predicted entity. The similarity is the Jaccard Index between the two sets of ancestors; however, each ancestor is weighted with a factor equal to: dw where d is the number of steps between the attributed concept and the ancestor. w is a constant greater than zero and lower than or equal to 1. If both the reference and predicted entities are assigned the same concept, then the sets of ancestors are equal and W is equal to 1. If the pair of entities has different concept attributions, W is lower than 1 and depends on the relative depth of the lowest common ancestor. The lower the common ancestor is, the higher the value of W. The exponentiation by the w constant ensures that the weight of the ancestors decreases non-linearly. This similarity thus favors predictions in the vicinity of the reference concept. Note that since the ontology root is the ancestor of all concepts, W is always strictly greater than zero. (Wang et al, 2007) showed experimentally that a value of 0.8 for the w constant is optimal for clustering purposes. However we noticed that w high values tend to favor sibling predictions over ancestor/descendant predictions that are preferable here, whilst low w values do not penalize enough ontology root predictions. We settled w with a value of 0.65, which ensures that ancestor/descendant predictions always have a greater value than sibling predictions, while root predictions never yield a similarity greater than 0.5. As specified above, if the similarity M < 1, then the entity pair is a substitution. We define the importance of the substitution S as: S = 1 - M Prediction score. Most IE tasks measure the quality of a prediction with Precision and Recall, eventually merged into an F1. However the pairing detects false positives and false negatives, but also substitutions. In such cases, the Recall and Precision factor the substitutions twice, and thus underestimate false negatives and false positives. We therefore used the Slot Error Rate (SER) that has been devised to undertake this shortcoming (Makhoul et al, 1999): SER = (S + I + D) / N where: - S represents the number of substitutions. 
165
- I represents the total number of insertions. - D represents the total number of deletions. - N is the number of entities in the reference. The SER is a measure of errors, so the lower it is the better. A SER equal to zero means that the prediction is perfect. The SER is unbound, though a value greater than one means that there are more mistakes in the prediction than entities in the reference. We also computed the Recall, the Precision and F1 measures in order to facilitate the interpretation of results: Recall =M / N Precision = M / P where M is the sum of the similarity M for all pairs in the optimal pairing, N is the number of entities in the reference, and P the number of entities in the prediction. Sub-task 2. In sub-task 2, the participants had to predict relations between candidate arguments, which are Bacteria, Habitat and Geographical entities. This task can be viewed as a categorization task of all pairs of entities. Thus, we evaluate submissions with Recall, Precision and F1. Sub-task 3. Sub-task 3 is similar to sub-task 2, but it includes entity prediction. This is the same setting as the BB task in BioNLP-ST 2011, except for entity categorization. We used the same evaluation metrics based on Recall, Precision and F1 (Bossy et al, 2012). The highlights of this measure are: ? it is based on the pairing between reference and predicted relations that maximizes a similarity; ? the similarity of the boundaries of Habitat and Geographical entities is relaxed and defined as the Jaccard Index (in the same way as in sub-task 1); ? the boundaries of Bacteria is strict: the evaluation rejects all relations where the Bacteria has incorrect boundaries. 7 Results  7.1 Participating systems Five teams submitted ten predictions to the three BB sub-tasks. LIMSI (CNRS, France), see (Grouin, 2013) is the only team that submitted to the three sub-tasks. LIPN (U. Paris-Nord, France), (Bannour et al, 2013) only submitted to 
sub-task 1. TEES (TUCS, Finland), (Bj?rne and Salakoski, 2013) only submitted to sub-task 2. Finally, IRISA (INRIA, France), (Claveau, 2013))) and Boun (U. Bo?azi?i, Turkey), (Karadeniz and ?zg?r), submitted to sub-tasks 1 and 2. The scores of the submissions according to the official metrics are shown in decreasing rank order in Tables 3 to 6.  Participant Rank  SER  F1  IRISA 1  0.46 0.57  Boun 2  0.48 0.59  LIPN 3  0.49 0.61  LIMSI 4  0.66 0.44 Table 3. Scores for Sub-task 1 of the BB Task.   Participant Entity  detection Category  assignment  SER F1 SER  F1  IRISA  0.43 0.60  0.35 0.67  Boun  0.42 0.65  0.36 0.71  LIPN  0.46 0.64  0.38 0.72  LIMSI  0.45 0.71  0.66 0.50 Table 4. Detailed scores for Sub-task 1 of the BB Task. Participant systems to sub-task 1 obtained high scores despite the novelty of the task (0.46 SER for the 1st, IRISA). The results of the first three systems are very close despite the diversity of the methods. The decomposition of the scores of the predictions of entities with correct boundaries and their assignment to the right category are shown in Table 4. They are quite balanced with a slightly better rate for category assignment, with the exception of the LIMSI system, which is notably better in entity detection. This table also shows the dependency of the two entity detection and categorization steps. Errors in the entity boundaries affect the quality of categorization. Table 5 details the scores for sub-task 2. The prediction of location relations remains a difficult problem even with the entities being given. There are two reasons for this. First, there is high diversity of bacteria and locations. The many mentions of different bacteria and locations in the same paragraph make it a challenge to select the right pairing among candidate arguments. This is particularly true for the PartOf relation compared to the Localization relation (columns 5 and 6). All systems obtained 
166
a recall much lower than the precision, which may be interpreted training data overfitting.  Participant Rec. Prec.  F1  F1 PartOf F1 Loc.  TEES 2.1  0.28 0.82  0.42  0.22 0.49  IRISA  0.36 0.46  0.40  0.2 0.45  Boun  0.21 0.38  0.27  0.2 0.29  LIMSI  0.4 0.19  0.6  0.0 0.7 Table 5. Scores of Sub-task 2 for the BB Task. The second challenge is the high frequency of anaphora, especially with a bacteria antecedent. For BioNLP-ST 2011, we already pointed out that coreference resolution is critical in order to capture all relations that are not expressed inside a sentence. Participant Rec.  Prec.  F1   TEES 2.1 0.12 (0.41) 0.18 (0.61) 0.14  (0.49)  LIMSI 0.4 (0.9) 0.12 (0.82)   0.6  (0.15) Table 6. Scores of Sub-task 3 for the BB Task. (the relaxed scores are given in parentheses.) The results of sub-task 3 (Table 6) may appear disappointing compared to the first two sub-tasks and BB?11. Further analysis shows that the system scores were affected by their poor entity boundary detection and the PartOf relation predictions. In order to demonstrate this we computed a relaxed score that differs from the primary score by: - removing PartOf relations from the reference and the prediction; - accepting Localization relations even if the Bacteria entity boundaries  do not match; - removing the penalty for the incorrect boundaries of Habitat entities. This relaxed score is equivalent to ignoring PartOf relations and considering the boundaries of predicted entities as perfect. The result is exhibited in Table 6 between parentheses. The most determinant factor is the relaxation of Bacteria entity boundaries because errors are severely penalized. An error analysis of the submitted predictions revealed that more than half of the rejected Localization predictions had a Bacteria argument with incorrect boundaries.  7.2 Systems description and result analysis The participants deployed various assortments of methods ranging from linguistics and machine learning to hand-coded pattern-matching. Sub-
task 1 was handled in two successive steps, candidate entity detection and category assignment. Entity detection. The approaches combine  (1) the use of lexicons (IRISA and LIMSI), (2) then text analysis by chunking (IRISA), noun phrase analysis (Boun), term analysis by BioYaTeA (LIPN) and Cocoa entity detection (LIMSI),  (3) with additional rules (TextMarker by LIPN) or machine learning (CRF by LIMSI) for the adaptation to the corpus.  The LIMSI system combining Cocoa entity detection (BioNLP supporting resource) with CRF obtained the best result, 11 points over the less linguistics-based approach of IRISA as shown in Table 4.  Assignment of categories to entities. It was mainly realized using hand-coded rules (LIMSI, Boun), machine learning with Whisk (LIPN) or a similarity between ontology labels and the text entities (IRISA). It is interesting to note that although the approaches are very different, the three types of methods obtained close results ranging from 0.35 to 0.38 SER, apart one outlier. Prediction of relations. Sub-task 2 was completed by applying hand-coded rules (LIMSI, Boun), that were much less successful than the two machine-learning-based approaches, i.e. kNN by IRISA and multi-step SVM by TEES-2.1. In the case of TEES-2.1 attributes were generated by McCCJ parses, which may explain its success in the prediction of PartOf relations that is 20 point over the second method that did not use any parsing. Prediction of entities and relations. Sub-task 3 was completed by LIMSI using the successive application of its methods from sub-tasks 1 and 2. TEES-2.1 applied its multi-step SVM classification of sub-task 2 for relation prediction completed by additional SVM steps for candidate entity detection. These experiments allow for the comparison of very different state-of-the-art methods, resources and integration strategies. However the tight gap between the scores of the different systems prevents us from drawing a definitive conclusion. Additional criteria other than scores may also be taken into account: the simplicity of deployment, the ease of adaptation to new 
167
domains, the availability of relevant resources and the potential for improvement. 8 Conclusion After BioNLP-ST?11, the second edition of the Bacteria Biotope Task provides a wealth of new information on the generalization of the entity categorization methods to a large set of categories. The final submissions of the 5 teams show very promising results with a broad variety of methods. The introduction of new metrics appeared appropriate to reveal the quality of the results and to highlight relevant contrasts. The prediction of events still remains challenging in documents where the candidate arguments are very dense, and where most relations involve several sentences. A thorough analysis of the results indicates clear directions for improvement.  Acknowledgments This work has been partially supported by the Quaero program, funded by OSEO, the French state agency for innovation and the INRA OntoBiotope Network. References Sondes Bannour, Laurent Audibert, Henry Soldano. 2013. Ontology-based semantic annotation: an automatic hybrid rule-based method. Present volume. Jari Bj?rne, Tapio Salakoski. 2013. TEES 2.1: Automated Annotation Scheme Learning in the BioNLP 2013 Shared Task. Present volume. Robert Bossy, Julien Jourde, Alain-Pierre Manine A., Philippe Veber, Erick Alphonse, Maarten van de Guchte, Philippe Bessi?res, Claire N?dellec. 2012. BioNLP Shared Task - The Bacteria Track. BMC Bioinformatics 13(Suppl 11):S3, June .  Vincent Claveau. 2013. IRISA participation to BioNLP-ST 2013: lazy-learning and information retrieval for information extraction tasks. Present volume. Liolios K., Chen I.M., Mavromatis K., Tavernarakis N., Hugenholtz P., Markowitz V.M., Kyrpides N.C. (2010). The Genomes On Line Database (GOLD) in 2009: status of genomic and metagenomic projects and their associated metadata. Nucleic Acids Res., 38(Database issue):D346-54. EnvDB database. http://metagenomics.uv.es/envDB/ EnvO Project. http://environmentontology.org 
Dawn Field et al 2008. Towards a richer description of our complete collection of genomes and metagenomes: the ?Minimum Information about a Genome Sequence? (MIGS) specification. Nature Biotechnology. 26: 541-547. Cyril Grouin. 2013. Building A Contrasting Taxa Extractor for Relation Identification from Assertions: BIOlogical Taxonomy & Ontology Phrase Extraction System. Present volume. ?lknur Karadeniz, Arzucan ?zg?r. 2013. Bacteria Biotope Detection, Ontology-based Normalization, and Relation Extraction using Syntactic Rules. Present volume. Korbel J.O., Doerks T., Jensen L.J., Perez-Iratxeta C., Kaczanowski S., Hooper S.D., Andrade M.A., Bork P. (2005). Systematic association of genes to phenotypes by genome and literature mining. PLoS Biol., 3(5):e134. Melissa M. Floyd, Jane Tang, Matthew Kane and David Emerson. 2005. Captured Diversity in a Culture Collection: Case Study of the Geographic and Habitat Distributions of Environmental Isolates Held at the American Type Culture Collection. Applied and Environmental Microbiology. 71(6):2813-23. GenBank. http://www.ncbi.nlm.nih.gov/  GOLD. http://www.genomesonline.org/cgi-bin/GOLD/bin/gold.cgi Ivanova N., Tringe S.G., Liolios K., Liu W.T., Morrison N., Hugenholtz P., Kyrpides N.C. (2010). A call for standardized classification of metagenome projects. Environ. Microbiol., 12(7):1803-5. John Makhoul, Francis Kubala, Richard Schwartz, and Ralph Weischedel. 1999. Performance measures for information extraction, in Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February.  von Mering C., Hugenholtz P., Raes J., Tringe S.G., Doerks T., Jensen L.J., Ward N., Bork P. (2007). Quantitative phylogenetic assessment of microbial communities in diverse environments. Science, 315(5815):1126-30. Metagenome Classification. /metagenomic_classification_tree.cgi MicrobeWiki. http://microbewiki.kenyon.edu/index.php/MicrobeWiki  Microbial Genomics Program at JGI. http://genome.jgi-psf.org/programs/bacteria-archaea/index.jsf Microorganisms sequenced at Genoscope. http://www.genoscope.cns.fr/spip/Microorganisms-sequenced-at.html 
168
Miguel Pignatelli, Andr?s Moya, Javier Tamames.  (2009). EnvDB, a database for describing the environmental distribution of prokaryotic taxa. Environmental Microbiology Reports. 1:198-207. Fr?d?ric Papazian, Robert Bossy and Claire N?dellec. 2012. AlvisAE: a collaborative Web text annotation editor for knowledge acquisition. The 6th Linguistic Annotation Workshop (The LAW VI), Jeju, Korea. Prokaryote Genome Projects at NCBI. http://www.ncbi.nlm.nih.gov/genomes/lproks.cgi  Zorana Ratkovic, Wiktoria Golik, Pierre Warnier. 2012. Event extraction of bacteria biotopes: a knowledge-intensive NLP-based approach. BMC Bioinformatics 2012, 13(Suppl 11):S8, 26June. .  Javier Tamames and Victor de Lorenzo. 2010. EnvMine: A text-mining system for the automatic extraction of contextual information. BMC Bioinformatics. 11:294. James Z. Wang, Zhidian Du, Rapeeporn Payattakool, Philip S. Yu, and Chin-Fu Chen. 2007. A New Method to Measure the Semantic Similarity of GO Terms. Bioinformatics. 23: 1274-1281. 
169
