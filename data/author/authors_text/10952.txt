Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1423?1433, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Grounded Models of Semantic Representation
Carina Silberer and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
c.silberer@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
A popular tradition of studying semantic rep-
resentation has been driven by the assump-
tion that word meaning can be learned from
the linguistic environment, despite ample ev-
idence suggesting that language is grounded
in perception and action. In this paper we
present a comparative study of models that
represent word meaning based on linguistic
and perceptual data. Linguistic information is
approximated by naturally occurring corpora
and sensorimotor experience by feature norms
(i.e., attributes native speakers consider impor-
tant in describing the meaning of a word). The
models differ in terms of the mechanisms by
which they integrate the two modalities. Ex-
perimental results show that a closer corre-
spondence to human data can be obtained by
uncovering latent information shared among
the textual and perceptual modalities rather
than arriving at semantic knowledge by con-
catenating the two.
1 Introduction
Distributional models of lexical semantics have seen
considerable success at accounting for a wide range
of behavioral data in tasks involving semantic cog-
nition (Landauer and Dumais, 1997; Griffiths et
al., 2007). These models have also enjoyed last-
ing popularity in natural language processing. Ex-
amples involve information retrieval (Salton et al
1975), word sense discrimination (Schu?tze, 1998),
text segmentation (Choi et al 2001), and numerous
studies of lexicon acquisition (Grefenstette, 1994;
Lin, 1998). Despite their widespread use, distribu-
tional models have been criticized as ?disembodied?
in that they learn exclusively from linguistic infor-
mation but are not grounded in perception and ac-
tion (Perfetti, 1998; Barsalou, 1999; Glenberg and
Kaschak, 2002).
This lack of grounding contrasts with many ex-
perimental studies suggesting that word meaning is
acquired not only from exposure to the linguistic
environment but also from our interaction with the
physical world (Landau et al 1998; Bornstein et al
2004). Beyond language acquisition, there is consid-
erable evidence across both behavioral experiments
and neuroimaging studies that the perceptual asso-
ciates of words play an important role in language
processing (for a review see Barsalou (2008)).
It is thus no surprise that recent years have wit-
nessed the emergence of perceptually grounded dis-
tributional models. An important question in the for-
mulation of such models concerns the provenance
of perceptual information. A few models use fea-
ture norms as a proxy for sensorimotor experience
(Howell et al 2005; Andrews et al 2009; Steyvers,
2010; Johns and Jones, 2012). These are obtained
by asking native speakers to write down attributes
they consider important in describing the meaning
of a word. The attributes represent perceived phys-
ical and functional properties associated with the
referents of words. For example, apples are typi-
cally green or red, round, shiny, smooth, crunchy,
tasty, and so on; dogs have four legs and bark,
whereas chairs are used for sitting. Other models fo-
cus solely on the visual modality under the assump-
tion that it represents a major source of data from
1423
which humans can learn semantic representations
of both linguistic and non-linguistic communicative
actions (Regier, 1996). For example, Feng and Lap-
ata (2010) learn semantic representations from cor-
pora of texts paired with naturally co-occurring im-
ages (e.g., news articles and their associated pic-
tures), whereas Bruni et al(2011) learn textual and
visual representations independently from distinct
data sources.
Aside from the type of data used to capture per-
ceptual information, another important issue con-
cerns how the two modalities (perceptual and tex-
tual) are integrated. A simple solution would be
to learn both modalities independently (Bruni et al
2011) or to infer one modality by means of the other
(Johns and Jones, 2012) and to arrive at a grounded
representation simply by concatenating the two. An
alternative is to learn from both modalities jointly
(Andrews et al 2009; Feng and Lapata, 2010;
Steyvers, 2010). According to this view, seman-
tic knowledge is gained by simultaneously learning
from the statistical structure within each modality
assuming both data sources have been generated by
a shared set of meanings or topics.
In this paper we undertake the first comparative
study of perceptually grounded distributional mod-
els. We examine three models with different as-
sumptions regarding the integration of perceptual
and linguistic data. The first model, originally pro-
posed by Andrews et al(2009), is an extension of
latent Dirichlet alcation (LDA, Blei et al(2003)).
It simultaneously considers the distribution of words
across contexts in a text corpus and the distribu-
tion of words across perceptual features and extracts
joint information from both data sources. Our sec-
ond model is based on Johns and Jones (2012) who
represent the meaning of a word as the concatena-
tion of its textual and its perceptual vector. Interest-
ingly, their model allows to infer a perceptual vector
for words without feature norms, simply by taking
into account similar words for which perceptual in-
formation is available.
Finally, we propose Canonical Correlation Anal-
ysis (Hotelling, 1936; Hardoon et al 2004) as our
third model. CCA is a data analysis and dimen-
sionality reduction method similar to PCA. While
PCA deals with only one data space, CCA is a tech-
nique for joint dimensionality reduction across two
Features table dog apple
has 4 legs .28 .60 0
used for eating .50 0 0
a pet 0 .40 0
is brown 0 0 0
is crunchy 0 0 .58
is round .22 0 .42
has fangs 0 0 0
Table 1: Feature norms for the nouns table, dog, and
apple shown as a distribution.
(or more) spaces that provide heterogeneous repre-
sentations of the same objects. The assumption is
that the representations in these two spaces contain
some joint information that is reflected in correla-
tions between them.
In all three models we use feature norms as a
proxy for perceptual information. Despite their
shortcomings (e.g., they often cover a small frac-
tion of the vocabulary of an adult speaker due to
the effort involved in eliciting them), feature norms
provide detailed knowledge about meaning repre-
sentations and are a useful starting point for study-
ing the integration of perceptual and textual infor-
mation without being susceptible to the effects of
noise, e.g., coming from image processing. In other
words, feature norms can serve as an upper bound
of what can be achieved when integrating detailed
perceptual information with vanilla text-based dis-
tributional models.
Our experimental results demonstrate that joint
models give a better fit to human word similarity and
association data than a model that considers only
one data source, or the simple concatenation of the
two sources.
2 Perceptually Grounded Models
In this study we examine semantic representation
models that rely on linguistic and perceptual data.
The linguistic environment is approximated by cor-
pora such as the British National Corpus (BNC).
As mentioned earlier, we resort to feature norms
as proxy for perceptual information. In our exper-
iments, we relied on the norming study of McRae et
al. (2005), in which a large number of human par-
ticipants were presented with a series of words and
1424
asked to list relevant features of the words? refer-
ents. Table 1 presents examples of features partici-
pants listed for the nouns apple, dog, and table. The
number of participants listing a certain feature for a
word can be used to compute a probability distribu-
tion over features given the word:
P( fk|w) =
f ( fk,w)
F
?
m=1
f ( fm,w)
(1)
where f ( fk,w) is the number of participants who
listed feature fk for word w and F is the total number
of features.
In the remainder of this section we will describe
our models and how they arrive at an integrated per-
ceptual and linguistic representation.
2.1 Feature-topic Model
Andrews et al(2009) present an extension of LDA
(Blei et al 2003) where words in documents as well
as their associated features are treated as observed
variables that are explained by a generative process.
The underlying training data consists of a corpus D
where each document is represented by words and
their frequency of occurrence within the document.
In addition, those words of a document that are also
included in the feature norms are paired with one of
their features, where a feature is sampled according
to the feature distribution given that word. For ex-
ample, suppose a document d j consists of the sen-
tence Mix in the apple, celery, raisins, and apple
juice. Suppose further that all content words ex-
cept of mix and juice are included in the feature
norms. Then, a representation for d j is mix:1, ap-
ple;is red:2, celery;has leaves:1, raisin;is edible:1,
juice:1.
The plate diagram in Figure 1 illustrates the
graphical model in detail. Each document d j
in D is generated by a mixture of components
{x1, ...,xc, ...,xC} ? C ; a component xc comprises a
latent discourse topic coupled with a feature clus-
ter originating from the feature norms. A dis-
course topic belonging to xc, in turn, is a distribu-
tion ?c ? ?= {?1, ...,?C} over words, and a feature
cluster is a distribution ?c ? ?= {?1, ...,?C} over
features.
In order to create document d j, a distribution pi j
over components is sampled from a Dirichlet distri-
pi
x
? w, f ?
?
? ?
?xc ? C ?i ? {1, ...,n j}
? j ? {1, ...,D}
?xc ? C
Figure 1: Feature-topic model. The components x ji of a
document d j are sampled from pi j. For each xc = x ji, a
word w ji is drawn from distribution ?c and a feature f ji is
drawn from distribution ?c.
bution parametrized by ?. To generate each word
w ji ? {w j1, ...,w jn j}, a component xc = x ji is drawn
from pi j; w ji is then drawn from the corresponding
distribution ?c. If w ji is in the feature norms, it is
coupled with a feature f ji which is correspondingly
drawn from ?c. A symmetric Dirichlet prior with
hyperparameters ? and ? is placed on ? and ?, re-
spectively. The probability of the corpus D is de-
fined as:
P((w? f )1:D|?,?,?) =
D
?
j=1
?
dpi j
n j
?
i=1
P(pi j|?)
C
?
c=1
P(w ji|x ji = xc,?)P( f ji|x ji = xc,?)P(x ji = xc|pi j)
(2)
where D is the number of documents and C the
predefined number of components. Computing the
posterior distribution P(?,?,?,?,?|(w ? f )1:D) of
the hidden variables given the data is generally in-
tractable:
P(?,?,?,?,?|(w? f )1:D) ? P((w? f )1:D|?,?,?)
P(?|?)P(?|?)P(?)P(?)P(?)
(3)
Equation (3) may be approximated using the Gibbs
1425
[ x1 x2 x12 ... x28 x75 x107 x119 x125 x148 x182 ... x266 x326 x349 x350
apple 3e-5 3e-5 0 . . . 5e-4 9e-4 .09 .002 7.6e-5 2e-4 .003 . . . 0 0 3e-6 0
]
Figure 2: Example of the representation of the meaning of apple with the model of (Andrews et al 2009) .
[ ... d16 ... d322 ... d2469 d2470 ... dD
apple . . . 1 . . . 1 . . . 0 1 . . . 0
][ a f ruit has f angs is crunchy ... is yellow is red is green is round
0 0 0 . . . 0 0 0 0
]
[ ... d16 ... d322 ... d2469 d2470 ... dD
apple . . . 1 . . . 1 . . . 0 1 . . . 0
][ a f ruit has f angs is crunchy ... is yellow is red is green is round
.006 1.8e-5 8e-4 . . . .004 .004 .006 .02
]
Figure 3: Example representation for apple before (first row) and after (second row) applying the perceptual inference
method of Johns and Jones (2012).
sampling procedure described in Andrews et al
(2009).
Inducing feature-topic components from a docu-
ment collection D with the extended LDA model
just described gives two sets of parameters: word
probabilities given components PW (wi|X = xc) for
wi, i = 1, ...,N, and feature probabilities given com-
ponents PF( fk|X = xc) for fk, k= 1, ...,F . For exam-
ple, most of the probability mass of component x107
would be reserved for the words apple, fruit, lemon,
orange, tree and the features is red, tastes sweet,
is round and so on.
Word meaning in this model is represented by the
distribution PX |W over the learned components (see
Figure 2 for an example). Assuming a uniform dis-
tribution over components xc in D , PX |W can be ap-
proximated as:
PX=xc|W=wi =
P(wi|xc)P(xc)
P(wi)
?
P(wi|xc)
C
?
l=1
P(wi|xl)
(4)
where C is the total number of components. The
model can be also used to infer features for words
that were not originally included in the feature
norms. The probability distribution PF |W over fea-
tures given a word wi is simply inferred by summing
over all components xc for each feature fk:
PF( fk|W = wi) =
C
?
c=1
P( fk|xc)P(xc|wi) (5)
2.2 Global Similarity Model
Johns and Jones (2012) propose an approach for
generating perceptual representations for words by
means of global lexical similarity. Their model does
not place so much emphasis on the integration of
perceptual and linguistic information, rather its main
focus is on inducing perceptual representations for
words with no perceptual correlates. Their idea is to
assume that lexically similar words also share per-
ceptual features and hence it should be possible to
transfer perceptual information onto words that have
none from their linguistically similar neighbors.
Let T ? {1,0}N?D denote a binary term-
document matrix, where each cell records the pres-
ence or absence of a term in a document. Let
P ? [0,1]N?F denote a perceptual matrix, represent-
ing a probability distribution over features for each
word (see Table 1). A word?s meaning is repre-
sented by the concatenation of its textual and per-
ceptual vectors (see Figure 3). If a word has not
been normed, its perceptual vector will be all zeros.
Johns and Jones (2012) propose a two-step estima-
tion process for words without perceptual vectors.
Initially, a perceptual vector is constructed based on
the word?s weighted similarity to other words that
have non-zero perceptual vectors:
pin f =
N
?
i=1
ti ? sim(ti,p)? (6)
where p is the representation of a word with a tex-
tual vector but an empty perceptual vector, ts are
composite representations consisting of textual and
perceptual vectors, sim is a measure of distributional
similarity such as cosine, ? a weighting parameter,
and pin f the resulting inferred representation of the
word. The process is repeated a second time, so
as to incorporate the inferred perceptual vector in
the computation of the inferred vectors of all other
words. An example of this inference procedure is
illustrated in Figure 3.
1426
[ ... d16 ... d322 ... d2470 ... dD
apple . . . .006 . . . .003 . . . .1e-6 . . . 0
][ a f ruit has f angs is crunchy ... is yellow is red is green is round
.13 0 .06 . . . .04 .14 .09 .04
]
[ k1 k2 k3 ... k409 k410
apple ?.003 ?.01 .002 . . . ?.002 ?.01
][ k1 k2 k3 ... k409 k410
.008 ?.03 ?.008 . . . ?.02 ?.07
]
Figure 4: Example representation for apple before (first row) and after (second row) applying CCA.
2.3 Canonical Correlation Analysis
Our third model uses Canonical Correlation Analy-
sis (CCA, Hardoon et al(2004)) to learn a joint se-
mantic representation from the textual and percep-
tual views. Given two random variables x and y
(or two sets of vectors), CCA can be seen as de-
termining two sets of basis vectors in such a way,
that the correlation between the projections of the
variables onto these bases is mutually maximized
(Borga, 2001). In effect, the representation-specific
details pertaining to the two views of the same phe-
nomenon are discarded and the underlying hidden
factors responsible for the correlation are revealed.
In our case the linguistic view is represented by a
term-document matrix, T ? RN?D, containing infor-
mation about the occurrence of each word in each
document. The perceptual view is captured by a
perceptual matrix, P ? [0,1]N?F , representing words
as a probability distribution over normed features.
CCA is concerned with describing linear dependen-
cies between two sets of variables of relatively low
dimensionality. Since the correlation between the
linguistic and perceptual views may exist in some
nonlinear relationship, we used a kernelized version
of CCA (Hardoon et al 2004) which first projects
the data into a higher-dimensional feature space and
then performs CCA in this new feature space. The
two kernel matrices are KT = TT ? and KP = PP?.
After applying CCA we obtain two matrices pro-
jected onto L basis vectors, Ct ? RN?L, resulting
from the projection of the textual matrix T onto the
new basis and Cp ?RN?L, resulting from the projec-
tion of the corresponding perceptual feature matrix.
The meaning of a word can thus be represented by
its projected textual vector in CT , its projected per-
ceptual vector in CP or their concatenation. Figure 4
shows an example of the textual and perceptual vec-
tors for the word apple which were used as input for
CCA (first row) and their new representation after
the projection onto new basis vectors (second row).
The CCA model as sketched above will only ob-
tain full representations for words with perceptual
features available. One solution would be to apply
the method from Johns and Jones (2012) to infer the
perceptual vectors and then perform CCA on the in-
ferred vectors. Another approach which we assess
experimentally (see Section 4) is to create a percep-
tual vector for a word that has none from its k-most
(textually) similar neighbors, simply by taking the
average of their perceptual vectors. This inference
procedure can be applied to the original vectors or
the projected vectors in CT and CP, respectively,
once CCA has taken place.
2.4 Discussion
Johns and Jones (2012) primarily present a model of
perceptual inference, where textual data is used to
infer perceptual information for words not included
in feature norms. There is no means in this model to
obtain a joint representation resulting from the mu-
tual influence of the perceptual and textual views.
As shown in the example in Figure 3 the textual
vector on the left-hand side does not undergo any
transformation whatsoever. The generative model
put forward by Andrews et al(2009) learns meaning
representations by simultaneously considering doc-
uments and features. Rather than simply adding per-
ceptual information to textual data it integrates both
modalities jointly in a single representation which
is desirable, at least from a cognitive perspective.
It is unlikely that we have separate representations
for different aspects of word meaning (Rogers et al
2004). Similarly to Johns and Jones (2012), An-
drews et al (2009) feature-topic model can also
infer perceptual representations for words that have
none. The inference is performed automatically in
an implicit manner during component induction.
In CCA, textual and perceptual data represent two
different views of the same objects and the model
operates on these views directly without combining
or manipulating any of them a priori. Instead, the
combination of the two modalities is realized via
1427
correlating the linear relationships between them. A
drawback of the model lies in the need of additional
methods for inferring perceptual representations for
words not available in feature norms.
3 Experimental Setup
Data All our experiments used a lemmatized ver-
sion of the British National Corpus (BNC) as a
source of textual information. The feature norms of
McRae et al(2005) were used as a proxy for percep-
tual information. The BNC comprises 4,049 texts
totalling approximately 100 million words. McRae
et als feature norms consist of 541 words and 2,526
features; 824 of these features occur with at least two
different words.
Evaluation Tasks Our evaluation experiments
compared the models discussed above on three
tasks. Two of them have been previously used
to evaluate semantic representation models, namely
word association and word similarity. In order
to simulate word association, we used the human
norms collected by (Nelson et al 1998).1 These
were established by presenting a large number of
participants with a cue word (e.g., rice) and ask-
ing them to name an associate word in response
(e.g., Chinese, wedding, food, white). For each cue
word, the norms provide a set of associates and the
frequencies with which they were named. We can
thus compute the probability distribution over asso-
ciates for each cue. Analogously, we can estimate
the degree of similarity between a cue and its as-
sociates using our models (see the following sec-
tion for details on the similarity measures we em-
ployed). The norms contain 63,619 unique normed
cue-associate pairs in total. Of these, 25,968 pairs
were covered by all models and 520 appeared in
McRae et als (2005) norms. Using correlation anal-
ysis, we examined the degree of linear relationship
between the human cue-associate probabilities and
the automatically derived similarity values.
Our word similarity experiments used the
WordSimilarity-353 test collection (Finkelstein et
al., 2002)2 which consists of relatedness judgments
1Available at http://www.usf.edu/Freeassociation.
2Available at http://www.cs.technion.ac.il/?gabr/
resources/data/wordsim353/.
for word pairs. For each pair, a similarity judg-
ment (on a scale of 0 to 10) was elicited from 13 or
16 human subjects (e.g., tiger-cat are very similar,
whereas delay?racism are not). The average rating
for each pair represents an estimate of the perceived
similarity of the two words. The task varies slightly
from word association. Here, participants are asked
to rate perceived similarity rather than to generate
the first word that came to mind in response to a cue
word. The collection contains similarity ratings for
353 word pairs. Of these, 76 pairs appeared in our
corpus and 3 in McRae et als (2005) norms. Again,
we evaluated how well model produced similarities
correlate with human ratings. Throughout this paper
we report correlation coefficients using Pearson?s r.
Our third task assessed the models? ability to in-
fer perceptual vectors for words that have none. To
do this, we conducted 10-fold cross-validation on
McRae et als (2005) norms. We treated the per-
ceptual vectors in each test fold as unseen, and used
the data in the corresponding training fold together
with the models presented in Section 2 to infer them.
Then, for each word, we examined how close the in-
ferred vector was to the actual one, via correlation
analysis.
Model Parameters The feature-topic model has a
few parameters that must be instantiated. These in-
clude, C, the number of predefined components and
the priors ?, ?, and ?. Following Andrews et al
(2009), the components C were set to 350.3 A vague
inverse gamma prior was placed on ?, ?, and ?.4 To
measure word similarity within this model, we adopt
Griffiths et als (2007) definition. The underlying
idea is that word association can be expressed as a
conditional distribution. If we have seen word w1,
then we can determine the probability that w2 will
be also generated by computing P(w2|w1). Assum-
ing that both w1 and w2 came from a single compo-
nent, P(w2|w1) can be estimated as:
P(w2|w1) =
C
?
c=1
P(w2|xc)P(xc|w1)
P(xc|w1) ? P(w1|xc)P(xc)
(7)
3As we explain in Section 4 the feature-topic model was
compared to a vanilla LDA model trained on the BNC only.
For that model, C was set to 250.
4That is P(?) = exp(? 1? )?
?2.
1428
where P(xc) is uniform, a single component xc is
sampled from the distribution P(xc|w1), and an over-
all estimate is obtained by averaging over all C com-
ponents.
Johns and Jones? (2012) model uses binary tex-
tual vectors to represent word meaning. If the word
is present in a given document, that vector element
is coded as one; if it is absent, it is coded as zero.
We built a binary term-document matrix from the
BNC over 14,000 lemmas. The value of the similar-
ity weighting parameter ? was set to the same values
reported by Johns and Jones (?1=3 for Step 1 and
?2 = 13 for Step 2).
For the CCA model, we represented the textual
view with a term-document co-occurrence matrix.
Matrix cells were set to their tf-idf values.5 The tex-
tual and perceptual matrices were projected onto 410
vectors. As mentioned in Section 2.3, CCA does not
naturally lend itself to inferring perceptual vectors,
yet a perceptual vector for a word can be created
from its k-nearest neighbors. We inferred a percep-
tual vector by averaging over the perceptual vectors
of the word?s k most similar words; textual similarity
between two words was measured using the cosine
of the angle of the two vectors representing them.
To find the optimal value for k, we used one third of
Nelson?s (1998) cues as development set. The high-
est correlation was achieved with k = 2 when the
perceptual vectors were created prior to CCA and
k = 8 when they were inferred on the projected tex-
tual and perceptual matrices.
4 Results
Our experiments were designed to answer three
questions: (1) Does the integration of perceptual and
textual information yield a better fit with behavioral
data compared to a model that considers only one
data source? (2) What is the best way to integrate
the two modalities, e.g., via simple concatenation or
jointly? (3) How accurately can we approximate the
perceptual information when the latter is absent?
To answer the first question, we assessed the mod-
els? performance when textual and perceptual infor-
mation are both available. The results in Table 2
are thus computed on the subset of Nelson?s (1998)
5Experiments with a binarized version of the term-document
matrix consistently performed worse.
Models Modality Pearson?s r
Feature-topic +t +p .35
Feature-topic +t ?p .12
Feature-topic ?t +p .22
Global similarity +t +p .23
Global similarity +t ?p .11
Global similarity ?t +p .22
CCA +t +p .32
CCA +t ?p .14
CCA ?t +p .29
Upper Bound ? .91
Table 2: Performance of feature-topic, global similarity,
and CCA models on a subset of the Nelson et al(1998)
norms when taking into account the textual and percep-
tual modalities on their own (+t?p and ?t+p) and in
combination (+t+p). All correlation coefficients are sta-
tistically significant (p < 0.01).
norms (520 cue-associate pairs) that also appeared in
McRae et al(2005) and for which a perceptual vec-
tor was present. The table shows different instanti-
ations of the three models depending on the type of
modality taken into account: textual, perceptual or
both.
As can be seen, Andrews et als (2009) feature-
topic model provides a better fit with the association
data when both modalities are taken into account
(+t+p). A vanilla LDA model constructed solely
on the BNC (+t?p) or McRae et als (2005) fea-
ture norms (?t+p) yields substantially lower corre-
lations. We observe a similar pattern with Johns and
Jones? (2012) global similarity model. Concatena-
tion of perceptual and textual vectors yields the best
fit with the norming data, relying on perceptual in-
formation alone (?t+p) comes close, whereas tex-
tual information on its own seems to have a weaker
effect (+t?p).6 The CCA model takes perceptual
and textual information as input in order to find a
projection onto basis vectors that are maximally cor-
related. Although by definition the CCA model must
operate on the two views, we can nevertheless iso-
late the contribution of each modality by considering
the vectors resulting from the projection of the tex-
6In this evaluation setting, the model does not infer any per-
ceptual representations; perceptual vectors are taken directly
from McRae et al(2005).
1429
tual matrix (+t?p), the perceptual matrix (?t+p) or
their concatenation (+t+p). We obtain best results
with the latter representation; again we observe that
the perceptual information is more dominant.
Overall we find that the feature-topic model and
CCA perform best. In fact the correlations achieved
by the two models do not differ significantly, us-
ing a t-test (Cohen and Cohen, 1983). The per-
formance of the global similarity model is signifi-
cantly worse than the feature-topic model and CCA
(p < 0.01). Recall that the feature-topic model
(+t+p) represents words as distributions over com-
ponents, whereas the global similarity model sim-
ply concatenates the textual and perceptual vectors.
The same input is also given to CCA which in turn
attempts to interpret the data by inferring common
relationships between the two views. In sum, we
can conclude that the higher correlation with human
judgments indicates that integrating textual and per-
ceptual modalities jointly is preferable to concatena-
tion.
However, note that all models in Table 2 fall
short of the human upper bound which we mea-
sured by calculating the reliability of Nelson et als
(1998) norms. Reliability estimates the likelihood
of a similarly-composed group of participants pre-
sented with the same task under the same circum-
stances producing identical results. We split the col-
lected cue-associate pairs randomly into two halves
and computed the correlation between them; this
correlation was averaged across 200 random splits.
These correlations were adjusted by applying the
Spearman-Brown prediction formula (Voorspoels et
al., 2008).
The results in Table 2 are computed on a small
fraction of Nelson et als (1998) norms. One might
even argue that the comparison is slightly unfair as
the global similarity model is more geared towards
inferring perceptual vectors rather than integrating
the two modalities in the best possible way. To gain
a better understanding of the models? behavior and
to allow comparisons on a larger dataset and more
equal footing, we also report results on the entire
dataset (20,556 cue-associate pairs).7 This entails
that the models will infer perceptual vectors for the
7This excludes the data used as development set for tuning
the k-nearest neighbors for CCA.
Models Pearson?s r
Feature-topic .15
Global similarity .03
Global similarity CCA .12
k-NN CCA .11
CCA k-NN .12
Upper Bound .96
Table 3: Performance of the feature-topic, global simi-
larityand CCA models on the Nelson et al(1998) norms
(entire dataset). All correlation coefficients are statisti-
cally significant (p < 0.01).
words that are not attested in McRae et als norms.
Recall from Section 2.3 that CCA does not have
a dedicated inference mechanism. We thus experi-
mented with three options (a) interfacing the infer-
ence method of Johns and Jones (2012) with CCA
(global similarity  CCA) (b) creating a percep-
tual vector from the words? k-nearest neighbors be-
fore (k-NN  CCA) or (c) after CCA takes place
(CCA k-NN).
Our results are summarized in Table 3. The up-
per bound was estimated in the same fashion as for
the smaller dataset. Despite being statistically sig-
nificant (p < 0.01), the correlation coefficients are
lower. This is hardly surprising as perceptual infor-
mation is approximate and in several cases likely to
be wrong. Interestingly, we observe similar mod-
eling trends, irrespective of whether the models are
performing perceptual inference or not. The feature-
topic model achieves the best fit with the data, fol-
lowed by CCA. The inference method here does not
seem to have much of an impact: CCA  k-NN
does as well as global similarity  CCA. This is
perhaps expected as the inference procedure adopted
by Johns and Jones (2012) is a generalization of our
k-nearest neighbor approach. The global similarity
model performs worst; we conjecture that this is due
to the way semantic information is integrated rather
than the inference method itself. CCA works with
similar input, yet achieves better correlations with
the human data, due to its ability to represent the
commonalities shared by the two modalities. Taken
together the results in Tables 2 and 3 provide an an-
swer to our second question. Models that capture la-
tent information shared between the two modalities
1430
Models Pearson?s r
Feature-topic .17
Global similarity .25
Global similarity CCA .21
k-NN CCA .19
CCA k-NN .13
Table 4: Mean correlation coefficients between origi-
nal and inferred feature vectors in McRae et als (2005)
norms.
create more accurate semantic representations com-
pared to simply treating the two as independent data
sources.
In order to isolate the influence of the inference
method from the resulting semantic representation
we evaluated the inferred perceptual vectors on their
own by computing their correlation with the original
feature distributions in McRae et als (2005) norms.
The correlation coefficients are reported in Table 4
and were computed by averaging the coefficients ob-
tained for individual words. Here, the global simi-
larity model achieves the highest correlation, and for
a good reason. It is the only model with an empha-
sis on inference, the other two models do not have
such a dedicated mechanism. CCA has in fact none,
whereas in the feature-topic model the inference of
missing perceptual information is a by-product of
the generative process. The results in Table 4 indi-
cate that the perceptual vectors are not reconstructed
very accurately (the highest correlation coefficient
is .25) and that better inference mechanisms are re-
quired for perceptual information to have a positive
impact on semantic representation.
In Table 5 we examine the models? performance
on semantic similarity rather than association using
the WordSimilarity-353 dataset (Finkelstein et al
2002). The models were evaluated on 76 word pairs
that appeared in the BNC. We inferred the percep-
tual vectors for 51 words. We computed the upper
bound using the reliability method described ear-
lier. Again, the joint models achieve better results
than the simple concatenation model. The feature-
topic and CCA models perform comparably, with
the global similarity model lagging substantially be-
hind. In sum, our results indicate that the issue
of how to best integrate the two modalities has a
Models Pearson?s r
Feature-topic .35
Global similarity .08
Global similarity CCA .38
k-NN CCA .39
CCA k-NN .28
Upper Bound .98
Table 5: Model performance on predicting word similar-
ity. All correlation coefficients are statistically significant
(p < 0.01), except for the global similarity model.
greater impact on the resulting semantic representa-
tions compared to the mechanism by which missing
perceptual information is inferred.
5 Conclusions
In this paper, we have presented a comparative study
of semantic representation models which compute
word meaning on the basis of linguistic and per-
ceptual information. The models differ in terms
of the mechanisms by which they integrate the two
modalities. In the feature-topic model (Andrews et
al., 2009), the textual and perceptual views are in-
tegrated via a set of latent components that are in-
ferred from the joint distribution of textual words
and perceptual features. The model based on Canon-
ical Correlation Analysis (Hardoon et al 2004) in-
tegrates the two views by deriving a consensus rep-
resentation based on the correlation between the lin-
guistic and perceptual modalities. Johns and Jones?
(2012) similarity-based model simply concatentates
the two representations. In addition, it uses the lin-
guistic representations of words to infer perceptual
information when the latter is absent.
Experiments on word association and similarity
show that all models benefit from the integration of
perceptual data. We also find that joint models are
superior as they obtain a closer fit with human judg-
ments compared with an approach that simply con-
catenates the two views. We have also examined
how these models perform on the perceptual infer-
ence task which has implications for the wider appli-
cability of grounded semantic representation mod-
els. Johns and Jones? (2012) inference mechanism
goes some way towards reconstructing the informa-
tion contained in the feature norms, however, further
1431
work is needed to achieve representations accurate
enough to be useful in semantic tasks.
In this paper we have used McRae et als (2005)
norms without any extensive feature engineering
other than applying a frequency cut-off. In the fu-
ture we plan to experiment with feature selection
methods in an attempt to represent perceptual in-
formation more succinctly. For example, it may
be that different features are appropriate for differ-
ent word classes (e.g., color versus event denoting
nouns). Although feature norms are a useful first ap-
proximation of perceptual data, the effort involved in
eliciting them limits the scope of any computational
model based on normed data. A natural avenue for
future work would be to develop semantic represen-
tation models that exploit perceptual data that is both
naturally occurring and easily accessible (e.g., im-
ages, physical simulations).
Acknowledgments We are grateful to Brendan
Johns for his help with the re-implementation of his
model. Thanks to Frank Keller and Michael Roth for
their input on earlier versions of this work, Ioannis
Konstas for his help with the final version, and mem-
bers of the ILCC at the School of Informatics for
valuable discussions and comments. We acknowl-
edge the support of EPSRC through project grant
EP/I032916/1.
References
M. Andrews, G. Vigliocco, and D. Vinson. 2009. Inte-
grating Experiential and Distributional Data to Learn
Semantic Representations. Psychological Review,
116(3):463?498.
Lawrence Barsalou. 1999. Perceptual Symbol Systems.
Behavioral and Brain Sciences, 22:577?609.
Lawrence W. Barsalou. 2008. Grounded Cognition. An-
nual Review of Psychology, 59:617?845.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022, March.
Magnus Borga. 2001. Canonical Correlation - a Tutorial,
January.
M. H. Bornstein, L. R. Cote, S. Maital, K. Painter, S.-Y.
Park, and L. Pascual. 2004. Cross-linguistic Analy-
sis of Vocabulary in Young Children: Spanish, Dutch,
French, Hebrew, Italian, Korean, and American En-
glish. Child Development, 75(4):1115?1139.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional Semantics from Text and Images. In
Proceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics, pages
22?32, Edinburgh, UK, July. Association for Compu-
tational Linguistics.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for Text Seg-
mentation. In Proceedings of the 6th EMNLP, pages
109?117, Seattle, WA.
J Cohen and P Cohen. 1983. Applied Multiple Regres-
sion/Correlation Analysis for the Behavioral Sciences.
Hillsdale, NJ: Erlbaum.
Yansong Feng and Mirella Lapata. 2010. Visual Infor-
mation in Semantic Representation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 91?99, Los Ange-
les, California, June. Association for Computational
Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing Search in Context: The Con-
cept Revisited. ACMTransactions on Information Sys-
tems, 20(1):116?131, January.
Arthur M. Glenberg and Michael P. Kaschak. 2002.
Grounding Language in Action. Psychonomic Bulletin
and Review, 9(3):558?565.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
T. L. Griffiths, M. Steyvers, and J. B. Tenenbaum. 2007.
Topics in Semantic Representation. Psychological Re-
view, 114(2):211?244.
David R. Hardoon, Sandor R. Szedmak, and John R.
Shawe-Taylor. 2004. Canonical Correlation Analysis:
An Overview with Application to Learning Methods.
Neural Computation, 16(12):2639?2664.
H Hotelling. 1936. Relations between Two Sets of Vari-
ates. Biometrika, 28:312?377.
Steve R. Howell, Damian Jankowicz, and Suzanna
Becker. 2005. A Model of Grounded Language Ac-
quisition: Sensorimotor Features Improve Lexical and
Grammatical Learning. Journal of Memory and Lan-
guage, 53(2), 258-276, 53(2):258?276.
Brendan T. Johns and Michael N. Jones. 2012. Percep-
tual Inference through Global Lexical Similarity. Top-
ics in Cognitive Science, 4(1):103?120.
B. Landau, L. Smith, and S. Jones. 1998. Object Percep-
tion and Object Naming in Early Development. Trends
in Cognitive Science, 27:19?24.
T. Landauer and S. T. Dumais. 1997. A Solution to
Plato?s Problem: the Latent Semantic Analysis The-
ory of Acquisition, Induction, and Representation of
Knowledge. Psychological Review, 104(2):211?240.
1432
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the joint Annual
Meeting of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics, pages 768?774, Montre?al, Canada.
K. McRae, G. S. Cree, M. S. Seidenberg, and C. McNor-
gan. 2005. Semantic Feature Production Norms for a
Large Set of Living and Nonliving Things. Behavior
Research Methods, 37(4):547?559, November.
D. L. Nelson, C. L. McEvoy, and T. A. Schreiber. 1998.
The University of South Florida Word Association,
Rhyme, and Word Fragment Norms.
C. Perfetti. 1998. The Limits of Co-occurrence: Tools
and Theories in Language Research. Discourse Pro-
cesses, (25):363?377.
Terry Regier. 1996. The Human Semantic Potential.
MIT Press, Cambridge, MA.
T. T. Rogers, M. A. Lambon Ralph, P. Garrard, S. Bozeat,
J. L. McClelland, J. R. Hodges, and K. Patterson.
2004. Structure and Deterioration of Semantic Mem-
ory: A Neuropsychological and Computational Inves-
tigation. Psychological Review, 111(1):205?235.
G Salton, A Wang, and C Yang. 1975. A Vector-space
Model for Information Retrieval. Journal of the Amer-
ican Society for Information Science, 18:613?620.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1):97?124.
Mark Steyvers. 2010. Combining Feature Norms and
Text Data with Topic Models. Acta Psychologica,
133(3):234?342.
Wouter Voorspoels, Wolf Vanpaemel, and Gert Storms.
2008. Exemplars and Prototypes in Natural Language
Concepts: A Typicality-based Evaluation. Psycho-
nomic Bulletin & Review, 15:630?637.
1433
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572?582,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Models of Semantic Representation with Visual Attributes
Carina Silberer, Vittorio Ferrari, Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
c.silberer@ed.ac.uk, vferrari@inf.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
We consider the problem of grounding the
meaning of words in the physical world
and focus on the visual modality which we
represent by visual attributes. We create
a new large-scale taxonomy of visual at-
tributes covering more than 500 concepts
and their corresponding 688K images. We
use this dataset to train attribute classi-
fiers and integrate their predictions with
text-based distributional models of word
meaning. We show that these bimodal
models give a better fit to human word as-
sociation data compared to amodal models
and word representations based on hand-
crafted norming data.
1 Introduction
Recent years have seen increased interest in
grounded language acquisition, where the goal is
to extract representations of the meaning of nat-
ural language tied to the physical world. The
language grounding problem has assumed sev-
eral guises in the literature such as semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Kate and Mooney, 2007; Lu et
al., 2008; Bo?rschinger et al, 2011), mapping nat-
ural language instructions to executable actions
(Branavan et al, 2009; Tellex et al, 2011), associ-
ating simplified language to perceptual data such
as images or video (Siskind, 2001; Roy and Pent-
land, 2002; Gorniak and Roy, 2004; Yu and Bal-
lard, 2007), and learning the meaning of words
based on linguistic and perceptual input (Bruni
et al, 2012b; Feng and Lapata, 2010; Johns and
Jones, 2012; Andrews et al, 2009; Silberer and
Lapata, 2012).
In this paper we are concerned with the latter
task, namely constructing perceptually grounded
distributional models. The motivation for models
that do not learn exclusively from text is twofold.
From a cognitive perspective, there is mounting
experimental evidence suggesting that our inter-
action with the physical world plays an impor-
tant role in the way we process language (Barsa-
lou, 2008; Bornstein et al, 2004; Landau et al,
1998). From an engineering perspective, the abil-
ity to learn representations for multimodal data has
many practical applications including image re-
trieval (Datta et al, 2008) and annotation (Chai
and Hung, 2008), text illustration (Joshi et al,
2006), object and scene recognition (Lowe, 1999;
Oliva and Torralba, 2007; Fei-Fei and Perona,
2005), and robot navigation (Tellex et al, 2011).
One strand of research uses feature norms as a
stand-in for sensorimotor experience (Johns and
Jones, 2012; Andrews et al, 2009; Steyvers, 2010;
Silberer and Lapata, 2012). Feature norms are ob-
tained by asking native speakers to write down at-
tributes they consider important in describing the
meaning of a word. The attributes represent per-
ceived physical and functional properties associ-
ated with the referents of words. For example,
apples are typically green or red, round, shiny,
smooth, crunchy, tasty, and so on; dogs have four
legs and bark, whereas chairs are used for sit-
ting. Feature norms are instrumental in reveal-
ing which dimensions of meaning are psychologi-
cally salient, however, their use as a proxy for peo-
ple?s perceptual representations can itself be prob-
lematic (Sloman and Ripps, 1998; Zeigenfuse and
Lee, 2010). The number and types of attributes
generated can vary substantially as a function of
the amount of time devoted to each concept. It is
not entirely clear how people generate attributes
and whether all of these are important for repre-
senting concepts. Finally, multiple participants are
required to create a representation for each con-
572
cept, which limits elicitation studies to a small
number of concepts and the scope of any compu-
tational model based on feature norms.
Another strand of research focuses exclusively
on the visual modality, even though the grounding
problem could involve auditory, motor, and hap-
tic modalities as well. This is not entirely sur-
prising. Visual input represents a major source of
data from which humans can learn semantic rep-
resentations of linguistic and non-linguistic com-
municative actions (Regier, 1996). Furthermore,
since images are ubiquitous, visual data can be
gathered far easier than some of the other modali-
ties. Distributional models that integrate the visual
modality have been learned from texts and im-
ages (Feng and Lapata, 2010; Bruni et al, 2012b)
or from ImageNet (Deng et al, 2009), e.g., by
exploiting the fact that images in this database
are hierarchically organized according to WordNet
synsets (Leong and Mihalcea, 2011). Images are
typically represented on the basis of low-level fea-
tures such as SIFT (Lowe, 2004), whereas texts
are treated as bags of words.
Our work also focuses on images as a way
of physically grounding the meaning of words.
We, however, represent them by high-level vi-
sual attributes instead of low-level image fea-
tures. Attributes are not concept or category spe-
cific (e.g., animals have stripes and so do cloth-
ing items; balls are round, and so are oranges and
coins), and thus allow us to express similarities
and differences across concepts more easily. Fur-
thermore, attributes allow us to generalize to un-
seen objects; it is possible to say something about
them even though we cannot identify them (e.g., it
has a beak and a long tail). We show that this
attribute-centric approach to representing images
is beneficial for distributional models of lexical
meaning. Our attributes are similar to those pro-
vided by participants in norming studies, however,
importantly they are learned from training data (a
database of images and their visual attributes) and
thus generalize to new images without additional
human involvement.
In the following we describe our efforts to cre-
ate a new large-scale dataset that consists of 688K
images that match the same concrete concepts
used in the feature norming study of McRae et al
(2005). We derive a taxonomy of 412 visual at-
tributes and explain how we learn attribute clas-
sifiers following recent work in computer vision
(Lampert et al, 2009; Farhadi et al, 2009). Next,
we show that this attribute-based image represen-
tation can be usefully integrated with textual data
to create distributional models that give a better fit
to human word association data over models that
rely on human generated feature norms.
2 Related Work
Grounding semantic representations with visual
information is an instance of multimodal learn-
ing. In this setting the data consists of multiple
input modalities with different representations and
the learner?s objective is to extract a unified repre-
sentation that fuses the modalities together. The
literature describes several successful approaches
to multimodal learning using different variants of
deep networks (Ngiam et al, 2011; Srivastava and
Salakhutdinov, 2012) and data sources including
text, images, audio, and video.
Special-purpose models that address the fusion
of distributional meaning with visual information
have been also proposed. Feng and Lapata (2010)
represent documents and images by a common
multimodal vocabulary consisting of textual words
and visual terms which they obtain by quantizing
SIFT descriptors (Lowe, 2004). Their model is es-
sentially Latent Dirichlet Allocation (LDA, Blei et
al., 2003) trained on a corpus of multimodal docu-
ments (i.e., BBC news articles and their associated
images). Meaning in this model is represented as
a vector whose components correspond to word-
topic distributions. A related model has been pro-
posed by Bruni et al (2012b) who obtain distinct
representations for the textual and visual modali-
ties. Specifically, they extract a visual space from
images contained in the ESP-Game data set (von
Ahn and Dabbish, 2004) and a text-based seman-
tic space from a large corpus collection totaling
approximately two billion words. They concate-
nate the two modalities and subsequently project
them to a lower-dimensionality space using Sin-
gular Value Decomposition (Golub et al, 1981).
Traditionally, computer vision algorithms de-
scribe visual phenomena (e.g., objects, scenes,
faces, actions) by giving each instance a categor-
ical label (e.g., cat, beer garden, Brad Pitt, drink-
ing). The ability to describe images by their at-
tributes allows to generalize to new instances for
which there are no training examples available.
Moreover, attributes can transcend category and
task boundaries and thus provide a generic de-
scription of visual data.
Initial work (Ferrari and Zisserman, 2007)
573
focused on simple color and texture attributes
(e.g., blue, stripes) and showed that these can be
learned in a weakly supervised setting from im-
ages returned by a search engine when using the
attribute as a query. Farhadi et al (2009) were
among the first to use visual attributes in an ob-
ject recognition task. Using an inventory of 64 at-
tribute labels, they developed a dataset of approx-
imately 12,000 instances representing 20 objects
from the PASCAL Visual Object Classes Chal-
lenge 2008 (Everingham et al, 2008). Visual
semantic attributes (e.g., hairy, four-legged) were
used to identify familiar objects and to describe
unfamiliar objects when new images and bound-
ing box annotations were provided. Lampert et al
(2009) showed that attribute-based representations
can be used to classify objects when there are no
training examples of the target classes available.
Their dataset contained over 30,000 images repre-
senting 50 animal concepts and used 85 attributes
from the norming study of Osherson et al (1991).
Attribute-based representations have also been ap-
plied to the tasks of face detection (Kumar et al,
2009), action identification (Liu et al, 2011), and
scene recognition (Patterson and Hays, 2012).
The use of visual attributes in models of distri-
butional semantics is novel to our knowledge. We
argue that they are advantageous for two reasons.
Firstly, they are cognitively plausible; humans em-
ploy visual attributes when describing the proper-
ties of concept classes. Secondly, they occupy the
middle ground between non-linguistic low-level
image features and linguistic words. Attributes
crucially represent image properties, however by
being words themselves, they can be easily inte-
grated in any text-based distributional model thus
eschewing known difficulties with rendering im-
ages into word-like units.
A key prerequisite in describing images by
their attributes is the availability of training data
for learning attribute classifiers. Although our
database shares many features with previous work
(Lampert et al, 2009; Farhadi et al, 2009) it dif-
fers in focus and scope. Since our goal is to
develop distributional models that are applicable
to many words, it contains a considerably larger
number of concepts (i.e., more than 500) and at-
tributes (i.e., 412) based on a detailed taxonomy
which we argue is cognitively plausible and ben-
eficial for image and natural language processing
tasks. Our experiments evaluate a number of mod-
els previously proposed in the literature and in
Attribute Categories Example Attributes
color patterns (25) is red, has stripes
diet (35) eats nuts, eats grass
shape size (16) is small, is chubby
parts (125) has legs, has wheels
botany;anatomy (25;78) has seeds, has fur
behavior (in)animate (55) flies, waddles, pecks
texture material (36) made of metal, is shiny
structure (3) 2 pieces, has pleats
Table 1: Attribute categories and examples of at-
tribute instances. Parentheses denote the number
of attributes per category.
all cases show that the attribute-based represen-
tation brings performance improvements over just
using the textual modality. Moreover, we show
that automatically computed attributes are compa-
rable and in some cases superior to those provided
by humans (e.g., in norming studies).
3 The Attribute Dataset
Concepts and Images We created a dataset of
images and their visual attributes for the nouns
contained in McRae et al?s (2005) feature norms.
The norms cover a wide range of concrete con-
cepts including animate and inanimate things
(e.g., animals, clothing, vehicles, utensils, fruits,
and vegetables) and were collected by presenting
participants with words and asking them to list
properties of the objects to which the words re-
ferred. To avoid confusion, in the remainder of
this paper we will use the term attribute to refer to
properties of concepts and the term feature to refer
to image features, such as color or edges.
Images for the concepts in McRae et al?s (2005)
production norms were harvested from ImageNet
(Deng et al, 2009), an ontology of images based
on the nominal hierarchy of WordNet (Fellbaum,
1998). ImageNet has more than 14 million im-
ages spanning 21K WordNet synsets. We chose
this database due to its high coverage and the high
quality of its images (i.e., cleanly labeled and high
resolution). McRae et al?s norms contain 541 con-
cepts out of which 516 appear in ImageNet1 and
are represented by 688K images overall. The av-
erage number of images per concept is 1,310 with
the most popular being closet (2,149 images) and
the least popular prune (5 images).
1Some words had to be modified in order to match the cor-
rect synset, e.g., tank (container) was found as storage tank.
574
behavior eats, walks, climbs, swims, runs
diet drinks water, eats anything
shape size is tall, is large
anatomy has mouth, has head, has nose, has tail, has claws,
has jaws, has neck, has snout, has feet, has tongue
color patterns is black, is brown, is white
botany has skin, has seeds, has stem, has leaves, has pulp
color patterns purple, white, green, has green top
shape size is oval, is long
texture material is shiny
behavior rolls
parts has step through frame, has fork, has 2 wheels, has chain, has pedals
has gears, has handlebar, has bell, has breaks has seat, has spokes
texture material made of metal
color patterns different colors, is black, is red, is grey, is silver
Table 2: Human-authored attributes for bear, eggplant, and bike.
The images depicting each concept were ran-
domly partitioned into a training, development,
and test set. For most concepts the development
set contained a maximum of 100 images and the
test set a maximum of 200 images. Concepts with
less than 800 images in total were split into 1/8
test and development set each, and 3/4 training set.
The development set was used for devising and re-
fining our attribute annotation scheme. The train-
ing and test sets were used for learning and eval-
uating, respectively, attribute classifiers (see Sec-
tion 4).
Attribute Annotation Our aim was to develop a
set of visual attributes that are both discriminating
and cognitively plausible, i.e., humans would gen-
erally use them to describe a concrete concept. As
a starting point, we thus used the visual attributes
from McRae et al?s (2005) norming study. At-
tributes capturing other primary sensory informa-
tion (e.g., smell, sound), functional/motor proper-
ties, or encyclopaedic information were not taken
into account. For example, is purple is a valid vi-
sual attribute for an eggplant, whereas a vegetable
is not, since it cannot be visualized. Collating all
the visual attributes in the norms resulted in a to-
tal of 673 which we further modified and extended
during the annotation process explained below.
The annotation was conducted on a per-concept
rather than a per-image basis (as for example in
Farhadi et al (2009)). For each concept (e.g., bear
or eggplant), we inspected the images in the devel-
opment set and chose all McRae et al (2005) vi-
sual attributes that applied. If an attribute was gen-
erally true for the concept, but the images did not
provide enough evidence, the attribute was never-
theless chosen and labeled with <no evidence>.
For example, a plum has a pit, but most images in
ImageNet show plums where only the outer part
of the fruit is visible. Attributes supported by
the image data but missing from the norms were
added. For example, has lights and has bumper
are attributes of cars but are not included in the
norms. Attributes were grouped in eight general
classes shown in Table 1. Annotation proceeded
on a category-by-category basis, e.g., first all food-
related concepts were annotated, then animals, ve-
hicles, and so on. Two annotators (both co-authors
of this paper) developed the set of attributes for
each category. One annotator first labeled con-
cepts with their attributes, and the other annota-
tor reviewed the annotations, making changes if
needed. Annotations were revised and compared
per category in order to ensure consistency across
all concepts of that category.
Our methodology is slightly different from
Lampert et al (2009) in that we did not simply
transfer the attributes from the norms to the con-
cepts in question but refined and extended them
according to the visual data. There are several
reasons for this. Firstly, it makes sense to se-
lect attributes corroborated by the images. Sec-
ondly, by looking at the actual images, we could
eliminate errors in McRae et al?s (2005) norms.
For example, eight study participants erroneously
thought that a catfish has scales. Thirdly, dur-
ing the annotation process, we normalized syn-
onymous attributes (e.g., has pit and has stone)
and attributes that exhibited negligible variations
575
has 2 pieces, has pointed end, has strap, has thumb, has buckles, has heels
has shoe laces, has soles, is black, is brown, is white, made of leather, made of rubber
climbs, climbs trees, crawls, hops, jumps, eats, eats nuts, is small, has bushy tail
has 4 legs, has head, has neck, has nose, has snout, has tail, has claws
has eyes, has feet, has toes,
diff colours, has 2 legs, has 2 wheels, has windshield, has floorboard, has stand, has tank
has mudguard, has seat, has exhaust pipe, has frame, has handlebar, has lights, has mirror
has step-through frame, is black, is blue, is red, is white, made of aluminum, made of steel
Table 3: Attribute predictions for sandals, squirrel, and motorcycle.
in meaning (e.g., has stem and has stalk). Finally,
our aim was to collect an exhaustive list of vi-
sual attributes for each concept which is consis-
tent across all members of a category. This is un-
fortunately not the case in McRae et al?s norms.
Participants were asked to list up to 14 different
properties that describe a concept. As a result, the
attributes of a concept denote the set of properties
humans consider most salient. For example, both,
lemons and oranges have pulp. But the norms pro-
vide this attribute only for the second concept.
On average, each concept was annotated with
19 attributes; approximately 14.5 of these were
not part of the semantic representation created by
McRae et al?s (2005) participants for that con-
cept even though they figured in the representa-
tions of other concepts. Furthermore, on average
two McRae et al attributes per concept were dis-
carded. Examples of concepts and their attributes
from our database2 are shown in Table 2.
4 Attribute-based Classification
Following previous work (Farhadi et al, 2009;
Lampert et al, 2009) we learned one classifier per
attribute (i.e., 350 classifiers in total).3 The train-
ing set consisted of 91,980 images (with a maxi-
mum of 350 images per concept). We used an L2-
regularized L2-loss linear SVM (Fan et al, 2008)
to learn the attribute predictions. We adopted the
training procedure of Farhadi al. (2009).4 To learn
a classifier for a particular attribute, we used all
images in the training data. Images of concepts
annotated with the attribute were used as positive
examples, and the rest as negative examples. The
2Available from http://homepages.inf.ed.ac.uk/
mlap/index.php?page=resources.
3We only trained classifiers for attributes corroborated by
the images and excluded those labeled with <no evidence>.
4http://vision.cs.uiuc.edu/attributes/
data was randomly split into a training and valida-
tion set of equal size in order to find the optimal
cost parameter C. The final SVM for the attribute
was trained on the entire training data, i.e., on all
positive and negative examples.
The SVM learners used the four different fea-
ture types proposed in Farhadi et al (2009),
namely color, texture, visual words, and edges.
Texture descriptors were computed for each pixel
and quantized to the nearest 256 k-means centers.
Visual words were constructed with a HOG spa-
tial pyramid. HOG descriptors were quantized
into 1000 k-means centers. Edges were detected
using a standard Canny detector and their orien-
tations were quantized into eight bins. Color de-
scriptors were sampled for each pixel and quan-
tized to the nearest 128 k-means centers. Shapes
and locations were represented by generating his-
tograms for each feature type for each cell in a grid
of three vertical and horizontal blocks. Our clas-
sifiers used 9,688 features in total. Table 3 shows
their predictions for three test images.
Note that attributes are predicted on an image-
by-image basis; our task, however, is to describe a
concept w by its visual attributes. Since concepts
are represented by many images we must some-
how aggregate their attributes into a single repre-
sentation. For each image iw ? Iw of concept w,
we output an F-dimensional vector containing pre-
diction scores scorea(iw) for attributes a = 1, ...,F.
We transform these attribute vectors into a single
vector pw ? [0,1]1?F , by computing the centroid
of all vectors for concept w. The vector is nor-
malized to obtain a probability distribution over
attributes given w:
pw = (?iw?Iw scorea(iw))a=1,...,F?Fa=1?iw?Iw scorea(iw)
(1)
We additionally impose a threshold ? on pw by set-
576
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
P
r
e
c
i
s
i
o
n
Recall
Figure 1: Attribute classifier performance for dif-
ferent thresholds ? (test set).
ting each entry less than ? to zero.
Figure 1 shows the results of the attribute pre-
diction on the test set on the basis of the computed
centroids; specifically, we plot recall against pre-
cision based on threshold ?.5 Table 4 shows the
10 nearest neighbors for five example concepts
from our dataset. Again, we measure the cosine
similarity between a concept and all other con-
cepts in the dataset when these are represented by
their visual attribute vector pw.
5 Attribute-based Semantic Models
We evaluated the effectiveness of our attribute
classifiers by integrating their predictions with tra-
ditional text-only models of semantic representa-
tion. These models have been previously proposed
in the literature and were also described in a recent
comparative study (Silberer and Lapata, 2012).
We represent the visual modality by attribute
vectors computed as shown in Equation (1). The
linguistic environment is approximated by textual
attributes. We used Strudel (Baroni et al, 2010)
to obtain these attributes for the nouns in our
dataset. Given a list of target words, Strudel ex-
tracts weighted word-attribute pairs from a lem-
matized and pos-tagged text corpus (e.g., egg-
plant?cook-v, eggplant?vegetable-n). The weight
of each word-attribute pair is a log-likelihood ratio
score expressing the pair?s strength of association.
In our experiments we learned word-attribute pairs
from a lemmatized and pos-tagged (2009) dump
of the English Wikipedia.6 In the remainder of
this section we will briefly describe the models we
5Threshold values ranged from 0 to 0.9 with 0.1 stepsize.
6The corpus can be downloaded from http://wacky.
sslmit.unibo.it/doku.php?id=corpora.
Concept Nearest Neighbors
boat ship, sailboat, yacht, submarine, canoe,
whale, airplane, jet, helicopter, tank (army)
rooster chicken, turkey, owl, pheasant, peacock, stork,
pigeon, woodpecker, dove, raven
shirt blouse, robe, cape, vest, dress, coat, jacket,
skirt, camisole, nightgown
spinach lettuce, parsley, peas, celery, broccoli, cab-
bage, cucumber, rhubarb, zucchini, asparagus
squirrel chipmunk, raccoon, groundhog, gopher, por-
cupine, hare, rabbit, fox, mole, emu
Table 4: Ten most similar concepts computed on
the basis of averaged attribute vectors and ordered
according to cosine similarity.
used in our study and how the textual and visual
modalities were fused to create a joint representa-
tion.
Concatenation Model Variants of this model
were originally proposed in Bruni et al (2011)
and Johns and Jones (2012). Let T ? RN?D de-
note a term-attribute co-occurrence matrix, where
each cell records a weighted co-occurrence score
of a word and a textual attribute. Let P ? [0,1]N?F
denote a visual matrix, representing a probability
distribution over visual attributes for each word.
A word?s meaning can be then represented by the
concatenation of its normalized textual and visual
vectors.
Canonical Correlation Analysis The second
model uses Canonical Correlation Analysis (CCA,
Hardoon et al (2004)) to learn a joint semantic
representation from the textual and visual modali-
ties. Given two random variables x and y (or two
sets of vectors), CCA can be seen as determining
two sets of basis vectors in such a way, that the cor-
relation between the projections of the variables
onto these bases is mutually maximized (Borga,
2001). In effect, the representation-specific de-
tails pertaining to the two views of the same phe-
nomenon are discarded and the underlying hidden
factors responsible for the correlation are revealed.
The linguistic and visual views are the same as
in the simple concatenation model just explained.
We use a kernelized version of CCA (Hardoon et
al., 2004) that first projects the data into a higher-
dimensional feature space and then performs CCA
in this new feature space. The two kernel matrices
are KT = T T ? and KP = PP?. After applying CCA
we obtain two matrices projected onto l basis vec-
tors, T? ?RN?l , resulting from the projection of the
577
textual matrix T onto the new basis and P? ?RN?l ,
resulting from the projection of the corresponding
visual attribute matrix. The meaning of a word is
then represented by T? or P?.
Attribute-topic Model Andrews et al (2009)
present an extension of LDA (Blei et al, 2003)
where words in documents and their associated
attributes are treated as observed variables that
are explained by a generative process. The
idea is that each document in a document col-
lection D is generated by a mixture of com-
ponents {x1, ...,xc, ...,xC} ? C , where a compo-
nent xc comprises a latent discourse topic coupled
with an attribute cluster. Inducing these attribute-
topic components from D with the extended LDA
model gives two sets of parameters: word prob-
abilities given components PW (wi|X = xc) for wi,
i = 1, ...,n, and attribute probabilities given com-
ponents PA(ak|X = xc) for ak, k = 1, ...,F . For ex-
ample, most of the probability mass of a compo-
nent x would be reserved for the words shirt, coat,
dress and the attributes has 1 piece, has seams,
made of material and so on.
Word meaning in this model is represented by
the distribution PX |W over the learned compo-
nents. Assuming a uniform distribution over com-
ponents xc in D , PX |W can be approximated as:
PX=xc|W=wi =
P(wi|xc)P(xc)
P(wi) ?
P(wi|xc)
C?
l=1
P(wi|xl)
(2)
where C is the total number of components.
In our work, the training data is a corpus D of
textual attributes (rather than documents). Each
attribute is represented as a bag-of-concepts,
i.e., words demonstrating the property expressed
by the attribute (e.g., vegetable-n is a property of
eggplant, spinach, carrot). For some of these con-
cepts, our classifiers predict visual attributes. In
this case, the concepts are paired with one of their
visual attributes. We sample attributes for a con-
cept w from their distribution given w (Eq. (1)).
6 Experimental Setup
Evaluation Task We evaluated the distribu-
tional models presented in Section 5 on the
word association norms collected by Nelson et al
(1998).7 These were established by presenting
a large number of participants with a cue word
(e.g., rice) and asking them to name an associate
7From http://w3.usf.edu/FreeAssociation/.
word in response (e.g., Chinese, wedding, food,
white). For each cue, the norms provide a set
of associates and the frequencies with which they
were named. We can thus compute the prob-
ability distribution over associates for each cue.
Analogously, we can estimate the degree of sim-
ilarity between a cue and its associates using our
models. The norms contain 63,619 unique cue-
associate pairs. Of these, 435 pairs were covered
by McRae et al (2005) and our models. We also
experimented with 1,716 pairs that were not part
of McRae et al?s study but belonged to concepts
covered by our attribute taxonomy (e.g., animals,
vehicles), and were present in our corpus and Ima-
geNet. Using correlation analysis (Spearman?s ?),
we examined the degree of linear relationship be-
tween the human cue-associate probabilities and
the automatically derived similarity values.8
Parameter Settings In order to integrate the vi-
sual attributes with the models described in Sec-
tion 5 we must select the appropriate threshold
value ? (see Eq. (1)). We optimized this value
on the development set and obtained best results
with ? = 0. We also experimented with thresh-
olding the attribute prediction scores and with ex-
cluding attributes with low precision. In both
cases, we obtained best results when using all at-
tributes. We could apply CCA to the vectors rep-
resenting each image separately and then compute
a weighted centroid on the projected vectors. We
refrained from doing this as it involves additional
parameters and assumes input different from the
other models. We measured the similarity between
two words using the cosine of the angle. For the
attribute-topic model, the number of predefined
components C was set to 10. In this model, sim-
ilarity was measured as defined by Griffiths et al
(2007). The underlying idea is that word associa-
tion can be expressed as a conditional distribution.
With regard to the textual attributes, we
obtained a 9,394-dimensional semantic space
after discarding word-attribute pairs with a
log-likelihood ratio score less than 19.9 We also
discarded attributes co-occurring with less than
two different words.
8Previous work (Griffiths et al, 2007) which also predicts
word association reports how many times the word with the
highest score under the model was the first associate in the
human norms. This evaluation metric assumes that there are
many associates for a given cue which unfortunately is not
the case in our study which is restricted to the concepts rep-
resented in our attribute taxonomy.
9Baroni et al (2010) use a similar threshold of 19.51.
578
Nelson Concat CCA TopicAttr TextAttr
Concat 0.24
CCA 0.30 0.72
TopicAttr 0.26 0.55 0.28
TextAttr 0.21 0.80 0.83 0.34
VisAttr 0.23 0.65 0.52 0.40 0.39
Table 5: Correlation matrix for seen Nelson et al
(1998) cue-associate pairs and five distributional
models. All correlation coefficients are statisti-
cally significant (p < 0.01, N = 435).
7 Results
Our experiments were designed to answer four
questions: (1) Do visual attributes improve the
performance of distributional models? (2) Are
there performance differences among different
models, i.e., are some models better suited to the
integration of visual information? (3) How do
computational models fare against gold standard
norming data? (4) Does the attribute-based repre-
sentation bring advantages over more conventional
approaches based on raw image features?
Our results are broken down into seen (Table 5)
and unseen (Table 6) concepts. The former are
known to the attribute classifiers and form part
of our database, whereas the latter are unknown
and are not included in McRae et al?s (2005)
norms. We report the correlation coefficients we
obtain when human-derived cue-associate proba-
bilities (Nelson et al, 1998) are compared against
the simple concatenation model (Concat), CCA,
and Andrews et al?s (2009) attribute-topic model
(TopicAttr). We also report the performance of
a distributional model that is based solely on the
output of our attribute classifiers, i.e., without any
textual input (VisAttr) and conversely the perfor-
mance of a model that uses textual information
only (i.e., Strudel attributes) without any visual in-
put (TextAttr). The results are displayed as a cor-
relation matrix so that inter-model correlations can
also be observed.
As can be seen in Table 5 (second column), two
modalities are in most cases better than one when
evaluating model performance on seen data. Dif-
ferences in correlation coefficients between mod-
els with two versus one modality are all statis-
tically significant (p < 0.01 using a t-test), with
the exception of Concat when compared against
VisAttr. It is also interesting to note that Topi-
cAttr is the least correlated model when compared
against other bimodal models or single modali-
Nelson Concat CCA TopicAttr TextAttr
Concat 0.11
CCA 0.15 0.66
TopicAttr 0.17 0.69 0.48
TextAttr 0.11 0.65 0.25 0.39
VisAttr 0.13 0.57 0.87 0.57 0.34
Table 6: Correlation matrix for unseen Nelson
et al (1998) cue-associate pairs and five distribu-
tional models. All correlation coefficients are sta-
tistically significant (p < 0.01, N = 1,716).
ties. This indicates that the latent space obtained
by this model is most distinct from its constituent
parts (i.e., visual and textual attributes). Perhaps
unsuprisingly Concat, CCA, VisAttr, and TextAttr
are also highly intercorrelated.
On unseen pairs (see Table 6), Concat fares
worse than CCA and TopicAttr, achieving simi-
lar performance to TextAttr. CCA and TopicAttr
are significantly better than TextAttr and VisAttr
(p < 0.01). This indicates that our attribute classi-
fiers generalize well beyond the concepts found in
our database and can produce useful visual infor-
mation even on unseen images. Compared to Con-
cat and CCA, TopicAttr obtains a better fit with the
human association norms on the unseen data.
To answer our third question, we obtained dis-
tributional models from McRae et al?s (2005)
norms and assessed how well they predict Nelson
et al?s (1998) word-associate similarities. Each
concept was represented as a vector with dimen-
sions corresponding to attributes generated by par-
ticipants of the norming study. Vector components
were set to the (normalized) frequency with which
participants generated the corresponding attribute
when presented with the concept. We measured
the similarity between two words using the co-
sine coefficient. Table 7 presents results for dif-
ferent model variants which we created by ma-
nipulating the number and type of attributes in-
volved. The first model uses the full set of at-
tributes present in the norms (All Attributes). The
second model (Text Attributes) uses all attributes
but those classified as visual (e.g., functional, en-
cyclopaedic). The third model (Visual Attributes)
considers solely visual attributes.
We observe a similar trend as with our compu-
tational models. Taking visual attributes into ac-
count increases the fit with Nelson?s (1998) associ-
ation norms, whereas visual and textual attributes
on their own perform worse. Interestingly, CCA?s
579
Models Seen
All Attributes 0.28
Text Attributes 0.20
Visual Attributes 0.25
Table 7: Model performance on seen Nelson et
al. (1998) cue-associate pairs; models are based
on gold human generated attributes (McRae et al,
2005). All correlation coefficients are statistically
significant (p < 0.01, N = 435).
Models Seen Unseen
Concat 0.22 0.10
CCA 0.26 0.15
TopicAttr 0.23 0.19
TextAttr 0.20 0.08
VisAttr 0.21 0.13
MixLDA 0.16 0.11
Table 8: Model performance on a subset of Nelson
et al (1998) cue-associate pairs. Seen are concepts
known to the attribute classifiers and covered by
MixLDA (N = 85). Unseen are concepts covered
by LDA but unknown to the attribute classifiers
(N = 388). All correlation coefficients are statisti-
cally significant (p < 0.05).
performance is comparable to the All Attributes
model (see Table 5, second column), despite us-
ing automatic attributes (both textual and visual).
Furthermore, visual attributes obtained through
our classifiers (see Table 5) achieve a marginally
lower correlation coefficient against human gener-
ated ones (see Table 7).
Finally, to address our last question, we com-
pared our approach against Feng and Lapata
(2010) who represent visual information via quan-
tized SIFT features. We trained their MixLDA
model on their corpus consisting of 3,361 BBC
news documents and corresponding images (Feng
and Lapata, 2008). We optimized the model pa-
rameters on a development set consisting of cue-
associate pairs from Nelson et al (1998), exclud-
ing the concepts in McRae et al (2005). We
used a vocabulary of approximately 6,000 words.
The best performing model on the development set
used 500 visual terms and 750 topics and the asso-
ciation measure proposed in Griffiths et al (2007).
The test set consisted of 85 seen and 388 unseen
cue-associate pairs that were covered by our mod-
els and MixLDA.
Table 8 reports correlation coefficients for our
models and MixLDA against human probabili-
ties. All attribute-based models significantly out-
perform MixLDA on seen pairs (p < 0.05 using a
t-test). MixLDA performs on a par with the con-
catenation model on unseen pairs, however CCA,
TopicAttr, and VisAttr are all superior. Although
these comparisons should be taken with a grain
of salt, given that MixLDA and our models are
trained on different corpora (MixLDA assumes
that texts and images are collocated, whereas our
images do not have collateral text), they seem to
indicate that attribute-based information is indeed
beneficial.
8 Conclusions
In this paper we proposed the use of automatically
computed visual attributes as a way of physically
grounding word meaning. Our results demonstrate
that visual attributes improve the performance of
distributional models across the board. On a
word association task, CCA and the attribute-topic
model give a better fit to human data when com-
pared against simple concatenation and models
based on a single modality. CCA consistently out-
performs the attribute-topic model on seen data (it
is in fact slightly better over a model that uses gold
standard human generated attributes), whereas the
attribute-topic model generalizes better on unseen
data (see Tables 5, 6, and 8). Since the attribute-
based representation is general and text-based we
argue that it can be conveniently integrated with
any type of distributional model or indeed other
grounded models that rely on low-level image fea-
tures (Bruni et al, 2012a; Feng and Lapata, 2010)
In the future, we would like to extend our
database to actions and show that this attribute-
centric representation is useful for more applied
tasks such as image description generation and ob-
ject recognition. Finally, we have only scratched
the surface in terms of possible models for inte-
grating the textual and visual modality. Interest-
ing frameworks which we plan to explore are deep
belief networks and Bayesian non-parametrics.
References
M. Andrews, G. Vigliocco, and D. Vinson. 2009.
Integrating Experiential and Distributional Data to
Learn Semantic Representations. Psychological Re-
view, 116(3):463?498.
M. Baroni, B. Murphy, E. Barbu, and M. Poesio.
2010. Strudel: A Corpus-Based Semantic Model
580
Based on Properties and Types. Cognitive Science,
34(2):222?254.
L. W. Barsalou. 2008. Grounded Cognition. Annual
Review of Psychology, 59:617?845.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet Allocation. Journal of Machine Learning
Research, 3:993?1022, March.
M. Borga. 2001. Canonical Correlation ? a Tutorial,
January.
M. H. Bornstein, L. R. Cote, S. Maital, K. Painter, S.-Y.
Park, L. Pascual, M. G. Pe?cheux, J. Ruel, P. Venuti,
and A. Vyt. 2004. Cross-linguistic Analysis of
Vocabulary in Young Children: Spanish, Dutch,
French, Hebrew, Italian, Korean, and American En-
glish. Child Development, 75(4):1115?1139.
B. Bo?rschinger, B. K. Jones, and M. Johnson. 2011.
Reducing Grounded Learning Tasks to Grammatical
Inference. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1416?1425, Edinburgh, UK.
S.R.K. Branavan, H. Chen, L. S. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement Learning for
Mapping Instructions to Actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 82?90, Suntec, Singapore.
E. Bruni, G. Tran, and M. Baroni. 2011. Distributional
Semantics from Text and Images. In Proceedings of
the GEMS 2011 Workshop on GEometrical Models
of Natural Language Semantics, pages 22?32, Edin-
burgh, UK.
E. Bruni, G. Boleda, M. Baroni, and N. Tran. 2012a.
Distributional Semantics in Technicolor. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 136?145, Jeju Island, Korea.
E. Bruni, J. Uijlings, M. Baroni, and N. Sebe. 2012b.
Distributional semantics with eyes: Using im-
age analysis to improve computational representa-
tions of word meaning. In Proceedings of the
20th ACM International Conference on Multimedia,
pages 1219?1228., New York, NY.
C. Chai and C. Hung. 2008. Automatically Annotating
Images with Keywords: A Review of Image Annota-
tion Systems. Recent Patents on Computer Science,
1:55?68.
R. Datta, D. Joshi, J. Li, and J. Z. Wang. 2008. Image
Retrieval: Ideas, Influences, and Trends of the New
Age. ACM Computing Surveys, 40(2):1?60.
J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchi-
cal Image Database. In Proceedings of the IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition, pages 248?255, Miami,
Florida.
M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. 2008. The
PASCAL Visual Object Classes Challenge
2008 (VOC2008) Results. http://www.pascal-
network.org/challenges/VOC/voc2008/workshop.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
2008. LIBLINEAR: A Library for Large Linear
Classification. Journal of Machine Learning Re-
search, 9:1871?1874.
A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. 2009.
Describing Objects by their Attributes. In Proceed-
ings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, pages
1778?1785, Miami Beach, Florida.
L. Fei-Fei and P. Perona. 2005. A Bayesian Hierarchi-
cal Model for Learning Natural Scene Categories. In
Proceedings of the IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition,
pages 524?531, San Diego, California.
C. Fellbaum, editor. 1998. WordNet: an Electronic
Lexical Database. MIT Press.
Y. Feng and M. Lapata. 2008. Automatic image anno-
tation using auxiliary text information. In Proceed-
ings of ACL-08: HLT, pages 272?280, Columbus,
Ohio.
Y. Feng and M. Lapata. 2010. Visual Informa-
tion in Semantic Representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91?99, Los
Angeles, California. ACL.
V. Ferrari and A. Zisserman. 2007. Learning Visual
Attributes. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 433?440. MIT Press,
Cambridge, Massachusetts.
G. H. Golub, F. T. Luk, and M. L. Overton. 1981.
A block lanczoz method for computing the singular
values and corresponding singular vectors of a ma-
trix. ACM Transactions on Mathematical Software,
7:149?169.
P. Gorniak and D. Roy. 2004. Grounded Semantic
Composition for Visual Scenes. Journal of Artificial
Intelligence Research, 21:429?470.
T. L. Griffiths, M. Steyvers, and J. B. Tenenbaum.
2007. Topics in Semantic Representation. Psycho-
logical Review, 114(2):211?244.
D. R. Hardoon, S. R. Szedmak, and J. R. Shawe-
Taylor. 2004. Canonical Correlation Analysis: An
Overview with Application to Learning Methods.
Neural Computation, 16(12):2639?2664.
B. T. Johns and M. N. Jones. 2012. Perceptual Infer-
ence through Global Lexical Similarity. Topics in
Cognitive Science, 4(1):103?120.
D. Joshi, J.Z. Wang, and J. Li. 2006. The Story Pictur-
ing Engine?A System for Automatic Text illustra-
tion. ACM Transactions on Multimedia Computing,
Communications, and Applications, 2(1):68?89.
581
R. J. Kate and R. J. Mooney. 2007. Learning Lan-
guage Semantics from Ambiguous Supervision. In
Proceedings of the 22nd Conference on Artificial In-
telligence, pages 895?900, Vancouver, Canada.
N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Na-
yar. 2009. Attribute and Simile Classifiers for Face
Verification. In Proceedings of the IEEE 12th In-
ternational Conference on Computer Vision, pages
365?372, Kyoto, Japan.
C. H. Lampert, H. Nickisch, and S. Harmeling. 2009.
Learning To Detect Unseen Object Classes by
Between-Class Attribute Transfer. In Computer Vi-
sion and Pattern Recognition, pages 951?958, Mi-
ami Beach, Florida.
B. Landau, L. Smith, and S. Jones. 1998. Object Per-
ception and Object Naming in Early Development.
Trends in Cognitive Science, 27:19?24.
C. Leong and R. Mihalcea. 2011. Going Beyond
Text: A Hybrid Image-Text Approach for Measuring
Word Relatedness. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 1403?1407, Chiang Mai, Thailand.
J. Liu, B. Kuipers, and S. Savarese. 2011. Recognizing
Human Actions by Attributes. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 3337?3344, Colorado Springs,
Colorado.
D. G. Lowe. 1999. Object Recognition from Local
Scale-invariant Features. In Proceedings of the In-
ternational Conference on Computer Vision, pages
1150?1157, Corfu, Greece.
D. Lowe. 2004. Distinctive Image Features from
Scale-invariant Keypoints. International Journal of
Computer Vision, 60(2):91?110.
W. Lu, H. T. Ng, W.S. Lee, and L. S. Zettlemoyer.
2008. A Generative Model for Parsing Natural Lan-
guage to Meaning Representations. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 783?792, Hon-
olulu, Hawaii.
K. McRae, G. S. Cree, M. S. Seidenberg, and C. Mc-
Norgan. 2005. Semantic Feature Production Norms
for a Large Set of Living and Nonliving Things. Be-
havior Research Methods, 37(4):547?559.
D. L. Nelson, C. L. McEvoy, and T. A. Schreiber. 1998.
The University of South Florida Word Association,
Rhyme, and Word Fragment Norms.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and
A. Y. Ng. 2011. Multimodal deep learning. In
Proceedings of the 28th International Conference on
Machine Leanring, pages 689?696, Bellevue, Wash-
ington.
A. Oliva and A. Torralba. 2007. The Role of Context in
Object Recognition. Trends in Cognitive Sciences,
11(12):520?527.
D. N. Osherson, J. Stern, O. Wilkie, M. Stob, and E. E.
Smith. 1991. Default Probability. Cognitive Sci-
ence, 2(15):251?269.
G. Patterson and J. Hays. 2012. SUN Attribute
Database: Discovering, Annotating and Recogniz-
ing Scene Attributes. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 2751?2758, Providence, Rhode Island.
T. Regier. 1996. The Human Semantic Potential. MIT
Press, Cambridge, Massachusetts.
D. Roy and A. Pentland. 2002. Learning Words from
Sights and Sounds: A Computational Model. Cog-
nitive Science, 26(1):113?146.
C. Silberer and M. Lapata. 2012. Grounded Mod-
els of Semantic Representation. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433, Jeju
Island, Korea.
J. M. Siskind. 2001. Grounding the Lexical Semantics
of Verbs in Visual Perception using Force Dynamics
and Event Logic. Journal of Artificial Intelligence
Research, 15:31?90.
S. A. Sloman and L. J. Ripps. 1998. Similarity as an
Explanatory Construct. Cognition, 65:87?101.
N. Srivastava and R. Salakhutdinov. 2012. Multimodal
learning with deep boltzmann machines. In Pro-
ceedings of the 26th Annual Conference on Neural
Information Processing Systems, pages 2231?2239,
Lake Tahoe, Nevada.
M. Steyvers. 2010. Combining feature norms and
text data with topic models. Acta Psychologica,
133(3):234?342.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter,
A. Gopal Banerjee, S. Teller, and N. Roy. 2011.
Understanding Natural Language Commands for
Robotic Navigation and Manipulation. In Proceed-
ings of the 25th National Conference on Artificial
Intelligence, pages 1507?1514, San Francisco, Cali-
fornia.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceeings of the Human
Factors in Computing Systems Conference, pages
319?326, Vienna, Austria.
C. Yu and D. H. Ballard. 2007. A Unified Model of
Early Word Learning Integrating Statistical and So-
cial Cues. Neurocomputing, 70:2149?2165.
M. D. Zeigenfuse and M. D. Lee. 2010. Finding the
Features that Represent Stimuli. Acta Psychologi-
cal, 133(3):283?295.
J. M. Zelle and R. J. Mooney. 1996. Learning to Parse
Database Queries Using Inductive Logic Program-
ming. In Proceedings of the 13th National Con-
ference on Artificial Intelligence, pages 1050?1055,
Portland, Oregon.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
Map Sentences to Logical Form: Structured Classi-
fication with Probabilistic Categorial Grammars. In
Proceedings of the Conference on Uncertainty in Ar-
tificial Intelligence, pages 658?666, Edinburgh, UK.
582
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721?732,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Learning Grounded Meaning Representations with Autoencoders
Carina Silberer and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
c.silberer@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we address the problem of
grounding distributional representations of
lexical meaning. We introduce a new
model which uses stacked autoencoders to
learn higher-level embeddings from tex-
tual and visual input. The two modali-
ties are encoded as vectors of attributes
and are obtained automatically from text
and images, respectively. We evaluate our
model on its ability to simulate similar-
ity judgments and concept categorization.
On both tasks, our approach outperforms
baselines and related models.
1 Introduction
Recent years have seen a surge of interest in sin-
gle word vector spaces (Turney and Pantel, 2010;
Collobert et al, 2011; Mikolov et al, 2013) and
their successful use in many natural language ap-
plications. Examples include information retrieval
(Manning et al, 2008), search query expansions
(Jones et al, 2006), document classification (Se-
bastiani, 2002), and question answering (Yih et al,
2013). Vector spaces have been also popular in
cognitive science figuring prominently in simula-
tions of human behavior involving semantic prim-
ing, deep dyslexia, text comprehension, synonym
selection, and similarity judgments (see Griffiths
et al, 2007). In general, these models specify
mechanisms for constructing semantic representa-
tions from text corpora based on the distributional
hypothesis (Harris, 1970): words that appear in
similar linguistic contexts are likely to have related
meanings.
Word meaning, however, is also tied to the
physical world. Words are grounded in the exter-
nal environment and relate to sensorimotor experi-
ence (Regier, 1996; Landau et al, 1998; Barsalou,
2008). To account for this, new types of perceptu-
ally grounded distributional models have emerged.
These models learn the meaning of words based
on textual and perceptual input. The latter is ap-
proximated by feature norms elicited from humans
(Andrews et al, 2009; Steyvers, 2010; Silberer
and Lapata, 2012), visual information extracted
automatically from images, (Feng and Lapata,
2010; Bruni et al, 2012a; Silberer et al, 2013)
or a combination of both (Roller and Schulte im
Walde, 2013). Despite differences in formulation,
most existing models conceptualize the problem
of meaning representation as one of learning from
multiple views corresponding to different modali-
ties. These models still represent words as vectors
resulting from the combination of representations
with different statistical properties that do not nec-
essarily have a natural correspondence (e.g., text
and images).
In this work, we introduce a model, illus-
trated in Figure 1, which learns grounded mean-
ing representations by mapping words and im-
ages into a common embedding space. Our model
uses stacked autoencoders (Bengio et al, 2007)
to induce semantic representations integrating vi-
sual and textual information. The literature de-
scribes several successful approaches to multi-
modal learning using different variants of deep
networks (Ngiam et al, 2011; Srivastava and
Salakhutdinov, 2012) and data sources including
text, images, audio, and video. Unlike most pre-
vious work, our model is defined at a finer level
of granularity ? it computes meaning representa-
tions for individual words and is unique in its use
of attributes as a means of representing the textual
and visual modalities. We follow Silberer et al
(2013) in arguing that an attribute-centric repre-
sentation is expedient for several reasons.
Firstly, attributes provide a natural way of ex-
pressing salient properties of word meaning as
demonstrated in norming studies (e.g., McRae
et al, 2005) where humans often employ attributes
when asked to describe a concept. Secondly, from
721
a modeling perspective, attributes allow for eas-
ier integration of different modalities, since these
are rendered in the same medium, namely, lan-
guage. Thirdly, attributes are well-suited to de-
scribing visual phenomena (e.g., objects, scenes,
actions). They allow to generalize to new in-
stances for which there are no training exam-
ples available and to transcend category and task
boundaries whilst offering a generic description of
visual data (Farhadi et al, 2009).
Our model learns multimodal representations
from attributes which are automatically inferred
from text and images. We evaluate the embed-
dings it produces on two tasks, namely word sim-
ilarity and categorization. In the first task, model
estimates of word similarity (e.g., gem?jewel are
similar but glass?magician are not) are compared
against elicited similarity ratings. We performed
a large-scale evaluation on a new dataset consist-
ing of human similarity judgments for 7,576 word
pairs. Unlike previous efforts such as the widely
used WordSim353 collection (Finkelstein et al,
2002), our dataset contains ratings for visual and
textual similarity, thus allowing to study the two
modalities (and their contribution to meaning rep-
resentation) together and in isolation. We also
assess whether the learnt representations are ap-
propriate for categorization, i.e., grouping a set
of objects into meaningful semantic categories
(e.g., peach and apple are members of FRUIT,
whereas chair and table are FURNITURE). On both
tasks, our model outperforms baselines and related
models.
2 Related Work
The presented model has connections to several
lines of work in NLP, computer vision research,
and more generally multimodal learning. We re-
view related work in these areas below.
Grounded Semantic Spaces Grounded seman-
tic spaces are essentially distributional models
augmented with perceptual information. A model
akin to Latent Semantic Analysis (Landauer and
Dumais, 1997) is proposed in Bruni et al (2012b)
who concatenate two independently constructed
textual and visual spaces and subsequently project
them onto a lower-dimensional space using Singu-
lar Value Decomposition.
Several other models have been extensions of
Latent Dirichlet Allocation (Blei et al, 2003)
where topic distributions are learned from words
and other perceptual units. Feng and Lapata
(2010) use visual words which they extract from a
corpus of multimodal documents (i.e., BBC news
articles and their associated images), whereas oth-
ers (Steyvers, 2010; Andrews et al, 2009; Silberer
and Lapata, 2012) use feature norms obtained in
longitudinal elicitation studies (see McRae et al
(2005) for an example) as an approximation of the
visual environment. More recently, topic mod-
els which combine both feature norms and vi-
sual words have also been introduced (Roller and
Schulte im Walde, 2013). Drawing inspiration
from the successful application of attribute clas-
sifiers in object recognition, Silberer et al (2013)
show that automatically predicted visual attributes
act as substitutes for feature norms without any
critical information loss.
The visual and textual modalities on which our
model is trained are decoupled in that they are not
derived from the same corpus (we would expect
co-occurring images and text to correlate to some
extent) but unified in their representation by natu-
ral language attributes. The use of stacked autoen-
coders to extract a shared lexical meaning repre-
sentation is new to our knowledge, although, as
we explain below related to a large body of work
on deep learning.
Multimodal Deep Learning Our work employs
deep learning (a.k.a deep networks) to project lin-
guistic and visual information onto a unified rep-
resentation that fuses the two modalities together.
The goal of deep learning is to learn multiple lev-
els of representations through a hierarchy of net-
work architectures, where higher-level representa-
tions are expected to help define higher-level con-
cepts.
A large body of work has focused on projecting
words and images into a common space using a va-
riety of deep learning methods ranging from deep
and restricted Boltzman machines (Srivastava and
Salakhutdinov, 2012; Feng et al, 2013), to au-
toencoders (Wu et al, 2013), and recursive neural
networks (Socher et al, 2013b). Similar methods
have been employed to combine other modalities
such as speech and video (Ngiam et al, 2011) or
images (Huang and Kingsbury, 2013). Although
our model is conceptually similar to these studies
(especially those applying stacked autoencoders),
it differs considerably from them in at least two
aspects. Firstly, most of these approaches aim to
learn a shared representation between modalities
722
so as to infer some missing modality from others
(e.g., to infer text from images and vice versa); in
contrast, we aim to learn an optimal representa-
tion for each modality and their optimal combi-
nation. Secondly, our problem setting is different
from the former studies, which usually deal with
classification tasks and fine-tune the deep neural
networks using training data with explicit class la-
bels; in contrast we fine-tune our autoencoders us-
ing a semi-supervised criterion. That is, we use
indirect supervision in the form of object classifi-
cation in addition to the objective of reconstruct-
ing the attribute-centric input representation.
3 Autoencoders for Grounded Semantics
3.1 Background
Our model learns higher-level meaning represen-
tations for single words from textual and visual
input in a joint fashion. We first briefly review
autoencoders in Section 3.1 with emphasis on as-
pects relevant to our model which we then de-
scribe in Section 3.2.
Autoencoders An autoencoder is an unsuper-
vised neural network which is trained to recon-
struct a given input from its latent representation
(Bengio, 2009). It consists of an encoder f
?
which
maps an input vector x
(i)
to a latent representa-
tion y
(i)
= f
?
(x
(i)
) = s(Wx
(i)
+ b), with s being
a non-linear activation function, such as a sig-
moid function. A decoder g
?
?
then aims to recon-
struct input x
(i)
from y
(i)
, i.e., ?x
(i)
= g
?
?
(y
(i)
) =
s(W
?
y
(i)
+ b
?
). The training objective is the de-
termination of parameters
?
? = {W,b} and
?
?
?
=
{W
?
,b
?
} that minimize the average reconstruction
error over a set of input vectors {x
(1)
, ...,x
(n)
}:
?
?,
?
?
?
= argmin
?,?
?
1
n
n
?
i=1
L(x
(i)
,g
?
?
( f
?
(x
(i)
))), (1)
where L is a loss function, such as cross-entropy.
Parameters ? and ?
?
can be optimized by gradient
descent methods.
Autoencoders are a means to learn representa-
tions of some input by retaining useful features in
the encoding phase which help to reconstruct the
input, whilst discarding useless or noisy ones. To
this end, different strategies have been employed
to guide parameter learning and constrain the hid-
den representation. Examples include imposing
a bottleneck to produce an under-complete rep-
resentation of the input, using sparse representa-
tions, or denoising.
Denoising Autoencoders The training criterion
with denoising autoencoders is the reconstruction
of clean input x
(i)
given a corrupted version ?x
(i)
(Vincent et al, 2010). The underlying idea is that
the learned latent representation is good if the au-
toencoder is capable of reconstructing the actual
input from its corruption. The reconstruction error
for an input x
(i)
with loss function L then is:
L(x
(i)
,g
?
?
( f
?
(?x
(i)
))) (2)
One possible corruption process is masking noise,
where the corrupted version ?x
(i)
results from ran-
domly setting a fraction v of x
(i)
to 0.
Stacked Autoencoders Several (denoising) au-
toencoders can be used as building blocks to form
a deep neural network (Bengio et al, 2007; Vin-
cent et al, 2010). For that purpose, the autoen-
coders are pre-trained layer by layer, with the cur-
rent layer being fed the latent representation of the
previous autoencoder as input. Using this unsuper-
vised pre-training procedure, initial parameters are
found which approximate a good solution. Subse-
quently, the original input layer and hidden repre-
sentations of all the autoencoders are stacked and
all network parameters are fine-tuned with back-
propagation.
To further optimize the parameters of the net-
work, a supervised criterion can be imposed on top
of the last hidden layer such as the minimization
of a prediction error on a supervised task (Bengio,
2009). Another approach is to unfold the stacked
autoencoders and fine-tune them with respect to
the minimization of the global reconstruction error
(Hinton and Salakhutdinov, 2006). Alternatively,
a semi-supervised criterion can be used (Ranzato
and Szummer, 2008; Socher et al, 2011) through
combination of the unsupervised training criterion
(global reconstruction) with a supervised criterion
(prediction of some target given the latent repre-
sentation).
3.2 Semantic Representations
To learn meaning representations of single words
from textual and visual input, we employ stacked
(denoising) autoencoders (SAEs). Both input
modalities are vector-based representations of
words, or, more precisely, the objects they refer to
(e.g., canary, trolley). The vector dimensions cor-
respond to textual and visual attributes, examples
of which are shown in Table 1. We explain how
these representations are obtained in more detail
723
...
...
...
input x
TEXT
W
(1)
W
(3)
...
...
...
IMAGES
W
(2)
W
(4)
...
bimodal coding ?y
W
(5
?
)
W
(5)
...
softmax
?
t
W
(6)
...
...
W
(3
?
)
...
...
W
(4
?
)
...
reconstruction
?
x
W
(1
?
)
...
W
(2
?
)
Figure 1: Stacked autoencoder trained with semi-supervised objective. Input to the model are single-
word vector representations obtained from text and images. Vector dimensions correspond to textual and
visual attributes, respectively (see Table 1).
in Section 4.1. We first train SAEs with two hid-
den layers (codings) for each modality separately.
Then, we join these two SAEs by feeding their re-
spective second coding simultaneously to another
autoencoder, whose hidden layer thus yields the
fused meaning representation. Finally, we stack
all layers and unfold them in order to fine-tune
the SAE. Figure 1 illustrates the model.
Unimodal Autoencoders For both modalities,
we use the hyperbolic tangent function as activa-
tion function for encoder f
?
and decoder g
?
?
and an
entropic loss function for L. The weights of each
autoencoder are tied, i.e., W
?
= W
T
. We employ
denoising autoencoders (DAEs) for pre-training
the textual modality. Regarding the visual autoen-
coder, we derive a new (?denoised?) target vector
to be reconstructed for each input vector x
(i)
, and
treat x
(i)
itself as corrupted input. The unimodal
autoencoder is thus trained to denoise a given in-
put. The target vector is derived as follows: each
object o in our data is represented by multiple im-
ages, and each image is in turn represented by a
visual attribute vector x
(i)
. The target vector is the
sum of x
(i)
and the centroid x
(j)
of the remaining
attribute vectors representing object o.
Bimodal Autoencoder The bimodal autoen-
coder is fed with the concatenated final hidden
codings of the visual and textual modalities as in-
put and maps these inputs to a joint hidden layer ?y
with B units. We normalize both unimodal input
codings to unit length. Again, we use tied weights
for the bimodal autoencoder. We also encourage
the autoencoder to detect dependencies between
the two modalities while learning the mapping
to the bimodal hidden layer. We therefore apply
masking noise to one modality with a masking fac-
tor v (see Section 3.1), so that the corrupted modal-
ity optimally has to rely on the other modality in
order to reconstruct its missing input features.
Stacked Bimodal Autoencoder We finally
build a stacked bimodal autoencoder (SAE) with
all pre-trained layers and fine-tune them with re-
spect to a semi-supervised criterion. That is, we
unfold the stacked autoencoder and furthermore
add a softmax output layer on top of the bimodal
layer ?y that outputs predictions
?
t with respect to
the inputs? object labels (e.g., boat):
?
t
(i)
=
exp(W
(6)
?y
(i)
+b
(6)
)
?
O
k=1
exp(W
(6)
k.
?y
(i)
+b
(6)
k
)
, (3)
with weights W
(6)
? R
O?B
, b
(6)
? R
O?1
, where O
is the number of unique object labels. The over-
all objective to be minimized is therefore the
weighted sum of the reconstruction error L
r
and
the classification error L
c
:
L =
1
n
n
?
i=1
(
?
r
L
r
(x
(i)
, ?x
(i)
)+?
c
L
c
(t
(i)
,
?
t
(i)
)
)
+?R (4)
where ?
r
and ?
c
are weighting parameters that
give different importance to the partial objectives,
724
eats seeds has beak has claws has handlebar has wheels has wings is yellow made of wood
canary 0.05 0.24 0.15 0.00 ?0.10 0.19 0.34 0.00
V
i
s
u
a
l
trolley 0.00 0.00 0.00 0.30 0.32 0.00 0.00 0.25
bird:n breed:v cage:n chirp:v fly:v track:n ride:v run:v rail:n wheel:n
canary 0.16 0.19 0.39 0.13 0.13 0.00 0.00 0.00 0.00 ?0.05
T
e
x
t
u
a
l
trolley ?0.40 0.00 0.00 0.00 0.00 0.14 0.16 0.33 0.17 0.20
Table 1: Examples of attribute-based representations provided as input to our autoencoders.
L
c
and L
r
are entropic loss functions, and R is
a regularization term with R =
?
5
j=1
2||W
(j)
||
2
+
||W
(6)
||
2
. Finally,
?
t
(i)
is the object label vector pre-
dicted by the softmax layer for input vector x
(i)
,
and t
(i)
is the correct object label, represented as a
O-dimensional one-hot vector
1
.
The additional supervised criterion drives the
learning towards a representation capable of dis-
criminating between different objects. Further-
more, the semi-supervised setting affords flexibil-
ity, allowing to adapt the architecture to specific
tasks. For example, by setting the corruption pa-
rameter v for the textual modality to one and ?
r
to zero, a standard object classification model for
images can be trained. Setting v close to one for ei-
ther modality enables the model to infer the other
(missing) modality. As our input consists of nat-
ural language attributes, the model would infer
textual attributes given visual attributes and vice
versa.
4 Experimental Setup
In this section we present our experimental setup
for assessing the performance of our model. We
give details on the tasks and datasets used for eval-
uation, we explain how the textual and visual in-
puts were constructed, how the SAE model was
trained, and describe the approaches used for com-
parison with our own work.
4.1 Data
We learn meaning representations for the nouns
contained in McRae et al?s (2005) feature norms.
These are 541 concrete animate and inanimate ob-
jects (e.g., animals, clothing, vehicles, utensils,
fruits, and vegetables). The norms were elicited
by asking participants to list properties (e.g., barks,
an animal, has legs) describing the nouns they were
presented with.
1
In a one-hot vector, the element corresponding to the ob-
ject label is one and the others are zero.
As shown in Figure 1, our model takes as in-
put two (real-valued) vectors representing the vi-
sual and textual modalities. Vector dimensions
correspond to textual and visual attributes, respec-
tively. Textual attributes were extracted by run-
ning Strudel (Baroni et al, 2010) on a 2009 dump
of the English Wikipedia.
2
Strudel is a fully
automatic method for extracting weighted word-
attribute pairs (e.g., bat?species:n, bat?bite:v) from
a lemmatized and POS-tagged corpus. Weights
are log-likelihood ratio scores expressing how
strongly an attribute and a word are associated. We
only retained the ten highest scored attributes for
each target word. This returned a total of 2,362
dimensions for the textual vectors. Association
scores were scaled to the [?1,1] range.
To obtain visual vectors, we followed the
methodology put forward in Silberer et al (2013).
Specifically, we used an updated version of their
dataset to train SVM-based attribute classifiers
that predict visual attributes for images (Farhadi
et al, 2009). The dataset is a taxonomy of 636 vi-
sual attributes (e.g., has wings, made of wood) and
nearly 700K images from ImageNet (Deng et al,
2009) describing more than 500 of McRae et al?s
(2005) nouns. The classifiers perform reason-
ably well with an interpolated average precision
of 0.52. We only considered attributes assigned
to at least two nouns in the dataset, obtaining a
414 dimensional vector for each noun. Analo-
gously to the textual representations, visual vec-
tors were scaled to the [?1,1] range.
We follow Silberer et al?s (2013) partition of the
dataset into training, validation, and test set and
acquire visual vectors for each of the sets. We use
the visual vectors of the training and development
set for training the autoencoders, and the vectors
for the test set for evaluation.
2
The corpus is downloadable from http://wacky.
sslmit.unibo.it/doku.php?id=corpora.
725
4.2 Model Architecture
Model parameters were optimized on a subset of
the word association norms collected by Nelson
et al (1998).
3
These were established by present-
ing participants with a cue word (e.g., canary) and
asking them to name an associate word in response
(e.g., bird, sing, yellow). For each cue, the norms
provide a set of associates and the frequencies
with which they were named. The dataset con-
tains a very large number of cue-associate pairs
(63,619 in total) some of which luckily are cov-
ered in McRae et al (2005).
4
During training
we used correlation analysis (Spearman?s ?) to
monitor the degree of linear relationship between
model cue-associate (cosine) similarities and hu-
man probabilities.
The best autoencoder on the word association
task obtained a correlation coefficient of 0.33.
This performance is superior to the results re-
ported in Silberer et al (2013) (their correlation
coefficients range from 0.16 to 0.28). This model
has the following architecture: the textual autoen-
coder (see Figure 1, left-hand side) consists of 700
hidden units which are then mapped to the sec-
ond hidden layer with 500 units (the corruption
parameter was set to v = 0.1); the visual autoen-
coder (see Figure 1, right-hand side) has 170 and
100 hidden units, in the first and second layer, re-
spectively. The 500 textual and 100 visual hidden
units were fed to a bimodal autoencoder contain-
ing 500 latent units, and masking noise was ap-
plied to the textual modality with v = 0.2. The
weighting parameters for the joint training objec-
tive of the stacked autoencoder were set to ?
r
= 0.8
and ?
c
= 1 (see Equation (4)).
We used the model described above and the
meaning representations obtained from the out-
put of the bimodal latent layer for all the eval-
uation tasks detailed below. Some performance
gains could be expected if parameter optimization
took place separately for each task. However, we
wanted to avoid overfitting, and show that our pa-
rameters are robust across tasks and datasets.
4.3 Evaluation Tasks
Word Similarity We first evaluated how well
our model predicts word similarity ratings. Al-
though several relevant datasets exist, such as
3
http://w3.usf.edu/Freeassociation.
4
435 word pairs constitute the overlap between Nelson et
al.?s norms (1998) and McRae et al?s (2005) nouns.
the widely used WordSim353 (Finkelstein et al,
2002) or the more recent Rel-122 norms (Szum-
lanski et al, 2013), they contain many abstract
words, (e.g., love?sex or arrest?detention) which
are not covered in McRae et al (2005). This is for
a good reason, as most abstract words do not have
discernible attributes, or at least attributes that par-
ticipants would agree upon. We thus created a
new dataset consisting exclusively of McRae et al
(2005) nouns which we hope will be useful for the
development and evaluation of grounded semantic
space models.
5
Initially, we created all possible pairings over
McRae et al?s (2005) nouns and computed their
semantic relatedness using Patwardhan and Peder-
sen (2006)?s WordNet-based measure. We opted
for this specific measure as it achieves high corre-
lation with human ratings and has a high coverage
on our nouns. Next, for each word we randomly
selected 30 pairs under the assumption that they
are representative of the full variation of semantic
similarity. This resulted in 7,576 word pairs for
which we obtained similarity ratings using Ama-
zon Mechanical Turk (AMT). Participants were
asked to rate a pair on two dimensions, visual
and semantic similarity using a Likert scale of 1
(highly dissimilar) to 5 (highly similar). Each task
consisted of 32 pairs covering examples of weak
to very strong semantic relatedness. Two con-
trol pairs from Miller and Charles (1991) were in-
cluded in each task to potentially help identify and
eliminate data from participants who assigned ran-
dom scores. Examples of the stimuli and mean
ratings are shown in Table 2.
The elicitation study comprised overall 255
tasks, each task was completed by five volun-
teers. The similarity data was post-processed so
as to identify and remove outliers. We consid-
ered an outlier to be any individual whose mean
pairwise correlation fell outside two standard de-
viations from the mean correlation. 11.5% of
the annotations were detected as outliers and re-
moved. After outlier removal, we further ex-
amined how well the participants agreed in their
similarity judgments. We measured inter-subject
agreement as the average pairwise correlation co-
efficient (Spearman?s ?) between the ratings of all
annotators for each task. For semantic similarity,
the mean correlation was 0.76 (Min =0.34, Max
5
Available from http://homepages.inf.ed.ac.uk/
mlap/index.php?page=resources.
726
Word Pairs Semantic Visual
football?pillow 1.0 1.2
dagger?pencil 1.0 2.2
motorcycle?wheel 2.4 1.8
orange?pumpkin 2.5 3.0
cherry?pineapple 3.6 1.2
pickle?zucchini 3.6 4.0
canary?owl 4.0 2.4
jeans?sweater 4.5 2.2
pan?pot 4.7 4.0
hornet?wasp 4.8 4.8
airplane?jet 5.0 5.0
Table 2: Mean semantic and visual similarity rat-
ings for the McRae et al (2005) nouns using a
scale of 1 (highly dissimilar) to 5 (highly similar).
=0.97, StD =0.11) and for visual similarity 0.63
(Min =0.19, Max =0.90, SD =0.14). These re-
sults indicate that the participants found the task
relatively straightforward and produced similarity
ratings with a reasonable level of consistency. For
comparison, Patwardhan and Pedersen?s (2006)
measure achieved a coefficient of 0.56 on the
dataset for semantic similarity and 0.48 for vi-
sual similarity. The correlation between the aver-
age ratings of the AMT annotators and the Miller
and Charles (1991) dataset was ? = 0.91. In our
experiments (see Section 5), we correlate model-
based cosine similarities with mean similarity rat-
ings (again using Spearman?s ?).
Categorization The task of categorization
(i.e., grouping objects into meaningful categories)
is a classic problem in the field of cognitive
science, central to perception, learning, and the
use of language. We evaluated model output
against a gold standard set of categories created
by Fountain and Lapata (2010). The dataset
contains a classification, produced by human
participants, of McRae et al?s (2005) nouns into
(possibly multiple) semantic categories (40 in
total).
6
To obtain a clustering of nouns, we used Chi-
nese Whispers (Biemann, 2006), a randomized
graph-clustering algorithm. In the categorization
setting, Chinese Whispers (CW) produces a hard
clustering over a weighted graph whose nodes cor-
6
The dataset can be downloaded from http:
//homepages.inf.ed.ac.uk/s0897549/data/.
respond to words and edges to cosine similarity
scores between vectors representing their mean-
ing. CW is a non-parametric model, it induces the
number of clusters (i.e., categories) from the data
as well as which nouns belong to these clusters.
In our experiments, we initialized Chinese Whis-
pers with different graphs resulting from different
vector-based representations of the McRae et al
(2005) nouns. We also transformed the dataset
into hard categorizations by assigning each noun
to its most typical category as extrapolated from
human typicality ratings (for details see Foun-
tain and Lapata, 2010). CW can optionally ap-
ply a minimum weight threshold which we opti-
mized using the categorization dataset from Ba-
roni et al (2010). The latter contains a classifica-
tion of 82 McRae et al (2005) nouns into 10 cate-
gories. These nouns were excluded from the gold
standard (Fountain and Lapata, 2010) in our final
evaluation.
We evaluated the clusters produced by CW us-
ing the F-score measure introduced in the Se-
mEval 2007 task (Agirre and Soroa, 2007); it is
the harmonic mean of precision and recall defined
as the number of correct members of a cluster di-
vided by the number of items in the cluster and
the number of items in the gold-standard class, re-
spectively.
4.4 Comparison with Other Models
Throughout our experiments we compare a bi-
modal stacked autoencoder against unimodal au-
toencoders based solely on textual and visual in-
put (left- and right-hand sides in Figure 1, respec-
tively). We also compare our model against two
approaches that differ in their fusion mechanisms.
The first one is based on kernelized canonical cor-
relation (kCCA, Hardoon et al, 2004) with a lin-
ear kernel which was the best performing model
in Silberer et al (2013). The second one emulates
Bruni et al?s (2014) fusion mechanism. Specifi-
cally, we concatenate the textual and visual vec-
tors and project them onto a lower dimensional la-
tent space using SVD (Golub and Reinsch, 1970).
All these models run on the same datasets/items
and are given input identical to our model, namely
attribute-based textual and visual representations.
We furthermore report results obtained with
Bruni et al?s (2014) bimodal distributional model,
which employs SVD to integrate co-occurrence-
based textual representations with visual repre-
727
Semantic Visual
Models T V T+V T V T+V
McRae 0.71 0.49 0.68 0.58 0.52 0.62
Attributes 0.58 0.61 0.68 0.46 0.56 0.58
SAE 0.65 0.60 0.70 0.52 0.60 0.64
SVD ? ? 0.67 ? ? 0.57
kCCA ? ? 0.57 ? ? 0.55
Bruni ? ? 0.52 ? ? 0.46
RNN-640 0.41 ? ? 0.34 ? ?
Table 3: Correlation of model predictions against
similarity ratings for McRae et al (2005) noun
pairs (using Spearman?s ?).
sentations constructed from low-level image fea-
tures. In their model, the textual modality is
represented by the 30K-dimensional vectors ex-
tracted from UKWaC and WaCkypedia.
7
The
visual modality is represented by bag-of-visual-
words histograms built on the basis of clustered
SIFT features (Lowe, 2004). We rebuilt their
model on the ESP image dataset (von Ahn and
Dabbish, 2004) using Bruni et al?s (2013) publicly
available system.
Finally, we also compare to the word embed-
dings obtained using Mikolov et al?s (2011) re-
current neural network based language model.
These were pre-trained on Broadcast news data
(400M words) using the word2vec tool.
8
We re-
port results with the 640-dimensional embeddings
as they performed best.
5 Results
Table 3 presents our results on the word simi-
larity task. We report correlation coefficients of
model predictions against similarity ratings. As an
indicator to how well automatically extracted at-
tributes can approach the performance of clean hu-
man generated attributes, we also report results of
a distributional model induced from McRae et al?s
(2005) norms (see the row labeled McRae in the
table). Each noun is represented as a vector with
dimensions corresponding to attributes elicited by
participants of the norming study. Vector compo-
nents are set to the (normalized) frequency with
which participants generated the corresponding at-
tribute. We show results for three models, using all
attributes except those classified as visual (T), only
7
We thank Elia Bruni for providing us with their data.
8
Available from http://www.rnnlm.org/.
# Pair # Pair
1 pliers?tongs 11 cello?violin
2 cathedral?church 12 cottage?house
3 cathedral?chapel 13 horse?pony
4 pistol?revolver 14 gun?rifle
5 chapel?church 15 cedar?oak
6 airplane?helicopter 16 bull?ox
7 dagger?sword 17 dress?gown
8 pistol?rifle 18 bolts?screws
9 cloak?robe 19 salmon?trout
10 nylons?trousers 20 oven?stove
Table 4: Word pairs with highest semantic and vi-
sual similarity according to SAE model. Pairs are
ranked from highest to lowest similarity.
visual attributes (V), and all available attributes
(V+T).
9
As baselines, we also report the perfor-
mance of a model based solely on textual attributes
(which we obtain from Strudel), visual attributes
(obtained from our classifiers), and their concate-
nation (see row Attributes in Table 3, and columns
T, V, and T+V, respectively). The automatically
obtained textual and visual attribute vectors serve
as input to SVD, kCCA, and our stacked autoen-
coder (SAE). The third row in the table presents
three variants of our model trained on textual and
visual attributes only (T and V, respectively) and
on both modalities jointly (T+V).
Recall that participants were asked to provide
ratings on two dimensions, namely semantic and
visual similarity. We would expect the textual
modality to be more dominant when modeling se-
mantic similarity and conversely the perceptual
modality to be stronger with respect to visual sim-
ilarity. This is borne out in our unimodal SAEs.
The textual SAE correlates better with seman-
tic similarity judgments (? = 0.65) than its vi-
sual equivalent (? = 0.60). And the visual SAE
correlates better with visual similarity judgments
(? = 0.60) compared to the textual SAE (? = 0.52).
Interestingly, the bimodal SAE is better than the
unimodal variants on both types of similarity judg-
ments, semantic and visual. This suggests that
both modalities contribute complementary infor-
mation and that the SAE model is able to extract
a shared representation which improves general-
ization performance across tasks by learning them
9
Classification of attributes into categories is provided by
McRae et al (2005) in their dataset.
728
Models T V T+V
McRae 0.52 0.31 0.42
Attributes 0.35 0.37 0.33
SAE 0.36 0.35 0.43
SVD ? ? 0.39
kCCA ? ? 0.37
Bruni ? ? 0.34
RNN-640 0.32 ? ?
Table 5: F-score results on concept categorization.
jointly. The bimodal autoencoder (SAE, T+V)
outperforms all other bimodal models on both sim-
ilarity tasks. It yields a correlation coefficient
of ? = 0.70 on semantic similarity and ? = 0.64 on
visual similarity. Human agreement on the former
task is 0.76 and 0.63 on the latter. Table 4 shows
examples of word pairs with highest semantic and
visual similarity according to the SAE model.
We also observe that simply concatenating
textual and visual attributes (Attributes, T+V)
performs competitively with SVD and better
than kCCA. This indicates that the attribute-based
representation is a powerful predictor on its own.
Interestingly, both Bruni et al (2013) and Mikolov
et al (2011) which do not make use of attributes
are out-performed by all other attribute-based sys-
tems (see columns T and T+V in Table 3).
Our results on the categorization task are given
in Table 5. In this task, simple concatenation of vi-
sual and textual attributes does not yield improved
performance over the individual modalities (see
row Attributes in Table 5). In contrast, all bimodal
models (SVD, kCCA, and SAE) are better than
their unimodal equivalents and RNN-640. The
SAE outperforms both kCCA and SVD by a large
margin delivering clustering performance similar
to the McRae et al?s (2005) norms. Table 6 shows
examples of clusters produced by Chinese Whis-
pers when using vector representations provided
by the SAE model.
In sum, our experiments show that the bi-
modal SAE model delivers superior performance
across the board when compared against competi-
tive baselines and related models. It is interesting
to note that the unimodal SAEs are in most cases
better than the raw textual or visual attributes.
This indicates that higher level embeddings may
be beneficial to NLP tasks in general, not only to
those requiring multimodal information.
STICK-LIKE UTENSILS baton, ladle, peg, spatula,
spoon
RELIGIOUS BUILDINGS cathedral, chapel, church
WIND INSTRUMENTS clarinet, flute, saxophone, trom-
bone, trumpet, tuba
AXES axe, hatchet, machete, toma-
hawk
FURNITURE W/ LEGS bed, bench, chair, couch, desk,
rocker, sofa, stool, table
FURNITURE W/O LEGS bookcase, bureau, cabinet,
closet, cupboard, dishwasher,
dresser
LIGHTINGS candle, chandelier, lamp,
lantern
ENTRY POINTS door, elevator, gate
UNGULATES bison, buffalo, bull, calf, camel,
cow, donkey, elephant, goat,
horse, lamb, ox, pig, pony,
sheep
BIRDS crow, dove, eagle, falcon, hawk,
ostrich, owl, penguin, pigeon,
raven, stork, vulture, wood-
pecker
Table 6: Examples of clusters produced by CW
using the representations obtained from the SAE
model.
6 Conclusions
In this paper, we presented a model that uses
stacked autoencoders to learn grounded meaning
representations by simultaneously combining tex-
tual and visual modalities. The two modalities are
encoded as vectors of natural language attributes
and are obtained automatically from decoupled
text and image data. To the best of our knowl-
edge, our model is novel in its use of attribute-
based input in a deep neural network. Experimen-
tal results in two tasks, namely simulation of word
similarity and word categorization, show that our
model outperforms competitive baselines and re-
lated models trained on the same attribute-based
input. Our evaluation also reveals that the bimodal
models are superior to their unimodal counterparts
and that higher-level unimodal representations are
better than the raw input. In the future, we would
like to apply our model to other tasks, such as im-
age and text retrieval (Hodosh et al, 2013; Socher
et al, 2013b), zero-shot learning (Socher et al,
2013a), and word learning (Yu and Ballard, 2007).
Acknowledgment We would like to thank Vit-
torio Ferrari, Iain Murray and members of the
ILCC at the School of Informatics for their valu-
able feedback. We acknowledge the support of
EPSRC through project grant EP/I037415/1.
729
References
Agirre, Eneko and Aitor Soroa. 2007. SemEval-
2007 Task 02: Evaluating Word Sense Induc-
tion and Discrimination Systems. In Proceed-
ings of the Fourth International Workshop on
Semantic Evaluations. Prague, Czech Republic,
pages 7?12.
Andrews, M., G. Vigliocco, and D. Vinson. 2009.
Integrating Experiential and Distributional Data
to Learn Semantic Representations. Psycholog-
ical Review 116(3):463?498.
Baroni, M., B. Murphy, E. Barbu, and M. Poe-
sio. 2010. Strudel: A Corpus-Based Semantic
Model Based on Properties and Types. Cogni-
tive Science 34(2):222?254.
Barsalou, Lawrence W. 2008. Grounded Cogni-
tion. Annual Review of Psychology 59:617?845.
Bengio, Y., P. Lamblin, D. Popovici, and
H. Larochelle. 2007. Greedy Layer-Wise Train-
ing of Deep Networks. In Bernhard Sch?olkopf,
John Platt, and Thomas Hoffman, editors, Ad-
vances in Neural Information Processing Sys-
tems 19. MIT Press, pages 153?160.
Bengio, Yoshua. 2009. Learning Deep Architec-
tures for AI. Foundations and Trends in Ma-
chine Learning 2(1):1?127.
Biemann, Chris. 2006. Chinese Whispers ? an Ef-
ficient Graph Clustering Algorithm and its Ap-
plication to Natural Language Processing Prob-
lems. In Proceedings of TextGraphs: the 1st
Workshop on Graph Based Methods for Natu-
ral Language Processing. New York, NY, pages
73?80.
Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003.
Latent Dirichlet Allocation. Journal of Machine
Learning Research 3:993?1022.
Bruni, E., G. Boleda, M. Baroni, and N. Tran.
2012a. Distributional Semantics in Technicolor.
In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics.
Jeju Island, Korea, pages 136?145.
Bruni, E., U. Bordignon, A. Liska, J. Uijlings, and
I. Sergienya. 2013. Vsem: An open library for
visual semantics representation. In Proceedings
of the 51st Annual Meeting of the Association
for Computational Linguistics: System Demon-
strations. Sofia, Bulgaria, pages 187?192.
Bruni, E., N. Tran, and M. Baroni. 2014. Multi-
modal distributional semantics. J. Artif. Intell.
Res. (JAIR) 49:1?47.
Bruni, E., J. Uijlings, M. Baroni, and N. Sebe.
2012b. Distributional Semantics with Eyes: Us-
ing Image Analysis to Improve Computational
Representations of Word Meaning. In Proceed-
ings of the 20th ACM International Conference
on Multimedia. Nara, Japan, pages 1219?1228.
Collobert, R., J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural
Language Processing (almost) from Scratch.
Journal of Machine Learning Research
12:2493?2537.
Deng, J., W. Dong, R. Socher, L. Li, K. Li, and
L. Fei-Fei. 2009. ImageNet: A Large-Scale
Hierarchical Image Database. In Proceedings
of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition. Mi-
ami, Florida, pages 248?255.
Farhadi, A., I. Endres, D. Hoiem, and D. Forsyth.
2009. Describing Objects by their Attributes.
In Proceedings of the IEEE Computer Soci-
ety Conference on Computer Vision and Pat-
tern Recognition. Miami Beach, Florida, pages
1778?1785.
Feng, Fangxiang, Ruifan Li, and Xiaojie Wang.
2013. Constructing Hierarchical Image-tags Bi-
modal Representations for Word Tags Alter-
native Choice. In Proceedings of the ICML
2013 Workshop on Challenges in Representa-
tion Learning. Atlanta, Georgia.
Feng, Yansong and Mirella Lapata. 2010. Visual
Information in Semantic Representation. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of
the Association for Computational Linguistics.
Los Angeles, California, pages 91?99.
Finkelstein, L., E. Gabrilovich, Y. Matias,
E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.
2002. Placing Search in Context: The Concept
Revisited. ACM Transactions on Information
Systems 20(1):116?131.
Fountain, Trevor and Mirella Lapata. 2010. Mean-
ing Representation in Natural Language Cat-
egorization. In Proceedings of the 31st An-
nual Conference of the Cognitive Science Soci-
ety. Amsterdam, The Netherlands, pages 1916?
1921.
730
Golub, Gene and Christian Reinsch. 1970. Sin-
gular Value Decomposition and Least Squares
Solutions. Numerische Mathematik 14(5):403?
420.
Griffiths, T. L., M. Steyvers, and J. B. Tenenbaum.
2007. Topics in Semantic Representation. Psy-
chological Review 114(2):211?244.
Hardoon, D. R., S. R. Szedmak, and J. R. Shawe-
Taylor. 2004. Canonical Correlation Analy-
sis: An Overview with Application to Learning
Methods. Neural Computation 16(12):2639?
2664.
Harris, Zellig. 1970. Distributional Structure. In
Papers in Structural and Transformational Lin-
guistics, pages 775?794.
Hinton, Geoffrey E. and Ruslan R. Salakhutdinov.
2006. Reducing the Dimensionality of Data
with Neural Networks. Science 313(5786):504?
507.
Hodosh, Micah, Peter Young, and Julia Hocken-
maier. 2013. Framing Image Description as a
Ranking Task: Data, Models and Evaluation
Metrics. Journal of Artificial Intelligence Re-
search 47:853?899.
Huang, Jing and Brian Kingsbury. 2013. Audio-
visual Deep Learning for Noise Robust Speech
Recognition. In Proceedings of the 38th Inter-
national Conference on Acoustics, Speech, and
Signal Processing. Vancouver, Canada, pages
7596?7599.
Jones, R., B. Rey, O. Madani, and W. Greiner.
2006. Generating Query Substititions. In Pro-
ceedings of the 15th International Conference
on the World-Wide Web. Edinburgh, Scotland,
pages 387?396.
Landau, B., L. Smith, and S. Jones. 1998. Object
Perception and Object Naming in Early Devel-
opment. Trends in Cognitive Science 27:19?24.
Landauer, Thomas and Susan T. Dumais. 1997. A
Solution to Plato?s Problem: the Latent Seman-
tic Analysis Theory of Acquisition, Induction,
and Representation of Knowledge. Psychologi-
cal Review 104(2):211?240.
Lowe, D. 2004. Distinctive Image Features from
Scale-invariant Keypoints. International Jour-
nal of Computer Vision 60(2):91?110.
Manning, C. D., P. Raghavan, and H. Sch?utze.
2008. Introduction to Information Retrieval.
Cambridge University Press, New York, NY.
McRae, K., G. S. Cree, M. S. Seidenberg, and
C. McNorgan. 2005. Semantic Feature Pro-
duction Norms for a Large Set of Living and
Nonliving Things. Behavior Research Methods
37(4):547?559.
Mikolov, T., S. Kombrink, L. Burget, J.
?
Cernock?y,
and S. Khudanpur. 2011. Extensions of Recur-
rent Neural Network Language Model. In Pro-
ceedings of the 2011 IEEE International Con-
ference on Acoustics, Speech, and Signal Pro-
cessing. Prague, Czech Republic, pages 5528?
5531.
Mikolov, T., Wen-tau Yih, and G. Zweig. 2013.
Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the
2013 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies. At-
lanta, Georgia, pages 746?751.
Miller, George A. and Walter G. Charles. 1991.
Contextual Correlates of Semantic Similarity.
Language and Cognitive Processes 6(1).
Nelson, D. L., C. L. McEvoy, and T. A.
Schreiber. 1998. The University of South
Florida Word Association, Rhyme, and Word
Fragment Norms.
Ngiam, Jiquan, Aditya Khosla, Mingyu Kim,
Juhan Nam, Honglak Lee, and Andrew Ng.
2011. Multimodal Deep Learning. In Pro-
ceedings of the 28th International Conference
on Machine Learning. Bellevue, Washington,
pages 689?696.
Patwardhan, Siddharth and Ted Pedersen. 2006.
Using WordNet-based Context Vectors to Es-
timate the Semantic Relatedness of Concepts.
In Proceedings of the EACL 2006 Workshop
on Making Sense of Sense: Bringing Compu-
tational Linguistics and Psycholinguistics To-
gether. Trento, Italy, pages 1?8.
Ranzato, Marc?Aurelio and Martin Szummer.
2008. Semi-supervised Learning of Com-
pact Document Representations with Deep Net-
works. In Proceedings of the 25th International
Conference on Machine Learning. Helsinki,
Finland, pages 792?799.
Regier, Terry. 1996. The Human Semantic Poten-
tial. MIT Press, Cambridge, Massachusetts.
Roller, Stephen and Sabine Schulte im Walde.
2013. A Multimodal LDA Model integrating
731
Textual, Cognitive and Visual Modalities. In
Proceedings of the 2013 Conference on Empir-
ical Methods in Natural Language Processing.
Seattle, Washington, pages 1146?1157.
Sebastiani, Fabrizio. 2002. Machine Learning in
Automated Text Categorization. ACM Comput-
ing Surveys 34:1?47.
Silberer, C., V. Ferrari, and M. Lapata. 2013. Mod-
els of Semantic Representation with Visual At-
tributes. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics. Sofia, Bulgaria, pages 572?582.
Silberer, Carina and Mirella Lapata. 2012.
Grounded Models of Semantic Representation.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning. Jeju Island, Korea, pages 1423?1433.
Socher, R., M. Ganjoo, C. D. Manning, and A. Y.
Ng. 2013a. Zero-shot learning through cross-
modal transfer. In Advances in Neural Informa-
tion Processing Systems 26, pages 935?943.
Socher, R., Quoc V. Le, C. D. Manning, and A. Y.
Ng. 2013b. Grounded Compositional Seman-
tics for Finding and Describing Images with
Sentences. In Proceedings of the NIPS Deep
Learning Workshop.
Socher, R., J. Pennington, E. H. Huang, A. Y. Ng,
and C. D. Manning. 2011. Semi-Supervised Re-
cursive Autoencoders for Predicting Sentiment
Distributions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Edinburgh, Scotland, pages
151?161.
Srivastava, Nitish and Ruslan Salakhutdinov.
2012. Multimodal Learning with Deep Boltz-
mann Machines. In Advances in Neural In-
formation Processing Systems 25, pages 2231?
2239.
Steyvers, Mark. 2010. Combining Feature Norms
and Text Data with Topic Models. Acta Psycho-
logica 133(3):234?342.
Szumlanski, S. R., F. Gomez, and V. K. Sims.
2013. A New Set of Norms for Semantic Re-
latedness Measures. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics. Sofia, Bulgaria, pages 890?
895.
Turney, Peter D. and Patrick Pantel. 2010. From
Frequency to Meaning: Vector Space Models
of Semantics. Journal of Artificial Intelligence
Research 37(1):141?188.
Vincent, P., H. Larochelle, I. Lajoie, Y. Bengio,
and P. Manzagol. 2010. Stacked Denoising Au-
toencoders: Learning Useful Representations in
a Deep Network with a Local Denoising Cri-
terion. Journal of Machine Learning Research
11:3371?3408.
von Ahn, Luis and Laura Dabbish. 2004. Labeling
Images with a Computer Game. In Proceedings
of the SIGCHI Conference on Human Factors
in Computing Systems. Vienna, Austria, pages
319?326.
Wu, Pengcheng, Steven C. H. Hoi, Hao Xia, Peilin
Zhao, Dayong Wang, and Chunyan Miao. 2013.
Online Multimodal Deep Similarity Learning
with Application to Image Retrieval. In Pro-
ceedings of the 21st ACM International Con-
ference on Multimedia. Barcelona, Spain, pages
153?162.
Yih, Wen-tau, Ming-Wei Chang, Christopher
Meek, and Andrzej Pastusiak. 2013. Question
Answering Using Enhanced Lexical Semantic
Models. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics. Sofia, Bulgaria, pages 1744?1753.
Yu, C. and D. H. Ballard. 2007. A Unified Model
of Early Word Learning Integrating Statistical
and Social Cues. Neurocomputing 70:2149?
2165.
732
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 134?137,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
UHD: Cross-Lingual Word Sense Disambiguation Using
Multilingual Co-occurrence Graphs
Carina Silberer and Simone Paolo Ponzetto
Department of Computational Linguistics
Heidelberg University
{silberer,ponzetto}@cl.uni-heidelberg.de
Abstract
We describe the University of Heidelberg
(UHD) system for the Cross-Lingual Word
Sense Disambiguation SemEval-2010 task
(CL-WSD). The system performs CL-
WSD by applying graph algorithms pre-
viously developed for monolingual Word
Sense Disambiguation to multilingual co-
occurrence graphs. UHD has participated
in the BEST and out-of-five (OOF) eval-
uations and ranked among the most com-
petitive systems for this task, thus indicat-
ing that graph-based approaches represent
a powerful alternative for this task.
1 Introduction
This paper describes a graph-based system for
Cross-Lingual Word Sense Disambiguation, i.e.
the task of disambiguating a word in context by
providing its most appropriate translations in dif-
ferent languages (Lefever and Hoste, 2010, CL-
WSD henceforth). Our goal at SemEval-2010 was
to assess whether graph-based approaches, which
have been successfully developed for monolingual
Word Sense Disambiguation, represent a valid
framework for CL-WSD. These typically trans-
form a knowledge resource such as WordNet (Fell-
baum, 1998) into a graph and apply graph algo-
rithms to perform WSD. In our work, we follow
this line of research and apply graph-based meth-
ods to multilingual co-occurrence graphs which
are automatically created from parallel corpora.
2 Related Work
Our method is heavily inspired by previous pro-
posals from V?eronis (2004, Hyperlex) and Agirre
et al (2006). Hyperlex performs graph-based
WSD based on co-occurrence graphs: given a
monolingual corpus, for each target word a graph
is built where nodes represent content words co-
occurring with the target word in context, and
edges connect the words which co-occur in these
contexts. The second step iteratively selects the
node with highest degree in the graph (root hub)
and removes it along with its adjacent nodes. Each
such selection corresponds to isolating a high-
density component of the graph, in order to select
a sense of the target word. In the last step the root
hubs are linked to the target word and the Mini-
mum Spanning Tree (MST) of the graph is com-
puted to disambiguate the target word in context.
Agirre et al (2006) compare Hyperlex with an al-
ternative method to detect the root hubs based on
PageRank (Brin and Page, 1998). PageRank has
the advantage of requiring less parameters than
Hyperlex, whereas the authors ascertain equal per-
formance of the two methods.
3 Graph-based Cross-Lingual WSD
We start by building for each target word a mul-
tilingual co-occurrence graph based on the target
word?s aligned contexts found in parallel corpora
(Sections 3.1 and 3.2). Multilingual nodes are
linked by translation edges, labeled with the target
word?s translations observed in the corresponding
contexts. We then use an adapted PageRank al-
gorithm to select the nodes which represent the
target word?s different senses (Section 3.3) and,
given these nodes, we compute the MST, which
is used to select the most relevant words in con-
text to disambiguate a given test instance (Section
3.4). Translations are finally given by the incom-
ing translation edges of the selected context words.
134
3.1 Monolingual Graph
Let C
s
be all contexts of a target word w in
a source language s, i.e. English in our case,
within a (PoS-tagged and lemmatized) monolin-
gual corpus. We first construct a monolingual co-
occurrence graph G
s
= ?V
s
, E
s
?. We collect all
pairs (cw
i
, cw
j
) of co-occurring nouns or adjec-
tives in C
s
(excluding the target word itself) and
add each word as a node into the initially empty
graph. Each co-occurring word pair is connected
with an edge (v
i
, v
j
) ? E
s
, which is assigned a
weight w(v
i
, v
j
) based on the strength of associa-
tion between the respective words cw
i
and cw
j
:
w(v
i
, v
j
) = 1?max [p(cw
i
|cw
j
), p(cw
j
|cw
i
)].
The conditional probability of word cw
i
given
word cw
j
is estimated by the number of contexts
in which cw
i
and cw
j
co-occur divided by the
number of contexts containing cw
j
.
3.2 Multilingual Graph
Given a set of target languages L, we then ex-
tend G
s
to a labeled multilingual graph G
ML
=
?V
ML
, E
ML
? where:
1. V
ML
= V
s
?
?
l?L
V
l
is a set of nodes represent-
ing content words from either the source (V
s
) or
the target (V
l
) languages;
2. E
ML
= E
s
?
?
l?L
{E
l
? E
s,l
} is a set of
edges. These include (a) co-occurrence edges
E
l
? V
l
?V
l
between nodes representing words
in a target language (V
l
), weighted in the same
way as the edges in the monolingual graph;
(b) labeled translation edges E
s,l
which repre-
sent translations of words from the source lan-
guage into a target language. These edges are
assigned a complex label t ? T
w,l
compris-
ing a translation of the word w in the target
language l and its frequency of translation, i.e.
E
s,l
? V
s
? T
w,l
? V
l
.
The multilingual graph is built based on a word-
aligned multilingual parallel corpus and a multi-
lingual dictionary. The pseudocode is presented in
Algorithm 1. We start with the monolingual graph
from the source language (line 1) and then for each
target language l ? L in turn, we add the transla-
tion edges (v
s
, t, v
l
) ? E
s,l
of each word in the
source language (lines 5-15). In order to include
the information about the translations of w in the
different target languages, each translation edge
Algorithm 1 Multilingual co-occurrence graph.
Input: target word w and its contexts C
s
monolingual graph G
s
= ?V
s
, E
s
?
set of target languages L
Output: a multilingual graph G
ML
1: G
ML
= ?V
ML
, E
ML
? ? G
s
= ?V
s
, E
s
?
2: for each l ? L
3: V
l
? ?
4: C
l
:= aligned sentences of C
s
in lang. l
5: for each v
s
? V
s
6: T
v
s
,l
:= translations of v
s
found in C
l
7: C
v
s
? C
s
:= contexts containing w and v
s
8: for each translation v
l
? T
v
s
,l
9: C
v
l
:= aligned sentences of C
v
s
in lang. l
10: T
w,C
v
l
? translation labels of w from C
v
l
11: if v
l
/? V
ML
then
12: V
ML
? V
ML
? v
l
13: V
l
? V
l
? v
l
14: for each t ? T
w,C
v
l
15: E
ML
? E
ML
? (v
s
, t, v
l
)
16: for each v
i
? V
l
17: for each v
j
? V
l
, i 6= j
18: if v
i
and v
j
co-occur in C
l
then
19: E
ML
? E
ML
? (v
i
, v
j
)
20: return G
ML
(v
s
, t, v
l
) receives a translation label t. Formally,
let C
v
s
? C
s
be the contexts where v
s
and w co-
occur, and C
v
l
the word-aligned contexts in lan-
guage l of C
v
s
, where v
s
is translated as v
l
. Then
each edge between nodes v
s
and v
l
is labeled with
a translation label t (lines 14-15): this includes a
translation of w in C
v
l
, its frequency of transla-
tion and the information of whether the transla-
tion is monosemous, as found in a multilingual
dictionary, i.e. EuroWordNet (Vossen, 1998) and
PanDictionary (Mausam et al, 2009). Finally, the
multilingual graph is further extended by inserting
all possible co-occurrence edges (v
i
, v
j
) ? E
l
be-
tween the nodes for the target language l (lines 16-
19, i.e. we apply the step from Section 3.1 to l and
C
l
). As a result of the algorithm, the multilingual
graph is returned (line 20).
3.3 Computing Root Hubs
We compute the root hubs in the multilingual
graph to discriminate the senses of the target word
in the source language. Hubs are found using the
adapted PageRank from Agirre et al (2006):
135
PR(v
i
) = (1? d) + d
?
j?deg(v
i
)
w
ij
?
k?deg(v
j
)
w
jk
PR(v
j
)
where d is the so-called damping factor (typically
set to 0.85), deg(v
i
) is the number of adjacent
nodes of node v
i
and w
ij
is the weight of the co-
occurrence edge between nodes v
i
and v
j
.
Since this step aims to induce the senses for
the target word, only nodes referring to words
in English can become root hubs. However, in
order to use additional evidence from other lan-
guages, we furthermore include in the computa-
tion of PageRank co-occurrence edges from the
target languages, as long as these occur in con-
texts with ?safe?, i.e. monosemous, translations of
the target word. Given an English co-occurrence
edge (v
s,i
, v
s,j
) and translation edges (v
s,i
, v
l,i
)
and (v
s,j
, v
l,j
) to nodes in the target language
l, labeled with monosemous translations, we in-
clude the co-occurrence edge (v
l,i
, v
l,j
) in the
PageRank computation. For instance, animal and
biotechnology are translated in German as Tier
and Biotechnologie, both with edges labeled with
the monosemous Pflanze: accordingly, we in-
clude the edge (Tier,Biotechnologie) in the com-
putation of PR(v
i
), where v
i
is either animal or
biotechnology.
Finally, following V?eronis (2004), a MST is
built with the target word as its root and the root
hubs of G
ML
forming its first level. By using a
multilingual graph, we are able to obtain MSTs
which contain translation nodes and edges.
3.4 Multilingual Disambiguation
Given a context W for the target word w in the
source language, we use the MST to find the most
relevant words in W for disambiguating w. We
first map each content word cw ? W to nodes
in the MST. Since each word is dominated by ex-
actly one hub, we can find the relevant nodes by
computing the correct hub disHub (i.e. sense) and
then only retain those nodes linked to disHub. Let
W
h
be the set of mapped content words dominated
by hub h. Then, disHub can be found as:
disHub = argmax
h
?
cw?W
h
d(cw)
dist(cw, h) + 1
where d(cw) is a function which assigns a weight
to cw according to its distance to w, i.e. the more
words occur between w and cw within W , the
smaller the weight, and dist(cw, h) is given by
the number of edges between cw and h in the
MST. Finally, we collect the translation edges of
the retained context nodes W
disHub
and we sum
the translation counts to rank each translation.
4 Results and Analysis
Experimental Setting. We submitted two runs
for the task (UHD-1 and UHD-2 henceforth).
Since we were interested in assessing the impact
of using different resources with our methodology,
we automatically built multilingual graphs from
different sentence-aligned corpora, i.e. Europarl
(Koehn, 2005) for UHD-1, augmented with the
JRC-Acquis corpus (Steinberger et al, 2006) for
UHD-2
1
. Both corpora were tagged and lemma-
tized with TreeTagger (Schmid, 1994) and word
aligned using GIZA++ (Och and Ney, 2003). For
German, in order to avoid the sparseness deriving
from the high productivity of compounds, we per-
formed a morphological analysis using Morphisto
(Zielinski et al, 2009).
To build the multilingual graph (Section 3.2),
we used a minimum frequency threshold of 2 oc-
currences for a word to be inserted as a node,
and retained only those edges with a weight less
or equal to 0.7. After constructing the multilin-
gual graph, we additionally removed those trans-
lations with a frequency count lower than 10 (7
in the case of German, due to the large amount
of compounds). Finally, the translations gener-
ated for the BEST evaluation setting were ob-
tained by applying the following rule onto the
ranked answer translations: add translation tr
i
while count(tr
i
) ? count(tr
i?1
)/3, where i is
the i-th ranked translation.
Results and discussion. The results for the
BEST and out-of-five (OOF) evaluations are pre-
sented in Tables 1 and 2 respectively. Results are
computed using the official scorer (Lefever and
Hoste, 2010) and no post-processing is applied to
the system?s output, i.e. we do not back-off to the
baseline most frequent translation in case the sys-
tem fails to provide an answer for a test instance.
For the sake of brevity, we present the results for
UHD-1, since we found no statistically significant
difference in the performance of the two systems
(e.g. UHD-2 outperforms UHD-1 only by +0.7%
on the BEST evaluation for French).
1
As in the case of Europarl, only 1-to-1-aligned sentences
were extracted.
136
Language P R Mode P Mode R
FRENCH 20.22 16.21 17.59 14.56
GERMAN 12.20 9.32 11.05 7.78
ITALIAN 15.94 12.78 12.34 8.48
SPANISH 20.48 16.33 28.48 22.19
Table 1: BEST results (UHD-1).
Language P R Mode P Mode R
FRENCH 39.06 32.00 37.00 26.79
GERMAN 27.62 22.82 25.68 21.16
ITALIAN 33.72 27.49 27.54 21.81
SPANISH 38.78 31.81 40.68 32.38
Table 2: OOF results (UHD-1).
Overall, in the BEST evaluation our system
ranked in the middle for those languages where
the majority of systems participated ? i.e. sec-
ond and fourth out of 7 submissions for FRENCH
and SPANISH. When compared against the base-
line, i.e. the most frequent translation found in
Europarl, our method was able to achieve in the
BEST evaluation a higher precision for ITALIAN
and SPANISH (+1.9% and +2.1%, respectively),
whereas FRENCH and GERMAN lie near below the
baseline scores (?0.5% and?1.0%, respectively).
The trade-off is a recall always below the base-
line. In contrast, we beat the Mode precision base-
line for all languages, i.e. up to +5.1% for SPAN-
ISH. The fact that our system is strongly precision-
oriented is additionally proved by a low perfor-
mance in the OOF evaluation, where we always
perform below the baseline (i.e. the five most fre-
quent translations in Europarl).
5 Conclusions
We presented in this paper a graph-based system
to perform CL-WSD. Key to our approach is the
use of a co-occurrence graph built from multilin-
gual parallel corpora, and the application of well-
studied graph algorithms for monolingual WSD
(V?eronis, 2004; Agirre et al, 2006). Future work
will concentrate on extensions of the algorithms,
e.g. computing hubs in each language indepen-
dently and combining them as a joint problem, as
well as developing robust techniques for unsuper-
vised tuning of the graph weights, given the obser-
vation that the most frequent translations tend to
receive too much weight and accordingly crowd
out more appropriate translations. Finally, we
plan to investigate the application of our approach
directly to multilingual lexical resources such as
PanDictionary (Mausam et al, 2009) and Babel-
Net (Navigli and Ponzetto, 2010).
References
Eneko Agirre, David Mart??nez, Oier L?opez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art WSD. In Proc. of EMNLP-06,
pages 585?593.
Sergey Brin and Lawrence Page. 1998. The
anatomy of a large-scale hypertextual web search
engine. Computer Networks and ISDN Systems,
30(1?7):107?117.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit X.
Els Lefever and Veronique Hoste. 2010. SemEval-
2010 Task 3: Cross-lingual Word Sense Disam-
biguation. In Proc. of SemEval-2010.
Mausam, Stephen Soderland, Oren Etzioni, Daniel
Weld, Michael Skinner, and Jeff Bilmes. 2009.
Compiling a massive, multilingual dictionary via
probabilistic inference. In Proc. of ACL-IJCNLP-
09, pages 262?270.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a very large multilingual seman-
tic network. In Proc. of ACL-10.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing (NeMLaP ?94), pages 44?49.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma?z Erjavec, Dan Tufis?, and
D?aniel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proc. of LREC ?06.
Jean V?eronis. 2004. Hyperlex: lexical cartography
for information retrieval. Computer Speech & Lan-
guage, 18(3):223?252.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Kluwer, Dordrecht, The Netherlands.
Andrea Zielinski, Christian Simon, and Tilman Wittl.
2009. Morphisto: Service-oriented open source
morphology for German. In State of the Art in Com-
putational Morphology, volume 41 of Communica-
tions in Computer and Information Science, pages
64?75. Springer.
137
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 1?10,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Casting Implicit Role Linking as an Anaphora Resolution Task
Carina Silberer?
School of Informatics
University of Edinburgh
Edinburgh, UK
c.silberer@ed.ac.uk
Anette Frank
Department of Computational Linguistics
Heidelberg University
Heidelberg, Germany
frank@cl.uni-heidelberg.de
Abstract
Linking implicit semantic roles is a challeng-
ing problem in discourse processing. Unlike
prior work inspired by SRL, we cast this prob-
lem as an anaphora resolution task and embed
it in an entity-based coreference resolution
(CR) architecture. Our experiments clearly
show that CR-oriented features yield strongest
performance exceeding a strong baseline. We
address the problem of data sparsity by apply-
ing heuristic labeling techniques, guided by
the anaphoric nature of the phenomenon. We
achieve performance beyond state-of-the art.
1 Introduction
A widespread phenomenon that is still poorly stud-
ied in NLP is the meaning contribution of unfilled
semantic roles of predicates in discourse interpreta-
tion. Such roles, while linguistically unexpressed,
can often be anaphorically bound to antecedent ref-
erents in the discourse context. Capturing such im-
plicit semantic roles and linking them to their an-
tecedents is a challenging problem. But it bears im-
mense potential for establishing discourse coherence
and for getting closer to the aim of true NLU.
Linking of implicit semantic roles in discourse
has recently been introduced as a shared task in
the SemEval 2010 competition Linking Events and
Their Participants in Discourse (Ruppenhofer et al,
2009, 2010). The task consists in detecting un-
filled semantic roles of events and determining an-
tecedents in the discourse context that these roles
? The work reported in this paper is based on a Master?s
Thesis conducted at Heidelberg University (Silberer, 2011).
can be understood to refer to. In (1), e.g., the pred-
icate jealousy introduces two implicit roles, one for
the experiencer, the other for the object of jealousy
involved. These roles can be bound to Watson and
the speaker (I) in the non-local preceding context.
(1) Watson won?t allow that I know anything of art but
that is mere jealousy because our views upon the
subject differ.
(2) IReader was sitting reading in the chairPlace.
In contrast to implicit roles that can be discourse-
bound to an antecedent as in (1), roles can be inter-
preted existentially, as in (2), with an unfilled TEXT
role of the READING frame that cannot be anchored
in prior discourse. The FrameNet paradigm (Fill-
more et al, 2003) that was used for annotation in
the SemEval task classifies these interpretation dif-
ferences as definite (DNI) vs. indefinite (INI) null
instantiations (NI) of roles, respectively.
2 Implicit Role Reference: A Short History
Early studies. The phenomenon of implicit role re-
ference is not new. It has been studied in a number
of early approaches. Palmer et al (1986) treated un-
filled semantic roles as special cases of anaphora and
coreference resolution (CR). Resolution was guided
by domain knowledge encoded in a knowledge-
based system. Similarly, Whittemore et al (1991)
analyzed the resolution of unexpressed event roles
as a special case of CR. A formalization in DRT was
fully worked out, but automation was not addressed.
Later studies emphasize the role of implicit role
reference in a frame-semantic discourse analysis.
Fillmore and Baker (2001) provide an analysis of
1
a newspaper text that indicates the importance of
frames and roles in establishing discourse coher-
ence. Burchardt et al (2005) offer a formalization
of the involved factors: the interplay of frames and
frame relations with factors of contextual contigu-
ity. The work includes no automation, but suggests a
corpus-based approach using antecedent-role coref-
erence patterns collected from corpora.
Tetreault (2002), finally, offers an automated anal-
ysis for resolving implicit role reference. The small-
scale study is embedded in a rule-based CR setup.
SemEval 2010 Task 10: Linking Roles. Trig-
gered by the SemEval 2010 competition (Ruppen-
hofer et al, 2010), research on resolving implicit
role reference has gained momentum again, in a field
where both semantic role labeling (SRL) and coref-
erence resolution have seen tremendous progress.
However, the systems that participated in the NI-
only task on implicit role resolution achieved mod-
erate success in the initial subtasks: (i) recog-
nition of implicit roles and (ii) classification as
discourse-bound vs. existential interpretation (DNI
vs. INI). Yet, (iii) identification of role antecedents
was bluntly unsuccessful, with around 1% F-score.
Ruppenhofer et al clearly relate the task to
coreference resolution. The participating systems,
though, framed the task as a special case of SRL.
Chen et al (2010) participated with their SRL sys-
tem SEMAFOR (Das et al, 2010). They cast the task
as one of extended SRL, by admitting constituents
from a larger context. To overcome the lack and
sparsity of syntactic path features, they include lex-
ical association and similarity scores for semantic
roles and role fillers; classical SRL order and dis-
tance features are adapted to larger distances.
VENSES++ by Tonelli and Delmonte (2010) is
a semantic processing system that includes lexico-
semantic processing, anaphora resolution and deep
semantic resolution components. Anaphora resolu-
tion is performed in a rule-based manner; pronom-
inals are replaced with their antecedents? lexical
information. For role linking, the system applies
diverse heuristics including search for predicate-
argument structures with compatible arguments, as
well as semantic relatedness scores between poten-
tial fillers of (overt and implicit) semantic roles.
More recently Tonelli and Delmonte (2011) recur
to a leaner approach for role binding, estimating a
relevance score for potential antecedents from role
fillers observed in training. They report an F-score
of 8 points for role binding on SemEval data. How-
ever, being strongly lexicalized, their trained model
seems heavily dependent on the training data.
Ruppenhofer et al (2011) use semantic types for
identifying DNI role antecedents, reporting an error
reduction of 14% on Chen et al (2010)?s results.
The poor performance results in the SemEval task
clearly indicate the difficulty of resolving implicit
role reference. A major factor seems to relate to data
sparsity: the training set covers only 245 DNI anno-
tations linked to an antecedent.
Linking implicit arguments of nominals. Ger-
ber and Chai (2010) (G&C henceforth) investigate a
closely related task of argument binding, tied to the
linking of implicit arguments for nominal predicates
using the PropBank role labeling scheme. In con-
trast to the SemEval task, which focuses on a verbs
and nouns, their system is only applied to nouns and
is restricted to 10 predicates with substantial training
set sizes (avg: 125, median: 103).
G&C propose a discriminative model that selects
an antecedent for an implicit role from an extended
context window. The approach incorporates some
aspects relating to CR that go beyond the SRL-
oriented SemEval systems: A candidate represen-
tation includes information about all the candidates?
coreferent mentions (determined by automatic CR),
in particular their semantic roles (provided by gold
annotations) and WordNet synsets. Patterns of se-
mantic associations between filler candidates and
implicit roles are learned for all mentions contained
in the candidate?s entity chain. They achieve an F-
score of 42.3, against a baseline of 26.5.
Gerber (2011) presents an extended model that in-
corporates strategies suggested in Burchardt et al
(2005): using frame relations as well as coreference
patterns acquired from large corpora. This model
achieves an F-score of 50.3 (baseline: 28.9).
3 Casting Implicit Role Linking as an
Anaphora Resolution Task
3.1 Implicit role = anaphora resolution
Recent models for role binding mainly draw on tech-
niques from SRL, enriched with concepts from CR.
2
In this paper, we explicitly formulate implicit role
linking as an anaphora resolution task. This is in
line with the predominant conception in early work,
and also highlights the close relationship with zero
anaphora (Kameyama, 1985). Computational treat-
ments of zero anaphora (e.g., Imamura et al (2009))
are in fact employing techniques well-known from
SRL. Recent work by Iida and Poesio (2011), by
contrast, offers an analysis of zero anaphora in a
CR architecture. Further support comes from psy-
cholinguistic studies in Garrod and Terras (2000),
who establish commonalities between implicit role
reference and other types of anaphora resolution.
The contributions of our work are as follows:
i. We cast implicit role binding as a CR task, us-
ing an entity-mention model and discriminative
classification for antecedent selection.
ii. We examine the effectiveness of model features
for classical SRL vs. CR features to clarify the
nature of this special phenomenon.
iii. We automatically acquire heuristically labeled
data to address the sparse data problem.
i. An entity-mention model for anaphoric role
resolution. In our model implicit roles that are
discourse-bound (i.e. classified as DNI) are treated
as anaphoric, similar to zero anaphora: the implicit
role will be bound to a discourse antecedent.
In line with recent research in CR, we adopt an
entity-mention model, where an entity is represented
by all mentions pertaining to a coreference chain
(see i.a. Rahman and Ng (2011), Cai and Strube
(2010)). Our model is based on binary classifier de-
cisions that take as input the anaphoric role and an
entity candidate from the preceding discourse. The
final classification of a role linking to an entity is ob-
tained by discriminative ranking of the binary clas-
sifiers? probability estimates. Details on the system
architecture are given in Section 3.2.
ii. SRL vs. CR: Analysis of feature sets. The
linking of implicit semantic roles represents an inter-
esting mixture of SRL and CR that displays excep-
tional characteristics of both types of phenomena.
In contrast to classical SRL, the relation between
a predicate?s semantic role and a candidate role filler
? being realized outside the local syntactic context ?
cannot be characterized by syntactic path features.
But similar to SRL we can compute a semantic class
type expected by the role and determine which can-
didate is most appropriate to fill the semantic role.
Anaphoric binding of unfilled roles also diverges
from classical CR in that the anaphoric element is
not overtly expressed. This excludes typical CR fea-
tures that refer to overt realization, such as agree-
ment or string overlap. Again, we can make use of a
semantic characterization of role fillers to determine
the role?s most appropriate antecedent entity in the
discourse. This closely relates to semantic class fea-
tures employed in CR (e.g., Rahman and Ng (2011)).
Thus, semantic association features are important
modeling aspects, but they do not contribute to clari-
fying the nature of the phenomenon. We will include
additional properties that are considered characteris-
tic for CR, such as the semantics of an entity (as op-
posed to individual mentions), or salience properties
of antecedents (cf. Section 4.3). Thus, the model we
propose substantially differs from prior work.
We classify the features of our models as SRL vs.
CR features, plus a mixture class that relates to both
phenomena. We examine which type of features is
most effective for resolving implicit role reference.
iii. Heuristic data acquisition. In response to the
sparse data problem encountered with the SemEval
data set and the general lack of annotated resources
for implicit role binding, we experiment with tech-
niques for heuristic data acquisition. The strategy
we apply builds on our working hypothesis that im-
plicit role reference is best understood as a special
case of (zero) anaphora resolution.
We process manually annotated coreference data
sets that are jointly labeled with semantic roles.
From these we extract entity chains that contain
anaphoric pronouns that fill a predicate?s semantic
role. We artificially delete the pronoun?s role label
and transfer it to its closest antecedent in its chain.
In this way, we convert the example to an instance
that is structurally similar to one involving a locally
unfilled semantic role that is bound to an overt an-
tecedent. An example is given below: in (3.a) we
identify a pronoun that fills the SPEAKER role of the
frame STATEMENT. We transfer this role label to its
closest antecedent (3.b).
3
(3) a. Riadyk spoke in hisk 21-story office building
on the outskirts of Jakarta. [...] The timing of
hisk,Speaker statementStatement is important.
b. Riadyk spoke in hisk,Speaker 21-story office
building on the outskirts of Jakarta. [...] The tim-
ing of ? statementStatement is important.
Clearly such artificially created annotation instances
are only approximations of naturally occurring cases
of implicit role binding. But we expect to acquire
numerous data points for relevant features: semantic
class information for the antecedent entity, the pred-
icate?s frame and roles and coherence properties.
3.2 System Architecture
Our approach is embedded in an architecture for su-
pervised CR using an entity-mention model. The
main processing steps of the system include: (1) en-
tity detection, (2) instance creation with feature ex-
traction and (3) classification. As we are focusing
on the resolution of implicit DNI roles, we assume
that the text is already augmented with standard CR
information (we make use of gold data and automati-
cally assigned coreference chains). Accordingly, the
description of modules focuses exclusively on the
resolution of DNIs.
(1) Entity Detection. We first collect the entire
entity set E mentioned in the discourse. This set
forms the overall set of candidates to consider for
DNI linking. For each DNI dk to be linked, a subset
of candidates Ek ? E is chosen as candidate search
space for resolving dk. We experiment with differ-
ent strategies for constructing Ek (cf. Section 4).
(2) Instance Creation. The next step consists in
the creation of (training) instances for classification
including the extraction of features for all instances.
An instance instej ,dk consists of the active DNI
dk, its frame and a candidate entity ej ? Ek. In-
stance creation follows an entity-based adaption of
the standard procedure of Soon et al (2001), which
has been applied by Yang et al (2004, 2008). Pro-
cessing the discourse from left to right, for each DNI
dk, instances Ik are created by processing Ek from
right to left according to each entity?s most recent
mention, starting with the entity closest to dk. Note
that, as entities instead of mentions are considered,
only one instance is created for an entity which is
mentioned several times in the search space.
In training, the instance creation stops when the
correct antecedent, i.e. a positive instance, as well as
at least one negative instance have been found.1
(3) Classification. From the acquired training in-
stances we learn a binary classifier that predicts for
an instance instej ,dk whether it is positive, i.e. en-
tity ej is a correct antecedent for DNI dk. Fur-
ther, the classifier provides a probability estimate for
instej ,dk being positive. We obtain classifications
for all instances in Ik. Among the positive classified
instances, we select the antecedent e with the high-
est estimate. That is, we apply the best-first strategy
(Ng and Cardie, 2002). In case of a tie, we choose
the antecedent which is closer to the target. If no
instance is classified as positive, dk is left unfilled.
4 Data and Experiments
4.1 SEMEVAL 2010 task and data set
We adhere to the SemEval 2010 task by Ruppen-
hofer et al (2009) as test bed for our experiments.
The main focus of our work is on part (iii), the iden-
tification of antecedents for DNIs. Subtasks (i) and
(ii), the recognition and interpretation of NIs will be
only tackled to enable comparison to the participat-
ing systems of the SemEval NI-only task.
The SemEval task is based on fiction stories by
A. C. Doyle, one story as training data and another
two chapters as test set, enriched with coreference
and FrameNet-style frame annotations. Information
about the training section is found in Table 1. The
test data comprise 710 NIs (349 DNIs, 361 INIs), of
which 259 DNIs are linked.
4.2 Heuristic data acquisition
Since the training data has a critically small amount
of linked DNIs, we heuristically labeled training
data on the basis of data sets with manually anno-
tated coreference information: OntoNotes 3.0 (Hovy
et al, 2006), as well as ACE-2 (Mitchell et al, 2003)
and MUC-6 (Chinchor and Sundheim, 2003).
OntoNotes 3.0 was merged with gold SRL an-
notations from the CoNLL-2005 shared task. By
means of SemLink-1.1 (Loper et al, 2007) and a
mapping included in the SemEval data, these Prop-
Bank (PB, Palmer et al (2005)) annotations were
1We additionally impose several restrictions, e.g., a valid
candidate must not already fill another role of the active frame.
4
#ent avg avg #frames #frame#DNI #DNI
#ent/doc size types types
SemEval 141 141 9 1,370 317 245 155
ONotes 7899 23 3 12,770 258 2,220 270
ACE-2 3564 11 4 58,204 757 4,265 578
MUC-6 1841 15 3 20,140 654 997 310
corpus coref semantic roles
ONotes manual manual PB CoNLL05, ported to FN
ACE-2 manual automatic FN (Semafor)
MUC-6 manual automatic FN (Semafor)
Table 1: SemEval vs. heuristically acquired data
mapped to their FrameNet (FN) counterparts, if ex-
istent. For the ACE-2 and MUC-6 corpora, we used
Semafor (Das and Smith, 2011) for automatic anno-
tation with FN semantic roles. From these data sets
we acquired heuristically annotated instances of role
linking using the strategy explained in 3.1.
Table 1 summarizes the resulting training data.
The heuristically labeled data extends the manually
labeled DNI instances by an order of magnitude.
4.3 Model parameters
Entity sets Edni. For definition of the set of can-
didate entities to consider for DNI linking, Edni,
we determined different parameter settings with re-
strictions on the types, distances and prominence of
candidate antecedents. For instance, unlike in noun
phrase CR, antecedents for a DNI can be realized by
a wide range of constituents other than NPs, such as
prepositional (PP), adverbial (ADVP), verb phrases
(VP) and even sentences (S) referring to proposi-
tions.
These settings, stated in Table 2, were inferred by
experiments on the training data and by examining
its statistics: AllChains is motivated by the fact that
72% of the DNIs are linked to referents with non-
singleton chains. On the other hand, the majority of
DNI antecedents ? not only non-singletons, but also
phrases of a certain type or terminals that overtly
fill other roles ? are located in the current and the
two preceding sentences (69.6%), which motivates
SentWin. However, antecedents are also located far
beyond this window span which is probably due to
the nature of the SemEval texts, with prominent en-
tities being accessible over longer stretches of dis-
course. Chains+Win is designed by taking into ac-
AllChains This set contains all the entities repre-
sented by non-singleton coreference chains that
were introduced in the discourse up to the cur-
rent DNI position, assuming that this way only
more salient entities are considered.
SentWin Comprises constituents with a certain
phrase type2 or terminals that overtly fill a role,
occurring within the current or the preceding
two sentences.
Chain+Win This set comprises SentWin plus all
entities mentioned at least five times up to the
current DNI position (i.e. salient entities).
Table 2: Entity set settings Edni
count all previous observations.
Training data sets. We made use of different mix-
tures of training data: SemEval plus different exten-
sions using the heuristically acquired data summa-
rized in Table 1.
4.4 Feature sets: SRL, mixed and CR-oriented
Table 3 lists the most important features used for
training our models. Features 1-13 were used in the
best model and are ordered by their strength based
on feature ablation experiments (cf. Section 5). All
features are marked for their general type; the last
column marks features employed by G&C.3
Below we give some details for selected features.
Feat. 1: Prominence. We first compute average
prominence of an entity e (Eq. 2) by summing over
the size (= nb. of mentions) of all entities e in a win-
dow w4 of preceding sentences and dividing by the
nb. of entities E in w. Prominence of e (Eq. 1) is
set to the difference between its size in w and the
average prominence score.5 The final feature value
records the relative rank of e?s prominence score
compared to the scores of the other candidates.
prom(e, w) = #mentions(e, w)? avg prom(w) (1)
avg prom(w) =
?
e?E #mentions(e, w)
|E|
(2)
2The phrase type must be NPB, S, VP, SBAR, or SG.
3? marks features that are similar to G&C features. Note
that their only CR features are distance features.
4We set w = 2 based on experiments on the training data.
5This prominence score was proposed by Dolata (2010)
within an entity grid approach to role linking.
5
nr feature type G&C
1 prominence prominence score of the entity in the current discourse position CR -
2 pos.dist mention PoS or phrase type of the most recent explicit mention (CR) -
concatenated with sentence distance to the target
3 dist mentions minimum distance between DNI and entity in mentions CR -
4 dist sentences minimum distance between DNI and entity in sentences CR +
5 vnroles dni.entity the counterparts of the DNI in VerbNet (VN, Kipper et al (2000)) mixed +
concatenated with the VN roles the entity already instantiates
6 roles dni.entity concatenation of the DNI with the FN roles the entity already instantiates mixed ?
7 semType dni.entity semantic type of the DNI concatenated with mixed -
the semantic types of the roles the entity already instantiates
8 avgDist sentences average sentence distance between the entity and the DNI CR +
9 sp supersense agreement of the selectional preferences for the DNI mixed -
and the most frequent supersense of the entity
10 function (target) grammatical function of the target SRL -
11 wnss ent.st dni pointwise mutual information between the entity?s WN supersense ss and mixed -
the DNI?s FN semantic type st: pmi(ss, st) = log2P (ss|st)/P (ss)
12 nbRoles dni.entity like feature 5, but with NomBank arguments 0 and 1 mixed ?
13 frame.dni frame name concatenated with the DNI SRL -
Table 3: Best features used for training. Feat. 11 was computed on the FN dataset and the SemEval training data.
Feat. 9: SelPrefs. We compute selectional prefer-
ences following the information-theoretic approach
of Resnik (1993, 1996). Similar to Erk (2007), we
used an adapted version which we computed for se-
mantic roles by means of the FN database rather than
for verb argument positions. The WordNet classes
over which the preferences are defined are WordNet
lexicographer?s files (supersenses).
The selectional association values ?(dni, ss) of
the DNI?s selectional preferences are retrieved for
the supersense ss of each candidate antecedent?s
head. As for Feat. 1, we define a candidate?s fea-
ture value by its rank in the ordered list of these ?s.
4.5 Experiments
Evaluation measures. We adopt the precision (P),
recall (R) and F1 measures in Ruppenhofer et al
(2010). A true positive is a DNI which has been
linked to the correct entity as given by the gold data.
Classifiers and feature selection. For DNI link-
ing, we use BayesNet (Cooper and Herskovits,
1992) as classifier, implemented in Weka (Witten
and Frank, 2000).6 For each parameter combination,
we perform feature selection by means of leave-one-
out 10-fold cross-validation on the SemEval train-
ing data with successively removing/determining the
6We experimented with different learners and selected the
algorithm that performed best for the different subtasks.
best features. The resulting models Mi are then eval-
uated on the SemEval test data in different setups:
Exp1: Linking DNIs. Exp1 evaluates our models
on the DNI linking task proper (NI-only step (iii)).
This setting uses the gold coreference, SRL and DNI
information in the test data.
Exp2: Full NI-only. For benchmarking on the
SemEval task, we perform the complete NI-only
task. Here, the test data is only enriched w/ SRL la-
beling. Each frame f in the test corpus is processed,
involving the following steps:
(i) Recognition of NIs is performed by consulting
the FN database7 and determining the FN core roles
that are unfilled. From this NI set, roles that are
conceptually redundant or competing with f?s overt
roles are rejected as they don?t need to or must not
be linked, respectively.
(ii) For predicting the interpretation of an NI, we
use LibSVM (Chang and Lin, 2001) as classifier
which further assigns each NI a probability estimate
of the NI being definite. We use a small set of fea-
tures: the FN semantic type of the NI and a boolean
feature indicating whether the target is in passive
voice and the agent (object) not realized. Further,
we use a statistical feature which gives the relative
7We used the FrameNetAPI by Reiter (2010).
6
model add. entity frame DNI Linking (%)
data set anno. P R F1
M0 - AllChains gold 25.6 25.1 25.3
M1 ON2-10 Chains+Win proj 30.8 25.1 27.7
M1? ON2-24 AllChains proj 35.6 20.1 25.7
M1?? ON2-24 SentWin proj 23.3 22.4 22.8
M2 MUC Chains+Win auto 26.1 24.3 25.3
M3 ACE AllChains auto 24.0 21.2 22.5
Prom ? Chains+Win ? 20.5 20.5 20.5
Table 4: Exp1: Best performing models for different en-
tity and data settings. Test data contain gold CR chains.
frequency of the role?s realization as DNI and INI,
respectively, in the training data.
(iii) DNI linking is performed for each of f?s pre-
dicted DNIs Df in descending order of their prob-
ability estimates. If an antecedent em can be de-
termined for a predicted DNI, the role is labeled
as such and linked to em. As the DNI?s role has
been filled now, competing or redundant DNIs are
removed from Df before moving to the next pre-
dicted DNI. Only DNIs for which an antecedent is
found are labeled as such.
Exp2 is evaluated on both gold coreference an-
notation and automatically assigned coreference
chains, using the CR system of Cai et al (2011).
5 Evaluation and Results
5.1 Exp1: DNI linking evaluation
Table 4 shows the best performing models for DNI
linking for each parameter setting8. We compare
them to a strong baseline Prom (last row) that links
each DNI to the antecedent candidate with highest
prominence score. Its F1-score is beaten by the other
models, with a gain of 7.2 points for model M1. The
high performance of the baseline can be taken as ev-
idence that salience factors are crucial for this task.
The best performing model M1 (27.7 F1) uses
about a fifth of the ON data with Chains+Win. When
using SentWin as entity set, F1 drops to 18.5 (not
shown). The best performing model using SentWin
(M1??) performs 4.9 points below M1. Hence, re-
liance on the Chains+Win set seems beneficial. Per-
formance of the AllChains setting varies over the
8We consider the 3 types of entity sets and different train-
ing setups ? additional data (Section 4.3); additional data with
gold, projected or automatic frame annotations. The ON data
was also evaluated with roughly a fifth of ON to evaluate the
effect of different amounts of data of the same type of data.
Features P ( %) R (%) F1 (%)
all 30.8 25.1 27.7
- 1-4,8 (CR) 21.6 8.1 11.8
- 10,13 (SRL) 31.0 25.9 28.2
- 5-7,9,11-12 (mixed) 20.6 20.5 20.5
Table 5: Results of ablation study.
different data sets: the strongest model is M0 with-
out additional data. An explanation could be the dif-
ferent data domains (story vs. news), leading to a
different nature (length and number) of the entities.
In general, the models seem to profit from heuris-
tically labeled training data. We note strong gains
(up to 10 pts) in precision for 3 of these 5 best mod-
els, compared to M0. Finally, we observe higher
performance when using additional data with gold/
projected semantic frame annotations (M1, M1?).
Analysis of the best model. Table 5 states the re-
sults for M1 when leaving out one of the feature
types at a time. The serious drop of F1 from 27.7%
to 11.8% when omitting CR features clearly demon-
strates that this feature type has by far the greatest
impact on the task performance. Rejection of the
mixed features decreases F1 to a score equal to the
prominence baseline, whereas leaving out the SRL-
features even slightly increases F1. The weakness of
Feature 13 could still be attributed to data sparsity.
5.2 Exp2: Full NI-only evaluation
Table 6 lists the results for the full NI-only task ob-
tained with the presented models with different addi-
tional training data sets (lines 2-5). When perform-
ing all three steps, the F1-score of the best model
M1 drops to 10.1% (-17.6 pts, col. 10) under us-
age of automatic coreference annotations in the test
data (i.e. under the real task conditions). When us-
ing gold coreference annotations, the F1-score is
at 18.1% (col. 11), which can be seen as an upper
bound for our current models on this task. The dif-
ference of 9.6 points between only performing DNI
linking (Table 4) and the full NI-only task reflects
the fact that recognizing (step i) and interpreting
(step ii) NIs bear difficulties on their own.9
Comparison of our models with the two SemEval
9When not performing step (iii), NI recognition achieves
77.6% recall and 67% relative precision.
7
Null Instantiations (%)
model add. entity frame recogn. interpret. (precision) DNI Linking (%)
data set anno. recall relative absolute P R F1 F1(crf)
M0 - AllChains gold 58 68 40 6.0 8.9 7.1 12.5
M1 ON2-10 Chains+Win proj 56 69 38 9.2 11.2 10.1 18.1
M2 MUC Chains+Win auto 52 70 36 7.0 8.5 7.6 11.0
M3 ACE AllChains auto 56 68 38 5.9 8.1 6.8 11.3
M3? ACE Chains+Win auto 56 68 38 6.9 9.7 8.0 9.5
SEMAFOR ? 63 55 35 1.40
VENSES++ ? 8 64 5 1.21
T&D ? 54 75 40 13.0 6.0 8
Table 6: Exp2 results obtained for our models (lines 1-5) and comparable systems (lines 6-8). Column 5 gives the
score for correctly recognized NIs. Cols. 6 and 7 report precision for correctly interpreted NIs on the basis of the
correctly recognized (relative) vs. all gold NIs to be recognized (absolute). The scores in the last column (F1(crf))
were obtained with gold CR annotations.
task participants10 (lines 7-8) shows that our models
clearly outperform these systems ? with a gain of
+5.7 and +8.89 points in F1-score in DNI linking.11
Compared to Tonelli and Delmonte (2011)
(T&D), M1 has a higher F1-score in linking of
+2.1 points. In contrast to our method, their link-
ing approach is (admittedly) heavily lexicalized and
strongly tailored to the domain of the used data.
6 Conclusion
We cast the problem of linking implicit semantic
roles as a special case of (zero) anaphora resolution,
drawing on insights from earlier work and parallels
observed with zero anaphora. Our results strongly
support this analysis: (i) Feature selection clearly
determines CR-related features as strongest support
for DNI linking. (ii) Our models beat a strong base-
line using a prominence score to determine DNI ref-
erence. (iii) We devise a method for heuristically la-
beling training data that simulates implicit role refer-
ence. Using this data we obtain system performance
beyond state-of-the-art, with high gains in precision.
While these findings clearly corroborate our con-
ceptual approach, overall performance is still mea-
ger. Comparison to G&C?s setting suggests that
training data is a serious issue. We addressed the
10The F1-scores are from http://semeval2.fbk.eu/
semeval2.php?location=Rankings/ranking10.html
11Moreover, note that Ruppenhofer et al describe a weaker
evaluation, that judges DNI linkings as correct if the span of the
linked referent contains the gold referent. Further, they consider
14 linked INIs in the test data, although linking INIs conflicts
with the definition of INIs.
problem of training set size using heuristic data ac-
quisition. The nature of semantic role annotations
may be another problem, as FrameNet-style roles do
not generalize well. Finally, implicit roles pertaining
to nominalizations tend to be more local than those
pertaining to verbs12 and might be less diverse.
Our model is closer in spirit to G&C than the Se-
mEval systems, but differs by being embedded in
an entity-based CR architecture using discriminative
antecedent selection. Also, we address a more prin-
cipled issue, by exploring the nature of the task using
a qualitative feature analysis. Our system compares
favorably to related work. Benchmarking against
the SemEval participants and T&D shows clear im-
provements. Also, T&D?s model is closely tied to
domain data, while ours is enhanced with out-of-
domain data. Exact comparison to G&C needs to be
conducted on the same data set and labeling scheme.
In sum, within the chosen setting we can show
that implicit role reference is best modeled as a spe-
cial case of anaphora resolution. We observe that
models trained on cleaner data perform better than
on larger, but more noisy data sets. Thus, it is es-
sential to further enhance the quality of heuristically
labeled data. Applying the classifiers for steps (i)
and (ii) as a filter could help to better constrain the
data to the target phenomenon.
Acknowledgements. We would like to thank Mateusz
Dolata for his help with salience and coherence features,
and Michael Roth for his server support.
12This is confirmed by analysis of the SemEval vs. NomBank
corpus of G&C.
8
References
Aljoscha Burchardt, Anette Frank, and Manfred Pinkal.
2005. Building Text Meaning Representations from
Contextually Related Frames ? A Case Study. In Pro-
ceedings of the 6th International Workshop on Com-
putational Semantics, IWCS-6, pages 66?77, Tilburg,
The Netherlands.
Jie Cai and Michael Strube. 2010. End-to-end coref-
erence resolution via hypergraph partitioning. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 143?151, Beijing,
China.
Jie Cai, Eva Mu?jdricza-Maydt, and Michael Strube.
2011. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Shared
Task of 15th Conference on Computational Natural
Language Learning, pages 56?60, Portland, Oregon.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a Library for Support Vector Machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. SEMAFOR: Frame Argument
Resolution with Log-Linear Models. In Proceedings
of the 5th International Workshop on Semantic Evalu-
ation, pages 264?267, Uppsala, Sweden, July.
Nancy Chinchor and Beth Sundheim, 2003. Message
Understanding Conference (MUC) 6. Linguistic Data
Consortium, Philadelphia.
Gregory F. Cooper and Edward Herskovits. 1992. A
Bayesian Method for the Induction of Probabilistic
Networks from Data. Machine Learning, 9(4):309?
347.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised frame-semantic parsing for unknown pred-
icates. In Dekang Lin, Yuji Matsumoto, and Rada Mi-
halcea, editors, ACL, pages 1435?1444. The Associa-
tion for Computer Linguistics.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic Frame-Semantic
Parsing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
948?956, Los Angeles, California, June.
Mateusz Dolata. 2010. Extending the Entity-Grid Model
for the Processing of Implicit Roles in Discourse.
Bachelor?s thesis, Department of Computational Lin-
guistics, Heidelberg University, Germany.
Katrin Erk. 2007. A Simple, Similarity-based Model for
Selectional Preferences. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics, ACL ?07, pages 216?223, Prague, Czech
Republic, June.
Charles J. Fillmore and Collin F. Baker. 2001. Frame Se-
mantics for Text Understanding. In Proceedings of the
NAACL 2001 Workshop on WordNet and Other Lexical
Resources, Pittsburgh, June.
Charles J. Fillmore, Christopher R. Johnson, and Miriam
R. L. Petruck. 2003. Background to Framenet. Inter-
national Journal of Lexicography, 16(3):235?250.
Simon Garrod and Melody Terras. 2000. The Contribu-
tion of Lexical and Situational Knowledge to Resolv-
ing Discourse Roles: Bonding and Resolution. Jour-
nal of Memory and Language, 42(4):526?544.
Matthew Gerber and Joyce Chai. 2010. Beyond Nom-
Bank: A Study of Implicit Arguments for Nominal
Predicates. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1583?1592, Uppsala, Sweden, July.
Matthew Steven Gerber. 2011. Semantic Role Labeling
of Implicit Arguments for Nominal Predicates. Ph.D.
thesis, Michigan State University.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT-NAACL ?06, pages 57?60, New York, New
York, June.
Ryu Iida and Massimo Poesio. 2011. A Cross-Lingual
ILP Solution to Zero Anaphora Resolution. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 804?813,
Portland, Oregon.
Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009. Discriminative Approach to Predicate-
Argument Structure Analysis with Zero-Anaphora
Resolution. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing,
ACL-IJCNLP ?09, pages 85?88, Suntec, Singapore,
August.
Megumi Kameyama. 1985. Zero Anaphora: The case of
Japanese. Ph.D. thesis, Stanford University.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-Based Construction of a Verb Lexicon.
In Proceedings of the 17th National Conference
on Artificial Intelligence and 12th Conference
on Innovative Applications of Artificial Intelli-
gence, pages 691?696, Austin, Texas. AAAI Press.
http://verbs.colorado.edu/?mpalmer/
projects/verbnet.html.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining Lexical Resources: Mapping between
9
PropBank and VerbNet. In Proceedings of the 7th In-
ternational Workshop on Computational Linguistics.
Alexis Mitchell, Stephanie Strassel, Mark Przybocki,
JK Davis, George Doddington, Ralph Grishman,
Adam Meyers, Ada Brunstein, Lisa Ferro, and Beth
Sundheim, 2003. ACE-2 Version 1.0. Linguistic Data
Consortium, Philadelphia.
Vincent Ng and Claire Cardie. 2002. Improving Ma-
chine Learning Approaches to Coreference Resolu-
tion. In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics, ACL ?02,
pages 104?111, Philadelphia, Pennsylvania.
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
man, Lynette Hirschman, Marcia Linebarger, and John
Dowding. 1986. Recovering Implicit Information. In
Proceedings of the 24th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 10?19,
New York, New York, USA.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106, March.
Altaf Rahman and Vincent Ng. 2011. Narrowing the
modeling gap: A cluster-ranking approach to corefer-
ence resolution. Journal of Artificial Intelligence Re-
search, 40:469?521.
Nils Reiter. 2010. FrameNet API. http://www.cl.
uni-heidelberg.de/trac/FrameNetAPI.
Philip Resnik. 1996. Selectional Constraints: an
Information-theoretic Model and its Computational
Realization. Cognition, 61(1-2):127?159, November.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2009. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the NAACL-HLT 2009
Workshop on Semantic Evaluations: Recent Achieve-
ments and Future Directions (SEW-09), pages 106?
111, Boulder, Colorado, June.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45?50, Up-
psala, Sweden, July.
Josef Ruppenhofer, Philip Gorinski, and Caroline
Sporleder. 2011. In Search of Missing Arguments:
A Linguistic Approach. In Proceedings of the Inter-
national Conference Recent Advances in Natural Lan-
guage Processing, pages 331?338, Hissar, Bulgaria,
September.
Carina Silberer. 2011. Linking Implicit Semantic Roles
in Discourese Using Coreference Resolution Methods.
Master?s thesis, Department of Computational Lin-
guistics, Heidelberg University, Germany.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A Machine Learning Approach to Coref-
erence Resolution of Noun Phrases. Computational
Linguistics, 27:521?544, December.
Joel R. Tetreault. 2002. Implicit Role Reference. In
International Symposium on Reference Resolution for
Natural Language Processing, pages 109?115, Ali-
cante, Spain.
Sara Tonelli and Rodolfo Delmonte. 2010. VENSES++:
Adapting a Deep Semantic Processing System to the
Identification of Null Instantiations. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tions, pages 296?299, Uppsala, Sweden, July.
Sara Tonelli and Rodolfo Delmonte. 2011. Desperately
Seeking Implicit Arguments in Text. In Proceedings of
the ACL 2011 Workshop on Relational Models of Se-
mantics, pages 54?62, Portland, Oregon, USA, June.
G. Whittemore, M. Macpherson, and G. Carlson. 1991.
Event-building through role filling and anaphora reso-
lution. In Proceedings of the 29th Annual Meeting on
Association for Computational Linguistics, pages 17?
24, Morristown, NJ, USA.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann, San Fran-
cisco, CA, USA.
Xiaofeng Yang, Jian Su, Guodong Zhou, and Chew Lim
Tan. 2004. An NP-cluster Based Approach to Coref-
erence Resolution. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics,
COLING ?04, pages 226?232, Geneva, Switzerland.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An Entity-Mention Model
for Coreference Resolution with Inductive Logic Pro-
gramming. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, ACL ?08:HLT, pages
843?851, Columbus, Ohio, June.
10
