Proceedings of NAACL HLT 2009: Short Papers, pages 257?260,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Recognising the Predicate?argument Structure of Tagalog
Meladel Mistica
Australian National University ? Linguistics
The University of Melbourne ? CSSE
The University of Sydney ? Linguistics
mmistica@csse.unimelb.edu.au
Timothy Baldwin
CSSE
The University of Melbourne
tim@csse.unimelb.edu.au
Abstract
This paper describes research on parsing
Tagalog text for predicate?argument structure
(PAS). We first outline the linguistic phe-
nomenon and corpus annotation process, then
detail a series of PAS parsing experiments.
1 Introduction
Predicate?argument structure (PAS) has been
shown to be highly valuable in tasks such as infor-
mation extraction (Surdeanu et al, 2003; Miyao et
al., 2009). In this research, we develop a resource for
analysing the predicate?argument structure of Taga-
log, a free word order language native to the Philip-
pines, and carry out preliminary empirical investiga-
tion of PAS parsing methods over Tagalog.
The motivation for this research is the investiga-
tion of the interaction between information structure
and word order in Tagalog. That is, we wish to de-
termine the utility of discourse-based contextual in-
formation in predicting word order in Tagalog, in a
natural language generation context. We see PAS as
the natural representation for this exploration. This
research clearly has implications beyond our imme-
diate interests, however, in terms of resource cre-
ation for an NLP resource-poor language, and the
facilitation of research on parsing and parsing-based
applications in Tagalog. It is also one of the first in-
stances of research on PAS parsing over a genuinely
free word order language.
2 Background
Tagalog is an Austronesian language of the Malayo-
Polynesian branch, which forms the basis of the na-
tional language of the Philippines, Filipino (a.k.a.
Pilipino) (Gordon, 2005). It is a verb-initial lan-
guage, with relatively free word order of verbal
arguments (Kroeger, 1993), as exemplified in the
word-order variants provided with (1). There are
no discernible meaning differences between the pro-
vided variants, but there are various soft constraints
on free word order, as discussed by Kroeger (1993)
and Sells (2000).
(1) Nagbigay
gave
ng
GEN
libro
book
sa
DAT
babae
woman
ang
NOM
lalaki
man
?The man gave the woman a book?
Nagbigay ng libro ang lalaki sa babae
Nagbigay sa babae ng libro ang lalaki
Nagbigay sa babae ang lalaki ng libro
Nagbigay ang lalaki sa babae ng libro
Nagbigay ang lalaki ng librosa babae
In addition to these free word order possibilities,
Tagalog exhibits voice marking, a morpho-syntactic
phenomenon which is common in Austronesian lan-
guages and gives prominence to an element in a sen-
tence (Schachter and Otanes, 1972; Kroeger, 1993).
This poses considerable challenges to generation,
because of the combinatorial explosion in the pos-
sible ways of expressing what is seemingly the same
proposition. Below, we provide a brief introduction
to Tagalog syntax, with particular attention to voice
marking.
2.1 Constituency
There are three case markers in Tagalog: ang, ng
and sa, which are by convention written as separate
preposing words, as in (1). These markers normally
prepose phrasal arguments of a given verb.
The sa marker is predominantly used for goals,
recipients, locations and definite objects, while ng
marks possessors, actors, instruments and indefinite
objects (Kroeger, 1993). Ang is best explained in
terms of Tagalog?s voice-marking system.
257
2.2 Tagalog Voice Marking
Tagalog has rich verbal morphology which gives
prominence to a particular dependent via voice
marking (Schachter and Otanes, 1972); this special
dependent in the sentence is the ang-marked argu-
ment.
There are 5 major voice types in Tagalog: Ac-
tor Voice (AV); Patient/Object Voice (OV); Da-
tive/Locative Voice (DV); Instrumental Voice (IV);
and Benefactive Voice (BV) (Kroeger, 1993). This
voice marking, manifested on the verb, reflects the
semantic role of the ang-marked constituent, as seen
in the sentences below from Kroeger (1993), illus-
trating the 3 voice types of AV, OV, and BV.
(2) Actor Voice (AV)
Bumili
buy
ang
NOM
lalake
man
ng
GEN
isda
fish
sa
DAT
tindahan
store
?The man bought fish at the store?
(3) Object Voice (OV)
Binili
buy
ng
GEN
lalake
man
ang
NOM
isda
fish
sa
DAT
tindahan.
store
?The man bought fish at the store?
(4) Benefactive Voice (BV)
Ibinili
buy
ng
GEN
lalake
man
ng
GEN
isda
fish
ang
NOM
bata.
child
?The man bought fish for the child?
In each case, the morphological marking on the verb
(which indicates the voice type) is presented in bold,
along with the focused ang argument.
In addition to displaying free word order, there-
fore, Tagalog presents the further choice of which
voice to encode the proposition with.
3 Data and Resources
For this research, we annotated our own corpus of
Tagalog text for PAS. This is the first such resource
to be created for the Tagalog language. To date,
we have marked up two chapters (about 2500 to-
kens) from a narrative obtained from the Guten-
berg Project1 called Hiwaga ng Pagibig (?The Mys-
tery of Love?); we intend to expand the amount of
1http://www.gutenberg.org/etext/
18955
annotated data in the future. The annotated data
is available from www.csse.unimelb.edu.au/
research/lt/resources/tagalog-pas.
3.1 Part-of-speech Mark-up
First, we developed a set of 5 high-level part-of-
speech (POS) tags for the task, with an additional
tag for sundries such as punctuation. The tags are as
follows:
Description Example(s)
proper name names of people/cities
pronoun personal pronouns
open-class word nouns, verbs, adjectives
closed-class word conjunctions
function word case markers
other punctuation
These tags are aimed at assisting the identification
of constituent boundaries, focusing primarily on dif-
ferentiating words that have semantic content from
those that perform a grammatical function, with the
idea that function words, such as case markers, gen-
erally mark the start of an argument, while open-
class words generally occur within a predicate or ar-
gument. Closed-class words, on the other hand (e.g.
sentence conjuncts) tend not to be found inside pred-
icates and arguments.
The advantage of having a coarse-grained set of
tags is that there is less margin for error and dis-
agreement on how a word can be tagged. For future
work, we would like to compare a finer-grained set
of tags, such as that employed by dela Vega et al
(2002), with our tags to see if a more detailed dis-
tinction results in significant benefits.
In Section 4, we investigate the impact of the in-
clusion of this extra annotation on PAS recogni-
tion, to gauge whether the annotation effort was war-
ranted.
3.2 Predicate and Argument Mark-up
Next, we marked up predicates and their (core) argu-
ments, employing the standard IOB tag scheme. We
mark up two types of predicates: PRD and PRD-SUB.
The former refers to predicates that belong to main
clauses, whilst the latter refers to predicates that oc-
cur in subordinate or dependent clauses.
We mark up 4 types of arguments: ANG, NG,
SA and NG-COMP. The first three mark nominal
258
phrases, while the last marks sentential comple-
ments (e.g. the object of quotative verbs).
We follow the multi-column format used in
the CoNLL 2004 semantic role labelling (SRL)
task (Carreras and Ma`rquez, 2004), with as many
columns as there are predicates in a sentence, and
one predicate and its associated arguments per col-
umn.
3.3 Annotation
Our corpus consists of 259 predicates (47 of which
are subordinate, i.e. PRD-SUB), and 435 arguments.
The following is a breakdown of the arguments:
Argument type: SA ANG NG NG-CMP
Count: 83 193 147 12
3.4 Morphological Processing
In tandem with the corpus annotation, we developed
a finite-state morphological analyser using XFST and
LEXC (Beesley and Karttunen, 2003), that extracts
morphological features for individual words in the
form of a binary feature vector.2 While LEXC is or-
dinarily used to define a lexicon of word stems, we
opted instead to list permissible syllables, based on
the work of French (1988). This decision was based
purely on resource availability: we did not have an
extensive list of stems in Tagalog, or the means to
generate such a list.
4 Experiments
In this section, we report on preliminary results for
PAS recognition over our annotated data. The ap-
proach we adopt is similar to the conventional ap-
proach adopted in CoNLL-style semantic role la-
belling: a two-phase approach of first identifying the
predicates, then identifying arguments and attaching
them to predicates, in a pipeline architecture. Pri-
mary areas of investigation in our experiments are:
(1) the impact of POS tags on predicate prediction;
and (2) the impact of morphological processing on
overall performance.
In addition to experimenting with the finite state
morphological processing (see Section 3.4), we ex-
periment with a character n-gram method, where we
simply take the first and last n characters of a word
2Thanks to Steven Bird for help with infixation and defining
permissible syllables for the morphological analyser
as features. In our experiments, we set n to 3 and 2
characters for prefix and suffixes, respectively.
We treat each step in the pipeline as a structured
learning task, which we model with conditional ran-
dom fields (Lafferty et al, 2001) using CRF++.3
All of the results were arrived at via leave-one-out
cross-validation, defined at the sentence level, and
the evaluation was carried out in terms of precision
(P), recall (R) and F-score (F) using the evaluation
software from the CoNLL 2004 SRL task.
4.1 Predicate identification
First, we attempt to identify the predicate(s) in a
given sentence. Here, we experiment with word
context windows of varying width (1?6 words),
and also POS features in the given context win-
dow. Three different strategies are used to derive
the POS tags: (1) from CRF++, with a word bi-
gram context window of width 3 (AUTO1); (2) again
from CRF++, with a word bigram context window
of width 1 (AUTO2); and (3) from gold-standard
POS tags, sourced from the corpus (GOLD). AUTO1
and AUTO2 were the two best-performing POS tag-
ging methods amongst a selection of configurations
tested, both achieving a word accuracy of 0.914.
We compare these three POS tagging options with
a method which uses no POS tag information (NO
POS). The results for the different POS taggers with
each word context width size are presented in Ta-
ble 1.
Our results indicate that the optimal window size
for the predicate identification is 5 words. We also
see that POS contributes to the task, and that the rel-
ative difference between the gold-standard POS tags
and the best of the automatic POS taggers (AUTO2)
is small. Of the two POS taggers, the best per-
formance for AUTO2 is clearly superior to that for
AUTO1.
4.2 Argument Identification and Attachment
We next turn to argument identification and attach-
ment, i.e. determining the word extent of arguments
which attach to each predicate identified in the first
step of the pipeline. Here, we build three predicate
recognisers from Section 4.1: NO POS, AUTO2 and
3http://sourceforge.net/projects/
crfpp/
259
Window NO POS AUTO1 AUTO2 GOLD
size P R F P R F P R F P R F
1 .255 .086 .129 .406 .140 .208 .421 .143 .214 .426 .144 .215
2 .436 .158 .232 .487 .272 .349 .487 .262 .340 .529 .325 .403
3 .500 .190 .275 .477 .255 .332 .500 .262 .344 .571 .335 .422
4 .478 .190 .272 .509 .290 .370 .542 .280 .369 .523 .325 .401
5 .491 .204 .278 .494 .274 .351 .558 .349 .429 .571 .360 .442
6 .478 .190 .272 .484 .269 .346 .490 .262 .341 .547 .338 .418
Table 1: Results for predicate identification (best score in each column in bold)
Morphological NO POS AUTO2 GOLD
analysis P R F P R F P R F
FINITE STATE .362 .137 .199 .407 .201 .269 .420 .207 .278
CHAR n-GRAMS .624 .298 .404 .643 .357 .459 .623 .377 .470
COMBINED .620 .307 .410 .599 .362 .451 .623 .386 .477
Table 2: Results for argument identification and attachment (best score in each column in bold)
GOLD, all based on a window size of 5. We com-
bine these with morphological features from: (1) the
finite-state morphological analyser, (2) character n-
grams, and (3) the combination of the two. The re-
sults of the different combinations are shown in Ta-
ble 2, all based on a word context window of 3, as
this was found to be superior for the task in all cases.
The results with character n-grams were in all
cases superior to those for the morphological anal-
yser, although slight gains were seen when the two
were combined in most cases (most notably in re-
call). There was surprisingly little difference be-
tween the GOLD results (using gold-standard POS
tags) and the AUTO2 results.
5 Conclusion
In this paper, we have presented a system that recog-
nises PAS in Tagalog text. As part of this, we cre-
ated the first corpus of PAS for Tagalog, and pro-
duced preliminary results for predicate identification
and argument identification and attachment.
In future work, we would like to experiment with
larger datasets, include semantic features, and trial
other learners amenable to structured learning tasks.
References
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI Publications, Stanford, USA.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role label-
ing. In Proc. of CoNLL-2004, pages 89?97, Boston,
USA.
Ester D. dela Vega, Melvin Co, and Rowena Cristina
Guevara. 2002. Language model for predicting parts
of speech of Filipino sentences. In Proceedings of the
3rd National ECE Conference.
Koleen Matsuda French. 1988. Insights into Tagalog.
Summer Institute of Linguistics, Dallas, USA.
Raymond Gordon, Jr. 2005. Ethnologue: Languages
of the World. SIL International, Dallas, USA, 15th
edition.
Paul Kroeger. 1993. Phrase Structure and Grammati-
cal Relations in Tagalog. CSLI Publications, Stanford,
USA.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML 2001, pages 282?289, Williamstown, USA.
Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2009. Evaluating contri-
butions of natural language parsers to protein?protein
interaction extraction. Bioinformatics, 25(3):394?400.
Paul Schachter and Fe T. Otanes. 1972. Tagalog Refer-
ence Grammar. University of California Press, Berke-
ley.
Peter Sells. 2000. Raising and the order of clausal
constituents in the Philippine languages. In Ileana
Paul, Vivianne Phillips, and Lisa Travis, editors, For-
mal Issues in Austronesian Linguistics, pages 117?
143. Kluwer Academic Publishers, Dordrecht, Ger-
many.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proc. of ACL 2003,
pages 8?15, Sapporo, Japan.
260
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 577?584
Manchester, August 2008
Applying Discourse Analysis and Data Mining Methods to
Spoken OSCE Assessments
Meladel Mistica, Timothy Baldwin
The University of Melbourne
CSSE
{mmistica,tim}
@csse.unimelb.edu.au
Marisa Cordella, Simon Musgrave
Monash University
School of Languages, Cultures and Linguistics
{marisa.cordella,simon.musgrave}
@arts.monash.edu.au
Abstract
This paper looks at the transcribed data of
patient-doctor consultations in an exami-
nation setting. The doctors are interna-
tionally qualified and enrolled in a bridg-
ing course as preparation for their Aus-
tralian Medical Council examination. In
this study, we attempt to ascertain if there
are measurable linguistic features of the
consultations, and to investigate whether
there is any relevant information about
the communicative styles of the qualify-
ing doctors that may predict satisfactory
or non-satisfactory examination outcomes.
We have taken a discourse analysis ap-
proach in this study, where the core unit of
analysis is a ?turn?. We approach this prob-
lem as a binary classification task and em-
ploy data mining methods to see whether
the application of which to richly anno-
tated dialogues can produce a system with
an adequate predictive capacity.
1 Introduction
This paper describes our experimentation with ap-
plying data mining methods to transcribed doctor?
patient consultations. It is in essence a discovery
project: we apply methods to a field and task that
is not ordinarily associated with such approaches
in order to ascertain whether this could make for a
tractable learning task.
The task involves the extraction of dis-
course features from doctor?patient consultations
performed by international medical graduates
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
(IMGs), and what is known as ?simulated pa-
tients? (Vu et al, 1994), respectively. The IMGs
are enrolled in a bridging course in Melbourne as
preparation for their Australian Medical Council
(AMC) examination, the successful completion of
which is one of the pre-requisites to becoming a
fully accredited practitioner in Australia. This par-
tially replicates the AMC examination by studying
in detail how IMGs perform two objective struc-
tured clinical examinations (OSCEs). See Sec-
tion 2 for full details of the examination environ-
ment and participants involved.
The main questions raised when initiating this
study were:
? How objective is the testing?
? What is the importance placed on language
skills in OSCE environments?
? What makes for a successful OSCE?
In this research, we aim to build classifiers
that make reasonable predictions of the data being
tested, and possibly point us in the right direction
with respect to the questions above. From the clas-
sifiers we build, we also hope to ascertain which of
our features best predict a successful examination.
We organise the paper as follows. In Section 2,
we briefly describe the examination environment
and process, the marking scheme, and the partic-
ipants involved in the testing of the IMGs. We
also outline some of the issues that have arisen
with regard to the current methods of IMG testing.
In Section 3, we present details of the data used.
Section 4 describes the features we develop for the
task and discusses the reasoning behind the selec-
tion of features from a discourse analysis perspec-
tive. Section 5 discusses the results of the exper-
iments, with further examination of the data. The
577
last two sections, Sections 6 and 7, comprise a dis-
cussion of the results and concluding remarks.
2 Background
With Western nations becoming increasingly re-
liant on medical professionals trained overseas,
there is, in turn, a growing need to develop a re-
liable means of objectively assessing IMGs. The
shortage of medical doctors is a worldwide phe-
nomenon currently affecting many Western so-
cieties such as the UK, Canada, US and New
Zealand, which compete for the best medical prac-
titioners available around the world. Australia is
not immune to this global phenomenon, and in
the last two decades the shortage of local medi-
cal practitioners in Australia has worsened (Bir-
rell et al, 2004). Challenges to the healthcare
system in the country are particularly evident in
the areas of providing medical care for a grow-
ing elderly population and of servicing rural ar-
eas, where locally trained doctors do not feel par-
ticularly attracted to practise medicine (Han et al,
2006). Currently 35% of the rural medical work-
force and 20% of the total national medical work-
force consist of IMGs (Flynn, 2006). These figures
may increase even further in some regions (Spike,
2006), as preparation of fully educated and trained
local medical graduates takes up to thirteen years
to complete.
There is considerable disparity among IMGs in
their background training, clinical skills, under-
standing of the health system and communication
skills (McGrath, 2004). In order to be registered
to practice in Australia, IMGs must successfully
complete the Australian Medical Council exami-
nations and a period of supervised training. The
medical knowledge of IMGs is assessed in two
ways: by multiple choice examinations and by
clinical examinations. This second form of ex-
amination consists of a series of simulated medi-
cal consultations in which a role-player takes the
part of the patient, and the IMG?s professional
knowledge, lay-cultural knowledge, socio-cultural
assumptions, institutional norms, and values and
personal experiences are all in full display during
the unfolding of the medical event (Roberts et al,
2003). Whenever cultural factors are not shared
with their patients, the interpretative schema and
therefore the comprehension of speech are affected
by this lack of commonality in the participants?
inferences and contextual cues (Gumperz, 1999).
Such effects are likely to cause miscommunica-
tion in medical visits and have a potential nega-
tive effect on patients? satisfaction in the consul-
tation. Identification of the communication diffi-
culties faced by IMGs can therefore inform mod-
ifications to the training provided to IMGs when
they prepare for the Australian Medical Council
examinations, as well as suggesting more nuanced
and targeted procedures for assessing communica-
tive skills within those examinations, all with the
goal of working toward a better equipped medical
workforce for the future. The use of automated an-
alytic procedures to try to establish objective crite-
ria for communicative success is an important step
in this process.
Assessing language knowledge and competence
quantitatively is not a novel concept in second lan-
guage learning assessment. However, the applica-
tion of data mining methods to automatically as-
sess language proficiency in a discourse setting is
novel. Levow et al (1999) propose an architec-
ture to automatically assess language proficiency.
In their paper, they propose an architecture that
employs data mining methods, but do not build
classifiers over their spoken data to test this pro-
posal. A closely related line of research is on the
automatic classification of discourse elements to
assess the quality of a written genre (Burstein et
al., 2001). Like this work, it focuses on extracting
features from the discourse as a whole. But unlike
this study, the authors extract high level features,
such as rhetorical structure, of written discourse.
The study we present in this paper is rather unique
in its approach to language assessment.
3 Data
The data is taken from transcribed recordings of
examinations from students enrolled in a bridg-
ing course at Box Hill Hospital in Melbourne,
Australia. Each candidate was video-recorded en-
acting medical consultation scenarios with what
is known as a standardised or simulated patient
(SP). This method of testing is known as an ob-
jective structured clinical examination (OSCE),
which is an emulation of a doctor?patient consul-
tation, much like a role-play setting.
In this study, the role of the patient (SP) is en-
acted by a qualified doctor who follows a script,
and has well-defined ailment(s) and accompanying
concerns. Even though the SP assumes the same
ailment and disposition with all the candidates, the
578
interaction between the candidate and the SP is un-
cued and free-form. They simply present the infor-
mation in a standardised manner across all candi-
dates and perform the role of the patient as felici-
tously as possible.
For this set of examinations there are 2 types of
OSCE stations referred to as STD (sexually trans-
mitted disease ? genital herpes) and BC (bowel
cancer). The SP for the STD station is played by
a female doctor. The patient she plays has genital
herpes and is concerned about how this will affect
her chances of falling pregnant, and how this con-
dition may also affect her baby. The SP for the BC
station is played by an older male. A tumour is
discovered in his bowel lining and he is reluctant
to undergo any treatment because one of his good
friends suffered a similar condition and his quality
of life was severely diminished.
Even though the consultation is free to be nego-
tiated between doctor (candidate) and patient (sim-
ulated patient), each of the OSCEs cannot exceed
8 minutes, and is terminated by the examiner if it
does so.
3.1 Transcription
The recordings are transcribed in ELAN, a multi-
media annotation tool developed at theMax Planck
Institute, to help encode low-level linguistic fea-
tures such as overlapping and timing information.
The information and features extracted from the
discourse are largely based on a ?turn?.
Here we consider a turn as being normally domi-
nated by one speaker. It can be made up of multiple
intonation units. When there is backchannelling,
overlapping, or any interruption by the other par-
ticipant, then the turn is encoded as ending at the
end of the interrupted intonation unit. Otherwise,
transition pauses commonly signal turn changes,
unless latching occurs.
Given that the OSCE setting aims to emulate
as close as possible a real medical consultation,
this interaction, like all uncued spoken dialogues,
also has evidence of complicated turn-taking ne-
gotiations, disfluent and unintelligible speech, in-
terrupted speech, challenges for the floor, and the
like, all of which must be encoded and noted in
ELAN. Transcribing such data is not a trivial mat-
ter. In addition, transcribing the data in order to
extract these features is also a demanding task in
itself, which makes creating data for such tasks an
involved process.
Disfluencies and repairs are encoded in a limited
way, only by way of marking up truncated or un-
finished words. We also do not take a fine-grained
approach in encoding delaying strategies (Clark et
al., 2002), that is we do not differentiate whether
the uh or ah encoded represents lexical search, a
wish to hold the floor, a wish to give up the floor
or buying time to construct what to say next.
3.2 OSCE scoring
In an OSCE setting, candidates are given an over-
all pass or fail rating for each station by an OSCE
examiner observing the interaction. This overall
evaluation can be based on a number of perfor-
mance criteria which tests the candidates medical,
clinical and communication skills (Grand?Maison
et al, 1992). The OSCE marking scheme used for
this study consists of 5 assessable categories, as
follows:
APPROACH: the ability of the candidate to com-
municate with the patient;
HISTORY: the ability of the candidate to collect
medical history;
INTERPRETATION: how well does the candidate
interpret his or her investigation in order to
formulate an appropriate diagnosis;
MANAGEMENT: how well does the candidate
formulate a management plan for the diagno-
sis;
COUNSELLING: is the candidate able to give ap-
propriate counselling to the patient.
The first category tests language knowledge and
competency both at the lexical and discourse level,
while the remaining four categories test medical
knowledge and clinical competency.
4 Feature Engineering
We extracted a total of 38 features from the tran-
scribed data. Some of these features are based on
what is marked up according to the transcription
scheme, while others are based on timing informa-
tion or lexical information as encoded in ELAN.
These include features such as signals for delaying
speaking or hesitation (Clark et al, 2002), features
of conversational dominance (Itakura, 2000), the
manner in which turn-taking is negotiated (Sacks
et al, 1974), temporal features such as pausing (ten
Bosch et al, 2005), as well as our own features,
579
which include ?lexical introduction?, and ?lexical
repeat?.
In encoding features of conversational
dominance, we focus on participatory domi-
nance (Itakura, 2000), which looks at which
speaker contributes most to the dialogue in terms
of content.
Lexical introduction refers to a non-stop word
that is introduced by the doctor (IMG) or the pa-
tient (SP), while lexical repeat encodes how many
times a word introduced by the other interlocutor
is repeated by the speaker.
Almost all of the features developed are contin-
uous, based on timing information or word counts.
The only binary feature used encodes whether the
doctor initiates the consultation or not.
As mentioned in the previous section, the fea-
tures developed were largely based on turns. This
is to capture, along with other features such as
overlapping and pauses, the interactional aspect of
the communication. For example, conversational
cooperation and speaker reassurance can be cap-
tured with these features. Another aspect to the
development of these features, particularly for the
lexical-based features, is whether the IMG has a
suitable vocabularly and if they employ it appro-
priately in the interaction.
We arrive at 11 feature sets from which we build
our classifiers, as described in Table 1.
Not all features are exclusive to any one feature
set, that is, it is possible for a single feature to be-
long to a number of feature sets.
The sets were designed to isolate possible char-
acteristics of not only the discourse as a whole, but
how the participants negotiated their interaction.
These features sets were developed from observ-
ing each of the consultations with the expectation
that these were salient and determining features of
a successful examination.
5 Experiments
There was a total of 11 OSCE candidates, all of
whom performed an STD and a BC station, giv-
ing us in total 22 instances for this binary classi-
fication task to predict a pass or fail examination
result. Of the 22 instances, we had 5 failures and
17 passes. Given the small number of instances,
we maximised our dataset by employing 10-fold
stratified cross-validation, as well as leave-one-out
cross-validation which uses all but one instance in
training and the held-out instance for testing.
Feature set Example features
all - all 38 features
cooperation - overall word count
- length of interaction
- number of turns
hesitation - number of uh and ah
- number of unfinished words
overlap - number of overlapping words
- length of overlap (time)
pause - transition pauses
- within turn pauses
timeBased - all time-based features
turns - all turn-based features
- number of turns
- longest turn
- single word responses
uniqNrepeat - number of introduced content
words by each speaker
- number of times speaker
uses word introduced by other
wordBased - number of words in dialogue
- longest number of words
in a turn
patient - all SP-based features
doctor - all IMG-based features
Table 1: The 11 feature sets developed
The baseline system we use for comparison is
zero-R, or majority vote. For our supervised clas-
sifier, we employ a lazy learner in the form of the
IB1 algorithm implemented in WEKA.
5.1 Results for Feature Sets
Our initial classifiers held some promise. The clas-
sifier built from all of the features was equivalent
to the baseline system, and the combination of the
word-based features surpassed the baseline?s re-
sults, as shown in Table 2.
To evaluate our system, we employ simple clas-
sification accuracy, in addition to precision, recall
and F-score. Classification accuracy is the propor-
tion of correct predictions by the classifier, irre-
spective of class. Precision gauges how successful
the pass predictions of a given classifier are, while
recall gives us an indication of how successful a
given classifier is at identifying the candidates who
actually passed. Finally, F-score is a composite of
precision and recall, and gives us an overall perfor-
mance rating relative to passed candidates.
The least successful classifier was built on the
580
10-fold cross validation Leave-one-out cross validation
Feature set Accuracy Precision Recall F-score Accuracy Precision Recall F-score
baseline .773 .773 1.00 .872 .773 .773 1.00 .872
all .773 .773 1.00 .872 .773 .773 1.00 .872
cooperation .682 .789 .824 .806 .682 .778 .824 .800
hesitation .636 .737 .824 .778 .636 .737 .824 .778
overlap .773 .833 .882 .857 .773 .833 .882 .857
pause .682 .778 .824 .800 .727 .789 .882 .833
timeBased .500 .647 .688 .667 .545 .706 .706 .706
turns .727 .789 .882 .833 .727 .789 .882 .833
uniqNrepeat .636 .765 .765 .765 .682 .778 .824 .800
wordBased .864 .850 1.00 .919 .864 .850 1.00 .919
patient .727 .867 .765 .813 .727 .867 .765 .813
doctor .733 .800 .941 .865 .727 .789 .882 .833
Table 2: Classification results for STD and BC
feature set based on timing, which contains in-
formation such as the overall length of the dia-
logue, the overall length of transition pauses, in-
turn pauses and other time-based features. This
was most surprising because as a general observa-
tion, candidates who allowed extended pauses and
uncomfortable silences were those who seemed to
perform poorly, and those who did not leave too
many silences, and could maintain the flow of the
dialogue, seemed to perform well.
Given the small number of training instances
each classifier is based on, these first results were
somewhat encouraging. With respect to the base-
line, the overall performance of two of the sys-
tems equalled or surpassed the baseline in terms
of F-score. Most of the classifiers performed well
in terms of precision but less well in terms of re-
call, i.e. when the classifiers predicted a pass they
were generally correct, but there were significant
numbers of candidates who were predicted to have
failed but passed in practice.
5.2 Data Introspection Retrospectively
Although the results show promise, it was ex-
pected that more of the feature sets would return
more favourable results. The possible reasons why
the time-based features, and many of the other fea-
ture sets developed, did not perform as well as ex-
pected may have been because the features used in
building the classifiers could have been combined
in a better way, or because the data itself had too
many anomalies or was too disparate. We would
expect that extra data could iron out such anoma-
lies, but developing additional data is expensive
and more recordings are not always available. The
advantage of having a small dataset is that we are
able to do fine-grained annotation of the data, but
the obvious disadvantage is that we cannot easily
generate extra amounts of training data.
One very noticeable feature of the OSCE sta-
tions was that the STD SP had a very different
communicative style to that of the the BC SP.
Based on this observation we conducted tests given
the hypothesis that the possible bias in the data
could have stemmed from having two very differ-
ent testing approaches from the two SPs. In gen-
eral, the BC SP was more leading and in a sense
more forgiving with the candidates. In contrast to
this, the STD SP tended to be more felicitous in
her role as a patient, allowing awkward silences
and not prompting the candidates for further ex-
ploration.
We conduct the Mann-Whitney test, a rank sum
test, over the data in order to diagnose whether the
poor results were due to the distribution of the data
or whether the classifiers built with the selected
features were simply poor predictors. The Mann-
Whitney test ascertains whether there is a differ-
ence in the population mean of the two samples
given, without making any assumptions about the
distribution of the data.
We sub-sample the data in two ways in exam-
ining its homogeneity: (a) FAIL juxtaposed with
PASS candidates; and (b) BC juxtaposed with STD
stations. Test (a) essentially tests which exam-
inable category contributes the most to a pass or
fail outcome, whilst test (b) examines whether
there is an inherent difference in the way the test-
581
Category OVERALL APPROACH HISTORY INTERPRETATION MANAGEMENT COUNSELLING
z-score 1.84 1.21 -0.03 2.53 0.85 1.64
Table 3: Mann-Whitney z-score for BC and STD samples (OVERALL is the cumulative total of all 5
categories)
Category OVERALL APPROACH HISTORY INTERPRETATION MANAGEMENT COUNSELLING
z-score -3.29 -3.13 -2.43 -2.31 -2.31 -1.57
Table 4: Mann-Whitney z-score for failed and passed samples
ing was conducted between the BC and STD sta-
tions.
BC vs. STD
We use the ranking from the 5 assessable cate-
gories outlined in Section 3 and obtain the Mann-
Whitney z-score for each category. The z-score
gives us an indication of how disparate the two
separated datasets, BC and STD, are. The further
away from 0 the z-score is, the greater the evidence
that BC and STD data are not from the same pop-
ulation, and should be treated as such. The results
of this test, as seen in Table 3, show that these two
groups differ quite markedly: the candidates were
consistently marked differently for all assessable
categories except HISTORY. This is a striking pe-
culiarity because each candidate was tested in both
the STD and BC stations.
Based on the above, we can posit that the dis-
tinct testing styles of the STD and BC SPs were
the reason for our original lacklustre results, and
that the two data samples need to be treated sepa-
rately for the classifiers to perform consistently.
FAIL vs. PASS
In addition to the BC vs. STD test, we also test
how the failing candidates differ from the passing
candidates across the evaluation criteria.
The main idea behind this test is to see which
of the assessable categories contributed the most
in the overall outcome of the examination. For this
test, we would not expect the absolute z-score of
any of the assessment components to exceed the
absolute z-score of the OVERALL category given
that it is the cumulative scores of all categories.
The results in Table 4 suggest that APPROACH
correlates most highly with the pass/fail divide
in the OSCE assessments, followed by HISTORY,
then INTERPRETATION and MANAGEMENT, and
finally COUNSELLING. Recall that APPROACH is
the component that assesses language and commu-
nication skills. In particular, it assesses the style
and appropriateness of the way candidates con-
vey information, from lexical choice to display-
ing empathy through communication style. Given
that APPROACH correlates most strongly with the
assessment result, the decision to focus our fea-
ture engineering efforts on linguistic aspects of the
doctor?patient interaction would appear justified.
5.3 Results for STD & BC Data
Given the results from the Mann-Whitney tests re-
ported in the previous section, we separate the data
into two lots: those from the STD station, and
those from the BC station.
Even though there were very few instances in
the original dataset, we aim to see in these experi-
ments whether this separation improves the perfor-
mance of the classifiers. We build classifiers over
each dataset using the same features as before.
The results of the tests performed over the sep-
arated datasets, as shown in Table 5, show a big
improvement over the baseline for STD, while the
BC dataset is more problematic.
In the STD group, we see that four feature sets,
all, turns, wordBased and patient equal or surpass
the baseline F-score.
In contrast to this, upon examination of the
performance of the classifiers built over the BC
dataset, we do not observe any improvements over
the baseline and the results are markedly worse
than those for the combined dataset. Having said
this, when we combine the outputs of the two com-
ponent classifiers, the F-score for all features is
0.882, an improvement over the original combined
system.
6 Discussion
The OSCE assessment does not merely examine
the language skills of the candidates, but it also as-
582
BC STD
Feature Set Accuracy Precision Recall F-score Accuracy Precision Recall F-score
baseline .818 .818 1.00 .900 .727 .727 1.00 .842
all .727 .875 .778 .824 .909 .889 1.00 .941
cooperation .727 .800 .889 .842 .364 .571 .500 .533
hesitation .636 .778 .778 .778 .636 .700 .875 .778
overlap .727 .800 .889 .842 .636 .700 .875 .778
pause .818 .889 .889 .889 .545 .667 .750 .706
timeBased .636 .857 .667 .750 .545 .667 .750 .706
turns .636 .778 .778 .778 .818 .800 1.00 .889
uniqNrepeat .727 .800 .889 .842 .727 .778 .875 .824
wordBased .636 .778 .778 .778 .909 .889 1.00 .941
patient .727 .875 .778 .824 .818 .875 .875 .875
doctor .818 .818 1.00 .900 .455 .625 .625 .625
Table 5: Results for separated BC and STD datasets (leave-one-out)
sesses the efficacy of their communication skills in
conveying correct and accurate medical informa-
tion within a clinical setting. It can be seen from
Table 4 that there is a high correlation between the
overall pass or fail and the assessable category AP-
PROACH.
The examiners? subjectivity of overall perfor-
mance is minimised by the highly structured exam-
ination setup and well-defined assessment criteria.
However, as shown in Table 3, the communicative
style of the SP is a contributing factor to the per-
ception of successful clinical and communication
skills. The Mann-Whitney tests suggest that an
SP?s approach and their apparent satisfaction dur-
ing the clinical encounter can affect the judgement
of the examiner.
Additional inspection of the data revealed that
the assessment criteria which focused on language
and communication skills correlated highly with
an overall pass grade, moreso than the other cri-
teria. This seems to suggest that more emphasis
should be placed on language skills and communi-
cation style in the assessment of the candidates.
Assessing language competency is no trivial
matter, and capturing the linguistic features of di-
alogues in an attempt to define competence, as we
have done here, is a demanding task in itself. Al-
though many of our features were focused on turn-
taking, speaker response and interaction, we did
not develop features that encompass the informa-
tion structure of the communicative event.
It is assumed that miscommunication between
non-native and native speakers of a language is due
to a lack of language knowledge pertaining to syn-
tax, morphology or lexical semantics. However
many of these communication difficulties arise not
because of this lack of grammatical knowledge,
but through a difference in discourse styles or in-
formation structure as governed by different cul-
tures (Wiberg, 2003; Li, 1999).
Given that the word-based feature sets were
the most successful predictors of an OSCE out-
come, future work of this kind could make use of
medical-based lexicons to gauge whether technical
or non-technical word usage in such environments
is judged favourably. In addition, further work
should be done to test the hypothesis that informa-
tion structure or rhetorical structure does impact on
overall perception of a successful communication,
such as a variation on the methods employed by
Burstein et al (2001).
One obvious improvement to this study would
be to reduce the expense in producing the anno-
tated data. Future work could also be done in auto-
matically extracting features from non-transcribed
data, such as timing information based on pause
length and the turn length of each speaker.
7 Conclusions
In this research, we have built classifiers over
transcribed doctor?patient consultations in an at-
tempt to predict OSCE outcomes. We achieved
encouraging results based on a range of lexical and
discourse-oriented features.
In our first experiments, we combined the data
from two discrete stations in an attempt to max-
imise training data, and achieved modest results.
Subsequent analysis with the Mann-Whitney test
583
indicated both that success in the APPROACH cat-
egory correlates strongly with an overall success-
ful OSCE, and that the data for the two stations is
markedly different in nature. Based on this find-
ing, we conduct tests over the data for the individ-
ual stations with noticeable improvements to the
results.
The results of this exploratory study have been
quite encouraging, given the novel domain and
limited data. We have shown that a data mining
approach to OSCE assessment is feasible, which
we hope will open the way to increased interest in
automated medical assessment based on linguistic
analysis.
References
Birrell, Bob, Lesleyanne Hawthorne. 2004. Medicare
Plus and overseas trained doctors. People and Place,
12(2):83?99.
ten Bosch, Louis, Nelleke Oostdijk, Lou Boves. 2005.
On temporal aspects of turn taking in conversational
dialogues. Speech Communication, 47(2005):80?
86.
Burstein, Jill, Daniel Marcu, Slava Andreyev, Martin
Chodorow. 2001. Towards Automatic Classification
of Discourse Elements in Essays. ACL, 90-97.
Clark, Herber H., Jean E. Fox Tree. 2002. Using
uh and um in spontaneous speaking. Cognition,
84(2002):73?111.
Flynn, Joanna. 2006. Medical Release. Australian
Medical Council, 17(August 2006).
Grand?Maison, Paul, Jo?elle Lescop, Paul Rainsberry,
Carlos A. Brailovsky. 1992. Large-scale use of an
objective, structured clinical examination for licens-
ing family physicians. Canadian Medical Associa-
tion, 146(10):1735?1740.
Gumperz, John. 1999. On Interactional Sociolinguis-
tic Method. In Talk, Work and Institutional Order.
Discourse in Medical, Mediation and Management
Settings S. Sarangi and C. Robers (eds), 453?471.
Han, Gil-Soo, John .S Humphreys. 2006. Integratoin
and retention of international medical graduates in
rural communities. A typological analysis. The Aus-
tralian Sociological Association, 42(2):189?207.
Itakura, Hiroko. 2000. Describing conversational
dominance. Journal of Pragmatics, 33(2001):1859?
1880.
Levow, Gina-Anne, Mari Broman Olsen. 1999. Mod-
eling the language assessment process and result:
Proposed architecture for an automatic oral profi-
ciency assessment. Workshop On Computer Medi-
ated Language Assessment And Evaluation In Natu-
ral Language Processing.
Li, Han Zao. 1999. Comunication Information in Con-
versations: A Cross-cultural Comparison. Interna-
tional Journal of Intercultural Relations, 23(3):387?
409.
McGrath, Barry. 2004. Overseas-trained doctors. Inte-
gration of overseas-trained doctors in the Australian
medical workforce. The Medical Journal of Aus-
tralia, 181(11/12):640?642.
Roberts, Celia, Val Wass, Roger Jones, Srikant Sarangi,
Annie Gillett. 2003. A discourse analysis study
of ?good? and ?poor? communication in an OSCE:
a proposed new framework for teaching students.
Medical Education, 50:192?201.
Sacks, Harvey, Emanuel A. Schegloff, Gail Jefferson.
1974. A Simplest Systematics for the Organiza-
tion of Turn-Taking for Conversation. Language,
50(4):696?735.
Spike, Neil. 2006. International Medical Graduates:
The Australian perspective. Acad Med, 81):842?
846.
Vu, Nu Viet, Howard S. Barrows. 1994. Use of Stan-
dardized Patients in Clinical Assessments: Recents
Developments and Measurement Findings. Educa-
tional Researcher, 23(3):23?30.
Wiberg, Eva. 2003. Interactional context in L2 dia-
logues. Journal of Pragmatics, 35(2003):389?407.
584
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 550?560,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ParGramBank: The ParGram Parallel Treebank
Sebastian Sulger and Miriam Butt
University of Konstanz, Germany
{sebastian.sulger|miriam.butt}@uni-konstanz.de
Tracy Holloway King
eBay Inc., USA
tracyking@ebay.com
Paul Meurer
Uni Research AS, Norway
paul.meurer@uni.no
Tibor Laczko? and Gyo?rgy Ra?kosi
University of Debrecen, Hungary
{laczko.tibor|rakosi.gyorgy}@arts.unideb.hu
Cheikh Bamba Dione and Helge Dyvik and Victoria Rose?n and Koenraad De Smedt
University of Bergen, Norway
dione.bamba@lle.uib.no, {dyvik|victoria|desmedt}@uib.no
Agnieszka Patejuk
Polish Academy of Sciences
aep@ipipan.waw.pl
O?zlem C?etinog?lu
University of Stuttgart, Germany
ozlem@ims.uni-stuttgart.de
I Wayan Arka* and Meladel Mistica+
*Australian National University and Udayana University, Indonesia
+Australian National University
wayan.arka@anu.edu.au, meladel.mistica@gmail.com
Abstract
This paper discusses the construction of
a parallel treebank currently involving ten
languages from six language families. The
treebank is based on deep LFG (Lexical-
Functional Grammar) grammars that were
developed within the framework of the
ParGram (Parallel Grammar) effort. The
grammars produce output that is maxi-
mally parallelized across languages and
language families. This output forms the
basis of a parallel treebank covering a
diverse set of phenomena. The treebank
is publicly available via the INESS tree-
banking environment, which also allows
for the alignment of language pairs. We
thus present a unique, multilayered paral-
lel treebank that represents more and dif-
ferent types of languages than are avail-
able in other treebanks, that represents
deep linguistic knowledge and that allows
for the alignment of sentences at sev-
eral levels: dependency structures, con-
stituency structures and POS information.
1 Introduction
This paper discusses the construction of a parallel
treebank currently involving ten languages that
represent several different language families, in-
cluding non-Indo-European. The treebank is based
on the output of individual deep LFG (Lexical-
Functional Grammar) grammars that were deve-
loped independently at different sites but within
the overall framework of ParGram (the Parallel
Grammar project) (Butt et al, 1999a; Butt et al,
2002). The aim of ParGram is to produce deep,
wide coverage grammars for a variety of lan-
guages. Deep grammars provide detailed syntactic
analysis, encode grammatical functions as well as
550
other grammatical features such as tense or aspect,
and are linguistically well-motivated. The Par-
Gram grammars are couched within the linguis-
tic framework of LFG (Bresnan, 2001; Dalrymple,
2001) and are constructed with a set of grammati-
cal features that have been commonly agreed upon
within the ParGram group. ParGram grammars are
implemented using XLE, an efficient, industrial-
strength grammar development platform that in-
cludes a parser, a generator and a transfer sys-
tem (Crouch et al, 2012). XLE has been devel-
oped in close collaboration with the ParGram
project. Over the years, ParGram has continu-
ously grown and includes grammars for Ara-
bic, Chinese, English, French, German, Georgian,
Hungarian, Indonesian, Irish, Japanese, Mala-
gasy, Murrinh-Patha, Norwegian, Polish, Spanish,
Tigrinya, Turkish, Urdu, Welsh and Wolof.
ParGram grammars produce output that has
been parallelized maximally across languages ac-
cording to a set of commonly agreed upon uni-
versal proto-type analyses and feature values. This
output forms the basis of the ParGramBank paral-
lel treebank discussed here. ParGramBank is con-
structed using an innovative alignment methodol-
ogy developed in the XPAR project (Dyvik et al,
2009) in which grammar parallelism is presup-
posed to propagate alignment across different pro-
jections (section 6). This methodology has been
implemented with a drag-and-drop interface as
part of the LFG Parsebanker in the INESS infras-
tructure (Rose?n et al, 2012; Rose?n et al, 2009).
ParGramBank has been constructed in INESS and
is accessible in this infrastructure, which also of-
fers powerful search and visualization.
In recent years, parallel treebanking1 has gained
in importance within NLP. An obvious applica-
tion for parallel treebanking is machine transla-
tion, where treebank size is a deciding factor for
whether a particular treebank can support a par-
ticular kind of research project. When conduct-
ing in-depth linguistic studies of typological fea-
tures, other factors such as the number of in-
cluded languages, the number of covered phe-
nomena, and the depth of linguistic analysis be-
come more important. The treebanking effort re-
ported on in this paper supports work of the lat-
ter focus, including efforts at multilingual depen-
dency parsing (Naseem et al, 2012). We have
1Throughout this paper ?treebank? refers to both phrase-
structure resources and their natural extensions to depen-
dency and other deep annotation banks.
created a parallel treebank whose prototype in-
cludes ten typologically diverse languages and re-
flects a diverse set of phenomena. We thus present
a unique, multilayered parallel treebank that rep-
resents more languages than are currently avail-
able in other treebanks, and different types of lan-
guages as well. It contains deep linguistic knowl-
edge and allows for the parallel and simultane-
ous alignment of sentences at several levels. LFG?s
f(unctional)-structure encodes dependency struc-
tures as well as information that is equivalent to
Quasi-Logical Forms (van Genabith and Crouch,
1996). LFG?s c(onstituent)-structure provides in-
formation about constituency, hierarchical rela-
tions and part-of-speech. Currently, ParGramBank
includes structures for the following languages
(with the ISO 639-3 code and language fam-
ily): English (eng, Indo-European), Georgian (kat,
Kartvelian), German (deu, Indo-European), Hun-
garian (hun, Uralic), Indonesian (ind, Austrone-
sian), Norwegian (Bokma?l) (nob, Indo-European),
Polish (pol, Indo-European), Turkish (tur, Altaic),
Urdu (urd, Indo-European) and Wolof (wol, Niger-
Congo). It is freely available for download under
the CC-BY 3.0 license via the INESS treebanking
environment and comes in two formats: a Prolog
format and an XML format.2
This paper is structured as follows. Section
2 discusses related work in parallel treebanking.
Section 3 presents ParGram and its approach to
parallel treebanking. Section 4 focuses on the tree-
bank design and its construction. Section 5 con-
tains examples from the treebank, focusing on ty-
pological aspects and challenges for parallelism.
Section 6 elaborates on the mechanisms for paral-
lel alignment of the treebank.
2 Related Work
There have been several efforts in parallel tree-
banking across theories and annotation schemes.
Kuhn and Jellinghaus (2006) take a mini-
mal approach towards multilingual parallel tree-
banking. They bootstrap phrasal alignments over
a sentence-aligned parallel corpus of English,
French, German and Spanish and report concrete
treebank annotation work on a sample of sen-
tences from the Europarl corpus. Their annotation
2http://iness.uib.no. The treebank is in the
public domain (CC-BY 3.0). The use of the INESS platform
itself is not subject to any licensing. To access the treebank,
click on ?Treebank selection? and choose the ParGram collec-
tion.
551
scheme is the ?leanest? possible scheme in that it
consists solely of a bracketing for a sentence in
a language (where only those units that play the
role of a semantic argument or modifier in a larger
unit are bracketed) and a correspondence relation
of the constituents across languages.
Klyueva and Marec?ek (2010) present a small
parallel treebank using data and tools from two
existing treebanks. They take a syntactically an-
notated gold standard text for one language and
run an automated annotation on the parallel text
for the other language. Manually annotated Rus-
sian data are taken from the SynTagRus treebank
(Nivre et al, 2008), while tools for parsing the cor-
responding text in Czech are taken from the Tec-
toMT framework (Popel and Z?abokrtsky?, 2010).
The SMULTRON project is concerned with con-
structing a parallel treebank of English, German
and Swedish. The sentences have been POS-tagged
and annotated with phrase structure trees. These
trees have been aligned on the sentence, phrase
and word level. Additionally, the German and
Swedish monolingual treebanks contain lemma in-
formation. The treebank is distributed in TIGER-
XML format (Volk et al, 2010).
Megyesi et al (2010) discuss a parallel English-
Swedish-Turkish treebank. The sentences in each
language are annotated morphologically and syn-
tactically with automatic tools, aligned on the
sentence and the word level and partially hand-
corrected.3
A further parallel treebanking effort is Par-
TUT, a parallel treebank (Sanguinetti and Bosco,
2011; Bosco et al, 2012) which provides depen-
dency structures for Italian, English and French
and which can be converted to a CCG (Combina-
tory Categorial Grammar) format.
Closest to our work is the ParDeepBank, which
is engaged in the creation of a highly paral-
lel treebank of English, Portuguese and Bulgar-
ian. ParDeepBank is couched within the linguistic
framework of HPSG (Head-Driven Phrase Struc-
ture Grammar) and uses parallel automatic HPSG
grammars, employing the same tools and imple-
mentation strategies across languages (Flickinger
et al, 2012). The parallel treebank is aligned on
the sentence, phrase and word level.
In sum, parallel treebanks have so far fo-
cused exclusively on Indo-European languages
3The paper mentions Hindi as the fourth language, but
this is not yet available: http://stp.lingfil.uu.
se/?bea/turkiska/home-en.html.
(with Turkish providing the one exception) and
generally do not extend beyond three or four
languages. In contrast, our ParGramBank tree-
bank currently includes ten typologically differ-
ent languages from six different language families
(Altaic, Austronesian, Indo-European, Kartvelian,
Niger-Congo, Uralic).
A further point of comparison with ParDeep-
Bank is that it relies on dynamic treebanks, which
means that structures are subject to change dur-
ing the further development of the resource gram-
mars. In ParDeepBank, additional machinery is
needed to ensure correct alignment on the phrase
and word level (Flickinger et al, 2012, p. 105).
ParGramBank contains finalized analyses, struc-
tures and features that were designed collabora-
tively over more than a decade, thus guaranteeing
a high degree of stable parallelism. However, with
the methodology developed within XPAR, align-
ments can easily be recomputed from f-structure
alignments in case of grammar or feature changes,
so that we also have the flexible capability of
allowing ParGramBank to include dynamic tree-
banks.
3 ParGram and its Feature Space
The ParGram grammars use the LFG formalism
which produces c(onstituent)-structures (trees)
and f(unctional)-structures as the syntactic anal-
ysis. LFG assumes a version of Chomsky?s Uni-
versal Grammar hypothesis, namely that all lan-
guages are structured by similar underlying prin-
ciples (Chomsky, 1988; Chomsky, 1995). Within
LFG, f-structures encode a language universal
level of syntactic analysis, allowing for crosslin-
guistic parallelism at this level of abstraction. In
contrast, c-structures encode language particular
differences in linear word order, surface morpho-
logical vs. syntactic structures, and constituency
(Dalrymple, 2001). Thus, while the Chomskyan
framework is derivational in nature, LFG departs
from this view by embracing a strictly representa-
tional approach to syntax.
ParGram tests the LFG formalism for its uni-
versality and coverage limitations to see how far
parallelism can be maintained across languages.
Where possible, analyses produced by the gram-
mars for similar constructions in each language are
parallel, with the computational advantage that the
grammars can be used in similar applications and
that machine translation can be simplified.
552
The ParGram project regulates the features and
values used in its grammars. Since its inception
in 1996, ParGram has included a ?feature com-
mittee?, which collaboratively determines norms
for the use and definition of a common multilin-
gual feature and analysis space. Adherence to fea-
ture committee decisions is supported technically
by a routine that checks the grammars for com-
patibility with a feature declaration (King et al,
2005); the feature space for each grammar is in-
cluded in ParGramBank. ParGram also conducts
regular meetings to discuss constructions, analy-
ses and features.
For example, Figure 1 shows the c-structure
of the Urdu sentence in (1) and the c-structure
of its English translation. Figure 2 shows the f-
structures for the same sentences. The left/upper
c- and f-structures show the parse from the En-
glish ParGram grammar, the right/lower ones from
Urdu ParGram grammar.4,5 The c-structures en-
code linear word order and constituency and thus
look very different; e.g., the English structure is
rather hierarchical while the Urdu structure is flat
(Urdu is a free word-order language with no evi-
dence for a VP; Butt (1995)). The f-structures, in
contrast, are parallel aside from grammar-specific
characteristics such as the absence of grammati-
cal gender marking in English and the absence of
articles in Urdu.6
(1) ? Aj J
K. Q?K
QK A 	JK @ ?

	
G
	
?A??
kisAn=nE apnA
farmer.M.Sg=Erg self.M.Sg
TrEkTar bEc-A
tractor.M.Sg sell-Perf.M.Sg
?Did the farmer sell his tractor??
With parallel analyses and parallel features, maxi-
mal parallelism across typologically different lan-
guages is maintained. As a result, during the con-
struction of the treebank, post-processing and con-
version efforts are kept to a minimum.
4The Urdu ParGram grammar makes use of a translitera-
tion scheme that abstracts away from the Arabic-based script;
the transliteration scheme is detailed in Malik et al (2010).
5In the c-structures, dotted lines indicate distinct func-
tional domains; e.g., in Figure 1, the NP the farmer and the
VP sell his tractor belong to different f-structures: the former
maps onto the SUBJ f-structure, while the latter maps onto the
topmost f-structure (Dyvik et al, 2009). Section 6 elaborates
on functional domains.
6The CASE feature also varies: since English does not
distinguish between accusative, dative, and other oblique
cases, the OBJ is marked with a more general obl CASE.
Figure 1: English and Urdu c-structures
We emphasize the fact that ParGramBank is
characterized by a maximally reliable, human-
controlled and linguistically deep parallelism
across aligned sentences. Generally, the result of
automatic sentence alignment procedures are par-
allel corpora where the corresponding sentences
normally have the same purported meaning as
intended by the translator, but they do not nec-
essarily match in terms of structural expression.
In building ParGramBank, conscious attention is
paid to maintaining semantic and constructional
parallelism as much as possible. This design fea-
ture renders our treebank reliable in cases when
the constructional parallelism is reduced even at f-
structure. For example, typological variation in the
presence or absence of finite passive constructions
represents a case of potential mismatch. Hungar-
ian, one of the treebank languages, has no produc-
tive finite passives. The most common strategy in
translation is to use an active construction with a
topicalized object, with no overt subject and with
3PL verb agreement:
(2) A fa?-t ki-va?g-t-a?k.
the tree-ACC out-cut-PAST-3PL
?The tree was cut down.?
In this case, a topicalized object in Hungarian has
to be aligned with a (topical) subject in English.
Given that both the sentence level and the phrase
level alignments are human-controlled in the tree-
bank (see sections 4 and 6), the greatest possible
parallelism is reliably captured even in such cases
of relative grammatical divergence.
553
Figure 2: Parallel English and Urdu f-structures
4 Treebank Design and Construction
For the initial seeding of the treebank, we focused
on 50 sentences which were constructed manu-
ally to cover a diverse range of phenomena (tran-
sitivity, voice alternations, interrogatives, embed-
ded clauses, copula constructions, control/raising
verbs, etc.). We followed Lehmann et al (1996)
and Bender et al (2011) in using coverage of
grammatical constructions as a key component for
grammar development. (3) lists the first 16 sen-
tences of the treebank. An expansion to 100 sen-
tences is scheduled for next year.
(3) a. Declaratives:
1. The driver starts the tractor.
2. The tractor is red.
b. Interrogatives:
3. What did the farmer see?
4. Did the farmer sell his tractor?
c. Imperatives:
5. Push the button.
6. Don?t push the button.
d. Transitivity:
7. The farmer gave his neighbor an old
tractor.
8. The farmer cut the tree down.
9. The farmer groaned.
e. Passives and traditional voice:
10. My neighbor was given an old tractor
by the farmer.
11. The tree was cut down yesterday.
12. The tree had been cut down.
13. The tractor starts with a shudder.
f. Unaccusative:
14. The tractor appeared.
g. Subcategorized declaratives:
15. The boy knows the tractor is red.
16. The child thinks he started the tractor.
The sentences were translated from English
into the other treebank languages. Currently, these
languages are: English, Georgian, German, Hun-
garian, Indonesian, Norwegian (Bokma?l), Polish,
Turkish, Urdu and Wolof. The translations were
done by ParGram grammar developers (i.e., expert
linguists and native speakers).
The sentences were automatically parsed with
ParGram grammars using XLE. Since the pars-
ing was performed sentence by sentence, our re-
sulting treebank is automatically aligned at the
sentence level. The resulting c- and f-structures
were banked in a database using the LFG Parse-
banker (Rose?n et al, 2009). The structures were
disambiguated either prior to banking using XLE
or during banking with the LFG Parsebanker and
its discriminant-based disambiguation technique.
The banked analyses can be exported and down-
loaded in a Prolog format using the LFG Parse-
banker interface. Within XLE, we automatically
convert the structures to a simple XML format and
make these available via ParGramBank as well.
The Prolog format is used with applications
which use XLE to manipulate the structures, e.g.
for further semantic processing (Crouch and King,
2006) or for sentence condensation (Crouch et al,
2004).
554
5 Challenges for Parallelism
We detail some challenges in maintaining paral-
lelism across typologically distinct languages.
5.1 Complex Predicates
Some languages in ParGramBank make extensive
use of complex predicates. For example, Urdu uses
a combination of predicates to express concepts
that in languages like English are expressed with
a single verb, e.g., ?memory do? = ?remember?,
?fear come? = ?fear?. In addition, verb+verb com-
binations are used to express permissive or as-
pectual relations. The strategy within ParGram is
to abstract away from the particular surface mor-
phosyntactic expression and aim at parallelism
at the level of f-structure. That is, monoclausal
predications are analyzed via a simple f-structure
whether they consist of periphrastically formed
complex predicates (Urdu, Figure 3), a simple
verb (English, Figure 4), or a morphologically de-
rived form (Turkish, Figure 5).
In Urdu and in Turkish, the top-level PRED
is complex, indicating a composed predicate. In
Urdu, this reflects the noun-verb complex predi-
cate sTArT kar ?start do?, in Turkish it reflects a
morphological causative. Despite this morphosyn-
tactic complexity, the overall dependency struc-
ture corresponds to that of the English simple verb.
(4) ?


?
f
A

KQ ? HPA

J ? ? ? Q

 ? K
Q

K P?

J K
 @P

X
DrAIvar TrEkTar=kO
driver.M.Sg.Nom tractor.M.Sg=Acc
sTArT kartA hE
start.M.Sg do.Impf.M.Sg be.Pres.3Sg
?The driver starts the tractor.?
(5) su?ru?cu? trakto?r-u? c?al?s?-t?r-?yor
driver.Nom tractor-Acc work-Caus-Prog.3Sg
?The driver starts the tractor.?
The f-structure analysis of complex predicates
is thus similar to that of languages which do not
use complex predicates, resulting in a strong syn-
tactic parallelism at this level, even across typo-
logically diverse languages.
5.2 Negation
Negation also has varying morphosyntactic sur-
face realizations. The languages in ParGramBank
differ with respect to their negation strategies.
Languages such as English and German use inde-
pendent negation: they negate using words such as
Figure 3: Complex predicate: Urdu analysis of (4)
Figure 4: Simple predicate: English analysis of (4)
adverbs (English not, German nicht) or verbs (En-
glish do-support). Other languages employ non-
independent, morphological negation techniques;
Turkish, for instance, uses an affix on the verb, as
in (6).
555
Figure 5: Causative: Turkish analysis of (5)
(6) du?g?me-ye bas-ma
button-Dat push-Neg.Imp
?Don?t push the button.?
Within ParGram we have not abstracted away
from this surface difference. The English not in
(6) functions as an adverbial adjunct that modifies
the main verb (see top part of Figure 6) and infor-
mation would be lost if this were not represented
at f-structure. However, the same cannot be said of
the negative affix in Turkish ? the morphological
affix is not an adverbial adjunct. We have there-
fore currently analyzed morphological negation as
adding a feature to the f-structure which marks the
clause as negative, see bottom half of Figure 6.
5.3 Copula Constructions
Another challenge to parallelism comes from co-
pula constructions. An approach advocating a uni-
form treatment of copulas crosslinguistically was
advocated in the early years of ParGram (Butt et
al., 1999b), but this analysis could not do justice to
the typological variation found with copulas. Par-
GramBank reflects the typological difference with
three different analyses, with each language mak-
ing a language-specific choice among the three
possibilities that have been identified (Dalrymple
et al, 2004; Nordlinger and Sadler, 2007; Attia,
2008; Sulger, 2011; Laczko?, 2012).
The possible analyses are demonstrated here
with respect to the sentence The tractor is red.
The English grammar (Figure 7) uses a raising ap-
proach that reflects the earliest treatments of cop-
ulas in LFG (Bresnan, 1982). The copula takes
a non-finite complement whose subject is raised
to the matrix clause as a non-thematic subject of
the copula. In contrast, in Urdu (Figure 8), the
Figure 6: Different f-structural analyses for nega-
tion (English vs. Turkish)
copula is a two-place predicate, assigning SUBJ
and PREDLINK functions. The PREDLINK function
is interpreted as predicating something about the
subject. Finally, in languages like Indonesian (Fig-
ure 9), there is no overt copula and the adjective is
the main predicational element of the clause.
Figure 7: English copula example
556
Figure 8: Urdu copula example
Figure 9: Indonesian copula example
5.4 Summary
This section discussed some challenges for main-
taining parallel analyses across typologically di-
verse languages. Another challenge we face is
when no corresponding construction exists in a
language, e.g. with impersonals as in the English
It is raining. In this case, we provide a translation
and an analysis of the structure of the correspond-
ing translation, but note that the phenomenon be-
ing exemplified does not actually exist in the lan-
guage. A further extension to the capabilities of
the treebank could be the addition of pointers from
the alternative structure used in the translation to
the parallel aligned set of sentences that corre-
spond to this alternative structure.
6 Linguistically Motivated Alignment
The treebank is automatically aligned on the sen-
tence level, the top level of alignment within Par-
GramBank. For phrase-level alignments, we use
the drag-and-drop alignment tool in the LFG Parse-
banker (Dyvik et al, 2009). The tool allows the
alignment of f-structures by dragging the index
of a subsidiary source f-structure onto the index
of the corresponding target f-structure. Two f-
structures correspond if they have translationally
matching predicates, and the arguments of each
predicate correspond to an argument or adjunct in
the other f-structure. The tool automatically com-
putes the alignment of c-structure nodes on the
basis of the manually aligned corresponding f-
structures.7
7Currently we have not measured inter-annotator agree-
ment (IAA) for the f-structure alignments. The f-structure
alignments were done by only one person per language pair.
We anticipate that multiple annotators will be needed for this
This method is possible because the c-structure
to f-structure correspondence (the ? relation) is
encoded in the ParGramBank structures, allow-
ing the LFG Parsebanker tool to compute which c-
structure nodes contributed to a given f-structure
via the inverse (??1) mapping. A set of nodes
mapping to the same f-structure is called a ?func-
tional domain?. Within a source and a target
functional domain, two nodes are automatically
aligned only if they dominate corresponding word
forms. In Figure 10 the nodes in each func-
tional domain in the trees are connected by whole
lines while dotted lines connect different func-
tional domains. Within a functional domain, thick
whole lines connect the nodes that share align-
ment; for simplicity the alignment is only indi-
cated for the top nodes. The automatically com-
puted c-structural alignments are shown by the
curved lines. The alignment information is stored
as an additional layer and can be used to ex-
plore alignments at the string (word), phrase (c-
)structure, and functional (f-)structure levels.
We have so far aligned the treebank pairs
English-Urdu, English-German, English-Polish
and Norwegian-Georgian. As Figure 10 illustrates
for (7) in an English-Urdu pairing, the English ob-
ject neighbor is aligned with the Urdu indirect ob-
ject (OBJ-GO) hamsAyA ?neighbor?, while the En-
glish indirect object (OBJ-TH) tractor is aligned
with the Urdu object TrEkTar ?tractor?. The c-
structure correspondences were computed auto-
matically from the f-structure alignments.
(7) AK
X Q?K
QK A 	K @QK ?? ?
G
A???f ?

	
?K @ ?

	
G
	
?A??
kisAn=nE apnE
farmer.M.Sg=Erg self.Obl
hamsAyE=kO purAnA
neighbor.M.Sg.Obl=Acc old.M.Sg
TrEkTar di-yA
tractor.M.Sg give-Perf.M.Sg
?The farmer gave his neighbor an old tractor.?
The INESS platform additionally allows for the
highlighting of connected nodes via a mouse-over
technique. It thus provides a powerful and flexible
tool for the semi-automatic alignment and subse-
task in the future, in which case we will measure IAA for this
step.
557
Figure 10: Phrase-aligned treebank example English-Urdu: The farmer gave his neighbor an old tractor.
quent inspection of parallel treebanks which con-
tain highly complex linguistic structures.8
7 Discussion and Future Work
We have discussed the construction of ParGram-
Bank, a parallel treebank for ten typologically
different languages. The analyses in ParGram-
Bank are the output of computational LFG Par-
Gram grammars. As a result of ParGram?s cen-
trally agreed upon feature sets and prototypical
analyses, the representations are not only deep
in nature, but maximally parallel. The representa-
tions offer information about dependency relations
as well as word order, constituency and part-of-
speech.
In future ParGramBank releases, we will pro-
vide more theory-neutral dependencies along with
the LFG representations. This will take the form of
triples (King et al, 2003). We also plan to provide
a POS-tagged and a named entity marked up ver-
sion of the sentences; these will be of use for more
general NLP applications and for systems which
use such markup as input to deeper processing.
8One reviewer inquires about possibilities of linking
(semi-)automatically between languages, for example using
lexical resources such as WordNets or Panlex. We agree that
this would be desirable, but unrealizable, since many of the
languages included in ParGramBank do not have a WordNet
resource and are not likely to achieve an adequate one soon.
Third, the treebank will be expanded to include
100 more sentences within the next year. We also
plan to include more languages as other ParGram
groups contribute structures to ParGramBank.
ParGramBank, including its multilingual sen-
tences and all annotations, is made freely avail-
able for research and commercial use under the
CC-BY 3.0 license via the INESS platform, which
supports alignment methodology developed in the
XPAR project and provides search and visualiza-
tion methods for parallel treebanks. We encourage
the computational linguistics community to con-
tribute further layers of annotation, including se-
mantic (Crouch and King, 2006), abstract knowl-
edge representational (Bobrow et al, 2007), Prop-
Bank (Palmer et al, 2005), or TimeBank (Mani
and Pustejovsky, 2004) annotations.
References
Mohammed Attia. 2008. A Unified Analysis of Cop-
ula Constructions. In Proceedings of the LFG ?08
Conference, pages 89?108. CSLI Publications.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2011. Grammar Engineering and Linguistic Hy-
pothesis Testing: Computational Support for Com-
plexity in Syntactic Analysis. In Emily M. Bender
and Jennifer E. Arnold, editors, Languages from a
Cognitive Perspective: Grammar, Usage and Pro-
cessing, pages 5?30. CSLI Publications.
558
Daniel G. Bobrow, Cleo Condoravdi, Dick Crouch,
Valeria de Paiva, Lauri Karttunen, Tracy Holloway
King, Rowan Nairn, Lottie Price, and Annie Zaenen.
2007. Precision-focused Textual Inference. In Pro-
ceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Cristina Bosco, Manuela Sanguinetti, and Leonardo
Lesmo. 2012. The Parallel-TUT: a multilingual and
multiformat treebank. In Proceedings of the Eighth
International Conference on Language Resources
and Evaluation (LREC-2012), pages 1932?1938, Is-
tanbul, Turkey. European Language Resources As-
sociation (ELRA).
Joan Bresnan. 1982. The Passive in Lexical Theory. In
Joan Bresnan, editor, The Mental Representation of
Grammatical Relations, pages 3?86. The MIT Press.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell Publishing.
Miriam Butt, Stefanie Dipper, Anette Frank, and
Tracy Holloway King. 1999a. Writing Large-
Scale Parallel Grammars for English, French and
German. In Proceedings of the LFG99 Conference.
CSLI Publications.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nin?o, and Fre?de?rique Segond. 1999b. A Grammar
Writer?s Cookbook. CSLI Publications.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project. In Proceedings of the
COLING-2002 Workshop on Grammar Engineering
and Evaluation, pages 1?7.
Miriam Butt. 1995. The Structure of Complex Predi-
cates in Urdu. CSLI Publications.
Noam Chomsky. 1988. Lectures on Government and
Binding: The Pisa Lectures. Foris Publications.
Noam Chomsky. 1995. The Minimalist Program. MIT
Press.
Dick Crouch and Tracy Holloway King. 2006. Seman-
tics via F-structure Rewriting. In Proceedings of the
LFG06 Conference, pages 145?165. CSLI Publica-
tions.
Dick Crouch, Tracy Holloway King, John T. Maxwell
III, Stefan Riezler, and Annie Zaenen. 2004. Ex-
ploiting F-structure Input for Sentence Condensa-
tion. In Proceedings of the LFG04 Conference,
pages 167?187. CSLI Publications.
Dick Crouch, Mary Dalrymple, Ronald M. Kaplan,
Tracy Holloway King, John T. Maxwell III, and
Paula Newman, 2012. XLE Documentation. Palo
Alto Research Center.
Mary Dalrymple, Helge Dyvik, and Tracy Holloway
King. 2004. Copular Complements: Closed or
Open? In Proceedings of the LFG ?04 Conference,
pages 188?198. CSLI Publications.
Mary Dalrymple. 2001. Lexical Functional Gram-
mar, volume 34 of Syntax and Semantics. Academic
Press.
Helge Dyvik, Paul Meurer, Victoria Rose?n, and Koen-
raad De Smedt. 2009. Linguistically Motivated Par-
allel Parsebanks. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8), pages 71?82, Milan, Italy. EDU-
Catt.
Dan Flickinger, Valia Kordoni, Yi Zhang, Anto?nio
Branco, Kiril Simov, Petya Osenova, Catarina Car-
valheiro, Francisco Costa, and Se?rgio Castro. 2012.
ParDeepBank: Multiple Parallel Deep Treebank-
ing. In Proceedings of the 11th International Work-
shop on Treebanks and Linguistic Theories (TLT11),
pages 97?107, Lisbon. Edic?o?es Colibri.
Tracy Holloway King, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ronald Kaplan. 2003. The
PARC700 Dependency Bank. In Proceedings of the
EACL03: 4th International Workshop on Linguisti-
cally Interpreted Corpora (LINC-03).
Tracy Holloway King, Martin Forst, Jonas Kuhn, and
Miriam Butt. 2005. The Feature Space in Paral-
lel Grammar Writing. In Emily M. Bender, Dan
Flickinger, Frederik Fouvry, and Melanie Siegel, ed-
itors, Research on Language and Computation: Spe-
cial Issue on Shared Representation in Multilingual
Grammar Engineering, volume 3, pages 139?163.
Springer.
Natalia Klyueva and David Marec?ek. 2010. To-
wards a Parallel Czech-Russian Dependency Tree-
bank. In Proceedings of the Workshop on Anno-
tation and Exploitation of Parallel Corpora, Tartu.
Northern European Association for Language Tech-
nology (NEALT).
Jonas Kuhn and Michael Jellinghaus. 2006. Multilin-
gual Parallel Treebanking: A Lean and Flexible Ap-
proach. In Proceedings of the LREC 2006, Genoa,
Italy. ELRA/ELDA.
Tibor Laczko?. 2012. On the (Un)Bearable Lightness
of Being an LFG Style Copula in Hungarian. In Pro-
ceedings of the LFG12 Conference, pages 341?361.
CSLI Publications.
Sabine Lehmann, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herve? Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP ?
Test Suites for Natural Language Processing. In
Proceedings of COLING, pages 711 ? 716.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian
Sulger, Tina Bo?gel, Atif Gulzar, Ghulam Raza, Sar-
mad Hussain, and Miriam Butt. 2010. Transliter-
ating Urdu for a Broad-Coverage Urdu/Hindi LFG
Grammar. In Proceedings of the Seventh Con-
ference on International Language Resources and
Evaluation (LREC 2010), Valletta, Malta.
559
Inderjeet Mani and James Pustejovsky. 2004. Tem-
poral Discourse Models for Narrative Structure. In
Proceedings of the 2004 ACL Workshop on Dis-
course Annotation, pages 57?64.
Bea?ta Megyesi, Bengt Dahlqvist, E?va A?. Csato?, and
Joakim Nivre. 2010. The English-Swedish-Turkish
Parallel Treebank. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC?10), Valletta, Malta. Euro-
pean Language Resources Association (ELRA).
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective Sharing for Multilingual Depen-
dency Parsing. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 629?637,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Joakim Nivre, Igor Boguslavsky, and Leonid Iomdin.
2008. Parsing the SynTagRus Treebank. In Pro-
ceedings of COLING08, pages 641?648.
Rachel Nordlinger and Louisa Sadler. 2007. Verb-
less Clauses: Revealing the Structure within. In An-
nie Zaenen, Jane Simpson, Tracy Holloway King,
Jane Grimshaw, Joan Maling, and Chris Manning,
editors, Architectures, Rules and Preferences: A
Festschrift for Joan Bresnan, pages 139?160. CSLI
Publications.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: Modular NLP Framework. In Proceedings
of the 7th International Conference on Advances in
Natural Language Processing (IceTAL 2010), pages
293?304.
Victoria Rose?n, Paul Meurer, and Koenraad de Smedt.
2009. LFG Parsebanker: A Toolkit for Building and
Searching a Treebank as a Parsed Corpus. In Pro-
ceedings of the 7th International Workshop on Tree-
banks and Linguistic Theories (TLT7), pages 127?
133, Utrecht. LOT.
Victoria Rose?n, Koenraad De Smedt, Paul Meurer, and
Helge Dyvik. 2012. An Open Infrastructure for Ad-
vanced Treebanking. In META-RESEARCH Work-
shop on Advanced Treebanking at LREC2012, pages
22?29, Istanbul, Turkey.
Manuela Sanguinetti and Cristina Bosco. 2011. Build-
ing the Multilingual TUT Parallel Treebank. In Pro-
ceedings of Recent Advances in Natural Language
Processing, pages 19?28.
Sebastian Sulger. 2011. A Parallel Analysis of have-
Type Copular Constructions in have-Less Indo-
European Languages. In Proceedings of the LFG
?11 Conference. CSLI Publications.
Josef van Genabith and Dick Crouch. 1996. Direct and
Underspecified Interpretations of LFG f-structures.
In Proceedings of the 16th International Conference
on Computational Linguistics (COLING-96), vol-
ume 1, pages 262?267, Copenhagen, Denmark.
Martin Volk, Anne Go?hring, Torsten Marek,
and Yvonne Samuelsson. 2010. SMUL-
TRON (version 3.0) ? The Stock-
holm MULtilingual parallel TReebank.
http://www.cl.uzh.ch/research/paralleltreebanks en.
html.
560
