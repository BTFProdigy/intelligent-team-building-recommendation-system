Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94?99,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx? , Su Nam Kim? , Zornitsa Kozareva? , Preslav Nakov? ,
Diarmuid O? Se?aghdha?, Sebastian Pado?? , Marco Pennacchiotti??,
Lorenza Romano??, Stan Szpakowicz??
Abstract
We present a brief overview of the main
challenges in the extraction of semantic
relations from English text, and discuss the
shortcomings of previous data sets and shared
tasks. This leads us to introduce a new
task, which will be part of SemEval-2010:
multi-way classification of mutually exclusive
semantic relations between pairs of common
nominals. The task is designed to compare
different approaches to the problem and to
provide a standard testbed for future research,
which can benefit many applications in
Natural Language Processing.
1 Introduction
The computational linguistics community has a con-
siderable interest in robust knowledge extraction,
both as an end in itself and as an intermediate step
in a variety of Natural Language Processing (NLP)
applications. Semantic relations between pairs of
words are an interesting case of such semantic
knowledge. It can guide the recovery of useful facts
about the world, the interpretation of a sentence, or
even discourse processing. For example, pears and
bowl are connected in a CONTENT-CONTAINER re-
lation in the sentence ?The bowl contained apples,
?University of Antwerp, iris.hendrickx@ua.ac.be
?University of Melbourne, snkim@csse.unimelb.edu.au
?University of Alicante, zkozareva@dlsi.ua.es
?National University of Singapore, nakov@comp.nus.edu.sg
?University of Cambridge, do242@cl.cam.ac.uk
?University of Stuttgart, pado@stanford.edu
??Yahoo! Inc., pennacc@yahoo-inc.com
??Fondazione Bruno Kessler, romano@fbk.eu
??University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
pears, and oranges.?, while ginseng and taste are in
an ENTITY-ORIGIN relation in ?The taste is not from
alcohol, but from the ginseng.?.
The automatic recognition of semantic relations
can have many applications, such as information
extraction (IE), document summarization, machine
translation, or construction of thesauri and seman-
tic networks. It can also facilitate auxiliary tasks
such as word sense disambiguation, language mod-
eling, paraphrasing or recognizing textual entail-
ment. For example, semantic network construction
can benefit from detecting a FUNCTION relation be-
tween airplane and transportation in ?the airplane
is used for transportation? or a PART-WHOLE rela-
tion in ?the car has an engine?. Similarly, all do-
mains that require deep understanding of text rela-
tions can benefit from knowing the relations that de-
scribe events like ACQUISITION between named en-
tities in ?Yahoo has made a definitive agreement to
acquire Flickr?.
In this paper, we focus on the recognition of se-
mantic relations between pairs of common nomi-
nals. We present a task which will be part of the
SemEval-2010 evaluation exercise and for which we
are developing a new benchmark data set. This data
set and the associated task address three significant
problems encountered in previous work: (1) the def-
inition of a suitable set of relations; (2) the incorpo-
ration of context; (3) the desire for a realistic exper-
imental design. We outline these issues in Section
2. Section 3 describes the inventory of relations we
adopted for the task. The annotation process, the
design of the task itself and the evaluation method-
ology are presented in Sections 4-6.
94
2 Semantic Relation Classification: Issues
2.1 Defining the Relation Inventory
A wide variety of relation classification schemes ex-
ist in the literature, reflecting the needs and granular-
ities of various applications. Some researchers only
investigate relations between named entities or in-
ternal to noun-noun compounds, while others have a
more general focus. Some schemes are specific to a
domain such as biomedical text.
Rosario and Hearst (2001) classify noun com-
pounds from the domain of medicine into 13 classes
that describe the semantic relation between the head
noun and the modifier. Rosario et al (2002) classify
noun compounds using the MeSH hierarchy and a
multi-level hierarchy of semantic relations, with 15
classes at the top level. Stephens et al (2001) pro-
pose 17 very specific classes targeting relations be-
tween genes. Nastase and Szpakowicz (2003) ad-
dress the problem of classifying noun-modifier rela-
tions in general text. They propose a two-level hier-
archy, with 5 classes at the first level and 30 classes
at the second one; other researchers (Kim and Bald-
win, 2005; Nakov and Hearst, 2008; Nastase et al,
2006; Turney, 2005; Turney and Littman, 2005)
have used their class scheme and data set. Moldovan
et al (2004) propose a 35-class scheme to classify
relations in various phrases; the same scheme has
been applied to noun compounds and other noun
phrases (Girju et al, 2005). Lapata (2002) presents a
binary classification of relations in nominalizations.
Pantel and Pennacchiotti (2006) concentrate on five
relations in an IE-style setting. In short, there is little
agreement on relation inventories.
2.2 The Role of Context
A fundamental question in relation classification is
whether the relations between nominals should be
considered out of context or in context. When one
looks at real data, it becomes clear that context does
indeed play a role. Consider, for example, the noun
compound wood shed : it may refer either to a shed
made of wood, or to a shed of any material used to
store wood. This ambiguity is likely to be resolved
in particular contexts. In fact, most NLP applica-
tions will want to determine not all possible relations
between two words, but rather the relation between
two instances in a particular context. While the in-
tegration of context is common in the field of IE (cf.
work in the context of ACE1), much of the exist-
ing literature on relation extraction considers word
pairs out of context (thus, types rather than tokens).
A notable exception is SemEval-2007 Task 4 Clas-
sification of Semantic Relations between Nominals
(Girju et al, 2007; Girju et al, 2008), the first to of-
fer a standard benchmark data set for seven semantic
relations between common nouns in context.
2.3 Style of Classification
The design of SemEval-2007 Task 4 had an im-
portant limitation. The data set avoided the chal-
lenge of defining a single unified standard classifi-
cation scheme by creating seven separate training
and test sets, one for each semantic relation. That
made the relation recognition task on each data set
a simple binary (positive / negative) classification
task.2 Clearly, this does not easily transfer to prac-
tical NLP settings, where any relation can hold be-
tween a pair of nominals which occur in a sentence
or a discourse.
2.4 Summary
While there is a substantial amount of work on re-
lation extraction, the lack of standardization makes
it difficult to compare different approaches. It is
known from other fields that the availability of stan-
dard benchmark data sets can provide a boost to the
advancement of a field. As a first step, SemEval-
2007 Task 4 offered many useful insights into the
performance of different approaches to semantic re-
lation classification; it has also motivated follow-
up research (Davidov and Rappoport, 2008; Ka-
trenko and Adriaans, 2008; Nakov and Hearst, 2008;
O? Se?aghdha and Copestake, 2008).
Our objective is to build on the achievements of
SemEval-2007 Task 4 while addressing its short-
comings. In particular, we consider a larger set of
semantic relations (9 instead of 7), we assume a
proper multi-class classification setting, we emulate
the effect of an ?open? relation inventory by means
of a tenth class OTHER, and we will release to the
research community a data set with a considerably
1http://www.itl.nist.gov/iad/mig/tests/
ace/
2Although it was not designed for a multi-class set-up, some
subsequent publications tried to use the data sets in that manner.
95
larger number of examples than SemEval-2007 Task
4 or other comparable data sets. The last point is cru-
cial for ensuring the robustness of the performance
estimates for competing systems.
3 Designing an Inventory of Semantic Re-
lations Between Nominals
We begin by considering the first of the problems
listed above: defining of an inventory of semantic
relations. Ideally, it should be exhaustive (should al-
low the description of relations between any pair of
nominals) and mutually exclusive (each pair of nom-
inals in context should map onto only one relation).
The literature, however, suggests no such inventory
that could satisfy all needs. In practice, one always
must decide on a trade-off between these two prop-
erties. For example, the gene-gene relation inven-
tory of Stephens et al (2001), with relations like X
phosphorylates Y, arguably allows no overlaps, but
is too specific for applications to general text.
On the other hand, schemes aimed at exhaus-
tiveness tend to run into overlap issues, due
to such fundamental linguistic phenomena as
metaphor (Lakoff, 1987). For example, in the sen-
tence Dark clouds gather over Nepal., the relation
between dark clouds and Nepal is literally a type of
ENTITY-DESTINATION, but in fact it refers to the
ethnic unrest in Nepal.
We seek a pragmatic compromise between the
two extremes. We have selected nine relations with
sufficiently broad coverage to be of general and
practical interest. We aim at avoiding ?real? overlap
to the extent that this is possible, but we include two
sets of similar relations (ENTITY-ORIGIN/ENTITY-
DESTINATION and CONTENT-CONTAINER/COM-
PONENT-WHOLE/MEMBER-COLLECTION), which
can help assess the models? ability to make such
fine-grained distinctions.3
As in Semeval-2007 Task 4, we give ordered two-
word names to the relations, where each word de-
scribes the role of the corresponding argument. The
full list of our nine relations follows4 (the definitions
we show here are intended to be indicative rather
than complete):
3COMPONENT-WHOLE and MEMBER-COLLECTION are
proper subsets of PART-WHOLE, one of the relations in
SemEval-2007 Task 4.
4We have taken the first five from SemEval-2007 Task 4.
Cause-Effect. An event or object leads to an effect.
Example: Smoking causes cancer.
Instrument-Agency. An agent uses an instrument.
Example: laser printer
Product-Producer. A producer causes a product to
exist. Example: The farmer grows apples.
Content-Container. An object is physically stored
in a delineated area of space, the container. Ex-
ample: Earth is located in the Milky Way.
Entity-Origin. An entity is coming or is derived
from an origin (e.g., position or material). Ex-
ample: letters from foreign countries
Entity-Destination. An entity is moving towards a
destination. Example: The boy went to bed.
Component-Whole. An object is a component of a
larger whole. Example: My apartment has a
large kitchen.
Member-Collection. A member forms a nonfunc-
tional part of a collection. Example: There are
many trees in the forest.
Communication-Topic. An act of communication,
whether written or spoken, is about a topic. Ex-
ample: The lecture was about semantics.
We add a tenth element to this set, the pseudo-
relation OTHER. It stands for any relation which
is not one of the nine explicitly annotated relations.
This is motivated by modelling considerations. Pre-
sumably, the data for OTHER will be very nonho-
mogeneous. By including it, we force any model of
the complete data set to correctly identify the deci-
sion boundaries between the individual relations and
?everything else?. This encourages good generaliza-
tion behaviour to larger, noisier data sets commonly
seen in real-world applications.
3.1 Semantic Relations versus Semantic Roles
There are three main differences between our task
(classification of semantic relations between nomi-
nals) and the related task of automatic labeling of
semantic roles (Gildea and Jurafsky, 2002).
The first difference is to do with the linguistic
phenomena described. Lexical resources for theo-
ries of semantic roles such as FrameNet (Fillmore et
96
al., 2003) and PropBank (Palmer et al, 2005) have
been developed to describe the linguistic realization
patterns of events and states. Thus, they target pri-
marily verbs (or event nominalizations) and their de-
pendents, which are typically nouns. In contrast,
semantic relations may occur between all parts of
speech, although we limit our attention to nominals
in this task. Also, semantic role descriptions typi-
cally relate an event to a set of multiple participants
and props, while semantic relations are in practice
(although not necessarily) binary.
The second major difference is the syntactic con-
text. Theories of semantic roles usually developed
out of syntactic descriptions of verb valencies, and
thus they focus on describing the linking patterns of
verbs and their direct dependents, phenomena like
raising and noninstantiations notwithstanding (Fill-
more, 2002). Semantic relations are not tied to
predicate-argument structures. They can also be es-
tablished within noun phrases, noun compounds, or
sentences more generally (cf. the examples above).
The third difference is that of the level of gen-
eralization. FrameNet currently contains more than
825 different frames (event classes). Since the se-
mantic roles are designed to be interpreted at the
frame level, there is a priori a very large number
of unrelated semantic roles. There is a rudimen-
tary frame hierarchy that defines mappings between
roles of individual frames,5 but it is far from com-
plete. The situation is similar in PropBank. Prop-
Bank does use a small number of semantic roles, but
these are again to be interpreted at the level of in-
dividual predicates, with little cross-predicate gen-
eralization. In contrast, all of the semantic relation
inventories discussed in Section 1 contain fewer than
50 types of semantic relations. More generally, se-
mantic relation inventories attempt to generalize re-
lations across wide groups of verbs (Chklovski and
Pantel, 2004) and include relations that are not verb-
centered (Nastase and Szpakowicz, 2003; Moldovan
et al, 2004). Using the same labels for similar se-
mantic relations facilitates supervised learning. For
example, a model trained with examples of sell re-
lations should be able to transfer what it has learned
to give relations. This has the potential of adding
5For example, it relates the BUYER role of the COM-
MERCE SELL frame (verb sell ) to the RECIPIENT role of the
GIVING frame (verb give).
1. People in Hawaii might be feeling
<e1>aftershocks</e1> from that power-
ful <e2>earthquake</e2> for weeks.
2. My new <e1>apartment</e1> has a
<e2>large kitchen</e2>.
Figure 1: Two example sentences with annotation
crucial robustness and coverage to analysis tools in
NLP applications based on semantic relations.
4 Annotation
The next step in our study will be the actual annota-
tion of relations between nominals. For the purpose
of annotation, we define a nominal as a noun or a
base noun phrase. A base noun phrase is a noun and
its pre-modifiers (e.g., nouns, adjectives, determin-
ers). We do not include complex noun phrases (e.g.,
noun phrases with attached prepositional phrases or
relative clauses). For example, lawn is a noun, lawn
mower is a base noun phrase, and the engine of the
lawn mower is a complex noun phrase.
We focus on heads that are common nouns. This
emphasis distinguishes our task from much work in
IE, which focuses on named entities and on consid-
erably more fine-grained relations than we do. For
example, Patwardhan and Riloff (2007) identify cat-
egories like Terrorist organization as participants in
terror-related semantic relations, which consists pre-
dominantly of named entities. We feel that named
entities are a specific category of nominal expres-
sions best dealt with using techniques which do not
apply to common nouns; for example, they do not
lend themselves well to semantic generalization.
Figure 1 shows two examples of annotated sen-
tences. The XML tags <e1> and <e2> mark the
target nominals. Since all nine proper semantic re-
lations in this task are asymmetric, the ordering of
the two nominals must be taken into account. In
example 1, CAUSE-EFFECT(e1, e2) does not hold,
although CAUSE-EFFECT(e2, e1) would. In exam-
ple 2, COMPONENT-WHOLE(e2, e1) holds.
We are currently developing annotation guide-
lines for each of the relations. They will give a pre-
cise definition for each relation and some prototypi-
cal examples, similarly to SemEval-2007 Task 4.
The annotation will take place in two rounds. In
the first round, we will do a coarse-grained search
97
for positive examples for each relation. We will
collect data from the Web using a semi-automatic,
pattern-based search procedure. In order to ensure
a wide variety of example sentences, we will use
several dozen patterns per relation. We will also
ensure that patterns retrieve both positive and nega-
tive example sentences; the latter will help populate
the OTHER relation with realistic near-miss negative
examples of the other relations. The patterns will
be manually constructed following the approach of
Hearst (1992) and Nakov and Hearst (2008).6
The example collection for each relation R will
be passed to two independent annotators. In order to
maintain exclusivity of relations, only examples that
are negative for all relations but R will be included
as positive and only examples that are negative for
all nine relations will be included as OTHER. Next,
the annotators will compare their decisions and as-
sess inter-annotator agreement. Consensus will be
sought; if the annotators cannot agree on an exam-
ple it will not be included in the data set, but it will
be recorded for future analysis.
Finally, two other task organizers will look for
overlap across all relations. They will discard any
example marked as positive in two or more relations,
as well as examples in OTHER marked as positive in
any of the other classes. The OTHER relation will,
then, consist of examples that are negatives for all
other relations and near-misses for any relation.
Data sets. The annotated data will be divided into
a training set, a development set and a test set. There
will be 1000 annotated examples for each of the
ten relations: 700 for training, 100 for development
and 200 for testing. All data will be released under
the Creative Commons Attribution 3.0 Unported Li-
cense7. The annotation guidelines will be included
in the distribution.
5 The Classification Task
The actual task that we will run at SemEval-2010
will be a multi-way classification task. Not all pairs
of nominals in each sentence will be labeled, so the
gold-standard boundaries of the nominals to be clas-
sified will be provided as part of the test data.
6Note that, unlike in Semeval 2007 Task 4, we will not re-
lease the patterns to the participants.
7http://creativecommons.org/licenses/by/
3.0/
In contrast with Semeval 2007 Task 4, in which
the ordering of the entities was provided with each
example, we aim at a more realistic scenario in
which the ordering of the labels is not given. Par-
ticipants in the task will be asked to discover both
the relation and the order of the arguments. Thus,
the more challenging task is to identify the most
informative ordering and relation between a pair
of nominals. The stipulation ?most informative?
is necessary since with our current set of asym-
metrical relations that includes OTHER, each pair
of nominals that instantiates a relation in one di-
rection (e.g., REL(e1, e2)), instantiates OTHER in
the inverse direction (OTHER (e2, e1)). Thus, the
correct answers for the two examples in Figure 1
are CAUSE-EFFECT (earthquake, aftershocks) and
COMPONENT-WHOLE (large kitchen, apartment).
Note that unlike in SemEval-2007 Task 4, we will
not provide manually annotated WordNet senses,
thus making the task more realistic. WordNet senses
did, however, serve for disambiguation purposes in
SemEval-2007 Task 4. We will therefore have to
assess the effect of this change on inter-annotator
agreement.
6 Evaluation Methodology
The official ranking of the participating systems will
be based on their macro-averaged F-scores for the
nine proper relations. We will also compute and re-
port their accuracy over all ten relations, including
OTHER. We will further analyze the results quan-
titatively and qualitatively to gauge which relations
are most difficult to classify.
Similarly to SemEval-2007 Task 4, in order to
assess the effect of varying quantities of training
data, we will ask the teams to submit several sets of
guesses for the labels for the test data, using varying
fractions of the training data. We may, for example,
request test results when training on the first 50, 100,
200, 400 and all 700 examples from each relation.
We will provide a Perl-based automatic evalua-
tion tool that the participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
7 Conclusion
We have introduced a new task, which will be part of
SemEval-2010: multi-way classification of semantic
98
relations between pairs of common nominals. The
task will compare different approaches to the prob-
lem and provide a standard testbed for future re-
search, which can benefit many NLP applications.
The description we have presented here should
be considered preliminary. We invite the in-
terested reader to visit the official task web-
site http://semeval2.fbk.eu/semeval2.
php?location=tasks\#T11, where up-to-
date information will be published; there is also a
discussion group and a mailing list.
References
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. EMNLP 2004, pages 33?40.
Dmitry Davidov and Ari Rappoport. 2008. Classifica-
tion of semantic relationships between nominals using
pattern clusters. In Proc. ACL-08: HLT, pages 227?
235.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Charles J. Fillmore. 2002. FrameNet and the linking be-
tween semantic and syntactic relations. In Proc. COL-
ING 2002, pages 28?36.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Roxana Girju, Dan Moldovan, Marta Tatu, , and Dan An-
tohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19:479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 task 04: Classification of semantic re-
lations between nominals. In Proc. 4th Semantic Eval-
uation Workshop (SemEval-2007).
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2008.
Classification of semantic relations between nominals.
Language Resources and Evaluation. In print.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING
92, pages 539?545.
Sophia Katrenko and Pieter Adriaans. 2008. Semantic
types of some generic relation arguments: Detection
and evaluation. In Proc. ACL-08: HLT, Short Papers,
pages 185?188.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet sim-
ilarity. In Proc. IJCAI, pages 945?956.
George Lakoff. 1987. Women, fire, and dangerous
things. University of Chicago Press, Chicago, IL.
Maria Lapata. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28:357?388.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. In HLT-NAACL
2004: Workshop on Computational Lexical Semantics,
pages 60?67.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proc. ACL-08: HLT, pages 452?460.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,
and Stan Szpakowicz. 2006. Learning noun-modifier
semantic relations with corpus-based and WordNet-
based features. In Proc. AAAI, pages 781?787.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. COLING 2008, pages 649?656.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?106.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proc. COLING/ACL, pages
113?120.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
information extraction with semantic affinity patterns
and relevant regions. In Proc. EMNLP-CoNLL), pages
717?727.
Barbara Rosario and Marti Hearst. 2001. Classifying the
semantic relations in noun compounds via a domain-
specific lexical hierarchy. In Proc. EMNLP 2001,
pages 82?90.
Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002. The descent of hierarchy, and selection in re-
lational semantics. In Proc. ACL-02, pages 247?254.
Matthew Stephens, Mathew Palakal, Snehasis
Mukhopadhyay, Rajeev Raje, and Javed Mostafa.
2001. Detecting gene relations from Medline ab-
stracts. In Pacific Symposium on Biocomputing, pages
483?495.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1-3):251?278.
Peter D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. IJCAI, pages 1136?
1141.
99
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 141?144,
Prague, June 2007. c?2007 Association for Computational Linguistics
FBK-IRST: Kernel Methods for Semantic Relation Extraction
Claudio Giuliano and Alberto Lavelli and Daniele Pighin and Lorenza Romano
FBK-IRST, Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Povo (TN), ITALY
{giuliano,lavelli,pighin,romano}@itc.it
Abstract
We present an approach for semantic rela-
tion extraction between nominals that com-
bines shallow and deep syntactic processing
and semantic information using kernel meth-
ods. Two information sources are consid-
ered: (i) the whole sentence where the re-
lation appears, and (ii) WordNet synsets and
hypernymy relations of the candidate nom-
inals. Each source of information is rep-
resented by kernel functions. In particu-
lar, five basic kernel functions are linearly
combined and weighted under different con-
ditions. The experiments were carried out
using support vector machines as classifier.
The system achieves an overall F1 of 71.8%
on the Classification of Semantic Relations
between Nominals task at SemEval-2007.
1 Introduction
The starting point of our research is an approach
for identifying relations between named entities ex-
ploiting only shallow linguistic information, such as
tokenization, sentence splitting, part-of-speech tag-
ging and lemmatization (Giuliano et al, 2006). A
combination of kernel functions is used to represent
two distinct information sources: (i) the global con-
text where entities appear and (ii) their local con-
texts. The whole sentence where the entities appear
(global context) is used to discover the presence of
a relation between two entities. Windows of limited
size around the entities (local contexts) provide use-
ful clues to identify the roles played by the entities
within a relation (e.g., agent and target of a gene in-
teraction). In the task of detecting protein-protein
interactions, we obtained state-of-the-art results on
two biomedical data sets. In addition, promising re-
sults have been recently obtained for relations such
as work for and org based in in the news domain1.
In this paper, we investigate the use of the above
approach to discover semantic relations between
nominals. In addition to the original feature rep-
resentation, we have integrated deep syntactic pro-
cessing of the global context and semantic informa-
tion for each candidate nominals using WordNet as
external knowledge source. Each source of informa-
tion is represented by kernel functions. A tree kernel
(Moschitti, 2004) is used to exploit the deep syn-
tactic processing obtained using the Charniak parser
(Charniak, 2000). On the other hand, bag of syn-
onyms and hypernyms is used to enhance the repre-
sentation of the candidate nominals. The final sys-
tem is based on five basic kernel functions (bag-of-
words kernel, global context kernel, tree kernel, su-
persense kernel, bag of synonyms and hypernyms
kernel) linearly combined and weighted under dif-
ferent conditions. The experiments were carried out
using support vector machines (Vapnik, 1998) as
classifier.
We present results on the Classification of Seman-
tic Relations between Nominals task at SemEval-
2007, in which sentences containing ordered pairs
of marked nominals, possibly semantically related,
have to be classified. On this task, we achieve an
overall F1 of 71.8% (B category evaluation), largely
outperforming all the baselines.
1These results appear in a paper currently under revision.
141
2 Kernel Methods for Relation Extraction
In order to implement the approach based on syntac-
tic and semantic information, we employed a linear
weighted combination of kernels, using support vec-
tor machines as classifier. We designed two families
of basic kernels: syntactic kernels and semantic ker-
nels. These basic kernels are combined by exploit-
ing the closure properties of kernels. We define our
composite kernel KC(x1, x2) as follows
n
?
i=1
wi
Ki(x1, x2)
?
Ki(x1, x1)Ki(x2, x2)
, (1)
where each basic kernel Ki is normalized and wi ?
{0, 1} is the kernel weight. The normalization factor
plays an important role in allowing us to integrate in-
formation from heterogeneous knowledge sources.
All basic kernels, but the tree kernel (see Section
2.1.3), are explicitly calculated as follows
Ki(x1, x2) = ??(x1), ?(x2)?, (2)
where ?(?) is the embedding vector. Even though
the resulting feature space has high dimensionality,
an efficient computation of Equation 2 can be carried
out explicitly since the input representations defined
below are extremely sparse.
2.1 Syntactic Kernels
Syntactic kernels are defined over the whole sen-
tence where the candidate nominals appear.
2.1.1 Global Context Kernel
Bunescu and Mooney (2005) and Giuliano et al
(2006) successfully exploited the fact that relations
between named entities are generally expressed us-
ing only words that appear simultaneously in one of
the following three contexts.
Fore-Between Tokens before and between the two
entities, e.g. ?the head of [ORG], Dr. [PER]?.
Between Only tokens between the two entities, e.g.
?[ORG] spokesman [PER]?.
Between-After Tokens between and after the two
entities, e.g. ?[PER], a [ORG] professor?.
Here, we investigate whether this assumption is
also correct for semantic relations between nomi-
nals. Our global context kernel operates on the con-
texts defined above, where each context is repre-
sented using a bag-of-words. More formally, given
a) S1
S
NP
PRP
I
VP
VBD
found
NP
DT
some
NN
candy
PP
IN
in
NP
PRP$
my
NN
underwear
.
.
b) S
VP
VBD
found
NP
NNS
agent
PP
IN
in
NP
NN
target
Figure 1: A content-container relation test sentence
parse tree (a) and the corresponding RT structure (b).
a relation example R, we represent a context C as a
row vector
?C(R) = (tf(t1, C), tf(t2, C), . . . , tf(tl, C)) ? Rl, (3)
where the function tf(ti, C) records how many
times a particular token ti is used in C . Note that
this approach differs from the standard bag-of-words
as punctuation and stop words are included in ?C ,
while the nominals are not. To improve the classi-
fication performance, we have further extended ?C
to embed n-grams of (contiguous) tokens (up to n =
3). By substituting ?C into Equation 2, we obtain
the n-gram kernel Kn, which counts uni-grams, bi-
grams, . . . , n-grams that two patterns have in com-
mon2. The Global Context kernel KGC(R1, R2) is
then defined as
KF B(R1, R2) +KB(R1, R2) +KBA(R1, R2), (4)
where KFB , KB and KBA are n-gram kernels
that operate on the Fore-Between, Between and
Between-After patterns respectively.
2.1.2 Bag-of-Words Kernel
The bag-of-words kernel is defined as the previ-
ous kernel but it operates on the whole sentence.
2.1.3 Tree Kernel
Tree kernels can trigger automatic feature selec-
tion and represent a viable alternative to the man-
2In the literature, it is also called n-spectrum kernel.
142
ual design of attribute-value syntactic features (Mos-
chitti, 2004). A tree kernel KT (t1, t2) evaluates
the similarity between two trees t1 and t2 in terms
of the number of fragments they have in common.
Let Nt be the set of nodes of a tree t and F =
{f1, f2, . . . , f|F|} be the fragment space of t1 and
t2. Then
KT (t1, t2) =
P
ni?Nt1
P
nj?Nt2
?(ni, nj) , (5)
where ?(ni, nj) =
?|F|
k=1 Ik(ni) ? IK(nj) and
Ik(n) = 1 if k is rooted in n, 0 otherwise.
For this task, we defined an ad-hoc class of struc-
tured features (Moschitti et al, 2006), the Reduced
Tree (RT), which can be derived from a sentence
parse tree t by the following steps: (1) remove all the
terminal nodes but those labeled as relation entities
and those POS tagged as verbs, auxiliaries, prepo-
sitions, modals or adverbs; (2) remove all the in-
ternal nodes not covering any remaining terminal;
(3) replace the entity words with placeholders that
indicate the direction in which the relation should
hold. Figure 1 shows a parse tree and the resulting
RT structure.
2.2 Semantic Kernels
In (Giuliano et al, 2006), we used the local context
kernel to infer semantic information on the candi-
date entities (i.e., roles played by the entities). As
the task organizers provide the WordNet sense and
role for each nominal, we directly use this informa-
tion to enrich the feature space and do not include
the local context kernel in the combination.
2.2.1 Bag of Synonyms and Hypernyms Kernel
By using the WordNet sense key provided, each
nominal is represented by the bag of its synonyms
and hypernyms (direct and inherited hypernyms).
Formally, given a relation example R, each nominal
N is represented as a row vector
?N(R) = (f(t1, N), f(t2, N), . . . , f(tl, N)) ? Rl, (6)
where the binary function f(ti, N) records if a par-
ticular lemma ti is contained into the bag of syn-
onyms and hypernyms of N. The bag of synonyms
and hypernyms kernel KS&H(R1, R2) is defined as
Ktarget(R1, R2) +Kagent(R1, R2), (7)
where Ktarget and Kagent are defined by substitut-
ing the embedding of the target and agent nominals
into Equation 2 respectively.
2.2.2 Supersense Kernel
WordNet synsets are organized into 45 lexicogra-
pher files, based on syntactic category and logical
groupings. E.g., noun.artifact is for nouns denoting
man-made objects, noun.attribute for nouns denot-
ing attributes for people and objects etc. The super-
sense kernel KSS(R1, R2) is a variant of the previ-
ous kernel that uses the names of the lexicographer
files (i.e., the supersense) to index the feature space.
3 Experimental Setup and Results
Sentences have been tokenized, lemmatized, and
POS tagged with TextPro3. We considered each re-
lation as a different binary classification task, and
each sentence in the data set is a positive or negative
example for the relation. The direction of the rela-
tion is considered labelling the first argument of the
relation as agent and the second as target.
All the experiments were performed using the
SVM package SVMLight-TK4, customized to em-
bed our own kernels. We optimized the linear com-
bination weights wi and regularization parameter c
using 10-fold cross-validation on the training set.
We set the cost-factor j to be the ratio between the
number of negative and positive examples.
Table 1 shows the performance on the test set. We
achieve an overall F1 of 71.8% (B category evalua-
tion), largely outperforming all the baselines, rang-
ing from 48.5% to 57.0%. The average training plus
test running time for a relation is about 10 seconds
on a Intel Pentium M755 2.0 GHz. Figure 2 shows
the learning curves on the test set. For all relations
but theme-tool, accurate classifiers can be learned
using a small fraction of training.
4 Discussion and Conclusion
Experimental results show that our kernel-based ap-
proach is appropriate also to detect semantic rela-
tions between nominals. However, differently from
relation extraction between named entities, there is
not a common kernel setup for all relations. E.g.,
3http://tcc.itc.it/projects/textpro/
4http://ai-nlp.info.uniroma2.it/moschitti/
143
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 30  40  50  60  70  80  90  100
F 1
Percentage of Training
Learning Curve
Cause-Effect
Instrument-Agency
Product-Producer
Origin-Entity
Theme-Tool
Part-Whole
Content-Container
Figure 2: Learning curves on the test set.
Relation P R F1 Acc
Cause-Effect 67.3 90.2 77.1 72.5
Instrument-Agency 76.9 78.9 77.9 78.2
Product-Producer 76.2 77.4 76.8 68.8
Origin-Entity 62.2 63.9 63.0 66.7
Theme-Tool 69.2 62.1 65.5 73.2
Part-Whole 65.5 73.1 69.1 76.4
Content-Container 78.8 68.4 73.2 74.3
Avg 70.9 73.4 71.8 72.9
Table 1: Results on the test set.
for content-container we obtain the best perfor-
mance combining the tree kernel and the bag of syn-
onyms and hypernyms kernel; on the other hand, for
instrument-agency the best performance is obtained
by combining the global kernel and the supersense
kernel. Surprisingly, the supersense kernel alone
works quite well and obtains results comparable to
the bag of synonyms and hypernyms kernel. This
result is particularly interesting as a supersense tag-
ger can easily provide a satisfactory accuracy (Cia-
ramita and Altun, 2006). On the other hand, ob-
taining an acceptable accuracy in word sense disam-
biguation (required for a realistic application of the
bag of synonyms and hypernyms kernel) is imprac-
tical as a sufficient amount of training for at least all
nouns is currently not available. Hence, the super-
sense could play a crucial role to improve the perfor-
mance when approaching this task without the nomi-
nals disambiguated. To model the global context us-
ing the Fore-Between, Between and Between-After
contexts did not produce a significant improvement
with respect to the bag-of-words model. This is
mainly due to the fact that examples have been col-
lected from the Web using heuristic patterns/queries,
most of which implying Between patterns/contexts
(e.g., for the cause-effect relation ?* comes from *?,
?* out of *? etc.).
5 Acknowledgements
Claudio Giuliano, Alberto Lavelli and Lorenza Ro-
mano are supported by the X-Media project (http:
//www.x-media-project.org), sponsored
by the European Commission as part of the Infor-
mation Society Technologies (IST) programme un-
der EC grant number IST-FP6-026978.
References
Razvan Bunescu and Raymond J. Mooney. 2005. Subse-
quence kernels for relation extraction. In Proceedings
of the 19th Conference on Neural Information Pro-
cessing Systems, Vancouver, British Columbia.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 132?139, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594?602,
Sydney, Australia, July.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In Pro-
ceedings of the Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-2006), Trento, Italy, 5-7 April.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
joint inference. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
CoNLL-X.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages 335?
342, Barcelona, Spain, July.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, New York, NY.
144
Exploiting Shallow Linguistic Information for
Relation Extraction from Biomedical Literature
Claudio Giuliano and Alberto Lavelli and Lorenza Romano
ITC-irst
Via Sommarive, 18
38050, Povo (TN)
Italy
{giuliano,lavelli,romano}@itc.it
Abstract
We propose an approach for extracting re-
lations between entities from biomedical
literature based solely on shallow linguis-
tic information. We use a combination of
kernel functions to integrate two different
information sources: (i) the whole sen-
tence where the relation appears, and (ii)
the local contexts around the interacting
entities. We performed experiments on ex-
tracting gene and protein interactions from
two different data sets. The results show
that our approach outperforms most of the
previous methods based on syntactic and
semantic information.
1 Introduction
Information Extraction (IE) is the process of find-
ing relevant entities and their relationships within
textual documents. Applications of IE range from
Semantic Web to Bioinformatics. For example,
there is an increasing interest in automatically
extracting relevant information from biomedi-
cal literature. Recent evaluation campaigns on
bio-entity recognition, such as BioCreAtIvE and
JNLPBA 2004 shared task, have shown that sev-
eral systems are able to achieve good performance
(even if it is a bit worse than that reported on news
articles). However, relation identification is more
useful from an applicative perspective but it is still
a considerable challenge for automatic tools.
In this work, we propose a supervised machine
learning approach to relation extraction which is
applicable even when (deep) linguistic process-
ing is not available or reliable. In particular, we
explore a kernel-based approach based solely on
shallow linguistic processing, such as tokeniza-
tion, sentence splitting, Part-of-Speech (PoS) tag-
ging and lemmatization.
Kernel methods (Shawe-Taylor and Cristianini,
2004) show their full potential when an explicit
computation of the feature map becomes compu-
tationally infeasible, due to the high or even infi-
nite dimension of the feature space. For this rea-
son, kernels have been recently used to develop
innovative approaches to relation extraction based
on syntactic information, in which the examples
preserve their original representations (i.e. parse
trees) and are compared by the kernel function
(Zelenko et al, 2003; Culotta and Sorensen, 2004;
Zhao and Grishman, 2005).
Despite the positive results obtained exploiting
syntactic information, we claim that there is still
room for improvement relying exclusively on shal-
low linguistic information for two main reasons.
First of all, previous comparative evaluations put
more stress on the deep linguistic approaches and
did not put as much effort on developing effec-
tive methods based on shallow linguistic informa-
tion. A second reason concerns the fact that syn-
tactic parsing is not always robust enough to deal
with real-world sentences. This may prevent ap-
proaches based on syntactic features from produc-
ing any result. Another related issue concerns the
fact that parsers are available only for few lan-
guages and may not produce reliable results when
used on domain specific texts (as is the case of
the biomedical literature). For example, most of
the participants at the Learning Language in Logic
(LLL) challenge on Genic Interaction Extraction
(see Section 4.2) were unable to successfully ex-
ploit linguistic information provided by parsers. It
is still an open issue whether the use of domain-
specific treebanks (such as the Genia treebank1)
1http://www-tsujii.is.s.u-tokyo.ac.jp/
401
can be successfully exploited to overcome this
problem. Therefore it is essential to better investi-
gate the potential of approaches based exclusively
on simple linguistic features.
In our approach we use a combination of ker-
nel functions to represent two distinct informa-
tion sources: the global context where entities ap-
pear and their local contexts. The whole sentence
where the entities appear (global context) is used
to discover the presence of a relation between two
entities, similarly to what was done by Bunescu
and Mooney (2005b). Windows of limited size
around the entities (local contexts) provide use-
ful clues to identify the roles of the entities within
a relation. The approach has some resemblance
with what was proposed by Roth and Yih (2002).
The main difference is that we perform the extrac-
tion task in a single step via a combined kernel,
while they used two separate classifiers to identify
entities and relations and their output is later com-
bined with a probabilistic global inference.
We evaluated our relation extraction algorithm
on two biomedical data sets (i.e. the AImed cor-
pus and the LLL challenge data set; see Section
4). The motivations for using these benchmarks
derive from the increasing applicative interest in
tools able to extract relations between relevant en-
tities in biomedical texts and, consequently, from
the growing availability of annotated data sets.
The experiments show clearly that our approach
consistently improves previous results. Surpris-
ingly, it outperforms most of the systems based on
syntactic or semantic information, even when this
information is manually annotated (i.e. the LLL
challenge).
2 Problem Formalization
The problem considered here is that of iden-
tifying interactions between genes and proteins
from biomedical literature. More specifically, we
performed experiments on two slightly different
benchmark data sets (see Section 4 for a detailed
description). In the former (AImed) gene/protein
interactions are annotated without distinguishing
the type and roles of the two interacting entities.
The latter (LLL challenge) is more realistic (and
complex) because it also aims at identifying the
roles played by the interacting entities (agent and
target). For example, in Figure 1 three entities
are mentioned and two of the six ordered pairs of
GENIA/topics/Corpus/GTB.html
entities actually interact: (sigma(K), cwlH) and
(gerE, cwlH).
Figure 1: A sentence with two relations, R12 and
R32, between three entities, E1, E2 and E3.
In our approach we cast relation extraction as a
classification problem, in which examples are gen-
erated from sentences as follows.
First of all, we describe the complex case,
namely the protein/gene interactions (LLL chal-
lenge). For this data set entity recognition is per-
formed using a dictionary of protein and gene
names in which the type of the entities is unknown.
We generate examples for all the sentences con-
taining at least two entities. Thus the number of
examples generated for each sentence is given by
the combinations of distinct entities (N ) selected
two at a time, i.e. NC2. For example, as the sen-
tence shown in Figure 1 contains three entities, the
total number of examples generated is 3C2 = 3. In
each example we assign the attribute CANDIDATE
to each of the candidate interacting entities, while
the other entities in the example are assigned the
attribute OTHER, meaning that they do not partici-
pate in the relation. If a relation holds between the
two candidate interacting entities the example is
labeled 1 or 2 (according to the roles of the inter-
acting entities, agent and target, i.e. to the direc-
tion of the relation); 0 otherwise. Figure 2 shows
the examples generated from the sentence in Fig-
ure 1.
Figure 2: The three protein-gene examples gener-
ated from the sentence in Figure 1.
Note that in generating the examples from the
sentence in Figure 1 we did not create three neg-
402
ative examples (there are six potential ordered re-
lations between three entities), thereby implicitly
under-sampling the data set. This allows us to
make the classification task simpler without loos-
ing information. As a matter of fact, generating
examples for each ordered pair of entities would
produce two subsets of the same size containing
similar examples (differing only for the attributes
CANDIDATE and OTHER), but with different clas-
sification labels. Furthermore, under-sampling al-
lows us to halve the data set size and reduce the
data skewness.
For the protein-protein interaction task (AImed)
we use the correct entities provided by the manual
annotation. As said at the beginning of this sec-
tion, this task is simpler than the LLL challenge
because there is no distinction between types (all
entities are proteins) and roles (the relation is sym-
metric). As a consequence, the examples are gen-
erated as described above with the following dif-
ference: an example is labeled 1 if a relation holds
between the two candidate interacting entities; 0
otherwise.
3 Kernel Methods for Relation
Extraction
The basic idea behind kernel methods is to embed
the input data into a suitable feature space F via
a mapping function ? : X ? F , and then use
a linear algorithm for discovering nonlinear pat-
terns. Instead of using the explicit mapping ?, we
can use a kernel function K : X ? X ? R, that
corresponds to the inner product in a feature space
which is, in general, different from the input space.
Kernel methods allow us to design a modular
system, in which the kernel function acts as an
interface between the data and the learning algo-
rithm. Thus the kernel function is the only domain
specific module of the system, while the learning
algorithm is a general purpose component. Po-
tentially any kernel function can work with any
kernel-based algorithm. In our approach we use
Support Vector Machines (Vapnik, 1998).
In order to implement the approach based on
shallow linguistic information we employed a
linear combination of kernels. Different works
(Gliozzo et al, 2005; Zhao and Grishman, 2005;
Culotta and Sorensen, 2004) empirically demon-
strate the effectiveness of combining kernels in
this way, showing that the combined kernel always
improves the performance of the individual ones.
In addition, this formulation allows us to evalu-
ate the individual contribution of each informa-
tion source. We designed two families of kernels:
Global Context kernels and Local Context kernels,
in which each single kernel is explicitly calculated
as follows
K(x1, x2) =
??(x1), ?(x2)?
??(x1)???(x2)?
, (1)
where ?(?) is the embedding vector and ? ? ? is the
2-norm. The kernel is normalized (divided) by the
product of the norms of embedding vectors. The
normalization factor plays an important role in al-
lowing us to integrate information from heteroge-
neous feature spaces. Even though the resulting
feature space has high dimensionality, an efficient
computation of Equation 1 can be carried out ex-
plicitly since the input representations defined be-
low are extremely sparse.
3.1 Global Context Kernel
In (Bunescu and Mooney, 2005b), the authors ob-
served that a relation between two entities is gen-
erally expressed using only words that appear si-
multaneously in one of the following three pat-
terns:
Fore-Between: tokens before and between the
two candidate interacting entities. For in-
stance: binding of [P1] to [P2], interaction in-
volving [P1] and [P2], association of [P1] by
[P2].
Between: only tokens between the two candidate
interacting entities. For instance: [P1] asso-
ciates with [P2], [P1] binding to [P2], [P1],
inhibitor of [P2].
Between-After: tokens between and after the two
candidate interacting entities. For instance:
[P1] - [P2] association, [P1] and [P2] interact,
[P1] has influence on [P2] binding.
Our global context kernels operate on the patterns
above, where each pattern is represented using a
bag-of-words instead of sparse subsequences of
words, PoS tags, entity and chunk types, or Word-
Net synsets as in (Bunescu and Mooney, 2005b).
More formally, given a relation example R, we
represent a pattern P as a row vector
?P (R) = (tf(t1, P ), tf(t2, P ), . . . , tf(tl, P )) ? Rl, (2)
where the function tf(ti, P ) records how many
times a particular token ti is used in P . Note that,
403
this approach differs from the standard bag-of-
words as punctuation and stop words are included
in ?P , while the entities (with attribute CANDI-
DATE and OTHER) are not. To improve the clas-
sification performance, we have further extended
?P to embed n-grams of (contiguous) tokens (up
to n = 3). By substituting ?P into Equation 1, we
obtain the n-gram kernel Kn, which counts com-
mon uni-grams, bi-grams, . . . , n-grams that two
patterns have in common2. The Global Context
kernel KGC(R1, R2) is then defined as
KFB(R1, R2) +KB(R1, R2) +KBA(R1, R2), (3)
where KFB , KB and KBA are n-gram kernels
that operate on the Fore-Between, Between and
Between-After patterns respectively.
3.2 Local Context Kernel
The type of the candidate interacting entities can
provide useful clues for detecting the agent and
target of the relation, as well as the presence of the
relation itself. As the type is not known, we use
the information provided by the two local contexts
of the candidate interacting entities, called left and
right local context respectively. As typically done
in entity recognition, we represent each local con-
text by using the following basic features:
Token The token itself.
Lemma The lemma of the token.
PoS The PoS tag of the token.
Orthographic This feature maps each token into
equivalence classes that encode attributes
such as capitalization, punctuation, numerals
and so on.
Formally, given a relation example R, a local con-
text L = t?w, . . . , t?1, t0, t+1, . . . , t+w is repre-
sented as a row vector
?L(R) = (f1(L), f2(L), . . . , fm(L)) ? {0, 1}m, (4)
where fi is a feature function that returns 1 if it is
active in the specified position of L, 0 otherwise3.
The Local Context kernel KLC(R1, R2) is defined
as
Kleft(R1, R2) +Kright(R1, R2), (5)
whereKleft andKright are defined by substituting
the embedding of the left and right local context
into Equation 1 respectively.
2In the literature, it is also called n-spectrum kernel.
3In the reported experiments, we used a context window
of ?2 tokens around the candidate entity.
Notice that KLC differs substantially from
KGC as it considers the ordering of the tokens and
the feature space is enriched with PoS, lemma and
orthographic features.
3.3 Shallow Linguistic Kernel
Finally, the Shallow Linguistic kernel
KSL(R1, R2) is defined as
KGC(R1, R2) +KLC(R1, R2). (6)
It follows directly from the explicit construction
of the feature space and from closure properties of
kernels that KSL is a valid kernel.
4 Data sets
The two data sets used for the experiments concern
the same domain (i.e. gene/protein interactions).
However, they present a crucial difference which
makes it worthwhile to show the experimental re-
sults on both of them. In one case (AImed) in-
teractions are considered symmetric, while in the
other (LLL challenge) agents and targets of genic
interactions have to be identified.
4.1 AImed corpus
The first data set used in the experiments is the
AImed corpus4, previously used for training pro-
tein interaction extraction systems in (Bunescu et
al., 2005; Bunescu and Mooney, 2005b). It con-
sists of 225 Medline abstracts: 200 are known
to describe interactions between human proteins,
while the other 25 do not refer to any interaction.
There are 4,084 protein references and around
1,000 tagged interactions in this data set. In this
data set there is no distinction between genes and
proteins and the relations are symmetric.
4.2 LLL Challenge
This data set was used in the Learning Language
in Logic (LLL) challenge on Genic Interaction
extraction5 (Nede?llec, 2005). The objective of
the challenge was to evaluate the performance of
systems based on machine learning techniques to
identify gene/protein interactions and their roles,
agent or target. The data set was collected by
querying Medline on Bacillus subtilis transcrip-
tion and sporulation. It is divided in a training set
(80 sentences describing 271 interactions) and a
4ftp://ftp.cs.utexas.edu/pub/mooney/
bio-data/interactions.tar.gz
5http://genome.jouy.inra.fr/texte/
LLLchallenge/
404
test set (87 sentences describing 106 interactions).
Differently from the training set, the test set con-
tains sentences without interactions. The data set
is decomposed in two subsets of increasing diffi-
culty. The first subset does not include corefer-
ences, while the second one includes simple cases
of coreference, mainly appositions. Both subsets
are available with different kinds of annotation:
basic and enriched. The former includes word and
sentence segmentation. The latter also includes
manually checked information, such as lemma and
syntactic dependencies. A dictionary of named
entities (including typographical variants and syn-
onyms) is associated to the data set.
5 Experiments
Before describing the results of the experiments,
a note concerning the evaluation methodology.
There are different ways of evaluating perfor-
mance in extracting information, as noted in
(Lavelli et al, 2004) for the extraction of slot
fillers in the Seminar Announcement and the Job
Posting data sets. Adapting the proposed classi-
fication to relation extraction, the following two
cases can be identified:
? One Answer per Occurrence in the Document
? OAOD (each individual occurrence of a
protein interaction has to be extracted from
the document);
? One Answer per Relation in a given Docu-
ment ? OARD (where two occurrences of the
same protein interaction are considered one
correct answer).
Figure 3 shows a fragment of tagged text drawn
from the AImed corpus. It contains three different
interactions between pairs of proteins, for a total
of seven occurrences of interactions. For example,
there are three occurrences of the interaction be-
tween IGF-IR and p52Shc (i.e. number 1, 3 and
7). If we adopt the OAOD methodology, all the
seven occurrences have to be extracted to achieve
the maximum score. On the other hand, if we use
the OARD methodology, only one occurrence for
each interaction has to be extracted to maximize
the score.
On the AImed data set both evaluations were
performed, while on the LLL challenge only the
OAOD evaluation methodology was performed
because this is the only one provided by the eval-
uation server of the challenge.
Figure 3: Fragment of the AImed corpus with all
proteins and their interactions tagged. The pro-
tein names have been highlighted in bold face and
their same subscript numbers indicate interaction
between the proteins.
5.1 Implementation Details
All the experiments were performed using the
SVM package LIBSVM6 customized to embed our
own kernel. For the LLL challenge submission,
we optimized the regularization parameter C by
10-fold cross validation; while we used its default
value for the AImed experiment. In both exper-
iments, we set the cost-factor Wi to be the ratio
between the number of negative and positive ex-
amples.
5.2 Results on AImed
KSL performance was first evaluated on the
AImed data set (Section 4.1). We first give an
evaluation of the kernel combination and then we
compare our results with the Subsequence Ker-
nel for Relation Extraction (ERK) described in
(Bunescu and Mooney, 2005b). All experiments
are conducted using 10-fold cross validation on
the same data splitting used in (Bunescu et al,
2005; Bunescu and Mooney, 2005b).
Table 1 shows the performance of the three ker-
nels defined in Section 3 for protein-protein in-
teractions using the two evaluation methodologies
described above.
We report in Figure 4 the precision-recall curves
of ERK andKSL using OARD evaluation method-
ology (the evaluation performed by Bunescu and
Mooney (2005b)). As in (Bunescu et al, 2005;
Bunescu andMooney, 2005b), the graph points are
obtained by varying the threshold on the classifi-
6http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
405
OAOD
Kernel Precision Recall F1
KGC 57.7 60.1 58.9
KLC 37.3 56.3 44.9
KSL 60.9 57.2 59.0
OARD
Kernel Precision Recall F1
KGC 58.9 66.2 62.2
KLC 44.8 67.8 54.0
KSL 64.5 63.2 63.9
ERK 65.0 46.4 54.2
Table 1: Performance on the AImed data set us-
ing the two evaluation methodologies, OAOD and
OARD.
cation confidence7. The results clearly show that
KSL outperforms ERK, especially in term of re-
call (see Table 1).
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Pr
ec
is
io
n
Recall
KSL vs. ERK
ERK
KSL
Figure 4: Precision-recall curves on the AImed
data set using OARD evaluation methodology.
Finally, Figure 5 shows the learning curve of the
combined kernel KSL using the OARD evaluation
methodology. The curve reaches a plateau with
around 100 Medline abstracts.
5.3 Results on LLL challenge
The system was evaluated on the ?basic? version
of the LLL challenge data set (Section 4.2).
Table 2 shows the results of KSL returned by
the scoring service8 for the three subsets of the
training set (with and without coreferences, and
with their union). Table 3 shows the best results
obtained at the official competition performed in
April 2005. Comparing the results we see that
KSL trained on each subset outperforms the best
7For this purpose the probability estimate output of LIB-
SVM is used.
8http://genome.jouy.inra.fr/texte/
LLLchallenge/scoringService.php
0
0.2
0.4
0.6
0.8
1
0 50 100 150 200
F 1
Number of documents
Figure 5: KSL learning curve on the AImed data
set using OARD evaluation methodology.
Coref. Precision Recall F1
all 56.0 61.4 58.6
with 29.0 31.0 30.0
without 54.8 62.9 58.6
Table 2: KSL performance on the LLL challenge
test set using only the basic linguistic information.
systems of the LLL challenge9. Notice that the
best results at the challenge were obtained by dif-
ferent groups and exploiting the linguistic ?en-
riched? version of the data set. As observed in
(Nede?llec, 2005), the scores obtained using the
training set without coreferences and the whole
training set are similar.
We also report in Table 4 an analysis of the ker-
nel combination. Given that we are interested here
in the contribution of each kernel, we evaluated
the experiments by 10-fold cross-validation on the
whole training set avoiding the submission pro-
cess.
5.4 Discussion of Results
The experimental results show that the combined
kernel KSL outperforms the basic kernels KGC
andKLC on both data sets. In particular, precision
significantly increases at the expense of a lower re-
call. High precision is particularly advantageous
when extracting knowledge from large corpora,
because it avoids overloading end users with too
many false positives.
Although the basic kernels were designed to
model complementary aspects of the task (i.e.
9After the challenge deadline, Reidel and Klein (2005)
achieved a significant improvement, F1 = 68.4% (without
coreferences) and F1 = 64.7% (with and without corefer-
ences).
406
Test set Coref. Precision Recall F1
Enriched all 55.6 53.0 54.3
with 29.0 31.0 24.4
without 60.9 46.2 52.6
Basic all n/a n/a n/a
with 14.0 82.7 24.0
without 50.0 53.8 51.8
Table 3: Best performance on basic and enriched
test sets obtained by participants in the official
competition at the LLL challenge.
Kernel Precision Recall F1
KGC 55.1 66.3 60.2
KLC 44.8 60.1 53.8
KSL 62.1 61.3 61.7
Table 4: Comparison of the performance of kernel
combination on the LLL challenge using 10-fold
cross validation.
presence of the relation and roles of the interact-
ing entities), they perform reasonably well even
when considered separately. In particular, KGC
achieved good performance on both data sets. This
result was not expected on the LLL challenge be-
cause this task requires not only to recognize the
presence of relationships between entities but also
to identify their roles. On the other hand, the out-
comes of KLC on the AImed data set show that
such kernel helps to identify the presence of rela-
tionships as well.
At first glance, it may seem strange that KGC
outperforms ERK on AImed, as the latter ap-
proach exploits a richer representation: sparse
sub-sequences of words, PoS tags, entity and
chunk types, or WordNet synsets. However, an
approach based on n-grams is sufficient to identify
the presence of a relationship. This result sounds
less surprising, if we recall that both approaches
cast the relation extraction problem as a text cate-
gorization task. Approaches to text categorization
based on rich linguistic information have obtained
less accuracy than the traditional bag-of-words ap-
proach (e.g. (Koster and Seutter, 2003)). Shallow
linguistics information seems to be more effective
to model the local context of the entities.
Finally, we obtained worse results performing
dimensionality reduction either based on generic
linguistic assumptions (e.g. by removing words
from stop lists or with certain PoS tags) or using
statistical methods (e.g. tf.idf weighting schema).
This may be explained by the fact that, in tasks like
entity recognition and relation extraction, useful
clues are also provided by high frequency tokens,
such as stop words or punctuation marks, and by
the relative positions in which they appear.
6 Related Work
First of all, the obvious references for our work
are the approaches evaluated on AImed and LLL
challenge data sets.
In (Bunescu and Mooney, 2005b), the authors
present a generalized subsequence kernel that
works with sparse sequences containing combina-
tions of words and PoS tags.
The best results on the LLL challenge were ob-
tained by the group from the University of Ed-
inburgh (Reidel and Klein, 2005), which used
Markov Logic, a framework that combines log-
linear models and First Order Logic, to create a
set of weighted clauses which can classify pairs of
gene named entities as genic interactions. These
clauses are based on chains of syntactic and se-
mantic relations in the parse or Discourse Repre-
sentation Structure (DRS) of a sentence, respec-
tively.
Other relevant approaches include those that
adopt kernel methods to perform relation extrac-
tion. Zelenko et al (2003) describe a relation ex-
traction algorithm that uses a tree kernel defined
over a shallow parse tree representation of sen-
tences. The approach is vulnerable to unrecover-
able parsing errors. Culotta and Sorensen (2004)
describe a slightly generalized version of this ker-
nel based on dependency trees, in which a bag-of-
words kernel is used to compensate for errors in
syntactic analysis. A further extension is proposed
by Zhao and Grishman (2005). They use compos-
ite kernels to integrate information from different
syntactic sources (tokenization, sentence parsing,
and deep dependency analysis) so that process-
ing errors occurring at one level may be overcome
by information from other levels. Bunescu and
Mooney (2005a) present an alternative approach
which uses information concentrated in the short-
est path in the dependency tree between the two
entities.
As mentioned in Section 1, another relevant ap-
proach is presented in (Roth and Yih, 2002). Clas-
sifiers that identify entities and relations among
them are first learned from local information in
the sentence. This information, along with con-
straints induced among entity types and relations,
is used to perform global probabilistic inference
407
that accounts for the mutual dependencies among
the entities.
All the previous approaches have been evalu-
ated on different data sets so that it is not possi-
ble to have a clear idea of which approach is better
than the other.
7 Conclusions and Future Work
The good results obtained using only shallow lin-
guistic features provide a higher baseline against
which it is possible to measure improvements ob-
tained using methods based on deep linguistic pro-
cessing. In the near future, we plan to extend our
work in several ways.
First, we would like to evaluate the contribu-
tion of syntactic information to relation extraction
from biomedical literature. With this aim, we will
integrate the output of a parser (possibly trained on
a domain-specific resource such the Genia Tree-
bank). Second, we plan to test the portability of
our model on ACE and MUC data sets. Third,
we would like to use a named entity recognizer
instead of assuming that entities are already ex-
tracted or given by a dictionary. Our long term
goal is to populate databases and ontologies by
extracting information from large text collections
such as Medline.
8 Acknowledgements
We would like to thank Razvan Bunescu for pro-
viding detailed information about the AImed data
set and the settings of the experiments. Clau-
dio Giuliano and Lorenza Romano have been sup-
ported by the ONTOTEXT project, funded by the
Autonomous Province of Trento under the FUP-
2004 research program.
References
Razvan Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, Van-
couver, B.C, October.
Razvan Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In
Proceedings of the 19th Conference on Neural In-
formation Processing Systems, Vancouver, British
Columbia.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155. Special Issue on Sum-
marization and Information Extraction from Medi-
cal Documents.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL 2004), Barcelona,
Spain.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strappar-
ava. 2005. Domain kernels for word sense disam-
biguation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), Ann Arbor, Michigan, June.
Cornelis H. A. Koster and Mark Seutter. 2003. Taming
wild phrases. In Advances in Information Retrieval,
25th European Conference on IR Research (ECIR
2003), pages 161?176, Pisa, Italy.
Alberto Lavelli, Mary Elaine Califf, Fabio Ciravegna,
Dayne Freitag, Claudio Giuliano, Nicholas Kushm-
erick, and Lorenza Romano. 2004. IE evaluation:
Criticisms and recommendations. In Proceedings of
the AAAI 2004 Workshop on Adaptive Text Extrac-
tion and Mining (ATEM 2004), San Jose, California.
Claire Nede?llec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the ICML-2005 Workshop on Learning Lan-
guage in Logic (LLL05), pages 31?37, Bonn, Ger-
many, August.
Sebastian Reidel and Ewan Klein. 2005. Genic
interaction extraction with semantic and syntactic
chains. In Proceedings of the ICML-2005 Workshop
on Learning Language in Logic (LLL05), pages 69?
74, Bonn, Germany, August.
D. Roth and W. Yih. 2002. Probabilistic reasoning
for entity & relation recognition. In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING-02), Taipei, Taiwan.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press, New York, NY, USA.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, New York.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for information
extraction. Journal of Machine Learning Research,
3:1083?1106.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), Ann Arbor, Michigan, June.
408
Investigating a Generic Paraphrase-based Approach
for Relation Extraction
Lorenza Romano
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
romano@itc.it
Milen Kouylekov
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
kouylekov@itc.it
Idan Szpektor
Department of Computer Science
Bar Ilan University
Ramat Gan, 52900, Israel
szpekti@cs.biu.ac.il
Ido Dagan
Department of Computer Science
Bar Ilan University
Ramat Gan, 52900, Israel
dagan@cs.biu.ac.il
Alberto Lavelli
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
lavelli@itc.it
Abstract
Unsupervised paraphrase acquisition has
been an active research field in recent
years, but its effective coverage and per-
formance have rarely been evaluated. We
propose a generic paraphrase-based ap-
proach for Relation Extraction (RE), aim-
ing at a dual goal: obtaining an applicative
evaluation scheme for paraphrase acquisi-
tion and obtaining a generic and largely
unsupervised configuration for RE.We an-
alyze the potential of our approach and
evaluate an implemented prototype of it
using an RE dataset. Our findings reveal a
high potential for unsupervised paraphrase
acquisition. We also identify the need for
novel robust models for matching para-
phrases in texts, which should address syn-
tactic complexity and variability.
1 Introduction
A crucial challenge for semantic NLP applica-
tions is recognizing the many different ways for
expressing the same information. This seman-
tic variability phenomenon was addressed within
specific applications, such as question answering,
information extraction and information retrieval.
Recently, the problem was investigated within
generic application-independent paradigms, such
as paraphrasing and textual entailment.
Eventually, it would be most appealing to apply
generic models for semantic variability to concrete
applications. This paper investigates the applica-
bility of a generic ?paraphrase-based? approach to
the Relation Extraction (RE) task, using an avail-
able RE dataset of protein interactions. RE is
highly suitable for such investigation since its goal
is to exactly identify all the different variations in
which a target semantic relation can be expressed.
Taking this route sets up a dual goal: (a) from
the generic paraphrasing perspective - an objective
evaluation of paraphrase acquisition performance
on a concrete application dataset, as well as identi-
fying the additional mechanisms needed to match
paraphrases in texts; (b) from the RE perspective -
investigating the feasibility and performance of a
generic paraphrase-based approach for RE.
Our configuration assumes a set of entailing
templates (non-symmetric ?paraphrases?) for the
target relation. For example, for the target rela-
tion ?X interact with Y? we would assume a set of
entailing templates as in Tables 3 and 7. In addi-
tion, we require a syntactic matching module that
identifies template instances in text.
First, we manually analyzed the protein-
interaction dataset and identified all cases in which
protein interaction is expressed by an entailing
template. This set a very high idealized upper
bound for the recall of the paraphrase-based ap-
proach for this dataset. Yet, obtaining high cover-
age in practice would require effective paraphrase
acquisition and lexical-syntactic template match-
ing. Next, we implemented a prototype that uti-
lizes a state-of-the-art method for learning en-
tailment relations from the web (Szpektor et al,
2004), the Minipar dependency parser (Lin, 1998)
and a syntactic matching module. As expected,
the performance of the implemented system was
much lower than the ideal upper bound, yet ob-
taining quite reasonable practical results given its
unsupervised nature.
The contributions of our investigation follow
409
the dual goal set above. To the best of our knowl-
edge, this is the first comprehensive evaluation
that measures directly the performance of unsuper-
vised paraphrase acquisition relative to a standard
application dataset. It is also the first evaluation of
a generic paraphrase-based approach for the stan-
dard RE setting. Our findings are encouraging for
both goals, particularly relative to their early ma-
turity level, and reveal constructive evidence for
the remaining room for improvement.
2 Background
2.1 Unsupervised Information Extraction
Information Extraction (IE) and its subfield Rela-
tion Extraction (RE) are traditionally performed
in a supervised manner, identifying the different
ways to express a specific information or relation.
Given that annotated data is expensive to produce,
unsupervised or weakly supervised methods have
been proposed for IE and RE.
Yangarber et al (2000) and Stevenson and
Greenwood (2005) define methods for automatic
acquisition of predicate-argument structures that
are similar to a set of seed relations, which rep-
resent a specific scenario. Yangarber et al (2000)
approach was evaluated in two ways: (1) manually
mapping the discovered patterns into an IE system
and running a full MUC-style evaluation; (2) using
the learned patterns to perform document filtering
at the scenario level. Stevenson and Greenwood
(2005) evaluated their method through document
and sentence filtering at the scenario level.
Sudo et al (2003) extract dependency subtrees
within relevant documents as IE patterns. The goal
of the algorithm is event extraction, though perfor-
mance is measured by counting argument entities
rather than counting events directly.
Hasegawa et al (2004) performs unsupervised
hierarchical clustering over a simple set of fea-
tures. The algorithm does not extract entity pairs
for a given relation from a set of documents but
rather classifies all relations in a large corpus. This
approach is more similar to text mining tasks than
to classic IE problems.
To conclude, several unsupervised approaches
learn relevant IE templates for a complete sce-
nario, but without identifying their relevance to
each specific relation within the scenario. Accord-
ingly, the evaluations of these works either did not
address the direct applicability for RE or evaluated
it only after further manual postprocessing.
2.2 Paraphrases and Entailment Rules
A generic model for language variability is us-
ing paraphrases, text expressions that roughly con-
vey the same meaning. Various methods for auto-
matic paraphrase acquisition have been suggested
recently, ranging from finding equivalent lexical
elements to learning rather complex paraphrases
at the sentence level1.
More relevant for RE are ?atomic? paraphrases
between templates, text fragments containing vari-
ables, e.g. ?X buy Y ? X purchase Y?. Under a
syntactic representation, a template is a parsed text
fragment, e.g. ?X subj? interact mod? with pcomp?n? Y?
(based on the syntactic dependency relations of
the Minipar parser). The parses include part-of-
speech tags, which we omit for clarity.
Dagan and Glickman (2004) suggested that a
somewhat more general notion than paraphrasing
is that of entailment relations. These are direc-
tional relations between two templates, where the
meaning of one can be entailed from the meaning
of the other, e.g. ?X bind to Y? X interact with Y?.
For RE, when searching for a target relation, it is
sufficient to identify an entailing template since it
implies that the target relation holds as well. Un-
der this notion, paraphrases are bidirectional en-
tailment relations.
Several methods extract atomic paraphrases by
exhaustively processing local corpora (Lin and
Pantel, 2001; Shinyama et al, 2002). Learn-
ing from a local corpus is bounded by the cor-
pus scope, which is usually domain specific (both
works above processed news domain corpora). To
cover a broader range of domains several works
utilized the Web, while requiring several manu-
ally provided examples for each input relation,
e.g. (Ravichandran and Hovy, 2002). Taking a
step further, the TEASE algorithm (Szpektor et al,
2004) provides a completely unsupervised method
for acquiring entailment relations from the Web
for a given input relation (see Section 5.1).
Most of these works did not evaluate their re-
sults in terms of application coverage. Lin and
Pantel (2001) compared their results to human-
generated paraphrases. Shinyama et al (2002)
measured the coverage of their learning algorithm
relative to the paraphrases present in a given cor-
pus. Szpektor et al (2004) measured ?yield?, the
number of correct rules learned for an input re-
1See the 3rd IWP workshop for a sample of recent works
on paraphrasing (http://nlp.nagaokaut.ac.jp/IWP2005/).
410
lation. Ravichandran and Hovy (2002) evaluated
the performance of a QA system that is based
solely on paraphrases, an approach resembling
ours. However, they measured performance using
Mean Reciprocal Rank, which does not reveal the
actual coverage of the learned paraphrases.
3 Assumed Configuration for RE
Phenomenon Example
Passive form ?Y is activated by X?
Apposition ?X activates its companion, Y?
Conjunction ?X activates prot3 and Y?
Set ?X activates two proteins, Y and Z?
Relative clause ?X, which activates Y?
Coordination ?X binds and activates Y?
Transparent head ?X activates a fragment of Y?
Co-reference ?X is a kinase, though it activates Y?
Table 1: Syntactic variability phenomena, demon-
strated for the normalized template ?X activate Y?.
The general configuration assumed in this pa-
per for RE is based on two main elements: a list
of lexical-syntactic templates which entail the re-
lation of interest and a syntactic matcher which
identifies the template occurrences in sentences.
The set of entailing templates may be collected ei-
ther manually or automatically. We propose this
configuration both as an algorithm for RE and as
an evaluation scheme for paraphrase acquisition.
The role of the syntactic matcher is to iden-
tify the different syntactic variations in which tem-
plates occur in sentences. Table 1 presents a list
of generic syntactic phenomena that are known in
the literature to relate to linguistic variability. A
phenomenon which deserves a few words of ex-
planation is the ?transparent head noun? (Grish-
man et al, 1986; Fillmore et al, 2002). A trans-
parent noun N1 typically occurs in constructs of
the form ?N1 preposition N2? for which the syn-
tactic relation involving N1, which is the head of
the NP, applies to N2, the modifier. In the example
in Table 1, ?fragment? is the transparent head noun
while the relation ?activate? applies to Y as object.
4 Manual Data Analysis
4.1 Protein Interaction Dataset
Bunescu et al (2005) proposed a set of tasks re-
garding protein name and protein interaction ex-
traction, for which they manually tagged about
200 Medline abstracts previously known to con-
tain human protein interactions (a binary symmet-
ric relation). Here we consider their RE task of
extracting interacting protein pairs, given that the
correct protein names have already been identi-
fied. All protein names are annotated in the given
gold standard dataset, which includes 1147 anno-
tated interacting protein pairs. Protein names are
rather complex, and according to the annotation
adopted by Bunescu et al (2005) can be substrings
of other protein names (e.g., <prot> <prot>
GITR </prot> ligand </prot>). In such
cases, we considered only the longest names and
protein pairs involving them. We also ignored all
reflexive pairs, in which one protein is marked
as interacting with itself. Altogether, 1052 inter-
actions remained. All protein names were trans-
formed into symbols of the type ProtN , where N
is a number, which facilitates parsing.
For development purposes, we randomly split
the abstracts into a 60% development set (575 in-
teractions) and a 40% test set (477 interactions).
4.2 Dataset analysis
In order to analyze the potential of our approach,
two of the authors manually annotated the 575 in-
teracting protein pairs in the development set. For
each pair the annotators annotated whether it can
be identified using only template-based matching,
assuming an ideal implementation of the configu-
ration of Section 3. If it can, the normalized form
of the template connecting the two proteins was
annotated as well. The normalized template form
is based on the active form of the verb, stripped
of the syntactic phenomena listed in Table 1. Ad-
ditionally, the relevant syntactic phenomena from
Table 1 were annotated for each template instance.
Table 2 provides several example annotations.
A Kappa value of 0.85 (nearly perfect agree-
ment) was measured for the agreement between
the two annotators, regarding whether a protein
pair can be identified using the template-based
method. Additionally, the annotators agreed on
96% of the normalized templates that should be
used for the matching. Finally, the annotators
agreed on at least 96% of the cases for each syn-
tactic phenomenon except transparent heads, for
which they agreed on 91% of the cases. This high
level of agreement indicates both that template-
based matching is a well defined task and that nor-
malized template form and its syntactic variations
are well defined notions.
Several interesting statistics arise from the an-
411
Sentence Annotation
We have crystallized a complex between human FGF1 and
a two-domain extracellular fragment of human FGFR2.
? template: ?complex between X and Y?
? transparent head: ?fragment of X?
CD30 and its counter-receptor CD30 ligand (CD30L) are
members of the TNF-receptor / TNFalpha superfamily and
function to regulate lymphocyte survival and differentiation.
? template: ?X?s counter-receptor Y?
? apposition
? co-reference
iCdi1, a human G1 and S phase protein phosphatase that
associates with Cdk2.
? template: ?X associate with Y?
? relative clause
Table 2: Examples of annotations of interacting protein pairs. The annotation describes the normalized
template and the different syntactic phenomena identified.
Template f Template f Template f
X interact with Y 28 interaction of X with Y 12 X Y interaction 5
X bind to Y 22 X associate with Y 11 X interaction with Y 4
X Y complex 17 X activate Y 6 association of X with Y 4
interaction between X and Y 16 binding of X to Y 5 X?s association with Y 3
X bind Y 14 X form complex with Y 5 X be agonist for Y 3
Table 3: The 15 most frequent templates and their instance count (f ) in the development set.
notation. First, 93% of the interacting protein pairs
(537/575) can be potentially identified using the
template-based approach, if the relevant templates
are provided. This is a very promising finding,
suggesting that the template-based approach may
provide most of the requested information. We
term these 537 pairs as template-based pairs. The
remaining pairs are usually expressed by complex
inference or at a discourse level.
Phenomenon % Phenomenon %
transparent head 34 relative clause 8
apposition 24 co-reference 7
conjunction 24 coordination 7
set 13 passive form 2
Table 4: Occurrence percentage of each syntactic
phenomenon within template-based pairs (537).
Second, for 66% of the template-based pairs
at least one syntactic phenomenon was annotated.
Table 4 contains the occurrence percentage of each
phenomenon in the development set. These results
show the need for a powerful syntactic matcher on
top of high performance template acquisition, in
order to correctly match a template in a sentence.
Third, 175 different normalized templates were
identified. For each template we counted its tem-
plate instances, the number of times the tem-
plate occurred, counting only occurrences that ex-
press an interaction of a protein pair. In total,
we counted 341 template instances for all 175
templates. Interestingly, 50% of the template in-
stances (184/341) are instances of the 21 most fre-
quent templates. This shows that, though protein
interaction can be expressed in many ways, writ-
ers tend to choose from among just a few common
expressions. Table 3 presents the most frequent
templates. Table 5 presents the minimal number
of templates required to obtain the range of differ-
ent recall levels.
Furthermore, we grouped template variants
that are based on morphological derivations (e.g.
?X interact with Y? and ?X Y interaction?)
and found that 4 groups, ?X interact with Y?,
?X bind to Y?, ?X associate with Y? and ?X com-
plex with Y?, together with their morphological
derivations, cover 45% of the template instances.
This shows the need to handle generic lexical-
syntactic phenomena, and particularly morpholog-
ical based variations, separately from the acquisi-
tion of normalized lexical syntactic templates.
To conclude, this analysis indicates that the
template-based approach provides very high cov-
erage for this RE dataset, and a small number of
normalized templates already provides significant
recall. However, it is important to (a) develop
a model for morphological-based template vari-
ations (e.g. as encoded in Nomlex (Macleod et
al., )), and (b) apply accurate parsing and develop
syntactic matching models to recognize the rather
412
complex variations of template instantiations in
text. Finally, we note that our particular figures
are specific to this dataset and the biological ab-
stracts domain. However, the annotation and anal-
ysis methodologies are general and are suggested
as highly effective tools for further research.
R(%) # templates R(%) # templates
10 2 60 39
20 4 70 73
30 6 80 107
40 11 90 141
50 21 100 175
Table 5: The number of most frequent templates
necessary to reach different recall levels within the
341 template instances.
5 Implemented Prototype
This section describes our initial implementation
of the approach in Section 3.
5.1 TEASE
The TEASE algorithm (Szpektor et al, 2004) is
an unsupervised method for acquiring entailment
relations from the Web for a given input template.
In this paper we use TEASE for entailment rela-
tion acquisition since it processes an input tem-
plate in a completely unsupervised manner and
due to its broad domain coverage obtained from
the Web. The reported percentage of correct out-
put templates for TEASE is 44%.
The TEASE algorithm consists of 3 steps,
demonstrated in Table 6. TEASE first retrieves
from the Web sentences containing the input tem-
plate. From these sentences it extracts variable in-
stantiations, termed anchor-sets, which are identi-
fied as being characteristic for the input template
based on statistical criteria (first column in Ta-
ble 6). Characteristic anchor-sets are assumed to
uniquely identify a specific event or fact. Thus,
any template that appears with such an anchor-set
is assumed to have an entailment relationship with
the input template. Next, TEASE retrieves from
the Web a corpus S of sentences that contain the
characteristic anchor-sets (second column), hop-
ing to find occurrences of these anchor-sets within
templates other than the original input template.
Finally, TEASE parses S and extracts templates
that are assumed to entail or be entailed by the
input template. Such templates are identified as
maximal most general sub-graphs that contain the
anchor sets? positions (third column in Table 6).
Each learned template is ranked by number of oc-
currences in S.
5.2 Transformation-based Graph Matcher
In order to identify instances of entailing templates
in sentences we developed a syntactic matcher that
is based on transformations rules. The matcher
processes a sentence in 3 steps: 1) parsing the sen-
tence with the Minipar parser, obtaining a depen-
dency graph2; 2) matching each template against
the sentence dependency graph; 3) extracting can-
didate term pairs that match the template variables.
A template is considered directly matched in a
sentence if it appears as a sub-graph in the sen-
tence dependency graph, with its variables instan-
tiated. To further address the syntactic phenomena
listed in Table 1 we created a set of hand-crafted
parser-dependent transformation rules, which ac-
count for the different ways in which syntactic
relationships may be realized in a sentence. A
transformation rule maps the left hand side of the
rule, which strictly matches a sub-graph of the
given template, to the right hand side of the rule,
which strictly matches a sub-graph of the sentence
graph. If a rule matches, the template sub-graph is
mapped accordingly into the sentence graph.
For example, to match the syntactic tem-
plate ?X(N) subj? activate(V) obj? Y(N)? (POS
tags are in parentheses) in the sentence ?Prot1
detected and activated Prot2? (see Figure 1) we
should handle the coordination phenomenon.
The matcher uses the transformation rule
?Var1(V) ? and(U)mod? Word(V) conj? Var1(V)?
to overcome the syntactic differences. In this
example Var1 matches the verb ?activate?, Word
matches the verb ?detect? and the syntactic rela-
tions for Word are mapped to the ones for Var1.
Thus, we can infer that the subject and object
relations of ?detect? are also related to ?activate?.
6 Experiments
6.1 Experimental Settings
To acquire a set of entailing templates we first ex-
ecuted TEASE on the input template ?X subj? in-
teract mod? with pcomp?n? Y?, which corresponds to
the ?default? expression of the protein interaction
2We chose a dependency parser as it captures directly the
relations between words; we use Minipar due to its speed.
413
Extracted Anchor-set Sentence containing Anchor-set Learned Template
X=?chemokines?,
Y=?specific receptors?
Chemokines bind to specific receptors on the target
cells
X subj? bind mod?
to
pcomp?n
? Y
X=?Smad3?, Y=?Smad4? Smad3 / Smad4 complexes translocate to the nucleus X Y nn? complex
Table 6: TEASE output at different steps of the algorithm for ?X subj? interact mod? with pcomp?n? Y?.
1. X bind to Y 7. X Y complex 13. X interaction with Y
2. X activate Y 8. X recognize Y 14. X trap Y
3. X stimulate Y 9. X block Y 15. X recruit Y
4. X couple to Y 10. X binding to Y 16. X associate with Y
5. interaction between X and Y 11. X Y interaction 17. X be linked to Y
6. X become trapped in Y 12. X attach to Y 18. X target Y
Table 7: The top 18 correct templates learned by TEASE for ?X interact with Y?.
detect(V )
subjwwppp
pp
pp
pp
pp
conj

mod
''NN
NN
NN
NN
NN
N
obj // Prot2(N)
Prot1(N) activate(V ) and(U)
Figure 1: The dependency parse graph of the sen-
tence ?Prot1 detected and activated Prot2?.
relation. TEASE learned 118 templates for this
relation. Table 7 lists the top 18 learned templates
that we considered as correct (out of the top 30
templates in TEASE output). We then extracted
interacting protein pair candidates by applying the
syntactic matcher to the 119 templates (the 118
learned plus the input template). Candidate pairs
that do not consist of two proteins, as tagged in the
input dataset, were filtered out (see Section 4.1;
recall that our experiments were applied to the
dataset of protein interactions, which isolates the
RE task from the protein name recognition task).
In a subsequent experiment we iteratively ex-
ecuted TEASE on the 5 top-ranked learned tem-
plates to acquire additional relevant templates. In
total, we obtained 1233 templates that were likely
to imply the original input relation. The syntactic
matcher was then reapplied to extract candidate in-
teracting protein pairs using all 1233 templates.
We used the development set to tune a small
set of 10 generic hand-crafted transformation rules
that handle different syntactic variations. To han-
dle transparent head nouns, which is the only phe-
nomenon that demonstrates domain dependence,
we extracted a set of the 5 most frequent trans-
parent head patterns in the development set, e.g.
?fragment of X?.
In order to compare (roughly) our performance
with supervised methods applied to this dataset, as
summarized in (Bunescu et al, 2005), we adopted
their recall and precision measurement. Their
scheme counts over distinct protein pairs per ab-
stract, which yields 283 interacting pairs in our test
set and 418 in the development set.
6.2 Manual Analysis of TEASE Recall
experiment pairs instances
input 39% 37%
input + iterative 49% 48%
input + iterative + morph 63% 62%
Table 8: The potential recall of TEASE in terms of
distinct pairs (out of 418) and coverage of template
instances (out of 341) in the development set.
Before evaluating the system as a whole we
wanted to manually assess in isolation the cover-
age of TEASE output relative to all template in-
stances that were manually annotated in the devel-
opment set. We considered a template as covered
if there is a TEASE output template that is equal
to the manually annotated template or differs from
it only by the syntactic phenomena described in
Section 3 or due to some parsing errors. Count-
ing these matches, we calculated the number of
template instances and distinct interacting protein
pairs that are covered by TEASE output.
Table 8 presents the results of our analysis. The
414
1st line shows the coverage of the 119 templates
learned by TEASE for the input template ?X inter-
act with Y?. It is interesting to note that, though we
aim to learn relevant templates for the specific do-
main, TEASE learned relevant templates also by
finding anchor-sets of different domains that use
the same jargon, such as particle physics.
We next analyzed the contribution of the itera-
tive learning for the additional 5 templates to recall
(2nd line in Table 8). With the additional learned
templates, recall increased by about 25%, showing
the importance of using the iterative steps.
Finally, when allowing matching between a
TEASE template and a manually annotated tem-
plate, even if one is based on a morphologi-
cal derivation of the other (3rd line in Table 8),
TEASE recall increased further by about 30%.
We conclude that the potential recall of the cur-
rent version of TEASE on the protein interaction
dataset is about 60%. This indicates that signif-
icant coverage can be obtained using completely
unsupervised learning from the web, as performed
by TEASE. However, the upper bound for our cur-
rent implemented system is only about 50% be-
cause our syntactic matching does not handle mor-
phological derivations.
6.3 System Results
experiment recall precision F1
input 0.18 0.62 0.28
input + iterative 0.29 0.42 0.34
Table 9: System results on the test set.
Table 9 presents our system results for the test
set, corresponding to the first two experiments in
Table 8. The recall achieved by our current imple-
mentation is notably worse than the upper bound
of the manual analysis because of two general set-
backs of the current syntactic matcher: 1) parsing
errors; 2) limited transformation rule coverage.
First, the texts from the biology domain pre-
sented quite a challenge for the Minipar parser.
For example, in the sentences containing the
phrase ?X bind specifically to Y? the parser consis-
tently attaches the PP ?to? to ?specifically? instead
of to ?bind?. Thus, the template ?X bind to Y? can-
not be directly matched.
Second, we manually created a small number of
transformation rules that handle various syntactic
phenomena, since we aimed at generic domain in-
dependent rules. The most difficult phenomenon
to model with transformation rules is transparent
heads. For example, in ?the dimerization of Prot1
interacts with Prot2?, the transparent head ?dimer-
ization of X? is domain dependent. Transforma-
tion rules that handle such examples are difficult
to acquire, unless a domain specific learning ap-
proach (either supervised or unsupervised) is used.
Finally, we did not handle co-reference resolution
in the current implementation.
Bunescu et al (2005) and Bunescu and Mooney
(2005) approached the protein interaction RE task
using both handcrafted rules and several super-
vised Machine Learning techniques, which uti-
lize about 180 manually annotated abstracts for
training. Our results are not directly comparable
with theirs because they adopted 10-fold cross-
validation, while we had to divide the dataset into
a development and a test set, but a rough compari-
son is possible. For the same 30% recall, the rule-
based method achieved precision of 62% and the
best supervised learning algorithm achieved preci-
sion of 73%. Comparing to these supervised and
domain-specific rule-based approaches our system
is noticeably weaker, yet provides useful results
given that we supply very little domain specific in-
formation and acquire the paraphrasing templates
in a fully unsupervised manner. Still, the match-
ing models need considerable additional research
in order to achieve the potential performance sug-
gested by TEASE.
7 Conclusions and Future Work
We have presented a paraphrase-based approach
for relation extraction (RE), and an implemented
system, that rely solely on unsupervised para-
phrase acquisition and generic syntactic template
matching. Two targets were investigated: (a) a
mostly unsupervised, domain independent, con-
figuration for RE, and (b) an evaluation scheme
for paraphrase acquisition, providing a first evalu-
ation of its realistic coverage. Our approach differs
from previous unsupervised IE methods in that we
identify instances of a specific relation while prior
methods identified template relevance only at the
general scenario level.
We manually analyzed the potential of our ap-
proach on a dataset annotated with protein in-
teractions. The analysis shows that 93% of the
interacting protein pairs can be potentially iden-
tified with the template-based approach. Addi-
415
tionally, we manually assessed the coverage of
the TEASE acquisition algorithm and found that
63% of the distinct pairs can be potentially rec-
ognized with the learned templates, assuming an
ideal matcher, indicating a significant potential re-
call for completely unsupervised paraphrase ac-
quisition. Finally, we evaluated our current system
performance and found it weaker than supervised
RE methods, being far from fulfilling the poten-
tial indicated in our manual analyses due to insuf-
ficient syntactic matching. But, even our current
performance may be considered useful given the
very small amount of domain-specific information
used by the system.
Most importantly, we believe that our analysis
and evaluation methodologies for an RE dataset
provide an excellent benchmark for unsupervised
learning of paraphrases and entailment rules. In
the long run, we plan to develop and improve our
acquisition and matching algorithms, in order to
realize the observed potential of the paraphrase-
based approach. Notably, our findings point to the
need to learn generic morphological and syntactic
variations in template matching, an area which has
rarely been addressed till now.
Acknowledgements
This work was developed under the collaboration
ITC-irst/University of Haifa. Lorenza Romano
has been supported by the ONTOTEXT project,
funded by the Autonomous Province of Trento un-
der the FUP-2004 research program.
References
Razvan Bunescu and Raymond J. Mooney. 2005. Sub-
sequence kernels for relation extraction. In Proceed-
ings of the 19th Conference on Neural Information
Processing Systems, Vancouver, British Columbia.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155. Special Issue on Sum-
marization and Information Extraction from Medi-
cal Documents.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling of
language variability. In Proceedings of the PAS-
CAL Workshop on Learning Methods for Text Un-
derstanding and Mining, Grenoble, France.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. Seeing arguments through transparent struc-
tures. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC 2002), pages 787?791, Las Palmas, Spain.
Ralph Grishman, Lynette Hirschman, and Ngo Thanh
Nhan. 1986. Discovery procedures for sublanguage
selectional patterns: Initial experiments. Computa-
tional Linguistics, 12(3).
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discoverying relations among named
entities from large corpora. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2004), Barcelona, Spain.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation on
MINIPAR. In Proceedings of LREC-98 Workshop
on Evaluation of Parsing Systems, Granada, Spain.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. Nomlex: A lexi-
con of nominalizations. In Proceedings of the 8th
International Congress of the European Association
for Lexicography, Liege, Belgium.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a Question Answering
system. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), Philadelphia, PA.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase ac-
quisition from news articles. In Proceedings of
the Human Language Technology Conference (HLT
2002), San Diego, CA.
Mark Stevenson and Mark A. Greenwood. 2005. A
semantic approach to IE pattern induction. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), Ann
Arbor, Michigan.
K. Sudo, S. Sekine, and R. Grishman. 2003. An im-
proved extraction pattern representation model for
automatic IE pattern acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2003), Sapporo, Japan.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisi-
tion of entailment relations. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2004), Barcelona,
Spain.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
Proceedings of the 18th International Conference on
Computational Linguistics, Saarbruecken, Germany.
416
Simple Information Extraction (SIE):
A Portable and Effective IE System
Claudio Giuliano and Alberto Lavelli and Lorenza Romano
ITC-irst
Via Sommarive, 18
38050, Povo (TN)
Italy
{giuliano,lavelli,romano}@itc.it
Abstract
This paper describes SIE (Simple Infor-
mation Extraction), a modular information
extraction system designed with the goal
of being easily and quickly portable across
tasks and domains. SIE is composed by
a general purpose machine learning algo-
rithm (SVM) combined with several cus-
tomizable modules. A crucial role in the
architecture is played by Instance Filter-
ing, which allows to increase efficiency
without reducing effectiveness. The re-
sults obtained by SIE on several standard
data sets, representative of different tasks
and domains, are reported. The experi-
ments show that SIE achieves performance
close to the best systems in all tasks, with-
out using domain-specific knowledge.
1 Introduction
In designing Information Extraction (IE) systems
based on supervised machine learning techniques,
there is usually a tradeoff between carefully tun-
ing the system to specific tasks and domains and
having a ?generic? IE system able to obtain good
(even if not the topmost) performance when ap-
plied to different tasks and domains (requiring a
very reduced porting time). Usually, the former
alternative is chosen and system performance is
often shown only for a very limited number of
tasks (sometimes even only for a single task), af-
ter a careful tuning. For example, in the Bio-entity
Recognition Shared Task at JNLPBA 2004 (Kim
et al, 2004) the best performing system obtained
a considerable performance improvement adopt-
ing domain specific hacks.
A second important issue in designing IE sys-
tems concerns the fact that usually IE data sets are
highly unbalanced (i.e., the number of positive ex-
amples constitutes only a small fraction with re-
spect to the number of negative examples). This
fact has important consequences. In some ma-
chine learning algorithms the unbalanced distri-
bution of examples can yield a significant loss in
classification accuracy. Moreover, very large data
sets can be problematic to process due to the com-
plexity of many supervised learning techniques.
For example, using kernel methods, such as word
sequence and tree kernels, can become prohibitive
due to the difficulty of kernel based algorithms,
such as Support Vector Machines (SVM) (Cortes
and Vapnik, 1995), to scale to large data sets. As
a consequence, reducing the number of instances
without degrading the prediction accuracy is a cru-
cial issue for applying advanced machine learning
techniques in IE, especially in the case of highly
unbalanced data sets.
In this paper, we present SIE (Simple Informa-
tion Extraction), an information extraction system
based on a supervised machine learning approach
for extracting domain-specific entities from docu-
ments. In particular, IE is cast as a classification
problem by applying SVM to train a set of classi-
fiers, based on a simple and general-purpose fea-
ture representation, for detecting the boundaries of
the entities to be extracted.
SIE was designed with the goal of being easily
and quickly portable across tasks and domains. To
support this claim, we conducted a set of exper-
iments on several tasks in different domains and
languages. The results show that SIE is competi-
tive with the state-of-the-art systems, and it often
outperforms systems customized to a specific do-
main.
SIE resembles the ?Level One? of the ELIE
algorithm (Finn and Kushmerick, 2004). How-
9
ever, a key difference between the two algorithms
is the capability of SIE to drastically reduce the
computation time by exploiting Instance Filtering
(Gliozzo et al, 2005a). This characteristic allows
scaling from toy problems to real-world data sets
making SIE attractive in applicative fields, such as
bioinformatics, where very large amounts of data
have to be analyzed.
2 A Simple IE system
SIE has a modular system architecture. It is com-
posed by a general purpose machine learning algo-
rithm combined with several customizable com-
ponents. The system components are combined
in a pipeline, where each module constrains the
data structures provided by the previous ones.
This modular specification brings significant ad-
vantages. Firstly, a modular architecture is sim-
pler to implement. Secondly, it allows to easily
integrate different machine learning algorithms.
Finally, it allows, if necessary, a fine tuning to
a specific task by simply specializing few mod-
ules. Furthermore, it is worth noting that we tested
SIE across different domains using the same basic
configuration without exploiting any domain spe-
cific knowledge, such as gazetteers, and ad-hoc
pre/post-processing.
Instance
Filtering
Feature
Extraction
Learning
Algorithm
Tag
Matcher
Classification
Algorithm
Instance
Filtering
Feature
Extraction
Lexicon
Training Corpus New Documents
Data Model
Tagged
Documents
Filter Model
Extraction
Script
Extraction
Script
Figure 1: The SIE Architecture.
The architecture of the system is shown in Fig-
ure 1. The information extraction task is per-
formed in two phases. SIE learns off-line a set of
data models from a specified labeled corpus, then
the models are applied to tag new documents.
In both phases, the Instance Filtering module
(Section 3) removes certain tokens from the data
set in order to speed-up the whole process, while
Feature Extraction module (Section 4) is used to
extract a pre-defined set of features from the to-
kens. In the training phase, the Learning Mod-
ule (Section 5) learns two distinct models for each
entity, one for the beginning boundary and an-
other for the end boundary (Ciravegna, 2000; Fre-
itag and Kushmerick, 2000). In the recognition
phase, as a consequence, the Classification mod-
ule (Section 5) identifies the entity boundaries as
distinct token classifications. A Tag Matcher mod-
ule (Section 6) is used to match the boundary pre-
dictions made by the Classification module. Tasks
with multiple entities are considered as multiple
independent single-entity extraction tasks (i.e. SIE
only extracts one entity at a time).
3 Instance Filtering
The purpose of the Instance Filtering (IF) mod-
ule is to reduce the data set size and skewness
by discarding harmful and superfluous instances
without degrading the prediction accuracy. This
is a generic module that can be exploited by any
supervised system that casts IE as a classification
problem.
Instance Filtering (Gliozzo et al, 2005a) is
based on the assumption that uninformative words
are not likely to belong to entities to recognize,
being their information content very low. A naive
implementation of this assumption consists in fil-
tering out very frequent words in corpora because
they are less likely to be relevant than rare words.
However, in IE relevant entities can be composed
by more than one token and in some domains a few
of such tokens can be very frequent in the corpus.
For example, in the field of bioinformatics, protein
names often contain parentheses, whose frequency
in the corpus is very high.
To deal with this problem, we exploit a set of In-
stance Filters (called Stop Word Filters), included
in a Java tool called jInFil1. These filters per-
form a ?shallow? supervision to identify frequent
words that are often marked as positive examples.
The resulting filtering algorithm consists of two
stages. First, the set of uninformative tokens is
identified by training the term filtering algorithm
on the training corpus. Second, instances describ-
ing ?uninformative? tokens are removed from both
the training and the test sets. Note that instances
are not really removed from the data set, but just
1http://tcc.itc.it/research/textec/
tools-resources/jinfil/
10
marked as uninformative. In this way the learning
algorithm will not learn from these instances, but
they will still appear in the feature description of
the remaining instances.
A Stop Word Filter is fully specified by a list of
stop words. To identify such a list, different fea-
ture selection methods taken from the text catego-
rization literature can be exploited. In text catego-
rization, feature selection is used to remove non-
informative terms from representations of texts. In
this sense, IF is closely related to feature selection:
in the former non-informative words are removed
from the instance set, while in the latter they are
removed from the feature set. Below, we describe
the different metrics used to collect a stop word
list from the training corpora.
Information Content (IC) The most commonly
used feature selection metric in text categoriza-
tion is based on document frequency (i.e, the num-
ber of documents in which a term occurs). The
basic assumption is that very frequent terms are
non-informative for document indexing. The fre-
quency of a term in the corpus is a good indica-
tor of its generality, rather than of its information
content. From this point of view, IF consists of
removing all tokens with a very low information
content2.
Correlation Coefficient (CC) In text catego-
rization the ?2 statistic is used to measure the lack
of independence between a term and a category
(Yang and Pedersen, 1997). The correlation coef-
ficient CC2 = ?2 of a term with the negative class
can be used to find those terms that are less likely
to express relevant information in texts.
Odds Ratio (OR) Odds ratio measures the ra-
tio between the odds of a term occurring in the
positive class, and the odds of a term occurring in
the negative class. In text categorization the idea
is that the distribution of the features on the rel-
evant documents is different from the distribution
on non-relevant documents (Raskutti and Kowal-
czyk, 2004). Following this assumption, a term
is non-informative when its probability of being a
negative example is sensibly higher than its prob-
ability of being a positive example (Gliozzo et al,
2005b).
2The information content of a word w can be measured
by estimating its probability from a corpus by the equation
I(w) = ?p(w) log p(w).
An Instance Filter is evaluated by using two
metrics: the Filtering Rate (?), the total percent-
age of filtered tokens in the data set, and the Pos-
itive Filtering Rate (?+), the percentage of pos-
itive tokens (wrongly) removed. A filter is opti-
mized by maximizing ? and minimizing ?+; this
allows us to reduce as much as possible the data
set size preserving most of the positive instances.
We fixed the accepted level of tolerance () on ?+
and found the maximum ? by performing 5-fold
cross-validation on the training set.
4 Feature Extraction
The Feature Extraction module is used to extract
for each input token a pre-defined set of features.
As said above, we consider each token an instance
to be classified as a specific entity boundary or
not. To perform Feature Extraction an applica-
tion called jFex3 was implemented. jFex gener-
ates the features specified by a feature extraction
script, indexes them, and returns the example set,
as well as the mapping between the features and
their indices (lexicon). If specified, it only ex-
tracts features for the instances not marked as ?un-
informative? by instance filtering. jFex is strongly
inspired by FEX (Cumby and Yih, 2003), but it
introduces several improvements. First of all, it
provides an enriched feature extraction language.
Secondly, it makes possible to further extend this
language through a Java API, providing a flexi-
ble tool to define task specific features. Finally,
jFex can output the example set in formats di-
rectly usable by LIBSVM (Chang and Lin, 2001),
SVMlight (Joachims, 1998) and SNoW (Carlson
et al, 1999).
4.1 Corpus Format
The corpus must be prepared in IOBE notation, a
extension of the IOB notation. Both notations do
not allow nested and overlapping entities. Tokens
outside entities are tagged with O, while the first
token of an entity is tagged with B-entity-type, the
last token is tagged E-entity-type, and all the to-
kens inside the entity boundaries are tagged with
I-entity-type, where entity-type is the type of the
marked entity (e.g. protein, person).
Beside the tokens and their types, the nota-
tion allows to represent general purpose and task-
specific annotations defining new columns. Blank
3http://tcc.itc.it/research/textec/
tools-resources/jfex.html.
11
lines can be used to specify sentence or document
boundaries. Table 1 shows an example of a pre-
pared corpus. The columns are: the entity-type,
the PoS tag, the actual token, the token index, and
the output of the instance filter (the ?uninforma-
tive? tokens are marked with 0) respectively.
O TO To 2.12 0
O VB investigate 2.13 0
O IN whether 2.14 0
O DT the 2.15 0
B-cell type NN tumor 2.16 1
O NN expression 2.17 1
O IN of 2.18 0
B-protein NN Beta-2-Microglobulin 2.19 1
O ( ( 2.20 1
B-protein NN Beta 2.21 1
I-protein NN 2 2.22 1
I-protein NN - 2.22 1
E-protein NN M 2.22 1
O ) ) 2.23 1
Table 1: A corpus fragment represented in IOBE
notation.
4.2 Extraction Language
As input to the begin and end classifiers, we use
a bit-vector representation. Each instance is rep-
resented encoding all the following basic features
for the actual token and for all the tokens in a con-
text window of fixed size (in the reported experi-
ments, 3 words before and 3 words after the actual
token):
Token The actual token.
POS The Part of Speech (PoS) of the token.
Token Shapes This feature maps each token into
equivalence classes that encode attributes
such as capitalization, numerals, single char-
acter, and so on.
Bigrams of tokens and PoS tags.
The Feature Extraction language allows to
formally encode the above problem description
through a script. Table 2 provides the extraction
script used in all the tasks4. More details about the
Extraction Language are provided in (Cumby and
Yih, 2003; Giuliano et al, 2005).
4In JNLPBA shared task we added some orthographic fea-
tures borrowed from the bioinformatics literature.
-1 inc loc: w [-3, 3]
-1 inc loc: coloc(w,w) [-3, 3]
-1 inc loc: t [-3, 3]
-1 inc loc: coloc(t,t) [-3, 3]
-1 inc loc: sh [-3, 3]
Table 2: The extraction script used in all tasks.
5 Learning and Classification Modules
As already said, we approach IE as a classifica-
tion problem, assigning an appropriate classifica-
tion label to each token in the data set except for
the tokens marked as irrelevant by the instance fil-
ter. As learning algorithm we use SVM-light5. In
particular, we identify the boundaries that indi-
cate the beginning and the end of each entity as
two distinct classification tasks, following the ap-
proach adopted in (Ciravegna, 2000; Freitag and
Kushmerick, 2000). All tokens that begin(end) an
entity are considered positive instances for the be-
gin(end) classifier, while all the remaining tokens
are negative instances. In this way, two distinct
models are learned, one for the beginning bound-
ary and another for the end boundary. All the pre-
dictions produced by the begin and end classifiers
are then paired by the Tag Matcher module.
When we have to deal with more than one en-
tity (i.e., with a multi-class problem) we train 2n
binary classifiers (where n is the number of entity-
types for the task). Again, all the predictions are
paired by the Tag Matcher module.
6 Tag Matcher
All the positive predictions produced by the begin
and end classifiers are paired by the Tag Matcher
module. If nested or overlapping entities occur,
even if they are of different types, the entity with
the highest score is selected. The score of each
entity is proportional to the entity length probabil-
ity (i.e., the probability that an entity has a certain
length) and the scores assigned by the classifiers to
the boundary predictions. Normalizing the scores
makes it possible to consider the score function as
a probability distribution. The entity length distri-
bution is estimated from the training set.
For example, in the corpus fragment of Table 3
the begin and end classifiers have identified four
possible entity boundaries for the speaker of a
seminar. In the table, the left column shows the
5http://svmlight.joachims.org/
12
Table 3: A corpus fragment with multiple predic-
tions.
O The
O speaker
O will
O be
B-speaker Mr. B-speaker (0.23)
I-speaker John B-speaker (0.1), E-speaker (0.12)
E-speaker Smith E-speaker (0.34)
O .
Table 4: The length distribution for the entity
speaker.
entity len 1 2 3 4 5 ...
P(entity len) 0.10 0.33 0.28 0.02 0.01 ...
actual label, while the right column shows the pre-
dictions and their normalized scores. The match-
ing algorithm has to choose among three mutu-
ally exclusive candidates: ?Mr. John?, ?Mr. John
Smith? and ?John Smith?, with scores 0.23 ?
0.12 ? 0.33 = 0.009108, 0.23 ? 0.34 ? 0.28 =
0.021896 and 0.1 ? 0.34 ? 0.33 = 0.01122, re-
spectively. The length distribution for the entity
speaker is shown in Table 4. In this example, the
matcher, choosing the candidate that maximizes
the score function, namely the second one, extracts
the actual entity.
7 Evaluation
In order to demonstrate that SIE is domain and
language independent we tested it on several tasks
using exactly the same configuration. The tasks
and the experimental settings are described in Sec-
tion 7.1. The results (Section 7.2) show that the
adopted filtering technique decreases drastically
the computation time while preserving (and some-
times improving) the overall accuracy of the sys-
tem.
7.1 The Tasks
SIE was tested on the following IE benchmarks:
JNLPBA Shared Task This shared task (Kim
et al, 2004) is an open challenge task proposed
at the ?International Joint Workshop on Natural
Language Processing in Biomedicine and its Ap-
plications?6. The data set consists of 2, 404 MED-
LINE abstracts from the GENIA project (Kim et
6http://research.nii.ac.jp/?collier/
workshops/JNLPBA04st.htm.
al., 2003), annotated with five entity types: DNA,
RNA, protein, cell-line, and cell-type. The GE-
NIA corpus is split into two partitions: training
(492,551 tokens), and test (101,039 tokens). The
fraction of positive examples with respect to the
total number of tokens in the training set varies
from 0.2% to 6%.
CoNLL 2002 & 2003 Shared Tasks These
shared tasks (Tjong Kim Sang, 2002; Tjong
Kim Sang and De Meulder, 2003)7 concern
language-independent named entity recognition.
Four types of named entities are considered:
persons (PER), locations (LOC), organizations
(ORG) and names of miscellaneous (MISC) en-
tities that do not belong to the previous three
groups. SIE was applied to the Dutch and English
data sets. The Dutch corpus is divided into three
partitions: training and validation (on the whole
258, 214 tokens), and test (73, 866 tokens). The
fraction of positive examples with respect to the
total number of tokens in the training set varies
from 1.1% to 2%. The English corpus is divided
into three partitions: training and validation (on
the whole 274, 585 tokens), and test (50, 425 to-
kens). The fraction of positive examples with re-
spect to the total number of tokens in the training
set varies from 1.6% to 3.3%.
TERN 2004 The TERN (Time Expression
Recognition and Normalization) 2004 Evaluation8
requires systems to detect and normalize temporal
expressions occurring in English text (SIE did not
address the normalization part of the task). The
TERN corpus is divided into two partitions: train-
ing (249,295 tokens) and test (72,667 tokens). The
fraction of positive examples with respect to the
total number of tokens in the training set is about
2.1%.
Seminar Announcements The Seminar An-
nouncements (SA) collection (Freitag, 1998) con-
sists of 485 electronic bulletin board postings. The
purpose of each document in the collection is to
announce or relate details of an upcoming talk or
seminar. The documents were annotated for four
entities: speaker, location, stime, and etime. The
corpus is composed by 156, 540 tokens. The frac-
tion of positive examples varies from about 1% to
7http://www.cnts.ua.ac.be/conll2002/
ner/, http://www.cnts.ua.ac.be/conll2003/
ner/.
8http://timex2.mitre.org/tern.html.
13
Metric  ?train/test R P F1 T
0 66.4 67.0 66.7 615
CC 1 64.1/62.3 67.5 67.3 67.4 420
2.5 80.1/78.0 66.6 69.1 67.8 226
5 88.9/86.4 64.8 68.1 66.4 109
OR 1 70.7/68.9 68.3 67.3 67.8 308
2.5 81.0/79.1 67.5 68.3 67.9 193
5 87.8/85.6 65.4 68.2 66.8 114
IC 1 37.3/36.9 58.5 65.7 61.9 570
2.5 38.4/38.0 56.9 65.4 60.9 558
5 39.5/38.9 55.6 65.5 60.1 552
Zhou and Su (2004) 76.0 69.4 72.6
baseline 52.6 43.6 47.7
Table 5: Filtering Rate, Micro-averaged Recall,
Precision, F1 and Time for JNLPBA.
Metric  ?train/test R P F1 T
0 73.6 78.7 76.1 134
CC 1 64.4/64.4 71.6 79.9 75.5 70
2.5 75.1/73.3 72.8 80.3 76.4 50
5 88.6/84.2 66.6 64.7 65.6 24
OR 1 71.5/71.6 72.0 78.3 75.0 61
2.5 82.1/80.7 73.6 78.9 76.2 39
5 90.5/86.1 66.8 64.5 65.6 19
IC 1 47.3/47.5 67.0 79.2 72.6 101
2.5 51.3/51.5 65.9 79.3 72.0 95
5 55.7/56.0 63.8 78.9 70.5 89
Carreras et al (2002) 76.3 77.8 77.1
baseline 45.4 81.3 58.3
Table 6: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for
CoNLL-2002 (Dutch).
about 2%. The entire document collection is ran-
domly partitioned five times into two sets of equal
size, training and test (Lavelli et al, 2004). For
each partition, learning is performed on the train-
ing set and performance is measured on the corre-
sponding test set. The resulting figures are aver-
aged over the five test partitions.
7.2 Results
The experimental results in terms of filtering rate,
recall, precision, F1, and computation time for
JNLPBA, CoNLL-2002, CoNLL-2003, TERN and
SA are given in Tables 5, 6, 7, 8 and 9 respectively.
To show the differences among filtering strategies
for JNLPBA, CoNLL-2002, TERN 2004 we used
CC, OR and IC filters, while the results for SA
and CoNLL-2003 are reported only for OR filter
(which usually produces the best performance).
For all filters we report results obtained by set-
ting four different values for parameter , the max-
imum value allowed for the Filtering Rate of pos-
itive examples.  = 0 means that no filter is used.
Metric  ?train/test R P F1 T
0 76.7 90.5 83.1 228
OR 1 70.4/83.9 78.2 88.1 82.8 74
2.5 83.6/95.6 76.4 62.6 68.8 33
5 90.5/97.2 75.3 66.5 70.7 14
Florian et al (2003) 88.5 89.0 88.8
baseline 50.9 71.9 59.6
Table 7: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for
CoNLL-2003 (English).
Metric  ?train/test R P F1 T
0 77.9 89.8 83.4 82
CC 1 41.8/41.2 76.6 90.7 83.1 57
2.5 64.5/62.8 60.3 88.6 71.7 41
5 86.9/81.7 59.7 76.0 66.9 14
OR 1 56.4/54.6 77.5 91.1 83.8 48
2.5 69.4/66.7 59.8 88.1 71.2 36
5 82.9/79.0 59.5 88.6 71.2 20
IC 1 17.8/17.4 74.9 91.2 82.3 48
2.5 24.0/23.3 74.8 91.5 82.3 36
5 27.6/27.1 75.0 91.5 82.5 20
Table 8: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for
TERN.
The results indicate that both CC and OR do ex-
hibit good performance and are far better than IC
in all the tasks. For example, in the JNLPBA data
set, OR allows to remove more than 70% of the in-
stances, losing less than 1% of the positive exam-
ples. These results pinpoint the importance of us-
ing a supervised metric to collect stop words. The
results also highlight that both CC and OR are ro-
bust against overfitting, because the difference be-
tween the filtering rates in the training and test sets
is minimal. We also report a significant reduction
of the data skewness. Table 10 shows that all the IF
techniques reduce sensibly the skewness ratio, the
ratio between the number of negative and positive
examples, on the JNLPBA data set9. As expected,
both CC and OR consistently outperform IC.
The computation time10 reported includes the
time to perform the overall process of training and
testing the boundary classifiers for each entity11.
The results indicate that both CC and OR are far
superior to IC, allowing a drastic reduction of the
time. Supervised IF techniques are then particu-
9We only report results for this data set as it exhibits the
highest skewness ratios.
10All the experiments have been performed using a dual
1.66 GHz Power Mac G5.
11Execution time for filter optimization is not reported be-
cause it is negligible.
14
Metric  ?train/test R P F1 T
0 81.3 92.5 86.6 179
OR 1 53.6/86.2 81.5 92.1 86.5 91
2.5 69.1/90.8 81.6 90.5 85.9 44
5 74.7/90.8 81.0 85.0 83.0 31
Table 9: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for SA.
entity  CC OR IC
protein 0 17.1 17.1 17.1
1 7.5 3.8 9.6
2.5 3.0 2.5 9.0
5 1.5 1.4 8.8
DNA 0 59.3 59.3 59.3
1 26.4 18.5 33.2
2.5 14.7 12.6 31.7
5 8.3 8.6 32.4
RNA 0 596.2 596.2 596.2
1 250.7 253.1 288.4
2.5 170.4 170.1 274.5
5 92.4 111.1 280.7
cell type 0 72.9 72.9 72.9
1 13.8 13.4 43.2
2.5 6.3 6.5 43.9
5 3.4 4.4 44.5
cell line 0 146.4 146.4 146.4
1 40.4 41.6 87.7
2.5 24.2 25.9 87.5
5 13.6 14.6 89.6
Table 10: Skewness ratio of each entity for
JNLPBA.
larly convenient when dealing with large data sets.
For example, using the CC metric the time re-
quired by SIE to perform the JNLPBA task is re-
duced from 615 to 109 minutes (see Table 5).
Both OR and CC allow to drastically reduce
the computation time and maintain the prediction
accuracy12 with small values of . Using OR,
for example, with  = 2.5% on JNLPBA, F1 in-
creases from 66.7% to 67.9%. On the contrary,
for CoNLL-2002 and TERN, for  > 2.5% and
 > 1% respectively, the performance of all the
filters rapidly declines. The explanation for this
behavior is that, for the last two tasks, the differ-
ence between the filtering rates on the training and
test sets becomes much larger for  > 2.5% and
 > 1%, respectively. That is, the data skewness
changes significantly from the training to the test
set. It is not surprising that an extremely aggres-
sive filtering step reduces too much the informa-
tion available to the classifiers, leading the overall
12For JNLPBA, CoNLL 2002 & 2003 and Tern 2004, re-
sults are obtained using the official evaluation software made
available by the organizers of the tasks.
performance to decrease.
SIE achieves results close to the best systems in
all tasks13. It is worth noting that state-of-the-art
IE systems often exploit external, domain-specific
information (e.g. gazetteers (Carreras et al, 2002)
and lexical resources (Zhou and Su, 2004)) while
SIE adopts exactly the same feature set and does
not use any external or task dependent knowledge
source.
8 Conclusion and Future Work
The portability, the language independence and
the efficiency of SIE suggest its applicability in
practical problems (e.g. semantic web, infor-
mation extraction from biological data) in which
huge collections of texts have to be processed ef-
ficiently. In this perspective we are pursuing the
recognition of bio-entities from several thousands
of MEDLINE abstracts. In addition, the effective-
ness of instance filtering will allow us to experi-
ment with complex kernel methods. For the fu-
ture, we plan to implement more aggressive in-
stance filtering schemata for Entity Recognition,
by performing a deeper semantic analysis of the
texts.
Acknowledgments
SIE was developed in the context of the IST-
Dot.Kom project (http://www.dot-kom.
org), sponsored by the European Commission as
part of the Framework V (grant IST-2001-34038).
Claudio Giuliano and Lorenza Romano have been
supported by the ONTOTEXT project, funded by
the Autonomous Province of Trento under the
FUP-2004 research program.
References
Andrew J. Carlson, ChadM. Cumby, Jeff L. Rosen, and
Dan Roth. 1999. SNoW user?s guide. Technical
Report UIUCDCS-DCS-R-99-210, Department of
Computer Science, University of Illinois at Urbana-
Champaign, April.
Xavier Carreras, Llu??s Ma?rques, and Llu??s Padro?.
2002. Named entity extraction using adaboost. In
Proceedings of CoNLL-2002, Taipei, Taiwan.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
13Note that the TERN results cannot be disclosed, so no di-
rect comparison can be provided. For the reasons mentioned
in (Lavelli et al, 2004), direct comparison cannot be provided
for Seminar Announcements as well.
15
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Fabio Ciravegna. 2000. Learning to tag for infor-
mation extraction. In F. Ciravegna, R. Basili, and
R. Gaizauskas, editors, Proceedings of the ECAI
workshop on Machine Learning for Information Ex-
traction, Berlin.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?
297.
Chad Cumby and W. Yih. 2003. FEX user guide.
Technical report, Department of Computer Science,
University of Illinois at Urbana-Champaign, April.
Aidan Finn and Nicholas Kushmerick. 2004. Multi-
level boundary classification for information extrac-
tion. In Proceedings of the 15th European Confer-
ence on Machine Learning, Pisa, Italy.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Walter Daele-
mans and Miles Osborne, editors, Proceedings of
CoNLL-2003, pages 168?171. Edmonton, Canada.
Dayne Freitag and Nicholas Kushmerick. 2000.
Boosted wrapper induction. In Proceedings of the
17th National Conference on Artificial Intelligence
(AAAI 2000), pages 577?583.
Dayne Freitag. 1998. Machine Learning for Informa-
tion Extraction in Informal Domains. Ph.D. thesis,
Carnegie Mellon University.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2005. Simple information extraction (SIE).
Technical report, ITC-irst.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and
Raffaella Rinaldi. 2005a. Instance filtering for en-
tity recognition. SIGKDD Explorations (special is-
sue on Text Mining and Natural Language Process-
ing), 7(1):11?18, June.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and
Raffaella Rinaldi. 2005b. Instance pruning by fil-
tering uninformative words: an Information Extrac-
tion case study. In Proceedings of the Sixth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-2005), Mexico
City, Mexico, 13-19 February.
T. Joachims. 1998. Making large-scale support
vector machine learning practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
J. Kim, T. Ohta, Y. Tateishi, and J. Tsujii. 2003. Ge-
nia corpus - a semantically annotated corpus for bio-
textmining. Bioinformatics, 19(Suppl.1):180?182.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recog-
nition task at JNLPBA. In N. Collier, P. Ruch,
and A. Nazarenko, editors, Proceedings of the In-
ternational Joint Workshop on Natural Language
Processing in Biomedicine and its Applications
(JNLPBA-2004), pages 70?75, Geneva, Switzer-
land, August 28?29.
A. Lavelli, M. Califf, F. Ciravegna, D. Freitag, C. Giu-
liano, N. Kushmerick, and L. Romano. 2004. IE
evaluation: Criticisms and recommendations. In
AAAI-04 Workshop on Adaptive Text Extraction and
Mining (ATEM-2004), San Jose, California.
Bhavani Raskutti and Adam Kowalczyk. 2004.
Extreme re-balancing for SVMs: a case study.
SIGKDD Explor. Newsl., 6(1):60?69.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142?147. Edmon-
ton, Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of
CoNLL-2002, pages 155?158. Taipei, Taiwan.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Douglas H. Fisher, editor, Proceedings of the
14th International Conference on Machine Learning
(ICML-97), pages 412?420, Nashville, US. Morgan
Kaufmann Publishers, San Francisco, US.
Guo Dong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recogni-
tion. In Proceedings of 2004 Joint Workshop on Nat-
ural Processing in Biomedicine and its Applications,
Geneva, Switzerland.
16
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 33?38,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx
?
, Su Nam Kim
?
, Zornitsa Kozareva
?
, Preslav Nakov
?
,
Diarmuid
?
O S
?
eaghdha
?
, Sebastian Pad
?
o
?
, Marco Pennacchiotti
??
,
Lorenza Romano
??
, Stan Szpakowicz
??
Abstract
SemEval-2 Task 8 focuses on Multi-way
classification of semantic relations between
pairs of nominals. The task was designed
to compare different approaches to seman-
tic relation classification and to provide a
standard testbed for future research. This
paper defines the task, describes the train-
ing and test data and the process of their
creation, lists the participating systems (10
teams, 28 runs), and discusses their results.
1 Introduction
SemEval-2010 Task 8 focused on semantic rela-
tions between pairs of nominals. For example, tea
and ginseng are in an ENTITY-ORIGIN relation in
?The cup contained tea from dried ginseng.?. The
automatic recognition of semantic relations has
many applications, such as information extraction,
document summarization, machine translation, or
construction of thesauri and semantic networks.
It can also facilitate auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing, and recognizing textual entailment.
Our goal was to create a testbed for automatic
classification of semantic relations. In developing
the task we met several challenges: selecting a
suitable set of relations, specifying the annotation
procedure, and deciding on the details of the task
itself. They are discussed briefly in Section 2; see
also Hendrickx et al (2009), which includes a sur-
vey of related work. The direct predecessor of Task
8 was Classification of semantic relations between
nominals, Task 4 at SemEval-1 (Girju et al, 2009),
?
University of Lisbon, iris@clul.ul.pt
?
University of Melbourne, snkim@csse.unimelb.edu.au
?
Information Sciences Institute/University of Southern
California, kozareva@isi.edu
?
National University of Singapore, nakov@comp.nus.edu.sg
?
University of Cambridge, do242@cl.cam.ac.uk
?
University of Stuttgart, pado@ims.uni-stuttgart.de
??
Yahoo! Inc., pennacc@yahoo-inc.com
??
Fondazione Bruno Kessler, romano@fbk.eu
??
University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
which had a separate binary-labeled dataset for
each of seven relations. We have defined SemEval-
2010 Task 8 as a multi-way classification task in
which the label for each example must be chosen
from the complete set of ten relations and the map-
ping from nouns to argument slots is not provided
in advance. We also provide more data: 10,717 an-
notated examples, compared to 1,529 in SemEval-1
Task 4.
2 Dataset Creation
2.1 The Inventory of Semantic Relations
We first decided on an inventory of semantic rela-
tions. Ideally, it should be exhaustive (enable the
description of relations between any pair of nomi-
nals) and mutually exclusive (each pair of nominals
in context should map onto only one relation). The
literature, however, suggests that no relation inven-
tory satisfies both needs, and, in practice, some
trade-off between them must be accepted.
As a pragmatic compromise, we selected nine
relations with coverage sufficiently broad to be of
general and practical interest. We aimed at avoid-
ing semantic overlap as much as possible. We
included, however, two groups of strongly related
relations (ENTITY-ORIGIN / ENTITY-DESTINA-
TION and CONTENT-CONTAINER / COMPONENT-
WHOLE / MEMBER-COLLECTION) to assess mod-
els? ability to make such fine-grained distinctions.
Our inventory is given below. The first four were
also used in SemEval-1 Task 4, but the annotation
guidelines have been revised, and thus no complete
continuity should be assumed.
Cause-Effect (CE). An event or object leads to an
effect. Example: those cancers were caused
by radiation exposures
Instrument-Agency (IA). An agent uses an in-
strument. Example: phone operator
Product-Producer (PP). A producer causes a
product to exist. Example: a factory manu-
factures suits
33
Content-Container (CC). An object is physically
stored in a delineated area of space. Example:
a bottle full of honey was weighed
Entity-Origin (EO). An entity is coming or is de-
rived from an origin (e.g., position or mate-
rial). Example: letters from foreign countries
Entity-Destination (ED). An entity is moving to-
wards a destination. Example: the boy went
to bed
Component-Whole (CW). An object is a com-
ponent of a larger whole. Example: my
apartment has a large kitchen
Member-Collection (MC). A member forms a
nonfunctional part of a collection. Example:
there are many trees in the forest
Message-Topic (MT). A message, written or spo-
ken, is about a topic. Example: the lecture
was about semantics
2.2 Annotation Guidelines
We defined a set of general annotation guidelines
as well as detailed guidelines for each semantic
relation. Here, we describe the general guidelines,
which delineate the scope of the data to be col-
lected and state general principles relevant to the
annotation of all relations.
1
Our objective is to annotate instances of seman-
tic relations which are true in the sense of hold-
ing in the most plausible truth-conditional inter-
pretation of the sentence. This is in the tradition
of the Textual Entailment or Information Valida-
tion paradigm (Dagan et al, 2009), and in con-
trast to ?aboutness? annotation such as semantic
roles (Carreras and M`arquez, 2004) or the BioNLP
2009 task (Kim et al, 2009) where negated rela-
tions are also labelled as positive. Similarly, we
exclude instances of semantic relations which hold
only in speculative or counterfactural scenarios. In
practice, this means disallowing annotations within
the scope of modals or negations, e.g., ?Smoking
may/may not have caused cancer in this case.?
We accept as relation arguments only noun
phrases with common-noun heads. This distin-
guishes our task from much work in Information
Extraction, which tends to focus on specific classes
of named entities and on considerably more fine-
grained relations than we do. Named entities are a
specific category of nominal expressions best dealt
1
The full task guidelines are available at http://docs.
google.com/View?id=dfhkmm46_0f63mfvf7
with using techniques which do not apply to com-
mon nouns. We only mark up the semantic heads of
nominals, which usually span a single word, except
for lexicalized terms such as science fiction.
We also impose a syntactic locality requirement
on example candidates, thus excluding instances
where the relation arguments occur in separate sen-
tential clauses. Permissible syntactic patterns in-
clude simple and relative clauses, compounds, and
pre- and post-nominal modification. In addition,
we did not annotate examples whose interpretation
relied on discourse knowledge, which led to the
exclusion of pronouns as arguments. Please see
the guidelines for details on other issues, includ-
ing noun compounds, aspectual phenomena and
temporal relations.
2.3 The Annotation Process
The annotation took place in three rounds. First,
we manually collected around 1,200 sentences for
each relation through pattern-based Web search. In
order to ensure a wide variety of example sentences,
we used a substantial number of patterns for each
relation, typically between one hundred and several
hundred. Importantly, in the first round, the relation
itself was not annotated: the goal was merely to
collect positive and near-miss candidate instances.
A rough aim was to have 90% of candidates which
instantiate the target relation (?positive instances?).
In the second round, the collected candidates for
each relation went to two independent annotators
for labeling. Since we have a multi-way classifi-
cation task, the annotators used the full inventory
of nine relations plus OTHER. The annotation was
made easier by the fact that the cases of overlap
were largely systematic, arising from general phe-
nomena like metaphorical use and situations where
more than one relation holds. For example, there is
a systematic potential overlap between CONTENT-
CONTAINER and ENTITY-DESTINATION depend-
ing on whether the situation described in the sen-
tence is static or dynamic, e.g., ?When I came,
the <e1>apples</e1> were already put in the
<e2>basket</e2>.? is CC(e1, e2), while ?Then,
the <e1>apples</e1> were quickly put in the
<e2>basket</e2>.? is ED(e1, e2).
In the third round, the remaining disagreements
were resolved, and, if no consensus could be
achieved, the examples were removed. Finally, we
merged all nine datasets to create a set of 10,717
instances. We released 8,000 for training and kept
34
the rest for testing.
2
Table 1 shows some statistics about the dataset.
The first column (Freq) shows the absolute and rel-
ative frequencies of each relation. The second col-
umn (Pos) shows that the average share of positive
instances was closer to 75% than to 90%, indicating
that the patterns catch a substantial amount of ?near-
miss? cases. However, this effect varies a lot across
relations, causing the non-uniform relation distribu-
tion in the dataset (first column).
3
After the second
round, we also computed inter-annotator agreement
(third column, IAA). Inter-annotator agreement
was computed on the sentence level, as the per-
centage of sentences for which the two annotations
were identical. That is, these figures can be inter-
preted as exact-match accuracies. We do not report
Kappa, since chance agreement on preselected can-
didates is difficult to estimate.
4
IAA is between
60% and 95%, again with large relation-dependent
variation. Some of the relations were particularly
easy to annotate, notably CONTENT-CONTAINER,
which can be resolved through relatively clear cri-
teria, despite the systematic ambiguity mentioned
above. ENTITY-ORIGIN was the hardest relation to
annotate. We encountered ontological difficulties
in defining both Entity (e.g., in contrast to Effect)
and Origin (as opposed to Cause). Our numbers
are on average around 10% higher than those re-
ported by Girju et al (2009). This may be a side
effect of our data collection method. To gather
1,200 examples in realistic time, we had to seek
productive search query patterns, which invited
certain homogeneity. For example, many queries
for CONTENT-CONTAINER centered on ?usual sus-
pect? such as box or suitcase. Many instances of
MEMBER-COLLECTION were collected on the ba-
sis of from available lists of collective names.
3 The Task
The participating systems had to solve the follow-
ing task: given a sentence and two tagged nominals,
predict the relation between those nominals and the
direction of the relation.
We released a detailed scorer which outputs (1) a
confusion matrix, (2) accuracy and coverage, (3)
2
This set includes 891 examples from SemEval-1 Task 4.
We re-annotated them and assigned them as the last examples
of our training dataset to ensure that the test set was unseen.
3
To what extent our candidate selection produces a biased
sample is a question that we cannot address within this paper.
4
We do not report Pos or IAA for OTHER, since OTHER is
a pseudo-relation that was not annotated in its own right. The
numbers would therefore not be comparable to other relations.
Relation Freq Pos IAA
Cause-Effect 1331 (12.4%) 91.2% 79.0%
Component-Whole 1253 (11.7%) 84.3% 70.0%
Entity-Destination 1137 (10.6%) 80.1% 75.2%
Entity-Origin 974 (9.1%) 69.2% 58.2%
Product-Producer 948 (8.8%) 66.3% 84.8%
Member-Collection 923 (8.6%) 74.7% 68.2%
Message-Topic 895 (8.4%) 74.4% 72.4%
Content-Container 732 (6.8%) 59.3% 95.8%
Instrument-Agency 660 (6.2%) 60.8% 65.0%
Other 1864 (17.4%) N/A
4
N/A
4
Total 10717 (100%)
Table 1: Annotation Statistics. Freq: Absolute and
relative frequency in the dataset; Pos: percentage
of ?positive? relation instances in the candidate set;
IAA: inter-annotator agreement
precision (P), recall (R), and F
1
-Score for each
relation, (4) micro-averaged P, R, F
1
, (5) macro-
averaged P, R, F
1
. For (4) and (5), the calculations
ignored the OTHER relation. Our official scoring
metric is macro-averaged F
1
-Score for (9+1)-way
classification, taking directionality into account.
The teams were asked to submit test data pre-
dictions for varying fractions of the training data.
Specifically, we requested results for the first 1000,
2000, 4000, and 8000 training instances, called
TD1 through TD4. TD4 was the full training set.
4 Participants and Results
Table 2 lists the participants and provides a rough
overview of the system features. Table 3 shows the
results. Unless noted otherwise, all quoted numbers
are F
1
-Scores.
Overall Ranking and Training Data. We rank
the teams by the performance of their best system
on TD4, since a per-system ranking would favor
teams with many submitted runs. UTD submit-
ted the best system, with a performance of over
82%, more than 4% better than the second-best
system. FBK IRST places second, with 77.62%,
a tiny margin ahead of ISI (77.57%). Notably, the
ISI system outperforms the FBK IRST system for
TD1 to TD3, where it was second-best. The accu-
racy numbers for TD4 (Acc TD4) lead to the same
overall ranking: micro- versus macro-averaging
does not appear to make much difference either.
A random baseline gives an uninteresting score of
6%. Our competitive baseline system is a simple
Naive Bayes classifier which relies on words in the
sentential context only; two systems scored below
this baseline.
35
System Institution Team Description Res. Class.
Baseline Task organizers local context of 2 words only BN
ECNU-SR-1 East China Normal
University
Man Lan, Yuan
Chen, Zhimin
Zhou, Yu Xu
stem, POS, syntactic patterns S SVM
(multi)
ECNU-SR-2,3 features like ECNU-SR-1, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-4 stem, POS, syntactic patterns,
hyponymy and meronymy rela-
tions
WN,
S
SVM
(multi)
ECNU-SR-5,6 features like ECNU-SR-4, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-7 majority vote of ECNU-1,2,4,5
FBK IRST-6C32 Fondazione Bruno
Kessler
Claudio Giu-
liano, Kateryna
Tymoshenko
3-word window context features
(word form, part of speech, or-
thography) + Cyc; parameter
estimation by optimization on
training set
Cyc SVM
FBK IRST-12C32 FBK IRST-6C32 + distance fea-
tures
FBK IRST-12VBC32 FBK IRST-12C32 + verbs
FBK IRST-6CA,
-12CA, -12VBCA
features as above, parameter es-
timation by cross-validation
FBK NK-RES1 Fondazione Bruno
Kessler
Matteo Negri,
Milen Kouylekov
collocations, glosses, semantic
relations of nominals + context
features
WN BN
FBK NK-RES 2,3,4 like FBK NK-RES1 with differ-
ent context windows and collo-
cation cutoffs
ISI Information Sci-
ences Institute,
University of
Southern Califor-
nia
Stephen Tratz features from different re-
sources, a noun compound
relation system, and various
feature related to capitalization,
affixes, closed-class words
WN,
RT, G
ME
ISTI-1,2 Istituto di sci-
enca e tecnologie
dell?informazione
?A. Faedo?
Andrea Esuli,
Diego Marcheg-
giani, Fabrizio
Sebastiani
Boosting-based classification.
Runs differ in their initializa-
tion.
WN 2S
JU Jadavpur Univer-
sity
Santanu Pal, Partha
Pakray, Dipankar
Das, Sivaji Bandy-
opadhyay
Verbs, nouns, and prepositions;
seed lists for semantic relations;
parse features and NEs
WN,
S
CRF
SEKA Hungarian
Academy of
Sciences
Eszter Simon, An-
dras Kornai
Levin and Roget classes, n-
grams; other grammatical and
formal features
RT,
LC
ME
TUD-base Technische Univer-
sit?at Darmstadt
Gy?orgy Szarvas,
Iryna Gurevych
word, POS n-grams, depen-
dency path, distance
S ME
TUD-wp TUD-base + ESA semantic re-
latedness scores
+WP
TUD-comb TUD-base + own semantic relat-
edness scores
+WP,WN
TUD-comb-threshold TUD-comb with higher thresh-
old for OTHER
UNITN University of
Trento
Fabio Celli punctuation, context words,
prepositional patterns, estima-
tion of semantic relation
? DR
UTD University of Texas
at Dallas
Bryan Rink, Sanda
Harabagiu
context wods, hypernyms, POS,
dependencies, distance, seman-
tic roles, Levin classes, para-
phrases
WN,
S, G,
PB/NB,
LC
SVM,
2S
Table 2: Participants of SemEval-2010 Task 8. Res: Resources used (WN: WordNet data; WP:
Wikipedia data; S: syntax; LC: Levin classes; G: Google n-grams, RT: Roget?s Thesaurus, PB/NB:
PropBank/NomBank). Class: Classification style (ME: Maximum Entropy; BN: Bayes Net; DR: Decision
Rules/Trees; CRF: Conditional Random Fields; 2S: two-step classification)
36
System TD1 TD2 TD3 TD4 Acc TD4 Rank Best Cat Worst Cat-9
Baseline 33.04 42.41 50.89 57.52 50.0 - MC (75.1) IA (28.0)
ECNU-SR-1 52.13 56.58 58.16 60.08 57.1
4
CE (79.7) IA (32.2)
ECNU-SR-2 46.24 47.99 69.83 72.59 67.1 CE (84.4) IA (52.2)
ECNU-SR-3 39.89 42.29 65.47 68.50 62.0 CE (83.4) IA (46.5)
ECNU-SR-4 67.95 70.58 72.99 74.82 70.5 CE (84.6) IA (61.4)
ECNU-SR-5 49.32 50.70 72.63 75.43 70.2 CE (85.1) IA (60.7)
ECNU-SR-6 42.88 45.54 68.87 72.19 65.8 CE (85.2) IA (56.7)
ECNU-SR-7 58.67 58.87 72.79 75.21 70.2 CE (86.1) IA (61.8)
FBK IRST-6C32 60.19 67.31 71.78 76.81 72.4
2
ED (82.6) IA (69.4)
FBK IRST-12C32 60.66 67.91 72.04 76.91 72.4 MC (84.2) IA (68.8)
FBK IRST-12VBC32 62.64 69.86 73.19 77.11 72.3 ED (85.9) PP (68.1)
FBK IRST-6CA 60.58 67.14 71.63 76.28 71.4 CE (82.3) IA (67.7)
FBK IRST-12CA 61.33 67.80 71.65 76.39 71.4 ED (81.8) IA (67.5)
FBK IRST-12VBCA 63.61 70.20 73.40 77.62 72.8 ED (86.5) IA (67.3)
FBK NK-RES1 55.71
?
64.06
?
67.80
?
68.02 62.1
7
ED (77.6) IA (52.9)
FBK NK-RES2 54.27
?
63.68
?
67.08
?
67.48 61.4 ED (77.4) PP (55.2)
FBK NK-RES3 54.25
?
62.73
?
66.11
?
66.90 60.5 MC (76.7) IA (56.3)
FBK NK-RES4 44.11
?
58.85
?
63.06
?
65.84 59.4 MC (76.1) IA/PP (58.0)
ISI 66.68 71.01 75.51 77.57 72.7 3 CE (87.6) IA (61.5)
ISTI-1 50.49
?
55.80
?
61.14
?
68.42 63.2
6
ED (80.7) PP (53.8)
ISTI-2 50.69
?
54.29
?
59.77
?
66.65 61.5 ED (80.2) IA (48.9)
JU 41.62
?
44.98
?
47.81
?
52.16 50.2 9 CE (75.6) IA (27.8)
SEKA 51.81 56.34 61.10 66.33 61.9 8 CE (84.0) PP (43.7)
TUD-base 50.81 54.61 56.98 60.50 56.1
5
CE (80.7) IA (31.1)
TUD-wp 55.34 60.90 63.78 68.00 63.5 ED (82.9) IA (44.1)
TUD-comb 57.84 62.52 66.41 68.88 64.6 CE (83.8) IA (46.8)
TUD-comb-? 58.35 62.45 66.86 69.23 65.4 CE (83.4) IA (46.9)
UNITN 16.57
?
18.56
?
22.45
?
26.67 27.4 10 ED (46.4) PP (0)
UTD 73.08 77.02 79.93 82.19 77.9 1 CE (89.6) IA (68.5)
Table 3: F
1
-Score of all submitted systems on the test dataset as a function of training data: TD1=1000,
TD2=2000, TD3=4000, TD4=8000 training examples. Official results are calculated on TD4. The results
marked with
?
were submitted after the deadline. The best-performing run for each participant is italicized.
As for the amount of training data, we see a sub-
stantial improvement for all systems between TD1
and TD4, with diminishing returns for the transi-
tion between TD3 and TD4 for many, but not all,
systems. Overall, the differences between systems
are smaller for TD4 than they are for TD1. The
spread between the top three systems is around 10%
at TD1, but below 5% at TD4. Still, there are clear
differences in the influence of training data size
even among systems with the same overall archi-
tecture. Notably, ECNU-SR-4 is the second-best
system at TD1 (67.95%), but gains only 7% from
the eightfold increase of the size of the training data.
At the same time, ECNU-SR-3 improves from less
than 40% to almost 69%. The difference between
the systems is that ECNU-SR-4 uses a multi-way
classifier including the class OTHER, while ECNU-
SR-3 uses binary classifiers and assigns OTHER
if no other relation was assigned with p>0.5. It
appears that these probability estimates for classes
are only reliable enough for TD3 and TD4.
The Influence of System Architecture. Almost
all systems used either MaxEnt or SVM classifiers,
with no clear advantage for either. Similarly, two
systems, UTD and ISTI (rank 1 and 6) split the task
into two classification steps (relation and direction),
but the 2nd- and 3rd-ranked systems do not. The
use of a sequence model such as a CRF did not
show a benefit either.
The systems use a variety of resources. Gener-
ally, richer feature sets lead to better performance
(although the differences are often small ? compare
the different FBK IRST systems). This improve-
ment can be explained by the need for semantic
generalization from training to test data. This need
can be addressed using WordNet (contrast ECNU-1
to -3 with ECNU-4 to -6), the Google n-gram col-
lection (see ISI and UTD), or a ?deep? semantic
resource (FBK IRST uses Cyc). Yet, most of these
resources are also included in the less successful
systems, so beneficial integration of knowledge
sources into semantic relation classification seems
to be difficult.
System Combination. The differences between
the systems suggest that it might be possible to
achieve improvements by building an ensemble
37
system. When we combine the top three systems
(UTD, FBK IRST-12VBCA, and ISI) by predict-
ing their majority vote, or OTHER if there was none,
we obtain a small improvement over the UTD sys-
tem with an F
1
-Score of 82.79%. A combination of
the top five systems using the same method shows
a worse performance, however (80.42%). This sug-
gests that the best system outperforms the rest by
a margin that cannot be compensated with system
combination, at least not with a crude majority vote.
We see a similar pattern among the ECNU systems,
where the ECNU-SR-7 combination system is out-
performed by ECNU-SR-5, presumably since it
incorporates the inferior ECNU-SR-1 system.
Relation-specific Analysis. We also analyze the
performance on individual relations, especially the
extremes. There are very stable patterns across all
systems. The best relation (presumably the eas-
iest to classify) is CE, far ahead of ED and MC.
Notably, the performance for the best relation is
75% or above for almost all systems, with compar-
atively small differences between the systems. The
hardest relation is generally IA, followed by PP.
5
Here, the spread among the systems is much larger:
the highest-ranking systems outperform others on
the difficult relations. Recall was the main prob-
lem for both IA and PP: many examples of these
two relations are misclassified, most frequently as
OTHER. Even at TD4, these datasets seem to be
less homogeneous than the others. Intriguingly, PP
shows a very high inter-annotator agreement (Ta-
ble 1). Its difficulty may therefore be due not to
questionable annotation, but to genuine variability,
or at least the selection of difficult patterns by the
dataset creator. Conversely, MC, among the easiest
relations to model, shows only a modest IAA.
Difficult Instances. There were 152 examples
that are classified incorrectly by all systems. We
analyze them, looking for sources of errors. In ad-
dition to a handful of annotation errors and some
borderline cases, they are made up of instances
which illustrate the limits of current shallow mod-
eling approaches in that they require more lexical
knowledge and complex reasoning. A case in point:
The bottle carrier converts your <e1>bottle</e1>
into a <e2>canteen</e2>. This instance of
OTHER is misclassified either as CC (due to the
5
The relation OTHER, which we ignore in the overall F
1
-
score, does even worse, often below 40%. This is to be ex-
pected, since the OTHER examples in our datasets are near
misses for other relations, thus making a very incoherent class.
nominals) or as ED (because of the preposition
into). Another example: [...] <e1>Rudders</e1>
are used by <e2>towboats</e2> and other ves-
sels that require a high degree of manoeuvrability.
This is an instance of CW misclassified as IA, prob-
ably on account of the verb use which is a frequent
indicator of an agentive relation.
5 Discussion and Conclusion
There is little doubt that 19-way classification is a
non-trivial challenge. It is even harder when the
domain is lexical semantics, with its idiosyncrasies,
and when the classes are not necessarily disjoint,
despite our best intentions. It speaks to the success
of the exercise that the participating systems? per-
formance was generally high, well over an order
of magnitude above random guessing. This may
be due to the impressive array of tools and lexical-
semantic resources deployed by the participants.
Section 4 suggests a few ways of interpreting
and analyzing the results. Long-term lessons will
undoubtedly emerge from the workshop discussion.
One optimistic-pessimistic conclusion concerns the
size of the training data. The notable gain TD3?
TD4 suggests that even more data would be helpful,
but that is so much easier said than done: it took
the organizers well in excess of 1000 person-hours
to pin down the problem, hone the guidelines and
relation definitions, construct sufficient amounts of
trustworthy training data, and run the task.
References
X. Carreras and L. M`arquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role label-
ing. In Proc. CoNLL-04, Boston, MA.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009.
Recognizing textual entailment: Rational, evalua-
tion and approaches. Natural Language Engineer-
ing, 15(4):i?xvii.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2009. Classification of semantic
relations between nominals. Language Resources
and Evaluation, 43(2):105?121.
I. Hendrickx, S. Kim, Z. Kozareva, P. Nakov, D.
?
O
S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Romano,
and S. Szpakowicz. 2009. SemEval-2010 Task
8: Multi-way classification of semantic relations be-
tween pairs of nominals. In Proc. NAACL Workshop
on Semantic Evaluations, Boulder, CO.
J. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of BioNLP?09 shared task on event
extraction. In Proc. BioNLP-09, Boulder, CO.
38
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104?107,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BART: A Multilingual Anaphora Resolution System
Samuel Broscheit?, Massimo Poesio?, Simone Paolo Ponzetto?, Kepa Joseba Rodriguez?,
Lorenza Romano?, Olga Uryupina?, Yannick Versley?, Roberto Zanoli?
?Seminar fu?r Computerlinguistik, University of Heidelberg
?CiMeC, University of Trento
?Fondazione Bruno Kessler
?SFB 833, University of Tu?bingen
broscheit@cl.uni-heidelberg.de, massimo.poesio@unitn.it,
ponzetto@cl.uni-heidelberg.de, kepa.rodriguez@unitn.it,
romano@fbk.eu, uryupina@gmail.com,
versley@sfs.uni-tuebingen.de, zanoli@fbk.eu
Abstract
BART (Versley et al, 2008) is a highly mod-
ular toolkit for coreference resolution that
supports state-of-the-art statistical approaches
and enables efficient feature engineering. For
the SemEval task 1 on Coreference Resolu-
tion, BART runs have been submitted for Ger-
man, English, and Italian.
BART relies on a maximum entropy-based
classifier for pairs of mentions. A novel entity-
mention approach based on Semantic Trees is
at the moment only supported for English.
1 Introduction
This paper presents a multilingual coreference reso-
lution system based on BART (Versley et al, 2008).
BART is a modular toolkit for coreference resolution
that supports state-of-the-art statistical approaches
to the task and enables efficient feature engineer-
ing. BART has originally been created and tested
for English, but its flexible modular architecture en-
sures its portability to other languages and domains.
In SemEval-2010 task 1 on Coreference Resolution,
BART has shown reliable performance for English,
German and Italian.
In our SemEval experiments, we mainly focus on
extending BART to cover multiple languages. Given
a corpus in a new language, one can re-train BART
to obtain baseline results. Such a language-agnostic
system, however, is only used as a starting point:
substantial improvements can be achieved by incor-
porating language-specific information with the help
of the Language Plugin. This design provides ef-
fective separation between linguistic and machine
learning aspects of the problem.
2 BART Architecture
The BART toolkit has five main components: pre-
processing pipeline, mention factory, feature extrac-
tion module, decoder and encoder. In addition, an
independent LanguagePlugin module handles all the
language specific information and is accessible from
any component. The architecture is shown on Figure
1. Each module can be accessed independently and
thus adjusted to leverage the system?s performance
on a particular language or domain.
The preprocessing pipeline converts an input doc-
ument into a set of lingustic layers, represented
as separate XML files. The mention factory uses
these layers to extract mentions and assign their
basic properties (number, gender etc). The fea-
ture extraction module describes pairs of mentions
{M
i
,M
j
}, i < j as a set of features.
The decoder generates training examples through
a process of sample selection and learns a pairwise
classifier. Finally, the encoder generates testing ex-
amples through a (possibly distinct) process of sam-
ple selection, runs the classifier and partitions the
mentions into coreference chains.
3 Language-specific issues
Below we briefly describe our language-specific ex-
tensions to BART. These issues are addressed in
more details in our recent papers (Broscheit et al,
2010; Poesio et al, 2010).
3.1 Mention Detection
Robust mention detection is an essential component
of any coreference resolution system. BART sup-
ports different pipelines for mention detection. The
104
Parser
Dep-to-Const
Converter
Morphology
Preprocessing
Mention
Factory
Decoder
Basic features
Syntactic features
Knowledge-based
features
MaxEnt
Classifier
Mention
(with basic
 properties):
- Number
- Gender
- Mention Type
- Modifiers
Unannotated
Text
Coreference
Chains
LanguagePlugin
Figure 1: BART architecture
choice of a pipeline depends crucially on the avail-
ability of linguistic resources for a given language.
For English and German, we use the Parsing
Pipeline and Mention Factory to extract mentions.
The parse trees are used to identify minimal and
maximal noun projections, as well as additional fea-
tures such as number, gender, and semantic class.
For English, we use parses from a state-of-the-art
constituent parser (Petrov et al, 2006) and extract
all base noun phrases as mentions. For German,
the SemEval dependency tree is transformed to a
constituent representation and minimal and maxi-
mal phrases are extracted for all nominal elements
(pronouns, common nouns, names), except when the
noun phrase is in a non-referring syntactic position
(for example, expletive ?es?, predicates in copula
constructions).
For Italian, we use the EMD Pipeline and Men-
tion Factory. The Typhoon (Zanoli et al, 2009)
and DEMention (Biggio et al, 2009) systems were
used to recognize mentions in the test set. For each
mention, its head and extension were considered.
The extension was learned by using the mention an-
notation provided in the training set (13th column)
whereas the head annotation was learned by exploit-
ing the information produced by MaltParser (Nivre
et al, 2007). In addition to the features extracted
from the training set, such as prefixes and suffixes
(1-4 characters) and orthographic information (capi-
talization and hyphenation), a number of features ex-
tracted by using external resources were used: men-
tions recognized by TextPro (http://textpro.fbk.eu),
gazetteers of generic proper nouns extracted from
the Italian phone-book and Wikipedia, and other fea-
tures derived from WordNet. Each of these features
was extracted in a local context of ?2 words.
3.2 Features
We view coreference resolution as a binary classifi-
cation problem. Each classification instance consists
of two markables, i.e. an anaphor and potential an-
tecedent. Instances are modeled as feature vectors
(cf. Table 1) and are handed over to a binary clas-
sifier that decides, given the features, whether the
anaphor and the candidate are coreferent or not. All
the feature values are computed automatically, with-
out any manual intervention.
Basic feature set. We use the same set of rela-
tively language-independent features as a backbone
of our system, extending it with a few language-
specific features for each subtask. Most of them are
used by virtually all the state-of-the-art coreference
resolution systems. A detailed description can be
found, for example, in (Soon et al, 2001).
English. Our English system is based on a novel
model of coreference. The key concept of our model
is a Semantic Tree ? a filecard associated with each
discourse entity containing the following fields:
? Types: the list of types for mentions of a given
entity. For example, if an entity contains the
mention ?software from India?, the shallow
predicate ?software? is added to the types.
? Attributes: this field collects the premodifiers.
For instance, if one of the mentions is ?the ex-
pensive software? the shallow attribute ?expen-
sive? is added to the list of attributes.
? Relations: this field collects the prepositional
postmodifiers. If an entity contains the men-
tion ?software from India?, the shallow relation
?from(India)? is added to the list of relations.
105
For each mention BART creates such a filecard
using syntactic information. If the classifier decides
that both mentions are corefering, the filecard of
the anaphora is merged into the filecard of the an-
tecedent (cf. Section 3.3 below).
The SemanticTreeCompatibility feature
extractor checks whether individual slots of the
anaphor?s filecard are compatible with those of the
antecedent?s.
The StrudelRelatedness feature relies on
Strudel ? a distributional semantic model (Baroni et
al., 2010). We compute Strudel vectors for the sets
of types of the anaphor and the antecedent. The re-
latedness value is determined as the cosine between
the two.
German. We have tested extra features for Ger-
man in our previous study (Broscheit et al, 2010).
The NodeDistance feature measures the num-
ber of clause nodes (SIMPX, R-SIMPX) and preposi-
tional phrase nodes (PX) along the path between M
j
and M
i
in the parse tree.
The PartialMorphMatch feature is a sub-
string match with a morphological extension for
common nouns. In German the frequent use of
noun composition makes a simple string match for
common nouns unfeasible. The feature checks for
a match between the noun stems of M
i
and M
j
.
We extract the morphology with SMOR/Morphisto
(Schmid et al, 2004).
The GermanetRelatedness feature uses the
Pathfinder library for GermaNet (Finthammer and
Cramer, 2008) that computes and discretizes raw
scores into three categories of semantic relatedness.
In our experiments we use the measure from Wu and
Palmer (1994), which has been found to be the best
performing on our development data.
Italian. We have designed a feature to cover Ital-
ian aliasing patterns. A list of company/person des-
ignators (e.g., ?S.p.a? or ?D.ssa?) has been manually
crafted. We have collected patterns of name variants
for locations. Finally, we have relaxed abbreviation
constraints, allowing for lower-case characters in the
abbreviations. Our pilot experiments suggest that,
although a universal aliasing algorithm is able to re-
solve some coreference links between NEs, creating
a language-specific module boosts the system?s per-
formance for Italian substantially.
Basic feature set
MentionType(M
i
),MentionType(M
j
)
SemanticClass(M
i
), SemanticClass(M
j
)
GenderAgreement(M
i
,M
j
)
NumberAgreement(M
i
,M
j
)
AnimacyAgreement(M
i
,M
j
)
StringMatch(M
i
,M
j
)
Distance(M
i
,M
j
)
Basic features used for English and Italian
Alias(M
i
,M
j
)
Apposition(M
i
,M
j
)
FirstMention(M
i
)
English
IsSubject(M
i
)
SemanticTreeCompatibility(M
i
,M
j
)
StrudelRelatedness(M
i
,M
j
)
German
InQuotedSpeech(M
i
), InQuotedSpeech(M
j
)
NodeDistance(M
i
,M
j
)
PartialMorphMatch(M
i
,M
j
)
GermanetRelatedness(M
i
,M
j
)
Italian
AliasItalian(M
i
,M
j
)
Table 1: Features used by BART: each feature describes
a pair of mentions {M
i
,M
j
}, i < j, where M
i
is a can-
didate antecedent and M
j
is a candidate anaphor
3.3 Resolution Algorithm
The BART toolkit supports several models of coref-
erence (pairwise modeling, rankers, semantic trees),
as well as different machine learning algorithms.
Our final setting relies on a pairwise maximum en-
tropy classifier for Italian and German.
Our English system is based on an entity-mention
model of coreference. The key concept of our model
is a Semantic Tree - a filecard associated to each dis-
course entity (cf. Section 3.2). Semantic trees are
used for both computing feature values and guiding
the resolution process.
We start by creating a Semantic Tree for each
mention. We process the document from left to
right, trying to find an antecedent for each men-
tion (candidate anaphor). When the antecedent is
found, we extend its Semantic Tree with the types,
attributes and relations of the anaphor, provided
they are mutually compatible. Consider, for ex-
106
ample, a list of mentions, containing, among oth-
ers, ?software from India?, ?the software? and ?soft-
ware from China?. Initially, BART creates the fol-
lowing semantic trees: ?(type: software) (relation:
from(India))?, ?(type: software)? and ?(type: soft-
ware) (relation: from(China))?. When the second
mention gets resolved to the first one, their seman-
tic trees are merged to ?(type: software) (relation:
from(India)?. Therefore, when we attempt to resolve
the third mention, both candidate antecedents are re-
jected, as their relation attributes are incompatible
with ?from(China)?. This approach helps us avoid
erroneous links (such as the link between the second
and the third mentions in our example) by leveraging
entity-level information.
4 Evaluation
The system was evaluated on the SemEval task 1
corpus by using the SemEval scorer.
First, we have evaluated our mention detection
modules: the system?s ability to recognize both the
mention extensions and the heads in the regular set-
ting. BART has achieved the best score for men-
tion detection in German and has shown reliable
figures for English. For Italian, the moderate per-
formance level is due to the different algorithms
for identifying the heads: the MaltParser (trained
on TUT: http://www.di.unito.it/?tutreeb) produces a
more semantic representation, while the SemEval
scorer seems to adopt a more syntactic approach.
Second, we have evaluated the quality of our
coreference resolution modules. For German, BART
has shown better performance than all the other sys-
tems on the regular track.
For English, the only language targeted by all sys-
tems, BART shows good performance over all met-
rics in the regular setting, usually only outperformed
by systems that were tuned to a particular metric.
Finally, the Italian version of BART shows re-
liable figures for coreference resolution, given the
mention alignment problem discussed above.
5 Conclusion
We have presented BART ? a multilingual toolkit
for coreference resolution. Due to its highly modu-
lar architecture, BART allows for efficient language-
specific feature engineering. Our effort represents
the first steps towards building a freely available
coreference resolution system for many languages.
References
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222?254.
Silvana Marianela Bernaola Biggio, Claudio Giuliano,
Massimo Poesio, Yannick Versley, Olga Uryupina, and
Roberto Zanoli. 2009. Local entity detection and
recognition task. In Proc. of Evalita-09.
Samuel Broscheit, Simone Paolo Ponzetto, Yannick Ver-
sley, and Massimo Poesio. 2010. Extending BART to
provide a coreference resolution system for German.
In Proc. of LREC ?10.
Marc Finthammer and Irene Cramer. 2008. Explor-
ing and navigating: Tools for GermaNet. In Proc. of
LREC ?08.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov, Leon Barett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of COLING-ACL-06.
Massimo Poesio, Olga Uryupina, and Yannick Versley.
2010. Creating a coreference resolution system for
Italian. In Proc. of LREC ?10.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology cover-
ing derivation, composition and inflection. In Proc. of
LREC ?04.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics (Special Issue on Computational Anaphora
Resolution), 27(4):521?544.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Proceedings of the Linguistic Coreference Work-
shop at the International Conference on Language Re-
sources and Evaluation (LREC-2008).
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proc. of ACL-94, pages 133?
138.
Roberto Zanoli, Emiliano Pianta, and Claudio Giuliano.
2009. Named entity recognition through redundancy
driven classifier. In Proc. of Evalita-09.
107
