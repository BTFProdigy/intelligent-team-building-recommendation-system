Coling 2008: Companion volume ? Posters and Demonstrations, pages 87?90
Manchester, August 2008
Easily Identifiable Discourse Relations
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, Aravind Joshi
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104
Abstract
We present a corpus study of local dis-
course relations based on the Penn Dis-
course Tree Bank, a large manually anno-
tated corpus of explicitly or implicitly re-
alized relations. We show that while there
is a large degree of ambiguity in temporal
explicit discourse connectives, overall con-
nectives are mostly unambiguous and al-
low high-accuracy prediction of discourse
relation type. We achieve 93.09% accu-
racy in classifying the explicit relations
and 74.74% accuracy overall. In addition,
we show that some pairs of relations oc-
cur together in text more often than ex-
pected by chance. This finding suggests
that global sequence classification of the
relations in text can lead to better results,
especially for implicit relations.
1 Introduction
Discourse relations between textual units are con-
sidered key for the ability to properly interpret
or produce discourse. Various theories of dis-
course have been developed (Moore and Wiemer-
Hastings, 2003) and different relation taxonomies
have been proposed (Hobbs, 1979; McKeown,
1985; Mann and Thompson, 1988; Knott and
Sanders, 1998). Among the most cognitively
salient relations are causal (contingency), contrast
(comparison), and temporal.
Very often, the discourse relations are explicit,
signaled directly by the use of appropriate dis-
course connectives:
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
(E1) He is very tired because he played tennis all morning.
(E2) He is not very strong, but he can run amazingly fast.
(E3) We had some tea in the afternoon and later went to a
restaurant for a big dinner.
Discourse relations can also be implicit, inferred
by the context of the utterance and general world
knowledge.
(I1) I took my umbrella this morning. [because] The forecast
was rain in the afternoon.
(I2) She is never late for meetings. [but] He always arrives
10 minutes late.
(I3) She woke up early. [afterward] She had breakfast and
went for a walk in the park.
An additional complication for automatic clas-
sification of discourse relations is that even in the
presence of an explicit discourse connective, the
connective might be ambiguous between several
senses. For example, since can be used to signal
either a temporal or a contingency relation.
They have not spoken to each other since they argued last
fall. (Temporal)
I assumed you were not coming since you never replied to
the invitation. (Causal)
Several questions directly related to efforts in
automatic recognition of discourse relations arise:
In a general text, what is the proportion of ex-
plicit versus implicit relations? Since implicit rela-
tions are presumably harder to recognize automati-
cally, the larger their proportion, the more difficult
the overall prediction of discourse relation will be.
How ambiguous are discourse connectives?
The degree of ambiguity would give an upper
bound on the accuracy with which explicit rela-
tions can be identified. The more ambiguous dis-
course connectives are, the more difficult it would
be to automatically decide which discourse rela-
tion is expressed in a given sentence, even in the
presence of a connective.
87
In a text, are adjacent discourse relations inde-
pendent of each other or are certain sequences of
relations more likely? In the latter case, a ?dis-
course grammar? of text can be used and easy to
identify relations such as unambiguous explicit re-
lations can help determine the class of implicit re-
lations that immediately follow or precede them.
In this study, we address the above questions us-
ing the largest existing corpus manually annotated
with discourse relations?the Penn Discourse Tree
Bank (Prasad et al, 2008).
2 The Penn Discourse Tree Bank
The Penn Discourse Treebank (PDTB) is a new re-
source (Prasad et al, 2008) of annotated discourse
relations. The annotation covers the same 1 mil-
lion word Wall Street Journal (WSJ) corpus used
for the Penn Treebank (Marcus et al, 1994).
he PDTB is the first corpus to systematically
identify and distinguish explicit and implicit dis-
course relations. By definition, an explicit relation
is triggered by the presence of a discourse con-
nective which occurs overtly in the text. The dis-
course connective can essentially be viewed as a
discourse-level predicate which takes two clausal
arguments. For example, sentence E1 above could
be represented as BECAUSE(?He is very tired?,
?he played tennis all morning?). The corpus rec-
ognizes 100 such explicit connectives and contains
annotations for 19,458 explicit relations
1
.
The PDTB also contains provisions for the an-
notation of implicit discourse relations between
adjacent sentences which are inferred by the reader
but are not overtly marked by a discourse connec-
tive. In this case, the annotator was asked to pro-
vide a connective that best captured the inferred
relation. There are a total of 16,584 implicit rela-
tions annotated in the corpus.
2
In addition to discourse relations and their ar-
guments, the PDTB also provides the senses of
each relation(Miltsakaki et al, 2008). The tagset
of senses is organized hierarchically into three lev-
els - class, type, and subtype. The top class level
contains the four major semantic classes: Expan-
sion, Comparison, Contingency and Temporal.
1
The PDTB allows annotators to tag a relation with multi-
ple senses. In this work we count both of the annotated senses.
So even though there are only 18,459 explicit relations, there
are 19,458 explicit senses.
2
Again, because of multiple senses per relation, the 16,584
senses are part of 16,224 relations.
Class Explicit (%) Implicit (%) Total
Comparison 5590 (69.05%) 2505 (30.95%) 8095
Contingency 3741 (46.75%) 4261 (53.25%) 8002
Temporal 3696 (79.55%) 950 (20.45%) 4646
Expansion 6431 (42.04%) 8868 (57.96%) 15299
Table 1: Discourse relation distribution in seman-
tic and explicit/implicit classes in the PDTB
3 Distribution and ambiguity of
connectives
Table 1 shows the distribution of discourse rela-
tions between the four main relation classes and
their type of realization (implicit or explicit). In-
terestingly, temporal and comparison relations are
predominantly explicit. About 80% and 70%, re-
spectively, of their occurrences are marked by a
discourse connective. The contingency relations
are almost evenly distributed between explicit and
implicit. The expansion relations, the overall
largest class of discourse relations, is in most cases
implicit and not marked by a discourse connective.
Given the figures in Table 1, we would expect
that overall temporal and comparison relations will
be more easily identified since they are overtly
marked. Of course this would only be the case if
discourse markers are mostly unambiguous.
Here we show all connectives that appear more
than 50 times in the PDTB, their predominant
sense (comparison, contingency, temporal or ex-
pansion), as well as the percentage of occurrences
of the connective in its predominant sense. For
example the connective but has comparison as its
predominant sense and 97.19% of the 3,308 occur-
rences of this connective were comparisons.
Comparison but (3308; 97.19%), while (781; 66.07%),
however (485; 99.59%), although (328; 99.70%),
though (320; 100.00%), still (190; 98.42%), yet (101;
97.03%)
Expansion and (3000; 96.83%), also (1746; 99.94%), for
example (196; 100.00%), in addition (165; 100.00%),
instead (112; 97.32%), indeed (104; 95.19%), more-
over (101; 100.00%), for instance (98, 100.00%), or
(98; 96.94%), unless (95; 98.95%), in fact (82; 92.68%)
separately (74; 100.00%)
Contingency if (1223; 95.99%), because (858, 100.00%),
so (263; 100.00%), since (184; 52.17%), thus (112;
100.00%), as a result (78; 100.00%)
Temporal when (989; 80.18%), as (743; 70.26%), af-
ter (577; 99.65%), then (340; 93.24%), before (326;
100.00%), meanwhile (193; 48.70%), until (162;
87.04%), later (91; 98.90%), once (84; 95.24%)
The connectives that signal comparison and
contingency are mostly unambiguous. Obvious
exceptions are two of the connectives that are often
used to signal temporal relations: while and since.
88
The predominant senses of these connectives are
comparison (66.07%) and contingency (52.17%)
respectively. Disambiguating these problematic
connectives has already been addressed in previ-
ous work (Miltsakaki et al, 2005), but even the
predominantly temporal connectives are rather am-
biguous. For example less than 95% of the occur-
rances of meanwhile, as, when, until, and then are
temporal relaions.
While some connectives such as ?since? are am-
biguous, most are not. The discourse connec-
tives in the corpus appear in their predominant
sense 93.43% (for comparsion), 94.72% (for con-
tingency), 84.10% (for temporal), and 97.63% (for
expansion) of the time. Temporal connectives are
most ambiguous and connectives signaling expan-
sion are least ambiguous.
4 Automatic classification
The analyses in the previous section show two very
positive trends: many of the discourse relations are
explicitly marked by the use of a discourse connec-
tive, especially comparison and temporal relations,
and discourse connectives are overall mostly un-
ambiguous. These facts would suggest that even
based only on the connective, classification of dis-
course relations could be done well for all data (in-
cluding both implicit and explicit examples) and
particularly well for explicit examples alone. In-
deed, Table 2 shows the performance of a decision
tree classifier for discourse relations, on all data
and on the explicit subset in the second and third
column respectively.
We use the natural distribution of relation
classes found in theWall Street Journal texts, with-
out downsampling to get balanced distribution of
classes. There are four task settings, distinguishing
each type of relation from all others. For example,
comparison relations can be distinguished from all
other relations in the corpus with overall accuracy
of 91.28%, based only on the discourse connective
(first line in Table 2). The recall for recognizing
comparison relations is 0.66, directly reflecting the
fact that 31% of all comparison relations are im-
plicit (Table 1) and the connective feature did not
help at all in those cases. Over explicit data only,
the classification accuracy for comparison relation
versus any other relation is 97.23%, and precision
and recall is 0.95 and above.
As expected, the overall accuracy of identify-
ing contingency and expansion relations is lower,
Task All relations Explicit relations only
Comparison 91.28% (76.54%) 97.23% (69.72%)
Contingency 84.44% (76.81%) 93.99% (79.73%)
Temporal 94.79% (86.54%) 95.4% (79.98%)
Expansion 77.51% (55.67%) 97.61% (65.16%)
Table 2: Decision tree classification accuracy us-
ing only the presence of connectives as binary fea-
tures. The majority class is given in brackets.
Class Precision Recall
Temporal 0.841 [0.841] 0.729 [0.903]
Expansion 0.658 [0.973] 0.982 [0.957]
Contingency 0.948 [0.947] 0.369 [0.844]
Comparison 0.935 [0.935] 0.671 [0.971]
Table 3: Four-way classification. The first number
is for all data, thesecond for explicit relations only.
84.44% and 77.51% on all data respectively, re-
flecting the fact that these relations are often im-
plicit. But by themselves these accuracy numbers
are actually reasonable, setting a rather high base-
line for any more sophisticated method of classify-
ing discourse relations. On explicit data only, the
two-way classification accuracy for the four main
types of relations is 94% and higher.
In four-way classification, disambiguating be-
tween the four main semantic types of discourse
relations leads to 74.74% classification accuracy.
The accuracy for four-way classification of explicit
relations is 93.09%. The precision and recall for
each class is shown in Table 4. The worst per-
formance on the explicit portion of the data is the
precision for temporal relations and the recall for
contingency relations, both of which are 0.84.
5 N-gram discourse relation models
We have shown above that some relations, such as
comparison, can be easily identified because they
are often explicit and are expressed by an unam-
biguous connective. However, one must build a
more subtle automatic classifier to find the implicit
relations. We now look at the frequencies in which
various relations are adjacent in the PDTB. Results
from previous studies of discourse relations sug-
gest that the context of a relation can be helpful in
disambiguating the relation (Wellner et al, 2006).
Here we identify specific dependencies that exist
between sequences of relations.
We computed ?
2
statistics to test the indepen-
dence of each pair of relations. The question is:
do relations A and B occur adjacent to each other
more than they would simply due to chance? The
89
First Relation Second Relation ?
2
p-value
E. Comparison I. Contingency 20.1 .000007
E. Comparison E. Comparison 17.4 .000030
E. Comparison I. Expansion 9.91 .00161
I. Temporal E. Temporal 9.42 .00214
I. Contingency E. Contingency 9.29 .00230
I. Expansion E. Expansion 6.34 .0118
E. Expansion I. Expansion 5.50 .0191
I. Contingency E. Comparison 4.95 .0260
Table 4: ?
2
results for pairs of relations
pairs of implicit and explicit relations which have
significant associations with each other (pval <
0.05) are shown in Table 4. For example, ex-
plicit comparison and implicit contingency co-
occur much more often than would be expected if
they were independent. As explicit comparisons
are generally fairly easy to identify, knowing that
they tend to co-occur may be helpful when search-
ing for implicit contingency relations in a text.
6 Conclusion
We have tried to summarize the difficulty of find-
ing discourse relations using the Penn Discourse
Treebank. We noted that explicit and implicit rela-
tions are approximately evenly distributed overall,
making the task easier than many researchers have
feared. We have found that some relations, such as
temporal and comparison, are more likely to be ex-
plicit than implicit, making them relatively easier
to find, while contingency and expansion are more
often implicit. Among the discourse connectives,
the majority are not very ambiguous between the
different types of relations, with some notable ex-
ceptions such as since and meanwhile.
We have carried out a novel quantitative study
of the patterns of dependencies between discourse
relations. We found that while there does not ap-
pear to be a clear template for the sequence of
relations, there are individual relation pairs that
tend to co-occur. Specifically, we found that even
though contingency relations are likely to be im-
plicit and thus difficult to find, they are likely to
be found near an explicit comparison. We plan to
exploit these findings in future work, addressing
discourse relation labeling in text as a sequence la-
beling problem and using the explicit cue words
of surrounding relations as features for finding the
?hidden? implicit relations.
7 Acknowledgments
This work was partially supported by an Integra-
tive Graduate Education and Research Traineeship
grant from National Science Foundation (NSF-
IGERT 0504487) and by NSF Grant IIS -07-
05671. We would like to thank Nikhil Dinesh for
help with the PDTB, and Rashmi Prasad, Bonnie
Webber and Eleni Miltsakaki for insightful discus-
sions.
References
Hobbs, J. 1979. Coherence and coreference. Cognitive
Science, 3:67?90.
Knott, A. and T. Sanders. 1998. The classification of
coherence relations and their linguistic markers: An
exploration of two languages. Journal of Pragmat-
ics, 30(2):135?175.
Mann, W. and S. Thompson. 1988. Rhetorical struc-
ture theory: Toward a functional theory of text orga-
nization. Text, 8:243?281.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
McKeown, Kathleen R. 1985. Text generation: us-
ing discourse strategies and focus constraints to gen-
erate natural language text. Cambridge University
Press, New York, NY, USA.
Miltsakaki, E., N. Dinesh, R. Prasad, A. Joshi, and
B. Webber. 2005. Experiments on sense annotations
and sense disambiguation of discourse connectives.
In Proceedings of the Fourth Workshop on Treebanks
and Linguistic Theories (TLT2005).
Miltsakaki, Eleni, Livio Robaldo, Alan Lee, and Ar-
avind Joshi. 2008. Sense annotation in the penn dis-
course treebank. Computational Linguistics and In-
telligent Text Processing, Lecture Notes in Computer
Science, 4919:275?286.
Moore, J. and P. Wiemer-Hastings. 2003. Discourse in
computational linguistics and artificial intelligence.
Prasad, R., N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation (LREC).
Wellner, B., J. Pustejovsky, C. Havasi, R. Sauri, and
A. Rumshisky. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of the 7th
SIGDIAL Workshop on Discourse and Dialogue.
90
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 186?195,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Revisiting Readability: A Unified Framework for Predicting Text Quality
Emily Pitler
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler@seas.upenn.edu
Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
nenkova@seas.upenn.edu
Abstract
We combine lexical, syntactic, and discourse
features to produce a highly predictive model
of human readers? judgments of text readabil-
ity. This is the first study to take into ac-
count such a variety of linguistic factors and
the first to empirically demonstrate that dis-
course relations are strongly associated with
the perceived quality of text. We show that
various surface metrics generally expected to
be related to readability are not very good pre-
dictors of readability judgments in our Wall
Street Journal corpus. We also establish that
readability predictors behave differently de-
pending on the task: predicting text readabil-
ity or ranking the readability. Our experi-
ments indicate that discourse relations are the
one class of features that exhibits robustness
across these two tasks.
1 Introduction
The quest for a precise definition of text quality?
pinpointing the factors that make text flow and easy
to read?has a long history and tradition. Way back
in 1944 Robert Gunning Associates was set up, of-
fering newspapers, magazines and business firms
consultations on clear writing (Gunning, 1952).
In education, teaching good writing technique and
grading student writing has always been of key
importance (Spandel, 2004; Attali and Burstein,
2006). Linguists have also studied various aspects of
text flow, with cohesion-building devices in English
(Halliday and Hasan, 1976), rhetorical structure the-
ory (Mann and Thompson, 1988) and centering the-
ory (Grosz et al, 1995) among the most influential
contributions.
Still, we do not have unified computational mod-
els that capture the interplay between various as-
pects of readability. Most studies focus on a sin-
gle factor contributing to readability for a given in-
tended audience. The use of rare words or technical
terminology for example can make text difficult to
read for certain audience types (Collins-Thompson
and Callan, 2004; Schwarm and Ostendorf, 2005;
Elhadad and Sutaria, 2007). Syntactic complexity
is associated with delayed processing time in un-
derstanding (Gibson, 1998) and is another factor
that can decrease readability. Text organization (dis-
course structure), topic development (entity coher-
ence) and the form of referring expressions also de-
termine readability. But we know little about the rel-
ative importance of each factor and how they com-
bine in determining perceived text quality.
In our work we use texts from the Wall Street
Journal intended for an educated adult audience
to analyze readability factors including vocabulary,
syntax, cohesion, entity coherence and discourse.
We study the association between these features and
reader assigned readability ratings, showing that dis-
course and vocabulary are the factors most strongly
linked to text quality. In the easier task of text qual-
ity ranking, entity coherence and syntax features
also become significant and the combination of fea-
tures allows for ranking prediction accuracy of 88%.
Our study is novel in the use of gold-standard dis-
course features for predicting readability and the si-
multaneous analysis of various readability factors.
186
2 Related work
2.1 Readability with respect to intended
readers
The definition of what one might consider to be
a well-written and readable text heavily depends
on the intended audience (Schriver, 1989). Obvi-
ously, even a superbly written scientific paper will
not be perceived as very readable by a lay person
and a great novel might not be appreciated by a
third grader. As a result, the vast majority of prior
work on readability deals with labeling texts with
the appropriate school grade level. A key observa-
tion in even the oldest work in this area is that the
vocabulary used in a text largely determines its read-
ability. More common words are easier, so some
metrics measured text readability by the percent-
age of words that were not among the N most fre-
quent in the language. It was also observed that fre-
quently occurring words are often short, so word
length was used to approximate readability more
robustly than using a predefined word frequency
list. Standard indices were developed based on the
link between word frequency/length and readabil-
ity, such as Flesch-Kincaid (Kincaid, 1975), Auto-
mated Readability Index (Kincaid, 1975), Gunning
Fog (Gunning, 1952), SMOG (McLaughlin, 1969),
and Coleman-Liau (Coleman and Liau, 1975). They
use only a few simple factors that are designed to
be easy to calculate and are rough approximations
to the linguistic factors that determine readability.
For example, Flesch-Kincaid uses the average num-
ber of syllables per word to approximate vocabulary
difficulty and the average number of words per sen-
tence to approximate syntactic difficulty.
In recent work, the idea of linking word frequency
and text readability has been explored for making
medical information more accessible to the general
public. (Elhadad and Sutaria, 2007) classified words
in medical texts as familiar or unfamiliar to a gen-
eral audience based on their frequencies in corpora.
When a description of the unfamiliar terms was pro-
vided, the perceived readability of the texts almost
doubled.
A more general and principled approach to using
vocabulary information for readability decisions has
been the use of language models. For any given text,
it is easy to compute its likelihood under a given lan-
guage model, i.e. one for text meant for children,
or for text meant for adults, or for a given grade
level. (Si and Callan, 2001), (Collins-Thompson and
Callan, 2004), (Schwarm and Ostendorf, 2005), and
(Heilman et al, 2007) used language models to pre-
dict the suitability of texts for a given school grade
level. But even for this type of task other factors
besides vocabulary use are at play in determining
readability. Syntactic complexity is an obvious fac-
tor: indeed (Heilman et al, 2007) and (Schwarm and
Ostendorf, 2005) also used syntactic features, such
as parse tree height or the number of passive sen-
tences, to predict reading grade levels. For the task
of deciding whether a text is written for an adult or
child reader, (Barzilay and Lapata, 2008) found that
adding entity coherence to (Schwarm and Ostendorf,
2005)?s list of features improves classification accu-
racy by 10%.
2.2 Readability as coherence for competent
language users
In linguistics and natural language processing, the
text properties rather than those of the reader are em-
phasized. Text coherence is defined as the ease with
which a person (tacitly assumed to be a competent
language user) understands a text. Coherent text is
characterized by various types of cohesive links that
facilitate text comprehension (Halliday and Hasan,
1976).
In recent work, considerable attention has been
devoted to entity coherence in text quality, espe-
cially in relation to information ordering. In many
applications such as text generation and summariza-
tion, systems need to decide the order in which se-
lected sentences or generated clauses should be pre-
sented to the user. Most models attempting to cap-
ture local coherence between sentences were based
on or inspired by centering theory (Grosz et al,
1995), which postulated strong links between the
center of attention in comprehension of adjacent
sentences and syntactic position and form of refer-
ence. In a detailed study of information ordering
in three very different corpora, (Karamanis et al, to
appear) assessed the performance of various formu-
lations of centering. Their results were somewhat
unexpected, showing that while centering transition
preferences were useful, the most successful strat-
egy for information ordering was based on avoid-
187
ing rough shifts, that is, sequences of sentences that
share no entities in common. This supports previous
findings that such types of transitions are associated
with poorly written text and can be used to improve
the accuracy of automatic grading of essays based
on various non-discourse features (Miltsakaki and
Kukich, 2000). In a more powerful generalization
of centering, Barzilay and Lapata (2008) developed
a novel approach which doesn?t postulate a prefer-
ence for any type of transition but rather computes
a set of features that capture transitions of all kinds
in the text and their relative proportion. Their en-
tity coherence features prove to be very suitable for
various tasks, notably for information ordering and
reading difficulty level.
Form of reference is also important in well-
written text and appropriate choices lead to im-
proved readability. Use of pronouns for reference
to highly salient entities is perceived as more de-
sirable than the use of definite noun phrases (Gor-
don et al, 1993; Krahmer and Theune, 2002). The
syntactic forms of first mention?when an entity is
first introduced in a text?differ from those of subse-
quent mentions (Poesio and Vieira, 1998; Nenkova
and McKeown, 2003) and can be exploited for im-
proving and predicting text coherence (Siddharthan,
2003; Nenkova and McKeown, 2003; Elsner and
Charniak, 2008).
3 Data
The objective of our study is to analyze various
readability factors, including discourse relations, be-
cause few empirical studies exist that directly link
discourse structure with text quality. In the past,
subsections of the Penn Treebank (Marcus et al,
1994) have been annotated for discourse relations
(Carlson et al, 2001; Wolf and Gibson, 2005). For
our study we chose to work with the newly released
Penn Discourse Treebank which is the largest anno-
tated resource which focuses exclusively on implicit
local relations between adjacent sentences and ex-
plicit discourse connectives.
3.1 Discourse annotation
The Penn Discourse Treebank (Prasad et al, 2008)
is a new resource with annotations of discourse con-
nectives and their senses in the Wall Street Journal
portion of the Penn Treebank (Marcus et al, 1994).
All explicit relations (those marked with a discourse
connective) are annotated. In addition, each adjacent
pair of sentences within a paragraph is annotated. If
there is a discourse relation, then it is marked im-
plicit and annotated with one or more connectives. If
there is a relation between the sentences but adding a
connective would be inappropriate, it is marked Al-
tLex. If the consecutive sentences are only related
by entity-based coherence (Knott et al, 2001) they
are annotated with EntRel. Otherwise, they are an-
notated with NoRel.
Besides labeling the connective, the PDTB also
annotates the sense of each relation. The relations
are organized into a hierarchy. The top level rela-
tions are Expansion, Comparison, Contingency, and
Temporal. Briefly, an expansion relation means that
the second clause continues the theme of the first
clause, a comparison relation indicates that some-
thing in the two clauses is being compared, contin-
gency means that there is a causal relation between
the clauses, and temporal means they occur either at
the same time or sequentially.
3.2 Readability ratings
We randomly selected thirty articles from the Wall
Street Journal corpus that was used in both the Penn
Treebank and the Penn Discourse Treebank.1 Each
article was read by at least three college students,
each of whom was given unlimited time to read the
texts and perform the ratings.2 Subjects were asked
the following questions:
? How well-written is this article?
? How well does the text fit together?
? How easy was it to understand?
? How interesting is this article?
For each question, they provided a rating between 1
and 5, with 5 being the best and 1 being the worst.
1One of the selected articles was missing from the Penn
Treebank. Thus, results that do not require syntactic informa-
tion (Tables 1, 2, 4, and 6) are over all thirty articles, while
Tables 3, 5, and 7 report results for the twenty-nine articles with
Treebank parse trees.
2(Lapata, 2006) found that human ratings are significantly
correlated with self-paced reading times, a more direct measure
of processing effort which we plan to explore in future work.
188
After collecting the data, it turned out that most of
the time subjects gave the same rating to all ques-
tions. For competent language users, we view text
readability and text coherence as equivalent prop-
erties, measuring the extent to which a text is well
written. Thus for all subsequent analysis, we will
use only the first question (?On a scale of 1 to 5,
how well written is this text??). The score of an arti-
cle was then the average of all the ratings it received.
The article scores ranged from 1.5 to 4.33, with a
mean of 3.2008 and a standard deviation of .7242.
The median score was 3.286.
We define our task as predicting this average rat-
ing for each article. Note that this task may be
more difficult than predicting reading level, as each
of these articles appeared in the Wall Street Journal
and thus is aimed at the same target audience. We
suspected that in classifying adult text, more subtle
features might be necessary.
4 Identifying correlates of text quality
4.1 Baseline measures
We first computed the Pearson correlation coeffi-
cients between the simple metrics that most tradi-
tional readability formulas use and the average hu-
man ratings. These results are shown in Table 1. We
tested the average number of characters per word,
average number of words per sentence, maximum
number of words per sentence, and article length
(F7).3 Article length (F7) was the only significant
baseline factor, with correlation of -0.37. Longer ar-
ticles are perceived as less well-written and harder
to read than shorter ones. None of the other baseline
metrics were close to being significant predictors of
readability.
Average Characters/Word r = -.0859, p = .6519
Average Words/Sentence r = .1637, p = .3874
Max Words/Sentence r = .0866, p = .6489
F7 text length r = -.3713, p = .0434
Table 1: Baseline readability features
3For ease of reference, we number each non-baseline feature
in the text and tables.
4.2 Vocabulary
We use a unigram language model, where the prob-
ability of an article is:
?
w
P (w|M)C(w) (1)
P (w|M) is the probability of word-type w accord-
ing to a background corpus M , and C(w) is the
number of times w appears in the article.
The log likelihood of an article is then:
?
w
C(w) log(P (w|M)) (2)
Note that this model will be biased in favor of
shorter articles. Since each word has probability less
than 1, the log probability of each word is less than
0, and hence including additional words decreases
the log likelihood. We compensate for this by per-
forming linear regressions with the unigram log like-
lihood and with the number of words in the article as
an additional variable.
The question then arises as to what to use as a
background corpus. We chose to experiment with
two corpora: the entire Wall Street Journal corpus
and a collection of general AP news, which is gen-
erally more diverse than the financial news found in
the WSJ. We predicted that the NEWS vocabulary
would be more representative of the types of words
our readers would be familiar with. In both cases we
used Laplace smoothing over the word frequencies
and a stoplist.
The vocabulary features we used are article like-
lihood estimated from a language model from WSJ
(F5), and article likelihood according to a unigram
language model from NEWS (F6). We also combine
the two likelihood features with article length, in or-
der to get a better estimate of the language model?s
influence on readability independent of the length of
the article.
F5 Log likelihood, WSJ r = .3723, p = .0428
F6 Log likelihood, NEWS r= .4497, p = .0127
LL with length, WSJ r = .3732, p = .0422
LL with length, NEWS r = .6359, p = .0002
Table 2: Vocabulary features
Both vocabulary-based features (F5 and F6) are
significantly correlated with the readability judg-
ments, with p-values smaller than 0.05 (see Table 2).
189
The correlations are positive: the more probable an
article was based on its vocabulary, the higher it was
generally rated. As expected, the NEWS model that
included more general news stories had a higher cor-
relation with people?s judgments. When combined
with the length of the article, the unigram language
model from the NEWS corpus becomes very predic-
tive of readability, with the correlation between the
two as high as 0.63.
4.3 Syntactic features
Syntactic constructions affect processing difficulty
and so might also affect readability judgments.
We examined the four syntactic features used in
(Schwarm and Ostendorf, 2005): average parse tree
height (F1), average number of noun phrases per
sentence (F2), average number of verb phrases per
sentence (F3), and average number of subordinate
clauses per sentence(SBARs in the Penn Treebank
tagset) (F4). The sentence ?We?re talking about
years ago [SBAR before anyone heard of asbestos
having any questionable properties].? contains an
example of an SBAR clause.
Having multiple noun phrases (entities) in each
sentence requires the reader to remember more
items, but may make the article more interesting.
(Barzilay and Lapata, 2008) found that articles writ-
ten for adults tended to contain many more entities
than articles written for children. While including
more verb phrases in each sentence increases the
sentence complexity, adults might prefer to have re-
lated clauses explicitly grouped together.
F1 Average Parse Tree Height r = -.0634, p = .7439
F2 Average Noun Phrases r = .2189, p = .2539
F3 Average Verb Phrases r = .4213, p = .0228
F4 Average SBARs r = .3405, p = .0707
Table 3: Syntax-related features
The correlations between readability and syntac-
tic features is shown in Table 3. The strongest corre-
lation is that between readability and number of verb
phrases (0.42). This finding is in line with prescrip-
tive clear writing advice (Gunning, 1952; Spandel,
2004), but is to our knowledge novel in the compu-
tational linguistics literature. As (Bailin and Graf-
stein, 2001) point out, the sentences in (1) are eas-
ier to comprehend than the sentences in (2), even
though they are longer.
(1) It was late at night, but it was clear. The stars
were out and the moon was bright.
(2) It was late at night. It was clear. The stars were
out. The moon was bright.
Multiple verb phrases in one sentence may be in-
dicative of explicit discourse relations, which we
will discuss further in section 4.6.
Surprisingly, the use of clauses introduced
by a (possibly empty) subordinating conjunction
(SBAR), are actually positively correlated (and al-
most approaching significance) with readability. So
while for children or less educated adults these con-
structions might pose difficulties, they were favored
by our assessors. On the other hand, the average
parse tree height negatively correlated with readabil-
ity as expected, but surprisingly the correlation is
very weak (-0.06).
4.4 Elements of lexical cohesion
In their classic study of cohesion in English, (Hal-
liday and Hasan, 1976) discuss the various aspects
of well written discourse, including the use of cohe-
sive devices such as pronouns, definite descriptions
and topic continuity from sentence to sentence.4 To
measure the association between these features and
readability rankings, we compute the number of pro-
nouns per sentence (F11) and the number of defi-
nite articles per sentence (F12). In order to qual-
ify topic continuity from sentence to sentence in
the articles, we compute average cosine similarity
(F8), word overlap (F9) and word overlap over just
nouns and pronouns (F10) between pairs of adjacent
sentences5. Each sentence is turned into a vector
of word-types, where each type?s value is its tf-idf
(where document frequency is computed over all the
articles in the WSJ corpus). The cosine similarity
metric is then:
cos (s, t) =
s ? t
|s| |t|
(3)
4Other cohesion building devises discussed by Halliday
and Hansan include lexical reiteration and discourse relations,
which we address next.
5Similar features have been used for automatic essay grad-
ing as well (Higgins et al, 2004).
190
F8 Avr. Cosine Overlap r = -.1012, p = .5947
F9 Avr. Word Overlap r = -.0531, p = .7806
F10 Avr. Noun+Pronoun Overlap r = .0905, p = .6345
F11 Avr. # Pronouns/Sent r = .2381, p = .2051
F12 Avr # Definite Articles r = .2309, p = .2196
Table 4: Superficial measures of topic continuity and pro-
noun and definite description use
None of these features correlate significantly with
readability as can be seen from the results in Ta-
ble 4. The overlap features are particularly bad
predictors of readability, with average word/cosine
overlap in fact being negatively correlated with read-
ability. The form of reference?use of pronouns
and definite descriptions?exhibit a higher correla-
tion with readability (0.23), but these values are not
significant for the size of our corpus.
4.5 Entity coherence
We use the Brown Coherence Toolkit6 to compute
entity grids (Barzilay and Lapata, 2008) for each ar-
ticle. In each sentence, an entity is identified as the
subject (S), object (O), other (X) (for example, part
of a prepositional phrase), or not present (N). The
probability of each transition type is computed. For
example, an S-O transition occurs when an entity
is the subject in one sentence then an object in the
next; X-N transition occurs when an entity appears
in non-subject or object position in one sentence and
not present in the next, etc.7 The entity coherence
features are the probability of each of these pairs of
transitions, for a total of 16 features (F17?32; see
complete results in Table 5).
None of the entity grid features are significantly
correlated with the readability ratings. One very in-
teresting result is that the proportion of S-S transi-
tions in which the same entity was mentioned in sub-
ject position in two adjacent sentences, is negatively
correlated with readability. In centering theory, this
is considered the most coherent type of transition,
keeping the same center of attention. Moreover, the
feature most strongly correlated with readability is
the S-N transition (0.31) in which the subject of one
sentence does not appear at all in the following sen-
6http://www.cs.brown.edu/ melsner/manual.html
7The Brown Coherence Toolkit identifies NPs as the same
entity if they have identical head nouns.
F17 Prob. of S-S transition r = -.1287, p = .5059
F18 Prob. of S-O transition r = -.0427, p = .8261
F19 Prob. of S-X transition r = -.1450, p = .4529
F20 Prob. of S-N transition r = .3116, p = .0999
F21 Prob. of O-S transition r = .1131, p = .5591
F22 Prob. of O-O transition r = .0825, p = .6706
F23 Prob. of O-X transition r = .0744, p = .7014
F24 Prob. of O-N transition r = .2590, p = .1749
F25 Prob. of X-S transition r = .1732, p = .3688
F26 Prob. of X-O transition r = .0098, p = .9598
F27 Prob. of X-X transition r = -.0655, p = .7357
F28 Prob. of X-N transition r = .1319, p = .4953
F29 Prob. of N-S transition r = .1898, p = .3242
F30 Prob. of N-O transition r = .2577, p = .1772
F31 Prob. of N-X transition r = .1854, p = .3355
F32 Prob. of N-N transition r = -.2349, p = .2200
Table 5: Linear correlation between human readability
ratings and entity coherence.
tence. Of course, it is difficult to interpret the en-
tity grid features one by one, since they are inter-
dependent and probably it is the interaction of fea-
tures (relative proportions of transitions) that capture
overall readability patterns.
4.6 Discourse relations
Discourse relations are believed to be a major factor
in text coherence. We computed another language
model which is over discourse relations instead of
words. We treat each text as a bag of relations rather
than a bag of words. Each relation is annotated
for both its sense and how it is realized (implicit
or explicit). For example, one text might contain
{Implicit Comparison, Explicit Temporal, NoRel}.
We computed the probability of each of our articles
according to a multinomial model, where the proba-
bility of a text with n relation tokens and k relation
types is:
P (n)
n!
x1!...xk!
px11 ...p
xk
k (4)
P (n) is the probability of an article having length
n, xi is the number of times relation i appeared, and
pi is the probability of relation i based on the Penn
Discourse Treebank. P (n) is the maximum likeli-
hood estimation of an article having n discourse re-
lations based on the entire Penn Discourse Treebank
(the number of articles with exactly n discourse re-
lations, divided by the total number of articles).
191
The log likelihood of an article based on its dis-
course relations (F13) feature is defined as:
log(P (n)) + log(n!) +
k?
i=1
(xi log(pi)? log(xi!))
(5)
The multinomial distribution is particularly suit-
able, because it directly incorporates length, which
significantly affects readability as we discussed ear-
lier. It also captures patterns of relative frequency of
relations, unlike the simpler unigram model. Note
also that this equation has an advantage over the un-
igram model that was not present for vocabulary.
While every article contains at least one word, some
articles do not contain any discourse relations. Since
the PDTB annotated all explicit relations and re-
lations between adjacent sentences in a paragraph,
an article with no discourse connectives and only
single sentence paragraphs would not contain any
annotated discourse relations. Under the unigram
model, these articles? probabilities cannot be com-
puted. Under the multinomial model, the probabil-
ity of an article with zero relations is estimated as
Pr(N = 0), which can be calculated from the cor-
pus.
As in the case of vocabulary features, the presence
of more relations will lead to overall lower probabil-
ities so we also consider the number of discourse
relations (F14) and the log likelihood combined with
the number of relations as features. In order to iso-
late the effect of the type of discourse relation (ex-
plicitly expressed by a discourse connective such as
?because? or ?however? versus implicitly expressed
by adjacency), we also compute multinomial model
features for the explicit discourse relations (F15) and
over just the implicit discourse relations (F16).
F13 LogL of discourse rels r = .4835, p = .0068
F14 # of discourse relations r = -.2729, p = .1445
LogL of rels with # of rels r = .5409, p = .0020
# of relations with # of words r = .3819, p = .0373
F15 Explicit relations only r = .1528, p = .4203
F16 Implicit relations only r = .2403, p = .2009
Table 6: Discourse features
The likelihood of discourse relations in the text
under a multinomial model is very highly and sig-
nificantly correlated with readability ratings, espe-
cially after text length is taken into account. Cor-
relations are 0.48 and 0.54 respectively. The prob-
ability of the explicit relations alone is not a suffi-
ciently strong indicator of readability. This fact is
disappointing as the explicit relations can be iden-
tified much more easily in unannotated text (Pitler
et al, 2008). Note that the sequence of just the im-
plicit relations is also not sufficient. This observa-
tion implies that the proportion of explicit and im-
plicit relations may be meaningful but we leave the
exploration of this issue for later work.
4.7 Summary of findings
So far, we introduced six classes of factors that have
been discussed in the literature as readability cor-
relates. Through statistical tests of associations we
identified the individual factors significantly corre-
lated with readability ratings. These are, in decreas-
ing order of association strength:
LogL of Discourse Relations (r = .4835)
LogL, NEWS (r= .4497)
Average Verb Phrases (.4213)
LogL, WSJ (r = .3723)
Number of words (r = -.3713)
Vocabulary and discourse relations are the
strongest predictors of readability, followed by aver-
age number of verb phrases and length of the text.
This empirical confirmation of the significance of
discourse relations as a readability factor is novel for
the computational linguistics literature. Note though
that for our work we use oracle discourse annota-
tions directly from the PDTB and no robust systems
for automatic discourse annotation exist today.
The significance of the average number of verb
phrases as a readability predictor is somewhat sur-
prising but intriguing. It would lead to reexamina-
tion of the role of verbs/predicates in written text,
which we also plan to address in future work. None
of the other factors showed significant association
with readability ratings, even though some correla-
tions had relatively large positive values.
5 Combining readability factors
In this section, we turn to the question of how the
combination of various factors improves the predic-
tion of readability. We use the leaps package in R
to find the best subset of features for linear regres-
sion, for subsets of size one to eight. We use the
192
squared multiple correlation coefficient (R2) to as-
sess the effectiveness of predictions. R2 is the pro-
portion of variance in readability ratings explained
by the model. If the model predicts readability per-
fectly, R2 = 1, and if the model has no predictive
capability, R2 = 0.
F13, R2 = 0.2662
F6 + F7, R2 = 0.4351
F6 + F7 + F13, R2 = 0.5029
F6 + F7 + F13 + F14, R2 = 0.6308
F1 + F6 + F7 + F10 + F13, R2 = 0.6939
F1 + F6 + F7 + F10 + F13 + F23, R2 = 0.7316
F1 + F6 + F7 + F10 + F13 + F22 + F23, R2 = 0.7557
F1+F6+F7+F10+F11+F13+F19+F30, R2 = 0.776.
The linear regression results confirm the expec-
tation that the combination of different factors is a
rather complex issue. As expected, discourse, vo-
cabulary and length which were the significant in-
dividual factors appear in the best model for each
feature set size. Their combination gives the best
result for regression with three predictors, and they
explain half of the variance in readability ratings,
R2 = 0.5029.
But the other individually significant feature, av-
erage number of verb phrases per sentence (F3)
never appears in the best models. Instead, F1?the
depth of the parse tree?appears in the best model
with more than four features.
Also unexpectedly, two of the superficial cohe-
sion features appear in the larger models: F10 is
the average word overlap over nouns and pronouns
and F11 is the average number of pronouns per sen-
tence. Entity grid features also make their way into
the best models when more features are used for pre-
diction: S-X, O-O, O-X, N-O transitions (F19, F22,
F23, F30).
6 Readability as ranking
In this section we consider the problem of pairwise
ranking of text readability. That is, rather than try-
ing to predict the readability of a single document,
we consider pairs of documents and predict which
one is better. This task may in fact be the more natu-
ral one, since in most applications the main concern
is with the relative quality of articles rather than their
absolute scores. This setting is also beneficial in
terms of data use, because each pair of articles with
different average readability scores now becomes a
data point for the classification task.
We thus create a classification problem: given two
articles, is article 1 more readable than article 2?
For each pair of texts whose readability ratings on
the 1 to 5 scale differed by at least 0.5, we form
one data point for the ranking problem, resulting in
243 examples. The predictors are the differences be-
tween the two articles? features. For classification,
we used WEKA?s linear support vector implemen-
tation (SMO) and performance was evaluated using
10-fold cross-validation.
Features Accuracy
None (Majority Class) 50.21%
ALL 88.88%
log l discourse rels 77.77%
number discourse rels 74.07%
N-O transition 70.78%
O-N transition 69.95%
Avg VPs sen 69.54%
log l NEWS 66.25%
number of words 65.84%
Grid only 79.42%
Discourse only 77.36%
Syntax only 74.07%
Vocab only 66.66%
Length only 65.84%
Cohesion only 64.60%
no cohesion 89.30%
no vocab 88.88%
no length 88.47%
no discourse 88.06%
no grid 84.36%
no syntax 82.71%
Table 7: SVM prediction accuracy, linear kernel
The classification results are shown in Table 7.
When all features are used for prediction, the ac-
curacy is high, 88.88%. The length of the article
can serve as a baseline feature?longer articles are
ranked lower by the assessors, so this feature can
be taken as baseline indicator of readability. Only
six features used by themselves lead to accuracies
higher than the length baseline. These results indi-
cate that the most important individual factors in the
readability ranking task, in decreasing order of im-
portance, are log likelihood of discourse relations,
number of discourse relations, N-O transitions, O-N
193
transitions, average number of VPs per sentence and
text probability under a general language model.
In terms of classes of features, the 16 entity
grid features perform the best, leading to an accu-
racy of 79.41%, followed by the combination of
the four discourse features (77.36%), and syntax
features (74.07%). This is evidence for the fact
that there is a complex interplay between readabil-
ity factors: the entity grid factors which individ-
ually have very weak correlation with readability
combine well, while adding the three additional dis-
course features to the likelihood of discourses rela-
tions actually worsens performance slightly. Simi-
lar indication for interplay between features is pro-
vided by the class ablation classification results, in
which classes of features are removed. Surprisingly,
removing syntactic features causes the biggest dete-
rioration in performance, a drop in accuracy from
88.88% to 82.71%. The removal of vocabulary,
length, or discourse features has a minimal negative
impact on performance, while removing the cohe-
sion features actually boosts performance.
7 Conclusion
We have investigated which linguistic features cor-
relate best with readability judgments. While sur-
face measures such as the average number of words
per sentence or the average number of characters
per word are not good predictors, there exist syn-
tactic, semantic, and discourse features that do cor-
relate highly. The average number of verb phrases
in each sentence, the number of words in the article,
the likelihood of the vocabulary, and the likelihood
of the discourse relations all are highly correlated
with humans? judgments of how well an article is
written.
While using any one out of syntactic, lexical, co-
herence, or discourse features is substantally better
than the baseline surface features on the discrim-
ination task, using a combination of entity coher-
ence and discourse relations produces the best per-
formance.
8 Acknowledgments
This work was partially supported by an Inte-
grative Graduate Education and Research Trainee-
ship grant from National Science Foundation (NS-
FIGERT 0504487) and by NSF Grant IIS-07-05671.
We thank Aravind Joshi, Bonnie Webber, and the
anonymous reviewers for their many helpful com-
ments.
References
Y. Attali and J. Burstein. 2006. Automated essay scoring
with e-rater v.2. The Journal of Technology, Learning
and Assessment, 4(3).
A. Bailin and A. Grafstein. 2001. The linguistic assump-
tions underlying readability formulae a critique. Lan-
guage and Communication, 21(3):285?301.
R. Barzilay and M. Lapata. 2008. Modeling local coher-
ence: An entity-based approach. Computational Lin-
guistics, 34(1):1?34.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proceedings of the
Second SIGdial Workshop, pages 1?10.
M. Coleman and TL Liau. 1975. A computer readabil-
ity formula designed for machine scoring. Journal of
Applied Psychology, 60(2):283?284.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of HLT/NAACL?04.
Noemie Elhadad and Komal Sutaria. 2007. Mining a lex-
icon of technical terms and lay equivalents. In Biolog-
ical, translational, and clinical language processing,
pages 49?56, Prague, Czech Republic. Association for
Computational Linguistics.
M. Elsner and E. Charniak. 2008. Coreference-inspired
coherence modeling. In Proceedings of ACL-HLT?08,
(short paper).
E. Gibson. 1998. Linguistic complexity: locality of syn-
tactic dependencies. Cognition, 68:1?76.
P. Gordon, B. Grosz, and L. Gilliom. 1993. Pronouns,
names, and the centering of attention in discourse.
Cognitive Science, 17:311?347.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering:
a framework for modelling the local coherence of dis-
course. Computational Linguistics, 21(2):203?226.
Robert Gunning. 1952. The technique of clear writing.
McGraw-Hill; Fouth Printing edition.
Michael A.K. Halliday and Ruqaiya Hasan. 1976. Cohe-
sion in English. Longman Group Ltd, London, U.K.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining Lexical and Grammatical
Features to Improve Readability Measures for First
and Second Language Texts. Proceedings of NAACL
HLT, pages 460?467.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004.
Evaluating multiple aspects of coherence in student es-
says. In Proceedings of HLT/NAACL?04.
194
N. Karamanis, M. Poesio, C. Mellish, and J. Oberlander.
(to appear). Evaluating centering for information or-
dering using corpora. Computational Linguistics.
JP Kincaid. 1975. Derivation of New Readability For-
mulas (Automated Readability Index, Fog Count and
Flesch Reading Ease Formula) for Navy Enlisted Per-
sonnel.
A. Knott, J. Oberlander, M. ODonnell, and C. Mellish.
2001. Beyond elaboration: The interaction of relations
and focus in coherent text. Text representation: lin-
guistic and psycholinguistic aspects, pages 181?196.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In K. van
Deemter and R. Kibble, editors, Information Sharing:
Reference and Presupposition in Language Genera-
tion and Interpretation, pages 223?264. CSLI Publi-
cations.
M. Lapata. 2006. Automatic evaluation of information
ordering: Kendalls tau. Computational Linguistics,
32(4):471?484.
W. Mann and S. Thompson. 1988. Rhetorical structure
theory: Towards a functional theory of text organiza-
tion. Text, 8.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
G.H. McLaughlin. 1969. SMOG grading: A new read-
ability formula. Journal of Reading, 12(8):639?646.
E. Miltsakaki and K. Kukich. 2000. The role of centering
theory?s rough-shift in the teaching and evaluation of
writing skills. In Proceedings of ACL?00, pages 408?
415.
A. Nenkova and K. McKeown. 2003. References to
named entities: a corpus study. In Proceedings of
HLT/NAACL 2003 (short paper).
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee,
and A. Joshi. 2008. Easily identifiable discourse re-
lations. In Coling 2008: Companion volume: Posters
and Demonstrations, pages 85?88, Manchester, UK,
August.
M. Poesio and R. Vieira. 1998. A corpus-based investi-
gation of definite description use. Computational Lin-
guistics, 24(2):183?216.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008. The penn discourse
treebank 2.0. In Proceedings of LREC?08.
KA Schriver. 1989. Evaluating text quality: the con-
tinuum from text-focused toreader-focused methods.
Professional Communication, IEEE Transactions on,
32(4):238?255.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of ACL?05, pages
523?530.
L. Si and J. Callan. 2001. A statistical model for sci-
entific readability. Proceedings of the tenth interna-
tional conference on Information and knowledge man-
agement, pages 574?576.
A. Siddharthan. 2003. Syntactic simplification and Text
Cohesion. Ph.D. thesis, University of Cambridge, UK.
V. Spandel. 2004. Creating writers through 6-trait writ-
ing assessment and instruction. Allyn & Bacon.
F. Wolf and E. Gibson. 2005. Representing discourse
coherence: A corpus-based study. Computational Lin-
guistics, 31(2):249?288.
195
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1428?1436,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Using Word-Sense Disambiguation Methods to Classify Web Queries by
Intent
Emily Pitler
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler@seas.upenn.edu
Ken Church
Johns Hopkins University
Human Language Technology Center of Excellence
Baltimore, MD 21211
Kenneth.Church@jhu.edu
Abstract
Three methods are proposed to classify
queries by intent (CQI), e.g., navigational,
informational, commercial, etc. Follow-
ing mixed-initiative dialog systems, search
engines should distinguish navigational
queries where the user is taking the ini-
tiative from other queries where there are
more opportunities for system initiatives
(e.g., suggestions, ads). The query in-
tent problem has a number of useful appli-
cations for search engines, affecting how
many (if any) advertisements to display,
which results to return, and how to ar-
range the results page. Click logs are
used as a substitute for annotation. Clicks
on ads are evidence for commercial in-
tent; other types of clicks are evidence for
other intents. We start with a simple Na??ve
Bayes baseline that works well when there
is plenty of training data. When train-
ing data is less plentiful, we back off
to nearby URLs in a click graph, using
a method similar to Word-Sense Disam-
biguation. Thus, we can infer that de-
signer trench is commercial because it is
close to www.saksfifthavenue.com, which
is known to be commercial. The baseline
method was designed for precision and
the backoff method was designed for re-
call. Both methods are fast and do not re-
quire crawling webpages. We recommend
a third method, a hybrid of the two, that
does no harm when there is plenty of train-
ing data, and generalizes better when there
isn?t, as a strong baseline for the CQI task.
1 Classify Queries By Intent (CQI)
Determining query intent is an important prob-
lem for today?s search engines. Queries are short
(consisting of 2.2 terms on average (Beitzel et al,
2004)) and contain ambiguous terms. Search en-
gines need to derive what users want from this lim-
ited source of information. Users may be search-
ing for a specific page, browsing for information,
or trying to buy something. Guessing the correct
intent is important for returning relevant items.
Someone searching for designer trench is likely
to be interested in results or ads for trench coats,
while someone searching for world war I trench
might be irritated by irrelevant clothing advertise-
ments.
Broder (2002) and Rose and Levinson (2004)
categorized queries into those with navigational,
informational, and transactional or resource-
seeking intent. Navigational queries are queries
for which a user has a particular web page in mind
that they are trying to navigate to, such as grey-
hound bus. Informational queries are those like
San Francisco, in which the user is trying to gather
information about a topic. Transactional queries
are those like digital camera or download adobe
reader, where the user is seeking to make a trans-
action or access an online resource.
Knowing the intent of a query greatly affects the
type of results that are relevant. For many queries,
Wikipedia articles are returned on the first page
of results. For informational queries, this is usu-
ally appropriate, as a Wikipedia article contains
summaries of topics and links to explore further.
However, for navigational or transactional queries,
Wikipedia is not as appropriate. A user looking
for the greyhound bus homepage is probably not
interested in facts about the company. Similarly,
someone looking to download adobe reader will
not be interested in Wikipedia?s description of the
product?s history. Conversely, for informational
queries, Wikipedia articles tend to be appropriate
while advertisements are not. The user searching
for world war I trench might find the Wikipedia
article on trench warfare useful, while he is prob-
1428
(a) The advertisements and related searches are probably more likely to be clicked on than
the top result for designer trench.
(b) The top result will receive more clicks than the spelling suggestion. Wikipedia often
receives lots of clicks, but not for commercial queries like bestbuy.
Figure 1: Results pages from two major search engines. A search results page has limited real estate that
must be divided between search results, spelling suggestions, query suggestions, and ads.
ably not interested in purchasing clothing, or even
World War I related products. We noticed empiri-
cally that queries in the logs tend to have a high
proportion of clicks on the Wikipedia article or
the ads, but almost never both. The Wikipedia
page for Best Buy in Figure 1(b) is probably a
waste of space. Knowing whether a particular
query is navigational, informational, or transac-
tional would improve search and advertising rel-
evance.
After a query is issued, search engines return
a list of results, and possibly also advertisements,
suggestions of related searches, and spelling sug-
gestions. For different queries, these alternatives
have varying utilities to the users. Consider the
queries in Figures 1(a) and 1(b). For designer
trench, the advertisements may well be more use-
ful to the user than the standard set of results. The
query suggestions for designer trench all would
help refine the query, whereas the suggestions for
bestbuy are less useful, as they would either re-
turn the same set of results or take the user to Best
Buy?s competitors? sites. The spelling suggestion
for best buy instead of bestbuy is also unnecessary.
Devoting more page space to the content that is
likely to be clicked on could help improve the user
experience.
In this paper we consider the task of: given a
class of queries, which types of answer (standard
search, ads, query suggestions, or spelling sug-
1429
gestions) are likely to be clicked on? Typos will
tend to have more clicks on the spelling sugges-
tions, informational queries will have more clicks
on Wikipedia pages, and commercial queries will
have more clicks on the ads. The observed behav-
ior of where users click tells us something about
the hidden intentions of the users when they issue
that query.
We focus on commercial intent (Dai et al,
2006), the intent to purchase a product or service,
to illustrate our method of predicting query intent.
The business model of web search today is heav-
ily dependent on advertising. Advertisers bid on
queries, and then the search results page also con-
tains ?sponsored? sites by the advertisers who won
the auction for that query. It is thus advantageous
for the advertisers to bid on queries which are most
likely to result in a commercial transaction. If
a query is classified as likely implying commer-
cial intent, but the advertisers have overlooked this
query, then the search engine may want to sug-
gest that advertisers bid on that query. The search
engine may also want to treat queries classified
as having commercial intent differently, by rear-
ranging the appearance of the page, or by showing
more or fewer advertisements.
This paper starts with a simple Na??ve Bayes
baseline to classify queries by intent (CQI). Super-
vised methods work well, especially when there is
plenty of annotated data for testing and training.
Unfortunately, since we don?t have as much anno-
tated data as we might like, we propose two work-
arounds:
1. Use click logs as a substitute for annotated
data. Clicks on ads are evidence for commer-
cial intent; other types of clicks are evidence
for other intents.
2. We propose a method similar to Yarowsky
(1995) to generalize beyond the training set.
2 Related Work
Click logs have been used for a variety of tasks
involved in information retrieval, including pre-
dicting which pages are the best results for queries
(Piwowarski and Zaragoza, 2007; Joachims, 2002;
Xue et al, 2004), choosing relevant advertise-
ments (Chakrabarti et al, 2008), suggesting re-
lated queries (Beeferman and Berger, 2000), and
personalizing results (Tan et al, 2006). Queries
that have a navigational intent tended to have
a highly skewed click distribution, while users
clicked on a wider range of results after issuing
informational queries. Lee et al (2005) used the
click distributions to classify navigational versus
informational intents.
While navigational, informational, and
resource-seeking are very broad intentions, other
researchers have looked at personalization and
intent on a per user basis. Downey et al (2008)
use the last URL visited in a session or the last
search engine result visited as a proxy for the
user?s information goal, and then looked at the
correspondence between information needs and
queries (how the goals are expressed).
We are interested in a granularity of intent
in between navigational/informational/resource-
seeking and personalized intents. For these sorts
of intents, the web pages associated with queries
provide useful information. To classify queries
into an ontology of commercial queries, Broder
et al (2007) found that a classifier that used the
text of the top result pages performed much bet-
ter than a classifier that used only the query string.
While the results are quite good on their hierarchy
of 6000 types of commercial intents, they manu-
ally constructed about 150 hand-picked examples
each for each of the 6000 intents. Beitzel et al
(2005) do semi-supervised learning over the query
logs to classify queries into topics, but also train
with hundreds of thousands of manually annotated
queries. Thus, while we also use the query logs
and the identities of web pages of associated with
each query, we are interested in finding methods
that can be applied when that much annotation is
prohibitive.
Semi-supervised methods over the click graph
make it possible to train classifiers after starting
from a much smaller set of seed queries. Li et al
(2008) used the semi-supervised learning method
described in Zhou et al (2004) to gain a much
larger training set of examples, and then trained
classifiers for product search or job search on the
expanded set. Random walk methods over the
click graph have also been used to propagate re-
lations between URLs, for tasks such as finding
?adult? content (Craswell and Szummer, 2007)
and suggesting related queries (Antonellis et al,
2008) and content (Baluja et al, 2008). In our
work we also seek to classify query intent us-
ing the click graph, but we demonstrate the ef-
fectiveness of a simple method by building deci-
1430
sion lists of URLs. In addition, we evaluate our
method automatically by using user click rates,
rather than assembling hand-labeled examples for
training and testing.
Dai et al (2006) also classified queries by com-
mercial intent, but their method involved crawling
the top landing pages for each query, which can
be quite time-consuming. In this paper we investi-
gate the commercial intent problem when crawling
pages is not feasible, and use only the identities of
the top URLs.
3 Using Click Logs as a Substitute for
Annotation
Prior work has used click logs in lieu of manual
annotations of relevance ratings, either of web-
pages (Joachims, 2002) or of sponsored search ad-
vertisements (Ciaramita et al, 2008). Here we use
the click logs as a large-scale source of intents.
Logs from Microsoft?s Live Search are used for
training and test purposes. Logs from May 2008
were used for training, and logs from June 2008
were used for testing.
The logs distinguish four types of clicks: (a)
search results, (b) ads, (c) spelling suggestions and
(d) query suggestions. Some prototypical queries
of each type are shown in Table 1. As mentioned
above, clicks on ads are evidence for commercial
intent; other types of clicks are evidence for other
intents. The query, ebay official, is assumed to be
commercial intent, because a large fraction of the
clicks are on ads. In contrast, typos tend to have
relatively more clicks on ?did-you-mean? spelling
suggestions.
The query logs contain over a terabyte of
data for each day, and our experiments were
done using months of logs at a time. We
used SCOPE (Chaiken et al, 2008), a script-
ing programming language designed for doing
Map-Reduce (Dean and Ghemawat, 2004) style
computations, to distribute the task of aggre-
gating the counts of each query over thousands
of servers. As the same query is often issued
several times by multiple users across an en-
tire month of search logs, we summarize each
query with four ratios?search results clicks:overall
clicks, ad clicks:overall clicks, spelling sugges-
tion clicks:overall clicks, and query suggestion
clicks:overall clicks.
A couple of steps were taken to ensure reliable
ratios. We are classifying types, not tokens, and
so limited ourselves to those queries with 100 or
more clicks. This still leaves us with over half a
million distinct queries for training and for test-
ing, yet alows us to use click ratios as a substitute
for annotating these huge data sets. If a query was
only issued once and the user clicked on an ad,
that may be more a reflection of the user, rather
than reflecting that the query is 100% commer-
cial. In addition, the ratios compare clicks of one
type with clicks of another, rather than compar-
ing clicks with impressions. There is less risk of a
failure to find fallacy if we count events (clicks) in-
stead of non-events (non-clicks). There are many
reasons for non-clicks, only some of which tell us
about the meaning of the query. There are bots that
crawl pages and never click. Many links can?t be
seen (e.g., if they are below the fold).
Queries are labeled as positive examples of
commercial intent if their ratio is in the top half of
the training set, and negative otherwise. A similar
procedure is used to label queries with the three
other intents.
Our task is to predict future click patterns based
on past click patterns. Note that a query may ap-
pear in both the test set and the training set, al-
though not necessarily with the same label. In fact,
because of the robustness requirement of 100+
clicks, many queries appear in both sets; 506,369
out of 591,122 of the test queries were also present
in the training month. The overlap reflects natural
processes on the web, with a long tail (of queries
that will never be seen again) and a big fat head (of
queries that come up again and again). Throwing
away the overlap would both drastically reduce the
size of the data and make the problem less realistic
for a commercial application.
We therefore report results on various training
set sizes so that we can show both: (a) the abil-
ity of the proposed method to generalize to unseen
queries, and (b) the high performance of the base-
lines in a realistic setting. We vary the number of
new queries by training the methods on subsets of
20%, 40%, 60%, 80%, and 100% of the positive
examples (along with all the negative examples)
in the training set. This led to the test set having
17%, 34%, 52%, 67%, and 86% actual overlap of
these queries, respectively, with the training sets.
1431
Click Type Query Type Example
(Area on Results Page) (Intent)
Spelling Suggestion Typo www.lastmintue.com.au
Ad Commercial Intent ebay official
Query Suggestion Suggestible sears employees (where there are some popular query suggestions
indicating how current employees can navigate to the benefits site,
as well as how others can apply for employment)
Search Result Standard Search craigslist, denver, co
Table 1: Queries with a high percentage of clicks in each category
4 Three CQI Methods
4.1 Method 1: Look-up Baseline
The baseline method checks if a query was present
in the training set, and if so, outputs the label from
the training set. If the query was not present, it
backs off to the appropriate default label: ?non-
commercial? for the commercial intent task (and
?non-suggestible?, ?not a typo?, etc. for the other
CQI tasks). This very simple baseline method
is effective because the ratios tend to be fairly
stable from one month to the next. The query,
ebay official, for example, has relatively high ad
clicks in both the training month as well as the
test month. The next section will propose an al-
ternative method to address the main weakness of
the baseline method, the inability to generalize be-
yond the queries in the training set.
Figure 2: saks and bluefly trench coats are known
to be commercial, while world war I trench is
known to be non-commercial. What about de-
signer trench? We can classify it as commercial
because it shares URLs with the known commer-
cial queries.
4.2 Method 2: Using Click Graph Context to
Generalize Beyond the Queries in the
Training Set
To address the generalization concern, we propose
a method inspired by Yarowsky (1994). Word
sense disambiguation is a classic problem in nat-
ural language processing. Some words have mul-
tiple senses; for instance, bank can either mean
a riverbank or a financial institution, and for var-
ious tasks such as information retrieval, parsing,
or information extraction, it is useful to be able to
differentiate between the possible meanings.
When a word is being used in each sense, it
tends to appear in a different context. For exam-
ple, if the word muddy is nearby bank, the author
is probably using the riverbank sense of the term,
while if the word deposit is nearby, the word is
probably being used with the financial sense.
Yarowksy (1995) thus creates a list of each pos-
sible context, sorted by how strong the evidence is
for a particular sense. To classify a new example,
Yarowsky (1994) finds the most informative collo-
cation pattern that applies to the test example.
In this work, rather than using the surrounding
words as context as in text classification, we con-
sider the surrounding URLs in the click graph as
context. A sample portion of the click graph is
shown in figure 2. The figure shows queries on
the left and URLs on the right. The click graph
was computed on a very large sample of logs com-
puted well before the training period. There is an
edge from a query q to a URL u if at least 10 users
issued q and then clicked on u.
For each URL, we look at its neighboring
queries and calculate the log likelihood ratio of
their labels in the training set. We classify a new
query q according to URL
?
, the neighboring URL
with the strongest opinion (highest absolute value
of the log likelihood ratio). That is, we compute
URL
?
with:
1432
argmax
U
i
?Nbr(q)
?
?
?
?
log
Pr(Intent|U
i
)
Pr(?Intent|U
i
)
?
?
?
?
If the neighboring opinion is positive (that is,
Pr(Intent|URL
?
) > Pr(?Intent|URL
?
)), then
the query q is assigned a positive label. Otherwise,
q is assigned a negative label.
In Figure 2, we classify designer trench as a
commercial query based on the neighbor with
the strongest opinion. In this case, there
was a tie between two neighbors with equally
strong opinions: www.saksfifthavenue.com and
www.bluefly.com/Designer-Trench-Coats. Both
neighbors are associated with queries that were
labeled commercial in the training set: saks and
bluefly trench coats, respectively.
This method allows the labels of training set
queries to propagate through the URLs to new test
set queries.
4.3 Method 3: Hybrid (?Better Together?)
We recommend a hybrid of the two methods:
? Method 1: the look-up baseline
? Method 2: use click graph context to gener-
alize beyond the queries in the training set
Method 1 is designed for precision and method 2
is designed for recall. The hybrid uses method
1 when applicable, and otherwise, backs off to
method 2.
5 Results
5.1 Commercial Intent
Table 2 and Figures 3(a) and 3(b) compare the per-
formance on the proposed hybrid method with the
baseline. When there is plenty of training mate-
rial, both methods perform about equally well (the
look-up baseline has an F-score of 84.1%, com-
pared with the hybrid method?s F-score of 85.3%),
but generalization becomes important when train-
ing data is severely limited. Figure 3(a) shows
that the proposed method does no harm and might
even help a little when there is plenty of training
data. The hybrid?s main benefit is generalization
to queries beyond the training set. If we severely
limit the size of the training set to just 20% of the
month, as in Figure 3(b), then the proposed hybrid
method is substantially better than the baseline. In
this case, the proposed hybrid method?s F-score
is 65.8%, compared with the look-up method?s F-
score of 28.4%.
5.2 Other types of clicks
Table 3 and Figures 4(a) and 4(b) show a similar
pattern for the query suggestion task. In fact, the
pattern is perhaps even stronger for the query sug-
gestion task than commercial intent. When the full
training set is used, the hybrid method achieves
an F-score of 91.9% (precision = 91.5%, recall =
92.3%). When only 20% of the training data is
used, the hybrid method has an F-score of 73.9%,
compared with the baseline?s F-score of 29.6%. A
similar pattern was observed for clicks on search
results.
The one exception is the spelling suggestion
task, where the context heuristic proved ineffec-
tive, for reasons that should not be surprising in
retrospect. Click graph distance is an effective
heuristic for many intents, but not for typos. Users
who issue misspelled the query have the same
goals as users who correctly spell the query, so
we shouldn?t expect URLs to be able to differ-
entiate them. For misspelled queries, for exam-
ple, yuotube, there are correctly spelled queries,
like youtube, with the same intent that will tend to
be associated with the same set of URLs (such as
www.youtube.com).
6 Conclusion and Future Work
We would like to be able to distinguish web
queries by intent. Unfortunately, we don?t have
annotated data for query intent, but we do have
access to large quantities of click logs. The logs
distinguish four types of clicks: (a) search results,
(b) ads, (c) spelling suggestions and (d) query sug-
gestions. Clicks on ads are evidence for commer-
cial intent; other types of clicks are evidence for
other intents. Click logs are huge sources of data,
and while there are privacy concerns, anonymized
logs are beginning to be released for research pur-
poses (Craswell et al, 2009).
Besides commercial intent, queries can also be
divided into two broader classes: queries in which
the user is browsing and queries for which the user
is navigating. Clicks on the ads and query sug-
gestions indicate that users are browsing and will-
ing to look at these alternative suggestions, while
clicks on the search results indicate that the users
were navigating to what they were searching for.
Clicks on typos indicate neither, as presumably the
users are not entering typos on purpose.
Just as dialogue management systems learn
policies for when to allow user initiative (the user
1433
(a) (b)
Figure 3: Better together: proposed hybrid is no worse than baseline (left) and generalizes better to
unseen tail queries (right). The two panels are the same, except that the training set was reduced on the
right to test generalization error.
(a) (b)
Figure 4: Similar to Figures 3(a) and 3(b), adding the decision list method generalizes over the look-up
method for the ?suggestible? task.
can respond in an open way) versus system ini-
tiative (the system asks the user questions with a
restricted set of possible answers) (Rela?no et al,
1999; Scheffler and Young, 2002; Singh et al,
2002), search engines may want to learn policies
for when the user just wants the search results or
when the user is open to suggestions. When users
want help (they want the search engine to suggest
results), more space on the page should be devoted
to the ads and the query suggestions. When the
users know what it is they want, more of the page
should be given to the search results they asked
for.
We started with a simple baseline for predicting
click location that had great precision, but didn?t
generalize well beyond the queries in the train-
ing set. To improve recall, we proposed a con-
text heuristic that backs off in the click graph.
The backoff method is similar to Yarowsky?s Word
Sense Disambiguation method, except that context
is defined in terms of URLs nearby in click graph
distance, as opposed to words nearby in the text.
Our third method, a hybrid of the baseline
method and the backoff method, is the strongest
baseline we have come up with. The evaluation
showed that the hybrid does no harm when there
is plenty of training data, and generalizes better
when there isn?t.
A direction for further research would be to see
if propagating query intent through URLs that are
not direct neighbors but are further away, perhaps
through random walk methods (Baluja et al, 2008;
1434
Training Size F-score Precision / Recall
Baseline Method 2 Hybrid Baseline Method 2 Hybrid
100% 84.1 75.6 85.3 88.2 / 80.4 76.6 / 74.6 85.7 / 85.0
80% 74.4 74.8 83.5 88.2 / 64.3 79.3 / 70.7 86.7 / 80.6
60% 62.4 72.9 80.7 88.3 / 48.2 82.5 / 65.3 87.9 / 74.6
40% 47.9 70.1 76.0 77.5 / 34.7 78.5 / 63.3 80.7 / 66.0
20% 28.4 62.5 65.8 77.6 / 17.4 75.9 / 53.1 74.3 / 59.1
Table 2: The baseline and hybrid methods have comparable F-scores when there is plenty of training
data, but generalization becomes important when training data is severely limited. The proposed hybrid
method generalizes better as indicated by the widening gap in F-scores with smaller and smaller training
sets.
Training Size F-score Precision / Recall
Baseline Method 2 Hybrid Baseline Method 2 Hybrid
100% 91.0 86.2 91.9 94.9 / 87.4 90.7 / 82.3 91.5 / 92.3
80% 80.5 85.2 90.6 94.9 / 69.9 91.6 / 79.7 91.9 / 89.4
60% 67.6 83.3 88.6 94.9 / 52.4 92.6 / 75.8 92.3 / 85.1
40% 51.0 79.5 84.7 94.9 / 34.9 87.6 / 72.7 93.0 / 77.8
20% 29.6 69.8 73.9 81.5 / 18.1 90.6 / 56.8 94.0 / 60.8
Table 3: F-scores on the query suggestion task. As in the commercial intent task, the proposed hybrid
method does no harm when there is plenty of training data, but generalizes better when training data is
severely limited.
Antonellis et al, 2008) improves classification.
Similar methods could be applied in future work
to many other applications such labeling queries
and URLs by: language, market, location, time,
intended for a search vertical (such as medicine,
recipes), intended for a type of answer (maps, pic-
tures), as well as inappropriate intent (porn, spam).
In addition to click type, there are many other
features in the logs that could prove useful for
classifying queries by intent, e.g., who issued the
query, when and where. Similar methods could
also be used to personalize search (Teevan et al,
2008); for queries that mean different things to dif-
ferent people, the Yarowsky method could be ap-
plied to variables such as user, time and place, so
the results reflect what a particular user intended
in a particular context.
7 Acknowledgments
We thank Sue Dumais for her helpful comments
on an early draft of this work. We would also like
to thank the members of the Text Mining, Search,
and Navigation (TMSN) group at Microsoft Re-
search for useful discussions and the anonymous
reviewers for their helpful comments.
References
I. Antonellis, H. Garcia-Molina, and C.C. Chang.
2008. Simrank++: query rewriting through link
analysis of the clickgraph (poster). WWW.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008.
Video suggestion and discovery for youtube: taking
random walks through the view graph. WWW.
D. Beeferman and A. Berger. 2000. Agglomerative
clustering of a search engine query log. In SIGKDD,
pages 407?416.
S.M. Beitzel, E.C. Jensen, A. Chowdhury, D. Gross-
man, and O. Frieder. 2004. Hourly analysis of a
very large topically categorized web query log. SI-
GIR, pages 321?328.
S.M. Beitzel, E.C. Jensen, O. Frieder, D.D. Lewis,
A. Chowdhury, and A. Kolcz. 2005. Improving
automatic query classification via semi-supervised
learning. ICDM, pages 42?49.
A.Z. Broder, M. Fontoura, E. Gabrilovich, A. Joshi,
V. Josifovski, and T. Zhang. 2007. Robust classifi-
cation of rare queries using web knowledge. SIGIR,
pages 231?238.
A. Broder. 2002. A taxonomy of web search. SIGIR,
36(2).
R. Chaiken, B. Jenkins, P.
?
A. Larson, B. Ramsey,
D. Shakib, S. Weaver, and J. Zhou. 2008. SCOPE:
1435
Easy and efficient parallel processing of massive
data sets. Proceedings of the VLDB Endowment
archive, 1(2):1265?1276.
D. Chakrabarti, D. Agarwal, and V. Josifovski. 2008.
Contextual advertising by combining relevance with
click feedback. WWW.
M. Ciaramita, V. Murdock, and V. Plachouras. 2008.
Online learning from click data for sponsored
search.
N. Craswell and M. Szummer. 2007. Random walks
on the click graph. In Proceedings of the 30th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 239?246.
N. Craswell, R. Jones, G. Dupret, and E. Viegas (Con-
ference Chairs). 2009. Wscd ?09: Proceedings of
the 2009 workshop on web search click data.
H.K. Dai, L. Zhao, Z. Nie, J.R. Wen, L. Wang, and
Y. Li. 2006. Detecting online commercial intention
(OCI). WWW, pages 829?837.
J. Dean and S. Ghemawat. 2004. MapReduce: Sim-
plified Data Processing on Large Clusters. OSDI,
pages 137?149.
D. Downey, S. Dumais, D. Liebling, and E. Horvitz.
2008. Understanding the relationship between
searchers? queries and information goals. In CIKM.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD), ACM.
U. Lee, Z. Liu, and J. Cho. 2005. Automatic identifi-
cation of user goals in Web search. In WWW, pages
391?400.
X. Li, Y.Y. Wang, and A. Acero. 2008. Learning
query intent from regularized click graphs. In SI-
GIR, pages 339?346.
B. Piwowarski and H. Zaragoza. 2007. Predictive user
click models based on click-through history. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
pages 175?182.
J. Rela?no, D. Tapias, M. Rodr??guez, M. Charfuel?an,
and L. Hern?andez. 1999. Robust and flexible
mixed-initiative dialogue for telephone services. In
Proceedings of EACL.
D.E. Rose and D. Levinson. 2004. Understanding user
goals in web search. WWW, pages 13?19.
K. Scheffler and S. Young. 2002. Automatic learn-
ing of dialogue strategy using dialogue simulation
and reinforcement learning. In Proceedings of HLT,
pages 12?19.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue management with reinforce-
ment learning: Experiments with the NJFun sys-
tem. Journal of Artificial Intelligence Research,
16(1):105?133.
Bin Tan, Xuehua Shen, and ChengXiang Zhai. 2006.
Mining long-term search history to improve search
accuracy. pages 718?723. KDD.
J. Teevan, S.T. Dumais, and D.J. Liebling. 2008. To
personalize or not to personalize: modeling queries
with variation in user intent. SIGIR, pages 163?170.
Gui-Rong Xue, Hua-Jun Zeng, Zheng Chen, Yong
Yu, Wei-Ying Ma, WenSi Xi, and WeiGuo Fan.
2004. Optimizing web search using web click-
through data. In CIKM ?04: Proceedings of the thir-
teenth ACM international conference on Informa-
tion and knowledge management, pages 118?126.
D. Yarowsky. 1994. Decision lists for lexical ambigu-
ity resolution: Application to accent restoration in
Spanish and French. ACL, pages 88?95.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. ACL, pages
189?196.
D. Zhou, O. Bousquet, T.N. Lal, J. Weston, and
B. Scholkopf. 2004. Learning with Local and
Global Consistency. In NIPS.
1436
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 683?691,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Automatic sense prediction for implicit discourse relations in text
Emily Pitler, Annie Louis, Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler,lannie,nenkova@seas.upenn.edu
Abstract
We present a series of experiments on au-
tomatically identifying the sense of im-
plicit discourse relations, i.e. relations
that are not marked with a discourse con-
nective such as ?but? or ?because?. We
work with a corpus of implicit relations
present in newspaper text and report re-
sults on a test set that is representative
of the naturally occurring distribution of
senses. We use several linguistically in-
formed features, including polarity tags,
Levin verb classes, length of verb phrases,
modality, context, and lexical features. In
addition, we revisit past approaches using
lexical pairs from unannotated text as fea-
tures, explain some of their shortcomings
and propose modifications. Our best com-
bination of features outperforms the base-
line from data intensive approaches by 4%
for comparison and 16% for contingency.
1 Introduction
Implicit discourse relations abound in text and
readers easily recover the sense of such relations
during semantic interpretation. But automatic
sense prediction for implicit relations is an out-
standing challenge in discourse processing.
Discourse relations, such as causal and contrast
relations, are often marked by explicit discourse
connectives (also called cue words) such as ?be-
cause? or ?but?. It is not uncommon, though, for a
discourse relation to hold between two text spans
without an explicit discourse connective, as the ex-
ample below demonstrates:
(1) The 101-year-old magazine has never had to woo ad-
vertisers with quite so much fervor before.
[because] It largely rested on its hard-to-fault demo-
graphics.
In this paper we address the problem of au-
tomatic sense prediction for discourse relations
in newspaper text. For our experiments, we use
the Penn Discourse Treebank, the largest exist-
ing corpus of discourse annotations for both im-
plicit and explicit relations. Our work is also
informed by the long tradition of data intensive
methods that rely on huge amounts of unanno-
tated text rather than on manually tagged corpora
(Marcu and Echihabi, 2001; Blair-Goldensohn et
al., 2007).
In our analysis, we focus only on implicit dis-
course relations and clearly separate these from
explicits. Explicit relations are easy to iden-
tify. The most general senses (comparison, con-
tingency, temporal and expansion) can be disam-
biguated in explicit relations with 93% accuracy
based solely on the discourse connective used to
signal the relation (Pitler et al, 2008). So report-
ing results on explicit and implicit relations sepa-
rately will allow for clearer tracking of progress.
In this paper we investigate the effectiveness of
various features designed to capture lexical and
semantic regularities for identifying the sense of
implicit relations. Given two text spans, previous
work has used the cross-product of the words in
the spans as features. We examine the most infor-
mative word pair features and find that they are not
the semantically-related pairs that researchers had
hoped. We then introduce several other methods
capturing the semantics of the spans (polarity fea-
tures, semantic classes, tense, etc.) and evaluate
their effectiveness. This is the first study which
reports results on classifying naturally occurring
implicit relations in text and uses the natural dis-
tribution of the various senses.
2 Related Work
Experiments on implicit and explicit relations
Previous work has dealt with the prediction of dis-
course relation sense, but often for explicits and at
the sentence level.
Soricut and Marcu (2003) address the task of
683
parsing discourse structures within the same sen-
tence. They use the RST corpus (Carlson et al,
2001), which contains 385 Wall Street Journal ar-
ticles annotated following the Rhetorical Structure
Theory (Mann and Thompson, 1988). Many of
the useful features, syntax in particular, exploit
the fact that both arguments of the connective are
found in the same sentence. Such features would
not be applicable to the analysis of implicit rela-
tions that occur intersententially.
Wellner et al (2006) used the GraphBank (Wolf
and Gibson, 2005), which contains 105 Associated
Press and 30 Wall Street Journal articles annotated
with discourse relations. They achieve 81% accu-
racy in sense disambiguation on this corpus. How-
ever, GraphBank annotations do not differentiate
between implicits and explicits, so it is difficult to
verify success for implicit relations.
Experiments on artificial implicits Marcu and
Echihabi (2001) proposed a method for cheap ac-
quisition of training data for discourse relation
sense prediction. Their idea is to use unambiguous
patterns such as [Arg1, but Arg2.] to create syn-
thetic examples of implicit relations. They delete
the connective and use [Arg1, Arg2] as an example
of an implicit relation.
The approach is tested using binary classifica-
tion between relations on balanced data, a setting
very different from that of any realistic applica-
tion. For example, a question-answering appli-
cation that needs to identify causal relations (i.e.
as in Girju (2003)), must not only differentiate
causal relations from comparison relations, but
also from expansions, temporal relations, and pos-
sibly no relation at all. In addition, using equal
numbers of examples of each type can be mislead-
ing because the distribution of relations is known
to be skewed, with expansions occurring most fre-
quently. Causal and comparison relations, which
are most useful for applications, are less frequent.
Because of this, the recall of the classification
should be the primary metric of success, while
the Marcu and Echihabi (2001) experiments report
only accuracy.
Later work (Blair-Goldensohn et al, 2007;
Sporleder and Lascarides, 2008) has discovered
that the models learned do not perform as well on
implicit relations as one might expect from the test
accuracies on synthetic data.
3 Penn Discourse Treebank
For our experiments, we use the Penn Discourse
Treebank (PDTB; Prasad et al, 2008), the largest
available annotated corpora of discourse relations.
The PDTB contains discourse annotations over the
same 2,312 Wall Street Journal (WSJ) articles as
the Penn Treebank.
For each explicit discourse connective (such as
?but? or ?so?), annotators identified the two text
spans between which the relation holds and the
sense of the relation.
The PDTB also provides information about lo-
cal implicit relations. For each pair of adjacent
sentences within the same paragraph, annotators
selected the explicit discourse connective which
best expressed the relation between the sentences
and then assigned a sense to the relation. In Exam-
ple (1) above, the annotators identified ?because?
as the most appropriate connective between the
sentences, and then labeled the implicit discourse
relation Contingency.
In the PDTB, explicit and implicit relations are
clearly distinguished, allowing us to concentrate
solely on the implicit relations.
As mentioned above, each implicit and explicit
relation is annotated with a sense. The senses
are arranged in a hierarchy, allowing for annota-
tions as specific as Contingency.Cause.reason. In
our experiments, we use only the top level of the
sense annotations: Comparison, Contingency, Ex-
pansion, and Temporal. Using just these four rela-
tions allows us to be theory-neutral; while differ-
ent frameworks (Hobbs, 1979; McKeown, 1985;
Mann and Thompson, 1988; Knott and Sanders,
1998; Asher and Lascarides, 2003) include differ-
ent relations of varying specificities, all of them
include these four core relations, sometimes under
different names.
Each relation in the PDTB takes two arguments.
Example (1) can be seen as the predicate Con-
tingency which takes the two sentences as argu-
ments. For implicits, the span in the first sentence
is called Arg1 and the span in the following sen-
tence is called Arg2.
4 Word pair features in prior work
Cross product of words Discourse connectives
are the most reliable predictors of the semantic
sense of the relation (Marcu, 2000; Pitler et al,
2008). However, in the absence of explicit mark-
ers, the most easily accessible features are the
684
words in the two text spans of the relation. In-
tuitively, one would expect that there is some rela-
tionship that holds between the words in the two
arguments. Consider for example the following
sentences:
The recent explosion of country funds mirrors the ?closed-
end fund mania? of the 1920s, Mr. Foot says, when narrowly
focused funds grew wildly popular. They fell into oblivion
after the 1929 crash.
The words ?popular? and ?oblivion? are almost
antonyms, and one might hypothesize that their
occurrence in the two text spans is what triggers
the contrast relation between the sentences. Sim-
ilarly, a pair of words such as (rain, rot) might be
indicative of a causal relation. If this hypothesis is
correct, pairs of words (w1, w2) such that w1 ap-
pears in the first sentence and w2 appears in the
second sentence would be good features for iden-
tifying contrast relations.
Indeed, word pairs form the basic feature
of most previous work on classifying implicit
relations (Marcu and Echihabi, 2001; Blair-
Goldensohn et al, 2007; Sporleder and Las-
carides, 2008) or the simpler task of predicting
which connective should be used to express a rela-
tion (Lapata and Lascarides, 2004).
Semantic relations vs. function word pairs If
the hypothesis for word pair triggers of discourse
relations were true, the analysis of unambiguous
relations can be used to discover pairs of words
with causal or contrastive relations holding be-
tween them. Yet, feature analysis has not been per-
formed in prior studies to establish or refute this
possibility.
At the same time, feature selection is always
necessary for word pairs, which are numerous and
lead to data sparsity problems. Here, we present a
meta analysis of the feature selection work in three
prior studies.
One approach for reducing the number of fea-
tures follows the hypothesis of semantic rela-
tions between words. Marcu and Echihabi (2001)
considered only nouns, verbs and and other cue
phrases in word pairs. They found that even
with millions of training examples, prediction re-
sults using all words were superior to those based
on only pairs of non-function words. However,
since the learning curve is steeper when function
words were removed, they hypothesize that using
only non-function words will outperform using all
words once enough training data is available.
In a similar vein, Lapata and Lascarides (2004)
used pairings of only verbs, nouns and adjectives
for predicting which temporal connective is most
suitable to express the relation between two given
text spans. Verb pairs turned out to be one of the
best features, but no useful information was ob-
tained using nouns and adjectives.
Blair-Goldensohn et al (2007) proposed sev-
eral refinements of the word pair model. They
show that (i) stemming, (ii) using a small fixed
vocabulary size consisting of only the most fre-
quent stems (which would tend to be dominated
by function words) and (iii) a cutoff on the mini-
mum frequency of a feature, all result in improved
performance. They also report that filtering stop-
words has a negative impact on the results.
Given these findings, we expect that pairs of
function words are informative features helpful in
predicting discourse relation sense. In our work
that we describe next, we use feature selection to
investigate the word pairs in detail.
5 Analysis of word pair features
For the analysis of word pair features, we use
a large collection of automatically extracted ex-
plicit examples from the experiments in Blair-
Goldensohn et al (2007). The data, from now on
referred to as TextRels, has explicit contrast and
causal relations which were extracted from the En-
glish Gigaword Corpus (Graff, 2003) which con-
tains over four million newswire articles.
The explicit cue phrase is removed from each
example and the spans are treated as belonging to
an implicit relation. Besides cause and contrast,
the TextRels data include a no-relation category
which consists of sentences from the same text that
are separated by at least three other sentences.
To identify features useful for classifying com-
parison vs other relations, we chose a random sam-
ple of 5000 examples for Contrast and 5000 Other
relations (2500 each of Cause and No-relation).
For the complete set of 10,000 examples, word
pair features were computed. After removing
word pairs that appear less than 5 times, the re-
maining features were ranked by information gain
using the MALLET toolkit1.
Table 1 lists the word pairs with highest infor-
mation gain for the Contrast vs. Other and Cause
vs. Other classification tasks. All contain very fre-
quent stop words, and interestingly for the Con-
1mallet.cs.umass.edu
685
trast vs. Other task, most of the word pairs contain
discourse connectives.
This is certainly unexpected, given that word
pairs were formed by deleting the discourse con-
nectives from the sentences expressing Contrast.
Word pairs containing ?but? as one of their ele-
ments in fact signal the presence of a relation that
is not Contrast.
Consider the example shown below:
The government says it has reached most isolated townships
by now, but because roads are blocked, getting anything but
basic food supplies to people remains difficult.
Following Marcu and Echihabi (2001), the pair
[The government says it has reached most isolated
townships by now, but] and [roads are blocked,
getting anything but basic food supplies to peo-
ple remains difficult.] is created as an example of
the Cause relation. Because of examples like this,
?but-but? is a very useful word pair feature indi-
cating Cause, as the but would have been removed
for the artifical Contrast examples. In fact, the top
17 features for classifying Contrast versus Other
all contain the word ?but?, and are indications that
the relation is Other.
These findings indicate an unexpected anoma-
lous effect in the use of synthetic data. Since re-
lations are created by removing connectives, if an
unambiguous connective remains, its presence is a
reliable indicator that the example should be clas-
sified as Other. Such features might work well and
lead to high accuracy results in identifying syn-
thetic implicit relations, but are unlikely to be use-
ful in a realistic setting of actual implicits.
Comparison vs. Other Contingency vs. Other
the-but s-but the-in the-and in-the the-of
of-but for-but but-but said-said to-of the-a
in-but was-but it-but a-and a-the of-the
to-but that-but the-it* to-and to-to the-in
and-but but-the to-it* and-and the-the in-in
a-but he-but said-in to-the of-and a-of
said-but they-but of-in in-and in-of s-and
Table 1: Word pairs with highest information gain.
Also note that the only two features predic-
tive of the comparison class (indicated by * in
Table 1): the-it and to-it, contain only func-
tion words rather than semantically related non-
function words. This ranking explains the obser-
vations reported in Blair-Goldensohn et al (2007)
where removing stopwords degraded classifier
performance and why using only nouns, verbs or
adjectives (Marcu and Echihabi, 2001; Lapata and
Lascarides, 2004) is not the best option2.
6 Features for sense prediction of
implicit discourse relations
The contrast between the ?popular?/?oblivion? ex-
ample we started with above can be analyzed in
terms of lexical relations (near antonyms), but also
could be explained by different polarities of the
two words: ?popular? is generally a positive word,
while ?oblivion? has negative connotations.
While we agree that the actual words in the ar-
guments are quite useful, we also define several
higher-level features corresponding to various se-
mantic properties of the words. The words in the
two text spans of a relation are taken from the
gold-standard annotations in the PDTB.
Polarity Tags: We define features that represent
the sentiment of the words in the two spans. Each
word?s polarity was assigned according to its en-
try in the Multi-perspective Question Answering
Opinion Corpus (Wilson et al, 2005). In this re-
source, each sentiment word is annotated as posi-
tive, negative, both, or neutral. We use the number
of negated and non-negated positive, negative, and
neutral sentiment words in the two text spans as
features. If a writer refers to something as ?nice?
in Arg1, that counts towards the positive sentiment
count (Arg1Positive); ?not nice? would count to-
wards Arg1NegatePositive. A sentiment word is
negated if a word with a General Inquirer (Stone
et al, 1966) Negate tag precedes it. We also have
features for the cross products of these polarities
between Arg1 and Arg2.
We expected that these features could help
Comparison examples especially. Consider the
following example:
Executives at Time Inc. Magazine Co., a subsidiary of
Time Warner, have said the joint venture with Mr. Lang
wasn?t a good one. The venture, formed in 1986, was sup-
posed to be Time?s low-cost, safe entry into women?s maga-
zines.
The word good is annotated with positive po-
larity, however it is negated. Safe is tagged as
having positive polarity, so this opposition could
indicate the Comparison relation between the two
sentences.
Inquirer Tags: To get at the meanings of the
spans, we look up what semantic categories each
2In addition, an informal inspection of 100 word pairs
with high information gain for Contrast vs. Other (the longest
word pairs were chosen, as those are more likely to be content
words) found only six semantically opposed pairs.
686
word falls into according to the General Inquirer
lexicon (Stone et al, 1966). The General In-
quirer has classes for positive and negative polar-
ity, as well as more fine-grained categories such as
words related to virtue or vice. The Inquirer even
contains a category called ?Comp? that includes
words that tend to indicate Comparison, such as
?optimal?, ?other?, ?supreme?, or ?ultimate?.
Several of the categories are complementary:
Understatement versus Overstatement, Rise ver-
sus Fall, or Pleasure versus Pain. Pairs where one
argument contains words that indicate Rise and the
other argument indicates Fall might be good evi-
dence for a Comparison relation.
The benefit of using these tags instead of just
the word pairs is that we see more observations for
each semantic class than for any particular word,
reducing the data sparsity problem. For example,
the pair rose:fell often indicates a Comparison re-
lation when speaking about stocks. However, oc-
casionally authors refer to stock prices as ?jump-
ing? rather than ?rising?. Since both jump and rise
are members of the Rise class, new jump examples
can be classified using past rise examples.
Development testing showed that including fea-
tures for all words? tags was not useful, so we in-
clude the Inquirer tags of only the verbs in the two
arguments and their cross-product. Just as for the
polarity features, we include features for both each
tag and its negation.
Money/Percent/Num: If two adjacent sen-
tences both contain numbers, dollar amounts, or
percentages, it is likely that a comparison rela-
tion might hold between the sentences. We in-
cluded a feature for the count of numbers, percent-
ages, and dollar amounts in Arg1 and Arg2. We
also included the number of times each combina-
tion of number/percent/dollar occurs in Arg1 and
Arg2. For example, if Arg1 mentions a percent-
age and Arg2 has two dollar amounts, the feature
Arg1Percent-Arg2Money would have a count of 2.
This feature is probably genre-dependent. Num-
bers and percentages often appear in financial texts
but would be less frequent in other genres.
WSJ-LM: This feature represents the extent to
which the words in the text spans are typical of
each relation. For each sense, we created uni-
gram and bigram language models over the im-
plicit examples in the training set. We compute
each example?s probability according to each of
these language models. The features are the ranks
of the spans? likelihoods according to the vari-
ous language models. For example, if of the un-
igram models, the most likely relation to generate
this example was Contingency, then the example
would include the feature ContingencyUnigram1.
If the third most likely relation according to the
bigram models was Expansion, then it would in-
clude the feature ExpansionBigram3.
Expl-LM: This feature ranks the text spans ac-
cording to language models derived from the ex-
plicit examples in the TextRels corpus. However,
the corpus contains only Cause, Contrast and No-
relation, hence we expect the WSJ language mod-
els to be more helpful.
Verbs: These features include the number of
pairs of verbs in Arg1 and Arg2 from the same
verb class. Two verbs are from the same verb class
if each of their highest Levin verb class (Levin,
1993) levels (in the LCS Database (Dorr, 2001))
are the same. The intuition behind this feature is
that the more related the verbs, the more likely the
relation is an Expansion.
The verb features also include the average
length of verb phrases in each argument, as well
as the cross product of this feature for the two ar-
guments. We hypothesized that verb chunks that
contain more words, such as ?They [are allowed to
proceed]? often contain rationales afterwards (sig-
nifying Contingency relations), while short verb
phrases like ?They proceed? might occur more of-
ten in Expansion or Temporal relations.
Our final verb features were the part of speech
tags (gold-standard from the Penn Treebank) of
the main verb. One would expect that Expansion
would link sentences with the same tense, whereas
Contingency and Temporal relations would con-
tain verbs with different tenses.
First-Last, First3: The first and last words of
a relation?s arguments have been found to be par-
ticularly useful for predicting its sense (Wellner et
al., 2006). Wellner et al (2006) suggest that these
words are such predictive features because they
are often explicit discourse connectives. In our
experiments on implicits, the first and last words
are not connectives. However, some implicits have
been found to be related by connective-like ex-
pressions which often appear in the beginning of
the second argument. In the PDTB, these are an-
notated as alternatively lexicalized relations (Al-
tLexes). To capture such effects, we included the
first and last words of Arg1 as features, the first
687
and last words of Arg2, the pair of the first words
of Arg1 and Arg2, and the pair of the last words.
We also add two additional features which indicate
the first three words of each argument.
Modality: Modal words, such as ?can?,
?should?, and ?may?, are often used to express
conditional statements (i.e. ?If I were a wealthy
man, I wouldn?t have to work hard.?) thus signal-
ing a Contingency relation. We include a feature
for the presence or absence of modals in Arg1 and
Arg2, features for specific modal words, and their
cross-products.
Context: Some implicit relations appear imme-
diately before or immediately after certain explicit
relations far more often than one would expect due
to chance (Pitler et al, 2008). We define a feature
indicating if the immediately preceding (or follow-
ing) relation was an explicit. If it was, we include
the connective trigger of the relation and its sense
as features. We use oracle annotations of the con-
nective sense, however, most of the connectives
are unambiguous.
One might expect a different distribution of re-
lation types in the beginning versus further in the
middle of a paragraph. We capture paragraph-
position information using a feature which indi-
cates if Arg1 begins a paragraph.
Word pairs Four variants of word pair mod-
els were used in our experiments. All the models
were eventually tested on implicit examples from
the PDTB, but the training set-up was varied.
Wordpairs-TextRels In this setting, we trained
a model on word pairs derived from unannotated
text (TextRels corpus).
Wordpairs-PDTBImpl Word pairs for training
were formed from the cross product of words in
the textual spans (Arg1 x Arg2) of the PDTB im-
plicit relations.
Wordpairs-selected Here, only word pairs from
Wordpairs-PDTBImpl with non-zero information
gain on the TextRels corpus were retained.
Wordpairs-PDTBExpl In this case, the model
was formed by using the word pairs from the ex-
plicit relations in the sections of the PDTB used
for training.
7 Classification Results
For all experiments, we used sections 2-20 of the
PDTB for training and sections 21-22 for testing.
Sections 0-1 were used as a development set for
feature design.
We ran four binary classification tasks to iden-
tify each of the main relations from the rest. As
each of the relations besides Expansion are infre-
quent, we train using equal numbers of positive
and negative examples of the target relation. The
negative examples were chosen at random. We
used all of sections 21 and 22 for testing, so the
test set is representative of the natural distribution.
The training sets contained: Comparison (1927
positive, 1927 negative), Contingency (3500
each), Expansion3 (6356 each), and Temporal
(730 each).
The test set contained: 151 examples of Com-
parison, 291 examples of Contingency, 986 exam-
ples of Expansion, 82 examples of Temporal, and
13 examples of No-relation.
We used Naive Bayes, Maximum Entropy
(MaxEnt), and AdaBoost (Freund and Schapire,
1996) classifiers implemented in MALLET.
7.1 Non-Wordpair Features
The performance using only our semantically in-
formed features is shown in Table 7. Only the
Naive Bayes classification results are given, as
space is limited and MaxEnt and AdaBoost gave
slightly lower accuracies overall.
The table lists the f-score for each of the target
relations, with overall accuracy shown in brack-
ets. Given that the experiments are run on natural
distribution of the data, which are skewed towards
Expansion relations, the f-score is the more impor-
tant measure to track.
Our random baseline is the f-score one would
achieve by randomly assigning classes in propor-
tion to its true distribution in the test set. The best
results for all four tasks are considerably higher
than random prediction, but still low overall. Our
features provide 6% to 18% absolute improve-
ments in f-score over the baseline for each of the
four tasks. The largest gain was in the Contin-
gency versus Other prediction task. The least im-
provement was for distinguishing Expansion ver-
sus Other. However, since Expansion forms the
largest class of relations, its f-score is still the
highest overall. We discuss the results per relation
class next.
Comparison We expected that polarity features
would be especially helpful for identifying Com-
3The PDTB also contains annotations of entity relations,
which most frameworks consider a subset of Expansion.
Thus, we include relations annotated as EntRel as positive
examples of Expansion.
688
Features Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Four-way
Money/Percent/Num 19.04 (43.60) 18.78 (56.27) 22.01 (41.37) 10.40 (23.05) (63.38)
Polarity Tags 16.63 (55.22) 19.82 (76.63) 71.29 (59.23) 11.12 (18.12) (65.19)
WSJ-LM 18.04 (9.91) 0.00 (80.89) 0.00 (35.26) 10.22 (5.38) (65.26)
Expl-LM 18.04 (9.91) 0.00 (80.89) 0.00 (35.26) 10.22 (5.38) (65.26)
Verbs 18.55 (26.19) 36.59 (62.44) 59.36 (52.53) 12.61 (41.63) (65.33)
First-Last, First3 21.01 (52.59) 36.75 (59.09) 63.22 (56.99) 15.93 (61.20) (65.40)
Inquirer tags 17.37 (43.8) 15.76 (77.54) 70.21 (58.04) 11.56 (37.69) (62.21)
Modality 17.70 (17.6) 21.83 (76.95) 15.38 (37.89) 11.17 (27.91) (65.33)
Context 19.32 (56.66) 29.55 (67.42) 67.77 (57.85) 12.34 (55.22) (64.01)
Random 9.91 19.11 64.74 5.38
Table 2: f-score (accuracy) using different features; Naive Bayes.
parison relations. Surprisingly, polarity was actu-
ally one of the worst classes of features for Com-
parison, achieving an f-score of 16.33 (in contrast
to using the first, last and first three words of the
sentences as features, which leads to an f-score of
21.01). We examined the prevalence of positive-
negative or negative-positive polarity pairs in our
training set. 30% of the Comparison examples
contain one of these opposite polarity pairs, while
31% of the Other examples contain an opposite
polarity pair. To our knowledge, this is the first
study to examine the prevalence of polarity words
in the arguments of discourse relations in their
natural distributions. Contrary to popular belief,
Comparisons do not tend to have more opposite
polarity pairs.
The two most useful classes of features for rec-
ognizing Comparison relations were the first, last
and first three words in the sentence and the con-
text features that indicate the presence of a para-
graph boundary or of an explicit relation just be-
fore or just after the location of the hypothesized
implicit relation (19.32 f-score).
Contingency The two best features for the Con-
tingency vs. Other distinction were verb informa-
tion (36.59 f-score) and first, last and first three
words in the sentence (36.75 f-score). Context
again was one of the features that led to improve-
ment. This makes sense, as Pitler et al (2008)
found that implicit contingencies are often found
immediately following explicit comparisons.
We were surprised that the polarity features
were helpful for Contingency but not Comparison.
Again we looked at the prevalence of opposite po-
larity pairs. While for Comparison versus Other
there was not a significant difference, for Contin-
gency there are quite a few more opposite polarity
pairs (52%) than for not Contingency (41%).
The language model features were completely
useless for distinguishing contingencies from
other relations.
Expansion As Expansion is the majority class
in the natural distribution, recall is less of a prob-
lem than precision. The features that help achieve
the best f-score are all features that were found to
be useful in identifying other relations.
Polarity tags, Inquirer tags and context were
the best features for identifying expansions with
f-scores around 70%.
Temporal Implicit temporal relations are rela-
tively rare, making up only about 5% of our test
set. Most temporal relations are explicitly marked
with a connective like ?when? or ?after?.
Yet again, the first and last words of the sen-
tence turned out to be useful indicators for tem-
poral relations (15.93 f-score). The importance of
the first and last words for this distinction is clear.
It derives from the fact that temporal implicits of-
ten contain words like ?yesterday? or ?Monday? at
the end of the sentence. Context is the next most
helpful feature for temporal relations.
7.2 Which word pairs help?
For Comparison and Contingency, we analyze the
behavior of word pair features under several differ-
ent settings. Specifically we want to address two
important related questions raised in recent work
by others: (i) is unannotated data from explicits
useful for training models that disambiguate im-
plicit discourse relations and (ii) are explicit and
implicit relations intrinsically different from each
other.
Wordpairs-TextRels is the worst approach. The
best use of word pair features is Wordpairs-
selected. This model gives 4% better absolute f-
score for Comparison and 14% for Contingency
over Wordpairs-TextRels. In this setting the Tex-
tRels data was used to choose the word pair fea-
tures, but the probabilities for each feature were
estimated using the training portion of the PDTB
689
Comp. vs. Other
Wordpairs-TextRels 17.13 (46.62)
Wordpairs-PDTBExpl 19.39 (51.41)
Wordpairs-PDTBImpl 20.96 (42.55)
First-last, first3 (best-non-wp) 21.01 (52.59)
Best-non-wp + Wordpairs-selected 21.88 (56.40)
Wordpairs-selected 21.96 (56.59)
Cont. vs. Other
Wordpairs-TextRels 31.10 (41.83)
Wordpairs-PDTBExpl 37.77 (56.73)
Wordpairs-PDTBImpl 43.79 (61.92)
Polarity, verbs, first-last, first3,
modality, context (best-non-wp)
42.14 (66.64)
Wordpairs-selected 45.60 (67.10)
Best-non-wp + Wordpairs-selected 47.13 (67.30)
Expn. vs. Other
Best-non-wp + wordpairs 62.39 (59.55)
Wordpairs-PDTBImpl 63.84 (60.28)
Polarity, inquirer tags, context (best-
non-wp)
76.42 (63.62)
Temp. vs. Other
First-last, first3 (best-non-wp) 15.93 (61.20)
Wordpairs-PDTBImpl 16.21 (61.98)
Best-non-wp + Wordpairs-PDTBImpl 16.76 (63.49)
Table 3: f-score (accuracy) of various feature sets;
Naive Bayes.
implicit examples.
We also confirm that even within the PDTB,
information from annotated explicit relations
(Wordpairs-PDTBExpl) is not as helpful as
information from annotated implicit relations
(Wordpairs-PDTBImpl). The absolute difference
in f-score between the two models is close to 2%
for Comparison, and 6% for Contingency.
7.3 Best results
Adding other features to word pairs leads to im-
proved performance for Contingency, Expansion
and Temporal relations, but not for Comparison.
For contingency detection, the best combina-
tion of our features included polarity, verb in-
formation, first and last words, modality, context
with Wordpairs-selected. This combination led
to a definite improvement, reaching an f-score of
47.13 (16% absolute improvement in f-score over
Wordpairs-TextRels).
For detecting expansions, the best combination
of our features (polarity+Inquirer tags+context)
outperformed Wordpairs-PDTBImpl by a wide
margin, close to 13% absolute improvement (f-
scores of 76.42 and 63.84 respectively).
7.4 Sequence Model of Discourse Relations
Our results from the previous section show that
classification of implicits benefits from informa-
tion about nearby relations, and so we expected
improvements using a sequence model, rather than
classifying each relation independently.
We trained a CRF classifier (Lafferty et al,
2001) over the sequence of implicit examples from
all documents in sections 02 to 20. The test set
is the same as used for the 2-way classifiers. We
compare against a 6-way4 Naive Bayes classifier.
Only word pairs were used as features for both.
Overall 6-way prediction accuracy is 43.27% for
the Naive Bayes model and 44.58% for the CRF
model.
8 Conclusion
We have presented the first study that predicts im-
plicit discourse relations in a realistic setting (dis-
tinguishing a relation of interest from all others,
where the relations occur in their natural distri-
butions). Also unlike prior work, we separate the
task from the easier task of explicit discourse pre-
diction. Our experiments demonstrate that fea-
tures developed to capture word polarity, verb
classes and orientation, as well as some lexical
features are strong indicators of the type of dis-
course relation.
We analyze word pair features used in prior
work that were intended to capture such semantic
oppositions. We show that the features in fact do
not capture semantic relation but rather give infor-
mation about function word co-occurrences. How-
ever, they are still a useful source of information
for discourse relation prediction. The most bene-
ficial application of such features is when they are
selected from a large unannotated corpus of ex-
plicit relations, but then trained on manually an-
notated implicit relations.
Context, in terms of paragraph boundaries and
nearby explicit relations, also proves to be useful
for the prediction of implicit discourse relations.
It is helpful when added as a feature in a standard,
instance-by-instance learning model. A sequence
model also leads to over 1% absolute improvement
for the task.
9 Acknowledgments
This work was partially supported by NSF grants
IIS-0803159, IIS-0705671 and IGERT 0504487.
We would like to thank Sasha Blair-Goldensohn
for providing us with the TextRels data and for
the insightful discussion in the early stages of our
work.
4the four main relations, EntRel, NoRel
690
References
N. Asher and A. Lascarides. 2003. Logics of conver-
sation. Cambridge University Press.
S. Blair-Goldensohn, K.R. McKeown, and O.C. Ram-
bow. 2007. Building and Refining Rhetorical-
Semantic Relation Models. In Proceedings of
NAACL HLT, pages 428?435.
L. Carlson, D. Marcu, and M.E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. In Proceedings
of the Second SIGdial Workshop on Discourse and
Dialogue, pages 1?10.
B.J. Dorr. 2001. LCS Verb Database. Technical Re-
port Online Software Database, University of Mary-
land, College Park, MD.
Y. Freund and R.E. Schapire. 1996. Experiments with
a New Boosting Algorithm. In Machine Learning:
Proceedings of the Thirteenth International Confer-
ence, pages 148?156.
R. Girju. 2003. Automatic detection of causal relations
for Question Answering. In Proceedings of the ACL
2003 workshop on Multilingual summarization and
question answering-Volume 12, pages 76?83.
D. Graff. 2003. English gigaword corpus. Corpus
number LDC2003T05, Linguistic Data Consortium,
Philadelphia.
J. Hobbs. 1979. Coherence and coreference. Cogni-
tive Science, 3:67?90.
A. Knott and T. Sanders. 1998. The classification of
coherence relations and their linguistic markers: An
exploration of two languages. Journal of Pragmat-
ics, 30(2):135?175.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Interna-
tional Conference on Machine Learning 2001, pages
282?289.
M. Lapata and A. Lascarides. 2004. Inferring
sentence-internal temporal relations. In HLT-
NAACL 2004: Main Proceedings.
B. Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. Chicago, IL.
W.C. Mann and S.A. Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. Text, 8.
D. Marcu and A. Echihabi. 2001. An unsupervised
approach to recognizing discourse relations. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 368?375.
D. Marcu. 2000. The Theory and Practice of Dis-
course and Summarization. The MIT Press.
K. McKeown. 1985. Text Generation: Using Dis-
course strategies and Focus Constraints to Gener-
ate Natural Language Text. Cambridge University
Press, Cambridge, England.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING08), short paper.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In HLT-NAACL.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
P.J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Sauri. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 347?354.
F. Wolf and E. Gibson. 2005. Representing discourse
coherence: A corpus-based study. Computational
Linguistics, 31(2):249?288.
691
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13?16,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Using Syntax to Disambiguate Explicit Discourse Connectives in Text
?
Emily Pitler and Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler,nenkova@seas.upenn.edu
Abstract
Discourse connectives are words or
phrases such as once, since, and on
the contrary that explicitly signal the
presence of a discourse relation. There
are two types of ambiguity that need to
be resolved during discourse processing.
First, a word can be ambiguous between
discourse or non-discourse usage. For
example, once can be either a temporal
discourse connective or a simply a word
meaning ?formerly?. Secondly, some
connectives are ambiguous in terms of the
relation they mark. For example since
can serve as either a temporal or causal
connective. We demonstrate that syntactic
features improve performance in both
disambiguation tasks. We report state-of-
the-art results for identifying discourse
vs. non-discourse usage and human-level
performance on sense disambiguation.
1 Introduction
Discourse connectives are often used to explicitly
mark the presence of a discourse relation between
two textual units. Some connectives are largely
unambiguous, such as although and additionally,
which are almost always used as discourse con-
nectives and the relations they signal are unam-
biguously identified as comparison and expansion,
respectively. However, not all words and phrases
that can serve as discourse connectives have these
desirable properties.
Some linguistic expressions are ambiguous be-
tween DISCOURSE AND NON-DISCOURSE US-
AGE. Consider for example the following sen-
tences containing and and once.
?
This work was partially supported by NSF grants IIS-
0803159, IIS-0705671 and IGERT 0504487.
(1a) Selling picked up as previous buyers bailed out of their
positions and aggressive short sellers? anticipating fur-
ther declines?moved in.
(1b) My favorite colors are blue and green.
(2a) The asbestos fiber, crocidolite, is unusually resilient
once it enters the lungs, with even brief exposures to
it causing symptoms that show up decades later, re-
searchers said.
(2b) A form of asbestos once used to make Kent cigarette
filters has caused a high percentage of cancer deaths
among a group of workers exposed to it more than 30
years ago, researchers reported.
In sentence (1a), and is a discourse connec-
tive between the two clauses linked by an elabo-
ration/expansion relation; in sentence (1b), the oc-
currence of and is non-discourse. Similarly in sen-
tence (2a), once is a discourse connective marking
the temporal relation between the clauses ?The as-
bestos fiber, crocidolite is unusually resilient? and
?it enters the lungs?. In contrast, in sentence (2b),
once occurs with a non-discourse sense, meaning
?formerly? and modifying ?used?.
The only comprehensive study of discourse vs.
non-discourse usage in written text
1
was done in
the context of developing a complete discourse
parser for unrestricted text using surface features
(Marcu, 2000). Based on the findings from a
corpus study, Marcu?s parser ?ignored both cue
phrases that had a sentential role in a majority of
the instances in the corpus and those that were
too ambiguous to be explored in the context of a
surface-based approach?.
The other ambiguity that arises during dis-
course processing involves DISCOURSE RELA-
TION SENSE. The discourse connective since for
1
The discourse vs. non-discourse usage ambiguity is even
more problematic in spoken dialogues because there the num-
ber of potential discourse markers is greater than that in writ-
ten text, including common words such as now, well and
okay. Prosodic and acoustic features are the most powerful
indicators of discourse vs. non-discourse usage in that genre
(Hirschberg and Litman, 1993; Gravano et al, 2007)
13
instance can signal either a temporal or a causal
relation as shown in the following examples from
Miltsakaki et al (2005):
(3a) There have been more than 100 mergers and acquisi-
tions within the European paper industry since the most
recent wave of friendly takeovers was completed in the
U.S. in 1986.
(3b) It was a far safer deal for lenders since NWA had a
healthier cash flow and more collateral on hand.
Most prior work on relation sense identifica-
tion reports results obtained on data consisting of
both explicit and implicit relations (Wellner et al,
2006; Soricut and Marcu, 2003). Implicit relations
are those inferred by the reader in the absence of
a discourse connective and so are hard to identify
automatically. Explicit relations are much easier
(Pitler et al, 2008).
In this paper, we explore the predictive power of
syntactic features for both the discourse vs. non-
discourse usage (Section 3) and discourse relation
sense (Section 4) prediction tasks for explicit con-
nectives in written text. For both tasks we report
high classification accuracies close to 95%.
2 Corpus and features
2.1 Penn Discourse Treebank
In our work we use the Penn Discourse Treebank
(PDTB) (Prasad et al, 2008), the largest public
resource containing discourse annotations. The
corpus contains annotations of 18,459 instances
of 100 explicit discourse connectives. Each dis-
course connective is assigned a sense from a three-
level hierarchy of senses. In our experiments
we consider only the top level categories: Ex-
pansion (one clause is elaborating information in
the other), Comparison (information in the two
clauses is compared or contrasted), Contingency
(one clause expresses the cause of the other), and
Temporal (information in two clauses are related
because of their timing). These top-level discourse
relation senses are general enough to be annotated
with high inter-annotator agreement and are com-
mon to most theories of discourse.
2.2 Syntactic features
Syntactic features have been extensively used
for tasks such as argument identification: di-
viding sentences into elementary discourse units
among which discourse relations hold (Soricut
and Marcu, 2003; Wellner and Pustejovsky, 2007;
Fisher and Roark, 2007; Elwell and Baldridge,
2008). Syntax has not been used for discourse vs.
non-discourse disambiguation, but it is clear from
the examples above that discourse connectives ap-
pear in specific syntactic contexts.
The syntactic features we used were extracted
from the gold standard Penn Treebank (Marcus et
al., 1994) parses of the PDTB articles:
Self Category The highest node in the tree
which dominates the words in the connective but
nothing else. For single word connectives, this
might correspond to the POS tag of the word, how-
ever for multi-word connectives it will not. For
example, the cue phrase in addition is parsed as
(PP (IN In) (NP (NN addition) )). While the POS
tags of ?in? and ?addition? are preposition and
noun, respectively, together the Self Category of
the phrase is prepositional phrase.
Parent Category The category of the immedi-
ate parent of the Self Category. This feature is
especially helpful for disambiguating cases simi-
lar to example (1b) above in which the parent of
and would be an NP (the noun phrase ?blue and
green?), which will rarely be the case when and
has a discourse function.
Left Sibling Category The syntactic category
of the sibling immediately to the left of the Self
Category. If the left sibling does not exist, this fea-
tures takes the value ?NONE?. Note that having no
left sibling implies that the connective is the first
substring inside its Parent Category. In example
(1a), this feature would be ?NONE?, while in ex-
ample (1b), the left sibling of and is ?NP?.
Right Sibling Category The syntactic category
of the sibling immediately to the right of the Self
Category. English is a right-branching language,
and so dependents tend to occur after their heads.
Thus, the right sibling is particularly important as
it is often the dependent of the potential discourse
connective under investigation. If the connective
string has a discourse function, then this depen-
dent will often be a clause (SBAR). For example,
the discourse usage in ?After I went to the store,
I went home? can be distinguished from the non-
discourse usage in ?After May, I will go on vaca-
tion? based on the categories of their right siblings.
Just knowing the syntactic category of the right
sibling is sometimes not enough; experiments on
the development set showed improvements by in-
cluding more features about the right sibling.
Consider the example below:
(4) NASA won?t attempt a rescue; instead, it will try to pre-
dict whether any of the rubble will smash to the ground
14
and where.
The syntactic category of ?where? is SBAR, so the
set of features above could not distinguish the sin-
gle word ?where? from a full embedded clause
like ?I went to the store?. In order to address
this deficiency, we include two additional features
about the contents of the right sibling, Right Sib-
ling Contains a VP and Right Sibling Contains
a Trace.
3 Discourse vs. non-discourse usage
Of the 100 connectives annotated in the PDTB,
only 11 appear as a discourse connective more
than 90% of the time: although, in turn, af-
terward, consequently, additionally, alternatively,
whereas, on the contrary, if and when, lest, and on
the one hand...on the other hand. There is quite
a range among the most frequent connectives: al-
though appears as a discourse connective 91.4% of
the time, while or only serves a discourse function
2.8% of the times it appears.
For training and testing, we used explicit dis-
course connectives annotated in the PDTB as pos-
itive examples and occurrences of the same strings
in the PDTB texts that were not annotated as ex-
plicit connectives as negative examples.
Sections 0 and 1 of the PDTB were used for de-
velopment of the features described in the previous
section. Here we report results using a maximum
entropy classifier
2
using ten-fold cross-validation
over sections 2-22.
The results are shown in Table 3. Using the
string of the connective as the only feature sets
a reasonably high baseline, with an f-score of
75.33% and an accuracy of 85.86%. Interest-
ingly, using only the syntactic features, ignoring
the identity of the connective, is even better, re-
sulting in an f-score of 88.19% and accuracy of
92.25%. Using both the connective and syntactic
features is better than either individually, with an
f-score of 92.28% and accuracy of 95.04%.
We also experimented with combinations of
features. It is possible that different con-
nectives have different syntactic contexts for
discourse usage. Including pair-wise interac-
tion features between the connective and each
syntactic feature (features like connective=also-
RightSibling=SBAR) raised the f-score about
1.5%, to 93.63%. Adding interaction terms be-
tween pairs of syntactic features raises the f-score
2
http://mallet.cs.umass.edu
Features Accuracy f-score
(1) Connective Only 85.86 75.33
(2) Syntax Only 92.25 88.19
(3) Connective+Syntax 95.04 92.28
(3)+Conn-Syn Interaction 95.99 93.63
(3)+Conn-Syn+Syn-Syn Interaction 96.26 94.19
Table 1: Discourse versus Non-discourse Usage
slightly more, to 94.19%. These results amount
to a 10% absolute improvement over those ob-
tained by Marcu (2000) in his corpus-based ap-
proach which achieves an f-score of 84.9%
3
for
identifying discourse connectives in text. While
bearing in mind that the evaluations were done on
different corpora and so are not directly compara-
ble, as well as that our results would likely drop
slightly if an automatic parser was used instead of
the gold-standard parses, syntactic features prove
highly beneficial for discourse vs. non-discourse
usage prediction, as expected.
4 Sense classification
While most connectives almost always occur with
just one of the senses (for example, because is al-
most always a Contingency), a few are quite am-
biguous. For example since is often a Temporal
relation, but also often indicates Contingency.
After developing syntactic features for the dis-
course versus non-discourse usage task, we inves-
tigated whether these same features would be use-
ful for sense disambiguation.
Experiments and results We do classification be-
tween the four senses for each explicit relation
and report results on ten-fold cross-validation over
sections 2-22 of the PDTB using a Naive Bayes
classifier
4
.
Annotators were allowed to provide two senses
for a given connective; in these cases, we consider
either sense to be correct
5
. Contingency and Tem-
poral are the senses most often annotated together.
The connectives most often doubly annotated in
the PDTB are when (205/989), and (183/2999),
and as (180/743).
Results are shown in Table 4. The sense clas-
sification accuracy using just the connective is al-
ready quite high, 93.67%. Incorporating the syn-
tactic features raises performance to 94.15% accu-
3
From the reported precision of 89.5% and recall of
80.8%
4
We also ran a MaxEnt classifier and achieved quite sim-
ilar but slightly lower results.
5
Counting only the first sense as correct leads to about 1%
lower accuracy.
15
Features Accuracy
Connective Only 93.67
Connective+Syntax+Conn-Syn 94.15
Interannotator agreement 94
on sense class (Prasad et al, 2008)
Table 2: Four-way sense classification of explicits
racy. While the improvement is not huge, note that
we seem to be approaching a performance ceiling.
The human inter-annotator agreement on the top
level sense class was also 94%, suggesting further
improvements may not be possible. We provide
some examples to give a sense of the type of er-
rors that still occur.
Error Analysis While Temporal relations are the
least frequent of the four senses, making up only
19% of the explicit relations, more than half of
the errors involve the Temporal class. By far
the most commonly confused pairing was Contin-
gency relations being classified as Temporal rela-
tions, making up 29% of our errors.
A random example of each of the most common
types of errors is given below.
(5) Builders get away with using sand and financiers junk
[when] society decides it?s okay, necessary even, to
look the other way. Predicted: Temporal Correct:
Contingency
(6) You get a rain at the wrong time [and] the crop is ruined.
Predicted: Expansion Correct: Contingency
(7) In the nine months, imports rose 20% to 155.039 trillion
lire [and] exports grew 18% to 140.106 trillion lire.
Predicted: Expansion Correct: Comparison
(8) [The biotechnology concern said] Spanish authorities
must still clear the price for the treatment [but] that
it expects to receive such approval by year end. Pre-
dicted: Comparison Correct: Expansion
Examples (6) and (7) show the relatively rare
scenario when and does not signal expansion, and
Example (8) shows but indicating a sense besides
comparison. In these cases where the connective
itself is not helpful in classifying the sense of the
relation, it may be useful to incorporate features
that were developed for classifying implicit rela-
tions (Sporleder and Lascarides, 2008).
5 Conclusion
We have shown that using a few syntactic features
leads to state-of-the-art accuracy for discourse vs.
non-discourse usage classification. Including syn-
tactic features also helps sense class identification,
and we have already attained results at the level of
human annotator agreement. These results taken
together show that explicit discourse connectives
can be identified automatically with high accuracy.
References
R. Elwell and J. Baldridge. 2008. Discourse connec-
tive argument identification with connective specific
rankers. In Proceedings of the International Confer-
ence on Semantic Computing, Santa Clara, CA.
S. Fisher and B. Roark. 2007. The utility of parse-
derived features for automatic discourse segmenta-
tion. In Proceedings of ACL, pages 488?495.
A. Gravano, S. Benus, H. Chavez, J. Hirschberg, and
L. Wilcox. 2007. On the role of context and prosody
in the interpretation of ?okay?. In Proceedings of
ACL, pages 800?807.
J. Hirschberg and D. Litman. 1993. Empirical stud-
ies on the disambiguation of cue phrases. Computa-
tional linguistics, 19(3):501?530.
D. Marcu. 2000. The rhetorical parsing of unrestricted
texts: A surface-based approach. Computational
Linguistics, 26(3):395?448.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19(2):313?330.
E. Miltsakaki, N. Dinesh, R. Prasad, A. Joshi, and
B. Webber. 2005. Experiments on sense annota-
tion and sense disambiguation of discourse connec-
tives. In Proceedings of the Fourth Workshop on
Treebanks and Linguistic Theories (TLT 2005).
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. In COLING, short paper.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In Proceedings of
LREC?08.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In HLT-NAACL.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
B. Wellner and J. Pustejovsky. 2007. Automatically
identifying the arguments of discourse connectives.
In Proceedings of EMNLP-CoNLL, pages 92?101.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Sauri. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue.
16
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 886?894,
Beijing, August 2010
Using Web-scale N-grams to Improve Base NP Parsing Performance
Emily Pitler
Computer and Information Science
University of Pennsylvania
epitler@seas.upenn.edu
Shane Bergsma
Department of Computing Science
University of Alberta
sbergsma@ualberta.ca
Dekang Lin
Google, Inc.
lindek@google.com
Kenneth Church
Human Language Technology Center of Excellence
Johns Hopkins University
kenneth.church@jhu.edu
Abstract
We use web-scale N-grams in a base NP
parser that correctly analyzes 95.4% of the
base NPs in natural text. Web-scale data
improves performance. That is, there is no
data like more data. Performance scales
log-linearly with the number of parame-
ters in the model (the number of unique
N-grams). The web-scale N-grams are
particularly helpful in harder cases, such
as NPs that contain conjunctions.
1 Introduction
Noun phrases (NPs) provide an index to the
world?s information. About 70% of web queries
are NPs (Barr et al, 2008). A robust NP parser
could help search engines improve retrieval per-
formance on multi-word NP queries (Zhai, 1997).
For example, by knowing the correct parse of
?washed (baby carrots),? a search engine could
ensure that returned pages (and advertisements)
concern clean carrots rather than clean babies. NP
structure is also helpful for query expansion and
substitution (Jones et al, 2006).
This paper is concerned with base NP pars-
ing. We are given a base NP string as input,
and the task is to produce a parse tree as output.
Base NPs are NPs that do not contain embedded
noun phrases. These are sometimes called NP
chunks, or core/non-recursive NPs (Church, 1988;
Ramshaw and Marcus, 1995). Correctly parsing
(or, equivalently, bracketing) base NPs is chal-
lenging because the same part-of-speech (POS)
sequence can be parsed differently depending on
the specific words involved. For example, ?retired
(science teacher)? and ?(social science) teacher?
have different structures even though they have
identical POS sequences.
Lexical statistics are therefore needed in order
to parse the above examples, and they must be
computed over a lot of text to avoid sparsity. All
of our lexical statistics are derived from a new
and improved web-scale N-gram corpus (Lin et
al., 2010), which we call Google V2.
Despite the importance of base NPs, most
sentence parsers do not parse base NPs, since
the main training corpus for parsers, the Penn
Treebank (PTB) (Marcus et al, 1994), leaves a
flat structure for base NPs. Recent annotations
by Vadas and Curran (2007a) added NP structure
to the PTB. We use these annotations (described
in Section 3) for our experiments.
NP parsers usually focus on bracketing three-
word noun compounds. Parsing three-word noun
compounds is a fairly artificial task; we show that
sequences of three nouns make up less than 1%
of the three-word-or-longer base NPs in natural
text. As the NP length increases, the number of
possible binary trees (parses) increases with the
Catalan numbers (Church and Patil, 1982). NPs of
length three have just two possible parses (chance
is 50%), while NPs of length six already have
forty-two possible parses (chance is 2%). Long
NPs therefore provide much more opportunity to
improve performance over the baseline. In Table
1 (Section 7), we show the distribution of base NP
length in the PTB. While most NPs are of length
three, NP length has a long tail.
886
The three-word noun compound assumption
also restricts research to the case in which all
words are nouns, while base NPs also contain de-
terminers, possessives, adjectives, and conjunc-
tions. Conjunctions and their scopes are particu-
larly challenging. For example, in the NP, ?French
television and movie producers,? a parser should
conjoin ?(television) and (movie),? as opposed to
?(French television) and (movie),? ?(French tele-
vision) and (movie producers)? or ?(television)
and (movie producers).?
To resolve these issues, we train a classifier
which uses contextual information from the entire
NP and lexical statistics derived from the web-
scale N-gram corpus to predict if a given span
is a constituent. Our parser then uses this clas-
sifier to produce a score for every possible NP-
internal bracketing and creates a chart of bracket-
ing scores. This chart can be used as features in a
full sentence parser or parsed directly with a chart
parser. Our parses are highly accurate, creating a
strong new standard for this task.
Finally, we present experiments that investigate
the effects of N-gram frequency cutoffs and vari-
ous sources of N-gram data. We show an interest-
ing relationship between accuracy and the number
of unique N-gram types in the data.
2 Related Work
2.1 Three-Word Noun Compounds
The most commonly used data for NP parsing is
from Lauer (1995), who extracted 244 three-word
noun compounds from the Grolier encyclopedia.
When there are only three words, this task reduces
to a binary decision:
? Left Branching: * [retired science] teacher
? Right Branching: retired [science teacher]
In Lauer (1995)?s set of noun compounds, two-
thirds are left branching.
The main approach to these three-word noun
compounds has been to compute association
statistics between pairs of words and then choose
the bracketing that corresponds to the more highly
associated pair. The two main models are the
adjacency model (Marcus, 1980; Liberman and
Sproat, 1992; Pustejovsky et al, 1993; Resnik,
1993) and the dependency model (Lauer, 1995).
Under the adjacency model, the bracketing deci-
sion is made by comparing the associations be-
tween words one and two versus words two and
three (i.e. comparing retired science versus sci-
ence teacher). In contrast, the dependency model
compares the associations between one and two
versus one and three (retired science versus retired
teacher). Lauer (1995) compares the two models
and finds the dependency model to be more accu-
rate.
Nakov and Hearst (2005) compute the associ-
ation scores using frequencies, conditional proba-
bilities, ?2, and mutual information, for both pairs
of words and for linguistically-motivated para-
phrases. Lapata and Keller (2005) found that us-
ing web-scale data for associations is better than
using the (smaller) 100M-word British National
Corpus.
2.2 Longer NPs
Focusing on only the three word case misses a
large opportunity for base NP parsing. NPs longer
than three words commonly occur, making up
29% of our test set. In addition, a chance baseline
does exponentially worse as the length of the NP
increases. These longer NPs are therefore a major
opportunity to improve overall base NP parsing.
Since in the general case, NP parsing can no
longer be thought of as a single binary classifica-
tion problem, different strategies are required.
Barker (1998) reduces the task of parsing
longer NPs to making sequential three-word de-
cisions, moving a sliding window along the NP.
The window is first moved from right-to-left, in-
serting right bracketings, and then again from left-
to-right, finalizing left bracketings. While Barker
(1998) assumes that these three-word decisions
can be made in isolation, this is not always valid.1
Vadas and Curran (2007b) employ Barker?s algo-
rithm, but use a supervised classifier to make the
sequential bracketing decisions. Because these
approaches rely on a sequence of binary decisions,
1E.g., although the right-most three words are identical
in 1) ?soap opera stars and television producers,? and 2)
?movie and television producers,? the initial right-bracketing
decision for ?and television producers? should be different
in each.
887
early mistakes can cascade and lead to a chain of
incorrect bracketings.
Our approach differs from previous work in NP
parsing; rather than greedily inserting brackets as
in Barker?s algorithm, we use dynamic program-
ming to find the global maximum-scoring parse.
In addition, unlike previous approaches that have
used local features to make local decisions, we use
the full NP to score each potential bracketing.
A related line of research aims to segment
longer phrases that are queried on Internet search
engines (Bergsma and Wang, 2007; Guo et al,
2008; Tan and Peng, 2008). Bergsma and Wang
(2007) focus on NP queries of length four or
greater. They use supervised learning to make
segmentation decisions, with features derived
from the noun compound bracketing literature.
Evaluating the benefits of parsing NP queries,
rather than simply segmenting them, is a natural
application of our system.
3 Annotated Data
Our training and testing data are derived from re-
cent annotations by Vadas and Curran (2007a).
The original PTB left a flat structure for base noun
phrases. For example, ?retired science teacher,?
would be represented as:
(NP (JJ retired) (NN science) (NN teacher))
Vadas and Curran (2007a) annotated NP-internal
structure by adding annotations whenever there is
a left-bracketing. If no annotations were added,
right-branching is assumed. The inter-annotator
agreement for exactly matching the brackets on an
NP was 98.5%.
This data provides a valuable new resource for
parsing research, but little work has so far made
use of it. Vadas and Curran (2007b) perform
some preliminary experiments on NP bracketing,
but use gold standard part-of-speech and named-
entity annotations as features in their classifier.
Our work establishes a strong and realistic stan-
dard on this data; our results will serve as a basis
for further research on this topic.
4 Unlabeled N-gram Data
All of our N-gram features described in Sec-
tion 6.1 rely on probabilities derived from unla-
beled data. To use the largest amount of data
possible, we exploit web-scale N-gram corpora.
N-gram counts are an efficient way to compress
large amounts of data (such as all the text on the
web) into a manageable size. An N-gram corpus
records how often each unique sequence of words
occurs. Co-occurrence probabilities can be calcu-
lated directly from the N-gram counts. To keep
the size manageable, N-grams that occur with a
frequency below a particular threshold can be fil-
tered.
The corpus we use is Google V2 (Lin et al,
2010): a new N-gram corpus with N-grams of
length 1-5 that we created from the same 1 tril-
lion word snapshot of the web as Google N-grams
Version 1 (Brants and Franz, 2006), but with sev-
eral enhancements. Duplicate sentences are re-
moved, as well as ?sentences? which are probably
noise (indicated by having a large proportion of
non-alphanumeric characters, being very long, or
being very short). Removing duplicate sentences
is especially important because automatically-
generated websites, boilerplate text, and legal dis-
claimers skew the source web data, with sentences
that may have only been authored once occurring
millions of times. We use the suffix array tools
described in Lin et al (2010) to quickly extract
N-gram counts.
5 Base NP Parsing Approach
Our goal is to take a base NP string as input and
produce a parse tree as output. In practice, it
would be most useful if the NP parse could be
integrated into a sentence parser. Previous NP
parsers are difficult to apply in practice.2 Work
in prepositional phrase attachment that assumes
gold-standard knowledge of the competing attach-
ment sites has been criticized as unrealistic (At-
terer and Schu?tze, 2007).
Our system can easily be integrated into full
parsers. Its input can be identified quickly and
reliably and its output is compatible with down-
stream parsers.
2For example, Vadas and Curran (2007b) report results on
NP parsing, but these results include NPs containing preposi-
tional or adverbial phrases (confirmed by personal communi-
cation). Practical application of their system would therefore
require resolving prepositional phrase attachment as a pre-
processing step.
888
Our parser?s input is base NPs, which can be
identified with very high accuracy. Kudo and Mat-
sumoto (2001) report 95.8% NP chunking accu-
racy on PTB data.
Once provided with an NP, our system uses a
supervised classifier to predict the probability of
a particular contiguous subsequence (span) of the
NP being a constituent, given the entire NP as con-
text. This probability can be inserted into the chart
that a standard chart parser would use.
For example, the base NP ?French television
and movie producers? would be decomposed into
nine different classification problems, scoring the
following potential bracketings:
(French television) and movie producers
French (television and) movie producers
(French television and) movie producers ...
French television and (movie producers)
In Section 6, we detail the set of statistical and
structural features used by the classifier.
The output of our classifier can be easily used
as a feature in a full-sentence structured prediction
parser, as in Taskar et al (2004). Alternatively,
our work could be integrated into a full-sentence
parser by using our feature representations di-
rectly in a discriminative CFG parser (Finkel et
al., 2008), or in a parse re-ranker (Ratnaparkhi et
al., 1994; Collins and Koo, 2005; Charniak and
Johnson, 2005).
While our main objective is to use web-scale
lexical statistics to create an accurate classifier for
base NP-internal constituents, we do produce a
parse tree for evaluation purposes. The probabil-
ity of a parse tree is defined as the product of the
probabilities of all the spans (constituents) in the
tree. The most probable tree is computed with the
CYK algorithm.
6 Features
Over the course of development experiments, we
discovered that the more position-specific our fea-
tures were, the more effectively we could parse
NPs. We define a word?s position as its distance
from the right of the full NP, as the semantic head
of NPs is most often the right-most word. Ulti-
mately, we decided to conjoin each feature with
the position of the proposed bracketing. Since
the features for differing proposed bracketings are
now disjoint, this is equivalent to scoring bracket-
ings with different classifiers, with each classifier
chosen according to the bracketing position. We
now outline the feature types that are common,
but weighted differently, in each proposed brack-
eting?s feature set.
6.1 N-gram Features
All of the features described in this section require
estimates of the probability of specific words or
sequences of words. All probabilities are com-
puted using Google V2 (Section 4).
6.1.1 PMI
Recall that the adjacency model for the three-
word task uses the associations of the two pairs of
adjacent words, while the dependency model uses
the associations of the two pairs of attachment
sites for the initial noun. We generalize the ad-
jacency and dependency models by including the
pointwise mutual information (Church and Hanks,
1990) between all pairs of words in the NP:
PMI(x, y) = log p(?x y?)p(?x?)p(?y?) (1)
For NPs of length n, for each proposed bracket-
ing, we include separate features for the PMI be-
tween all
(n
2
)
pairs of words in the NP. For NPs in-
cluding conjunctions, we include additional PMI
features (Section 6.1.2).
Since these features are also tied to the pro-
posed bracketing positions (as explained above),
this allows us to learn relationships between var-
ious associations within the NP and each poten-
tial bracketing. For example, consider a proposed
bracketing from word 4 to word 5. We learn that
a high association of words inside a bracketing
(here, a high association between word 4 and word
5) indicates a bracketing is likely, while a high
association between words that cross a proposed
bracketing (e.g., a high association between word
3 and word 4) indicates the bracketing is unlikely.
The value of these features is the PMI, if it is
defined. If the PMI is undefined, we include one
of two binary features:
p(?x y?) = 0 or p(?x?) ? p(?y?) = 0.
889
We illustrate the PMI features with an example.
In deciding whether (movie producers) is a rea-
sonable bracketing within ?French television and
movie producers,? the classifier weighs features
for all of:
PMI(French, television)
PMI(French, and)
. . .
PMI(television, producers)
PMI(and, producers)
PMI(movie, producers)
6.1.2 Conjunctions
Properly handling NPs containing conjunc-
tions (NP+conj) requires special statistical fea-
tures. For example, television and movie are
commonly conjoined, but the relevant statistics
that suggest placing brackets around the phrase
?television and movie? are not provided by the
above PMI features (i.e., this is not clear from
PMI(television, and), PMI(television, movie), nor
PMI(and, movie)). Rather, we want to know if the
full phrase ?television and movie? is common.
We thus have additional NP+conj features that
consider the PMI association across the word and:
PMIand(x, y) = log
p(?x and y?)
p(?x and?)p(?and y?) (2)
When PMIand between a pair of words is high,
they are likely to be the constituents of a conjunc-
tion.
Let NP=(w1 . . . wi?1, ?and?, wi+1 . . . wn) be
an NP+conj. We include the PMIand features be-
tween wi?1 and all w ? wi+1 . . . wn. In the exam-
ple ?French television and movie producers,? we
would include features PMIand(television, movie)
and PMIand(television, producers).
In essence, we are assuming wi?1 is the head
of one of the items being conjoined, and we score
the likelihood of each of the words to the right
of the and being the head for the other item. In
our running example, the conjunction has narrow
scope, and PMIand(television, movie) is greater
than PMIand(television, producers), indicating to
our classifier that (television and movie) is a good
bracketing. In other examples the conjunction will
join heads that are further apart, as in ((French TV)
and (British radio)) stars, where both of the fol-
lowing hold:
PMIand(TV, radio) > PMIand(TV, British)
PMIand(TV, radio) > PMIand(TV, stars)
6.2 Lexical
We include a binary feature to indicate the pres-
ence of a particular word at each position in the
NP. We learn that, for instance, the word Inc. in
names tends to occur outside of brackets.
6.3 Shape
Previous work on NP bracketing has used gold-
standard named entity tags (Vadas and Curran,
2007b) as features. We did not want to use any
gold-standard features in our experiments, how-
ever NER information is helpful in separating pre-
modifiers from names, i.e. (news reporter) (Wal-
ter Cronkite).
As an expedient way to get both NER informa-
tion and useful information from hyphenated ad-
jectives, abbreviations, and other punctuation, we
normalize each string using the following regular
expressions:
[A-Z]+ ? A [a-z]+ ? a
We use this normalized string as an indicator
feature. E.g. the word ?Saudi-born? will fire the
binary feature ?Aa-a.?
6.4 Position
We also include the position of the proposed
bracketing as a feature. This represents the prior
of a particular bracketing, regardless of the actual
words.
7 Experiments
7.1 Experimental Details
We use Vadas and Curran (2007a)?s annotations
(Section 3) to create training, development and
testing data for base NPs, using standard splits of
the Penn Treebank (Table 1). We consider all non-
trivial base NPs, i.e., those longer than two words.
For training, we expand each NP in our train-
ing set into independent examples corresponding
to all the possible internal NP-bracketings, and
represent these examples as feature vectors (Sec-
tion 5). Each example is positively labeled if it is
890
Data Set Train Dev Test Chance
PTB Section 2-22 24 23
Length=3 41353 1428 2498 50%
Length=4 12067 445 673 20%
Length=5 3930 148 236 7%
Length=6 1272 34 81 2%
Length>6 616 29 34 < 1%
Total NPs 59238 2084 3522
Table 1: Breakdown of the PTB base NPs used in
our experiments. Chance = 1/Catalan(length).
Features All NPs NP+conj NP-conj
All features 95.4 89.7 95.7
-N-grams 94.0 84.0 94.5
-lexical 92.2 87.4 92.5
-shape 94.9 89.7 95.2
-position 95.3 89.7 95.6
Right 72.6 58.3 73.5bracketing
Table 2: Accuracy (%) of base NPs parsing; abla-
tion of different feature classes.
consistent with the gold-standard bracketing, oth-
erwise it is a negative example.
We train using LIBLINEAR, an efficient linear
Support Vector Machine (SVM).3 We use an L2-
loss function, and optimize the regularization pa-
rameter on the development set (reaching an opti-
mum at C=1). We converted the SVM output to
probabilities.4 Perhaps surprisingly, since SVMs
are not probabilistic, performance on the devel-
opment set with these SVM-derived probabilities
was higher than using probabilities from the LIB-
LINEAR logistic regression solver.
At test time, we again expand the NPs and cal-
culate the probability of each constituent, insert-
ing the score into a chart. We run the CYK algo-
rithm to find the most probable parse of the entire
NP according to the chart. Our evaluation metric
is Accuracy: the proportion of times our proposed
parse of the NP exactly matches the gold standard.
8 Results
8.1 Base NPs
Our method improves substantially over the base-
line of assuming a completely right-branching
structure, 95.4% versus 72.6% (Table 2). The ac-
curacy of the constituency classifier itself (before
the CYK parser is used) is 96.1%.
The lexical features are most important, but all
feature classes are somewhat helpful. In particu-
lar, including N-gram PMI features significantly
improves the accuracy, from 94.0% to 95.4%.5
Correctly parsing more than 19 base NPs out of 20
is an exceptional level of accuracy, and provides a
strong new standard on this task. The most com-
parable result is by Vadas and Curran (2007b),
who achieved 93.0% accuracy on a different set of
PTB noun phrases (see footnote 2), but their clas-
sifier used features based on gold-standard part-
of-speech and named-entity information.
Exact match is a tough metric for parsing, and
the difficulty increases as the length of the NP
increases (because there are more decisions to
make correctly). At three word NPs, our accu-
racy is 98.5%; by six word NPs, our accuracy
drops to 79.0% (Figure 1). Our method?s accu-
racy decreases as the length of the NP increases,
but much less rapidly than a right-bracketing or
chance baseline.
8.2 Base NPs with Conjunctions
N-gram PMI features help more on NP+conj than
on those that do not contain conjunctions (NP-
conj) (Table 2). N-gram PMI features are the most
important features for NP+conj, increasing accu-
racy from 84.0% to 89.7%, a 36% relative reduc-
tion in error.
8.3 Effect of Thresholding N-gram data
We now address two important related questions:
1) how does our parser perform as the amount
of unlabeled auxiliary data varies, and 2) what
is the effect of thresholding an N-gram corpus?
The second question is of widespread relevance as
3www.csie.ntu.edu.tw/
?
cjlin/liblinear/
4Following instructions in http://www.csie.ntu.
edu.tw/
?
cjlin/liblinear/FAQ.html
5McNemar?s test, p < 0.05
891
 1
 10
 100
6543
Ac
cu
ra
cy
 (%
)
Length of Noun Compound (words)
Proposed
Right-bracketing
Chance
Figure 1: Accuracy (log scale) over different NP
lengths, of our method, the right-bracketing base-
line, and chance (1/Catalan(length)).
thresholded N-gram corpora are now widely used
in NLP. Without thresholds, web-scale N-gram
data can be unmanageable.
While we cannot lower the threshold after cre-
ating the N-gram corpus, we can raise it, filtering
more N-grams, and then measure the relationship
between threshold and performance.
Threshold Unique N-grams Accuracy
10 4,145,972,000 95.4%
100 391,344,991 95.3%
1,000 39,368,488 95.2%
10,000 3,924,478 94.8%
100,000 386,639 94.8%
1,000,000 37,567 94.4%
10,000,000 3,317 94.0%
Table 3: There is no data like more data. Accuracy
improves with the number of parameters (unique
N-grams).
We repeat the parsing experiments while in-
cluding in our PMI features only N-grams with
a count ?10 (the whole data set), ?100, ?1000,
. . ., ?107. All other features (lexical, shape, posi-
tion) remain unchanged. The N-gram data almost
perfectly exhibits Zipf?s power law: raising the
threshold by a factor of ten decreases the number
of unique N-grams by a factor of ten (Table 3).
The improvement in accuracy scales log-linearly
with the number of unique N-grams. From a prac-
tical standpoint, we see a trade-off between stor-
Corpus # of tokens ? # of types
NEWS 3.2 B 1 3.7 B
Google V1 1,024.9 B 40 3.4 B
Google V2 207.4 B 10 4.1 B
Table 4: N-gram data, with total number of words
(tokens) in the original corpus (in billions, B), fre-
quency threshold used to filter the data, ? , and to-
tal number of unique N-grams (types) remaining
in the data after thresholding.
age and accuracy. There are consistent improve-
ments in accuracy from lowering the threshold
and increasing the amount of auxiliary data. If for
some application it is necessary to reduce storage
by several orders of magnitude, then one can eas-
ily estimate the resulting impact on performance.
We repeat the thresholding experiments using
two other N-gram sources:
NEWS: N-gram data created from a large set
of news articles including the Reuters and Giga-
word (Graff, 2003) corpora, not thresholded.
Google V1: The original web-scale N-gram
corpus (Section 4).
Details of these sources are given in Table 4.
For a given number of unique N-grams, using
any of the three sources does about the same (Fig-
ure 2). It does not matter that the source corpus
for Google V1 is about five times larger than the
source corpus for Google V2, which in turn is
sixty-five times larger than NEWS (Table 4). Ac-
curacies increase linearly with the log of the num-
ber of types in the auxiliary data set.
Google V1 is the one data source for which
the relationship between accuracy and number of
N-grams is not monotonic. After about 100 mil-
lion unique N-grams, performance starts decreas-
ing. This drop shows the need for Google V2.
Since Google V1 contains duplicated web pages
and sentences, mistakes that should be rare can
appear to be quite frequent. Google V2, which
comes from the same snapshot of the web as
Google V1, but has only unique sentences, does
not show this drop.
We regard the results in Figure 2 as a compan-
ion to Banko and Brill (2001)?s work on expo-
nentially increasing the amount of labeled train-
ing data. Here we see that varying the amount of
892
 94
 94.5
 95
 95.5
 96
1e91e81e71e61e51e4
Ac
cu
ra
cy
 (%
)
Number of Unique N-grams
Google V1
Google V2
NEWS
Figure 2: There is no data like more data. Ac-
curacy improves with the number of parameters
(unique N-grams). This trend holds across three
different sources of N-grams.
unlabeled data can cause an equally predictable
improvement in classification performance, with-
out the cost of labeling data.
Suzuki and Isozaki (2008) also found a log-
linear relationship between unlabeled data (up to
a billion words) and performance on three NLP
tasks. We have shown that this trend continues
well beyond Gigaword-sized corpora. Brants et
al. (2007) also found that more unlabeled data (in
the form of input to a language model) leads to
improvements in BLEU scores for machine trans-
lation.
Adding noun phrase parsing to the list of prob-
lems for which there is a ?bigger is better? rela-
tionship between performance and unlabeled data
shows the wide applicability of this principle. As
both the amount of text on the web and the power
of computer architecture continue to grow expo-
nentially, collecting and exploiting web-scale aux-
iliary data in the form of N-gram corpora should
allow us to achieve gains in performance linear in
time, without any human annotation, research, or
engineering effort.
9 Conclusion
We used web-scale N-grams to produce a new
standard in performance of base NP parsing:
95.4%. The web-scale N-grams substantially im-
prove performance, particularly in long NPs that
include conjunctions. There is no data like more
data. Performance improves log-linearly with the
number of parameters (unique N-grams). One can
increase performance with larger models, e.g., in-
creasing the size of the unlabeled corpora, or by
decreasing the frequency threshold. Alternatively,
one can decrease storage costs with smaller mod-
els, e.g., decreasing the size of the unlabeled cor-
pora, or by increasing the frequency threshold. Ei-
ther way, the log-linear relationship between accu-
racy and model size makes it easy to estimate the
trade-off between performance and storage costs.
Acknowledgments
We gratefully acknowledge the Center for Lan-
guage and Speech Processing at Johns Hopkins
University for hosting the workshop at which this
research was conducted.
References
Atterer, M. and H. Schu?tze. 2007. Prepositional
phrase attachment without oracles. Computational
Linguistics, 33(4):469?476.
Banko, M. and E. Brill. 2001. Scaling to very very
large corpora for natural language disambiguation.
In ACL.
Barker, K. 1998. A trainable bracketer for noun mod-
ifiers. In Twelfth Canadian Conference on Artificial
Intelligence (LNAI 1418).
Barr, C., R. Jones, and M. Regelson. 2008. The lin-
guistic structure of English web-search queries. In
EMNLP.
Bergsma, S. and Q.I. Wang. 2007. Learning noun
phrase query segmentation. In EMNLP-CoNLL.
Brants, T. and A. Franz. 2006. The Google Web 1T
5-gram Corpus Version 1.1. LDC2006T13.
Brants, T., A.C. Popat, P. Xu, F.J. Och, and J. Dean.
2007. Large language models in machine transla-
tion. In EMNLP.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In ACL.
Church, K.W. and P. Hanks. 1990. Word associa-
tion norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Church, K. and R. Patil. 1982. Coping with syntactic
ambiguity or how to put the block in the box on the
table. Computational Linguistics, 8(3-4):139?149.
893
Church, K.W. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In ANLP.
Collins, M. and T. Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational
Linguistics, 31(1):25?70.
Finkel, J.R., A. Kleeman, and C.D. Manning. 2008.
Efficient, feature-based, conditional random field
parsing. In ACL.
Graff, D. 2003. English Gigaword. LDC2003T05.
Guo, J., G. Xu, H. Li, and X. Cheng. 2008. A unified
and discriminative model for query refinement. In
SIGIR.
Jones, R., B. Rey, O. Madani, and W. Greiner. 2006.
Generating query substitutions. In WWW.
Kudo, T. and Y. Matsumoto. 2001. Chunking with
support vector machines. In NAACL.
Lapata, M. and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1):1?31.
Lauer, M. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In ACL.
Liberman, M. and R. Sproat. 1992. The stress and
structure of modified noun phrases in English. Lex-
ical matters, pages 131?181.
Lin, D., K. Church, H. Ji, S. Sekine, D. Yarowsky,
S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,
K. Dalwani, and S. Narsale. 2010. New tools for
web-scale n-grams. In LREC.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Marcus, M.P. 1980. Theory of Syntactic Recogni-
tion for Natural Languages. MIT Press, Cambridge,
MA, USA.
Nakov, P. and M. Hearst. 2005. Search engine statis-
tics beyond the n-gram: Application to noun com-
pound bracketing. In CoNLL.
Pustejovsky, J., P. Anick, and S. Bergler. 1993. Lex-
ical semantic techniques for corpus analysis. Com-
putational Linguistics, 19(2):331?358.
Ramshaw, L.A. and M.P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In 3rd
ACL Workshop on Very Large Corpora.
Ratnaparkhi, A., S. Roukos, and R.T. Ward. 1994.
A maximum entropy model for parsing. In Third
International Conference on Spoken Language Pro-
cessing.
Resnik, P. 1993. Selection and information: a class-
based approach to lexical relationships. Ph.D. the-
sis, University of Pennsylvania.
Suzuki, J. and H. Isozaki. 2008. Semi-supervised se-
quential labeling and segmentation using giga-word
scale unlabeled data. In ACL.
Tan, B. and F. Peng. 2008. Unsupervised query
segmentation using generative language models and
Wikipedia. In WWW.
Taskar, B., D. Klein, M. Collins, D. Koller, and
C. Manning. 2004. Max-margin parsing. In
EMNLP.
Vadas, D. and J.R. Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In ACL.
Vadas, D. and J.R. Curran. 2007b. Large-scale su-
pervised models for noun phrase bracketing. In PA-
CLING.
Zhai, C. 1997. Fast statistical parsing of noun phrases
for document indexing. In ANLP.
894
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 478?488, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Dynamic Programming for Higher Order Parsing of Gap-Minding Trees
Emily Pitler, Sampath Kannan, Mitchell Marcus
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
epitler,kannan,mitch@seas.upenn.edu
Abstract
We introduce gap inheritance, a new struc-
tural property on trees, which provides a way
to quantify the degree to which intervals of de-
scendants can be nested. Based on this prop-
erty, two new classes of trees are derived that
provide a closer approximation to the set of
plausible natural language dependency trees
than some alternative classes of trees: unlike
projective trees, a word can have descendants
in more than one interval; unlike spanning
trees, these intervals cannot be nested in ar-
bitrary ways. The 1-Inherit class of trees has
exactly the same empirical coverage of natural
language sentences as the class of mildly non-
projective trees, yet the optimal scoring tree
can be found in an order of magnitude less
time. Gap-minding trees (the second class)
have the property that all edges into an interval
of descendants come from the same node, and
thus an algorithm which uses only single in-
tervals can produce trees in which a node has
descendants in multiple intervals.
1 Introduction
Dependency parsers vary in what space of possi-
ble tree structures they search over when parsing
a sentence. One commonly used space is the set
of projective trees, in which every node?s descen-
dants form a contiguous interval in the input sen-
tence. Finding the optimal tree in the set of projec-
tive trees can be done efficiently (Eisner, 2000), even
when the score of a tree depends on higher order fac-
tors (McDonald and Pereira, 2006; Carreras, 2007;
Koo and Collins, 2010). However, the projectivity
assumption is too strict for all natural language de-
pendency trees; for example, only 63.6% of Dutch
sentences from the CoNLL-X training set are pro-
jective (Table 1).
At the other end of the spectrum, some parsers
search over all spanning trees, a class of structures
much larger than the set of plausible linguistic struc-
tures. The maximum scoring directed spanning tree
can be found efficiently when the score of a tree de-
pends only on edge-based factors (McDonald et al
2005b). However, it is NP-hard to extend MST to in-
clude sibling or grandparent factors (McDonald and
Pereira, 2006; McDonald and Satta, 2007). MST-
based non-projective parsers that use higher order
factors (Martins et al2009; Koo et al2010), uti-
lize different techniques than the basic MST algo-
rithm. In addition, learning is done over a relaxation
of the problem, so the inference procedures at train-
ing and at test time are not identical.
We propose two new classes of trees between pro-
jective trees and the set of all spanning trees. These
two classes provide a closer approximation to the set
of plausible natural language dependency trees: un-
like projective trees, a word can have descendants in
more than one interval; unlike spanning trees, these
intervals cannot be nested in arbitrary ways. We in-
troduce gap inheritance, a new structural property
on trees, which provides a way to quantify the de-
gree to which these intervals can be nested. Differ-
ent levels of gap inheritance define each of these two
classes (Section 3).
The 1-Inherit class of trees (Section 4) has exactly
the same empirical coverage (Table 1) of natural lan-
guage sentences as the class of mildly non-projective
trees (Bodirsky et al2005), yet the optimal scoring
tree can be found in an order of magnitude less time
(Section 4.1).
Gap-minding trees (the second class) have the
478
property that all edges into an interval of descen-
dants come from the same node. Non-contiguous
intervals are therefore decoupled given this single
node, and thus an algorithm which uses only single
intervals (as in projective parsing) can produce trees
in which a node has descendants in multiple inter-
vals (as in mildly non-projective parsing (Go?mez-
Rodr??guez et al2011)). A procedure for finding
the optimal scoring tree in this space is given in Sec-
tion 5, which can be searched in yet another order of
magnitude faster than the 1-Inherit class.
Unlike the class of spanning trees, it is still
tractable to find the optimal tree in these new spaces
when higher order factors are included. An exten-
sion which finds the optimal scoring gap-minding
tree with scores over pairs of adjacent edges (grand-
parent scoring) is given in Section 6. These gap-
minding algorithms have been implemented in prac-
tice and empirical results are presented in Section 7.
2 Preliminaries
In this section, we review some relevant defini-
tions from previous work that characterize degrees
of non-projectivity. We also review how well
these definitions cover empirical data from six lan-
guages: Arabic, Czech, Danish, Dutch, Portuguese,
and Swedish. These are the six languages whose
CoNLL-X shared task data are either available open
source1 or from the LDC2.
A dependency tree is a rooted, directed spanning
tree that represents a set of dependencies between
words in a sentence.3 The tree has one artificial root
node and vertices that correspond to the words in an
input sentence w1, w2,...,wn. There is an edge from
h to m if m depends on (or modifies) h.
Definition 1. The projection of a node is the set of
words in the subtree rooted at it (including itself).
A tree is projective if, for every node in the tree,
that node?s projection forms a contiguous interval in
the input sentence order.
A tree is non-projective if the above does not hold,
i.e., there exists at least one word whose descendants
1http://ilk.uvt.nl/conll/free_data.html
2LDC catalogue numbers LDC2006E01 and LDC2006E02
3Trees are a reasonable assumption for most, but not all,
linguistic structures. Parasitic gaps are an example in which
a word perhaps should have multiple parents.
do not form a contiguous interval.
Definition 2. A gap of a node v is a non-empty, max-
imal interval that does not contain any words in the
projection of v but lies between words that are in
the projection of v. The gap degree of a node is
the number of gaps it has. The gap degree of a tree
is the maximum of the gap degrees of its vertices.
(Bodirsky et al2005)
Note that a projective tree will have gap degree 0.
Two subtrees interleave if there are vertices l1, r1
from one subtree and l2, r2 from the other such that
l1 < l2 < r1 < r2.
Definition 3. A tree is well-nested if no two disjoint
subtrees interleave (Bodirsky et al2005).
Definition 4. A mildly non-projective tree has gap
degree at most one and is well-nested.
Mildly non-projective trees are of both theoret-
ical and practical interest, as they correspond to
derivations in Lexicalized Tree Adjoining Grammar
(Bodirsky et al2005) and cover the overwhelming
majority of sentences found in treebanks for Czech
and Danish (Kuhlmann and Nivre, 2006).
Table 1 shows the proportion of mildly non-
projective sentences for Arabic, Czech, Danish,
Dutch, Portuguese, and Swedish, ranging from
95.4% of Portuguese sentences to 99.9% of Ara-
bic sentences.4 This definition covers a substan-
tially larger set of sentences than projectivity does
? an assumption of projectivity covers only 63.6%
(Dutch) to 90.2% (Swedish) of examples (Table 1).
3 Gap Inheritance
Empirically, natural language sentences seem to be
mostly mildly non-projective trees, but mildly non-
projective trees are quite expensive to parse (O(n7)
(Go?mez-Rodr??guez et al2011)). The parsing com-
plexity comes from the fact that the definition al-
lows two non-contiguous intervals of a projection to
be tightly coupled, with an unbounded number of
edges passing back and forth between the two inter-
vals; however, this type of structure seems unusual
4While some of the treebank structures are ill-nested or have
a larger gap degree because of annotation decisions, some lin-
guistic constructions in German and Czech are ill-nested or
require at least two gaps under any reasonable representation
(Chen-Main and Joshi, 2010; Chen-Main and Joshi, 2012).
479
Arabic Czech Danish Dutch Portuguese Swedish Parsing
Mildly non-proj 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(n7)
Mild+1-Inherit 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(n6)
Mild+0-Inherit 1394 (95.5) 70695 (97.2) 4985 (96.1) 12068 (90.4) 8481 (93.5) 10787 (97.7) O(n5)
Projective 1297 (88.8) 55872 (76.8) 4379 (84.4) 8484 (63.6) 7353 (81.1) 9963 (90.2) O(n3)
# Sentences 1460 72703 5190 13349 9071 11042
Table 1: The number of sentences from the CoNLL-X training sets whose parse trees fall into each of the above
classes. The two new classes of structures, Mild+0-Inherit and Mild+1-Inherit, have more coverage of empirical data
than projective structures, yet can be parsed faster than mildly non-projective structures. Parsing times assume an edge-
based factorization with no pruning of edges. The corresponding algorithms for Mild+1-Inherit and Mild+0-Inherit
are in Sections 4 and 5.
for natural language. We therefore investigate if we
can define further structural properties that are both
appropriate for describing natural language trees and
which admit more efficient parsing algorithms.
Let us first consider an example of a tree which
both has gap degree at most one and satisfies well-
nestedness, yet appears to be an unrealistic struc-
ture for a natural language syntactic tree. Consider
a tree which is rooted at node xn+2, which has one
child, node xn+1, whose projection is [x1, xn+1] ?
[xn+3, x2n+2], with n children (x1, ..., xn), and each
child xi has a child at x2n?i+3. This tree is well-
nested, has gap degree 1, but all n of xn+1?s children
have edges into the other projection interval.
We introduce a further structural restriction in this
section, and show that trees satisfying our new prop-
erty can be parsed more efficiently with no drop in
empirical coverage.
Definition 5. A child is gap inheriting if its parent
has gap degree 1 and it has descendants on both
sides of its parent?s gap. The inheritance degree of
a node is the number of its children which inherit its
gap. The inheritance degree of a tree is the maximum
inheritance degree over all its nodes.
Figure 1 gives examples of trees with varying de-
grees of gap inheritance. Each projection of a node
with a gap is shown with two matching rectangles. If
a child has a projection rectangle nested inside each
of the parent?s projection rectangles, then that child
inherits the parent?s gap. Figure 1(a) shows a mildly
projective tree (with inheritance degree 2), with both
node 2 and node 11 inheriting their parent (node 3)?s
gap (note that both the dashed and dotted rectangles
each show up inside both of the solid rectangles).
Figure 1(b) shows a tree with inheritance degree 1:
there is now only one pair of rectangles (the dot-
ted ones) which show up in both of the solid ones.
Figure 1(c) shows a tree with inheritance degree 0:
while there are gaps, each set of matching rectangles
is contained within a single rectangle (projection in-
terval) of its parent, i.e., the two dashed rectangles
of node 2?s projection are contained within the left
interval of node 3; the two dotted rectangles of node
12?s projection are contained within the right inter-
val of node 3, etc.
We now ask:
1. How often does gap inheritance occur in the
parses of natural language sentences found in
treebanks?
2. Furthermore, how often are there multiple gap
inheriting children of the same node (inheri-
tance degree at least two)?
Table 1 shows what proportion of mildly non-
projective trees have the added property of gap in-
heritance degree 0 (Mild+0-Inherit) or have gap in-
heritance degree 1 (Mild+1-Inherit). Over all six
languages, there are no examples of multiple gap
inheritance ? Mild+1-Inherit has exactly the same
empirical coverage as the unrestricted set of mildly
non-projective trees.
4 Mild+1-Inherit Trees
There are some reasons from syntactic theory why
we might expect at most one child to inherit its par-
ent?s gap. Traditional Government and Binding the-
ories of syntax (Chomsky, 1981) assume that there
is an underlying projective (phrase structure) tree,
and that gaps primarily arise through movement of
480
6
2 3 41 5 7 11 12 1398 10
(a) Mildly Non-Projective: The projec-
tions (set of descendants) of both node 2
(the dashed red rectangles) and node 11
(dotted magenta) appear in both of node
3?s intervals (the solid blue rectangles).
6
2 3 41 5 7 11 12 1398 10
(b) Mild+1-Inherit: Only node 2 inherits
node 3?s gap: the dashed red rectangles
appear in each of the two solid blue rect-
angles.
6
2 3 41 5 7 11 12 1398 10
(c) Mild+0-Inherit: Even though node 3
has children with gaps (node 2 and node
12), neither of them inherit node 3?s gap.
There are several nodes with gaps, but
every node with a gap is properly con-
tained within just one of its parent?s in-
tervals.
Figure 1: Rectangles that match in color and style indicate the two projection intervals of a node, separated by a gap.
In all three trees, node 3?s two projection intervals are shown in the two solid blue rectangles. The number of children
which inherit its gap vary, however; in 1(a), two children have descendants within both sides; in 1(b) only one child
has descendants on both sides; in 1(c), none of its children do.
subtrees (constituents). One of the fundamental as-
sumptions of syntactic theory is that movement is
upward in the phrase structure tree.5
Consider one movement operation and its effect
on the gap degree of all other nodes in the tree: (a) it
should have no effect on the gap degree of the nodes
in the subtree itself, (b) it can create a gap for an an-
cestor node if it moves out of its projection interval,
and (c) it can create a gap for a non-ancestor node
if it moves in to its projection interval. Now con-
sider which cases can lead to gap inheritance: in case
(b), there is a single path from the ancestor to the
root of the subtree, so the parent of the subtree will
have no gap inheritance and any higher ancestors
will have a single child inherit the gap created by this
movement. In case (c), it is possible for there to be
multiple children that inherit this newly created gap
if multiple children had descendents on both sides.
However, the assumption of upward movement in
the phrase structure tree should rule out movement
into the projection interval of a non-ancestor. There-
fore, under these syntactic assumptions, we would
expect at most one child to inherit a parent?s gap.
5The Proper Binding Condition (Fiengo, 1977) asserts that a
moved element leaves behind a trace (unpronounced element),
which must be c-commanded (Reinhart, 1976) by the corre-
sponding pronounced material in its final location. Informally,
c-commanded means that the first node is descended from the
lowest ancestor of the other that has more than one child.
4.1 Parsing Mild+1-Inherit Trees
Finding the optimal Mild+1-Inherit tree can be done
by bottom-up constructing the tree for each node and
its descendants. We can maintain subtrees with two
intervals (two endpoints each) and one root (O(n5)
space). Consider the most complicated possible
case: a parent that has a gap, a (single) child which
inherits the gap, and additional children. An exam-
ple of this is seen with the parent node 3 in Figure
1(b).
This subtree can be constructed by first starting
with the child spanning the gap, updating its root
index to be the parent, and then expanding the inter-
val indices to the left and right to include the other
children. In each case, only one index needs to be
updated at a time, so the optimal tree can be found
in O(n6) time. In the Figure 1(b) example, the sub-
tree rooted at 3 would be built by starting with the
intervals [1, 2] ? [12, 13] rooted at 2, first adding the
edge from 2 to 3 (so the root is updated to 3), then
adding an edge from 3 to 4 to extend the left inter-
val to [1, 5], and then adding an edge from 3 to 11 to
extend the right interval to [8, 13]. The subtree cor-
responds to the completed item [1, 5]? [8, 13] rooted
at 3.
This procedure corresponds to Go?mez-Rodr??guez
et al2011)?s O(n7) algorithm for parsing mildly
non-projective structures if the most expensive step
(Combine Shrinking Gap Centre) is dropped; this
step would only ever be needed if a parent node has
481
more than one child inheriting its gap.
This is also similar in spirit to the algorithm de-
scribed in Satta and Schuler (1998) for parsing a
restricted version of TAG, in which there are some
limitations on adjunction operations into the spines
of trees.6 That algorithm has similar steps and items,
with the root portion of the item replaced with a
node in a phrase structure tree (which may be a non-
terminal).
5 Gap-minding Trees
The algorithm in the previous section used O(n5)
space and O(n6) time. While more efficient than
parsing in the space of mildly projective trees, this
is still probably not practically implementable. Part
of the difficulty lies in the fact that gap inheritance
causes the two non-contiguous projection intervals
to be coupled.
Definition 6. A tree is called gap-minding7 if it has
gap degree at most one, is well-nested, and has gap
inheritance degree 0.
Gap-minding trees still have good empirical cov-
erage (between 90.4% for Dutch and 97.7% for
Swedish). We now turn to the parsing of gap-
minding trees and show how a few consequences of
its definition allow us to use items ranging over only
one interval.
In Figure 1(c), notice how each rectangle has
edges incoming from exactly one node. This is not
unique to this example; all projection intervals in a
gap-minding tree have incoming edges from exactly
one node outside the interval.
Claim 1. Within a gap-minding tree, consider any
node n with a gap (i.e., n?s projection forms two
non-contiguous intervals [xi, xj ] ? [xk, xl]). Let p
be the parent of n.
1. For each of the intervals of n?s projection:
(a) If the interval contains n, the only edge
incoming to that interval is from p to n.
6That algorithm has a running time of O(Gn5), where as
written G would likely add a factor of n2 with bilexical selec-
tional preferences; this can be lowered to n using the same tech-
nique as in Eisner and Satta (2000) for non-restricted TAG.
7The terminology is a nod to the London Underground but
imagines parents admonishing children to mind the gap.
(b) If the interval does not contain n, all edges
incoming to that interval come from n.
2. For the gap interval ([xj+1, xk?1]):
(a) If the interval contains p, then the only
edge incoming is from p?s parent to p
(b) If the interval does not contain p, then all
edges incoming to that interval come from
p.
As a consequence of the above, [xi, xj ] ? {n} forms
a gap-minding tree rooted at n, [xk, xl] ? {n}
also forms a gap-minding tree rooted at n, and
[xj+1, xk?1] ? {p} forms a gap-minding tree rooted
at p.
Proof. (Part 1): Assume there was a directed edge
(x, y) such that y is inside a projection interval of n
and x is not inside the same interval, and x 6= y 6= n.
y is a descendant of n since it is contained in n?s pro-
jection. Since there is a directed edge from x to y,
x is y?s parent, and thus x must also be a descen-
dant of n and therefore in another of n?s projection
intervals. Since x and y are in different intervals,
then whichever child of n that x and y are descended
from would have inherited n?s gap, leading to a con-
tradiction.
(Part 2): First, suppose there existed a set of nodes
in n?s gap which were not descended from p. Then
p has a gap over these nodes. (p clearly has descen-
dants on each side of the gap, because all descen-
dants of n are also descendants of p). n, p?s child,
would then have descendants on both sides of p?s
gap, which would violate the property of no gap in-
heritance. It is also not possible for there to be edges
incoming from other descendants of p outside the
gap, as that would imply another child of p being
ill-nested with respect to n.
From the above, we can build gap-minding trees
using only single intervals, potentially with a sin-
gle node outside of the interval. Our objective is
to find the maximum scoring gap-minding tree, in
which the score of a tree is the sum of the scores of
its edges. Let Score(p,x) indicate the score of the
directed edge from p to x.
Therefore, the main type of sub-problems we will
use are:
482
1. C[i, j,p]: The maximum score of any gap-
minding tree, rooted at p, with vertices [i, j] ?
{p} (p may or may not be within [i, j]).
This improves our space requirement, but not nec-
essarily the time requirement. For example, if we
built up the subtree in Figure 1(c) by concatenating
the three intervals [1, 5] rooted at 3, [6, 7] rooted at 6,
and [8, 13] rooted at 3, and add the edge 6 ? 3, we
would still need 6 indices to describe this operation
(the four interval endpoints and the two roots), and
so we have not yet improved the running time over
the Inherit-1 case.
By part 2, we can concatenate one interval of a
child with its gap, knowing that the gap is entirely
descended from the child?s parent, and forget the
concatenation split point between the parent?s other
descendants and this side of the child. This allows us
to substitute all operations involving 6 indices with
two operations involving just 5 indices. For exam-
ple, in Figure 1(c), we could first merge [6, 7] rooted
at 6 with [8, 13] rooted at 3 to create an interval
[6, 13] and say that it is descended from 6, with the
rightmost side descended from its child 3. That step
required 5 indices. The following step would merge
this concatenated interval ([6, 13] rooted at 6 and 3)
with [1, 5] rooted at 3. This step also requires only 5
indices.
Our helper subtype we make use of is then:
2. D[i, j,p,x,b]: The maximum score of any set
of two gap-minding trees, one rooted at p, one
rooted at x, with vertices [i, j] ? {p, x} (x /?
[i, j], p may or may not be in [i, j]), such that
for some k, vertices [i, k] are in the tree rooted
at p if b = true (and at x if b = false), and
vertices [k+1, j] are in the tree rooted at x (p).
Consider an optimum scoring gap-minding tree T
rooted at p with vertices V = [i, j] ? {p} and edges
E, where E 6= ?. The form of the dynamic program
may depend on whether:
? p is within (i, j) (I) or external to [i, j] (E)8
8In the discussion we will assume that p 6= i and p 6= j,
since any optimum solution with V = [i, j] ? {i} and a root
at i will be equivalent to V = [i + 1, j] ? {i} rooted at i (and
similarly for p = j).
We can exhaustively enumerate all possibilities for
T by considering all valid combinations of the fol-
lowing binary cases:
? p has a single child (S) or multiple children (M)
? i and j are descended from the same child of p
(C) or different children of p (D)
Note that case (S/D) is not possible: i and j cannot
be descended from different children of p if p has
only a single child. We therefore need to find the
maximum scoring tree over the three cases of S/C,
M/C, and M/D.
Claim 2. Let T be the optimum scoring gap-
minding tree rooted at p with vertices V = [i, j] ?
{p}. Then T and its score are derived from one of
the following:
S/C If p has a single child x in T , then if p ? (i, j)
(I), T ?s score is Score(p,x)+C[i,p?1,x]+
C[p+ 1, j,x]; if p /? [i, j] (E), T ?s score is
Score(p,x) +C[i, j,x].
M/C If p has multiple children in T and i and j
are descended from the same child x in T , then
there is a split point k such that T ?s score is:
Score(p,x)+C[i,k,x]+D[k+ 1, j,p,x,T]
if x is on the left side of its own gap, and
T ?s score is: Score(p,x) + C[k, j,x] +
D[i,k? 1,p,x,F] if x is on the right side.
M/D If p has multiple children in T and i and j
are descended from different children in T , then
there is a split point k such that T ?s score is
C[i,k,p] +C[k+ 1, j,p].
T has the maximum score over each of the above
cases, for all valid choices of x and k.
Proof. Case S/C: If p has exactly one child x,
then the tree can be decomposed into the edge
from p to x and the subtree rooted at x. If p
is outside the interval, then the maximum scor-
ing such tree is clearly Score(p,x) + C[i, j,x].
If p is inside, then x has a gap across p, and
so using Claim 1, the maximum scoring tree
rooted at p with a single child x has score of
Score(p,x) +C[i,p? 1,x] +C[p+ 1, j,x].
Case M/C: If there are multiple children and the
endpoints are descended from the same child x, then
483
the child x has to have gap degree 1. x itself is on
either the left or right side of its gap. For the mo-
ment, assume x is in the left interval. By Claim 1,
we can split up the score of the tree as the score of
the edge from p to x (Score(p,x)), the score of the
subtree corresponding to the projection of x to the
left of its gap (C[i,k,x]), and the score of the sub-
trees rooted at p with its remaining children and the
subtree rooted at x corresponding to the right side
of x?s projection (D[k+ 1, j,p,x,T]). The case in
which x is on the right side of its gap is symmetric.
Case M/D: If there are multiple children and the
endpoints are descended from different children of
p, then there must exist a split point k that parti-
tions the children of p into two non-empty sets, such
that each child?s projection is either entirely on the
left or entirely on the right of the split point. We
show one such split point to demonstrate that there
always exists at least one. Let x be the child of p
that i is descended from, and let xl and xr be x?s
leftmost and right descendants, respectively.9 Con-
sider all the children of p (whose projections taken
together partition [i, j] ? {p}). No child can have
descendants both to the left of xr and to the right
of xr, because otherwise that child and x would be
ill-nested. Therefore we can split up the interval at
xr to have two gap-minding trees, both rooted at p.
The score of T is then the sum of the scores of the
best subtree rooted at p over [i, k] (C[i,k,p]) and
the score of the best subtree rooted at p over [k+1, j]
(C[k+ 1, j,p]).
The above cases cover all non-empty gap-
minding trees, so the maximum will be found.
Using Claim 2 to Devise an Algorithm The above
claim showed that any problem of type C can be
decomposed into subproblems of types C and D.
From the definition of D, any problem of type D can
clearly be decomposed into two problems of type C
? simply split the interval at the split point known
to exist and assign p or x as the roots for each side
of the interval, as prescribed by the boolean b:
D(i, j,p,x,T) = maxkC[i,k,p] +C[k+ 1, j,x]
D(i, j,p,x,F) = maxkC[i,k,x] +C[k+ 1, j,p]
9Note that xl = i by construction, and xr 6= j (because the
endpoints are descended from different children).
Algorithm 1 makes direct use of the above claims.
Note that in every gap-minding tree referred to
in the cases above, all vertices that were not the
root formed a single interval. Algorithm 1 builds
up trees in increasing sizes of [i, j] ? {p}. The
tree in C[i, j,p] corresponds to the maximum of
four subroutines: SingleChild (S/C), EndpointsDiff
(M/D), EndsFromLeftChild (M/C), and EndsFrom-
RightChild (M/C). The D subproblems are filled in
with the subroutine Max2Subtrees, which uses the
above discussion. The maximum score of any gap-
minding tree is then found in C[1,n,0], and the tree
itself can be found using backpointers.
5.1 Runtime analysis
If the input is assumed to be the complete graph (any
word can have any other word as its parent), then
the above algorithm takes O(n5) time. The most
expensive steps are M/C, which take O(n2) time to
fill in each of the O(n3) C cells. and solving a D
subproblem, which takes O(n) time on each of the
O(n4) possible such problems.
Pruning: In practice, the set of edges considered
(m) is not necessarily O(n2). Many edges can be
ruled out beforehand, either based on the distance
in the sentence between the two words (Eisner and
Smith, 2010), the predictions of a local ranker (Mar-
tins et al2009), or the marginals computed from a
simpler parsing model (Carreras et al2008).
If we choose a pruning strategy such that each
word has at most k potential parents (incoming
edges), then the running time drops to O(kn4). The
five indices in an M/C step were: i, j, k, p, and x.
As there must be an edge from p to x, and x only has
k possible parents, there are now only O(kn4) valid
such combinations. Similarly, each D subproblem
(which ranges over i, j, k, p, x) may only come into
existence because of an edge from p to x, so again
the runtime of these such steps drops to O(kn4).
6 Extension to Grandparent
Factorizations
The ability to define slightly non-local features has
been shown to improve parsing performance. In this
section, we assume a grandparent-factored model,
where the score of a tree is now the sum over scores
of (g, p, c) triples, where (g, p) and (p, c) are both
484
directed edges in the tree. Let Score(g,p, c) indi-
cate the score of this grandparent-parent-child triple.
We now show how to extend the above algorithm
to find the maximum scoring gap-minding tree with
grandparent scoring.
Our two subproblems are now C[i, j,p,g] and
D[i, j,p,x,b,g]; each subproblem has been aug-
mented with an additional grandparent index g,
which has the meaning that g is p?s parent. Note that
g must be outside of the interval [i, j] (if it were not,
a cycle would be introduced). Edge scores are now
computed over (g, p, x) triples. In particular, claim
2 is modified:
Claim 3. Let T be the optimum scoring gap-
minding tree rooted at p with vertices V = [i, j] ?
{p}, where p ? (i, j) (I), with a grandparent index
g (g /? V ). Then T and its score are derived from
one of the following:
S/C If p has a single child x in T , then if
p ? (i, j) (I), T ?s score is Score(g,p,x) +
C[i,p?1,x,p]+C[p+ 1, j,x,p]; if p /? [i, j]
(E), T ?s score is Score(g,p,x)+C[i, j,x,p].
M/C If p has multiple children in T and i
and j are descended from the same child
x in T , then there is a split point k
such that T ?s score is: Score(g,p,x) +
C[i,k,x,p] + D[k+ 1, j,p,x,T,g] if x is
on the left side of its own gap, and T ?s
score is: Score(g,p,x) + C[k, j,x,p] +
D[i,k? 1,p,x,F,g] if x is on the right side.
M/D If p has multiple children in T and i and j
are descended from different children in T , then
there is a split point k such that T ?s score is
C[i,k,p,g] +C[k+ 1, j,p,g].
T has the maximum score over each of the above
cases, for all valid choices of x and k.
Note that for subproblems rooted at p, g is the
grandparent index, while for subproblems rooted at
x, p is the updated grandparent index. The D sub-
problems with the grandparent index are shown be-
low:
D(i, j,p,x,T,g) = maxkC[i,k,p,g] +C[k+ 1, j,x,p]
D(i, j,p,x,F,g) = maxkC[i,k,x,p] +C[k+ 1, j,p,g]
We have added another index which ranges over
n, so without pruning, we have now increased the
running time to O(n6). However, every step now in-
cludes both a g and a p (and often an x), so there is
at least one implied edge in every step. If pruning
is done in such a way that each word has at most k
parents, then each word?s set of grandparent and par-
ent possibilities is at most k2. To run all of the S/C
steps, we therefore need O(k2n3) time; for all of the
M/C steps, O(k2n4) time; for all of the M/D steps,
O(kn4); for all of the D subproblems, O(k2n4). The
overall running time is therefore O(k2n4), and we
have shown that when edges are sufficiently pruned,
grandparent factors add only an extra factor of k, and
not a full extra factor of n.
7 Experiments
The space of projective trees is strictly contained
within the space of gap-minding trees which is
strictly contained within spanning trees. Which
space is most appropriate for natural language pars-
ing may depend on the particular language and the
type and frequencies of non-projective structures
found in it. In this section we compare the parsing
accuracy across languages for a parser which uses
either the Eisner algorithm (projective), MST (span-
ning trees), or MaxGapMindingTree (gap-minding
trees) as its decoder for both training and inference.
We implemented both the basic gap-minding al-
gorithm and the gap-minding algorithm with grand-
parent scoring as extensions to MSTParser10. MST-
Parser (McDonald et al2005b; McDonald et al
2005a) uses the Margin Infused Relaxed Algo-
rithm (Crammer and Singer, 2003) for discrimina-
tive training. Training requires a decoder which
produces the highest scoring tree (in the space of
valid trees) under the current model weights. This
same decoder is then used to produce parses at test
time. MSTParser comes packaged with the Eis-
ner algorithm (for projective trees) and MST (for
spanning trees). MSTParser also includes two sec-
ond order models: one of which is a projective de-
coder that also scores siblings (Proj+Sib) and the
other of which produces non-projective trees by re-
arranging edges after producing a projective tree
(Proj+Sib+Rearr). We add a further decoder with
10http://sourceforge.net/projects/mstparser/
485
the algorithm presented here for gap minding trees,
and plan to make the extension publicly available.
The gap-minding decoder has both an edge-factored
implementation and a version which scores grand-
parents as well.11
The gap-minding algorithm is much more effi-
cient when edges have been pruned so that each
word has at most k potential parents. We use the
weights from the trained MST models combined
with the Matrix Tree Theorem (Smith and Smith,
2007; Koo et al2007; McDonald and Satta, 2007)
to produce marginal probabilities of each edge. We
wanted to be able to both achieve the running time
bound and yet take advantage of the fact that the
size of the set of reasonable parent choices is vari-
able. We therefore use a hybrid pruning strategy:
each word?s set of potential parents is the smaller of
a) the top k parents (we chose k = 10) or b) the set
of parents whose probabilities are above a thresh-
old (we chose th = .001). The running time for
the gap-minding algorithm is then O(kn4); with the
grandparent features the gap-minding running time
is O(k2n4).
The training and test sets for the six languages
come from the CoNLL-X shared task.12 We train
the gap-minding algorithm on sentences of length
at most 10013 (the vast majority of sentences). The
projective and MST models are trained on all sen-
tences and are run without any pruning. The Czech
training set is much larger than the others and so for
Czech only the first 10,000 training sentences were
used. Testing is on the full test set, with no length
restrictions.
The results are shown in Table 2. The first three
lines show the first order gap-minding decoder com-
pared with the first order projective and MST de-
11The grandparent features used were identical to the fea-
tures provided within MSTParser for the second-order sibling
parsers, with one exception ? many features are conjoined with
a direction indicator, which in the projective case has only two
possibilities. We replaced this two-way distinction with a six-
way distinction of the six possible orders of the grandparent,
parent, and child.
12MSTParser produces labeled dependencies on CoNLL for-
matted input. We replace all labels in the training set with a
single dummy label to produce unlabeled dependency trees.
13Because of long training times, the gap-minding with
grandparent models for Portuguese and Swedish were trained
on only sentences up to 50 words.
Ar Cz Da Du Pt Sw
Proj. 78.0 80.0 88.2 79.8 87.4 86.9
MST 78.0 80.4 88.1 84.6 86.7 86.2
Gap-Mind 77.6 80.8 88.6 83.9 86.8 86.0
Proj+Sib 78.2 80.0 88.9 81.1 87.5 88.1
+Rearr 78.5 81.3 89.3 85.4 88.2 87.7
GM+Grand 78.3 82.1 89.1 84.6 87.7 88.5
Table 2: Unlabeled Attachment Scores on the CoNLL-X
shared task test set.
coders. The gap-minding decoder does better than
the projective decoder on Czech, Danish, and Dutch,
the three languages with the most non-projectivity,
even though it was at a competitive disadvantage in
terms of both pruning and (on languages with very
long sentences) training data. The gap-minding de-
coder with grandparent features is better than the
projective decoder with sibling features on all six
of the languages. On some languages, the local
search decoder with siblings has the absolute high-
est accuracy in Table 2; on other languages (Czech
and Swedish) the gap-minding+grandparents has the
highest accuracy. While not directly comparable be-
cause of the difference in features, the promising
performance of the gap-minding+grandparents de-
coder shows that the space of gap-minding trees is
larger than the space of projective trees, yet unlike
spanning trees, it is tractable to find the best tree with
higher order features. It would be interesting to ex-
tend the gap-minding algorithm to include siblings
as well.
8 Conclusion
Gap inheritance, a structural property on trees, has
implications both for natural language syntax and
for natural language parsing. We have shown that
the mildly non-projective trees present in natural
language treebanks all have zero or one children in-
herit each parent?s gap. We also showed that the as-
sumption of 1 gap inheritance removes a factor of
n from parsing time, and the further assumption of
0 gap inheritance removes yet another factor of n.
The space of gap-minding trees provides a closer fit
to naturally occurring linguistic structures than the
space of projective trees, and unlike spanning trees,
the inclusion of higher order factors does not sub-
stantially increase the difficulty of finding the maxi-
mum scoring tree in that space.
486
Acknowledgments
We would like to thank Aravind Joshi for comments
on an earlier draft. This material is based upon
work supported under a National Science Founda-
tion Graduate Research Fellowship.
Algorithm 1: MaxGapMindingTree
Init: ?i?[1,n]C[i, i, i] = 0
for size = 0 to n? 1 do
for i = 1 to n? size do
j = i+ size
/* Endpoint parents */
if size > 0 then
C[i, j, i] = C[i+ 1, j, i]
C[i, j, j] = C[i, j ? 1, j]
/* Interior parents */
for p = i+ 1 to j ? 1 do
C[i, j, p] = max (SingleChild(i,j,p),
EndpointsDiff(i,j,p),
EndsFromLeftChild(i,j,p),
EndsFromRightChild(i,j,p))
/* Exterior parents */
forall the p ? [0, i? 1] ? [j + 1, n] do
C[i, j, p] = max (SingleChild(i,j,p),
EndpointsDiff(i,j,p),
EndsFromLeftChild(i,j,p),
EndsFromRightChild(i,j,p))
/* Helper subproblems */
for p ? [0, n] do
forall the x ? PosChild[p] ? x /? [i, j] do
if p 6= j then
D[i, j, p, x, T ] = Max2Subtrees(i, j, p, x, T )
if p 6= i then
D[i, j, p, x, F ] = Max2Subtrees(i, j, p, x, F )
Final answer: C[1, n, 0]
Function SingleChild(i,j,p)
X = PosChild[p] ? [i, j]
/* Interior p */
if p > i ? p < j then
return maxx?X C[i, p? 1, x]
+C[p+ 1, j, x] + Score(p, x)
/* Exterior p */
else
return maxx?X C[i, j, x] + Score(p, x)
Function EndpointsDiff(i,j,p)
return maxk?[i,j?1] C[i, k, p] + C[k + 1, j, p]
Function EndsFromLeftChild(i,j,p)
/* Interior p */
if p > i ? p < j then
X = PosChild[p] ? [i, p? 1]
forall the x ? X ? x < p do
K[x] = [x, p? 1]
/* Exterior p */
else
X = PosChild[p] ? [i, j]
forall the x ? X do
K = [x, j ? 2]
return maxx?X,k?K[x] C[i, k, x]
+Score(p, x) +D[k + 1, j, p, x, T ]
Function EndsFromRightChild(i,j,p)
/* Interior p */
if p > i ? p < j then
X = PosChild[p] ? [p+ 1, j]
forall the x ? X ? x > p do
K[x] = [p+ 1, x]
/* Exterior p */
else
X = PosChild[p] ? [i, j]
forall the x ? X do
K[x] = [i+ 2, x]
return maxx?X,k?K[x] C[k, j, x]
+Score(p, x) +D[i, k ? 1, p, x, F ]
Function Max2Subtrees(i,j,p,x,pOnLeft)
/* Interior p */
if p ? i ? p ? j then
if pOnLeft then
K = [p, j ? 1]
return maxk?K C[i, k, p] + C[k + 1, j, x]
else
K = [i, p? 1]
return maxk?K C[i, k, x] + C[k + 1, j, p]
/* Exterior p */
else
K = [i, j ? 1]}
if pOnLeft then
return maxk?K C[i, k, p] + C[k + 1, j, x]
else
return maxk?K C[i, k, x] + C[k + 1, j, p]
487
References
M. Bodirsky, M. Kuhlmann, and M. Mo?hl. 2005. Well-
nested drawings as models of syntactic structure. In
In Tenth Conference on Formal Grammar and Ninth
Meeting on Mathematics of Language, pages 88?1.
University Press.
X. Carreras, M. Collins, and T. Koo. 2008. Tag, dynamic
programming, and the perceptron for efficient, feature-
rich parsing. In Proceedings of CoNLL, pages 9?16.
Association for Computational Linguistics.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, vol-
ume 7, pages 957?961.
J. Chen-Main and A. Joshi. 2010. Unavoidable ill-
nestedness in natural language and the adequacy of
tree local-mctag induced dependency structures. In
Proceedings of the Tenth International Workshop on
Tree Adjoining Grammar and Related Formalisms
(TAG+ 10).
J. Chen-Main and A.K. Joshi. 2012. A depen-
dency perspective on the adequacy of tree local multi-
component tree adjoining grammar. In Journal of
Logic and Computation. (to appear).
N. Chomsky. 1981. Lectures on Government and Bind-
ing. Dordrecht: Foris.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research, 3:951?991, March.
J. Eisner and G. Satta. 2000. A faster parsing algorithm
for lexicalized tree-adjoining grammars. In Proceed-
ings of the 5th Workshop on Tree-Adjoining Grammars
and Related Formalisms (TAG+5), pages 14?19.
J. Eisner and N.A. Smith. 2010. Favor short dependen-
cies: Parsing with soft and hard constraints on depen-
dency length. In Harry Bunt, Paola Merlo, and Joakim
Nivre, editors, Trends in Parsing Technology: Depen-
dency Parsing, Domain Adaptation, and Deep Parsing,
chapter 8, pages 121?150. Springer.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 29?62. Kluwer Academic
Publishers, October.
R. Fiengo. 1977. On trace theory. Linguistic Inquiry,
8(1):35?61.
C. Go?mez-Rodr??guez, J. Carroll, and D. Weir. 2011. De-
pendency parsing schemata and mildly non-projective
dependency parsing. Computational Linguistics,
37(3):541?586.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of ACL, pages 1?11.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proceedings of EMNLP-CoNLL.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proceedings of EMNLP,
pages 1288?1298.
M. Kuhlmann and J. Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings of
COLING/ACL, pages 507?514.
A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL, pages 342?
350.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81?88.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 121?132.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of HLT-EMNLP,
pages 523?530.
T. Reinhart. 1976. The Syntactic Domain of Anaphora.
Ph.D. thesis, Massachusetts Institute of Technology.
G. Satta and W. Schuler. 1998. Restrictions on tree ad-
joining languages. In Proceedings of COLING-ACL,
pages 1176?1182.
D.A. Smith and N.A. Smith. 2007. Probabilistic models
of nonprojective dependency trees. In Proceedings of
EMNLP-CoNLL.
488
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 544?554,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Linguistic Quality in Multi-Document
Summarization
Emily Pitler, Annie Louis, Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler,lannie,nenkova@seas.upenn.edu
Abstract
To date, few attempts have been made
to develop and validate methods for au-
tomatic evaluation of linguistic quality in
text summarization. We present the first
systematic assessment of several diverse
classes of metrics designed to capture var-
ious aspects of well-written text. We train
and test linguistic quality models on con-
secutive years of NIST evaluation data in
order to show the generality of results. For
grammaticality, the best results come from
a set of syntactic features. Focus, coher-
ence and referential clarity are best evalu-
ated by a class of features measuring local
coherence on the basis of cosine similarity
between sentences, coreference informa-
tion, and summarization specific features.
Our best results are 90% accuracy for pair-
wise comparisons of competing systems
over a test set of several inputs and 70%
for ranking summaries of a specific input.
1 Introduction
Efforts for the development of automatic text sum-
marizers have focused almost exclusively on im-
proving content selection capabilities of systems,
ignoring the linguistic quality of the system out-
put. Part of the reason for this imbalance is the
existence of ROUGE (Lin and Hovy, 2003; Lin,
2004), the system for automatic evaluation of con-
tent selection, which allows for frequent evalua-
tion during system development and for report-
ing results of experiments performed outside of
the annual NIST-led evaluations, the Document
Understanding Conference (DUC)1 and the Text
Analysis Conference (TAC)2. Few metrics, how-
ever, have been proposed for evaluating linguistic
1http://duc.nist.gov/
2http://www.nist.gov/tac/
quality and none have been validated on data from
NIST evaluations.
In their pioneering work on automatic evalua-
tion of summary coherence, Lapata and Barzilay
(2005) provide a correlation analysis between hu-
man coherence assessments and (1) semantic re-
latedness between adjacent sentences and (2) mea-
sures that characterize how mentions of the same
entity in different syntactic positions are spread
across adjacent sentences. Several of their models
exhibit a statistically significant agreement with
human ratings and complement each other, yield-
ing an even higher correlation when combined.
Lapata and Barzilay (2005) and Barzilay and
Lapata (2008) both show the effectiveness of
entity-based coherence in evaluating summaries.
However, fewer than five automatic summarizers
were used in these studies. Further, both sets
of experiments perform evaluations of mixed sets
of human-produced and machine-produced sum-
maries, so the results may be influenced by the
ease of discriminating between a human and ma-
chine written summary. Therefore, we believe it is
an open question how well these features predict
the quality of automatically generated summaries.
In this work, we focus on linguistic quality eval-
uation for automatic systems only. We analyze
how well different types of features can rank good
and poor machine-produced summaries. Good
performance on this task is the most desired prop-
erty of evaluation metrics during system develop-
ment. We begin in Section 2 by reviewing the
various aspects of linguistic quality that are rel-
evant for machine-produced summaries and cur-
rently used in manual evaluations. In Section 3,
we introduce and motivate diverse classes of fea-
tures to capture vocabulary, sentence fluency, and
local coherence properties of summaries. We eval-
uate the predictive power of these linguistic qual-
ity metrics by training and testing models on con-
secutive years of NIST evaluations (data described
544
in Section 4). We test the performance of differ-
ent sets of features separately and in combination
with each other (Section 5). Results are presented
in Section 6, showing the robustness of each class
and their abilities to reproduce human rankings of
systems and summaries with high accuracy.
2 Aspects of linguistic quality
We focus on the five aspects of linguistic qual-
ity that were used to evaluate summaries in DUC:
grammaticality, non-redundancy, referential clar-
ity, focus, and structure/coherence.3 For each of
the questions, all summaries were manually rated
on a scale from 1 to 5, in which 5 is the best.
The exact definitions that were provided to the
human assessors are reproduced below.
Grammaticality: The summary should have no datelines,
system-internal formatting, capitalization errors or obviously
ungrammatical sentences (e.g., fragments, missing compo-
nents) that make the text difficult to read.
Non-redundancy: There should be no unnecessary repeti-
tion in the summary. Unnecessary repetition might take the
form of whole sentences that are repeated, or repeated facts,
or the repeated use of a noun or noun phrase (e.g., ?Bill Clin-
ton?) when a pronoun (?he?) would suffice.
Referential clarity: It should be easy to identify who or what
the pronouns and noun phrases in the summary are referring
to. If a person or other entity is mentioned, it should be clear
what their role in the story is. So, a reference would be un-
clear if an entity is referenced but its identity or relation to
the story remains unclear.
Focus: The summary should have a focus; sentences should
only contain information that is related to the rest of the sum-
mary.
Structure and Coherence: The summary should be well-
structured and well-organized. The summary should not just
be a heap of related information, but should build from sen-
tence to sentence to a coherent body of information about a
topic.
These five questions get at different aspects of
what makes a well-written text. We therefore pre-
dict each aspect of linguistic quality separately.
3 Indicators of linguistic quality
Multiple factors influence the linguistic quality of
text in general, including: word choice, the ref-
erence form of entities, and local coherence. We
extract features which serve as proxies for each of
the factors mentioned above (Sections 3.1 to 3.5).
In addition, we investigate some models of gram-
maticality (Chae and Nenkova, 2009) and coher-
ence (Graesser et al, 2004; Soricut and Marcu,
2006; Barzilay and Lapata, 2008) from prior work
(Sections 3.6 to 3.9).
3http://www-nlpir.nist.gov/projects/
duc/duc2006/quality-questions.txt
All of the features we investigate can be com-
puted automatically directly from text, but some
require considerable linguistic processing. Several
of our features require a syntactic parse. To extract
these, all summaries were parsed by the Stanford
parser (Klein and Manning, 2003).
3.1 Word choice: language models
Psycholinguistic studies have shown that people
read frequent words and phrases more quickly
(Haberlandt and Graesser, 1985; Just and Carpen-
ter, 1987), so the words that appear in a text might
influence people?s perception of its quality. Lan-
guage models (LM) are a way of computing how
familiar a text is to readers using the distribution
of words from a large background corpus. Bigram
and trigram LMs additionally capture grammati-
cality of sentences using properties of local tran-
sitions between words. For this reason, LMs are
widely used in applications such as generation and
machine translation to guide the production of sen-
tences. Judging from the effectiveness of LMs in
these applications, we expect that they will pro-
vide a strong baseline for the evaluation of at least
some of the linguistic quality aspects.
We built unigram, bigram, and trigram lan-
guage models with Good-Turing smoothing over
the New York Times (NYT) section of the English
Gigaword corpus (over 900 million words). We
used the SRI Language Modeling Toolkit (Stol-
cke, 2002) for this purpose. For each of the three
ngram language models, we include the min, max,
and average log probability of the sentences con-
tained in a summary, as well as the overall log
probability of the entire summary.
3.2 Reference form: Named entities
This set of features examines whether named enti-
ties have informative descriptions in the summary.
We focus on named entities because they appear
often in summaries of news documents and are of-
ten not known to the reader beforehand. In addi-
tion, first mentions of entities in text introduce the
entity into the discourse and so must be informa-
tive and properly descriptive (Prince, 1981; Frau-
rud, 1990; Elsner and Charniak, 2008).
We run the Stanford Named Entity Recognizer
(Finkel et al, 2005) and record the number of
PERSONs, ORGANIZATIONs, and LOCATIONs.
First mentions to people Feature exploration on
our development set found that under-specified
545
references to people are much more disruptive
to a summary than short references to organiza-
tions or locations. In fact, prior work in Nenkova
and McKeown (2003) found that summaries that
have been rewritten so that first mentions of peo-
ple are informative descriptions and subsequent
mentions are replaced with more concise reference
forms are overwhelmingly preferred to summaries
whose entity references have not been rewritten.
In this class, we include features that reflect
the modification properties of noun phrases (NPs)
in the summary that are first mentions to people.
Noun phrases can include pre-modifiers, apposi-
tives, prepositional phrases, etc. Rather than pre-
specifying all the different ways a person expres-
sion can be modified, we hoped to discover the
best patterns automatically, by including features
for the average number of each Part of Speech
(POS) tag occurring before, each syntactic phrase
occurring before4, each POS tag occurring after,
and each syntactic phrase occurring after the head
of the first mention NP for a PERSON. To measure
if the lack of pre or post modification is particu-
larly detrimental, we also include the proportion
of PERSON first mention NPs with no words be-
fore and with no words after the head of the NP.
Summarization specific Most summarization
systems today are extractive and create summaries
using complete sentences from the source docu-
ments. A subsequent mention of an entity in a
source document which is extracted to be the first
mention of the entity in the summary is proba-
bly not informative enough. For each type of
named entity (PERSON, ORGANIZATION, LO-
CATION), we separately record the number of in-
stances which appear as first mentions in the sum-
mary but correspond to non-first mentions in the
source documents.
3.3 Reference form: NP syntax
Some summaries might not include people and
other named entities at all. To measure how en-
tities are referred to more generally, we include
features about the overall syntactic patterns found
in NPs: the average number of each POS tag and
each syntactic phrase occurring inside NPs.
4We define a linear order based on a preorder traversal of
the tree, so syntactic phrases which dominate the head are
considered occurring before the head.
3.4 Local coherence: Cohesive devices
In coherent text, constituent clauses and sentences
are related and depend on each other for their in-
terpretation. Referring expressions such as pro-
nouns link the current utterance to those where the
entities were previously mentioned. In addition,
discourse connectives such as ?but? or ?because?
relate propositions or events expressed by differ-
ent clauses or sentences. Both these categories
are known cohesive or linking devices in human-
produced text (Halliday and Hasan, 1976). The
mere presence of such items in a text would be in-
dicative of better structure and coherence.
We compute a number of shallow features that
provide a cheap way of capturing the above intu-
itions: the number of demonstratives, pronouns,
and definite descriptions as well as the number of
sentence-initial discourse connectives.
3.5 Local coherence: Continuity
This class of linguistic quality indicators is a com-
bination of factors related to coreference, adjacent
sentence similarity, and summary-specific context
of surface cohesive devices.
Summarization specific Extractive multi-
document summaries often lack appropriate
antecedents for pronouns and proper context for
the use of discourse connectives.
In fact, early work in summarization (Paice,
1980; Paice, 1990) has pointed out that the pres-
ence of cohesive devices described in the previous
section might in fact be the source of problems.
A manual analysis of automatic summaries (Ot-
terbacher et al, 2002) also revealed that anaphoric
references that cannot be resolved and unclear dis-
course relations constitute more than 30% of all
revisions required to manually rewrite summaries
into a more coherent form.
To identify these potential problems, we adapt
the features for surface cohesive devices to indi-
cate whether referring expressions and discourse
connectives appear in the summary with the same
context as in the input documents.
For each of the cohesive devices discussed in
Section 3.4?demonstratives, pronouns, definite
descriptions, and sentence-initial discourse con-
nectives?we compare the previous sentence in
the summary with the previous sentence in the in-
put article. Two features are computed for each
type of cohesive device: (1) number of times the
preceding sentence in the summary is the same
546
as the preceding sentence in the input and (2) the
number of times the preceding sentence in sum-
mary is different from that in the input. Since
the previous sentence in the input text often con-
tains the antecedent of pronouns in the current
sentence, if the previous sentence from the input
is also included in the summary, the pronoun is
highly likely to have a proper antecedent.
We also compute the proportion of adjacent sen-
tences in the summary that were extracted from the
same input document.
Coreference Steinberger et al (2007) compare the
coreference chains in input documents and in sum-
maries in order to locate potential problems. We
instead define a set of more general features re-
lated to coreference that are not specific to sum-
marization and are applicable for any text. Our
features check the existence of proper antecedents
for pronouns in the summary without reference to
the text of the input documents.
We use the publicly available pronoun reso-
lution system described in Charniak and Elsner
(2009) to mark possible antecedents for pronouns
in the summary. We then compute as features the
number of times an antecedent for a pronoun was
found in the previous sentence, in the same sen-
tence, or neither. In addition, we modified the pro-
noun resolution system to also output the probabil-
ity of the most likely antecedent and include the
average antecedent probability for the pronouns
in the text. Automatic coreference systems are
trained on human-produced texts and we expect
their accuracies to drop when applied to automat-
ically generated summaries. However, the predic-
tions and confidence scores still reflect whether
or not possible antecedents exist in previous sen-
tences that match in gender/number, and so may
still be useful for coherence evaluation.
Cosine similarity We use cosine similarity to
compute the overlap of words in adjacent sen-
tences si and si+1 as a measure of continuity.
cos? =
vsi .vsi+1
||vsi ||||vsi+1 ||
(1)
The dimensions of the two vectors (vsi and
vsi+1) are the total number of word types from
both sentences si and si+1. Stop words were re-
tained. The value of each dimension for a sentence
is the number of tokens of that word type in that
sentence. We compute the min, max, and average
value of cosine similarity over the entire summary.
While some repetition is beneficial for cohe-
sion, too much repetition leads to redundancy in
the summary. Cosine similarity is thus indicative
of both continuity and redundancy.
3.6 Sentence fluency: Chae and Nenkova
(2009)
We test the usefulness of a suite of 38 shallow
syntactic features studied by Chae and Nenkova
(2009). These features are weakly but signif-
icantly correlated with the fluency of machine
translated sentences. These include sentence
length, number of fragments, average lengths of
the different types of syntactic phrases, total length
of modifiers in noun phrases, and various other
syntactic features. We expect that these structural
features will be better at detecting ungrammatical
sentences than the local language model features.
Since all of these features are calculated over in-
dividual sentences, we use the average value over
all the sentences in a summary in our experiments.
3.7 Coh-Metrix: Graesser et al (2004)
The Coh-Metrix tool5 provides an implementation
of 54 features known in the psycholinguistic lit-
erature to correlate with the coherence of human-
written texts (Graesser et al, 2004). These include
commonly used readability metrics based on sen-
tence length and number of syllables in constituent
words. Other measures implemented in the sys-
tem are surface text properties known to contribute
to text processing difficulty. Also included are
measures of cohesion between adjacent sentences
such as similarity under a latent semantic analysis
(LSA) model (Deerwester et al, 1990), stem and
content word overlap, syntactic similarity between
adjacent sentences, and use of discourse connec-
tives. Coh-Metrix has been designed with the
goal of capturing properties of coherent text and
has been used for grade level assessment, predict-
ing student essay grades, and various other tasks.
Given the heterogeneity of features in this class,
we expect that they will provide reasonable accu-
racies for all the linguistic quality measures. In
particular, the overlap features might serve as a
measure of redundancy and local coherence.
5http://cohmetrix.memphis.edu/
547
3.8 Word coherence: Soricut and Marcu
(2006)
Word co-occurrence patterns across adjacent sen-
tences provide a way of measuring local coherence
that is not linguistically informed but which can
be easily computed using large amounts of unan-
notated text (Lapata, 2003; Soricut and Marcu,
2006). Word coherence can be considered as the
analog of language models at the inter-sentence
level. Specifically, we used the two features in-
troduced by Soricut and Marcu (2006).
Soricut and Marcu (2006) make an analogy to
machine translation: two words are likely to be
translations of each other if they often appear in
parallel sentences; in texts, two words are likely to
signal local coherence if they often appear in ad-
jacent sentences. The two features we computed
are forward likelihood, the likelihood of observ-
ing the words in sentence si conditioned on si?1,
and backward likelihood, the likelihood of observ-
ing the words in sentence si conditioned on sen-
tence si+1. ?Parallel texts? of 5 million adjacent
sentences were extracted from the NYT section of
GigaWord. We used the GIZA++6 implementa-
tion of IBM Model 1 to align the words in adjacent
sentences and obtain all relevant probabilities.
3.9 Entity coherence: Barzilay and Lapata
(2008)
Linguistic theories, and Centering theory (Grosz
et al, 1995) in particular, have hypothesized that
the properties of the transition of attention from
entities in one sentence to those in the next, play a
major role in the determination of local coherence.
Barzilay and Lapata (2008), inspired by Center-
ing, proposed a method to compute the local co-
herence of texts on the basis of the sequences of
entity mentions appearing in them.
In their Entity Grid model, a text is represented
by a matrix with rows corresponding to each sen-
tence in a text, and columns to each entity men-
tioned anywhere in the text. The value of a cell
in the grid is the entity?s grammatical role in that
sentence (Subject, Object, Neither, or Absent). An
entity transition is a particular entity?s role in two
adjacent sentences. The actual entity coherence
features are the fraction of each type of these tran-
sitions in the entire entity grid for the text. One
would expect that coherent texts would contain
a certain distribution of entity transitions which
6http://www.fjoch.com/GIZA++.html
would differ from those in incoherent sequences.
We use the Brown Coherence Toolkit7 (Elsner
et al, 2007) to construct the grids. The tool does
not perform full coreference resolution. Instead,
noun phrases are considered to refer to the same
entity if their heads are identical.
Entity coherence features are the only ones that
have been previously applied with success for pre-
dicting summary coherence. They can therefore
be considered to be the state-of-the-art approach
for automatic evaluation of linguistic quality.
4 Summarization data
For our experiments, we use data from the
multi-document summarization tasks of the Doc-
ument Understanding Conference (DUC) work-
shops (Over et al, 2007).
Our training and development data comes from
DUC 2006 and our test data from DUC 2007.
These were the most recent years in which the
summaries were evaluated according to specific
linguistic quality questions. Each input consists
of a set of 25 related documents on a topic and the
target length of summaries is 250 words.
In DUC 2006, there were 50 inputs to be sum-
marized and 35 summarization systems which par-
ticipated in the evaluation. This included 34 au-
tomatic systems submitted by participants, and a
baseline system that simply extracted the lead-
ing sentences from the most recent article. In
DUC 2007, there were 45 inputs and 32 different
summarization systems. Apart from the leading
sentences baseline, a high performance automatic
summarizer from a previous year was also used
as a baseline. All these automatic systems are in-
cluded in our evaluation experiments.
4.1 System performance on linguistic quality
Each summary was evaluated according to the
five linguistic quality questions introduced in Sec-
tion 2: grammaticality, non-redundancy, referen-
tial clarity, focus, and structure. For each of these
questions, all summaries were manually rated on a
scale from 1 to 5, in which 5 is the best.
The distributions of system scores in the 2006
data are shown in Figure 1. Systems are currently
the worst at structure, middling at referential clar-
ity, and relatively better at grammaticality, focus,
7http://www.cs.brown.edu/
?
melsner/
manual.html
548
Figure 1: Distribution of system scores on the five
linguistic quality questions
Gram Non-redun Ref Focus Struct
Content .02 -.40 * .29 .28 .09
Gram .38 * .25 .24 .54 *
Non-redun -.07 -.09 .27
Ref .89 * .76 *
Focus .80 *
Table 1: Spearman correlations between the man-
ual ratings for systems averaged over the 50 inputs
in 2006; * p < .05
and non-redundancy. Structure is the aspect of lin-
guistic quality where there is the most room for
improvement. The only system with an average
structure score above 3.5 in DUC 2006 was the
leading sentences baseline system.
As can be expected, people are unlikely to be
able to focus on a single aspect of linguistic quality
exclusively while ignoring the rest. Some of the
linguistic quality ratings are significantly corre-
lated with each other, particularly referential clar-
ity, focus, and structure (Table 1).
More importantly, the systems that produce
summaries with good content8 are not necessar-
ily the systems producing the most readable sum-
maries. Notice from the first row of Table 1 that
none of the system rankings based on these mea-
sures of linguistic quality are significantly posi-
tively correlated with system rankings of content.
The development of automatic linguistic quality
measurements will allow researchers to optimize
both content and linguistic quality.
8as measured by summary responsiveness ratings on a 1
to 5 scale, without regard to linguistic quality
5 Experimental setup
We use the summaries from DUC 2006 for train-
ing and feature development and DUC 2007
served as the test set. Validating the results on con-
secutive years of evaluation is important, as results
that hold for the data in one year might not carry
over to the next, as happened for example in Con-
roy and Dang (2008)?s work.
Following Barzilay and Lapata (2008), we re-
port summary ranking accuracy as the fraction of
correct pairwise rankings in the test set.
We use a Ranking SVM (SV M light (Joachims,
2002)) to score summaries using our features. The
Ranking SVM seeks to minimize the number of
discordant pairs (pairs in which the gold stan-
dard has x1 ranked strictly higher than x2, but the
learner ranks x2 strictly higher than x1). The out-
put of the ranker is always a real valued score, so a
global rank order is always obtained. The default
regularization parameter was used.
5.1 Combining predictions
To combine information from the different feature
classes, we train a meta ranker using the predic-
tions from each class as features.
First, we use a leave-one out (jackknife) pro-
cedure to get the predictions of our features for
the entire 2006 data set. To predict rankings of
systems on one input, we train all the individual
rankers, one for each of the classes of features in-
troduced above, on data from the remaining in-
puts. We then apply these rankers to the sum-
maries produced for the held-out input. By repeat-
ing this process for each input in turn, we obtain
the predicted scores for each summary.
Once this is done, we use these predicted scores
as features for the meta ranker, which is trained on
all 2006 data. To test on a new summary pair in
2007, we first apply each individual ranker to get
its predictions, and then apply the meta ranker.
In either case (meta ranker or individual feature
class), all training is performed on 2006 data, and
all testing is done on 2007 data which guarantees
the results generalize well at least from one year
of evaluation to the next.
5.2 Evaluation of rankings
We examine the predictive power of our features
for each of the five linguistic quality questions in
two settings. In system-level evaluation, we would
like to rank all participating systems according to
549
their performance on the entire test set. In input-
level evaluation, we would like to rank all sum-
maries produced for a single given input.
For input-level evaluation, the pairs are formed
from summaries of the same input. Pairs in which
the gold standard ratings are tied are not included.
After removing the ties, the test set consists of 13K
to 16K pairs for each linguistic quality question.
Note that there were 45 inputs and 32 automatic
systems in DUC 2007. So, there are a total of
45?
(32
2
)
= 22, 320 possible summary pairs.
For system-level evaluation, we treat the real-
valued output of the SVM ranker for each sum-
mary as the linguistic quality score. The 45 indi-
vidual scores for summaries produced by a given
system are averaged to obtain an overall score for
the system. The gold-standard system-level qual-
ity rating is equal to the average human ratings for
the system?s summaries over the 45 inputs. At the
system level, there are about 500 non-tied pairs in
the test set for each question.
For both evaluation settings, a random baseline
which ranked the summaries in a random order
would have an expected pairwise accuracy of 50%.
6 Results and discussion
6.1 System-level evaluation
System-level accuracies for each class of features
are shown in Table 2. All classes of features per-
form well, with at least a 20% absolute increase
in accuracy over the random baseline (50% ac-
curacy). For each of the linguistic quality ques-
tions, the corresponding best class of features
gives prediction accuracies around 90%. In other
words, if these features were used to fully auto-
matically compare systems that participated in the
2007 DUC evaluation, only one out of ten com-
parisons would have been incorrect. These results
set a high standard for future work on automatic
system-level evaluation of linguistic quality.
The state-of-the-art entity coherence features
perform well but are not the best for any of the five
aspects of linguistic quality. As expected, sentence
fluency is the best feature class for grammatical-
ity. For all four other questions, the best feature
set is Continuity, which is a combination of sum-
marization specific features, coreference features
and cosine similarity of adjacent sentences. Conti-
nuity features outperform entity coherence by 3 to
4% absolute difference on referential quality, fo-
cus, and coherence. Accuracies from the language
Feature set Gram. Redun. Ref. Focus Struct.
Lang. models 87.6 83.0 91.2 85.2 86.3
Named ent. 78.5 83.6 82.1 74.0 69.6
NP syntax 85.0 83.8 87.0 76.6 79.2
Coh. devices 82.1 79.5 82.7 82.3 83.7
Continuity 88.8 88.5 92.9 89.2 91.4
Sent. fluency 91.7 78.9 87.6 82.3 84.9
Coh-Metrix 87.2 86.0 88.6 83.9 86.3
Word coh. 81.7 76.0 87.8 81.7 79.0
Entity coh. 90.2 88.1 89.6 85.0 87.1
Meta ranker 92.9 87.9 91.9 87.8 90.0
Table 2: System-level prediction accuracies (%)
model features are within 1% of entity coherence
for these three aspects of summary quality.
Coh-Metrix, which has been proposed as a com-
prehensive characterization of text, does not per-
form as well as the language model and the en-
tity coherence classes, which contain considerably
fewer features related to only one aspect of text.
The classes of features specific to named enti-
ties and noun phrase syntax are the weakest pre-
dictors. It is apparent from the results that conti-
nuity, entity coherence, sentence fluency and lan-
guage models are the most powerful classes of fea-
tures that should be used in automation of evalu-
ation and against which novel predictors of text
quality should be compared.
Combining all feature classes with the meta
ranker only yields higher results for grammatical-
ity. For the other aspects of linguistic quality, it is
better to use Continuity by itself to rank systems.
One certainly unexpected result is that features
designed to capture one aspect of well-written text
turn out to perform well for other questions as
well. For instance, entity coherence and continuity
features predict grammaticality with very high ac-
curacy of around 90%, and are surpassed only by
the sentence fluency features. These findings war-
rant further investigation because we would not
expect characteristics of local transitions indica-
tive of text structure to have anything to do with
sentence grammaticality or fluency. The results
are probably due to the significant correlation be-
tween structure and grammaticality (Table 1).
6.2 Input-level evaluation
The results of the input-level ranking experiments
are shown in Table 3. Understandably, input-
level prediction is more difficult and the results are
lower compared to the system-level predictions:
even with wrong predictions for some of the sum-
maries by two systems, the overall judgment that
550
one system is better than the other over the entire
test set can still be accurate.
While for system-level predictions the meta
ranker was only useful for grammaticality, at the
input level it outperforms every individual feature
class for each of the five questions, obtaining ac-
curacies around 70%.
These input-level accuracies compare favorably
with automatic evaluation metrics for other nat-
ural language processing tasks. For example, at
the 2008 ACL Workshop on Statistical Machine
Translation, all fifteen automatic evaluation met-
rics, including variants of BLEU scores, achieved
between 42% and 56% pairwise accuracy with hu-
man judgments at the sentence level (Callison-
Burch et al, 2008).
As in system-level prediction, for referential
clarity, focus, and structure, the best feature class
is Continuity. Sentence fluency again is the best
class for identifying grammaticality.
Coh-Metrix features are now best for determin-
ing redundancy. Both Coh-Metrix and Continuity
(the top two features for redundancy) include over-
lap measures between adjacent sentences, which
serve as a good proxy for redundancy.
Surprisingly, the relative performance of the
feature classes at input level is not the same as
for system-level prediction. For example, the lan-
guage model features, which are the second best
class for the system-level, do not fare as well at
the input-level. Word co-occurrence which ob-
tained good accuracies at the system level is the
least useful class at the input level with accuracies
just above chance in all cases.
6.3 Components of continuity
The class of features capturing sentence-to-
sentence continuity in the summary (Section 3.5)
are the most effective for predicting referential
clarity, focus, and structure at the input level.
We now investigate to what extent each of its
components?summary-specific features, corefer-
ence, and cosine similarity between adjacent
sentences?contribute to performance.
Results obtained after excluding each of the
components of continuity is shown in Table 4;
each line in the table represents Continuity mi-
nus a feature subclass. Removing cosine over-
lap causes the largest drop in prediction accuracy,
with results about 10% lower than those for the
complete Continuity class. Summary specific fea-
Feature set Gram. Redun. Ref. Focus Struct.
Lang. models 66.3 57.6 62.2 60.5 62.5
Named ent. 52.9 54.4 60.0 54.1 52.5
NP Syntax 59.0 50.8 59.1 54.5 55.1
Coh. devices 56.8 54.4 55.2 52.7 53.6
Continuity 61.7 62.5 69.7 65.4 70.4
Sent. fluency 69.4 52.5 64.4 61.9 62.6
Coh-Metrix 65.5 67.6 67.9 63.0 62.4
Word coh. 54.7 55.5 53.3 53.2 53.7
Entity coh. 61.3 62.0 64.3 64.2 63.6
Meta ranker 71.0 68.6 73.1 67.4 70.7
Table 3: Input-level prediction accuracies (%)
tures, which compare the context of a sentence
in the summary with the context in the original
document where it appeared, also contribute sub-
stantially to the success of the Continuity class in
predicting structure and referential clarity. Accu-
racies drop by about 7% when these features are
excluded. However, the coreference features do
not seem to contribute much towards predicting
summary linguistic quality. The accuracies of the
Continuity class are not affected at all when these
coreference features are not included.
6.4 Impact of summarization methods
In this paper, we have discussed an analysis of the
outputs of current research systems. Almost all
of these systems still use extractive methods. The
summarization specific continuity features reward
systems that include the necessary preceding con-
text from the original document. These features
have high prediction accuracies (Section 6.3) of
linguistic quality, however note that the support-
ing context could often contain less important con-
tent. Therefore, there is a tension between strate-
gies for optimizing linguistic quality and for op-
timizing content, which warrants the development
of abstractive methods.
As the field moves towards more abstractive
summaries, we expect to see differences in both
a) summary linguistic quality and b) the features
predictive of linguistic aspects.
As discussed in Section 4.1, systems are cur-
rently worst at structure/coherence. However,
grammaticality will become more of an issue as
systems use sentence compression (Knight and
Marcu, 2002), reference rewriting (Nenkova and
McKeown, 2003), and other techniques to produce
their own sentences.
The number of discourse connectives is cur-
rently significantly negatively correlated with
structure/coherence (Spearman correlation of r =
551
Ref. Focus Struct.
Continuity 69.7 65.4 70.4
- Sum-specific 63.9 64.2 63.5
- Coref 70.1 65.2 70.6
- Cosine 60.2 56.6 60.7
Table 4: Ablation within the Continuity class;
pairwise accuracy for input-level predictions (%)
-.06, p = .008 on DUC 2006 system summaries).
This can be explained by the fact that they of-
ten lack proper context in an extractive summary.
However, an abstractive system could plan a dis-
course structure and insert appropriate connectives
(Saggion, 2009). In this case, we would expect the
presence of discourse connectives to be a mark of
a well-written summary.
6.5 Results on human-written abstracts
Since abstractive summaries would have markedly
different properties from extracts, it would be in-
teresting to know how well these sets of features
would work for predicting the quality of machine-
produced abstracts. However, since current sys-
tems are extractive, such a data set is not available.
Therefore we experiment on human-written ab-
stracts to get an estimate of the expected per-
formance of our features on abstractive system
summaries. In both DUC 2006 and DUC 2007,
ten NIST assessors wrote summaries for the var-
ious inputs. There are four human-written sum-
maries for each input and these summaries were
judged on the same five linguistic quality aspects
as the machine-written summaries. We train on the
human-written summaries from DUC 2006 and
test on the human-written summaries from DUC
2007, using the same set-up as in Section 5.
These results are shown in Table 5. We only re-
port results on the input level, as we are interested
in distinguishing between the quality of the sum-
maries, not the NIST assessors? writing skills.
Except for grammaticality, the prediction accu-
racies of the best feature classes for human ab-
stracts are better than those at input level for ma-
chine extracts. This result is promising, as it shows
that similar features for evaluating linguistic qual-
ity will be valid for abstractive summaries as well.
Note however that the relative performance of
the feature sets changes between the machine and
human results. While for the machines Continu-
ity feature class is the best predictor of referential
clarity, focus, and structure (Table 3), for humans,
language models and sentence fluency are best for
Feature set Gram. Redun. Ref. Focus Struct.
Lang. models 52.1 60.8 76.5 71.9 78.4
Named ent. 62.5 66.7 47.1 43.9 59.1
NP Syntax 64.6 49.0 43.1 49.1 58.0
Coh. devices 54.2 68.6 66.7 49.1 64.8
Continuity 54.2 49.0 62.7 61.4 71.6
Sent. fluency 54.2 64.7 80.4 71.9 72.7
Coh-Metrix 54.2 52.9 68.6 56.1 69.3
Word coh. 62.5 58.8 62.7 70.2 60.2
Entity coh. 45.8 49.0 54.9 52.6 56.8
Meta ranker 62.5 56.9 80.4 50.9 67.0
Table 5: Input-level prediction accuracies for
human-written summaries (%)
these three aspects of linguistic quality. A possi-
ble explanation for this difference could be that in
system-produced extracts, incoherent organization
influences human perception of linguistic quality
to a great extent and so local coherence features
turned out very predictive. But in human sum-
maries, sentences are clearly well-organized and
here, continuity features appear less useful. Sen-
tence level fluency seems to be more predictive of
the linguistic quality of these summaries.
7 Conclusion
We have presented an analysis of a wide variety
of features for the linguistic quality of summaries.
Continuity between adjacent sentences was con-
sistently indicative of the quality of machine gen-
erated summaries. Sentence fluency was useful for
identifying grammaticality. Language model and
entity coherence features also performed well and
should be considered in future endeavors for auto-
matic linguistic quality evaluation.
The high prediction accuracies for input-level
evaluation and the even higher accuracies for
system-level evaluation confirm that questions re-
garding the linguistic quality of summaries can be
answered reasonably using existing computational
techniques. Automatic evaluation will make test-
ing easier during system development and enable
reporting results obtained outside of the cycles of
NIST evaluation.
Acknowledgments
This material is based upon work supported under
a National Science Foundation Graduate Research
Fellowship and NSF CAREER award 0953445.
We would like to thank Bonnie Webber for pro-
ductive discussions.
552
References
R. Barzilay and M. Lapata. 2008. Modeling local co-
herence: An entity-based approach. Computational
Linguistics, 34(1):1?34.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2008. Further meta-evaluation of ma-
chine translation. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 70?
106.
J. Chae and A. Nenkova. 2009. Predicting the fluency
of text with shallow structural features: case studies
of machine translation and human-written text. In
Proceedings of EACL, pages 139?147.
E. Charniak and M. Elsner. 2009. EM works for pro-
noun anaphora resolution. In Proceedings of EACL,
pages 148?156.
J.M. Conroy and H.T. Dang. 2008. Mind the gap: dan-
gers of divorcing evaluations of summary content
from linguistic quality. In Proceedings of COLING,
pages 145?152.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society
for Information Science, 41:391?407.
M. Elsner and E. Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of
ACL/HLT: Short Papers, pages 41?44.
M. Elsner, J. Austerweil, and E. Charniak. 2007. A
unified local and global model for discourse coher-
ence. In Proceedings of NAACL/HLT.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of ACL, pages 363?370.
K. Fraurud. 1990. Definiteness and the processing of
noun phrases in natural discourse. Journal of Se-
mantics, 7(4):395.
A.C. Graesser, D.S. McNamara, M.M. Louwerse, and
Z. Cai. 2004. Coh-Metrix: Analysis of text on co-
hesion and language. Behavior Research Methods
Instruments and Computers, 36(2):193?202.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering:
a framework for modelling the local coherence of
discourse. Computational Linguistics, 21(2):203?
226.
K.F. Haberlandt and A.C. Graesser. 1985. Component
processes in text comprehension and some of their
interactions. Journal of Experimental Psychology:
General, 114(3):357?374.
M.A.K. Halliday and R. Hasan. 1976. Cohesion in
English. Longman Group Ltd, London, U.K.
T. Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133?142.
M.A. Just and P.A. Carpenter. 1987. The psychology
of reading and language comprehension. Allyn and
Bacon Boston, MA.
D. Klein and C.D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL, pages 423?
430.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91?107.
M. Lapata and R. Barzilay. 2005. Automatic evalua-
tion of text coherence: Models and representations.
In International Joint Conference On Artificial In-
telligence, volume 19, page 1085.
M. Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings
of ACL, pages 545?552.
C.Y. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of NAACL/HLT, page 78.
C.Y. Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. In Proceedings of the Work-
shop on Text Summarization Branches Out (WAS
2004), pages 25?26.
A. Nenkova and K. McKeown. 2003. References to
named entities: a corpus study. In Proceedings of
HLT/NAACL 2003 (short paper).
J. Otterbacher, D. Radev, and A. Luo. 2002. Revi-
sions that improve cohesion in multi-document sum-
maries: a preliminary study. In Proceedings of the
Workshop on Automatic Summarization, ACL.
P. Over, H. Dang, and D. Harman. 2007. Duc
in context. Information Processing Management,
43(6):1506?1520.
C.D. Paice. 1980. The automatic generation of litera-
ture abstracts: an approach based on the identifica-
tion of self-indicating phrases. In Proceedings of the
3rd annual ACM conference on Research and devel-
opment in information retrieval, pages 172?191.
C.D. Paice. 1990. Constructing literature abstracts by
computer: Techniques and prospects. Information
Processing Management, 26(1):171?186.
E.F. Prince. 1981. Toward a taxonomy of given-new
information. Radical pragmatics, 223:255.
H. Saggion. 2009. A Classification Algorithm for Pre-
dicting the Structure of Summaries. Proceedings
of the 2009 Workshop on Language Generation and
Summarisation, page 31.
553
R. Soricut and D. Marcu. 2006. Discourse generation
using utility-trained coherence models. In Proceed-
ings of ACL.
J. Steinberger, M. Poesio, M.A. Kabadjov, and K. Jeek.
2007. Two uses of anaphora resolution in sum-
marization. Information Processing Management,
43(6):1663?1680.
A. Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing, volume 3.
554
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865?874,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Creating Robust Supervised Classifiers via Web-Scale N-gram Data
Shane Bergsma
University of Alberta
sbergsma@ualberta.ca
Emily Pitler
University of Pennsylvania
epitler@seas.upenn.edu
Dekang Lin
Google, Inc.
lindek@google.com
Abstract
In this paper, we systematically assess the
value of using web-scale N-gram data in
state-of-the-art supervised NLP classifiers.
We compare classifiers that include or ex-
clude features for the counts of various
N-grams, where the counts are obtained
from a web-scale auxiliary corpus. We
show that including N-gram count features
can advance the state-of-the-art accuracy
on standard data sets for adjective order-
ing, spelling correction, noun compound
bracketing, and verb part-of-speech dis-
ambiguation. More importantly, when op-
erating on new domains, or when labeled
training data is not plentiful, we show that
using web-scale N-gram features is essen-
tial for achieving robust performance.
1 Introduction
Many NLP systems use web-scale N-gram counts
(Keller and Lapata, 2003; Nakov and Hearst,
2005; Brants et al, 2007). Lapata and Keller
(2005) demonstrate good performance on eight
tasks using unsupervised web-based models. They
show web counts are superior to counts from a
large corpus. Bergsma et al (2009) propose un-
supervised and supervised systems that use counts
from Google?s N-gram corpus (Brants and Franz,
2006). Web-based models perform particularly
well on generation tasks, where systems choose
between competing sequences of output text (such
as different spellings), as opposed to analysis
tasks, where systems choose between abstract la-
bels (such as part-of-speech tags or parse trees).
In this work, we address two natural and related
questions which these previous studies leave open:
1. Is there a benefit in combining web-scale
counts with the features used in state-of-the-
art supervised approaches?
2. How well do web-based models perform on
new domains or when labeled data is scarce?
We address these questions on two generation
and two analysis tasks, using both existing N-gram
data and a novel web-scale N-gram corpus that
includes part-of-speech information (Section 2).
While previous work has combined web-scale fea-
tures with other features in specific classification
problems (Modjeska et al, 2003; Yang et al,
2005; Vadas and Curran, 2007b), we provide a
multi-task, multi-domain comparison.
Some may question why supervised approaches
are needed at all for generation problems. Why
not solely rely on direct evidence from a giant cor-
pus? For example, for the task of prenominal ad-
jective ordering (Section 3), a system that needs
to describe a ball that is both big and red can sim-
ply check that big red is more common on the web
than red big, and order the adjectives accordingly.
It is, however, suboptimal to only use N-gram
data. For example, ordering adjectives by direct
web evidence performs 7% worse than our best
supervised system (Section 3.2). No matter how
large the web becomes, there will always be plau-
sible constructions that never occur. For example,
there are currently no pages indexed by Google
with the preferred adjective ordering for bedrag-
gled 56-year-old [professor]. Also, in a particu-
lar domain, words may have a non-standard usage.
Systems trained on labeled data can learn the do-
main usage and leverage other regularities, such as
suffixes and transitivity for adjective ordering.
With these benefits, systems trained on labeled
data have become the dominant technology in aca-
demic NLP. There is a growing recognition, how-
ever, that these systems are highly domain de-
pendent. For example, parsers trained on anno-
tated newspaper text perform poorly on other gen-
res (Gildea, 2001). While many approaches have
adapted NLP systems to specific domains (Tsu-
ruoka et al, 2005; McClosky et al, 2006; Blitzer
865
et al, 2007; Daume? III, 2007; Rimell and Clark,
2008), these techniques assume the system knows
on which domain it is being used, and that it has
access to representative data in that domain. These
assumptions are unrealistic in many real-world sit-
uations; for example, when automatically process-
ing a heterogeneous collection of web pages. How
well do supervised and unsupervised NLP systems
perform when used uncustomized, out-of-the-box
on new domains, and how can we best design our
systems for robust open-domain performance?
Our results show that using web-scale N-gram
data in supervised systems advances the state-of-
the-art performance on standard analysis and gen-
eration tasks. More importantly, when operating
out-of-domain, or when labeled data is not plen-
tiful, using web-scale N-gram data not only helps
achieve good performance ? it is essential.
2 Experiments and Data
2.1 Experimental Design
We evaluate the benefit of N-gram data on multi-
class classification problems. For each task, we
have some labeled data indicating the correct out-
put for each example. We evaluate with accuracy:
the percentage of examples correctly classified in
test data. We use one in-domain and two out-of-
domain test sets for each task. Statistical signifi-
cance is assessed with McNemar?s test, p<0.01.
We provide results for unsupervised approaches
and the majority-class baseline for each task.
For our supervised approaches, we represent the
examples as feature vectors, and learn a classi-
fier on the training vectors. There are two fea-
ture classes: features that use N-grams (N-GM)
and those that do not (LEX). N-GM features are
real-valued features giving the log-count of a par-
ticular N-gram in the auxiliary web corpus. LEX
features are binary features that indicate the pres-
ence or absence of a particular string at a given po-
sition in the input. The name LEX emphasizes that
they identify specific lexical items. The instantia-
tions of both types of features depend on the task
and are described in the corresponding sections.
Each classifier is a linear Support Vector Ma-
chine (SVM), trained using LIBLINEAR (Fan et al,
2008) on the standard domain. We use the one-vs-
all strategy when there are more than two classes
(in Section 4). We plot learning curves to mea-
sure the accuracy of the classifier when the num-
ber of labeled training examples varies. The size
of the N-gram data and its counts remain constant.
We always optimize the SVM?s (L2) regulariza-
tion parameter on the in-domain development set.
We present results with L2-SVM, but achieve sim-
ilar results with L1-SVM and logistic regression.
2.2 Tasks and Labeled Data
We study two generation tasks: prenominal ad-
jective ordering (Section 3) and context-sensitive
spelling correction (Section 4), followed by two
analysis tasks: noun compound bracketing (Sec-
tion 5) and verb part-of-speech disambiguation
(Section 6). In each section, we provide refer-
ences to the origin of the labeled data. For the
out-of-domain Gutenberg and Medline data used
in Sections 3 and 4, we generate examples our-
selves.1 We chose Gutenberg and Medline in order
to provide challenging, distinct domains from our
training corpora. Our Gutenberg corpus consists
of out-of-copyright books, automatically down-
loaded from the Project Gutenberg website.2 The
Medline data consists of a large collection of on-
line biomedical abstracts. We describe how la-
beled adjective and spelling examples are created
from these corpora in the corresponding sections.
2.3 Web-Scale Auxiliary Data
The most widely-used N-gram corpus is the
Google 5-gram Corpus (Brants and Franz, 2006).
For our tasks, we also use Google V2: a new
N-gram corpus (also with N-grams of length one-
to-five) that we created from the same one-trillion-
word snapshot of the web as the Google 5-gram
Corpus, but with several enhancements. These in-
clude: 1) Reducing noise by removing duplicate
sentences and sentences with a high proportion
of non-alphanumeric characters (together filtering
about 80% of the source data), 2) pre-converting
all digits to the 0 character to reduce sparsity for
numeric expressions, and 3) including the part-of-
speech (POS) tag distribution for each N-gram.
The source data was automatically tagged with
TnT (Brants, 2000), using the Penn Treebank tag
set. Lin et al (2010) provide more details on the
1http://webdocs.cs.ualberta.ca/?bergsma/Robust/
provides our Gutenberg corpus, a link to Medline, and also
the generated examples for both Gutenberg and Medline.
2
www.gutenberg.org. All books just released in 2009 and
thus unlikely to occur in the source data for our N-gram cor-
pus (from 2006). Of course, with removal of sentence dupli-
cates and also N-gram thresholding, the possible presence of
a test sentence in the massive source data is unlikely to affect
results. Carlson et al (2008) reach a similar conclusion.
866
N-gram data and N-gram search tools.
The third enhancement is especially relevant
here, as we can use the POS distribution to collect
counts for N-grams of mixed words and tags. For
example, we have developed an N-gram search en-
gine that can count how often the adjective un-
precedented precedes another adjective in our web
corpus (113K times) and how often it follows one
(11K times). Thus, even if we haven?t seen a par-
ticular adjective pair directly, we can use the posi-
tional preferences of each adjective to order them.
Early web-based models used search engines to
collect N-gram counts, and thus could not use cap-
italization, punctuation, and annotations such as
part-of-speech (Kilgarriff and Grefenstette, 2003).
Using a POS-tagged web corpus goes a long way
to addressing earlier criticisms of web-based NLP.
3 Prenominal Adjective Ordering
Prenominal adjective ordering strongly affects text
readability. For example, while the unprecedented
statistical revolution is fluent, the statistical un-
precedented revolution is not. Many NLP systems
need to handle adjective ordering robustly. In ma-
chine translation, if a noun has two adjective mod-
ifiers, they must be ordered correctly in the tar-
get language. Adjective ordering is also needed
in Natural Language Generation systems that pro-
duce information from databases; for example, to
convey information (in sentences) about medical
patients (Shaw and Hatzivassiloglou, 1999).
We focus on the task of ordering a pair of adjec-
tives independently of the noun they modify and
achieve good performance in this setting. Follow-
ing the set-up of Malouf (2000), we experiment
on the 263K adjective pairs Malouf extracted from
the British National Corpus (BNC). We use 90%
of pairs for training, 5% for testing, and 5% for
development. This forms our in-domain data.3
We create out-of-domain examples by tokeniz-
ing Medline and Gutenberg (Section 2.2), then
POS-tagging them with CRFTagger (Phan, 2006).
We create examples from all sequences of two ad-
jectives followed by a noun. Like Malouf (2000),
we assume that edited text has adjectives ordered
fluently. We extract 13K and 9.1K out-of-domain
pairs from Gutenberg and Medline, respectively.4
3BNC is not a domain per se (rather a balanced corpus),
but has a style and vocabulary distinct from our OOD data.
4Like Malouf (2000), we convert our pairs to lower-case.
Since the N-gram data includes case, we merge counts from
the upper and lower case combinations.
The input to the system is a pair of adjectives,
(a1, a2), ordered alphabetically. The task is to
classify this order as correct (the positive class) or
incorrect (the negative class). Since both classes
are equally likely, the majority-class baseline is
around 50% on each of the three test sets.
3.1 Supervised Adjective Ordering
3.1.1 LEX features
Our adjective ordering model with LEX features is
a novel contribution of this paper.
We begin with two features for each pair: an in-
dicator feature for a1, which gets a feature value of
+1, and an indicator feature for a2, which gets a
feature value of ?1. The parameters of the model
are therefore weights on specific adjectives. The
higher the weight on an adjective, the more it is
preferred in the first position of a pair. If the alpha-
betic ordering is correct, the weight on a1 should
be higher than the weight on a2, so that the clas-
sifier returns a positive score. If the reverse order-
ing is preferred, a2 should receive a higher weight.
Training the model in this setting is a matter of as-
signing weights to all the observed adjectives such
that the training pairs are maximally ordered cor-
rectly. The feature weights thus implicitly produce
a linear ordering of all observed adjectives. The
examples can also be regarded as rank constraints
in a discriminative ranker (Joachims, 2002). Tran-
sitivity is achieved naturally in that if we correctly
order pairs a ? b and b ? c in the training set,
then a ? c by virtue of the weights on a and c.
While exploiting transitivity has been shown
to improve adjective ordering, there are many
conflicting pairs that make a strict linear order-
ing of adjectives impossible (Malouf, 2000). We
therefore provide an indicator feature for the pair
a1a2, so the classifier can memorize exceptions
to the linear ordering, breaking strict order tran-
sitivity. Our classifier thus operates along the lines
of rankers in the preference-based setting as de-
scribed in Ailon and Mohri (2008).
Finally, we also have features for all suffixes of
length 1-to-4 letters, as these encode useful infor-
mation about adjective class (Malouf, 2000). Like
the adjective features, the suffix features receive a
value of +1 for adjectives in the first position and
?1 for those in the second.
3.1.2 N-GM features
Lapata and Keller (2005) propose a web-based
approach to adjective ordering: take the most-
867
System IN O1 O2
Malouf (2000) 91.5 65.6 71.6
web c(a1, a2) vs. c(a2, a1) 87.1 83.7 86.0
SVM with N-GM features 90.0 85.8 88.5
SVM with LEX features 93.0 70.0 73.9
SVM with N-GM + LEX 93.7 83.6 85.4
Table 1: Adjective ordering accuracy (%). SVM
and Malouf (2000) trained on BNC, tested on
BNC (IN), Gutenberg (O1), and Medline (O2).
frequent order of the words on the web, c(a1, a2)
vs. c(a2, a1). We adopt this as our unsupervised
approach. We merge the counts for the adjectives
occurring contiguously and separated by a comma.
These are indubitably the most important N-GM
features; we include them but also other, tag-based
counts from Google V2. Raw counts include cases
where one of the adjectives is not used as a mod-
ifier: ?the special present was? vs. ?the present
special issue.? We include log-counts for the
following, more-targeted patterns:5 c(a1 a2 N.*),
c(a2 a1 N.*), c(DT a1 a2 N.*), c(DT a2 a1 N.*).
We also include features for the log-counts of
each adjective preceded or followed by a word
matching an adjective-tag: c(a1 J.*), c(J.* a1),
c(a2 J.*), c(J.* a2). These assess the positional
preferences of each adjective. Finally, we include
the log-frequency of each adjective. The more fre-
quent adjective occurs first 57% of the time.
As in all tasks, the counts are features in a clas-
sifier, so the importance of the different patterns is
weighted discriminatively during training.
3.2 Adjective Ordering Results
In-domain, with both feature classes, we set a
strong new standard on this data: 93.7% accuracy
for the N-GM+LEX system (Table 1). We trained
and tested Malouf (2000)?s program on our data;
our LEX classifier, which also uses no auxiliary
corpus, makes 18% fewer errors than Malouf?s
system. Our web-based N-GM model is also su-
perior to the direct evidence web-based approach
of Lapata and Keller (2005), scoring 90.0% vs.
87.1% accuracy. These results show the benefit
of our new lexicalized and web-based features.
Figure 1 gives the in-domain learning curve.
With fewer training examples, the systems with
N-GM features strongly outperform the LEX-only
system. Note that with tens of thousands of test
5In this notation, capital letters (and regular expressions)
are matched against tags while a1 and a2 match words.
 60
 65
 70
 75
 80
 85
 90
 95
 100
1e51e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM+LEX
N-GM
LEX
Figure 1: In-domain learning curve of adjective
ordering classifiers on BNC.
 60
 65
 70
 75
 80
 85
 90
 95
 100
1e51e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM+LEX
N-GM
LEX
Figure 2: Out-of-domain learning curve of adjec-
tive ordering classifiers on Gutenberg.
examples, all differences are highly significant.
Out-of-domain, LEX?s accuracy drops a shock-
ing 23% on Gutenberg and 19% on Medline (Ta-
ble 1). Malouf (2000)?s system fares even worse.
The overlap between training and test pairs helps
explain. While 59% of the BNC test pairs were
seen in the training corpus, only 25% of Gutenberg
and 18% of Medline pairs were seen in training.
While other ordering models have also achieved
?very poor results? out-of-domain (Mitchell,
2009), we expected our expanded set of LEX fea-
tures to provide good generalization on new data.
Instead, LEX is very unreliable on new domains.
N-GM features do not rely on specific pairs in
training data, and thus remain fairly robust cross-
domain. Across the three test sets, 84-89% of
examples had the correct ordering appear at least
once on the web. On new domains, the learned
N-GM system maintains an advantage over the un-
supervised c(a1, a2) vs. c(a2, a1), but the differ-
ence is reduced. Note that training with 10-fold
868
cross validation, the N-GM system can achieve up
to 87.5% on Gutenberg (90.0% for N-GM + LEX).
The learning curve showing performance on
Gutenberg (but still training on BNC) is particu-
larly instructive (Figure 2, performance on Med-
line is very similar). The LEX system performs
much worse than the web-based models across
all training sizes. For our top in-domain sys-
tem, N-GM + LEX, as you add more labeled ex-
amples, performance begins decreasing out-of-
domain. The system disregards the robust N-gram
counts as it is more and more confident in the LEX
features, and it suffers the consequences.
4 Context-Sensitive Spelling Correction
We now turn to the generation problem of context-
sensitive spelling correction. For every occurrence
of a word in a pre-defined set of confusable words
(like peace and piece), the system must select the
most likely word from the set, flagging possible
usage errors when the predicted word disagrees
with the original. Contextual spell checkers are
one of the most widely used NLP technologies,
reaching millions of users via compressed N-gram
models in Microsoft Office (Church et al, 2007).
Our in-domain examples are from the New York
Times (NYT) portion of Gigaword, from Bergsma
et al (2009). They include the 5 confusion sets
where accuracy was below 90% in Golding and
Roth (1999). There are 100K training, 10K devel-
opment, and 10K test examples for each confusion
set. Our results are averages across confusion sets.
Out-of-domain examples are again drawn from
Gutenberg and Medline. We extract all instances
of words that are in one of our confusion sets,
along with surrounding context. By assuming the
extracted instances represent correct usage, we la-
bel 7.8K and 56K out-of-domain test examples for
Gutenberg and Medline, respectively.
We test three unsupervised systems: 1) Lapata
and Keller (2005) use one token of context on the
left and one on the right, and output the candidate
from the confusion set that occurs most frequently
in this pattern. 2) Bergsma et al (2009) measure
the frequency of the candidates in all the 3-to-5-
gram patterns that span the confusable word. For
each candidate, they sum the log-counts of all pat-
terns filled with the candidate, and output the can-
didate with the highest total. 3) The baseline pre-
dicts the most frequent member of each confusion
set, based on frequencies in the NYT training data.
System IN O1 O2
Baseline 66.9 44.6 60.6
Lapata and Keller (2005) 88.4 78.0 87.4
Bergsma et al (2009) 94.8 87.7 94.2
SVM with N-GM features 95.7 92.1 93.9
SVM with LEX features 95.2 85.8 91.0
SVM with N-GM + LEX 96.5 91.9 94.8
Table 2: Spelling correction accuracy (%). SVM
trained on NYT, tested on NYT (IN) and out-of-
domain Gutenberg (O1) and Medline (O2).
 70
 75
 80
 85
 90
 95
 100
1e51e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM+LEX
N-GM
LEX
Figure 3: In-domain learning curve of spelling
correction classifiers on NYT.
4.1 Supervised Spelling Correction
Our LEX features are typical disambiguation fea-
tures that flag specific aspects of the context. We
have features for the words at all positions in
a 9-word window (called collocation features by
Golding and Roth (1999)), plus indicators for a
particular word preceding or following the con-
fusable word. We also include indicators for all
N-grams, and their position, in a 9-word window.
For N-GM count features, we follow Bergsma
et al (2009). We include the log-counts of all
N-grams that span the confusable word, with each
word in the confusion set filling the N-gram pat-
tern. These features do not use part-of-speech.
Following Bergsma et al (2009), we get N-gram
counts using the original Google N-gram Corpus.
While neither our LEX nor N-GM features are
novel on their own, they have, perhaps surpris-
ingly, not yet been evaluated in a single model.
4.2 Spelling Correction Results
The N-GM features outperform the LEX features,
95.7% vs. 95.2% (Table 2). Together, they
achieve a very strong 96.5% in-domain accuracy.
869
This is 2% higher than the best unsupervised ap-
proach (Bergsma et al, 2009). Web-based models
again perform well across a range of training data
sizes (Figure 3).
The error rate of LEX nearly triples on Guten-
berg and almost doubles on Medline (Table 2). Re-
moving N-GM features from the N-GM + LEX sys-
tem, errors increase around 75% on both Guten-
berg and Medline. The LEX features provide no
help to the combined system on Gutenberg, while
they do help significantly on Medline. Note the
learning curves for N-GM+LEX on Gutenberg and
Medline (not shown) do not display the decrease
that we observed in adjective ordering (Figure 2).
Both the baseline and LEX perform poorly on
Gutenberg. The baseline predicts the majority
class from NYT, but it?s not always the majority
class in Gutenberg. For example, while in NYT
site occurs 87% of the time for the (cite, sight,
site) confusion set, sight occurs 90% of the time in
Gutenberg. The LEX classifier exploits this bias as
it is regularized toward a more economical model,
but the bias does not transfer to the new domain.
5 Noun Compound Bracketing
About 70% of web queries are noun phrases (Barr
et al, 2008) and methods that can reliably parse
these phrases are of great interest in NLP. For
example, a web query for zebra hair straightener
should be bracketed as (zebra (hair straightener)),
a stylish hair straightener with zebra print, rather
than ((zebra hair) straightener), a useless product
since the fur of zebras is already quite straight.
The noun compound (NC) bracketing task is
usually cast as a decision whether a 3-word NC
has a left or right bracketing. Most approaches are
unsupervised, using a large corpus to compare the
statistical association between word pairs in the
NC. The adjacency model (Marcus, 1980) pro-
poses a left bracketing if the association between
words one and two is higher than between two
and three. The dependency model (Lauer, 1995a)
compares one-two vs. one-three. We include de-
pendency model results using PMI as the associ-
ation measure; results were lower with the adja-
cency model.
As in-domain data, we use Vadas and Curran
(2007a)?s Wall-Street Journal (WSJ) data, an ex-
tension of the Treebank (which originally left NPs
flat). We extract all sequences of three consec-
utive common nouns, generating 1983 examples
System IN O1 O2
Baseline 70.5 66.8 84.1
Dependency model 74.7 82.8 84.4
SVM with N-GM features 89.5 81.6 86.2
SVM with LEX features 81.1 70.9 79.0
SVM with N-GM + LEX 91.6 81.6 87.4
Table 3: NC-bracketing accuracy (%). SVM
trained on WSJ, tested on WSJ (IN) and out-of-
domain Grolier (O1) and Medline (O2).
 60
 65
 70
 75
 80
 85
 90
 95
 100
1e310010
Ac
cu
ra
cy
 (%
)
Number of labeled examples
N-GM+LEX
N-GM
LEX
Figure 4: In-domain NC-bracketer learning curve
from sections 0-22 of the Treebank as training, 72
from section 24 for development and 95 from sec-
tion 23 as a test set. As out-of-domain data, we
use 244 NCs from Grolier Encyclopedia (Lauer,
1995a) and 429 NCs from Medline (Nakov, 2007).
The majority class baseline is left-bracketing.
5.1 Supervised Noun Bracketing
Our LEX features indicate the specific noun at
each position in the compound, plus the three pairs
of nouns and the full noun triple. We also add fea-
tures for the capitalization pattern of the sequence.
N-GM features give the log-count of all subsets
of the compound. Counts are from Google V2.
Following Nakov and Hearst (2005), we also in-
clude counts of noun pairs collapsed into a single
token; if a pair occurs often on the web as a single
unit, it strongly indicates the pair is a constituent.
Vadas and Curran (2007a) use simpler features,
e.g. they do not use collapsed pair counts. They
achieve 89.9% in-domain on WSJ and 80.7% on
Grolier. Vadas and Curran (2007b) use compara-
ble features to ours, but do not test out-of-domain.
5.2 Noun Compound Bracketing Results
N-GM systems perform much better on this task
(Table 3). N-GM+LEX is statistically significantly
870
better than LEX on all sets. In-domain, errors
more than double without N-GM features. LEX
performs poorly here because there are far fewer
training examples. The learning curve (Figure 4)
looks much like earlier in-domain curves (Fig-
ures 1 and 3), but truncated before LEX becomes
competitive. The absence of a sufficient amount of
labeled data explains why NC-bracketing is gen-
erally regarded as a task where corpus counts are
crucial.
All web-based models (including the depen-
dency model) exceed 81.5% on Grolier, which
is the level of human agreement (Lauer, 1995b).
N-GM + LEX is highest on Medline, and close
to the 88% human agreement (Nakov and Hearst,
2005). Out-of-domain, the LEX approach per-
forms very poorly, close to or below the base-
line accuracy. With little training data and cross-
domain usage, N-gram features are essential.
6 Verb Part-of-Speech Disambiguation
Our final task is POS-tagging. We focus on one
frequent and difficult tagging decision: the distinc-
tion between a past-tense verb (VBD) and a past
participle (VBN). For example, in the troops sta-
tioned in Iraq, the verb stationed is a VBN; troops
is the head of the phrase. On the other hand, for
the troops vacationed in Iraq, the verb vacationed
is a VBD and also the head. Some verbs make the
distinction explicit (eat has VBD ate, VBN eaten),
but most require context for resolution.
Conflating VBN/VBD is damaging because it af-
fects downstream parsers and semantic role la-
belers. The task is difficult because nearby POS
tags can be identical in both cases. When the
verb follows a noun, tag assignment can hinge on
world-knowledge, i.e., the global lexical relation
between the noun and verb (E.g., troops tends to
be the object of stationed but the subject of vaca-
tioned).6 Web-scale N-gram data might help im-
prove the VBN/VBD distinction by providing rela-
tional evidence, even if the verb, noun, or verb-
noun pair were not observed in training data.
We extract nouns followed by a VBN/VBD in the
WSJ portion of the Treebank (Marcus et al, 1993),
getting 23K training, 1091 development and 1130
test examples from sections 2-22, 24, and 23, re-
spectively. For out-of-domain data, we get 21K
6HMM-style taggers, like the fast TnT tagger used on our
web corpus, do not use bilexical features, and so perform es-
pecially poorly on these cases. One motivation for our work
was to develop a fast post-processor to fix VBN/VBD errors.
examples from the Brown portion of the Treebank
and 6296 examples from tagged Medline abstracts
in the PennBioIE corpus (Kulick et al, 2004).
The majority class baseline is to choose VBD.
6.1 Supervised Verb Disambiguation
There are two orthogonal sources of information
for predicting VBN/VBD: 1) the noun-verb pair,
and 2) the context around the pair. Both N-GM
and LEX features encode both these sources.
6.1.1 LEX features
For 1), we use indicators for the noun and verb,
the noun-verb pair, whether the verb is on an in-
house list of said-verb (like warned, announced,
etc.), whether the noun is capitalized and whether
it?s upper-case. Note that in training data, 97.3%
of capitalized nouns are followed by a VBD and
98.5% of said-verbs are VBDs. For 2), we provide
indicator features for the words before the noun
and after the verb.
6.1.2 N-GM features
For 1), we characterize a noun-verb relation via
features for the pair?s distribution in Google V2.
Characterizing a word by its distribution has a
long history in NLP; we apply similar techniques
to relations, like Turney (2006), but with a larger
corpus and richer annotations. We extract the 20
most-frequent N-grams that contain both the noun
and the verb in the pair. For each of these, we con-
vert the tokens to POS-tags, except for tokens that
are among the most frequent 100 unigrams in our
corpus, which we include in word form. We mask
the noun of interest as N and the verb of interest
as V . This converted N-gram is the feature label.
The value is the pattern?s log-count. A high count
for patterns like (N that V), (N have V) suggests
the relation is a VBD, while patterns (N that were
V), (N V by), (V some N) indicate a VBN. As al-
ways, the classifier learns the association between
patterns and classes.
For 2), we use counts for the verb?s context co-
occurring with a VBD or VBN tag. E.g., we see
whether VBD cases like troops ate or VBN cases
like troops eaten are more frequent. Although our
corpus contains many VBN/VBD errors, we hope
the errors are random enough for aggregate counts
to be useful. The context is an N-gram spanning
the VBN/VBD. We have log-count features for all
five such N-grams in the (previous-word, noun,
verb, next-word) quadruple. The log-count is in-
871
System IN O1 O2
Baseline 89.2 85.2 79.6
ContextSum 92.5 91.1 90.4
SVM with N-GM features 96.1 93.4 93.8
SVM with LEX features 95.8 93.4 93.0
SVM with N-GM + LEX 96.4 93.5 94.0
Table 4: Verb-POS-disambiguation accuracy (%)
trained on WSJ, tested on WSJ (IN) and out-of-
domain Brown (O1) and Medline (O2).
 80
 85
 90
 95
 100
1e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM (N,V+context)
LEX (N,V+context)
N-GM (N,V)
LEX (N,V)
Figure 5: Out-of-domain learning curve of verb
disambiguation classifiers on Medline.
dexed by the position and length of the N-gram.
We include separate count features for contexts
matching the specific noun and for when the noun
token can match any word tagged as a noun.
ContextSum: We use these context counts in an
unsupervised system, ContextSum. Analogously
to Bergsma et al (2009), we separately sum the
log-counts for all contexts filled with VBD and
then VBN, outputting the tag with the higher total.
6.2 Verb POS Disambiguation Results
As in all tasks, N-GM+LEX has the best in-domain
accuracy (96.4%, Table 4). Out-of-domain, when
N-grams are excluded, errors only increase around
14% on Medline and 2% on Brown (the differ-
ences are not statistically significant). Why? Fig-
ure 5, the learning curve for performance on Med-
line, suggests some reasons. We omit N-GM+LEX
from Figure 5 as it closely follows N-GM.
Recall that we grouped the features into two
views: 1) noun-verb (N,V) and 2) context. If we
use just (N,V) features, we do see a large drop out-
of-domain: LEX (N,V) lags N-GM (N,V) even us-
ing all the training examples. The same is true us-
ing only context features (not shown). Using both
views, the results are closer: 93.8% for N-GM and
93.0% for LEX. With two views of an example,
LEX is more likely to have domain-neutral fea-
tures to draw on. Data sparsity is reduced.
Also, the Treebank provides an atypical num-
ber of labeled examples for analysis tasks. In a
more typical situation with less labeled examples,
N-GM strongly dominates LEX, even when two
views are used. E.g., with 2285 training exam-
ples, N-GM+LEX is statistically significantly bet-
ter than LEX on both out-of-domain sets.
All systems, however, perform log-linearly with
training size. In other tasks we only had a handful
of N-GM features; here there are 21K features for
the distributional patterns of N,V pairs. Reducing
this feature space by pruning or performing trans-
formations may improve accuracy in and out-of-
domain.
7 Discussion and Future Work
Of all classifiers, LEX performs worst on all cross-
domain tasks. Clearly, many of the regularities
that a typical classifier exploits in one domain do
not transfer to new genres. N-GM features, how-
ever, do not depend directly on training examples,
and thus work better cross-domain. Of course, us-
ing web-scale N-grams is not the only way to cre-
ate robust classifiers. Counts from any large auxil-
iary corpus may also help, but web counts should
help more (Lapata and Keller, 2005). Section 6.2
suggests that another way to mitigate domain-
dependence is having multiple feature views.
Banko and Brill (2001) argue ?a logical next
step for the research community would be to di-
rect efforts towards increasing the size of anno-
tated training collections.? Assuming we really do
want systems that operate beyond the specific do-
mains on which they are trained, the community
also needs to identify which systems behave as in
Figure 2, where the accuracy of the best in-domain
system actually decreases with more training ex-
amples. Our results suggest better features, such
as web pattern counts, may help more than ex-
panding training data. Also, systems using web-
scale unlabeled data will improve automatically as
the web expands, without annotation effort.
In some sense, using web counts as features
is a form of domain adaptation: adapting a web
model to the training domain. How do we ensure
these features are adapted well and not used in
domain-specific ways (especially with many fea-
tures to adapt, as in Section 6)? One option may
872
be to regularize the classifier specifically for out-
of-domain accuracy. We found that adjusting the
SVM misclassification penalty (for more regular-
ization) can help or hurt out-of-domain. Other
regularizations are possible. In each task, there
are domain-neutral unsupervised approaches. We
could encode these systems as linear classifiers
with corresponding weights. Rather than a typical
SVM that minimizes the weight-norm ||w|| (plus
the slacks), we could regularize toward domain-
neutral weights. This regularization could be opti-
mized on creative splits of the training data.
8 Conclusion
We presented results on tasks spanning a range of
NLP research: generation, disambiguation, pars-
ing and tagging. Using web-scale N-gram data
improves accuracy on each task. When less train-
ing data is used, or when the system is used on a
different domain, N-gram features greatly improve
performance. Since most supervised NLP systems
do not use web-scale counts, further cross-domain
evaluation may reveal some very brittle systems.
Continued effort in new domains should be a pri-
ority for the community going forward.
Acknowledgments
We gratefully acknowledge the Center for Lan-
guage and Speech Processing at Johns Hopkins
University for hosting the workshop at which part
of this research was conducted.
References
Nir Ailon and Mehryar Mohri. 2008. An efficient re-
duction of ranking to classification. In COLT.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In ACL.
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English web-search
queries. In EMNLP.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2009. Web-scale N-gram models for lexical disam-
biguation. In IJCAI.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Thorsten Brants and Alex Franz. 2006. The Google
Web 1T 5-gram Corpus Version 1.1. LDC2006T13.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In EMNLP.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In ANLP.
Andrew Carlson, Tom M. Mitchell, and Ian Fette.
2008. Data analysis project: Leveraging massive
textual corpora using n-gram statistics. Technial Re-
port CMU-ML-08-107.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with Golomb
coding. In EMNLP-CoNLL.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9.
Dan Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459?484.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the Web as corpus.
Computational Linguistics, 29(3):333?347.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein,
Lyle Ungar, Scott Winters, and Pete White. 2004.
Integrated annotation for biomedical information ex-
traction. In BioLINK 2004: Linking Biological Lit-
erature, Ontologies and Databases.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Transactions on Speech and Language Processing,
2(1):1?31.
Mark Lauer. 1995a. Corpus statistics meet the noun
compound: Some empirical results. In ACL.
Mark Lauer. 1995b. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale N-grams. In LREC.
873
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In ACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Mitchell P. Marcus. 1980. Theory of Syntactic Recog-
nition for Natural Languages. MIT Press, Cam-
bridge, MA, USA.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In COLING-ACL.
Margaret Mitchell. 2009. Class-based ordering of
prenominal modifiers. In 12th European Workshop
on Natural Language Generation.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the Web in machine learning for
other-anaphora resolution. In EMNLP.
Preslav Nakov and Marti Hearst. 2005. Search engine
statistics beyond the n-gram: Application to noun
compound bracketing. In CoNLL.
Preslav Ivanov Nakov. 2007. Using the Web as an Im-
plicit Training Set: Application to Noun Compound
Syntax and Semantics. Ph.D. thesis, University of
California, Berkeley.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English
POS Tagger. crftagger.sourceforge.net.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains.
In EMNLP.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In ACL.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
David Vadas and James R. Curran. 2007a. Adding
noun phrase structure to the Penn Treebank. In ACL.
David Vadas and James R. Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In
PACLING.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005.
Improving pronoun resolution using statistics-based
semantic compatibility information. In ACL.
874
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 768?776,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Attacking Parsing Bottlenecks with Unlabeled Data and Relevant
Factorizations
Emily Pitler
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
epitler@seas.upenn.edu
Abstract
Prepositions and conjunctions are two of
the largest remaining bottlenecks in parsing.
Across various existing parsers, these two
categories have the lowest accuracies, and
mistakes made have consequences for down-
stream applications. Prepositions and con-
junctions are often assumed to depend on lex-
ical dependencies for correct resolution. As
lexical statistics based on the training set only
are sparse, unlabeled data can help amelio-
rate this sparsity problem. By including un-
labeled data features into a factorization of
the problem which matches the representation
of prepositions and conjunctions, we achieve
a new state-of-the-art for English dependen-
cies with 93.55% correct attachments on the
current standard. Furthermore, conjunctions
are attached with an accuracy of 90.8%, and
prepositions with an accuracy of 87.4%.
1 Introduction
Prepositions and conjunctions are two large remain-
ing bottlenecks in parsing. Across various exist-
ing parsers, these two categories have the lowest
accuracies, and mistakes made on these have con-
sequences for downstream applications. Machine
translation is sensitive to parsing errors involving
prepositions and conjunctions, because in some lan-
guages different attachment decisions in the parse
of the source language sentence produce differ-
ent translations. Preposition attachment mistakes
are particularly bad when translating into Japanese
(Schwartz et al, 2003) which uses a different post-
position for different attachments; conjunction mis-
takes can cause word ordering mistakes when trans-
lating into Chinese (Huang, 1983).
Prepositions and conjunctions are often assumed
to depend on lexical dependencies for correct resolu-
tion (Jurafsky and Martin, 2008). However, lexical
statistics based on the training set only are typically
sparse and have only a small effect on overall pars-
ing performance (Gildea, 2001). Unlabeled data can
help ameliorate this sparsity problem. Backing off
to cluster membership features (Koo et al, 2008) or
by using association statistics from a larger corpus,
such as the web (Bansal and Klein, 2011; Zhou et
al., 2011), have both improved parsing.
Unlabeled data has been shown to improve the ac-
curacy of conjunctions within complex noun phrases
(Pitler et al, 2010; Bergsma et al, 2011). How-
ever, it has so far been less effective within full
parsing ? while first-order web-scale counts notice-
ably improved overall parsing in Bansal and Klein
(2011), the accuracy on conjunctions actually de-
creased when the web-scale features were added
(Table 4 in that paper).
In this paper we show that unlabeled data can help
prepositions and conjunctions, provided that the de-
pendency representation is compatible with how the
parsing problem is decomposed for learning and in-
ference. By incorporating unlabeled data into factor-
izations which capture the relevant dependencies for
prepositions and conjunctions, we produce a parser
for English which has an unlabeled attachment ac-
curacy of 93.5%, over an 18% reduction in error
over the best previously published parser (Bansal
and Klein, 2011) on the current standard for depen-
dency parsing. The best model for conjunctions at-
768
taches them with 90.8% accuracy (42.5% reduction
in error over MSTParser), and the best model for
prepositions with 87.4% accuracy (18.2% reduction
in error over MSTParser).
We describe the dependency representations of
prepositions and conjunctions in Section 2. We dis-
cuss the implications of these representations for
how learning and inference for parsing are decom-
posed (Section 3) and how unlabeled data may be
used (Section 4). We then present experiments ex-
ploring the connection between representation, fac-
torization, and unlabeled data in Sections 5 and 6.
2 Dependency Representations
A dependency tree is a rooted, directed tree (or ar-
borescence), in which the vertices are the words in
the sentence plus an artificial root node, and each
edge (h,m) represents a directed dependency rela-
tion from the head h to the modifier m. Through-
out this section, we will use Y to denote a particular
parse tree, and (h,m) ? Y to denote a particular
edge in Y .
The Wall Street Journal Penn Treebank (PTB)
(Marcus et al, 1993) contains parsed constituency
trees (where each sentence is represented as a
context-free-grammar derivation). Dependency
parsing requires a conversion from these con-
stituency trees to dependency trees. The Tree-
bank constituency trees left noun phrases (NPs)
flat, although there have been subsequent projects
which annotate the internal structure of noun phrases
(Vadas and Curran, 2007; Weischedel et al, 2011).
The presence or absence of these noun phrase in-
ternal annotations interacts with constituency-to-
dependency conversion program in ways which have
effects on conjunctions and prepositions.
We consider two such mapping regimes here:
1. PTB trees ? Penn2Malt1 ? Dependencies
2. PTB trees patched with NP-internal annota-
tions (Vadas and Curran, 2007) ? penncon-
verter2 ? Dependencies
1http://w3.msi.vxu.se/?nivre/research/
Penn2Malt.html
2Johansson and Nugues (2007) http://nlp.cs.lth.
se/software/treebank_converter/
Regime (1) is very commonly done in papers
which report dependency parsing experiments (e.g.,
(McDonald and Pereira, 2006; Nivre et al, 2007;
Zhang and Clark, 2008; Huang and Sagae, 2010;
Koo and Collins, 2010)). Penn2Malt uses the head
finding table from Yamada and Matsumoto (2003).
Regime (2) is based on the recommendations of
the two converter tools; as of the date of this writing,
the Penn2Malt website says: ?Penn2Malt has been
superseded by the more sophisticated pennconverter,
which we strongly recommend?. The pennconverter
website ?strongly recommends? patching the Tree-
bank with the NP annotations of Vadas and Curran
(2007). A version of pennconverter was used to pre-
pare the data for the CoNLL Shared Tasks of 2007-
2009, so the trees produced by Regime 2 are similar
(but not identical)3 to these shared tasks. As far as
we are aware, Bansal and Klein (2011) is the only
published work which uses both steps in Regime (2).
The dependency representations produced by
Regime 2 are designed to be more useful for ex-
tracting semantics (Johansson and Nugues, 2007).
The parsing attachment accuracy of MALTPARSER
(Nivre et al, 2007) was lower using pennconverter
than Penn2Malt, but using the output of MALT-
PARSER under the new format parses produces a
much better semantic role labeler than using its out-
put with Penn2Malt (Johansson and Nugues, 2007).
Figures 1 and 2 show how conjunctions and
prepositions, respectively, are represented after the
two different conversion processes. These differ-
ences are not rare?70.7% of conjunctions and 5.2%
of prepositions in the development set have a differ-
ent parent under the two conversion types. These
representational differences have serious implica-
tions for how well various factorizations will be able
to capture these two phenomena.
3 Implications of Representations on the
Scope of Factorization
Parsing requires a) learning to score potential parse
trees, and b) given a particular scoring function,
finding the highest scoring tree according to that
function. The number of potential trees for a sen-
3The CoNLL data does not include the NP annotations; it
does include annotations of named entities (Weischedel and
Brunstein, 2005) so had some internal NP edges.
769
Conversion 1 Conversion 2
Committee
the House
Ways
and Means
(a)
Committee
the House
Ways
and
Means
(b)
debt
notes and other
(c)
notes
and
debt
other
(d)
sell
or merge 600 by
(e)
sell
or
merge
600 by
(f)
Figure 1: Examples of conjunctions: the House Ways
and Means Committee, notes and other debt, and sell or
merge 600 by. The conjunction is bolded, the left con-
junct (in the linear order of the sentence) is underlined,
and the right conjunct is italicized.
tence is exponential, so parsing is made tractable by
decomposing the problem into a set of local sub-
structures which can be combined using dynamic
programming. Four possible factorizations are: sin-
gle edges (edge-based), pairs of edges which share
a parent (siblings), pairs of edges where the child
of one is the parent of the other (grandparents), and
triples of edges where the child of one is the parent
of two others (grandparent+sibling). In this section,
we discuss these factorizations and their relevance
to conjunction and preposition representations.
3.1 Edge-based Scoring
One possible factorization corresponds to first-order
parsing, in which the score of a parse tree Y decom-
poses completely across the edges in the tree:
S(Y ) =
?
(h,m)?Y
S(h,m) (1)
Conversion 1 Conversion 2
plan
in
law
(a)
plan
in
law
(b)
yesterday
opening of
trading
here
(c)
opening
of
trading
here yesterday
(d)
whose
plans
for
issues
(e)
plans
whose
for
issues
(f)
Figure 2: Examples of prepositions: plan in the S&L
bailout law, opening of trading here yesterday, and whose
plans for major rights issues. The preposition is bolded
and the (semantic) head is underlined.
Conjunctions: Under Conversion 1, we can see
three different representations of conjunctions in
Figures 1(a), 1(c), and 1(e). Under edge-based scor-
ing, the conjunction would be scored along with nei-
ther of its conjuncts in 1(a). In Figure 1(c), the con-
junction is scored along with its right conjunct only;
in figure 1(e) along with its left conjunct only. The
inconsistency here is likely to make learning more
difficult, as what is learned is split across these three
cases. Furthermore, the conjunction is connected
with an edge to either zero or one of its two argu-
ments; at least one of the arguments is completely
ignored in terms of scoring the conjunction.
In Figures 1(c) and 1(e), the words being con-
joined are connected to each other by an edge. This
overloads the meaning of an edge; an edge indicates
both a head-modifier relationship and a conjunction
relationship. For example, compare the two natural
phrases dogs and cats and really nice. dogs and cats
are a good pair to conjoin, but cats is not a good
modifier for dogs, so there is a tension when scoring
an edge like (dogs, cats): it should get a high score
770
when actually indicating a conjunction and low oth-
erwise. (nice, really) shows the opposite pattern?
really is a good modifier for nice, but nice and re-
ally are not two words which should be conjoined.
This may be partially compensated for by including
features about the surrounding words (McDonald et
al., 2005), but any feature templates which would be
identical across the two contexts will be in tension.
In Figures 1(b), 1(d) and 1(f), the conjunction par-
ticipates in a directed edge with each of the con-
juncts. Thus, in edge-based scoring, at least under
Conversion 2 neither of the conjuncts is being ig-
nored; however, the factorization scores each edge
independently, so how compatible these two con-
juncts are with each other cannot be included in the
scoring of a tree.
Prepositions: For all of the examples in Figure 2,
there is a directed edge from the head of the phrase
that the preposition modifies to the preposition. Dif-
ferences in head finding rules account for the dif-
ferences in preposition representations. In the sec-
ond example, the first conversion scheme chooses
yesterday as the head of the overall NP, resulting in
the edge yesterday? of, while the second conver-
sion scheme ignores temporal phrases when finding
the head, resulting in the more semantically mean-
ingful opening?of. Similarly, in the third example,
the preposition for attaches to the pronoun whose in
the first conversion scheme, while it attaches to the
noun plans in the second.
With edge-based scoring, the object is not acces-
sible when scoring where the preposition should at-
tach, and PP-attachment is known to depend on the
object of the preposition (Hindle and Rooth, 1993).
3.2 Sibling Scoring
Another alternative factorization is to score sib-
lings as well as parent-child edges (McDonald and
Pereira, 2006). Scores decompose as:
S(Y ) =
?
?
?
?
(h,m, s) (h,m) ? Y, (h, s) ? Y,
(m, s) ? Sib(Y )
?
?
?
S(h,m, s) (2)
where Sib(Y ) is the set containing ordered and ad-
jacent sibling pairs in Y : if (m, s) ? Sib(Y ), there
must exist a shared parent h such that (h,m) ? Y
and (h, s) ? Y , m and s must be on the same side
of h, m must be closer to h than s in the linear order
of the sentence, and there must not exist any other
children of h in between m and s.
Under this factorization, two of the three ex-
amples in Conversion 1 (and none of the exam-
ples in Conversion 2) in Figure 1 now include the
conjunction and both conjuncts in the same score
(Figures 1(c) and 1(e)). The scoring for head-
modifier dependencies and conjunction dependen-
cies are again being overloaded: (debt, notes, and)
and (debt, and, other) are both sibling parts in Fig-
ure 1(c), yet only one of them represents a conjunc-
tion. The position of the conjunction in the sibling
is not enough to determine whether one is scoring a
true conjunction relation or just the conjunction and
a different sibling; in 1(c) the conjunction is on the
right of its sibling argument, while in 1(e) the con-
junction is on the left.
For none of the other preposition or conjunc-
tion examples does a sibling factorization bring
more of the arguments into the scope of what is
scored along with the preposition/conjunction. Sib-
ling scoring may have some benefit in that preposi-
tions/conjunctions should have only one argument,
so for prepositions (under both conversions) and
conjunctions (under Conversion 2), the model can
learn to disprefer the existence of any siblings and
thus enforce choosing a single child.
3.3 Grandparent Scoring
Another alternative over pairs of edges scores grand-
parents instead of siblings, with factorization:
S(Y ) =
?
{
(h,m, c) (h,m) ? Y, (m, c) ? Y
}
S(h,m, c) (3)
Under Conversion 2, we would expect this fac-
torization to perform much better on conjunctions
and prepositions than edge-based or sibling-based
factorizations. Both conjunctions and prepositions
are consistently represented by exactly one grand-
parent relation (with one relevant argument as the
grandparent, the preposition/conjunction as the par-
ent, and the other argument as the child), so this is
the first factorization that has allowed the compati-
bility of the two arguments to affect the attachment
of the preposition/conjunction.
Under Conversion 1, this factorization is particu-
larly appropriate for prepositions, but would be un-
likely to help conjunctions, which have no children.
771
3.4 Grandparent-Sibling Scoring
A further widening of the factorization takes grand-
parents and siblings simultaneously:
S(Y ) =
?
?
?
?
(g, h,m, s) (g, h) ? Y, (h,m) ? Y,
(h, s) ? Y, (m, s) ? Sib(Y )
?
?
?
S(g, h,m, s) (4)
For projective parsing, dynamic programming for
this factorization was derived in Koo and Collins
(2010) (Model 1 in that paper), and for non-
projective parsing, dual decomposition was used for
this factorization in Koo et al (2010).
This factorization should combine all the ben-
efits of the sibling and grandparent factorizations
described above?for Conversion 1, sibling scoring
may help conjunctions and grandparent scoring may
help prepositions, and for Conversion 2, grandparent
scoring should help both, while sibling scoring may
or may not add some additional gains.
4 Using Unlabeled Data Effectively
Associations from unlabeled data have the poten-
tial to improve both conjunctions and prepositions.
We predict that web counts which include both con-
juncts (for conjunctions), or which include both the
attachment site and the object of a preposition (for
prepositions) will lead to the largest improvements.
For the phrase dogs and cats, edge-based counts
would measure the associations between dogs and
and, and and and cats, but never any web counts
that include both dogs and cats. For the phrase ate
spaghetti with a fork, edge-based scoring would not
use any web counts involving both ate and fork.
We use associations rather than raw counts. The
phrases trading and transacting versus trading and
what provide an example of the difference between
associations and counts. The phrase trading and
what has a higher count than the phrase trading and
transacting, but trading and transacting are more
highly associated. In this paper, we use point-wise
mutual information (PMI) to measure the strength of
associations of words participating in potential con-
junctions or prepositions.4 For three words h, m, c,
this is calculated with:
PMI(h,m, c) = log
P (h .* m .* c)
P (h)P (m)P (c)
(5)
4PMI can be unreliable when frequency counts are small
(Church and Hanks, 1990), however the data used was thresh-
olded, so all counts used are at least 10.
The probabilities are estimated using web-scale
n-gram counts, which are looked up using the
tools and web-scale n-grams described in Lin et al
(2010). Defining the joint probability using wild-
cards (rather than the exact sequence h m c) is
crucially important, as determiners, adjectives, and
other words may naturally intervene between the
words of interest.
Approaches which cluster words (i.e., Koo et
al. (2008)) are also designed to identify words
which are semantically related. As manually labeled
parsed data is sparse, this may help generalize across
similar words. However, if edges are not connected
to the semantic head, cluster-based methods may be
less effective. For example, the choice of yesterday
as the head of opening of trading here yesterday in
Figure 2(c) or whose in 2(e) may make cluster-based
features less useful than if the semantic heads were
chosen (opening and plans, respectively).
5 Experiments
The previous section motivated the use of unlabeled
data for attaching prepositions and conjunctions. We
have also hypothesized that these features will be
most effective when the data representation and the
learning representation both capture relevant prop-
erties of prepositions and conjunctions. We predict
that Conversion 2 and a factorization which includes
grand-parent scoring will achieve the highest perfor-
mance. In this section, we investigate the impact
of unlabeled data on parsing accuracy using the two
conversions and using each of the factorizations de-
scribed in Section 3.1-3.4.
5.1 Unlabeled Data Feature Set
Clusters: We replicate the cluster-based features
from Koo et al (2008), which includes features over
all edges (h,m), grand-parent triples (h,m, c), and
parent sibling triples (h,m, s). The features were
all derived from the publicly available clusters pro-
duced by running the Brown clustering algorithm
(Brown et al, 1992) over the BLLIP corpus with the
Penn Treebank sentences excluded.5
Preposition and conjunction-inspired features
(motivated by Section 4) are described below:
5people.csail.mit.edu/maestro/papers/
bllip-clusters.gz
772
Web Counts: For each set of words of interest, we
compute the PMI between the words, and then in-
clude binary features for whether the mutual infor-
mation is undefined, if it is negative, and whether it
is greater than each positive integer.
For conjunctions, we only do this for triples of
both conjunct and the conjunction (and if the con-
junction is and or or and the two potential conjuncts
are the same coarse grained part-of-speech). For
prepositions, we consider only cases in which the
parent is a noun or a verb and the child is a noun
(this corresponds to the cases considered by Hindle
and Rooth (1993) and others). Prepositions use as-
sociation features to score both the triple (parent,
preposition, child) and all pairs within that triple.
The counts features are not used if all the words in-
volved are stopwords. For the scope of this paper we
use only the above counts related to prepositions and
conjunctions.
5.2 Parser
We use the Model 1 version of dpo3, a state-of-the-
art third-order dependency parser (Koo and Collins,
2010))6. We augment the feature set used with the
web-counts-based features relevant to prepositions
and conjunctions and the cluster-based features. The
only other change to the parser?s existing feature set
was the addition of binary features for the part-of-
speech tag of the child of the root node, alone and
conjoined with the tags of its children. For further
details about the parser, see Koo and Collins (2010).
5.3 Experimental Set-up
Training was done on Section 2-21 of the Penn
Treebank. Section 22 was used for development,
and Section 23 for test. We use automatic part-
of-speech tags for both training and testing (Rat-
naparkhi, 1996). The set of potential edges was
pruned using the marginals produced by a first-order
parser trained using exponentiated gradient descent
(Collins et al, 2008) as in Koo and Collins (2010).
We train the full parser for 15 iterations of averaged
perceptron training (Collins, 2002), choose the itera-
tion with the best unlabeled attachment score (UAS)
on the development set, and apply the model after
that iteration to the test set.
6http://groups.csail.mit.edu/nlp/dpo3/
We also ran MSTParser (McDonald and Pereira,
2006), the Berkeley constituency parser (Petrov and
Klein, 2007), and the unmodified dpo3 Model 1
(Koo and Collins, 2010) using Conversion 2 (the
current recommendations) for comparison. Since
the converted Penn Treebank now contains a few
non-projective sentences, we ran both the projective
and non-projective versions of the second order (sib-
ling) MSTParser. The Berkeley parser was trained
on the constituency trees of the PTB patched with
Vadas and Curran (2007), and then the predicted
parses were converted using pennconverter.
6 Results and Discussion
Table 1 shows the unlabeled attachment scores,
complete sentence exact match accuracies, and the
accuracies of conjunctions and prepositions under
Conversion 2.7 The incorporation of the unlabeled
data features (clusters and web counts) into the dpo3
parser yields a significantly better parser than dpo3
alone (93.54 UAS versus 93.21)8, and is more than
a 1.5% improvement over MSTParser.
6.1 Impact of Factorization
In all four metrics (attachment of all non-
punctuation tokens, sentence accuracy, prepositions,
and conjunctions), there is no significant difference
between the version of the parser which uses the
grandparent and sibling factorization (Grand+Sib)
and the version which uses just the grandparent fac-
torization (Grand). A parser which uses only grand-
parents (referred to as Model 0 in Koo and Collins
(2010)) may therefore be preferable, as it contains
far fewer parameters than a third-order parser.
While the grandparent factorization and the sib-
ling factorization (Sib) are both ?second-order?
parsers, scoring up to two edges (involving three
words) simultaneously, their results are quite dif-
ferent, with the sibling factorization scoring much
worse. This is particularly notable in the conjunc-
tion case, where the sibling model is over 5% abso-
lute worse in accuracy than the grandparent model.
7As is standard for English dependency parsing, five punc-
tuation symbols :, ,, ?, ?, and . are excluded from the results
(Yamada and Matsumoto, 2003).
8If the (deprecated) Conversion 1 is used, the new features
improve the UAS of dpo3 from 93.04 to 93.51.
773
Model UAS Exact Match Conjunctions Prepositions
MSTParser (proj) 91.96 38.9 84.0 84.2
MSTParser (non-proj) 91.98 38.7 83.8 84.6
Berkeley (converted) 90.98 36.0 85.6 84.3
dpo3 (Grand+Sib) 93.21 44.8 89.6 86.9
dpo3+Unlabeled (Edges) 93.12 43.6 85.3 87.0
dpo3+Unlabeled (Sib) 93.15 43.7 85.5 86.8
dpo3+Unlabeled (Grand) 93.55 46.1 90.6 87.5
dpo3+Unlabeled (Grand+Sib) 93.54 46.0 90.8 87.4
- Clusters 93.10 45.0 90.5 87.5
- Prep,Conj Counts 93.52 45.8 89.9 87.1
Table 1: Test set accuracies under Conversion 2 of unlabeled attachment scores, complete sentence exact match accu-
racies, conjunction accuracy, and preposition accuracy. Bolded items are the best in each column, or not significantly
different from the best in that column (sign test, p < .05).
6.2 Impact of Unlabeled Data
The unlabeled data features improved the already
state-of-the-art dpo3 parser in UAS, complete sen-
tence accuracy, conjunctions, and prepositions.
However, because the sample sizes are much smaller
for the latter three cases, only the UAS improvement
is statistically significant.9 Overall, the results in Ta-
ble 1 show that while the inclusion of unlabeled data
improves parser performance, increasing the size of
factorization matters even more. Ablation experi-
ments showed that cluster features have a larger im-
pact on overall UAS, while count features have a
larger impact on prepositions and conjunctions.
6.3 Comparison with Other Parsers
The resulting dpo3+Unlabeled parser is significantly
better than both versions of MSTParser and the
Berkeley parser converted to dependencies across all
four evaluations. dpo3+Unlabeled has an UAS 1.5%
higher than MSTParser, which has an UAS 1.0%
higher than the converted constituency parser. The
MSTParser uses sibling scoring, so it is unsurpris-
ing that it performs less well on the new conversion.
While the converted constituency parser is not
as good on dependencies as MSTParser overall,
note that it is over a percent and a half better than
MSTParser on attaching conjunctions (85.6% versus
84.0%). Conjunction scope may benefit from paral-
lelism and higher-level structure, which is easily ac-
cessible when joining two matching non-terminals
9There are 52,308 non-punctuation tokens in the test set,
compared with 2416 sentences, 1373 conjunctions, and 5854
prepositions.
in a context-free grammar, but much harder to
determine in the local views of graph-based de-
pendency parsers. The dependencies arising from
the Berkeley constituency trees have higher con-
junction accuracies than either the edge-based or
sibling-based dpo3+Unlabeled parser. However,
once grandparents are included in the factorization,
the dpo3+Unlabeled is significantly better at attach-
ing conjunctions than the constituency parser, at-
taching conjunctions with an accuracy over 90%.
Therefore, some of the disadvantages of dependency
parsing compared with constituency parsing can be
compensated for with larger factorizations.
Conjunctions
Conversion 1 Conversion 2
Scoring (deprecated)
Edge 86.3 85.3
Sib 87.8 85.5
Grand 87.2 90.6
Grand+Sib 88.3 90.8
Table 2: Unlabeled attachment accuracy for conjunc-
tions. Bolded items are the best in each column, or not
significantly different (sign test, p < .05).
6.4 Impact of Data Representation
Tables 2 and 3 show the results of the
dpo3+Unlabeled parser for conjunctions and
prepositions, respectively, under the two different
conversions. The data representation has an impact
on which factorizations perform best. Under
Conversion 1, conjunctions are more accurate under
a sibling parser than a grandparent parser, while the
774
Prepositions
Conversion 1 Conversion 2
Scoring (deprecated)
Edge 87.4 87.0
Sib 87.5 86.8
Grand 87.9 87.5
Grand+Sib 88.4 87.4
Table 3: Unlabeled attachment accuracy for prepositions.
Bolded items are the best in each column, or not signifi-
cantly different (sign test, p < .05).
pattern is reversed for Conversion 2.
Conjunctions show a much stronger need for
higher order factorizations than prepositions do.
This is not too surprising, as prepositions have more
of a selectional preference than conjunctions, and
so the preposition itself is more informative about
where it should attach. While prepositions do im-
prove with larger factorizations, the improvement
beyond edge-based is not significant for Conversion
2. One hypothesis for why Conversion 1 shows more
of an improvement is that the wider scope leads to
the semantic head being included; in Conversion
2, the semantic head is chosen as the parent of the
preposition, so the wider scope is less necessary.
6.5 Preposition Error Analysis
Prepositions are still the largest source of errors in
the dpo3+Unlabeled parser. We therefore analyze
the errors made on the development set to determine
whether the difficult remaining cases for parsers cor-
respond to the Hindle and Rooth (1993) style PP-
attachment classification task. In the PP-attachment
classification task, the two choices for where the
preposition attaches are the previous verb or the pre-
vious noun, and the preposition itself has a noun ob-
ject. The ones that do attach to the preceeding noun
or verb (not necessarily the preceeding word) and
have a noun object (2323 prepositions) are attached
by the dpo3+Unlabeled grandparent-scoring parser
with 92.4% accuracy, while those that do not fit that
categorization (1703 prepositions) have the correct
parent only 82.7% of the time.
Local attachments are more accurate ? preposi-
tions are attached with 94.8% accuracy if the correct
parent is the immediately preceeding word (2364
cases) and only 79.1% accuracy if it is not (1662
cases). The preference is not necessarily for low
attachments though: the prepositions whose parent
is not the preceeding word are attached more accu-
rately if the parent is the root word (usually corre-
sponding to the main verb) of the sentence (90.8%,
587 cases) than if the parent is lower in the tree
(72.7%, 1075 cases).
7 Conclusion
Features derived from unlabeled data (clusters and
web counts) significantly improve a state-of-the-art
dependency parser for English. We showed how
well various factorizations are able to take advantage
of these unlabeled data features, focusing our anal-
ysis on conjunctions and prepositions. Including
grandparents in the factorization increases the accu-
racy of conjunctions over 5% absolute over edge-
based or sibling-based scoring. The representation
of the data is extremely important for how the prob-
lem should be factored?under the old Penn2Malt de-
pendency representation, a sibling parser was more
accurate than a grandparent parser. As some impor-
tant relationships were represented as siblings and
some as grandparents, there was a need to develop
third-order parsers which could exploit both simul-
taneously (Koo and Collins, 2010). Under the new
pennconverter standard, a grandparent parser is sig-
nificantly better than a sibling parser, and there is no
significant improvement when including both.
Acknowledgments
I would like to thank Terry Koo for making the dpo3
parser publically available and for his help with us-
ing the parser. I would also like to thank Mitch Mar-
cus and Kenneth Church for useful discussions. This
material is based upon work supported under a Na-
tional Science Foundation Graduate Research Fel-
lowship.
References
M. Bansal and D. Klein. 2011. Web-scale features for
full-scale parsing. In Proceedings of ACL, pages 693?
702.
S. Bergsma, D. Yarowsky, and K. Church. 2011. Using
large monolingual and bilingual corpora to improve
coordination disambiguation. In Proceedings of ACL,
pages 1346?1355.
775
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and
J.C. Lai. 1992. Class-based n-gram models of natural
language. Computational linguistics, 18(4):467?479.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1):22?29.
M. Collins, A. Globerson, T. Koo, X. Carreras, and P.L.
Bartlett. 2008. Exponentiated gradient algorithms
for conditional random fields and max-margin markov
networks. The Journal of Machine Learning Research,
9:1775?1822.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of EMNLP, pages 167?202.
D. Hindle and M. Rooth. 1993. Structural ambigu-
ity and lexical relations. Computational Linguistics,
19(1):103?120.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings of
ACL, pages 1077?1086.
X. Huang. 1983. Dealing with conjunctions in a machine
translation environment. In Proceedings of EACL,
pages 81?85.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA), pages 105?112.
D. Jurafsky and J.H. Martin. 2008. Speech and language
processing: an introduction to natural language pro-
cessing, computational linguistics and speech recogni-
tion. Prentice Hall.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of ACL, pages 1?11.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceedings
of ACL, pages 595?603.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proceedings of EMNLP,
pages 1288?1298.
D. Lin, K. Church, H. Ji, S. Sekine, D. Yarowsky,
S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,
et al 2010. New tools for web-scale n-grams. In Pro-
ceedings of LREC.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, pages 91?98.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(2):95?135.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404?411.
E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Us-
ing web-scale n-grams to improve base np parsing per-
formance. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages 886?
894.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP,
pages 133?142.
L. Schwartz, T. Aikawa, and C. Quirk. 2003. Disam-
biguation of English PP attachment using multilingual
aligned data. In Proceedings of MT Summit IX.
D. Vadas and J. Curran. 2007. Adding noun phrase struc-
ture to the Penn Treebank. In ACL, pages 240?247.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Linguistic Data
Consortium, Philadelphia.
R. Weischedel, M. Palmer, M. Marcus, E. Hovy, S. Prad-
han, L. Ramshaw, N. Xue, A. Taylor, J. Kaufman,
M. Franchini, et al 2011. Ontonotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of International Workshop of Parsing Tech-
nologies, pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: in-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of EMNLP, pages 562?571.
G. Zhou, J. Zhao, K. Liu, and L. Cai. 2011. Exploiting
web-derived selectional preference to improve statisti-
cal dependency parsing. In Proceedings of ACL, pages
1556?1565.
776
Transactions of the Association for Computational Linguistics, 1 (2013) 13?24. Action Editor: Giorgio Satta.
Submitted 11/2012; Published 3/2013. c?2013 Association for Computational Linguistics.
Finding Optimal 1-Endpoint-Crossing Trees
Emily Pitler, Sampath Kannan, Mitchell Marcus
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
epitler,kannan,mitch@seas.upenn.edu
Abstract
Dependency parsing algorithms capable of
producing the types of crossing dependencies
seen in natural language sentences have tra-
ditionally been orders of magnitude slower
than algorithms for projective trees. For 95.8-
99.8% of dependency parses in various nat-
ural language treebanks, whenever an edge
is crossed, the edges that cross it all have a
common vertex. The optimal dependency tree
that satisfies this 1-Endpoint-Crossing prop-
erty can be found with an O(n4) parsing al-
gorithm that recursively combines forests over
intervals with one exterior point. 1-Endpoint-
Crossing trees also have natural connections
to linguistics and another class of graphs that
has been studied in NLP.
1 Introduction
Dependency parsing is one of the fundamental prob-
lems in natural language processing today, with ap-
plications such as machine translation (Ding and
Palmer, 2005), information extraction (Culotta and
Sorensen, 2004), and question answering (Cui et
al., 2005). Most high-accuracy graph-based depen-
dency parsers (Koo and Collins, 2010; Rush and
Petrov, 2012; Zhang and McDonald, 2012) find the
highest-scoring projective trees (in which no edges
cross), despite the fact that a large proportion of nat-
ural language sentences are non-projective. Projec-
tive trees can be found in O(n3) time (Eisner, 2000),
but cover only 63.6% of sentences in some natural
language treebanks (Table 1).
The class of directed spanning trees covers all
treebank trees and can be parsed in O(n2) with
edge-based features (McDonald et al, 2005), but it
is NP-hard to find the maximum scoring such tree
with grandparent or sibling features (McDonald and
Pereira, 2006; McDonald and Satta, 2007).
There are various existing definitions of mildly
non-projective trees with better empirical coverage
than projective trees that do not have the hardness of
extensibility that spanning trees do. However, these
have had parsing algorithms that are orders of mag-
nitude slower than the projective case or the edge-
based spanning tree case. For example, well-nested
dependency trees with block degree 2 (Kuhlmann,
2013) cover at least 95.4% of natural language struc-
tures, but have a parsing time of O(n7) (G?mez-
Rodr?guez et al, 2011).
No previously defined class of trees simultane-
ously has high coverage and low-degree polynomial
algorithms for parsing, allowing grandparent or sib-
ling features.
We propose 1-Endpoint-Crossing trees, in which
for any edge that is crossed, all other edges that
cross that edge share an endpoint. While simple
to state, this property covers 95.8% or more of de-
pendency parses in natural language treebanks (Ta-
ble 1). The optimal 1-Endpoint-Crossing tree can
be found in faster asymptotic time than any previ-
ously proposed mildly non-projective dependency
parsing algorithm. We show how any 1-Endpoint-
Crossing tree can be decomposed into isolated sets
of intervals with one exterior point (Section 3). This
is the key insight that allows efficient parsing; the
O(n4) parsing algorithm is presented in Section 4.
1-Endpoint-Crossing trees are a subclass of 2-planar
graphs (Section 5.1), a class that has been studied
13
in NLP. 1-Endpoint-Crossing trees also have some
linguistic interpretation (pairs of cross serial verbs
produce 1-Endpoint-Crossing trees, Section 5.2).
2 Definitions of Non-Projectivity
Definition 1. Edges e and f cross if e and f have
distinct endpoints and exactly one of the endpoints
of f lies between the endpoints of e.
Definition 2. A dependency tree is 1-Endpoint-
Crossing if for any edge e, all edges that cross e
share an endpoint p.
Table 1 shows the percentage of dependency
parses in the CoNLL-X training sets that are 1-
Endpoint-Crossing trees. Across six languages with
varying amounts of non-projectivity, 95.8-99.8%
of dependency parses in treebanks are 1-Endpoint-
Crossing trees.1
We next review and compare other relevant def-
initions of non-projectivity from prior work: well-
nested with block degree 2, gap-minding, projective,
and 2-planar.
The definitions of block degree and well-
nestedness are given below:
Definition 3. For each node u in the tree, a block of
the node is ?a longest segment consisting of descen-
dants of u.? (Kuhlmann, 2013). The block-degree of
u is ?the number of distinct blocks of u?. The block
degree of a tree is the maximum block degree of any
of its nodes. The gap degree is the number of gaps
between these blocks, and so by definition is one less
than the block degree. (Kuhlmann, 2013)
Definition 4. Two trees ?T1 and T2 interleave iff
there are nodes l1,r1 ? T1 and l2,r2 ? T2 such that
l1 < l2 < r1 < r2.? A tree is well-nested if no two
disjoint subtrees interleave. (Bodirsky et al, 2005)
As can be seen in Table 1, 95.4%-99.9% of depen-
dency parses across treebanks are both well-nested
and have block degree 2. The optimal such tree can
be found in O(n7) time and O(n5) space (G?mez-
Rodr?guez et al, 2011).
1Conventional edges from the artificial root node to the
root(s) of the sentence reduce the empirical coverage of 1-
Endpoint-Crossing trees. Excluding these artificial root edges,
the empirical coverage for Dutch rises to 12949 (97.0%). These
edges have no effect on the coverage of well-nested trees with
block degree at most 2, gap-minding trees, or projective trees.
a c d e fb g
(a)
a b c d fe g h
(b)
Figure 1: 1a is 1-Endpoint-Crossing, but is neither
block degree 2 nor well-nested; 1b is gap-minding
but not 2-planar.
Definition 5. A tree is gap-minding if it is well-
nested, has gap degree at most 1, and has gap inher-
itance degree 0. Gap inheritance degree 0 requires
that there are no child nodes with descendants in
more than one of their parent?s blocks. (Pitler et
al., 2012)
Gap-minding trees can be parsed in O(n5) (Pitler
et al, 2012). They have slightly less empirical cov-
erage, however: 90.4-97.7% (Table 1).
Definition 6. A tree is projective if it has block de-
gree 1 (gap degree 0).
This definition has the least coverage (as low as
63.6% for Dutch), but can be parsed in O(n3) (Eis-
ner, 2000).
Definition 7. A tree is 2-planar if each edge can be
drawn either above or below the sentence such that
no edges cross (G?mez-Rodr?guez and Nivre, 2010).
G?mez-Rodr?guez and Nivre (2010) presented a
transition-based parser for 2-planar trees, but there
is no known globally optimal parsing algorithm for
2-planar trees.
Clearly projective ( gap-minding ( well-nested
with block degree at most 2. In Section 5.1, we
prove the somewhat surprising fact that 1-Endpoint-
Crossing ( 2-planar. These are two distinct hi-
erarchies capturing different dimensions of non-
projectivity: 1-Endpoint-Crossing 6? well-nested
with block degree 2 (Figure 1a), and gap-minding
6? 2-planar (Figure 1b).
3 Edges (and their Crossing Point) Define
Isolated Crossing Regions
We introduce notation to facilitate the discussion:
14
Arabic Czech Danish Dutch Portuguese Swedish Parsing
1-Endpoint-Crossing 1457 (99.8) 71810 (98.8) 5144 (99.1) 12785 (95.8) 9007 (99.3) 10902 (98.7) O(n4)
Well-nested, block degree 2 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(n7)
Gap-Minding 1394 (95.5) 70695 (97.2) 4985 (96.1) 12068 (90.4) 8481 (93.5) 10787 (97.7) O(n5)
Projective 1297 (88.8) 55872 (76.8) 4379 (84.4) 8484 (63.6) 7353 (81.1) 9963 (90.2) O(n3)
Sentences 1460 72703 5190 13349 9071 11042
Table 1: Over 95% of the dependency parse trees in the CoNLL-X training sets are 1-Endpoint-Crossing
trees. Coverage statistics and parsing times of previously proposed properties are shown for comparison.
Definition 8. Within a 1-Endpoint-Crossing tree,
the (crossing) pencil2 of an edge e (P(e)) is defined
as the set of edges (sharing an endpoint) that cross e.
The (crossing pencil) point of an edge e (Pt(e)) is
defined as the endpoint that all edges in P(e) share.
We will use euv to indicate an edge in either direc-
tion between u and v, i.e., either u? v or u? v.
Before defining the parsing algorithm, we first
give some intuition by analogy to parsing for pro-
jective trees. (This argument mirrors that of Eisner
(2000, pps.38-39).) Projective trees can be produced
using dynamic programming over intervals. Inter-
vals are sufficient for projective trees: consider any
edge euv in a projective tree.
The vertices in (u, v) must only have edges to
vertices in [u, v]. If there were an edge between a
vertex in (u, v) and a vertex outside [u, v], such an
edge would cross euv, which would contradict the
assumption of projectivity. Thus every edge in a
projective tree creates one interior interval isolated
from the rest of the tree, allowing dynamic program-
ming over intervals. We can analyze the case of 1-
Endpoint-Crossing trees in a similar fashion:
Definition 9. An isolated interval [i, j] has no edges
between the vertices in (i, j) and the vertices out-
side of [i, j]. An interval and one exterior vertex
[i, j] ? {x} is called an isolated crossing region if
the following two conditions are satisfied:
1. There are no edges between the vertices? (i, j)
and vertices /? [i, j] ? {x}
2. None of the edges between x and vertices ?
(i, j) are crossed by any edges with both end-
points ? (i, j)
2This notation comes from an analogy to geometry: ?A set
of distinct, coplanar, concurrent lines is a pencil of lines? (Rin-
genberg, 1967, p. 221); concurrent lines all intersect at the same
single point.
u v p
(a) [u, v] ? {p}
u v p
(b) [v, p] ? {u}
u p v
(c) [u, p] ? {v}
u p v
(d) [p, v] ? {u}
Figure 2: An edge euv and Pt(euv) = p form two
sets of isolated crossing regions (Lemma 1). 2a and
2b show p /? (u, v); 2c and 2d show p ? (u, v).
Lemma 1. Consider any edge euv and Pt(euv) = p
in a 1-Endpoint-Crossing forest F . Let l, r, and m
denote the leftmost, rightmost, and middle point out
of {u, v, p}, respectively. Then the three points u,
v, and p define two isolated crossing regions: (1)
[l,m] ? {r}, and (2) [m, r] ? {l}.
Proof. First note that as p = Pt(euv), P(euv) is
non-empty: there must be at least one edge between
vertices ? (u, v) and vertices /? [u, v]. p is either
/? [u, v] (i.e., p = l?p = r) or? (u, v) (i.e., p = m):
Case 1: p = l ? p = r: Assume without loss of
generality that u < v < p (i.e., p = r).
(a) [u, v] ? {p} is an isolated crossing region
(Figure 2a): Condition 1: Assume for the sake of
contradiction that there were an edge between a ver-
tex? (u, v) and a vertex /? [u, v]?{p}. Then such an
edge would cross euv without having an endpoint at
p, which contradicts the 1-Endpoint-Crossing prop-
erty for euv.
Condition 2: Assume that for some epa such that
a ? (u, v), epa was crossed by an edge in the interior
of (u, v). The interior edge would not share an end-
point with euv; since euv also crosses epa, this con-
tradicts the 1-Endpoint-Crossing property for epa.
15
(b) [v, p] ? {u} is an isolated crossing region
(Figure 2b): Condition 1: Assume there were an
edge eab with a ? (v, p) and b /? [v, p] ? {u}. b
cannot be in (u, v) (by above). Thus, b /? [u, p],
which implies that eab crosses the edges in P(euv);
as euv does not share a vertex with eab, this contra-
dicts the 1-Endpoint-Crossing property for all edges
in P(euv).
Condition 2: Assume that for some eua such that
a ? (v, p), eua was crossed by an edge in the interior
of (v, p). eua would also be crossed by all the edges
in P(euv); as the interior edge would not share an
endpoint with any of the edges inP(euv), this would
contradict the 1-Endpoint-Crossing property for eua.
Case 2: p = m :
(a) [u, p] ? {v} is an isolated crossing region
(Figure 2c): Condition 1: Assume there were an
edge eab with a ? (u, p) and b /? [u, p] ? {v}
(b ? (p, v) ? b /? [u, v]). First assume b ? (p, v).
Then eab crosses all edges in P(euv); as eab does not
share an endpoint with euv, this contradicts the 1-
Endpoint-Crossing property for the edges inP(euv).
Next assume b /? [u, v]. Then eab crosses euv; since
a 6= p?b 6= p, this violates the 1-Endpoint-Crossing
property for euv.
Condition 2: Assume that for some eva with a ?
(u, p), eva was crossed by an edge in the interior of
(u, v). eva is also crossed by all the edges inP(euv);
as the interior edge will not share an endpoint with
the edges inP(euv), this contradicts the 1-Endpoint-
Crossing property for eva.
(b) [p, v] ? {u} is an isolated crossing region
(Figure 2d): Symmetric to the above.
4 Parsing Algorithm
The optimal 1-Endpoint-Crossing tree can be found
using a dynamic programming algorithm that ex-
ploits the fact that edges and their crossing points
define intervals and isolated crossing regions. This
section assumes an arc-factored model, in which the
score of a tree is defined as the sum of the scores of
its edges; scoring functions for edges are generally
learned from data.
(a) Only edges inci-
dent to the Left point
of the interval may
cross the edges from
the exterior point
(b) Only edges in-
cident to the Right
point of the inter-
val may cross the
edges from the exte-
rior point
(c) both (LR) (d) Neither
Figure 3: Isolated crossing region sub-problems.
The dynamic program uses five types of sub-
problems: interval sub-problems for each interval
[i, j], denoted Int[i, j], and four types of isolated
crossing region sub-problems for each interval and
exterior point [i, j] ? {x}, which differ in whether
edges from the exterior point may be crossed by
edges with an endpoint at the Left point of the inter-
val, the Right point, both LR, or Neither (Figure 3).
L[i, j, x], for example, refers to an isolated crossing
region over the interval [i, j] with an exterior point
of x, in which edges incident to i (the left boundary
point) can cross edges between x and (i, j).
These distinctions allow the 1-Endpoint-Crossing
property to be globally enforced; crossing edges in
one region may constrain edges in another. For ex-
ample, consider that Figure 2a allows edges with an
endpoint at v to cross the edges from p, while Figure
2b allows edges from u into (v, p). Both simultane-
ously would cause a 1-Endpoint-Crossing violation
for the edges in P(euv). Figures 4 and 5 show valid
combinations of the sub-problems in Figure 3.
The full dynamic program is shown in Appendix
A. The final answer must be a valid dependency tree,
which requires each word to have exactly one parent
and prohibits cycles. We use booleans (bi, bj , bx) for
each sub-problem, in which the boolean is set to true
if and only if the solution to the sub-problem must
contain the incoming (parent) edge for the corre-
sponding boundary point. We use the suffix AFromB
for a sub-problem to enforce that a boundary point A
must be descended from boundary point B (to avoid
cycles). We will occasionally mention these issues,
16
(a) If l ? (k, j]:
ki l j
(b) If l ? (i, k):
li k j
(i) If the dashed edge exists:
All the edges from l into (i, k) must choose k
as their Pt. The interval decomposes into
S[eik] +R[i, k, l] + Int[k, l] + L[l, j, k]:
ki l j
(ii) If no edges like the dashed edge exist:
All edges from l into (i, k) may choose either i
or k as their Pt. The interval decomposes into
S[eik] + LR[i, k, l] + Int[k, l] + Int[l, j]:
i k l j
(i) If dashed edge exists: All the edges from l into
(k, j] must choose i as their Pt. The interval decom-
poses into S[eik] + Int[i, l] + L[l, k, i] +N [k, j, l]:
li k j
(ii) If no edges like the dashed edge exist: All edges
from l may choose k as their Pt. The interval decom-
poses into S[eik] +R[i, l, k] + Int[l, k] + L[k, j, l]:
li k j
Figure 4: Decomposing an Int[i, j] sub-problem, with Pt(eik) = l
but for simplicity focus the discussion on the decom-
position into crossing regions and the maintenance
of the 1-Endpoint-Crossing property. Edge direction
does not affect these points of focus, and so we will
refer simply to S[euv] to mean the score of either the
edge from u to v or vice-versa.
In the following subsections, we show that the op-
timal parse for each type of sub-problem can be de-
composed into smaller valid sub-problems. If we
take the maximum over all these possible combina-
tions of smaller solutions, we can find the maximum
scoring parse for that sub-problem. Note that the
overall tree is a valid sub-problem (over the inter-
val [0, n]), so the argument will also hold for finding
the optimal overall tree. Each individual vertex and
each pair of adjacent vertices (with no edges) triv-
ially form isolated intervals (as there is no interior);
this forms the base case of the dynamic program.
The overall dynamic program takes O(n4) time:
there are O(n2) interval sub-problems, each of
which needs two free split points to find the max-
imum, and O(n3) region sub-problems, each of
which is a maximization over one free split point.
4.1 Decomposing an Int sub-problem
Consider an isolated interval sub-problem Int[i, j].
There are three cases: (1) there are no edges between
i and the rest of the interval, (2) the longest edge in-
cident to i is not crossed, (3) the longest edge inci-
dent to i is crossed. An Int sub-problem can be de-
composed into smaller valid sub-problems in each of
these three cases. Finding the optimal Int forest can
be done by taking the maximum over these cases:
No edges between i and [i + 1, j]: The same set
of edges is also a valid Int[i + 1, j] sub-problem.
bi must be true for the Int[i + 1, j] sub-problem to
ensure i+ 1 receives a parent.
Furthest edge from i is not crossed: If the furthest
edge is to j, the problem can be decomposed into
S[eij ] + Int[i, j], as that edge has no effect on the
interior of the interval. Clearly, this is only appli-
cable if the boundary point needed a parent (as in-
dicated by the booleans) and the boolean must then
be updated accordingly. If the furthest edge is to
some k in (i, j), the problem is decomposed into
S[eik] + Int[i, k] + Int[k, j].
Furthest edge from i is crossed: This is the most
17
interesting case, which uses two split points: the
other endpoint of the edge (k), and l = Pt(eik). The
dynamic program depends on the order of k and l.
l /? (i,k) (Figure 4a): By Lemma 1, [i, k]?{l} and
[k, l]?{i} form isolated regions. (l, j] is the remain-
der of the interval, and the only vertex from [i, l) that
can have edges into (l, j] is k: (i, k) and (k, l) are
part of isolated regions, and i is ruled out because k
was i?s furthest neighbor.
If at least one edge from k into (l, j] (the dashed
line in Figure 4a) exists, the decomposition is as in
Figure 4a, Case i; otherwise, it is as in Figure 4a,
Case ii. In Case i, eik and the edge(s) between k and
(l, j] force all of the edges between l and (i, k) to
have k as their Pt. Thus, the region [i, k]?{l}must
be a sub-problem of type R (Figure 3b), as these
edges from l can only be crossed by edges with an
endpoint at k (the right endpoint of [i, k]). All of the
edges between k and (l, j] have l as their Pt, as they
are crossed by all the edges in P(eik), and so the
sub-problem corresponding to the region [l, j]?{k}
is of type L (Figure 3a). In Case ii, each of the edges
in P(eik) may choose either i or k as their Pt, so the
sub-problem [i, k] ? {l} is of type LR (Figure 3c).
Note that l = j is a special case of Case ii in which
the rightmost interval Int[l, j] is empty.
l ? (i,k) (Figure 4b): [i, l] ? {k} and [l, k] ? {i}
form isolated crossing regions by Lemma 1. There
cannot both be edges between i and (l, k) and be-
tween k and (i, l), as this would violate 1-Endpoint-
Crossing for the edges in P(eik). If there are any
edges between i and (l, k) (i.e., Case i in Figure 4b),
then all of the edges in P(eik) must choose i as their
Pt, and so these edges cannot be crossed at all in
the region [k, j]?{l}, and there cannot be any edges
from k into (i, l). If there are no such edges (Case
ii in 4b), then k must be a valid Pt for all edges in
P(eik), and so there can both be edges from k into
(i, l) and [k, j] ? {l} may be of type L (allowing
crossings with an endpoint at k).
4.2 Decomposing an LR sub-problem
An LR sub-problem is over an isolated crossing re-
gion [i, j] ? {x}, such that edges from x into (i, j)
may be crossed by edges with an endpoint at either i
or j. This sub-problem is only defined when neither
i nor j get their parent from this sub-problem. From
a top-down perspective, this case is only used when
there will be an edge between i and j (as in one of
the sub-problems in Figure 4a, Case ii).
If none of the edges from x are crossed by any
edges with an endpoint at i, this can be considered
an R problem. Similarly, if none are crossed by any
edges with an endpoint at j, this may be considered
an L sub-problem. The only case which needs dis-
cussion is when both edges with an endpoint at i and
also at j cross edges from x; see Figure 3c for a
schematic. In that scenario, there must exist a split
point such that: (1) to the left of the point, all edges
crossing x-edges have an endpoint at i, and to the
right of the point, all such edges have an endpoint at
j, and (2) no edges in the region cross the split point.
Let ri be i?s rightmost child in (i, j); let lj be
j?s leftmost child in (i, j). Every edge from x into
(i, ri) is crossed by eiri ; every edge between x and
(lj , j) is crossed by eljj . eiri cannot cross eljj , as
that would either violate 1-Endpoint-Crossing (be-
cause of the x-interior edges) or create a cycle (if
both children are also connected by an edge to x). ri
and lj also cannot be equal: as neither i nor j may
be assigned a parent, they must both be in the direc-
tion of the child, and the child cannot have multiple
parents. Thus, ri is to the left of lj .
Any split point between ri and lj clearly satis-
fies (1). There is at least one point within [ri, lj ]
that satisfies (2) as long as there is not a chain
of crossing edges from eiri to eljj . The proof is
omitted for space reasons, but such a chain can be
ruled out using a counting argument similar to that
in the proof in Section 5.1. The decomposition is:
L[i, k, x] +R[k, j, x] for some k ? (i, j).
4.3 Decomposing an N sub-problem
Consider the maximum scoring forest of type N
over [i, j] ? {x} (Figure 3d; no edges from x are
crossed in this sub-problem). If there are no edges
from x, then it is also a valid Int[i, j] sub-problem.
If there are edges between x and the endpoints i or j,
then the forest with that edge removed is still a valid
N sub-problem (with the ancestor and parent book-
keeping updated). Otherwise, if there are edges be-
tween x and (i, j), choose the neighbor of x closest
to j (call it k). Since the edge exk is not crossed,
there are no edges from [i, k) into (k, j]; since k was
the neighbor of x closest to j, there are no edges
from x into (k, j]. Thus, the region decomposes into
18
x k ji
(i) If dashed edge exists: All the edges from i into
(k, j] must choose x as their Pt. The interval decom-
poses into S[exk] + L[i, k, x] +N [k, j, i]:
x k ji
(ii) If no edges like the dashed edge exist: Edges
from i into (k, j] may choose k as their Pt. The in-
terval decomposes into S[exk]+Int[i, k]+L[k, j, i]:
x k ji
Figure 5: An L sub-problem over [i, j] ? {x}, k is
the neighbor of x furthest from i in the interval.
S[eik] + Int[k, j] +N [i, k, x].
As an aside, if bx was true (x needed a parent
from this sub-problem), and k was a child of x,
then x?s parent must come from the [i, k]?{x} sub-
problem. However, it cannot be a descendant of k,
as that would cause a cycle. Thus in this case, we
call the sub-problem a N_XFromI problem, to in-
dicate that x needs a parent, i and k do not, and x
must be descended from i, not k.
4.4 Decomposing an L or R sub-problem
An L sub-problem over [i, j]?{x} requires that any
edges in this region that cross an edge with an end-
point at x have an endpoint at i (the left endpoint). If
there are no edges between x and [i, j] in an L sub-
problem, then it is also a valid Int sub-problem over
[i, j]. If there are edges between x and i or j, then
the sub-problem can be decomposed into that edge
plus the rest of the forest with that edge removed.
The interesting case is when there are edges be-
tween x and the interior (Figure 5). Let k be the
neighbor of x within (i, j) that is furthest from i. As
all edges that cross exk will have an endpoint at i,
there are no edges between (i, k) and (k, j]. Com-
bined with the fact that k was the neighbor of x clos-
est to j, we have that [i, k] ? {x} must form an iso-
a b c d e f
Figure 6: 2-planar but not 1-Endpoint-Crossing
lated crossing region, as must [k, j] ? {i}.
If there are additional edges between x and the in-
terior (Case i in 5), all of the edges from i into (k, j]
cross both the edge exk and the other edges from x
into (i, k). The Pt for all these edges must there-
fore be x, and as x is not in the region [k, j] ? {i},
those edges cannot be crossed at all in that region
(i.e., [k, j] ? {i} must be of type N ). If there are no
additional edges from x into (i, k) (Case ii in Fig-
ure 5), then all of the edges from i into (k, j) must
choose either x or k as their Pt. As there will be no
more edges from x, choosing k as their Pt allows
strictly more trees, and so [k, j]? {i} can be of type
L (allowing edges from i to be crossed in that region
by edges with an endpoint at k).
An R sub-problem is identical, with k instead
chosen to be the neighbor of x furthest from j.
5 Connections
5.1 Graph Theory: All 1-Endpoint-Crossing
Trees are 2-Planar
The 2-planar characterization of dependency struc-
tures in G?mez-Rodr?guez and Nivre (2010) exactly
correspond to 2-page book embeddings in graph the-
ory: an embedding of the vertices in a graph onto
a line (by analogy, along the spine of a book), and
the edges of the graph onto one of 2 (more gener-
ally, k) half-planes (pages of the book) such that no
edges on the same page cross (Bernhart and Kainen,
1979). The problem of finding an embedding that
minimizes the number of pages required is a natural
formulation of many problems arising in disparate
areas of computer science, for example, sorting a se-
quence using the minimum number of stacks (Even
and Itai, 1971), or constructing fault-tolerant layouts
in VLSI design (Chung et al, 1987).
In this section we prove 1-Endpoint-Crossing ?
2-planar. These classes are not equal (Figure 6).
We first prove some properties about the crossings
graphs (G?mez-Rodr?guez and Nivre, 2010) of 1-
Endpoint-Crossing trees. The crossings graph of a
19
(a,b) (a,c)
(b,d) (c,e)
(d,f)
(a)
(a,b) (a,c)
(b,e)
(g,d)(h,f)
(b,g)
(g,h)
(b)
Figure 7: The crossing graphs for Figures 1a and 1b.
graph has a vertex corresponding to each edge in
the original, and an edge between two vertices if the
two edges they correspond to cross. The crossings
graphs for the dependency trees in Figures 1a and
1b are shown in Figures 7a and 7b, respectively.
Lemma 2. No 1-Endpoint-Crossing tree has a cycle
of length 3 in its crossings graph.
Proof. Assume there existed a cycle e1, e2, e3. e1
and e3 must share an endpoint, as they both cross
e2. Since e1 and e3 share an endpoint, e1 and e3 do
not cross. Contradiction.
Lemma 3. Any odd cycle of size n (n ? 4) in a
crossings graph of a 1-Endpoint-Crossing tree uses
at most n distinct vertices in the original graph.
Proof. Let e1, e2, ..., en be an odd cycle in a cross-
ings graph of a 1-Endpoint-Crossing tree with n ?
4. Since n ? 4, e1, e2, en?1, and en are distinct
edges. Let a be the vertex that e1 and en?1 share
(because they both cross en) and let b be the vertex
that e2 and en share (both cross e1). Note that e1
and en?1 cannot contain b and that e2 and en cannot
contain a (otherwise they would not cross an edge
adjacent to them along the cycle).
We will now consider how many vertices each
edge can introduce that are distinct from all vertices
previously seen in the cycle. e1 and e2 necessarily
introduce two distinct vertices each.
Let eo be the first odd edge that contains b (we
know one exists since en contains b). (o is at least 3,
since e1 does not contain b.) eo?s other vertex must
be the one shared with eo?2 (eo?2 does not contain b,
since eo was the first odd edge to contain b). There-
fore, both of eo?s vertices have already been seen
along the cycle.
Similarly, let ee be the first even edge that con-
tains an a. By the same reasoning, ee must not in-
troduce any new vertices.
All other edges ei such that i > 2 and ei 6= eo and
ei 6= ee introduce at most one new vertex, since one
must be shared with the edge ei?2. There are n ? 4
such edges.
Counting up all possibilities, the maximum num-
ber of distinct vertices is 4 + (n? 4) = n.
Theorem 1. 1-Endpoint-Crossing trees ? 2-planar.
Proof. Assume there existed an odd cycle in the
crossings graph of a 1-Endpoint-Crossing tree. The
cycle has size at least 5 (by Lemma 2). There are
at least as many edges as vertices in the subgraph of
the forest induced by the vertices used in the cycle
(by Lemma 3). That implies the existence of a cycle
in the original graph, contradicting that the original
graph was a tree.
Since there are no odd cycles in the crossings
graph, the crossings graph of edges is bipartite. Each
side of the bipartite graph can be assigned to a page,
such that no two edges on the same page cross.
Therefore, the original graph was 2-planar.
5.2 Linguistics: Cross-serial Verb
Constructions and Successive Cyclicity
Cross-serial verb constructions were used to provide
evidence for the ?non-context-freeness? of natural
language (Shieber, 1985). Cross-serial verb con-
structions with two verbs form 1-Endpoint-Crossing
trees. Below is a cross-serial sentence from Swiss-
German, from (1) in Shieber (1985):
das mer em Hans es huus h?lfed aastriiche
that we HansDAT the houseACC helped paint
The edges (that , helped), (helped ,we), and
(helped ,Hans) are each only crossed by an edge
with an endpoint at paint; the edge (paint , house)
is only crossed by edges with an endpoint at helped.
More generally, with a set of two cross serial verbs
in a subordinate clause, each verb should suffice as
the crossing point for all edges incident to the other
verb that are crossed.
Cross-serial constructions with three or more
verbs would have dependency trees that violate 1-
20
What did say BA C ... Z ate t ?nsaid 1 said 2t1 t2
Figure 8: An example of wh-movement over a poten-
tially unbounded number of clauses. The edges be-
tween the heads of each clause cross the edges from
trace to trace, but all obey 1-Endpoint-Crossing.
Endpoint-Crossing. Psycholinguistically, between
two and three verbs is exactly where there is a large
change in the sentence processing abilities of human
listeners (based on both grammatical judgments and
scores on a comprehension task) (Bach et al, 1986).
More speculatively, there may be a connection
between the form of 1-Endpoint-Crossing trees and
phases (roughly, propositional units such as clauses)
in Minimalism (Chomsky et al, 1998). Figure 8
shows an example of wh-movement over a poten-
tially unbounded number of clauses. The phase-
impenetrability condition (PIC) states that only the
head of the phase and elements that have moved to
its edge are accessible to the rest of the sentence
(Chomsky et al, 1998, p.22). Movement is there-
fore required to be successive cyclic, with a moved
element leaving a chain of traces at the edge of
each clause on its way to its final pronounced loca-
tion (Chomsky, 1981). In Figure 8, notice that the
crossing edges form a repeated pattern that obeys
the 1-Endpoint-Crossing property. More generally,
we suspect that trees satisfying the PIC will tend to
also be 1-Endpoint-Crossing. Furthermore, if the
traces were not at the edge of each clause, and in-
stead were positioned between a head and one of
its arguments, 1-Endpoint-Crossing would be vio-
lated. For example, if t2 in Figure 8 were be-
tween C and said2, then the edge (t1, t2) would cross
(say, said1), (said1, said2), and (C, said2), which
do not all share an endpoint. An exploration of these
linguistic connections may be an interesting avenue
for further research.
6 Conclusions
1-Endpoint-Crossing trees characterize over 95% of
structures found in natural language treebank, and
can be parsed in only a factor of n more time than
projective trees. The dynamic programming algo-
rithm for projective trees (Eisner, 2000) has been
extended to handle higher order factors (McDonald
and Pereira, 2006; Carreras, 2007; Koo and Collins,
2010), adding at most a factor of n to the edge-
based running time; it would be interesting to ex-
tend the algorithm presented here to include higher
order factors. 1-Endpoint-Crossing is a condition
on edges, while properties such as well-nestedness
or block degree are framed in terms of subtrees.
Three edges will always suffice as a certificate of a
1-Endpoint-Crossing violation (two vertex-disjoint
edges that both cross a third). In contrast, for a
property like ill-nestedness, two nodes might have
a least common ancestor arbitrarily far away, and so
one might need the entire graph to verify whether
the sub-trees rooted at those nodes are disjoint and
ill-nested. We have discussed cross-serial depen-
dencies; a further exploration of which linguistic
phenomena would and would not have 1-Endpoint-
Crossing dependency trees may be revealing.
Acknowledgments
We would like to thank Julie Legate for an in-
teresting discussion. This material is based upon
work supported under a National Science Foun-
dation Graduate Research Fellowship, NSF Award
CCF 1137084, and Army Research Office MURI
grant W911NF-07-1-0216.
A Dynamic Program to find the maximum
scoring 1-Endpoint-Crossing Tree
Input: Matrix S: S[i, j] is the score of the directed edge (i, j)
Output: Maximum score of a 1-Endpoint-Crossing tree over
vertices [0, n], rooted at 0
Init: ?i Int[i, i, F, F ] = Int[i, i+ 1, F, F ] = 0
Int[i, i, T, F ] = Int[i, i, F, T ] = Int[i, i, T, T ] = ??
Final: Int[0, n, F, T ]
Shorthand for booleans: T F (x, S) :=
if x = T , exactly one of the set S is true
if x = F , all of the set S must be false
bi, bj , bx are true iff the corresponding boundary point has its
incoming edge (parent) in that sub-problem. For the LR sub-
problem, bi and bj are always false, and so omitted. For all
sub-problems with the suffix AFromB, the boundary point A
has its parent edge in the sub-problem solution; the other two
boundary points do not. For example, L_XFromI would cor-
respond to having booleans bi = bj = F and bx = T , with the
restriction that x must be a descendant of i.
21
Int[i, j, F, bj ]? max?
??????????
??????????
Int[i+ 1, j, T, F ] if bj = F
S[i, j] + Int[i, j, F, F ] if bj = T
max
k?(i,j)
S[i, k]+
?
???????
???????
Int[i, k, F, F ] + Int[k, j, F, bj ]
max
T F (bj ,{bl,br})
LR[i, k, j, bl] + Int[k, j, F, br]
maxl?(k,j),T F (T,{bl,bm,br}){
R[i, k, l, F, F, bl] + Int[k, l, F, bm] + L[l, j, k, br, bj , F ]
LR[i, k, l, bl] + Int[k, l, F, bm] + Int[l, j, br, bj ]
maxl?(i,k),T F (T,{bl,bm,br}){
Int[i, l, F, bl] + L[l, k, i, bm, F, F ] +N [k, j, l, F, bj , br]
R[i, l, k, F, bl, F ] + Int[l, k, bm, F ] + L[k, j, l, F, bj , br]
Int[i, j, T, F ]? symmetric to Int[i, j, F, T ]
Int[i, j, T, T ]? ??
LR[i, j, x, bx]? max?
??
??
L[i, j, x, F, F, bx]
R[i, j, x, F, F, bx]
maxk?(i,j),T F (bx,{bxl,bxr}),T F (T,{bkl,bkr})
L[i, k, x, F, bkl, bxl] +R[k, j, x, bkr, F, bxr]
N [i, j, x, bi, bj , F ]? max?
???
???
Int[i, j, bi, bj ]
S[x, i] +N [i, j, x, F, bj , F ] if bi = T
S[x, j] +N [i, j, x, bi, F, F ] if bj = T
max
k?(i,j)
S[x, k] +N [i, k, x, bi, F, F ] + Int[k, j, F, bj ]
N [i, j, x, F, bj , T ]? max?
???????
???????
S[i, x] +N [i, j, x, F, bj , F ]
S[x, j] +N_XFromI[i, j, x] if bj = T
S[j, x] +N [i, j, x, F, F, F ] if bj = F
S[j, x] + Int[i, j, F, T ] if bj = T
max
k?(i,j)
S[x, k] +N_XFromI[i, k, x] + Int[k, j, F, bj ]
max
k?(i,j)
S[k, x]+
{
Int[i, k, F, T ] + Int[k, j, F, bj ]
N [i, k, x, F, F, F ] + Int[k, j, T, bj ]
N [i, j, x, T, F, T ]? symmetric to N [i, j, x, F, T, T ]
N [i, j, x, T, T, T ]? ??
N_XFromI[i, j, x]? max?
??
??
S[i, x] +N [i, j, x, F, F, F ]
maxk?(i,j){
S[x, k] +N_XFromI[i, k, x] + Int[k, j, F, F ]
S[k, x] + Int[i, k, F, T ] + Int[k, j, F, F ]
N_IFromX[i, j, x]? max{
S[x, i] +N [i, j, x, F, F, F ]
max
k?(i,j)
S[x, k] +N [i, k, x, T, F, F ] + Int[k, j, F, F ]
N_XFromJ [i, j, x]? symmetric to N_XFromI[i, j, x]
N_JFromX[i, j, x]? symmetric to N_IFromX[i, j, x]
L[i, j, x, bi, bj , F ]? max?
?????
?????
Int[i, j, bi, bj ]
S[x, i] + L[i, j, x, F, bj , F ] if bi = T
S[x, j] + L[i, j, x, bi, F, F ] if bj = T
max
k?(i,j),T F (bi,{bl,br})
S[x, k]+
{
L[i, k, x, bl, F, F ] +N [k, j, i, F, bj , br]
Int[i, k, bl, F ] + L[k, j, i, F, bj , br]
L[i, j, x, F, bj , T ]? max?
?????????
?????????
S[i, x] + L[i, j, x, F, bj , F ]
S[x, j] + L_XFromI[i, j, x] if bj = T
S[j, x] + L[i, j, x, F, F, F ] if bj = F
S[j, x] + L_JFromI[i, j, x] if bj = T
max
k?(i,j)
S[x, k] + L_XFromI[i, k, x] +N [k, j, i, F, bj , F ]
max
k?(i,j)
S[k, x]+
?
??
??
L_JFromI[i, k, x] +N [k, j, i, F, bj , F ]
L[i, k, x, F, F, F ] +N [k, j, i, T, bj , F ]
max
T F (T,{bl,br})
Int[i, k, F, bl] + L[k, j, i, br, bj , F ]
L[i, j, x, T, bj , T ]? not reachable
L_XFromI[i, j, x]? max?
??????
??????
S[i, x] + L[i, j, x, F, F, F ]
max
k?(i,j)
S[x, k] + L_XFromI[i, k, x] +N [k, j, i, F, F, F ]
max
k?(i,j)
S[k, x]+
?
??
??
L_JFromI[i, k, x] +N [k, j, i, F, F, F ]
L[i, k, x, F, F, F ] +N_IFromX[k, j, i]
Int[i, k, F, T ] + L[k, j, i, F, F, F ]
Int[i, k, F, F ] + L_IFromX[k, j, i]
L_IFromX[i, j, x]? max?
?????
?????
S[x, i] + L[i, j, x, F, F, F ]
max
k?(i,j)
S[x, k]+
?
??
??
L[i, k, x, T, F, F ] +N [k, j, i, F, F, F ]
L[i, k, x, F, F, F ] +N_XFromI[k, j, i]
Int[i, k, T, F ] + L[k, j, i, F, F, F ]
Int[i, k, F, F ] + L_XFromI[k, j, i]
L_JFromX[i, j, x]? max?
???
???
S[x, j] + L[i, j, x, F, F, F ]
max
k?(i,j)
S[x, k]+
{
L[i, k, x, F, F, F ] + Int[k, j, F, T ]
Int[i, k, F, F ] + L_JFromI[k, j, i]
L_JFromI[i, j, x]? max?
???
???
Int[i, j, F, T ]
max
k?(i,j)
S[x, k]+
{
L[i, k, x, F, F, F ] +N_JFromX[k, j, i]
Int[i, k, F, F ] + L_JFromX[k, j, i]
22
R[i, j, x, bi, bj , F ]? symmetric to L[i, j, x, bi, bj , F ]
R[i, j, x, bi, F, T ]? symmetric to L[i, j, x, F, bj , T ]
R[i, j, x, bi, T, T ]? not reachable
R_XFromJ [i, j, x]? symmetric to L_XFromI[i, j, x]
R_JFromX[i, j, x]? symmetric to L_IFromX[i, j, x]
R_IFromX[i, j, x]? symmetric to L_JFromX[i, j, x]
R_IFromJ [i, j, x]? symmetric to L_JFromI[i, j, x]
References
E. Bach, C. Brown, and W. Marslen-Wilson. 1986.
Crossed and nested dependencies in german and dutch:
A psycholinguistic study. Language and Cognitive
Processes, 1(4):249?262.
F. Bernhart and P.C. Kainen. 1979. The book thickness
of a graph. Journal of Combinatorial Theory, Series
B, 27(3):320 ? 331.
M. Bodirsky, M. Kuhlmann, and M. M?hl. 2005. Well-
nested drawings as models of syntactic structure. In
In Tenth Conference on Formal Grammar and Ninth
Meeting on Mathematics of Language, pages 88?1.
University Press.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, vol-
ume 7, pages 957?961.
N. Chomsky, Massachusetts Institute of Technology.
Dept. of Linguistics, and Philosophy. 1998. Minimal-
ist inquiries: the framework. MIT occasional papers
in linguistics. Distributed by MIT Working Papers in
Linguistics, MIT, Dept. of Linguistics.
N. Chomsky. 1981. Lectures on Government and Bind-
ing. Dordrecht: Foris.
F. Chung, F. Leighton, and A. Rosenberg. 1987. Em-
bedding graphs in books: A layout problem with ap-
plications to VLSI design. SIAM Journal on Algebraic
Discrete Methods, 8(1):33?58.
H. Cui, R. Sun, K. Li, M.Y. Kan, and T.S. Chua. 2005.
Question answering passage retrieval using depen-
dency relations. In Proceedings of the 28th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 400?407.
ACM.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 423. Association for Compu-
tational Linguistics.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
541?548. Association for Computational Linguistics.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 29?62. Kluwer Academic
Publishers, October.
S. Even and A. Itai. 1971. Queues, stacks, and graphs.
In Proc. International Symp. on Theory of Machines
and Computations, pages 71?86.
C. G?mez-Rodr?guez and J. Nivre. 2010. A transition-
based parser for 2-planar dependency structures. In
Proceedings of ACL, pages 1492?1501.
C. G?mez-Rodr?guez, J. Carroll, and D. Weir. 2011. De-
pendency parsing schemata and mildly non-projective
dependency parsing. Computational Linguistics,
37(3):541?586.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of ACL, pages 1?11.
M. Kuhlmann. 2013. Mildly non-projective dependency
grammar. Computational Linguistics, 39(2).
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81?88.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 121?132.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, pages 523?530. As-
sociation for Computational Linguistics.
E. Pitler, S. Kannan, and M. Marcus. 2012. Dynamic
programming for higher order parsing of gap-minding
trees. In Proceedings of EMNLP, pages 478?488.
L.A. Ringenberg. 1967. College geometry. Wiley.
A. Rush and S. Petrov. 2012. Vine pruning for effi-
cient multi-pass dependency parsing. In Proceedings
of NAACL, pages 498?507.
S.M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philoso-
phy, 8(3):333?343.
H. Zhang and R. McDonald. 2012. Generalized higher-
order dependency parsing with cube pruning. In Pro-
ceedings of EMNLP, pages 320?331.
23
24
A Crossing-Sensitive Third-Order Factorization for Dependency Parsing
Emily Pitler?
Google Research
76 9th Avenue
New York, NY 10011
epitler@google.com
Abstract
Parsers that parametrize over wider scopes are
generally more accurate than edge-factored
models. For graph-based non-projective
parsers, wider factorizations have so far im-
plied large increases in the computational
complexity of the parsing problem. This paper
introduces a ?crossing-sensitive? generaliza-
tion of a third-order factorization that trades
off complexity in the model structure (i.e.,
scoring with features over multiple edges)
with complexity in the output structure (i.e.,
producing crossing edges). Under this model,
the optimal 1-Endpoint-Crossing tree can be
found in O(n4) time, matching the asymp-
totic run-time of both the third-order projec-
tive parser and the edge-factored 1-Endpoint-
Crossing parser. The crossing-sensitive third-
order parser is significantly more accurate
than the third-order projective parser under
many experimental settings and significantly
less accurate on none.
1 Introduction
Conditioning on wider syntactic contexts than sim-
ply individual head-modifier relationships improves
parsing accuracy in a wide variety of parsers and
frameworks (Charniak and Johnson, 2005; McDon-
ald and Pereira, 2006; Hall, 2007; Carreras, 2007;
Martins et al., 2009; Koo and Collins, 2010; Zhang
and Nivre, 2011; Bohnet and Kuhn, 2012; Martins
et al., 2013). This paper proposes a new graph-
based dependency parser that efficiently produces
?The majority of this work was done while at the University
of Pennsylvania.
the globally optimal dependency tree according to a
third-order model (that includes features over grand-
parents and siblings in the tree) in the class of 1-
Endpoint-Crossing trees (that includes all projective
trees and the vast majority of non-projective struc-
tures seen in dependency treebanks).
Within graph-based projective parsing, the third-
order parser of Koo and Collins (2010) has a run-
time of O(n4), just one factor of n more expensive
than the edge-factored model of Eisner (2000). In-
corporating richer features and producing trees with
crossing edges has traditionally been a challenge,
however, for graph-based dependency parsers. If
parsing is posed as the problem of finding the op-
timal scoring directed spanning tree, then the prob-
lem becomes NP-hard when trees are scored with a
grandparent and/or sibling factorization (McDonald
and Pereira, 2006; McDonald and Satta, 2007). For
various definitions of mildly non-projective trees,
even edge-factored versions are expensive, with
edge-factored running times between O(n4) and
O(n7) (Go?mez-Rodr??guez et al., 2011; Pitler et al.,
2012; Pitler et al., 2013; Satta and Kuhlmann, 2013).
The third-order projective parser of Koo and
Collins (2010) and the edge-factored 1-Endpoint-
Crossing parser described in Pitler et al. (2013) have
some similarities: both use O(n4) time and O(n3)
space, using sub-problems over intervals with one
exterior vertex, which are constructed using one
free split point. The two parsers differ in how the
exterior vertex is used: Koo and Collins (2010)
use the exterior vertex to store a grandparent in-
dex, while Pitler et al. (2013) use the exterior ver-
tex to introduce crossed edges between the point and
41
Transactions of the Association for Computational Linguistics, 2 (2014) 41?54. Action Editor: Joakim Nivre.
Submitted 9/2013; Revised 11/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
Projective 1-Endpoint-Crossing
Edge O(n
3) O(n4)
Eisner (2000) Pitler et al. (2013)
CS-GSib O(n
4) O(n4)
Koo and Collins (2010) This paper
Table 1: Parsing time for various output spaces and model
factorizations. CS-GSib refers to the (crossing-sensitive)
grand-sibling factorization described in this paper.
the interval. This paper proposes merging the two
parsers to achieve the best of both worlds ? produc-
ing the best tree in the wider range of 1-Endpoint-
Crossing trees while incorporating the identity of
the grandparent and/or sibling of the child in the
score of an edge whenever the local neighborhood
of the edge does not contain crossing edges. The
crossing-sensitive grandparent-sibling 1-Endpoint-
Crossing parser proposed here takes O(n4) time,
matching the runtime of both the third-order pro-
jective parser and of the edge-factored 1-Endpoint-
Crossing parser (see Table 1).
The parsing algorithms of Koo and Collins (2010)
and Pitler et al. (2013) are reviewed in Section 2.
The proposed crossing-sensitive factorization is de-
fined in Section 3. The parsing algorithm that finds
the optimal 1-Endpoint-Crossing tree according to
this factorization is described in Section 4. The
implemented parser is significantly more accurate
than the third-order projective parser in a variety
of languages and treebank representations (Section
5). Section 6 discusses the proposed approach in the
context of prior work on non-projective parsing.
2 Preliminaries
In a projective dependency tree, each subtree forms
one consecutive interval in the sequence of input
words; equivalently (assuming an artificial root node
placed as either the first or last token), when all
edges are drawn in the half-plane above the sen-
tence, no two edges cross (Ku?bler et al., 2009). Two
vertex-disjoint edges cross if their endpoints inter-
leave. A 1-Endpoint-Crossing tree is a dependency
tree such that for each edge, all edges that cross it
share a common vertex (Pitler et al., 2013). Note
that the class of projective trees is properly included
within the class of 1-Endpoint-Crossing trees.
To avoid confusion between intervals and edges,
g h e
=
g h m
+
h m e
(a) m is the child of h that e is descended from
g h
=
g h
+
hm ss m
(b) The edge ~ehm is added to the tree; s is m?s
adjacent inner sibling
= +
hm h s r+1r msh
(c) r is s?s outermost descendant; r + 1 is m?s
innermost descendant
Figure 1: Algorithm for grand-sibling projective parsing;
the figures replicate Figure 6 in Koo and Collins (2010).
~eij denotes the directed edge from i to j (i.e., i is the
parent of j). Interval notation ((i, j), [i, j], (i, j], or
[i, j)) is used to denote sets of vertices between i and
j, with square brackets indicating closed intervals
and round brackets indicating open intervals.
2.1 Grand-Sibling Projective Parsing
A grand-sibling factorization allows features over
4-tuples of (g, h,m, s), where h is the parent of
m, g is m?s grandparent, and s is m?s adjacent in-
ner sibling. Features over these grand-sibling 4-
tuples are referred to as ?third-order? because they
scope over three edges simultaneously (~egh, ~ehs, and
~ehm). The parser of Koo and Collins (2010) pro-
duces the highest-scoring projective tree according
to this grand-sibling model by adding an external
grandparent index to each of the sub-problems used
in the sibling factorization (McDonald and Pereira,
2006). Figure 6 in Koo and Collins (2010) provided
a pictorial view of the algorithm; for convenience, it
is replicated in Figure 1. An edge ~ehm is added to the
tree in the ?trapezoid? step (Figure 1b); this allows
the edge to be scored conditioned on m?s grandpar-
ent (g) and its adjacent inner sibling (s), as all four
relevant indices are accessible.
2.2 Edge-factored 1-Endpoint-Crossing
Parsing
The edge-factored 1-Endpoint-Crossing parser of
Pitler et al. (2013) produces the highest scoring 1-
42
* Which cars do Americans0 1 2 3 4 ?daysfavor most these 98765
Figure 2: A 1-Endpoint-Crossing non-projective English
sentence from the WSJ Penn Treebank (Marcus et al.,
1993), converted to dependencies with PennConverter
(Johansson and Nugues, 2007).
do Americans favor
do ?daysfavor most these
* do
* Which cars do favor
Figure 3: Constructing a 1-Endpoint-Crossing tree with
intervals with one exterior vertex (Pitler et al., 2013).
Endpoint-Crossing tree with each edge ~ehm scored
according to Score(Edge(h,m)). The 1-Endpoint-
Crossing property allows the tree to be built up in
edge-disjoint pieces each consisting of intervals with
one exterior point that has edges into the interval.
For example, the tree in Figure 2 would be built up
with the sub-problems shown in Figure 3.
To ensure that crossings within a sub-problem are
consistent with the crossings that happen as a result
of combination steps, the algorithm uses four dif-
ferent ?types? of sub-problems, indicating whether
the edges incident to the exterior point may be inter-
nally crossed by edges incident to the left boundary
point (L), the right (R), either (LR), or neither (N ).
In Figure 3, the sub-problem over [*, do] ? {favor}
would be of type R, and [favor, ?]? {do} of type L.
2.2.1 Na??ve Approach to Including
Grandparent Features
The example in Figure 3 illustrates the difficulty of
incorporating grandparents into the scoring of all
edges in 1-Endpoint-Crossing parsing. The vertex
favor has a parent or child in all three of the sub-
problems. In order to use grandparent scoring for
the edges from favor to favor?s children in the other
two sub-problems, we would need to augment the
problems with the grandparent index do. We also
must add the parent index do to the middle sub-
problem to ensure consistency (i.e., that do is in fact
the parent assigned). Thus, a first attempt to score all
edges with grandparent features within 1-Endpoint-
Crossing trees raises the runtime from O(n4) to
O(n7) (all of the four indices need a ?predicted par-
ent? index; at least one edge is always implied so
one of these additional indices can be dropped).
3 Crossing-Sensitive Factorization
Factorizations for projective dependency parsing
have often been designed to allow efficient pars-
ing. For example, the algorithms in Eisner (2000)
and McDonald and Pereira (2006) achieve their ef-
ficiency by assuming that children to the left of the
parent and to the right of the parent are independent
of each other. The algorithms of Carreras (2007)
and Model 2 in Koo and Collins (2010) include
grandparents for only the outermost grand-children
of each parent for efficiency reasons. In a similar
spirit, this paper introduces a variant of the Grand-
Sib factorization that scores crossed edges indepen-
dently (as a CrossedEdge part) and uncrossed edges
under either a grandparent-sibling, grandparent, sib-
ling, or edge-factored model depending on whether
relevant edges in its local neighborhood are crossed.
A few auxiliary definitions are required. For any
parent h and grandparent g, h?s children are parti-
tioned into interior children (those between g and h)
and exterior children (the complementary set of chil-
dren).1 Interior children are numbered from closest
to h through furthest from h; exterior children are
first numbered on the side closer to h from closest
to h through furthest, then the enumeration wraps
around to include the vertices on the side closer to g.
Figure 4 shows a parent h, its grandparent g, and a
possible sequence of three interior and four exterior
children. Note that for a projective tree, there would
not be any children on the far side of g.
Definition 1. Let h be m?s parent. Outer(m) is the
set of siblings of m that are in the same subset of h?s
children and are later in the enumeration than m is.
For example, in the tree in Figure 2,
1Because dependency trees are directed trees, each node ex-
cept for the artificial root has a unique parent. To ensure that
grandparent is defined for the root?s children, assume an artifi-
cial parent of the root for notational convenience.
43
e1 e2i1i2i3 hge3 e4
Figure 4: The exterior children are numbered first begin-
ning on the side closest to the parent, then the side closest
to the grandparent. There must be a path from the root to
g, so the edges from h to its exterior children on the far
side of g are guaranteed to be crossed.
Crossed(~ehs) ?Crossed(~ehs)
?GProj (~ehm) Edge(h,m) Sib(h,m, s)
GProj (~ehm) Grand(g, h,m) GrandSib(g, h,m, s)
Table 2: Part type for an uncrossed edge ~ehm for
the crossing-sensitive third-order factorization (g is m?s
grandparent; s is m?s inner sibling).
Outer(most) = {days, cars}.
Definition 2. An uncrossed edge ~ehm is GProj if
both of the following hold:
1. The edge ~egh from h?s parent to h is not crossed
2. None of the edges from h to Outer(m) (m?s
outer siblings) are crossed
Uncrossed GProj edges include the grandparent
in the part. The part includes the sibling if the edge
~ehs from the parent to the sibling is not crossed. Ta-
ble 2 gives the factorization for uncrossed edges.
The parser in this paper finds the optimal 1-
Endpoint-Crossing tree according to this factorized
form. A fully projective tree would decompose into
exclusively GrandSib parts (as all edges would be
uncrossed and GProj ). As all projective trees are
within the 1-Endpoint-Crossing search space, the
optimization problem that the parser solves includes
all projective trees scored with grand-sibling fea-
tures everywhere. Projective parsing with grand-
sibling scores can be seen as a special case, as the
crossing-sensitive 1-Endpoint-Crossing parser can
simulate a grand-sibling projective parser by setting
all Crossed(h,m) scores to ??.
In Figure 2, the edge from do to Americans is
not GProj because Condition (1) is violated, while
the edge from favor to most is not GProj because
Condition (2) is violated. Under this definition, the
vertices do and favor (which have children in mul-
tiple sub-problems) do not need external grandpar-
ent indices in any of their sub-problems. Table 3
CrossedEdge(*,do) Sib(cars, Which, -)
CrossedEdge(favor,cars) Sib(do, Americans, -)
Sib(do, favor, Americans) CrossedEdge(do,?)
Sib(favor, most, -) Sib(favor, days, most)
GSib(favor, days, these, -)
Table 3: Decomposing Figure 2 according to the
crossing-sensitive third-order factorization described in
Section 3. Null inner siblings are indicated with -.
lists the parts in the tree in Figure 2 according to this
crossing-sensitive third-order factorization.
4 Parsing Algorithm
The parser finds the maximum scoring 1-Endpoint-
Crossing tree according to the factorization in Sec-
tion 3 with a dynamic programming procedure rem-
iniscent of Koo and Collins (2010) (for scoring un-
crossed edges with grandparent and/or sibling fea-
tures) and of Pitler et al. (2013) (for including
crossed edges). The parser also uses novel sub-
problems for transitioning between portions of the
tree with and without crossed edges. This formula-
tion of the parsing problem presents two difficulties:
1. The parser must know whether an edge is
crossed when it is added.
2. For uncrossed edges, the parser must use
the appropriate part for scoring according to
whether other edges are crossed (Table 2).
Difficulty 1 is solved by adding crossed and un-
crossed edges to the tree in distinct sub-problems
(Section 4.1). Difficulty 2 is solved by producing
different versions of subtrees over the same sets of
vertices, both with and without a grandparent index,
which differ in their assumptions about the tree out-
side of that set (Section 4.2). The list of all sub-
problems with their invariants and the full dynamic
program are provided in the supplementary material.
4.1 Enforcing Crossing Edges
The parser adds crossed and uncrossed edges in
distinct portions of the dynamic program. Un-
crossed edges are added only through trapezoid sub-
problems (that may or may not have a grandpar-
ent index), while crossed edges are added in non-
trapezoid sub-problems. To add all uncrossed edges
44
in trapezoid sub-problems, the parser (a) enforces
that any edge added anywhere else must be crossed,
and (b) includes transitional sub-problems to build
trapezoids when the edge ~ehm is not crossed, but the
edge to its inner sibling ~ehs is (and so the construc-
tion step shown in Figure 1b cannot be used).
4.1.1 Crossing Conditions
Pitler et al. (2013) included crossing edges by using
?crossing region? sub-problems over intervals with
an external vertex that optionally contained edges
between the interval and the external vertex. An
uncrossed edge could then be included either by a
derivation that prohibited it from being crossed or
a derivation which allowed (but did not force) it to
be crossed. This ambiguity is removed by enforcing
that (1) each crossing region contains at least one
edge incident to the exterior vertex, and (2) all such
edges are crossed by edges in another sub-problem.
For example, by requiring at least one edge between
do and (favor, ?] and also between favor and (*, do),
the edges in the two sets are guaranteed to cross.
4.1.2 Trapezoids with Edge to Inner Sibling
Crossed
To add all uncrossed edges in trapezoid-style sub-
problems, we must be able to construct a trapezoid
over vertices [h,m] whenever the edge ~ehm is not
crossed. The construction used in Koo and Collins
(2010), repeated graphically in Figure 5a, cannot
be used if the edge ~ehs is crossed, as there would
then exist edges between (h, s) and (s,m), making
s an invalid split point. The parser therefore includes
some ?transitional glue? to allow alternative ways to
construct the trapezoid over [h,m] when ~ehm is not
crossed but the edge ~ehs to m?s inner sibling is.
The two additional ways of building trapezoids
are shown graphically in Figures 5b and 5c. Con-
sider the ?chain of crossing edges? that includes the
edge ~ehs. If none of these edges are in the subtree
rooted at m, then we can build the tree involving
m and its inner descendants separately (Figure 5b)
from the rest of the tree rooted at h. Within the in-
terval [h, e? 1] the furthest edge incident to h (~ehs)
must be crossed: these intervals are parsed choosing
s and the crossing point of ~ehs simultaneously (as in
Figure 4 in Pitler et al. (2013)).
Otherwise, the sub-tree rooted at m is involved in
g h
=
g h
+
hm ss m
(a) Edge from h to inner sibling s is not crossed (re-
peats Figure 1b)
g h
=
hm mh
+
ee?1
(b) ~ehs is crossed, but the chain of crossing edges
involving ~ehs does not include any descendants of m.
e is m?s descendant furthest from m within (h,m).
s ? (h, e? 1).
h m
+
d
=
mg h h d
(c) ~ehs is crossed, and the chain of crossing edges
involving ~ehs includes descendants of m. Of m?s de-
scendants that are incident to edges in the chain, d is
the one closest to m (d can be m itself). s ? (h, d).
Figure 5: Ways to build a trapezoid when the edge ~ehs to
m?s inner sibling may be crossed.
the chain of crossing edges (Figure 5c). The chain
of crossing edges between h and d (m?s descendant,
which may be m itself) is built up first then concate-
nated with the triangle rooted at m containing m?s
inner descendants not involved in the chain.
Chains of crossing edges are constructed by re-
peatedly applying two specialized types of L items
that alternate between adding an edge from the in-
terval to the exterior point (right-to-left) or from
the exterior point to the interval (left-to-right) (Fig-
ure 6). The boundary edges of the chain can
be crossed more times without violating the 1-
Endpoint-Crossing property, and so the beginning
and end of the chain can be unrestricted crossing
regions. These specialized chain sub-problems are
also used to construct boxes (Figure 1c) over [s,m]
with shared parent h when neither edge ~ehs nor ~ehm
is crossed, but the subtrees rooted at m and at s cross
each other (Figure 7).
Lemma 1. The GrandSib-Crossing parser adds all
uncrossed edges and only uncrossed edges in a tree
in a ?trapezoid? sub-problem.
Proof. The only part is easy: when a trapezoid is
built over an interval [h,m], all edges are internal to
the interval, so no earlier edges could cross ~ehm. Af-
45
= +
h s k s k
+
s k dh d
di k
x i d di k
= +
k k
+
x i
x i d
= +
k k
+
x idx i
= +
i d x i d
Figure 6: Constructing a chain of crossing edges
h d m
+
h d m h me
=
h s m h s d
=
d e
+
Figure 7: Constructing a box when edges in m and s?s
subtrees cross each other.
ter the trapezoid is built, only the interval endpoints
h and m are accessible for the rest of the dynamic
program, and so an edge between a vertex in (h,m)
and a vertex /? [h,m] can never be added. The
Crossing Conditions ensure that every edge added
in a non-trapezoid sub-problem is crossed.
Lemma 2. The GrandSib-Crossing parser con-
siders all 1-Endpoint-Crossing trees and only 1-
Endpoint-Crossing trees.
Proof. All trees that could have been built in Pitler
et al. (2013) are still possible. It can be verified that
the additional sub-problems added all obey the 1-
Endpoint-Crossing property.
4.2 Reduced Context in Presence of Crossings
A crossed edge (added in a non-trapezoid sub-
problem) is scored as a CrossedEdge part. An
uncrossed edge added in a trapezoid sub-problem,
however, may need to be scored according to a
GrandSib, Grand, Sib, or Edge part, depending on
whether the relevant other edges are crossed. In this
section we show that sibling and grandparent fea-
tures are included in the GrandSib-Crossing parser
as specified by Table 2.
do favor most these days
(a) For good contexts
favor most these daysdo
(b) For bad contexts
Figure 8: For each of the interval sub-problems in Koo
and Collins (2010), the parser constructs versions with
and without the additional grandparent index. Figure 8b
is used if the edge from do to favor is crossed, or if there
are any crossed edges from favor to children to the left of
do or to the right of days. Otherwise, Figure 8a is used.
4.2.1 Sibling Features
Lemma 3. The GrandSib-Crossing parser scores an
uncrossed edge ~ehm with a Sib or GrandSib part if
and only if ~ehs is not crossed.
Proof. Whether the edge to an uncrossed edge?s in-
ner sibling is crossed is known bottom-up through
how the trapezoid is constructed, since the inner sib-
ling is internal to the sub-problem. When ~ehs is not
crossed, the trapezoid is constructed as in Figure 5a,
using the inner sibling as the split point. When the
edge ~ehs is crossed, the trapezoid is constructed as in
Figure 5b or 5c; note that both ways force the edge
to the inner sibling to be crossed.
4.2.2 Grandparent Features for GProj Edges
Koo and Collins (2010) include an external grand-
parent index for each of the sub-problems that the
edges within use for scoring. We want to avoid
adding such an external grandparent index to any
of the crossing region sub-problems (to stay within
the desired time and space constraints) or to inter-
val sub-problems when the external context would
make all internal edges ?GProj .
For each interval sub-problem, the parser con-
structs versions both with and without a grandpar-
ent index (Figure 8). Which version is used de-
pends on the external context. In a bad context, all
edges to children within an interval are guaranteed
to be ?GProj . This section shows that all boundary
points in crossing regions are placed in bad contexts,
and then that edges are scored with grandparent fea-
tures if and only if they are GProj .
Bad Contexts for Interval Boundary Points For
exterior vertex boundary points, all edges from it to
its children will be crossed (Section 4.1.1), so it does
not need a grandparent index.
46
Lemma 4. If a boundary point i?s parent (call it g)
is within a sub-problem over vertices [i, j] or [i, j]?
{x}, then for all uncrossed edges ~eim with m in the
sub-problem, the tree outside of the sub-problem is
irrelevant to whether ~eim is GProj .
Proof. The sub-problem contains the edge ~egi, so
Condition (1) is checked internally. m cannot be
x, since ~eim is uncrossed. If g is x, then ~eim is
?GProj regardless of the outer context. If both g
and m ? (i, j], then Outer(m) ? (i, j]: If m is an
interior child of i (m ? (i, g)) then Outer(m) ?
(m, g) ? (i, j]. Otherwise, if m is an exterior child
(m ? (g, j]), by the ?wrapping around? definition of
Outer , Outer(m) ? (g,m) ? (i, j]. Thus Condi-
tion (2) is also checked internally.
We can therefore focus on interval boundary
points with their parent outside of the sub-problem.
Definition 3. The left boundary vertex of an inter-
val [i, j] is in a bad context (BadContextL(i, j)) if
i receives its parent (call it g) from outside of the
sub-problem and either of the following hold:
1. Grand-Edge Crossed: ~egi is crossed
2. Outer-Child-Edge Crossed: An edge from i to
a child of i outside of [i, j] and Outer to j will
be crossed (recall this includes children on the
far side of g if g is to the left of i)
BadContextR(i, j) is defined symmetrically regard-
ing j and j?s parent and children.
Corollary 1. If BadContextL(i, j), then for all ~eim
with m ? (i, j], ~eim is ?GProj . Similarly, if
BadContextR(i, j), for all ~ejm with m ? [i, j), ~ejm
is ?GProj .
No Grandparent Indices for Crossing Regions
We would exceed the desired O(n4) run-time if
any crossing region sub-problems needed any grand-
parent indices. In Pitler et al. (2013), LR sub-
problems with edges from the exterior point crossed
by both the left and the right boundary points were
constructed by concatenating an L and an R sub-
problem. Since the split point was not necessar-
ily incident to a crossed edge, the split point might
have GProj edges to children on the side other than
where it gets its parent; accommodating this would
add another factor of n to the running time and space
x k jx i j = +kix
Figure 9: For all split points k, the edge from k?s parent
to k is crossed, so all edges from k to children on either
side were ?GProj . The case when the split point?s parent
is from the right is symmetric.
x i k j
(a) x is Outer to all
children of k in (k, j].
x i k j
(b) x is Outer to all
children of k in [i, k).
Figure 10: The edge ~ekx is guaranteed to be crossed, so
k is in a BadContext for whichever side it does not get
its parent from.
to store the split point?s parent. To avoid this in-
crease in running time, they are instead built up as
in Figure 9, which chooses the split point so that the
edge from the parent of the split point to it is crossed.
Lemma 5. For all crossing region sub-problems
[i, j] ? {x} with i?s parent /? [i, j] ? {x},
BadContextL(i, j). Similarly, when j?s parent /?
[i, j] ? {x}, BadContextR(i, j).
Proof. Crossing region sub-problems either com-
bine to form intervals or larger crossing regions.
When they combine to form intervals as in Figure
3, it can be verified that all boundary points are in
a bad context. LR sub-problems were discussed
above. Split points for the L/R/N sub-problems by
construction are incident to a crossed edge to a fur-
ther vertex. If that edge is from the split point?s par-
ent to the split point, then the grand-edge is crossed
and so both sides are in a bad context. If the crossed
edge is from the split point to a child, then that child
is Outer to all other children on the side in which it
does not get its parent (see Figure 10).
Corollary 2. No grandparent indices are needed for
any crossing region sub-problem.
Triangles and Trapezoids with and without
Grandparent Indices The presentation that fol-
lows assumes left-headed versions. Uncrossed
edges are added in two distinct types of trapezoids:
(1) TrapG[h,m, g,L] with an external grandpar-
ent index g, scores the edge ~ehm with grandpar-
47
ent features, and (2) Trap[h,m,L] without a grand-
parent index, scores the edge ~ehm without grand-
parent features. Triangles also have versions with
(TriG[h, e, g,L] and without (Tri[h, e,L]) a grand-
parent index. What follows shows that all GProj
edges are added in TrapG sub-problems, and all
?GProj uncrossed edges are added in Trap sub-
problems.
Lemma 6. For all k ? (i, j), if BadContextL(i, j),
then BadContextL(i, k). Similarly, if
BadContextR(i, j), then BadContextR(k, j).
Proof. BadContextL(i, j) implies either the edge
from i?s parent to i is crossed and/or an edge from i
to a child of i outer to j is crossed. If the edge from
i?s parent to i is crossed, then BadContextL(i, k). If
a child of i is outer to j, then since k ? (i, j), such a
child is also outer to k.
Lemma 7. All left-rooted triangle sub-problems
Tri[i, j,L] without a grandparent index are in a
BadContextL(i, j). Similarly for all Tri[i, j,R],
BadContextR(i, j).
Proof. All triangles without grandparent indices are
either placed immediately into a bad context (by
adding a crossed edge to the triangle?s root from its
parent, or a crossed edge from the root to an outer
child) or are combined with other sub-trees to form
larger crossing regions (and therefore the triangle is
in a bad context, using Lemmas 5 and 6).
Lemma 8. All triangle sub-problems with a grand-
parent index TriG[h, e, g,L] are placed in a
?BadContextL(h, e). Similarly, TriG[e, h, g,R]
are only placed in ?BadContextR(h, e).
Proof. Consider where a non-empty triangle (h 6=
e) with a grandparent index TriG[h, e, g,L] can be
placed in the full dynamic program and what each
step would imply about the rest of the tree.
If the triangle contains exterior children of h (e
and g are on opposite sides of h), then it can either
combine with a trapezoid to form another larger tri-
angle (as in Figure 1a) or it can combine with an-
other sub-problem to form a box with a grandpar-
ent index (Figure 1c or 7). Boxes with a grandpar-
ent index can only combine with another trapezoid
to form a larger trapezoid (Figure 1b). Both cases
force ~egh to not be crossed and prevent h from hav-
ing any outer crossed children, as h becomes an in-
ternal node within the larger sub-problem.
If the triangle contains interior children of h (e
lies between g and h), then it can either form a trape-
zoid from g to h by combining with a triangle (Fig-
ure 5b) or a chain of crossing edges (Figure 5c), or it
can be used to build a box with a grandparent index
(Figures 1c and 7), which then can only be used to
form a trapezoid from g to h. In either case, a trape-
zoid is constructed from g to h, enforcing that ~egh
cannot be crossed. These steps prevent h from hav-
ing any additional children between g and e (since h
does not appear in the adjacent sub-problems at all
whenever h 6= e), so again the children of h in (e, h)
have no outer siblings.
Lemma 9. In a TriG[h, e, g,L] sub-problem, if an
edge ~ehm is not crossed and no edges from i to sib-
lings of m in (m, e] are crossed, then ~ehm is GProj .
Proof. This follows from (1) the edge ~ehm is not
crossed, (2) the edge ~egh is not crossed by Lemma 8,
and (3) no outer siblings are crossed (outer siblings
in (m, e] are not crossed by assumption and siblings
outer to e are not crossed by Lemma 8).
Lemma 10. An edge ~ehm scored with a GrandSib
or Grand part (added through a TrapG[h,m, g, L]
or TrapG[m,h, g,R] sub-problem) is GProj .
Proof. A TrapG can either (1) combine with de-
scendants of m to form a triangle with a grandparent
index rooted at h (indicating that m is the outermost
inner child of h) or (2) combine with descendants
of m and of m?s adjacent outer sibling (call it o),
forming a trapezoid from h to o (indicating that ~eho
is not crossed). Such a trapezoid could again only
combine with further uncrossed outer siblings until
eventually the final triangle rooted at h with grand-
parent index g is built. As ~ehm was not crossed, no
edges from h to outer siblings within the triangle are
crossed, and ~ehm is within a TriG sub-problem, ~ehm
is GProj by Lemma 9.
Lemma 11. An uncrossed edge ~ehm scored with a
Sib or Edge part (added through a Trap[h,m,L] or
Trap[m,h,R] sub-problem) is ?GProj .
48
Proof. A Trap can only (1) form a triangle without
a grandparent index, or (2) form a trapezoid to an
outer sibling of m, until eventually a final triangle
rooted at h without a grandparent index is built. This
triangle without a grandparent index is then placed
in a bad context (Lemma 7) and so ~ehm is ?GProj
(Corollary 1).
4.3 Main Results
Lemma 12. The crossing-sensitive third-order
parser runs in O(n4) time and O(n3) space when
the input is an unpruned graph. When the input
to the parser is a pruned graph with at most k in-
coming edges per node, the crossing-sensitive third-
order parser runs in O(kn3) time and O(n3) space.
Proof. All sub-problems are either over intervals
(two indices), intervals with a grandparent index
(three indices), or crossing regions (three indices).
No crossing regions require any grandparent indices
(Corollary 2). The only sub-problems that require
a maximization over two internal split points are
over intervals and need no grandparent indices (as
the furthest edges from each root are guaranteed to
be crossed within the sub-problem). All steps ei-
ther contain an edge in their construction step or in
the invariant of the sub-problem, so with a pruned
graph as input, the running time is the number of
edges (O(kn)) times the number of possibilities for
the other two free indices (O(n2)). The space is not
reduced as there is not necessarily an edge relation-
ship between the three stored vertices.
Theorem 1. The GrandSib-Crossing parser cor-
rectly finds the maximum scoring 1-Endpoint-
Crossing tree according to the crossing-sensitive
third-order factorization (Section 3) in O(n4) time
and O(n3) space. When the input to the parser is
a pruned graph with at most k incoming edges per
node, the GrandSib-Crossing parser correctly finds
the maximum scoring 1-Endpoint-Crossing tree that
uses only unpruned edges in O(kn3) time and
O(n3) space.
Proof. The correctness of scoring follows from
Lemmas 3, 10, and 11. The search space of 1-
Endpoint-Crossing trees was in Lemma 2 and the
time and space complexity in Lemma 12.
The parser produces the optimal tree in a well-
defined output space. Pruning edges restricts the
output space the same way that constraints enforc-
ing projectivity or the 1-Endpoint-Crossing property
also restrict the output space. Note that if the optimal
unconstrained 1-Endpoint-Crossing tree does not in-
clude any pruned edges, then whether the parser uses
pruning or not is irrelevant; both the pruned and un-
pruned parsers will produce the exact same tree.
5 Experiments
The crossing-sensitive third-order parser was imple-
mented as an alternative parsing algorithm within
dpo3 (Koo and Collins, 2010).2 To ensure a fair
comparison, all code relating to input/output, fea-
tures, learning, etc. was re-used from the origi-
nal projective implementation, and so the only sub-
stantive differences between the projective and 1-
Endpoint-Crossing parsers are the dynamic pro-
gramming charts, the parsing algorithms, and the
routines that extract the maximum scoring tree from
the completed chart.
The treebanks used to prepare the CoNLL shared
task data (Buchholz and Marsi, 2006; Nivre et al.,
2007) vary widely in their conventions for repre-
senting conjunctions, modal verbs, determiners, and
other decisions (Zeman et al., 2012). The exper-
iments use the newly released HamleDT software
(Zeman et al., 2012) that normalizes these treebanks
into one standard format and also provides built-in
transformations to other conjunction styles. The un-
normalized treebanks input to HamleDT were from
the CoNLL 2006 Shared Task (Buchholz and Marsi,
2006) for Danish, Dutch, Portuguese, and Swedish
and from the CoNLL 2007 Shared Task (Nivre et al.,
2007) for Czech.
The experiments include the default Prague
style (Bo?hmova? et al., 2001), Mel?c?ukian style
(Mel?c?uk, 1988), and Stanford style (De Marneffe
and Manning, 2008) for conjunctions. Under the
grandparent-sibling factorization, the two words be-
ing conjoined would never appear in the same scope
for the Prague style (as they are siblings on differ-
ent sides of the conjunct head). In the Mel?c?ukian
style, the two conjuncts are in a grandparent rela-
tionship and in the Stanford style the two conjuncts
2http://groups.csail.mit.edu/nlp/dpo3/
49
are in a sibling relationship, and so we would expect
to see larger gains for including grandparents and
siblings under the latter two representations. The
experiments also include a nearly projective dataset,
the English Penn Treebank (Marcus et al., 1993),
converted to dependencies with PennConverter (Jo-
hansson and Nugues, 2007).
The experiments use marginal-based pruning
based on an edge-factored directed spanning tree
model (McDonald et al., 2005). Each word?s set of
potential parents is limited to those with a marginal
probability of at least .1 times the probability of the
most probable parent, and cut off this list at a max-
imum of 20 potential parents per word. To ensure
that there is always at least one projective and/or 1-
Endpoint-Crossing tree achievable, the artificial root
is always included as an option. The pruning param-
eters were chosen to keep 99.9% of the true edges
on the English development set.
Following Carreras (2007) and Koo and Collins
(2010), before training the training set trees are
transformed to be the best achievable within the
model class (i.e., the closest projective tree or 1-
Endpoint-Crossing tree). All models are trained
for five iterations of averaged structured perceptron
training. For English, the model after the iteration
that performs best on the development set is used;
for all other languages, the model after the fifth iter-
ation is used.
5.1 Results
Results for edge-factored and (crossing-sensitive)
grandparent-sibling factored models for both projec-
tive and 1-Endpoint-Crossing parsing are in Tables
4 and 5. In 14 out of the 16 experimental set-ups,
the third-order 1-Endpoint-Crossing parser is more
accurate than the third-order projective parser. It is
significantly better than the projective parser in 9 of
the set-ups and significantly worse in none.
Table 6 shows how often the 1-EC CS-GSib
parser used each of the GrandSib, Grand, Sib,
Edge, and CrossedEdge parts for the Mel?c?ukian
and Stanford style test sets. In both representations,
3Following prior work in graph-based dependency parsing
(for example, Rush and Petrov (2012)), English results use au-
tomatically produced part-of-speech tags and results exclude
punctuation, while the results for all other languages use gold
part-of-speech tags and include punctuation.
Model Du Cz Pt Da Sw
Prague
Proj GSib 80.45 85.12 88.85 88.17 85.50
Proj Edge 80.38 84.04 88.14 88.29 86.09
1-EC CS-GSib 82.78 85.90 89.74 88.64 85.70
1-EC Edge 83.33 84.97 89.21 88.19 86.46
Mel?c?ukian
Proj GSib 82.26 87.96 89.19 90.23 89.59
Proj Edge 82.09 86.18 88.73 89.29 89.00
1-EC CS-GSib 86.03 87.89 90.34 90.50 89.34
1-EC Edge 85.28 87.57 89.96 90.14 88.97
Stanford
Proj GSib 81.16 86.83 88.80 88.84 87.27
Proj Edge 80.56 86.18 88.61 88.69 87.92
1-EC CS-GSib 84.67 88.34 90.20 89.22 88.15
1-EC Edge 83.62 87.13 89.43 88.74 87.36
Table 4: Overall Unlabeled Attachment Scores (UAS) for
all words.3 CS-GSib is the proposed crossing-sensitive
grandparent-sibling factorization. For each data set, we
bold the most accurate model and those not significantly
different from the most accurate (sign test, p < .05). Lan-
guages are sorted in increasing order of projectivity.
Model UAS
Proj GSib 93.10
Proj Edge 92.63
1-EC CS-GSib 93.22
1-EC Edge 92.80
Table 5: English results
the parser is able to score with a sibling context
more often than it is able to score with a grandpar-
ent, perhaps explaining why the datasets using the
Stanford conjunction representation saw the largest
gains from including the higher order factors into the
1-Endpoint-Crossing parser.
Across languages, the third-order 1-Endpoint-
Crossing parser runs 2.1-2.7 times slower than the
third-order projective parser (71-104 words per sec-
ond, compared with 183-268 words per second).
Parsing speed is correlated with the amount of prun-
ing. The level of pruning mentioned earlier is rela-
tively permissive, retaining 39.0-60.7% of the edges
in the complete graph; a higher level of pruning
could likely achieve much faster parsing times with
the same underlying parsing algorithms.
50
Part Used Du Cz Pt Da Sw
Mel?c?ukian
CrossedEdge 8.5 4.5 3.2 1.4 1.2
GrandSib 81.2 89.1 90.7 95.7 96.2
Grand 1.1 0.5 0.8 0.3 0.2
Sib 9.0 5.8 5.2 2.6 2.3
Edge < 0.1 < 0.1 0 < 0.1 0
Stanford
CrossedEdge 8.4 5.1 3.3 2.0 1.8
GrandSib 81.4 87.8 90.5 94.2 95.2
Grand 1.1 0.5 0.7 0.3 0.3
Sib 8.9 6.5 5.2 3.5 2.6
Edge < 0.1 0.1 0 < 0.1 0
Table 6: The proportion of edges in the predicted output
trees from the CS-GSib 1-Endpoint-Crossing parser that
would have used each of the five part types for scoring.
6 Discussion
There have been many other notable approaches to
non-projective parsing with larger scopes than single
edges, including transition-based parsers, directed
spanning tree graph-based parsers, and mildly non-
projective graph-based parsers.
Transition-based parsers score actions that the
parser may take to transition between different
configurations. These parsers typically use either
greedy or beam search, and can condition on any
tree context that is in the history of the parser?s
actions so far. Zhang and Nivre (2011) signifi-
cantly improved the accuracy of an arc-eager tran-
sition system (Nivre, 2003) by adding several ad-
ditional classes of features, including some third-
order features. Basic arc-eager and arc-standard
(Nivre, 2004) models that parse left-to-right using
a stack produce projective trees, but transition-based
parsers can be modified to produce crossing edges.
Such modifications include pseudo-projective pars-
ing in which the dependency labels encode transfor-
mations to be applied to the tree (Nivre and Nilsson,
2005), adding actions that add edges to words in the
stack that are not the topmost item (Attardi, 2006),
adding actions that swap the positions of words
(Nivre, 2009), and adding a second stack (Go?mez-
Rodr??guez and Nivre, 2010).
Graph-based approaches to non-projective pars-
ing either consider all directed spanning trees or re-
stricted classes of mildly non-projective trees. Di-
rected spanning tree approaches with higher order
features either use approximate learning techniques,
such as loopy belief propagation (Smith and Eis-
ner, 2008), or use dual decomposition to solve relax-
ations of the problem (Koo et al., 2010; Martins et
al., 2013). While not guaranteed to produce optimal
trees within a fixed number of iterations, these dual
decomposition techniques do give certificates of op-
timality on the instances in which the relaxation is
tight and the algorithm converges quickly.
This paper described a mildly non-projective
graph-based parser. Other parsers in this class find
the optimal tree in the class of well-nested, block
degree two trees (Go?mez-Rodr??guez et al., 2011),
or in a class of trees further restricted based on
gap inheritance (Pitler et al., 2012) or the head-split
property (Satta and Kuhlmann, 2013), with edge-
factored running times of O(n5) ? O(n7). The
factorization used in this paper is not immediately
compatible with these parsers: the complex cases in
these parsers are due to gaps, not crossings. How-
ever, there may be analogous ?gap-sensitive? factor-
izations that could allow these parsers to be extended
without large increases in running times.
7 Conclusion
This paper proposed an exact, graph-based algo-
rithm for non-projective parsing with higher order
features. The resulting parser has the same asymp-
totic run time as a third-order projective parser, and
is significantly more accurate for many experimental
settings. An exploration of other factorizations that
facilitate non-projective parsing (for example, an
analogous ?gap-sensitive? variant) may be an inter-
esting avenue for future work. Recent work has in-
vestigated faster variants for third-order graph-based
projective parsing (Rush and Petrov, 2012; Zhang
and McDonald, 2012) using structured prediction
cascades (Weiss and Taskar, 2010) and cube prun-
ing (Chiang, 2007). It would be interesting to extend
these lines of work to the crossing-sensitive third-
order parser as well.
Acknowledgments
I would like to thank Sampath Kannan, Mitch Mar-
cus, Chris Callison-Burch, Michael Collins, Mark
Liberman, Ben Taskar, Joakim Nivre, and the three
anonymous reviewers for valuable comments on ear-
lier versions of this material.
51
References
G. Attardi. 2006. Experiments with a multilanguage
non-projective dependency parser. In Proceedings of
CoNLL, pages 166?170.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2001.
The Prague Dependency Treebank: Three-level anno-
tation scenario. In Anne Abeille?, editor, Treebanks:
Building and Using Syntactically Annotated Corpora,
pages 103?127. Kluwer Academic Publishers.
B. Bohnet and J. Kuhn. 2012. The best of both worlds
? a graph-based completion model for transition-based
parsers. In Proceedings of EACL, pages 77?87.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proceedings
of CoNLL, pages 149?164.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, pages
957?961.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of ACL, pages 173?180.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
M. De Marneffe and C. Manning. 2008. Stanford typed
dependencies manual.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 29?62. Kluwer Academic
Publishers.
C. Go?mez-Rodr??guez and J. Nivre. 2010. A transition-
based parser for 2-planar dependency structures. In
Proceedings of ACL, pages 1492?1501.
C. Go?mez-Rodr??guez, J. Carroll, and D. Weir. 2011. De-
pendency parsing schemata and mildly non-projective
dependency parsing. Computational Linguistics,
37(3):541?586.
K. Hall. 2007. K-best spanning tree parsing. In Proceed-
ings of ACL, pages 392?399.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of the 16th Nordic Conference on Com-
putational Linguistics (NODALIDA), pages 105?112.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of ACL, pages 1?11.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proceedings of EMNLP,
pages 1288?1298.
T. Koo. 2010. Advances in discriminative dependency
parsing. Ph.D. thesis, Massachusetts Institute of Tech-
nology.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies, 2(1):1?127.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations for
dependency parsing. In Proceedings of ACL, pages
342?350.
A. Martins, M. Almeida, and N. A. Smith. 2013. Turn-
ing on the turbo: Fast third-order non-projective turbo
parsers. In Proceedings of ACL (Short Papers), pages
617?622.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81?88.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 121?132.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proceedings of HLT/EMNLP,
pages 523?530.
I. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proceedings of ACL, pages 99?106.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL,
pages 915?932.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies, pages 149?
160.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Proceedings of the Workshop on In-
cremental Parsing: Bringing Engineering and Cogni-
tion Together, pages 50?57.
J. Nivre. 2009. Non-projective dependency parsing in
expected linear time. In Proceedings of ACL, pages
351?359.
E. Pitler, S. Kannan, and M. Marcus. 2012. Dynamic
programming for higher order parsing of gap-minding
trees. In Proceedings of EMNLP, pages 478?488.
E. Pitler, S. Kannan, and M. Marcus. 2013. Find-
ing optimal 1-Endpoint-Crossing trees. Transac-
tions of the Association for Computational Linguistics,
1(Mar):13?24.
52
A. Rush and S. Petrov. 2012. Vine pruning for effi-
cient multi-pass dependency parsing. In Proceedings
of NAACL, pages 498?507.
G. Satta and M. Kuhlmann. 2013. Efficient parsing for
head-split dependency trees. Transactions of the As-
sociation for Computational Linguistics, 1(July):267?
278.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proceedings of EMNLP, pages
145?156.
D. Weiss and B. Taskar. 2010. Structured Prediction
Cascades. In AISTATS, pages 916?923.
D. Zeman, D. Marec?ek, M. Popel, L. Ramasamy,
J. S?te?pa?nek, Z. Z?abokrtsky?, and J. Hajic?. 2012. Ham-
leDT: To parse or not to parse? In Proceedings of
the Eight International Conference on Language Re-
sources and Evaluation (LREC?12), pages 2735?2741.
H. Zhang and R. McDonald. 2012. Generalized higher-
order dependency parsing with cube pruning. In Pro-
ceedings of EMNLP, pages 320?331.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Pro-
ceedings of ACL (Short Papers), pages 188?193.
53
54
