Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 155?163,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Jointly Identifying Predicates, Arguments and Senses using Markov Logic
Ivan Meza-Ruiz? Sebastian Riedel??
?School of Informatics, University of Edinburgh, UK
?Department of Computer Science, University of Tokyo, Japan
?Database Center for Life Science, Research Organization of Information and System, Japan
?I.V.Meza-Ruiz@sms.ed.ac.uk ? sebastian.riedel@gmail.com
Abstract
In this paper we present a Markov Logic Net-
work for Semantic Role Labelling that jointly
performs predicate identification, frame dis-
ambiguation, argument identification and ar-
gument classification for all predicates in a
sentence. Empirically we find that our ap-
proach is competitive: our best model would
appear on par with the best entry in the
CoNLL 2008 shared task open track, and at
the 4th place of the closed track?right be-
hind the systems that use significantly better
parsers to generate their input features. More-
over, we observe that by fully capturing the
complete SRL pipeline in a single probabilis-
tic model we can achieve significant improve-
ments over more isolated systems, in particu-
lar for out-of-domain data. Finally, we show
that despite the joint approach, our system is
still efficient.
1 Introduction
Semantic Role Labelling (SRL, Ma?rquez et al,
2008) is generally understood as the task of iden-
tifying and classifying the semantic arguments and
modifiers of the predicates mentioned in a sentence.
For example, in the case of the following sentence:
we are to find out that for the predicate token ?plays?
with sense ?play a role? (play.02) the phrase headed
by the token ?Haag? is referring to the player (A0)
of the play event, and the phrase headed by the token
?Elianti? is referring to the role (A1) being played.
SRL is considered as a key task for applications that
require to answer ?Who?, ?What?, ?Where?, etc.
questions, such as Information Extraction, Question
Answering and Summarization.
Any real-world SRL system needs to make sev-
eral decisions, either explicitly or implicitly: which
are the predicate tokens of a sentence (predicate
identification), which are the tokens that have se-
mantic roles with respect to these predicates (argu-
ment identification), which are the roles these to-
kens play (argument classification), and which is the
sense of the predicate (sense disambiguation).
In this paper we use Markov Logic (ML), a Statis-
tical Relational Learning framework that combines
First Order Logic and Markov Networks, to develop
a joint probabilistic model over all decisions men-
tioned above. The following paragraphs will moti-
vate this choice.
First, it allows us to readily capture global cor-
relations between decisions, such as the constraint
that a predicate can only have one agent. This type
of correlations has been successfully exploited in
several previous SRL approaches (Toutanova et al,
2005; Punyakanok et al, 2005).
Second, we can use the joint model to evaluate
the benefit of incorporating decisions into the joint
model that either have not received much attention
within the SRL community (predicate identification
and sense disambiguation), or been largely made in
isolation (argument identification and classification
for all predicates of a sentence).
Third, our ML model is essentially a template that
describes a class of Markov Networks. Algorithms
can perform inference in terms of this template with-
155
out ever having to fully instantiate the complete
Markov Network (Riedel, 2008; Singla and Domin-
gos, 2008). This can dramatically improve the effi-
ciency of an SRL system when compared to a propo-
sitional approach such as Integer Linear Program-
ming (ILP).
Finally, when it comes to actually building an
SRL system with ML there are ?only? four things
to do: preparing input data files, converting out-
put data files, and triggering learning and inference.
The remaining work can be done by an off-the-
shelf Markov Logic interpreter. This is to be con-
trasted with pipeline systems where several compo-
nents need to be trained and connected, or Integer
Linear Programming approaches for which we need
to write additional wrapper code to generate ILPs.
Empirically we find that our system is
competitive?our best model would appear on
par with the best entry in the CoNLL 2008 shared
task open track, and at the 4th place of the closed
track?right behind systems that use significantly
better parsers1 to generate their input features.
We also observe that by integrating frame disam-
biguation into the joint SRL model, and by extract-
ing all arguments for all predicates in a sentence
simultaneously, significant improvements compared
to more isolated systems can be achieved. These
improvements are particularly large in the case of
out-of-domain data, suggesting that a joint approach
helps to increase the robustness of SRL. Finally, we
show that despite the joint approach, our system is
still efficient.
Our paper is organised as follows: we first intro-
duce ML (section 2), then we present our model in
terms of ML (section 3) and illustrate how to per-
form learning and inference with it (section 4). How
this model will be evaluated is explained in section 5
with the corresponding evaluation presented in sec-
tion 6. We conclude in section 7.
2 Markov Logic
Markov Logic (ML, Richardson and Domingos,
2005) is a Statistical Relational Learning language
based on First Order Logic and Markov Networks.
It can be seen as a formalism that extends First Or-
der Logic to allow formulae that can be violated with
1Our unlabelled accuracy for syntactic dependencies is at
least 3% points under theirs.
some penalty. From an alternative point of view, it is
an expressive template language that uses First Or-
der Logic formulae to instantiate Markov Networks
of repetitive structure.
Let us describe ML by considering the predicate
identification task. In ML we can model this task by
first introducing a set of logical predicates2 such as
isPredicate(Token) or word(Token,Word). Then we
specify a set of weighted first order formulae that
define a distribution over sets of ground atoms of
these predicates (or so-called possible worlds).
Ideally, the distribution we define with these
weighted formulae assigns high probability to possi-
ble worlds where SRL predicates are correctly iden-
tified and a low probability to worlds where this is
not the case. For example, a suitable set of weighted
formulae would assign a high probability to the
world3
{word (1,Haag) , word(2, plays),
word(3,Elianti), isPredicate(2)}
and a low one to
{word (1,Haag) , word(2, plays),
word(3,Elianti), isPredicate(3)}
In Markov Logic a set of weighted formulae is called
a Markov Logic Network (MLN). Formally speak-
ing, an MLN M is a set of pairs (?,w) where ? is a
first order formula and w a real weight. M assigns
the probability
p (y) = 1Z exp
?
? ?
(?,w)?M
w
?
c?C?
f?c (y)
?
? (1)
to the possible world y. Here C? is the set of all
possible bindings of the free variables in ? with the
constants of our domain. f?c is a feature function
that returns 1 if in the possible world y the ground
formula we get by replacing the free variables in ?
by the constants in c is true and 0 otherwise. Z
is a normalisation constant. Note that this distri-
bution corresponds to a Markov Network (the so-
called Ground Markov Network) where nodes repre-
sent ground atoms and factors represent ground for-
mulae.
2In the cases were is not obvious whether we refer to SRL
or ML predicates we add the prefix SRL or ML, respectively.
3?Haag plays Elianti? is a segment of a sentence in the train-
ing corpus.
156
For example, if M contains the formula ?
word (x, take) ? isPredicate (x)
then its corresponding log-linear model has, among
others, a feature f?t1 for which x in ? has been re-
placed by the constant t1 and that returns 1 if
word (1, take) ? isPredicate (1)
is true in y and 0 otherwise.
We will refer predicates such as word as observed
because they are known in advance. In contrast, is-
Predicate is hidden because we need to infer it at test
time.
3 Model
Conceptually we divide our SRL system into three
stages: one stage that identifies the predicates of
a sentence, one stage that identifies and classifies
the arguments of these predicates, and a final stage
that predicts the sense of each predicate. We should
stress that this architecture is intended to illustrate
a typical SRL system, and to describe the pipeline-
based approach we will compare our models to.
However, it does not correspond to the way in-
ference is performed in our proposed model?we
jointly infer all decisions described above.
Note that while the proposed division into con-
ceptual stages seems somewhat intuitive, it is by no
means uncontroversial. In fact, for the CoNLL 2008
shared task slightly more than one half of the par-
ticipants performed sense disambiguation before ar-
gument identification and classification; most other
participants framed the problem in the reverse or-
der.4
We define five hidden predicates for the three
stages of the task. Figure 1 illustrates these pred-
icates and the stage they belong to. For predicate
identification, we use the predicate isPredicate. is-
Predicate(p) indicates that the word in the position
p is an SRL predicate. For argument identifica-
tion and classification, we use the predicates isAr-
gument, hasRole and role. The atom isArgument(a)
signals that the word in the position a is a SRL ar-
gument of some (unspecified) SRL predicate while
hasRole(p,a) indicates that the token at position a is
4However, for almost all pipeline based systems, predicate
identification was the first stage of the role labelling process.
isPredicate
sense
isArgument
hasRole
role
PredicateIdentification
ArgumentIdentification &clasification
SenseDisambiguation
Bo
tto
m-
up
Top
-Do
wn
Pip
elin
e d
ire
ctio
n
Figure 1: MLN hidden predicates divided in stages
an argument of the predicate in position p. The pred-
icate role(p,a,r) corresponds to the decision that the
argument at position a has the role r with respect to
the predicate in position p. Finally, for sense disam-
biguation we define the predicate sense(p,e) which
signals that the predicate in position p has the sense
e.
Before we continue to describe the formulae of
our Markov Logic Network we would like to high-
light the introduction of the isArgument predicate
mentioned above. This predicate corresponds to a
decision that is usually made implicitly: a token is
an argument if there exists a predicate for which it
plays a semantic role. Here we model this decision
explicitly, assuming that there exist cases where a
token clearly has to be an argument of some pred-
icate, regardless of which predicate in the sentence
this might be. It is this assumption that requires us to
infer the arguments for all predicates of a sentence
at once?otherwise we cannot make sure that for a
marked argument there exists at least one predicate
for which the argument plays a semantic role.
In addition to the hidden predicates, we define
observable predicates to represent the information
available in the corpus. Table 1 presents these pred-
icates.
3.1 Local formulae
A formula is local if its groundings relate any num-
ber of observed ground atoms to exactly one hidden
ground atom. For example, two groundings of the
local formula
lemma(p,+l1)?lemma(a,+l2) ? hasRole(p, a)
can be seen in the Factor Graph of Figure 2. Both
connect a single hidden hasRole ground atom with
157
word(i,w) Token i has word w
lemma(i,l) Token i has lemma l
ppos(i,p) Token i has POS tag p
cpos(i,p) Token i has coarse POS tag p
voice(i,v) Token i is verb and has voice v
(Active/Passive).
subcat(i,f) Token i has subcategorization
frame f
dep(i,j,d) Token h is head of token m and
has dependency label d
palmer(i,j) Token j can be semantic argu-
ment for token i according to
high recall heuristic?
depPath(i,j,p) Dependency path between to-
kens i and j is p?
depFrame(i,j,f) f is a syntactic (dependency)
frame in which tokens i and j
are designated as ?pivots??
Table 1: Observable predicates; predicates marked with
? are dependency parsing-based versions for features of
Xue and Palmer (2004).
two observed lemma ground atoms. The + notation
indicates that the MLN contains one instance of the
rule, with a separate weight, for each assignment of
the variables with a plus sign (?).
The local formulae for isPredicate, isArgument
and sense aim to capture the relation of the tokens
with their lexical and syntactic surroundings. This
includes formulae such as
subcat(p,+f) ? isPredicate(p)
which implies that a certain token is a predicate
with a weight that depends on the subcategorization
frame of the token. Further local formulae are con-
structed using those observed predicates in table 1
that relate single tokens and their properties.
The local formulae for role and hasRole focus on
properties of the predicate and argument token?the
formula illustrated in figure 2 is an example of this?
and on the relation between the two tokens. An ex-
ample of the latter type is the formula
depPath(p, a,+d) ? role(p, a,+r)
which implies that token a plays the semantic role r
with respect to token p, and for which the weight de-
pends on the syntactic (dependency) path d between
p and a and on the actual role to assign. Again,
further formulae are constructed using the observed
Figure 2: Factor graph for the first local formula in sec-
tion 3.1. Here round nodes represent variables (corre-
sponding to the states of ground atoms) and the rectan-
gular nodes represent the factor and their parameters at-
tached to the ground formulae.
predicates in table 1; however, this time we consider
both predicates that relate tokens to their individual
properties and predicates that describe the relation
between tokens.
Unfortunately, the complete set of local formulae
is too large to be exhaustively described in this pa-
per. Its size results from the fact that we also con-
sider conjunctions of several atoms as conditions,
and lexical windows around tokens. Hence, instead
of describing all local formulae we refer the reader
to our MLN model files.5 They can be used both as
a reference and as input to our Markov Logic En-
gine,6 and thus allow the reader to easily reproduce
our results.
3.2 Global formulae
Global formulae relate several hidden ground atoms.
We use this type of formula for two purposes: to
ensure consistency between the predicates of all
SRL stages, and to capture some of our background
knowledge about SRL. We will refer to formulae
that serve the first purpose as structural constraints.
For example, a structural constraint is given by the
(deterministic) formula
role(p, a, r) ? hasRole(p, a)
which ensures that, whenever the argument a is
given a label r with respect to the predicate p, this
argument must be an argument of a as denoted by
hasRole(p,a). Note that this formula by itself models
the traditional ?bottom-up? argument identification
and classification pipeline (Xue and Palmer, 2004):
5http://code.google.com/p/thebeast/
source/browse/#svn/mlns/naacl-hlt
6http://code.google.com/p/thebeast
158
it is possible to not assign a role r to an predicate-
argument pair (p, a) proposed by the identification
stage; however, it is impossible to assign a role r
to token pairs (p, a) that have not been proposed as
potential arguments.
An example of another class of structural con-
straints is
hasRole(p, a) ? ?r.role(p, a, r)
which, by itself, models an inverted or ?top-down?
pipeline. In this architecture the argument classifi-
cation stage can assign roles to tokens that have not
been proposed by the argument identification stage.
However, it must assign a label to any token pair the
previous stage proposes.
For the SRL predicates that perform a labelling
task (role and sense) we also need a structural con-
straint which ensures that not more than one label is
assigned. For instance,
(role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2))
forbids two different semantic roles for a pair of
words.
There are three global formulae that capture our
linguistic background knowledge. The first one is
a deterministic constraint that had been frequently
applied in the SRL literature. It forbids cases where
distinct arguments of a predicate have the same role
unless the role describes a modifier:
role (p, a1, r) ? ?mod (r) ? a1 6= a2 ?
?role (p, a2, r)
The second ?linguistic? global formula is
role(p, a,+r) ? lemma(p,+l) ? sense(p,+s)
which implies that when a predicate p with lemma l
has an argument awith role r it has to have the sense
s. Here the weight depends on the combination of
role r, lemma l and sense s.
The third and final ?linguistic? global formula is
lemma(p,+l) ? ppos(a,+p)
?hasRole(p, a) ? sense(p,+f)
It implies that if a predicate p has the lemma l and an
argument a with POS tag p it has to have the sense
s. This time the weight depends on the combination
of POS tag p, lemma l and sense s.
Note that the final two formulae evaluate the se-
mantic frame of a predicate and become local for-
mulae in a pipeline system that performs sense dis-
ambiguation after argument identification and clas-
sification.
Table 2 summarises the global formulae we use in
this work.
4 Inference and Learning
Assuming that we have an MLN, a set of weights
and a given sentence then we need to predict the
choice of predicates, frame types, arguments and
role labels with maximal a posteriori probabil-
ity (MAP). To this end we apply a method that
is both exact and efficient: Cutting Plane Infer-
ence (CPI, Riedel, 2008) with Integer Linear Pro-
gramming (ILP) as base solver.
Instead of fully instantiating the Markov Network
that a Markov Logic Network describes, CPI begins
with a subset of factors/edges?in our case we use
the factors that correspond to the local formulae of
our model?and solves the MAP problem for this
subset using the base solver. It then inspects the
solution for ground formulae/features that are not
yet included but could, if added, lead to a different
solution?this process is usually referred to as sep-
aration. The ground formulae that we have found
are added and the network is solved again. This pro-
cess is repeated until the network does not change
anymore.
This type of algorithm could also be realised for
an ILP formulation of SRL. However, it would re-
quire us to write a dedicated separation routine for
each type of constraint we want to add. In Markov
Logic, on the other hand, separation can be gener-
ically implemented as the search for variable bind-
ings that render a weighted first order formulae true
(if its weight is negative) or false (if its weight is
positive). In practise this means that we can try new
global formulae/constraints without any additional
implementation overhead.
We learn the weights associated with each MLN
using 1-best MIRA (Crammer and Singer, 2003)
Online Learning method. As MAP inference
method that is applied in the inner loop of the on-
line learner we apply CPI, again with ILP as base
159
Bottom-up
sense(p, s) ? isPredicate(p)
hasRole(p, a) ? isPredicate(p)
hasRole(p, a) ? isArgument(a)
role(p, a, r) ? hasLabel(p, a)
Top-Down
isPredicate(p) ? ?s.sense(p, s)
isPredicate(p) ? ?a.hasRole(p, a)
isArgument(a) ? ?p.hasRole(p, a)
hasLabel(p, a) ? ?r.role(p, a, r)
Unique Labels role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2)sense(p, s1) ? s1 6= s2 ? ?sense(p, r2)
Linguistic
role (p, a1, r) ? ?mod (r) ? a1 6= a2 ? ?role (p, a2, r)
lemma(p,+l) ? ppos(a,+p) ? hasRole(p, a) ? sense(p,+f)
lemma(p,+l) ? role(p, a,+r) ? sense(p,+f)
Table 2: Global formulae for ML model
solver.
5 Experimental Setup
For training and testing our SRL systems we used a
version of the CoNLL 2008 shared task (Surdeanu
et al, 2008) dataset that only mentions verbal predi-
cates, disregarding the nominal predicates available
in the original corpus.7 While the original (open
track) corpus came with MALT (Nivre et al, 2007)
dependencies, we observed slightly better results
when using the dependency parses generated with
a Charniak parser (Charniak, 2000). Hence we used
the latter for all our experiments.
To assess the performance of our model, and it to
evaluate the possible gains to be made from consid-
ering a joint model of the complete SRL pipeline,
we set up several systems. The full system uses a
Markov Logic Network with all local and global for-
mulae described in section 3. For the bottom-up sys-
tem we removed the structural top-down constraints
from the complete model?previous work Riedel
and Meza-Ruiz (2008) has shown that this can lead
to improved performance. The bottom-up (-arg) sys-
tem is equivalent to the bottom-up system, but it
does not include any formulae that mention the hid-
den isArgument predicate.
For the systems presented so far we perform joint
inference and learning. The pipeline system dif-
fers in this regard. For this system we train a sep-
arate model for each stage in the pipeline of figure
1. The predicate identification stage identifies the
predicates (using all local isPredicate formulae) of
7The reason for this choice where license problems.
a sentence. The next stage predicts arguments and
their roles for the identified predicates. Here we in-
clude all local and global formulae that involve only
the predicates of this stage. In the last stage we pre-
dict the sense of each identified predicate using all
formulae that involve the sense, without the struc-
tural constraints that connect the sense predicate to
the previous stages of the pipeline (these constraints
are enforced by architecture).
6 Results
Table 3 shows the results of our systems for the
CoNLL 2008 development set and the WSJ and
brown test sets. The scores are calculated using the
semantic evaluation metric of the CoNLL-08 shared
task (Surdeanu et al, 2008). This metric measures
the precision, recall and F1 score of the recovered
semantic dependencies. A semantic dependency is
created for each predicate and its arguments, the
label of such dependency is the role of the argu-
ment. Additionally, there is a semantic dependency
for each predicate and aROOT argument which has
the sense of the predicate as label.
To put these results into context, let us compare
them to those of the participants of the CoNLL 2008
shared task (see the last three rows of table 3).8 Our
best model, Bottom-up, would reach the highest F1
WSJ score, and second highest Brown score, for
the open track. Here the best-performing participant
was Vickrey and Koller (2008).
Table 3 also shows the results of the best (Jo-
hansson and Nugues, 2008) and fourth best sys-
8Results of other systems were extracted from Table 16 of
the shared task overview paper (Surdeanu et al, 2008).
160
tem (Zhao and Kit, 2008) of the closed track. We
note that we do significantly worse than Johansson
and Nugues (2008), and roughly equivalent to Zhao
and Kit (2008); this places us on the fourth rank of
19 participants. However, note that all three sys-
tems above us, as well as Zhao and Kit (2008), use
parsers with at least about 90% (unlabelled) accu-
racy on the WSJ test set (Johansson?s parser has
about 92% unlabelled accuracy).9 By contrast, with
about 87% unlabelled accuracy our parses are sig-
nificantly worse.
Finally, akin to Riedel and Meza-Ruiz (2008) we
observe that the bottom-up joint model performs
better than the full joint model.
System Devel WSJ Brown
Full 76.93 79.09 67.64
Bottom-up 77.96 80.16 68.02
Bottom-up (-arg) 77.57 79.37 66.70
Pipeline 75.69 78.19 64.66
Vickrey N/A 79.75 69.57
Johansson N/A 86.37 71.87
Zhao N/A 79.40 66.38
Table 3: Semantic F1 scores for our systems and three
CoNLL 2008 shared task participants. The Bottom-up
results are statistically significantly different to all others
(i.e., ? ? 0.05 according to the sign test).
6.1 Joint Model vs. Pipeline
Table 3 suggests that by including sense disam-
biguation into the joint model (as is the case for all
systems but the pipeline) significant improvements
can be gained. Where do these improvements come
from? We tried to answer this question by taking a
closer look at how accurately the pipeline predicts
the isPredicate, isArgument, hasRole, role and
sense relations, and how this compares to the result
of the joint full model.
Table 4 shows that the joint model mainly does
better when it comes to predicting the right predi-
cate senses. This is particularly true for the case of
the Brown corpus?here we gain about 10% points.
These results suggest that a more joint approach may
be particularly useful in order to increase the robust-
ness of an SRL system in out-of-domain scenarios.10
9Since our parses use a different label set we could not com-
WSJ Brown
Pipe. Fu. Pipe. Fu.
isPredicate 96.6 96.5 92.2 92.5
isArgument 90.3 90.6 85.9 86.9
hasRole 88.0 87.9 83.6 83.8
role 75.4 75.5 64.2 64.6
sense 85.5 88.5 67.3 77.1
Table 4: F1 scores for M predicates; Pipe. refers to the
Pipeline system, Fu. to the full system.
6.2 Modelling if a Token is an Argument
In table 3 we also observe that improvements can be
made if we explicitly model the decision whether a
token is a semantic argument of some predicate or
not. As we mentioned in section 3, this aspect of our
model requires us to jointly perform inference for
all predicates of a sentence, and hence our results
justify the per-sentence SRL approach proposed in
this paper.
In order to analyse where these improvements
come from, we again list our results on a per-SRL-
predicate basis. Table 5 shows that by including the
isArgument predicate and the corresponding for-
mulae we gain around 0.6% and 1.0% points across
the board for WSJ and Brown, respectively.11 As
shown in table 3, these improvements result in about
1.0% improvements for both WSJ and Brown in
terms of the CoNLL 2008 metric. Hence, an ex-
plicit model of the ?is an argument? decision helps
the SRL at all levels.
How the isArgument helps to improve the over-
all role labelling score can be illustrated with the
example in figure 3. Here the model without a
hidden isArgument predicate fails to attach the
preposition ?on? to the predicate ?start.01? (here 01
refers to the sense of the predicate). Apparently
the model has not enough confidence to assign the
preposition to either ?start.01? or ?get.03?, so it just
drops the argument altogether. However, because
the isArgument model knows that most preposi-
tions have to be modifying some predicate, pres-
pare labelled accuracy.
10The differences between results of the full and joint model
are statistically significant with the exception of the results for
the isPredicate predicate for the WSJ test set.
11The differences between results of the w/ and w/o model
are statistically significant with the exception of the results for
the sense predicate for the Brown test set.
161
Figure 3: Segment of the CoNLL 2008 development set
for which the bottom-up model w/o isArgument predi-
cate fails to attach the preposition ?on? as an ?AM-LOC?
for ?started?. The joint bottom-up model attaches the
preposition correctly.
sure is created that forces a decision between the
two predicates. And because for the role model
?start.01? looks like a better fit than ?get.03?, the
correct attachment is found.
WSJ Brown
w/o w/ w/o w/
isPredicate 96.3 96.5 91.4 92.5
hasRole 87.1 87.7 82.5 83.6
role 76.9 77.5 65.2 66.2
sense 88.3 89.0 76.1 77.5
Table 5: F1 scores for ML predicates; w/o refers to
a Bottom-up system without isArgument predicate, w/
refers to a Bottom-up system with isArgument predicate.
6.3 Efficiency
In the previous sections we have shown that our joint
model indeed does better than an equivalent pipeline
system. However, usually most joint approaches
come at a price: efficiency. Interestingly, in our case
we observe the opposite: our joint model is actually
faster than the pipeline. This can be seen in table 6,
where we list the time it took for several different
system to process the WSJ and Brown test corpus,
respectively. When we compare the times for the
bottom-up model to those of the pipeline, we note
that the joint model is twice as fast. While the indi-
vidual stages within the pipeline may be faster than
the joint system (even when we sum up inference
times), extracting results from one system and feed-
ing them into another creates overhead which offsets
this potential reduction.
Table 6 also lists the run-time of a bottom-up
system that solves the inference problem by fully
grounding the Markov Network that the Markov
Logic (ML) model describes, mapping this network
to an Integer Linear Program, and finding the most
likely assignment using an ILP solver. This sys-
tem (Bottom-up (-CPI)) is four times slower than the
equivalent system that uses Cutting Plane Inference
(Bottom-up). This suggests that if we were to imple-
ment the same joint model using ILP instead of ML,
our system would either be significantly slower, or
we would need to implement a Cutting Plane algo-
rithm for the corresponding ILP formulation?when
we use ML this algorithm comes ?for free?.
System WSJ Brown
Full 9.2m 1.5m
Full (-CPI) 38.4m 7.47m
Bottom-up 9.5m 1.6m
Bottom-up (-CPI) 38.8m 6.9m
Pipeline 18.9m 2.9m
Table 6: Testing times for full model and bottom-up when
CPI algorithm is not used. TheWSJ test set contains 2414
sentences, the Brown test set 426. Our best systems thus
takes on average 230ms per WSJ sentence (on a 2.4Ghz
system).
7 Conclusion
In this paper we have presented aMarkov Logic Net-
work that jointly models all predicate identification,
argument identification and classification and sense
disambiguation decisions for a sentence. We have
shown that this approach is competitive, in particular
if we consider that our input parses are significantly
worse than those of the top CoNLL 2008 systems.
We demonstrated the benefit of jointly predicting
senses and semantic arguments when compared to a
pipeline system that first picks arguments and then
senses. We also showed that by modelling whether
a token is an argument of some predicate and jointly
picking arguments for all predicates of a sentence,
further improvements can be achieved.
Finally, we demonstrated that our system is effi-
cient, despite following a global approach. This ef-
ficiency was also shown to stem from the first order
inference method our Markov Logic engine applies.
Acknowledgements
The authors are grateful to Mihai Surdeanu for pro-
viding the version of the corpus used in this work.
162
References
Eugene Charniak. A maximum-entropy-inspired
parser. In Proceedings of NAACL-2000, 2000.
Koby Crammer and Yoram Singer. Ultraconserva-
tive online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?
991, 2003.
Richard Johansson and Pierre Nugues. Dependency-
based semantic role labeling of propbank. In Pro-
ceedings of EMNLP-2008., 2008.
Llu??s Ma?rquez, Xavier Carreras, Ken Litkowski, and
Suzanne Stevenson. Semantic role labeling. Com-
putational Linguistics, 34(2), 2008. Introduction
to the Special Issue on Semantic Role Labeling.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kuebler, S. Marinov, and E. Marsi. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language
Engineering, 13(02):95?135, 2007.
V. Punyakanok, D. Roth, and W. Yih. General-
ized inference with multiple semantic role label-
ing systems. In Ido Dagan and Dan Gildea, ed-
itors, CoNLL ?05: Proceedings of the Annual
Conference on Computational Natural Language
Learning, pages 181?184, 2005.
Matthew Richardson and Pedro Domingos. Markov
logic networks. Technical report, University of
Washington, 2005.
Sebastian Riedel. Improving the accuracy and ef-
ficiency of map inference for markov logic. In
UAI ?08: Proceedings of the Annual Conference
on Uncertainty in AI, 2008.
Sebastian Riedel and Ivan Meza-Ruiz. Collective
semantic role labelling with markov logic. In
Conference on Computational Natural Language
Learning, 2008.
P. Singla and P. Domingos. Lifted First-Order Belief
Propagation. Association for the Advancement of
Artificial Intelligence (AAAI), 2008.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. The CoNLL-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of the
12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008), 2008.
Kristina Toutanova, Aria Haghighi, and Christo-
pher D. Manning. Joint learning improves seman-
tic role labeling. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Compu-
tational Linguistics, Morristown, NJ, USA, 2005.
David Vickrey and Daphne Koller. Applying sen-
tence simplification to the conll-2008 shared task.
In Proceedings of CoNLL-2008., 2008.
Nianwen Xue and Martha Palmer. Calibrating fea-
tures for semantic role labeling. In EMNLP ?04:
Proceedings of the Annual Conference on Em-
pirical Methods in Natural Language Processing,
2004.
Hai Zhao and Chunyu Kit. Parsing syntactic and se-
mantic dependencies with two single-stage max-
imum entropy models. In CoNLL 2008: Pro-
ceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, Manchester,
England, 2008.
163
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 226?230, New York City, June 2006. c?2006 Association for Computational Linguistics
Multi-lingual Dependency Parsing with Incremental Integer Linear
Programming
Sebastian Riedel and Ruket C?ak?c? and Ivan Meza-Ruiz
ICCS
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, UK
S.R.Riedel,R.Cakici,I.V.Meza-Ruiz@sms.ed.ac.uk
Abstract
Our approach to dependency parsing is
based on the linear model of McDonald
et al(McDonald et al, 2005b). Instead of
solving the linear model using the Max-
imum Spanning Tree algorithm we pro-
pose an incremental Integer Linear Pro-
gramming formulation of the problem that
allows us to enforce linguistic constraints.
Our results show only marginal improve-
ments over the non-constrained parser. In
addition to the fact that many parses did
not violate any constraints in the first place
this can be attributed to three reasons: 1)
the next best solution that fulfils the con-
straints yields equal or less accuracy, 2)
noisy POS tags and 3) occasionally our
inference algorithm was too slow and de-
coding timed out.
1 Introduction
This paper presents our submission for the CoNLL
2006 shared task of multilingual dependency pars-
ing. Our parser is inspired by McDonald et
al.(2005a) which treats the task as the search for the
highest scoring Maximum Spanning Tree (MST) in
a graph. This framework is efficient for both pro-
jective and non-projective parsing and provides an
online learning algorithm which combined with a
rich feature set creates state-of-the-art performance
across multiple languages (McDonald and Pereira,
2006).
However, McDonald and Pereira (2006) mention
the restrictive nature of this parsing algorithm. In
their original framework, features are only defined
over single attachment decisions. This leads to cases
where basic linguistic constraints are not satisfied
(e.g. verbs with two subjects). In this paper we
present a novel way to implement the parsing al-
gorithms for projective and non-projective parsing
based on a more generic incremental Integer Linear
Programming (ILP) approach. This allows us to in-
clude additional global constraints that can be used
to impose linguistic information.
The rest of the paper is organised in the following
way. First we give an overview of the Integer Linear
Programming model and how we trained its param-
eters. We then describe our feature and constraint
sets for the 12 different languages of the task (Hajic?
et al, 2004; Chen et al, 2003; Bo?hmova? et al, 2003;
Kromann, 2003; van der Beek et al, 2002; Brants
et al, 2002; Kawata and Bartels, 2000; Afonso et
al., 2002; Dz?eroski et al, 2006; Civit Torruella and
Mart?? Anton??n, 2002; Nilsson et al, 2005; Oflazer et
al., 2003; Atalay et al, 2003). Finally, our results are
discussed and error analyses for Chinese and Turk-
ish are presented.
2 Model
Our model is based on the linear model presented in
McDonald et al (2005a),
s (x,y) =
?
(i,j)?y
s (i, j) =
?
w ? f (i, j)(1)
where x is a sentence, y a parse and s a score func-
tion over sentence-parse pairs. f (i, j) is a multidi-
226
mensional feature vector representation of the edge
from token i to token j and w the corresponding
weight vector. Decoding in this model amounts to
finding the y for a given x that maximises s (x,y)
y? = argmaxys (x,y)
and y contains no cycles, attaches exactly one head
to each non-root token and no head to the root node.
2.1 Decoding
Instead of using the MST algorithm (McDonald et
al., 2005b) to maximise equation 1, we present an
equivalent ILP formulation of the problem. An ad-
vantage of a general purpose inference technique is
the addition of further linguistically motivated con-
straints. For instance, we can add constraints that
enforce that a verb can not have more than one sub-
ject argument or that coordination arguments should
have compatible types. Roth and Yih (2005) is
similarly motivated and uses ILP to deal with ad-
ditional hard constraints in a Conditional Random
Field model for Semantic Role Labelling.
There are several explicit formulations of the
MST problem as integer programs in the literature
(Williams, 2002). They are based on the concept of
eliminating subtours (cycles), cuts (disconnections)
or requiring intervertex flows (paths). However, in
practice these cause long solving times. While the
first two types yield an exponential number of con-
straints, the latter one scales cubically but produces
non-fractional solutions in its relaxed version, caus-
ing long runtime of the branch and bound algorithm.
In practice solving models of this form did not con-
verge after hours even for small sentences.
To get around this problem we followed an incre-
mental approach akin to Warme (1998). Instead of
adding constraints that forbid all possible cycles in
advance (this would result in an exponential num-
ber of constraints) we first solve the problem without
any cycle constraints. Only if the result contains cy-
cles we add constraints that forbid these cycles and
run the solver again. This process is repeated un-
til no more violated constraints are found. Figure 1
shows this algorithm.
Groetschel et al (1981) showed that such an ap-
proach will converge after a polynomial number of
iterations with respect to the number of variables.
1. Solve IP Pi
2. Find violated constraints C in the solution of Pi
3. if C = ? we are done
4. Pi+1 = Pi ? C
5. i = i + 1
6. goto (1)
Figure 1: Incremental Integer Linear Programming
In practice, this technique showed fast convergence
(less than 10 iterations) in most cases, yielding solv-
ing times of less than 0.5 seconds. However, for
some sentences in certain languages, such as Chi-
nese or Swedish, an optimal solution could not be
found after 500 iterations.
In the following section we present the bjective
function, variables and linear constraints that make
up the Integer Linear Program.
2.1.1 Variables
In the implementation1 of McDonald et al
(2005b) dependency labels are handled by finding
the best scoring label for a given token pair so that
s (i, j) = max s (i, j, label)
goes into Equation 1. This is only exact as long as no
further constraints are added. Since our aim is to add
constraints our variables need to explicitly model la-
bel decisions. Therefore, we introduce binary vari-
ables
li,j,label?i ? 0..n, j ? 1..n, label ? bestb (i, j)
where n is the number of tokens and the index 0
represents the root token. bestb (i, j) is the set of b
labels with maximal s (i, j, label). li,j,label equals 1
if there is a dependency with the label label between
token i (head) and j (child), 0 otherwise.
Furthermore, we introduce binary auxiliary vari-
ables
di,j?i ? 0..n, j ? 1..n
representing the existence of a dependency between
tokens i and j. We connect these to the li,j,label vari-
ables by a constraint
di,j =
?
label
li,j,label
.
1Note, however, that labelled parsing is not described in the
publication.
227
2.1.2 Objective Function
Given the above variables our objective function
can be represented as
?
i,j
?
label?bestk(i,j)
s (i, j, label) ? li,j,label
with a suitable k.
2.1.3 Constraints Added in Advance
Only One Head In all our languages every token
has exactly one head. This yields
?
i>0
di,j = 1
for non-root tokens j > 0 and
?
i
di,0 = 0
for the artificial root node.
Typed Arity Constraints We might encounter so-
lutions of the basic model that contain, for instance,
verbs with two subjects. To forbid these we simply
augment our model with constraints such as
?
j
li,j,subject ? 1
for all verbs i in a sentence.
2.1.4 Incremental Constraints
No Cycles If a solution contains one or more cy-
cles C we add the following constraints to our IP:
For every c ? C we add
?
(i,j)?c
di,j ? |c| ? 1
to forbid c.
Coordination Argument Constraints In coordi-
nation conjuncts have to be of compatible types. For
example, nouns can not coordinate with verbs. We
implemented this constraint by checking the parses
for occurrences of incompatible arguments. If we
find two arguments j, k for a conjunction i: di,j and
di,k and j is a noun and k is a verb then we add
di,j + di,k ? 1
to forbid configurations in which both dependencies
are active.
Projective Parsing In the incremental ILP frame-
work projective parsing can be easily implemented
by checking for crossing dependencies after each it-
eration and forbidding them in the next. If we see
two dependencies that cross, di,j and dk,l, we add
the constraint
di,j + dk,l ? 1
to prevent this in the next iteration. This can also
be used to prevent specific types of crossings. For
instance, in Dutch we could only allow crossing de-
pendencies as long as none of the dependencies is a
?Determiner? relation.
2.2 Training
We used single-best MIRA(Crammer and Singer,
2003).For all experiments we used 10 training iter-
ations and non-projective decoding. Note that we
used the original spanning tree algorithm for decod-
ing during training as it was faster.
3 System Summary
We use four different feature sets. The first fea-
ture set, BASELINE, is taken from McDonald and
Pereira (2005b). It uses the FORM and the POSTAG
fields. This set alo includes features that combine
the label and POS tag of head and child such as
(Label, POSHead) and (Label, POSChild?1). For
our Arabic and Japanese development sets we ob-
tained the best results with this configuration. We
also use this configuration for Chinese, German and
Portuguese because training with other configura-
tions took too much time (more than 7 days).
The BASELINE also uses pseudo-coarse-POS tag
(1st character of the POSTAG) and pseudo-lemma
tag (4 characters of the FORM when the length
is more than 3). For the next configuration we
substitute these pseudo-tags by the CPOSTAG and
LEMMA fields that were given in the data. This con-
figuration was used for Czech because for other con-
figurations training could not be finished in time.
The third feature set tries to exploit the generic
FEATS field, which can contain a list features such
as case and gender. A set of features per depen-
dency is extracted using this information. It con-
sists of cross product of the features in FEATS. We
used this configuration for Danish, Dutch, Spanish
228
and Turkish where it showed the best results during
development.
The fourth feature set uses the triplet of la-
bel, POS child and head as a feature such as
(Label, POSHead, POSChild). It also uses the
CPOSTAG and LEMMA fields for the head. This
configuration is used for Slovene and Swedish data
where it performed best during development.
Finally, we add constraints for Chinese, Dutch,
Japanese and Slovene. In particular, arity constraints
to Chinese and Slovene, coordination and arity con-
straints to Dutch, arity and selective projectivity
constraints for Japanese2. For all experiments b was
set to 2. We did not apply additional constraints to
any other languages due to lack of time.
4 Results
Our results on the test set are shown in Table 1.
Our results are well above the average for all lan-
guages but Czech. For Chinese we perform signif-
icantly better than all other participants (p = 0.00)
and we are in the top three entries for Dutch, Ger-
man, Danish. Although Dutch and Chinese are lan-
guages were we included additional constraints, our
scores are not a result of these. Table 2 compares the
result for the languages with additional constraints.
Adding constraints only marginally helps to improve
the system (in the case of Slovene a bug in our im-
plementation even degraded accuracy). A more de-
tailed explanation to this observation is given in the
following section. A possible explanation for our
high accuracy in Chinese could be the fact that we
were not able to optimise the feature set on the de-
velopment set (see the previous section). Maybe this
prevented us from overfitting. It should be noted that
we did use non-projective parsing for Chinese, al-
though the corpus was fully projective. Our worst
results in comparison with other participants can be
seen for Czech. We attribute this to the reduced
training set we had to use in order to produce a
model in time, even when using the original MST
algorithm.
2This is done in order to capture the fact that crossing de-
pendencies in Japanese could only be introduced through dis-
fluencies.
4.1 Chinese
For Chinese the parser was augmented with a set of
constraints that disallowed more than one argument
of the types head, goal, nominal, range, theme, rea-
son, DUMMY, DUMMY1 and DUMMY2.
By enforcing arity constraints we could either turn
wrong labels/heads into right ones and improve ac-
curacy or turn right labels/heads into wrong ones and
degrade accuracy. For the test set the number of im-
provements (36) was higher than the number of er-
rors (22). However, this margin was outweighed by
a few sentences we could not properly process be-
cause our inference method timed out. Our overall
improvement was thus unimpressive 7 tokens.
In the context of duplicate ?head? dependencies
(that is, dependencies labelled ?head?) the num-
ber of sentences where accuracy dropped far out-
weighed the number of sentences where improve-
ments could be gained. Removing the arity con-
straints on ?head? labels therefore should improve
our results.
This shows the importance of good second best
dependencies. If the dependency with the second
highest score is the actual gold dependency and its
score is close to the highest score, we are likely to
pick this dependency in the presence of additional
constraints. On the other hand, if the dependency
with the second highest score is not the gold one and
its score is too high, we will probably include this
dependency in order to fulfil the constraints.
There may be some further improvement to be
gained if we train our model using k-best MIRA
with k > 1 since it optimises weights with respect
to the k best parses.
4.2 Turkish
There is a considerable gap between the unlabelled
and labelled results for Turkish. And in terms of la-
bels the POS type Noun gives the worst performance
because many times a subject was classified as ob-
ject or vice a versa.
Case information in Turkish assigns argument
roles for nouns by marking different semantic roles.
Many errors in the Turkish data might have been
caused by the fact that this information was not ad-
equately used. Instead of fine-tuning our feature set
to Turkish we used the feature cross product as de-
229
Model AR CH CZ DA DU GE JP PO SL SP SW TU
OURS 66.65 89.96 67.64 83.63 78.59 86.24 90.51 84.43 71.20 77.38 80.66 58.61
AVG 59.94 78.32 67.17 78.31 70.73 78.58 85.86 80.63 65.16 73.53 76.44 55.95
TOP 66.91 89.96 80.18 84.79 79.19 87.34 91.65 87.60 73.44 82.25 84.58 65.68
Table 1: Labelled accuracy on the test sets.
Constraints DU CH SL JA
with 3927 4464 3612 4526
without 3928 4471 3563 4528
Table 2: Number of tokens correctly classified with
and without constraints.
scribed in Section 3. Some of the rather meaning-
less combinations might have neutralised the effect
of sensible ones. We believe that using morpho-
logical case information in a sound way would im-
prove both the unlabelled and the labelled dependen-
cies. However, we have not performed a separate ex-
periment to test if using the case information alone
would improve the system any better. This could be
the focus of future work.
5 Conclusion
In this work we presented a novel way of solving the
linear model of McDonald et al (2005a) for projec-
tive and non-projective parsing based on an incre-
mental ILP approach. This allowed us to include
additional linguistics constraints such as ?a verb can
only have one subject.?
Due to time constraints we applied additional
constraints to only four languages. For each one
we gained better results than the baseline without
constraints, however, this improvement was only
marginal. This can be attributed to 4 main rea-
sons: Firstly, the next best solution that fulfils the
constraints was even worse (Chinese). Secondly,
noisy POS tags caused coordination constraints to
fail (Dutch). Thirdly, inference timed out (Chinese)
and fourthly, constraints were not violated that often
in the first place (Japanese).
However, the effect of the first problem might be
reduced by training with a higher k. The second
problem could partly be overcome by using a bet-
ter tagger or by a special treatment within the con-
straint handling for word types which are likely to
be mistagged. The third problem could be avoidable
by adding constraints during the branch and bound
algorithm, avoiding the need to resolve the full prob-
lem ?from scratch? for every constraint added. With
these remedies significant improvements to the ac-
curacy for some languages might be possible.
6 Acknowledgements
We would like to thank Beata Kouchnir, Abhishek
Arun and James Clarke for their help during the
course of this project.
References
Koby Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. J. Mach. Learn.
Res., 3:951?991.
M. Groetschel, L. Lovasz, and A. Schrijver. 1981. The ellipsoid
method and its consequences in combinatorial optimization.
Combinatorica, I:169? 197.
R. McDonald and F. Pereira. 2006. Online learning of approx-
imate dependency parsing algorithms. In Proc. of the 11th
Annual Meeting of the EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. Online
large-margin training of dependency parsers. In Proc. of the
43rd Annual Meeting of the ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Ha-
jic. 2005b. Non-projective dependency parsing using span-
ning tree algorithms. In Proceedings of HLT/EMNLP 2005,
Vancouver, B.C., Canada.
D. Roth and W. Yih. 2005. Integer linear programming in-
ference for conditional random fields. In Proc. of the In-
ternational Conference on Machine Learning (ICML), pages
737?744.
David Michael Warme. 1998. Spanning Trees in Hypergraphs
with Application to Steiner Trees. Ph.D. thesis, University of
Virginia.
Justin C. Williams. 2002. A linear-size zero - one program-
ming model for the minimum spanning tree problem in pla-
nar graphs. Networks, 39:53?60.
230
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 85?90,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Semantic Role Labelling with Markov Logic
Ivan Meza-Ruiz? Sebastian Riedel??
?School of Informatics, University of Edinburgh, UK
?Department of Computer Science, University of Tokyo, Japan
?Database Center for Life Science, Research Organization of Information and System, Japan
?I.V.Meza-Ruiz@sms.ed.ac.uk ? sebastian.riedel@gmail.com
Abstract
This paper presents our system for the CoNLL
2009 Shared Task on Syntactic and Semantic
Dependencies in Multiple Languages (Hajic?
et al, 2009). In this work we focus only on the
Semantic Role Labelling (SRL) task. We use
Markov Logic to define a joint SRL model and
achieve the third best average performance in
the closed Track for SRLOnly systems and the
sixth including for both SRLOnly and Joint
systems.
1 Markov Logic
Markov Logic (ML, Richardson and Domingos,
2006) is a Statistical Relational Learning language
based on First Order Logic and Markov Networks.
It can be seen as a formalism that extends First Or-
der Logic to allow formulae that can be violated with
some penalty. From an alternative point of view, it is
an expressive template language that uses First Or-
der Logic formulae to instantiate Markov Networks
of repetitive structure.
In the ML framework, we model the SRL task
by first introducing a set of logical predicates1 such
as word(Token,Ortho) or role(Token,Token,Role). In
the case of word/2 the predicate represents a word
of a sentence, the type Token identifies the position
of the word and the type Ortho its orthography. In
the case of role/3, the predicate represents a seman-
tic role. The first token identifies the position of the
predicate, the second the syntactic head of the argu-
ment and finally the type Role signals the semantic
role label. We will refer to predicates such as word/2
1In the cases were is not obvious whether we refer to SRL
or ML predicates we add the prefix SRL or ML, respectively.
as observed because they are known in advance. In
contrast, role/3 is hidden because we need to infer it
at test time.
With the ML predicates we specify a set of
weighted first order formulae that define a distribu-
tion over sets of ground atoms of these predicates (or
so-called possible worlds). A set of weighted formu-
lae is called a Markov Logic Network (MLN). For-
mally speaking, an MLN M is a set of pairs (?,w)
where ? is a first order formula and w a real weight.
M assigns the probability
p (y) = 1Z exp
?
? ?
(?,w)?M
w
?
c?C?
f?c (y)
?
? (1)
to the possible world y. Here C? is the set of all
possible bindings of the free variables in ? with the
constants of our domain. f?c is a feature function
that returns 1 if in the possible world y the ground
formula we get by replacing the free variables in ?
by the constants in c is true and 0 otherwise. Z
is a normalisation constant. Note that this distri-
bution corresponds to a Markov Network (the so-
called Ground Markov Network) where nodes repre-
sent ground atoms and factors represent ground for-
mulae.
In this work we use 1-best MIRA (Crammer and
Singer, 2003) Online Learning in order to train the
weights of an MLN. To find the SRL assignment
with maximal a posteriori probability according to
an MLN and observed sentence, we use Cutting
Plane Inference (CPI, Riedel, 2008) with ILP base
solver. This method is used during both test time
and the MIRA online learning process.
85
2 Model
In order to model the SRL task in the ML frame-
work, we propose four hidden predicates. Consider
the example of the previous section:
argument/1 indicates the phrase for which its head
is a specific position is an SRL argument.
In our example argument(2) signals that the
phrase for which the word in position 2 is its
head is an argument (i.e., Ms. Haag).
hasRole/2 relates a SRL predicate to a SRL argu-
ment. For example, hasRole(3,2) relates the
predicate in position 3 (i.e., play) to the phrase
which head is in position 2 (i.e., Ms. Haag).
role/3 identifies the role for a predicate-argument
pair. For example, role(3,2,ARG0) denotes the
role ARG0 for the SRL predicate in the posi-
tion 2 and the SRL argument in position 3.
sense/2 denotes the sense of a predicate at a specific
position. For example, sense(3,02) signals that
the predicate in position 3 has the sense 02.
We also define three sets of observable predicates.
The first set represents information about each token
as provided in the shared task corpora for the closed
track: word for the word form (e.g. word(3,plays));
plemma/2 for the lemma; ppos/2 for the POS tag;
feat/3 for each feature-value pair; dependency/3 for
the head dependency and relation; predicate/1 for
tokens that are predicates according to the ?FILL-
PRED? column. We will refer to these predicates as
the token predicates.
The second set extends the information provided
in the closed track corpus: cpos/2 is a coarse POS
tag (first letter of actual POS tag); possibleArg/1 is
true if the POS tag the token is a potential SRL argu-
ment POS tag (e.g., PUNC is not); voice/2 denotes
the voice for verbal tokens based on heuristics that
use syntactic information, or based on features in the
FEAT column of the data. We will refer to these
predicates as the extended predicates.
Finally, the third set represents dependency infor-
mation inspired by the features proposed by Xue and
Palmer (2004). There are two types of predicates
in this set: paths and frames. Paths capture the de-
pendency path between two tokens, and frames the
subcategorisation frame for a token or a pair of to-
kens. There are directed and undirected versions of
paths, and labelled (with dependency relations) and
unlabelled versions of paths and frames. Finally, we
have a frame predicate with the distance from the
predicate to its head. We will refer to the paths and
most of the frames predicates as the path predicates,
while we will consider the frame predicates for a
unique token part token predicates.
The ML predicates here presented are used within
the formulae of our MLN. We distinguish between
two types of formula: local and global.
2.1 Local formulae
A formula is local if its groundings relate any num-
ber of observed ground atoms to exactly one hidden
ground atom. For example, a grounding of the local
formula
lemma(p,+l1)?lemma(a,+l2) ? hasRole(p, a)
connects a hidden hasRole/2 ground atom to two ob-
served plemma/2 ground atoms. This formula can be
interpreted as the feature for the predicate and argu-
ment lemmas in the argument identification stage of
a pipeline SRL system. Note that the ?+? prefix indi-
cates that there is a different weight for each possible
pair of lemmas (l1, l2).
We divide our local formulae into four sets, one
for each hidden predicate. For instance, the set for
argument/1 only contains formulae in which the hid-
den predicate is argument/1.
The sets for argument/1 and sense/2 predicates
have similar formulae since each predicate only in-
volves one token at time: the SRL argument or the
SRL predicate token. The formulae in these sets are
defined using only token or extended observed pred-
icates.
There are two differences between the argument/1
and sense/2 formulae. First, the argument/1 for-
mulae use the possibleArg/1 predicate as precondi-
tion, while the sense formulae are conditioned on
the predicate/1 predicate. For instance, consider the
argument/1 formula based on word forms:
word(a,+w) ? possibleArg(a) ? argument(a),
and the equivalent version for the sense/2 predicate:
word(p,+w) ? predicate(p) ? sense(p,+s).
This means we only apply the argument/1 formulae
if the token is a potential SRL argument, and the
sense/2 formulae if the token is a SRL predicate.
86
The second difference is the fact that for the
sense/2 formulae we have different weights for each
possible sense (as indicated by the +s term in the
second formula above), while for the argument/1
formulae this is not the case. This follows naturally
from the fact that argument/1 do not explicitly con-
sider senses.
Table 1 presents templates for the local formuale
of argument/1 and sense/2. Templates allow us to
compactly describe the FOL clauses of a ML. The
template column shows the body of a clause. The
last two columns of the table indicate if there is a
clause with the given body and argument(i) (I) or
sense(i,+s) (S) head, respectively. For example,
consider the first row: since the last two columns
of the row are marked, this template expands into
two formulae: word(i,+w) ? argument(i) and
word(i,+w) ? sense(i,+s). Including the pre-
conditions for each hidden predicate we obtain the
following formulae:
possibleArg(i) ? word(i,+w) ? argument(i)
and
predicate(i) ? word(i,+w) ? sense(i,+s).
In the case of the template marked with a ?*?
sign, the parameters P and I, where P ?
{ppos, plemma} and I ? {?2,?1, 0, 1, 2}, have to
be replaced by any combination of possible values.
Since we generate argument and sense formulae
for this template, the row corresponds to 20 formu-
lae in total.
Table 2 shows the local formuale for hasRole/2
and role/3 predicates, for these formulae we use to-
ken, extended and path predicates. In this case,
these templates have as precondition the formula
predicate(p) ? possibleArg(a). This ensures that
the formulae are only applied for SRL predicates
and potential SRL arguments. In the table we in-
clude the values to replace the template parame-
ters with. Some of these formulae capture a no-
tion of distance between SRL predicate and SRL
argument and are implicitely conjoined with a
distance(p, a,+d) atom. If a formulae exists both
with and without distance atom, we write Both in
the ?Dist? column; if it only exists with the distance
atom, we write Only, otherwise No.
Note that Tables 1 and 2 do not mention
the feature information provided in the cor-
Template I S
word(i,+w) X X
P(i+ I,+v)* X X
cpos(i+ 1,+c1) ? cpos(i? 1,+c2) X X
cpos(i+ 1,+c1) ? cpos(i? 1,+c2) ?
cpos(i+ 2,+c3) ? cpos(i? 2,+c4)
X X
dep(i, ,+d) X X
dep( , i,+d) X X
ppos(i,+o) ? dep(i, j,+d) X X
ppos(i,+o1) ? ppos(j,+o2) ?
dep(i, j,+d)
X X
ppos(j,+o1) ? ppos(k,+o2) ?
dep(j, k, ) ? dep(k, i,+d)
X X
plemma(i,+l) ? dep(j, i,+d) X X
frame(i,+f) X X
(Empty Body) X
Table 1: Templates of the local formulae for argument/1
and sense/2. I: head of clause is argument(i), S: head of
clause is sense(i,+s)
pora because this information was not avail-
able for every language. We therefore group
the formulae which consider the feature/3 pred-
icate into another a set we call feature formu-
lae. This is the summary of these formulae:
feat(p,+f,+v) ? sense(p,+s)
feat(p,+f,+v) ? argument(a)
feat(p,+f,+v1) ? feat(p, f,+v2) ?
hasRole(p, a)
feat(p,+f,+v1) ? feat(p, f,+v2) ?
role(p, a,+r)
Additionally, we define a set of language spe-
cific formulae. They are aimed to capture the re-
lations between argument and its siblings for the
hasRole/2 and role/3 predicates. In practice in
turned out that these formulae were only beneficial
for the Japanese language. This is a summary of
such formulae which we called argument siblings:
dep(a, h, ) ? dep(h, c, ) ? ppos(a,+p1)?
ppos(c,+p2) ? hasRole(p, a)
dep(a, h, ) ? dep(h, c, ) ? ppos(a,+p1)?
ppos(c,+p2) ? role(p, a,+r)
dep(a, h, ) ? dep(h, c, ) ? plemma(a,+p1)?
ppos(c,+p2) ? hasRole(p, a)
dep(a, h, ) ? dep(h, c, ) ? plemma(a,+p1)?
ppos(c,+p2) ? role(p, a,+r)
With these sets of formulae we can build specific
MLNs for each language in the shared task. We
group the formulae into the modules: argument/1,
87
Template Parameters Dist. H R
P(p,+v) P ? S1 Both X X
plemma(p,+l) ? ppos(a,+o) No X
ppos(p,+o) ? plemma(a,+l) No X
plemma(p,+l1) ? plemma(a,+l2) Only X X
ppos(p,+o1) ? ppos(a,+o2) Only X
ppos(p,+o1) ? ppos(a+ I,+o2) I ? {?1, 0, 1} Only X
plemma(p,+l) Only X
voice(p,+e) ? lemma(a,+l) Only X
cpos(p,+c1) ? cpos(p+ I,+c2) ? cpos(a,+c3) ? cpos(a+ J, c4) I,J ? {?1, 1}2 No X X
ppos(p,+v1) ? ppos(a, IN) ? dep(a,m, ) ?P(m,+v2) P ? S1 No X X
plemma(p,+v1) ? ppos(a, IN) ? dep(a,m, ) ? ppos(m,+v2) No X X
P(p, a,+v) P ? S2 No X X
P(p, a,+v) ? plemma(p,+l) P ? S3 No X X
P(p, a,+v) ? plemma(p,+l1) ? plemma(a,+l2) P ? S4 No X X
pathFrame(p, a,+t) ? plemma(p,+l) ? voice(p,+e) No X X
pathFrameDist(p, a,+t) Only X X
pathFrameDist(p, a,+t) ? voice(p,+e) Only X X
pathFrameDist(p, a,+t) ? plemma(p,+l) Only X X
P(p, a,+v) ? plemma(a,+l) P ? S5 Only X X
P(p, a,+v) ? ppos(p,+o) P ? S5 Only X X
pathFrameDist(p, a,+t) ? ppos(p,+o1) ? ppos(a,+o2) Only X X
path(p, a,+t) ? plemma(p,+l) ? cpos(a,+c) Only X X
dep( , a,+d) Only X X
dep( , a,+) ? voice(p,+e) Only X X
dep( , a,+d1) ? dep( , p,+d2) Only X X
(EmptyBody) No X X
Table 2: Templates of the local formulae for hasRole/2 and role/3. H: head of clause is hasRole(p, a), R:
head of clause is role(p, a,+r) and S1 = {ppos, plemma}, S2 = {frame, unlabelFrame, path}, S3 =
{frame, pathFrame}, S4 = {frame, pathFrame, path}, S5 = {pathFrameDist, path}
hasRole/2, role/3, sense/3, feature and argument sib-
lings. Table 3 shows the different configurations of
such modules that we used for the individual lan-
guages. We omit to mention the argument/1, has-
Role/2 and role/3 modules because they are present
for all languages.
A more detailed description of the formulae can
be found in our MLN model files.2 They can be
used both as a reference and as input to our Markov
Logic Engine,3 and thus allow the reader to easily
reproduce our results.
2.2 Global formulae
Global formulae relate several hidden ground atoms.
We use them for two purposes: to ensure consis-
2http://thebeast.googlecode.com/svn/
mlns/conll09
3http://thebeast.googlecode.com
Set Feature sense/2 Argument
siblings
Catalan Yes Yes No
Chinese No Yes No
Czech Yes No No
English No Yes No
German Yes Yes No
Japanese Yes No Yes
Spanish Yes Yes No
Table 3: Different configuration of the modules for the
formulae of the languages.
88
tency between the decisions of all SRL stages and
to capture some of our intuition about the task. We
will refer to formulae that serve the first purpose
as structural constraints. For example, a structural
constraint is given by the (deterministic) formula
role(p, a, r) ? hasRole(p, a)
which ensures that, whenever the argument a is
given a label r with respect to the predicate p, this
argument must be an argument of a as denoted by
hasRole(p,a).
The global formulae that capture our intuition
about the task itself can be further divided into two
classes. The first one uses deterministic or hard con-
straints such as
role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2)
which forbids cases where distinct arguments of a
predicate have the same role unless the role de-
scribes a modifier.
The second class of global formulae is soft or non-
deterministic. For instance, the formula
lemma(p,+l) ? ppos(a,+p)
?hasRole(p, a) ? sense(p,+f)
is a soft global formula. It captures the observation
that the sense of a verb or noun depends on the type
of its arguments. Here the type of an argument token
is represented by its POS tag.
Table 4 presents the global formulae used in this
model.
3 Results
For our experiments we use the corpora provided in
the SRLonly track of the shared task. Our MLN
is tested on the following languages: Catalan and
Spanish (Taule? et al, 2008) , Chinese (Palmer and
Xue, 2009), Czech (Hajic? et al, 2006),4 English
(Surdeanu et al, 2008), German (Burchardt et al,
2006), Japanese (Kawahara et al, 2002).
Table 5 presents the F1-scores and training/test
times for the development and in-domain corpora.
Clearly, our model does better for English. This is
4For training we use only sentences shorter than 40 words in
this corpus.
Structural constraints
hasRole(p, a) ? argument(a)
role(p, a, r) ? hasRole(p, a)
argument(a) ? ?p.hasRole(p, a)
hasRole(p, a) ? ?r.role(p, a, r)
Hard constraints
role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2)
sense(p, s1) ? s1 6= s2 ? ?sense(p, r2)
role (p, a1, r) ? ?mod (r) ? a1 6= a2 ?
?role (p, a2, r)
Soft constraints
role (p, a1, r) ? ?mod (r) ? a1 6= a2 ?
?role (p, a2, r)
plemma(p,+l)?ppos(a,+p)?hasRole(p, a) ?
sense(p,+f)
plemma(p,+l)? role(p, a,+r) ? sense(p,+f)
Table 4: Global formulae for ML model
Language Devel Test Train Test
time time
Average 77.25% 77.46% 11h 29m 23m
Catalan 78.10% 78.00% 6h 11m 14m
Chinese 77.97% 77.73% 36h 30m 34m
Czech 75.98% 75.75% 14h 21m 1h 7m
English 82.28% 83.34% 12h 26m 16m
German 72.05% 73.52% 2h 28m 7m
Japanese 76.34% 76.00% 2h 17m 4m
Spanish 78.03% 77.91% 6h 9m 16m
Table 5: F-scores for in-domain in corpora for each lan-
guage.
in part because the original model was developed for
English.
To put these results into context: our SRL system
is the third best in the SRLOnly track of the Shared
Task, and it is the sixth best on both Joint and SR-
LOnly tracks. For five of the languages the differ-
ence to the F1 scores of the best system is 3%. How-
ever, for German it is 6.19% and for Czech 10.76%.
One possible explanation for the poor performance
on Czech data will be given below. Note that in com-
parison our system does slightly better in terms of
precision than in terms of recall (we have the fifth
best average precision and the eighth average recall).
Table 6 presents the F1 scores of our system for
the out of domain test corpora. We observe a similar
tendency: our system is the sixth best for both Joint
and SRLOnly tracks. We also observe similar large
differences between our scores and the best scores
for German and Czech (i.e., > 7.5%), while for En-
glish the difference is relatively small (i.e., < 3%).
89
Language Czech English German
F-score 77.34% 71.86% 62.37%
Table 6: F-scores for out-domain in corpora for each lan-
guage.
Finally, we evaluated the effect of the argument
siblings set of formulae introduced for the Japanese
MLN. Without this set the F-score is 69.52% for the
Japanese test set. Hence argument siblings formulae
improve performance by more than 6%.
We found that the MLN for Czech was the one
with the largest difference in performance when
compared to the best system. By inspecting our
results for the development set, we found that for
Czech many of the errors were of a rather techni-
cal nature. Our system would usually extract frame
IDs (such as ?play.02?) by concatenating the lemma
of the token and outcome of the sense/2 prediction
(for the ?02? part). However, in the case of Czech
some frame IDs are not based on the lemma of the
token, but on an abstract ID in a vocabulary (e.g.,
?v-w1757f1?). In these cases our heuristic failed,
leading to poor results for frame ID extraction.
4 Conclusion
We presented a Markov Logic Network that per-
forms joint multi-lingual Semantic Role Labelling.
This network achieves the third best semantic F-
scores in the closed track among the SRLOnly sys-
tems of the CoNLL-09 Shared Task, and sixth best
semantic scores among SRLOnly and Joint systems
for the closed task.
We observed that the inclusion of features which
take into account information about the siblings of
the argument were beneficial for SRL performance
on the Japanese dataset. We also noticed that our
poor performance with Czech are caused by our
frame ID heuristic. Further work has to be done in
order to overcome this problem.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado?, and Manfred
Pinkal. The SALSA corpus: a German corpus
resource for lexical semantics. In Proceedings of
LREC-2006, Genoa, Italy, 2006.
Koby Crammer and Yoram Singer. Ultraconserva-
tive online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?
991, 2003. ISSN 1533-7928.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, and Zdene?k Z?abokrtsky?. Prague
dependency treebank 2.0, 2006.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??,
Llu??s Ma`rquez, Adam Meyers, Joakim Nivre, Se-
bastian Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mi-
ahi Surdeanu, Nianwen Xue, and Yi Zhang. The
CoNLL-2009 shared task: Syntactic and semantic
dependencies in multiple languages. In Proceed-
ings of CoNLL-2009), Boulder, Colorado, USA,
2009.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti
Hasida. Construction of a Japanese relevance-
tagged corpus. In Proceedings of the LREC-2002,
pages 2008?2013, Las Palmas, Canary Islands,
2002.
Martha Palmer and Nianwen Xue. Adding semantic
roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172, 2009.
Matt Richardson and Pedro Domingos. Markov
logic networks. Machine Learning, 62:107?136,
2006.
Sebastian Riedel. Improving the accuracy and ef-
ficiency of map inference for markov logic. In
UAI ?08: Proceedings of the Annual Conference
on Uncertainty in AI, 2008.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. The CoNLL-
2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of
CoNLL-2008, 2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta
Recasens. AnCora: Multilevel Annotated Cor-
pora for Catalan and Spanish. In Proceedings of
LREC-2008, Marrakesh, Morroco, 2008.
Nianwen Xue and Martha Palmer. Calibrating fea-
tures for semantic role labeling. In EMNLP ?04:
Proceedings of the Annual Conference on Em-
pirical Methods in Natural Language Processing,
2004.
90
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 193?197
Manchester, August 2008
Collective Semantic Role Labelling with Markov Logic
Sebastian Riedel Ivan Meza-Ruiz
Institute for Communicating and Collaborative Systems
School of Informatics
University of Edinburgh, Scotland
{S.R.Riedel,I.V.Meza-Ruiz}@sms.ed.ac.uk
Abstract
This paper presents our system for the
Open Track of the CoNLL 2008 Shared
Task (Surdeanu et al, 2008) in Joint De-
pendency Parsing
1
and Semantic Role La-
belling. We use Markov Logic to define
a joint SRL model and achieve a semantic
F-score of 74.59%, the second best in the
Open Track.
1 Introduction
Many SRL systems use a two-stage pipeline that
first extracts possible argument candidates (argu-
ment identification) and then assigns argument
labels to these candidates (argument classifica-
tion) (Xue and Palmer, 2004). If we also con-
sider the necessary previous step of identifying
the predicates and their senses (predicate identi-
fication) this yields a three-stage pipeline: predi-
cate identification, argument identification and ar-
gument classification.
Our system, on the other hand, follows a joint
approach in the spirit of Toutanova et al (2005)
and performs the above steps collectively . We de-
cided to use Markov Logic (ML, Richardson and
Domingos, 2005), a First Order Probabilistic Lan-
guage, to develop a global probabilistic model of
SRL. By using ML we are able to incorporate the
dependencies between the decisions of different
stages in the pipeline and the well-known global
correlations between the arguments of a predi-
cate (Punyakanok et al, 2005). And since learning
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
Note that in this work we do not consider the parsing
task; instead we use the provided dependencies of the open
track datatsets.
and inference methods were already implemented
in theML software we use, only minimal engineer-
ing efforts had to be done.
In contrast to the work of Toutanova et al (2005)
our system applies online learning to train its pa-
rameters and exact inference to predict a collective
role labelling. Moreover, we jointly label the argu-
ments of all predicates in a sentence. This allows
us, for example, to require that certain tokens have
to be an argument of some predicates in the sen-
tence.
In this paper we also investigate the impact of
different levels of interaction between the layers of
the joint SRL model. We find that a probabilis-
tic model which resembles a traditional bottom-up
pipeline (though jointly trained and globally nor-
malised) performs better than the complete joint
model on the WSJ test set and worse on the Brown
test set. The worst performance is observed when
no interaction between SRL stages is allowed.
In terms of semantic F-score (74.59%) our sub-
mitted results are the second best in the Open
Track of the Shared Task. Our error analysis in-
dicates that a) the training regime can be improved
and b) nominalizations are difficult to handle for
the model as it is.
In the next sections we will first briefly intro-
duce Markov Logic. Then we present the Markov
Logic model we used in our final submission. We
present and analyse our results in section 4 before
we conclude in Section 5.
2 Markov Logic
Markov Logic (ML, Richardson and Domingos,
2005) is a Statistical Relational Learning language
based on First Order Logic and Markov Networks.
It can be seen as a formalism that extends First
Order Logic to allow formulae that can be vi-
olated with some penalty. From an alternative
193
point of view, it is an expressive template language
that uses First Order Logic formulae to instantiate
Markov Networks of repetitive structure.
Let us describe Markov Logic by considering
the predicate identification task. In Markov Logic
we can model this task by first introducing a set
of logical predicates
2
such as isPredicate(Token)
or word(Token,Word). Then we specify a set of
weighted first order formulae that define a distribu-
tion over sets of ground atoms of these predicates
(or so-called possible worlds).
Ideally, the distribution we define with these
weighted formulae assigns high probability to pos-
sible worlds where SRL predicates are correctly
identified and a low probability to worlds where
this is not the case. For example, a suitable set of
weighted formulae would assign a high probability
to the world
3
{word (1,Haag) , word(2, plays),
word(3, Elianti), isPredicate(2)}
and a low one to
{word (1,Haag) , word(2, plays),
word(3, Elianti), isPredicate(3)}
In Markov Logic a set M = {(?,w
?
)}
?
of
weighted first order formulae is called a Markov
Logic Network (MLN). It assigns the probability
p (y) =
1
Z
exp
?
?
?
(?,w)?M
w
?
c?C
n
?
f
?
c
(y)
?
?
(1)
to the possible world y. Here f
?
c
is a feature func-
tion that returns 1 if in the possible world y the
ground formula we get by replacing the free vari-
ables in ? by the constants in c is true and 0 oth-
erwise. C
n
?
is the set of all tuples of constants we
can replace the free variables in ? with. Z is a nor-
malisation constant. Note that this distribution cor-
responds to a Markov Network where nodes rep-
resent ground atoms and factors represent ground
formulae.
For example, if M contains the formula ?
word
(
x,
?
take
?
)
? isPredicate (x)
then its corresponding log-linear model has,
among others, a feature f
?
t1
for which x in ? has
2
In the cases were is not obvious whether we refer to SRL
or ML predicates we add the prefix SRL or ML, respectively.
3
?Haag plays Elianti? is a segment of a sentence in train-
ing corpus.
been replaced by the constant t
1
and that returns 1
if
word
(
1,
?
take
?
)
? isPredicate (1)
is true in y and 0 otherwise.
We will refer predicates such as word as ob-
served because they are known in advance. In con-
trast, isPredicate is hidden because we need to in-
fer it at test time.
2.1 Learning
An MLN we use to model the collective SRL task
is presented in section 3. We learn the weights as-
sociated this MLN using 1-best MIRA (Crammer
and Singer, 2003) Online Learning method.
2.2 Inference
Assuming that we have an MLN, a set of weights
and a given sentence then we need to predict
the choice of predicates, frame types, arguments
and role labels with maximal a posteriori prob-
ability. To this end we apply a method that
is both exact and efficient: Cutting Plane Infer-
ence (CPI, Riedel, 2008) with Integer Linear Pro-
gramming (ILP) as base solver. We use it for infer-
ence at test time as well as during the MIRA online
learning process.
3 Model
We define five hidden predicates for the three
stages of the task. For predicate identification, we
use the predicates isPredicate and sense. isPred-
icate(p) indicates that the word in the position p
is an SRL predicate while sense(p,e) signals that
predicate in position p has the sense e.
For argument identification, we use the predi-
cates isArgument and hasRole. The atom isArgu-
ment(a) signals that the word in the position a is
a SRL argument of some (unspecified) SRL predi-
cate while hasRole(p,a) indicates that the token at
position a is an argument of the predicate in posi-
tion p.
Finally, for the argument classification stage we
define the predicate role. Here role(p,a,r) corre-
sponds to the decision that the argument in the po-
sition a has the role r with respect to the predicate
in the position p.
3.1 Local formulae
We define a set of local formulae. A formula is lo-
cal if its groundings relate any number of observed
ground atoms to exactly one hidden ground atom.
For example, a grounding of the local formula
lemma(p,+l
1
)?lemma(a,+l
2
) ? hasRole(p, a)
194
Figure 1: Factor graph for the local formula in sec-
tion 3.1.
can be seen in the Markov Network of Figure 1. It
connects a hidden hasRole ground atom to two ob-
served lemma ground atoms. Note that the ?+? pre-
fix for variables indicates that there is a different
weight for each possible pair of lemmas (l
1
, l
2
).
For the hasRole and role predicates we defined
local formulae that aimed to reproduce the stan-
dard features used in previous work (Xue and
Palmer, 2004). This also required us to develop
dependency-based versions of the constituent-
based features such as the syntactic path between
predicate and argument, as proposed by Xue and
Palmer (2004).
The remaining hidden predicates, isPredicate,
isArgument and sense, have local formulae that
relate their ground atoms to properties of a con-
textual window around the token the atom corre-
sponds to. For this we used the information pro-
vided in the closed track training corpus of the
shared task (i.e. both versions of lemma and POS
tags plus a coarse version of the POS tags).
Instead of describing the local feature set in
more detail we refer the reader to our MLN model
files.
4
They can be used both as a reference and
as input to our Markov Logic Engine
5
, and thus al-
low the reader to easily reproduce our results. We
believe that this is another advantage of explicitly
separating model and algorithms by using first or-
der probabilistic logic languages.
3.2 Global formulae
Global formulae relate several hidden ground
atoms. We use them for two purposes: to en-
sure consistency between the decisions of all SRL
stages and to capture some of our intuition about
the task. We will refer to formulae that serve the
first purpose as structural constraints.
For example, a structural constraint is given by
4
http://thebeast.googlecode.com/svn/
mlns/conll08
5
http://thebeast.googlecode.com
the (deterministic) formula
role(p, a, r) ? hasRole(p, a)
which ensures that, whenever the argument a is
given a label r with respect to the predicate p, this
argument must be an argument of a as denoted by
hasRole(p,a). Note that this formula by itself mod-
els the traditional ?bottom-up? argument identifi-
cation and classification pipeline: it is possible to
not assign a role r to an predicate-argument pair
(p, a) proposed by the identification stage; how-
ever, it is impossible to assign a role r to token
pairs (p, a) that have not been proposed as poten-
tial arguments.
One example of another class of structural con-
straints is
hasRole(p, a) ? ?r.role(p, a, r)
which, by itself, models an inverted or ?top-down?
pipeline. In this architecture the argument classi-
fication stage can assign roles to tokens that have
not been proposed by the argument identification
stage. However, it must assign a label to any token
pair the previous stage proposes. Figure 2 illus-
trates the structural formulae we use in form of a
Markov Network.
The formulae we use to ensure consistency be-
tween the remaining hidden predicates are omitted
for brevity as they are very similar to the bottom-
up and top-down formulae we presented above.
For the SRL predicates that perform a labelling
task (role and sense) we also need a structural con-
straint which ensures that not more than one label
is assigned. For instance,
(role(p, a, r
1
) ? r
1
6= r
2
? ?role(p, a, r
2
))
forbids two different semantic roles for a pair of
words.
The global formulae that capture our intuition
about the task itself can be further divided into two
classes. The first one uses deterministic or hard
constraints such as
role (p, a
1
, r) ? ?mod (r) ? a
1
6= a
2
?
?role (p, a
2
, r)
which forbids cases where distinct arguments of
a predicate have the same role unless the role de-
scribes a modifier.
The second class of global formulae is soft or
nondeterministic. For instance, the formula
lemma(p,+l) ? ppos(a,+p)
?hasRole(p, a) ? sense(p,+f)
195
Figure 2: Markov Network that illustrates the
structural constraints we use.
is a soft global formula. It captures the observation
that the sense of a verb or noun depends on the type
of its arguments. Here the type of an argument
token is represented by its POS tag.
4 Results
We only submitted results for the Open Track of
the Shared Task. Moreover, we focused on SRL
and did not infer dependencies; instead we used
the MALT dependencies parses provided in the
Open Track dataset. Our submission was ranked
second out of five with a semantic F1-score of
74.59%.
6
After submission we also set up additional ex-
periments to evaluate different types and degrees
of connectivity between the decisions made by our
model. To this end we created four new models:
a model that omits top-down structural constraints
and thus resembles a (globally trained) bottom-
up pipeline (Up); a model that does not contain
bottom-up structural constraints and thus resem-
bles a top-down architecture (Down); a model
in which stages are not connected at all (Iso-
lated); and finally, a model in which additional
global formulae are omitted and the only remain-
ing global formulae are structural (Structural). The
results we submitted were generated using the full
model (Full).
Table 1 summarises the results for each of these
models. We report the F-scores for the WSJ and
Brown test corpora provided for the task. In addi-
tion we show training and test times for each sys-
tem.
There are four findings we take from this. First,
and somewhat surprisingly, the jointly trained
bottom-up model (Up) performs substantially bet-
6
While we did use information of the open dataset we do
believe that it is possible to train a stacked parsing-SRL sys-
tem that would perform similarily. If so, our system would
have the 5th best semantic scores among the 20 participants
of the closed track.
Model WSJ Brown Train Test
Time Time
Full 75.72% 65.38% 25h 24m
Up 76.96% 63.86% 11h 14m
Down 73.48% 59.34% 22h 23m
Isolated 60.49% 48.12% 11h 14m
Structural 74.93% 64.23% 22h 33m
Table 1: F-scores for different models.
ter than the full model on the WSJ test corpus. We
will try to give an explanation for this result in
the next section. Second, the bottom-up model is
twice as fast compared to both the full and the top-
down model. This is due to the removal of formu-
lae with existential quantifiers that would result in
large clique sizes of the ground Markov Network.
Third, the isolated model performs extremely poor,
particularly for argument classification. Here fea-
tures defined for the role predicate can not make
any use of the information in previous stages. Fi-
nally, the additional global formulae do improve
performance, although not substantially.
4.1 Analysis
A substantial amount of errors in our submitted re-
sults (Full) can be attributed to the seemingly ran-
dom assignment of the very low frequency label
?R-AA? (appears once in the training set) to token
pairs that should either have a different role or no
role at all. Without these false positives, precision
would increase by about 1%. Interestingly, this
type of error completely disappears for the bottom-
up model (Up) and thus seem to be crucial in order
understand why this model can outperform the full
model.
We believe that this type of error is an artifact of
the training regime. For the full model the weights
of the role predicate only have ensure that the right
(true positive) role is the relative winner among
all roles. In the bottom-up model they also have
to make sure that their cumulative weight is non-
negative ? otherwise simply not assigning a role
r for (p, a) would increase the score even if has-
Role(p,a) is predicted with high confidence. Thus
more weight is shifted towards the correct roles.
This helps the right label to win more likely over
the ?R-AA? label, whose weights have rarely been
touched and are closer to zero.
Likewise, in the bottom-up model the total
weight of the hasRole features of a wrong (false
positive) candidate token pair must be nonpositive.
Otherwise picking the wrong candidate would in-
crease overall score and no role features can re-
196
ject this decision because the corresponding struc-
tural constraints are missing. Thus more weight
is shifted away from false positive candidates, re-
sulting in a higher precision of the hasRole pred-
icate. This also means that less wrong candidates
are proposed, for which the ?R-AA? role is more
likely to be picked because its weights have hardly
been touched.
However, it seems that by increasing precision
in this way, we decrease recall for out-of-domain
data. This leads to a lower F1 score for the bottom-
up model on the Brown test set.
Another prominent type of errors appear for
nominal predicates. Our system only recovers only
about 80% of predicates with ?NN?, ?NNS? and
?NNP? tags (and classifies about 90% of these with
the right predicate sense). Argument identification
and classification performs equally bad. For exam-
ple, for the ?A0? argument of ?VB? predicates we
get an F-score of 82.00%. For the ?A0? of ?NN?
predicates F-score is 65.92%. The features of our
system are essentially taken from the work done on
PropBank predicates and we did only little work
to adapt these to the case of nominal predicates.
Putting more effort into designing features specific
to the case of nominal predicates might improve
this situation.
5 Conclusion
We presented a Markov Logic Network that jointly
performs predicate identification, argument identi-
fication and argument classification for SRL. This
network achieves the second best semantic F-
scores in the Open Track of the CoNLL shared
task.
Experimentally we show that results can be fur-
ther improved by using an MLN that resembles a
conventional SRL bottom-up pipeline (but is still
jointly trained and globally normalised) instead
of a fully connected model. We hypothesise that
when training this model more weight is shifted
away from wrong argument candidates and more
weight is shifted towards correct role labels. This
results in higher precision for argument identifica-
tion and better accuracy for argument classifica-
tion.
Possible future work includes better treatment
of nominal predicates, for which we perform quite
poorly. We would also like to investigate the im-
pact of linguistically motivated global formulae
more thoroughly. So far our model benefits from
them, albeit not substantially.
References
Koby Crammer and Yoram Singer. Ultraconserva-
tive online algorithms for multiclass problems.
Journal of Machine Learning Research, 2003.
V. Punyakanok, D. Roth, and W. Yih. Generalized
inference with multiple semantic role labeling
systems. In Proceedings of the Annual Con-
ference on Computational Natural Language
Learning, 2005.
Matthew Richardson and Pedro Domingos.
Markov logic networks. Technical report,
University of Washington, 2005.
Sebastian Riedel. Improving the accuracy and ef-
ficiency of map inference for markov logic. In
Proceedings of the Annual Conference on Un-
certainty in AI, 2008.
Mihai Surdeanu, Richard Johansson, Adam Mey-
ers, Llu??s M`arquez, and Joakim Nivre. The
CoNLL-2008 shared task on joint parsing of
syntactic and semantic dependencies. In Pro-
ceedings of the 12th Conference on Compu-
tational Natural Language Learning (CoNLL-
2008), 2008.
Kristina Toutanova, Aria Haghighi, and Christo-
pher D. Manning. Joint learning improves se-
mantic role labeling. In Proceedings of the 43rd
Annual Meeting on Association for Computa-
tional Linguistics, 2005.
Nianwen Xue and Martha Palmer. Calibrating fea-
tures for semantic role labeling. In Proceedings
of the Annual Conference on Empirical Methods
in Natural Language Processing, 2004.
197
