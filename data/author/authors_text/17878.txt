Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 152?162,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning Latent Word Representations for Domain Adaptation
using Supervised Word Clustering
Min Xiao and Feipeng Zhao and Yuhong Guo
Department of Computer and Information Sciences
Temple University
Philadelphia, PA 19122, USA
{minxiao,feipeng.zhao,yuhong}@temple.edu
Abstract
Domain adaptation has been popularly stud-
ied on exploiting labeled information from a
source domain to learn a prediction model in
a target domain. In this paper, we develop a
novel representation learning approach to ad-
dress domain adaptation for text classification
with automatically induced discriminative la-
tent features, which are generalizable across
domains while informative to the prediction
task. Specifically, we propose a hierarchical
multinomial Naive Bayes model with latent
variables to conduct supervised word cluster-
ing on labeled documents from both source
and target domains, and then use the produced
cluster distribution of each word as its la-
tent feature representation for domain adapta-
tion. We train this latent graphical model us-
ing a simple expectation-maximization (EM)
algorithm. We empirically evaluate the pro-
posed method with both cross-domain doc-
ument categorization tasks on Reuters-21578
dataset and cross-domain sentiment classifica-
tion tasks on Amazon product review dataset.
The experimental results demonstrate that our
proposed approach achieves superior perfor-
mance compared with alternative methods.
1 Introduction
Supervised prediction models typically require a
large amount of labeled data for training. However,
manually collecting data annotations is expensive in
many real-world applications such as document cat-
egorization or sentiment classification. Recently, do-
main adaptation has been proposed to exploit exist-
ing labeled data in a related source domain to assist
the prediction model training in the target domain
(Ben-David et al, 2006; Blitzer et al, 2006; Daume?
III, 2007; Blitzer et al, 2011; Chen et al, 2012). As
an effective tool to reduce annotation effort, domain
adaptation has achieved success in various cross-
domain natural language processing (NLP) systems
such as document categorization (Dai et al, 2007),
sentiment classification (Blitzer et al, 2007; Chen
et al, 2012; Mejova and Srinivasan, 2012; Chen
et al, 2011), email spam detection (Jiang and Zhai,
2007), and a number of other NLP tasks (Blitzer
et al, 2011; Daume? III, 2007).
One primary challenge of domain adaptation lies
in the distribution divergence of the two domains
in the original feature representation space. For ex-
ample, documents about books may contain very
different high-frequency words and discriminative
words from documents about kitchen. A good cross-
domain feature representation thus has been viewed
as critical for bridging the domain divergence gap
and facilitating domain adaptation in the NLP area
(Ben-David et al, 2006, 2010). Many domain adap-
tation works have been proposed to learn new
cross-domain feature representations (Blitzer et al,
2006, 2011). Though demonstrated good perfor-
mance on certain problems, these works mostly in-
duce new feature representations in an unsupervised
way, without taking the valuable label information
into account.
In this work, we present a novel supervised rep-
resentation learning approach to discover a latent
representation of words which is not only general-
izable across domains but also informative to the
classification task. Specifically, we propose a hier-
152
archical multinomial Naive Bayes model with la-
tent word cluster variables to perform supervised
word clustering on labeled documents from both do-
mains. Our model directly models the relationships
between the observed document label variables and
the latent word cluster variables. The induced clus-
ter representation of each word thus will be infor-
mative for the classification labels, and hence dis-
criminative for the target classification task. We train
this directed graphical model using an expectation-
maximization (EM) algorithm, which maximizes the
log-likelihood of the observations of labeled docu-
ments. The induced cluster distribution of each word
can then be used as its generalizable representa-
tion to construct new cluster-based representation of
each document. For domain adaptation, we train a
supervised learning system with labeled data from
both domains in the new representation space and
apply it to categorize test documents in the target do-
main. In order to evaluate the proposed technique,
we conduct extensive experiments on the Reuters-
21578 dataset for cross-domain document catego-
rization and on Amazon product review dataset for
cross-domain sentiment classification. The experi-
mental results show the proposed approach can pro-
duce more effective representations than the com-
parison domain adaptation methods.
2 Related Work
Domain adaptation has recently been popularly
studied in natural language processing and a variety
of domain adaptation approaches have been devel-
oped, including instance weighting adaptation meth-
ods and feature representation learning methods.
Instance weighting adaptation methods improve
the transferability of a prediction model by training
an instance weighted learning system. Much work in
this category has been developed to address differ-
ent weighting schemas (Sugiyama et al, 2007; Wan
et al, 2011). Jiang and Zhai (2007) applied instance
weighting algorithms to tackle cross-domain NLP
tasks and proposed to remove misleading source
training data and assign less weights to labeled data
from the source domain than labeled data from the
target domain. Dai et al (2007) proposed to increase
the weights of mistakenly predicted instances from
the target domain and decrease the weights of incor-
rectly predicted instances from the source domain
during an iterative training process.
Representation learning methods bridge do-
main divergence either by differentiating domain-
invariant features from domain-specific features
(Daume? III, 2007; Daume? III et al, 2010; Blitzer
et al, 2011; Finkel and Manning, 2009) or seeking
generalizable latent features across domains (Blitzer
et al, 2006, 2007; Prettenhofer and Stein, 2010).
Daume? III (2007); Daume? III et al (2010) proposed
a simple heuristic feature replication method to rep-
resent common, source specific and target specific
features. Finkel and Manning (2009) proposed a for-
mer version of it based on the use of a hierarchi-
cal Bayesian prior. Blitzer et al (2011) proposed
a coupled subspace learning method, which learns
two projectors, one for each domain, to project the
original features into domain-sharing and domain-
specific features. Blitzer et al (2006) proposed a
structural correspondence learning (SCL) method to
model the correlation between pivot features and
non-pivot features. It uses the correlation to in-
duce latent domain-invariant features as augment-
ing features for supervised learning. Extensions of
this work include improving pivot feature selection
(Blitzer et al, 2007; Prettenhofer and Stein, 2010),
and improving the correlation modeling between
pivot and non-pivot features (Tan, 2009).
The proposed approach in this paper belongs to
representation learning methods. However, unlike
the unsupervised representation learning methods
reviewed above, our proposed approach learns gen-
eralizable feature representations of words by ex-
ploiting data labels from the two domains.
3 Learning Latent Word Representations
using Supervised Word Clustering
In this paper, we address domain adaptation for
text classification. Given a source domain DS with
plenty of labeled documents, and a target domain
DT with a very few labeled documents, the task is
to learn a classifier from the labeled documents in
both domains, and use it to classify the unlabeled
documents in the target domain. The documents in
the two domains share the same universal vocabu-
lary V = {w1, w2, ? ? ? , wn}, but the word distri-
butions in the two domains are typically different.
153
Therefore, training the classification model directly
from the original word feature space V may not gen-
eralize well in the target domain.
We propose to address this problem by first learn-
ing a supervised mapping function ? : V ?? Z
from the labeled documents in both domains, which
maps the input word features in the large vocabu-
lary set V into a low dimensional latent feature space
Z . By filtering out unimportant details and noises,
we expect the low dimensional mapping can cap-
ture the intrinsic structure of the input data that is
discriminative for the classification task and gener-
alizable across domains. In particular, we learn such
a mapping function by conducting supervised word
clustering on the labeled documents using a hierar-
chical multinomial Naive Bayes model. Below, we
will first introduce this supervised word clustering
model and then use the mapping function produced
to transform documents in different domains into the
same low-dimensional space for training cross do-
main text classification systems.
3.1 Supervised Word Clustering
Given all labeled documents from the source and
target domains, D = {(wt, yt)}Tt=1, where the t-th
labeled document is expressed as a bag of words,
wt = {wt1, wt2, ? ? ? , wtNt}, and its label value is
yt ? Y for Y = {1, ? ? ? ,K}, we propose to per-
form supervised word clustering by modeling the
document-label pair distribution using a hierarchical
multinomial Naive Bayes model given in Figure 1,
which has a middle layer of latent cluster variables.
In this plate model, the variable Yt denotes the
observed class label for the t-th document, and all
the label variables, {Yt}Tt=1, share the same multi-
nomial distribution ?Y across documents. The la-
tent variable Ct,i denotes the cluster membership
of the word Wt,i, and all the cluster variables,
{Ct,i}T,Ntt=1,i=1, share the same set of conditional dis-
tributions {?C|y}Ky=1 across documents and words.
The variable Wt,i denotes the i-th observed word
in the t-th document, and all the word variables,
{Wt,i}T,Ntt=1,i=1, share the same set of conditional dis-
tributions {?W |c}mc=1. Here we assume the number
of word clusters is m. For simplicity, we do not show
the distribution parameter variables in the Figure.
Following the Markov property of directed graph-
Figure 1: Supervised word clustering model.
ical models, we can see that given the cluster vari-
able values, the document label variables will be
completely independent of the word variables. By
learning this latent directed graphical model, we
thus expect the important classification information
expressed in the input observation words can be
effectively summarized into the latent cluster vari-
ables. This latent model is much simpler than the
supervised topic models (Blei and McAuliffe, 2007),
but we will show later that it can suitably produce a
generalizable feature mapping function for domain
adaptation.
To train the latent graphical model in Fig-
ure 1 on labeled documents D, we use a standard
expectation-maximization (EM) algorithm (Demp-
ster et al, 1977) to maximize the marginal log-
likelihood of the observations:
LL(D;?) =
?
t
logP (yt,wt|?) (1)
The EM algorithm is an iterative procedure. In each
iteration, it takes an alternative E-step and M-step
to maximize the lower bound of the marginal log-
likelihood function. In our experiments, we start
from a random initialization of the model parame-
ters and the latent variable values, and then perform
iterative EM updates until converge to a local opti-
mal solution.
3.2 Induced Word Representation
After training the supervised clustering model using
EM algorithm, a set of local optimal model parame-
ters ?? will be returned, which define a joint distri-
bution over the three groups of variables in the di-
rected graphical model. Next we define a supervised
latent feature mapping function ? from this trained
154
model to map each word w in the vocabulary V into
a conditional distribution vector over the word clus-
ter variable, such as
?(w)=[P (c=1|w,??), ? ? ? , P (c=m|w,??)]. (2)
The conditional distributions involved in this map-
ping function can be computed as
P (c|w,??)=
?
y?YP (w|c,??)P (c|y,??)P (y|??)
P (w)
(3)
where P (w|c,??) = ??w|c P (c|y,??) = ??c|y and
P (y|??) = ??y can be determined from the model
parameters directly, and p(w) can be computed as
the empirical frequency of word w among all the
other words in all the training documents.
We then define a transformation matrix ? ?
Rn?m based on the mapping function ? defined in
Eq. (2), such that ?i: = ?(wi) where wi is the i-th
word in the vocabulary V . That is, each row of ?
is the induced representation vector for one word. ?
can be viewed as a soft word clustering matrix, and
?i,j denotes the probability of word wi belongs to
the j-th cluster. Given the original document-word
frequency matrix Xtr ? RT?n for the labeled train-
ing documents from the two domains, we can con-
struct its representations Ztr ? RT?m in the pre-
dictive latent clustering space by performing the fol-
lowing transformation:
Ztr = Xtr?. (4)
Similarly, we can construct the new representation
matrix Zts for the test data Xts in the target domain.
We then train a classification model on the labeled
data Ztr and apply it to classify the test data Zts.
4 Experiments
We evaluate the proposed approach with experi-
ments on cross domain document categorization of
Reuters data and cross domain sentiment classifi-
cation of Amazon product reviews, comparing to a
number of baseline and existing domain adaptation
methods. In this section, we report the experimental
setting and results on these two data sets.
4.1 Approaches
We compared our proposed supervised word cluster-
ing approach (SWC) with the following five compar-
ison methods for domain adaptation:
(1) BOW: This is a bag-of-word baseline method,
which trains a SVM classifier with labeled data
from both domains using the original bag-of-
word features.
(2) PLSA: This is an unsupervised word clustering
method, which first applies the probabilistic la-
tent semantic analysis (PLSA) (Hofmann, 1999)
to obtain word clusterings with both labeled and
unlabeled data from the two domains and then
uses the soft word clusterings as augmenting
features to train SVM classifiers.
(3) FDLDA: This is an alternative supervised word
clustering method we built by training the
Fast-Discriminative Latent Dirichlet Allocation
model (Shan et al, 2009) with all labeled data
from the two domains. After training the model,
we used the learned topic distribution p(z) and
the conditional word distributions p(w|z) to
compute the conditional distribution over topics
p(z|w) for each word as the soft clustering of the
word. We then used the soft word clusterings as
augmenting features to train SVM classifiers.
(4) SCL: This is the structural correspondence
learning based domain adaptation method
(Blitzer et al, 2006). It first induces generaliz-
able features with all data from both domains
by modeling the correlations between pivot fea-
tures and non-pivot features, and then uses the
produced generalizable features as augmenting
features to train SVM classifiers.
(5) CPSP: This is coupled subspace learning based
domain adaptation method (Blitzer et al, 2011).
It first learns two domain projectors using all
data from the two domains by approximating
multi-view dimensionality reduction, and then
projects the labeled data to low dimensional la-
tent feature space to train SVM Classifiers.
We used the LIBSVM package (Chang and Lin,
2011) with its default parameter setting to train lin-
ear SVM classifiers as the base classification model
for all comparison methods.
155
Table 1: Average results (accuracy?standard deviation) for three cross-domain document categorization tasks on
Reuters-21578 dataset.
Task BOW PLSA FDLDA SCL CPSP SWC
Orgs vs People 76.07?0.39 76.50?0.10 76.95?0.23 78.71?0.20 77.58?0.21 81.27?0.23
Orgs vs Places 73.88?0.58 74.68?0.20 74.87?0.29 76.71?0.23 75.76?0.28 78.33?0.64
People vs Places 61.80?0.44 63.36?0.40 63.46?0.40 64.65?0.40 62.73?0.53 67.48?0.20
4.2 Experiments on Reuters Data Set
We used the popularly studied Reuters-21578
dataset (Dai et al, 2007), which contains three cross-
domain document categorization tasks, Orgs vs Peo-
ple, Orgs vs Places, People vs Places. The source
and target domains of each task contain documents
sampled from different non-overlapping subcate-
gories. From example, the task of Orgs vs People
assigns a document into one of the two top cate-
gories (Orgs, People), and the source domain doc-
uments and the target domain documents are sam-
pled from different subcategories of Orgs and Peo-
ple. There are 1237 source documents and 1208 tar-
get documents for the task of Orgs vs People, 1016
source documents and 1043 target documents for the
task of Orgs vs Places, and 1077 source documents
and 1077 target documents for the task ofPeople vs
Places. For each task, we built a unigram vocabulary
based on all the documents from the two domains
and represented each document as a feature vector
containing term frequency values.
4.2.1 Experimental Results for Cross-Domain
Document Categorization
For each of the three cross-domain document cat-
egorization tasks on Reuters-21578 dataset, we used
all the source documents as labeled training data
while randomly selecting 100 target documents as
labeled training data and setting the rest as unla-
beled test data. For the BOW baseline method, we
used the term-frequency features. The other five ap-
proaches are based on representation learning, and
we selected the dimension size of the representation
learning, i.e., the cluster number in our proposed ap-
proach, from {5, 10, 20, 50, 100} according to the
average classification results over 3 runs on the task
of Orgs vs People. The dimension sizes of the in-
duced representations for the five approaches, PLSA,
FDLDA, SCL, CPSP and SWC are 20, 20, 100, 100
and 20 respectively.
We then repeated each experiment 10 times on
each task with different random selections of the 100
labeled target documents to compare the six compar-
ison approaches. The average classification results
in terms of accuracy and standard deviations are re-
ported in Table 1. We can see that by simply combin-
ing labeled documents from the two domains with-
out adaptation, the BOW method performs poorly
across the three tasks. The PLSA method outper-
forms the BOW method over all the three tasks with
small improvements. The supervised word cluster-
ing method FDLDA, though performing slightly bet-
ter than the unsupervised clustering method PLSA,
produces poor performance comparing to the pro-
posed SWC method. One possible reason is that
the FDLDA model is not specialized for supervised
word clustering, and it uses a logistic regression
model to predict the labels from the word topics,
while the final soft word clustering is computed from
the learned distribution p(z) and p(w|z). That is,
in the FDLDA model the labels only influence the
word clusterings indirectly and hence its influence
can be much smaller than the influence of labels as
direct parent variables of the word cluster variables
in the SWC model. The two domain adaptation ap-
proaches, SCL and CPSP, both produce significant
improvements over BOW, PLSA and FDLDA on the
two tasks of Orgs vs People and Orgs vs Places,
while the CPSP method produces slightly inferior
performance than PLSA and FDLDA on the task of
People vs Places. The proposed method SWC on
the other hand consistently and significantly outper-
forms all the other comparison methods across all
the three tasks.
We also studied the sensitivity of the proposed
approach with respect to the number of clusters,
156
20 40 60 80 10060
65
70
75
80
85
Reuters?21578
Number of Cluster
A
cc
ur
ac
y
 
 
Orgs vs People
Orgs vs Places
People vs Places
Figure 2: Sensitivity analysis of the proposed approach
w.r.t. the number of clusters for the three cross-domain
document categorization tasks on Reuters-21578 dataset.
i.e., the dimension size of the learned representa-
tion. We experimented with a set of different val-
ues m ? {5, 10, 20, 50, 100} as the number of clus-
ters. For each m value, we used the same experimen-
tal setting as above and repeated the experiments 10
times to obtain the average comparison results. The
classification accuracy results on the three tasks are
reported in Figure 2. We can see that the proposed
method is not very sensitive to the number of clus-
ters, across the set of increasing values we consid-
ered, and its performance becomes very stable after
the cluster number reaches 20.
4.2.2 Document Categorization Accuracy vs
Label Complexity in Target Domain
We next conducted experiments to compare the
six approaches by varying the amount of the labeled
data from the target domain. We tested a set of dif-
ferent values, s ? {100, 200, 300, 400, 500}, as the
number of labeled documents from the target do-
main. For each different s value, we repeated the ex-
periments 10 times by randomly selecting s labeled
documents from the target domain using the same
experimental setting as before. The comparison re-
sults across the set of s values are plotted in Fig-
ure 3. We can see that in general the performance of
each method improves with the increase of the num-
ber of labeled documents from the target domain.
The proposed method SWC and the domain adapta-
tion method SCL clearly outperform the other four
methods. Moreover, the proposed method SWC not
only maintains consistent and significant advantages
over all other methods across the range of differ-
ent s values, its performance with 300 labeled tar-
get instances is even superior to the other methods
with 500 labeled target instances. All these results
suggest the proposed approach is very effective for
adapting data across domains.
4.3 Experiments on Amazon Product Reviews
We conducted cross-domain sentiment classification
on the widely used Amazon product reviews (Blitzer
et al, 2007), which contains review documents dis-
tributed in four categories: Books(B), DVD(D), Elec-
tronics(E) and Kitchen(K). Each category contains
1000 positive and 1000 negative reviews. We con-
structed 12 cross-domain sentiment classification
tasks, one for each source-target domain pair, B2D,
B2E, B2K, D2B, D2E, D2K, E2B, E2D, E2K, K2B,
K2D, K2E. For example, the task B2D means that
we use the Books reviews as the source domain and
the DVD reviews as the target domain. For each pair
of domains, we built a vocabulary with both uni-
gram and bigram features extracted from all the doc-
uments of the two domains, and then represented
each review document as a feature vector with term
frequency values.
4.3.1 Experimental Results for Cross-Domain
Sentiment Classification
For each of the twelve cross-domain sentiment
classification tasks on Amazon product reviews, we
used all the source reviews as labeled data and ran-
domly selected 100 target reviews as labeled data
while treating the rest as unlabeled test data. For the
baseline method BOW, we used binary indicator val-
ues as features, which has been shown to work better
than the term-frequency features for sentiment clas-
sification tasks (Pang et al, 2002; Na et al, 2004).
For all the other representation learning based meth-
ods, we selected the dimension size of learned repre-
sentation according to the average results over 3 runs
on the B2D task. The dimension sizes selected for
the methods PLSA, FDLDA, SCL, CPSP, and SWC
are 10, 50, 50, 100 and 10, respectively.1
150 and 100 are also the suggested values for SCL (Blitzer
et al, 2007) and CPSP (Blitzer et al, 2011) respectively on this
cross-domain sentiment classification dataset.
157
100 200 300 400 500
74
76
78
80
82
84
86
Orgs vs People
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
72
74
76
78
80
82
Orgs vs Places
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
60
62
64
66
68
70
72
74
People vs Places
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
Figure 3: Average classification results for three cross-domain document categorization tasks on Reuters-21578 dataset
by varying the amount of labeled training data from the target domain.
Table 2: Average results (accuracy?standard deviation) for twelve cross-domain sentiment classification tasks on
Amazon product reviews.
Task BOW PLSA FDLDA SCL CPSP SWC
B2D 76.58?0.14 76.01?0.10 75.95?0.16 80.17?0.16 77.53?0.14 81.66?0.23
B2K 75.48?0.34 74.68?0.20 74.87?0.15 78.13?0.21 76.38?0.15 82.26?0.20
B2E 72.92?0.37 73.36?0.19 73.46?0.21 74.79?0.19 73.31?0.17 77.04?0.64
D2B 74.10?0.29 74.04?0.20 74.08?0.18 78.73?0.23 77.07?0.15 79.95?0.25
D2K 75.19?0.33 75.37?0.31 75.44?0.31 76.98?0.19 76.77?0.10 82.13?0.20
D2E 73.01?0.34 74.21?0.30 74.09?0.31 75.69?0.25 73.83?0.21 76.98?0.54
E2B 67.58?0.24 68.48?0.15 68.44?0.17 70.21?0.16 70.47?0.16 72.11?0.46
E2D 70.15?0.27 70.16?0.23 70.06?0.22 72.83?0.25 71.76?0.20 73.81?0.59
E2K 82.23?0.12 82.24?0.18 82.26?0.19 84.69?0.11 81.31?0.14 85.33?0.16
K2B 70.67?0.18 72.18?0.21 72.18?0.16 73.91?0.21 72.18?0.19 75.78?0.55
K2D 71.51?0.26 72.00?0.18 72.05?0.19 74.82?0.26 72.59?0.18 76.88?0.49
K2E 80.81?0.12 80.39?0.18 80.46?0.18 82.96?0.11 80.81?0.14 84.78?0.19
We then repeated each experiment 10 times based
on different random selections of 100 labeled re-
views from the target domain to compare the six
methods on the twelve tasks. The average classifica-
tion results are reported in Table 2. We can see that
the PLSA and FDLDA methods do not show much
advantage over the baseline method BOW. CPSP
performs better than PLSA and BOW on many of
the twelve tasks, but with small advantages, while
SCL outperforms CPSP on most tasks. The proposed
method SWC however demonstrates a clear advan-
tage over all the other methods and produces the best
results on all the twelve tasks.
We also conducted sensitivity analysis over the
proposed approach regarding the number of clus-
ters on the twelve cross-domain sentiment classifi-
cation tasks, by testing a set of cluster number val-
ues m = {5, 10, 20, 50, 100}. The average results
are plotted in Figure 5. Similar as before, we can
see the proposed approach has stable performance
across the set of different cluster numbers. More-
over, these results also clearly show that domain
adaptation is not a symmetric process, as we can see
it is easier to conduct domain adaptation from the
source domain Books to the target domain Kitchen
(with an accuracy around 82%), but it is more diffi-
cult to make domain adaptation from the source do-
main Kitchen to the target domain Books (with an ac-
158
100 200 300 400 500
74
76
78
80
82
B2D
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
70
72
74
76
78
80
B2E
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
74
76
78
80
82
B2K
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
72
74
76
78
80
D2B
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
72
74
76
78
80
D2E
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
74
76
78
80
82
D2K
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
66
68
70
72
74
E2B
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
68
70
72
74
76
78
E2D
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
76
78
80
82
84
86
88
E2K
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
68
70
72
74
76
K2B
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
70
72
74
76
78
K2D
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
100 200 300 400 500
76
78
80
82
84
86
K2E
#Labeled instances
A
cc
ur
ac
y
 
 
BOW
PLSA
FDLDA
SCL
CPSP
SWC
Figure 4: Average results (accuracy?standard deviation) for the 12 cross-domain sentiment classification tasks on
Amazon product reviews with different numbers of labeled training data from the target domain.
159
20 40 60 80 100
72
74
76
78
80
82
84
Books
Number of cluster
A
cc
u
ra
cy
 
 
DVD
Kitchen
Electronics
20 40 60 80 100
72
74
76
78
80
82
84
DVD
Number of cluster
A
cc
u
ra
cy
 
 
Books
Kitchen
Electronics
20 40 60 80 100
70
75
80
85
Electronics
Number of cluster
A
cc
u
ra
cy
 
 
Books
DVD
Kitchen
20 40 60 80 100
72
74
76
78
80
82
84
86
Kitchen
Number of cluster
A
cc
u
ra
cy
 
 
Books
DVD
Electronics
Figure 5: Sensitivity analysis of the proposed approach wrt the number of clusters for the twelve cross-domain senti-
ment classification tasks. Each figure shows experimental results for three tasks with the same source domain.
curacy around 75%). It also shows that the degree of
relatedness of the two domains is an important factor
for the effectiveness of knowledge adaptation. For
example, one can see that it is much easier to con-
duct domain adaptation from Kitchen to Electronics
(with an accuracy around 84%) than from Kitchen to
Books (with an accuracy around 75%), as Kitchen is
more closely related to Electronics than Books.
4.3.2 Sentiment Classification Accuracy vs
Label Complexity in Target Domain
Similar as before, we tested the proposed ap-
proach using a set of different values s ?
{100, 200, 300, 400, 500} as the number of labeled
reviews from the target domain. For each given s
value, we conducted the comparison experiments us-
ing the same setting above. The average results are
reported in Figure 4. We can see that the perfor-
mance of each approach in general improves with
the increase of the number of labeled reviews from
the target domain. The proposed approach maintains
a clear advantage over all the other methods on all
the twelve tasks across different label complexities.
All those empirical results demonstrate the effec-
tiveness of the proposed approach for cross-domain
sentiment classification.
4.3.3 Illustration of the Word Clusters
Finally, we would also like to demonstrate the
hard word clusters produced by the proposed su-
pervised word clustering method. We assign a word
into the cluster it most likely belongs to according
to its soft clustering representation, such as c? =
argmaxc P (c|w,??). Table 3 presents the top repre-
sentative words (i.e., the most frequent words) of the
10 word clusters produced on the task of B2K. We
can see that the first three clusters (C1, C2, and C3)
contain words with positive sentiment polarity in
different degrees. The two clusters (C4 and C5) con-
tain words used to express the degree of opinions.
The next four clusters (C6, C7, C8, and C9) contain
content words related to Books or Kitchen. The last
cluster (C10) contains words of negative sentiment
polarity. These results demonstrate that the proposed
supervised word clustering can produce task mean-
ingful word clusters and hence label-informative la-
tent features, which justifies its effectiveness.
5 Conclusion
In this paper, we proposed a novel supervised rep-
resentation learning method to tackle domain adap-
tation by inducing predictive latent features based
on supervised word clustering. With the soft word
clustering produced, we can transform all docu-
ments from the two domains into a unified low-
dimensional feature space for effective training of
cross-domain NLP prediction system. We conducted
extensive experiments on cross-domain document
categorization tasks on Reuters-21578 dataset and
cross-domain sentiment classification tasks on Ama-
zon product reviews. Our empirical results demon-
strated the efficacy of the proposed approach.
References
S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira.
Analysis of representations for domain adapta-
160
Table 3: Clustering illustration for the task of B2K on Amazon product reviews.
C1 recommend excellent wonderful beautiful love powerful happy satisfied outstanding
C2 enjoyed fantastic glad i liked nicely was great benefits pleasure amazingly
C3 good and made me most people ordered this standards accurately check out
C4 was a kind of basically is only half of first of as if and still anything about have some
C5 ever may still going maybe either at least of all totally sort of are very
C6 life work machine size design bottom business picture hand hook gas sink turner shelves
C7 way coffee pan keep cooking maker heat job working children handle meet core wine
C8 people us world come fact man place stars during example went short bathroom apple price
C9 pot friends daily light fire tells knew holds keep the continued meal hooked silver wind
C10 disappointed waste unfortunately worse poorly sorry weak not worth stupid fails awful useless
tion. In Advances in Neural Information Process-
ing Systems (NIPS), 2006.
S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza,
F. Pereira, and J. Vaughan. A theory of learning
from different domains. Machine Learng, 79(1-
2):151?175, 2010.
D. Blei and J. McAuliffe. Supervised topic mod-
els. In Advances in Neural Information Process-
ing Systems (NIPS), 2007.
J. Blitzer, R. McDonald, and F. Pereira. Domain
adaptation with structural correspondence learn-
ing. In Proc. of the Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP), 2006.
J. Blitzer, M. Dredze, and F. Pereira. Biographies,
bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Proc.
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL), 2007.
J. Blitzer, D. Foster, and S. Kakade. Domain adapta-
tion with coupled subspaces. In Proc. of the Inter-
national Conference on Artificial Intelligence and
Statistics (AISTATS), 2011.
C. Chang and C. Lin. LIBSVM: A library for sup-
port vector machines. ACM Transactions on In-
telligent Systems and Technology, 2:27:1?27:27,
2011.
M. Chen, K. Weinberger, and J. Blitzer. Co-training
for domain adaptation. In Advances in Neural In-
form. Process. Systems (NIPS), 2011.
M. Chen, Z. Xu, K. Weinberger, and F. Sha.
Marginalized denoising autoencoders for domain
adaptation. In Proc. of the International Conf. on
Machine Learning (ICML), 2012.
W. Dai, Q. Yang, G. Xue, and Y. Yu. Boosting for
transfer learning. In Proc. of the International
Conf. on Machine Learning (ICML), 2007.
H. Daume? III. Frustratingly easy domain adaptation.
In Proc. of the Annual Meeting of the Association
for Comput. Linguistics (ACL), 2007.
H. Daume? III, A. Kumar, and A. Saha. Co-
regularization based semi-supervised domain
adaptation. In Advances in Neural Information
Processing Systems (NIPS), 2010.
A. Dempster, N. Laird, and D. Rubin. Maximum
likelihood from incomplete data via the em algo-
rithm. Journal of the royal statistical society, 39
(1):1?38, 1977.
J. Finkel and C. Manning. Hierarchical bayesian
domain adaptation. In Proc. of the Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL), 2009.
T. Hofmann. Probabilistic latent semantic analysis.
In Proc. of the Conference on Uncertainty in Ar-
tificial Intelligence (UAI), 1999.
J. Jiang and C. Zhai. Instance weighting for domain
adaptation in nlp. In Proc. of the Annual Meeting
of the Association for Computational Linguistics
(ACL), 2007.
Y. Mejova and P. Srinivasan. Crossing media
streams with sentiment: Domain adaptation in
161
blogs, reviews and twitter. In Proc. of the Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM), 2012.
J. Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou. Effec-
tiveness of simple linguistic processing in auto-
matic sentiment classification of product reviews.
In Proc. of the Conf. of the Inter. Society for
Knowledge Organization, 2004.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs
up?: sentiment classification using machine learn-
ing techniques. In Proc. of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), 2002.
P. Prettenhofer and B. Stein. Cross-language
text classification using structural correspondence
learning. In Proc. of the Annual Meeting of the
Association for Comput. Linguistics (ACL), 2010.
H. Shan, A. Banerjee, and N. Oza. Discriminative
mixed-membership models. In Proc. of the IEEE
Inter. Conference on Data Mining (ICDM), 2009.
M. Sugiyama, S. Nakajima, H. Kashima, P. von
Bu?nau, and M. Kawanabe. Direct importance es-
timation with model selection and its application
to covariate shift adaptation. In Advances in Neu-
ral Information Processing Systems (NIPS), 2007.
S. Tan. Improving scl model for sentiment-transfer
learning. In Proc. of the Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL), 2009.
C. Wan, R. Pan, and J. Li. Bi-weighting domain
adaptation for cross-language text classification.
In Proc. of the International Joint Conference on
Artificial Intelligence (IJCAI), 2011.
162
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1465?1475,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Semi-Supervised Representation Learning for
Cross-Lingual Text Classification
Min Xiao and Yuhong Guo
Department of Computer and Information Sciences
Temple University
Philadelphia, PA 19122, USA
{minxiao,yuhong}@temple.edu
Abstract
Cross-lingual adaptation aims to learn a pre-
diction model in a label-scarce target lan-
guage by exploiting labeled data from a label-
rich source language. An effective cross-
lingual adaptation system can substantially re-
duce the manual annotation effort required in
many natural language processing tasks. In
this paper, we propose a new cross-lingual
adaptation approach for document classifica-
tion based on learning cross-lingual discrim-
inative distributed representations of words.
Specifically, we propose to maximize the log-
likelihood of the documents from both lan-
guage domains under a cross-lingual log-
bilinear document model, while minimizing
the prediction log-losses of labeled docu-
ments. We conduct extensive experiments on
cross-lingual sentiment classification tasks of
Amazon product reviews. Our experimental
results demonstrate the efficacy of the pro-
posed cross-lingual adaptation approach.
1 Introduction
With the rapid development of linguistic resources
in different languages, developing cross-lingual nat-
ural language processing (NLP) systems becomes
increasingly important (Bel et al, 2003; Shanahan
et al, 2004). Recently, cross-lingual adaptation
methods have been studied to exploit labeled infor-
mation from an existing source language domain
where labeled training data is abundant for use in
a target language domain where annotated training
data is scarce (Prettenhofer and Stein, 2010). Pre-
vious work has shown that cross-lingual adaptation
can greatly reduce labeling effort for a variety of
cross language NLP tasks such as document catego-
rization (Bel et al, 2003; Amini et al, 2009), genre
classification (Petrenz and Webber, 2012), and sen-
timent classification (Shanahan et al, 2004; Wei and
Pal, 2010; Prettenhofer and Stein, 2010).
The fundamental challenge of cross-lingual adap-
tation stems from a lack of overlap between the fea-
ture space of the source language data and that of
the target language data. To address this challenge,
previous work in the literature mainly relies on au-
tomatic machine translation tools. They first trans-
late all the text data from one language domain into
the other and then apply techniques such as domain
adaptation (Wan et al, 2011; Rigutini and Maggini,
2005; Ling et al, 2008) and multi-view learning
(Amini et al, 2009; Guo and Xiao, 2012b; Wan,
2009) to achieve cross-lingual adaptation. However,
machine translation tools may not be freely available
for all languages. Moreover, translating all the text
data in one language into the other language is too
time-consuming in reality. As an economic alter-
native solution, cross-lingual representation learn-
ing has recently been used in the literature to learn
language-independent representations of the data for
cross language text classification (Prettenhofer and
Stein, 2010; Petrenz and Webber, 2012).
In this paper, we propose to tackle cross language
text classification by inducing cross-lingual predic-
tive data representations with both labeled and un-
labeled documents from the two language domains.
Specifically, we propose a cross-lingual log-bilinear
document model to learn distributed representations
of words, which can capture both the semantic sim-
1465
ilarities of words across languages and the predic-
tive information with respect to the target classifi-
cation task. We conduct the representation learn-
ing by maximizing the log-likelihood of all docu-
ments from both language domains under the cross-
lingual log-bilinear document model and minimiz-
ing the prediction log-losses of labeled documents.
We formulate the learning problem as a joint non-
convex minimization problem and solve it using a
local optimization algorithm. To evaluate the effec-
tiveness of the proposed approach, we conduct ex-
periments on the task of cross language sentiment
classification of Amazon product reviews. The em-
pirical results show the proposed approach is very
effective for cross-lingual document classification,
and outperforms other comparison methods.
2 Related Work
Much work in the literature proposes to construct
cross-lingual representations by using aligned paral-
lel data. Basically, they first employ machine trans-
lation tools to translate documents from one lan-
guage domain to the other one and then induce low
dimensional latent representations as interlingual
representations (Littman et al, 1998; Vinokourov
et al, 2002; Platt et al, 2010; Pan et al, 2011; Guo
and Xiao, 2012a). Littman et al (1998) proposed
a cross-language latent semantic indexing method
to induce interlingual representations by perform-
ing latent semantic indexing over a dual-language
document-term matrix, where each dual-language
document contains its original words and the corre-
sponding translation text. Vinokourov et al (2002)
proposed a cross-lingual kernel canonical corre-
lation analysis method, which learns two projec-
tions (one for each language) by conducting kernel
canonical correlation analysis over a paired bilin-
gual corpus and then uses the two projections to
project documents from language-specific feature
spaces to the shared multilingual semantic feature
space. Platt et al (2010) employed oriented prin-
cipal component analysis (Diamantaras and Kung,
1996) over concatenated parallel documents, which
learns a multilingual projection by simultaneously
minimizing the projected distance between paral-
lel documents and maximizing the projected covari-
ance of documents across different languages. Pan
et al (2011) proposed a bi-view non-negative matrix
tri-factorization method for cross-lingual sentiment
classification on the parallel training and test data.
Guo and Xiao (2012a) developed a transductive
subspace representation learning method for cross-
lingual text classification based on non-negative ma-
trix factorization. Some other works exploited par-
allel data by using multilingual topic models to ex-
tract cross-language latent topics as interlingual rep-
resentations (Mimno et al, 2009; Ni et al, 2011;
Platt et al, 2010; Smet et al, 2011) and using neu-
ral probabilistic language modes to learn word em-
beddings as cross-lingual distributed representations
(Klementiev et al, 2012). Most of them were de-
veloped by applying the latent Dirichlet alocation
(LDA) model (Blei et al, 2003) in a multilingual set-
ting, including the polylingual topic model (Mimno
et al, 2009), the bilingual LDA model (Smet et al,
2011), and the multilingual LDA model (Ni et al,
2011). Platt et al (2010) extended the probabilis-
tic latent semantic analysis (PLSA) model (Hof-
mann, 1999) and presented two variants of multilin-
gual topic models: the joint PLSA model and the
coupled PLSA model. Recently, Klementiev et al
(2012) extended the neural probabilistic language
model (Bengio et al, 2000) to induce cross-lingual
word distributed representations on a set of word-
level aligned parallel sentences. The applicability
of these approaches however is limited by the avail-
ability of parallel corpus. Translating the whole set
of documents to produce parallel corpus is too time-
consuming, expensive and even practically impossi-
ble for some language pairs. We thus do not evaluate
those approaches in our empirical study.
Another group of works propose to use bilin-
gual dictionaries to learn interlingual representa-
tions (Gliozzo, 2006; Prettenhofer and Stein, 2010).
Gliozzo (2006) first translated each term from one
language to the other using a bilingual dictionary
and used the translated terms to augment origi-
nal documents. Then they conducted latent se-
mantic analysis (LSA) over the document-term ma-
trix with concatenated vocabularies to obtain in-
terlingual representations. Prettenhofer and Stein
(2010) proposed a cross-language structural cor-
respondence learning (CL-SCL) method to induce
language-independent features by using word trans-
lation oracles. They first selected a subset of source
1466
language features, which have the highest mutual in-
formation with respect to the class labels in the la-
beled documents from the source language domain,
to translate them into the target language domain,
and then used these pivot pairs to induce cross-
lingual representations by modeling the correlations
between pivot features and non-pivot features. Our
proposed approach shares a similarity with the CL-
SCL method in (Prettenhofer and Stein, 2010) on
only requiring a small amount of word translations.
But our approach performs representation learning
in a semi-supervised manner by directly incorporat-
ing discriminative information with respect to the
target prediction task, while CL-SCL only exploits
labels when selecting pivot features and the struc-
tural correspondence learning process is conducted
in a fully unsupervised fashion.
Some other bilingual resources, such as multilin-
gual WordNet (Fellbaum, 1998) and universal part-
of-speech (POS) tags (Petrov et al, 2012), have also
been exploited in the literature for interlingual learn-
ing. Gliozzo (2006) proposed to use MultiWordNet
to map words from different languages to a common
synset-id as language-sharing terms. A similar work
was proposed in A.R. et al (2012), which trans-
formed words from different languages to WordNet
synset identifiers as interlingual sense-based rep-
resentations. However, multilingual WordNet re-
sources are not always available for different lan-
guage pairs. Recently, Petrenz and Webber (2012)
used language-specific POS taggers to tag each word
and then mapped those language-specific POS tags
to twelve universal POS tags as interlingual features
for cross language fine-grained genre classification.
This approach requires a POS tagger for each lan-
guage and it may be adversely affected by the POS
tagging accuracy.
3 Semi-Supervised Representation
Learning for Cross-Lingual Text
Classification
In this section, we introduce a semi-supervised
cross-lingual representation learning method and
then use it for cross language text classification.
Assume we have ?s labeled and us unlabeled doc-
uments in the source language domain S and ?t la-
beled and ut unlabeled documents in the target lan-
guage domain T . We assume all the documents are
independent and identically distributed in each lan-
guage domain, and each document xi is represented
as a bag of words, xi = {wi1, wi2, . . . , wiNi}. We
use (x?i , yi) to denote the i-th labeled document and
its label, and consider exploiting the labeled docu-
ments in the source domain S for learning classifiers
in the target domain T .
To build connections between the two language
domains, we first construct a set of critical bilingual
word pairs M = {(wsi , wtj)}mi=1, where wsi is a crit-
ical word in the source language domain, wtj is its
translation in the target language domain, and m is
the number of word pairs. Here being critical means
the word should be discriminative for the prediction
task and occur frequently in both language domains.
Following the work (Prettenhofer and Stein, 2010),
we select bilingual word pairs in a heuristic way.
First we select a subset of words from the source lan-
guage domain, which have the highest mutual infor-
mation with the class labels in labeled source docu-
ments. The mutual information is computed based
on the empirical distributions of words and labels
in the labeled source documents. Then we translate
the selected words into the target language using a
translation tool to produce word pairs. Finally we
produce the M set by eliminating any candidate pair
(ws, wt), if either ws occurs less than a predefined
threshold value ? in all source language documents
or wt occurs less than ? in all target language docu-
ments. Given the constructed bilingual word pair set
M , the words appearing in the source language doc-
uments but not in M can be put together to form a
source specific vocabulary set Vs = {ws1, . . . , wsvs}.
Similarly, the words appearing in the target language
documents but not in M can be put together to form
a target specific vocabulary set Vt = {wt1, . . . , wtvt}.
An overall cross-lingual vocabulary set can then be
constructed as V = Vs ? Vt ? M , which has a total
of v = vs + vt + m entries. This cross-lingual vo-
cabulary set covers all words appearing in both do-
mains, while mapping each bilingual pair in M into
the same entry.
To tackle cross language text classification, we
then propose a cross-lingual log-bilinear document
model to learn a predictive cross-lingual represen-
tation of words, which maps each entry in the vo-
cabulary set V to one row vector in a word embed-
1467
ding matrix R ? Rv?k. Similar to the log-bilinear
language model (Mnih and Hinton, 2007) and the
log-bilinear document model (Maas et al, 2011),
our proposed model learns a dense feature vector for
each word to capture semantic similarities between
the vocabulary entries. But unlike the previous two
models which only work with a monolingual lan-
guage, our model also captures semantic similarities
across different languages. Moreover, we explicitly
incorporate the label information into our proposed
approach, rendering the induced word embeddings
more discriminative to the target prediction task.
3.1 Cross-Lingual Word Embeddings
As mentioned above, we assume a unified embed-
ding matrix R which contains the distributed vec-
tor representations of words in the two language
domains. However, even in a unified representa-
tion space, the distribution of words in the two do-
mains will be different. To capture the distribution
divergence of the two domains and facilitate cross-
lingual learning, we split the word embedding ma-
trix into three parts: source language specific part
Rs ? Rv?ks , common part Rc ? Rv?kc and tar-
get language specific part Rt ? Rv?kt , such that
k = ks + kc + kt. Intuitively, we assume that source
language words contain no target language specific
representations and target language words contain
no source language specific representations. Thus
for words in the two language domains, we retrieve
their distributed vector representations from the em-
bedding matrix R using two mapping functions, ?S
and ?T , one for each language domain. The two
mapping functions are defined as
?S(w) =[Rs(w), Rc(w),0t]T (1)
?T (w) =[0s, Rc(w), Rt(w)]T (2)
where 0t is a kt-dimensional row vector of zeros,
0s is a ks-dimensional row vector of zeros, Rs(w)
denotes the row vector of Rs matrix corresponding
to the word w, Rc(w) denotes the row vector of Rc
matrix corresponding to the word w, and Rt(w) de-
notes the row vector of Rt matrix corresponding to
the word w. It is easy to see that each pair of words
in M will share the same vector from Rc. To encode
more information into the common part of represen-
tation for better knowledge transfer from the source
language domain to the target language domain, we
assume kc ? ks and kc ? kt. The form of three part
feature representations has been exploited in previ-
ous work of domain adaptation with heterogeneous
feature spaces (Duan et al, 2012). However, their
approach simply duplicates the original features as
language-specific representations, while we will au-
tomatically learn those three part latent representa-
tions in our approach.
3.2 Semi-Supervised Cross-Lingual
Representation Learning
Given the word representation scheme above, we
conduct cross-lingual representation learning by si-
multaneously maximizing the log-likelihood of all
documents and the conditional likelihood of labeled
documents from the two language domains
max
?
?
L?{S,T }
?
xi?L
Ni
?
j=1
logPL(wij |?)+
?
?
L?{S,T }
?
x?i?L
logPL(yi|x?i , ?) (3)
where ? denotes the model parameters and ? is a
trade-off parameter. The first part of the objective
function captures the likelihood of the documents
being generated with the learned representation R.
PL(wij |?) is the probability of word wij appearing
in the document xi from the language domain L, and
is defined as
PL(wij |?) =
exp (?EL(wij , ?))
?
w??V exp (?EL(w?, ?))
(4)
The term EL(wij , ?) is a log-bilinear energy func-
tion, defined as
EL(wij , ?) = ?dTi ?L(wij) ? bwij (5)
where di is a k-dimensional weight vector for docu-
ment xi and bwij is the bias for word wij . Below we
will use b to denote a v-dimensional vector contain-
ing all words? biases.
The second part of the objective function in (3)
takes the label information into account and aims
to render the latent word representations more task-
predictive. We use a logistic regression model to
1468
compute the conditional probability of the class la-
bel given the document with the induced word rep-
resentations, such that
PL(yi|x?i , ?) =
1
1 + exp
(
?yi
(
wT?L(x?i) + q
))
(6)
where w, q are model parameters of the logistic re-
gression model, ?L(xi) is the k-dimensional vector
representation of the document xi in the language
domain L. We compute ?L(xi) by taking average
over all words in the document xi such as
?L(xi) =
1
Ni
Ni
?
j=1
?L(wij) (7)
By summing over all descriptions above, we can
see that the proposed semi-supervised representa-
tion learning has a set of model parameters, ? =
{R, {di},b,w, q}. In order to avoid overfitting, we
add regularization terms for the parameters R, {di}
and w, which leads to the final optimization problem
below
max
?
?
L?{S,T }
?
xi?L
(
Ni
?
j=1
logPL(wij |?) ? ??di?22
)
+ ?
?
L?{S,T }
?
x?i?L
logPL(yi|x?i , ?)
? ??R?2F ? ??w?22 (8)
where ?, ?, ? are trade-off parameters, ? ? ?F denote
the Frobenius norm and ? ? ?2 denote the Euclidean-
norm. This objective function is not jointly convex
in all model parameters. We develop a gradient-
based iterative optimization procedure to seek a lo-
cal optimal solution. We first randomly initialize the
model parameters {di}, R,w and set b and q to ze-
ros. Then we iteratively make gradient-based up-
dates over the model parameters until reach a local
optimal solution.
3.3 Cross-Lingual Document Classification
After solving (8), we obtain a word embedding ma-
trix R. The distributed vector representation of any
given document can then be computed using Eq. (7)
based on Eq. (1) or Eq. (2). Under the distributed
vector representations of the documents in both lan-
guage domains, we perform cross-lingual document
classification by training a supervised classification
model using labeled data from both language do-
mains and then applying it to classify test documents
in the target language domain .
4 Experiments
We empirically evaluate the proposed approach us-
ing the cross language sentiment classification tasks
of Amazon product reviews in four languages. In
this section, we report our experimental results.
4.1 Dataset
We used the multilingual sentiment classification
dataset1 provided by Prettenhofer and Stein (2010),
which contains Amazon product reviews in four dif-
ferent languages, English (E), French (F), German
(G) and Japanese (J). The English product reviews
were sampled from previous cross-domain senti-
ment classification datasets (Blitzer et al, 2007),
while the other three language product reviews were
crawled from Amazon by the authors in November
2009. In the dataset, each language contains three
categories of product reviews, Books (B), DVD (D)
and Music (M). Each language-category pair con-
tains a balanced training set and test set, each of
which consists of 1000 positive reviews and 1000
negative reviews. Each review is represented as
a unigram bag-of-word feature vector with term-
frequency values. Following the work (Prettenhofer
and Stein, 2010), we used the original English re-
views as the source language while treating the other
three languages as target languages. Thus, we con-
struct nine cross language sentiment classification
tasks (GB, GD, GM, FB, FD, FM, JB, JD, JM), one
for each target language-category pair. For example,
the task GB means that the target language is Ger-
man and the training and test data are samples from
Books reviews.
4.2 Approaches
We compare our proposed semi-supervised cross-
lingual representation learning (CL-RL) approach
to the following approaches for cross-lingual doc-
ument classification.
1http://www.webis.de/research/corpora/
1469
Table 1: Average classification accuracies and standard deviations for the 9 cross-lingual sentiment classification tasks.
The bold format indicates that the difference between the results of CL-RL and MT is significant with p < 0.05 under
a McNemar paired test for labeling disagreements.
Task TB CL-Dict CLD-LSA CL-SCL MT CL-RL
GB 66.25?0.64 69.40?0.61 70.30?0.44 73.78?0.32 78.05?0.64 79.89?0.30
GD 63.16?0.66 66.37?0.63 66.85?0.46 71.99?0.25 75.75?0.58 77.14?0.16
GM 65.42?0.77 68.81?0.51 68.93?0.58 71.58?0.35 74.85?0.62 77.27?0.16
FB 65.98?0.51 69.35?0.48 69.98?0.51 73.89?0.16 78.00?0.49 78.25?0.32
FD 63.76?0.37 67.96?0.60 68.88?0.43 73.79?0.28 75.75?0.71 74.83?0.30
FM 65.94?0.56 67.98?0.69 68.42?0.60 71.20?0.28 74.85?0.49 78.71?0.32
JB 63.86?0.80 59.40?0.29 62.62?0.62 62.49?0.23 67.20?0.80 71.11?0.21
JD 63.59?0.74 62.13?0.26 63.87?0.72 65.54?0.29 67.70?0.57 73.12?0.23
JM 65.84?0.90 63.01?0.46 65.67?0.72 65.49?0.36 68.30?0.61 74.38?0.40
? TB: This is a target baseline method, which
trains a supervised monolingual classifier on
the labeled training data from the target lan-
guage domain without representation learning.
? CL-Dict: This is a simple baseline compar-
ison method, which uses the bilingual word
pairs directly to align features from different
language domains into a unified feature dictio-
nary and then trains a supervised classifier on
this aligned feature space with labeled training
data from both language domains.
? CLD-LSA: This is the cross-lingual represen-
tation learning method developed in (Gliozzo,
2006), which first translates each document
from one language into the other language via
a bilingual dictionary to produce augmenting
features, and then performs latent semantic
analysis (LSA) over the augmented bilingual
document-term matrix.
? CL-SCL: This is the cross language structural
correspondence learning method developed in
(Prettenhofer and Stein, 2010).
? MT: This is a machine translation based com-
parison method, which first uses an existing
machine translation tool (google translation) to
translate the target language documents into the
source language and then trains a monolingual
classifier with labeled training data from both
domains in the source language.
In all experiments, we used a linear support vec-
tor machine (SVM) for sentiment classification. For
implementation, we used the liblinear package (Fan
et al, 2008) with all of its default parameters. For
the CL-SCL method, we used the same parame-
ter setting as suggested in the paper (Prettenhofer
and Stein, 2010): the number of pivot features is
set as 450, the threshold value for selecting pivot
features is 30, and the reduced dimensionality af-
ter singular value decomposition is 100. For the
CLD-LSA method, we set the dimensionality of la-
tent representation as 1000. Similarly, for our pro-
posed approach, we built the cross-lingual vocabu-
lary M by setting m = 450 and ? = 30. For
our representation learning, we set ? = 1, ? =
? = ? = 1e?4, and set ks, kc, kt to be 25, 50,
25, respectively. The values of ?, ?, ? and ? are
selected using the first cross language classifica-
tion task GB. We selected the ? value from the
set {0.01, 0.1, 1, 10, 100} and selected ?, ?, ? values
from the set {1e?5, 1e?4, 1e?3, 1e?2, 1e?5} by re-
peating the experiment three times with random data
partitions and choosing the parameter values that led
to the best average classification accuracy.
4.3 Classification Accuracy
For each of the nine cross language sentiment classi-
fication tasks with different target language-category
pairs, we used the training set in the source language
domain (English) as labeled data while treating the
test set in the source language domain as unlabeled.
1470
100 200 300 400 500
65
70
75
80
GB
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
60
65
70
75
GD
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
65
70
75
GM
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
65
70
75
80
FB
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
60
65
70
75
80
FD
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
65
70
75
80
FM
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
55
60
65
70
75
JB
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
60
65
70
75
JD
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
60
65
70
75
JM
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
Figure 1: Average classification accuracies and standard deviations for 10 runs with respect to different numbers of
labeled training documents in the target language domain.
For target language domain, we used the test set as
test data while randomly selecting 100 documents
from the training set as labeled data and treating the
rest as unlabeled data. Thus, for each task, we have
2000 labeled documents and 2000 unlabeled docu-
ments from the source language domain, and 100
labeled and 1900 unlabeled documents from the tar-
get language domain for training. We have 2000 test
documents from the target language domain as test-
ing data. In each experiment, a classifier is produced
by each approach with the training data and tested
on the testing data. We repeated each experiment
10 times with different random selections of 100 la-
beled training documents from the target language
domain. The average classification accuracies and
standard deviations are reported in Table 1.
From Table 1, we can see that the proposed semi-
supervised cross-lingual representation learning ap-
proach, CL-RL, clearly outperforms all other com-
parison methods on eight out of the nine tasks. The
target baseline TB performs poorly on all the nine
tasks, which suggests that 100 labeled instances
from the target language is far from enough to ob-
tain an accurate sentiment classifier in the target lan-
1471
100 200 300 400
65
70
75
80
German
#Dimension
A
cc
ur
ac
y
 
 
Books
DVD
Music
100 200 300 400
65
70
75
80
French
#Dimension
A
cc
ur
ac
y
 
 
Books
DVD
Music
100 200 300 40060
65
70
75
80
Japanese
#Dimension
A
cc
ur
ac
y
 
 
Books
DVD
Music
Figure 2: Average classification accuracy and standard deviation results for the proposed approach over 10 runs with
respect to different dimensionality for the induced cross-lingual representations.
guage domain. By exploiting the large amount of
labeled training data from the source language do-
main, even the simple cross-lingual adaptation ap-
proach, CL-Dict, produces effective improvements
over TB. However, its performance is not consis-
tent across the nine tasks. It has inferior perfor-
mance than TB on the three tasks of adapting En-
glish to the Japanese language domain. This sug-
gests the simple bilingual word-pair based feature
space unification method is far from ideal for pro-
viding effective cross-lingual representations, espe-
cially when two languages (English, Japanese) are
very different. With a better designed representa-
tion learning, CLD-LSA outperforms CL-Dict on all
the nine tasks, but the improvements are very small
on some tasks (e.g., GM). CL-SCL not only out-
performs CL-Dict on all tasks, but also performs
much better than CLD-LSA on most tasks. Its per-
formance nevertheless is inferior to the method of
MT. Though MT can greatly increase the test accu-
racies comparing to the other four methods, TB, CL-
Dict, CLD-LSA, and CL-SCL, the benefit is obtained
at the cost of whole document translations. In con-
trast, our proposed approach does not require whole
document translations, but relies on the same sim-
ple word-pair translations used in CL-Dict. It how-
ever consistently and significantly outperforms TB,
CL-Dict, CLD-LSA, and CL-SCL on all tasks, and
outperforms MT on eight out of the nine tasks.
We also conduct significance tests for our pro-
posed approach and MT using a McNemar paired
test for labeling disagreements (Gillick and Cox,
1989). The results in bold format indicate that they
are significant with p < 0.05. All these results
demonstrate the efficacy of our cross-lingual repre-
sentation learning method.
4.4 Classification Accuracy vs the Number of
Labeled Target Documents
Next, we investigated the performance of the six ap-
proaches by varying the number of labeled train-
ing documents from the target language domain.
We maintained the same experimental setting as be-
fore, but investigated a range of different values,
?t = {100, 200, 300, 400, 500}, as the number of la-
beled training documents from the target language
domain. In each experiment, for a given value ?t,
we randomly selected ?t documents from the train-
ing set of the target language domain as labeled data
and used the rest as unlabeled data. We still per-
formed prediction on the same 2000 test documents
in the target language domain. We repeated each
experiment 10 times based on different random se-
lections of the labeled training data from the target
language domain. The average classification accura-
cies and standard deviations across different ?t val-
ues for all comparison methods on all the nine tasks
are plotted in Figure 1.
We can see when the number of labeled target
documents is small, TB performs poorly, especially
for the first six tasks (GB, GD, GM, FB, FD, FM).
By increasing the size of labeled target training data,
TB can greatly increase its prediction accuracies and
even outperform the CL-Dict method. The sim-
ple CL-Dict method has inconsistent performance
across the nine tasks. Its performance is better than
1472
TB when the labeled training data in the target lan-
guage domain is very limited and is poor than TB
when the labeled target data reaches 300 for the six
tasks using German and French as target languages.
Moreover, when adapting a system from English to
a much more different target language (Japanese),
CL-Dict produces much lower accuracies for all the
three tasks comparing with TB. These results show
that CL-Dict has very limited capacity on transfer-
ring labeled information from a related source lan-
guage domain. Similar performance is observed for
CLD-LSA. With a more sophisticated representation
learning, the CL-SCL method consistently outper-
forms CL-Dict. However, it produces inferior per-
formance than CLD-LSA on the tasks of JB and JM.
By using more translation resources, the MT method
outperforms TB, CL-Dict, CLD-LSA, CL-SCL in all
the nine tasks across almost all scenarios. Our pro-
posed method CL-RL significantly outperforms all
the other five comparison methods across all experi-
ments except on the task of FD, where MT produces
similar performance. Moreover, it is especially im-
portant to notice that CL-RL achieves high test ac-
curacies even when the number of labeled target in-
stances is small. This is important for transferring
knowledge from a source language to reduce the la-
beling effort in the target language.
4.5 Sensitivity Analysis
We also investigated the sensitivity of the proposed
approach over the dimensionality of the induced
cross-lingual representations. We used the same ex-
perimental setting as before, and conducted experi-
ments with a set of different dimensionality values,
k = {100, 200, 300, 400}. For each value k, we
set ks = 0.25k, kc = 0.5k, kt = 0.25k. We re-
peated each experiment for 10 times based on dif-
ferent random selections of labeled target training
data and plotted the average prediction accuracies
and standard deviations in Figure 2 for all the nine
cross-lingual sentiment classification tasks. We can
see the proposed approach produces stable accuracy
results across the range of different k values. This
suggests the proposed approach is not very sensitive
to the dimensionality of the cross-lingual embedding
features within the considered range of values, and
with a small dimensionality of 100, the induced rep-
resentation can already perform very well.
4.6 Cross-Lingual Word Representations
Finally, we used the first task GB, which adapts the
Books reviews from English to German, to gain in-
tuitive understandings over the learned cross-lingual
word representations. Given an English word as
seed word, we find its five closest neighboring En-
glish words and German words according to the Eu-
clidean distances calculated in the induced cross-
lingual representation space. We present a few re-
sults in Table 2. From Table 2, we can see that the re-
trieved words in both language domains are seman-
tically close to the seed words, which indicates that
our proposed method can capture semantic similar-
ities of words not only in a monolingual setting but
also in a multilingual setting.
5 Conclusion
In this paper, we proposed a semi-supervised cross-
lingual representation learning approach to address
cross-lingual text classification. The distributed
word representation induced by the proposed ap-
proach can capture semantic similarities of words
across languages while maintaining predictive infor-
mation with respect to the target classification tasks.
To evaluate the proposed approach, we conducted
experiments on nine cross language sentiment clas-
sification tasks constructed from the Amazon prod-
uct reviews in four languages, comparing to a num-
ber of comparison methods. The empirical results
showed that the proposed approach can produce
effective cross-lingual adaptation performance and
significantly outperform other comparison methods.
References
M. Amini, N. Usunier, and C. Goutte. Learning from
multiple partially observed views - an application
to multilingual text categorization. In Advances in
Neural Information Processing Systems (NIPS),
2009.
B. A.R., A. Joshi, and P. Bhattacharyya. Cross-
lingual sentiment analysis for indian languages
using linked wordnets. In Proceedings of the
International Conference on Computational Lin-
guistics (COLING), 2012.
N. Bel, C. Koster, and M. Villegas. Cross-lingual
1473
Table 2: Examples of source seed words together with five closest English words and five closest German words
estimated using the Euclidean distance in the cross-lingual representation space on the task GB.
books absolutely love
English German English German English German
books buch absolutely absolut love liebe
book bu?cher definitely absolute loved lieben
text text completely definitiv like wie
page blatt certainly komplett fond wieder
words wo?rter totally sicher feel fu?hlen
expensive good not
English German English German English German
expensive teuer good gut not nicht
expense ho?her better besser no nie
overpriced ho?chsten well nett cannot nein
costly hoch nice gro?artig non keine
price preis great gro??ten never keines
text categorization. In Proceedings of European
Conference on Digital Libraries (ECDL), 2003.
Y. Bengio, R. Ducharme, and P. Vincent. A neu-
ral probabilistic language model. In Advances in
Neural Information Processing Systems (NIPS),
2000.
D. Blei, A. Ng, and M. Jordan. Latent dirichlet al
location. Journal of Machine Learning Research
(JMLR), 3:993?1022, 2003.
J. Blitzer, M. Dredze, and F. Pereira. Biographies,
bollywood, boomboxes and blenders: Domain
adaptation for sentiment classification. In Pro-
ceedings of the Annual Meeting of the Asso. for
Computational Linguistics (ACL), 2007.
K. Diamantaras and S. Kung. Principal component
neural networks: theory and applications. Wiley-
Interscience, 1996.
L. Duan, D. Xu, and I. Tsang. Learning with aug-
mented features for heterogeneous domain adap-
tation. In Proceedings of the International Con-
ference on Machine Learning (ICML), 2012.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
LIBLINEAR: A library for large linear classifi-
cation. Journal of Machine Learning Research
(JMLR), 9:1871?1874, 2008.
C. Fellbaum, editor. WordNet: an electronic lexical
database. MIT Press, 1998.
L. Gillick and S. Cox. Some statistical issues
in the comparison of speech recognition algo-
rithms. In Proceedings of the International Con-
ference on Acoustics, Speech, and Signal Process-
ing (ICASSP), 1989.
A. Gliozzo. Exploiting comparable corpora and
bilingual dictionaries for cross-language text cat-
egorization. In Proceedings of the International
Conference on Computational Linguistics and the
Annual Meeting of the Association for Computa-
tional Linguistics (ICCL-ACL), 2006.
Y. Guo and M. Xiao. Transductive representation
learning for cross-lingual text classification. In
Proceedings of the IEEE International Confer-
ence on Data Mining (ICDM), 2012a.
Y. Guo and M. Xiao. Cross language text clas-
sification via subspace co-regularized multi-view
learning. In Proceedings of the International Con-
ference on Machine Learning (ICML), 2012b.
T. Hofmann. Probabilistic latent semantic analysis.
In Proceedings of Uncertainty in Artificial Intelli-
gence (UAI), 1999.
A. Klementiev, I. Titov, and B. Bhattarai. Inducing
crosslingual distributed representations of words.
1474
In Proceedings of the International Conference on
Computational Linguistics (COLING), 2012.
X. Ling, G. Xue, W. Dai, Y. Jiang, Q. Yang, and
Y. Yu. Can chinese web pages be classified with
english data source? In Proceedings of the Inter-
national Conference on World Wide Web (WWW),
2008.
M. Littman, S. Dumais, and T. Landauer. Automatic
Cross-Language Information Retrieval using La-
tent Semantic Indexing, chapter 5, pages 51?62.
Kluwer Academic Publishers, 1998.
A. Maas, R. Daly, P. Pham, D. Huang, A. Ng, and
C. Potts. Learning word vectors for sentiment
analysis. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies (ACL), 2011.
D. Mimno, H. Wallach, J. Naradowsky, D. Smith,
and A. McCallum. Polylingual topic models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Vol-
ume 2 - Volume 2, 2009.
A. Mnih and G. Hinton. Three new graphical mod-
els for statistical language modelling. In Proceed-
ings of the International Conference on Machine
Learning (ICML), 2007.
X. Ni, J. Sun, J. Hu, and Z. Chen. Cross lingual
text classification by mining multilingual topics
from wikipedia. In Proceedings of the ACM In-
ternational Conference on Web Search and Data
Mining (WSDM), 2011.
J. Pan, G. Xue, Y. Yu, and Y. Wang. Cross-lingual
sentiment classification via bi-view non-negative
matrix tri-factorization. In Proceedings of the
Pacific-Asia conference on Advances in knowl-
edge discovery and data mining (PAKDD), 2011.
P. Petrenz and B. Webber. Label propagation for
fine-grained cross-lingual genre classification. In
Proceedings of the NIPS xLiTe workshop, 2012.
S. Petrov, D. Das, and R. McDonald. A universal
part-of-speech tagset. In Proceedings of the Inter-
national Conference on Language Resources and
Evaluation (LREC), 2012.
J. Platt, K. Toutanova, and W. Yih. Translingual doc-
ument representations from discriminative projec-
tions. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), 2010.
P. Prettenhofer and B. Stein. Cross-language
text classification using structural correspondence
learning. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL), 2010.
L. Rigutini and M. Maggini. An em based train-
ing algorithm for cross-language text categoriza-
tion. In Proceedings of the Web Intelligence Con-
ference, 2005.
J. Shanahan, G. Grefenstette, Y. Qu, and D. Evans.
Mining multilingual opinions through classifica-
tion and translation. In Proceedings of AAAI
Spring Symposium on Exploring Attitude and Af-
fect in Text, 2004.
W. Smet, J. Tang, and M. Moens. Knowledge trans-
fer across multilingual corpora via latent topics.
In Proceedings of the Pacific-Asia conference on
Advances in knowledge discovery and data min-
ing (PAKDD), 2011.
A. Vinokourov, J. Shawe-taylor, and N. Cristian-
ini. Inferring a semantic representation of text
via cross-language correlation analysis. In Ad-
vances in Neural Information Processing Systems
(NIPS), 2002.
C. Wan, R. Pan, and J. Li. Bi-weighting domain
adaptation for cross-language text classification.
In Proceedings of the International Joint Confer-
ence on Artificial Intelligence (IJCAI), 2011.
X. Wan. Co-training for cross-lingual sentiment
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), 2009.
B. Wei and C. Pal. Cross lingual adaptation: An
experiment on sentiment classifications. In Pro-
ceedings of the Annual Meeting of the Asso. for
Computational Linguistics (ACL), 2010.
1475
Learning Representations for Weakly
Supervised Natural Language
Processing Tasks
Fei Huang?
Temple University
Arun Ahuja??
Northwestern University
Doug Downey?
Northwestern University
Yi Yang?
Northwestern University
Yuhong Guo?
Temple University
Alexander Yates?
Temple University
Finding the right representations for words is critical for building accurate NLP systems when
domain-specific labeled data for the task is scarce. This article investigates novel techniques for
extracting features from n-gram models, Hidden Markov Models, and other statistical language
models, including a novel Partial Lattice Markov Random Field model. Experiments on part-
of-speech tagging and information extraction, among other tasks, indicate that features taken
from statistical language models, in combination with more traditional features, outperform
traditional representations alone, and that graphical model representations outperform n-gram
models, especially on sparse and polysemous words.
? 1805 N. Broad St., Wachman Hall 324, Philadelphia, PA 19122, USA.
E-mail: {fei.huang,yuhong,yates}@temple.edu.
?? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: ahuja@eecs.northwestern.edu.
? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: ddowney@eecs.northwestern.edu.
? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: yya518@eecs.northwestern.edu.
Submission received: 13 June 2012; revised submission received: 25 November 2012, accepted for publication:
15 January 2013.
doi:10.1162/COLI a 00167
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
1. Introduction
NLP systems often rely on hand-crafted, carefully engineered sets of features to achieve
strong performance. Thus, a part-of-speech (POS) tagger would traditionally use a
feature like, ?the previous token is the? to help classify a given token as a noun
or adjective. For supervised NLP tasks with sufficient domain-specific training data,
these traditional features yield state-of-the-art results. However, NLP systems are in-
creasingly being applied to the Web, scientific domains, personal communications like
e-mails and tweets, among many other kinds of linguistic communication. These texts
have very different characteristics from traditional training corpora in NLP. Evidence
from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing
(Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan,
Ward, and Martin 2007), among other NLP tasks (Daume? III and Marcu 2006; Chelba
and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer,
Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades
significantly when tested on domains different from those used for training. Collecting
labeled training data for each new target domain is typically prohibitively expensive.
In this article, we investigate representations that can be applied to weakly supervised
learning, that is, learning when domain-specific labeled training data are scarce.
A growing body of theoretical and empirical evidence suggests that traditional,
manually crafted features for a variety of NLP tasks limit systems? performance in this
weakly supervised learning for two reasons. First, feature sparsity prevents systems
from generalizing accurately, because many words and features are not observed in
training. Also because word frequencies are Zipf-distributed, this often means that there
is little relevant training data for a substantial fraction of parameters (Bikel 2004b), espe-
cially in new domains (Huang and Yates 2009). For example, word-type features form
the backbone of most POS-tagging systems, but types like ?gene? and ?pathway? show
up frequently in biomedical literature, and rarely in newswire text. Thus, a classifier
trained on newswire data and tested on biomedical data will have seen few training
examples related to sentences with features ?gene? and ?pathway? (Blitzer, McDonald,
and Pereira 2006; Ben-David et al. 2010).
Further, because words are polysemous, word-type features prevent systems from
generalizing to situations in which words have different meanings. For instance, the
word type ?signaling? appears primarily as a present participle (VBG) in Wall Street
Journal (WSJ) text, as in, ?Interest rates rose, signaling that . . . ? (Marcus, Marcinkiewicz,
and Santorini 1993). In biomedical text, however, ?signaling? appears primarily in the
phrase ?signaling pathway,? where it is considered a noun (NN) (PennBioIE 2005); this
phrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates 2010).
Our response to the sparsity and polysemy challenges with traditional NLP repre-
sentations is to seek new representations that allow systems to generalize to previously
unseen examples. That is, we seek representations that permit classifiers to have close
to the same accuracy on examples from other domains as they do on the domain of the
training data. Our approach depends on the well-known distributional hypothesis,
which states that a word?s meaning is identified with the contexts in which it appears
(Harris 1954; Hindle 1990). Our goal is to develop probabilistic statistical language
models that describe the contexts of individual words accurately. We then construct
representations, or mappings from word tokens and types to real-valued vectors,
from statistical language models. Because statistical language models are designed to
model words? contexts, the features they produce can be used to combat problems
with polysemy. And by careful design of the statistical language models, we can limit
86
Huang et al. Computational Linguistics
the number of features that they produce, controlling how sparse those features are in
training data.
Our specific contributions are as follows:
1. We show how to generate representations from a variety of language
models, including n-gram models, Brown clusters, and Hidden Markov
Models (HMMs). We also introduce a Partial-Lattice Markov Random
Field (PL-MRF), which is a tractable variation of a Factorial Hidden
Markov Model (Ghahramani and Jordan 1997) for language modeling,
and we show how to produce representations from it.
2. We quantify the performance of these representations in experiments
on POS tagging in a domain adaptation setting, and weakly supervised
information extraction (IE). We show that the graphical models outperform
n-gram representations, even when the n-gram models leverage larger
corpora for training. The PL-MRF representation achieves a state-of-the-art
93.8% accuracy on a biomedical POS tagging task, which represents a
5.5 percentage point absolute improvement over more traditional POS
tagging representations, a 4.8 percentage point improvement over a tagger
using an n-gram representation, and a 0.7 percentage point improvement
over a tagger with an n-gram representation using several orders of
magnitude more training data. The HMM representation improves
over the n-gram model by 7 percentage points on our IE task.
3. We analyze how sparsity, polysemy, and differences between domains
affects the performance of a classifier using different representations.
Results indicate that statistical language model representations, and
especially graphical model representations, provide the best features
for sparse and polysemous words.
The next section describes background material and related work on representation
learning for NLP. Section 3 presents novel representations based on statistical language
models. Sections 4 and 5 discuss evaluations of the representations, first on sequence-
labeling tasks in a domain adaptation setting, and second on a weakly supervised set-
expansion task. Section 6 concludes and outlines directions for future work.
2. Background and Previous Work on Representation Learning
2.1 Terminology and Notation
In a traditional machine learning task, the goal is to make predictions on test data using
a hypothesis that is optimized on labeled training data. In order to do so, practitioners
predefine a set of features and try to estimate classifier parameters from the observed
features in the training data. We call these feature sets representations of the data.
Formally, let X be an instance space for a learning problem. Let Z be the space of
possible labels for an instance, and let f : X ? Z be the target function to be learned.
A representation is a function R: X ? Y , for some suitable feature space Y (such as Rd).
We refer to dimensions of Y as features, and for an instance x ? X we refer to values
for particular dimensions of R(x) as features of x. Given a set of training examples, a
learning machine?s task is to select a hypothesis h from the hypothesis space H, a subset
of ZR(X ). Errors by the hypothesis are measured using a loss function L(x, R, f, h) that
87
Computational Linguistics Volume 40, Number 1
measures the cost of the mismatch between the target function f (x) and the hypothesis
h(R(x)).
As an example, the instance set for POS tagging in English is the set of all English
sentences, and Z is the space of POS sequences containing labels like NN (for noun) and
VBG (for present participle). The target function f is the mapping between sentences
and their correct POS labels. A traditional representation in NLP converts sentences
into sequences of vectors, one for each word position. Each vector contains values for
features like, ?+1 if the word at this position ends with -tion, and 0 otherwise.? A
typical loss function would count the number of words that are tagged differently by
f (x) and h(R(x)).
2.2 Representation-Learning Problem Formulation
Machine learning theory assumes that there is a distribution D over X from which
data is sampled. Given a training set S = {(x1, z1), . . . , (xN, zN )} ? (D(X ),Z )N, a fixed
representation R, a hypothesis space H, and a loss function L, a machine learning
algorithm seeks to identify the hypothesis in H that will minimize the expected loss
over samples from distribution D:
h? = argmin
h?H
Ex?D(X )L(x, R, f, h) (1)
The representation-learning paradigm breaks the traditional notion of a fixed rep-
resentation R. Instead, we allow a space of possible representations R. The full learning
problem can then be formulated as the task of identifying the best R ? R and h ? H
simultaneously:
R?, h? = argmin
R?R,h?H
Ex?D(X )L(x, R, f, h) (2)
The representation-learning problem formulation in Equation (2) can in fact be
reduced to the general learning formulation in Equation (1) by setting the fixed rep-
resentation R to be the identity function, and setting the hypothesis space to be R?H
from the representation-learning task. We introduce the new formulation primarily as
a way of changing the perspective on the learning task: most NLP systems consider
a fixed, manually crafted transformation of the original data to some new space, and
investigate hypothesis classes over that space. In the new formulation, systems learn
the transformation to the feature space, and then apply traditional classification or
regression algorithms.
2.3 Theory on Domain Adaptation
We refer to the distribution D over the instance space X as a domain. For example,
the newswire domain is a distribution over sentences that gives high probability to
sentences about governments and current events; the biomedical literature domain
gives high probability to sentences about proteins and regulatory pathways. In domain
adaptation, a system observes a set of training examples (R(x), f (x)), where instances
x ? X are drawn from a source domain DS, to learn a hypothesis for classifying ex-
amples drawn from a separate target domain DT. We assume that large quantities of
unlabeled data are available for the source domain and target domain, and call these
88
Huang et al. Computational Linguistics
samples US and UT, respectively. For any domain D, let R(D) represent the induced
distribution over the feature space Y given by PrR(D)[y] = PrD[{x such that R(x) = y}].
Previous work by Ben-David et al. (2007, 2010) proves theoretical bounds on an
open-domain learning machine?s performance. Their analysis shows that the choice of
representation is crucial to domain adaptation. A good choice of representation must
allow a learning machine to achieve low error rates on the source domain. Just as
important, however, is that the representation must simultaneously make the source
and target domains look as similar to one another as possible. That is, if the labeling
function f is the same on the source and target domains, then for every h ? H, we can
provably bound the error of h on the target domain by its error on the source domain
plus a measure of the distance between DS and DT:
Ex?DTL(x, R, f, h) ? Ex?DSL(x, R, f, h) + d1(R(DS), R(DT )) (3)
where the variation divergence d1 is given by
d1(D,D?) = 2 sup
B?B
|PrD[B] ? PrD? [B]| (4)
where B is the set of measurable sets under D and D? (Ben-David et al. 2007, 2010).
Crucially, the distance between domains depends on the features in the representa-
tion. The more that features appear with different frequencies in different domains, the
worse this bound becomes. In fact, one lower bound for the d1 distance is the accuracy
of the best classifier for predicting whether an unlabeled instance y = R(x) belongs to
domain S or T (Ben-David et al. 2010). Thus, if R provides one set of common features for
examples from S, and another set of common features for examples from T, the domain
of an instance becomes easy to predict, meaning the distance between the domains
grows, and the bound on our classifier?s performance grows worse.
In light of Ben-David et al.?s theoretical findings, traditional representations in
NLP are inadequate for domain adaptation because they contribute to the d1 distance
between domains. Although many previous studies have shown that lexical features
allow learning systems to achieve impressively low error rates during training, they also
make texts from different domains look very dissimilar. For instance, a feature based on
the word ?bank? or ?CEO? may be common in a domain of newswire text, but scarce
or nonexistent in, say, biomedical literature. Ben David et al.?s theory predicts greater
variance in the error rate of the target domain classifier as the distance grows.
At the same time, traditional representations contribute to data sparsity, a lack of
sufficient training data for the relevant parameters of the system. In traditional super-
vised NLP systems, there are parameters for each word type in the data, or perhaps
even combinations of word types. Because vocabularies can be extremely large, this
leads to an explosion in the number of parameters. As a consequence, for many of their
parameters, supervised NLP systems have zero or only a handful of relevant labeled
examples (Bikel 2004a, 2004b). No matter how sophisticated the learning technique, it
is difficult to estimate parameters without relevant data. Because vocabularies differ
across domains, domain adaptation greatly exacerbates this issue of data sparsity.
2.4 Problem Formulation for the Domain Adaptation Setting
Formally, we define the task of representation learning for domain adaptation as the
following optimization problem: Given a set of unlabeled instances US drawn from the
89
Computational Linguistics Volume 40, Number 1
source domain and unlabeled instances UT from the target domain, as well as a set of
labeled instances LS drawn from the source domain, identify a function R? from the
space of possible representations R that minimizes
R?, h? = argmin
R?R,h?H
(
Ex?DSL(x, R, f, h)
)
+ ?d1(R(DS), R(DT )) (5)
where ? is a free parameter.
Note that there is an underlying tension between the two terms of the objec-
tive function: The best representation for the source domain would naturally include
domain-specific features, and allow a hypothesis to learn domain-specific patterns.
We are aiming, however, for the best general classifier, which happens to be trained
on training data from one domain (or a few domains). The domain-specific features
contribute to distance between domains, and to classifier errors on data taken from
domains not seen in training. By optimizing for this combined objective function, we
allow the optimization method to trade off between features that are best for classifying
source-domain data and features that allow generalization to new domains.
Unlike the representation-learning problem-formulation in Equation (2), Equa-
tion (5) does not reduce to the standard machine-learning problem (Equation (1)). In
a sense, the d1 term acts as a regularizer on R, which also affects H. Representation
learning for domain adaptation is a fundamentally novel learning task.
2.5 Tractable Representation Learning: Statistical Language Models
as Representations
For most hypothesis classes and any interesting space of representations, Equations (2)
and (5) are completely intractable to optimize exactly. Even given a fixed representation,
it is intractable to compute the best hypothesis for many hypothesis classes. And the d1
metric is intractable to compute from samples of a distribution, although Ben-David
et al. (2007, 2010) propose some tractable bounds. We view these problem formulations
as high-level goals rather than as computable objectives.
As a tractable objective, in this work we describe an investigation into the use of
statistical language models as a way to represent the meanings of words. This approach
depends on the well-known distributional hypothesis, which states that a word?s
meaning is identified with the contexts in which it appears (Harris 1954; Hindle 1990).
From this hypothesis, we can formulate the following testable prediction, which we call
the statistical language model representation hypothesis, or LMRH:
To the extent that a model accurately describes a word?s possible contexts, parameters
of that model are highly informative descriptors of the word?s meaning, and are
therefore useful as features in NLP tasks like POS tagging, chunking, NER, and
information extraction.
The LMRH says, essentially, that for NLP tasks, we can decouple the task of optimiz-
ing a representation from the task of optimizing a hypothesis. To learn a representation,
we can train a statistical language model on unlabeled text, and then use parameters
or latent states from the statistical language model to create a representation function.
Optimizing a hypothesis then follows the standard learning framework, using the
representation from the statistical language model.
90
Huang et al. Computational Linguistics
The LMRH is similar to the manifold and cluster assumptions behind other semi-
supervised approaches to machine learning, such as Alternating Structure Optimization
(ASO) (Ando and Zhang 2005) and Structural Correspondence Learning (SCL) (Blitzer,
McDonald, and Pereira 2006). All three of these techniques use predictors built on
unlabeled data as a way to harness the manifold and cluster assumptions. However,
the LMRH is distinct from at least ASO and SCL in important ways. Both ASO and SCL
create multiple ?synthetic? or ?pivot? prediction tasks using unlabeled data, and find
transformations of the input feature space that perform well on these tasks. The LMRH,
on the other hand, is more specific ? it asserts that for language problems, if we opti-
mize word representations on a single task (the language modeling task), this will lead
to strong performance on weakly supervised tasks. In reported experiments on NLP
tasks, both ASO and SCL use certain synthetic predictors that are essentially language
modeling tasks, such as the task of predicting whether the next token is of word type w.
To the extent that these techniques? performance relies on language-modeling tasks as
their ?synthetic predictors,? they can be viewed as evidence in support of the LMRH.
One significant consequence of the LMRH is that it allows us to leverage well-
developed techniques and models from statistical language modeling. Section 3
presents a series of statistical language models that we investigate for learning repre-
sentations for NLP.
2.6 Previous Work
There is a long tradition of NLP research on representations, mostly falling into one of
four categories: 1) vector space models of meaning based on document-level lexical co-
occurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010);
2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990;
Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and
Jordan 2003; Va?yrynen and Honkela 2004, 2005; Va?yrynen, Honkela, and Lindqvist
2007); 3) using clusters that are induced from distributional similarity (Brown et al.
1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse
features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu
2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao
et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008;
Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih
and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert
and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering
for representations, but where previous work has used bigram and trigram statistics to
form clusters, we build sophisticated models that attempt to capture the context of a
word, and hence its similarity to other words, more precisely. Our experiments show
that the new graphical models provide representations that outperform those from
previous work on several tasks.
Neural network statistical language models have recently achieved state-of-the-art
perplexity results (Mnih and Hinton 2009), and representations based on them have im-
proved in-domain chunking, NER, and SRL (Weston, Ratle, and Collobert 2008; Turian,
Bergstra, and Bengio 2009; Turian, Ratinov, and Bengio 2010). As far as we are aware,
Turian, Ratinov, and Bengio (2010) is the only other work to test a learned representation
on a domain adaptation task, and they show improvement on out-of-domain NER
with their neural net representations. Though promising, the neural network models
are computationally expensive to train, and these statistical language models work
only on fixed-length histories (n-grams) rather than full observation sequences. Turian,
91
Computational Linguistics Volume 40, Number 1
Ratinov, and Bengio?s (2010) tests also show that Brown clusters perform as well or
better than neural net models on all of their chunking and NER tests. We concentrate on
probabilistic graphical models with discrete latent states instead. We show that HMM-
based and other representations significantly outperform the more commonly used
Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings
of sequence-labeling tasks.
Most previous work on domain adaptation has focused on the case where some
labeled data are available in both the source and target domains (Chan and Ng 2006;
Daume? III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daume? III 2007; Jiang
and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze,
Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are
known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this
problem setting have focused on appropriately weighting examples from the source and
target domains so that the learning algorithm can balance the greater relevance of the
target-domain data with the larger source-domain data set. In some cases, researchers
combine this approach with semi-supervised learning to include unlabeled examples
from the target domain as well (Daume? III, Kumar, and Saha 2010). These techniques
do not handle open-domain corpora like the Web, where they require expert input to
acquire labels for each new single-domain corpus, and it is difficult to come up with
a representative set of labeled training data for each domain. Our technique requires
only unlabeled data from each new domain, which is significantly easier and cheaper to
acquire. Where target-domain labeled data is available, however, these techniques can
in principle be combined with ours to improve performance, although this has not yet
been demonstrated empirically.
A few researchers have considered the more general case of domain adaptation
without labeled data in the target domain. Perhaps the best known is Blitzer, McDonald,
and Pereira?s (2006) Structural Correspondence Learning (SCL). SCL uses ?pivot? words
common to both source and target domains, and trains linear classifiers to predict these
pivot words from their context. After an SVD reduction of the weight vectors for these
linear classifiers, SCL projects the original features through these weight vectors to
obtain new features that are added to the original feature space. Like SCL, our language
modeling techniques attempt to predict words from their context, and then use the
output of these predictions as new features. Unlike SCL, we attempt to predict all words
from their context, and we rely on traditional probabilistic methods for language mod-
eling. Our best learned representations, which involve significantly different techniques
from SCL, especially latent-variable probabilistic models, significantly outperform SCL
in POS tagging experiments.
Other approaches to domain adaptation without labeled data from the target do-
main include Satpal and Sarawagi (2007), who show that by changing the optimization
function during conditional random field (CRF) training, they can learn classifiers that
port well to new domains. Their technique selects feature subsets that minimize the
distance between training text and unlabeled test text, but unlike our techniques, theirs
cannot learn representations with features that do not appear in the original feature set.
In contrast, we learn hidden features through statistical language models. McClosky,
Charniak, and Johnson (2010) use classifiers from multiple source domains and features
that describe how much a target document diverges from each source domain to deter-
mine an optimal weighting of the source-domain classifiers for parsing the target text.
However, it is unclear if this ?source-combination? technique works well on domains
that are not mixtures of the various source domains. Dai et al. (2007) use KL-divergence
between domains to directly modify the parameters of their naive Bayes model for a
92
Huang et al. Computational Linguistics
text classification task trained purely on the source domain. These last two techniques
are not representation learning, and are complementary to our techniques.
Our representation-learning approach to domain adaptation is an instance of
semi-supervised learning. Of the vast number of semi-supervised approaches to
sequence labeling in NLP, the most relevant ones here include Suzuki and Isozaki?s
(2008) combination of HMMs and CRFs that uses over a billion words of unlabeled text
to achieve the current best performance on in-domain chunking, and semi-supervised
approaches to improving in-domain SRL with large quantities of unlabeled text
(Weston, Ratle, and Collobert 2008; Deschacht and Moens 2009; and Fu?rstenau and
Lapata 2009). Ando and Zhang?s (2005) semi-supervised sequence labeling technique
has been tested on a domain adaptation task for POS tagging (Blitzer, McDonald, and
Pereira 2006); our representation-learning approaches outperform it. Unlike most semi-
supervised techniques, we concentrate on a particularly simple task decomposition: un-
supervised learning for new representations, followed by standard supervised learning.
In addition to our task decomposition being simple, our learned representations are also
task-independent, so we can learn the representation once, and then apply it to any task.
One of the best-performing representations that we consider for domain adaptation
is based on the HMM (Rabiner 1989). HMMs have of course also been used for super-
vised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and
Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised
POS tagging have focused on incorporating prior knowledge into the POS induction
model (Grac?a et al. 2009; Toutanova and Johnson 2007), or on new training techniques
like contrastive estimation (Smith and Eisner 2005) for alternative sequence models.
Despite the fact that completely connected, standard HMMs perform poorly at the POS
induction task (Johnson 2007), we show that they still provide very useful features
for a supervised POS tagger. Experiments in information extraction have previously
also shown that HMMs provide informative features for this quite different, semantic
processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010).
This article extends our previous work on learning representations for do-
main adaptation (Huang and Yates 2009, 2010) by investigating new language
representations?the naive Bayes representation and PL-MRF representation (Huang
et al. 2011)?by analyzing results in terms of polysemy, sparsity, and domain diver-
gence; by testing on new data sets including a Chinese POS tagging task; and by pro-
viding an empirical comparison with Brown clusters as representations.
3. Learning Representations of Distributional Similarity
In this section, we will introduce several representation learning models.
3.1 Traditional POS-Tagging Representations
As an example of our terminology, we begin by describing a representation used in
traditional POS taggers (this representation will later form a baseline for our POS
tagging experiments). The instance set X is the set of English sentences, and Z is the set
of POS tag sequences. A traditional representation TRAD-R maps a sentence x ? X to a
sequence of boolean-valued vectors, one vector per word xi in the sentence. Dimensions
for each latent vector include indicators for the word type of xi and various orthographic
features. Table 1 presents the full list of features in TRAD-R. Because our IE task classifies
word types rather than tokens, this baseline is not appropriate for that task. Herein, we
93
Computational Linguistics Volume 40, Number 1
Table 1
Summary of features provided by our representations. ?a1[g(a)] represents a set of boolean
features, one for each value of a, where the feature is true iff g(a) is true. xi represents a token at
position i in sentence x, w represents a word type, Suffixes = {-ing,-ogy,-ed,-s,-ly,-ion,-tion,-ity},
k (and k) represents a value for a latent state (set of latent states) in a latent-variable model, y?
represents the maximum a posteriori sequence of states y for x, yi is the latent variable for xi, and
yi,j is the latent variable for xi at layer j. prefix(y,p) is the p-length prefix of the Brown cluster y.
Representation Features
TRAD-R ?w1[xi = w]
?s?Suffixes1[xi ends with s]
1[xi contains a digit]
n-GRAM-R ?w? ,w??P(w?ww??)/P(w)
LSA-R ?w,j{v?left(w)}j
?w,j{v?right(w)}j
NB-R ?k1[y?i = k]
HMM-TOKEN-R ?k1[y?i = k]
HMM-TYPE-R ?kP(y = k|x = w)
I-HMM-TOKEN-R ?j,k1[y?i,j = k]
I-HMM-TYPE-R ?j,kP(y.,j = k|x = w)
BROWN-TOKEN-R ?j?{?2,?1,0,1,2}
?p?{4,6,10,20} prefix(yi+j, p)
BROWN-TYPE-R ?p prefix(y, p)
LATTICE-TOKEN-R ?j,k1[y?i,j = k]
LATTICE-TYPE-R ?kP(y = k|x = w)
describe how we can learn representations R by using a variety of statistical language
models, for use in both our IE and POS tagging tasks. All representations for POS
tagging inherit the features from TRAD-R; all representations for IE do not.
3.2 n-gram Representations
n-gram representations, which we call n-GRAM-R, model a word type w in terms of the
n-gram contexts in which w appears in a corpus. Specifically, for word w we generate
the vector P(w?ww??)/P(w), the conditional probability of observing the word sequence
w? to the left and w?? to the right of w. Each dimension in this vector represents a com-
bination of the left and right words. The experimental section describes the particular
corpora and statistical language modeling methods used for estimating probabilities.
Note that these features depend only on the word type w, and so for every token xi = w,
n-GRAM-R provides the same set of features regardless of local context.
One drawback of n-GRAM-R is that it does not handle sparsity well?the features
are as sparsely observed as the lexical features in TRAD-R, except that n-GRAM-R fea-
tures can be obtained from larger corpora. As an alternative, we apply latent semantic
analysis (LSA) (Deerwester et al. 1990) to compute a reduced-rank representation. For
word w, let vright(w) represent the right context vector of w, which in each dimension
contains the value of P(ww??)/P(w) for some word w??, as observed in the n-gram
model. Similarly, let vleft(w) be the left context vector of w. We apply LSA to the set
94
Huang et al. Computational Linguistics
   
 


	 	 	 	
 	


 
Figure 1
A graphical representation of the naive Bayes statistical language model. The B and E are special
dummy words for the beginning and end of the sentence.
of right context vectors and the set of left context vectors separately,1 to find reduced-
rank versions v?right(w) and v
?
left(w), where each dimension represents a combination
of several context word types. We then use each component of v?right(w) and v
?
left(w)
as features. After experimenting with different choices for the number of dimensions to
reduce our vectors to, we choose a value of 10 dimensions as the one that maximizes
the performance of our supervised sequence labelers on held-out data. We call this
model LSA-R.
3.3 A Context-Dependent Representation Using Naive Bayes
The n-GRAM-R and LSA-R representations always produce the same features F for a
given word type w, regardless of the local context of a particular token xi = w. Our
remaining representations are all context-dependent, in the sense that the features
provided for token xi depend on the local context around xi. We begin with a statis-
tical language model based on the Naive Bayes model with categorical latent states
S = {1, . . . , K}. First, we form trigrams from our sentences. For each trigram, we form a
separate Bayes net in which each token from the trigram is conditionally independent
given the latent state. For tokens xi?1, xi, and xi+1, the probability of this trigram given
latent state Yi = y is given by:
P(xi?1, xi, xi+1|yi) = Pleft(xi?1|yi)Pmid(xi|yi)Pright(xi+1|yi) (6)
where Pleft, Pmid, and Pright are multinomial distributions conditioned on the latent state.
The probability of a whole sentence is then given by the product of the probabilities
of its trigrams. Figure 1 shows a graphical representation of this model. We train our
models using standard expectation-maximization (Dempster, Laird, and Rubin 1977)
with random initialization of the parameters.
Because our factorization of the sentence does not take into account the fact that the
trigrams overlap, the resulting statistical language model is mass-deficient. Worse still,
it is throwing away information from the dependencies among trigrams which might
help make better clustering decisions. Nevertheless, this model closely mirrors many
of the clustering algorithms used in previous approaches to representation learning for
sequence labeling (Ushioda 1996; Miller, Guinness, and Zamanian 2004; Koo, Carreras,
1 Compare with Dhillon, Foster, and Ungar (2011), who use canonical correlation analysis to find a
simultaneous reduction of the left and right context vectors, a significantly more complex undertaking.
95
Computational Linguistics Volume 40, Number 1
and Collins 2008; Lin and Wu 2009; Ratinov and Roth 2009), and therefore serves as an
important benchmark.
Given a naive Bayes statistical language model, we construct an NB-R representa-
tion that produces |S| boolean features Fs(xi) for each token xi and each possible latent
state s ? S:
Fs(xi) =
{
true if s = arg maxs??SP(xi?1, xi, xi+1|yi = s?),
false otherwise.
For a reasonable choice of S (i.e., |S|  |V|), each feature should be observed often
in a sufficiently large training data set. Therefore, compared with n-GRAM-R, NB-R
produces far fewer features. On the other hand, its features for xi depend not just on
the contexts in which xi has appeared in the statistical language model?s training data,
but also on xi?1 and xi+1 in the current sentence. Furthermore, because the range of
the features is much more restrictive than real-valued features, it is less prone to data
sparsity or variations across domains than real-valued features.
3.4 Context-Dependent, Structured Representations: The Hidden Markov Model
In previous work, we have implemented several representations based on hidden
Markov models (Rabiner 1989), which we used for both sequential labeling (like POS
tagging [Huang et al. 2011] and NP chunking [Huang and Yates 2009]) and IE (Downey,
Schoenmackers, and Etzioni 2007). Figure 2 shows a graphical model of an HMM. An
HMM is a generative probabilistic model that generates each word xi in the corpus
conditioned on a latent variable yi. Each yi in the model takes on integral values from 1
to K, and each one is generated by the latent variable for the preceding word, yi?1. The
joint distribution for a corpus x = (x1, . . . , xN ) and a set of state vectors y = (y1, . . . , yN )
is given by: P(x, y) =
?
i P(xi|yi)P(yi|yi?1). Using expectation-maximization (EM)
(Dempster, Laird, and Rubin 1977), it is possible to estimate the distributions for
P(xi|yi) and P(yi|yi?1) from unlabeled data.
We construct two different representations from HMMs, one for sequence-labeling
tasks and one for IE. For sequence labeling, we use the Viterbi algorithm to produce the
optimal setting y? of the latent states for a given sentence x, or y? = argmax
y
P(x, y). We
use the value of y?i as a new feature for xi that represents a cluster of distributionally
similar words. For IE, we require features for word types w, rather than tokens xi.
Applying Bayes? rule to the HMM parameters, we compute a distribution P(Y|x = w),
where Y is a single latent node, x is a single token, and w is its word type. We then use
each of the K values for P(Y = k|x = w), where k ranges from 1 to K, as features. This set
   
 

	 	 	 	
 	

Figure 2
The Hidden Markov Model.
96
Huang et al. Computational Linguistics
of features represents a ?soft clustering? of w into K different clusters. We refer to these
representations as HMM-TOKEN-R and HMM-TYPE-R, respectively.
We also compare against a multi-layer variation of the HMM from our previous
work (Huang and Yates 2010). This model trains an ensemble of M independent HMM
models on the same corpus, initializing each one randomly. We can then use the Viterbi-
optimal decoded latent state of each independent HMM model as a separate feature for
a token, or the posterior distribution for P(Y|x = w) from each HMM as a separate set
of features for each word type. We refer to this statistical language model as an I-HMM,
and the representations as I-HMM-TOKEN-R and I-HMM-TYPE-R, respectively.
Finally, we compare against Brown clusters (Brown et al. 1992) as learned features.
Although not traditionally described as such, Brown clustering involves constructing
an HMM model in which each word type is restricted to having exactly one latent state
that may generate it. Brown et al. describe a greedy agglomerative clustering algorithm
for training this model on unlabeled text. Following Turian, Ratinov, and Bengio (2010),
we use Percy Liang?s implementation of this algorithm for our comparison, and we test
runs with 100, 320, 1,000 and 3,200 clusters. We use features from these clusters identical
to Turian et al.?s.2 Turian et al. have shown that Brown clusters match or exceed the
performance of neural network-based statistical language models in domain adaptation
experiments for named-entity recognition, as well as in-domain experiments for NER
and chunking.
Because HMM-based representations offer a small number of discrete states as
features, they have a much greater potential to combat sparsity than do n-gram mod-
els. Furthermore, for token-based representations, these models can potentially handle
polysemy better than n-gram statistical language models by providing different features
in different contexts.
3.5 A Novel Lattice Statistical Language Model Representation
Our final statistical language model is a novel latent-variable statistical language model,
called a Partial Lattice MRF (PL-MRF), with rich latent structure, shown in Figure 3. The
model contains a lattice of M ? N latent states, where N is the number of words in a
sentence and M is the number of layers in the model. The dotted and solid lines in the
figure together form a complete lattice of edges between these nodes; the PL-MRF uses
only the solid edges. Formally, let c = 	N2 
, where N is the length of the sentence; let i
denote a position in the sentence, and let j denote a layer in the lattice. If i < c and j is
odd, or if j is even and i > c, we delete edges between yi,j and yi,j+1 from the complete
lattice. The same set of nodes remains, but the partial lattice contains fewer edges and
paths between the nodes. A central ?trunk? at i = c connects all layers of the lattice, and
branches from this trunk connect either to the branches in the layer above or the layer
below (but not both).
The result is a model that retains most of the edges of the complete lattice, but
unlike the complete lattice, it supports tractable inference. As M, N ? ?, five out of
every six edges from the complete lattice appear in the PL-MRF. However, the PL-MRF
makes the branches conditionally independent from one another, except through the
trunk. For instance, the left branch between layers 1 and 2 ((y1,1, y1,2) and (y2,1, y2,2)) in
Figure 3 are disconnected; similarly, the right branch between layers 2 and 3 ((y4,2, y4,3)
and (y5,2, y5,3)) are disconnected, except through the trunk and the observed nodes. As
2 Percy Liang?s implementation is available at http://metaoptimize.com/projects/wordreprs/.
97
Computational Linguistics Volume 40, Number 1
y4,1
y3,1
y4,2
y3,2
y4,3
y3,3
y4,4
y3,4
y4,5
y3,5
x1
y2,1
y1,1
x2
y2,2
y1,2
x3
y2,3
y1,3
x4
y2,4
y1,4
x5
y2,5
y1,5
Figure 3
The PL-MRF model for a five-word sentence and a four-layer lattice. Dashed gray edges are part
of a complete lattice, but not part of the PL-MRF.
a result, excluding the observed nodes, this model has a low tree-width of 2 (excluding
observed nodes), and a variety of efficient dynamic programming and message-passing
algorithms for training and inference can be readily applied (Bodlaender 1988). Our
inference algorithm passes information from the branches inwards to the trunk, and
then upward along the trunk, in time O(K4MN). In contrast, a fully connected lattice
model has tree-width = min(M, N), making inference and learning intractable (Sutton,
McCallum, and Rohanimanesh 2007), partly because of the difficulty in enumerating
and summing over the exponentially-many configurations y for a given x.
We can justify the choice of this model from a linguistic perspective as a way to
capture the multi-dimensional nature of words. Linguists have long argued that words
have many different features in a high dimensional space: They can be separately
described by part of speech, gender, number, case, person, tense, voice, aspect, mass
vs. count, and a host of semantic categories (agency, animate vs. inanimate, physical vs.
abstract, etc.), to name a few (Sag, Wasow, and Bender 2003). In the PL-MRF, each layer
of nodes is intended to represent some latent dimension of words.
We represent the probability distribution for PL-MRFs as log-linear models that
decompose over cliques in the MRF graph. Let Cliq(x, y) represent the set of all maximal
cliques in the graph of the MRF model for x and y. Expressing the lattice model in log-
linear form, we can write the marginal probability P(x) of a given sentence x as:
?
y
?
c?Cliq(x,y) score(c, x, y)
?
x?,y?
?
c?Cliq(x?,y? ) score(c, x
?, y?)
where score(c, x, y) = exp(?c ? fc(xc, yc)). Our model includes parameters for transitions
between two adjacent latent variables on layer j: ?transi,s,i+1,s?,j for yi,j = s and yi+1,j = s
?. It
also includes observation parameters for latent variables and tokens, as well as for pairs
of adjacent latent variables in different layers and their tokens: ?obsi,j,s,w and ?
obs
i,j,s,j+1,s?,w for
yi,j = s, yi,j+1 = s?, and xi = w.
98
Huang et al. Computational Linguistics
As with our HMM models, we create two representations from PL-MRFs, one for
tokens and one for types. For tokens, we decode the model to compute y?, the matrix of
optimal latent state values for sentence x. For each layer j and and each possible latent
state value k, we add a boolean feature for token xi that is true iff y?i,j = k. For word
types, we compute distributions over the latent state space. Let y be a column vector of
latent variables for word type w. For a PL-MRF model with M layers of binary variables,
there are 2M possible values for y. Our type representation computes a probability
distribution over these 2M possible values, and uses each probability as a feature for
w.3 We refer to these two representations as LATTICE-TOKEN-R and LATTICE-TYPE-R,
respectively.
We train the PL-MRF using contrastive estimation (Smith and Eisner 2005), which
iteratively optimizes the following objective function on a corpus X:
?
x?X
log
?
y
?
c?Cliq(x,y) score(c, x, y)
?
x??N (x),y?
?
c?Cliq(x?,y? ) score(c, x
?, y?)
(7)
where N (x), the neighborhood of x, indicates a set of perturbed variations of the original
sentence x. Contrastive estimation seeks to move probability mass away from the per-
turbed neighborhood sentences and onto the original sentence. We use a neighborhood
function that includes all sentences which can be obtained from the original sentence by
swapping the order of a consecutive pair of words. Training uses gradient descent over
this non-convex objective function with a standard software package (Liu and Nocedal
1989) and converges to a local maximum or saddle point.
For tractability, we modify the training procedure to train the PL-MRF one layer
at a time. Let ?i represent the set of parameters relating to features of layer i, and let
??i represent all other parameters. We fix ??0 = 0, and optimize ?0 using contrastive
estimation. After convergence, we fix ??1, and optimize ?1, and so on. For training each
layer, we use a convergence threshold of 10?6 on the objective function in Equation (7),
and each layer typically converges in under 100 iterations.
4. Domain Adaptation with Learned Representations
We evaluate the representations described earlier on POS tagging and NP chunking
tasks in a domain adaptation setting.
4.1 A Rich Problem Setting for Representation Learning
Existing supervised NLP systems are domain-dependent: There is a substantial drop in
their performance when tested on data from a new domain. Domain adaptation is the
task of overcoming this domain dependence. The aim is to build an accurate system for
3 This representation is only feasible for small numbers of layers, and in our experiments that require type
representations, we used M = 10. For larger values of M, other representations are also possible. We also
experimented with a representation which included only M possible values: For each layer l, we included
P(yl = 0|w) as a feature. We used the less-compact representation in our experiments because results
were better.
99
Computational Linguistics Volume 40, Number 1
a target domain by training on labeled examples from a separate source domain. This
problem is sometimes also called transfer learning (Raina et al. 2007).
Two of the challenges for NLP representations, sparsity and polysemy, are exacer-
bated by domain adaptation. New domains come with new words and phrases that
appear rarely (or even not at all) in the training domain, thus increasing problems
with data sparsity. And even for words that do appear commonly in both domains, the
contexts around the words will change from the training domain to the target domain.
As a result, domain adaptation adds to the challenge of handling polysemous words,
whose meaning depends on context.
In short, domain adaptation is a challenging setting for testing NLP representations.
We now present several experiments testing our representations against state-of-the-
art POS taggers in a variety of domain adaptation settings, showing that the learned
representations surpass the previous state-of-the-art, without requiring any labeled data
from the target domain.
4.2 Experimental Set-up
For domain adaptation, we test our representations on two sequence labeling tasks:
POS tagging and chunking. To incorporate learned representation into our models, we
follow this general procedure, although the details vary by experiment and are given in
the following sections. First, we collect a set of unannotated text from both the training
domain and test domain. Second, we learn representations on the unannotated text.
We then automatically annotate both the training and test data with features from the
learned representation. Finally, we train a supervised linear-chain CRF model on the
annotated training set and apply it to the test set.
A linear-chain CRF is a Markov random field (Darroch, Lauritzen, and Speed 1980)
in which the latent variables form a path with edges only between consecutive nodes in
the path, and all latent variables are globally conditioned on the observations. Let X be a
random variable over data sequences, and Z be a random variable over corresponding
label sequences. The conditional distribution over the label sequence Z given X has the
form
p?(Z = z|X = x) ? exp
?
?
?
i
?
j
?j fj(zi?1, zi, x, i)
?
? (8)
where fj(zi?1, zi, x, i) is a real-valued feature function of the entire observation sequence
and the labels at positions i and i ? 1 in the label sequence, and ?j is a parameter to be
estimated from training data.
We use an open source CRF software package designed by Sunita Sarawagi to train
and apply our CRF models.4 As is standard, we use two kinds of feature functions:
transition and observation. Transition feature functions indicate, for each pair of labels
l and l?, whether zi = l and zi?1 = l?. Boolean observation feature functions indicate, for
each label l and each feature f provided by a representation, whether zi = l and xi has
feature f . For each label l and each real-valued feature f in representation R, real-valued
observation feature functions have value f (x) if zi = l, and are zero otherwise.
4 Available from http://sourceforge.net/projects/crf/.
100
Huang et al. Computational Linguistics
4.3 Domain Adaptation for POS Tagging
Our first experiment tests the performance of all the representations we introduced
earlier on an English POS tagging task, trained on newswire text, to tag biomedical re-
search literature. We follow Blitzer et al.?s experimental set-up. The labeled data consists
of the WSJ portion of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993)
as source domain data, and 561 labeled sentences (9,576 tokens) from the biomedical
research literature database MEDLINE as target domain data (PennBioIE 2005). Fully
23% of the tokens in the labeled test text are never seen in the WSJ training data. The
unlabeled data consists of the WSJ text plus 71,306 additional sentences of MEDLINE
text (Blitzer, McDonald, and Pereira 2006). As a preprocessing step, we replace hapax
legomena (defined as words that appear once in our unlabeled training data) with
the special symbol *UNKNOWN*, and do the same for words in the labeled test sets that
never appeared in any of our unlabeled training text.
For representations, we tested TRAD-R, n-GRAM-R, LSA-R, NB-R, HMM-TOKEN-
R, I-HMM-TOKEN-R (between 2 and 8 layers), and LATTICE-TOKEN-R (8, 12, 16,
and 20 layers). Each latent node in the I-HMMs had 80 possible values, creating
808 ? 1015 possible configurations of the eight-layer I-HMM for a single word. Each
node in our PL-MRF is binary, creating a much smaller number (220 ? 106) of possible
configurations for each word in a 20-layer representation. To give the n-gram model
the largest training data set available, we trained it on the Web 1Tgram corpus (Brants
and Franz 2006). We included the top 500 most common n-grams for each word type,
and then used mutual information on the training data to select the top 10,000 most
relevant n-gram features for all word types, in order to keep the number of features
manageable. We incorporated n-gram features as binary values indicating whether xi
appeared with the n-gram or not. For comparison, we also report on the performance of
Brown clusters (100, 320, 1,000, and 3,200 possible clusters), following Turian, Ratinov,
and Bengio (2010). Finally, we compare against Blitzer, McDonald, and Pereira (2006)
SCL technique, described in Section 2.6, and the standard semi-supervised learning
algorithm ASO (Ando and Zhang 2005), whose results on this task were previously
reported by Blitzer, McDonald, and Pereira (2006).
Table 2 shows the results for the best variation of each kind of model?20 layers for
the PL-MRF, 7 layers for the I-HMM, and 3,200 clusters for the Brown clustering. All
statistical language model representations outperform the TRAD-R baseline.
In nearly all cases, learned representations significantly outperformed TRAD-R. The
best representation, the 20-layer LATTICE-TOKEN-R, reduces error by 47% (35% on
OOV) relative to the baseline TRAD-R, and by 44% (24% on out-of-vocabulary words
(OOV)) relative to the benchmark SCL system. For comparison, this model achieved a
96.8% in-domain accuracy on Sections 22?24 of the Penn Treebank, about 0.5 percentage
point shy of a state-of-the-art in-domain system with more sophisticated supervised
learning (Shen, Satta, and Joshi 2007). The BROWN-TOKEN-R representation, which
Turian, Ratinov, and Bengio (2010) demonstrated performed as well or better than
a variety of neural network statistical language models as representations, achieved
accuracies between the SCL system and the HMM-TOKEN-R. The WEB1T-n-GRAM-R,
I-HMM-TOKEN-R, and LATTICE-TOKEN-R all performed quite close to one another,
but the I-HMM-TOKEN-R and LATTICE-TOKEN-R were trained on many orders of
magnitude less text. The LSA-R and NB-R outperformed the TRAD-R baseline but
not the SCL system. The n-GRAM-R, which was trained on the same text as the
other representations except the WEB1T-n-GRAM-R, performed far worse than the
WEB1T-n-GRAM-R.
101
Computational Linguistics Volume 40, Number 1
Table 2
Learned representations, and especially latent-variable statistical language model
representations, significantly outperform a traditional CRF system on domain adaptation for
POS tagging. Percent error is shown for all words and out-of-vocabulary (OOV) words. The
SCL+500bio system was given 500 labeled training sentences from the biomedical domain.
1.8% of tokens in the biomedical test set had POS tags like ?HYPHENATED?, which are not
part of the tagset for the training data, and were labeled incorrectly by all systems without
access to labeled data from the biomedical domain. As a result, an error rate of 1.8 + 3.9 = 5.7
serves as a reasonable lower bound for a system that has never seen labeled examples from
the biomedical domain.
Model All words OOV words
TRAD-R 11.7 32.7
n-GRAM-R 11.7 32.2
LSA-R 11.6 31.1
NB-R 11.6 30.7
ASO 11.6 29.1
SCL 11.1 28
BROWN-TOKEN-R 10.0 25.2
HMM-TOKEN-R 9.5 24.8
WEB1T-n-GRAM-R 6.9 24.4
I-HMM-TOKEN-R 6.7 24
LATTICE-TOKEN-R 6.2 21.3
SCL+500bio 3.9 ?
The amount of unlabeled training data has a significant impact on the performance
of these representations. This is apparent in the difference between WEB1T-n-GRAM-
R and n-GRAM-R, but it is also true for our other representations. Figure 4 shows the
accuracy of a representative subset of our taggers on words not seen in labeled training
data, as we vary the amount of unlabeled training data available to the language
Figure 4
Learning curve for representations: target domain accuracy of our taggers on OOV words
(not seen in labeled training data), as a function of the number of unlabeled examples given
to the language models.
102
Huang et al. Computational Linguistics
models. Performance grows steadily for all representations we measured, and none
of the learning curves appears to have peaked. Furthermore, the margin between the
more complex graphical models and the simpler n-gram models grows with increasing
amounts of training data.
4.3.1 Sparsity and Polysemy. We expected that statistical language model represen-
tations would perform well in part because they provide meaningful features for
sparse and polysemous words. For sparse tokens, these trends are already evident
in the results in Table 2: Models that provide a constrained number of features, like
HMM-based models, tend to outperform models that provide huge numbers of fea-
tures (each of which, on average, is only sparsely observed in training data), like
TRAD-R.
As for polysemy, HMM models significantly outperform naive Bayes models and
the n-GRAM-R. The n-GRAM-R?s features do not depend on a token type?s context at all,
and the NB-R?s features depend only on the tokens immediately to the right and left of
the current word. In contrast, the HMM takes into account all tokens in the surrounding
sentence (although the strength of the dependence on more distant words decreases
rapidly). Thus the performance of the HMM compared with n-GRAM-R and NB-R,
as well as the performance of the LATTICE-TOKEN-R compared with the WEB1T-n-
GRAM-R, suggests that representations that are sensitive to the context of a word
produce better features.
To test these effects more rigorously, we selected 109 polysemous word types from
our test data, along with 296 non-polysemous word types. The set of polysemous word
types was selected by filtering for words in our labeled data that had at least two
POS tags that began with distinct letters (e.g., VBZ and NNS). An initial set of non-
polysemous word types was selected by filtering for types that appeared with just
one POS tag. We then manually inspected these initial selections to remove obvious
cases of word types that were in fact polysemous within a single part-of-speech, such
as ?bank.? We further define sparse word types as those that appear five times or
fewer in all of our unlabeled data, and we define non-sparse word types as those that
appear at least 50 times in our unlabeled data. Table 3 shows our POS tagging results
on the tokens of our labeled biomedical data with word types matching these four
categories.
As expected, all of our statistical language models outperform the baseline by
a larger margin on polysemous words than on non-polysemous words. The margin
between graphical model representations and the WEB1T-n-GRAM-R model also in-
creases on polysemous words, except for the NB-R. The WEB1T-n-GRAM-R uses none
of the local context to decide which features to provide, and the NB-R uses only the
immediate left and right context, so both models ignore most of the context. In contrast,
the remaining graphical models use Viterbi decoding to take into account all tokens
in the surrounding sentence, which helps to explain their relative improvement over
WEB1T-n-GRAM-R on polysemous words.
The same behavior is evident for sparse words, as compared with non-sparse
words: All of the statistical language model representations outperform the baseline
by a larger margin on sparse words than not-sparse words, and all of the graphical
models perform better relative to the WEB1T-n-GRAM-R on sparse words than not-
sparse words. By reducing the feature space from millions of possible n-gram fea-
tures to L categorical features, these models ensure that each of their features will
be observed often in a reasonably sized training data set. Thus representations based
103
Computational Linguistics Volume 40, Number 1
Table 3
Graphical models consistently outperform n-gram models by a larger margin on sparse words
than not-sparse words, and by a larger margin on polysemous words than not-polysemous
words. One exception is the NB-R, which performs worse relative to WEB1T-n-GRAM-R on
polysemous words than non-polysemous words. For each graphical model representation,
we show the difference in performance between that representation and WEB1T-n-GRAM-R
in parentheses. For each representation, differences in accuracy on polysemous and
non-polysemous subsets were statistically significant at p < 0.01 using a two-tailed
Fisher?s exact test. Likewise for performance on sparse vs. non-sparse categories.
polysemous not polysemous sparse not sparse
tokens 159 4,321 463 12,194
TRAD-R 59.5 78.5 52.5 89.6
WEB1T-n-GRAM-R 68.2 85.3 61.8 94.0
NB-R 64.5 88.7 57.8 89.4
(-WEB1T-n-GRAM-R) (?3.7) (+3.4) (?4.0) (?4.6)
HMM-TOKEN-R 67.9 83.4 60.2 91.6
(-WEB1T-n-GRAM-R) (?0.3) (?1.9) (?1.6) (?2.4)
I-HMM-TOKEN-R 75.6 85.2 62.9 94.5
(-WEB1T-n-GRAM-R) (+7.4) (?0.1) (+1.1) (+0.5)
LATTICE-TOKEN-R 70.5 86.9 65.2 94.6
(-WEB1T-n-GRAM-R) (+2.3) (+1.6) (+3.4) (+0.6)
on graphical models help address two key issues in building representations for POS
tagging.
4.3.2 Domain Divergence. Besides sparsity and polysemy, Ben-David et al.?s (2007, 2010)
theoretical analysis of domain adaptation shows that the distance between two domains
under a representation R of the data is crucial for a good representation. We test their
predictions using learned representations.
Ben-David et al.?s (2007, 2010) analysis depends on a particular notion of distance,
the d1 divergence, that is computationally intractable to calculate. For our analysis, we
resort instead to two different computationally efficient approximations of this measure.
The first uses a more standard notion of distance: the Jensen-Shannon Divergence (dJS),
a distance metric for probability distributions:
dJS(p||q) = 12
?
i
[
pilog
( pi
mi
)
+ qilog
( qi
mi
)]
where mi =
pi+qi
2 .
Intuitively, we aim to measure the distance between two domains by measuring
whether features appear more commonly in one domain than in the other. For instance,
the biomedical domain is far from the newswire domain under the TRAD-R repre-
sentation because word-based features like protein, gene, and pathway appear far more
commonly in the biomedical domain than the newswire domain. Likewise, bank and
president appear far more commonly in newswire text. Since the d1 distance is related
to the optimal classifier for distinguishing two domains, it makes sense to measure the
distance by comparing the frequencies of these features: a classifier can easily use the
occurrence of words like bank and protein to accurately predict whether a given sentence
belongs to the newswire or biomedical domain.
104
Huang et al. Computational Linguistics
More formally, let S and T be two domains, and let f be a feature5 in representation
R?that is, a dimension of the image space of R. Let V be the set of possible values
that f can take on. Let US be an unlabeled sample drawn from S, and likewise for
UT. We first compute the relative frequencies of the different values of f in R(US) and
R(UT ), and then compute dJS between these empirical distributions. Let pf represent the
empirical distribution over V estimated from observations of feature f in R(US), and let
qf represent the same distribution estimated from R(UT ).
Definition 1
JS domain divergence for a feature or df (US, UT ) is the domain divergence between
domains S and T under feature f from representation R, and is given by
df (US, UT ) = dJS(pf ||qf )
For a multidimensional representation, we compute the full domain divergence as a
weighted sum over the domain divergences for its features. Because individual features
may vary in their relevance to a sequence-labeling task, we use weights to indicate
their importance to the overall distance between the domains. We set the weight wf
for feature f proportional to the L1 norm of CRF parameters related to f in the trained
POS tagger. That is, let ? be the CRF parameters for our trained POS tagger, and let
?f = {?l,v|l be the state for zi and v be the value for f}. We set wf =
||?f ||1
||?||1 .
Definition 2
JS Domain Divergence or dR(US, UT ), is the distance between domains S and T under
representation R, and is given by
dR(US, UT ) =
?
f
wf df (US, UT )
Blitzer (2008) uses a different notion of domain divergence to approximate the d1
divergence, which we also experimented with. He trains a CRF classifier on examples
labeled with a tag indicating which domain the example was drawn from. We refer to
this type of classifier as a domain classifier. Note that these should not be confused
with our CRFs used for POS tagging, which take as input examples which are labeled
with POS sequences. For the domain classifier, we tag every token from the WSJ domain
as 0, and every token from the biomedical domain as 1. Blitzer then uses the accuracy
of his domain classifier on a held-out test set as his measure of domain divergence. A
high accuracy for the domain classifier indicates that the representation makes the two
domains easy to separate, and thus high accuracy signifies a high domain divergence. To
measure domain divergence using a domain classifier, we trained our representations
on all of the unlabeled data for this task, as before. We then used 500 randomly sampled
sentences from the WSJ domain, and 500 randomly sampled biomedical sentences, and
labeled these with 0 for the WSJ data and 1 for the biomedical data. We measured
the error rate of our domain-classifier CRF as the average error rate across folds when
performing three-fold cross-validation on these 1,000 sentences.
5 For simplicity, the definition we provide here works only for discrete features, although it is possible to
extend this definition to continuous-valued features.
105
Computational Linguistics Volume 40, Number 1
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.32 0.37 0.42 0.47
Ta
rg
et
 D
om
ai
n 
 
Ta
gg
in
g 
Ac
cu
ra
cy
 
Domain Divergence under the Representation 
LATTICE-R
I-HMM-R
Trad-R
Ngram-R
1 HMM 
7 HMMs 
8 layer LATTICE 
20 layer LATTICE 
Figure 5
Target-domain POS tagging accuracy for a model developed using a representation R correlates
strongly with lower JS domain divergence between WSJ and biomedical text under each
representation R. The correlation coefficients r2 for the linear regressions drawn in the
figure are both greater than 0.97.
Figure 5 plots the accuracies and JS domain divergences for our POS taggers.
Figure 6 shows the difference between target-domain error and source-domain error
as a function of JS domain divergence. Figures 7 and 8 show the same information,
except that the x axis plots the accuracy of a domain classifier as the way of mea-
suring domain divergence. These results give empirical support to Ben-David et al.?s
(2007, 2010) theoretical analysis: Smaller domain divergence?whether measured by
JS domain divergence or by the accuracy of a domain classifier?correlates strongly
with better target-domain accuracy. Furthermore, smaller domain divergence correlates
strongly with a smaller difference in the accuracy of the taggers on the source and
target domains.
Figure 6
Smaller JS domain divergence correlates with a smaller difference between target-domain error
and source-domain error.
106
Huang et al. Computational Linguistics
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98
Ta
rg
et
 D
om
ai
n 
 
Ta
gg
in
g 
Ac
cu
ra
cy
 
Domain Classification Accuracy 
Trad Rep
I-HMM
Ngram
PL-MRF
1 HMM 
7 HMMs 
20 layer PL-MRF 
8 layer PL-MRF 
Figure 7
Target-domain tagging accuracy decreases with the accuracy of a CRF domain classifier.
Intuitively, this means that training data from a source domain is less helpful for tagging in
a target domain when source-domain data is easy to distinguish from target-domain data.
Figure 8
Better domain classification correlates with a larger difference between target-domain error and
source-domain error.
Although both the JS domain divergence and the domain classifier provide only
approximations of the d1 metric for domain divergence, they agree very strongly:
In both cases, the LATTICE-TOKEN-R representations had the lowest domain diver-
gence, followed by the I-HMM-TOKEN-R representations, followed by TRAD-R, with
n-GRAM-R somewhere between LATTICE-TOKEN-R and I-HMM-TOKEN-R. The main
difference between the two metrics appears to be that the JS domain divergence gives
a greater domain divergence to the eight-layer LATTICE-TOKEN-R model and the
n-GRAM-R, placing them past the four- through eight-layer I-HMM-TOKEN-R represen-
tations. The domain classifier places these models closer to the other LATTICE-TOKEN-R
representations, just past the seven-layer I-HMM-TOKEN-R representation.
107
Computational Linguistics Volume 40, Number 1
The domain divergences of all models, using both techniques for measuring diver-
gence, remain significantly far from zero, even under the best representation. As a result,
there is ample room to experiment with even less-divergent representations of the two
domains, to see if they might yield ever-increasing target-domain accuracies. Note that
this is not simply a matter of adding more layers to the layered models. The I-HMM-
TOKEN-R model performed best with seven layers, and the eight-layer representation
had about the same accuracy and domain divergence as the five-layer model. This
may be explained by the fact that the I-HMM layers are trained independently, and so
additional layers may be duplicating other ones, and causing the supervised classifier
to overfit. But it also shows that our current methodology has no built-in technique
for constraining the domain divergence in our representations?the decrease in domain
divergence from our more sophisticated representations is a coincidental byproduct of
our training methodology, but there is no guarantee that our current mechanisms will
continue to decrease domain divergence simply by increasing the number of layers. An
important consideration for future research is to devise explicit learning mechanisms
that guide representations towards smaller domain divergences.
4.4 Domain Adaptation for Noun-Phrase Chunking and Chinese POS Tagging
We test the generality of our representations by using them for other tasks, domains, and
languages. Here, we report on further sequence-labeling tasks in a domain adaptation
setting: noun phrase chunking for adaptation from news text to biochemistry journals,
and POS tagging in Mandarin for a variety of domains. In the next section, we describe
the use of our representations in a weakly supervised information extraction task.
For chunking, the training set consists of the CoNLL 2000 shared task data for
source-domain labeled data (Sections 15?18 of the WSJ portion of the Penn Treebank,
labeled with chunk tags) (Tjong, Sang, and Buchholz 2000). For test data, we used
biochemistry journal data from the Open American National Corpus6 (OANC). One
of the authors manually labeled 198 randomly selected sentences (5,361 tokens) from
the OANC biochemistry text with noun-phrase chunk information.7 We focus on noun
phrase chunks because they are relatively easy to annotate manually, but contain a large
variety of open-class words that vary from domain to domain. The labeled training set
consists of 8,936 sentences and 211,726 tokens. Twenty-three percent of chunks in the
test set begin with an OOV word (especially adjective-noun constructions like ?aqueous
formation? and ?angular recess?), and 29% begin with a word seen at most twice in
training data; we refer to these as OOV chunks and rare chunks. For our unlabeled
data, we use 15,000 sentences (358,000 tokens; Sections 13?19) of the Penn Treebank
and 45,000 sentences (1,083,000 tokens) from the OANC?s biochemistry section. We
tested TRAD-R (augmented with features for automatically generated POS tags), LSA-R,
n-GRAM-R, NB-R, HMM-TOKEN-R, I-HMM-TOKEN-R (7 layers, which performed best
for POS tagging) and LATTICE-TOKEN-R (20 layers) representations.
Figure 9 shows our NP chunking results for this domain adaptation task. The
performance improvements for the HMM-based chunkers are impressive: LATTICE-
TOKEN-R reduces error by 57% with respect to TRAD-R, and comes close to state-of-the-
art results for chunking on newswire text. The results suggest that this representation
allows the CRF to generalize almost as well to out-of-domain text as in-domain text.
6 Available from http://www.anc.org/OANC/.
7 The labeled data for this experiment are available from the first author?s Web site.
108
Huang et al. Computational Linguistics
F1
on
B
io
ch
em
is
tr
y
Te
xt
0.72 0.74 
0.75 0.76 
0.84 
0.87 0.89 0.86 0.87 0.87 0.88 
0.91 
0.94 0.94 
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Trad-R Ngram-R LSA-R NB-R HMM-R I-HMM-R LATTICE-R
OOV ALL
Freq: 0 1 2 all
Chunks: 284 39 39 1,258
R P R P R P R P
TRAD-R .74 .70 .85 .87 .79 .86 .86 .87
n-GRAM-R .74 .74 .85 .85 .79 .86 .87 .87
LSA-R .76 .74 .82 .83 .78 .85 .87 .88
NB-R .73 .78 .86 .73 .86 .75 .88 .88
HMM-TOKEN-R .80 .89 .92 .88 .92 .90 .91 .90
I-HMM-TOKEN-R .90 .86 .92 .95 .87 .97 .95 .92
LATTICE-TOKEN-R .92 .85 .94 .95 .87 .97 .95 .93
Figure 9
On biomedical journal data from the OANC, our best NP chunker outperforms the baseline
CRF chunker by 0.17 F1 on chunks that begin with OOV words, and by 0.08 on all chunks. The
table shows performance breakdowns (recall and precision) for chunks whose first word has
frequency 0, 1, and 2 in training data, and the number of chunks in test data that fall into each
of these categories.
Improvements are greatest on OOV and rare chunks, where LATTICE-TOKEN-R made
absolute improvements over TRAD-R by 0.17 and 0.09 F1, respectively. Improvements
for the single-layer HMM-TOKEN-R were smaller but still significant: 36% relative re-
duction in error overall, and 32% for OOV chunks.
The improved performance from our HMM-based chunker caused us to wonder
how well the chunker could work without some of its other features. We removed all
tag features and orthographic features and all features for word types that appear fewer
than 20 times in training. This chunker still achieves 0.91 F1 on OANC data, and 0.93
F1 on WSJ data (Section 20), outperforming the TRAD-R system in both cases. It has
only 20% as many features as the baseline chunker, greatly improving its training time.
Thus these features are more valuable to the chunker than features from automatically
produced tags and features for all but the most common words.
For Chinese POS tagging, we use text from the UCLA Corpus of Written Chinese
(Tao and Xiao 2007), which is part of the Lancaster Corpus of Mandarin Chinese
(LCMC). The UCLA Corpus consists of 11,192 sentences of word-segmented and POS-
tagged text in 13 genres (see Table 4). We use gold-standard word segmentation labels
for training and testing. The LCMC tagset consists of 50 Chinese POS tags. On average,
each genre contains 5,284 word tokens, for a total of 68,695 tokens among all genres. We
use the ?news? genre as our source domain, which we use for training and development
109
Computational Linguistics Volume 40, Number 1
Table 4
POS tagging accuracy: the LATTICE-TOKEN-R and other graphical model representations
outperform TRAD-R and state-of-the-art Chinese POS taggers on all target domains. For target
domains, * indicates the performance is statistically significantly better than the Stanford and
TRAD-R baselines at p < 0.05, using a two-tailed ?2 test; ** indicates significance at p < 0.01.
On the news domain, the Stanford tagger is significantly different from all other systems
using a two-tailed ?2 test with p < 0.01.
Domain Stanford TRAD NGR LSA NB HMM I-H LAT
lore 88.4 84.0 84.2 85.3 85.3 89.7 89.9 90.1*
religion 83.5 79.1 79.4 79.8 80.0 85.2 85.6 85.9*
humour 89.0 84.2 84.5 86.2 86.8 89.6 89.6 89.9*
general-fic 87.5 84.5 85.0 85.3 85.7 89.4 89.7 89.9*
essay 88.4 83.2 83.7 84.0 84.3 89.0 89.1 90.1*
mystery 87.4 82.4 83.4 84.3 85.3 90.1 91.1 91.3**
romance 87.5 84.2 84.5 85.3 86.1 89.0 89.5 89.8**
science-fic 88.6 82.1 82.5 83.0 83.0 87.0 88.3 88.6
skills 82.7 77.3 77.7 78.2 78.4 84.9 85.0 85.1**
science 86.0 82.0 82.3 82.4 82.4 87.8 87.8 87.9*
adventure-fic 82.1 74.3 75.2 76.1 77.8 81.7 82.0 82.2
report 91.7 84.2 85.1 85.3 86.1 91.9 91.9 91.9
news 98.8** 96.9 92.3 93.4 94.3 94.2 97.0 97.1
all but news 87.0 81.2 82.0 82.8 83.6 88.1 88.4 88.8**
all domains 88.7 83.2 83.6 84.4 85.5 89.5 89.7 90.0**
data. For test data, we randomly select 20% of every other genre. For our unlabeled
data, we use all of the ?news? text, plus the remaining 80% of the texts from the other
genres. As before, we replace hapax legomena in the unlabeled data with the special
symbol *UNKNOWN*, and do the same for word types in the labeled test sets that never
appear in our unlabeled training texts. We compare against a state-of-the-art Chinese
POS tagger for in-domain text, the CRF-based Stanford tagger (Tseng, Jurafsky, and
Manning 2005). We obtained the code for this tagger,8 and retrained it on our training
data set.
The Chinese POS tagging results are shown in Table 4. The LATTICE-TOKEN-R
outperforms the state-of-the-art Stanford tagger on all target domains. Overall, on all
out-of-domain tests, LATTICE-TOKEN-R provides a relative reduction in error of 13.8%
compared with the Stanford tagger. The best performance is on the ?mystery? domain,
where the LATTICE-TOKEN-R model reaches 91.3% accuracy, a 3.9 percentage points
improvement over the Stanford tagger. Its performance on the in-domain ?news? test set
is significantly worse (1.7 percentage points) than the Stanford tagger, suggesting that
the Stanford tagger relies on domain-dependent features that are helpful for tagging
news, but not for tagging in general. The LATTICE-TOKEN-R?s accuracy is still signifi-
cantly worse on out-of-domain text than in-domain text, but the gap between the two
(8.3 percentage points) is better than the gap for the Stanford tagger (11.8 percentage
points). We believe that the lower out-of-domain performance of our Chinese POS
tagger, compared with our English POS tagger and our chunker, was at least in part
due to having far less unlabeled text available for this task.
8 Available at http://nlp.stanford.edu/software/tagger.shtml.
110
Huang et al. Computational Linguistics
5. Information Extraction Experiments
In this section, we evaluate our learned representations on their ability to capture
semantic, rather than syntactic, information. Specifically, we investigate a set-expansion
task in which we?re given a corpus and a few ?seed? noun phrases from a semantic
category (e.g., Superheroes), and our goal is to identify other examples of the category
in the corpus. This is a different type of weakly supervised task from the earlier domain
adaptation tasks because we are given only a handful of positive examples from a cate-
gory, rather than a large sample of positively and negatively labeled training examples
from a separate domain.
Existing set-expansion techniques utilize the distributional hypothesis: Candidate
noun phrases for a given semantic class are ranked based on how similar their contex-
tual distributions are to those of the seeds. Here, we measure how performance on the
set-expansion task varies when we employ different representations for the contextual
distributions.
5.1 Methods
The set-expansion task we address is formalized as follows. Given a corpus, a set of
seeds from some semantic category C, and a separate set of candidate phrases P, output
a ranking of the phrases in P in decreasing order of likelihood of membership in the
semantic category C.
For any given representation R, the set-expansion algorithm we investigate is
straightforward: We rank candidate phrases in increasing order of the distance between
their feature vectors and those of the seeds. The particular distance metrics utilized are
detailed subsequently.
Because set expansion is performed at the level of word types rather than to-
kens, it requires type-based representations. We compare HMM-TYPE-R, n-GRAM-R,
LATTICE-TYPE-R, and BROWN-TYPE-R in this experiment. We used a 25-state HMM,
and the LATTICE-TYPE-R as described in the previous section. Following previous set-
expansion experiments with n-grams (Ahuja and Downey 2010), we use a trigram
model with Kneser-Ney smoothing for n-GRAM-R.
The distances between the candidate phrases and the seeds for HMM-TYPE-R,
n-GRAM-R, and LATTICE-TYPE-R representations are calculated by first creating a
prototypical ?seed feature vector? equal to the mean of the feature vectors for each
of the seeds in the given representation. Then, we rank candidate phrases in order
of increasing distance between their feature vector and the seed feature vector. As a
distance measure between vectors (in this case, probability distributions), we compute
the average of five standard distance measures, including KL and JS divergence, and
cosine, Euclidean, and L1 distance. In experiments, we found that improving upon
this simple averaging was not easy?in fact, tuning a weighted average of the distance
measures for each representation did not improve results significantly on held-out data.
For Brown clusters, we use prefixes of all possible lengths as features. We define
the similarity between two Brown representation feature vectors to be the number of
features they share in common (this is equivalent to the length of the longest common
prefix between the two original Brown cluster labels). The candidate phrases are then
ranked in decreasing order of the sum of their similarity scores to each of the seeds. We
experimented with normalizing the similarity scores by the longer of the two vector
lengths, and found this to decrease results slightly. We use unnormalized (integer)
similarity scores for Brown clusters in our experiments.
111
Computational Linguistics Volume 40, Number 1
5.2 Data Sets
We utilized a set of approximately 100,000 sentences of Web text, joining multi-word
named entities in the corpus into single tokens using the Lex algorithm (Downey,
Broadhead, and Etzioni 2007). This process enables each named entity (the focus of the
set-expansion experiments) to be treated as a single token, with a single representation
vector for comparison. We developed all word type representations using this corpus.
To obtain examples of multiple semantic categories, we utilized selected Wikipedia
?listOf? pages from Pantel et al. (2009) and augmented these with our own manually
defined categories, such that each list contained at least ten distinct examples occurring
in our corpus. In all, we had 432 examples across 16 distinct categories such as Coun-
tries, Greek Islands, and Police TV Dramas.
5.3 Results
For each semantic category, we tested five different random selections of five seed
examples, treating the unselected members of the category as positive examples, and
all other candidate phrases as negative examples. We evaluate using the area under the
precision-recall curve (AUC) metric.
The results are shown in Table 5. All representations improve performance over
a random baseline, equal to the average AUC over five random orderings for each
category, and the graphical models outperform the n-gram representation. I-HMM-
TYPE-R and Brown clustering in the particular case of 1,000 clusters perform best, with
HMM-TYPE-R performing nearly as well. Brown clusters give somewhat lower results
as the number of clusters varies.
As with POS tagging, we expect that language model representations improve
performance on the IE task by providing informative features for sparse word types.
However, because the IE task classifies word types rather than tokens, we expect the rep-
resentations to provide less benefit for polysemous word types. To test these hypotheses,
we measured how IE performance changed in sparse or polysemous settings. We identi-
fied polysemous categories as those for which fewer than 90% of the category members
had the category as a clear dominant sense (estimated manually); other categories were
considered non-polysemous. Categories whose members had a median number of
occurrences in the corpus of less than 30 were deemed sparse, and others non-sparse.
Table 5
I-HMM-TYPE-R outperforms the other methods, improving performance over a random
baseline by twice as much as either n-GRAM-R or LATTICE-TYPE-R.
model AUC
I-HMM-TYPE-R 0.18
HMM-TYPE-R 0.17
BROWN-TYPE-R-3200 0.16
BROWN-TYPE-R-1000 0.18
BROWN-TYPE-R-320 0.15
BROWN-TYPE-R-100 0.13
LATTICE-TYPE-R 0.11
n-GRAM-R baseline 0.10
Random baseline 0.10
112
Huang et al. Computational Linguistics
Table 6
Graphical models as representations for IE consistently perform better relative to n-gram models
on sparse words, but not necessarily polysemous words.
polysemous not-polysemous sparse not-sparse
types 222 210 266 166
categs. 12 4 13 3
n-GRAM-R 0.07 0.17 0.06 0.25
LATTICE-TYPE-R 0.09 0.15 0.1 0.19
-n-GRAM-R +0.02 ?0.02 +0.04 ?0.06
HMM-TYPE-R 0.14 0.26 0.15 0.32
-n-GRAM-R +0.07 +0.09 +0.09 +0.07
IE performance on these subsets of the data are shown in Table 6. Both graphical
model representations outperform the n-gram representation more on sparse words, as
expected. For polysemy, the picture is mixed: The LATTICE-TYPE-R outperforms
n-GRAM-R on polysemous categories, whereas HMM-TYPE-R?s performance advan-
tage over n-GRAM-R decreases.
One surprise on the IE task is that the LATTICE-TYPE-R performs significantly less
well than the HMM-TYPE-R, whereas the reverse is true on POS tagging. We suspect
that the difference is due to the issue of classifying types vs. tokens. Because of their
more complex structure, PL-MRFs tend to depend more on transition parameters than
do HMMs. Furthermore, our decision to train the PL-MRFs using contrastive estimation
with a neighborhood that swaps consecutive pairs of words also tends to emphasize
transition parameters. As a result, we believe the posterior distribution over latent states
given a word type is more informative in our HMM model than the PL-MRF model.
We measured the entropy of these distributions for the two models, and found that
H(PPL-MRF(y|x = w)) = 9.95 bits, compared with H(PHMM(y|x = w)) = 2.74 bits, which
supports the hypothesis that the drop in the PL-MRF?s performance on IE is due to its
dependence on transition parameters. Further experiments are warranted to investigate
this issue.
5.4 Testing the Language Model Representation Hypothesis in IE
The language model representation hypothesis (Section 2) suggests that all else being
equal, more accurate language models will provide features that lead to better perfor-
mance on NLP tasks. Here, we test this hypothesis on the set expansion IE task.
Figures 10 and 11 show how the performance of the HMM-TYPE-R varies with the
language modeling accuracy of the underlying HMM. Language modeling accuracy
is measured in terms of perplexity on held-out text. Here, we use set expansion data
sets from previous work (Ahuja and Downey 2010). The first two are composed of
extractions from the TextRunner information extraction system (Banko et al. 2007) and
are denoted as Unary (361 examples) and Binary (265 examples). The second, Wikipedia
(2,264 examples), is a sample of Wikipedia concept names. We evaluate the performance
of several different trained HMMs with numbers of latent states K ranging from 5 to
1,600 (to help illustrate how IE and LM performance varies even when model capacity
is fixed, we include three distinct models with K = 100 states trained separately over
the full corpus). We used a distributed implementation of HMM training and corpus
113
Computational Linguistics Volume 40, Number 1
K = 5
K = 10
K = 25 K = 50
K = 100
K = 100
K = 100
K = 200
K = 400
Figure 10
Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracy of
the HMM varies, on TextRunner data sets. IE accuracy (in terms of area under the precision-recall
curve) tends to increase as language modeling accuracy improves (i.e., perplexity decreases).
5 10
25 50
100
100
100
200 400
5
5
25
2550
50
100
200
100
200
800
1600 400
Figure 11
Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracy
of the HMM varies on the Wikipedia data set. Number labels indicate the number of latent
states K, and performance is shown for three training corpus sizes (the full corpus consists of
approximately 60 million tokens). IE accuracy (in terms of area under the precision-recall curve)
tends to increase as language modeling accuracy improves (i.e., perplexity decreases).
partitioning techniques (Yang, Yates, and Downey 2013) to enable training of our larger
capacity HMM models on large data sets.
The results provide support for the language model representation hypothesis,
showing that IE performance does tend to improve as language model perplexity
decreases. On the smaller Unary and Binary sets (Figure 10), although IE accuracy
114
Huang et al. Computational Linguistics
does decrease for the lowest-perplexity models, overall language model perplexity
exhibits a negative correlation with IE area under the precision-recall curve (the Pearson
correlation coefficient is ?0.18 for Unary, and ?0.28 for Binary). For Wikipedia (Fig-
ure 11), the trend is more consistent, with IE performance increasing monotonically
as perplexity decreases for models trained on the full training corpus (the Pearson
correlation coefficient is ?0.90).
Figure 11 also illustrates how LM and IE performance changes as the amount
of training text varies. In general, increasing the training corpus size increases IE
performance and decreases perplexity. Over all data points in the figure, IE perfor-
mance correlates most strongly with model perplexity (?0.68 Pearson correlation, ?0.88
Spearman correlation), followed by corpus size (0.66, 0.71) and model capacity (?0.05,
0.38). The small negative Pearson correlation between model capacity and IE perfor-
mance is primarily due to the model with 1,600 states trained on 4% of the corpus.
This model has a large parameter space and sparse training data, and thus suffers from
overfitting in terms of both model perplexity and IE performance. If we ignore this
overfit model, the Pearson correlation between model capacity and IE performance for
the other models in the Figure is 0.24.
Our results show that IE based on distributional similarity tends to improve as the
quality of the latent variable model used to measure distributional similarity improves.
A similar trend was exhibited in our previous work (Ahuja and Downey 2010); here, we
extend the previous results to models with more latent states and a larger, more reliable
test set (Wikipedia). The results suggest that scaling up the training of latent variable
models to utilize larger training corpora and more latent states may be a promising
direction for improving IE capabilities.
6. Conclusion and Future Work
Our study of representation learning demonstrates that by using statistical language
models to aggregate information across many unannotated examples, it is possible to
find accurate distributional representations that can provide highly informative features
to weakly supervised sequence labelers and named-entity classifiers. For both domain
adaptation and weakly supervised set expansion, our results indicate that graphical
models outperform n-gram models as representations, in part for their greater ability to
handle sparsity and polysemy. Our IE task provides important evidence to support the
Language Model Representation Hypothesis, showing that the AUC of the IE system
correlates more with language model perplexity than the size of the training data or
the capacity of the language model. Finally, our sequence labeling experiments provide
empirical evidence in support of theoretical work on domain adaptation, showing that
target-domain tagging accuracy is highly correlated with two different measures of
domain divergence.
Representation learning remains a promising area for finding further improve-
ments in various NLP tasks. The representations we have described are trained in
an unsupervised fashion, so a natural extension is to investigate supervised or semi-
supervised representation-learning techniques. As mentioned previously, our current
techniques have no built-in methods for enforcing that they provide similar features in
different domains; devising a mechanism that enforces this could allow for less domain-
divergent and potentially more accurate representations. We have considered sequence
labeling, but another promising direction is to apply these techniques to more complex
structured prediction tasks, like parsing or relation extraction. Our current approach
to sequence labeling requires retraining of a CRF for every new domain; incremental
115
Computational Linguistics Volume 40, Number 1
retraining techniques for new domains would speed up the process. Finally, models
that combine our representation learning approach with instance weighting and other
forms of supervised domain adaptation may take better advantage of labeled data in
target domains, when it is available.
Acknowledgments
This material is based on work supported
by the National Science Foundation under
grant no. IIS-1065397.
References
Ahuja, Arun and Doug Downey. 2010.
Improved extraction assessment through
better language models. In Proceedings of
the Annual Meeting of the North American
Chapter of the Association of Computational
Linguistics (NAACL-HLT), pages 225?228,
Los Angeles, CA.
Ando, Rie Kubota and Tong Zhang. 2005.
A high-performance semi-supervised
learning method for text chunking.
In Proceedings of the ACL, pages 1?9,
Ann Arbor, MI.
Banko, Michele, Michael J. Cafarella,
Stephen Soderland, Matt Broadhead, and
Oren Etzioni. 2007. Open information
extraction from the web. In Proceedings of
the IJCAI, pages 2670?2676, Hyderabad.
Banko, Michele and Robert C. Moore.
2004. Part of speech tagging in context.
In Proceedings of the COLING, pages
556?561, Geneva.
Ben-David, Shai, John Blitzer, Koby
Crammer, Alex Kulesza, Fernando Pereira,
and Jenn Wortman. 2010. A theory of
learning from different domains. Machine
Learning, 79:151?175.
Ben-David, Shai, John Blitzer, Koby
Crammer, and Fernando Pereira. 2007.
Analysis of representations for domain
adaptation. In Advances in Neural
Information Processing Systems 20,
pages 127?144, Vancouver.
Bengio, Yoshua. 2008. Neural net language
models. Scholarpedia, 3(1):3,881.
Bengio, Yoshua, Re?jean Ducharme, Pascal
Vincent, and Christian Janvin. 2003.
A neural probabilistic language model.
Journal of Machine Learning Research,
3:1,137?1,155.
Bengio, Yoshua, Jerome Louradour,
Ronan Collobert, and Jason Weston.
2009. Curriculum learning. In Proceedings
of the International Conference on Machine
Learning (ICML), pages 41?48,
Montreal.
Bikel, Daniel M. 2004a. A distributional
analysis of a lexicalized statistical
parsing model. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 182?189,
Barcelona.
Bikel, Daniel M. 2004b. Intricacies of Collins?
parsing model. Computational Linguistics,
30(4):479?511.
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent Dirichlet allocation.
Journal of Machine Learning Research,
3:993?1,022.
Blitzer, John. 2008. Domain Adaptation of
Natural Language Processing Systems.
Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
Blitzer, John, Koby Crammer, Alex Kulesza,
Fernando Pereira, and Jenn Wortman.
2007. Learning bounds for domain
adaptation. In Advances in Neural
Information Processing Systems,
pages 129?136, Vancouver.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification.
In Association for Computational Linguistics
(ACL), pages 40?47, Prague.
Blitzer, John, Ryan McDonald, and
Fernando Pereira. 2006. Domain
adaptation with structural correspondence
learning. In Proceedings of the EMNLP,
pages 120?128, Sydney.
Bodlaender, Hans L. 1988. Dynamic
programming on graphs with bounded
treewidth. In Proceedings of the 15th
International Colloquium on Automata,
Languages and Programming,
pages 105?118, Tampere.
Brants, Thorsten and Alex Franz. 2006.
Web 1t 5-gram version 1. www.ldc.
upenn.edu/catalog/.
Brown, Peter F., Vincent J. Della Pietra,
Peter V. deSouza, Jenifer C. Lai, and
Robert L. Mercer. 1992. Class-based
n-gram models of natural language.
Computational Linguistics, 18:467?479.
Candito, Marie and Benoit Crabbe. 2009.
Improving generative statistical parsing
with semi-supervised word clustering.
In Proceedings of the IWPT, pages 138?141,
Paris.
116
Huang et al. Computational Linguistics
Chan, Yee Seng and Hwee Tou Ng. 2006.
Estimating class priors in domain
adaptation for word sense disambiguation.
In Proceedings of the Association for
Computational Linguistics (ACL),
pages 89?96, Sydney.
Chelba, Ciprian and Alex Acero. 2004.
Adaptation of maximum entropy
classifier: Little data can help a lot.
In Proceedings of the EMNLP,
pages 285?292, Barcelona.
Collobert, Robert and Jason Weston. 2008. A
unified architecture for natural language
processing: Deep neural networks with
multitask learning. In Proceedings of the
International Conference on Machine Learning
(ICML), pages 160?167, Helsinki.
Dai, Wenyuan, Gui-Rong Xue, Qiang Yang,
and Yong Yu. 2007. Transferring naive
Bayes classifiers for text classification.
In Proceedings of the National Conference
on Artificial Intelligence (AAAI),
pages 540?545, Vancouver.
Darroch, J. N., S. L. Lauritzen, and
T. P. Speed. 1980. Markov fields and
log-linear interaction models for
contingency tables. The Annals of
Statistics, 8(3):522?539.
Daume? III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
ACL, pages 256?263, Prague.
Daume? III, Hal, Abhishek Kumar, and
Avishek Saha. 2010. Frustratingly easy
semi-supervised domain adaptation.
In Proceedings of the ACL Workshop
on Domain Adaptation (DANLP),
pages 53?59, Uppsala.
Daume? III, Hal and Daniel Marcu. 2006.
Domain adaptation for statistical
classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
Deerwester, Scott, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer,
and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
American Society of Information Science,
41(6):391?407.
Dempster, Arthur, Nan Laird, and Donald
Rubin. 1977. Likelihood from incomplete
data via the EM algorithm. Journal of
the Royal Statistical Society, Series B,
39(1):1?38.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the latent words language
model. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 21?29,
Singapore.
Dhillon, Paramveer S., Dean Foster, and
Lyle Ungar. 2011. Multi-View Learning of
Word Embeddings via CCA. In Proceedings
of the Advances in Neural Information
Processing Systems (NIPS), volume 24,
pages 886?874, Granada.
Downey, Doug, Matthew Broadhead, and
Oren Etzioni. 2007. Locating complex
named entities in web text. In Proceedings
of the 20th International Joint Conference
on Artificial Intelligence (IJCAI 2007),
pages 2,733?2,739, Hyderabad.
Downey, Doug, Stefan Schoenmackers, and
Oren Etzioni. 2007. Sparse information
extraction: Unsupervised language models
to the rescue. In Proceedings of the ACL,
pages 696?703, Prague.
Dredze, Mark and Koby Crammer. 2008.
Online methods for multi-domain learning
and adaptation. In Proceedings of EMNLP,
pages 689?697, Honolulu, HI.
Dredze, Mark, Alex Kulesza, and Koby
Crammer. 2010. Multi-domain learning
by confidence weighted parameter
combination. Machine Learning,
79:123?149.
Finkel, Jenny Rose and Christopher D.
Manning. 2009. Hierarchical Bayesian
domain adaptation. In Proceedings of
HLT-NAACL, pages 602?610, Boulder, CO.
Fu?rstenau, Hagen and Mirella Lapata. 2009.
Semi-supervised semantic role labeling.
In Proceedings of the 12th Conference of the
European Chapter of the ACL, pages 220?228,
Athens.
Ghahramani, Zoubin and Michael I. Jordan.
1997. Factorial hidden Markov models.
Machine Learning, 29(2-3):245?273.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Conference on
Empirical Methods in Natural Language
Processing, pages 167?202, Pittsburgh, PA.
Goldwater, Sharon and Thomas L. Griffiths.
2007. A fully Bayesian approach to
unsupervised part-of-speech tagging.
In Proceedings of the ACL, pages 744?751,
Prague.
Grac?a, Joa?o V., Kuzman Ganchev, Ben Taskar,
and Fernando Pereira. 2009. Posterior vs.
parameter sparsity in latent variable
models. In Proceedings of the Neural
Information Processing Systems Conference
(NIPS), pages 664?672, Vancouver.
Harris, Z. 1954. Distributional structure.
Word, 10(23):146?162.
Hindle, Donald. 1990. Noun classification
from predicage-argument structures.
In Proceedings of the ACL, pages 268?275,
Pittsburgh, PA.
117
Computational Linguistics Volume 40, Number 1
Honkela, Timo. 1997. Self-organizing
maps of words for natural language
processing applications. In Proceedings
of the International ICSC Symposium on
Soft Computing, pages 401?407, Millet,
Alberta.
Huang, Fei and Alexander Yates. 2009.
Distributional representations for
handling sparsity in supervised sequence
labeling. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 495?503,
Singapore.
Huang, Fei and Alexander Yates. 2010.
Exploring representation-learning
approaches to domain adaptation. In
Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language
Processing (DANLP), pages 23?30, Uppsala.
Huang, Fei, Alexander Yates, Arun Ahuja,
and Doug Downey. 2011. Language
models as representations for weakly
supervised NLP tasks. In Proceedings
of the Conference on Natural Language
Learning (CoNLL), pages 125?134,
Portland, OR.
Jiang, Jing and ChengXiang Zhai. 2007a.
Instance weighting for domain
adaptation in NLP. In Proceedings
of ACL, pages 264?271, Prague.
Jiang, Jing and ChengXiang Zhai. 2007b. A
two-stage approach to domain adaptation
for statistical classifiers. In Proceedings of
the Conference on Information and Knowledge
Management (CIKM), pages 401?410, Lisbon.
Johnson, Mark. 2007. Why doesn?t EM find
good HMM POS-taggers. In Proceedings of
the EMNLP, pages 296?305, Prague.
Kaski, S. 1998. Dimensionality reduction
by random mapping: Fast similarity
computation for clustering. In
Proceedings of the IJCNN, pages 413?418,
Washington, DC.
Koo, Terry, Xavier Carreras, and Michael
Collins. 2008. Simple semi-supervised
dependency parsing. In Proceedings of
the Annual Meeting of the Association of
Computational Linguistics (ACL),
pages 595?603, Columbus, OH.
Lin, Dekang and Xiaoyun Wu. 2009. Phrase
clustering for discriminative learning.
In Proceedings of the ACL-IJCNLP,
pages 1,030?1,038, Singapore.
Liu, Dong C. and Jorge Nocedal. 1989. On
the limited memory method for large scale
optimization. Mathematical Programming B,
45(3):503?528.
Mansour, Y., M. Mohri, and
A. Rostamizadeh. 2009. Domain
adaptation with multiple sources.
In Proceedings of the Advances in Neural
Information Processing Systems,
pages 1,041?1,048, Vancouver.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Martin, Sven, Jorg Liermann, and Hermann
Ney. 1998. Algorithms for bigram and
trigram word clustering. Speech
Communication, 24:19?37.
McClosky, David. 2010. Any Domain Parsing:
Automatic Domain Adaptation for Parsing.
Ph.D. thesis, Brown University,
Providence, RI.
McClosky, David, Eugene Charniak, and
Mark Johnson. 2010. Automatic domain
adaptation for parsing. In North American
Chapter of the Association for Computational
Linguistics - Human Language Technologies
2010 Conference (NAACL-HLT 2010),
pages 28?36, Los Angeles, CA.
Miller, Scott, Jethran Guinness, and
Alex Zamanian. 2004. Name tagging with
word clusters and discriminative training.
In Proceedings of the Annual Meeting of the
North American Chapter of the Association of
Computational Linguistics (HLT-NAACL),
pages 337?342, Boston, MA.
Mnih, Andriy and Geoffrey Hinton. 2007.
Three new graphical models for statistical
language modelling. In Proceedings of
the 24th International Conference on
Machine Learning, pages 641?648,
Corvallis, OR.
Mnih, Andriy and Geoffrey Hinton. 2009.
A scalable hierarchical distributed
language model. In Proceedings of the
Neural Information Processing Systems
(NIPS), pages 1,081?1,088, Vancouver.
Mnih, Andriy, Zhang Yuecheng, and
Geoffrey Hinton. 2009. Improving a
statistical language model through
non-linear prediction. Neurocomputing,
72(7-9):1414?1418.
Morin, Frederic and Yoshua Bengio. 2005.
Hierarchical probabilistic neural network
language model. In Proceedings of the
International Workshop on Artificial
Intelligence and Statistics, pages 246?252,
Barbados.
Pantel, Patrick, Eric Crestan, Arkady
Borkovsky, Ana-Maria Popescu,
and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set
expansion. In Proceedings of the EMNLP,
pages 938?947, Singapore.
118
Huang et al. Computational Linguistics
PennBioIE. 2005. Mining the bibliome
project. http://bioie.ldc.upenn.edu/.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 183?190, Columbus, OH.
Pradhan, Sameer, Wayne Ward, and James H.
Martin. 2007. Towards robust semantic role
labeling. In Proceedings of NAACL-HLT,
pages 556?563, Rochester, NY.
Rabiner, Lawrence R. 1989. A tutorial on
hidden Markov models and selected
applications in speech recognition.
Proceedings of the IEEE, 77(2):257?285.
Raina, Rajat, Alexis Battle, Honglak Lee,
Benjamin Packer, and Andrew Y. Ng.
2007. Self-taught learning: Transfer
learning from unlabeled data.
In Proceedings of the 24th International
Conference on Machine Learning,
pages 759?766, Corvallis, OR.
Ratinov, Lev and Dan Roth. 2009. Design
challenges and misconceptions in named
entity recognition. In Proceedings of the
Conference on Natural Language Learning
(CoNLL), pages 147?155, Boulder, CO.
Ritter, H. and T. Kohonen. 1989.
Self-organizing semantic maps.
Biological Cybernetics, 61(4):241?254.
Sag, Ivan A., Thomas Wasow, and Emily M.
Bender. 2003. Synactic Theory: A Formal
Introduction. CSLI Publications, Stanford,
CA, second edition.
Sahlgren, Magnus. 2001. Vector-based
semantic analysis: Representing word
meanings based on random labels.
In Proceedings of the Semantic Knowledge
Acquisition and Categorization Workshop,
pages 1?12, Helsinki.
Sahlgren, Magnus. 2005. An introduction
to random indexing. In Methods and
Applications of Semantic Indexing Workshop
at the 7th International Conference on
Terminology and Knowledge Engineering
(TKE), 87:1?9.
Sahlgren, Magnus. 2006. The word-space
model: Using distributional analysis to
represent syntagmatic and paradigmatic
relations between words in high-dimensional
vector spaces. Ph.D. thesis, Stockholm
University.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill.
Satpal, Sandeep and Sunita Sarawagi.
2007. Domain adaptation of conditional
probability models via feature subsetting.
In Proceedings of ECML/PKDD,
pages 224?235, Warsaw.
Sekine, Satoshi. 1997. The domain
dependence of parsing. In Proceedings of
Applied Natural Language Processing
(ANLP), pages 96?102, Washington, DC.
Shen, Libin, Giorgio Satta, and Aravind K.
Joshi. 2007. Guided learning for
bidirectional sequence classification.
In Proceedings of the ACL, pages 760?767,
Prague.
Smith, Noah A. and Jason Eisner. 2005.
Contrastive estimation: Training
log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 354?362,
Ann Arbor, MI.
Sutton, Charles, Andrew McCallum, and
Khashayar Rohanimanesh. 2007. Dynamic
conditional random fields: Factorized
probabilistic models for labeling and
segmenting sequence data. Journal of
Machine Learning Research, 8:693?723.
Suzuki, Jun and Hideki Isozaki. 2008.
Semi-supervised sequential labeling and
segmentation using giga-word scale
unlabeled data. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL-HLT),
pages 665?673, Columbus, OH.
Suzuki, Jun, Hideki Isozaki, Xavier Carreras,
and Michael Collins. 2009. An empirical
study of semi-supervised structured
conditional models for dependency
parsing. In Proceedings of the EMNLP,
pages 551?560, Singapore.
Tao, Hongyin and Richard Xiao. 2007.
The UCLA Chinese corpus. UCREL.
www.lancaster.ac.uk/fass/projects/
corpus/UCLA/.
Tjong, Erik F., Kim Sang, and Sabine
Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking.
In Proceedings of the 4th Conference
on Computational Natural Language
Learning, pages 127?132, Lisbon.
Toutanova, Kristina and Mark Johnson.
2007. A Bayesian LDA-based model for
semi-supervised part-of-speech
tagging. In Proceedings of the NIPS,
pages 1,521?1,528, Vancouver.
Tseng, Huihsin, Daniel Jurafsky, and
Christopher Manning. 2005.
Morphological features help POS
tagging of unknown words across
language varieties. In Proceedings
of the Fourth SIGHAN Workshop,
pages 32?39, Jeju Island.
119
Computational Linguistics Volume 40, Number 1
Turian, Joseph, James Bergstra, and
Yoshua Bengio. 2009. Quadratic
features and deep architectures for
chunking. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics - Human
Language Technologies (NAACL HLT),
pages 245?248, Boulder, CO.
Turian, Joseph, Lev Ratinov, and Yoshua
Bengio. 2010. Word representations:
A simple and general method for
semi-supervised learning. In Proceedings
of the Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 384?394, Uppsala.
Turney, Peter D. and Patrick Pantel. 2010.
From frequency to meaning: Vector
space models of semantics. Journal of
Artificial Intelligence Research, 37:141?188.
Ushioda, Akira. 1996. Hierarchical clustering
of words. In Proceedings of the International
Conference on Computational Linguistics
(COLING), pages 1,159?1,162, Copenhagen.
Va?yrynen, Jaakko and Timo Honkela. 2004.
Word category maps based on emergent
features created by ICA. In Proceedings of
the STePs 2004 Cognition + Cybernetics
Symposium, pages 173?185, Tikkurila.
Va?yrynen, Jaakko and Timo Honkela. 2005.
Comparison of independent component
analysis and singular value decomposition
in word context analysis. In Proceedings
of the International and Interdisciplinary
Conference on Adaptive Knowledge
Representation and Reasoning (AKRR),
pages 135?140, Espoo.
Va?yrynen, Jaakko, Timo Honkela, and
Lasse Lindqvist. 2007. Towards explicit
semantic features using independent
component analysis. In Proceedings of the
Workshop Semantic Content Acquisition
and Representation (SCAR), pages 20?27,
Stockholm.
Weston, Jason, Frederic Ratle, and
Ronan Collobert. 2008. Deep learning
via semi-supervised embedding.
In Proceedings of the 25th International
Conference on Machine Learning,
pages 1,168?1,175, Helsinki.
Yang, Yi, Alexander Yates, and Doug
Downey. 2013. Overcoming the memory
bottleneck in distributed training
of latent variable models of text.
In Proceedings of the NAACL-HLT,
pages 579?584, Atlanta, GA.
Zhao, Hai, Wenliang Chen, Chunyu Kit,
and Guodong Zhou. 2009. Multilingual
dependency learning: A huge feature
engineering method to semantic
dependency parsing. In Proceedings of the
CoNLL 2009 Shared Task, pages 55?60,
Boulder, CO.
120
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 1?9,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Online Active Learning for Cost Sensitive Domain Adaptation
Min Xiao and Yuhong Guo
Department of Computer and Information Sciences
Temple University
Philadelphia, PA 19122, USA
{minxiao,yuhong}@temple.edu
Abstract
Active learning and domain adaptation are
both important tools for reducing labeling
effort to learn a good supervised model in
a target domain. In this paper, we inves-
tigate the problem of online active learn-
ing within a new active domain adapta-
tion setting: there are insufficient labeled
data in both source and target domains,
but it is cheaper to query labels in the
source domain than in the target domain.
Given a total budget, we develop two cost-
sensitive online active learning methods, a
multi-view uncertainty-based method and
a multi-view disagreement-based method,
to query the most informative instances
from the two domains, aiming to learn a
good prediction model in the target do-
main. Empirical studies on the tasks of
cross-domain sentiment classification of
Amazon product reviews demonstrate the
efficacy of the proposed methods on re-
ducing labeling cost.
1 Introduction
In many application domains, it is difficult or ex-
pensive to obtain labeled data to train supervised
models. It is critical to develop effective learning
methods to reduce labeling effort or cost. Active
learning and domain adaptation are both impor-
tant tools for reducing labeling cost on learning
good supervised prediction models. Active learn-
ing reduces the cost of labeling by selecting the
most informative instances to label, whereas do-
main adaptation obtains auxiliary label informa-
tion by exploiting labeled data in related domains.
Combining the efforts from both areas to further
reduce the labeling cost is an important research
direction to explore.
In this paper, we consider online active learn-
ing with domain adaptations. Online learning has
been widely studied (Borodin and El-Yaniv, 1998)
due to its advantages of low memory requirement
and fast computation speed. Dredze and Crammer
(2008) applied online learning on domain adap-
tation and proposed to combine multiple similar
source domains to perform online learning for the
target domain, which provides a new opportunity
for conducting active learning with domain adap-
tation. Online active learning with domain adap-
tation, to our knowledge, has just gained atten-
tion recently and has been addressed in (Rai et al,
2010; Saha et al, 2011). The active online do-
main adaptation methods developed in (Rai et al,
2010; Saha et al, 2011) leverage information from
the source domain by domain adaptation to intelli-
gently query labels for instances only in the target
domain in an online fashion with a given budget.
They assumed a large amount of labeled data is
readily available in the source domain.
In this work, we however tackle online active
learning with domain adaptation in a different set-
ting, where source domains with a large amount of
free labeled data are not available. Instead we as-
sume there are very few labeled instances in both
the source and target domains and labels in both
domains can be acquired with a cost. Moreover,
we assume the annotation cost for acquiring la-
bels in the source domain is much lower than the
annotation cost in the target domain. This is a
practical setting in many domain adaptation sce-
narios. For example, one aims to learn a good
review classification model for high-end comput-
ers. It may be expensive to acquire labels for such
product reviews. However, but it might be rela-
tively much cheaper (but not free) to acquire la-
bels for reviews on movies or restaurants. In such
an active learning scenario, will a source domain
with lower annotation cost still be helpful for re-
ducing the labeling cost required to learn a good
prediction model in the target domain? Our re-
search result in this paper will answer this ques-
1
Figure 1: The framework of online active learning
with domain adaptation.
tion. Specifically, we address this online active do-
main adaptation problem by extending the online
active learning framework in (Cesa-Bianchi et al,
2006) to consider active label acquirement in both
domains. We first initialize the prediction model
based on the initial labeled data in both the source
and target domains (LS and LT ). Then in each
round of the online learning, we receive one un-
labeled instance from each domain (DS and DT ),
on which we need to decide whether to query la-
bels. Whenever a label is acquired, we update the
prediction model using the newly labeled instance
if necessary. The framework of this online active
learning setting is demonstrated in Figure 1. We
exploit multi-view learning principles to measure
the informativeness of instances and propose two
cost-sensitive online active learning methods, a
multi-view uncertainty-based method and a multi-
view disagreement-based method, to acquire la-
bels for the most informative instances. Our em-
pirical studies on the tasks of cross-domain sen-
timent classification of Amazon product reviews
show the proposed methods can effectively ac-
quire the most informative labels given a budget,
comparing to alternative methods.
2 Related Work
The proposed work in this paper involves re-
search developments in multiple areas, including
online active learning, active domain adaptation
and multi-view active learning. In this section, we
will cover the most related work in the literature.
Online active learning has been widely stud-
ied in the literature, including the perceptron-type
methods in (Cesa-Bianchi et al, 2006; Monteleoni
and Ka?a?ria?inen, 2007; Dasgupta et al, 2009).
Cesa-Bianchi et al (2006) proposed a selective
sampling perceptron-like method (CBGZ), which
serves as a general framework of online active
learning. Monteleoni and Ka?a?ria?inen (2007) em-
pirically studied online active learning algorithms,
including the CBGZ, for optical character recogni-
tion applications. Dasgupta et al (2009) analyzed
the label complexity of the perceptron algorithm
and presented a combination method of a modifi-
cation of the perceptron update with an adaptive
filtering rule. Our proposed online active learn-
ing methods are placed on an extended framework
of (Cesa-Bianchi et al, 2006), by incorporating
domain adaptation and multi-view learning tech-
niques in an effective way.
Active domain adaptation has been studied in
(Chan and Ng, 2007; Rai et al, 2010; Saha et al,
2011; Li et al, 2012). Chan and Ng (2007) pre-
sented an early study on active domain adaptation
and empirically demonstrated that active learn-
ing can be successfully applied on out-of-domain
word sense disambiguation systems. Li et al
(2012) proposed to first induce a shared subspace
across domains and then actively label instances
augmented with the induced latent features. On-
line active domain adaptation, however, has only
been recently studied in (Rai et al, 2010; Saha
et al, 2011). Nevertheless, the active online do-
main adaptation method (AODA) and its vari-
ant method, domain-separator based AODA (DS-
AODA), proposed in these works assume a large
amount of labeled data in the source domain and
conduct online active learning only in the target
domain, which is different from our problem set-
ting in this paper.
Multi-view learning techniques have recently
been employed in domain adaptation (Tur, 2009;
Blitzer et al, 2011; Chen et al, 2011). In par-
ticular, instead of using data with conditional in-
dependent views assumed in standard multi-view
learning (Blum and Mitchell, 1998), Blitzer et al
(2011) and Chen et al (2011) randomly split
original features into two disjoint subsets to pro-
duce two views, and demonstrate the usefulness
of multi-view learning with synthetic two views.
On the other hand, multi-view active learning has
been studied in (Muslea et al, 2000, 2002; Wang
and Zhou, 2008, 2010). These works all suggest
to query labels for contention points (instances
on which different views predict different labels).
Our proposed methods will exploit this multi-view
2
principle and apply it in our multi-view online ac-
tive domain adaptation setting.
In addition, our proposed work is also related
to cost-sensitive active learning. But different
from the traditional cost-sensitive active learn-
ing, which assumes multiple oracles with different
costs exist for the same set of instances (Donmez
and Carbonell, 2008; Arora et al, 2009), we as-
sume two oracles, one for the source domain and
one for the target domain. Overall, the problem we
study in this paper is novel, practical and impor-
tant. Our research will demonstrate a combination
of advances in multiple research areas.
3 Multi-View Online Active Learning
with Domain Adaptation
Our online active learning is an extension of the
online active perceptron learning framework of
(Cesa-Bianchi et al, 2006; Rai et al, 2010) in the
cost-sensitive online active domain adaption set-
ting. We will present two multi-view online ac-
tive methods in this section under the framework
shown in Figure 1.
Assume we have a target domain (DT ) and a
related source domain (DS) with a few labeled in-
stances, LT and LS , in each of them respectively.
The instances in the two domains are drawn from
the same input space but with two different distri-
butions specified by each domain. An initial pre-
diction model (w0) can then be trained with the
current labeled data from both domains. Many
domain adaptation techniques (Sugiyama, 2007;
Blitzer et al, 2011) can be used for training here.
However, for simplicity of demonstrating the ef-
fectiveness of online active learning strategies, we
use vanilla Perceptron to train the initial prediction
model on all labeled instances, as the perceptron
algorithm is widely used in various works (Saha
et al, 2011) and can be combined seamlessly with
the online perceptron updates. It can be viewed as
a simple supervised domain adaptation training.
The very few initial labeled instances are far
from being sufficient to train a good prediction
model in the target domain. Additional labeled
data needs to be acquired to reach a reasonable
prediction model. However it takes time, money,
and effort to acquire labels in all problem domains.
For simplicity of demonstration, we use money to
measure the cost and effort of labeling instances
in each domain. Assume the cost of labeling one
instance in the source domain is cs and the cost
of labeling one instance in the target domain is ct,
where ct > cs. Note the condition ct > cs is
one criterion to be guaranteed when selecting use-
ful source domains. It does not make sense to se-
lect source domains with more expensive labeling
cost. Given a budget B, we need to make wise de-
cisions about which instances to query in the on-
line learning setting. We aim to learn the best pre-
diction model in the target domain with the labels
purchased under the given budget.
Then online active learning will be conducted in
a sequence of rounds. In each round r, we will re-
ceive two randomly sampled unlabeled instances
in parallel, xs,r and xt,r, one from each domain,
xs,r ? DS and xt,r ? DT . Active learning strate-
gies will be used to judge the informativeness of
the two instances in a cost-sensitive manner and
decide whether to query labels for any one of them
to improve the prediction model in the target do-
main. After new labels being acquired, we use the
newly labeled instances to make online perceptron
updates if the true labels are different from the pre-
dicted labels.
In this work, we focus on binary prediction
problems where the labels have binary values, y ?
{+1,?1}. We adopt the online perceptron-style
learning model of (Cesa-Bianchi et al, 2006) for
the online updates of the supervised perceptron
model. Moreover, we extend principles of multi-
view active learning into our online active learn-
ing framework. As we introduced before, syn-
thetic multi-views produced by splitting the orig-
inal feature space into disjoint subsets have been
demonstrated effective in a few previous work
(Blitzer et al, 2011; Chen et al, 2011). We adopt
this idea to generate two views of the instances
in both domains by randomly splitting the com-
mon feature space into two disjoint feature sub-
sets, such that xs,r = {x(1)s,r ,x(2)s,r} and xt,r =
{x(1)t,r ,x
(2)
t,r }. Thus the initial prediction model will
include two predictors (f (1), f (2)) with model pa-
rameters (w(1)0 ,w
(2)
0 ), each trained on one view
of the labeled data using the perceptron algorithm.
Correspondingly, the online updates will be made
on the two predictors.
The critical challenge of this cost-sensitive on-
line active learning problem nevertheless lies in
how to select the most informative instances for
labeling. Based on different measurements of
instance informativeness, we propose two on-
line active learning algorithms: a Multi-view
3
Uncertainty-based instance Selection (MUS) al-
gorithm and a Multi-view Disagreement-based
instance Selection (MDS) algorithm for cost-
sensitive online active domain adaptation, which
we will present below.
3.1 Multi-View Uncertainty-based Instance
Selection Algorithm
We use the initial model (f (1), f (2)), trained on
the two views of the initial labeled data and rep-
resented by the model parameters (w(1)0 ,w
(2)
0 ), as
the starting point of the online active learning.
In each round r of the online active learning,
we receive two instances xs,r = {x(1)s,r ,x(2)s,r} and
xt,r = {x(1)t,r ,x
(2)
t,r }, one for each domain. For the
received instances, we need to make two sequen-
tial decisions:
1. Between the instance (xs,r) from the source
domain and the instance (xt,r) from the tar-
get domain, which one should we select for
further consideration?
2. For the selected instance, do we really need
to query its label?
We answer the first question based on the label-
ing cost ratio, ct/cs, from the two domains and
define the following probability
Pc = e??(ct/cs?1) (1)
where ? is a domain preference weighting param-
eter. Then with a probability Pc we select the tar-
get instance xt,r and with a probability 1 ? Pc we
select the source instance xs,r. Our intuition is that
one should query the less expensive source domain
more frequently. Thus more labeled instances can
be collected within the fix budget. On the other
hand, the more useful and relevant but expensive
instances from the target domain should also be
queried at a certain rate.
For the selected instance x?,r, we then use a
multi-view uncertainty strategy to decide whether
to query its label. We first calculate the prediction
confidence and predicted labels of the selected in-
stance based on the current predictors trained from
each view
mk = |w(k)?x(k)?,r |, y?(k) = sign(w(k)?x(k)?,r ) (2)
where k = 1 or 2, standing for each of the two
views. If the two predictors disagree over the pre-
diction label, i.e., y?(1) 6= y?(2), the selected in-
stance is a contention point and contains useful
Algorithm 1 MUS Algorithm
Input: B, Pc, cs, ct, b,
initial model (w(1)0 ,w
(2)
0 )
Output: prediction model (w(1),w(2))
Initialize: w(1) = w(1)0 , w(2) = w
(2)
0
for each round r = 1, 2, ? ? ? do
Receive two instances xs,r, xt,r
Sample d ? U(0, 1)
if B < ct then d = 1 end if
if d > Pc then x?,r= xs,r, c = cs
else x?,r= xt,r, c = ct
end if
Compute m1,m2, y?(1), y?(2) by Eq.(2)
Compute z1, z2 by Eq.(3)
if z1 = 1 or z2 = 1 or y?(1) 6= y?(2) then
Query label y for x?,r, B = B? c
Update (w(1),w(2)) by Eq (4)
end if
if B < cs then break end if
end for
information for at least one predictor, according
to the principle of multi-view active learning. We
then decide to pay a cost (cs or ct) to query its la-
bel. Otherwise, we make the query decision based
on the two predictors? uncertainty (i.e., the inverse
of the prediction confidence mk) over the selected
instance. Specifically, we sample two numbers,
one for each view, according to
zk = Bernoulli(b/(b + mk)) (3)
where b is a prior hyperparameter, specifying the
tendency of querying labels. In our experiments,
we use b = 0.1. If either z1 = 1 or z2 = 1,
which means that at least one view is uncertain
about the selected instance, we will query for the
label y. The prediction model will be updated us-
ing the new labeled instances when the true labels
are different from the predicted ones; i.e.,
w(k) = w(k) + (yx(k)?,r )I[y 6= y?(k)] (4)
for k = 1, 2, where I[?] is an indicator function.
This multi-view uncertainty-based instance selec-
tion algorithm (MUS) is given in Algorithm 1.
3.2 Multi-View Disagreement-based Instance
Selection Algorithm
MUS is restrained to query at most one instance
at each round of the online active learning. In
this section, we present an alternative multi-view
4
disagreement-based instance selection algorithm
(MDS) within the same framework.
In each round r of the online active learning,
given the two instances xs,r and xt,r we received,
the MDS algorithm evaluates both instances for
potential label acquisition using the multi-view in-
formation provided by the two per-view predic-
tors. Let y?(1)s and y?(2)s denote the predicted la-
bels of instance xs,r produced by the two predic-
tors according to Eq (2). Similarly let y?(1)t and
y?(2)t denote the predicted labels of instance xt,r.
Follow the principle suggested in the multi-view
active learning work (Muslea et al, 2000, 2002;
Wang and Zhou, 2008, 2010) that querying labels
for contention points (instances on which different
views predict different labels) can lead to superior
information gain than querying uncertain points,
we identify the non-redundant contention points
from the two domains for label acquisition.
Specifically, there are three cases: (1) If only
one of the instances is a contention point, we query
its label with probability Pc (Eq (1)) when the in-
stance is from the target domain, and query its la-
bel with probability 1 ? Pc when the instance is
from the source domain. (2) If both instances are
contention points, i.e., y?(1)s 6= y?(2)s and y?(1)t 6= y?
(2)
t ,
but the predicted labels for the two instances are
the same, i.e., y?(k)s = y?(k)t for k = 1, 2, it suggests
the two instances contain similar information with
respect to the prediction model and we only need
to query one of them. We then select the instance
in a cost-sensitive manner stated in the MUS algo-
rithm by querying the target instance with a prob-
ability Pc and querying the source instance with a
probability 1 ? Pc. (3) If both instances are con-
tention points but with different predicted labels, it
suggests the two instances contain complementary
information with respect to the prediction model,
and we thus query labels for both of them.
For any new labeled instance from the target
domain or the source domain, we update the pre-
diction model of each review using Equation (4)
when the acquired true label is different from the
predicted label. The overall MDS algorithm is
given in Algorithm 2.
3.3 Multi-View Prediction
After the training process, we use the two predic-
tors to predict labels of the test instances from
the target domain. Given a test instance xt =
Algorithm 2 MDS Algorithm
Input: B, Pc, cs, ct, b,
initial model (w(1)0 ,w
(2)
0 )
Output: prediction model (w(1),w(2))
Initialize: w(1) = w(1)0 , w(2) = w
(2)
0
for each round r = 1, 2, ? ? ? do
Receive two instances xs,r, xt,r
Compute y?(1)s , y?(2)s , y?(1)t , y?
(2)
t by Eq (2)
Let ds = I[y?(1)s = y?(2)s ], dt = I[y?(1)t = y?
(2)
t ]
Let qs = 0, qt = 0
if B < ct then dt = 0 end if
Sample d ? U(0, 1)
if ds = 1 and dt = 0 then
if d > Pc then qs = 1 end if
else if ds = 0 and dt = 1 then
if d ? Pc then qt = 1 end if
else if ds = 1 and dt = 1
if y?(1)s = y?(1)t then
if d > Pc then qs = 1 else qt = 1 end if
else qs = 1, qt = 1
end if
end if
if qs = 1 then
Query label ys for xs,r, B = B? cs
Update (w(1),w(2)) by Eq (4)
end if
if B < ct then qt = 0 end if
if qt = 1 then
Query label yt for xt,r, B = B? ct
Update (w(1),w(2)) by Eq (4)
end if
if B < cs then break end if
end for
(x(1)t ,x
(2)
t ), we use the predictor that have larger
prediction confidence to determine its label y?.
The prediction confidence of the kth view predic-
tor on xt is defined as the absolute prediction value
|w(k)?x(k)t |. We then select the most confident
predictor for this instance as
k? = argmax
k?{1,2}
|w(k)?x(k)t | (5)
The predicted label is final computed as
y? = sign(w(k?)?x(k
?)
t ) (6)
With this multi-view prediction on the test data,
the multi-view strengths can be exploited in the
testing phase as well.
5
4 Experiments
In this section, we present the empirical evaluation
of the proposed online active learning methods on
the task of sentiment classification comparing to
alternative baseline methods. We first describe the
experimental setup, and then present the results
and discussions.
4.1 Experimental Setup
Dataset For the sentiment classification task, we
use the dataset provided in (Prettenhofer and Stein,
2010). The dataset contains reviews with four
different language versions and in three domains,
Books (B), DVD (D) and Music (M). Each domain
contains 2000 positive reviews and 2000 negative
reviews, with a term-frequency (TF) vector rep-
resentation. We used the English version and con-
structed 6 source-target ordered domain pairs from
the original 3 domains: B2D, D2B, B2M, M2B,
D2M, M2D. For example, for the task of B2D, we
use the Books reviews as the source domain and
the DVD reviews as the target domain. For each
pair of domains, we built a unigram vocabulary
over the combined 4000 source reviews and 4000
target reviews. We further preprocessed the data
by removing features that appear less than twice
in either domain, replacing TF with TFIDF, and
normalizing the attribute values into [0, 1].
Approaches In the experiments, we mainly
compared the proposed MUS and MDS algorithms
with the following three baseline methods. (1)
MTS (Multi-view Target instance Selection): It
is a target-domain variant of the MUS algorithm,
and selects the most uncertain instance received
from the target domain to query according to the
procedure introduced for MUS method. (2) TCS
(Target Contention instance Selection): It is a
target-domain variant of the MDS algorithm, and
uses multi-view predictors to query contention in-
stances received from the target domain. (3) SUS
(Single-view Uncertainty instance Selection): It
selects target vs source instances according to Pc
(see Eq.(1)), and then uses uncertainty measure to
make query decision. This is a single view vari-
ant of the MUS algorithm. In the experiments, we
used ? = 1 for the Pc computation in Eq.(1).
4.2 Classification Accuracy
We first conducted experiments over the 6 do-
main adaptation tasks constructed from the sen-
timent classification data with a fixed cost ratio
ct/cs = 3. We set cs = 1 and ct = 3. Given a bud-
getB = 900, we measure the classification perfor-
mance of the prediction model learned by each on-
line active learning method during the process of
budget being used. We started with 50 labeled in-
stances from the source domain and 10 labeled in-
stances from the target domain. The classification
performance is measured over 1000 test instances
from the target domain. All other instances are
used as inputs in the online process. We repeated
the experiments 10 times using different random
online instance input orders. The average results
are reported in Figure 2.
The results indicate the proposed two algo-
rithms, MUS and MDS, in general greatly out-
perform the other alternative methods. The SUS
method, which is a single-view variant of MUS,
presents very poor performance across all 6 tasks
comparing to the other multi-view based methods,
which demonstrates the efficacy of the multi-view
instance selection mechanism. Among the multi-
view based active learning methods, the MTS
method and TCS method, which only query labels
for more relevant but expensive instances from
the target domain, demonstrated inferior perfor-
mance, comparing to their cost-sensitive counter-
parts, MUS and MDS, respectively. This suggests
that a cheaper source domain is in general helpful
on reducing the labeling cost for learning a good
prediction model in the target domain and our pro-
posed active learning strategies are effective.
4.3 Domain Divergence
To further validate and understand our experimen-
tal results on the sentiment classification data, we
evaluated the domain divergence over the three
pairs of domains we used in the experiments
above. Note, if the domain divergence is very
small, it will be natural that a cheaper source do-
main should help on reducing the labeling cost in
the target domain. If the domain divergence is very
big, the space of exploring a cheaper source do-
main will be squeezed.
The divergence of two domains can be mea-
sured using the A-distance (Ben-David et al,
2006). We adopted the method of (Rai et al, 2010)
to proximate the A-distance. We train a linear
classifier over all 8000 instances, 4000 instances
from each domain, to separate the two domains.
The average per-instance hinge-loss for this sepa-
rator subtracted from 1 was used as the estimate
6
0 200 400 600 800
64
66
68
70
72
74
76
78
B2D
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
0 200 400 600 800
62
64
66
68
70
72
74
76
B2M
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
0 200 400 600 800
60
62
64
66
68
70
72
74
76
D2M
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
(a) (b) (c)
0 200 400 600 800
62
64
66
68
70
72
74
76
D2B
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
0 200 400 600 800
64
66
68
70
72
74
76
M2B
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
0 200 400 600 800
64
66
68
70
72
74
76
78
M2D
Total budget
A
cc
ur
ac
y
 
 
MTS
TCS
SUS
MUS
MDS
(d) (e) (f)
Figure 2: Online active learning results over the 6 domain adaptation tasks for sentiment classification,
with a total budget B=900 and a fixed cost ratio ct/cs = 3.
of the proxy A-distance. A score of 1 means per-
fectly separable distributions and 0 means the two
distributions from the two domains are identical.
In general, a higher score means a larger diver-
gence between the two domains.
Table 1: Proxy A-distance over domain pairs.
Domains A-distance
Books vs. DVD 0.7221
Books vs. Music 0.8562
DVD vs. Music 0.7831
The proxy A-distances over the 3 domain pairs
from the sentiment classification dataset are re-
ported in Table 1. It shows that all the 3 pairs
of domains are reasonably far apart. This justi-
fied the effectiveness of the online active domain
adaptation methods we developed and the results
we reported above. It suggests the applicability of
the proposed active learning scheme is not bound
to the existence of highly similar source domains.
Moreover, the A-distance between Books and Mu-
sic is the largest among the three pairs. Thus it
is most challenging to exploit the source domain
in the adaptation tasks, B2M and M2B. This ex-
plains the good performance of the target-domain
method TCS on these two tasks. Nevertheless, the
proposed MUS and MDS maintained consistent
good performance even on these two tasks.
4.4 Robustness to Cost Ratio
We then studied the empirical behavior of the pro-
posed online active domain adaptation algorithms
with different cost ratio values ct/cs.
Given a fixed budget B = 900, we set cs = 1
and run a few sets of experiments on the senti-
ment classification data by setting ct as different
values from {1, 2, 3, 4}, under the same experi-
mental setting described above. In addition to
the five comparison methods used before, we also
added a baseline marker, SCS, which is a source-
domain variant of the MDS algorithm and queries
contention instances from only the source domain.
The final classification performance of the predic-
tion model learned with each approach is recorded
7
1 1.5 2 2.5 3 3.5 4
66
68
70
72
74
76
78
80
82
B2D
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
1 1.5 2 2.5 3 3.5 4
64
66
68
70
72
74
76
78
B2M
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
1 1.5 2 2.5 3 3.5 4
64
66
68
70
72
74
76
78
D2M
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
(a) (b) (c)
1 1.5 2 2.5 3 3.5 4
66
68
70
72
74
76
78
D2B
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
1 1.5 2 2.5 3 3.5 4
62
64
66
68
70
72
74
76
78
M2B
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
1 1.5 2 2.5 3 3.5 465
70
75
80
M2D
ct/cs
A
cc
ur
ac
y
 
 
MTS
TCS
SCS
SUS
MUS
MDS
(d) (e) (f)
Figure 3: Online active learning results over the 6 domain adaptation tasks for sentiment classification,
with different cost ratio values ct/cs = {1, 2, 3, 4}.
after the whole budget being used. The average
results over 10 runs are reported in Figure 3.
We can see that: (1) With the increasing of
the labeling cost in the target domain, the perfor-
mance of all methods except SCS decreases since
the same budget can purchase fewer labeled in-
stances from the target domain. (2) The three cost-
sensitive methods (SUS, MUS, and MDS), which
consider the labeling cost when making query de-
cisions, are less sensitive to the cost ratios than the
MTS and TCS methods, whose performance de-
grades very quickly with the increasing of ct/cs.
(3) It is reasonable that when ct/cs is very big,
the SCS, which simply queries source instances,
produces the best performance. But the proposed
two cost-sensitive active learning methods, MUS
and MDS, are quite robust to the cost ratios across
a reasonable range of ct/cs values, and outper-
form both source-domain only and target-domain
only methods. When ct = cs, the proposed cost-
sensitive methods automatically favor target in-
stances and thus achieve similar performance as
TCS. When ct becomes much larger than cs, the
proposed cost-sensitive methods automatically ad-
just to favor cheaper source instances and maintain
their good performance.
5 Conclusion
In this paper, we investigated the online active do-
main adaptation problem in a novel but practical
setting where we assume labels can be acquired
with a lower cost in the source domain than in the
target domain. We proposed two multi-view on-
line active learning algorithms, MUS and MDS, to
address the proposed problem. The proposed al-
gorithms exploit multi-view active learning learn-
ing principles to measure the informativeness of
instances and select instances in a cost-sensitive
manner. Our empirical studies on the task of cross-
domain sentiment classification demonstrate the
efficacy of the proposed methods. This research
shows that a cheaper source domain can help on
reducing labeling cost for learning a good pre-
diction model in the related target domain, with
proper designed active learning algorithms.
8
References
S. Arora, E. Nyberg, and C. P. Rose?. Estimating
annotation cost for active learning in a multi-
annotator environment. In Proceedings of the
NAACL-HLT 2009 Workshop on Active Learn-
ing for Natural Language Processing, 2009.
S. Ben-David, J. Blitzer, K. Crammer, and
F. Pereira. Analysis of representations for do-
main adaptation. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2006.
J. Blitzer, D. Foster, and S. Kakade. Domain adap-
tation with coupled subspaces. In Proceedings
of the Conference on Artificial Intelligence and
Statistics (AISTATS), 2011.
A. Blum and T. Mitchell. Combining labeled and
unlabeled data with co-training. In Proceedings
of the Conference on Computational Learning
Theory (COLT), 1998.
A. Borodin and R. El-Yaniv. Online computation
and competitive analysis. Cambridge Univer-
sity Press, 1998.
N. Cesa-Bianchi, C. Gentile, and L. Zaniboni.
Worst-case analysis of selective sampling for
linear classification. Journal of Machine Learn-
ing Research (JMLR), 7:1205?1230, 2006.
Y. Chan and H. Ng. Domain adaptation with active
learning for word sense disambiguation. In Pro-
ceedings of the Annual Meeting of the Assoc. of
Computational Linguistics (ACL), 2007.
M. Chen, K. Weinberger, and J. Blitzer. Co-
training for domain adaptation. In Advances in
Neural Information Processing Systems (NIPS),
2011.
S. Dasgupta, A. T. Kalai, and C. Monteleoni.
Analysis of perceptron-based active learning.
Journal of Machine Learning Research (JMLR),
10:281?299, 2009.
P. Donmez and J. G. Carbonell. Proactive learn-
ing: cost-sensitive active learning with multi-
ple imperfect oracles. In Proceedings of the
ACM Conference on Information and knowl-
edge management (CIKM), 2008.
M. Dredze and K. Crammer. Online methods for
multi-domain learning and adaptation. In Pro-
ceedings of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP), 2008.
L. Li, X. Jin, S. Pan, and J. Sun. Multi-domain ac-
tive learning for text classification. In Proceed-
ings of the ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining
(KDD), 2012.
C. Monteleoni and M. Ka?a?ria?inen. Practical on-
line active learning for classification. In Pro-
ceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, Online Learn-
ing for Classification Workshop, 2007.
I. Muslea, S. Minton, and C. Knoblock. Selective
sampling with redundant views. In Proceedings
of the National Conference on Artificial Intelli-
gence (AAAI), 2000.
I. Muslea, S. Minton, and C. A. Knoblock. Ac-
tive + semi-supervised learning = robust multi-
view learning. In Proceedings of the In-
ternational Conference on Machine Learning
(ICML), 2002.
P. Prettenhofer and B. Stein. Cross-language text
classification using structural correspondence
learning. In Proceedings of the Annual Meeting
for the Association of Computational Linguis-
tics (ACL), 2010.
P. Rai, A. Saha, H. Daume? III, and S. Venkata-
subramanian. Domain adaptation meets active
learning. In Proceedings of the North American
Chapter of the Association for Computational
Linguistics (NAACL), 2010.
A. Saha, P. Rai, H. Daume? III, S. Venkata-
subramanian, and S. DuVall. Active super-
vised domain adaptation. In Proceedings of
the European Conference on Machine Learning
(ECML), 2011.
M. Sugiyama. Direct importance estimation with
model selection and its application to covariate
shift adaptation. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2007.
G. Tur. Co-adaptation: Adaptive co-training for
semi-supervised learning. In Proceedings of the
IEEE Inter. Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2009.
W. Wang and Z. Zhou. On multi-view active learn-
ing and the combination with semi-supervised
learning. In Proceedings of the international
conference on Machine learning (ICML), 2008.
W. Wang and Z. Zhou. Multi-view active learn-
ing in the non-realizable case. In Advances in
Neural Information Processing Systems (NIPS),
2010.
9
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 119?129,
Baltimore, Maryland USA, June 26-27 2014. c?2014 Association for Computational Linguistics
Distributed Word Representation Learning for Cross-Lingual
Dependency Parsing
Min Xiao and Yuhong Guo
Department of Computer and Information Sciences
Temple University
Philadelphia, PA 19122, USA
{minxiao,yuhong}@temple.edu
Abstract
This paper proposes to learn language-
independent word representations to ad-
dress cross-lingual dependency parsing,
which aims to predict the dependency
parsing trees for sentences in the target
language by training a dependency parser
with labeled sentences from a source lan-
guage. We first combine all sentences
from both languages to induce real-valued
distributed representation of words under
a deep neural network architecture, which
is expected to capture semantic similari-
ties of words not only within the same lan-
guage but also across different languages.
We then use the induced interlingual word
representation as augmenting features to
train a delexicalized dependency parser on
labeled sentences in the source language
and apply it to the target sentences. To in-
vestigate the effectiveness of the proposed
technique, extensive experiments are con-
ducted on cross-lingual dependency pars-
ing tasks with nine different languages.
The experimental results demonstrate the
superior cross-lingual generalizability of
the word representation induced by the
proposed approach, comparing to alterna-
tive comparison methods.
1 Introduction
With the rapid development of linguistic resources
and tools in multiple languages, it is very im-
portant to develop cross-lingual natural language
processing (NLP) systems. Cross-lingual depen-
dency parsing is the task of inferring dependency
trees for observed sentences in a target language
where there are few or no labeled training sen-
tences by using a dependency parser trained on
a large amount of sentences with annotated de-
pendency trees in a source language (Durrett et
al., 2012; McDonald et al., 2011; Zhao et al.,
2009). Cross-lingual dependency parsing is pop-
ularly studied in natural language processing area
as it can greatly reduce the expensive manual an-
notation effort in the target language by exploit-
ing the dependency annotations from a source lan-
guage (Durrett et al., 2012; McDonald et al., 2011;
Ta?ckstro?m et al., 2012).
One fundamental challenge of cross-lingual de-
pendency parsing stems from the word-level rep-
resentation divergence across languages. Since
sentences in different languages are expressed
using different vocabularies, if we train a de-
pendency parser on the word-level features of
sentences from a source language, it will fail
to parse the sentences in a different target lan-
guage. A variety of work in the literature has at-
tempted to bridge the word-level representation di-
vergence across languages. One intuitive method
delexicalizes the dependency parser by replac-
ing the language-specific word-level features with
language-independent features such as universal
part-of-speech tags (Petrov et al., 2012). With the
universal POS tag features, this method provides
a possible way to transfer dependency parsing in-
formation from the source language to the target
language and has demonstrated some good empir-
ical results (McDonald et al., 2011). However, the
number of universal POS tags is small, which lim-
its their discriminative capacity as input features
for dependency parsing. A few other works hence
propose to improve the delexicalized system by
learning more effective cross-lingual features such
as bilingual word clusters (Ta?ckstro?m et al., 2012)
and other interlingual representations (Durrett et
al., 2012).
In this paper, we propose to address cross-
lingual dependency parsing by learning distributed
interlingual word representations using a deep
neural network architecture. We first combine
all the sentences from two language domains and
119
build cross language word connections based on
Wikitionary, which works as a free bilingual dic-
tionary. Then by exploiting a deep learning archi-
tecture, we learn real-valued dense feature vectors
for the words in the given sentences as the high-
level interlingual representations, which capture
semantic similarities across languages. Finally, we
use the induced distributed word representation as
augmenting features to train a delexicalized de-
pendency parser on the annotated sentences in the
source language and applied it on the sentences in
the target language. In order to evaluate the pro-
posed cross-lingual learning technique, we con-
duct extensive experiments on eight cross-lingual
dependency parsing tasks with nine different lan-
guages. The experimental results demonstrate the
efficacy of the proposed approach in transferring
dependency parsers across languages, comparing
to other methods.
The remainder of the paper is organized as fol-
lows. Section 2 reviews related work. Section
3 describes the main approach of cross-lingual
word representation learning with deep neural net-
works and cross-lingual dependency parsing with
induced interlingual features. Section 4 presents
the empirical study on eight cross language depen-
dency parsing tasks. We then conclude the paper
in Section 5.
2 Related Work
Previous works developed in the literature have
tackled cross-lingual dependency parsing by us-
ing cross-lingual annotation projection methods,
multilingual model learning methods, and cross-
lingual representation learning methods.
Cross-lingual annotation projection methods
use parallel sentences to project the annotations
from the source language side to the target lan-
guage side and then train dependency parsers on
the target data with projected annotations (Hwa
et al., 2005; Liu et al., 2013; Smith and Eis-
ner, 2009; Zhao et al., 2009). For cross-lingual
annotation projection methods, both the word
alignment training step and the annotation pro-
jection step can introduce errors or noise. Thus
much work developed in the literature has fo-
cused on designing robust projection algorithms
such as graph-based projection with label prop-
agations (Das and Petrov, 2011), improving pro-
jection performance by using auxiliary resources
such as Wikipedia metadata (Kim and Lee, 2012)
or WordNet (Khapra et al., 2010), or boosting pro-
jection performance by heuristically modifying or
correcting the projected annotations (Hwa et al.,
2005; Kim et al., 2010). Some work has also
proposed to project the discrete dependency arc
instances instead of treebank as the training set
(Liu et al., 2013). Moreover, besides cross-lingual
dependency parsing, cross-lingual annotation pro-
jection methods have also demonstrated success
in various other sequence labeling tasks includ-
ing POS tagging (Das and Petrov, 2011; Yarowsky
and Ngai, 2001), relation extraction (Kim et al.,
2012), named entity recognition (Kim et al., 2010;
Kim and Lee, 2012), constituent syntax parsing
(Jiang et al., 2011), and word sense disambigua-
tion (Khapra et al., 2010).
Multilingual model learning methods train
cross-lingual dependency parsers with parameter
constraints obtained from parallel data (Liu et al.,
2013; Ganchev et al., 2009) or linguistic knowl-
edges (Naseem et al., 2010; Naseem et al., 2012).
Among these methods, some proposed to train
a joint dependency parsing system with parame-
ters shared across the dependency parsing models
in individual languages (Liu et al., 2013). Other
works used posterior regularization techniques to
encode the linguistic constraints in learning de-
pendency parsing models (Ganchev et al., 2009;
Naseem et al., 2010; Naseem et al., 2012). The
linguistic constraints may either come from man-
ually constructed universal dependency parsing
rules (Naseem et al., 2010) or manually specified
typological features (Naseem et al., 2012), or be
learned from parallel sentences (Ganchev et al.,
2009). Besides cross-lingual dependency parsing,
multilingual model learning methods have also
achieved good empirical results for other multilin-
gual NLP tasks, including named entity recogni-
tion (Burkett et al., 2010; Che et al., 2013; Wang
and Manning, 2014), syntactic parsing (Burkett
et al., 2010), semantic role labeling (Zhuang and
Zong, 2010; Kozhevnikov and Titov, 2012), and
word sense disambiguation (Guo and Diab, 2010).
Cross-lingual representation learning methods
induce language-independent features to bridge
the cross-lingual difference in the original word-
level representation space and build connections
across different languages. They train a depen-
dency parser in the induced representation space
by exploiting labeled data from the source lan-
guage and apply it in the target language (Dur-
120
rett et al., 2012; Ta?ckstro?m et al., 2012; Zhang
et al., 2012). A variety of auxiliary resources
have been used to induce interlingual features, in-
cluding bilingual lexicon (Durrett et al., 2012),
and unlabeled parallel sentences (Ta?ckstro?m et al.,
2013). Based on different learning mechanisms
(whether or not using labeled data) for induc-
ing language-independent features, cross-lingual
representation learning methods can be cate-
gorized into unsupervised representation learn-
ing (Ta?ckstro?m et al., 2013) and supervised
representation learning (Durrett et al., 2012).
The language-independent features include bilin-
gual word clusters (Ta?ckstro?m et al., 2012),
language-independent projection features (Durrett
et al., 2012), and automatically induced language-
independent POS tags (Zhang et al., 2012). Be-
sides cross-lingual dependency parsing, in the lit-
erature cross-lingual representation learning meth-
ods have also demonstrated efficacy in different
NLP applications such as cross language named
entity recognition (Ta?ckstro?m et al., 2012) and
cross language semantic role labeling (Titov and
Klementiev, 2012). Our work shares similarity
with these cross-lingual representation learning
methods on inducing new language-independent
features, but differs from them in that we learn
cross-lingual word embeddings. Though multilin-
gual word embeddings have been employed in the
literature, they are developed for other NLP tasks
such as cross-lingual sentiment analysis (Klemen-
tiev et al., 2012), and machine translation (Zou
et al., 2013). Moreover, the method in (Klemen-
tiev et al., 2012) requires parallel sentences with
observed word-level alignments, and the method
in (Zou et al., 2013) first learns language-specific
word embeddings in each language separately
and then transforms representations from one lan-
guage to another language with machine trans-
lation alignments, while we jointly learn cross-
lingual word embeddings in the two languages by
only exploiting a small set of bilingual word pairs.
From the perspective of applying deep networks
in natural language processing systems, there are
a number of works in the literature (Collobert and
Weston, 2008; Collobert et al., 2011; Henderson,
2004; Socher et al., 2011; Titov and Henderson,
2010; Turian et al., 2010). Socher et al. (2011) ap-
plied recursive autoencoders to address sentence-
level sentiment classification problems. Collobert
and Weston (2008) and Collobert et al. (2011)
employed a deep learning framework for jointly
multi-task learning and empirically evaluated it
with four NLP tasks, including part-of-speech tag-
ging, chunking, named entity recognition, and se-
mantic role labeling. Henderson (2004) proposed
discriminative training methods for learning a neu-
ral network statistical parser. Titov and Hender-
son (2010) extended the incremental sigmoid Be-
lief networks (Titov and Henderson, 2007) to a
generative latent variable model for dependency
parsing. Turian et al. (2010) employed neural net-
works to induce word representations for sequence
labeling tasks such as named entity recognition.
3 Cross-Lingual Dependency Parsing
with Word Representation Learning
In this work, we aim to tackle cross-lingual depen-
dency parsing by learning language-independent
distributed word representations with deep neural
networks. We first build connections across lan-
guages using free bilingual dictionaries. Then we
introduce the deep neural network framework for
cross-lingual word representation learning and de-
scribe how to employ the induced dense word em-
beddings for cross-lingual dependency parsing.
3.1 Building Cross Language Connections
To induce cross-lingual word representations, we
first need to build connections between the source
and target languages. In this work, we produce
such connections by finding cross-lingual word
pairs using the Wikitionary1, which works as free
bilingual dictionaries between language pairs.
Specifically, we first constructed a source lan-
guage dictionary with all words that appeared in
the sentences from the source language domain
and translate these words to the target language
using the Wikitionary. Then we filtered the pro-
duced word-to-word translations by dropping the
ones where either the same source language word
has multiple different word translations in the tar-
get language or the same target language word
corresponds to multiple different source language
words. We further dropped the word pairs where
the translated word in the target language does not
appear in the given sentences in the target lan-
guage domain. After the processing, we have a set
of one-to-one bilingual word pairs to build con-
nections between the two language domains. Fi-
nally, we built a unified bilingual vocabulary V
1http://en.wikitionary.org
121
Figure 1: The architecture of the deep neural net-
work for learning cross-lingual word representa-
tions. Each word w
i
from the training sample x
is mapped to an interlingual representation vector
R(w
i
) through the embedding matrix R.
with words from all sentences of the two language
domains. For each one-to-one bilingual word pair
we constructed, we assume the two words have
equivalent semantic meaning and map them to the
same entry in V . Next we will learn a distributed
vector representation for each entry of the bilin-
gual vocabulary V using deep neural networks.
By sharing the same representation vectors, the
constructed bilingual word pairs will serve as the
bridge across languages.
3.2 Interlingual Word Representation
Learning with Deep Neural Networks
Given the constructed bilingual vocabulary V with
v entries, we will learn a latent word embedding
matrix R ? Rk?v over the sentences in the two
language domains by using a deep neural network
model. This embedding matrix will map each
word w in the vocabulary V into a real valued rep-
resentation vector R(w) with length k. For each
bilingual pair of words that are mapped into the
same entry of V , they will be mapped into the
same vector in R as well. Following the strat-
egy of (Collobert et al., 2011), we construct a
simple two-class classification problem over the
given sentences. We use the sub-sentences with
fixed window size c constructed from the given
sentences in the two language domains as posi-
tive samples and construct the negative samples by
replacing the middle word of each positive sub-
sentence with a random word from V . We then
train a deep neural network for this two-class clas-
sification problem, while simultaneously learning
the latent embedding matrix R.
The deep neural network architecture is given
in Figure 1. The bottom layer of the deep archi-
tecture is the input layer, which takes a sequence
of word tokens, x = w
1
, w
2
, . . . , w
c
, with a fixed
window size c as the input instance. Then we map
each word w
i
in this sequence to an embedding
vector R(w
i
) by treating the bilingual embedding
matrix R as a look-up table. The embedding vec-
tors of the sequence of words x will be concate-
nated into a long vector R(x) ? Rck such that
R(x) = [R(w
1
);R(w
2
); . . . ;R(w
c
)]. (1)
R(x) will then be used as input for the hidden
layer above it. The deep neural network has mul-
tiple hidden layers. The first hidden layer applies
a nonlinear hyperbolic tangent activation function
over the linear transformation of its input vector
R(x), such that
H
1
(x) = tanh (W
1
?R(x) + b
1
) (2)
where W
1
? R
h
1
?ck is the weight parameter
matrix, b
1
? R
h
1 is the bias parameter vector,
H
1
(x) ? R
h
1 is the output vector, and h
1
is the
number of hidden units in the first hidden layer.
Similarly, each of the other hidden layers takes the
previous layer?s output as its input and performs
a nonlinear transformation to produce an output
vector. For example, for the i-th hidden layer, we
used H
i?1
(x) as its input and H
i
(x) as its output
such that
H
i
(x) = tanh (W
i
?H
i?1
(x) + b
i
) (3)
where W
i
? R
h
i
?h
i?1 is the weight parameter ma-
trix and b
i
is the bias parameter vector for the i-
th hidden layer; h
i
denotes the number of hidden
units of the i-th hidden layer.
Given t hidden layers, the output representation
of the last layer will then be used to generate a
final score value for the prediction task, such that
s(x) = ? ?H
t
(x) + u (4)
122
where ? ? Rht is the weight parameter vector and
u is the bias parameter for the output layer.
In summary, the model parameters of the deep
neural network architecture include the look-up ta-
ble R, the parameters {W
i
,b
i
}
t
i=1
for the hidden
layers, and the output layer parameters (?, u).
3.3 The Training Procedure
The model parameters of the deep network archi-
tecture are learned by training a two-class classifi-
cation model over the the constructed positive and
negative samples. Let D = {x
i
,
?
x
i
}
N
i=1
denote
the constructed training set, where x
i
is a positive
sample and ?x
i
is a negative sample constructed by
replacing the middle word of x
i
with a random
word from V . It is desirable for the model to pro-
duce an output score s(x
i
) that is much larger than
the score s(?x
i
) for each pair of training instances.
Thus we perform training to maximize the separa-
tion margins between the pairs of scores over pos-
itive and negative samples under a hinge loss; that
is we minimize the following training loss
J(D) =
1
N
N
?
i=1
max(0, 1? s(x
i
) + s(
?
x
i
)) (5)
We perform a random initialization over the
look-up table and weight model parameters, and
set all the bias model parameters to zeros. Then we
use a stochastic gradient descent (Bottou, 1991)
algorithm to perform optimization.
3.4 Cross-Lingual Dependency Parsing
The training of deep network model above will
produce a word embedding matrix R for all words
in the two language domains. Moreover, by hav-
ing each translated bilingual pair of words shar-
ing the same representation vector in R in the
training process, the embedding matrix R is ex-
pected to capture consistent and comparable se-
mantic meanings across languages, and provide a
language-independent and distributed representa-
tion for each word in the bilingual dictionary V .
Given R, for each sentence x = w
1
, w
2
, . . . , w
n
from the two language domains, we retrieved the
representation vector R(w
i
) for each word w
i
.
Moreover, we further delexicalized the sentence
by replacing the sequence of language-specific
words with a sequence of universal POS tags
(Petrov et al., 2012). Finally we train a delexical-
ized dependency parser on the labeled sentences
in the source language based on the universal POS
tag features and the learned distributed features.
and apply it to perform dependency parsing on the
sentences in the target language domain.
4 Experiments
We empirically evaluated the proposed cross-
lingual word representation learning for cross-
lingual dependency parsing. In this section, we
present the experimental setup and the results.
4.1 Dataset
We used the dataset from the CoNLL shared task
(Buchholz and Marsi, 2006; Nivre et al., 2007) for
cross-lingual dependency parsing. We conducted
experiments with the following nine languages:
English (EN), Danish (DA), German (DE), Greek
(EL), Spanish (ES), Italian (IT), Dutch (NL), Por-
tuguese (PT) and Swedish (SV). For each lan-
guage, there is a separate training set and a test set.
We used English, which usually has more labeled
resources, as the source language, while treat-
ing the others as target languages. We thus con-
structed eight cross-lingual dependency parsing
tasks (EN2DA, EN2DE, EN2EL, EN2ES, EN2IT,
EN2NL, EN2PT, EN2SV), one for each of the
eight target languages. For example, the task
EN2DA means that we used Danish (DA) as the
target language while using English (EN) as the
source language. For each cross language de-
pendency parsing task, we first performed repre-
sentation learning and then conducted dependency
parsing training and test.
In this dataset, each sentence is labeled with
gold standard part-of-speech tags. To produce
delexicalized cross-lingual dependency parsers,
we mapped these language-specific part-of-speech
tags into twelve universal POS tags (Petrov et al.,
2012): ADJ (adjectives), ADP (prepositions or
postpositions), ADV (adverbs), CONJ (conjunc-
tions), DET (determiners), NOUN (nouns), NUM
(numerals), PRON (pronouns), PRT (particles),
PUNC (punctuation marks), VERB (verbs) and X
(for others).
4.2 Representation Learning
For each language pair, we produced a set of one-
to-one bilingual word pairs using Wikitionary to
build cross language connections. The numbers
of bilingual word pairs produced for all the eight
language pairs and the numbers of words in each
language are given in Table 1.
123
Table 1: The number of words in each language and the number of selected bilingual word pairs for each
of the eight language pairs.
Language Pairs # Source Words # Target Words # Bilingual Word Pairs
English vs Danish 26599 17934 1140
English vs Dutch 26599 27829 2976
English vs German 26599 69336 1905
English vs Greek 26599 13318 869
English vs Italian 26599 13523 2347
English vs Portuguese 26599 27782 2408
English vs Spanish 26599 16465 2910
English vs Swedish 26599 19072 1779
Table 2: The feature templates used for the cross-lingual dependency parsing. dir denotes the direction
of the dependency relationship, which has two values {left, right}. dist denotes the distance between
the head word and the dependent word, which has five values {1, 2, 3-5, 6-10, 11+}.
Feature Template Feature Description
UPOS(w
h
) the head word?s universal POS tag
UPOS(w
d
) the dependent word?s universal POS tag
UPOS(w
h
, w
d
) the universal POS tag pair of the head and dependent word
R(w
h
) the head word?s distributed representation
R(w
d
) the dependent word?s distributed representation
dir&UPOS conjunction features related to the dependency direction
dist&UPOS conjunction features related to the dependency distance
dir&dist&UPOS conjunction features related to the dependency direction and distance
To perform distributed cross-lingual representa-
tion learning using the proposed deep network ar-
chitecture, we first constructed the two-class train-
ing dataset from all the sentences (training and
test sentences) of the two language domains. This
requires the creation of sub-sentences with fixed
window size c from the given sentences. We used
window size c = 5 in the experiments. For ex-
ample, for a given sentence ?I visited New York
.? , we can produce a number of sub-sentences,
including ?<PAD> <S> I visited New?, ?<S> I
visited New York?, ?I visited New York .?, ?vis-
ited New York . </S>?, and ?New York . </S>
<PAD>?, where <PAD> is special token to fill
the length requirement. Negative samples are con-
structed by simply replace the middle word of each
sub-sentence with a random word.
With the constructed training data, we then per-
formed training over the deep neural network. We
used 3 hidden layers with 100 hidden units in
each layer, considering the model capacity and the
training effort. The dimension k of the embedding
word vectors in R is set as 200.
4.3 Cross-lingual Dependency Parsing
We used the MSTParser (McDonald et al., 2005a;
McDonald et al., 2005b) as the basic dependency
parsing model. MSTParser uses spanning tree
algorithms to seek for the candidate dependency
trees and employs an online large margin train-
ing optimization algorithm. MSTParser is widely
used in the literature for dependency parsing tasks
and has demonstrated good empirical results in the
CoNLL shared tasks on multilingual dependency
parsing (Buchholz and Marsi, 2006; Nivre et al.,
2007). For this dependency parsing model, there
are a few parameters to be set: the number of max-
imum iterations for the perceptron training, and
the number of best-k dependency tree candidates.
We set the number of iterations to be 10 and only
considered the best-1 dependency tree candidate.
For the proposed cross-lingual dependency
parsing approach, we used both the delexi-
124
Table 3: Test performance in terms of UAS (unlabeled attachment score) on the eight cross-lingual
dependency parsing tasks. ? denotes the improvements of each method over the Baseline method.
Tasks Baseline Proj ? Proposed ? X-lingual
EN2DA 36.53 41.25 4.72 42.56 6.03 38.70
EN2DE 46.24 49.15 2.91 49.54 3.30 50.70
EN2EL 61.53 62.36 0.83 62.96 1.43 63.00
EN2ES 52.05 54.54 2.49 55.72 3.67 62.90
EN2IT 56.37 57.71 1.34 59.05 2.68 68.80
EN2NL 61.96 64.41 2.45 65.13 3.17 54.30
EN2PT 68.68 71.47 2.79 72.38 3.70 71.00
EN2SV 57.79 60.99 3.20 61.88 4.09 56.90
Average 55.14 57.74 2.60 58.90 3.51 58.29
calized universal POS tag based features and
the language-independent word features produced
from the deep learning as input features for the
MSTParser. The set of universal POS tag based
feature templates is given in Table 2. For each
dependency relationship between a head word w
h
and a dependent word w
d
, a set of features can
be produced from the feature templates in Ta-
ble 2, which can be further augmented by R(w
h
)
and R(w
d
). We compared our proposed approach
(Proposed) with three other methods, Baseline,
Proj and X-lingual. The Baseline method uses a
delexicalized MSTParser based only on the uni-
versal POS tag features. The Proj method is devel-
oped in (Durrett et al., 2012), which uses a bilin-
gual dictionary to learn cross-lingual features and
then uses them as augmenting features to train a
delexicalized MSTParser. The X-lingual method
uses unlabeled parallel sentences to learn cross-
lingual word clusters and used them as augment-
ing features to train a delexicalized MSTParser
(Ta?ckstro?m et al., 2012). All parsers except X-
lingual are trained on the labeled sentences in the
source language domain and tested on the test
sentences in the target language domain in the
given dataset. The performance is measured using
the standard unlabeled attachment score (UAS).
The X-lingual method uses different auxiliary re-
sources (parallel sentences), and we hence directly
cited the results reported in (Ta?ckstro?m et al.,
2012) on the same dataset.
4.4 Results and Discussions
We reported the empirical comparison results in
terms of unlabeled attachment score (UAS) in Ta-
ble 3. We can see that the Baseline method per-
Table 4: Statistic differences. For each task, we
report the percentage of sentences in the test data
from the target language which share the same se-
quence of universal POS tags with some sentences
in the source language but with different depen-
dency trees.
Target Language Sentence Difference
Danish 0.31%
Dutch 1.81%
German 1.40%
Greek 1.20%
Italian 2.40%
Portuguese 1.04%
Spanish 0.97%
Swedish 2.31%
forms poorly across all the tasks. The average un-
labeled attachment score for this approach across
all the eight tasks is very low (about 55.14), which
suggests that the twelve universal POS tags are far
from enough to produce a good cross-lingual de-
pendency parser. Considering the small number
of universal POS tags, its limited discriminative
capacity as input features for dependency pars-
ing is understandable. To further verify this, we
calculated the percentage of sentences in the test
data which share the same sequence of universal
POS tags with a training sentence in the source
language but have different dependency parsing
structures. The values for the eight tasks are pre-
sented in Table 4. The non-trivial values reported
verified the universal POS tags? drawback on lack-
ing discriminative capacity.
By relexicalizing the delexicalized MSTParser
125
via augmenting the POS tag sequences with
learned interlingual features, both the Proj method
and the proposed method overcome the draw-
back of using solely universal POS tags and pro-
duce significant improvements over the Baseline
method across all the tasks. Moreover, the pro-
posed method consistently outperforms both Base-
line and Proj for all the eight tasks. By exploit-
ing only free bilingual dictionaries, the proposed
method achieves similar average performance to
the X-lingual method which requires additional
parallel sentences. All these results demonstrated
the efficacy of our word representation learning
method for cross-lingual dependency parsing.
4.5 Impact of Labeled Training Data in
Target Language
In the experiments above, all the labeled sen-
tences for dependency parsing training are from
the source language. We wonder how much bene-
fit we can get if there are a small number of labeled
sentences in the target language as well. To answer
this question, we conducted experiments by using
a small number (?
t
) of labeled sentences in the
target language domain together with the labeled
sentences in the source language domain to train
cross-lingual dependency parsers. Again the per-
formance of the parsers are evaluated on the test
sentences in the target language. We tested a few
different ?
t
values with ?
t
? {500, 1000, 1500}.
We reported the unlabeled attachment score for all
the eight cross-lingual dependency parsing tasks
in Figure 2. We can see that the Baseline method
still performs poorly across the range of different
setting for all the eight tasks. The Proj method
and the proposed method again consistently out-
perform the baseline method across all the tasks,
while the proposed method achieves the best re-
sults across all the eight tasks.
4.6 Impact of the Number of Bilingual Word
Pairs
For the eight language pairs, we have reported
the numbers of words in each language domain
and the numbers of selected bilingual word pairs
in Table 1. Next we investigated how the num-
ber of word pairs affects the performance of the
proposed cross-lingual dependency parsing. With
the selected full set of bilingual word pairs in
Table 1, we random selected m% of them with
m ? {50, 75, 100} to conduct experiments. Note
when m = 50, we only used 435 word pairs for
the EN2EL (English vs. Greek) task, which is
1.6% of the number of source words and 3.3% of
the number of target words. The results are re-
ported in Figure 3. We can see that by reducing the
number of bilingual word pairs, the performance
of the proposed cross-lingual dependency parsing
method degrades on all tasks. This is reasonable
since the word pairs serve as the pivots for learn-
ing cross-lingual word embeddings. Nevertheless,
by preserving 75% of the selected word pairs, the
proposed approach can still outperform the Proj
method across all the tasks. Even with only 50%
of the word pairs, our method still outperforms
the Proj method on most tasks. These results sug-
gest that the proposed cross-lingual word embed-
ding method only requires a reasonable amount of
bilingual word pairs to effectively transfer a de-
pendency parser from the source language to the
target language.
EN2DA EN2DE EN2EL EN2ES EN2IT EN2NL EN2PT EN2SV
40
45
50
55
60
65
70
75
UAS vs # of Bilingual Word Pairs
Task
UA
S
 
 
Proj Proposed?50% Proposed?75% Proposed?100%
Figure 3: Test performance in terms of UAS (unla-
beled attachment score) in the target language with
different numbers of bilingual word pairs.
5 Conclusion
In this paper, we proposed to automatically learn
language-independent features within a deep neu-
ral network architecture to address cross-lingual
dependency parsing problems. We first con-
structed a set of bilingual word pairs with Wiki-
tionary, which serve as the pivots in the bilingual
vocabulary for building connections across lan-
guages. We then conducted distributed word rep-
resentation learning by training a constructed aux-
iliary classifier using deep neural networks, which
induced a real-valued embedding vector for each
word of the bilingual vocabulary to capture con-
126
0 500 1000 1500
35
36
37
38
39
40
41
42
43
EN2DA
Labeled target training data
U
A
S
 
 
Baseline
Proj
Proposed
0 500 1000 1500
45
46
47
48
49
50
EN2DE
Labeled target training data
U
A
S
 
 
Baseline
Proj
Proposed
0 500 1000 1500
60
60.5
61
61.5
62
62.5
63
63.5
64
EN2EL
Labeled target training data
U
A
S
 
 
Baseline
Proj
Proposed
0 500 1000 1500
51
52
53
54
55
56
EN2ES
Labeled target training data
U
A
S
 
 
Baseline
Proj
Proposed
0 500 1000 1500
55
56
57
58
59
60
EN2IT
Labeled target training data
U
A
S
 
 
Baseline
Proj
Proposed
0 500 1000 1500
60
61
62
63
64
65
66
EN2NL
Labeled target training data
U
A
S
 
 
Baseline
Proj
Proposed
0 500 1000 1500
67
68
69
70
71
72
73
EN2PT
Labeled target training data
U
A
S
 
 
Baseline
Proj
Proposed
0 500 1000 1500
56
57
58
59
60
61
62
EN2SV
Labeled target training data
U
A
S
 
 
Baseline
Proj
Proposed
Figure 2: Unlabeled attachment score (UAS) on the test sentences in the target language by using differ-
ent number of additional labeled training sentences in the target language.
sistent semantic similarities for words in the two
language domains. The distributed word embed-
ding vectors were then used to augment the uni-
versal POS tags to train cross-lingual dependency
parsers. We empirically evaluated the proposed
method on eight cross-lingual dependency parsing
tasks between eight language pairs. The experi-
mental results demonstrated the effectiveness of
the proposed method, comparing to other cross-
lingual dependency parsing methods.
References
L. Bottou. 1991. Stochastic gradient learning in neural
networks. In Proceedings of Neuro-N??mes.
S. Buchholz and E. Marsi. 2006. Conll-x shared task
on multilingual dependency parsing. In Proceedings
of the Conference on Computational Natural Lan-
guage Learning (CoNLL).
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
Learning better monolingual models with unanno-
tated bilingual text. In Proceedings of the Confer-
ence on Computational Natural Language Learning
(CoNLL).
W. Che, M. Wang, C. Manning, and T. Liu. 2013.
Named entity recognition with bilingual constraints.
In Proceedings of Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL).
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
127
networks with multitask learning. In Proceedings of
the International Conference on Machine Learning
(ICML).
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal
of Machine Learning Research (JMLR), 12:2493?
2537.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projec-
tions. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies (ACL).
G. Durrett, A. Pauls, and D. Klein. 2012. Syntactic
transfer using a bilingual lexicon. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL).
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Proceedings of the Joint Conference
of the Annual Meeting of the ACL and the Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP (ACL-IJCNLP).
W. Guo and M. Diab. 2010. Combining orthogonal
monolingual and multilingual sources of evidence
for all words wsd. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
J. Henderson. 2004. Discriminative training of a neu-
ral network statistical parser. In Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural Language
Engineering, 11:11?311.
W. Jiang, Q. Liu, and Y. Lu?. 2011. Relaxed cross-
lingual projection of constituent syntax. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
M. Khapra, S. Sohoney, A. Kulkarni, and P. Bhat-
tacharyya. 2010. Value for money: Balancing an-
notation effort, lexicon building and accuracy for
multilingual wsd. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING).
S. Kim and G. Lee. 2012. A graph-based cross-
lingual projection approach for weakly supervised
relation extraction. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
S. Kim, M. Jeong, J. Lee, and G. Lee. 2010. A cross-
lingual annotation projection approach for relation
detection. In Proceedings of the International Con-
ference on Computational Linguistics (COLING).
S. Kim, K. Toutanova, and H. Yu. 2012. Multilin-
gual named entity recognition using parallel data
and metadata from wikipedia. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
A. Klementiev, I. Titov, and B. Bhattarai. 2012. In-
ducing crosslingual distributed representations of
words. In Proceedings of the International Confer-
ence on Computational Linguistics (COLING).
M. Kozhevnikov and I. Titov. 2012. Cross-lingual
bootstrapping for semantic role labeling. In Pro-
ceedings of the NIPS Workshop on Crosslingual
Technologies (XLITE).
K. Liu, Y. Lu?, W. Jiang, and Q. Liu. 2013. Bilingually-
guided monolingual dependency parsing grammar
induction. In Proceedings of the Conference on An-
nual Meeting of the Association for Computational
Linguistics (ACL).
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the Annual Meeting on Association
for Computational Linguistics (ACL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005b. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT-EMNLP).
R. McDonald, S. Petrov, and K. Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
T. Naseem, H. Chen, R. Barzilay, and M. Johnson.
2010. Using universal linguistic knowledge to guide
grammar induction. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
T. Naseem, R. Barzilay, and A. Globerson. 2012. Se-
lective sharing for multilingual dependency parsing.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007
shared task on dependency parsing. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
S. Petrov, D. Das, and R. McDonald. 2012. A univer-
sal part-of-speech tagset. In Proceedings of the In-
ternational Conference on Language Resources and
Evaluation (LREC).
128
D. Smith and J. Eisner. 2009. Parser adaptation and
projection with quasi-synchronous grammar fea-
tures. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
R. Socher, J. Pennington, E. Huang, A. Ng, and
C. Manning. 2011. Semi-supervised recursive au-
toencoders for predicting sentiment distributions. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
O. Ta?ckstro?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL).
O. Ta?ckstro?m, R. McDonald, and J. Nivre. 2013.
Target language adaptation of discriminative trans-
fer parsers. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL).
I. Titov and J. Henderson. 2007. Constituent parsing
with incremental sigmoid belief networks. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
I. Titov and J. Henderson. 2010. A latent variable
model for generative dependency parsing. In Pro-
ceedings of the International Conference on Parsing
Technology (IWPT).
I. Titov and A. Klementiev. 2012. Crosslingual induc-
tion of semantic roles. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
M. Wang and C. Manning. 2014. Cross-
lingual pseudo-projected expectation regularization
for weakly supervised learning. Transactions of the
Association for Computational Linguistics (TACL),
2:55?66.
D. Yarowsky and G. Ngai. 2001. Inducing multilin-
gual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In Proceedings of the
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics on Language
Technologies (NAACL).
Y. Zhang, R. Reichart, R. Barzilay, and A. Glober-
son. 2012. Learning to map into a universal pos
tagset. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL).
H. Zhao, Y. Song, C. Kit, and G. Zhou. 2009. Cross
language dependency parsing using a bilingual lex-
icon. In Proceedings of the Joint Conference of the
Annual Meeting of the ACL and the International
Joint Conference on Natural Language Processing
of the AFNLP (ACL-IJCNLP).
T. Zhuang and C. Zong. 2010. Joint inference for bilin-
gual semantic role labeling. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
W. Zou, R. Socher, D. Cer, and C. Manning. 2013.
Bilingual word embeddings for phrase-based ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
129
