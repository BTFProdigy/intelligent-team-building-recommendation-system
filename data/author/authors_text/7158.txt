Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 26?27,
Vancouver, October 2005.
Japanese Speech Understanding Using Grammar Specialization
Manny Rayner, Nikos Chatzichrisafis, Pierrette Bouillon
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
mrayner@riacs.edu
{Pierrette.Bouillon,Nikolaos.Chatzichrisafis}@issco.unige.ch
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Beth Ann Hockey
UCSC/NASA Ames Research Center
Moffet Field, CA 94035
bahockey@riacs.edu
Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Marianne.Santaholma@eti.unige.ch
Marianne.Starlander@eti.unige.ch
The most common speech understanding archi-
tecture for spoken dialogue systems is a combination
of speech recognition based on a class N-gram lan-
guage model, and robust parsing. For many types
of applications, however, grammar-based recogni-
tion can offer concrete advantages. Training a
good class N-gram language model requires sub-
stantial quantities of corpus data, which is gen-
erally not available at the start of a new project.
Head-to-head comparisons of class N-gram/robust
and grammar-based systems also suggest that users
who are familiar with system coverage get better re-
sults from grammar-based architectures (Knight et
al., 2001). As a consequence, deployed spoken dia-
logue systems for real-world applications frequently
use grammar-based methods. This is particularly
the case for speech translation systems. Although
leading research systems like Verbmobil and NE-
SPOLE! (Wahlster, 2000; Lavie et al, 2001) usu-
ally employ complex architectures combining sta-
tistical and rule-based methods, successful practical
examples like Phraselator and S-MINDS (Phrasela-
tor, 2005; Sehda, 2005) are typically phrasal trans-
lators with grammar-based recognizers.
Voice recognition platforms like the Nuance
Toolkit provide CFG-based languages for writing
grammar-based language models (GLMs), but it is
challenging to develop and maintain grammars con-
sisting of large sets of ad hoc phrase-structure rules.
For this reason, there has been considerable inter-
est in developing systems that permit language mod-
els be specified in higher-level formalisms, normally
some kind of unification grammar (UG), and then
compile these grammars down to the low-level plat-
form formalisms. A prominent early example of this
approach is the Gemini system (Moore, 1998).
Gemini raises the level of abstraction signifi-
cantly, but still assumes that the grammars will be
domain-dependent. In the Open Source REGULUS
project (Regulus, 2005; Rayner et al, 2003), we
have taken a further step in the direction of increased
abstraction, and derive all recognizers from a sin-
gle linguistically motivated UG. This derivation pro-
cedure starts with a large, application-independent
UG for a language. An application-specific UG is
then derived using an Explanation Based Learning
(EBL) specialization technique. This corpus-based
specialization process is parameterized by the train-
ing corpus and operationality criteria. The training
corpus, which can be relatively small, consists of ex-
amples of utterances that should be recognized by
the target application. The sentences of the corpus
are parsed using the general grammar, then those
parses are partitioned into phrases based on the op-
erationality criteria. Each phrase defined by the
operationality criteria is flattened, producing rules
of a phrasal grammar for the application domain.
This application-specific UG is then compiled into
26
a CFG, formatted to be compatible with the Nuance
recognition platform. The CFG is compiled into the
runtime recognizer using Nuance tools.
Previously, the REGULUS grammar specialization
programme has only been implemented for English.
In this demo, we will show how we can apply the
same methodology to Japanese. Japanese is struc-
turally a very different language from English, so it
is by no means obvious that methods which work
for English will be applicable in this new context:
in fact, they appear to work very well. We will
demo the grammars and resulting recognizers in the
context of Japanese ? English and Japanese ?
French versions of the Open Source MedSLT medi-
cal speech translation system (Bouillon et al, 2005;
MedSLT, 2005).
The generic problem to be solved when building
any sort of recognition grammar is that syntax alone
is insufficiently constraining; many of the real con-
straints in a given domain and use situation tend to
be semantic and pragmatic in nature. The challenge
is thus to include enough non-syntactic constraints
in the grammar to create a language model that can
support reliable domain-specific speech recognition:
we sketch our solution for Japanese.
The basic structure of our current general
Japanese grammar is as follows. There are four main
groups of rules, covering NP, PP, VP and CLAUSE
structure respectively. The NP and PP rules each as-
sign a sortal type to the head constituent, based on
the domain-specific sortal constraints defined in the
lexicon. VP rules define the complement structure
of each syntactic class of verb, again making use of
the sortal features. There are also rules that allow
a VP to combine with optional adjuncts, and rules
which allow null constituents, in particular null sub-
jects and objects. Finally, clause-level rules form a
clause out of a VP, an optional subject and optional
adjuncts. The sortal features constrain the subject
and the complements combining with a verb, but the
lack of constraints on null constituents and optional
adjuncts still means that the grammar is very loose.
The grammar specialization mechanism flattens the
grammar into a set of much simpler structures, elim-
inating the VP level and only permitting specific pat-
terns of null constituents and adjuncts licenced by
the training corpus.
We will demo several different versions of the
Japanese-input medical speech translation system,
differing with respect to the target language and
the recognition architecture used. In particular, we
will show a) that versions based on the specialized
Japanese grammar offer fast and accurate recogni-
tion on utterances within the intended coverage of
the system (Word Error Rate around 5%, speed un-
der 0.1?RT), b) that versions based on the original
general Japanese grammar are much less accurate
and more than an order of magnitude slower.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: a case
study. In Proceedings of Eurospeech 2001, pages
1779?1782, Aalborg, Denmark.
A. Lavie, C. Langley, A. Waibel, F. Pianesi, G. Lazzari,
P. Coletti, L. Taddei, and F. Balducci. 2001. Ar-
chitecture and design considerations in NESPOLE!:
a speech translation system for e-commerce applica-
tions. In Proceedings of HLT: Human Language Tech-
nology Conference, San Diego, California.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 9 June 2005.
R. Moore. 1998. Using natural language knowledge
sources in speech recognition. In Proceedings of the
NATO Advanced Studies Institute.
Phraselator, 2005. http://www.phraselator.com/. As of 9
June 2005.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
Regulus, 2005. http://sourceforge.net/projects/regulus/.
As of 9 June 2005.
Sehda, 2005. http://www.sehda.com/. As of 9 June 2005.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
27
Evaluating Task Performance for a Unidirectional Controlled Language 
Medical Speech Translation System 
 
 
Nikos Chatzichrisafis, Pierrette Bouillon, Manny Rayner, Marianne Santaholma, 
Marianne Starlander 
University of Geneva, TIM/ISSCO 
40 bvd du Pont-d'Arve, CH-1211 Geneva 4, Switzerland 
 
Nikos.Chatzichrisafis@vozZup.com, Pierrette.Bouillon@issco.unige.ch, 
Emmanuel.Rayner@issco.unige.ch, Marianne.Santaholma@eti.unige.ch, 
Marianne.Starlander@eti.unige.ch  
 
Beth Ann Hockey 
UCSC 
NASA Ames Research Center 
Moffett Field, CA 94035 
bahockey@email.arc.nasa.gov 
 
  
Abstract 
We present a task-level evaluation of the 
French to English version of MedSLT, a 
medium-vocabulary unidirectional con-
trolled language medical speech transla-
tion system designed for doctor-patient 
diagnosis interviews. Our main goal was 
to establish task performance levels of 
novice users and compare them to expert 
users. Tests were carried out on eight 
medical students with no previous expo-
sure to the system, with each student us-
ing the system for a total of three 
sessions. By the end of the third session, 
all the students were able to use the sys-
tem confidently, with an average task 
completion time of about 4 minutes. 
1 Introduction 
Medical applications have emerged as one of the 
most promising application areas for spoken lan-
guage translation, but there is still little agreement 
about the question of architectures. There are in 
particular two architectural dimensions which we 
will address: general processing strategy (statistical 
or grammar-based), and top-level translation func-
tionality (unidirectional or bidirectional transla-
tion). Given the current state of the art in 
recognition and machine translation technology, 
what is the most appropriate combination of 
choices along these two dimensions? 
Reflecting current trends, a common approach 
for speech translation systems is the statistical one. 
Statistical translation systems rely on parallel cor-
pora of source and target language texts, from 
which a translation model is trained. However, this 
is not necessarily the best alternative in safety-
critical medical applications. Anecdotally, many 
doctors express reluctance to trust a translation 
device whose output is not readily predictable, and 
most of the speech translation systems which have 
reached the stage of field testing rely on various 
types of grammar-based recognition and rule-based 
translation (Phraselator, 2006; S-MINDS, 2006; 
MedBridge, 2006). Even though statistical systems 
exhibit many desirable properties (purely data-
driven, domain independence), grammar-based 
systems utilizing probabilistic context-free gram-
mar tuning appear to deliver better results when 
training data is sparse (Rayner et al, 2005a). 
One drawback of grammar-based systems is that 
out-of-coverage utterances will be neither recog-
nized nor translated, an objection that critics have 
sometimes painted as decisive. It is by no means 
obvious, however, that restricted coverage is such 
a serious problem. In text processing, work on sev-
eral generations of controlled language systems has 
developed a range of techniques for keeping users 
within the bounds of system coverage (Kittredge, 
2003; Mitamura, 1999). If these techniques work 
for text processing, it is surely not inconceivable 
that variants of them will be equally successful for 
spoken language applications. Users are usually 
able to adapt to a controlled language system given 
enough time. The critical questions are how to 
provide efficient support to guide them towards the 
system's coverage, and how much time they will 
then need before they have acclimatized. 
With regard to top-level translation functional-
ity, the choice is between unidirectional and bidi-
rectional systems. Bidirectional systems are 
certainly possible today1, but the arguments in fa-
vor of them are not as clear-cut as might first ap-
pear. Ceteris paribus, doctors would certainly 
prefer bidirectional systems; in particular, medical 
students are trained to conduct examination dia-
logues using ?open questions? (WH-questions), 
and to avoid leading the patient by asking YN-
questions. 
The problem with a bidirectional system is, 
however, that open questions only really work well 
if the system can reliably handle a broad spectrum 
of replies from the patients, which is over-
optimistic given the current state of the art. In prac-
tice, the system's coverage is always more or less 
restricted, and some experimentation is required 
before the user can understand what language it is 
capable of handling. A doctor, who uses the system 
regularly, will acquire the necessary familiarity. 
The same might be true for a few patients, if spe-
cial circumstances mean that they encounter 
speech translation applications reasonably fre-
quently. Most patients, however, will have had no 
previous exposure to the system, and may be un-
willing to use a type of technology which they 
have trouble understanding.  
A unidirectional system, in which the doctor 
mostly asks YN-questions, will never be ideal. If, 
                                                          
1
 For example, the S-MINDS system (S-MINDS, 2006) 
offers bidirectional translation. 
however, the doctor can become proficient in using 
it, it may still be very much better than the alterna-
tive of no translation assistance at all.  
To summarize, today?s technology definitely 
lets us build unidirectional grammar-based medical 
speech translation systems which work for regular 
users who have had time to adapt to their limita-
tions. While bidirectional systems are possible, the 
case for them is less obvious, since users on the 
patient side may not in practice be able to use them 
effectively. 
In this paper, we will empirically investigate the 
ability of medical students to adapt to the coverage 
of unidirectional spoken language translation sys-
tem. We report a series of experiments, carried out 
using a French to English speech translation sys-
tem, in which medical students with no previous 
experience to the system were asked to use it to 
carry out a series of verbal examinations on sub-
jects who were simulating the symptoms of various 
types of medical conditions. Evaluation will be 
focused on usability. We primarily want to know 
how quickly subjects learn to use the system, and 
how their performance compares to that of expert 
users. 
2 The MedSLT system 
MedSLT (MedSLT, 2005; Bouillon et al, 2005) 
is a unidirectional, grammar-based medical speech 
translation system intended for use in doctor-
patient diagnosis dialogues. The system is built on 
top of Regulus (Regulus, 2006), an Open Source 
platform for developing grammar-based speech 
applications. Regulus supports rapid construction 
of complex grammar-based language models using 
an example-based method (Rayner et al, 2003; 
Rayner et al, 2006), which extracts most of the 
structure of the model from a general linguistically 
motivated resource grammar. Regulus-based rec-
ognizers are reasonably easy to maintain, and 
grammar structure is shared automatically across 
different subdomains. Resource grammars are now 
available for several languages, including English, 
Japanese (Rayner et al, 2005b), French (Bouillon 
et al, 2006) and Spanish. 
MedSLT includes a help module, whose purpose 
is to add robustness to the system and guide the 
user towards the supported coverage. The help 
module uses a second backup recognizer, equipped 
with a statistical language model; it matches the 
results from this second recognizer against a cor-
pus of utterances, which are within system cover-
age and have already been judged to give correct 
translations. In previous studies (Rayner et al, 
2005a; Starlander et al, 2005), we showed that the 
grammar-based recognizer performs much better 
than the statistical one on in-coverage utterances, 
and rather worse on out-of-coverage ones. We also 
found that having the help module available ap-
proximately doubled the speed at which subjects 
learned to use the system, measured as the average 
difference in semantic error rate between the re-
sults for their first quarter-session and their last 
quarter-session. It is also possible to recover from 
recognition errors by selecting one of the displayed 
help sentences; in the cited studies, we found that 
this increased the number of acceptably processed 
utterances by about 10%. 
The version of MedSLT used for the experi-
ments described in the present paper was config-
ured to translate from spoken French into spoken 
English in the headache subdomain. Coverage is 
based on standard headache-related examination 
questions obtained from a doctor, and consists 
mostly of yes/no questions. WH-questions and el-
liptical constructions are also supported. A typical 
short session with MedSLT might be as follows: 
- is the pain in the side of the head? 
- does the pain radiate to the neck? 
- to the jaw? 
- do you usually have headaches in the morn-
ing ?  
The recognizer?s vocabulary is about 1000 sur-
face words; on in-grammar material, Word Error 
Rate is about 8% and semantic error rate (per ut-
terance) about 10% (Bouillon et al, 2006). Both 
the main grammar-based recognizer and the statis-
tical recognizer used by the help system were 
trained from the same corpus of about 975 utter-
ances. Help sentences were also taken from this 
corpus. 
3 Experimental Setup 
In previous work, we have shown how to build a 
robust and extendable speech translation system. 
We have focused on performance metrics defined 
in terms of recognition and translation quality, and 
tested the system on na?ve users without any medi-
cal background (Bouillon et al, 2005; Rayner et 
al., 2005a; Starlander et al, 2005). 
In this paper, our primary goal was rather to fo-
cus on task performance evaluation using plausible 
potential users. The basic methodology used is 
common in evaluating usability in software sys-
tems in general, and spoken language systems in 
particular (Cohen et. al 2000). We defined a simu-
lated situation, where a French-speaking doctor 
was required to carry out a verbal examination of 
an English-speaking patient who claimed to be suf-
fering from a headache, using the MedSLT system 
to translate all their questions. The patients were 
played by members of the development team, who 
had been trained to answer questions consistently 
with the symptoms of different medical conditions 
which could cause headaches. We recruited eight 
native French-speaking medical students to play 
the part of the doctor. All of the students had com-
pleted at least four years of medical school; five of 
them were already familiar with the symptoms of 
different types of headaches, and were experienced 
in real diagnosis situations. 
The experiment was designed to study how well 
users were able to perform the task using the 
MedSLT system. In particular, we wished to de-
termine how quickly they could adapt to the re-
stricted language and limited coverage of the 
system. As a comparison point, representing near-
perfect performance, we also carried out the same 
test on two developers who had been active in im-
plementing the system, and were familiar with its 
coverage. 
Since it seemed reasonable to assume that most 
users would not interact with the system on a daily 
basis, we conducted testing in three sessions, with 
an interval of two days between each session. At 
the beginning of the first session, subjects were 
given a standardized 10-minute introduction to the 
system. This consisted of instruction on how to set 
up the microphone, a detailed description of the 
MedSLT push-to-talk interface, and a video clip 
showing the system in action. At the end of the 
presentation, the subject was given four sample 
sentences to get familiar with the system. 
After the training was completed, subjects were 
asked to play the part of a doctor, and conduct an 
examination through the system. Their task was to 
identify the headache-related condition simulated 
by the ?patient?, out of nine possible conditions. 
Subjects were given definitions of the simulated 
headache types, which included conceptual infor-
mation about location, duration, frequency, onset 
and possible other symptoms the particular type of 
headache might exhibit. 
Subjects were instructed to signal the conclusion 
of their examination when they were sure about the 
type of simulated headache. The time required to 
reach a conclusion was noted in the experiment 
protocols by the experiment supervisor. 
The subjects repeated the same diagnosis task on 
different predetermined sets of simulated condi-
tions during the second and third sessions. The ses-
sions were concluded either when a time limit of 
30 minutes was reached, or when the subject com-
pleted three headache diagnoses. At the end of the 
third session, the subject was asked to fill out a 
questionnaire. 
4 Results 
Performance of a speech translation system is 
best evaluated by looking at system performance 
as a whole, and not separately for each subcompo-
nent in the systems processing pipeline (Rayner et. 
al. 2000, pp. 297-pp. 312). In this paper, we conse-
quently focus our analysis on objective and subjec-
tive usability-oriented measures. 
In Section 4.1, we present objective usability 
measures obtained by analyzing user-system inter-
actions and measuring task performance. In Sec-
tion 4.2, we present subjective usability figures and 
a preliminary analysis of translation quality. 
4.1 Objective Usability Figures 
4.1.1 Analysis of User Interactions 
Most of our analysis is based on data from the 
MedSLT system log, which records all interactions 
between the user and the system. An interaction is 
initiated when the user presses the ?Start Recogni-
tion? button. The system then attempts to recog-
nize what the user says. If it can do so, it next 
attempts to show the user how it has interpreted the 
recognition result, by first translating it into the 
Interlingua, and then translating it back into the 
source language (in this case, French). If the user 
decides that the back-translation is correct, they 
press the ?Translate? button. This results in the 
system attempting to translate the Interlingua rep-
resentation into the target language (in this case, 
English), and speak it using a Text-To-Speech en-
gine. The system also displays a list of ?help sen-
tences?, consisting of examples that are known to 
be within coverage, and which approximately 
match the result of performing recognition with the 
statistical language model. The user has the option 
of choosing a help sentence from the list, using the 
mouse, and submitting this to translation instead.  
We classify each interaction as either ?success-
ful? or ?unsuccessful?. An interaction is defined to 
be unsuccessful if either 
i) the user re-initiates recognition without 
asking the system for a translation, or 
ii) the system fails to produce a correct 
translation or back translation. 
Our definition of ?unsuccessful interaction? in-
cludes instances where users accidentally press the 
wrong button (i.e. ?Start Recognition? instead of 
?Translate?), press the button and then say nothing, 
or press the button and change their minds about 
what they want to ask half way through. We ob-
served all of these behaviors during the tests. 
Interactions where the system produced a trans-
lation were counted as successful, irrespective of 
whether the translation came directly from the 
user?s spoken input or from the help list. In at least 
some examples, we found that when the translation 
came from a help sentence it did not correspond 
directly to the sentence the user had spoken; to our 
surprise, it could even be the case that the help sen-
tence expressed the directly opposite question to 
the one the user had actually asked. This type of 
interaction was usually caused by some deficiency 
in the system, normally bad recognition or missing 
coverage. Our informal observation, however, was 
that, when this kind of thing happened, the user 
perceived the help module positively: it enabled 
them to elicit at least some information from the 
patient, and was less frustrating than being forced 
to ask the question again. 
Table I to Table III show the number of total in-
teractions per session, the proportion of successful 
interactions, and the proportion of interactions 
completed by selecting a sentence from the help 
list. The total number of interactions required to 
complete a session decreased over the three ses-
sions, declining from an average of 98.6 interac-
tions in the first session to 63.4 in the second (36% 
relative) and 53.9 in the third (45% relative). It is 
interesting to note that interactions involving the 
help system did not decrease in frequency, but re-
mained almost constant over the first two sessions 
(15.5% and 14.0%), and were in fact most com-
mon during the third session (21.7%). 
 
Session 1 
Subject Interactions % Successful % Help 
User 1 57 56.1% 0.0% 
User 2 98 52.0% 25.5% 
User 3 91 63.7% 15.4% 
User 4 156 69.9% 10.3% 
User 5 86 64.0% 22.1% 
User 6 134 47.0% 19.4% 
User 7 56 53.6% 5.4% 
User 8 111 63.1% 26.1% 
AVG 98.6 58.7% 15.5% 
Table I Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 1st session 
 
Session 2 
Subject Interactions % Successful % Help 
User 1 50 74.0% 2.0% 
User 2 63 55.6% 27.0% 
User 3 34 88.2% 23.5% 
User 4 96 57.3% 17.7% 
User 5 64 65.6% 21.9% 
User 6 93 68.8% 10.8% 
User 7 48 60.4% 4.2% 
User 8 59 79.7% 5.1% 
AVG 63.4 68.7% 14.0% 
Table II Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 2nd session 
 
Session 3 
Subject Interactions % Successful % Help 
User 1 33 90.9% 33.3% 
User 2 57 56.1% 22.8% 
User 3 48 72.9% 29.2% 
User 4 67 70.2% 16.4% 
User 5 68 73.5% 27.9% 
User 6 60 70.0% 6.7% 
User 7 41 65.9% 14.6% 
User 8 57 56.1% 22.8% 
AVG 53.9 69.5% 21.7% 
Table III Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 3rd session 
In order to establish a performance baseline, we 
also analyzed interaction data for two expert users, 
who performed the same experiment. The expert 
users were two native French-speaking system de-
velopers, which were both familiar with the diag-
nosis domain. Table IV summarizes the results of 
those users. One of our expert users, listed as Ex-
pert 2, is the French grammar developer, and had 
no failed interactions. This confirms that recogni-
tion is very accurate for users who know the cov-
erage. 
 
Session 1 / Expert Users 
Subject Interactions % Successful % Help 
Expert 1 36 77.8% 13.9% 
Expert 2 30 100.0% 3.3% 
AVG 33 88.9% 8.6% 
Table IV Number of interactions, and percentages 
of successful interactions, and interactions 
involving the help component 
 
The expert users were able to complete the ex-
periment using an average of 33 interaction rounds. 
Similar performance levels were achieved by some 
subjects during the second and third session, which 
suggests that it is possible for at least some new 
users to achieve performance close to expert level 
within a few sessions. 
4.1.2 Task Level Performance 
One of the important performance indicators for 
end users is how long it takes to perform a given 
task. During the experiments, the instructors noted 
completion times required to reach a definite diag-
nosis in the experiment log. Table VI shows task 
completion times, categorized by session (col-
umns) and task within the session (rows).  
 Session 1 Session 2 Session 3 
Diagnosis 1 17:00 min 11:00 min 7:54 min 
Diagnosis 2 11:00 min 6:18 min 5:34 min 
Diagnosis 3 7:54 min 4:10 min 4:00 min 
Table V Average time required by subjects to 
complete diagnoses 
 
In the last two sessions, after subjects had ac-
climatized to the system, a diagnosis takes an aver-
age of about four minutes to complete. This 
compares to a three-minute average required to 
complete a diagnosis by our expert users. 
4.1.3 System coverage 
Table VI shows the percentage of in-coverage 
sentences uttered by the users on interactions that 
did not involve invocation of the help component. 
 
 IN-COVERAGE SENTENCES 
Session 1 54.9% 
Session 2 60.7% 
Session 3 64.6% 
Table VI Percentage of in-coverage sentences 
 
This indicates that subjects learn and adapt to 
the system coverage as they use the system more. 
The average proportion of in-coverage utterances 
is 10 percent higher during the third session than 
during the first session. 
4.2 Subjective Usability Measures 
4.2.1 Results of Questionnaire 
After finishing the third session, subjects were 
asked to fill in a short questionnaire, where re-
sponses were on a five-point scale ranging from 1 
(?strongly disagree?) to 5 (?strongly agree?). The 
results are presented in Table VIII. 
 
STATEMENT SCORE 
I quickly learned how to use the system. 4.4 
System response times were generally 
satisfactory. 
4.5 
When the system did not understand me, 
the help system usually showed me an-
other way to ask the question. 
4.6 
When I knew what I could say, the sys-
tem usually recognized me correctly. 
4.3 
I was often unable to ask the questions I 
wanted. 
3.8 
I could ask enough questions that I was 
sure of my diagnosis. 
4.3 
This system is more effective than non-
verbal communication using gestures. 
4.3 
I would use this system again in a simi-
lar situation. 
4.1 
Table VIII Subject responses to questionnaire. 
Scores are on a 5-point scale, averaged over all 
answers. 
 
Answers were in general positive, and most of 
the subjects were clearly very comfortable with the 
system after just an hour and a half of use. Interest-
ingly, even though most of the subjects answered 
?yes? to the question ?I was often unable to ask the 
questions I wanted?, the good performance of the 
help system appeared to compensate adequately for 
missing coverage. 
4.2.2 Translation Performance 
In order to evaluate the translation quality of the 
newly developed French-to-English system, we 
conducted a preliminary performance evaluation, 
similar to the evaluation method described in 
(Bouillon 2005). 
We performed translation judgment in two 
rounds. In the first round, an English-speaking 
judge was asked to categorize target utterances as 
comprehensible or not without looking at corre-
sponding source sentences. 91.1% of the sentences 
were judged as comprehensible. The remaining 
8.9% consisted of sentences where the terminology 
used was not familiar to the judge and of sentences 
where the translation component failed to produce 
a sufficiently good translation. An example sen-
tence is 
- Are the headaches better when you experi-
ence dark room? 
which stems from the French source sentence 
- Vos maux de t?te sont ils soulag?s par obs-
curit?? 
In the second round, English-speaking judges, 
sufficiently fluent in French to understand source 
language utterances, were shown the French source 
utterance, and asked to decide whether the target 
language utterance correctly reflected the meaning 
of the source language utterance. They were also 
asked to judge the style of the target language ut-
terance. Specifically, judges were asked to classify 
sentences as ?BAD? if the meaning of the English 
sentence did not reflect the meaning of the French 
sentence. Sentences were categorized as ?OK? if 
the meaning was transferred correctly and the sen-
tence was comprehensible, but the style of the re-
sulting English sentence was not perfect. Sentences 
were judged as ?GOOD? when they were compre-
hensible, and both meaning and style were consid-
ered to be completely correct. Table VIII 
summarizes results of two judges. 
 
 Good OK Bad  
Judge 1 15.8% 73.80% 10.3% 
Judge 2 46.6% 47.1% 6.3% 
Table VIII Judgments of the quality of the transla-
tions of 546 utterances 
 
It is apparent that translation judging is a highly 
subjective process. When translations were marked 
as ?bad?, the problem most often seemed to be re-
lated to lexical items where it was challenging to 
find an exact correspondence between French and 
English. Two common examples were ?troubles de 
la vision?, which was translated as ?blurred vi-
sion?, and ?faiblesse musculaire?, which was trans-
lated as ?weakness?. It is likely that a more careful 
choice of lexical translation rules would deal with 
at least some of these cases. 
5 Summary 
We have presented a first end-to-end evaluation 
of the MedSLT spoken language translation sys-
tem. The medical students who tested it were all 
able to use the system well, with performance in 
some cases comparable to that of that of system 
developers after only two sessions. At least for the 
fairly simple type of diagnoses covered by our sce-
nario, the system?s performance appeared clearly 
adequate for the task.  
This is particularly encouraging, since the 
French to English version of the system is quite 
new, and has not yet received the level of attention 
required for a clinical system. The robustness 
added by the help system was sufficient to com-
pensate for that, and in most cases, subjects were 
able to find ways to maneuver around coverage 
holes and other problems. It is entirely reasonable 
to hope that performance, which is already fairly 
good, would be substantially better with another 
couple of months of development work. 
In summary, we feel that this study shows that 
the conservative architecture we have chosen 
shows genuine potential for use in medical diagno-
sis situations. Before the end of 2006, we hope to 
have advanced to the stage where we can start ini-
tial trials with real doctors and patients. 
 
 
Acknowledgments 
We would like to thank Agnes Lisowska, Alia 
Rahal, and Nancy Underwood for being impartial 
judges over our system?s results. 
This work was funded by the Swiss National 
Science Foundation. 
References 
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. 
Hockey, M. Santaholma, M. Starlander, Y. Nakao, K. 
Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain 
medical speech translation. In Proceedings of the 
10th Conference of the European Association for 
Machine Translation (EAMT), Budapest, Hungary. 
P. Bouillon, M. Rayner, B. Novellas, Y. Nakao, M. San-
taholma, M. Starlander, and N. Chatzichrisafis. 2006. 
Une grammaire multilingue partag?e pour la recon-
naissance et la g?n?ration. In Proceedings of TALN 
2006, Leuwen, Belgium. 
M. Cohen, J. Giangola, and J. Balogh. 2004, Voice User 
Interface Design. Addison Wesley Publishing. 
R. I. Kittredge. 2003. Sublanguages and comtrolled 
languages. In R. Mitkov, editor, The Oxford Hand-
book of Computational Linguistics, pages 430?447. 
Oxford University Press. 
MedBridge, 2006. http://www.medtablet.com/. As of 
15th March 2006. 
MedSLT, 2005. http://sourceforge.net/projects/medslt/. 
As of 15th March 2006. 
T. Mitamura. 1999. Controlled language for multilin-
gual machine translation. In Proceedings of Machine 
Translation Summit VII, Singapore. 
Phraselator, 2006. http://www.phraselator.com. As of 
15 February 2006. 
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An 
open source environment for compiling typed unifi-
cation grammars into speech recognisers. In Pro-
ceedings of the 10th EACL (demo track), Budapest, 
Hungary. 
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, 
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. 
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session, 
Vancouver, British Columbia, Canada. Association 
for Computational Linguistics. 
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. 
Hockey, M. Santaholma,M. Starlander, H. Isahara, 
K. Kankazi, and Y. Nakao. 2005a. A methodology for 
comparing grammar-based and robust approaches to 
speech understanding. In Proceedings of the 9th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Lisboa, Portugal. 
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and M. 
Wir?n. 2000. The Spoken Language Translator, 
Cambridge University Press.  
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, 
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. 
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session, 
Vancouver, British Columbia, Canada. Association 
for Computational Linguistics. 
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Put-
ting Linguistics into Speech Recognition: The 
Regulus Grammar Compiler. CSLI Press, Chicago.  
Regulus, 2006. http://sourceforge.net/projects/regulus/. 
As of 15 March 2006. 
S-MINDS, 2006. http://www.sehda.com/. As of 15 
March 2006. 
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. San-
taholma, M. Rayner, B.A. Hockey, H. Isahara, K. 
Kanzaki, and Y. Nakao. 2005. Practicing controlled 
language through a help system integrated into the 
medical speech translation system (MedSLT). In Pro-
ceedings of the MT Summit X, Phuket, Thailand 
 
MedSLT: A Limited-Domain Unidirectional Grammar-Based Medical
Speech Translator
Manny Rayner, Pierrette Bouillon, Nikos Chatzichrisafis, Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Pierrette.Bouillon@issco.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Marianne.Starlander@eti.unige.ch
Beth Ann Hockey
UCSC/NASA Ames Research Center, Moffet Field, CA 94035
bahockey@email.arc.nasa.gov
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Abstract
MedSLT is a unidirectional medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several differ-
ent language pairs and subdomains. Vo-
cabulary ranges from about 350 to 1000
surface words, depending on the language
and subdomain. We will demo both the
system itself and the development envi-
ronment, which uses a combination of
rule-based and data-driven methods to
construct efficient recognisers, generators
and transfer rule sets from small corpora.
1 Overview
The mainstream in speech translation work is for the
moment statistical, but rule-based systems are still a
very respectable alternative. In particular, nearly all
systems which have actually been deployed are rule-
based. Prominent examples are (Phraselator, 2006;
S-MINDS, 2006; MedBridge, 2006).
MedSLT (MedSLT, 2005; Bouillon et al, 2005)
is a unidirectional medical speech translation system
for use in doctor-patient diagnosis dialogues, which
covers several different language pairs and subdo-
mains. Recognition is performed using grammar-
based language models, and translation uses a rule-
based interlingual framework. The system, includ-
ing the development environment, is built on top of
Regulus (Regulus, 2006), an Open Source platform
for developing grammar-based speech applications,
which in turn sits on top of the Nuance Toolkit.
The demo will show how MedSLT can be used
to carry out non-trivial diagnostic dialogues. In par-
ticular, we will demonstrate how an integrated intel-
ligent help system counteracts the brittleness inher-
ent in rule-based processing, and rapidly leads new
users towards the supported system coverage. We
will also demo the development environment, and
show how grammars and sets of transfer rules can be
efficiently constructed from small corpora of a few
hundred to a thousand examples.
2 The MedSLT system
The MedSLT demonstrator has already been exten-
sively described elsewhere (Bouillon et al, 2005;
Rayner et al, 2005a), so this section will only
present a brief summary. The main components are
a set of speech recognisers for the source languages,
a set of generators for the target languages, a transla-
tion engine, sets of rules for translating to and from
interlingua, a simple discourse engine for dealing
with context-dependent translation, and a top-level
which manages the information flow between the
other modules and the user.
MedSLT also includes an intelligent help mod-
ule, which adds robustness to the system and guides
the user towards the supported coverage. The help
module uses a backup recogniser, equipped with a
statistical language model, and matches the results
from this second recogniser against a corpus of utter-
ances which are within system coverage and trans-
late correctly. In previous studies, we showed that
the grammar-based recogniser performs much bet-
ter than the statistical one on in-coverage utterances,
but worse on out-of-coverage ones. Having the help
system available approximately doubled the speed
at which subjects learned, measured as the average
difference in semantic error rate between the results
for their first quarter-session and their last quarter-
session (Rayner et al, 2005a). It is also possible to
recover from recognition errors by selecting a dis-
played help sentence; this typically increases the
number of acceptably processed utterances by about
10% (Starlander et al, 2005).
We will demo several versions of the system, us-
ing different source languages, target languages and
subdomains. Coverage is based on standard exami-
nation questions obtained from doctors, and consists
mainly of yes/no questions, though there is also sup-
port for WH-questions and elliptical utterances. Ta-
ble 1 gives examples of the coverage in the English-
input headache version, and Table 2 summarises
recognition performance in this domain for the three
main input languages. Differences in the sizes of the
recognition vocabularies are primarily due to differ-
ences in use of inflection. Japanese, with little in-
flectional morphology, has the smallest vocabulary;
French, which inflects most parts of speech, has the
largest.
3 The development environment
Although the MedSLT system is rule-based, we
would, for the usual reasons, prefer to acquire these
rules from corpora using some well-defined method.
There is, however, little or no material available for
most medical speech translation domains, including
ours. As noted in (Probst and Levin, 2002), scarcity
of data generally implies use of some strategy to ob-
tain a carefully structured training corpus. If the cor-
pus is not organised in this way, conflicts between
alternate learned rules occur, and it is hard to in-
Where?
?do you experience the pain in your jaw?
?does the pain spread to the shoulder?
When?
?have you had the pain for more than a month?
?do the headaches ever occur in the morning?
How long?
?does the pain typically last a few minutes?
?does the pain ever last more than two hours?
How often?
?do you get headaches several times a week?
?are the headaches occurring more often?
How?
?is it a stabbing pain?
?is the pain usually severe?
Associated symptoms?
?do you vomit when you get the headaches?
?is the pain accompanied by blurred vision?
Why?
?does bright light make the pain worse?
?do you get headaches when you eat cheese?
What helps?
?does sleep make the pain better?
?does massage help?
Background?
?do you have a history of sinus disease?
?have you had an e c g?
Table 1: Examples of English MedSLT coverage
duce a stable set of rules. As Probst and Levin sug-
gest, one obvious way to attack the problem is to
implement a (formal or informal) elicitation strat-
egy, which biases the informant towards translations
which are consistent with the existing ones. This is
the approach we have adopted in MedSLT.
The Regulus platform, on which MedSLT
is based, supports rapid construction of com-
plex grammar-based language models; it uses an
example-based method driven by small corpora
of disambiguated parsed examples (Rayner et al,
2003; Rayner et al, 2006), which extracts most of
the structure of the model from a general linguis-
tically motivated resource grammar. The result is
a specialised version of the general grammar, tai-
lored to the example corpus, which can then be com-
piled into an efficient recogniser or into a genera-
Language Vocab WER SemER
English 441 6% 18%
French 1025 8% 10%
Japanese 347 4% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognisers.
?Vocab? = number of surface words in source lan-
guage recogniser vocabulary; ?WER? = Word Error
Rate for source language recogniser, on in-coverage
material; ?SemER? = semantic error rate for source
language recogniser, on in-coverage material.
tion module. Regulus-based recognisers and gen-
erators are easy to maintain, and grammar struc-
ture is shared automatically across different subdo-
mains. Resource grammars are available for several
languages, including English, Japanese, French and
Spanish.
Nuance recognisers derived from the resource
grammars produce both a recognition string and a
semantic representation. This representation con-
sists of a list of key/value pairs, optionally including
one level of nesting; the format of interlingua and
target language representations is similar. The for-
malism is sufficiently expressive that a reasonable
range of temporal and causal constructions can be
represented (Rayner et al, 2005b). A typical exam-
ple is shown in Figure 1. A translation rule maps
a list of key/value pairs to a list of key/value pairs,
optionally specifying conditions requiring that other
key/value pairs either be present or absent in the
source representation.
When developing new coverage for a given lan-
guage pair, the developer has two main tasks. First,
they need to add new training examples to the
corpora used to derive the specialised grammars
used for the source and target languages; second,
they must add translation rules to handle the new
key/value pairs. The simple structure of the Med-
SLT representations makes it easy to support semi-
automatic acquisition of both of these types of in-
formation. The basic principle is to attempt to find
the minimal set of new rules that can be added to the
existing set, in order to cover the new corpus exam-
ple; this is done through a short elicitation dialogue
with the developer. We illustrate this with a simple
example.
Suppose we are developing coverage for the En-
glish ? Spanish version of the system, and that
the English corpus sentence ?does the pain occur at
night? fails to translate. The acquisition tool first
notes that processing fails when converting from in-
terlingua to Spanish. The interlingua representation
is
[[utterance_type,ynq],
[pronoun,you],
[state,have_symptom],
[symptom,pain],[tense,present],
[prep,in_time],[time,night]]
Applying Interlingua ? Spanish rules, the result is
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
failed:[time,night]]
where the tag failed indicates that the element
[time,night] could not be processed. The tool
matches the incomplete transferred representation
against a set of correctly translated examples, and
shows the developer the English and Spanish strings
for the three most similar ones, here
does it appear in the morning
-> tiene el dolor por la man?ana
does the pain appear in the morning
-> tiene el dolor por la man?ana
does the pain come in the morning
-> tiene el dolor por la man?ana
This suggests that a translation for ?does the pain
occur at night? consistent with the existing rules
would be ?tiene el dolor por la noche?. The devel-
oper gives this example to the system, which parses
it using both the general Spanish resource grammar
and the specialised grammar used for generation in
the headache domain. The specialised grammar fails
to produce an analysis, while the resource grammar
produces two analyses,
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[[utterance_type,ynq],[pronoun,you],[state,have_symptom],
[tense,present],[symptom,headache],[sc,when],
[[clause,[[utterance_type,dcl],[pronoun,you],
[action,drink],[tense,present],[cause,coffee]]]]
Figure 1: Representation of ?do you get headaches when you drink coffee?
[tense,present],
[prep,por_temporal],
[temporal,noche]]
and
[[utterance_type,dcl],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
[temporal,noche]]
The first of these corresponds to the YN-question
reading of the sentence (?do you have the pain at
night?), while the second is the declarative reading
(?you have the pain at night?). Since the first (YN-
question) reading matches the Interlingua represen-
tation better, the acquisition tool assumes that it is
the intended one. It can now suggest two pieces of
information to extend the system?s coverage.
First, it adds the YN-question reading of ?tiene
el dolor por la noche? to the corpus used to train
the specialised generation grammar. The piece
of information acquired from this example is that
[temporal,noche] should be realised in this
domain as ?la noche?. Second, it compares the cor-
rect Spanish representation with the incomplete one
produced by the current set of rules, and induces a
new Interlingua to Spanish translation rule. This will
be of the form
[time,night] -> [temporal,noche]
In the demo, we will show how the development
environment makes it possible to quickly add new
coverage to the system, while also checking that old
coverage is not broken.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
MedBridge, 2006. http://www.medtablet.com/index.html.
As of 15 March 2006.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 15 March 2005.
Phraselator, 2006. http://www.phraselator.com. As of 15
March 2006.
K. Probst and L. Levin. 2002. Challenges in automatic
elicitation of a controlled bilingual corpus. In Pro-
ceedings of the 9th International Conference on The-
oretical and Methodological Issues in Machine Trans-
lation.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, H. Isahara, K. Kankazi,
and Y. Nakao. 2005a. A methodology for comparing
grammar-based and robust approaches to speech un-
derstanding. In Proceedings of the 9th International
Conference on Spoken Language Processing (ICSLP),
Lisboa, Portugal.
M. Rayner, P. Bouillon, M. Santaholma, and Y. Nakao.
2005b. Representational and architectural issues in a
limited-domain medical speech translator. In Proceed-
ings of TALN/RECITAL, Dourdan, France.
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting
Linguistics into Speech Recognition: The Regulus
Grammar Compiler. CSLI Press, Chicago.
Regulus, 2006. http://sourceforge.net/projects/regulus/.
As of 15 March 2006.
S-MINDS, 2006. http://www.sehda.com. As of 15
March 2006.
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. Santa-
holma, M. Rayner, B.A. Hockey, H. Isahara, K. Kan-
zaki, and Y. Nakao. 2005. Practicing controlled lan-
guage through a help system integrated into the medi-
cal speech translation system (MedSLT). In Proceed-
ings of the MT Summit X, Phuket, Thailand.
Proceedings of SPEECHGRAM 2007, pages 41?48,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Bidirectional Grammar-Based Medical Speech Translator
Pierrette Bouillon1, Glenn Flores2, Marianne Starlander1, Nikos Chatzichrisafis1
Marianne Santaholma1, Nikos Tsourakis1, Manny Rayner1,3, Beth Ann Hockey4
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Pierrette.Bouillon@issco.unige.ch
Marianne.Starlander@eti.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Nikolaos.Tsourakis@issco.unige.ch
2 Medical College of Wisconsin, 8701 Watertown Plank Road, Milwaukee, WI 53226
gflores@mcw.edu
3 Powerset, Inc., 475 Brannan Street, San Francisco, CA 94107
manny@powerset.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
Abstract
We describe a bidirectional version of the
grammar-based MedSLT medical speech
system. The system supports simple medi-
cal examination dialogues about throat pain
between an English-speaking physician and
a Spanish-speaking patient. The physician?s
side of the dialogue is assumed to consist
mostly of WH-questions, and the patient?s of
elliptical answers. The paper focusses on the
grammar-based speech processing architec-
ture, the ellipsis resolution mechanism, and
the online help system.
1 Background
There is an urgent need for medical speech trans-
lation systems. The world?s current population
of 6.6 billion speaks more than 6,000 languages
(Graddol, 2004). Language barriers are associated
with a wide variety of deleterious consequences in
healthcare, including impaired health status, a lower
likelihood of having a regular physician, lower rates
of mammograms, pap smears, and other preven-
tive services, non-adherence with medications, a
greater likelihood of a diagnosis of more severe psy-
chopathology and leaving the hospital against med-
ical advice among psychiatric patients, a lower like-
lihood of being given a follow-up appointment af-
ter an emergency department visit, an increased risk
of intubation among children with asthma, a greater
risk of hospital admissions among adults, an in-
creased risk of drug complications, longer medical
visits, higher resource utilization for diagnostic test-
ing, lower patient satisfaction, impaired patient un-
derstanding of diagnoses, medications, and follow-
up, and medical errors and injuries (Flores, 2005;
Flores, 2006). Nevertheless, many patients who
need medical interpreters do not get them. For ex-
ample, in the United States, where 52 million peo-
ple speak a language other than English at home
and 23 million people have limited English profi-
ciency (LEP) (Census, 2007), one study found that
about half of LEP patients presenting to an emer-
gency department were not provided with a medical
interpreter (Baker et al, 1996). There is thus a sub-
stantial gap between the need for and availability of
language services in health care, a gap that could be
bridged through effective medical speech translation
systems.
An ideal system would be able to interpret ac-
curately and flexibly between patients and health
care professionals, using unrestricted language and
a large vocabulary. A system of this kind is, un-
fortunately, beyond the current state of the art.
It is, however, possible, using today?s technol-
ogy, to build speech translation systems for specific
scenarios and language-pairs, which can achieve
acceptable levels of reliability within the bounds
41
of a well-defined controlled language. MedSLT
(Bouillon et al, 2005) is an Open Source system
of this type, which has been under construction at
Geneva University since 2003. The system is built
on top of Regulus (Rayner et al, 2006), an Open
Source platform which supports development of
grammar-based speech-enabled applications. Regu-
lus has also been used to build several other systems,
including NASA?s Clarissa (Rayner et al, 2005b).
The most common architecture for speech trans-
lation today uses statistical methods to perform both
speech recognition and translation, so it is worth
clarifying why we have chosen to use grammar-
based methods. Even though statistical architec-
tures exhibit many desirable properties (purely data-
driven, domain independent), this is not necessar-
ily the best alternative in safety-critical medical ap-
plications. Anecdotally, many physicians express
reluctance to trust a translation device whose out-
put is not readily predictable, and most of the
speech translation systems which have reached the
stage of field testing rely on various types of
grammar-based recognition and rule-based transla-
tion (Phraselator, 2007; Fluential, 2007).
Statistical speech recognisers can achieve impres-
sive levels of accuracy when trained on enough data,
but it is a daunting task to collect training mate-
rial in the requisite quantities (usually, tens of thou-
sands of high-quality utterances) when trying to
build practical systems. Considering that the medi-
cal speech translation applications we are interested
in constructing here need to work for multiple lan-
guages and subdomains, the problem becomes even
more challenging. Our experience is that grammar-
based systems which also incorporate probabilistic
context-free grammar tuning deliver better results
than purely statistical ones when training data are
sparse (Rayner et al, 2005a).
Another common criticism of grammar-based
systems is that out-of-coverage utterances will
neither be recognized nor translated, an objec-
tion that critics have sometimes painted as de-
cisive. It is by no means obvious, however,
that restricted coverage is such a serious prob-
lem. In text processing, work on several gener-
ations of controlled language systems has devel-
oped a range of techniques for keeping users within
the bounds of system coverage (Kittredge, 2003;
Mitamura, 1999), and variants of these methods can
also be adapted for spoken language applications.
Our experiments with MedSLT show that even a
quite simple help system is enough to guide users
quickly towards the intended coverage of a medium-
vocabulary grammar-based speech translation appli-
cation, with most users appearing confident after just
an hour or two of exposure (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
Until recently, the MedSLT system only sup-
ported unidirectional processing in the physician
to patient direction. The assumption was that the
physician would mostly ask yes/no questions, to
which the patient would respond non-verbally, for
example by nodding or shaking their head. A uni-
directional architecture is easier to make habitable
than a bidirectional one. It is reasonable to as-
sume that the physician will use the system regu-
larly enough to learn the coverage, but most patients
will not have used the system before, and it is less
clear that they will be able to acclimatize within the
narrow window at their disposal. These consider-
ations must however be balanced against the fact
that a unidirectional system does not allow for a
patient-centered interaction characterized by mean-
ingful patient-clinician communication or shared de-
cision making. Multiple studies in the medical lit-
erature document that patient-centeredness, effec-
tive patient-clinician communication, and shared de-
cision making are associated with significant im-
provements in patient health outcomes, including
reduced anxiety levels, improved functional sta-
tus, reduced pain, better control of diabetes melli-
tus, blood pressure reduction among hypertensives,
improved adherence, increased patient satisfaction,
and symptom reduction for a variety of conditions
(Stewart, 1995; Michie et al, 2003). A bidirectional
system is considered close to essential from a health-
care perspective, since it appropriately addresses the
key issues of patient centeredness and shared de-
cision making. For these reasons, we have over
the last few months developed a bidirectional ver-
sion of MedSLT, initially focussing on a throat pain
scenario with an English-speaking physician and a
Spanish-speaking patient. The physician uses full
sentences, while the patient answers with short re-
sponses.
One of the strengths of the Regulus approach is
42
that it is very easy to construct parallel versions of
a grammar; generally, all that is required is to vary
the training corpus. (We will have more to say about
this soon). We have exploited these properties of
the platform to create two different configurations
of the bidirectional system, so that we can compare
competing approaches to the problem of accommo-
dating patients unfamiliar with speech technology.
In Version 1 (less restricted), the patient is allowed
to answer using both elliptical utterances and short
sentences, while in Version 2 (more restricted) they
are only permitted to use elliptical utterances. Thus,
for example, if the physician asks the question ?How
long have you had a sore throat??, Version 1 allows
the patient to respond both ?Desde algunos d??as?
(?For several days?) and ?Me ha dolido la garganta
desde algunos d??as? (?I have had a sore throat for
several days?), while Version 2 only allows the first
of these. Both the short and the long versions are
translated uniformly, with the short version resolved
using the context from the preceding question.
In both versions, if the patient finds it too chal-
lenging to use the system to answer WH-questions
directly, it is possible to back off to the earlier di-
alogue architecture in which the physician uses Y-
N questions and the patient responds with simple
yes/no answers, or nonverbally. Continuing the ex-
ample, if the patient is unable to find an appro-
priate way to answer the physician?s question, the
physician could ask ?Have you had a sore throat for
more than three days??; if the patient responds nega-
tively, they could continue with the follow-on ques-
tion ?More than a week??, and so on.
In the rest of the paper, we first describe the
system top-level (Section 2), the way in which
grammar-based processing is used (Section 3), the
ellipsis processing mechanism (Section 4), and the
help system (Section 5). Section 6 presents an ini-
tial evaluation, and the final section concludes.
2 Top-level architecture
The system is operated through the graphical user
interface (GUI) shown in Figures 1 and 2. In
accordance with the basic principles of patient-
centeredness and shared decision-making outlined
in Section 1, the patient and the physician each have
their own headset, use their own mouse, and share
the same view of the screen. This is in sharp contrast
to the majority of the medical speech translation sys-
tems described in the literature (Somers, 2006).
As shown in the screenshots, the main GUI win-
dow is separated into two tabbed panes, marked
?Doctor? and ?Patient?. Initially, the ?Doctor? view
(the one shown in Figure 1) is active. The physician
presses the ?Push to talk? button, and speaks into
the headset microphone. If recognition is success-
ful, the GUI displays four separate results, listed on
the right side of the screen. At the top, immediately
under the heading ?Question?, we can see the actual
words returned by speech recognition. Here, these
words are ?Have you had rapid strep test?. Below,
we have the help pane: this displays similar ques-
tions taken from the help corpus, which are known to
be within system coverage. The pane marked ?Sys-
tem understood? shows a back-translation, produced
by first translating the recognition result into inter-
lingua, and then translating it back into English. In
the present example, this corrects the minor mistake
the recogniser has made, missing the indefinite ar-
ticle ?a?, and confirms that the system has obtained
a correct grammatical analysis and interpretation at
the level of interlingua. At the bottom, we see the
target language translation. The left-hand side of the
screen logs the history of the conversation to date, so
that both sides can refer back to it.
If the physician decides that the system has cor-
rectly understood what they said, they can now press
the ?Play? button. This results in the system produc-
ing a spoken output, using the Vocalizer TTS engine.
Simultaneously with speaking, the GUI shifts to the
?Patient? configuration shown in Figure 2. This dif-
fers from the ?Doctor? configuration in two respects:
all text is in the patient language, and the help pane
presents its suggestions immediately, based on the
preceding physician question. The various process-
ing components used to support these functionalities
are described in the following sections.
3 Grammar-based processing
Grammar-based processing is used for source-
language speech recognition and target-side genera-
tion. (Source-language analysis is part of the recog-
nition process, since grammar-based recognition in-
cludes creating a parse). All of these functionalities
43
Figure 1: Screenshot showing the state of the GUI after the physician has spoken, but before he has pressed
the ?Play? button. The help pane shows similar queries known to be within coverage.
Figure 2: Screenshot showing the state of the GUI after the physician has pressed the ?Play? button. The
help pane shows known valid responses to similar questions.
44
are implemented using the Regulus platform, with
the task-specific grammars compiled out of general
feature grammar resources by the Regulus tools. For
both recognition and generation, the first step is
to extract a domain-specific feature grammar from
the general one, using a version of the Explanation
Based Learning (EBL) algorithm.
The extraction process is driven by a corpus of ex-
amples and a set of ?operationality criteria?, which
define how the rules in the original resource gram-
mar are recombined into domain-specific ones. It is
important to realise that the domain-specific gram-
mar is not merely a subset of the resource grammar;
a typical domain-specific grammar rule is created by
merging two to five resource grammar rules into a
single ?flatter? rule. The result is a feature gram-
mar which is less general than the original one, but
more efficient. For recognition, the grammar is then
processed further into a CFG language model, using
an algorithm which alternates expansion of feature
values and filtering of the partially expanded gram-
mar to remove irrelevant rules. Detailed descrip-
tions of the EBL learning and feature grammar ?
CFG compilation algorithms can be found in Chap-
ters 8 and 10 of (Rayner et al, 2006). Regulus fea-
ture grammars can also be compiled into generators
using a version of the Semantic Head Driven algo-
rithm (Shieber et al, 1990).
The English (physician) side recogniser is com-
piled from the large English resource grammar de-
scribed in Chapter 9 of (Rayner et al, 2006), and
was constructed in the same way as the one de-
scribed in (Rayner et al, 2005a), which was used for
a headache examination task. The operationality cri-
teria are the same, and the only changes are a differ-
ent training corpus and the addition of new entries
to the lexicon. The same resources, with a differ-
ent training corpus, were used to build the English
language generator. It is worth pointing out that, al-
though a uniform method was used to build these
various grammars, the results were all very differ-
ent. For example, the recognition grammar from
(Rayner et al, 2005a) is specialised to cover only
second-person questions (?Do you get headaches
in the mornings??), while the generator grammar
used in the present application covers only first-
person declarative statements (?I visited the doctor
last Monday.?). In terms of structure, each gram-
mar contains several important constructions that the
other lacks. For example, subordinate clauses are
central in the headache domain (?Do the headaches
occur when you are stressed??) but are not present
in the sore throat domain; this is because the stan-
dard headache examination questions mostly focus
on generic conditions, while the sore throat exami-
nation questions only relate to concrete ones. Con-
versely, relative clauses are important in the sore
throat domain (?I have recently been in contact with
someone who has strep throat?), but are not suffi-
ciently important in the headache domain to be cov-
ered there.
On the Spanish (patient) side, there are four
grammars involved. For recognition, we have
two different grammars, corresponding to the two
versions of the system; the grammar for Ver-
sion 2 is essentially a subset of that for Version
1. For generation, there are two separate and
quite different grammars: one is used for trans-
lating the physician?s questions, while the other
produces back-translations of the patient?s ques-
tions. All of these grammars are extracted from
a general shared resource grammar for Romance
languages, which currently combines rules for
French, Spanish and Catalan (Bouillon et al, 2006;
Bouillon et al, to appear 2007b).
One interesting consequence of our methodology
is related to the fact that Spanish is a prodrop lan-
guage, which implies that many sentences are sys-
tematically ambiguous between declarative and Y-N
question readings. For example, ?He consultado un
me?dico? could in principle mean either ?I visited a
doctor? or ?Did I visit a doctor??. When training the
specialised Spanish grammars, it is thus necessary to
specify which readings of the training sentences are
to be used. Continuing the example, if the sentence
occurred in training material for the answer gram-
mar, we would specify that the declarative reading
was the intended one1.
4 Ellipsis processing and contextual
interpretation
In Version 1 of the system, the patient is per-
mitted to answer using elliptical phrases; in Ver-
1The specification can be formulated as a preference that
applies uniformly to all the training examples in a given group.
45
sion 2, she is obliged to do so. Ability to pro-
cess elliptical responses makes it easier to guide the
patient towards the intended coverage of the sys-
tem, without degrading the quality of recognition
(Bouillon et al, to appear 2007a). The downside is
that ellipses are also harder to translate than full sen-
tences. Even in a limited domain like ours, and in a
closely related language-pair, ellipsis can generally
not be translated word for word, and it is necessary
to look at the preceding context if the rules are to
be applied correctly. In examples 1 and 2 below,
the locative phrase ?In your stomach? in the English
source becomes the subject in the Spanish transla-
tion. This implies that the translation of the ellipsis
in the second physician utterance needs to change
syntactic category: ?In your head? (PP) becomes
?La cabeza? (NP).
(1) Doctor: Do you have a pain in your
stomach?
(Trans): Le duele el estomago?
(2) Doctor: In your head?
(Trans): *En la cabeza?
Since examples like this are frequent, our sys-
tem implements a solution in which the patient?s
replies are translated in the context of the preced-
ing utterance. If the patient-side recogniser?s output
is classified as an ellipsis (this can done fairly reli-
ably thanks to use of suitably specialised grammars;
cf. Section 3), we expand the incomplete phrase
into a full sentence structure by adding appropriate
structural elements from the preceding physician-
side question; the expanded semantic structure is the
one which is then translated into interlingual form,
and thence back to the physician-side language.
Since all linguistic representations, including
those of elliptical phrases and their contexts, are rep-
resented as flat attribute-value lists, we are able to
implement the resolution algorithm very simply in
terms of list manipulation. In YN-questions, where
the elliptical answer intuitively adds information to
the question (?Did you visit the doctor??; ?El lunes?
? ?I visited the doctor on Monday?), the repre-
sentations are organised so that resolution mainly
amounts to concatenation of the two lists2. In WH-
questions, where the answer intuitively substitutes
the elliptical answer for the WH-phrase (?What is
2It is also necessary to replace second-person pronouns with
first-person counterparts.
your temperature??; ?Cuarenta grados?? ?My tem-
perature is forty degrees?), resolution substitutes the
representation of the elliptical phrase for that of a
semantically similar element in the question.
The least trivial aspect of this process is provid-
ing a suitable definition of ?semantically similar?.
This is done using a simple example-based method,
in which the grammar developer writes a set of dec-
larations, each of which lists a set of semantically
similar NPs. At compile-time, the grammar is used
to parse each NP, and extract a generalised skele-
ton, in which specific lexical information is stripped
away; at run-time, two NPs are held to be semanti-
cally similar if they can each be unified with skele-
tons in the same equivalence class. This ensures that
the definition of the semantic similarity relation is
stable across most changes to the grammar and lex-
icon. The issues are described in greater detail in
(Bouillon et al, to appear 2007a).
5 Help system
Since the performance of grammar-based speech un-
derstanding is only reliable on in-coverage mate-
rial, systems based on this type of architecture must
necessarily use a controlled language approach, in
which it is assumed that the user is able to learn the
relevant coverage. As previously noted, the Med-
SLT system addresses this problem by incorporat-
ing an online help system (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
On the physician side, the help system offers, af-
ter each recognition event, a list of related ques-
tions; similarly, on the patient side, it provides ex-
amples of known valid answers to the current ques-
tion. In both cases, the help examples are extracted
from a precompiled corpus of question-answer pairs,
which have been judged for correctness by system
developers. The process of selecting the examples
is slightly different on the two sides. For questions
(physician side), the system performs a second par-
allel recognition of the input speech, using a sta-
tistical recogniser. It then compares the recogni-
tion result, using an N-gram based metric, against
the set of known correct in-coverage questions from
the question-answer corpus, to extract the most sim-
ilar ones. For answers (patient side), the help sys-
tem searches the question-answer corpus to find the
46
questions most similar to the current one, and shows
the list of corresponding valid answers, using the
whole list in the case of Version 1 of the system, and
only the subset consisting of elliptical phrases in the
case of Version 2.
6 Evaluation
In previous studies, we have evaluated speech
recognition and speech understanding per-
formance for physician-side questions in
English (Bouillon et al, 2005) and Spanish
(Bouillon et al, to appear 2007b), and investi-
gated the impact on performance of the help system
(Rayner et al, 2005a; Starlander et al, 2005). We
have also carried out recent evaluations designed to
contrast recognition performance on elliptical and
full versions of the same utterance; here, our results
suggest that elliptical forms of (French-language)
MedSLT utterances are slightly easier to recognise
in terms of semantic error rate than full sentential
forms (Bouillon et al, to appear 2007a). Our initial
evaluation studies on the bidirectional system have
focussed on a specific question which has particular
relevance to this new version of MedSLT. Since
we are assuming that the patient will respond
using elliptical utterances, and that these utterances
will be translated in the context of the preceding
physician-side question, how confident can we
be that this context-dependent translation will be
correct?
In order to investigate these issues, we performed
a small data-collection using Version 2 of the sys-
tem, whose results we summarise here. One of the
authors of the paper played the role of an English-
speaking physician, in a simulated medical exam-
ination scenario where the goal was to determine
whether or not the ?patient? was suffering from a
viral throat infection. The six subjects playing the
role of the patient were all native speakers of Span-
ish, and had had no previous exposure to the system,
or indeed any kind of speech technology. They were
given cards describing the symptoms they were sup-
posed to be displaying, on which they were asked
to based their answers. From a total of 92 cor-
rectly recognised patient responses, we obtained 50
yes/no answers and 42 examples of real elliptical ut-
terances. Out of these, 36 were judged to have been
translated completely correctly, and a further 3 were
judged correct in terms of meaning, but less than flu-
ent. Only 3 examples were badly translated: of these
two were caused by problems in a translation rule,
and one by incorrect treatment of ellipsis resolution.
We show representative exchanges below; the last of
these is the one in which ellipsis processing failed to
work correctly.
(3) Doctor: For how long have you
had your sore throat?
Patient: Desde hace ma?s de
una semana
(Trans): I have had a sore
throat for more than one week
(4) Doctor: What were the results?
Patient: Negativo
(Trans): The results were negative
(5) Doctor: Have you seen a doctor
for your sore throat?
Patient: S?? el lunes
(Trans): I visited the doctor
for my sore throat monday
(6) Doctor: Have you been with anyone
recently who has a strep throat?
Patient: Si ma?s de dos semanas
(Trans): I was in contact with someone
more than two weeks recently
who had strep throat
7 Conclusions
We have presented a bidirectional grammar-based
English ? Spanish medical speech translation sys-
tem built using a linguistically motivated archi-
tecture, where all linguistic information is ulti-
mately derived from two resource grammars, one
for each language. We have shown how this en-
ables us to derive the multiple grammars needed,
which differ both with respect to function (recog-
nition/generation) and to domain (physician ques-
tions/patient answers). The system is currently un-
dergoing initial lab testing; we hope to advance to
initial trials on real patients some time towards the
end of the year.
References
[Baker et al1996] D.W. Baker, R.M. Parker, M.V.
Williams, W.C. Coates, and Kathryn Pitkin. 1996.
47
Use and effectiveness of interpreters in an emer-
gency department. Journal of the American Medical
Association, 275:783?8.
[Bouillon et al2005] P. Bouillon, M. Rayner,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara.
2005. A generic multi-lingual open source platform
for limited-domain medical speech translation. In
Proceedings of the 10th Conference of the European
Association for Machine Translation (EAMT), pages
50?58, Budapest, Hungary.
[Bouillon et al2006] P. Bouillon, M. Rayner, B. Novel-
las Vall, Y. Nakao, M. Santaholma, M. Starlander, and
N. Chatzichrisafis. 2006. Une grammaire multilingue
partage?e pour la traduction automatique de la parole.
In Proceedings of TALN 2006, Leuwen, Belgium.
[Bouillon et alto appear 2007a] P. Bouillon, M. Rayner,
M. Santaholma, and M. Starlander. to appear 2007a.
Les ellipses dans un syste`me de traduction automa-
tique de la parole. In Proceedings of TALN 2006,
Toulouse, France.
[Bouillon et alto appear 2007b] P. Bouillon, M. Rayner,
B. Novellas Vall, Y. Nakao, M. Santaholma, M. Star-
lander, and N. Chatzichrisafis. to appear 2007b. Une
grammaire partage?e multi-ta?che pour le traitement de
la parole : application aux langues romanes. Traite-
ment Automatique des Langues.
[Census2007] U.S. Census, 2007. Selected Social Char-
acteristics in the United States: 2005. Data Set: 2005
American Community Survey. Available here.
[Chatzichrisafis et al2006] N. Chatzichrisafis, P. Bouil-
lon, M. Rayner, M. Santaholma, M. Starlander, and
B.A. Hockey. 2006. Evaluating task performance for
a unidirectional controlled language medical speech
translation system. In Proceedings of the HLT-NAACL
International Workshop on Medical Speech Transla-
tion, pages 9?16, New York.
[Flores2005] G. Flores. 2005. The impact of medical in-
terpreter services on the quality of health care: A sys-
tematic review. Medical Care Research and Review,
62:255?299.
[Flores2006] G. Flores. 2006. Language barriers to
health care in the united states. New England Journal
of Medicine, 355:229?231.
[Fluential2007] Fluential, 2007.
http://www.fluentialinc.com. As of 24 March
2007.
[Graddol2004] D. Graddol. 2004. The future of lan-
guage. Science, 303:1329?1331.
[Kittredge2003] R. I. Kittredge. 2003. Sublanguages and
comtrolled languages. In R. Mitkov, editor, The Ox-
ford Handbook of Computational Linguistics, pages
430?447. Oxford University Press.
[Michie et al2003] S. Michie, J. Miles, and J. Weinman.
2003. Patient-centeredness in chronic illness: what is
it and does it matter? Patient Education and Counsel-
ing, 51:197?206.
[Mitamura1999] T. Mitamura. 1999. Controlled lan-
guage for multilingual machine translation. In Pro-
ceedings of Machine Translation Summit VII, Singa-
pore.
[Phraselator2007] Phraselator, 2007.
http://www.voxtec.com/. As of 24 March 2007.
[Rayner et al2005a] M. Rayner, P. Bouillon,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao.
2005a. A methodology for comparing grammar-based
and robust approaches to speech understanding. In
Proceedings of the 9th International Conference
on Spoken Language Processing (ICSLP), pages
1103?1107, Lisboa, Portugal.
[Rayner et al2005b] M. Rayner, B.A. Hockey, J.M. Ren-
ders, N. Chatzichrisafis, and K. Farrell. 2005b. A
voice enabled procedure browser for the International
Space Station. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (interactive poster and demo track), Ann Arbor,
MI.
[Rayner et al2006] M. Rayner, B.A. Hockey, and
P. Bouillon. 2006. Putting Linguistics into Speech
Recognition: The Regulus Grammar Compiler. CSLI
Press, Chicago.
[Shieber et al1990] S. Shieber, G. van Noord, F.C.N.
Pereira, and R.C. Moore. 1990. Semantic-head-driven
generation. Computational Linguistics, 16(1).
[Somers2006] H. Somers. 2006. Language engineering
and the path to healthcare: a user-oriented view. In
Proceedings of the HLT-NAACL International Work-
shop on Medical Speech Translation, pages 32?39,
New York.
[Starlander et al2005] M. Starlander, P. Bouillon,
N. Chatzichrisafis, M. Santaholma, M. Rayner, B.A.
Hockey, H. Isahara, K. Kanzaki, and Y. Nakao. 2005.
Practising controlled language through a help system
integrated into the medical speech translation system
(MedSLT). In Proceedings of MT Summit X, Phuket,
Thailand.
[Stewart1995] M.A. Stewart. 1995. Effective physician-
patient communication and health outcomes: a review.
Canadian Medical Association Journal, 152:1423?
1433.
48
Proceedings of SPEECHGRAM 2007, pages 49?52,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Development Environment
for Building Grammar-Based Speech-Enabled Applications
Elisabeth Kron1, Manny Rayner1,2, Marianne Santaholma1, Pierrette Bouillon1
1 University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
elisabethkron@yahoo.co.uk
Marianne.Santaholma@eti.unige.ch
Pierrette.Bouillon@issco.unige.ch
2 Powerset, Inc.
475 Brannan Street
San Francisco, CA 94107
manny@powerset.com
Abstract
We present a development environment for
Regulus, a toolkit for building unification
grammar-based speech-enabled systems, fo-
cussing on new functionality added over the
last year. In particular, we will show an
initial version of a GUI-based top-level for
the development environment, a tool that
supports graphical debugging of unification
grammars by cutting and pasting of deriva-
tion trees, and various functionalities that
support systematic development of speech
translation and spoken dialogue applications
built using Regulus.
1 The Regulus platform
The Regulus platform is a comprehensive toolkit
for developing grammar-based speech-enabled sys-
tems that can be run on the commercially avail-
able Nuance recognition environment. The plat-
form has been developed by an Open Source con-
sortium, the main partners of which have been
NASA Ames Research Center and Geneva Uni-
versity, and is freely available for download from
the SourceForge website1. Regulus has been used
to build several large systems, including Geneva
University?s MedSLT medical speech translator
(Bouillon et al, 2005) and NASA?s Clarissa proce-
dure browser (Rayner et al, 2005b)2.
Regulus is described at length in
(Rayner et al, 2006), the first half of which consists
of an extended tutorial introduction. The release
1http://sourceforge.net/projects/regulus/
2http://ic.arc.nasa.gov/projects/clarissa/
also includes extensive online documentation,
including several example applications.
The core functionality offered by Regulus is com-
pilation of typed unification grammars into parsers,
generators, and Nuance-formatted CFG language
models, and hence also into Nuance recognition
packages. Small unification grammars can be com-
piled directly into executable forms. The central
idea of Regulus, however, is to base as much of
the development work as possible on large, domain-
independent resource grammars. A resource gram-
mar for English is available from the Regulus web-
site; similar grammars for several other languages
have been developed under the MedSLT project at
Geneva University, and can be downloaded from the
MedSLT SourceForge website3.
Large resource grammars of this kind are over-
general as they stand, and it is not possible to com-
pile them directly into efficient recognisers or gener-
ators. The platform, however, provides tools, driven
by small corpora of examples, that can be used to
create specialised versions of these general gram-
mars using the Explanation Based Learning (EBL)
algorithm. We have shown in a series of exper-
iments that suitably specialised grammars can be
compiled into efficient executable forms. In particu-
lar, recognisers built in this way are very competitive
with ones created using statistical training methods
(Rayner et al, 2005a).
The Regulus platform also supplies a framework
for using the compiled resources ? parsers, gen-
erators and recognisers ? to build speech transla-
tion and spoken dialogue applications. The envi-
ronment currently supports 75 different commands,
3http://sourceforge.net/projects/medslt
49
which can be used to carry out a range of func-
tions including compilation of grammars into var-
ious forms, debugging of grammars and compiled
resources, and testing of applications. The environ-
ment exists in two forms. The simpler one, which
has been available from the start of the project, is a
command-line interface embedded within the SICS-
tus Prolog top-level. The focus will however be on
a new GUI-based environment, which has been un-
der development since late 2006, and which offers
a more user-friendly graphical/menu-based view of
the underlying functionality.
In the rest of the paper, we outline how Regulus
supports development both at the level of grammars
(Section 2), and at the level of the applications that
can be built using the executable forms derived from
them (Section 3).
2 Developing unification grammars
The Regulus grammar development toolset borrows
ideas from several other systems, in particular the
SRI Core Language Engine (CLE) and the Xerox
Language Engine (XLE). The basic functionalities
required are uncontroversial. As usual, the Regulus
environment lets the user parse example sentences
to create derivation trees and logical forms; in the
other direction, if the grammar has also been com-
piled into a generator, the user can take a logical
form and use it to generate a surface string and an-
other derivation tree. Once a derivation tree has been
created, either through parsing or through genera-
tion, it is possible to examine individual nodes to
view the information associated with each one. Cur-
rently, this information consists of the syntactic fea-
tures, the piece of logical form built up at the node,
and the grammar rule or lexical entry used to create
it.
The Regulus environment also provides a more
elaborate debugging tool, which extends the ear-
lier ?grammar stepper? implemented under the CLE
project. Typically, a grammar development problem
has the following form. The user finds a bad sen-
tence B which fails to get a correct parse; however,
there are several apparently similar or related sen-
tences G1...Gn which do get correct parses. In most
cases, the explanation is that some rule which would
appear in the intended parse for B has an incorrect
feature-assignment.
A simple strategy for investigating problems of
this kind is just to examine the structures of B and
G1...Gn by eye, and attempt to determine what the
crucial difference is. An experienced developer,
who is closely familiar with the structure of the
grammar, will quite often be able to solve the prob-
lem in this way, at least in simple cases. ?Solving
by inspection? is not, however, very systematic, and
with complex rule bugs it can be hard even for ex-
perts to find the offending feature assignment. The
larger the grammar becomes, especially in terms of
the average number of features per category, the
more challenging the ad hoc debugging approach
becomes.
A more systematic strategy was pioneered in the
CLE grammar stepper. The developer begins by
looking at the working examples G1...Gn, to de-
termine what the intended correct structure would
be for B. They then build up the corresponding
structure for the bad example, starting at the bot-
tom with the lexical items and manually selecting
the rules used to combine them. At some point, a
unification will fail, and this will normally reveal the
bad feature assignment. The problem is that manual
bottom-up construction of the derivation tree is very
time-consuming, since even quite simple trees will
usually have at least a dozen nodes.
The improved strategy used in the Regulus gram-
mar stepper relies on the fact that the G1...Gn can
usually be constructed to include all the individual
pieces of the intended derivation tree for B, since in
most cases the feature mis-match arises when com-
bining two subtrees which are each internally con-
sistent. We exploit this fact by allowing the devel-
oper to build up the tree for B by cutting up the trees
for G1...Gn into smaller pieces, and then attempting
to recombine them. Most often, it is enough to take
two of the Gi, cut an appropriate subtree out of each
one, and try to unify them together; this means that
the developer can construct the tree for B with only
five operations (two parses, two cuts, and a join),
rather than requiring one operation for each node in
B, as in the bottom-up approach.
A common pattern is that B and G1 are identical,
except for one noun-phrase constituent NP , and G2
consists of NP on its own. To take an example from
the MedSLT domain, B could be ?does the morning
50
Figure 1: Example of using the grammar stepper to discover a feature mismatch. The window on the
right headed ?Stepper? presents the list of available trees, together with the controls. The windows headed
?Tree 1? and ?Tree 4? present the trees for item 1 (?does red wine give you headaches?) and item 4 (?the
morning?). The popup window on the lower right presents the feature mismatch information.
give you headaches??, G1 the similar sentence ?does
red wine give you headaches?? and G2 the single
NP ?the morning?. We cut out the first NP subtree
from G1 to produce what is in effect a tree with an
NP ?slash category?, that can be rendered as ?does
NP give you headaches??; call this G?1. We then cut
out the single NP subtree (this accounts for most,
but not all, of the derivation) from G2, to produce
G?2. By attempting to unify G?2 with the NP ?hole?
left in G?1, we can determine the exact nature of the
feature mismatch. We discover that the problem is
in the sortal features: the value of the sortal feature
on G?2 is time, but the corresponding feature-value
in the NP ?hole? is action\/cause.
Figure 1 contains a screenshot of the development
environment in the example above, showing the state
when the feature mismatch is revealed. A detailed
example, including screenshots for each step, is in-
cluded in the online Regulus GUI tutorial4.
4Available in the file doc/RegulusGUITutorial.pdf from the
SourceForge Regulus website
3 Developing applications
The Regulus platform contains support for both
speech translation and spoken dialogue applications.
In each case, it is possible to run the development
top-loop in a mode appropriate to the type of appli-
cation, including carrying out systematic regression
testing using both text and speech input. For both
types of application, the platform assumes a uniform
architecture with pre-specified levels of representa-
tion.
Due to shortage of space, and because it is the
better-developed of the two, we focus on speech
translation. The framework is interlingua-based,
and also permits simple context-based translation
involving resolution of ellipsis5. Processing goes
through the following sequence of representations:
1. Spoken utterance in source language.
2. Recognised words in source language.
5Although it is often possible to translate ellipsis as ellipsis
in closely related language pairs, this is usually not correct in
more widely separated ones.
51
3. Source logical form. Source logical form and
all other levels of representation are (almost)
flat lists of attribute/value pairs.
4. ?Source discourse representation?. A regu-
larised version of the source logical form, suit-
able for carrying out ellipsis resolution.
5. ?Resolved source discourse representation?.
The output resulting from carrying out any nec-
essary ellipsis processing on the source dis-
course representation. Typically this will add
material from the preceding context represen-
tation to create a representation of a complete
clause.
6. Interlingua. A language-independent version
of the representation.
7. Target logical form.
8. Surface words in target language.
The transformations from source logical form
to source discourse representation, from resolved
source discourse representation to interlingua, and
from interlinga to target logical form are defined
using translation rules which map lists of at-
tribute/value pairs to lists of attribute/value pairs.
The translation trace includes all the levels of rep-
resentation listed above, the translation rules used at
each stage, and other information omitted here. The
?translation mode? window provided by the devel-
opment environment makes all these fields available
in a structured form which allows the user to select
for display only those that are currently of interest.
The framework for spoken dialogue systems is simi-
lar, except that in the last three steps ?Interlingua? is
replaced by ?Dialogue move?, ?Target logical form?
by ?Abstract response?, and ?Surface words in target
language? by ?Concrete response?.
The platform contains tools for performing sys-
tematic regression testing of both speech translation
and spoken dialogue applications, using both text
and speech input. Input in the required modality is
taken from a specified file and passed through all
stages of processing, with the output being written
to another file. The user is able to annotate the re-
sults with respect to correctness (the GUI presents
a simple menu-based interface for doing this) and
save the judgements permanently, so that they can
be reused for future runs.
The most interesting aspects of the framework
involve development of spoken dialogue systems.
With many other spoken dialogue systems, the ef-
fect of a dialogue move is distributed throughout the
program state, and true regression testing is very dif-
ficult. Here, our side-effect free approach to dia-
logue management means that the DM can be tested
straightforwardly as an isolated component, since
the context is fully encapsulated as an object. The
theoretical issues involved are explored further in
(Rayner and Hockey, 2004).
References
[Bouillon et al2005] P. Bouillon, M. Rayner,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara.
2005. A generic multi-lingual open source platform
for limited-domain medical speech translation. In
Proceedings of the 10th Conference of the European
Association for Machine Translation (EAMT), pages
50?58, Budapest, Hungary.
[Rayner and Hockey2004] M. Rayner and B.A. Hockey.
2004. Side effect free dialogue management in a voice
enabled procedure browser. In Proceedings of the 8th
International Conference on Spoken Language Pro-
cessing (ICSLP), Jeju Island, Korea.
[Rayner et al2005a] M. Rayner, P. Bouillon,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao.
2005a. Methodology for comparing grammar-based
and robust approaches to speech understanding. In
Proceedings of the 9th International Conference
on Spoken Language Processing (ICSLP), pages
1103?1107, Lisboa, Portugal.
[Rayner et al2005b] M. Rayner, B.A. Hockey, J.M. Ren-
ders, N. Chatzichrisafis, and K. Farrell. 2005b. A
voice enabled procedure browser for the international
space station. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (interactive poster and demo track), Ann Arbor,
MI.
[Rayner et al2006] M. Rayner, B.A. Hockey, and
P. Bouillon. 2006. Putting Linguistics into Speech
Recognition: The Regulus Grammar Compiler. CSLI
Press, Chicago.
52
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 32?35
Manchester, August 2008
The 2008 MedSLT System
Manny Rayner1, Pierrette Bouillon1, Jane Brotanek2, Glenn Flores2
Sonia Halimi1, Beth Ann Hockey3, Hitoshi Isahara4, Kyoko Kanzaki4
Elisabeth Kron5, Yukie Nakao6, Marianne Santaholma1
Marianne Starlander1, Nikos Tsourakis1
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon,Nikolaos.Tsourakis}@issco.unige.ch
{Sonia.Halimi,Marianne.Santaholma,Marianne.Starlander}@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
4 NICT, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
{isahara,kanzaki}@nict.go.jp
5 3 St Margarets Road, Cambridge CB3 0LT, England
elisabethkron@yahoo.co.uk
6 University of Nantes, LINA, 2, rue de la Houssinie`re, BP 92208 44322 Nantes Cedex 03
yukie.nakao@univ-nantes.fr
Abstract
MedSLT is a grammar-based medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several dif-
ferent subdomains and multiple language
pairs. Vocabulary ranges from about 350 to
1000 surface words, depending on the lan-
guage and subdomain. We will demo three
different versions of the system: an any-
to-any multilingual version involving the
languages Japanese, English, French and
Arabic, a bidirectional English ? Span-
ish version, and a mobile version run-
ning on a hand-held PDA. We will also
demo the Regulus development environ-
ment, focussing on features which sup-
port rapid prototyping of grammar-based
speech translation systems.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1 Introduction
MedSLT is a medium-vocabulary grammar-based
medical speech translation system built on top of
the Regulus platform (Rayner et al, 2006). It is
intended for use in doctor-patient diagnosis dia-
logues, and provides coverage of several subdo-
mains and a large number of different language-
pairs. Coverage is based on standard examina-
tion questions obtained from physicians, and fo-
cusses primarily on yes/no questions, though there
is also support for WH-questions and elliptical ut-
terances.
Detailed descriptions of MedSLT can be found
in earlier papers (Bouillon et al, 2005; Bouil-
lon et al, 2008)1. In the rest of this note, we
will briefly sketch several versions of the system
that we intend to demo at the workshop, each of
which displays new features developed over the
last year. Section 2 describes an any-language-to-
any-language multilingual version of the system;
Section 3, a bidirectional English ? Spanish ver-
sion; Section 4, a version running on a mobile PDA
1All MedSLT publications are available on-line
at http://www.issco.unige.ch/projects/
medslt/publications.shtml.
32
platform; and Section 5, the Regulus development
environment.
2 A multilingual version
During the last few months, we have reorganised
the MedSLT translation model in several ways2. In
particular, we give a much more central role to the
interlingua; we now treat this as a language in its
own right, defined by a normal Regulus grammar,
and using a syntax which essentially amounts to
a greatly simplified form of English. Making the
interlingua into another language has made it easy
to enforce tight constraints on well-formedness of
interlingual semantic expressions, since checking
well-formedness now just amounts to performing
generation using the interlingua grammar.
Another major advantage of the scheme is that
it is also possible to systematise multilingual de-
velopment, and only work with translation from
source language to interlingua, and from interlin-
gua to target language; here, the important point
is that the human-readable interlingua surface syn-
tax makes it feasible in practice to evaluate transla-
tion between normal languages and the interlingua.
Development of rules for translation to interlingua
is based on appropriate corpora for each source
language. Development of rules for translating
from interlingua uses a corpus which is formed by
merging together the results of translating each of
the individual source-language corpora into inter-
lingua.
We will demonstrate our new capabilities in
interlingua-based translation, using a version of
the system which translates doctor questions in the
headache domain from any language to any lan-
guage in the set {English, French, Japanese, Ara-
bic}. Table 1 gives examples of the coverage of the
English-input headache-domain version, and Ta-
ble 2 summarises recognition performance in this
domain for the three input languages where we
have so far performed serious evaluations. Differ-
ences in the sizes of the recognition vocabularies
are primarily due to differences in use of inflec-
tion.
3 A bidirectional version
The system from the preceding section is unidi-
rectional; all communication is in the doctor-to-
patient direction, the expectation being that the pa-
2The ideas in the section are described at greater length in
(Bouillon et al, 2008).
Language Vocab WER SemER
English 447 6% 11%
French 1025 8% 10%
Japanese 422 3% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognis-
ers. ?Vocab? = number of surface words in source
language recogniser vocabulary; ?WER? = Word
Error Rate for source language recogniser, on in-
coverage material; ?SemER? = semantic error rate
for source language recogniser, on in-coverage
material.
tient will respond non-verbally. Our second demo,
an early version of which is described in (Bouillon
et al, 2007), supports bidirectional translation for
the sore throat domain, in the English ? Spanish
pair. Here, the English-speaking doctor typically
asks WH-questions, and the Spanish-speaking pa-
tient responds with elliptical utterances, which are
translated as full sentence responses. A short ex-
ample dialogue is shown in Table 3.
Doctor: Where is the pain?
?Do?nde le duele?
Patient: En la garganta.
I experience the pain in my throat.
Doctor: How long have you had a pain
in your throat?
?Desde cua?ndo le duele la garganta?
Patient: Ma?s de tres d??as.
I have experienced the pain in my
throat for more than three days.
Table 3: Short dialogue with bidirectional English
? Spanish version. System translations are in ital-
ics.
4 A mobile platform version
When we have shown MedSLT to medical profes-
sionals, one of the most common complaints has
been that a laptop is not an ideal platform for use
in emergency medical situations. Our third demo
shows an experimental version of the system us-
ing a client/server architecture. The client, which
contains the user interface, runs on a Nokia Linux
N800 Internet Tablet; most of the heavy process-
ing, including in particular speech recognition, is
hosted on the remote server, with the nodes com-
municating over a wireless network. A picture of
33
Where? Is the pain above your eye?
When? Have you had the pain for more than a month?
How long? Does the pain typically last a few minutes?
How often? Do you get headaches several times a week?
How? Is it a stabbing pain?
Associated symptoms? Do you vomit when you get the headaches?
Why? Does bright light make the pain worse?
What helps? Does sleep make the pain better?
Background? Do you have a history of sinus disease?
Table 1: Examples of English MedSLT coverage
the tablet, showing the user interface, is presented
in Figure 1. The sentences appearing under the
back-translation at the top are produced by an on-
line help component, and are intended to guide the
user into the grammar?s coverage (Chatzichrisafis
et al, 2006).
The architecture is described further in
(Tsourakis et al, 2008), which also gives perfor-
mance results for another Regulus applications.
These strongly suggest that recognition perfor-
mance in the client/server environment is no
worse than on a laptop, as long as a comparable
microphone is used.
5 The development environment
Our final demo highlights the new Regulus devel-
opment environment (Kron et al, 2007), which has
over the last few months acquired a large amount
of new functionality designed to facilitate rapid
prototyping of spoken language applications3 . The
developer initially constructs and debugs her com-
ponents (grammar, translation rules etc) in a text
view. As soon as they are consistent, she is able
to compile the source-language grammar into a
recogniser, and combine this with other compo-
nents to run a complete speech translation system
within the development environment. Connections
between components are defined by a simple con-
fig file. Figure 2 shows an example.
References
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
3This work is presented in a paper currently under review.
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Bouillon, P., G. Flores, M. Starlander,
N. Chatzichrisafis, M. Santaholma, N. Tsourakis,
M. Rayner, and B.A. Hockey. 2007. A bidirectional
grammar-based medical speech translator. In Pro-
ceedings of the ACL Workshop on Grammar-based
Approaches to Spoken Language Processing, pages
41?48, Prague, Czech Republic.
Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-
hara, N. Tsourakis, M. Starlander, B.A. Hockey, and
M. Rayner. 2008. Developing non-european trans-
lation pairs in a medium-vocabulary medical speech
translation system. In Proceedings of LREC 2008,
Marrakesh, Morocco.
Chatzichrisafis, N., P. Bouillon, M. Rayner, M. Santa-
holma, M. Starlander, and B.A. Hockey. 2006. Eval-
uating task performance for a unidirectional con-
trolled language medical speech translation system.
In Proceedings of the HLT-NAACL International
Workshop on Medical Speech Translation, pages 9?
16, New York.
Kron, E., M. Rayner, P. Bouillon, and M. Santa-
holma. 2007. A development environment for build-
ing grammar-based speech-enabled applications. In
Proceedings of the ACL Workshop on Grammar-
based Approaches to Spoken Language Processing,
pages 49?52, Prague, Czech Republic.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
Tsourakis, N., M. Georghescul, P. Bouillon, and
M. Rayner. 2008. Building mobile spoken dialogue
applications using regulus. In Proceedings of LREC
2008, Marrakesh, Morocco.
34
Figure 1: Mobile version of the MedSLT system, running on a Nokia tablet.
Figure 2: Speech to speech translation from the development environment, using a Japanese to Arabic
translator built from MedSLT components. The user presses the Recognise button (top right), speaks in
Japanese, and receives a spoken translation in Arabic together with screen display of various processing
results. The application is defined by a config file which combines a Japanese recogniser and analy-
sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generation
grammar, and recorded Arabic wavfiles used to construct a spoken result.
35
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 9?16
Manchester, August 2008
Making Speech Look Like Text
in the Regulus Development Environment
Elisabeth Kron
3 St Margarets Road, Cambridge CB3 0LT, England
elisabethkron@yahoo.co.uk
Manny Rayner, Marianne Santaholma, Pierrette Bouillon, Agnes Lisowska
University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Marianne.Santaholma@eti.unige.ch
Pierrette.Bouillon@issco.unige.ch
Agnes.Lisowska@issco.unige.ch
Abstract
We present an overview of the de-
velopment environment for Regulus, an
Open Source platform for construction of
grammar-based speech-enabled systems,
focussing on recent work whose goal has
been to introduce uniformity between text
and speech views of Regulus-based appli-
cations. We argue the advantages of be-
ing able to switch quickly between text and
speech modalities in interactive and offline
testing, and describe how the new func-
tionalities enable rapid prototyping of spo-
ken dialogue systems and speech transla-
tors.
1 Introduction
Sex is not love, as Madonna points out at the be-
ginning of her 1992 book Sex, and love is not
sex. None the less, even people who agree with
Madonna often find it convenient to pretend that
these two concepts are synonymous, or at least
closely related. Similarly, although text is not
speech, and speech is not text, it is often conve-
nient to pretend that they are both just different as-
pects of the same thing.
In this paper, we will explore the similarities and
differences between text and speech, in the con-
crete setting of Regulus, a development environ-
ment for grammar based spoken dialogue systems.
Our basic goal will be to make text and speech
processing as similar as possible from the point
of view of the developer. Specifically, we arrange
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
things so that the developer is able to develop her
system using a text view; she will write text-based
rules, and initially test the system using text in-
put and output. At any point, she will be able to
switch to a speech view, compiling the text-based
processing rules into corresponding speech-based
versions, and test the resulting speech-based sys-
tem using speech input and output.
Paradoxically, the reason why it is so important
to be able to switch seamlessly between text and
speech viewpoints is that text and speech are in
fact not the same. For example, a pervasive prob-
lem in speech recognition is that of easily confus-
able pairs of words. This type of problem is of-
ten apparent after just a few minutes when running
the system in speech mode (the recogniser keeps
recognising one word as the other), but is invis-
ible in text mode. More subtly, some grammar
problems can be obvious in text mode, but hard
to see in speech mode. For instance, articles like
?the? and ?a? are short, and usually pronounced
unstressed, which means that recognisers can be
reasonably forgiving about whether or not to hy-
pothesise them when they are required or not re-
quired by the recognition grammar. In text mode, it
will immediately be clear if the grammar requires
an article in a given NP context: incorrect vari-
ants will fail to parse. In speech mode, the symp-
toms are far less obvious, and typically amount to
no more than a degradation in recognition perfor-
mance.
The rest of the paper is structured as follows.
Sections 2 and 3 provide background on the Reg-
ulus platform and development cycle respectively.
Section 4 describes speech and text support in the
interactive development environment, and 5 de-
scribes how the framework simplifies the task of
9
switching between modalities in regression testing.
Section 6 concludes.
2 The Regulus platform
The Regulus platform is a comprehensive toolkit
for developing grammar-based speech-enabled
systems that can be run on the commercially avail-
able Nuance recognition environment. The plat-
form has been developed by an Open Source con-
sortium, the main partners of which have been
NASA Ames Research Center and Geneva Univer-
sity, and is freely available for download from the
SourceForge website1. In terms of ideas (though
not code), Regulus is a descendent of SRI Inter-
national?s CLE and Gemini platforms (Alshawi,
1992; Dowding et al, 1993); other related systems
are LKB (Copestake, 2002), XLE (Crouch et al,
2008) and UNIANCE (Bos, 2002).
Regulus has already been used to build sev-
eral large applications. Prominent examples
are Geneva University?s MedSLT medical speech
translator (Bouillon et al, 2005), NASA?s Clarissa
procedure browser (Rayner et al, 2005) and Ford
Research?s experimental SDS in-car spoken dia-
logue system, which was awarded first prize at
the 2007 Ford internal demo fair. Regulus is de-
scribed at length in (Rayner et al, 2006), the first
half of which consists of an extended tutorial in-
troduction. The release includes a command-line
development environment, extensive online docu-
mentation, and several example applications.
The core functionality offered by Regulus is
compilation of typed unification grammars into
parsers, generators, and Nuance-formatted CFG
language models, and hence also into Nuance
recognition packages. These recognition packages
produced by Regulus can be invoked through the
Regulus SpeechServer (?Regserver?), which pro-
vides an interface to the underlying Nuance recog-
nition engine. The value added by the Regserver
is to provide a view of the recognition process
based on the Regulus unification grammar frame-
work. In particular, recognition results, originally
produced in the Nuance recognition platform?s in-
ternal format, are reformatted into the semantic no-
tation used by the Regulus grammar formalism.
There is extensive support within the Regulus
toolkit for development of both speech translation
and spoken dialogue applications. Spoken dia-
1http://sourceforge.net/projects/
regulus/
logue applications (Rayner et al, 2006, Chapter 5)
use a rule-based side-effect free state update model
similar in spirit to that described in (Larsson and
Traum, 2000). Very briefly, there are three types of
rules: state update rules, input management rules,
and output management rules. State update rules
take as input the current state, and a ?dialogue
move?; they produce as output a new state, and an
?abstract action?. Dialogue moves are abstract rep-
resentations of system inputs; these inputs can ei-
ther be logical forms produced by the grammar, or
non-speech inputs (for example, mouse-clicks in a
GUI). Similarly, abstract actions are, as the name
suggests, abstract representations of the concrete
actions the dialogue system will perform, for ex-
ample speaking or updating a visual display. Input
management rules map system inputs to dialogue
moves; output management rules map abstract ac-
tions to system outputs.
Speech translation applications are also rule-
based, using an interlingua model (Rayner et al,
2006, Chapter 6). The developer writes a second
grammar for the target language, using Regulus
tools to compile it into a generator; mappings from
source representation to interlingua, and from in-
terlingua to target representation, are defined by
sets of translation rules. The interlingua itself is
specified using a third Regulus grammar (Bouillon
et al, 2008).
To summarise, the core of a Regulus application
consists of several different linguistically oriented
rule-sets, some of which can be interpreted in ei-
ther a text or a speech modality, and all of which
need to interact correctly together. In the next sec-
tion, we describe how this determines the nature of
the Regulus development cycle.
3 The Regulus development cycle
Small unification grammars can be compiled di-
rectly into executable forms. The central idea
of Regulus, however, is to base as much of
the development work as possible on large,
domain-independent, linguistically motivated re-
source grammars. A resource grammar for En-
glish is available from the Regulus website; similar
grammars for several other languages have been
developed under the MedSLT project at Geneva
University, and can be downloaded from the Med-
SLT SourceForge website2. Regulus contains
2http://sourceforge.net/projects/
medslt
10
an extensive set of tools that permit specialised
domain-specific grammars to be extracted from the
larger resource grammars, using example-based
methods driven by small corpora (Rayner et al,
2006, Chapter 7). At the beginning of a project,
these corpora can consist of just a few dozen exam-
ples; for a mature application, they will typically
have grown to something between a few hundred
and a couple of thousand sentences. Specialised
grammars can be compiled by Regulus into effi-
cient recognisers and generators.
As should be apparent from the preceding de-
scription, the Regulus architecture is designed to
empower linguists to the maximum possible ex-
tent, in terms of increasing their ability directly
to build speech enabled systems; the greater part
of the core development teams in the large Reg-
ulus projects mentioned in Section 1 have indeed
come from linguistics backgrounds. Experience
with Regulus has however shown that linguists are
not quite as autonomous as they are meant to be,
and in particular are reluctant to work directly with
the speech view of the application. There are sev-
eral reasons.
First, non-toy Regulus projects require a range
of competences, including both software engineer-
ing and linguistics. In practice, linguist rule-
writers have not been able to test their rules in
the speech view without writing glue code, scripts,
and other infrastructure required to tie together the
various generated components. These are not nec-
essarily things that they want to spend their time
doing. The consequence can easily be that the lin-
guists end up working exclusively in the text view,
and over-refine the text versions of the rule-sets.
From a project management viewpoint, this results
in bad prioritisation decisions, since there are more
pressing issues to address in the speech view.
A second reason why linguist rule-writers have
been unhappy working in the speech view is the
lack of reproducibility associated with speech in-
put. One can type ?John loves Mary? into a text-
processing system any number of times, and ex-
pect to get the same result. It is much less reason-
able to expect to get the same result each time if
one says ?John loves Mary? to a speech recogniser.
Often, anomalous results occur, but cannot be de-
bugged in a systematic fashion, leading to general
frustration. The result, once again, is that linguists
have preferred to stick with the text view, where
they feel at home.
Yet another reason why rule-writers tend to
limit themselves to the text view is simply the
large number of top-level commands and inter-
mediate compilation results. The current Regulus
command-line environment includes over 110 dif-
ferent commands, and compilation from the initial
resource grammar to the final Nuance recognition
package involves creating a sequence of five com-
pilation steps, each of which requires the output
created by the preceding one. This makes it diffi-
cult for novice users to get their bearings, and in-
creases their cognitive load. Additionally, once the
commands for the text view have been mastered,
there is a certain temptation to consider that these
are enough, since the text and speech views can
reasonably be perceived as fairly similar.
In the next two sections, we describe an en-
hanced development environment for Regulus,
which addresses the key problems we have just
sketched. From the point of view of the linguist
rule-writer, we want speech-based development to
feel more like text-based development.
4 Speech and text in the online
development environment
The Regulus GUI (Kron et al, 2007) is intended
as a complete redesign of the development envi-
ronment, which simultaneously attacks all of the
central issues. Commands are organised in a struc-
tured set of functionality-based windows, each of
which has an appropriate set of drop-down menus.
Following normal GUI design practice (Dix et al,
1998, Chapters 3 and 4); (Jacko and Sears, 2003,
Chapter 13), only currently meaningful commands
are executable in each menu, with the others shown
greyed out.
Both compile-time and run-time speech-related
functionality can be invoked directly from the
command menus, with no need for external scripts,
Makefiles or glue code. Focussing for the moment
on the specific case of developing a speech transla-
tion application, the rule-writer will initially write
and debug her rules in text mode. She will be able
to manipulate grammar rules and derivation trees
using the Stepper window (Figure 1; cf. also (Kron
et al, 2007)), and load and test translation rules
in the Translate window (Figure 2). As soon as
the grammar is consistent, it can at any point be
compiled into a Nuance recognition package us-
ing the command menus. The resulting recogniser,
together with other speech resources (license man-
11
Figure 1: Using the Stepper window to browse trees in the Toy1 grammar from (Rayner et al, 2006,
Chapter 4). The upper left window shows the analysis tree for ?switch on the light in the kitchen?; the
lower left window shows one of the subtrees created by cutting the first tree at the higher NP node. Cut
subtrees can be recombined for debugging purposes (Kron et al, 2007).
Figure 2: Using the Translate window to test the toy English ? French translation application from
(Rayner et al, 2006, Chapter 6). The to- and from-interlingua rules used in the example are shown in the
two pop-up windows at the top of the figure.
12
regulus_config(regulus_grammar,
[toy1_grammars(toy1_declarations),
toy1_grammars(toy1_rules),
toy1_grammars(toy1_lexicon)]).
regulus_config(top_level_cat, ?.MAIN?).
regulus_config(nuance_grammar, toy1_runtime(recogniser)).
regulus_config(to_interlingua_rules,
toy1_prolog(?eng_to_interlingua.pl?)).
regulus_config(from_interlingua_rules,
toy1_prolog(?interlingua_to_fre.pl?)).
regulus_config(generation_rules, toy1_runtime(?generator.pl?)).
regulus_config(nuance_language_pack,
?English.America?).
regulus_config(nuance_compile_params, [?-auto_pron?, ?-dont_flatten?]).
regulus_config(translation_rec_params,
[package=toy1_runtime(recogniser), grammar=?.MAIN?]).
regulus_config(tts_command,
?vocalizer -num_channels 1 -voice juliedeschamps -voices_from_disk?).
Figure 3: Config file for a toy English ? French speech translation application, showing items relevant
to the speech view. Some declarations have been omitted for expositional reasons.
ager, TTS engine etc), can then be started using a
single menu command.
In accordance with the usual Regulus design
philosophy of declaring all the resources associ-
ated with a given application in its config file, the
speech resources are also specified here. Figure 3
shows part of the config file for a toy translation
application, in particular listing all the declara-
tions relevant to the speech view. If we needed to
change the speech resources, this would be done
just by modifying the last four lines. For example,
the config file as shown specifies construction of
a recogniser using acoustic models appropriate to
American English. We could change this to British
English by replacing the entry
regulus_config(nuance_language_pack,
?English.America?).
with
regulus_config(nuance_language_pack,
?English.UK?).
When the speech resources have been loaded,
the Translate window can take input equally easily
in text or speech mode; the Translate button pro-
cesses written text from the input pane, while the
Recognise button asks for spoken input. In each
case, the input is passed through the same process-
ing stages of source-to-interlingua and interlingua-
to-target translation, followed by target-language
generation. If a TTS engine or a set of recorded
target language wavfiles is specified, they are used
to realise the final result in spoken form (Figure 4).
Every spoken utterance submitted to recognition
is logged as a SPHERE-headed wavfile, in a time-
stamped directory started at the beginning of the
current session; this directory also contains a meta-
data file, which associates each recorded wavfile
with the recognition result it produced. The Trans-
late window?s History menu is constructed using
the meta-data file, and allows the user to select any
recorded utterance, and re-run it through the sys-
tem as though it were a new speech input. The
consequence is that speech input becomes just as
reproducible as text, with corresponding gains for
interactive debugging in speech mode.
5 Speech and text in regression testing
In earlier versions of the Regulus development
environment (Rayner et al, 2006, ?6.6), regres-
sion testing in speech mode was all based on
Nuance?s batchrec utility, which permits of-
fline recognition of a set of recorded wavfiles.
A test suite for spoken regression testing conse-
quently consisted of a list of wavfiles. These
were first passed through batchrec; outputs
were then post-processed into Regulus form, and
finally passed through Regulus speech understand-
ing modules, such as translation or dialogue man-
agement.
As Regulus applications grow in complexity,
this model has become increasingly inadequate,
since system input is very frequently not just a
list of monolingual speech events. In a multi-
modal dialogue system, input can consist of either
speech or screen events (text/mouse-clicks); con-
text is generally important, and the events have to
be processed in the order in which they occurred.
Dialogue systems which control real or simulated
robots, like the Wheelchair application of (Hockey
13
Figure 4: Speech to speech translation from the GUI, using a Japanese to Arabic translator built from
MedSLT components (Bouillon et al, 2008). The user presses the Recognise button (top right), speaks in
Japanese, and receives a spoken translation in Arabic together with screen display of various processing
results. The application is defined by a config file which combines a Japanese recogniser and analy-
sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generation
grammar, and recorded Arabic wavfiles used to construct a spoken result.
and Miller, 2007) will also receive asynchronous
inputs from the robot control and monitoring pro-
cess; once again, all inputs have to be processed in
the appropriate temporal order. A third example is
contextual bidirectional speech translation (Bouil-
lon et al, 2007). Here, the problem is slightly
different ? we have only speech inputs, but they
are for two different languages. The basic issue,
however, remains the same, since inputs have to be
processed in the right order to maintain the correct
context at each point.
With examples like these in mind, we have also
effected a complete redesign of the Regulus envi-
ronment?s regression testing facilities. A test suite
is now allowed to consist of a list of items of any
type ? text, wavfile, or non-speech input ? in any
order. Instead of trying to fit processing into the
constraints imposed by the batchrec utility, of-
fline processing now starts up speech resources in
the same way as the interactive environment, and
submits each item for appropriate processing in the
order in which it occurs. By adhering to the prin-
ciple that text and speech should be treated uni-
formly, we arrive at a framework which is simpler,
less error-prone (the underlying code is less frag-
ile) and above all much more flexible.
6 Summary and conclusions
The new functionality offered by the redesigned
Regulus top-level is not strikingly deep. In the
context of any given application, it could all
have been duplicated by reasonably simple scripts,
which linked together existing Regulus compo-
nents. Indeed, much of this new functionality is
implemented using code derived precisely from
such scripts. Our observation, however, has been
that few developers have actually taken the time
to write these scripts, and that when they have
been developed inside one project they have usu-
ally not migrated to other ones. One of the things
we have done, essentially, is to generalise previ-
ously ad hoc application-dependent functionality,
and make it part of the top-level development en-
vironment. The other main achievements of the
new Regulus top-level are to organise the existing
functionality in a more systematic way, so that it is
easier to find commands, and to package it all as a
normal-looking Swing-based GUI.
Although none of these items sound dramatic,
they make a large difference to the platform?s over-
14
all usability, and to the development cycle it sup-
ports. In effect, the Regulus top-level becomes
a generic speech-enabled application, into which
developers can plug their grammars, rule-sets and
derived components. Applications can be tested in
the speech view much earlier, giving a correspond-
ingly better chance of catching bad design deci-
sions before they become entrenched. The mecha-
nisms used to enable this functionality do not de-
pend on any special properties of Regulus, and
could readily be implemented in other grammar-
based development platforms, such as Gemini and
UNIANCE, which support compilation of feature
grammars into grammar-based language models.
At risk of stating the obvious, it is also worth
pointing out that many users, particularly younger
ones who have grown up using Windows and Mac
environments, expect as a matter of course that de-
velopment platforms will be GUI-based rather than
command-line. Addressing this issue, and sim-
plifying the transition between text- and speech-
based, views has the pleasant consequence of im-
proving Regulus as a vehicle for introducing lin-
guistics students to speech technology. An initial
Regulus-based course at the University of Santa
Cruz, focussing on spoken dialogue systems, is de-
scribed in (Hockey and Christian, 2008); a similar
one, but oriented towards speech translation and
using the new top-level described here, is currently
under way at the University of Geneva. We expect
to present this in detail in a later paper.
References
Alshawi, H., editor. 1992. The Core Language Engine.
MIT Press, Cambridge, Massachusetts.
Bos, J. 2002. Compilation of unification grammars
with compositional semantics to speech recognition
packages. In Proceedings of the 19th International
Conference on Computational Linguistics, Taipei,
Taiwan.
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Bouillon, P., G. Flores, M. Starlander,
N. Chatzichrisafis, M. Santaholma, N. Tsourakis,
M. Rayner, and B.A. Hockey. 2007. A bidirectional
grammar-based medical speech translator. In Pro-
ceedings of the ACL Workshop on Grammar-based
Approaches to Spoken Language Processing, pages
41?48, Prague, Czech Republic.
Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-
hara, N. Tsourakis, M. Starlander, B.A. Hockey, and
M. Rayner. 2008. Developing non-european trans-
lation pairs in a medium-vocabulary medical speech
translation system. In Proceedings of LREC 2008,
Marrakesh, Morocco.
Copestake, A. 2002. Implementing Typed Feature
Structure Grammars. CSLI Press, Chicago.
Crouch, R., M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman, 2008. XLE Documenta-
tion. http://www2.parc.com/isl/groups/nltt/xle/doc.
As of 29 Apr 2008.
Dix, A., J.E. Finlay, G.D. Abowd, and R. Beale, edi-
tors. 1998. Human Computer Interaction. Second
ed. Prentice Hall, England.
Dowding, J., M. Gawron, D. Appelt, L. Cherny,
R. Moore, and D. Moran. 1993. Gemini: A natural
language system for spoken language understanding.
In Proceedings of the Thirty-First Annual Meeting of
the Association for Computational Linguistics.
Hockey, B.A. and G. Christian. 2008. Zero to spoken
dialogue system in one quarter: Teaching computa-
tional linguistics to linguists using regulus. In Pro-
ceedings of the Third ACL Workshop on Teaching
Computational Linguistics (TeachCL-08), Colum-
bus, OH.
Hockey, B.A. and D. Miller. 2007. A demonstration of
a conversationally guided smart wheelchair. In Pro-
ceedings of the 9th international ACM SIGACCESS
conference on Computers and accessibility, pages
243?244, Denver, CO.
Jacko, J.A. and A. Sears, editors. 2003. The
human-computer interaction handbook: Fundamen-
tals, evolving technologies and emerging applica-
tions. Lawerence Erlbaum Associates, Mahwah,
New Jersey.
Kron, E., M. Rayner, P. Bouillon, and M. Santa-
holma. 2007. A development environment for build-
ing grammar-based speech-enabled applications. In
Proceedings of the ACL Workshop on Grammar-
based Approaches to Spoken Language Processing,
pages 49?52, Prague, Czech Republic.
Larsson, S. and D. Traum. 2000. Information state and
dialogue management in the TRINDI dialogue move
engine toolkit. Natural Language Engineering, Spe-
cial Issue on Best Practice in Spoken Language Di-
alogue Systems Engineering, pages 323?340.
Rayner, M., B.A. Hockey, J.M. Renders,
N. Chatzichrisafis, and K. Farrell. 2005. A
voice enabled procedure browser for the interna-
tional space station. In Proceedings of the 43rd
15
Annual Meeting of the Association for Compu-
tational Linguistics (interactive poster and demo
track), Ann Arbor, MI.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
16
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 25?32
Manchester, August 2008
Multilingual Grammar Resources in Multilingual Application
Development
Marianne Santaholma
Geneva University, ETI/TIM/ISSCO
40, bvd du Pont-d?Arve
1211 Geneva 4, Switzerland
Marianne.Santaholma@eti.unige.ch
Abstract
Grammar development makes up a large
part of the multilingual rule-based appli-
cation development cycle. One way to
decrease the required grammar develop-
ment efforts is to base the systems on
multilingual grammar resources. This pa-
per presents a detailed description of a
parametrization mechanism used for build-
ing multilingual grammar rules. We show
how these rules, which had originally been
designed and developed for typologically
different languages (English, Japanese and
Finnish) are applied to a new language
(Greek). The developed shared grammar
system has been implemented for a do-
main specific speech-to-speech translation
application. A majority of these rules
(54%) are shared amongst the four lan-
guages, 75% of the rules are shared for at
least two languages. The main benefit of
the described approach is shorter develop-
ment cycles for new system languages.
1 Introduction
Most of grammar based applications are built on
monolingual grammars. However, it is not unusual
that the same application is deployed for more
than one language. For these types of systems the
monolingual grammar approach is clearly not the
best choice, since similar grammar rules are writ-
ten several times, which increases overall develop-
ment efforts and makes maintenance laborious.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
One way to decrease these efforts is to share al-
ready developed linguistic resources between sys-
tem languages. Common approaches for shar-
ing information include grammar adaptation and
grammar sharing. Grammar adaptation is the tech-
nique of modifying an already existing grammar to
cover a new language as implemented among oth-
ers by Alshawi et al 1992; Kim et al 2003; and
Santaholma, 2005.
In grammar sharing, existing grammar rules are
directly shared between languages rather than just
being recycled as they are in grammar adapta-
tion. Compared to both the monolingual grammar
approach and the grammar adaptation approach,
grammar sharing reduces the amount of code that
needs to be written as the central rules are writ-
ten only once. This automatically leads to coher-
ence between language descriptions for different
languages, which improves grammar maintainabil-
ity, and eliminates the duplication effort that other-
wise occurs when monolingual grammars are used.
Multilingual grammars can share resources be-
tween languages in various ways. Ranta (2007)
has developed an abstract syntax that defines a
common semantic representation in a multilingual
grammar.
Another type of approach is implemented in
the LinGO Grammar Matrix project (Bender et al
2005; Bender, 2007). The Grammar Matrix con-
sists of a core grammar that contains the types and
constraints that are regarded as cross-linguistically
useful. This core is further linked to phenomenon-
specific libraries. These consist of rule reperto-
ries based on typological categories. The neces-
sary modules are put together like building blocks
according to language characteristics to form the
final grammar of a language.
The work described in this paper implements
25
a grammar sharing approach that is based on
language-independent parameterized rules that are
complemented with necessary language-specific
rules and features. These shared rules have been
implemented for MedSLT, a multilingual spoken
language translation system (Bouillon et al, 2005).
All of the central language processing compo-
nents of MedSLT, including the speech recog-
nizer, parser and generator, are derived from hand-
crafted general grammars of a language. The
biggest effort in adding a new language to the ex-
isting spoken language translation framework is
the grammar development cycle. As more lan-
guages are added to the existing spoken language
translation framework, the necessity for multilin-
gual grammar rules grows.
(Bouillon et al, 2006) first developed shared
MedSLT grammar rules for the Romance lan-
guages French, Spanish and Catalan. Compared
to the monolingual grammar system, the shared
grammar-based system facilitated application de-
velopment without degrading the performance of
any of its components (speech recognition, trans-
lation) on these languages.
We took this approach further and implemented
parameterized grammar rules for typologically dif-
ferent languages - English, Finnish and Japanese.
Experiments have shown that these shared rules
perform equally well on all three languages (San-
taholma, 2007). As these grammars had been de-
veloped in parallel, it was not clear how flexi-
ble the parameterized grammar approach would
be for new a language, which was not included
in the original development process. We thus ex-
tended the grammar to cover Modern Greek as a
new language. The paper describes the methodol-
ogy of adding this new language and evaluates the
parametrization mechanism.
The rest of the paper is structured as follows.
Section 2 describes the Regulus toolkit (Rayner
et al, 2006) and MedSLT, which form the devel-
opment environment and application framework
on which this work is based. Section 3 de-
scribes the parameterized multilingual grammar
and parametrization mechanism. Section 4 sum-
marizes techniques used to adding Modern Greek
to the shared grammar system. Section 5 con-
cludes.
2 Regulus Development environment and
MedSLT application
2.1 Regulus features
The Regulus grammar framework has been de-
signed for spoken language grammars, and thus
differs from popular grammar frameworks like
Lexical Functional Grammar (LFG) (Bresnan and
Kaplan, 1985) and Head-driven Phrase Structure
Grammar (HSPG) (Pollard and Sag, 1994). Reg-
ulus grammars are written with a feature-grammar
formalism that can easily be compiled into context
free grammars (CFG). These are required for com-
pilation of grammar-based language models used
in speech recognition. Characteristic for Regulus
grammars are finite valued features and exclusion
of complex feature-value structures. Consequently
the resulting rule-sets are perhaps more repeti-
tive and less elegant than the equivalent LFGs or
HPSGs. This design, however, enables compila-
tion of non-trivial feature-grammars to CFG.
Another Regulus feature that enables CFG com-
pilation is grammar specialization that reduces the
extent of the grammar. Grammar specialization
is performed by explanation-based learning (EBL)
1
. Multilingual grammar development can profit
from grammar specialization in various ways. The
general grammar of a language can be specialized
to specific domains based on domain specific in-
formation2. Thus the specialization serves as a
way to limit the ambiguities typical for general
grammars. Furthermore, the procedure is used to
specialize the grammars for different tasks. Ideally
a grammar should recognize variant forms but gen-
erate only one. This variation can be controlled by
specializing the Regulus grammars for these tasks.
Finally the multilingual Regulus grammar can be
specialized for specific languages by automatically
removing the unnecessary rules.
2.2 MedSLT
Most large-scale machine translation systems are
currently based on statistical language processing.
MedSLT, however, has been implemented with lin-
guistically motivated grammars mainly for the fol-
lowing reasons: (1) necessary data for inducing
the grammars and training the statistical language
1The method is described in detail in (Rayner et al, 2006),
Chapter 10.
2These include domain specific corpus, lexica and oper-
ationality criteria that control the granularity of specialized
grammar. Details provided by (Rayner et al, 2006).
26
models were not available for the required domain
and languages. (2) the medical domain demands
accurate translation performance, which can be
more easily guaranteed with rule based systems.
MedSLT is an unidirectional3 speech-to-speech
translation system that has been designed to help
in emergency situations where a common lan-
guage between the physician and the patient does
not exist. In addition to influencing the system
architecture, this communication goal also sig-
nificantly influences system coverage, and con-
sequently the grammars. The typical utterances
MedSLT translates consist of physician?s questions
about the intensity, location, duration and qual-
ity of pain, factors that increase and decrease the
pain, therapeutic processes and the family history
of the patient. These include yes-no questions like
?Does it hurt in the morning??, ?Is the pain stub-
bing?? and ?Do you have fever when you have
the headaches??. Other frequent type of questions
include wh-questions followed by elliptical utter-
ance, like ?Where is the pain??, ?In the front of the
head??, ?On both sides of the head??. Currently
MedSLT translates between Arabic, Catalan, En-
glish, Finnish, French, Japanese, and Spanish. The
translation is interlingua based.
The following sections describe the implemen-
tation of the shared parameterized grammar rules
for this specific application using the Regulus plat-
form.
3 Parameterized grammar rules
The parameterized grammar rules assemble the
common foundations of linguistic phenomena in
different languages. The framework for the
language-independent rules presented here was de-
veloped and tested with English, Japanese and
Finnish. These languages represent different types
of languages and hence express the same linguistic
phenomena in different ways. Consequently they
provided a good starting point for framework de-
sign.
The Regulus multilingual grammar is modular
and organized hierarchically. Parameterized rules
are stored in the ?language-independent core?
module. This is the most generic level and as such
is shared between all languages. The ?lower lev-
els? include the language-family specific modules
3Bidirectional MedSLT exists currently for English-
Spanish language pair. Details are provided in (Bouillon et
al., 2007).
and the language-specific modules. The modules
for related languages decrease redundancy as re-
lated languages commonly share characteristics at
least to some extent. 4 The information in this
modular structure is inherited top-down from the
most generic to language specific.
The first language to which we applied the pa-
rameterized rules and which had not been part of
the original shared grammar framework develop-
ment is Modern Greek.
In the following we first describe the parameter-
ized grammar rules. Then we focus on how these
rules are applied for Greek.
3.1 Coverage
The parameterized grammar currently covers basic
linguistic phenomena by focusing on the structures
required to process MedSLT system coverage. The
current coverage is summarized in Table 1.
Phenomena Construction
Sentence types declarative, yn-question,
wh-question, ellipsis
subordinate when clause
Tense present, past(imperfect)
Voice active, passive
Aspect continuous,
present perfect,
past perfect
Verb transitive, intransitive,
subcategorization predicative (be+adj),
existential (there+be+np)
Determiners article, number,
quantifier
Adpositional prepositional,
modifiers postpositional
Adverbial modifiers verb and sentence
modifying adverbs,
comparison adverbs
Pronouns personal, possessive,
dummy pronouns
Adjective modifiers predicative, attributive,
comparison
Table 1: Linguistic phenomena covered by the
shared grammar.
The general difficulty of spoken language for
grammar development is frequent ungrammatical
4However, as identical constructions and features also ex-
ist in unrelated languages the advantage of language family
modules is finally not so significant.
27
and non-standard use of language. This includes
for example incorrect use of case inflections in
Finnish and missing particles in spoken Japanese.
3.2 Parametrization - abstracting away from
language specific details
The parametrization aims to generalize the cross-
linguistic variation in grammar rules. English
yes-no questions require an auxiliary and in-
verted word order, in Finnish yes-no questions the
subject-verb inversion is combined with a certain
form of the main verb; in Finnish noun heads and
the modifying adjective agree in case and number,
in Greek they additionally agree in gender, and so
forth. The way of expressing the same linguistic
phenomena or constructions varies from one lan-
guage to another. Hence, shared grammar rules
need to abstract away from these kinds of details.
The multilingual Regulus rules are parameter-
ized using macro declarations. Macros are a stan-
dard tool in many development environments. In
Regulus grammars they are extensively used to
catch generalizations in the rules, and in partic-
ular in lexica. In multilingual grammar rules the
macros serve as ?links? towards language-specific
information.
The shared rules have a language-neutral sur-
face representation where macros invoke the re-
quired language-specific information. The macro
reference of a language-independent rule is re-
placed by the information contained in the macro
definition. The context of the macro reference de-
termines how the macro definition combines with
other parts of the description. The mechanism is
similar to LFG ?templates?, which encode linguis-
tic generalizations in a language description (Dal-
rymple et al, 2004).
The macro mechanism itself is rather simple.
The crucial is that the macros are defined in a trans-
parent and coherent way. Otherwise the grammar
developer will spend more time learning to how
to use the parameterized rule set than she would
spend to develop a new grammar from scratch.
When the macros are well defined, sharing the
rules for a new language is just a matter of defining
the language-specific macro definitions.
In the following we present some concrete ex-
amples of how cross-linguistic variation can be pa-
rameterized in a multilingual Regulus grammar us-
ing macros.
3.2.1 Parameterizing features
The following example shows how we param-
eterize the previously mentioned agreement fea-
tures required in different languages. In Regu-
lus grammars, like in other constraint-based gram-
mars, this fine-grained information is encoded in
feature-value pairs. We encode a basic declara-
tive sentence rule (s) that consists of a noun phrase
(np) and a verb phrase (vp):
s:[sem=concat(Np, Vp)] -->
np:[sem=Np, sem_np_type=T,
@noun_head_features(Head)],
vp:[sem=Vp, subj_sem_np_type=T,
@verb_head_features(Head)].
In Finnish sentences the subject and the main
verb agree in person and number. Japanese doesn?t
make use of these agreement features in this con-
text. Consequently, the common rules have to ex-
press the agreement in a parameterized way. For
this reason in the np we introduce a macro called
noun_head_features(Head) and in the vp
the macro verb_head_features(Head). 5
These macro declarations unify but don?t say any-
thing explicit about the unifying features them-
selves at this common level. The macros thus
?neutralize? the language-specific variation and
only point further down to language-specific infor-
mation.
In Finnish, the noun_head_features and
verb_head_featuresmacros invoke the lan-
guage specific features ?number? and ?person?:
macro
(noun_head_features([P, N]),
[person=P, number=N]).
macro
(verb_head_features(([P, N]),
[person=P, number=N]).
The macro references are replaced by these fea-
tures in the final Finnish declarative sentence rule
which takes the form:
s:[sem=concat(Np, Vp)] -->
np:[sem=Np, sem_np_type=T,
person=P, number=N],
vp:[sem=Vp, subj_sem_np_type=T,
person=P, number=N].
5The Regulus macro declaration is preceded by ?@?.
28
As Japanese does not apply either ?number? or
?person? features the macro definition consists of
an empty value:
macro(noun_head_features([]),
[]).
The final Japanese sentence rule takes after the
macro replacement the form:
s:[sem=concat(Np, Vp)] -->
np:[sem=Np, sem_np_type=T],
vp:[sem=Vp, subj_sem_np_type=T].
Similarly we can parameterize the value of
a specific feature. A vp could include a
verb_form feature that in English could take as
its value ?gerundive? and in Finnish ?infinite? in
that particular context. We can parameterize the
vp rule with a macro vform which invokes the
language-specific macro definition and replaces it
with the corresponding language-specific feature-
value pairs:
vp:[sem=concat(Aux, Vp)] -->
aux:[sem=Aux,@aux_features(Head)],
vp:[sem=Vp,
@vform(Vform),
@verb_head_features(Head)].
The English macro definition would be:
macro(vform(Vform),
[verb_form=gerund,
verb_form=Vform]).
The Finnish equivalent:
macro(vform(Vform),
[verb_form=finite,
verb_form=Vform]).
Macros can furthermore refer to other macro
definitions and in this way represent inclu-
sion relations between different features. This
forms a multilevel macro hierarchy. The macro
noun_head_features(Head) included in
np rule (1) could contain a macro arg (2), that
would further be defined by (3):
1)
np:[sem=Np, sem_np_type=SemType,
@noun_head_features(Head)].
2)
macro(noun_head_features([Agr,Case]),
[@agr(Agr), case=Case]).
3)
macro(agr([Case, Number]),
[case=Case, number=Number]).
3.2.2 Parameterizing the constituent order
The constituent order is defined by concate-
nation of linguistic categories in the wanted or-
der (vp:[sem=concat(Verb, NP)]). This
order can, similarly to features, also be parameter-
ized by using macros. We show here as an example
of how the order of a transitive main verb (verb)
and direct object (np) is parameterized in a verb
phrase:
vp:[sem=concat(Verb, NP)] -->
verb:[sem=Verb, subcat=transitive,
obj_sem_np_type=ObjType],
np:[sem=NP, sem_np_type=ObjType]).
In English the direct object follows the verb,
whereas in Japanese it precedes the verb. The
order of these constituents can be parame-
terized by introducing into the rule a macro
that in the example rule is represented by
?verb_transitive_np?:
vp:[sem=concat(Verb, NP)] -->
@verb_transitive_np(
verb:[sem=Verb, subcat=transitive,
obj_sem_np_type=ObjType],
np:[sem=NP, sem_np_type=ObjType]).
This macro invokes the language-specific rules that
define the order of the semantic values of cate-
gories required in a specific language. The seman-
tic value of the category verb is sem=Verb and
of noun sem=Noun. Consequently the English-
specific macro definition would be:
macro(verb_transitive_np
(Verb, Noun),(Verb, Noun)).
This rule specifies that when there is a seman-
tic value ?Verb? followed by a semantic value
?Noun? these should be processed in the order
?Verb?, ?Noun?. The order of constituents re-
mains unchanged.
The equivalent Japanese macro definition would
be:
macro(verb_transitive_np
(Verb, Noun),(Noun, Verb)).
29
Contrary to the English rule this rule specifies that
when there is a semantic value ?Verb? followed
by a semantic value ?Noun? these should be pro-
cessed in the order ?Noun?, ?Verb?. This changes
the order the of constituents. Details of Regulus
semantic processing are available in Rayner et al,
2006.
3.2.3 Ignoring rules/features and using empty
values
There exist several ways to ignore rules and
features or to introduce empty values in Regulus
grammars. These have proven practical in rule
parametrization. In the following we present some
frequent examples.
Features that are irrelevant for a particular lan-
guage (in a particular context) can take ?empty?
([]) as their value. This can be encoded in sev-
eral ways.
? Macro takes an empty value. This is encoded
by ?[]?
Example:
macro(noun_head_features([]),
[]).
? Feature takes an empty value. This is encoded
by ?_?:
Example:
macro(premod_case(Case),
[case=_]).
Rules that are applied to only one language are
organized in the language-specific modules. How-
ever most of the rules are necessary for two or
more languages. The rules that are used for groups
of specific languages can be ?tagged? using macro
declarations. For example a rule or feature that
is valid for English and Japanese could be simply
tagged with an identifier macro ?eng jap?:
@eng_jap
(?rule_body_here?).
The English and Japanese rules would call the rule
body by macro definition:
macro(eng_jap(Body), (Body).
The Finnish language-specific macro definition
would call an empty category that we call here
?dummy cat? and the rule would be ignored:
macro(eng_jap(Body),
(dummy_cat:[] --> dummy)).
Specialization of a grammar for a specific lan-
guage and into domain-specific form checks which
rules are necessary for processing the domain
specific-coverage in that particular language. Con-
sequently empty features of the general grammar
are automatically ignored and the language pro-
cessing remains efficient.
4 Processing Modern Greek with shared
parameterized grammar rules
Cross-linguistic comparison shows that the Greek
that belongs to the Indo-European language family
does not only share some features with English but
also with Japanese and Finnish. Common with En-
glish is, for example, the use of prepositions and
articles, and with Finnish and Japanese the pro-
drop.
The development of Greek grammar cover-
age equivalent to those of English, Japanese and
Finnish coverage in MedSLT took about two
weeks. For most part only the language-specific
macro definitions needed to be specified. Five new
rules were developed from scratch. The most sig-
nificant part of the development consisted of build-
ing the Greek lexicon and verifying that the anal-
yses produced by the shared grammar rules were
correct.
In the following we summarize Greek-specific
rules, features and macros.
4.1 Greek rules and features
In general, Greek word order is flexible, es-
pecially in spoken language. All permutations
of ordering of subject, object, and verb can be
found, though the language shows a preference
for Subject-Verb-Object ordering in neutral con-
texts. New parametrized constituent orders were
the most significant additions to the multilingual
grammar. These are listed below.
1. Yes-no questions, which are a central part of
the MedSLT application?s coverage, can be
expressed by both direct and indirect con-
stituent order in Greek. As these are both
common in spoken language, the Japanese
question rule (direct constituent order + ques-
tion particle ?ka?) was parameterized for
Greek.
30
2. The order of possessive pronoun and head
noun required parametrization. Until now
the shared grammar contained only the order
where a head noun is preceded by the pos-
sessive. In Greek the opposite order is used,
with the possessive following the head noun.
The existing rule was parameterized by a new
macro.
3. Similar parameterization was performed for
verb phrases including an indirect object. The
Greek constituent order is reversed relative to
English order. That is, the pronoun goes be-
fore the verb. A new macro was introduced to
parameterize the rule.
One main area of difference compared to En-
glish/Finnish/Japanese, is in the placement of
weak pronouns, generally referred to as ?clitics?.
Their position in Greek is relative to the verb.
In standard language they are placed before finite
verbs and after infinite verbs. Thus these weak pro-
nouns can occur in sentence-initial position. New
rules were developed to process these clitics as
well as the Greek genitive post-modifier structure.
Greek could mainly use the existing grammar
features. The difference, compared to the origi-
nal three languages, was in the extensive use of the
?gender? feature (possible values: feminine, mas-
culine and neuter). For example, Greek articles
agree with the head noun in gender, number, and
case. Furthermore, prepositions agree with the fol-
lowing nouns in gender, number and case.
4.2 Summary of multilingual rules
Table 2 summarizes current use of the multilingual
rules. The grammar includes a total of 80 rules
for English, Finnish, Japanese and Greek. 54%
of the rules are shared between all four languages
and 75% of the rules are shared between two or
more languages. Not everything can be parameter-
ized, and some language-specific rules are neces-
sary. The language-specific rules cover 25% of all
rules.
5 Conclusions
We have described a shared grammar approach
for multilingual application development. The de-
scribed approach is based on parametrization of
Regulus grammar rules using macros. We have
shown that these parameterized rules can with
comparably little effort be used for a new system
Languages N. of rules % of total
Eng + Fin + Jap + Gre 43 54%
Eng + Fin + Jap 0
Eng + Fin + Gre 4
Eng + Jap + Gre 0
Fin + Jap + Gre 6
TOTAL 10 12.5%
Fin + Jap 3
Eng + Fin 1
Eng + Jap 1
Gre + Eng 1
Gre + Jap 1
Gre + Fin 0
TOTAL 7 8.75%
Eng 9
Fin 0
Jap 6
Gre 5
TOTAL 20 25%
TOTAL 80 100%
Table 2: Grammar rules in total
language in a multilingual limited-domain appli-
cation. A majority of rules were shared between
all implemented languages and 75% of rules by at
least two languages. The deployment of a new lan-
guage was mainly based on already existing rules.
The shared grammar approach promotes consis-
tency across all system languages, effectively in-
creasing maintainability.
Acknowledgement
I would like to thank Pierrette Bouillon and Manny
Rayner for their advise, and Agnes Lisowska and
Nikos Chatzichrisafis for their suggestions and En-
glish corrections.
References
Bender, Emily and Dan Flickinger. 2005. Rapid Pro-
totyping of Scalable Grammars: Towards Modular-
ity in Extensions to a Language-Independent Core.
In: Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing IJCNLP-05
(Posters/Demos), Jeju Island, Korea.
Bender, Emily. 2007. Combining Research and
Pedagogy in the Development of a Crosslinguis-
tic Grammar Resource. In: Proceedings of the
workshop Grammar Engineering across Frameworks
2007, Stanford University.
31
Bouillon, Pierrette, Manny Rayner, Nikos
Chatzichrisafis, Beth Ann Hockey, Marianne
Santaholma, Marianne Starlander, Yukie Nakao,
Kyoko Kanzaki, Hitoshi Isahara. 2005. A generic
multilingual open source platform for limited-
domain medical speech translation. In: Proceedings
of the 10th Conference of the European Associa-
tion for Machine Translation, EAMT, Budapest,
Hungary.
Bouillon, Pierrette, Manny Rayner, Bruna Novellas
Vall, Marianne Starlander, Marianne Santaholma,
Nikos Chatzichrisafis. 2007. Une grammaire
partage multi-tache pour le traitement de la parole
: application aux langues romanes. TAL (Traitement
Automatique des Langues), Volume 47, 2006/3.
Bouillon, Pierrette, Glenn Flores, Marianne Starlan-
der, Nikos Chatzichrisafis, Marianne Santaholma,
Nikos Tsourakis, Manny Rayner, Beth Ann Hockey.
2007. A Bidirectional Grammar-Based Medical
Speech Translator. In: Proceedings of workshop on
Grammar-based approaches to spoken language pro-
cessing, ACL 2007, June 29, Prague, Czech Repub-
lic.
Bresnan, Joan and Ronald Kaplan. 1985. The mental
representation of grammatical relations. MIT Press,
Cambridge, MA.
Butt, Miriam , Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project. In: Proceedings of
COLING-2002 Workshop on Grammar Engineering
and Evaluation.
Dalrymple, Mary, Ron Kaplan, and Tracy Holloway
King. 2004. Linguistics Generalizations over De-
scriptions. In M. Butt and T.H. King (ed.) Proceed-
ings of the LFG04 Conference.
Kim, Roger, Mary Dalrymple, Ronald M. Kaplan,
Tracy Holloway King, Hiroshi Masuichi, Tomoko
Ohkuma. 2003. Language Multilingual Grammar
Development via Grammar Porting. In: Proceedings
of the ESSLLI Workshop on Ideas and Strategies for
Multilingual Grammar Development, Vienna, Aus-
tria.
Pollard, Carl and Ivan Sag. 1994. Head Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago.
Ranta, Aarne. 2007. Modular Grammar Engineering in
GF. Research on Language and Computation, Vol-
ume 5, 2/2007, 133?158.
Rayner, Manny, Beth Ann Hockey, Pierrette Bouillon.
2006. Regulus-Putting linguistics into speech recog-
nition. CSLI publications, California, USA.
Santaholma, Marianne. 2005. Linguistic representa-
tion of Finnish in a limited domain speech-to-speech
translation system. In: Proceedings of the 10th Con-
ference on European Association of Machine Trans-
lation, Budapest, Hungary.
Santaholma, Marianne. 2007. Grammar sharing tech-
niques for rule-based multilingual NLP systems. In:
Proceedings of NODALIDA 2007, the 16th Nordic
Conference of Computational Linguistics, Tartu, Es-
tonia.
32
