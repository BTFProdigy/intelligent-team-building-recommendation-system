A Multilingual News Summarizer 
Ilsin-Hsi Chen 
l)epartlnent of Computer Science and 
Information Engineering 
National Taiwan University 
Taipei, TAIWAN, R.O.C. 
hh chen @csie.ntu.edu.tw 
Chuan-Jie Lin 
Defmltment of Computer Science and 
Information Engineering 
National Taiwan University 
Taipei, TAIWAN, R.O.C. 
cjli n @ nlg2.csie.ntu.edu.tw 
Abstract 
Huge multilingual news articles are reported 
and disseminated on the Internet. ltow to 
extract the kcy information and savc the 
reading time is a crucial issue. This paper 
proposes architecture of multilingual news 
sumlnarizer, including monolingual and 
multilingual clustering, similarity measure 
among lneaningful ullits, and presentation of 
summarization results. Translation anlong 
news stories, idiosyncrasy among languages, 
itnplicit information, and user preference are 
addressed. 
Introduction 
Today many web sites on the lnternet provide 
online news services. Multilingual news articles 
are reported periodically, and across geographic 
barrier to disseminate to readers. Readers can 
access the news stories conveniently, but it takes 
much time l'or people to read all tile news. This 
paper will present a personal news secretariat that 
helps on-line readers absorb news information 
from multiple sources in different languages. 
Such a news secretariat eliminates the redundant 
information in tile news articles, reorganizes tile 
news for readers, and helps them resolve the 
language barriers. 
Reorganization of news is sonic sort of 
document summarization, which creates a short 
version of original document. Recently, many 
papers touch on single document summarization 
(ltovy and Marcu, 1998a). Only a few touch on 
multiple document sulnmarization (Chen and 
Huang, 1999; Mani and Bloedorn, 1997; Radev 
and McKeown, 1998) and multilingual document 
summarization (Hovy and Marcu, 1998b). For 
multilingual multiple news summarization, several 
issues have to be addressed: 
(1) Translation among news stories in 
different languages 
The basic idea in multiple doculnent 
sulnmarizations i  to identify which paris of news 
articles present similar reports. Because the 
news stories are in different languages, seine kind 
of Iranslation is required, e.g., term translation. 
Besides the problem of translation ambiguity, 
different news sites often use difl'erent names to 
refer tile same entity. The translation o1' named 
entities, which are usually ttnknown words, is 
another probleln. 
(2) Idiosyncrasy among languages 
1)ifferent languages have their own specific 
features. For example, a Chinese sentence is 
composed of characters without word boundary. 
Word segmentation is indispensable for Chinese. 
Besides, Chinese writers often assign l~unctuation 
ntarks at randonl, how to determine a mealfingful 
unit for similarity checking is a crucial issue. 
Thus seine tasks may be done for specific 
languages during SUlnmarization. 
(3) hnplicit information in news reports 
Some information is ilnplicit in news stories. 
For example, the name of a country is usually not 
mentioned in a news article reporting an event that 
happened in that country. On the contrary, the 
country name is important in foreign news. 
Besides, time zone is used to specify date/time 
implicitly in the news. 
(4) User preference 
When users want to read documents in their 
familiar languages, news fragments in some 
159 
9 ANt r::: 
Nl.lnlnlary N illllmal'y Sullllllary ,~ 11111111 al'y 
ltlr l 'venl 1 for l ivent 2 for l ivem 3 for Event m 
Figure 1. Architecture of 
Our Multilingual Sunmmrization System 
languages are preferred to those in other languages. 
Even machine translation should be introduced to 
translate news fragments. Besides, if a user 
prefers the news from tile view of his country, or 
more precisely, of some news sites, we should 
meet his need. 
Figure 1 shows the architecture of a 
multilingual summarization system, which is used 
to sulnmarize the news from multiple sources in 
different languages. It is composed of three 
m~tior components: several monolingual news 
clusterers, a multilingual news clusterer, and a 
news summarizer. Tile monolingual news 
clusterer receives a news stream from multiple on.- 
line newspapers in its respective language, and 
directs them into several output news streams by 
using events. The multilingual news clusterer 
then matches and merges the news streams of the 
same event but in different languages in a cluster. 
The news summarizer summarizes the news 
stories for each event. 
The possible tasks for each component 
depend on the languages used. Some major tasks 
of a monolingual clusterer are listed below. 
(1) identifying word boundaries for Chinese 
and Japanese sentences, 
(2) Extracting named entities like people, 
place, organization, time, date and monetary 
expressions, 
(3) Clustering news streams based on 
predefined topic set and named entities. 
The task for the multilingual clusterer is to 
align the news clusters in the same topic set, but in 
different languages. It is similar to document 
alignment in comparable corpus. Named entities 
are also useful cues. 
The major tasks for the news summarizer are 
shown as follows. 
(1) Partitioning a news story into several 
meaningful units (MUs), 
(2) Linking the lneaningful units, denoting 
the salne thing, from different news reports, 
(3) Displaying the summarization results 
under the consideration of language type users 
prefer, information decay and views of reporters. 
1. Clustering 
1.1 Monolingual Clustering 
We adopt a two-level approach to cluster the 
news t)o111 multiple sources. At first, news is 
classified on the basis of a predefined topic set. 
Then, tile news articles in the same topic set are 
partitioned into several clusters according to 
named emities. Classification is necessary. Oil 
tile one hand, a famous person may appear in 
many kinds of news stories. For example, 
President Clinton may make a public speech 
(political news), join an international meeting 
(international news), or even just show up in the 
opening of a baseball game (sports news). On 
the other hand, a common name is flequently seen 
but denotes different persons. Classification 
reduces the ambiguity introduced by famous 
persons and/or common names. 
An event in a news story is characterized by 
five basic entities such as people, affairs, time, 
places and things. These entities form important 
cues during clustering. Systems for named entity 
extraction in a famous lnessage understanding 
competition (MUC, 1998) demonstrate promising 
performances for English, Japanese and Chinese. 
In our multilingual summarization system, we 
focus on English and Chinese. Gazetteer 
approach is adopted to deal with English news 
articles. Comparatively, Chinese news articles 
are segmented at first. Then, several types of 
inforlnation fiom character, sentence and text 
levels are employed to extract Chinese named 
160 
entities. These tasks are similar to tile 
approaches ill tile papers (Chen and Lee, 1996; 
Chen, el al., 1998a). 
1.2 Multilingual Clustering 
Tile multilingual clusterer takes input from 
the lnonolingual clusterers, and determines which 
news clusters ill which languages talk about tile 
same story. Recall that a news cluster consists of 
several news articles reporting tile same event, and 
one news cluster exists lbr one event ariel 
monolingual clustering. Ill this way, there is at 
most one corresponding news cluster ill another 
language. Therefore, the main task of the 
multilingual news clusterer is to lind tile 
matchings among tile clusters ill different 
languages. Figure 2 shows an example, ill 
Topic !, cluster cHl is aligned to c itr, and cluster 
Cil 2 is aligned to c.ili. Clusters cii~z arid cjl 2 are 
left unaligned. That means the denoted events 
arc reported ill only one language. 
Similarity of two clusters is measured based 
on verbs, named entities, and the other nouns. 
Because Chinese words are less anibiguolls tMn 
English ones (Chen, Bian anti Lin, 1999), we 
translate nouns and verbs in the Chinese news 
articles into English. If a word Ms more than 
one translation, we select high fl-equent English 
translation. For tile named enlities not listed ill 
tile lexicon, name transliteration similar to tile 
algoritlnn (Chen, el al., 1998b) is introduced for 
matching in non-alpMbetic (e.g., Clfinese) and 
alphabetic languages (e.g., English). 
Alignment is made under the same topic. A 
news chlster c i is aligried to another cluster cj if 
their similarity is above a threshold, and is tile 
highest between q and the other clusters. If tile 
similarity of q and the other clusters is less than a 
given threshold, ci is not aligned. It is possible 
because local news is reported only ill tile 
restricted areas. 
2. Similarity Analysis 
2.1 Meauingful Units 
The basic idea during smnmarization is to tell 
which parts of the news articles are similar in the 
same event. The basic unit tbr similarity 
measure may be a paragraph or a sentence. For 
Language l., 
Language 1i 
Topic I Topic i 
I 
Topic I Topic t 
Figure 2. Matching among tile Clusters 
in Two Languages 
tile t'ormer, text segmentation is necessary for 
documents without paragraph markers (Chcn and 
Chen, 1995). For the latter, text segmentation is 
necessary ibr languages like Chinese. Unlike 
English writers, Chinese writers often assign 
punctuat ion marks at random (Chen, 1994). 
Thus the sentence boundary is not clear. 
Consider the following Chinese example (C l): 
(Central News Agency, 1999.12.02) 
(Although they were undeterred by mass arrests 
and a police crackdown, anti free-trade protesters 
still marched on downtown Seattle today. The 
protesters, carrying signs and chanting, opposed 
lhc global trade liberalization being worked on at 
a meeting of h+ade lninisters flom tile World Trade 
Organi zat ion.) 
It is composed of four sentence segments 
separated by commas. 11' a sentence segment is 
regarded as a unit for similarity checking, it may 
contain too little information. On tile contrary, if 
a sentence is regarded as a unit, it may contain too 
much M'ormation. Here we consider a 
meaningful unit (MU) as a basic unit for 
measurement. A MU is composed of several 
sentence segments and denotes a complete 
meaning. We will find two MUs shown as 
follows for (C 1): 
(Although they were undeterred by mass arrests 
and a police crackdown, anti free-trade protesters 
still marched on downtown Seattle today.) 
161 
-~.~ .e- ~ 4-/5,-- " 5, 
(The protesters, carrying signs and chanting, 
opposed the global trade liberalization being 
worked on at a meeting of trade ministers fl'om the 
World Trade Organization.) 
In our summarization system, an English 
sentence itself is an MU. Comparatively, it is a 
little harder to identify Chinese MUs. Three 
kinds of linguistic kuowledge- punctuation marks, 
linking elements and topic chaius, are proposed. 
(1) Punctuation marks 
There are fourteen marks in Chinese (Yang, 1981). 
Only period, question mark, exclamation mark, 
comma, semicolon and caesura mark are 
employed. The former three are sentence 
terminators, and the latter three are segment 
separators. 
(2) Linking elements 
There are three kinds of linking elements (Li and 
Thompson, 1981): forward-linking elements, 
backward-linking elements, and couple-linking 
elements. A segment with a forward-linking 
(backward-linking) elemeut is linked with its next 
(previous) segment. A couple-linking element is 
a pair of words that exist in two segments. 
Apparently, these two segments are joined 
together. Examples (C4)-(C6) show each ldnd of 
linkings. 
(C4) T~,~:-~,,..,-~ ' q~&d# g#~ ? 
(After school, I wanted to see a movie.) 
(I wanted to see a movie, but I couldn't get a 
ticket.) 
(C6) N -h&a-~ ~. ~'; g ' ,~a.rx & a_~-.-24 "~d 
:V4 o 
(Because I couldfft get a ticket, (so) 1 didn't 
see a movie.) 
(3) Topic chains 
The topic of a clausal segment is usually deleted 
under the identity with a topic in its preceding 
segment. The result of such a deleting process is 
a topic chain. We employ part of speech 
information to predict if a subject of a verb is 
missing. If it does, we postulate that it must 
appear in the previous segment and the two 
segments are connected to form a larger unit. 
Consider example (C1). The word "f'$ @" 
(although) is a forward linking element. Thus 
the first two segments are connected together (C2). 
The last segment does not have ally subject, so 
that it is connected to the previous one by topic 
chain (C3). In summary, two MUs are formed. 
2.2 Similarity Model 
Tile next step is to find the similarity among 
MUs in the news articles reporting the same event, 
and to link the similar MUs together. We 
analyze the news stories within the same language, 
and then the news stories among different 
languages. The key idea is similar at these two 
steps. That is, predicate argument structure 
forms the kernel of a sentence, thus verbs and 
nouns are regarded as important cues for similarity 
measures. The difference between these two 
steps is that we have to translate nouns and verbs 
in one language into another language. The 
approach of select-high-frequent translation and 
name transliteration shown in Section 1.2 is 
adopted here too. Consider (MUI) - (MU3). 
The former two are in Chinese and the last one is 
in English. They denote a similar event 
"Seattle's Curfew Hours". Each noun (verb) is 
enclosed by parentheses and assigned an index. 
There are 9 common terms between (MUI) and 
(MU2); 10 common terms between (MUI) and 
(MU3); and 8 common terms between (MU2) and 
(MU3). Note the time zones used in (MU2) and 
(MUI) are different, so are (MU2) and (MU3). 
(MU 1 ) .g (1 ~-J 5J~ N )(2 ~ ~ )(3 ~'~ ~ )(4 ~ )~I ~- ) ~'~ (5 
2-~-(11~)~%) ( 12"qc2 }\]~)(13~\]~)(14>J" ;~v)"~ ' kl5 ;~ 
G}I~"(16.T-.~")(,7,{g'a~) 1" (,s'?O" fl" '~) o 
(Chinatimes, 1999.12.02) 
(MU2) (, N~IflN)(2~-~v)(4Gdff), ~(5"1~)(6~}.~/N,~)(7 
(Formosa Television, 1999.12.02) 
(MU3) GSeattle) (2Mayor) (2sPaul) (3Schell) has 
Gdeclared) a (sState) of (scivil) (Temergency) and 
(13imposed) a (m7 p.m.) to (267:30 a.m) ((2v10 p.m.) 
EST - (2s10:30 a.m.) EST) (,4curfew) on 
(2,downtown) (29areas) of the (30city). 
(Reuters) 
162 
(s2) 
(s3) 
($4) 
once .  
(ss) 
Several strategies lnay be considered in 
similarity measure: 
(SI) Nouns in one MU are matched to nouns in 
another MU, so are verbs. 
The operations in (1) are exact matches. 
Thesauri are employed tu-ing matching. 
Each term specified in (S 1) is matched only 
Tile order of llOUllS and verbs in MU is not 
considered. 
($6) 'File order of nouns and verbs in MU is 
critical, but it is relaxed within a window. 
(S7) When continuous terms are matched, an 
extra score is added. 
($8) When tile object o1: transitive verbs arc not 
matched, a score is subtracled. 
($9) When date/time xpressions and monetary 
and percentage xpressions are matched, an extra 
score is added. 
l;ive models shown below are collstrtlcted 
under different combinations of tile strategies 
specified in tile above. 
(M t) (S 1)+($3)+($4)+($5) 
(M2) (S 1)+($3)+($4)+($6) 
(M3) (S 1)+(S3)+(S4)+(S5)+($7)+($8) 
(M4) (S 1)+($3)+($4)+($5)+($7)+($8)+($9) 
(M5) (S I )+($2)+($4)+($5)+($7)+($8)+($9) 
3. Experiments 
3.1 l ' reparat ion el'Testing Corpus 
Six events selected from Central l)aily News, 
China I)aily Newspaper, China Times Interactive, 
and FTV News Online in Taiwan arc used to 
lneasure tile performance of each lnodel. They 
are shown as follows: 
(1) military service: 6 articles 
(2) construction permit: 4 articles 
(3) landslide in Shah Jr: 6 articles 
(4) Buslfs sons: 4 articles 
(5) Typhoon Babis: 3 articles 
(6) stabilization fund: 5 articles 
The news events are selected from different 
editions, including social edition, economic 
edition, international edition, political edition, etc. 
An annotator eads all tile news articles, and 
connects tile MUs that discuss the same story. 
Because each MU is assigned a unique ID, the 
links among MUs form the answer keys for the 
performance evaluation. 
Table I. Perf iwmance of Similarity of MUs 
Model 
M I 
M2 
M 3 
M4 
M5 
Precision Rate 
0.5000 
0.4871 
0.5080 
0.5164 
0.5243 
Recall Rate 
0.5434 
0.3905 
(/.5888 
0.6198 
0.5579 
3.2 Resulls 
Traditional precision and recall are computed. 
Table 1 lists the perfornmnce of these five models. 
M I is regarded as a baseline model. M2 is 
different l'ronl M1 in that the matching order of 
nouns itl\](l verbs are kept conditionally. It tries to 
consider the subject-verl>object sequence. The 
experiment shows that tile performance is worse. 
The major reason is that we c~ltl express the same 
meaning using different syntactic structures. 
Movement ransformation may affect tile order of 
sulkiest-verb-object. Thus in M3 we give up the 
order criterion, but we add an extra score when 
continuous terms are matched, l ind  subtract some 
score when tile object of a transitive verb is not 
matched. Compared with M1, the precision is a 
little higher, and tile recall is improved about 4.5%. 
If we further consider some special named entities 
such as date/time xpressions and monetary and 
percentage expressions in M4, tile recall is 
increased about 7.6% at no expense of precision. 
M5 tries Io estimate tile function of tile thesauri. 
It uses exact matching. Tile precision is a little 
higher but the recall is decreased abollt G% 
compared with M4. 
Several m~\ior errors affect tile overall 
performance. Using nouns and verbs to find the 
similar MUs is not always workable. Tile same 
meaning may not be expressed in terms of the 
same words or synonymous words. Besides, we 
can use different format to express monetary and 
percentage xpressions. Word segmentation is
another source of errors. Two sentences 
denoting tile similar meaning may be segmented 
differently clue to tile segmentation strategies. 
Unknown words generate many single-character 
words. After tagging, these words tend to be 
nOUllS and verbs, which are used in computing tile 
scores for similarity measure. Thus errors may 
be introduced. 
163 
4. Presentation Model 
Two models, i.e., focusing inodel and 
browsing model, are proposed to display the 
sumlnarization results. In the focusing model, a 
SUlnlnarization is presented by voting fi'om 
reporters. For each event, a reporter ecords a 
news story from his own viewpoint. Recall that 
a news article is composed of several MUs. 
Those MUs that are similar in a specific event are 
COlnmon focuses of different reporters. In other 
words, they are worthy of reading. In the current 
ilnplementation, the MUs that are reported more 
than once are our target. For readability, the 
original sentences that cover the MUs are selected. 
For each set of similar MUs, the longest sentence 
in user-preferred language is displayed. The 
display order of the selected sentences is 
determined by relative position in the original 
news articles. 
In the browsing lnodel, the news articles are 
listed by information decay. The first news 
article is shown to the user in its whole content. 
In the latter shown news articles, the MUs 
denoting the inforlnation mentioned before are 
shadowed (or eliminated), so that the reader can 
focus on the new information. The alnount of 
information in a news article is lneasured in terms 
of the number of MUs, so that the article that 
contains lnore MUs is displayed before the others. 
For readability, a sentence is a display unit. In 
this model, users can read both the COlnmon views 
and different views of reporters. It saves the 
reading time by listing the colnlno11 view only 
once. 
5. Evaluation of Sumnmrization Results 
The same six events specified in Section 3.1 
are used to measure the performance of the two 
summarization models. Three kinds of metrics 
are considered - say, the document reduction rate, 
the reading-tilne reduction rate, and the 
inforlnation carried. The higher the document 
reduction rate is, the more time the reader may 
save, but the higher possibility the ilnportant 
information may be lost. Tables 2 and 3 list the 
document reduction rates for focusing and 
browsing summarization, respectively. Only 
focuses are displayed in focusing sutnmarization, 
Table 2. Reduction Rates for 
Focusing Summarization 
Event Name l)ocl,en Sum Len Sum/l)oc \[ Reduction 
mililary service 7658 2402 0.3137 68.63% 
construction permit 4182 1226 0.2932 70.68% 
laMslide in Shah ,It" 5491 1823 0.3320 66.80% 
Busies sons 6186 924 0.1494 85.06% 
Typhoon Babis 4068 1460 0.3589 64. I 1% 
stabilization ftmd 8434 2243 0.2659 73.41% 
Average 36019 10078 0.2798 72.02% 
Table 3. Reduction Rates 
for Browsing Summarization 
Event Name Doc Len Sum Len + Sum/l)oc Reduction 
military service 7658 2716 0.3547 64.53% 
construclion permit 4182 2916 0.6973 30.27% 
landslide in Shah Jr 5491 2946 0.5365 46.35% 
Buslfs sons 6186 5098 0.8241 17.59% 
Typhoon Babis 4068 2270 0.5580 44.20% 
stabilization fund 8434 4299 0.5(197 49.03% 
Average 36(/19 20245 0 .5621 43.79% 
Table 4. Assessors' Evaluation 
Event Name Document Question- Reading-Time 
Reduction Answering Reduction 
Rate Correct Rate Rate 
military service 64.53% 10(1% 45.24% 
3/I.27% 33.33% 33.54% construction permit 
landslide in Shah J|" 46.35% 80% I 10.28% 
gush's oils 17.59% 100% I 36.49% 
Typhoon Babis 44.20% 100% 35.10% 
stabilization fund 49.03% 100% 18.49~ 
Average 43.79% 88.46% 3(/.86% 
so that the average doculnent reduction rate is 
higher than that of browsing summarization. 
Besides the document reduction rate, we also 
measure the correct rate of question-answering, 
and reading-time reduction rate. Assessors read 
the highlight parts only in the browsing 
summarization, and answer 3 to 5 questions. 
Table 4 lists the evaluation results of the six 
events. The average doculnent reduction rate is 
43.79%. On the average, the summary saves 
30.86% of reading time. While reading the 
summary only, the correct rate of question- 
answering task is 88.46%. 
Conclusion 
This paper sketches architecture for 
multilingual news summarizer. In multilingual 
clustering, lnatching all pairs of news clusters in 
all languages is time-exhaustive. Because only 
English and Chinese news articles are considered 
in this paper, it is not a problem. In general, an 
164 
effective way is to predefine a sequence of 
language pairs according to the degree of 
translation ambiguity. The hmguage pair of less 
ambiguity is tried first. 
To discuss which fi'agments of multilingual 
news stories denote the salne things, this paper 
defines the concept of MUs. Punctuation marks, 
linking elements and topic chains are cues to 
identify MUs for Chinese. Select-high-frequent 
English translation and name transliteration are 
adopted to transhtte Chinese MUs into L;nglish. 
Five models are proposed to link the similar MUs 
together. Different formats used in time, date 
and monetary expressions, e.g., implicit time zone, 
affect the performance of linking. It should be 
studied in the fllture. 
In presentation o1' summarization results, the 
information decay strategy helps reduce the 
redundancy, and the user can get al the 
information provided by the news sites. 
However, the news sequence is not presented 
according to the importance. The user may quit 
reading and miss the information not shown yet. 
The voting strategy from reporters gives a shorter 
summarization in terlnS of user-preferred 
languages. However, it also misses some unique 
information reported only by one site. A hybrid 
strategy should be developed in the future to meet 
all the requirements. 
References 
Chen, H.H. (1994) "The Contextual Analysis of 
Chinese Sentences with Punctuation Marks," Litelwl 
aud Linguistic Computing, Oxford University Press, 
9(4), 1994, pp. 281-289. 
Chen, H.H; et al (1998a) "Descriplion of the NTU 
System Used for MET2." Proceedings of 7 a' 
Message Undel:s'tanding Conference, 1998. 
Chen, H.H.; et al (1998b) "Proper Name Translation in 
Cross-Language Information Retrieval," Proceedings 
of COLING-A CL98, 1998, pp. 232-236. 
Chen, H.H.; Bian, G.W. and Lin, W.C. (1999) 
"Resolving Translation Ambiguily and Target 
Polysemy in Cross-Language Inli)rmation Retriewd," 
PJweeedings of 37 'l' Auroral Meeting of the 
Association./'or Conqmtational Linguistics, 1999, pp. 
215-222. 
Chert, K.H. and Chert, H.H. (1995)"A Corpus-Based 
Approach to Text Partition," Pivceedings of 
International Col!/'erenee of Recent Advances on 
Natural Language Processing, Tzigov Chark, 
Bulgaria, 1995, pp. 152-160. 
Chen, ILH. and Huang, S.J. (1999) "A Sunnnarization 
System for Chinese News from Multiple Sources," 
Proceedings of 4 't' International Workshop on 
lqformation Retrieval with Asia l~nguages, 1999, pp. 
1-7. 
Chert, H.H. and Lee, J.C. (1996) "Identification and 
Classification of Proper Nouns in Chinese Texts," 
Proceedings" of 16th International Conference on 
Computational Linguistics, 1996, pp. 222-229. 
Hovy, E. and Mareu, D. (1998a) Automated Text 
SmmnaHzation, Tutorial in 17 'h ACL attd 36 '~' 
COLING, Montreal, Quebec, Canada, 1998. 
Hovy, E. and Marcu, D. (1998b) Multilingual Text 
Summarization, Tutorial in AMTA-98, 1998. 
IA, C.N. and Thompson, S.A. (1981) Mandarin 
Chinese: A Functional Re\[erence Giwmmar, 
University of California Press, 1981. 
Mani, I. and Bloedorn, E. (1997) "Multi-documenl 
Summarizalion by Graph Search and Matching," 
Proceedings of the Fourteenth National Con.lisrence 
oil Arti/icial Intelligence, Providence, RI, pp. 622- 
628. 
MUC (1998) Ptvceedings of 7 ~1' Message 
{hMet:s'tanding Cot!ferenc.e, http://www.muc.saic. 
corn/proceedings/proceedings index.broil. 
P, adev, I).P,. and McKeown, K.R. (1998)"Generating 
Natural Language Summaries from Multiple On-Line 
Sources," Computational Linguistics, Vol. 24, No. 3, 
pp. 469-500. 
Yang, Y. (1981) The Research on lhmetuation Marks, 
Tian-iian Publishing Company, ltong Kong, 1981. 
165 
Mining Tables from Large Scale HTML Texts 
Hsin-Hsi Chen, Shih-Chung Tsai and Jin-He Tsai 
Department o1' Computer Science and hfformation Engiueering 
National Taiwan University 
Taipei, TAIWAN, R.O.C. 
E-mail: hh_chen @csie.ntu.edu.tw 
Abstract 
Table is a very common presentation scheme, 
but few papers touch on table extraction in text 
data mining. This paper l'ocuscs on mining 
tables from large-scale HTML texts. Table 
filtering, recognition, interpretation, and 
presentation arc discussed. Heuristic rules and 
cell similarities arc employed to identify tables. 
The F-measure ot' table recognition is 86.50%. 
We also propose an algorithm to capture 
attribute-value r lationships alnong table cells. 
Finally, more structured ata is extracted and 
presented. 
Introduction 
Tables, which arc simple and easy to use, 
are very common presentation sclleme for 
writers to describe schedules, organize statistical 
data, summarize cxpcrilnental results, and so on, 
in texts ol' different domains. Because tables 
provide rich inlbrmation, table acquisition is 
useful for many applications such as document 
tmderstauding, question-and-answering, text 
retrieval, etc. However, most of previous 
approaches on text data mining focus on text 
parts, and only few touch on tabular ones 
(Appelt and Israel, 1997; Gaizauskas and Wilks, 
1998; Hurst, 1999a). Of the papers on table 
extractions (Douglas, Hurst and Quinn, 1995; 
Douglas and Hurst 1996; Hurst and Douglas, 
1997; Ng, Lim and Koo, 1999), plain texts arc 
their targets. 
I11 plain text, writers often use special 
symbols, e.g., tabs, blanks, dashes, etc., to inake 
tables. The following shows an example. It 
depicts book titles, authors, and prices. 
title author price 
Statistical Language Learning E.Chamiak $30 
Cross-Language Inforlnation P.elrieval G. Grefenstette $115 
NaturalLanguage Information Retrieval T.Slrzalkowski $144 
When detecting il' there is a table in free text, we 
should disambiguatc tile uses of tile special 
symbols. That is, the special symbol may be a 
separator or content o1' cells. Previous papers 
employ grammars (Green and Krishuainoorthy, 
1995), string-based cohesion measures (Hurst 
and Douglas, 1997), and learning methods (Ng, 
Lim and Koo, 1999) to deal with table 
recognition. 
Because of the silnplicity of table 
construction l ethods in free text, the expressive 
capability is limited. Comparatively, the 
markup languages like HTML provide very 
flexible constructs for writers to design tables. 
The flexibility also shows that table extraction i  
HTML texts is harder than that iu plain text. 
Because the HTML texts are huge on the web, 
and they arc important sources o1' knowledge, it
is indispensable to deal with table mining on 
HTML texts. Hurst (1999b) is the first attempt 
to collect a corpus froln HTML files, LAT~X 
files and a small number o1' ASCII files for table 
extraction. This paper focuses on HTML texts. 
We will discuss not only how to recognize tables 
from HTML texts, but also how to identify the 
roles of each cell (attribute and/or value), aud 
how to utilize the extracted tables. 
1 Tables in HTML 
HTML table begins with au optional 
caption t'ollowcd one or more rows. Each row 
is formed by one or more cells, which are 
classified into header and data cells. Cells 
can be merged across rows and colulnns. The 
following tags arc used: 
(1) <table...> </table> 
(2) <tr ...> </tr> 
(3) <td...> </td> 
(4) <th ...> </th> 
(5) <caption ...> </caption> 
166 
Table 1. All Example for a Tour Package ~ 
................ T~;t,r i~o'iic ................... } ..... isi'gi;)XR()iAii .......... } 
' . . . . . . . . . . .  ;diiii~i ............... i 1999iii;fiOJ-2iJO0103131 i 
i . . . . . .  Ci,,,L0i.~x/o;\];\],,;i ............ {i,;ccgn\[;/,ii c (!izls Y ll.;x{c;isii;ii i 
il i, Siligiei(i;t;ii; i, 35,450 I 2510 ' 
i Adtilt i11 l)oublc Room . . . .  :3:2;.5(J(J I i2)3i) I 
i II. ( i i x i ; :{ \ [ iG i  i . . . . . . .  3i/556 ......... -7}6 i . . . . . . . .  >\[ _ < , I 
' !d Occupatioll i 25800 i i430 i 
Child ii~l ExU'aBed i 23,850 i '7\]0" i 
. . . . . . . . . . . . . . . . . . . . . . .  ' i i t No Occt )a oi/I 22,900 360 i 
They denote main wrapper, table row, table data, 
table header, and caption for a table. Table 1 
shows an example that lists the prices for a tour. 
The interpretation of this table in terms of 
altribute-wdue relationships is shown as follows: 
Allribul? Vahtc 
Tour Code I)P91,AX01AI{ 
Valid 1999.04.01-2000.03.31 
Adult-l>ricc-Singlo Room-l~;conomic Class 35,450 
Adult-l'ricc-l)oublc l{oom-EconoMc Class 32,500 
Adult-l'ricc-Extra Ilcd-l:conomic Class 30,550 
Child-Pricc-OccutmtioiM :cononfic Class 25,800 
Child-t'rice-l';xlra Iled-l,;conomic Class 23,850 
Child-Price-No ()ccupalion-I.;conomic Class 22,900 
Adult-l'ricc-Single Room-l.:xlension 2,510 
Adtdt-Price-l)ouble l~oouM:,xlension 1,430 
Adtilt-lMcc-Fxtra Ilcd-Fxtcnsion 720 
Child l'ricc-()CCUl)ation-Fxicnsion 1,430 
Child-l'ricc-l';xh'a Bed-l';xtension 720 
Child Price-No ()ccupaiiolM,;xtcnsion 360 
Cell may play the role o1' attribute and/or value. 
Several cells may be concatenated to denote an 
altribute. For example, "AdulI-Price-Single 
\]),ecru-Economic Chlss" means the ;tdl.llt price 
for economic class and single room. The 
relationships may 13o read in column wise or in 
row wise depending on the interpretation. For 
example, the relationship for "Tour 
Code:I)P9LAXOIAB" is in row wise. The 
prices for "Economic Class" are in column wise. 
The table wrapper (<table> ... </table>) is 
a useful cue lkw table recognition. The H'FMI, 
text for the above example is shown as follows. 
The table tags are enclosed by a table wrapper. 
<lablc border> 
<if> 
<td COI~SIL,\N="3">Totu" Code</td> 
<ld COI,SI'AN="2">I)I'91,AXO1AB</Id> 
</tr> 
<11> 
<id COLS PAN="3">Valid</id> 
<ld C, OLS PAN="2"> 1999.04.01-2000.03.31 </td> 
</I r> 
<lr> 
' This example is selected from http://www.china- 
airlincs.com/cdl~ks/los7.-4.htm 
<td COI,Sl'AN:"3">Class/I.~xlensic, </td> 
<td>l':cononlic Class</td> 
<td>l';xlcnsion</td> 
</ir> 
<I1> 
<td ROW SPA N="3">Adult</td> 
<ld ROW.q PA N="6"><I)> P</l)> 
<l)>P,</p> 
<p>l</p> 
<p>C</p> 
<p>l :</ td> 
<ld>Single Rooni</td> 
<ld>35,450</Id> 
<1d>2,510</td> 
</t r> 
<tr> 
<ld>l)oubl? I~,oom</td> 
<ld>32,500</Id> 
<ld> 1 ,430</ td> 
<t t r> 
<h> 
<td>l:;xlra Hcd</td> 
<td>30,550</kl> 
<td>720</td> 
</tr> 
<Jr> 
<td>Chikl</id> 
<td>()ccupation<</td> 
<td>25,800</td> 
<td> 1,430</td> 
<It r> 
<11> 
<td>l ';xtra Ik'd</tcl> 
<td>23,850</td> 
<td>720</id> 
<It r> 
<11> 
<td>No ()CCUlmtion</td> 
<td>22,900</td> 
<kl>360</td> 
</It> 
<:/lalq,_.> 
ltowever, ;l taMe does not always exist when 
table wrapper al3pears in ft'I 'MI, text. This is 
because writers often employ table tags to 
i'cpresent forlll or IllOlltl. That allows users to 
input queries or make selections. 
Another fx)int that shoukt be mentioned is: 
table designers usually employ COLSPAN 
(ROWSPAN) to specify how many cohunns 
(rows) a table cell should span. In this example, 
the COI,SPAN of cell "Tour Code" is 3. That 
means "Tour Code" spans 3 columns. 
Similarly, the P, OWSI~AN o1' cell "Adult" is 3. 
This cell spans 3 rows. COLSPAN and 
ROWSPAN provide flexibility for users to 
design any kinds ot' tables, but they make 
automatic table interpretation more 
challengeable. 
167 
2 Flow of Table Mining 
The flow of table nfining is shown as 
Figure 1. It is composed of five modules. 
Hypertext processing module analyses HTML 
text, and extracts the table tags. Table filtering 
module filters out impossible cases by heuristic 
rules. The remaining candidates are sent to 
table recognition module for further analyses. 
The table interpretation module differentiates 
the roles of cells in the tables. The final 
module tackles how to present and employ the 
mining results. The first two modules are 
discussed in tile following paragraph, and the 
last three modules will be dealt with in the 
following sections in detail. 
table 
recognition 
It 
table 
interpretation 
hypertext 
plocessin~ 
It 
table 
filtering 
presentation 
of results 
Figure 1. Flow of Table Mining 
As specified above, table wrappers do not 
always introduce tables. Two filtering rules are 
employed to disambiguate heir functions: 
(1) A table must contain at least two cells 
to represent attribute and value. In other words, 
the structure with only one cell is filtered out. 
(2) If the content enclosed by table 
wrappers contain too much hyperlinks, forms 
and figures, then it is not regarded as a table. 
To evaluate the performance of table 
mining, we prepare the test data selected from 
airline information in travelling category o1' 
Chinese Yahoo web site (http://www.yaboo.com. 
tw). Table 2 shows the statistics of our test 
data. 
AMines 
Number of 
Pages 
# of 
Wrappers 
Number of 
Tables 
Table 2. Statistics o1' Test Data 
China 
AMine 
694 
2075 
751 
Eva 
Airline 
366 
568 
98 
Mandarin Singapore Fareast Sum 
AMine AMine Ml'line 
142 110 60 1372 
184 163 228 3218 
(2.35) 
23 40 6 918 
(0.67) 
Table 3. Pertbrmance of Filtering Rules 
China Eva Mandarin Singapore Fareast Sum 
Airline Airline AMine Airline Airline 
#of 2075 568 184 163 228 3218 
wrappers 
Number of 751 98 23 40 6 918 
Tables 
Number of 1324 470 161 123 222 2300 
\[Non-Tables 
Total 973 455 158 78 213 1877 
Filter 
Wrong 15 0 0 3 2 20 
Filter 
Correct 98.46% 100% 100% 96.15% ~)9.06% )8.93% 
Rate 
These four rows list tile names of aMines, total 
number of web pages, total number of table 
wrappers, and total number of tables, 
respectively. On the average, there are 2.35 
table wrappers, and 0.67 tables for each web 
page. The statistics hows that table tags are 
used quite often in HTML text, and only 28.53% 
are actual tables. Table 3 shows the results 
after we employ the filtering rules on the test 
data. Tile 5 th row shows how many non-table 
candidates are filtered out by the proposed rules, 
and tile 6 th row shows the nulnbcr of wrong 
filters. On the average, the correct rate is 
98.93%. Total 423 of 2300 nou-tables are 
remained. 
3 Table Recognition 
After simple analyses specified in the 
previous sectiou, there are still 423 non-tables 
passing the filtering criteria. Now we consider 
the content of the cells. A cell is much shorter 
than a senteuce in plain text. In our study, the 
length of 43,591 cells (of 61,770 cells) is smaller 
than 10 characters 2. Because of the space 
lilnitation in a table, writers often use shorthand 
notations to describe their intention. For 
a A Chinese character is represented by two bytes. 
That is, a cell contains 5 Chinese characters oil the 
average. 
168 
example, they may use a Chinese character (":~,\]", 
dao4) to represent a two-character word "~ j~"  
(dao4da2, arrive), and a character ("?~", 1i2) to 
denote the Chinese word ",~$ i~,~l" (li2kail, leave). 
They even employ special symbols like ? and 
Y to represent "increase" and "decrease". 
Thus it is hard to determine if a fragment of 
ttTML text is a table depending on a cell only. 
The context among cells is important. 
Value cells under the same attribute names 
demonstrate similar concepts. WE employ the 
following metrics to measure the cell similarity. 
(1) String similarity 
We measure how many characters are 
common in neighboring cells. I1' the 
lmmber is above zt threshold, we call 
lhe two cells are similar. 
(2) Named entity simihuily 
The metric considers emantics of cells. 
We adopt some named entity 
expressions defined in MUG (1998) 
such as date/time expressions and 
monetary and percentage xpressions. 
A role-based lnethod similar {o lhe 
paper (Chert, Ding, and Tsai, 1998) is 
employed to tell if a cell is a specific 
named entity. The neighboring cells 
belonging to the same llalned entity 
category are similar. 
(3) Number category similarily 
Number characters (0-9) appear very 
often. If total number characters in a 
cell exceeds a threshold, we call tlae 
cell belongs to !.he number category. 
The neighboring cells in number 
category are similar. 
We count how many neighboring cells are 
similar. If the percentage is above a threshold, 
the table tags are interpreted as a table. The 
data after table filtering (Section 2) is used to 
evaluate the strategies in table recognition. 
'Fables 4-6 show the experimental results when 
the three metrics are applied incrementally. 
Precision rate (P), recall rate (R), and 
F-measure (F) defined below are adopted to 
measure the performance. 
p = NumberQ/Correct7?tl)lesSystemGenerated 
TotalNumberO/TahlesSystem G n crated 
R = NumberOJ'CorrectTahlexSystemGenerated 
7btalNumberOfCorrectT"ables 
P+R 
2 
Table 4 shows that string similarity cannot 
capture the similar concept between eighboring 
cells very well. The F-measure is 55.50%. 
Table 5 tries to incorporate more semantic 
features, i.e., categories of named entity. 
Unlbrtunately, the result does not meet our 
expectation. The performance only increases a
little. The major reason is that the keywords 
(pro/am, $, %, etc.) for date/time expressions 
and monetary and percentage xpressions are 
usually omitted in {able description. Table 6 
shows that the F-measure achieves 86.50% 
when number category is used. Compared wilh 
Tables 4 and 5, the performance is improved 
Table 4. String Similarity 
China l';;'a 
Airline Airline 
Numhcr o f  751 98 
Tables 
Tables 150 4 I 
Proposed 
Correct 134 39 
l'rccision 89.33% 95.12% 
Ralc 
Recall Ralc 17.8'l% 39.80% 
l:-mcasurc 53.57% 67.46% 
Mandarin Singapore l"areast Nttm 
AMine AMine Airline 
23 4O 6 918 
7 17 5 220 
7 14 3 197 
lOOq~ 82.35% 6(Y/, 89.55c/~ 
30.43% 35.00% 50% 21.46% 
65.22~A 58.68% 55% 55.50% 
Table 5. String or Named Entity Similarity 
China l';wL Mandarin Singapore Farcasl Sum 
Airline Airline AMine Airline Airline 
Number of 751 98 23 40 6 918 
Tables 
Tables 151 42 7 17 5 222 
Proposed 
Correct 135 40 7 14 3 199 
Precision 89.40% 95.24% 100% 82.35% 60% 89.64% 
Rate 
Recall Rate 17.98% 40.82% 30.43% 35.00% 50% 21.68% 
F-measure 153.69% 68.03% 65.22% 58.68% 55% 55.66% 
Table 6. String, Named Entity, 
or Nulnber Category Similarity 
China 10,,a Mandarin Singapore Fai'cast Stllll 
AMine AMine Airline AMine AMine 
Nmnbm" of 751 98 23 40 6 918 
Tables 
Tables 668 60 16 41 6 791 
l'roposcd 
Correct 627 58 14 32 4 735 
Precision 93.86% 96.67% 87.50% 78.05% 66.67% 92.92% 
Rate 
P, ccall P, alc 83.49% 59.18% 60.87% 80.00% 66.67% 80.07% 
F-measure 88.88% 77.93(/,, 74.19% 79.03% 66.67% 86.50% 
169 
drastically. 
4 Tab le  In terpreta t ion  
As specified in Section 1, the 
attribute-value r lationship may be interpreted in
colunm wise or in row wise. If the table tags in 
questions do not contain COLSPAN 
(ROWSPAN), the problem is easier. The first 
row and/or the first column consist of the 
attribute cells, and the others are value cells. 
Cell similarity guides us how to read a table. 
We define row (or column) similarity in terms of 
cell similarity as follows. Two rows (or 
columns) are similar il' most of the 
corresponding cells between these two rows (or 
columns) are similar. 
A basic table interpretation algorithm is 
shown below. Assume there are n rows and m 
Let % denote a cell in i m row and jth co lu lnns .  
col u mn. 
(1 )  I1' there is only one row or column, 
then the problem is trivial. We jnst 
read it in row wise or column wise. 
(2) Otherwise, we start the similarity 
checking froln the right-bottom 
position, i.e., c,~,n. That is, the n th 
row and the in th column arc regarded 
as base for comparisons. 
(3) For each row i (1 _< i < n), compute 
the similarity of the two rows i and n. 
(4) Count how many pairs of rows are 
similar. 
(5) If the count is larger than (n-2)/2, and 
the similarity of row 1 and row n is 
smaller than the similarity of the other 
row pairs, then we say this table can 
be read in column wise. In other 
words, the first row contains attribute 
cells. 
(6) The interpretation from row wise is 
done in the similar way. We start 
checking from in th coluInn, compare it 
with each column j (1 < j < in), and 
count how many pail's of columns are 
similar. 
(7) If neither "row-wise" nor 
"column-wise" can be assigned, then 
the default is set to "row wise". 
Table 6 is an example. The first column 
contains attribute ceils. The other cells arc 
statistics of an expel'imel~tal result. We read it 
in row wise. ff COLSPAN (ROWSPAN) is 
used, the table iutet'pretation is more difficult. 
Table 1 is a typical example. Five COLSPANs 
and two ROWSPANs are used to create a better 
layout. The attributes are formed 
hierarchically. The following is an example of 
hierarchy. 
Adult  . . . . .  I ' r ice . . . . . . . . . . .  Double Room 
. . . . . . . . . . . . .  Single Room 
. . . . . . . . . . . .  Extra Bed 
Here, we extend the above algorithm to 
deal with table interpretation with COLSPAN 
(ROWSPAN). At first, we drop COLSPAN 
and ROWSPAN by duplicating several copies o1' 
cells in their proper positions. For example, 
COLSPAN=3 for "Tour Code" in Table 1, thus 
we duplicate "Tour Code" at colunms 2 and 3. 
Table 7 shows the final reformulation el' the 
example in Table 1. Then we employ the 
above algorithln with slight inodification to l'ind 
the reading direction. 
The modification is that spanning cells ale 
boundaries for similarity checking. Take Table 
7 as an example. We start the similarity 
checking from the i'ight-I~ottom cell, i.e., 360, 
and consider each row and column within 
boundaries. The cell "1999.04.01- 2000.03.31" 
is a spanning cell, so that 2 "a row is a boundary. 
"Price" is a spanning cell, thus 2 '''1 column is a 
boundary. In this case, we can interpret he 
table tags in both row wise and column wise. 
Table 7. Reformulation of Example in Table 1 
\] our  t Tot ' Co( e Code ~I'F?t "Co? e I+DP9LAX0 AB DP9LAX01ABi  
, .  . . . . . . .  i 1999.04.01- 1999.04.01-  vmd { Vand 1 Vanu t 
' t ' ~ ' I 2000.03 .31  2000.03 .31  
......... "C5~7; ' )  " " U~i~{g21++f++~Si i~ i ;~U+ l++Eco; ;o ; i i i c++V2.  . . . . . . . .  ~ ............. 
! extenmon Extension {ExtensionlExtension t Class ! " ' 
. . . . . . . . . . . . . . . . . . . . . . . . . .  : ........................................................................................................................... t " ~ \[ , S ingle Adult PIxICE I 35450 2,510 
f + ! ~'??m ! ' 
Double Adult PRICE I-" 32 500 1,430 
I ~ I Room | " '"  
...... ++el +++ +V+i++~ + ++(+i ~i o++ +`++++;i+++ + . . . . . . . . . . .  +++ ........ 
.... +++ i+++++ + ++a +<+++ ++ + ............ +++0 ......... 
+ + | 
Child | I 'R ICE + , . 22 900 360 
170 
After that, a second cycle begins. The 
starling points are moved lo new right-bottom 
positkms, i.e., (3, 5) and (9, 3). In this cycle, 
boundaries are reset.  The cells 
I)P9LAX01AIF' and "Adtll\[" ("(~.hild") itlC 
spalltlil\]g coils, so that I st row alld is| colun\]n {ll*C 
| l ow bou i ldar ios .  At this \ [ i l l le ,  " to \v - \v i se"  i:; 
selected. 
In final cycle, Ihc starting positions are (2,5) 
and (9, 2). The boundaries arc  0 'l' rOW and ()u~ 
column. Those two siib-tables are road it\] row 
wise. 
5 Presental ion of Table Extract ion 
The results of table interprctatioll arc a 
sequence of attributc-wfluc pairs. Consider the 
tour example. Table 8 shows the extracted 
pairs. We can find ihe following two 
phenomena: 
( I )  A cell may be a vahle of lliOre \[h~tll ()tic 
attribute. 
(2) A cell may ael as an attribute in one 
case, and a value in another case. 
We can concatenate two attributes logelher by 
using phenomenon (1). l;or example, "35,450" 
is a value of "Single Room" and "Economic 
Class", thus "Single Room-Econonfic Class" is 
formed. Besides l\[lal, we  Call find attribute 
hierarchy by using l)hcnomcnon (2). For 
example, "Single 1),oom" is a value o1" "Price", 
and "Price" is a vahie of "Adult", so that we can 
create a hierarchy "Adult-Price-Single Room". 
Merging the results from these two 
phononlena, we can create the in/erl~rclations 
that we listed in Section 1. For example, from 
the two facts: 
"35,450" is a wflue of "Single Room-L;conomic 
Class", and 
"Adult-Price-Single Room" is a hierarchical 
attribute, 
we can infer that 35,450 is a vahie o1' 
"Adult-Price-Single Rooin-Economic Class". 
In this way, we can transform unstructured 
data into more slrtictured representatioil for 
fttrther applications. Consider an application in 
quest |O | \ ]  al\]d answer ing .  G iver  a query  l ike  
"how much is the piice of a double |oom for all 
adult", the keywoMs are "price", "double 
Table 8 Tim Extracted Attril)ute-Value Pairs 
l ~' cycle 
2 '"1 t ' ) ' t ' |e  
3 'a t'ydc 
Altribulc Value 
Single P, oonl 35?150 
Single I{cx:,nl 2,510 
I )Otlblc l (oo in  
I )Otlble P, oo ln  
32,500 
1,430 
No Occul)atioll 22,900 
No Occultation 360 
I'conomic Class 35,450 
Economic Class 32,500 
Ec~monfic Class 22,900 
I:xlcnsion 2,510 
I'xtension 1,430 
I-xtension 
Class/I,;xtension 
360 
Economic Class 
Class/l';xicnsion l:~xtension 
Valid 1999.04.01-2000.03.31 
Price Single Room 
Price Double ROOlll 
I'RICI:, No ()cctqmtion 
Tour Code I)l'9t ,AX01ANB 
( ( (  Valid 199 ~.()4.01-2000.03.31 
Adul! Price 
Child Price 
room", and "adult". After consulting the 
database learning from HTMI. lexls, two wflues, 
32,500 and 1,430 with attributes economic lass 
and extension, are reported. With this table 
mining technology, knowledge lhat can be 
employed is beyond text level. 
Conclusion 
in this paper, we propose a systematic way 
to mine tables from HTML texts. Table 
filtering, table recognition, table interpretation 
and application of table extraction are discussed. 
The cues l'ron\] HTML lags and information in 
taMe cells are employed to recognize and 
interpret tables. The F-measure for table 
171 
recognition is 86.50%. 
There are still other spaces to improve 
performance. The cues from context of tables 
and the traversal paths of HTML pages may be 
also useful. In the text surrounding tables, 
writers usually explain the meaning of tables. 
For example, which row (or column) denotes 
what kind ol' meanings. From the description, 
we can know which cell may be an attribute, and 
along the same row (column) we can find their 
value cells. Besides that, the text can also 
show the selnantics ot' the cells. For exalnple, 
the table cell may be a monetary expression that 
denotes the price of a tour package. In this 
way, even money marker is not present in the 
table cell, we can still know it is a monetary 
expression. 
Note that HTML texts can be chained 
through hyperlinks like "previous" and "next". 
The context can be expanded further. Their 
effects on table mining will be studied in the 
future. Besides the possible extensions, 
another esearch line that can be considered is to 
set up a corpus for evaluation o1' attribute-value 
relationship. Because the role of a cell 
(attribute or value) is relative to other cells, to 
develop answering keys is indispensable for 
table interpretation. 
References 
Appelt, D. and Israel, D. (1997) "Tutorial Notes on 
Building Information Extraction Syslems," Tutorial 
on Fifth Conference on Applied Natural Language 
Processing, 1997. 
Chen, H.H.; Ding Y.W.; and Tsai, S.C. (1998) 
"Named Entity Extraction for Information 
Retrieval," Computer Processing of Oriental 
Languages, Special Issue on Information Retrieval 
on Oriental Languages, Vol. 12, No. 1, 1998, 
pp.75-85. 
Douglas, S.; Hurst, M. and Qui,m, D. (1995) "Using 
Natural Language Processing for Identifying and 
Interpreting Tables in Plain Text," Proceedings of 
Fourth Annual Symposium on Document Analysis 
and Informatiotl Retrieval, 1995, pp. 535-545. 
Douglas, S. and Hurst, M. (1996) "Layout and 
Language: Lists and Tables in Technical 
Documents," Proceedings of ACL SIGPARSE 
Workshop on Punctuation in Computational 
Linguistics, 1996, pp. 19-24. 
Gaizauskas, R. and Wilks, Y. (1998) " Infornmtion 
Extraction: Beyond Document Retriew~l," 
Computational Linguistics and Chinese Language 
Processing, Vol. 3, No. 2, 1998, pp. 17-59. 
Green, E. and Krishnanloorthy, M. (1995) 
"Recognition of Tables Using Grammars," 
Proceedings of the Fourth Annual Symposium on 
Document Analysis arm h{fom~ation Retrieval, 
1995, pp. 261-278. 
Hurst, M. and Douglas, S. (1997) "Layout and 
Language: Preliminary Experiments in Assigning 
Logical Structure to Table Cells," Proceedings of 
the Fifth Cot!ference on Applied Natural Lattguage 
Processing, 1997, pp. 217-220. 
Hurst, M. (1999a) "Layout and Language: Beyond 
Simple Text for Information Interaction - Modeling 
the Table," Proceedings of the 2rid htternatiottal 
Conference on Multimodal hlterJ?tces, Hong Kong, 
January 1999. 
Hurst, M. (1999b) "Layout and Language: A Corpus 
ol' Documents Containing Tables," Proceedings of 
AAAI Fall Symposium: Usillg Layout for the 
Generation, Undelwtanding arm Retrieval oJ 
Documetttx, 1999. 
Mikheev, A. and Finch, S. (1995) "A Workbench lot 
Acquisition of Ontological Knowledge from 
Natural Text," hvceedings of the 7th Conference 
o..\[ the European Chapter .?br Computational 
Litlguistics, 1995, pp. 194-201. 
MUC (1998) Proceedittgs of 7 'h Message 
UndelwtatMing Conferetlce, hltp://www.muc.saic. 
corn/proccedings/proceedil~gs ndex.html. 
Ng, H.T.; Lira, C.Y. and Koo, J.L.T. (1999) 
"Learning Io Recognize Tables in Free Text," 
Proceedings of the 37th Ammal Meeting of ACL, 
1999, pp. 443-450. 
172 
NLP and IR Approaches to Monolingual and  
Multilingual Link Detection 
 
Ying-Ju Chen 
Department of Computer Science and 
Information Engineering 
National Taiwan University 
Taipei, TAIWAN, 106 
yjchen@nlg2.csie.ntu.edu.tw 
Hsin-Hsi Chen 
Department of Computer Science and 
Information Engineering 
National Taiwan University 
Taipei, TAIWAN, 106 
hh_chen@csie.ntu.edu.tw 
 
Abstract  
This paper considers several important 
issues for monolingual and multilingual link 
detection.  The experimental results show 
that nouns, verbs, adjectives and compound 
nouns are useful to represent news stories; 
story expansion is helpful; topic 
segmentation has a little effect; and a 
translation model is needed to capture the 
differences between languages. 
Introduction 
In the digital era, how to assist users to deal with 
data explosion problem becomes emergent. 
News stories on the Internet contain a large 
amount of real-time and new information. 
Several attempts were made to extract 
information from news stories, e.g., 
multi-lingual multi-document summarization 
(Chen and Huang, 1999; Chen and Lin, 2000), 
topic detection and tracking (abbreviated as 
TDT hereafter, http://www.nist.gov/TDT), and 
so on. Of these, TDT, which is a long-term 
project, proposed many diverse applications, e.g., 
story segmentation (Greiff et al, 2000), topic 
tracking (Levow et al, 2000; Leek et al, 2002), 
topic detection (Chen and Ku, 2002) and link 
detection (Allan et al, 2000). 
This paper will focus on the link detection 
application. The TDT link detection aims to 
determine whether two stories discuss the same 
topic. Each story could discuss one or more than 
one topic, and the sizes of two stories compared 
may not be so comparable. For example, one 
story may contain 100 sentences and the other 
one may contain only 5 sentences. In addition, 
the stories may be represented in different 
languages. These are the main challenges of this 
task. In this paper, we will discuss and 
contribute on several issues: 
1. How to represent a news story? 
2. How to measure the similarity of news 
stories? 
3. How to expand a story vector using 
historic information? 
4. How to identify the subtopics 
embedded in a news story? 
5. How to deal with news stories in 
different languages? 
The multilingual issue was first introduced in 
1999 (TDT-3), and the source languages are 
mainly English and Mandarin. Dictionary-based 
translation strategy is applied broadly. In 
addition, some strategies were proposed to 
improve the translation accuracy. Leek et al, 
(2002) proposed probabilistic term translation 
and co-occurrence statistics strategies. The 
algorithm of co-occurrence statistics tended to 
favour those translations consistent with the rest 
of the document. Hui et al, (2001) proposed an 
enhanced translation approach for improving the 
translation by using a parallel corpus as an 
additional resource. Levow et al, (2000) 
proposed a corpus-based translation preference. 
English translation candidates were sorted in an 
order that reflected the dominant usage in the 
collection. Most of these methods need extra 
resources, e.g., a parallel corpus. In this paper, 
we will try to resolve multilingual issues with 
the lack of extra information. 
Topic segmentation is a technique extensively 
utilized in information retrieval and automatic 
document summarization (Hearst et al, 1993; 
Nakao, 2001). The effects were shown to be 
valid. This paper will introduce topic 
 Table 1. Performance of Link Detection under Different Feature Selection Strategies (I) 
Similarity Threshold  
0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11 0.12
All 1.6234 1.274 1.0275 0.8440 0.7245 0.6463 0.5911 0.5528 0.5268
N 0.7088 0.5547 0.4553 0.4012 0.3815 0.3743 0.3775 0.3834 0.3883
N&V 0.8152 0.6028 0.4899 0.4254 0.3922 0.3803 0.3780 0.3870 0.4002
N&J 0.6126 0.4671 0.3918 0.3624 0.3485 0.3437 0.3481 0.3628 0.3780
N&V&J 0.6955 0.5121 0.4200 0.3720 0.3498 0.3474 0.3480 0.3617 0.3795
segmentation in link detection. Several 
experiments will be conducted to investigate its 
effects. 
1 Environment 
LDC provides corpora to support the different 
applications of TDT (Fiscus et al, 2002). The 
corpora used in this paper are the TDT2 corpus 
and the augmented version of TDT3 corpus. We 
used the TDT2 corpus as training data, and 
evaluated the performance with the augmented 
version of TDT3 corpus. Both corpora are text 
and transcribed speech news from a number of 
sources in English and in Mandarin. The TDT2 
corpus spans January 1, 1998 to June 30, 1998. 
There are 200 topics for English, and 20 topics 
for Mandarin. The TDT3 corpus spans October 
1, 1998 to December 31, 1998. There are 120 
topics for both English and Mandarin. In the 
augmented version of TDT3 corpus, additional 
news data is added. These data spans from July 
1, 1998 to December 31, 1998. 
There are 34,908 story pairs (Fiscus et al, 
2002) for link detection in both monolingual and 
multilingual tasks. Of these, the numbers of 
target and non-target pairs are 4,908 and 30,000, 
respectively. In the monolingual task, Mandarin 
news stories are translated into English ones 
through a machine translation system. In the 
multilingual task, Mandarin news stories are 
represented in the original Mandarin characters. 
In both tasks, all the audio news stories are 
transcribed through an automatic speech 
recognition (ASR) system. 
We adopt the evaluation methodology defined 
in TDT to evaluate our system performance. The 
cost function for the task defined by TDT is 
shown as follows. The better the link detection 
is, the lower the normalized detection cost is. In 
the next sections, all experimental results are 
evaluated by this metric. 
CDet=CMiss?PMiss?Ptarget+CFA?PFA?Pnon-target, 
where CMiss and CFA are the costs of Miss and 
False Alarm errors, and PMiss and PFA are the 
probabilities of a Miss and a False Alarm, and 
Ptarget and Pnon-target are a priori probabilities of a 
story pair chosen at random discuss the same 
topic and discuss different topics. The cost of 
detection is normalized as follows: 
(CDet)Norm=CDet/min(CMiss?Ptarget,CFA?Pnon-target) 
2 Basic Link Detection System 
2.1 Basic Architecture 
The basic algorithm is shown as follows. Each 
story in a given pair is represented as a vector 
with tf*idf weights, where tf and idf denote term 
frequency and inverse document frequency as 
traditional IR defines. Then, the cosine function 
is used to measure the similarity of two stories. 
Finally, a predefined threshold, THdecision, is 
employed to decide whether two stories discuss 
the same topic or not. That is, two stories are on 
the same topic if their similarity is larger than 
the predefined threshold. The idf values and the 
thresholds are trained from TDT2 corpus. Each 
English story is tagged using ?Apple Pie Parser? 
(version 5.9). In addition, English words are 
stemmed by Porter?s algorithm, and function 
words are removed directly. 
2.2 Story Representation 
The noun terms denote interesting entities such 
as people names, location names, and 
organization names, and so on. The verb terms 
denote the specific events. In general, noun and 
verb terms are important features to identify the 
topic the story discusses. We conducted several 
experiments to investigate the performance of 
different story representations. Table 1 shows 
the performance of different story representation 
schemes under different similarity thresholds. 
The row denotes which lexical items are used. 
"All" means any kind of lexical items is 
 Table 2. Performance of Link Detection under Different Feature Selection Schemes (II) 
Similarity Threshold  
0.04 0.05 0.06 0.07 0.08 0.09 0.1
N&CNs 0.3825 0.3564 0.3612 0.3754 0.4026 0.4377 0.4700
N&V&CNs 0.4090 0.3572 0.3520 0.3658 0.3917 0.4279 0.4617
N&J&CNs 0.3372 0.3361 0.3353 0.3568 0.3845 0.4163 0.4471
N&V&J&CNs 0.3451 0.3398 0.3283 0.3446 0.3751 0.4055 0.4360
Table 3. Performance of Link Detection with Story Expansion Strategy 
THdecision 0.06 
THexpansion 0.06 0.07 0.08 0.1 0.11 0.13 
N&J&CNs 0.3713 0.3580 0.3392 0.3260 0.3230 0.3278 
N&V&J&CNs 0.3342 0.3363 0.3155 0.3061 0.3057 0.3073 
N&J&CNs (half) 0.2691 0.2638 0.2654 0.2785 None None 
N&V&J&CNs (half) 0.2797 0.2751 0.2826 0.3259 None None 
considered. N, V and J denote nouns, verbs, and 
adjectives, respectively. 
The experimental results show that the best 
performance is 0.3437 when only noun and 
adjective terms are used to represent stories, and 
the similarity threshold is 0.09. Examining why 
nouns and adjectives terms carry more 
information than verbs, we found that there are 
important adjectives like ?Asian?, ?financial?, 
etc., and some important people names are 
mis-tagged as adjectives. And the matched verb 
terms, such as ?keep?, ?lower?, etc., carry less 
information and the similarity would be 
overestimated. 
In the next experiments, we investigate the 
effects of compound nouns (abbreviated as CNs) 
in the story representation. The results are 
shown in Table 2. All performances are 
improved when using CNs. The best one is 
0.3283 when nouns, verbs, adjectives and CNs 
are adopted and the similarity threshold is 0.06. 
The performance is better than the result (i.e., 
0.3437) in Table 1. We found that the threshold 
for the best performance decreased in the CNs 
experiments. This is because matching CNs in 
two different news stories is more difficult than 
matching single terms, but the effect is very 
strong when matching is successful, such as 
?Red Cross?, ?Security Council?, etc. 
2.3 Story Expansion 
The length of stories may be diverse. With the 
method proposed in Section 2.1, there may be 
very few features remaining for short stories. 
And different reporters would use different 
words to describe the same event. In such 
situations, the similarity of two stories may be 
too small to tell if they belong to the same topic. 
To deal with the problems, we try to introduce a 
story expansion technique in the basic algorithm. 
The method we employed is quite different from 
that proposed by Allan (2000), which regarded 
local context analysis (LCA) as a smoothing 
technique. Each story is treated as a ?query? and 
is expanded using LCA.  
Our method is described below. When the 
similarity of two stories is higher than a 
predefined threshold THexpansion, which is always 
larger than or equal to THdecision, the two stories 
are related to some topic in more confidence. 
Thus, their relationship is kept in a database and 
will be used for story expansion later. For 
example, if the similarity of a story pair (A, B) is 
very high, we will expand the vector of A with B 
when a new pair (A, C) is considered. Table 3 
shows our experiments on TDT2 data. We 
conducted different lexical combinations and 
different weighting schemes for the expanded 
terms. 
Story expansion with the non-relevant terms 
would reduce the performance of a link 
detection system. That is, it may introduce some 
noise into the story and make the detection more 
difficult. We assigned the expanded terms two 
different weights. One is using the original 
weights, and the other one is using half of the 
original weights, which is denoted as ?half? in 
Table 3. 
The results show that story expansion 
 outperforms the basic method, and assigning 
expanded terms half weights would be better. 
The best performance when applying story 
expansion achieves 0.2638. The total miss rate 
was decreased to third fourths of the original 
amount. Sum up, story expansion is a good 
strategy to improve the link detection task. 
3 Topic Segmentation 
There is no presumption that each story 
discusses only one topic. Thus, we try to 
segment stories into small passages according to 
the discussing topics and compute passage 
similarity instead of document similarity. The 
basic idea is: the significance of some useful 
terms may be reduced in a long story because 
similarity measure on a large number of terms 
will decrease the effects of those important 
terms. Computing similarities between small 
passages could let some terms be more 
significant. 
 The first method we adopted is text tiling 
approach (Hearst, 1993). TextTiling subdivides 
text into multi-paragraph units that represent 
passages or subtopics. The approach uses 
quantitative lexical analyses to segment the 
documents. After through TextTiling algorithm, 
a file will be broken into tiles. Suppose one story 
is broken into three tiles and the other one is 
broken into four tiles. There are twelve (i.e., 3*4) 
similarities of these two stories. We conducted 
three different strategies to investigate the effect 
of topic segmentation. Strategy (I) is computing 
the similarity using the most similar passage pair. 
Strategy (II) is computing the similarity using 
passage-averaged similarity. Strategy (III) is 
computing the similarity using a two-state 
decision (Chen, 2002). But the result is not so 
good as we expected. Up to now, the best 
performance is almost the same as the original 
method without text tiling.  
 Next, we applied another topic segmentation 
algorithm developed by Utiyama et al (2001). 
The results show that this segmentation 
algorithm is better than TextTiling. But the 
improvement is still not obvious. Table 4 shows 
the experimental results for topic segmentation.  
For strategy (III), the first threshold is 0.06, 
which is also the best threshold for the basic 
method, and the second threshold varies from 
0.04 to 0.07 for segmentation. After applying 
topic segmentation, topic words would be 
centred on small passages. The amount of news 
stories discussing more than one topic is few in 
the test data and the overall performance 
depends on the segmentation algorithm. We 
make an index file similar to the original TDT 
index file. In this file, at least one story of each 
pair discusses multi-topics. We conducted 
different strategies to investigate the effect of 
topic segmentation. The experimental results 
demonstrate that topic segmentation is useful in 
this task (Chen, 2002). 
4 Multilingual Link Detection 
Algorithm 
The multilingual link detection should tell if two 
stories in different languages are discussing the 
same topic. In this paper, the stories are in 
English and in Chinese. Comparing to English 
stories, there is no apparent word boundary in 
Chinese stories. We have to segment the 
Chinese sentences into meaningful lexical units. 
We employed our own Chinese segmentation 
and tagging system to pre-process Chinese 
sentences. Similar to monolingual link detection, 
each story in a pair is represented as a vector and 
the cosine similarity is used to decide if two 
stories discuss the same topic. 
 In multilingual link detection, we have to 
deal with terms used in different languages. 
Consider the following three cases. E and C 
denote an English story and a Chinese story, 
respectively. (E, E) denotes an English pair; (C, 
C) denotes a Chinese pair; and (C, E) or (E, C) 
denotes a multilingual pair. 
    (a) (E, E): no translation is required. 
    (b) (C, E) or (E, C): C is translated to E?. 
The new E? could be an English vector or the 
vector is mixed in two languages if the original 
Table 4. Performances of Topic Segmentation in Link Detection 
 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10
Strategy (I) None None None 0.4338 0.3891 0.3766 0.3857 0.4063
Strategy (II) 0.3581 0.3490 0.3983 0.4629 0.5226 None None None
Strategy (III) None 0.3309 0.3280 0.3282 0.3288 None None None
 Chinese terms are included in the new English 
vector. 
    (c) (C, C): No translation is required; or 
both stories are translated into English and use 
English vectors; or these new English terms are 
added into the original Chinese vectors. 
The reason that we included the original 
Chinese terms in the new English vector is that 
we could not find the corresponding English 
translation candidates for some Chinese words. 
Including the Chinese terms could not lose 
information. 
We employed a simple approach to translate a 
Chinese story into an English one. A 
Chinese-English dictionary is consulted. There 
are 374,595 Chinese-English pairs in the 
dictionary. For each English term, there are 2.49 
Chinese translations. For each Chinese term, 
there are 1.87 English translations. In this 
dictionary, English translations are less 
ambiguous. Therefore, we translated Chinese 
stories into English ones. If a Chinese word 
corresponds to more than one English word, 
these English words are all selected. That is, we 
did not disambiguate the meaning of a Chinese 
word. To avoid the noise introduced by many 
English translations, each translation term is 
assigned a lower weight. The weight is 
determined as follows. We divided the weight of 
a Chinese term by the total number translation 
equivalents. 
w(d, te) = w(d, tc) / N, 
where w(d, tc) is the weight of a Chinese term in 
story d, w(d, te) is the weight of its English 
translation in story d, and N is the number of 
English translation candidates for the Chinese 
term. 
Table 5 shows the performances of 
multilingual link detection.  We conducted 
three experiments using different story 
representation schemes for Chinese stories. ?E? 
denotes Chinese stories are translated into 
English ones. ?C? denotes Chinese stories are 
compared directly without translation, but 
Chinese stories are translated into English ones 
in multilingual pairs. ?EC? denotes Chinese 
stories are represented in Chinese terms and 
their corresponding English translation 
candidates. The threshold for English story pairs 
is set to 0.12. The threshold for the other pairs 
varies from 0.1 to 0.5.  The results reveal that 
?E? is better than ?C? and ?EC?.  
Table 5. Performance of Multilingual Link Detection 
with Different Translation Schemes 
Similarity Threshold  
0.1 0.2 0.3 0.4 0.5
E 0.9925 0.6760 0.6359 0.6558 0.6864
C 1.0971 0.7204 0.6546 0.6701 0.6969
EC 1.1525 0.7712 0.7146 0.7410 0.7694
Comparing stories in translated English terms 
could bring some advantages. Some Chinese 
terms which denote the same concept but in 
different forms could be matched through their 
English translations, for example, "??" and "
??" (kill), as well as "??" and "??" 
(behaviour). 
The effect of English translations for Chinese 
stories is similar to the effect of thesaurus. We 
employed the CILIN (Mei et al, 1982) in 
multilingual link detection. We use the small 
category information and synonyms to expand 
the features we selected to represent a news 
story. The experimental results are shown in 
Table 6. 
Table 6. Performance of Multilingual Link Detection 
with Different Thesaurus Expansion Schemes 
Similarity Threshold  
0.1 0.2 0.3 0.4 0.5
Small 
Category 1.6576 0.9196 0.6656 0.6500 0.6832
Synonyms 0.9486 0.6260 0.6342 0.6734 0.7059
We found that the performances of ?E? 
translation and synonyms expansion schemes are 
very close. In our consideration, a good bilingual 
dictionary can be regarded as a thesaurus. 
The results of multilingual link detection are 
apparently worse than those of monolingual link 
detection. When the threshold is 0.2, the best 
performance is 0.6260 and the miss rate is 
0.4547. The value of miss rate is very high. To 
improve the performance, we have to reduce the 
miss rate. We found the similarity of two stories 
in different languages is very low in comparison 
with the similarity of two stories in the same 
language. It is unfair to set the same threshold 
for different languages, thus we introduced a 
two-threshold method to resolve this problem. 
The performance of the two-threshold method 
for synonyms expansion (denotes as "Syn") is 
shown in Table 7. "Chinese" means the 
 Table 8. Performances of Multilingual Link Detection under Different Feature Selection Scheme 
 Similarity Threshold 
Chinese 0.2 
Multi 0.03 0.04 0.05 0.06 
N 0.4707 0.4421 0.4319 0.4389 
N&J 0.4600 0.4162 0.4082 0.4126 
N&V 0.5162 0.4459 0.4233 0.4299 
N&V&J 0.5116 0.4248 0.4042 0.4093 
N&CNs 0.4685 0.4399 0.4297 0.4366 
N&J&CNs 0.4570 0.4193 0.4106 0.4199 
N&V&CNs 0.5010 0.4386 0.4162 0.4219 
N&V&J&CNs 0.4886 0.4152 0.3931 0.3978 
threshold for Chinese pairs and "Multi" means 
the threshold for multilingual pairs. 
Table 7. Performance of Multilingual Link Detection 
with a Two-threshold Method 
 Similarity Threshold 
Chinese 0.2 
Multi 0.01 0.02 0.03 0.04 0.05 0.06
Syn 1.2929 0.7804 0.5818 0.5166 0.5033 0.5124
The result reveals that there is a great 
improvement when applying the two-threshold 
method. The threshold for Chinese story pairs is 
0.2, the threshold for English story pairs is 0.12, 
and threshold for multilingual story pairs is 0.05. 
The similarity distributions for story pairs in 
different languages vary. As monolingual link 
detection, we did experiments about the 
combinations of different lexical terms. The 
results of these different combinations are 
shown in Table 8. It shows that the 
representation of the best performance in the 
multilingual task is different from that in the 
monolingual task. CNs bring positive influence. 
But using nouns, verbs and adjectives to 
represent a story is better than using nouns and 
adjectives only in multilingual link detection. 
Words in Chinese are seldom tagged as adjective. 
They are tagged as verbs in Chinese, but are 
tagged as adjectives in English ("??" vs. 
?safe?). 
We also adopted story expansion mentioned 
in Section 2.3 before computing the similarity. 
Note that only stories in the same language are 
used to expand each other. In Table 9, ?One? 
denotes the weights of expanded terms are the 
same as the original ones, and ?Half? denotes 
the weights of the expanded terms are only half 
of the original ones. The results reveal that 
expanded terms with half weights are better than 
with original ones. Giving expanded terms half 
weights could reduce the effect of noise. Nouns, 
verbs, adjectives and compound nouns are used 
to represent stories in Table 9, and the thresholds 
are set as the best ones in the previous 
experiments. The expansion threshold for 
Chinese pairs varies from 0.2 to 0.3. 
Table 9. Performances of Multilingual Link 
Detection with All the Best Strategies 
THexpansion 0.2 0.25 0.3
One 0.3852 0.3873 0.3916
Half 0.3721 0.3718 0.3734
5 Results of the Evaluation on TDT3 
corpus 
We applied the best strategies and the trained 
thresholds in above experiments for both 
monolingual and multilingual link detection 
tasks to TDT3 corpus. The results of our 
methods and of the other sites participating the 
TDT 2001 evaluation are shown in Table 10. In 
this evaluation, both published and unpublished 
topics are considered. 
For monolingual task, nouns, adjectives and 
CNs are used to represent story vectors. And the 
thresholds for decision and expansion are 0.06 
and 0.07, respectively. For multilingual task, 
nouns, verbs, adjectives and CNs are used to 
represent story vectors. The thresholds for 
English pairs are set the same as those in the 
monolingual task, and for Chinese pairs, they are 
0.2 and 0.25, respectively. The decision 
threshold for multilingual pairs is 0.05. 
Table 10. Link Detection Evaluation Results 
 CMU CUHK NTU UIowa
Monolingual 0.2734 None 0.2963 0.3375
Multilingual None 0.4143 0.3269 None
 In the multilingual task, our result (NTU) is 
better than The Chinese University of Hong 
Kong (CUHK). And the multilingual result is 
close to the monolingual result. This is a 
significant improvement. 
Conclusion and Future Work 
Several issues for link detection are considered 
in this paper. For both monolingual and 
multilingual tasks, the best features to represent 
stories are nouns, verbs, adjectives, and 
compound nouns. The story expansion using 
historic information is helpful. Story pairs in 
different languages have different similarity 
distributions. Using thresholds to model the 
differences is shown to be usable. 
Topic segmentation is an interesting issue. 
We expected it would bring some benefits, but 
the experiments for TDT testing environment 
showed that this factor did not gain as much as 
we expected. Few multi-topic story pairs and 
segmentation accuracy induced this result. We 
made an index file containing multi-topic story 
pairs and did experiments to investigate. The 
experimental results support our thought. 
We examined the similarities of story pairs 
and tried to figure out why the miss rate was not 
reduced. There are 919 pairs of 4,908 ones are 
mistaken. The mean similarity of miss pairs is 
much smaller than the decision threshold. That 
means there are no similar words between two 
stories even they are discussing the same topic. 
None or few match words result that the 
similarity does not exceed the threshold. That is 
the problem that we have to overcome.  
We also find that the people names may be 
spelled in different ways in different news 
agencies. For example, the name of a balloonist 
is spelled as ?Faucett? in VOA news stories, but 
is spelled as ?Fossett? in the other news sources. 
And for machine translated news stories, the 
people names would not be translated into their 
corresponding English names. Therefore, we 
could not find the same people name in two 
stories. In substance, people names are 
important features to discriminate from topics. 
This is another challenge issue to overcome. 
References  
Allan J., Lavrenko V., Frey D., and Khandelwal V. 
(2000) UMass at TDT 2000. In Proceedings of 
Topic Detection and Tracking Workshop. 
Chen H.H. and Huang S.J. (1999). A Summarization 
System for Chinese News from Multiple Sources. 
In Proceedings of the 4th International Workshop 
on Information Retrieval with Asian Languages, 
Taiwan, pp. 1-7. 
Chen H.H. and Lin C.J. (2000) A Multilingual News 
Summarizer. In Proceedings of 18th International 
Conference on Computational Linguistics, 
University of Saarlandes, pp. 159-165. 
Chen H.H. and Ku L.W (2002) An NLP & IR 
Approach to Topic Detection. In "Topic Detection 
and Tracking: Event-based Information 
Organization", Kluwer Academic Publishers, pp. 
243-261. 
Chen Y.J (2002) Monolingual and Multilingual Link 
Detection. Master Thesis. Department of 
Computer Science and Information Engineering, 
National Taiwan University, 2002. 
Fiscus J.G., Doddington G.R. (2002) Topic Detection 
and Tracking Evaluation Overview. In "Topic 
Detection and Tracking: Event-based Information 
Organization", Kluwer Academic Publishers, pp. 
17-32. 
Greiff W., Morgan A., Fish R., Richards M., Kundu 
A. (2000) MITRE TDT-2000 Segmentation 
System. In Proceedings of TDT2000 Workshop. 
Hearst M.A. and Plaunt C. (1993) Subtopic 
Structuring for Full-Length Document Access. In 
Proceedings of the 16th Annual International 
ACM SIGIR Conference. 
Hui K., Lam W., and Meng H.M. (2001) Discovery 
of Unknown Events From Multi-lingual News. In 
Proceedings of the International Conference on 
Computer Processing of Oriental Languages. 
Leek T., Schuartz R., Sista S. (2002) Probabilistic 
Approaches To Topic Detection and Tracking. In 
"Topic Detection and Tracking: Event-based 
Information Organization", Kluwer Academic 
Publishers, pp. 67-84. 
Levow G.A. and Oard D.W. (2000) Translingual 
Topic Detection: Applying Lessons from the MEI 
Project. In the Proceedings of Topic Detection 
and Tracking Workshop (TDT-2000). 
Mei, J. et al (1982) tong2yi4ci2ci2lin2 (CILIN), 
Shanghai Dictionary Press. 
Nakao Y. (2000) An Algorithm for One-page 
Summarization of a Long Text Based on 
Thematic Hierarchy Detection. In Proceeding of 
ACL 2000, pp. 302-309. 
Utiyama M. and Isahara H. (2001) A statistical 
Model for Domain-Independent Text 
Segmentation. ACL/EACL-2001, pp. 491-498. 
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 136?144,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Ranking Reader Emotions Using Pairwise Loss Minimization and  
Emotional Distribution Regression 
 
 
Kevin Hsin-Yih Lin and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1 Roosevelt Rd. Sec. 4, Taipei, Taiwan 
{f93141, hhchen}@csie.ntu.edu.tw 
 
 
 
 
 
 
Abstract 
This paper presents two approaches to ranking 
reader emotions of documents. Past studies 
assign a document to a single emotion cate-
gory, so their methods cannot be applied di-
rectly to the emotion ranking problem. 
Furthermore, whereas previous research ana-
lyzes emotions from the writer?s perspective, 
this work examines readers? emotional states. 
The first approach proposed in this paper 
minimizes pairwise ranking errors. In the sec-
ond approach, regression is used to model 
emotional distributions. Experiment results 
show that the regression method is more ef-
fective at identifying the most popular emo-
tion, but the pairwise loss minimization 
method produces ranked lists of emotions that 
have better correlations with the correct lists. 
1 Introduction 
Emotion analysis is an increasingly popular re-
search topic due to the emergence of large-scale 
emotion data on the web. Previous work primarily 
studies emotional contents of texts from the 
writer's perspective, where it is typically assumed 
that a writer expresses only a single emotion in a 
document. Unfortunately, this premise does not 
hold when analyzing a document from the reader's 
perspective, because readers rarely agree unani-
mously on the emotion that a document instills. 
Figure 1 illustrates this phenomenon. In the figure,  
 
0%
10%
20%
30%
40%
H
ea
rtw
ar
m
in
g
H
ap
py
S
ad
S
ur
pr
is
in
g
A
ng
ry
B
or
in
g
A
w
es
om
e
U
se
fu
l
Emotion
%
 o
f R
ea
de
rs
 
Figure 1. Emotional responses of 626 people after read-
ing a Yahoo! News article about an Iranian refugee 
mother and her two children who finally reunited with 
their family in the March of 2007 after been stranded in 
a Moscow airport for 10 months due to false passports. 
 
readers? responses are distributed among different 
emotion categories. In fact, none of the emotions in 
Figure 1 has a majority (i.e., more than 50%) of the 
votes. Intuitively, it is better to provide a ranking 
of emotions according to their popularity rather 
than associating a single reader emotion with a 
document. As a result, current writer-emotion 
analysis techniques for classifying a document into 
a single emotion category are not suitable for ana-
lyzing reader emotions. New methods capable of 
ranking emotions are required.  
Reader-emotion analysis has potential applica-
tions that differ from those of writer-emotion 
analysis. For example, by integrating emotion 
ranking into information retrieval, users will be 
able to retrieve documents that contain relevant 
contents and at the same time produce desired feel-
ings. In addition, reader-emotion analysis can as-
sist writers in foreseeing how their work will 
influence readers emotionally.  
136
In this paper, we present two approaches to 
ranking reader emotions. The first approach is in-
spired by the success of the pairwise loss minimi-
zation framework used in information retrieval to 
rank documents. Along a similar line, we devise a 
novel scheme to minimize the number of incor-
rectly-ordered emotion pairs in a document. In the 
second approach, regression is used to model 
reader-emotion distributions directly. Experiment 
results show that the regression method is more 
effective at identifying the most popular emotion, 
but the pairwise loss minimization method pro-
duces ordered lists of emotions that have better 
correlations with the correct lists. 
The rest of this paper is organized as follows. 
Section 2 describes related work. In Section 3, de-
tails about the two proposed approaches are pro-
vided. Section 4 introduces the corpus and Section 
5 presents how features are extracted from the cor-
pus. Section 6 shows the experiment procedures 
and results. Section 7 concludes the paper. 
2 Related Work 
Only a few studies in the past deal with the reader 
aspect of emotion analysis. For example, Lin et al 
(2007; 2008) classify documents into reader-
emotion categories. Most previous work focuses 
on the writer?s perspective. Pang et al (2002) de-
sign an algorithm to determine whether a docu-
ment?s author expresses a positive or negative 
sentiment. They discover that using Support Vec-
tor Machines (SVM) with word unigram features 
results in the best performance. Since then, more 
work has been done to find features better than 
unigrams. In (Hu et al, 2005), word sentiment in-
formation is exploited to achieve better classifica-
tion accuracy. 
Experiments have been done to extract emo-
tional information from texts at granularities finer 
than documents. Wiebe (2000) investigates the 
subjectivity of words, whereas Aman and Szpako-
wicz (2007) manually label phrases with emotional 
categories. In 2007, the SemEval-2007 workshop 
organized a task on the unsupervised annotation of 
news headlines with emotions (Strapparava and 
Mihalcea, 2007). 
As for the task of ranking, many machine-
learning algorithms have been proposed in infor-
mation retrieval. These techniques generate rank-
ing functions which predict the relevance of a 
document. One class of algorithms minimizes the 
errors resulting from ordering document pairs in-
correctly. Examples include (Joachims, 2002), 
(Freund et al, 2003) and (Qin et al, 2007). In par-
ticular, the training phase of the Joachims? Rank-
ing SVM (Joachims, 2002) is formulated as the 
following SVM optimization problem: 
 
min ?+ kjiCkji ,,T21,, ?? www,  
subject to: 
 
0  : 
1)),(),((  
:|),(),,(
,,
,,
T
,,
????
?????
>??
kji
kjijkik
jkikjkik
kji
dqdq
ssVdqdq
?
?w     (1) 
 
where V is the training corpus, ?(qk, di) is the fea-
ture vector of document di with respect to query qk, 
sk,i is the relevance score of di with respect to qk, w 
is a weight vector, C is the SVM cost parameter, 
and ?i,j,k are slack variables. The set of constraints 
at (1) means that document pairwise orders should 
be preserved. 
Unfortunately, the above scheme for exploiting 
pairwise order information cannot be applied di-
rectly to the emotion ranking task, because the task 
requires us to rank emotions within a document 
rather than provide a ranking of documents. In par-
ticular, the definitions of ?(qk,di), ?(qk,dj), sk,i and 
sk,j do not apply to emotion ranking. In the next 
section, we will show how the pairwise loss mini-
mization concept is adapted for emotion ranking. 
3 Ranking Reader Emotions 
In this section, we provide the formal description 
of the reader-emotion ranking problem. Then we 
describe the pairwise loss minimization (PLM) 
approach and the emotional distribution regression 
(EDR) approach to ranking emotions. 
3.1 Problem Specification 
The reader emotion ranking problem is defined as 
follows. Let D = {d1, d2, ?, dN} be the document 
space, and E = {e1, e2, ?, eM} be the emotion 
space. Let fi : E ? ? be the emotional probability 
function of di?D. That is, fi(ej) outputs the fraction 
of readers who experience emotion ej after reading 
document di. Our goal is to find a function r : D ? 
EM such that r(di) = (e?(1), e?(2), ?, e?(M)) where ? is 
137
Input: Set of emotion ordered pairs P 
1.  G ? a graph with emotions as vertices and no edge 
2.  while (P ? ?) 
3.    remove (ej,ek) with the highest confidence from P 
4.    if adding edge (ej,ek) to G produces a loop 
5.      then add (ek,ej) to G 
6.    else add (ej,ek) to G 
7.  return topological sort of G 
a permutation on {1, 2, ?, M}, and fi(e?(1)) ? fi(e?(2)) 
? ? ? fi(e?(M)). 
3.2 Pairwise Loss Minimization 
As explained in Section 2, the information retrieval 
framework for exploiting pairwise order informa-
tion cannot be applied directly to the emotion rank-
ing problem. Hence, we introduce a novel 
formulation of the emotion ranking problem into 
an SVM optimization problem with constraints 
based on pairwise loss minimization. 
Algorithm 1. Merge Pairwise Orders. 
 
We now describe how we rank the emotions of a 
previously unseen document using the M(M ? 1)/2 
pairwise ranking functions gjk created during the 
training phase. First, all of the pairwise ranking 
functions are applied to the unseen document, 
which generates the relative orders of every pair of 
emotions. These pairwise orders need to be com-
bined together to produce a ranked list of all the 
emotions. Algorithm 1 does exactly this.  
Whereas Ranking SVM generates only a single 
ranking function, our method creates a pairwise 
ranking function gjk : D ? ? for each pair of emo-
tions ej and ek, aiming at satisfying the maximum 
number of the inequalities: 
 In Algorithm 1, the confidence of an emotion 
ordered pair at Line 3 is the probability value re-
turned by a LIBSVM classifier for predicting the 
order. LIBSVM?s method for generating this prob-
ability is described in (Wu et al, 2003). Lines 4 
and 5 resolve the problem of conflicting emotion 
ordered pairs forming a loop in the ordering of 
emotions. The ordered list of emotions returned by 
Algorithm 1 at Line 7 is the final output of the 
PLM method. 
?di?D | fi(ej) > fi(ek) : gjk(di) > 0 
?di?D | fi(ej) < fi(ek) : gjk(di) < 0 
 
In other words, we want to minimize the number of 
incorrectly-ordered emotion pairs. We further re-
quire gjk(di) to have the linear form wT?(di) + b, 
where w is a weight vector, b is a constant, and 
?(di) is the feature vector of di. Details of feature 
extraction will be presented in Section 5. 
As Joachims (2002) points out, the above type 
of problem is NP-Hard. However, an approximate 
solution to finding gik can be obtained by solving 
the following SVM optimization problem: 
3.3 Emotional Distribution Regression 
In the second approach to ranking emotions, we 
use regression to model fi directly. A regression 
function hj : D ? ? is generated for each ej?E by 
learning from the examples (?(di), fi(ej)) for all 
documents di in the training corpus. 
 
min ?+ ib Ci ??, www T21,  
subject to: 
0  : 
1))((:)()(|
1)(:)()(|
T
T
??
??+??<??
??+?>??
i
iikijii
iikijii
i
bdefefQd
bdefefQd
?
?
?
w
w
 
The regression framework we adopt is Support 
Vector Regression (SVR), which is a regression 
analysis technique based on SVM (Sch?lkopf et al, 
2000). We require hj to have the form wT?(di) + b. 
Finding hj is equivalent to solving the following 
optimization problem: 
 
where C is the SVM cost parameter, ?i are slack 
variables, and Q is the training corpus. We assume 
each document di?Q is labeled with fi(ej) for every 
emotion ej?E. 
 
min )( 2,1,T2
1
,, 2,1, iib Cii ????, ++ ?www  
subject to: 
When formulated as an SVM optimization prob-
lem, finding gjk is equivalent to training an SVM 
classifier for classifying a document into the ej or 
ek category. Hence, we use LIBSVM, which is an 
SVM implementation, to obtain the solution.1 
0 , : 
)())((
))(()(
: 
2,1,
2,
T
1,
T
??
???+?
??+??
??
ii
ijii
iiji
i
i
efbd
bdef
Qd
??
??
??
w
w
 
                                                          
 1 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
138
02000
4000
6000
8000
[2
0%
,3
0%
)
[3
0%
,4
0%
)
[4
0%
,5
0%
)
[5
0%
,6
0%
)
[6
0%
,7
0%
)
[7
0%
,8
0%
)
[8
0%
,9
0%
)
[9
0%
,1
00
%
]
Percentage of Votes Received by Most Popular Emotion
N
um
be
r o
f N
ew
s 
A
rt
ic
le
s
 
Figure 2. News articles in the entire corpus grouped by 
the percentage of votes received by the most popular 
emotion. 
 
where C is the cost parameter, ? is the maximum 
difference between the predicted and actual values 
we wish to maintain, ?i,1 and ?i,2 are slack variables, 
and Q is the training corpus. To solve the above 
optimization problem, we use SVMlight?s SVR im-
plementation.2 
When ranking the emotions of a previously un-
seen document dk, we sort the emotions ej?E in 
descending order of hj(dk). 
4 Constructing the Corpus 
The training and test corpora used in this study 
comprise Chinese news articles from Yahoo! Kimo 
News3, which allows a user to cast a vote for one 
of eight emotions to express how a news article 
makes her feel. Each Yahoo! news article contains 
a list of eight emotions at the bottom of the web-
page. A reader may select one of the emotions and 
click on a submit button to submit the emotion. As 
with many websites which collect user responses, 
such as the Internet Movie Database, users are not 
forced to submit their responses. After submitting a 
response, the user can view a distribution of emo-
tions indicating how other readers feel about the 
same article. Figure 1 shows the voting results of a 
Yahoo! news article. 
The eight available emotions are happy, sad, 
angry, surprising, boring, heartwarming, awesome, 
and useful. Useful is not a true emotion. Rather, it 
means that a news article contains practical infor-
mation. The value fi(ej) is derived by normalizing 
the number of votes for emotion ej in document di 
by the total number votes in di. 
The entire corpus consists of 37,416 news arti-
cles dating from January 24, 2007 to August 7, 
2007. News articles prior to June 1, 2007 form the 
training corpus (25,975 articles), and the remaining 
ones form the test corpus (11,441 articles). We 
collect articles a week after their publication dates 
to ensure that the vote counts have stabilized. 
                                                          
                                                          2 http://svmlight.joachims.org/ 
3 http://tw.news.yahoo.com 
As mentioned earlier, readers rarely agree 
unanimously on the emotion of a document. Figure 
2 illustrates this. In 41% of all the news articles in 
the entire corpus, the most popular emotion re-
ceives less than 60% of the votes. 
5 Extracting Features 
After obtaining news articles, the next step is to 
determine how to convert them into feature vectors 
for SVM and SVR. That is, we want to instantiate 
?. For this purpose, three types of features are ex-
tracted. 
The first feature type consists of Chinese charac-
ter bigrams, which are taken from the headline and 
content of each news article. The presence of a bi-
gram is indicated by a binary feature value. 
Chinese words form the second type of features. 
Unlike English words, consecutive Chinese words 
in a sentence are not separated by spaces. To deal 
with this problem, we utilize Stanford NLP 
Group?s Chinese word segmenter to split a sen-
tence into words.4 As in the case of bigrams, bi-
nary feature values are used. 
We use character bigram features in addition to 
word features to increase the coverage of Chinese 
words. A Chinese word is formed by one or more 
contiguous Chinese characters. As mentioned ear-
lier, Chinese words in a sentence are not separated 
by any boundary symbol (e.g., a space), so a Chi-
nese word segmentation tool is always required to 
extract words from a sentence. However, a word 
segmenter may identify word boundaries errone-
ously, resulting in the loss of correct Chinese 
words. This problem is particularly severe if there 
are a lot of out-of-vocabulary words in a dataset. In 
Chinese, around 70% of all Chinese words are 
Chinese character bigrams (Chen et al, 1997). 
Thus, using Chinese character bigrams as features 
will allow us to identify a lot of Chinese words, 
which when combined with the words extracted by 
the word segmenter, will give us a wider coverage 
of Chinese words. 
The third feature type is extracted from news 
metadata. A news article?s metadata are its news 
4 http://nlp.stanford.edu/software/segmenter.shtml 
139
NDCG@k is used because ACC@k has the dis-
advantage of not taking emotional distributions 
into account. Take Figure 1 as an example. In the 
figure, heartwarming and happy have 31.3% and 
30.7% of the votes, respectively. Since the two 
percentages are very close, it is reasonable to say 
that predicting happy as the first item in a ranked 
list may also be acceptable. However, doing so 
would be completely incorrect according to 
ACC@k. In contrast, NDCG@k would consider it 
to be partially correct, and the extent of correctness 
depends on how much heartwarming and happy?s 
percentages of votes differ. To be exact, if happy is 
predicted as the first item, then the corresponding 
NDCG@1 would be 30.7% / 31.3% = 0.98. 
category, agency, hour of publication, reporter, and 
event location. Examples of news categories in-
clude sports and political. Again, we use binary 
feature values. News metadata are used because 
they may contain implicit emotional information. 
6 Experiments 
The experiments are designed to achieve the fol-
lowing four goals: (i) to compare the ranking per-
formance of different methods, (ii) to analyze the 
pairwise ranking quality of PLM, (iii) to analyze 
the distribution estimation quality of EDR, and (iv) 
to compare the ranking performance of different 
feature sets. The Yahoo! News training and test 
corpora presented in Section 4 are used in all ex-
periments. 
The third metric is SACC@k, or set accuracy at 
k. It is a variant of ACC@k. According to 
SACC@k, a predicted ranked list is correct if the 
set of its first k items is the same as the true ranked 
list?s set of first k items. In effect, SACC@k evalu-
ates a ranking method?s ability to place the top k 
most important items in the first k positions. 
6.1 Evaluation Metrics for Ranking 
We employ three metrics as indicators of ranking 
quality: ACC@k, NDCG@k and SACC@k. 
ACC@k stands for accuracy at position k. Ac-
cording to ACC@k, a predicted ranked list is cor-
rect if the list?s first k items are identical (i.e., same 
items in the same order) to the true ranked list?s 
first k items. If two emotions in a list have the 
same number of votes, then their positions are in-
terchangeable. ACC@k is computed by dividing 
the number of correctly-predicted instances by the 
total number of instances. 
6.2 Tuning SVM and SVR Parameters 
SVM and SVR are employed in PLM and EDR, 
respectively. Both SVM and SVR have the adjust-
able C cost parameter, and SVR has an additional ? 
parameter. To estimate the optimal C value for a 
combination of SVM and features, we perform 4-
fold cross-validation on the Yahoo! News training 
corpus, and select the C value which results in the 
highest binary classification accuracy during cross-
validation. The same procedure is used to estimate 
the best C and ? values for a combination of SVR 
and features. The C-? pair which results in the 
lowest mean squared error during cross-validation 
is chosen. The candidate C values for both SVM 
and SVR are 2-10, 2-9, ?, 2-6. The candidate ? val-
ues for SVR are 10-2 and 10-1. All cross-validations 
are performed solely on the training data. The test 
data are not used to tune the parameters. Also, 
SVM and SVR allow users to specify the type of 
kernel to use. Linear kernel is selected for both 
SVM and SVR. 
NDCG@k, or normalized discounted cumulative 
gain at position k (J?rvelin and Kek?l?inen, 2002), 
is a metric frequently used in information retrieval 
to judge the quality of a ranked list when multiple 
levels of relevance are considered. This metric is 
defined as 
? = += ki ik irelzk 1 2 )1(log@NDCG  
 
where reli is the relevance score of the predicted 
item at position i, and zk is a normalizing factor 
which ensures that a correct ranked list has an 
NDCG@k value of 1. In the emotion ranking prob-
lem, reli is the percentage of reader votes received 
by the emotion at position i. Note that the log2(i+1) 
value in the denominator is a discount factor which 
decreases the weights of items ranked later in a list. 
NDCG@k has the range [0, 1], where 1 is the best. 
In the experiment results, NDCG@k values are 
averaged over all instances in the test corpus. 
6.3 Nearest Neighbor Baseline 
The nearest neighbor (NN) method is used as the 
baseline. The ranked emotion list of a news article 
in the test corpus is predicted as follows. First, the
140
0.0
0.2
0.4
0.6
0.8
1 2 3 4 5 6 7 8
ACC@
A
cc
ur
ac
y
NN
EDR
PLM
 
0.5
0.6
0.7
0.8
0.9
1.0
1 2 3 4 5 6 7 8
NDCG@
N
D
C
G
 V
al
ue
NN
EDR
PLM
 
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8
SACC@
A
cc
ur
ac
y
NN
EDR
PLM
 
Figure 3. ACC@k Figure 4. NDCG@k Figure 5. SACC@k 
 
 
0%
20%
40%
60%
80%
100%
1 2 3 4 5 6 7 8
ACC@
%
 o
f T
es
t I
ns
ta
nc
es
Both
Incorrect
Only PLM
Correct
Only EDR
Correct
Both
Correct
 
Figure 6. Performance of PLM and EDR. 
 
test news article is compared to every training 
news article using cosine similarity, which is de-
fined as  
||||
||
),(cos
ii
ji
ji DD
DD
dd ?
?=  
 
where di and dj are two news articles, and Di and Dj 
are sets of Chinese character bigrams in di and dj, 
respectively. The ranked emotion list of the train-
ing article having the highest cosine similarity with 
the test article is used as the predicted ranked list. 
6.4 Comparison of Methods 
Figures 3 to 5 show the performance of different 
ranking methods on the test corpus. For both PLM 
and EDR, all of the bigram, word, and news meta-
data features are used. 
In Figure 3, EDR?s ACC@1 (0.751) is higher 
than those of PLM and NN, and the differences are 
statistically significant with p-value < 0.01. So, 
EDR is the best method at predicting the most 
popular emotion. However, PLM has the best 
ACC@k for k ? 2, and the differences from the 
other two methods are all significant with p-value 
< 0.01. This means that PLM?s predicted ranked 
lists better resemble the true ranked lists.  
Figure 3 displays a sharp decrease in ACC@k 
values as k increases. This trend indicates the hard-
ness of predicting a ranked list correctly. Looking 
from a different angle, the ranking task under the 
ACC@k metric is equivalent to the classification 
of news articles into one of 8!/(8 ? k)! classes, 
where we regard each unique emotion sequence of 
length k as a class. In fact, computing ACC@8 for 
a ranking method is the same as evaluating the 
method?s ability to classify a news article into one 
of 8! = 40,320 classes. So, producing a completely-
correct ranked list is a difficult task. 
In Figure 4, all of PLM and EDR?s NDCG@k 
improvements over NN are statistically significant 
with p-value < 0.01. For some values of k, the dif-
ference in NDCG@k between PLM and EDR is 
not significant. The high NDCG@k values (i.e., 
greater than 0.8) of PLM and EDR imply that al-
though it is difficult for PLM and EDR to generate 
completely-correct ranked lists, these two methods 
are effective at placing highly popular emotions to 
the beginning of ranked lists. 
In Figure 5, PLM outperforms the other two 
methods for 2 ? k ? 7, and the differences are all 
statistically significant with p-value < 0.01. For 
small values of k (e.g., 2 ? k ? 3), PLM?s higher 
SACC@k values mean that PLM is better at plac-
ing the highly popular emotions in the top posi-
tions of a ranked list. 
To further compare PLM and EDR, we examine 
their performance on individual test instances. Fig-
ure 6 shows the percentage of test instances where 
both PLM and EDR give incorrect lists, only PLM 
gives correct lists, only EDR gives ranked lists, 
and both methods give correct lists. The ?Only 
PLM Correct? and ?Only EDR Correct? categories 
are nonzero, so neither PLM nor EDR is always 
better than the other. 
In summary, EDR is the best at predicting the 
most popular emotion according to ACC@1, 
NDCG@1 and SACC@1. However, PLM gener-
ates ranked lists that better resemble the correct 
ranked lists according to ACC@k and SACC@k 
141
Method Average ?b Average p-value
PLM 0.584 0.068
EDR 0.474 0.114
NN 0.392 0.155
Table 1. Kendall?s ?b statistics. 
 
 He Su Sa Us Ha Bo An
Aw 0.80  0.75  0.78  0.77  0.82  0.76 0.79 
He  0.79  0.81  0.78  0.81  0.89 0.81 
Su   0.82  0.78  0.80  0.82 0.82 
Sa    0.78  0.80  0.84 0.82 
Us     0.82  0.91 0.82 
Ha      0.83 0.79 
Bo      0.80 
Table 2. Classification accuracies of SVM pairwise 
emotion classifiers on the test corpus. He = heartwarm-
ing, Su = surprising, Sa = sad, Us = useful, Ha = happy, 
Bo = boring, and An = angry. 
 
0.53
0.58
0.63
0.68
0.73
0.75 0.8 0.85 0.9
Accuracy of Pairwise Emotion Classification
A
ve
ra
ge
 D
is
cr
im
in
at
io
n
V
al
ue
 o
f E
m
ot
io
n 
P
ai
r
 
Figure 7. Accuracy of pairwise emotion classification 
and the corresponding average discrimination value. 
 
for k ? 2. Further analysis shows that neither 
method is always better than the other. 
6.5 Pairwise Ranking Quality of PLM 
In this subsection, we evaluate the performance of 
PLM in predicting pairwise orders. 
We first examine the quality of ranked lists gen-
erated by PLM in terms of pairwise orders. To do 
this, we use Kendall?s ?b correlation coefficient, 
which is a statistical measure for determining the 
correlation between two ranked lists when there 
may be ties between two items in a list (Liebetrau, 
1983). The value of ?b is determined based on the 
number of concordant pairwise orders and the 
number of discordant pairwise orders between two 
ranked lists. Therefore, this measure is appropriate 
for evaluating the effectiveness of PLM at predict-
ing pairwise orders correctly. ?b has the range [-1, 
1], where 1 means a perfect positive correlation, 
and -1 means two lists are the reverse of each other. 
When computing ?b of two ranked lists, we also 
calculate a p-value to indicate whether the correla-
tion is statistically significant. 
We compute ?b statistics between a predicted 
ranked list and the corresponding true ranked list. 
Table 1 shows the results. In Table 1, numbers in 
the ?Average ?b? and ?Average p-value? columns 
are averaged over all test instances. The statistics 
for EDR and NN are also included for comparison. 
From the table, we see that PLM has the highest 
average ?b value and the lowest average p-value, so 
PLM is better at preserving pairwise orders than 
EDR and NN methods. This observation verifies 
that PLM?s minimization of pairwise loss leads to 
better prediction of pairwise orders.  
We now look at the individual performance of 
the 28 pairwise emotion rankers gjk. As mentioned 
in Section 3.2, each pairwise emotion ranker gjk is 
equivalent to a binary classifier for classifying a 
document into the ej or ek category. So, we look at 
their classification accuracies in Table 2. In the 
table, accuracy ranges from 0.75 for the awesome-
surprising pair to 0.91 for the useful-boring pair. 
From the psychological perspective, the rela-
tively low accuracy of the awesome-surprising pair 
is expected, because awesome is surprising in a 
positive sense. So, readers should have a hard time 
distinguishing between these two emotions. And 
the SVM classifier, which models reader responses, 
should also find it difficult to discern these two 
emotions. Based on this observation, we suspect 
that the pairwise classification performance actu-
ally reflects the underlying emotional ambiguity 
experienced by readers. To verify this, we quantify 
the degree of ambiguity between two emotions, 
and compare the result to pairwise classification 
accuracy. 
To quantify emotional ambiguity, we introduce 
the concept of discrimination value between two 
emotions ej and ek in a document di, which is de-
fined as follows: 
 
)()(
)()(
kiji
kiji
efef
efef
+
?
 
 
where fi is the emotional probability function de-
fined in Section 3.1. Intuitively, the larger the dis-
crimination value is, the smaller the degree of 
ambiguity between two emotions is. 
Figure 7 shows the relationship between pair-
wise classification accuracy and the average dis-
crimination value of the corresponding emotion 
142
0.00
0.02
0.04
0.06
0.08
A
w
es
om
e
H
ea
rtw
ar
m
in
g
S
ur
pr
is
in
g
S
ad
U
se
fu
l
H
ap
py
B
or
in
g
A
ng
ry
Emotion
M
ea
n 
S
qu
ar
ed
 E
rr
or
NN
EDR
 
Figure 8. Mean squared error of NN and EDR for esti-
mating the emotional distributions of the test corpus. 
 
0.0
0.2
0.4
0.6
0.8
1 2 3 4 5 6 7 8
ACC@
A
cc
ur
ac
y
Metadata
Words
Bigrams
All
 
Figure 9. PLM performance using different features. 
 
pair. The general pattern is that as accuracy in-
creases, the discrimination value also increases. To 
provide concrete evidence, we use Pearson?s prod-
uct-moment correlation coefficient, which has the 
range of [-1, 1], where 1 means a perfect positive 
correlation (Moore, 2006). The coefficient for the 
data in Figure 7 is 0.726 with p-value < 0.01. Thus, 
pairwise emotion classification accuracy reflects 
the emotional ambiguity experienced by readers. 
In summary, PLM?s pairwise loss minimization 
leads to better pairwise order predictions than EDR 
and NN. Also, the pairwise classification results 
reveal the inherent ambiguity between emotions. 
6.6 Distribution Estimation Quality of EDR 
In this subsection, we evaluate EDR?s performance 
in estimating the emotional probability function fi. 
With the prior knowledge that a news article?s fi 
values sum to 1 over all emotions, and fi is between 
0 and 1, we adjust EDR?s fi predictions to produce 
proper distributions. It is done as follows. A pre-
dicted fi value greater than 1 or less than 0 is set to 
1 and 0, respectively. Then the predicted fi values 
are normalized to sum to 1 over all emotions.  
NN?s distribution estimation performance is in-
cluded for comparison. For NN, the predicted fi 
values of a test article are taken from the emotional 
distribution of the most similar training article.  
Figure 8 shows the mean squared error of EDR 
and NN for predicting fi. In the figure, the error 
generated by EDR is less than those by NN, and all 
the differences are statistically significant with p-
value < 0.01. Thus, EDR?s use of regression leads 
to better estimation of fi than the NN. 
6.7 Comparison of Features 
Figure 9 shows each of the three feature type?s 
ACC@k for predicting test instances? ranked lists 
when PLM is used. The feature comparison graph 
for EDR is not shown, because it exhibits a very 
similar trend as PLM. For both PLM and EDR, 
bigrams are better than words, which are in turn 
better than news metadata. In Figure 9, the combi-
nation of all three feature sets achieves the best 
performance. For both PLM and EDR, the im-
provements in ACC@k of using all features over 
words and metadata are all significant with p-value 
< 0.01, and the improvements over bigrams are 
significant for k ? 2. Hence, in general, it is better 
to use all three feature types together. 
7 Conclusions and Future Work 
This paper presents two methods to ranking reader 
emotions. The PLM method minimizes pairwise 
loss, and the EDR method estimates emotional dis-
tribution through regression. Experiments with 
significant tests show that EDR is better at predict-
ing the most popular emotion, but PLM produces 
ranked lists that have higher correlation with the 
correct lists. We further verify that PLM has better 
pairwise ranking performance than the other two 
methods, and EDR has better distribution estima-
tion performance than NN. 
As for future work, there are several directions 
we can pursue. An observation is that PLM ex-
ploits pairwise order information, whereas EDR 
exploits emotional distribution information. We 
plan to combine these two methods together. An-
other research direction is to improve EDR by 
finding better features. We would also like to inte-
grate emotion ranking into information retrieval. 
Acknowledgments 
We are grateful to the Computer and Information 
Networking Center, National Taiwan University, 
for the support of high-performance computing 
facilities. The research in this paper was partially 
supported by National Science Council, Taiwan, 
under the contract NSC 96-2628-E-002-240-MY3. 
143
References  
Saima Aman and Stan Szpakowicz. 2007. Identifying 
Expressions of Emotion in Text. In Proceedings of 
10th International Conference on Text, Speech and 
Dialogue, Lecture Notes in Computer Science 4629, 
196-205. Springer, Plze?, CZ. 
Aitao Chen, Jianzhang He, Liangjie Xu, Frederic Gey, 
and Jason Meggs. 1997. Chinese Text Retrieval 
wihtout using a Dictionary. In Proceedings of 20th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, 
42-49. Association for Computing Machinery, Phila-
delphia, US. 
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and 
Yoram Singer. 2003. An Efficient Boosting Algorithm 
for Combining Preferences. Journal of Machine 
Learning Research, 4, 933-969. 
Yi Hu, Jianyong Duan, Xiaoming Chen, Bingzhen Pei, 
and Ruzhan Lu. 2005. A New Method for Sentiment 
Classification in Text Retrieval. In Proceedings of 
2nd International Joint Conference on Natural Lan-
guage Processing, 1-9. Jeju Island, KR. 
Kalervo J?rvelin and Jaana Kek?l?inen. Cumulative 
Gain-based Evaluation of IR Techniques. 2002. 
ACM Transactions on Information Systems, 20(4), 
422-446. 
Thorsten Joachims. 2002. Optimizing Search Engines 
using Clickthrough Data. In Proceedings of 8th 
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining. Association for 
Computing Machinery, Edmonton, CA. 
Albert M. Liebetrau. 1983. Measures of Association. 
Sage Publications, Newbury Park, US. 
Kevin H. Lin, Changhua Yang, and Hsin-Hsi Chen. 
2007. What Emotions do News Articles Trigger in 
their Readers? In Proceedings of 30th ACM SIGIR 
Conference, 733-734. Association for Computing 
Machinery, Amsterdam, NL. 
Kevin H. Lin, Changhua Yang, and Hsin-Hsi Chen. 
2008. Emotion Classification of Online News Articles 
from the Reader?s Perspective. In Proceedings of In-
ternational Conference on Web Intelligence. Institute 
of Electrical and Electronics Engineers, Sydney, AU. 
David Moore. 2006. The Basic Practice of Statistics. 
W.H. Freeman and Company, New York, US. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment Classification Using 
Machine Learning Techniques. In Proceedings of 
2002 Conference on Empirical Methods in Natural 
Language Processing, 79-86. Association for Com-
putational Linguistics, Philadelphia, US. 
Tao Qin, Tie-Yan Liu, Wei Lai, Xu-Dong Zhang, De-
Sheng Wang, and Hang Li. 2007. Ranking with Mul-
tiple Hyperplanes. In Proceedings of 30th ACM 
SIGIR Conference, 279-286. Association for Com-
puting Machinery, Amsterdam, NL. 
Bernhard Sch?lkopf, Alex J. Smola, Robert C. William-
son, and Peter L. Barlett. 2000. New Support Vector 
Algorithms. Neural Computation, 12(5), 1207-1245. 
Carlo Strapparava and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text. In Proceedings of 4th 
International Workshop on Semantic Evaluations. 
Prague, CZ. 
Janyce M. Wiebe. 2000. Learning Subjective Adjectives 
from Corpora. In Proceedings of 17th Conference of 
the American Association for Artificial Intelligence, 
735-740. AAAI Press, Austin, US. 
Ting-Fan Wu, Chih-Jen Lin, and Ruby C. Weng. Prob-
ability Estimates for Multi-class Classification by 
Pairwise Coupling. 2004. Journal of Machine Learn-
ing Research, 5, 975-1005. 
144
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1260?1269,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Using Morphological and Syntactic Structures  
for Chinese Opinion Analysis 
 
 
Lun-Wei Ku Ting-Hao Huang Hsin-Hsi Chen 
 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan 
{lwku,thhuang}@nlg.csie.ntu.edu.tw;hhchen@ntu.edu.tw 
 
   
 
 
Abstract 
This paper employs morphological struc-
tures and relations between sentence seg-
ments for opinion analysis on words and 
sentences.  Chinese words are classified 
into eight morphological types by two 
proposed classifiers, CRF classifier and 
SVM classifier.  Experiments show that 
the injection of morphological information 
improves the performance of the word po-
larity detection.  To utilize syntactic struc-
tures, we annotate structural trios to repre-
sent relations between sentence segments.  
Experiments show that considering struc-
tural trios is useful for sentence opinion 
analysis.  The best f-score achieves 0.77 
for opinion word extraction, 0.62 for opin-
ion word polarity detection, 0.80 for opin-
ion sentence extraction, and 0.54 for opin-
ion sentence polarity detection. 
1 Introduction 
Sentiment analysis has attracted much attention 
in recent years because a large scale of subjective 
information is disseminated through various plat-
forms on the web.  Sentiment information can be 
applied to a wide variety of fields, including 
product recommendation, review summarization, 
public polling, and so on. 
Opinion dictionaries are important resources 
for identifying subjective information.  Several 
approaches were proposed to collect such re-
sources.  Wiebe (2000) learned subjective adjec-
tives from corpora.  Takamura et al (2005) ex-
tracted semantic orientations of words.  Ku et al 
(2007) measured sentiment degrees of Chinese 
words by averaging the sentiment scores of the 
composing characters.  When the opinion words 
are available, the polarities of sentences and 
documents can be determined by them.  Riloff 
and Wiebe (2003) learned the extraction patterns 
for subjective expressions.  Kim and Hovy (2004) 
found the polarity of subjective expressions.  
Pang et al (2002) and Dave et al (2003) ex-
plored various techniques at document level. 
Morphological information has been widely 
used in classifying words, telling the meanings, 
and doing other in-depth analysis (Tzeng and 
Chen, 2002).  However, morphological informa-
tion was seldom applied either in Chinese opin-
ion extraction, or in solving the coverage prob-
lem of opinion dictionary.  Instead of bag-of-
characters approach (Ku et al, 2007), this paper 
employs morphological structures of words to 
extract opinion words.   
Relations between sentence segments are also 
defined by linguistics in the Chinese language.  
These are similar to morphological structures 
between Chinese characters. Based on parsing 
trees of sentences, we identify these relations and 
utilize them for opinion analysis on sentences. 
As the experimental corpus, some researchers 
managed to generate annotated materials and 
gold standards under many constraints.  Ku set a 
standard for generating final answers from anno-
tations of multiple annotators (Ku et al, 2007), 
and Somasundaran annotated discourse informa-
tion from meeting dialogs to train a sentiment 
model (Somasundaran et al, 2007).  For multi-
lingual issues, researchers concerned mainly 
about the applicability of corpus and algorithms 
from the native language to foreign languages 
(Banea et al, 2008; Bautin et al, 2008). 
Several opinion analysis systems have been 
developed so far.  OASYS (Cesarano et al, 2007) 
and CopeOpi (Ku et al, 2007) allow users input 
their queries and select preferred data sources, 
1260
and then track opinions in a time zone.  For both 
systems, extracting opinions is the main focus, 
while holders and targets are identified implicitly 
when retrieving relevant documents.  Carenini?s 
team proposed a graphical user interface for 
evaluative texts (2006), in which color blocks 
were used to present the evaluations for compo-
nents of products.  Fair News Reader, a Japanese 
news Web system, incorporates sentiment infor-
mation insensibly in an interesting way (Kawai 
et al, 2007).   It provides readers ?balanced? re-
ports by analyzing the sentiment in news articles 
which readers have read, and suggests them new 
articles according to the analysis results.  It leads 
the application of opinion analysis to the direc-
tion of personalization. 
2 Chinese Morphological Structures 
In the Chinese language, a word is composed of 
one or more Chinese characters, and its meaning 
can be interpreted in terms of its composite char-
acters.  The morphological structures of Chinese 
words are formulated by three major processes in 
linguistics: compounding, affixation, and conver-
sion.  Compounding is a complex word-
formation process.  In most cases, two or more 
morphemes together are formed as a lexical item 
by this process.  Affixation is a morphological 
process, by which grammatical or lexical infor-
mation is added to a base form.  By the conver-
sion process, a word is changed from one part of 
speech into another without the addition or dele-
tion of any morphemes.   
Compounding is the most productive way to 
construct a Chinese word.  Mostly, a Chinese 
character itself carries meanings, so that a mor-
pheme can function as a character and has its 
own part of speech.  In some cases, a Chinese 
morpheme may carry no specific meaning and 
just makes a word more readable.  Cheng and 
Tian (1992) divided Chinese words into five 
morphological types based on the relations be-
tween the morphemes in compounding words.  
(1) Parallel Type: Two morphemes play coordi-
nate roles in a word.  For example, the mor-
phemes ??? (money) and ??? (wealth) are par-
allel in the word ???? (money-wealth). 
 
(2) Substantive-Modifier Type: A modified 
morpheme follows a modifying morpheme.  For 
example, the morpheme ??? (cry) is modified 
by ??? (bitterly) in the word ???? (bitterly-
cry). 
 
(3) Subjective-Predicate Type: One morpheme 
is an expresser and the other one is described.  
The structure is like a subject-verb sentence con-
densed in one word.  For example, the morpheme 
??? (heart) is a subject of the predicate ??? 
(hurt) in the word ???? (heart-hurt). 
 
(4) Verb-Object Type: The first morpheme is 
usually a verb which governs the second one, 
making this word similar to a verb followed by 
its object.  For example, the morpheme ??? 
(control) serves as the object of the verb ??? 
(lose) in the word ???? (lose-control). 
 
(5) Verb-Complement Type: The first mor-
pheme is usually a verb but sometimes can be an 
adjective, and the second morpheme explains the 
first one from different aspects.  For example, the 
morpheme ??? (clearly) expresses the aspects of 
the action ??? (look). 
 
Chinese words constructed by affixation proc-
ess can be one of the two cases ? say, morpheme 
and morpheme, or morpheme and non-morpheme.  
In the case of morpheme and morpheme, the af-
fixation word belongs to one of the above 5 types 
if the prefix and the suffix are neither negations 
nor confirmations.  Types 6 and 7 defined below 
represent the affixation words whose prefix or 
suffix is a negation or a confirmation.  The af-
fixation words whose prefix or suffix characters 
are not morphemes are classified into type 8. 
 
(6) Negation Type: There is at least one nega-
tion character in words of this type.  For example, 
the prefix ??? (no) is the negation morpheme in 
the word ???? (no-method). 
 
(7) Confirmation Type: There is at least one 
confirmation character in words of this type.  For 
example, the prefix  ??? (do) is a confirmation 
in the word ???? (do-depend on). 
 
(8) Others: Those words that do not belong to 
the above seven types are assigned to this type, 
such as words whose meanings are not a function 
of their composite characters, words whose com-
posite characters are not morphemes, such as ??
?? (nephew-suffix) and ???? (peppermint). 
3 Opinion Scores of Chinese Words  
The bag-of-characters approach proposed by Ku 
et al (2007) considers the observation probabili-
ties of characters in Chinese opinion words.  It 
calculates the observation probabilities of char-
acters from a set of seeds first, then dynamically 
enlarges the set and adjusts their probabilities.  In 
1261
this approach, the opinion score of a word is de-
termined by the combination of the observation 
probabilities of its composite characters defined 
by Formulas (1) and (2). 
 
??
?
==
=
+
= m
i
i
n
i
i
n
i
i
neg,Cf/neg,Cfpos,Cf/pos,Cf
pos,Cf/pos,Cf
posCP
11
1
)()()()(
)()(
),( (1)
??
?
==
=
+
= m
i
i
n
i
i
m
i
i
neg,Cf/neg,Cfpos,Cf/pos,Cf
neg,Cf/neg,Cf
negCP
11
1
)()()()(
)()(
),( (2)
),(),()( negCNposCPCS ?=
 
(3)
)(
1
)...(
1
21 ?
=
=
l
i
il CSl
CCCS
 
(4)
 
where C is an arbitrary Chinese character, f(C, 
polarity) counts the observed frequency of C in a 
set of Chinese words whose opinion polarity is 
positive (pos) or negative (neg); P(C, pos) and 
P(C, neg) denote the observation probabilities of 
C as a positive and a negative character, and n 
and m denote total number of unique characters 
in positive and negative words.  The difference 
of P(C, pos) and P(C, neg) in Formula (3) de-
termines the sentiment score of character C, de-
noted by S(C).  Formula (4) computes the opin-
ion score of a word of l characters C1C2?Cl by 
averaging their scores. 
Instead of counting the weights as in the bag-
of-characters approaches, we consider the word 
structures and propose a scoring function for 
each morphological type.  According to the Fre-
quency Dictionary of Modern Chinese, 96.5% of 
Chinese words are unigrams and bigrams (Chen, 
et al, 1997).  In the following functions, S(C1C2) 
computes the opinion scores of words with char-
acters C1 and C2.  SIGN(s) returns -1 if polarity 
degree s is smaller than 0, i.e., negative, and re-
turns 1 when positive. 
 
(1) Parallel Type: Since the two composite 
characters of a word of this type are homogene-
ous, the opinion score is the average score of two 
characters? opinion scores.   
 
 
2
)()(
)( 2121
CSCS
CCS
+
=
 (5)
 
 
(2) Substantive-Modifier Type: The first mor-
pheme of a word of this type modifies the second 
one, so that its opinion weight comes from the 
absolute opinion score of the first character, 
while the opinion polarity is determined by the 
occurrence of negative opinion characters.  If at 
least one negative opinion character appears, the 
word is negative, else it is positive.  For example, 
the word ???? (bitterly cry) is composed of 
??? (bitterly, negative) and ??? (cry, negative).  
Negative characters make this word negative and 
its opinion strength, i.e., the absolute value of the 
score, is decided by the first character for the 
degree of crying. 
 
)()()( else
)( 1- )( else      
 )(  )( then  )0)(  and  0)(( if      
 then)0)(  and  0)(( if
2121
121
12121
21
CSCSCCS
CSCCS
CSCCSCSCS
CSCS
+=
?=
=>>
??
 
(6)
      
 
(3) Subjective-Predicate Type: The first mor-
pheme of a word of this type is a subject and the 
second morpheme is the action it performs, so 
that the action decides the opinion score of the 
word.  If the action is not an opinion or it is neu-
tral, the subject determines the opinion score of 
this word.  For example, the word ???? (mud-
slide, negative) is composed of ??? (mountain, 
non-opinion) and ??? (collapse, negative). Its 
opinion score depends only on the second char-
acter ??? (collapse) since the first character is a 
subject and usually bears no opinions. 
 
)()( else
 )()( then )0)(( if
121
2212
CSCCS
CSCCSCS
=
=?  (7)
 
 
(4) Verb-Object Type: The first morpheme of 
words of this type acts upon the second mor-
pheme.  The effect depends not only on the ac-
tion but on the target.  The weight is determined 
by the action, but the polarity is the multiplica-
tion of the signs of the two morphemes.  For ex-
ample, the word ???? (to go away for the 
summer, positive) is composed of ??? (hide, 
negative) and ??? (hot summer, negative).  Its 
strength depends on the strength of ??? (hide) 
and polarity is positive from the multiplication of 
two negatives.  
 
)()()( else    
))(())(()()(    then  
)0)(  and  0)(( if
2121
21121
21
CSCSCCS
CSSIGNCSSIGNCSCCS
CSCS
+=
??=
??
(8)
 
 
(5) Verb-Complement Type: The scoring func-
tion for words of this type is defined the same as 
that of a Subjective-Predicate type in Formula 
(7).  The complement morpheme is the deciding 
factor of the opinion score.  For example, the 
word ???? (raise, positive) is composed of 
??? (carry or lift, non-opinion) and ??? (high, 
1262
positive).  The complement morpheme ?? ? 
(high) describes the resulting state of the verb 
morpheme ??? (raise), so both strength and po-
larity depend on the morpheme ??? (high). 
 
(6) Negation Type: A negative character speci-
fied in a predefined set NC has a negation effect 
on the opinion score of the other character.  The 
strength depends on the modified morpheme 
while the polarity of the word is the negation of 
the polarity of the modified morpheme. 
 
( )
( ) )(1)( else
 )(1)( then )( if
121
2211
CSCCS
CSCCSNCC
??=
??=?  (9)
 
 
(7) Confirmation Type: A positive character 
specified in a predefined set PC ensures that the 
opinion score of a word only comes from the 
other character.  Therefore, the opinion score of 
this word is determined by the modified mor-
pheme. 
 
)()( else )()( then )( if 1212211 CSCCSCSCCSPCC ==?  (10)
 
 
(8) Others: Since words of this type contain no 
clear cues for their morphological structures, we 
postulate that both characters have the same con-
tribution, and adopt Formula (5).  
4 Identification of Morphological Types 
To compute the opinion score of a word accord-
ing to formulae in Section 3, we must know its 
morphological type from the morphological 
structure, i.e., the parts of speech of the compos-
ite morphemes.  Currently, part of speech tag-
ging is performed at the word level rather than 
the morpheme level, and morpheme-tagging cor-
pus is not available.  We consider an on-line 
Chinese dictionary, Dictionary of Chinese Words 
by Ministry of Education, Taiwan (MOEDCW), 
as a corpus, and compute the statistics of each 
morpheme in it. 
Two classifiers, CRF classifier and SVM clas-
sifier are proposed to recognize morphological 
types (1)-(5).  Morphological types (6) to (8) are 
determined by rules such as whether two com-
posite characters are morphemes; whether there 
are confirmation/negation morphemes; and so on. 
4.1 MOEDCW Corpus 
MOEDCW corpus provides possible parts of 
speech for each morpheme by treating it as a uni-
gram word, and possible senses under each part 
of speech.  In each entry, there are a sense defini-
tion and some example words.  Figures 1 and 2 
show the specifications of two morphemes ??? 
and ???.  The morpheme ???  has three parts 
of speech (verb, adverb and noun) and includes 3, 
1, and 1 senses.  There are 3, 3, and 2 example 
words listed under the three verb senses. 
We can find the correct parts of speech of the 
composite characters of a word when it is an ex-
ample word in the dictionary.  However, not all 
words are listed in the corpus.  Consider the 
word ???? (sweat, verb).  Figure 1 shows that 
???? (sweat) is an example word listed under 
the verb sense of the character ??? (perspire), 
thus the character ??? (perspire) in the word ??
?? (sweat) functions as a verb.  However, ??
?? (sweat) is not an example for the character 
??? (sweat).  Figure 2 show that there are two 
possible parts of speech, noun and verb, for the 
character ??? (sweat).  We then show how to 
identify its function in the word ????.   
 
1
Goes out from the button to the top or 
from inside to outside.  For example, 
fume, smoking, and sweat. ?????
???????????????
?????????? 
2
Burst into or regardless of.  For example, 
take risk, to offend, and offense.  ?
??????????????
????????? 
verb 
3
Fake or on the pretext of.  For example, 
personate and to pretend to be. ???
??????????????? 
ad-
verb 1
Crude or rash.  For example, offensively 
and advance rashly.  ??????
???????????? 
noun 1 Family name. ?? 
Figure 1: Specification of ??? in MOEDCW 
1
Sweat.  For example, cold sweat, night 
sweat, sweatiness, and to drip with 
sweat. ?????????????
????????????????
??????????????? 
noun
2 Family name?? 
verb 1 To sweat ??????? 
Figure 2: Specification of  ??? in MOEDCW 
)( )( POS,CnsesNumberOfSePOS,CT =  (11)
 
The number of possible meanings one charac-
ter can bear when it functions as a certain part of 
1263
speech is employed to estimate how often this 
part of speech is used.  The function T(C, POS) 
shown in Formula (11) defines the score of a 
character C functioning as a particular part of 
speech POS.  Here, POS may be noun (N), adjec-
tive (ADJ), verb (V), adverb (ADV), auxiliary 
(AUX), conjunction (CONJ), pronoun (PRON), 
preposition (PREP), and interjection (INT).  In 
Figure 2, T(?<sweat>, N) = 2 and  T(?<sweat>, 
V) = 1. 
4.2 Features for Classifiers 
Features for training SVM and CRF classifiers 
include the pronunciation and the tone of the 
word, parts of speech of the first and the second 
characters of training words, and the position 
information of the composite characters.  The 
tone of the word is acquired from MOEDCW.  
The parts of speech are estimated by Formula 
(11).   f(C, POS, k, start/end) counts the number 
of k-grams (k=2, 3, 4).  In Figures 1 and 2, f(?, 
V, 2, start)=6, f(?, V, 2, end)= 2, f(?, ADV, 2, 
start) = 2, and f(?, ADV, 2, end)=0.  This ex-
ample shows that when the character ??? func-
tions as a verb or an adverb, it serves as the start-
ing character more often than the ending charac-
ter. 
4.3 CRF and SVM Classifier  
CRF and SVM are both common used algorithms 
for building classifiers (Lafferty et al, 2001).  
We adopted CRF++1 and libSVM (Chang and 
Lin, 2001) to develop our classifiers.  The fea-
tures for training our CRF and SVM classifiers 
include the input word W, the tone of W, the first 
and the second characters C1 and C2, T(C1, POS), 
T(C2, POS),  f(C1, POS, k, start), f(C1, POS, k, 
end), f(C2, POS, k, start), and f(C2, POS, k, end).  
POS denotes one of nine parts of speech in 
MOEDCW, and k equals to 2, 3 or 4. 
Using SVM is straightforward.  To classify a 
word into one of the morphological structure 
types, we construct the word's feature vector and 
input the vector into SVM.  When using CRF, a 
different approach is taken.  When predicting the 
classes of two successive instances, CRF takes 
the predicted class of the first instance into ac-
count when predicting the second instance's class.  
Here is how we exploit this capability.  In a nut-
shell, we perform classification at the character 
level instead of the word level.  Let W be a word 
composed of the two characters C1 and C2.  Let v 
                                                 
1 http://crfpp.sourceforge.net/ 
be the feature vector of W.  Let t be the morpho-
logical structure type of W.  We define C1's fea-
ture vector to be composed of the features in v 
which are related to C1, e.g., T(C1, verb).  Simi-
larly, C2's feature vector is composed of the fea-
tures in v which are related to C2.  C1's class and 
C2's class are defined as t_1 and t_2, respectively.  
Since t has five possible values, there are 10 
character classes. 
To determine a word W's morphological struc-
ture type, we first apply CRF on W's constituent 
characters C1 and C2's feature vectors.  For C1, 
CRF will return a set of probabilities P(C1,t_q), 
where q ? {1, 2}, indicating the likelihood of C1 
being an instance of class t_q.  Similarly, a set of 
probabilities P(C2,t_q) is returned for C2.  W's 
morphological structure type is defined as the 
value of t which maximizes the product of 
P(C1,t_1) and P(C2,t_2). 
Though CRF is mostly used for sequential la-
beling, the idea of using CRF is to tail this classi-
fication questions into a labeling question in or-
der to utilizing the position information of char-
acters.  As mentioned, if a word W of two char-
acters C1C2 is of type 1, CRF will label C1 1_1 
(type1_1st char) and C2 1_2 (type1_2nd char).  
The labeling of each character considers both the 
previous character's features and the next charac-
ter's features.  That is, if the current character is 
the first character, its previous character is an 
empty character (which is used for segmenting 
sequences in CRF); if the current character is the 
second character, its next character is an empty 
character. Hence the position information will be 
considered by CRF. 
5 Experiments and Discussion  
Experiments verify whether the morphological 
types benefit opinion polarity detection on words.  
The relation between the performance of mor-
phological classifiers and opinion polarity detec-
tion is discussed. 
5.1 Experimental Setup  
To compare the bag-of-characters approach (Ku 
et al, 2007) with our morphological structure 
approach, we adopt the same evaluation data set 
containing 836 words.  To evaluate the perform-
ance of our two morphological classifiers, we 
prepare two sets of words, including the testing 
set of 836 words for word-level opinion predic-
tion (abbreviated as OP), and a set of 8,186 
words selected from words in MOEDCW corpus 
and news documents except those can be classi-
1264
fied by patterns (abbreviated as TRAIN set), all 
with their morphological types annotated.  Table 
1 lists the distributions of morphological types in 
OP and TRAIN sets. 
The polarity of words is predicted by their 
opinion scores ranging between -1 to 1.  We set a 
positive threshold.  Those words with scores 
above it are considered as positive while those 
below this threshold multiplied by (-1) are re-
garded as negative.  The words with non-zero 
scores falling between the positive and negative 
thresholds are neutral.  Fifty grids from 0 to 0.5 
are searched for the best threshold.  Since the 
opinion extraction at word level concerns only 
word structure, no retraining for the best thresh-
old is need when domain shifts, which is a supe-
riority of our method. 
5.2 Morphological Type Classification and 
Polarity Detection  
The performances of CRF and SVM classifiers 
on each morphological type are listed in Table 2.  
We perform four-fold cross validation on the 
TRAIN set.  Results show that CRF classifier 
achieves better performance than SVM classifier 
in this task.  The accuracy of CRF classifier 
(0.70) is 8% higher than that of SVM classifier 
(0.62).  Note those type 8 words which could be 
extracted by rules are excluded from classifica-
tion experiment. The remaining type 8 words are 
usually proper names.  It is difficult for both 
classifiers to identify such words. 
Table 3 further shows the performance of po-
larity prediction using morphological types de-
termined by CRF classifier and SVM classifier.  
The performance of polarity detection is evalu-
ated by the f-score defined in Formula (12). 
The f-scores of polarity detection using CRF 
classified types and SVM classified types are 
0.5806 and 0.5938, respectively.  Both of them 
outperform baseline?s f-score 0.5455, i.e., the 
bag-of-characters approach (Ku et al, 2007).  
Experiments show that adopting morphological 
types annotated by two classifiers for polarity 
prediction has little difference.  In other words, 
CRF and SVM classifiers have an 8% f-score 
difference in their best performance of classifica-
tion, while the performance gap in word polarity 
prediction using morphological types provided 
by these two classifiers is around 1.3% only 
(0.5806 vs. 0.5938).  The reason may be that we 
define scoring functions of each morphological 
type in a straightforward way.  If they are not the 
best scoring functions, the benefit of considering 
the morphological type information could be re-
stricted.  Nevertheless, experimental results show 
that morphological type information is useful for 
word polarity detection (with p-value less than 
0.05). 
 
)(
)()(
opinionproposed
polaritycorrectopinioncorrect
P
?
= , 
)(
)()(
opiniongold
polaritycorrectopinioncorrect
R
?
= , 
RP
RP
scoref
+
??
=?
2 . 
(12)
 
set/type 1 2 3 4 5 6 7 8 
TRAIN 26.15 44.97 1.64 15.14 9.22 0 0 2.88 
OP 45.8 24.4 1.3 7.9 8.0 2.3 0.5 9.8 
Table 1: The Percentage of distribution for morphological types in TRAIN and OP sets 
MorphoType 1 2 3 4 5 8 Accuracy 
CRF 0.63 0.78 0.41 0.66 0.78 0.17 0.70 
SVM 0.49 0.73 0.22 0.52 0.55 0 0.62 
Table 2: The f-score of CRF and SVM classifiers 
We further examine how well our polarity de-
tection method works in combination with a 
word sentiment dictionary.  We use the NTUSD2 
word sentiment dictionary.  If a word appears in 
NTUSD, then the word's polarity is the one 
specified in NTUSD.  If a word does not appear 
in NTUSD, then the word's polarity is deter-
mined using our morphological type method. 
                                                 
2 http://nlg18.csie.ntu.edu.tw:8080/opinion/ 
After introducing a sentiment dictionary 
NTUSD3, CRF and SVM classifiers both achieve 
the f-score 0.77 for opinion word extraction, and 
achieve f-scores 0.61 and 0.62 for polarity detec-
tion, respectively.  Note that if only NTUSD is 
used to extract opinion words by string matching, 
the f-score is only 0.44. 
 
                                                 
3 http://nlg18.csie.ntu.edu.tw:8080/opinion/ 
1265
Polarity f-score Without NTUSD With NTUSD 
Ku 0.5455 0.5789 
CRF type 0.5806 0.6100 
SVM type 0.5938 0.6246 
 
Table 3: Prediction with Morphological Types 
We further analyze the improvement of polar-
ity prediction for each morphological type.  We 
find that the f-scores of polarity prediction of all 
morphological types are improved in different 
degrees, and among them the performance of 
type 2 words are improved the most.  We have 
shown that our method can assign an opinion 
score to an arbitrary word without any word 
thesauri by considering its morphological infor-
mation.  Moreover, since the Substantive-
Modifier (type 2) is the most common way to 
form a new word in the Chinese language 
(Cheng and Tian, 1992), the result presents the 
strength of our method in solving the coverage 
problem. 
6 Syntactic Structure for Chinese Opin-
ion Analysis 
As mentioned, the relations introduced in Section 
2 exist not only within words, but also between 
sentence segments.  Relations between sentence 
segments are represented by structural trios here-
after and will be introduced in next section.  We 
have already shown that morphological types are 
useful when extracting opinion words and would 
like to further testify whether structural trios also 
benefit the opinion analysis on sentences.  We 
annotate these relations manually, propose a 
method to identify these relations, and compare 
results of experimental settings using structural 
trios with those not using structural trios. 
6.1 Structural Trio 
Each node in a parsing tree dominates a word 
string in a sentence.  Linguistics have shown that 
there are also five relations between sentence 
segments: Parallel, Substantive-Modifier, Sub-
jective-Predicate, Verb-Object, and Verb-
Complement, same as morphological types (1) to 
(5).  Because parsing trees have hierarchical 
structures, we define a structural trio to represent 
a relation between two nodes as follows: 
(1) A structure trio contains two children 
nodes which bear a relation. 
(2) A structure trio contains one head node 
which is the nearest common parent of two 
children nodes in (1). 
 
 
Figure 3: Example of structural trios 
Figure 3 shows an example of a structure trio.  
It is a part of a parsing tree containing words ??
?? (obtain), ???? (happy), ???? (results).  
Two structural trios are shown in this example.  
The lower one contains two children nodes ??
?? (happy) and ???? (results), and is labeled 
as Substantive-Modifier (S-M (2)) in their near-
est common parent node, while the upper one 
contains two children nodes ???? (obtain) and 
?????? (happy results), and is labeled as 
Verb-Object (V-O (4)). 
6.2 Experimental Corpus 
To experiment with structural trios, we need the 
parsing trees of all experimental sentences.  For 
this purpose, we adopted Chinese Treebank 5.14 
as the experimental materials.  Chinese Treebank 
contains raw Chinese news documents together 
with their segmented, part of speech tagged, and 
parsed versions.  The parsed documents are 
adopted in experiments utilizing structural trios, 
and the part of speech tagged documents are used 
in experiments not utilizing structural trios. 
In Chinese Treebank, a unique ID is labeled 
on each sentence.  For each sentence, we had 
three annotators label their opinions and then we 
generate the gold standard following NTCIR 5 
MOAT protocol (Seki et al, 2008).  We also 
annotated structure trios in Chinese Treebank.  A 
total of 17,159 sentences are obtained after drop-
ping some faulty sentences such as empty sen-
tences and sentences composed of more than one 
parsing tree.  The statistics of opinion sentences 
and structural trios in the constructed experimen-
tal materials are shown in Table 4 and Table 5. 
 
 
                                                 
4 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? 
catalogId=LDC2005T01 
5 http://research.nii.ac.jp/ntcir/index-en.html 
1266
Opinion Non-Opinion 
Positive Neutral Negative 
6,380 1,537 1,714 # 
9,631 
7,528 
66.24 15.96 17.80 % 
56.13 
43.87 
Table 4: Statistics of opinion sentences 
Trio Type Number Percentage % 
2 18,483 36.85 
3 13,687 27.29 
4 15,970 31.84 
5 965 1.92 
Others 1,054 2.10 
Total 50,159 100.00 
Table 5: Statistics of structural trios 
6.3 Experiment Setup 
The aim of our experiments is to know how 
opinion analysis approach performs when mor-
phological and syntactic structures are incorpo-
rated.  They are compared with the bag-of-
character and bag-of-word approaches. We im-
plemented the bag-of-word approach proposed 
by Ku et al (2007) to show its performance on 
Chinese Treebank.  In their approach, the opin-
ion scores of words are summed to generate the 
opinion scores of sentences, and the negation 
words will negate the closest opinion words.  
Based on this approach, we further consider 
structural trios to experiment whether syntactic 
structures of sentences are beneficial for opinion 
analysis.  Because the scoring functions may not 
be straight forward as those we have adopted for 
opinion word extraction, we did not design scor-
ing functions for utilizing all types of structural 
trios.  Instead, we emphasize their original opin-
ion scores by multiplying a variable alpha to see 
whether these structures are important.  In this 
paper, alpha equals five. 
We have shown that word morphological 
structures benefit the word opinion extraction.  
When we experiment on sentences, we also in-
corporate the word morphological structures to 
see whether they are also useful for opinion 
analysis on sentences.  Five experimental set-
tings are listed as below:  
(1) bag[w]-bag[s]: structural information is 
not considered for both words and sen-
tences.  The bag-of-character approach 
is used to calculate the opinion scores of 
words, and the bag-of-word approach 
sentences. 
(2) struc[w]-bag[s]: morphological struc-
tures are utilized to calculate word opin-
ion scores, but structural trios are not 
considered. The bag-of-word approach 
is used to calculate the opinion scores of 
sentences. 
(3) bag[w]-struc[s]: structural trios are con-
sidered for calculating sentence opinion 
scores, while the bag-of-character ap-
proach is used to calculate the opinion 
scores of words. 
(4) struc[w]-(m)struc[s]: both word mor-
phological structures and manually la-
beled structural trios are adopted. 
(5) struc[w]-struc[s]: both morphological 
structure of words and system labeled 
structural trios are adopted. 
As we have shown that NTUSD is beneficial 
to the opinion analysis at word level, it is used as 
described in section 5.2 by default. 
Our system adopted CRF algorithm to label 
structural trios for setting (5).  The content string 
and the part of speech of the current node, its 
parent node, its offspring nodes in the next three 
generations, together with the depth of the cur-
rent node in the Chinese Treebank, are used as 
the features for each node in CRF.  The co-
occurrence of the current node and all its siblings 
are defined in CRF?s template file.  CRF will 
label whether the current node is the first child or 
the second child of a certain relation in a struc-
tural trio, or it is not part of any structural trios.  
A four-fold experiment is performed for the 
learning and testing of this labeling  process by 
CRF. 
6.4 Results and Discussion 
Table 6 shows the statistics of manually labeled 
structural trios in Chinese Treebank and identifi-
cation performance of CRF.  Table 7 shows the 
performance of five experiment settings de-
scribed in Section 6.3.  The experiment results 
show that the morphological structures of words 
do not have a large contribution for opinion sen-
tence analysis (setting 1 vs. setting 2; setting 3 vs. 
setting 4).  However, considering the structural 
trios improve the performance.   
 
 
 
 
 
 
 
 
1267
Trio Type Number Percentage f-Score
2 18,483 36.85% 0.4883
3 13,687 27.29% 0.4944
4 15,970 31.84% 0.6360
5 965 1.92% 0.2034
Others 1,054 2.10% 
Total 50159 100%  
Table 6: Statistics and Results of Identifying 
Structural Trios 
Setting Word [w] 
Sentence 
 [s] 
f-Score 
(opinion) 
f-Score 
(polarity)
1 bag bag 0.7073 0.4988 
2 struc bag 0.7162 0.5117 
3 bag struc 0.8000 0.5361 
4 struc (m)struc 0.7922 0.5297 
5 struc struc 0.7993 0.5187 
Table 7: Results of Opinion Extraction  
on Chinese Treebank 
By summarizing the experimental results in 
Section 5 and this section, we can conclude that 
considering the word morphological structures 
benefits the opinion polarity detection, but in the 
current approach its assistance to words does not 
propagate to sentences.  Considering the syntac-
tic structures, however, do help in opinion analy-
sis both for the opinion sentence extraction and 
the polarity detection.  The performance of opin-
ion extraction boosts to an f-score 0.80 and the 
performance of polarity detection an f-score 0.54.   
However, the utilization of structure trios 
needs the parsing tree of sentences as the prior 
knowledge.  Hence these two kinds of structural 
information may be suitable for different applica-
tions: structural trios for well written sentences 
such as those in the news articles, while the mor-
phological structures for casually written sen-
tences such as those appear in SMS messages or 
articles with limit length on the Web. 
Because there are no opinion experiments per-
formed on Chinese Treebank, we mention the 
performance of Ku?s approach (setting (1)) for 
opinion sentence extraction, f-score 0.6846, in 
NTCIR-7 MOAT task, on news articles, as a re-
sult for comparison.  Their approach was ranked 
the second in this task, and the best team 
achieved an f-score 0.7453. 
7 Conclusion and Future Work  
This paper considers morphological and syntac-
tic structures in analyzing Chinese opinion words 
and sentences.  For morphological structures, 
eight Chinese morphological types are defined. 
CRF classifier and SVM classifier for morpho-
logical type classification are proposed.  Experi-
ments show that CRF classifier achieves the best 
accuracy 0.70 in type classification, which is 8% 
better than SVM classifier.  We further show that 
word morphological structures benefit the opin-
ion word extraction significantly.  With the help 
of the sentiment dictionary NTUSD, the f-score 
of opinion word extraction achieves 0.77 and the 
f-score of the word polarity detection achieves 
0.62 when the word morphological types are 
provided by the SVM classifier.  They are com-
parably better than bag-of-character approach 
and the dictionary based approach. 
We defined structural trios to represent the re-
lations between sentence segments and also ex-
tract these relations using CRF algorithm.  Re-
sults show that considering structural trios bene-
fits the opinion analysis on sentences.  An f-
score 0.80 for opinion extraction and an f-score 
0.54 for polarity detection are achieved, which is 
a great improvement.  
The opinion scoring functions for morphologi-
cal types and structural trios are critical for polar-
ity detection, and scoring functions for words 
determine the scoring functions for sentences.  
Now we define these functions intuitively based 
on linguistic rules, but learning methods like re-
gression will be investigated in the future.  Ex-
amining the interaction of cues from word and 
sentence levels on the opinion sentence extrac-
tion and the opinion polarity detection is our next 
goal. 
Acknowledgement 
Research of this paper was partially supported by Na-
tional Science Council, Taiwan, under the contract 
NSC95-2221-E-002-265-MY3.  
References  
Banea, C., Mihalcea, R., Wiebe, J. and Hassan, S. 
2008. Multilingual Subjectivity Analysis Using 
Machine Translation. In Proceedings of Empirical 
Methods in Natural Language Processing (EMNLP 
2008). 
Bautin, M., Vijayarenu, L. and Skiena, S. 2008. Inter-
national sentiment analysis for news and blogs. In 
Proceedings of the International Conference on 
Weblogs and Social Media (ICWSM). 
Carenini, G., Ng, R. T. and Pauls, A. 2006. Interactive 
Multimedia Summaries of Evaluative Text. In Pro-
ceedings of the 11th International Conference on 
Intelligent User Interfaces (pp. 124-131), Sydney, 
Australia. 
1268
Cesarano, C., Picariello, A., Reforgiato, D. and 
Subrahmanian, V.S. 2007. The OASYS 2.0 Opin-
ion Analysis System.  Demo in Proceedings of In-
ternational Conference on Weblogs and Social 
Media (pp. 313-314), Boulder, CO USA. 
Chang, Chih-Chung and Lin, Chih-Jen. 2001. 
LIBSVM: a library for support vector machines, 
http://www.csie.ntu.edu.tw/~cjlin/libsvm  
Chen, A., Xu, L., Gey, F.C. and Meggs, J. 1997. Chi-
nese Text Retrieval without Using a Dictionary. 
ACM SIGIR Forum, Volume 31, Issue SI (pp. 42-
49). 
Cheng, X.-H. and Tian, X.-L. 1992. Modern Chinese.  
Bookman Books Ltd. 
Dave, K., Lawrence, S., and Pennock, D.M. 2003. 
Mining the Peanut Gallery: Opinion Extraction 
and Semantic Classification of Product Reviews. 
In Proc. of the 12th International WWW Confer-
ence (pp. 519-528). 
Kawai, Y., Kumamoto, T. and Tanaka, K. 2007. Fair 
News Reader: Recommending news articles with 
different sentiments based on user preference. In 
Proceedings of Knowledge-Based Intelligent In-
formation and Engineering Systems (KES), No. 
4692 in Lecture Notes in Computer Science (pp. 
612?622). 
Kim, S.-M. and Hovy, E. 2004. Determining the Sen-
timent of Opinions. In Proc. of the 20th ICCL (pp. 
1367-1373). 
Ku, L.-W. and Chen, H.-H. 2007. Mining Opinions 
from the Web: Beyond Relevance Retrieval. Jour-
nal of American Society for Information Science 
and Technology, Special Issue on Mining Web Re-
sources for Enhancing Information Retrieval, 
58(12), 1838-1850. 
Lafferty, J., McCallum, A. and Pereira, F. 2001. Con-
ditional Random Fields: Probabilistic Models for 
Segmenting and Labeling Sequence Data, In Proc. 
of ICML (pp.282-289). 
Pang, B., Lee, L. and Vaithyanathan, S. 2002. Thumbs 
up? Sentiment Classification Using Machine 
Learning Techniques. In Proc. of the 2002 Confer-
ence on EMNLP (pp. 79-86). 
Riloff, E. and Wiebe, J. 2003. Learning Extraction 
Patterns for Subjective Expressions. In Proc. of the 
2003 Conference on EMNLP (pp. 105-112).  
Seki, Y., Evans, D. K., Ku, L.-W., Sun, L., Chen, H.-H. 
and Kando, N. 2008.  Overview of Multilingual 
Opinion Analysis Task at NTCIR-7. In Proceed-
ings of the 7th NTCIR Workshop Meeting on 
Evaluation of Information Access Technologies: 
Information Retrieval, Question Answering, and 
Cross-Lingual Information Access. 
Somasundaran, S., Ruppenhofer, J. and Wiebe, J. 
2007. Detecting arguing and sentiment in meetings. 
Proceedings of the SIGdial Workshop on Dis-
course and Dialogue, 2007.8.6 
Takamura, H., Inui, T. and Okumura, M. 2005. Ex-
tracting Semantic Orientations of Words Using 
Spin Model. In Proc. of the 43rd Annual Meeting 
of the ACL (pp. 133-140). 
Tzeng, H. and Chen, K.-J. 2002. Design of Chinese 
Morphological Analyzer.  In Proc. of the 1st 
SIGHAN Workshop on Chinese Language Process-
ing, vol.18, 1-7. 
Wiebe, J. 2000. Learning Subjective Adjectives from 
Corpora. In Proc. of the 17th National Conference 
on AAAI and Twelfth Conference on IAAI (pp. 735-
740). 
 
1269
Classifying Biological Full-Text Articles for Multi-Database Curation 
Wen-Juan Hou, Chih Lee and Hsin-Hsi Chen
Department of Computer Science and Information Engineering, 
National Taiwan University, Taipei, Taiwan 
{wjhou, clee}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw
Abstract
In this paper, we propose an approach 
for identifying curatable articles from a 
large document set.  This system 
considers three parts of an article (title 
and abstract, MeSH terms, and captions) 
as its three individual representations 
and utilizes two domain-specific 
resources (UMLS and a tumor name list) 
to reveal the deep knowledge contained 
in the article.  An SVM classifier is 
trained and cross-validation is employed 
to find the best combination of 
representations.  The experimental 
results show overall high performance. 
1 Introduction 
Organism databases play a crucial role in 
genomic and proteomic research.  It stores the 
up-to-date profile of each gene of the species 
interested.  For example, the Mouse Genome 
Database (MGD) provides essential integration 
of experimental knowledge for the mouse 
system with information annotated from both 
literature and online sources (Bult et al, 2004).  
To provide biomedical scientists with easy 
access to complete and accurate information, 
curators have to constantly update databases 
with new information.  With the rapidly 
growing rate of publication, it is impossible for 
curators to read every published article.  Since 
fully automated curation systems have not met 
the strict requirement of high accuracy and recall, 
database curators still have to read some (if not 
all) of the articles sent to them.  Therefore, it 
will be very helpful if a classification system can 
correctly identify the curatable or relevant 
articles in a large number of biological articles. 
Recently, several attempts have been made to 
classify documents from biomedical domain 
(Hirschman et al, 2002).  Couto et al (2004) 
used the information extracted from related web 
resources to classify biomedical literature.  Hou 
et al (2005) used the reference corpus to help 
classifying gene annotation.  The Genomics 
Track (http://ir.ohsu.edu/genomics) of TREC 
2004 and 2005 organized categorization tasks.  
The former focused on simplified GO terms 
while the latter included the triage for "tumor 
biology", "embryologic gene expression", 
"alleles of mutant phenotypes" and "GO" articles.  
The increase of the numbers of participants at 
Genomics Track shows that biological 
classification problems attracted much attention. 
This paper employs the domain-specific 
knowledge and knowledge learned from full-text 
articles to classify biological text.  Given a 
collection of articles, various methods are 
explored to extract features to represent a 
document.  We use the experimental data 
provided by the TREC 2005 Genomics Track to 
evaluate different methods. 
The rest of this paper is organized as follows.  
Section 2 sketches the overview of the system 
architecture.  Section 3 specifies the test bed 
used to evaluate the proposed methods.  The 
details of the proposed system are explained in 
Section 4.  The experimental results are shown 
and discussed in Section 5.  Finally, we make 
conclusions and present some further work. 
2 System Overview 
Figure 1 shows the overall architecture of the 
proposed system.  At first, we preprocess each 
training article, and divide it into three parts, 
including (1) title and abstract, (2) MeSH terms 
assigned to this article, and (3) captions of 
figures and tables.  They are denoted as 
"Abstract", "MeSH", and "Caption" in this paper, 
respectively.  Each part is considered as a 
representation of an article.  With the help of 
domain-specific knowledge, we obtain more 
detail representations of an article.  In the 
model selection phase, we perform feature 
ranking on each representation of an article and 
employ cross-validation to determine the 
number of features to be kept.  Moreover, we 
use cross-validation to obtain the best 
combination of all the representations.  Finally, 
a support vector machine (SVM) (Vapnik, 1995; 
Hsu et al, 2003) classifier is obtained. 
159
3 Experimental Data
We train classifiers for classifying biomedical 
articles on the Categorization Task of the TREC 
2005 Genomics Track. The task uses data from 
the Mouse Genome Informatics (MGI) system
(http://www.informatics.jax.org/) for four
categorization tasks, including tumor biology,
embryologic gene expression, alleles of mutant
phenotypes and GO annotation. Given a 
document and a category, we have to identify
whether it is relevant to the given category.
The document set consists of some full-text 
data obtained from three journals, i.e., Journal of
Biological Chemistry, Journal of Cell Biology
and Proceedings of the National Academy of 
Science in 2002 and 2003.  There are 5,837
training documents and 6,043 testing documents.
4 Methods 
4.1 Document Preprocessing
In the preprocessing phase, we perform acronym
expansion on the articles, remove the remaining
tags from the articles and extract three parts of 
interest from each article.  Abbreviations are 
often used to replace long terms in writing 
articles, but it is possible that several long terms
share the same short form, especially for
gene/protein names. To avoid ambiguity and
enhance clarity, the acronym expansion 
operation replaces every tagged abbreviation 
with its long form followed by itself in a pair of 
parentheses.
4.2 Employing Domain-Specific Knowledge 
With the help of domain-specific knowledge, we 
can extract the deeper knowledge in an article. 
For example, with a gene name dictionary, we
can identify the gene names contained in an 
article.  Moreover, by further consulting
organism databases, we can get the properties of
the genes. Two domain-specific resources are
exploited in this study.  One is the Unified 
Medical Language System (UMLS) (Humphreys
et al, 1998) and the other is a list of tumor
names obtained from Mouse Tumor Biology
Database (MTB)1.
UMLS contains a huge dictionary of
biomedical terms ? the UMLS Metathesaurus
and defines a hierarchy of semantic types ? the 
UMLS Semantic Network. Each concept in the
Metathesaurus contains a set of strings, which
are variants of each other and belong to one or
more semantic types in the Semantic Network.
Therefore, given a string, we can obtain a set of 
semantic types to which it belongs. Then we 
obtain another representation of the article by 
gathering the semantic types found in the part of 
the article. Consequently, we get another three 
much deeper representations of an article after
this step. They are denoted as "AbstractSEM",
"MeSHSEM" and "CaptionSEM". 
We use the list of tumor names on the Tumor
task. We first tokenize all the tumor names and 
stem each unique token. With the resulting list 
of unique stemmed tokens, we use it as a filter to 
remove the tokens not in the list from the 
"Abstract" and "Caption", which produce 
"AbstractTM" and "CaptionTM".
4.3 Model Selection
As mentioned above, we generate several 
representations for an article. In this section, 
we explain how feature selection is done and
how the best combination of the representations 
1 http://tumor.informatics.jax.org/mtbwi/tumorSearch.do
A New 
Full-Text
Article
Full-Text
Training
Articles
Abstract
MeSH
Caption
Model
Selection
AbsSEM/TM
Preprocessing
MeSHSEM
CapSEM/TM
Domain-Specific
Knowledge
SVM
Classifier
Yes/NoPartsSEM/TMPreprocessing Multiple
Parts
Figure 1. System Architecture
160
of an article is obtained. 
For each representation, we first rank all the 
tokens in the training documents via the 
chi-square test of independence.  Postulating 
the ranking perfectly reflects the effectiveness of 
the tokens in classification, we then decide the 
number of tokens to be used in SVM 
classification by 4-fold cross-validation.  In 
cross-validation, we use the TF*IDF weighting 
scheme.  Each feature vector is then 
normalized to a unit vector.  We set C+ to ur* C-
because of the relatively small number of 
positive examples, where C+ and C- are the 
penalty constants on positive and negative 
examples in SVMs.  After that, we obtain the 
optimal number of tokens and the corresponding 
SVM parameters C- and gamma, a parameter in 
the radial basis kernel.  In the rest of this paper, 
"Abstract30" denotes the "Abstract" 
representation with top-30 tokens, 
"CaptionSEM10" denotes "CaptionSEM" with 
top-10 tokens, and so forth. 
After feature selection is done for each 
representation, we try to find the best 
combination by the following algorithm. 
Given the candidate representations with 
selected features, we start with an initial set 
containing some or zero representation.  For 
each iteration, we add one representation to the 
set by picking the one that enhances the 
cross-validation performance the most.  The 
iteration stops when we have exhausted all the 
representations or adding more representation to 
the set doesn?t improve the cross-validation 
performance. 
For classifying the documents with better 
features, we run the algorithm twice.  We first 
start with an empty set and obtain the best 
combination of the basic three representations, 
e.g., "Abstract10", "MeSH30" and "Caption10".  
Then, starting with this combination, we attempt 
to incorporate the three semantic representations, 
e.g., "Abstract30SEM", "MeSH30SEM" and 
"Caption10SEM", and obtain the final 
combination.  Instead of using this algorithm to 
incorporate the "AbstractTM" and "CaptionTM" 
representations, we use them to replace their 
unfiltered counterparts "Abstract" and "Caption" 
when the cross-validation performance is better. 
5 Results and Discussions 
Table 1 lists the cross-validation results of each 
representation for each category (in Normalized 
Utility (NU)2 measure).  For category Allele, 
"Caption" and "AbstractSEM" perform the best 
among the basic and semantic representations, 
respectively.  For category Expression, 
"Caption" plays an important role in identifying 
relevant documents, which agrees with the 
finding by the winner of KDD CUP 2002 task 1 
(Regev et al, 2002).  Similarly, MeSH terms 
are crucial to the GO category, which are used 
by top-performing teams (Dayanik et al, 2004; 
Fujita, 2004) in TREC Genomics 2004.  For 
category Tumor, MeSH terms are important, but 
after semantic type extraction, "AbstractSEM" 
exhibits relatively high cross-validation 
performance.  Since only 10 features are 
selected for the "AbstractSEM", using this 
representation alone may be susceptible to 
over-fitting.  Finally, by comparing the 
performance of the "AbstractTM" and 
"Abstract", we find the list of tumor names 
helpful for filtering abstracts. 
We list the results for the test data in Table 2.  
Column "Experiment" identifies our proposed 
methods.  We show six experiments in Table 2: 
one for Allele (AL), one for Expression (EX), 
one for GO (GO) and three for Tumor (TU, TN 
and TS).  Column "cv NU" shows the 
cross-validation NU measure, "NU" shows the 
performance on the test data and column 
"Combination" lists the combination of the 
representations used for each experiment.  In 
this table, "M30" is the abbreviation for 
"MeSH30", "CS10" is for "CaptionSEM10", and 
so on.  The combinations for the first 4 
experiments, i.e., AL, EX, GO and TU, are 
obtained by the algorithm described in Section 
4.3, while the combination for TN is obtained by 
substituting "AbstractTM30" for "Abstract30" in 
the combination for TU.  The experiment TS 
only uses the "AbstractSEM10" because its 
cross-validation performance beats all other 
combinations for the Tumor category. 
The combinations of the first 5 experiments 
illustrate that adding other inferior 
representations to the best one enhances the 
performance, which implies that the inferior 
ones may contain important exclusive 
information.  The cross-validation performance 
fairly predicts the performance on the test data, 
except for the last experiment TS, which relies 
on only 10 features and is therefore susceptible 
to over-fitting. 
                                                 
2 Please refer to the TREC 2005 Genomics Track Protocol 
(http://ir.ohsu.edu/genomics/2005protocol.html).
161
Allele Expression GO Tumor 
# Tokens / NU # Tokens / NU # Tokens / NU # Tokens / NU 
Abstract 10 / 0.7707 10 / 0.5586 10 / 0.4411 10 / 0.8055 
MeSH 10 / 0.7965 10 / 0.6044 10 / 0.4968 30 / 0.8106 
Caption 10 / 0.8179 10 / 0.7192 10 / 0.4091 10 / 0.7644 
AbstractSEM 10 / 0.7209 10 / 0.4811 10 / 0.3493 10 / 0.8814 
MeSHSEM 10 / 0.6942 10 / 0.4563 10 / 0.4403 10 / 0.7047 
CaptionSEM 30 / 0.6789 10 / 0.5433 10 / 0.2551 30 / 0.7160 
AbstractTM 30 / 0.8325 
CaptionTM 10 / 0.7498 
Table 1. Partial Cross-validation Results. 
Experiment cv NU NU Recall Precision F-score Combination 
AL (for Allele) 0.8717 0.8423 0.9488 0.3439 0.5048 M30+C10+A10+CS10+AS10+MS10 
EX (for Expression) 0.7691 0.7515 0.8190 0.1593 0.2667 M10+C10+CS10+MS10 
GO (for GO) 0.5402 0.5332 0.8803 0.1873 0.3089 M10+C10+MS10 
TU (for Tumor) 0.8742 0.8299 0.9000 0.0526 0.0994 M30+C30+A30+AS10+CS30 
TN (for Tumor) 0.8764 0.8747 0.9500 0.0518 0.0982 M30+C30+AT30+AS10+CS30 
TS (for Tumor) 0.8814 0.5699 0.6500 0.0339 0.0645 AS10
Table 2. Evaluation Results. 
Subtask NU (Best/Median) Recall (Best/Median) Precision (Best/Median) F-score (Best/Median) 
Allele 0.8710/0.7773 0.9337/0.8720 0.4669/0.3153 0.6225/0.5010 
Expression 0.8711/0.6413 0.9333/0.7286 0.1899/0.1164 0.3156/0.2005 
GO Annotation 0.5870/0.4575 0.8861/0.5656 0.2122/0.3223 0.3424/0.4107 
Tumor 0.9433/0.7610 1.0000/0.9500 0.0709/0.0213 0.1325/0.0417 
Table 3. Best and Median Results for Each Subtask on TREC 2005 (Hersh et al, 2005). 
To compare with our performance, we list the 
best and median results for each subtask on the 
genomics classification task of TREC 2005 in 
Table 3.  Comparing to Tables 2 and 3, it shows 
our experimental results have overall high 
performance. 
6 Conclusions and Further Work 
In this paper, we demonstrate how our system is 
constructed.  Three parts of an article are 
extracted to represent its content.  We 
incorporate two domain-specific resources, i.e., 
UMLS and a list of tumor names.  For each 
categorization work, we propose an algorithm to 
get the best combination of the representations 
and train an SVM classifier out of this 
combination.  Evaluation results show overall 
high performance in this study. 
Except for MeSH terms, we can try other 
sections in the article, e.g., Results, Discussions 
and Conclusions as targets of feature extraction 
besides the abstract and captions in the future.  
Finally, we will try to make use of other 
available domain-specific resources in hope of 
enhancing the performance of this system. 
Acknowledgements
Research of this paper was partially supported by 
National Science Council, Taiwan, under the 
contracts NSC94-2213-E-002-033 and 
NSC94-2752-E-001-001-PAE. 
References 
Bult, C.J., Blake, J.A., Richardson, J.E., Kadin, J.A., Eppig, 
J.T. and the Mouse Genome Database Group. The Mouse 
Genome Database (MGD): Integrating Biology with the 
Genome. Nucleic Acids Research, 32, D476?D481, 2004. 
Couto, F.M., Martins, B. and Silva, M.J. Classifying Biological 
Articles Using Web Resources. Proceedings of the 2004 
ACM Symposium on Applied Computing, 111-115, 2004. 
Dayanik, A., Fradkin, D., Genkin, A., Kantor, P., Lewis, D.D., 
Madigan, D. and Menkov, V. DIMACS at the TREC 2004 
Genomics Track. Proceedings of the Thirteenth Text 
Retrieval Conference, 2004. 
Fujita, S., Revisiting Again Document Length Hypotheses 
TREC-2004 Genomics Track Experiments at Patolis. 
Proceedings of the Thirteenth Text Retrieval Conference,
2004.
Hersh, W., Cohen, A., Yang, J., Bhuptiraju, R.T., Toberts, P. 
and Hearst, M. TREC 2005 Genomics Track Overview. 
Proceedings of the Fourteenth Text Retrieval Conference,
2005.
Hirschman, L., Park, J., Tsujii, J., Wong, L. and Wu, C.H. 
Accomplishments and Challenges in Literature Data 
Mining for Biology. Bioinformatics, 18(12): 1553-1561, 
2002.
Hou, W.J., Lee, C., Lin, K.H.Y. and Chen, H.H. A Relevance 
Detection Approach to Gene Annotation. Proceedings of the 
First International Symposium on Semantic Mining in 
Biomedicine, http://ceur-ws.org, 148: 15-23, 2005. 
Hsu, C.W., Chang, C.C. and Lin, C.J. A Practical Guide to 
Support Vector Classification. http://www.csie.ntu.edu.tw 
/~cjlin/libsvm/index.html, 2003. 
Humphreys, B.L., Lindberg, D.A., Schoolman, H.M. and 
Barnett, G.O. The Unified Medical Language System: an 
Informatics Research Collaboration. Journal of American 
Medical Information Association, 5(1):1-11, 1998. 
Regev, Y., Finkelstein-Landau, M. and Feldman, R. Rule-based 
Extraction of Experimental Evidence in the Biomedical 
Domain - the KDD Cup (Task 1). SIGKDD Explorations,
4(2):90-92, 2002. 
Vapnik, V. The Nature of Statistical Learning Theory,
Springer-Verlag, 1995. 
162
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 838 ? 848, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Integrating Punctuation Rules and Na?ve Bayesian  
Model for Chinese Creation Title Recognition 
Conrad Chen and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering,  
National Taiwan University, Taipei, Taiwan 
drchen@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw 
http://nlg.csie.ntu.edu.tw/ 
Abstract. Creation titles, i.e. titles of literary and/or artistic works, comprise 
over 7% of named entities in Chinese documents. They are the fourth large sort 
of named entities in Chinese other than personal names, location names, and 
organization names. However, they are rarely mentioned and studied before. 
Chinese title recognition is challenging for the following reasons. There are few 
internal features and nearly no restrictions in the naming style of titles. Their 
lengths and structures are varied. The worst of all, they are generally composed 
of common words, so that they look like common fragments of sentences. In 
this paper, we integrate punctuation rules, lexicon, and na?ve Bayesian models 
to recognize creation titles in Chinese documents. This pioneer study shows a 
precision of 0.510 and a recall of 0.685 being achieved. The promising results 
can be integrated into Chinese segmentation, used to retrieve relevant informa-
tion for specific titles, and so on. 
1   Introduction 
Named entities are important constituents to identify roles, meanings, and relation-
ships in natural language sentences. However, named entities are productive, so that it 
is difficult to collect them in a lexicon exhaustively. They are usually ?unknown? 
when we process natural language sentences. Recognizing named entities in docu-
ments is indispensable for many natural language applications such as information 
retrieval [2], summarization [3], question answering [7], and so on. 
Identifying named entities is even harder in Chinese than in many Indo-European 
languages like English. In Chinese, there are no delimiters to mark word boundaries and 
no special features such as capitalizations to indicate proper nouns, which constitute 
huge part of named entities. In the past, various approaches [1, 4, 10] have been pro-
posed to recognize Chinese named entities. Most of them just focused on MUC-style 
named entities [8], i.e., personal names, location names, and organization names. The 
extensive studies cover nearly 80% of named entities in real documents [1]. Although 
the performance of such kinds of named entity recognizers is satisfiable, the rest 20% of 
named entities are so far rarely mentioned and often ignored in previous studies.  
These rarely mentioned ones belong to various sorts, such as terminologies, aliases 
and nicknames, brands, etc. These sorts may not occur as frequently as personal 
names or location names in a corpus, but the importance of the former in documents 
 Integrating Punctuation Rules and Na?ve Bayesian Model 839 
of specific domains is no less than that of the latter. For example, knowing names of 
dishes would be very important to understand articles about cooking. Among these 
rarely addressed named entities, titles of creations, such as book names, song titles, 
sculpture titles, etc., are one of the most important sorts. According to Chen & Lee 
(2004)?s study [1] of Academia Sinica Balanced Corpus (abbreviated ASBC corpus 
hereafter), about 7% of named entities are titles of creations. In other words, more 
than one-third of rarely mentioned named entities are titles of creations.  
Chinese title recognition is challenging for the following reasons. There are no 
limitations in length and structures of titles. They might be a common word, e.g. ??
?? (Mistakes, a Chinese poem), a phrase, e.g. ??????? (Norwegian Wood, a 
song), a sentence, e.g. ?????????? (Don?t Cry for Me Argentina, a song), 
or even like nothing, e.g. ????????? (Rub ? Undescribable, a Chinese poetry 
collection). Besides, the choice of characters to name titles has no obvious prefer-
ences. Till now, few publications touch on Chinese title recognition. There are even 
no available corpora with titles being tagged.  
Several QA systems, such as Sekine and Nobata (2004) [9], used fixed patterns and 
dictionaries to recognize part of titles in English or Japanese. Lee et al (2004) [6] pro-
posed an iterative method that constructs patterns and dictionaries to recognize English 
titles. Their method cannot be adapted to Chinese, however, because the most important 
feature employed is capitalization, which does not exist in Chinese. 
In this paper, we propose a pioneer study of Chinese title recognition. An approach 
of integrating punctuation rules, lexicon, and na?ve Bayesian models is employed to 
recognize creation titles in Chinese documents. Section 2 discusses some cues for Chi-
nese title recognition. Section 3 gives a system overview.  Punctuation rules and title 
gazetteer identify part of titles and filter out part of non-titles. The rest of undetermined 
candidates are verified by na?ve Bayesian model. Section 4 addresses which features 
may be adopted in training na?ve Bayesian model. Section 5 lists the training and testing 
materials, and shows experimental results.  Section 6 concludes there marks. 
2   Cues for Chinese Creation Title Recognition 
Titles discussed in this paper cover a wide range of creations, including literature, 
music, painting, sculpture, dance, drama, movies, TV or radio programs, books, 
newspapers, magazines, research papers, albums, PC games, etc. All of these titles are 
treated as a single sort because they share the same characteristics, i.e., they are 
named by somebody with creativity, and thus there are nearly no regularity or limita-
tions on their naming styles.  
The challenging issue is that, unlike MUC-style named entities (MUC7, 1998), ti-
tles are usually composed of common words, and most of them have no internal fea-
tures like surnames or entity affixes, e.g. ??? (City) in ????? (Taipei City). In 
other words, most titles might look just like common strings in sentences. Thus it is 
even more difficult to decide which fragment of sentences might be a title than to 
determine if some fragment is a title. 
For the lack of internal features, external features or context information must be 
found to decide boundaries of titles. Table 1 shows some words preceding or follow-
ing titles in one-tenth sampling of ASBC corpus with titles tagged manually. We can 
840 C. Chen and H.-H. Chen 
observe that quotation marks are widely used. This is because writers usually quote 
titles with punctuation marks to make them clear for readers. The most common used 
ones are the two pairs of quotation marks ???? and ????. About 40% of titles are 
quoted in ???? or ???? in our test corpus. However, labeling proper nouns is 
only one of their functions.  Quotation marks are extensively used in various pur-
poses, like dialogues, emphasis, novel words, etc. In our analysis, only less than 7% 
of strings quoted in ???? or ???? are creation titles.  It means the disambiguation 
of the usages of quotation marks is necessary. 
Table 1. Preceding and Following Words of Titles in One-tenth Sampling of ASBC Corpus 
Preceding 
Word Frequency 
Following 
Word Frequency 
? 450     ? 442     
? 216     ? 216     
? 44     ? 56     
? 31     ? 32     
? 25     ? 26     
? 24     ? 16     
? 19     ? 13     
? 16     ? 11     
? 9     ? 7     
? 7     ? 6     
? 6     ? 6     
? 6     ? 6     
The most powerful external feature of creation titles is French quotes ????, 
which is defined to represent book names in the set of standard Simplified Chinese 
punctuation marks of China [5]. However, they are not standard punctuation marks in 
Traditional Chinese. Besides the usage of French quotes to mark book names, writers 
often use them to label various types of creation titles. According to our analysis on 
Web searching and the sampling corpus, about 20% of occurrences of titles in Tradi-
tional Chinese documents are quoted in ????, and nearly no strings other than titles 
would be quoted in ????. This punctuation mark shows a very powerful cue to deal 
with title recognition. 
Nevertheless, there are still 40% of titles without any marks around. These un-
marked titles usually stand for widely known or classic creations. In other words, 
these famous works are supposed to be mentioned in many documents many times. 
Such kinds of titles are extensively known by people like a common vocabulary. A 
lexicon of famous creations should cover a large part of these common titles. 
3   System Overview 
Based on the analyses in Section 2, we propose some punctuation rules that exploit 
the external features of titles to recognize possible boundaries of titles in Chinese 
 Integrating Punctuation Rules and Na?ve Bayesian Model 841 
documents. Most strings that cannot be titles are filtered by these rules. Titles with 
strong evidences like ?? ? ? are also identified by these rules. The rest undecided 
strings are denoted as ?possible titles.? To verify these candidates is somewhat similar 
to solve word sense disambiguation problem. Na?ve Bayesian classifier is adopted to 
tell whether a candidate is really a title or not. The overview of our system is shown 
in Figure 1. 
 
Fig. 1. System Overview 
 
Fig. 2. Decision Tree of Punctuation Rules and Lexicon 
Figure 2 shows the applications of the punctuation rules and the title lexicon, 
which are illustrated as a decision tree. HR1 exploits French quotes ???? to identify 
titles like ??????? (Taohua Shan, a traditional Chinese drama by Kong, Shang-
Ren) and ?????????? (El General En Su Laberinto, a novel by Garcia 
Marquez). HR2a and HR2b then look up the title lexicon to find famous titles like ??
???? (Cien Anos de Soledad) and ?????? (Romance of Three Kingdoms). 
HR3 limits our recognition scope to strings quoted in quotation marks, and HR4 and 
842 C. Chen and H.-H. Chen 
HR5 filter out a major sort of non-titles quoted in quotation marks, dialogues, such as 
????????????? (I said, ?the audience should be careful!?) and ????
???????????? (Rene Descarte said, ?I think, therefore I am.?). 
The title lexicon we use is acquired from the catalogues of the library of our uni-
versity. These titles are sent to Google as query terms. Only the ones that have ever 
been quoted in ?? ? ? in the first 1,000 returned summaries are kept. The remained 
titles are checked manually and those ones that possibly form a fragment of a com-
mon sentence are dropped to avoid false alarms. After filtering, there are about 7,200 
entries in this lexicon. Although the lexicon could cover titles of books only, it is still 
useful because books are the major sort of creations.  
The punctuation rules and lexicon divide all the strings of a document into three 
groups ? say, titles, non-titles, and possible titles. All strings that cannot be definitely 
identified by the punctuation rules and lexicon are marked as ?possible? titles. These 
possible titles are then verified by the second mechanism, the na?ve Bayesian model. 
The na?ve Bayesian model will be specified in the next section. 
4   Na?ve Bayesian Model 
Na?ve Bayesian classifier is widely used in various classification problems in natural 
language processing. Since it is simple to implement and easy to train, we adopt it in 
our system to verify the possible titles suggested by the decision tree.  
Na?ve Bayesian classification is based on the assumption that each feature being 
observed is independent of one another. The goal is to find the hypothesis that would 
maximize the posterior probability P(H|F), where H denotes the classifying hypothe-
ses and F denotes the features that determine H. According to Bayesian rule, the pos-
terior probability can be rewritten as: 
P(H | F) = P(H) P(F | H) / P(F) (1) 
Since P(F) is always the same under different hypotheses, we only need to find which 
hypothesis would obtain the maximal value of P(H)P(F|H). Besides, under the inde-
pendence assumption, Equation (1) is rewritten into: 
P(H | F) = P(H) ? P( fi | H)     where F = { f1, f2,?, fn } (2) 
In our system, we have two hypotheses: 
H1: candidate S is a title 
H2: candidate S is not a title 
Four features shown below will be considered. The detail will be discussed in the 
subsequent paragraphs. 
F1: Context  
F2: Component 
F3: Length 
F4: Recurrence 
 Integrating Punctuation Rules and Na?ve Bayesian Model 843 
Context. To exploit contextual features, our system adopts a word-based, position-
free unigram context model with a window size of 5. In other words, our context 
model can be viewed as a combination of ten different contextual features of the na?ve 
Bayesian classifier, five of them are left context and the other five are right context. It 
can be represented as:  
P(Fcontext|H) = P(L5, L4, L3, L2, L1, R1, R2, R3, R4, R5 | H) (3) 
Where Li and Ri denote preceding and following words of the possible title we want to 
verify, and H denotes the hypothesis.   
If we postulate that the contextual features are independent of each other, then 
equation (3) can be transformed to: 
P(Fcontext|H) = ? P(Li |H) ? P(Ri | H) (4) 
Equation (4) assumes that the distance from a contextual word to a possible title is 
not concerned both in training and testing. The reason is that we do not have a realis-
tic, vast, and well-tagged resource for training. On the other hand, if we want to ex-
ploit it in testing, we need a well-tagged corpus to learn the best weights we should 
assign to contextual words of different distances. 
Component. Context deals with features surroundings titles. In contrast, Component 
further considers the features within titles.  Similar to the above discussion, our 
component model is also a word-based, position-free unigram model. A possible title 
will be segmented into a word sequence by standard maximal matching. The words in 
the segmentation results are viewed as the ?components? of the possible title, and the 
component model can be represented as: 
P(Fcomp|H) = P(C1?Cn | H) = ? P(Ci | H) (5) 
Where Ci denotes the component of the possible title we want to verify, and H de-
notes the hypothesis. 
Similar to the context model, the position of a component word is not concerned 
both in training and testing. Besides the availability issue of large training corpus, the 
lengths of possible titles are varied so that positional information is difficult to be 
exploited. Different titles consist of different number of component words. There are 
no straightforward or intuitive ways of using positional information. 
Length. The definition of Length feature is the number of characters that constitute 
the possible title. It can be represented as:  
P(Flength|H) = P(the length of S | H) (6) 
Where S denotes the possible title to be verified and H denotes the hypothesis that S 
is a title. 
Recurrence. The definition of Recurrence feature is number of occurrences of the 
possible title in the input document. It can be represented as: 
P(FRec|H) = P(the appearing times of S | H) (7) 
Where S denotes the possible title to be verified and H denotes the hypothesis that S 
is a title. 
844 C. Chen and H.-H. Chen 
5   Experiment Results 
The estimation of P(H) and P(F|H) is the major issue in na?ve Bayesian model. There 
are no corpora with titles being tagged available. To overcome this problem, we used 
two different resources in our training process. The first one is a collection of about 
300,000 titles, which is acquired from library catalogues of our university. This col-
lection is used to estimate Component and Length features of titles. Besides, these 
titles are regarded as queries and submitted to Google. The returned summaries are 
segmented by maximal matching and then used to estimate Context features of titles. 
Since titles are usually composed of common words, not all query terms in retrieved 
results by Google are a title. Therefore, only the results with query terms quoted in 
French quotes ???? are adopted, which include totally 1,183,451 web page summa-
ries. Recall that French quotes are a powerful cue to recognize creation titles, which 
was discussed in Section 2. 
The second resource used in training is ASBC corpus. Since titles in ASBC corpus 
are not specially tagged and we are short-handed to tag them by ourselves, a compro-
mised approach is adopted. First, the decision tree shown in Figure 2 is used to group 
all strings of the training corpus into titles, non-titles, and possible titles. All titles thus 
extracted are used to estimate the Recurrence feature of titles, and all possible titles 
are treated as non-titles to estimate all features of non-titles. Since the probability of 
possible titles being titles are much less than being non-titles, the bias of the rough 
estimation is supposed to be tolerable.  
We separate one-tenth of ASBC corpus and tag it manually as our testing data. The 
rest nine-tenth is used for training. There are totally 610,760 words in this piece of 
data, and 982 publication or creation titles are found. During execution of our system, 
the testing data are segmented by maximal matching to obtain context and component 
words of possible titles. To estimate P(H), we randomly select 100 possible titles 
from the training part of ASBC corpus, and classify them into titles and non-titles 
manually. Then we count the probability of hypotheses from this small sample to 
approximate P(H).  
Table 2 shows the performance of the decision tree proposed in Figure 2 under the 
testing data. If we treat HR2a and HR2b as a single rule that asks ?Is the string an 
entry in the title lexicon and not in a general dictionary??, we could view our rules as 
an ordered sequence of decisions. Each rule tells if a part of undecided strings are 
titles or non-titles, which is denoted in the column ?Decision Type? of Table 2. The 
column ?Decided? shows how many strings can be decided by the corresponding 
rules, while the columns of ?Undecided Titles? and ?Undecided Non-Titles? denote 
how many titles and non-titles are remained in the testing data after applying the cor-
responding rule. The correctness of the decision is denoted in the columns of ?Cor-
rect? and ?Wrong?. 
Table 2 shows that these five rules are very good clues to recognize titles. HR1, 
HR2, HR4 and HR5 have precisions of 100%, 94.01%, 99.15%, and 100% respec-
tively. Because the number of non-titles is much larger than that of titles, the actual 
precision of HR3 is comparatively meaningless. These rules could efficiently solve a 
large part of the problem. The rest possible titles are then classified by the na?ve 
Bayesian classifier. The performance is listed in Table 3.  We try different combina-
tions of the four features.  F1, F2, F3, and F4 denote Context, Component, Length, 
 Integrating Punctuation Rules and Na?ve Bayesian Model 845 
and Recurrence, respectively.  The number of True Positives, True Negatives, and 
False Positives are listed.  Precision, recall and F-measure are considered as metrics to 
evaluate the performance.  
Table 2. Performance of Decision Tree in Figure 2 
 Decision 
Type Decided Correct Wrong 
Undecided 
Titles 
Undecided 
Non-Titles 
HR1 Title 216 216 0 766 ~|corpus|2/2 
HR2 Title 167 126 411 640 ~|corpus|2/2 
HR3 Non-Title ~|corpus|2/2 ~|corpus|2/2 186 454 5812 
HR4 Non-Title 1997 1980 17 437 3832 
HR5 Non-Title 372 372 0 437 3458 
Note that there are two different numbers in the False Positive, Precision, and F-
measure columns in Table 3. The left number shows the total number of false positive 
errors, and the right one ignores the errors caused by other sorts of named entities. 
This is because many false positive errors come from other types of named entities. 
For example, in the sentence ???????????????????????
??? (attend 1994 35th International Mathematical Olympiad), ?????????
????????????? (?1994 35th International Mathematical Olympiad?) is 
a contest name, however, ill-recognized as a title by our system. Because there are 
various sorts of ill-recognized named entities and most of them have not been thor-
oughly studied, there are no efficient ways available to solve these false alarms. For-
tunately, in many applications, there would be little harm incorrectly recognizing 
these named entities as titles.  
The other major source of false positive errors is appearances of monosyllabic 
words. For example, in the sentence ????????????? (?Render Good for 
Evil? is Lao Tzu?s speech), ?????? (?Render Good for Evil?) are ill-recognized 
as titles. The reason might be that many context and component words of titles are 
named entities or unknown words. During training, these named entities are neither 
tagged nor recognized, so that most of these named entities are segmented into se-
quences of monosyllabic words. Therefore, while the na?ve Bayesian classifier en-
counters monosyllabic context or component words, it would prefer recognizing the 
possible title as a title.  
From Table 3, we could observe that Context and Component are supportive in 
both precision and recall. Length boosts precision but decreases recall while Recur-
rence is on the contrary. The combination of F1+F2+F3 obtains the best F-measure, 
but the combination of all features might be more useful in practical applications, 
                                                          
1
  Total 31 of them can be easily corrected by a maximal-matching-driven segmentation. For 
example, ???? (x?n j?ng, Heart Sutra, a Buddha book) in ?????? (y?ng x?n j?ng y?ng) 
is an entry in the title lexicon. However, maximal matching prefers the segmentation of ??
? / ??? (y?ng x?n/j?ng y?ng) than ?? / ?? / ?? (y?ng/x?n j?ng/y?ng), so that this false 
alarm would be recovered. 
846 C. Chen and H.-H. Chen 
since it only sacrifices 1.4% of precision but gains 3% of recall in comparison with 
the former. Table 4 summaries the total performance of our creation title recognition 
system.  It achieves the F-measure of 0.585. 
Table 3. Performance of the Na?ve Bayesian Classifier Using Different Features 
  
True  
Positive 
True  
Negative
False  
Positive Precision Recall F-measure 
F1 277 160 959 / 772 0.224 / 0.264 0.634 0.331 / 0.373 
F2 153 284 532 / 332 0.223 / 0.315 0.350 0.273 / 0.332 
F1 + F3 273 164 859 / 676 0.241 / 0.288 0.625 0.348 / 0.394 
F2 + F3 148 289 453 / 247 0.246 / 0.375 0.339 0.285 / 0.356 
F1 + F2 288 149 976 / 722 0.228 / 0.285 0.659 0.339 / 0.398 
F1 + F4 289 148 1067 / 867 0.213 / 0.250 0.661 0.322 / 0.363 
F2 + F4 169 268 695 / 467 0.196 / 0.266 0.387 0.260 / 0.315 
F1 + F2 + F3 286 151 888 / 631 0.244 / 0.312 0.654 0.355 / 0.422 
F1 + F3 + F4 285 152 946 / 750 0.232 / 0.275 0.652 0.342 / 0.387 
F2 + F3 + F4 164 273 542 / 320 0.232 / 0.339 0.375 0.287 / 0.356 
All 299 138 967 / 703 0.236 / 0.298 0.684 0.351 / 0.416 
Table 4. Performance of the Title Recognition System 
 
True 
Positive 
True 
Negative
False  
Positive Precision Recall F-measure 
Decision Tree 342 203   41 / 10 0.915 / 0.978 0.685 0.783 / 0.806 
Na?ve Bayes-
ian 299 138  967 / 703 0.236 / 0.298 0.684 0.351 / 0.416 
Total 641 341 1008 / 713 0.424 / 0.510 0.685 0.524 / 0.585 
6   Conclusion 
This paper presents a pioneer study of Chinese title recognition.  It achieves the preci-
sion of 0.510 and the recall of 0.685.  The experiments reveal much valuable informa-
tion and experiences for further researches. 
First, the punctuation rules proposed in this paper are useful to recognize creation 
titles with a high precision. They can relief our burdens in building more resources, 
make supervised learning feasible, and give us some clues in similar studies like rec-
ognition of other sorts of named entities. These useful rules are also helpful for those 
applications needing high accuracies. For example, we can exploit these rules on an 
information retrieval system to filter out noises and show only the information about 
the requested creation or the publication. 
Second, na?ve Bayesian classifier could achieve a comparable recall on the verifi-
cation of possible titles. Since we only adopt simple features and use a rough estima-
tion in feature model building, the result shows that na?ve Bayesian classifier is  
 Integrating Punctuation Rules and Na?ve Bayesian Model 847 
practicable in recognizing creation titles. In future works, we may find other useful  
features and adopt more sophisticated models in na?ve Bayesian classifier to seek a 
higher performance, especially in precision.  
Third, our result shows that recognizing rarely seen sorts of named entities is 
practicable. Because un-recognized named entities might significantly affect subse-
quent applications in Chinese, in particular, segmentation, we should not ignore the 
problems introduced by Non-MUC style named entities. Our study suggests that the 
recognition of these rarely mentioned named entities is promising. The perform-
ances of many applications, such as natural language parsing and understanding, 
might be boosted through adding the mechanism of recognizing these rare  
named entities. 
Finally, our research can also be extended to other oriental languages, such as 
Japanese, in which there are no explicit features like specialized delimiters or capitali-
zations to mark creation titles. Just as Chinese, un-recognized named entities in these 
languages might affect the performances of natural language applications. Recogniz-
ing Non-MUC style named entities is an indispensable task to process these  
languages. 
Acknowledgement 
Research of this paper was partially supported by National Science Council, Taiwan, 
under the contract NSC94-2752-E001-001-PAE. 
References 
1. Chen, Conrad and Lee, Hsi-Jian. 2004. A Three-Phase System for Chinese Named Entity 
Recognition, Proceedings of ROCLING XVI, 2004, 39-48. 
2. Chen, Hsin-Hsi, Ding, Yung-Wei and Tsai, Shih-Chung. 1998. Named Entity Extraction 
for Information Retrieval, Computer Processing of Oriental Languages, Special Issue on 
Information Retrieval on Oriental Languages, 12(1), 1998, 75-85. 
3. Chen, Hsin-Hsi, Kuo, June-Jei, Huang, Sheng-Jie, Lin, Chuan-Jie and Wung, Hung-Chia. 
2003. A Summarization System for Chinese News from Multiple Sources, Journal of 
American Society for Information Science and Technology, 54(13), November 2003, 
1224-1236. 
4. Chen, Zheng, W. Y. Liu, and F. Zhang. 2002. A New Statistical Approach to Personal 
Name Extraction, Proceedings of ICML 2002, 67-74. 
5. Gong, Chian-Yian and Liu, Yi-Ling. 1996. Use of Punctuation Mark. GB/T15834-1995. 
http://202.205.177.129/moe-dept/yuxin-/content/gfbz/ managed/020.htm 
6. Lee, Joo-Young, Song, Young-In, Kim, Sang-Bum, Chung, Hoojung and Rim, Hae-
Chang. 2004. Title Recognition Using Lexical Pattern and Entity Dictionary, Proceedings 
of AIRS04, 342-348. 
7. Lin, Chuan-Jie, Chen, Hsin-Hsi, Liu, Che-Chia, Tsai, Ching-Ho and Wung, Hung-Chia. 
2001. Open Domain Question Answering on Heterogeneous Data, Proceedings of ACL 
Workshop on Human Language Technology and Knowledge Management, July 6-7 2001, 
Toulouse France, 79-85. 
848 C. Chen and H.-H. Chen 
8. MUC7. 1998. Proceedings of 7th Message Understanding Conference, Fairfax, VA, 1998, 
http://www.itl.nist.gov/iaui/894.02/related_projects/muc/index.html. 
9. Sakine, Satoshi and Nobata, Chikashi. 2004.  Definition, Dictionaries and Tagger for Ex-
tended Named Entity Hierarchy, Proceedings of LREC04. 
10. Sun, Jian, J. F. Gao, L. Zhang, M. Zhou, and C. N. Huang. 2002. Chinese Named Entity 
Identification Using Class-based Language Model, Proceedings of the 19th International 
Conference on Computational Linguistics, Taipei, 967-973 
Analysis of Intention in Dialogues Using Category Trees and 
Its Application to Advertisement Recommendation 
Hung-Chi Huang Ming-Shun Lin Hsin-Hsi Chen 
Department of Computer Science and Information Engineering  
National Taiwan University  
Taipei, Taiwan  
{hchuang, mslin}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw  
 
 
Abstract 
We propose an intention analysis system 
for instant messaging applications.  The 
system adopts Yahoo! directory as category 
trees, and classifies each dialogue into one 
of the categories of the directory. Two 
weighting schemes in information retrieval, 
i.e., tf and tf-idf, are considered in our ex-
periments.  In addition, we also expand 
Yahoo! directory with the accompanying 
HTML files and explore different features 
such as nouns, verbs, hypernym, hyponym, 
etc.  Experiments show that category trees 
expanded with snippets together with noun 
features under tf scheme achieves a best F-
score, 0.86, when only 37.46% of utter-
ances are processed on the average.  This 
methodology is employed to recommend 
advertisements relevant to the dialogue. 
1 Introduction 
Instant messaging applications such as Google 
Talk, Microsoft MSN Messenger, Yahoo Messen-
ger, QQ, and Skype are very popular.  In the 
blooming instant messaging markets, sponsor links 
and advertisements support the free service.  Fig-
ure 1 shows an example of sponsor links in instant 
message applications.  They are usually randomly 
proposed and may be irrelevant to the utterance.  
Thus, they may not attract users? attentions and 
have no effects on advertisements.  This paper 
deals with the analysis of intention in the dialogues 
and the recommendation of relevant sponsor links 
in an ongoing conversation. 
In the related works, Fain and Pedersen (2006) 
survey sponsored search, suggesting the impor-
tance of matching advertising content to user inten-
tions.  How to match advertiser content to user 
queries is an important issue.  Yih et al (2006) 
aimed at extracting advertisement keywords from 
the intention on the web pages.  However, these 
works did not address the issues in dialogues. 
 
Figure 1.  A Sponsor Link in an IM Application 
In conventional dialogue management, how to 
extract semantic concepts, identify the speech act, 
and formulate the dialogue state transitions are im-
portant tasks.  The domain shift is a challenging 
problem (Lin and Chen, 2004).  In instant message 
applications, more challenging issues have to be 
tackled.  Firstly, the discussing topics of dialogues 
are diverse.  Secondly, the conversation may be 
quite short, so that the system should be responsive 
instantly when detecting the intention.  Thirdly, the 
utterance itself can be purely free-style and far be-
yond the formal grammar.  That is, self-defined or 
symbolic languages may be used in the dialogues.  
The following shows some example utterances. 
 James: dud, i c ur foto on Kelly?s door~  ^^|| 
 Antony: Orz?.kill me pls. >< 
An intention detecting system has to extract words 
from incomplete sentences in dialogues. Fourthly, 
the system should consider up-to-date terms, in-
stead of just looking up conventional dictionaries. 
625
Capturing the intention in a dialogue and rec-
ommending the advertisements before its ending 
are the goal of this approach.  This paper is organ-
ized as follows.  Section 2 shows an overview of 
the system architecture.  Section 3 discusses the 
category trees and the weighting functions for 
identifying the intention.  Section 4 presents the 
experimental results comparing with different uses 
of the category trees and word features.  Section 5 
concludes and remarks. 
2 System Overview 
Fain and Pedersen (2006) outlined six basic 
elements for sponsored search.  They are shown as 
follows: 
(1) advertiser-provided content, 
(2) advertiser-provided bids, 
(3) ensuring that advertiser content is relevant 
to the target keyword, 
(4) matching advertiser content to user queries, 
(5) displaying advertiser content in some rank 
order,  
(6) gathering data, metering clicks and charg-
ing advertisers. 
In instant messaging applications, a dialogue is 
composed of several utterances issuing by at least 
two users.  They are different from sponsored 
search in that advertiser content is matched to user 
utterances instead of user queries.  While reading 
users? conversation, an intention detecting system 
recommends suitable advertiser information at a 
suitable time.  The time of the recommendation 
and the effect of advertisement have a strong rela-
tionship.  The earlier the correct recommendation 
is, the larger the effect is. 
However, time and accuracy are trade-off.  At 
the earlier stages of a dialogue, the system may 
have deficient information to predict suitable ad-
vertisement.  Thus, a false advertisement may be 
proposed.  On the other hand, the system may have 
enough information at the later stages.  However, 
users may complete their talk at any time in this 
case, so the advertisement effect may be lowered.   
Figure 2 shows architecture of our system.  In 
each round of the conversation, we retrieve an ut-
terance from a given instant message application.  
Then, we parse the utterance and try to predict in-
tention of the dialogue based on current and previ-
ous utterances, and consult the advertisement data-
bases that provide sponsor links accordingly.  If 
the information in the utterances is enough for pre-
diction, then several candidates are proposed.  Fi-
nally, based on predefined criteria, the best candi-
date is selected and proposed to the IM application 
as the sponsor link in Figure 1. 
In the following sections, we will explore when 
to make sure the intention of a dialogue with con-
fidence and to propose suitable recommendations.  
In addition, we will also discuss what word fea-
tures (called cue words hereafter) in the utterances 
are useful for the intention determination.  We as-
sume sponsor links or advertisements are adjunct 
on the given category trees.  
 
Figure 2.  System Architecture 
3 Categorization of Dialogues 
3.1 Web Directory Used for Categorization 
We employ Yahoo! directory1 to assign a dialogue 
or part of a dialogue in category representing its 
intention.  Every word in dialogues is classified by 
the directory.  For example, by searching the term 
BMW, we could retrieve the category path: 
  >Business and Economy>? Makers>Vehicles 
Each category contains subcategories, which in-
clude some subsidiary categories. Therefore, we 
could take the directory as a hierarchical tree for 
searching the intention.  Moreover, each node of 
the tree has attributes from the node itself and its 
ancestors.  Our idea is to summarize all intentions 
from words in a dialog, and then conclude the in-
tention accordingly. 
The nodes sometimes are overlapped, that is, 
one node could be found in more than one path. 
For example, the car maker BMW has at least two 
other nodes: 
                                                 
1 http://dir.yahoo.com 
626
  >Regional>Countries>Germany>Business and 
     Economy>?>Dealers 
  >Recreation>Automotive>?Clubs and Organi- 
    zations>BMW Car Club of America 
The categories of BMW include Business and 
Economy, Regional, and Recreation.  This demon-
strates the nature of the word ambiguity, and is 
challenging when the system identifies the inten-
tion embedded in the dialogs. 
The downloaded Yahoo! directory brings up 
HTML documents with three basic elements, in-
cluding titles, links and snippet as shown in Figure 
3.  The following takes the three elements from a 
popular site as an example. 
Title: The White House  
Link: www.WhiteHouse.gov  
Snippet: Features statements and press releases 
by President George W. Bush as well? 
 
Figure 3. Sample HTML in Yahoo! Directory Tree 
We will explore different ways to use the three 
elements during intention identification. Table 1 
shows different models and total nodes.  YahooO 
and YahooX are two extreme cases.  The former 
employs the original category tree, while the latter 
expands the category tree with titles, links and 
snippets.  Thus, the former contains 7,839 nodes 
and the latter 78,519 nodes. 
 
Table 1.  Tree Expansion Scenarios 
 
Table 2.  Examples of Expanded Nodes 
Table 2 lists some examples to demonstrate the 
category tree expansion.  Some words inside the 
three elements rarely appear in dictionaries or en-
cyclopedias. Thus, we can summarize these trees 
and build a new dictionary with definitions.  For 
example, we could find the hottest web sites You-
Tube and MySpace, and even the most popular 
Chinese gamble game, Mahjong. 
3.2 Scoring Functions for Categorization  
Given a fragment F of a dialogue, which is com-
posed of utterances reading up to now, Formula 1 
determines the intention IINT of F by counting total 
scores of cue words w in F contributing to I.   
?
?
?=
Fw
INT IwbwtfI ),()(maxarg  (1)
where tf(w) is term frequency of w in F, and b(w,I) 
is 1 when w is in the paths corresponding to the 
intention IINT; b(w,I) is 0 otherwise. 
Formula 2 considers the discriminating capabil-
ity of each cue word.  It is similar to tf-idf scheme 
in information retrieval. 
?
?
??=
FwI
INT Iwbwdf
N
wtfI ),(
)(
log)(maxarg
 
where N is total number of intention
(2)
s, and df(w) is 
 
marized in Table 3 with 
explanation and examples. 
total intentions in which w appears. 
3.3 Features of Cue Words 
The features of possible cue words including nouns,
verbs, stop-words, word length, hypernym, hypo-
nym, and synonym are sum
627
 
Table 3.  Cue Words Explored 
Nouns and verbs form skeletons of concepts are 
important cues for similarity measures (Chen et al, 
2003), so that they are considered as features in our 
model.  Word length is used to filter out some un-
necessary words because the shorter the word is, 
the less meaningful the word might be.  Here we 
postulate that instant messaging users are not will-
ing to type long terms if unnecessary. 
In this paper, we regard words in an utterance of 
dialogues as query terms.  Rosie et al (2006) 
showed that query substitution may be helpful to 
retrieve more meaningful results.  Here, we use 
hypernym, hyponym and synonym specified in 
WordNet (Fellbaum, 1998) to expand the original 
utterance.  
3.4 Candidate Recommendation 
The proposed model also provides the ability to 
show the related advertisements after intention is 
confirmed.  As discussed, for each of node in the 
category tree, there is an accompanying HTML file 
to show some related web sites and even sponsors. 
Therefore, we can also use the category tree to put 
sponsor links into the HTML files, and just fetch 
the sponsor links from the HTML file on the node 
to the customers. 
The algorithm to select the suitable candidates 
could be shortly described as the Longest Path 
First.  Once we select the category of the intention, 
the nodes appearing in the chosen category will 
then be collected into a set. We will check the 
longest path and provide the sponsor links from the 
node. 
4 Experimental Results 
4.1 Performance of Different Models 
To prepare the experimental materials, we col-
lected 50 real dialogs from end-users, and asked 
annotators to tag the 50 dialogs with 14 given Ya-
hoo! directory categories shown in Table 4.  Aver-
age number of sentences is 12.38 and average 
number of words is 56.04 in each dialog.  We 
compare the system output with the answer keys, 
and compute precision, recall, and F-score for each 
method. 
 
Table 4.  Category Abbreviation 
Table 5 shows the performance of using For-
mula 1 (i.e., tf scheme).  This model is a combina-
tion of a scenario shown in Table 1 and features 
shown in Table 3.  For example, the YahooS-noun 
matches cue words of POS noun from utterances to 
the category tree expanded with snippets.  WL de-
notes word length.  Only cue words of length ? 
WL is considered.  C denotes the number of dia-
logues correctly analyzed.  NA denotes the number 
of undecidable dialogues.  P, R and F denote preci-
sion, recall and F-score. 
Table 5 shows that YahooS with noun features 
achieves a best performance.  Noun feature works 
impressively well with the orders, YahooS, Ya-
hooT, YahooX, and YahooL.  That meets our ex-
pectation because the information from snippets is 
well enough and does not bring in noise as the Ya-
hooX.  YahooT, however, has good but insufficient 
information, while YahooL is only suitable for dia-
logs directly related to links.  
Moreover, the experimental results show that 
verb is not a good feature no matter whether the 
category tree is expanded or not.  Although some 
verbs can explicitly point out the intention of dia-
logues, such as buy, sell, purchase, etc, the lack of 
verbs in Yahoo! directory makes the verb features 
less useful in the experiments.  Table 6 shows the 
performance of using Formula 2 (i.e., tf-idf 
scheme).  The original category tree with hyponym 
achieves the best performance, i.e., 56.56%.  How-
ever, it cannot compete with most of models with tf 
scheme. 
628
 
Table 5.  Performance of Models with tf Scheme 
 
Table 6. Performance of Models with tf-idf Scheme 
4.2 Hit Speed 
Besides precision, recall and F-score, we are al-
so interested if the system captures the intention of 
the dialogue at better timing.  We define one more 
metric called hit speed in Formula (3).  It repre-
sents how fast the sponsor links could be correctly 
suggested during the progress of conversations.  
For each utterance in a dialogue, we mark either X 
or a predicted category.  Here X denotes undecid-
able. 
Assume we have a dialogue of 7 utterances and 
consider the following scenario.  At first, our sys-
tem could not propose any candidates in the first 
two utterances.  Then, it decides the third and the 
fourth utterances are talking about Business and 
Economy.  Finally, it determines the intention of 
the dialogue is Computer and Internet after reading 
the next three utterances.  In this example, we get 
an answer string, XXBBCCC, based on the nota-
tions shown in Table 4.  If the intention annotated 
by human is Computer and Internet, then the sys-
tem starts proposing a correct intention from the 5th 
utterance.  In other words, the information in the 
first 4 utterances is not sufficient to make any deci-
sion or make wrong decision.   
Let CPL be the length of correct postfix of an 
answer string, e.g., 3, and N be total utterances in a 
dialogue, e.g., 7.  HitSpeed is defined as follows. 
N
CPL
HitSpeed =  (3)
In this case, the hit speed of intention identification 
is 3/7.  Intuitively, our goal is to get the hit speed 
as high as possible.  The sooner we get the correct 
intention, the better the recommendation effect is. 
The average hit speed is defined by Formulas (4) 
and (5).  The former considers only the correct dia-
logues, and the latter considers all the dialogues.  
Let M and N denote total dialogues and total cor-
rect dialogues, respectively. 
N
HitSpeed
vgHitSpeed
M
i i? == 1A  (4)
M
HitSpeed
vgHitSpeed
M
i i? == 1A  (5)
 
Figure 4.  Average Hit Speed by Formula (4) 
 
Figure 5.  Average Hit Speed by Formula (5) 
629
Figures 4 and 5 demonstrate average hit speeds 
computed by Formulas (4) and (5), respectively.   
Here four leading models shown in Table 5 are 
adopted and nouns are regarded as cue words. Fig-
ure 4 shows that the average hit speed in correctly 
answered dialogues is around 70%.  It means these 
models can correctly answer the intention when a 
dialogue still has 70% to go in the set of correctly 
answered dialogs. 
Figure 5 considers all the dialogues no matter 
whether their intentions are identified correctly or 
not.  We can still capture the intention with the hit 
speed 62.54% for the best model, i.e., YahooS-
noun.   
5 Concluding Remarks 
This paper captures intention in dialogues of in-
stant messaging applications.  A web directory 
such as Yahoo! directory is considered as a cate-
gory tree.  Two schemes, revised tf and tf-idf, are 
employed to classify the utterances in dialogues.  
The experiments show that the tf scheme using the 
category tree expanded with snippets together with 
noun features achieves the best F-score, 0.86.  The 
hit speed evaluation tells us the system can start 
making good decision when near only 37.46% of 
total utterances are processed.  In other words, the 
recommended advertisements can be placed to at-
tract users? attentions in the rest 62.54% of total 
utterances. 
Though the best model in the experiments is to 
use nouns as features, we note that another impor-
tant language feature, verbs, is not helpful due to 
the characteristic of the category tree we adopted, 
that is, the absence of verbs in Yahoo! directory. If 
some other data sources can provide the cue infor-
mation, verbs may be taken as useful features to 
boost the performance.  
In this paper, only one intention is assigned to 
the utterances.  However, there may be many par-
ticipants involving in a conversation, and the topics 
they are talking about in a dialogue may be more 
than one. For example, two couples are discussing 
a trip schedule together.  After the topic is finished, 
they may continue the conversation for selection of 
hotels and buying funds separately in the same in-
stant messaging dialogue.  In this case, our system 
only decides the intention is Recreation, but not 
including Business & Economy.  
Long time delay of response is another interest-
ing topic for instant messaging dialogues. Some-
times one participant could send a message, but 
have to wait for minutes or even hours to get re-
sponse.  Because the receiver might be absent, 
busy or just off-line, the system should be capable 
of waiting such a long time delay before a com-
plete dialogue is finished in practical applications. 
Opinion mining is also important to the pro-
posed model.  For example, dialogue participants 
may talk about buying digital cameras, and one of 
them has negative opinions on some products.  In 
such a case, an intelligent recommendation system 
should not promote such products.  Once opinion 
extraction is introduced to intention analysis sys-
tems, customers can get not only the conversation-
related, but also personally preferred sponsor links.  
Acknowledgments 
Research of this paper was partially supported by 
Excellent Research Projects of National Taiwan 
University, under the contract 95R0062-AE00-02. 
References 
H.H. Chen, J.J. Kuo, S.J. Huang, C.J. Lin and H.C. 
Wung. 2003. A Summarization System for Chinese 
News from Multiple Sources. Journal of American 
Society for Information Science and Technology, 
54(13), pp. 1224-1236. 
D. C. Fain and J. O. Pedersen. 2006. Sponsored Search: 
A Brief History. Bulletin of the American Society 
for Information Science and Technology, January. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Database. The MIT Press. 
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. 
Generating Query Substitutions. In Proceedings of 
the 15th International Conference on World Wide 
Web, 2006, pp. 387-396. 
K.K. Lin and H.H. Chen. 2004. Extracting Domain 
Knowledge for Dialogue Model Adaptation. In 
Proceedings of 5th International Conference on In-
telligent Text Processing and Computational Lin-
guistics, Lecture Notes in Computer Science, 
LNCS 2945, Springer-Verlag, pp. 70-78. 
W. Yih, J. Goodman, and V. R. Carvalho. 2006. Finding 
Advertising Keywords on Web Pages. In Proceed-
ings of the 15th International Conference on World 
Wide Web, pp. 213-222.  
630
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1009?1016,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Novel Association Measures Using Web Search with Double Checking 
 
 
Hsin-Hsi Chen Ming-Shun Lin Yu-Chuan Wei 
Department of Computer Science and Information Engineering  
National Taiwan University  
Taipei, Taiwan  
hhchen@csie.ntu.edu.tw;{mslin,ycwei}@nlg.csie.ntu.edu.tw
 
  
 
Abstract 
A web search with double checking 
model is proposed to explore the web as 
a live corpus.  Five association measures 
including variants of Dice, Overlap Ratio, 
Jaccard, and Cosine, as well as Co-
Occurrence Double Check (CODC), are 
presented. In the experiments on Ruben-
stein-Goodenough?s benchmark data set, 
the CODC measure achieves correlation 
coefficient 0.8492, which competes with 
the performance (0.8914) of the model 
using WordNet.  The experiments on link 
detection of named entities using the 
strategies of direct association, associa-
tion matrix and scalar association matrix 
verify that the double-check frequencies 
are reliable.  Further study on named en-
tity clustering shows that the five meas-
ures are quite useful.  In particular, 
CODC measure is very stable on word-
word and name-name experiments.  The 
application of CODC measure to expand 
community chains for personal name dis-
ambiguation achieves 9.65% and 14.22% 
increase compared to the system without 
community expansion.  All the experi-
ments illustrate that the novel model of 
web search with double checking is fea-
sible for mining associations from the 
web. 
1 Introduction 
In statistical natural language processing, re-
sources used to compute the statistics are indis-
pensable.  Different kinds of corpora have made 
available and many language models have been 
experimented.  One major issue behind the cor-
pus-based approaches is: if corpora adopted can 
reflect the up-to-date usage.  As we know, lan-
guages are live.  New terms and phrases are used 
in daily life.  How to capture the new usages is 
an important research topic. 
The Web is a heterogeneous document collec-
tion.  Huge-scale and dynamic nature are charac-
teristics of the Web.  Regarding the Web as a 
live corpus becomes an active research topic re-
cently.  How to utilize the huge volume of web 
data to measure association of information is an 
important issue.  Resnik and Smith (2003) em-
ploy the Web as parallel corpora to provide bi-
lingual sentences for translation models.  Keller 
and Lapata (2003) show that bigram statistics for 
English language is correlated between corpus 
and web counts.  Besides, how to get the word 
counts and the word association counts from the 
web pages without scanning over the whole col-
lections is indispensable. Directly managing the 
web pages is not an easy task when the Web 
grows very fast. 
Search engine provides some way to return 
useful information.  Page counts for a query de-
note how many web pages containing a specific 
word or a word pair roughly.  Page count is dif-
ferent from word frequency, which denotes how 
many occurrences a word appear.  Lin and Chen 
(2004) explore the use of the page counts pro-
vided by different search engines to compute the 
statistics for Chinese segmentation.  In addition 
to the page counts, snippets returned by web 
search, are another web data for training.  A 
snippet consists of a title, a short summary of a 
web page and a hyperlink to the web page.  Be-
cause of the cost to retrieve the full web pages, 
short summaries are always adopted (Lin, Chen, 
and Chen, 2005). 
Various measures have been proposed to 
compute the association of objects of different 
granularity like terms and documents.  Rodr?guez 
and Egenhofer (2003) compute the semantic 
1009
similarity from WordNet and SDTS ontology by 
word matching, feature matching and semantic 
neighborhood matching.  Li et al (2003) investi-
gate how information sources could be used ef-
fectively, and propose a new similarity measure 
combining the shortest path length, depth and 
local density using WordNet.  Matsuo et al 
(2004) exploit the Jaccard coefficient to build 
?Web of Trust? on an academic community. 
This paper measures the association of terms 
using snippets returned by web search.  A web 
search with double checking model is proposed 
to get the statistics for various association meas-
ures in Section 2.  Common words and personal 
names are used for the experiments in Sections 3 
and 4, respectively.  Section 5 demonstrates how 
to derive communities from the Web using asso-
ciation measures, and employ them to disam-
biguate personal names.  Finally, Section 6 con-
cludes the remarks. 
2 A Web Search with Double Checking 
Model 
Instead of simple web page counts and complex 
web page collection, we propose a novel model, 
a Web Search with Double Checking (WSDC), to 
analyze snippets.  In WSDC model, two objects X 
and Y are postulated to have an association if we 
can find Y from X (a forward process) and find X 
from Y (a backward process) by web search.  The 
forward process counts the total occurrences of Y 
in the top N snippets of query X, denoted as 
f(Y@X).  Similarly, the backward process counts 
the total occurrences of X in the top N snippets of 
query Y, denoted as f(X@Y).  The forward and 
the backward processes form a double check op-
eration. 
Under WSDC model, the association scores 
between X and Y are defined by various formulas 
as follows. 
???
???
?
+
+
=
=
=
Otherwise
YfXf
YXfXYf
YXf
orXYfif
YXeVariantDic
)()(
)@()@(
0)@(    
   0)@( 
0
    
),(
   (1) 
)()(
))@(),@((
,(
YfXf
YXfXYfmin
Y)XineVariantCos ?=
     (2) 
))@(),@(()()(
))@(),@((
    
YXfXYfmaxYfXf
YXfXYfmin
(X,Y)cardVariantJac
?+=
       (3) 
{ }
)}(),({
)@(),@(
 ),(
YfXfmin
YXfXYfmin
YXrlapVariantOve = (4) 
???
???
?
=
=
=
???
?
???
? ? Otherwise
YXf
orXYfif
YXCODC
Yf
YXf
Xf
XYflog
e
?
)(
)@(
)(
)@(
0)@(    
   0)@( 
0
),(
   
(5) 
Where f(X) is the total occurrences of X in the 
top N snippets of query X, and, similarly, f(Y) is 
the total occurrences of Y in the top N snippets of 
query Y.  Formulas (1)-(4) are variants of the 
Dice, Cosine, Jaccard, and Overlap Ratio asso-
ciation measure.  Formula (5) is a function 
CODC (Co-Occurrence Double-Check), which 
measures the association in an interval [0,1].  In 
the extreme cases, when f(Y@X)=0 or f(X@Y)=0, 
CODC(X,Y)=0; and when f(Y@X)=f(X) and 
f(X@Y)=f(Y), CODC(X,Y)=1.  In the first case, X 
and Y are of no association.  In the second case, 
X and Y are of the strongest association. 
3 Association of Common Words 
We employ Rubenstein-Goodenough?s (1965) 
benchmark data set to compare the performance 
of various association measures.  The data set 
consists of 65 word pairs.  The similarities be-
tween words, called Rubenstein and Goodenough 
rating (RG rating), were rated on a scale of 0.0 to 
4.0 for ?semantically unrelated? to ?highly syn-
onymous? by 51 human subjects.  The Pearson 
product-moment correlation coefficient, rxy, be-
tween the RG ratings X and the association 
scores Y computed by a model shown as follows 
measures the performance of the model. 
yx
i
n
i
i
xy ssn
yyxx
r
)1(
))((
1
?
??
=
?
=                             (6) 
Where x  and y  are the sample means of xi and 
yi, and sx and sy are sample standard deviations of 
xi and yi and n is total samples. 
Most approaches (Resink, 1995; Lin, 1998; Li 
et al, 2003) used 28 word pairs only.  Resnik 
(1995) obtained information content from 
WordNet and achieved correlation coefficient 
0.745.  Lin (1998) proposed an information-
theoretic similarity measure and achieved a cor-
relation coefficient of 0.8224.  Li et al (2003) 
combined semantic density, path length and 
depth effect from WordNet and achieved the cor-
relation coefficient 0.8914. 
1010
 100 200 300 400 500 600 700 800 900 
VariantDice 0.5332 0.5169 0.5352 0.5406 0.5306 0.5347 0.5286 0.5421 0.5250
VariantOverlap 0.5517 0.6516 0.6973 0.7173 0.6923 0.7259 0.7473 0.7556 0.7459
VariantJaccard 0.5533 0.6409 0.6993 0.7229 0.6989 0.738 0.7613 0.7599 0.7486
VariantCosine 0.5552 0.6459 0.7063 0.7279 0.6987 0.7398 0.7624 0.7594 0.7501
CODC (?=0.15) 0.5629 0.6951 0.8051 0.8473 0.8438 0.8492 0.8222 0.8291 0.8182
Jaccard Coeff* 0.5847 0.5933 0.6099 0.5807 0.5463 0.5202 0.4855 0.4549 0.4622
Table 1. Correlation Coefficients of WSDC Model on Word-Word Experiments 
Model RG Rating 
Resnik 
(1995) 
Lin 
(1998) 
Li et al
(2003) 
VariantCosine 
(#snippets=700) 
WSDC 
CODC(?=0.15, 
#snippets=600)
WSDC 
Correlation Coefficient - 0.7450 0.8224 0.8914 0.7624 0.8492 
chord-smile 0.02 1.1762 0.20 0 0 0 
rooster-voyage 0.04 0 0 0 0 0 
noon-string 0.04 0 0 0 0 0 
glass-magician 0.44 1.0105 0.06 0 0 0 
monk-slave 0.57 2.9683 0.18 0.350 0 0 
coast-forest 0.85 0 0.16 0.170 0.0019 0.1686 
monk-oracle 0.91 2.9683 0.14 0.168 0 0 
lad-wizard 0.99 2.9683 0.20 0.355 0 0 
forest-graveyard 1 0 0 0.132 0 0 
food-rooster 1.09 1.0105 0.04 0 0 0 
coast-hill 1.26 6.2344 0.58 0.366 0 0 
car-journey 1.55 0 0 0 0.0014 0.2049 
crane-implement 2.37 2.9683 0.39 0.366 0 0 
brother-lad 2.41 2.9355 0.20 0.355 0.0027 0.1811 
bird-crane 2.63 9.3139 0.67 0.472 0 0 
bird-cock 2.63 9.3139 0.83 0.779 0.0058 0.2295 
food-fruit 2.69 5.0076 0.24 0.170 0.0025 0.2355 
brother-monk 2.74 2.9683 0.16 0.779 0.0027 0.1956 
asylum-madhouse 3.04 15.666 0.97 0.779 0.0015 0.1845 
furnace-stove 3.11 1.7135 0.18 0.585 0.0035 0.1982 
magician-wizard 3.21 13.666 1 0.999 0.0031 0.2076 
journey-voyage 3.58 6.0787 0.89 0.779 0.0086 0.2666 
coast-shore 3.6 10.808 0.93 0.779 0.0139 0.2923 
implement-tool 3.66 6.0787 0.80 0.778 0.0033 0.2506 
boy-lad 3.82 8.424 0.85 0.778 0.0101 0.2828 
Automobile-car 3.92 8.0411 1 1 0.0144 0.4229 
Midday-noon 3.94 12.393 1 1 0.0097 0.2994 
gem-jewel 3.94 14.929 1 1 0.0107 0.3530 
Table 2. Comparisons of WSDC with Models in Previous Researches 
In our experiments on the benchmark data set, 
we used information from the Web rather than 
WordNet.  Table 1 summarizes the correlation 
coefficients between the RG rating and the asso-
ciation scores computed by our WSDC model. 
We consider the number of snippets from 100 to 
900.  The results show that CODC > VariantCo-
sine > VariantJaccard > VariantOverlap > Vari-
antDice.  CODC measure achieves the best per-
formance 0.8492 when ?=0.15 and total snippets 
to be analyzed are 600.  Matsuo et al (2004) 
used Jaccard coefficient to calculate similarity 
between personal names using the Web. The co-
efficient is defined as follows.   
1011
)(
)(
),( 
YXf
YXf
YXCoffJaccard ?
?=                                  (7) 
Where f(X?Y) is the number of pages including 
X?s and Y?s homepages when query ?X and Y? is 
submitted to a search engine; f(X?Y) is the num-
ber of pages including X?s or Y?s homepages 
when query ?X or Y? is submitted to a search en-
gine.  We revised this formula as follows and 
evaluated it with Rubenstein-Goodenough?s 
benchmark. 
)(
)(
),( *
YXf
YXf
YXCoffJaccard
s
s
?
?=                             (8) 
Where fs(X?Y) is the number of snippets in 
which X and Y co-occur in the top N snippets of 
query ?X and Y?; fs(X?Y) is the number of snip-
pets containing X or Y in the top N snippets of 
query ?X or Y?.  We test the formula on the same 
benchmark.  The last row of Table 1 shows that 
Jaccard Coeff* is worse than other models when 
the number of snippets is larger than 100.  
Table 2 lists the results of previous researches 
(Resink, 1995; Lin, 1998; Li et al, 2003) and our 
WSDC models using VariantCosine and CODC 
measures.  The 28 word pairs used in the ex-
periments are shown.  CODC measure can com-
pete with Li et al (2003).  The word pair ?car-
journey? whose similarity value is 0 in the papers 
(Resink, 1995; Lin, 1998; Li et al, 2003) is cap-
tured by our model.  In contrast, our model can-
not deal with the two word pairs ?crane-
implement? and ?bird-crane?.  
4 Association of Named Entities 
Although the correlation coefficient of WSDC 
model built on the web is a little worse than that 
of the model built on WordNet, the Web pro-
vides live vocabulary, in particular, named enti-
ties.  We will demonstrate how to extend our 
WSDC method to mine the association of per-
sonal names.  That will be difficult to resolve 
with previous approaches.  We design two ex-
periments ? say, link detection test and named 
entity clustering, to evaluate the association of 
named entities. 
Given a named-entity set L, we define a link 
detection test to check if any two named entities 
NEi and NEj (i?j) in L have a relationship R using 
the following three strategies. 
? Direct Association: If the double check 
frequency of NEi and NEj is larger than 0,  
 
Figure 1. Three Strategies for Link Detection 
i.e., f(NEj@NEi)>0 and f(NEi@NEj)>0, 
then the link detection test says ?yes?, i.e., 
NEi and NEj have direct association.  Oth-
erwise, the test says ?no?.  Figure 1(a) 
shows the direct association. 
? Association Matrix: Compose an n?n bi-
nary matrix M=(mij), where mij=1 if 
f(NEj@NEi)>0 and f(NEi@NEj)>0; mij=0 
if f(NEj@NEi)=0 or f(NEi@NEj)=0; and n 
is total number of named entities in L.  Let 
Mt be a transpose matrix of M.  The matrix 
A=M?Mt is an association matrix.  Here 
the element aij in A means that total aij 
common named entities are associated 
with both NEi and NEj directly.  Figure 1(b) 
shows a one-layer indirect association.  
Here, aij=3.  We can define NEi and NEj 
have an indirect association if aij is larger 
than a threshold ?.  That is, NEi and NEj 
should associate with at least ? common 
named entities directly.  The strategy of 
association matrix specifies: if aij??, then 
the link detection test says ?yes?, other-
wise it says ?no?.  In the example shown 
in Figure 1(b), NEi and NEj are indirectly 
associated when 0<??3. 
? Scalar Association Matrix: Compose a 
binary association matrix B from the asso-
ciation matrix A as: bij=1 if aij>0 and bij=0 
if aij=0.  The matrix S= B?Bt is a scalar as-
1012
sociation matrix. NEi and NEj may indi-
rectly associate with a common named en-
tity NEk.  Figure 1(c) shows a two-layer 
indirect association.  The ? = ?= nk kjikij bbs 1  
denotes how many such an NEk there are. 
In the example of Figure 1(c), two named 
entities indirectly associate NEi and NEj at 
the same time.  We can define NEi and NEj 
have an indirect association if sij is larger 
than a threshold ?.  In other words, if sij >?, 
then the link detection test says ?yes?, 
otherwise it says ?no?. 
To evaluate the performance of the above 
three strategies, we prepare a test set extracted 
from domz web site (http://dmoz.org), the most 
comprehensive human-edited directory of the 
Web.  The test data consists of three communi-
ties: actor, tennis player, and golfer, shown in 
Table 3.  Total 220 named entities are considered.  
The golden standard of link detection test is: we 
compose 24,090 (=220?219/2) named entity 
pairs, and assign ?yes? to those pairs belonging 
to the same community.  
Category Path in domz.org # of Person Names 
Top: Sports: Golf: Golfers  10 
Top: Sports: Tennis: Players:  
Female (+Male)  90 
Top: Arts: People: Image Galleries: 
Female (+Male): Individual 120 
Table 3. Test Set for Association Evaluation of 
Named Entities 
When collecting the related values for com-
puting the double check frequencies for any 
named entity pair (NEi and NEj), i.e., f(NEj@NEi), 
f(NEi@NEj), f(NEi), and f(NEj), we consider 
naming styles of persons.  For example, ?Alba, 
Jessica? have four possible writing: ?Alba, Jes-
sica?, ?Jessica Alba?, ?J. Alba? and ?Alba, J.?  
We will get top N snippets for each naming style, 
and filter out duplicate snippets as well as snip-
pets of ULRs including dmoz.org and 
google.com.  Table 4 lists the experimental re-
sults of link detection on the test set.  The preci-
sions of two baselines are: guessing all ?yes? 
(46.45%) and guessing all ?no? (53.55%).  All 
the three strategies are better than the two base-
lines and the performance becomes better when 
the numbers of snippets increase.  The strategy 
of direct association shows that using double 
checks to measure the association of named enti-
ties also gets good effects as the association of 
common words.  For the strategy of association 
matrix, the best performance 90.14% occurs in 
the case of 900 snippets and ?=6.  When larger 
number of snippets is used, a larger threshold is 
necessary to achieve a better performance.  Fig-
ure 2(a) illustrates the relationship between pre-
cision and threshold (?).  The performance de-
creases when ?>6.  The performance of the strat-
egy of scalar association matrix is better than that 
of the strategy of association matrix in some ? 
and ?.  Figure 2(b) shows the relationship be-
tween precision and threshold ? for some number 
of snippets and ?. 
In link detection test, we only consider the bi-
nary operation of double checks, i.e., f(NEj@NEi) 
> 0 and f(NEi@NEj) > 0, rather than utilizing the 
magnitudes of f(NEj@NEi) and f(NEi@NEj).  
Next we employ the five formulas proposed in 
Section 2 to cluster named entities.  The same 
data set as link detection test is adopted. An ag-
glomerative average-link clustering algorithm is 
used to partition the given 220 named entities 
based on Formulas (1)-(5).  Four-fold cross-
validation is employed and B-CUBED metric 
(Bagga and Baldwin, 1998) is adopted to evalu-
ate the clustering results.  Table 5 summarizes 
the experimental results.  CODC (Formula 5), 
which behaves the best in computing association 
of common words, still achieves the better per-
formance on different numbers of snippets in 
named entity clustering.  The F-scores of the 
other formulas are larger than 95% when more 
snippets are considered to compute the double 
check frequencies. 
 
Strategies 100 200 300 400 500 600 700 800 900 
Direct  
Association 59.20% 62.86% 65.72% 67.88% 69.83% 71.35% 72.05% 72.46% 72.55%
Association 
Matrix 
71.53% 
(?=1) 
79.95% 
(?=1) 
84.00%
(?=2) 
86.08%
(?=3) 
88.13%
(?=4) 
89.67%
(?=5) 
89.98% 
(?=5) 
90.09% 
(?=6) 
90.14%
(?=6) 
Scalar Asso-
ciation 
Matrix 
73.93% 
(?=1, 
?=6) 
82.69% 
(?=2, 
?=9) 
86.70%
(?=4, 
?=9) 
88.61%
(?=5, 
?=10) 
90.90%
(?=6, 
?=12) 
91.93%
(?=7, 
?=12) 
91.90% 
(?=7, 
?=18) 
92.20% 
(?=10, 
?=16) 
92.35%
(?=10,
?=18) 
Table 4. Performance of Link Detection of Named Entities 
1013
 
                                   (a)                                                                            (b)                              
Figure 2. (a) Performance of association matrix strategy.  (b) Performance of scalar association matrix 
strategy (where ? is fixed and its values reference to scalar association matrix in Table 4) 
  100 200 300 400 500 600 700 800 900 
P 91.70% 88.71% 87.02% 87.49% 96.90% 100.00% 100.00% 100.00% 100.00%
R 55.80% 81.10% 87.70% 93.00% 89.67% 93.61% 94.42% 94.88% 94.88%VariantDice 
F 69.38% 84.73% 87.35% 90.16% 93.14% 96.69% 97.12% 97.37% 97.37%
P 99.13% 87.04% 85.35% 85.17% 88.16% 88.16% 88.16% 97.59% 98.33%
R 52.16% 81.10% 86.24% 93.45% 92.03% 93.64% 92.82% 90.82% 93.27%VariantOverlap  
F 68.35% 83.96% 85.79% 89.11% 90.05% 90.81% 90.43% 94.08% 95.73%
P 99.13% 97.59% 98.33% 95.42% 97.59% 88.16% 95.42% 100.00% 100.00%
R 55.80% 77.53% 84.91% 88.67% 87.18% 90.58% 88.67% 93.27% 91.64%VariantJaccard 
F 71.40% 86.41% 91.12% 91.92% 92.09% 89.35% 91.92% 96.51% 95.63%
P 84.62% 97.59% 85.35% 85.17% 88.16% 88.16% 88.16% 98.33% 98.33%
R 56.22% 78.92% 86.48% 93.45% 92.03% 93.64% 93.64% 93.27% 93.27%VariantCosine 
F 67.55% 87.26% 85.91% 89.11% 90.05% 90.81% 90.81% 95.73% 95.73%
P 91.70% 87.04% 87.02% 95.93% 98.33% 95.93% 95.93% 94.25% 94.25%
R 55.80% 81.10% 90.73% 94.91% 94.91% 96.52% 98.24% 98.24% 98.24%CODC 
(?=0.15) 
F 69.38% 83.96% 88.83% 95.41% 96.58% 96.22% 97.07% 96.20% 96.20%
Table 5. Performance of Various Scoring Formulas on Named Entity Clustering 
5 Disambiguation Using Association of 
Named Entities 
This section demonstrates how to employ asso-
ciation mined from the Web to resolve the ambi-
guities of named entities.  Assume there are n 
named entities, NE1, NE2, ?, and NEn, to be dis-
ambiguated.  A named entity NEj has m accom-
panying names, called cue names later, CNj1, 
CNj2, ?, CNjm.  We have two alternatives to use 
the cue names.  One is using them directly, i.e., 
NEj is represented as a community of cue names 
Community(NEj)={CNj1, CNj2, ?, CNjm}.  The 
other is to expand the cue names CNj1, CNj2, ?, 
CNjm for NEj using the web data as follows.  Let 
CNj1 be an initial seed.  Figure 3 sketches the 
concept of community expansion. 
(1) Collection: We submit a seed to 
Google, and select the top N returned 
snippets.  Then, we use suffix trees to 
extract possible patterns (Lin and Chen, 
2006). 
(2) Validation: We calculate CODC score 
of each extracted pattern (denoted Bi) 
with the seed A.  If CODC(A,Bi) is 
strong enough, i.e., larger than a 
1014
threshold ?, we employ Bi as a new 
seed and repeat steps (1) and (2).  This 
procedure stops either expected number 
of nodes is collected or maximum 
number of layers is reached. 
(3) Union: The community initiated by the 
seed CNji is denoted by Commu-
nity(CNji)={Bji1, Bji2, ?, BBjir}, where Bjik 
is a new seed.  The Cscore score, com-
munity score, of BjikB  is the CODC score 
of Bjik with its parent divided by the 
layer it is located.  We repeat Collec-
tion and Validation steps until all the 
cue names CNji (1?i?m) of NEj are 
processed.  Finally, we have 
)()( 1 ji
m
ij CNCommunityNECommunity =?=  
 
Figure 3. A Community for a Seed ????? 
(?Chien-Ming Wang?) 
In a cascaded personal name disambiguation 
system (Wei, 2006), association of named enti-
ties is used with other cues such as titles, com-
mon terms, and so on.  Assume k clusters, c1 c2 ... 
ck, have been formed using title cue, and we try 
to place NE1, NE2, ?, and NEl into a suitable 
cluster.  The cluster c  is selected by the similar-
ity measure defined below. 
)()(
1
    
),(
1 ii
s
i
qj
pnscoreCpncount
r
cNEscore
?= ?=             (9) 
),(maxarg
)kq1(c q
qj cNEscorec ??
=                     (10) 
Where pn1, pn2, ?, pns are names which appear 
in both Community(NEj) and Community(cq); 
count(pni) is total occurrences of pni in Commu-
nity(cq); r is total occurrences of names in Com-
munity(NEj); Cscore(pni) is community score of 
pni.  
If score(NEj, c ) is larger than a threshold, 
then NEj is placed into cluster c .  In other words, 
NEj denotes the same person as those in c .  We 
let the new Community( c ) be the old Commu-
nity( c )?{CNj1, CNj2, ?, CNjm}.  Otherwise, NEj 
is left undecided. 
To evaluate the personal name disambiguation, 
we prepare three corpora for an ambiguous name 
???? ? (Chien-Ming Wang) from United 
Daily News Knowledge Base (UDN), Google 
Taiwan (TW), and Google China (CN).  Table 6 
summarizes the statistics of the test data sets.  In 
UDN news data set, 37 different persons are 
mentioned.  Of these, 13 different persons occur 
more than once.  The most famous person is a 
pitcher of New York Yankees, which occupies 
94.29% of 2,205 documents.  In TW and CN 
web data sets, there are 24 and 107 different per-
sons.  The majority in TW data set is still the 
New York Yankees?s ?Chien-Ming Wang?.  He 
appears in 331 web pages, and occupies 88.03%.  
Comparatively, the majority in CN data set is a 
research fellow of Chinese Academy of Social 
Sciences, and he only occupies 18.29% of 421 
web pages.  Total 36 different ?Chien-Ming 
Wang?s occur more than once.  Thus, CN is an 
unbiased corpus. 
 UDN TW CN 
# of documents 2,205 376 421 
# of persons 37 24 107 
# of persons of 
occurrences>1 13 9 36 
Majority  94.29% 88.03% 18.29%
Table 6. Statistics of Test Corpora 
 M1 M2 
P 0.9742 0.9674 (?0.70%) 
R 0.9800 0.9677 (?1.26%) UDN
F 0.9771 0.9675 (?0.98%) 
P 0.8760 0.8786 (?0.07%) 
R 0.6207 0.7287 (?17.40%) TW
F 0.7266 0.7967 (?9.65%) 
P 0.4910 0.5982 (?21.83%) 
R 0.8049 0.8378 (?4.09%) CN
F 0.6111 0.6980 (?14.22%) 
Table 7. Disambiguation without/with Commu-
nity Expansion 
1015
Table 7 shows the performance of a personal 
name disambiguation system without (M1)/with 
(M2) community expansion.  In the news data set 
(i.e., UDN), M1 is a little better than M2.  Com-
pared to M1, M2 decreases 0.98% of F-score.  In 
contrast, in the two web data sets (i.e., TW and 
CN), M2 is much better than M1.  M2 has 9.65% 
and 14.22% increases compared to M1.  It shows 
that mining association of named entities from 
the Web is very useful to disambiguate ambigu-
ous names.  The application also confirms the 
effectiveness of the proposed association meas-
ures indirectly. 
6 Concluding Remarks 
This paper introduces five novel association 
measures based on web search with double 
checking (WSDC) model.  In the experiments on 
association of common words, Co-Occurrence 
Double Check (CODC) measure competes with 
the model trained from WordNet.  In the experi-
ments on the association of named entities, 
which is hard to deal with using WordNet, 
WSDC model demonstrates its usefulness.  The 
strategies of direct association, association ma-
trix, and scalar association matrix detect the link 
between two named entities.  The experiments 
verify that the double-check frequencies are reli-
able.   
Further study on named entity clustering 
shows that the five measures ? say, VariantDice, 
VariantOverlap, ariantJaccard, VariantCosine 
and CODC, are quite useful.  In particular, 
CODC is very stable on word-word and name-
name experiments.  Finally, WSDC model is 
used to expand community chains for a specific 
personal name, and CODC measures the associa-
tion of community member and the personal 
name.  The application on personal name disam-
biguation shows that 9.65% and 14.22% increase 
compared to the system without community ex-
pansion. 
Acknowledgements 
Research of this paper was partially supported by 
National Science Council, Taiwan, under the 
contracts 94-2752-E-001-001-PAE and 95-2752-
E-001-001-PAE. 
References 
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space 
Model.  Proceedings of 36th COLING-ACL Con-
ference, 79-85. 
F. Keller and M. Lapata. 2003. Using the Web to Ob-
tain Frequencies for Unseen Bigrams. Computa-
tional Linguistics, 29(3): 459-484. 
Y. Li, Z.A. Bandar and D. McLean. 2003. An Ap-
proach for Measuring Semantic Similarity between 
Words Using Multiple Information Sources.  IEEE 
Transactions on Knowledge and Data Engineering, 
15(4): 871-882. 
D. Lin. 1998. An Information-Theoretic Definition of 
Similarity. Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, 296-304. 
H.C. Lin and H.H. Chen. 2004. Comparing Corpus-
based Statistics and Web-based Statistics: Chinese 
Segmentation as an Example. Proceedings of 16th 
ROCLING Conference, 89-100. 
M.S. Lin, C.P. Chen and H.H. Chen. 2005. An Ap-
proach of Using the Web as a Live Corpus for 
Spoken Transliteration Name Access. Proceedings 
of 17th ROCLING Conference, 361-370. 
M.S. Lin and H.H. Chen. 2006. Constructing a 
Named Entity Ontology from Web Corpora. Pro-
ceedings of 5th International Conference on Lan-
guage Resources and Evaluation. 
Y. Matsuo, H. Tomobe, K. Hasida, and M. Ishizuka. 
2004. Finding Social Network for Trust Calcula-
tion. Proceedings of 16th European Conference on 
Artificial Intelligence, 510-514. 
P. Resnik. 1995. Using Information Content to Evalu-
ate Semantic Similarity in a Taxonomy. Proceed-
ings of the 14th International Joint Conference on 
Artificial Intelligence, 448-453. 
P. Resnik and N.A. Smith. 2003. The Web as a Paral-
lel Corpus. Computational Linguistics, 29(3): 349-
380. 
M.A. Rodr?guez and M.J. Egenhofer. 2003. Determin-
ing Semantic Similarity among Entity Classes from 
Different Ontologies.  IEEE Transactions on 
Knowledge and Data Engineering, 15(2): 442-456. 
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual Correlates of Synonymy. Communications of 
the ACM, 8(10): 627-633. 
Y.C. Wei. 2006. A Study of Personal Name Disam-
biguation. Master Thesis, Department of Computer 
Science and Information Engineering, National 
Taiwan University, Taiwan. 
 
 
 
 
1016
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 81?88,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A High-Accurate Chinese-English NE Backward Translation System 
Combining Both Lexical Information and Web Statistics 
 
 
Conrad Chen Hsin-Hsi Chen 
Department of Computer Science and Information Engineering, National 
Taiwan University, Taipei, Taiwan 
drchen@nlg.csie.ntu.edu.tw hhchen@csie.ntu.edu.tw 
 
  
 
Abstract 
Named entity translation is indispensable 
in cross language information retrieval 
nowadays. We propose an approach of 
combining lexical information, web sta-
tistics, and inverse search based on 
Google to backward translate a Chinese 
named entity (NE) into English. Our sys-
tem achieves a high Top-1 accuracy of 
87.6%, which is a relatively good per-
formance reported in this area until pre-
sent. 
1 Introduction 
Translation of named entities (NE) attracts much 
attention due to its practical applications in 
World Wide Web. The most challenging issue 
behind is: the genres of NEs are various, NEs are 
open vocabulary and their translations are very 
flexible. 
Some previous approaches use phonetic simi-
larity to identify corresponding transliterations, 
i.e., translation by phonetic values (Lin and Chen, 
2002; Lee and Chang, 2003). Some approaches 
combine lexical (phonetic and meaning) and se-
mantic information to find corresponding transla-
tion of NEs in bilingual corpora (Feng et al, 
2004; Huang et al, 2004; Lam et al, 2004). 
These studies focus on the alignment of NEs in 
parallel or comparable corpora.  That is called 
?close-ended? NE translation. 
In ?open-ended? NE translation, an arbitrary 
NE is given, and we want to find its correspond-
ing translations. Most previous approaches ex-
ploit web search engine to help find translating 
candidates on the Internet. Al-Onaizan and 
Knight (2003) adopt language models to generate 
possible candidates first, and then verify these 
candidates by web statistics. They achieve a Top-
1 accuracy of about 72.6% with Arabic-to-
English translation. Lu et al (2004) use statistics 
of anchor texts in web search result to identify 
translation and obtain a Top-1 accuracy of about 
63.6% in translating English out-of-vocabulary 
(OOV) words into Traditional Chinese. Zhang et 
al. (2005) use query expansion to retrieve candi-
dates and then use lexical information, frequen-
cies, and distances to find the correct translation. 
They achieve a Top-1 accuracy of 81.0% and 
claim that they outperform state-of-the-art OOV 
translation techniques then. 
In this paper, we propose a three-step ap-
proach based on Google to deal with open-ended 
Chinese-to-English translation. Our system inte-
grates various features which have been used by 
previous approaches in a novel way.  We observe 
that most foreign Chinese NEs would have their 
corresponding English translations appearing in 
their returned snippets by Google. Therefore we 
combine lexical information and web statistics to 
find corresponding translations of given Chinese 
foreign NEs in returned snippets. A highly effec-
tive verification process, inverse search, is then 
adopted and raises the performance in a signifi-
cant degree. Our approach achieves an overall 
Top-1 accuracy of 87.6% and a relatively high 
Top-4 accurracy of 94.7%.   
2 Background 
Translating NEs, which is different from translat-
ing common words, is an ?asymmetric? transla-
tion. Translations of an NE in various languages 
can be organized as a tree according to the rela-
tions of translation language pairs, as shown in 
Figure 1. The root of the translating tree is the 
NE in its original language, i.e., initially de-
81
nominated. We call the translation of an NE 
along the tree downward as a ?forward transla-
tion?. On the contrary, ?backward translation? is 
to translate an NE along the tree upward. 
 
Figure 1. Translating tree of ?Cien a?os soledad?. 
Generally speaking, forward translation is eas-
ier than backward translation. On the one hand, 
there is no unique answer to forward translation. 
Many alternative ways can be adopted to forward 
translate an NE from one language to another. 
For example, ?Jordan? can be translated into ??
?  (Qiao-Dan)?, ???  (Qiao-Deng)?, ??? 
(Yue-Dan)?, and so on. On the other hand, there 
is generally one unique corresponding term in 
backward translation, especially when the target 
language is the root of the translating tree.  
In addition, when the original NE appears in 
documents in the target language in forward 
translation, it often comes together with a corre-
sponding translation in the target language 
(Cheng et al, 2004). That makes forward transla-
tion less challenging. In this paper, we focus our 
study on Chinese-English backward translation, 
i.e., the original language of NE and the target 
language in translation is English, and the source 
language to be translated is Chinese.  
There are two important issues shown below 
to deal with backward translation of NEs or 
OOV words.  
? Where to find the corresponding translation? 
? How to identify the correct translation?  
NEs seldom appear in multi-lingual or even 
mono-lingual dictionaries, i.e., they are OOV or 
unknown words. For unknown words, where can 
we find its corresponding translation? A bilin-
gual corpus might be a possible solution. How-
ever, NEs appear in a vast context and bilingual 
corpora available can only cover a small propor-
tion. Most text resources are monolingual. Can 
we find translations of NEs in monolingual cor-
pora? While mentioning a translated name during 
writing, sometimes we would annotate it with its 
original name in the original foreign language, 
especially when the name is less commonly 
known. But how often would it happen? With 
our testing data, which would be introduced in 
Section 4, over 97% of translated NEs would 
have its original NE appearing in the first 100 
returned snippets by Google. Figure 2 shows 
several snippets returned by Google which con-
tains the original NE of the given foreign NE.  
Figure 2. Several Traditional Chinese snippets of 
?????? returned by Google which contains 
the translation ?The Old Man and the Sea?. 
When translations can be found in snippets, 
the next work would be identifying which name 
is the correct translation of NEs. First we should 
know how NEs would be translated. The com-
monest case is translating by phonetic values, or 
so-called transliteration. Most personal names 
and location names are transliterated. NEs may 
also be translated by meaning. It is the way in 
which most titles and nicknames and some or-
ganization names would be translated. Another 
common case is translating by phonetic values 
for some parts and by meaning for the others. For 
example, ?Sears Tower? is translated into ???
? (Xi-Er-Si) ? ? (tower)? in Chinese. NEs 
would sometimes be translated by semantics or 
contents of the entity it indicates, especially with 
movies. Table 1 summarizes the possible trans-
lating ways of NEs. From the above discussion, 
we may use similarities in phonetic values, 
meanings of constituent words, semantics, and so 
CEPS ???-- ????;-1  
??, ???????????????????. ???
?, Symbolic Means of the Author "The Old Man and the 
Sea" ... ??, ??????????????????? 
??????????????????????????
???????????? ... 
www.ceps.com.tw/ec/ecjnlarticleView.aspx?jnlcattype=1& 
jnlptype=4&jnltype=29&jnliid=1370&i... - 26k - ???? - ?
??? 
 
.:JSDVD Mall:. ????-????  
????-???? ? ?????-????(DTS) ? ????
??? ? ?????? 16-?? ? ?? ? ????? ? ???
?-????? ? ????-??? ... ????-????. The 
Old Man and The Sea. 4715320115018, ????????
? ... 
mall.jsdvd.com/product_info.php?products_id=3198 - 48k - ?
??? - ???? - ????  
82
on to identify corresponding translations. Besides 
these linguistic features, non-linguistic features 
such as statistical information may also help use 
well. We would discuss how to combine these 
features to identify corresponding translation in 
detail in the next section.  
3 Chinese-to-English NE Translation 
As we have mentioned in the last section, we 
could find most English translations in Chinese 
web page snippets. We thus base our system on 
web search engine: retrieving candidates from 
returned snippets, combining both linguistic and 
statistical information to find the correct transla-
tion. Our system can be split into three steps: 
candidate retrieving, candidate evaluating, and 
candidate verifying. An overview of our system 
is given in Figure 3.  
 
Figure 3. An Overview of the System. 
In the first step, the NE to be translated, GN, 
is sent to Google to retrieve traditional Chinese 
web pages, and a simple English NE recognition 
method and several preprocessing procedures 
are applied to obtain possible candidates from 
returned snippets. In the second step, four fea-
tures (i.e., phonetic values, word senses, recur-
rences, and relative positions) are exploited to 
give these candidates a score. In the last step, the 
candidates with higher scores are sent to Google 
again. Recurrence information and relative posi-
tions concerning with the candidate to be veri-
fied of GN in returned snippets are counted 
along with the scores to decide the final ranking 
of candidates. These three steps will be detailed 
in the following subsections. 
3.1 Retrieving Candidates 
Before we can identify possible candidates, we 
must retrieve them first. In the returned tradi-
tional Chinese snippets by Google, there are still 
many English fragments. Therefore, the first 
task our system would do is to separate these 
English fragments into NEs and non-NEs. We 
propose a simple method to recognize possible 
NEs. All fragments conforming to the following 
properties would be recognized as NEs: 
? The first and the last word of the fragment 
are numerals or capitalized. 
? There are no three or more consequent low-
ercase words in the fragment. 
? The whole fragment is within one sentence. 
After retrieving possible NEs in returned snip-
pets, there are still some works to do to make a 
Translating Way Description Examples 
Translating by Pho-
netic Values 
The translation would have a similar 
pronunciation to its original NE. 
 ?New York? and ???(pronounced as Niu-
Yue)? 
Translating by Mean-
ing 
The translation would have a similar or a 
related meaning to its original NE. 
?? (red)? (chamber)? (dream)? and ?The 
Dream of the Red Chamber? 
Translating by Pho-
netic Values for Some 
Parts and by Meaning 
for the Others 
The entire NE is supposed to be trans-
lated by its meaning and the name parts 
are transliterated. 
 ?Uncle Tom?s Cabin? and ???(pronounced 
as Tang-Mu)???(uncle?s)??(cabin)? 
Translating by Both 
Phonetic Values and 
Meaning 
The translation would have both a similar 
pronunciation and a similar meaning to 
its original NE. 
?New Yorker? and ???(pronounced as Niu-
Yue)?(people, pronounced as Ke)? 
Translating NEs by 
Heterography 
The NE is translated by these hetero-
graphic words in neighboring languages. 
???? and ?Yokohama?, ?????? and 
?Ichiro Suzuki? 
Translating by Se-
mantic or Content 
The NE is translated by its semantic or 
the content of the entity it refers to. 
?The Mask? and ??? (modern)? (great)?
(saint)? 
Parallel Names NE is initially denominated as more than 
one name or in more than one language. 
????(Sun Zhong-Shan)? and ?Sun Yat-Sen? 
Table 1. Possible translating ways of NEs. 
83
finer candidate list for verification. First, there 
might be many different forms for a same NE. 
For example, ?Mr. & Mrs. Smith? may also ap-
pear in the form of ?Mr. and Mrs. Smith?, ?Mr. 
And Mrs. Smith?, and so on. To deal with these 
aliasing forms, we transform all different forms 
into a standard form for the later ranking and 
identification. The standard form follows the 
following rules: 
? All letters are transformed into upper cases. 
? Words consist ???s are split. 
? Symbols are rewritten into words. 
For example, all forms of ?Mr. & Mrs. Smith? 
would be transformed into ?MR. AND MRS. 
SMITH?. 
The second work we should complete before 
ranking is filtering useless substrings. An NE 
may comprise many single words. These com-
ponent words may all be capitalized and thus all 
substrings of this NE would be fetched as candi-
dates of our translation work. Therefore, sub-
strings which always appear with a same preced-
ing and following word are discarded here, since 
they would have a zero recurrence score in the 
next step, which would be detailed in the next 
subsection. 
3.2 Evaluating Candidates 
After candidate retrieving, we would obtain a 
sequence of m candidates, C1, C2, ?, Cm. An 
integrated evaluating model is introduced to ex-
ploit four features (phonetic values, word senses, 
recurrences, and relative positions) to score 
these m candidates, as the following equation 
suggests: 
),(),(
),(
GNCLScoreGNCSScore
GNCScore
ii
i
?
=
 
LScore(Ci,GN) combines phonetic values and 
word senses to evaluate the lexical similarity 
between Ci and GN. SScore(Ci,GN) concerns 
both recurrences information and relative posi-
tions to evaluate the statistical relationship be-
tween Ci and GN. These two scores are then 
combined to obtain Score(Ci,GN). How to esti-
mate LScore(Cn, GN) and SScore(Cn, GN) would 
be discussed in detail in the following subsec-
tions. 
3.2.1 Lexical Similarity 
The lexical similarity concerns both phonetic 
values and word senses. An NE may consist of 
many single words. These component words 
may be translated either by phonetic values or 
by word senses. Given a translation pair, we 
could split them into fragments which could be 
bipartite matched according to their translation 
relationships, as Figure 4 shows.  
 
Figure 4. The translation relationships of ???
??????. 
To identify the lexical similarity between two 
NEs, we could estimate the similarity scores be-
tween the matched fragment pairs first, and then 
sum them up as a total score. We postulate that 
the matching with the highest score is the correct 
matching. Therefore the problem becomes a 
weighted bipartite matching problem, i.e., given 
the similarity scores between any fragment pairs, 
to find the bipartite matching with the highest 
score. In this way, our next problem is how to 
estimate the similarity scores between fragments.  
We treat an English single word as a fragment 
unit, i.e., each English single word corresponds 
to one fragment. An English candidate Ci con-
sisting of n single words would be split into n 
fragment units, Ci1, Ci2, ?, Cin. We define a Chi-
nese fragment unit that it could comprise one to 
four characters and may overlap each other. A 
fragment unit of GN can be written as GNab, 
which denotes the ath to bth characters of GN, 
and b - a < 4. The linguistic similarity score be-
tween two fragments is:  
)},(),,({
),(
ijabijab
ijab
CGNWSSimCGNPVSimMax
CGNLSim =
 
Where PVSim() estimates the similarity in pho-
netic values while WSSim() estimate it in word 
senses.  
 Phonetic Value 
In this paper, we adopt a simple but novel 
method to estimate the similarity in phonetic 
values. Unlike many approaches, we don?t in-
troduce an intermediate phonetic alphabet sys-
tem for comparison. We first transform the Chi-
nese fragments into possible English strings, and 
then estimate the similarity between transformed 
strings and English candidates in surface strings, 
as Figure 5 shows. However, similar pronuncia-
tions does not equal to similar surface strings. 
Two quite dissimilar strings may have very simi-
lar pronunciations. Therefore, we take this strat-
84
egy: generate all possible transformations, and 
regard the one with the highest similarity as the 
English candidate. 
 
Figure 5. Phonetic similarity estimation of our 
system. 
Edit distances are usually used to estimate the 
surface similarity between strings. However, the 
typical edit distance does not completely satisfy 
the requirement in the context of translation 
identification. In translation, vowels are an unre-
liable feature. There are many variations in pro-
nunciation of vowels, and the combinations of 
vowels are numerous. Different combinations of 
vowels may have a same phonetic value, how-
ever, same combinations may pronounce totally 
differently. The worst of all, human often arbi-
trarily determine the pronunciation of unfamiliar 
vowel combinations in translation. For these rea-
sons, we adopt the strategy that vowels can be 
ignored in transformation. That is to say when it 
is hard to determine which vowel combination 
should be generated from given Chinese frag-
ments, we can only transform the more certain 
part of consonants. Thus during the calculation 
of edit distances, the insertion of vowels would 
not be calculated into edit distances. Finally, the 
modified edit distance between two strings A 
and B is defined as follow: 
??
? =
=
??
?
=
??
??
?
??
??
?
+??
+?
+?
=
=
=
?
?
?
?
?
?
else
BAif
tsRep
consonantaisBif
vowlaisBif
tIns
tsReptsED
tsED
tInstsED
tsED
ssED
ttED
ts
t
t
BA
BA
BA
BA
BA
BA
,1
,0),(
,1
,0)(
),()1,1(
,1),1(
),()1,(
min),(
)0,(
),0(
 
The modified edit distances are then transformed 
to similarity scores: 
)}(),(max{
))(),((1),(
BLenALen
BLenALenEDBAPVSim BA??=  
Len() denotes the length of the string. In the 
above equation, the similarity scores are ranged 
from 0 to 1. 
We build the fixed transformation table manu-
ally. All possible transformations from Chinese 
transliterating characters to corresponding Eng-
lish strings are built. If we cannot precisely indi-
cate which vowel combination should be trans-
formed, or there are too many possible combina-
tions, we ignores vowels. Then we use a training 
set of 3,000 transliteration names to examine 
possible omissions due to human ignorance.  
 Word Senses 
More or less similar to the estimation of pho-
netic similarity, we do not use an intermediate 
representation of meanings to estimate word 
sense similarity. We treat the English transla-
tions in the C-E bilingual dictionary (reference 
removed for blind review) directly as the word 
senses of their corresponding Chinese word en-
tries. We adopt a simple 0-or-1 estimation of 
word sense similarity between two strings A and 
B, as the following equation suggests: 
???
???
?
=
dictionary in the
  ofon  translatia is  if ,1
dictionary in the    
  ofon  translatianot  is  if,0
),( AB
AB
BAWSSim  
All the Chinese foreign names appearing in test 
data is removed from the dictionary. 
From the above equations we could derive 
that LSim() of fragment pairs is also ranged from 
0 to 1. Candidates to be evaluated may comprise 
different number of component words, and this 
would result the different scoring base of the 
weighted bipartite matching. We should normal-
ize the result scores of bipartite matching. As a 
result, the following equation is applied: 
?
?
?
???
?
?
?
?
???
?
+??
=
?
?
GN
abCGNLSim
C
CGNLSim
GNCLScore
ijab
ijab
CGN ijab
i
CGN ijab
i
in  characters of # Total
 )1(),(
,
in   wordsof # Total
),(
min
),(
 and  pairs matched all
 and  pairs matched all  
3.2.2 Statistical Similarity 
Two pieces of information are concerned to-
gether to estimate the statistical similarity: recur-
rences and relative positions. A candidate Ci 
might appear l times in the returned snippets, as 
Ci,1, Ci,2, ?, Ci,l. For each Ci,k, we find the dis-
85
tance between it and the nearest GN in the re-
turned snippets, and then compute the relative 
position scores as the following equation: 
? ? 14/),(
1),(
,
,
+
=
ki
ki CGNDistance
GNCRP  
In other words, if the candidate is adjacent to the 
given NE, it would have a relative position score 
of 1. Relative position scores of all Ci,k would be 
summed up to obtain the primitive statistical 
score: 
PSS(Ci, GN) = ?k RP(Cn,k, GN) 
As we mentioned before, since the impreci-
sion of NE recognition, most substrings of NEs 
would also be recognized as candidates. This 
would result a problem. There are often typos in 
the information provided on the Internet. If some 
component word of an NE is misspelled, the 
substrings constituted by the rest words would 
have a higher statistical score than the correct 
NE. To prevent such kind of situations, we in-
troduce entropy of the context of the candidate. 
If a candidate has a more varied context, it is 
more possible to be an independent term instead 
of a substring of other terms. Entropy provides 
such a property: if the possible cases are more 
varied, there is higher entropy, and vice versa. 
Entropy function here concerns the possible 
cases of the most adjacent word at both ends of 
the candidate, as the following equation suggests: 
??
??
?
??
=
=
?
i iCT irNPTir
i
NCNCTNCNCT
CEntropy
else ,/log/
1context  possible of #  while,1
) ofContext (
 
Where NCTr and NCi denote the appearing times 
of the rth context CTr and the candidate Ci in the 
returned snippets respectively, and NPTi denotes 
the total number of different cases of the context 
of Ci. Since we want to normalize the entropy to 
0~1, we take NPTi as the base of the logarithm 
function. 
While concerning context combinations, only 
capitalized English word is discriminated. All 
other words would be viewed as one sort 
?OTHER?. For example, assuming the context 
of ?David? comprises three times of (Craig, 
OTHER), three times of (OTHER, Stern), and 
six times of (OTHER, OTHER), then: 
946.0)
12
6log
12
6
12
3log
12
3
12
3log
12
3(
)David"" ofContext (
333 =?+?+?
=Entropy
 
Next we use Entropy(Context of Ci) to weight 
the primitive score PSS(Ci, GN) to obtain the 
final statistical score.: 
)() ofContext (
)(
,GNCPSSCEntropy
,GNCSScore
ii
i
?
=
 
3.3 Verifying Candidates 
In evaluating candidate, we concern only the 
appearing frequencies of candidates when the 
NE to be translated is presented. In the other 
direction, we should also concern the appearing 
frequencies of the NE to be translated when the 
candidate is presented to prevent common words 
getting an improper high score in evaluation. We 
perform the inverse search approach for this 
sake. Like the evaluation of statistical scores in 
the last step, candidates are sent to Google to 
retrieve Traditional Chinese snippets, and the 
same equation of SScore() is computed concern-
ing the candidate. However, since there are too 
many candidates, we cannot perform this proc-
ess on all candidates. Therefore, an elimination 
mechanism is adopted to select candidates for 
verification. The elimination mechanism works 
as follows: 
1. Send the Top-3 candidates into Google for 
verification. 
2. Count SScore(GN, Ci). (Notice that the or-
der of the parameter is reversed.) Re-weight 
Score(Ci, GN) by multiplying SScore(GN, 
Ci) 
3. Re-rank candidates 
4. After re-ranking, if new candidates become 
the Top-3 ones, redo the first step. Other-
wise end this process. 
The candidates have been verified would be re-
corded to prevent duplicate re-weighting and 
unnecessary verification.  
There is one problem in verification we 
should concern. Since we only consider recur-
rence information in both directions, but not co-
occurrence information, this would result some 
problem when dealing rarely used translations. 
For example, ?Peter Pan? can be translated into 
????? or ????? (both pronounced as Bi-
De-Pan) in Chinese, but most people would use 
the former translation. Thus if we send ?Peter 
Pan? to verification when translating ?????, 
we would get a very low score.  
To deal with this situation, we adopt the strat-
egy of disbelieving verification in some situa-
86
tions. If all candidates have scores lower than 
the threshold, we presume that the given NE is a 
rarely used translation. In this situation, we use 
only Score(Cn, GN) estimated by  the evaluation 
step to rank its candidates, without multiplying 
SScore(GN, Ci) of the inverse search. The 
threshold is set to 1.5 by heuristic, since we con-
sider that a commonly used translation is sup-
posed to have their SScore() larger than 1 in both 
directions.   
4 Experiments 
To evaluate the performance of our system, 15 
common users are invited to provide 100 foreign 
NEs per user. These users are asked to simulate 
a scenario of using web search machine to per-
form cross-lingual information retrieval. The 
proportion of different types of NEs is roughly 
conformed to the real distribution, except for 
creation titles. We gathers a larger proportion of 
creation titles than other types of NEs, since the 
ways of translating creation titles is less regular 
and we may use them to test how much help 
could the web statistics provide. 
After removing duplicate entries provided by 
users, finally we obtain 1,119 nouns. Among 
them 7 are not NEs, 65 are originated from Ori-
ental languages (Chinese, Japanese, and Korean), 
and the rest 1,047 foreign NEs are our main ex-
perimental subjects. Among these 1,047 names 
there are 455 personal names, 264 location 
names, 117 organization names, 196 creation 
titles, and 15 other types of NEs.  
Table 2 and Figure 5 show the performance of 
the system with different types of NEs. We 
could observe that the translating performance is 
best with location names. It is within our expec-
tation, since location names are one of the most 
limited NE types. Human usually provide loca-
tion names in a very limited range, and thus 
there are less location names having ambiguous 
translations and less rare location names in the 
test data. Besides, because most location names 
are purely transliterated, it can give us some 
clues about the performance of our phonetic 
model.  
Our system performs worst with creation titles. 
One reason is that the naming and translating 
style of creation titles are less formulated. Many 
titles are not translated by lexical information, 
but by semantic information or else. For exam-
ple, ?Mr. & Mrs. Smith? is translated into ???
???(Smiths? Mission)? by the content of the 
creation it denotes. Another reason is that many 
titles are not originated from English, such as ?le 
Nozze di Figaro?. It results the C-E bilingual 
dictionary cannot be used in recognizing word 
sense similarity. A more serious problem with 
titles is that titles generally consist of more sin-
gle words than other types of NEs. Therefore, in 
the returned snippets by Google, the correct 
translation is often cut off. It would results a 
great bias in estimating statistical scores.  
Table 3 compares the result of different fea-
ture combinations. It considers only foreign NEs 
in the test data. From the result we could con-
clude that both statistical and lexical features are 
helpful for translation finding, while the inverse 
search are the key of our system to achieve a 
good performance. 
60%
65%
70%
75%
80%
85%
90%
95%
100%
1 5 9 13 17 21 25 29
Ranking
Re
ca
ll a
t T
OP
 N
PER
LOC
ORG
Title
Other
Oriental
Non-NE
 
Figure 5. Curve of recall versus ranking. 
Top-1 Top-2 Top-4 Top-M 
 Total Num Recall Num Recall Num Recall Num Recall 
PER 455 408 89.7% 430 94.5% 436 95.8% 443 97.3% 
LOC 264 242 91.7% 252 95.5% 253 95.8% 264 100.0% 
ORG 117 98 83.8% 106 90.6% 108 92.3% 114 97.4% 
TITLE 196 151 77.0% 168 85.7% 181 92.3% 189 96.4% 
Other 15 10 66.7% 13 86.7% 14 93.3% 15 100.0% 
All NE 1047 909 87.6% 969 92.6% 992 94.7% 1025 97.9% 
Oriental 65 47 72.3% 52 80.0% 55 84.6% 60 92.3% 
Non-NE 7 6 85.7% 6 85.7% 6 85.7% 7 100.0% 
Overall 1119 962 86.0% 1027 91.8% 1053 94.1% 1092 97.6% 
Table 2. Experiment results of our system with different NE types. 
 
87
Top-1 Top-2 Top-4 
 Num Recall Num Recall Num Recall 
SScore 540 51.6% 745 71.2% 887 84.7% 
LScore 721 68.9% 789 75.4% 844 80.6% 
SScore + LScore 837 79.9% 916 87.5% 953 91.0% 
+ Inverse Search 909 87.6% 969 92.6% 992 94.7% 
Table 3. Experiment results of our system with different feature combinations. 
 
From the result we could also find that our 
system has a high recall of 94.7% while consid-
ering top 4 candidates. If we only count in the 
given NEs with their correct translation appear-
ing in the returned snippets, the recall would go 
to 96.8%. This achievement may be not yet good 
enough for computer-driven applications, but it 
is certainly a good performance for user querying. 
5 Conclusion 
In this study we combine several relatively sim-
ple implementations of approaches that have 
been proposed in the previous studies and obtain 
a very good performance. We find that the Inter-
net is a quite good source for discovering NE 
translations. Using snippets returned by Google 
we can efficiently reduce the number of the pos-
sible candidates and acquire much useful infor-
mation to verify these candidates. Since the 
number of candidates is generally less than proc-
essing with unaligned corpus, simple models can 
performs filtering quite well and the over-fitting 
problem is thus prevented. 
From the failure cases of our system, (see Ap-
pendix A) we could observe that the performance 
of this integrated approach could still be boosted 
by more sophisticated models, more extensive 
dictionaries, and more delicate training mecha-
nisms. For example, performing stemming or 
adopting a more extensive dictionary might en-
hance the accuracy of estimating word sense 
similarity; the statistic formula can be replaced 
by more formal measures such as co-occurrences 
or mutual information to make a more precise 
assessment of statistical relationship. These tasks 
would be our future works in developing a more 
accurate and efficient NE translation system.  
Reference 
Al-Onaizan, Yaser and Kevin Knight. 2002. Translat-
ing Named Entities Using Monolingual and Bilin-
gual Resources. ACL 2002: 400-408. 
Cheng, Pu-Jen, J.W. Teng, R.C. Chen, J.H. Wang, 
W.H. Lu, and L.F. Chien. Translating unknown 
queries with web corpora for cross-language in-
formation retrieval. SIGIR 2004: 146-153. 
Feng, Donghui, Lv Y., and Zhou M. 2004. A New 
Approach for English-Chinese Named Entity 
Alignment. EMNLP 2004: 372-379. 
Huang, Fei, Stephan Vogel, and Alex Waibel. 2003. 
Improving Named Entity Translation Combining 
Phonetic and Semantic Similarities. HLT-NAACL 
2004: 281-288. 
Lam, Wai, Ruizhang Huang, and Pik-Shan Cheung. 
2004. Learning phonetic similarity for matching 
named entity translations and mining new transla-
tions. SIGIR 2004: 289-296. 
Lee, Chun-Jen and Jason S. Chang. 2003. Acquisition 
of. English-Chinese Transliterated Word Pairs 
from Parallel-Aligned Texts. HLT-NAACL 2003. 
Workshop on Data Driven MT: 96-103. 
Lin, Wei-Hao and Hsin-Hsi Chen. 2002. Backward 
Machine Transliteration by Learning Phonetic 
Similarity. Proceedings of CoNLL-2002: 139-145. 
Lu, Wen-Hsiang, Lee-Feng Chien, and Hsi-Jian Lee. 
2004. Anchor Text Mining for Translation of Web 
Queries: A Transitive Translation Approach. ACM 
Transactions on Information Systems 22(2): 242-
269. 
Zhang, Ying, Fei Huang, and Stephan Vogel. 2005. 
Mining translations of OOV terms from the web 
through cross-lingual query expansion. SIGIR 
2005: 669-670. 
Zhang, Ying and Phil Vines. 2004. Using the web for 
automated translation extraction in cross-language 
information retrieval. SIGIR 2004: 162-169.  
Appendix A. Some Failure Cases of Our 
System 
GN Top 1  Correct Translation Rank 
?? CBS SADDAM HUSSEIN 2 
??? JERSEY NEW JERSEY 2 
???? ONLINE ARABIAN NIGHTS 2 
???? ROYCE ROLLS ROYCE 2 
????? NBA JULIUS ERVING 2 
??? LAVIGNE AVRIL LAVIGNE 2 
?? JK JK. ROWLING 2 
???? RICKY DAVIS CELTICS 8 
???? MONET IMPRESSION SUNRISE 9 
?? TUPOLEV TU USSR 33 
????? NBA MEDVENDENKO N/A 
????? TOS SYMPHONY NO. 5 N/A 
???? AROUND03 CUORE N/A 
??? JACK LAYTON DEMOCRATIC PARTY N/A 
88
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 89?92,
Prague, June 2007. c?2007 Association for Computational Linguistics
Test Collection Selection and Gold Standard Generation  
for a Multiply-Annotated Opinion Corpus 
Lun-Wei Ku, Yong-Shen Lo and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
{lwku, yslo}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw 
Abstract 
Opinion analysis is an important research 
topic in recent years.  However, there are 
no common methods to create evaluation 
corpora.  This paper introduces a method 
for developing opinion corpora involving 
multiple annotators.  The characteristics of 
the created corpus are discussed, and the 
methodologies to select more consistent 
testing collections and their corresponding 
gold standards are proposed.    Under the 
gold standards, an opinion extraction sys-
tem is evaluated.  The experiment results 
show some interesting phenomena. 
1 Introduction 
Opinion information processing has been studied 
for several years.  Researchers extracted opinions 
from words, sentences, and documents, and both 
rule-based and statistical models are investigated  
(Wiebe et al, 2002; Pang et al, 2002).  The 
evaluation metrics precision, recall and f-measure 
are usually adopted.   
A reliable corpus is very important for the opin-
ion information processing because the annotations 
of opinions concern human perspectives.  Though 
the corpora created by researchers were analyzed 
(Wiebe et al, 2002), the methods to increase the 
reliability of them were seldom touched.  The strict 
and lenient metrics for opinions were mentioned, 
but not discussed in details together with the cor-
pora and their annotations. 
This paper discusses the selection of testing col-
lections and the generation of the corresponding 
gold standards under multiple annotations.  These 
testing collections are further used in an opinion 
extraction system and the system is evaluated with 
the corresponding gold standards.  The analysis of 
human annotations makes the improvements of 
opinion analysis systems feasible. 
2 Corpus Annotation 
Opinion corpora are constructed for the research of 
opinion tasks, such as opinion extraction, opinion 
polarity judgment, opinion holder extraction, 
opinion summarization, opinion question 
answering, etc..  The materials of our opinion 
corpus are news documents from NTCIR CIRB020 
and CIRB040 test collections.  A total of 32 topics 
concerning opinions are selected, and each 
document is annotated by three annotators.  
Because different people often feel differently 
about an opinion due to their own perspectives, 
multiple annotators are necessary to build a 
reliable corpus.  For each sentence, whether it is 
relevant to a given topic, whether it is an opinion, 
and if it is, its polarity, are assigned.   The holders 
of opinions are also annotated.  The details of this 
corpus are shown in Table 1. 
 
 Topics Documents Sentences
Quantity 32 843 11,907 
Table 1. Corpus size  
3 Analysis of Annotated Corpus  
As mentioned, each sentence in our opinion corpus 
is annotated by three annotators.  Although this is a 
must for building reliable annotations, the incon-
sistency is unavoidable.   In this section, all the 
possible combinations of annotations are listed and 
two methods are introduced to evaluate the quality 
of the human-tagged opinion corpora. 
3.1 Combinations of annotations 
Three major properties are annotated for sen-
tences in this corpus, i.e., the relevancy, the opin-
ionated issue, and the holder of the opinion.  The 
combinations of relevancy annotations are simple, 
and annotators usually have no argument over the 
opinion holders.  However, for the annotation of 
the opinionated issue, the situation is more com-
89
plex.  Annotations may have an argument about 
whether a sentence contains opinions, and their 
annotations may not be consistent on the polarities 
of an opinion.  Here we focus on the annotations of 
the opinionated issue.  Sentences may be consid-
ered as opinions only when more than two annota-
tors mark them opinionated.  Therefore, they are 
targets for analysis.   The possible combinations of 
opinionated sentences and their polarity are shown 
in Figure 1. 
 
A B 
  
C E 
  
D 
 
 
 
 
Positive/Neutral/Negative 
Figure 1. Possible combinations of annotations 
In Figure 1, Cases A, B, C are those sentences 
which are annotated as opinionated by all three 
annotators, while cases D, E are those sentences 
which are annotated as opinionated only by two 
annotators.  In case A and case D, the polarities 
annotated by annotators are identical.  In case B, 
the polarities annotated by two of three annotators 
are agreed.  However, in cases C and E, the polari-
ties annotated disagree with each other.  The statis-
tics of these five cases are shown in Table 2. 
 
Case A B C D E All
Number 1,660 1,076 124 2,413 1,826 7,099
Table 2. Statistics of cases A-E 
3.2 Inconsistency 
3 
P P P 
N N N 
X X X
3 
Multiple annotators bring the inconsistency.  There 
are several kinds of inconsistency in annotations, 
for example, relevant/non-relevant, opinion-
ated/non-opinionated, and the inconsistency of po-
larities.  The relevant/non-relevant inconsistency is 
more like an information retrieval issue.  For opin-
ions, because their strength varies, sometimes it is 
hard for annotators to tell if a sentence is opinion-
ated.  However, for the opinion polarities, the in-
consistency between positive and negative annota-
tions is obviously stronger than that between posi-
tive and neutral, or neutral and negative ones.  
Here we define a sentence ?strongly inconsistent? 
if both positive and negative polarities are assigned 
to a sentence by different annotators.  The strong 
inconsistency may occur in case B (171), C (124), 
and E (270).  In the corpus, only about 8% sen-
tences are strongly inconsistent, which shows the 
annotations are reliable. 
P P 
N N 
X X
2 3 
P X N 
P N X
N P X
N X P 
X P N 
X N P 
P N
3.3 Kappa value for agreement 
We further assess the usability of the annotated 
corpus by Kappa values.  Kappa value gives a 
quantitative measure of the magnitude of inter-
annotator agreement.  Table 3 shows a commonly 
used scale of the Kappa values. 
 
Kappa value Meaning 
<0 less than change agreement 
0.01-0.20 slight agreement 
0.21-0.40 fair agreement 
0.41-0.60 moderate agreement 
0.61-0.80 substantial agreement 
0.81-0.99 almost perfect agreement 
Table 3. Interpretation of Kappa value 
The inconsistency of annotations brings difficul-
ties in generating the gold standard.  Sentences 
should first be selected as the testing collection, 
N P
P X
N X
X P
NX
2
P P 
N N
X X
P NX 
90
and then the corresponding gold standard can be 
generated.  Our aim is to generate testing collec-
tions and their gold standards which agree mostly 
to annotators.  Therefore, we analyze the kappa 
value not between annotators, but between the an-
notator and the gold standard.  The methodologies 
are introduced in the next section. 
4 Testing Collections and Gold Standards 
The gold standard of relevance, the opinionated 
issue, and the opinion holder must be generated 
according to all the annotations.  Answers are cho-
sen based on the agreement of annotations.  Con-
sidering the agreement among annotations them-
selves, the strict and the lenient testing collections 
and their corresponding gold standard are gener-
ated.  Considering the Kappa values of each anno-
tator and the gold standard, topics with high agree-
ment are selected as the testing collection.  More-
over, considering the consistency of polarities, the 
substantial consistent testing collection is gener-
ated.  In summary, two metrics for generating gold 
standards and four testing collections are adopted. 
4.1 Strict and lenient 
Namely, the strict metric is different from the leni-
ent metric in the agreement of annotations.  For the 
strict metric, sentences with annotations agreed by 
all three annotators are selected as the testing col-
lection and the annotations are treated as the strict 
gold standard; for the lenient metric, sentences 
with annotations agreed by at least two annotators 
are selected as the testing collection and the major-
ity of annotations are treated as the lenient gold 
standard.  For example, for the experiments of ex-
tracting opinion sentences, sentences in cases A, B, 
and C in Figure 1 are selected in both strict and 
lenient testing collections, while sentences in cases 
D and E are selected only in the lenient testing col-
lection because three annotations are not totally 
agreed with one another.  For the experiments of 
opinion polarity judgment, sentences in case A in 
Figure 1 are selected in both strict and lenient test-
ing collections, while sentences in cases B, C, D 
and E are selected only in the lenient testing col-
lection.  Because every opinion sentence should be 
given a polarity, the polarities of sentences in cases 
B and D are the majority of annotations, while the 
polarity of sentences in cases C are given the po-
larity neutral in the lenient gold standard.  The po-
larities of sentences in case E are decided by rules 
P+X=P, N+X=N, and P+N=X.  As for opinion 
holders, holders are found in opinion sentences of 
each testing collection.  The strict and lenient met-
rics are also applied in annotations of relevance. 
4.2 High agreement 
To see how the generated gold standards agree 
with the annotations of all annotators, we analyze 
the kappa value from the agreements of each anno-
tator and the gold standard for all 32 topics.  Each 
topic has two groups of documents from NTCIR: 
very relevant and relevant to topic.  However, one 
topic has only the relevant type document, it re-
sults in a total of 63 (2*31+1) groups of documents.  
Note that the lenient metric is applied for generat-
ing the gold standard of this testing collection be-
cause the strict metric needs perfect agreement 
with each annotator?s annotations.  The distribu-
tion of kappa values of 63 groups is shown in Ta-
ble 4 and Table 5.  The cumulative frequency bar 
graphs of Table 4 and Table 5 are shown in Figure 
2 and Figure 3. 
 
Kappa <=00-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
Number 1 2 12 14 33 1 
Table 4. Kappa values for opinion extraction 
Kappa <=00-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
Number 9 0 7 21 17 9 
Table 5. Kappa values for polarity judgment 
Figure 2. Cumulative frequency of Table 4 
1 3
15
29
62 63
0
10
20
30
40
50
60
70
<=0 0-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
9 9
16
37
54
63
0
10
20
30
40
50
60
70
<=0 0-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
 
Figure 3. Cumulative frequency of Table 5 
According to Figure 2 and Figure 3, document 
groups with kappa values above 0.4 are selected as 
91
the high agreement testing collection, that is, 
document groups with moderate agreement in Ta-
ble 3.  A total of 48 document groups are collected 
for opinion extraction and 47 document groups are 
collected for opinion polarity judgment. 
4.3 Substantial Consistency 
In Section 3.2, sentences which are ?strongly in-
consistent? are defined.  The substantial consis-
tency test collection expels strongly inconsistent 
sentences to achieve a higher consistency.  Notice 
that this test collection is still less consistent than 
the strict test collection, which is perfectly consis-
tent with annotators.  The lenient metric is applied 
for generating the gold standard for this collection. 
5 An Opinion System -- CopeOpi 
A Chinese opinion extraction system for opinion-
ated information, CopeOpi, is introduced here. (Ku 
et al, 2007)  When judging the opinion polarity of 
a sentence in this system, three factors are consid-
ered: sentiment words, negation operators and 
opinion holders. Every sentiment word has its own 
sentiment score.  If a sentence consists of more 
positive sentiments than negative sentiments, it 
must reveal something good, and vice versa. How-
ever, a negation operator, such as ?not? 
and ?never?, may totally change the sentiment po-
larity of a sentiment word. Therefore, when a nega-
tion operator appears together with a sentiment 
word, the opinion score of the sentiment word S 
will be changed to -S to keep the strength but re-
verse the polarity. Opinion holders are also consid-
ered for opinion sentences, but how they influence 
opinions has not been investigated yet. As a result, 
they are weighted equally at first. A word is con-
sidered an opinion holder of an opinion sentence if 
either one of the following two criteria is met:  
1. The part of speech is a person name, organi-
zation name or personal. 
2. The word is in class A (human), type Ae (job) 
of the Cilin Dictionary (Mei et al, 1982). 
6 Evaluation Results and Discussions 
Experiment results of CopeOpi using four designed 
testing collections are shown in Table 6.  Under the 
lenient metric with the lenient test collection, f-
measure scores 0.761 and 0.383 are achieved by 
CopeOpi.  The strict metric is the most severe, and 
the performance drops a lot under it.  Moreover, 
when using high agreement (H-A) and substantial 
consistency (S-C) test collections, the performance 
of the system does not increase in portion to the 
increase of agreement.  According to the agree-
ment of annotators, people should perform best in 
the strict collection, and both high agreement and 
substantial consistency testing collections are eas-
ier than the lenient one.  This phenomenon shows 
that though this system?s performance is satisfac-
tory, its behavior is not like human beings.  For a 
computer system, the lenient testing collection is 
fuzzier and contains more information for judg-
ment.  However, this also shows that the system 
may only take advantage of the surface informa-
tion.  If we want our systems really judge like hu-
man beings, we should enhance the performance 
on strict, high agreement, and substantial consis-
tency testing collections.  This analysis gives us, or 
other researchers who use this corpus for experi-
ments, a direction to improve their own systems.  
 
 Opinion Extraction Opinion + Polarity 
Measure P R F P R F 
Lenient 0.664 0.890 0.761 0.335 0.448 0.383
Strict 0.258 0.921 0.404 0.104 0.662 0.180
H-A 0.677 0.885 0.767 0.339 0.455 0.388
S-C    0.308 0.452 0.367
Table 6. Evaluation results 
Acknowledgments  
Research of this paper was partially supported by Excel-
lent Research Projects of National Taiwan University, 
under the contract 95R0062-AE00-02.  
References 
Mei, J., Zhu, Y. Gao, Y. and Yin, H.. tong2yi4ci2ci2lin2. 
Shanghai Dictionary Press, 1982.  
Pang, B., Lee, L., and Vaithyanathan, S. (2002). 
Thumbs up? Sentiment classification using machine 
learning techniques. Proceedings of the 2002 Confer-
ence on EMNLP, pages 79-86.  
 Wiebe, J., Breck, E., Buckly, C., Cardie, C., Davis, P., 
Fraser, B., Litman, D., Pierce, D., Riloff, E., and 
Wilson, T. (2002). NRRC summer workshop on 
multi-perspective question answering, final report. 
ARDA NRRC Summer 2002 Workshop.  
Ku, L.-W., Wu, T.-H., Li, L.-Y. and Chen., H.-H. 
(2007). Using Polarity Scores of Words for Sentence-
level Opinion Extraction. Proceedings of the Sixth 
NTCIR Workshop. 
92
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 133?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
Building Emotion Lexicon from Weblog Corpora 
Changhua Yang        Kevin Hsin-Yih Lin        Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
#1 Roosevelt Rd. Sec. 4, Taipei, Taiwan 106 
{d91013, f93141, hhchen}@csie.ntu.edu.tw 
Abstract 
An emotion lexicon is an indispensable re-
source for emotion analysis.  This paper 
aims to mine the relationships between 
words and emotions using weblog corpora.  
A collocation model is proposed to learn 
emotion lexicons from weblog articles.  
Emotion classification at sentence level is 
experimented by using the mined lexicons 
to demonstrate their usefulness. 
1 Introduction 
Weblog (blog) is one of the most widely used cy-
bermedia in our internet lives that captures and 
shares moments of our day-to-day experiences, 
anytime and anywhere.  Blogs are web sites that 
timestamp posts from an individual or a group of 
people, called bloggers.  Bloggers may not follow 
formal writing styles to express emotional states.  
In some cases, they must post in pure text, so they 
add printable characters, such as ?:-)? (happy) and 
?:-(? (sad), to express their feelings.  In other cases, 
they type sentences with an internet messenger-
style interface, where they can attach a special set 
of graphic icons, or emoticons.  Different kinds of 
emoticons are introduced into text expressions to 
convey bloggers? emotions. 
Since thousands of blog articles are created eve-
ryday, emotional expressions can be collected to 
form a large-scale corpus which guides us to build 
vocabularies that are more emotionally expressive.  
Our approach can create an emotion lexicon free of 
laborious efforts of the experts who must be famil-
iar with both linguistic and psychological knowl-
edge. 
2 Related Works 
Some previous works considered emoticons from 
weblogs as categories for text classification.  
Mishne (2005), and Yang and Chen (2006) used 
emoticons as tags to train SVM (Cortes and Vap-
nik, 1995) classifiers at document or sentence level.  
In their studies, emoticons were taken as moods or 
emotion tags, and textual keywords were taken as 
features.  Wu et al (2006) proposed a sentence-
level emotion recognition method using dialogs as 
their corpus.  ?Happy, ?Unhappy?, or ?Neutral? 
was assigned to each sentence as its emotion cate-
gory.  Yang et al (2006) adopted Thayer?s model 
(1989) to classify music emotions.  Each music 
segment can be classified into four classes of 
moods.  In sentiment analysis research, Read (2005) 
used emoticons in newsgroup articles to extract 
instances relevant for training polarity classifiers. 
3 Training and Testing Blog Corpora 
We select Yahoo! Kimo Blog1 posts as our source 
of emotional expressions.  Yahoo! Kimo Blog 
service has 40 emoticons which are shown in Table 
1.  When an editing article, a blogger can insert an 
emoticon by either choosing it or typing in the 
corresponding codes.  However, not all articles 
contain emoticons.  That is, users can decide 
whether to insert emoticons into articles/sentences 
or not.  In this paper, we treat these icons as 
emotion categories and taggings on the 
corresponding text expressions. 
The dataset we adopt consists of 5,422,420 blog 
articles published at Yahoo! Kimo Blog from 
January to July, 2006, spanning a period of 212 
days.  In total, 336,161 bloggers? articles were col-
lected.  Each blogger posts 16 articles on average. 
We used the articles from January to June as the 
training set and the articles in July as the testing set.  
Table 2 shows the statistics of each set.  On aver-
age, 14.10% of the articles contain emotion-tagged 
expressions.  The average length of articles with 
tagged emotions, i.e., 272.58 characters, is shorter 
                                                 
1
 http://tw.blog.yahoo.com/ 
133
than that of articles without tagging, i.e., 465.37 
characters.  It seems that people tend to use emoti-
cons to replace certain amount of text expressions 
to make their articles more succinct. 
Figure 1 shows the three phases for the con-
struction and evaluation of emotion lexicons.  In 
phase 1, 1,185,131 sentences containing only one 
emoticon are extracted to form a training set to 
build emotion lexicons.  In phase 2, sentence-level 
emotion classifiers are constructed using the mined 
lexicons.  In phase 3, a testing set consisting of 
307,751 sentences is used to evaluate the classifi-
ers. 
4 Emotion Lexicon Construction 
The blog corpus contains a collection of bloggers? 
emotional expressions which can be analyzed to 
construct an emotion lexicon consisting of words 
that collocate with emoticons. We adopt a variation 
of pointwise mutual information (Manning and 
Sch?tze, 1999) to measure the collocation strength 
co(e,w) between an emotion e and a word w:  
)()(
),(log),(),o(
wPeP
weP
wecwec ?=  (1) 
where P(e,w)=c(e,w)/N, P(e)=c(e)/N, P(w)=c(w)/N, 
c(e)
 
and c(w) are the total occurrences of emoticon 
e and word w in a tagged corpus, respectively, 
c(e,w) is total co-occurrences of e and w, and N 
denotes the total word occurrences. 
A word entry of a lexicon may contain several 
emotion senses.  They are ordered by the colloca-
tion strength co.  Figure 2 shows two Chinese ex-
ample words, ??? ? (ha1ha1) and ??? ? 
(ke3wu4).  The former collocates with ?laughing? 
and ?big grin? emoticons with collocation strength 
25154.50 and 2667.11, respectively.  Similarly, the 
latter collocates with ?angry? and ?phbbbbt?.  
When all collocations (i.e., word-emotion pairs) 
are listed in a descending order of co, we can 
choose top n collocations to build an emotion lexi-
con.  In this paper, two lexicons (Lexicons A and B) 
are extracted by setting n to 25k and 50k.  Lexicon 
A contains 4,776 entries with 25,000 sense pairs 
and Lexicon B contains 11,243 entries and 50,000 
sense pairs. 
5 Emotion Classification 
Suppose a sentence S to be classified consists of n 
emotion words.  The emotion of S is derived by a 
mapping from a set of n emotion words to m emo-
tion categories as follows: 
 },...,{?},...,{ 11 m
tionclassifican
eeeewewS ???  
Table 1. Yahoo! Kimo Blog Emoticon Set. 
ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description 
1 
 
:) happy 11 
 
:O surprise 21 
 
0:) angel 31 
 
(:| yawn 
2 
 
:( sad 12 
 
X-( angry 22 
 
:-B nerd 32 
 
=P~ drooling 
3 
 
;) winking 13 
 
:> smug 23 
 
=; 
talk to  
the hand 33  :-? thinking 
4 
 
:D big grin 14 
 
B-) cool 24 
 
I-) asleep 34 
 
;)) hee hee 
5 
 
;;) batting  eyelashes 15  :-S worried 25  8-) rolling eyes 35  =D> applause 
6 
 
:-/ confused 16 
 
>:) devil 26 
 
:-& sick 36 
 
[-o< praying 
7 
 
:x love struck 17 
 
:(( crying 27 
 
:-$ don't tell  anyone 37  :-< sigh 
8 
 
:?> blushing 18 
 
:)) laughing 28 
 
[-( not talking 38 
 
>:P phbbbbt 
9 
 
:p tongue 19 
 
:| straight face 29 
 
:o) clown 39 
 
@};- rose 
10 
 
:* kiss 20 
 
/:) raised  eyebrow 30  @-) hypnotized 40  :@) pig 
 
Table 2. Statistics of the Weblog Dataset. 
Dataset Article # Tagged # Percentage Tagged Len. Untagged L. 
Training 4,187,737 575,009 13.86% 269.77 chrs. 468.14 chrs. 
Testing 1,234,683 182,999 14.92% 281.42 chrs. 455.82 chrs. 
Total 5,422,420 764,788 14.10% 272.58 chrs. 465.37 chrs. 
 
Testing Set 
Figure 1. Emotion Lexicon Construction and Evaluation. 
Extraction 
Blog 
Articles 
Features 
Classifiers 
Evaluation 
Lexicon 
 Construction 
Training Set 
Phase 2
Phase 3 
Emotion 
 Lexicon 
Phase 1 
134
For each emotion word ewi, we may find several 
emotion senses with the corresponding collocation 
strength co by looking up the lexicon.  Three alter-
natives are proposed as follows to label a sentence 
S with an emotion: 
(a) Method 1 
(1)  Consider all senses of ewi as votes.  Label S 
with the emotion that receives the most votes. 
(2)  If more than two emotions get the same num-
ber of votes, then label S with the emotion that 
has the maximum co. 
(b) Method 2 
    Collect emotion senses from all ewi.  Label S 
with the emotion that has the maximum co. 
(c) Method 3 
The same as Method 1 except that each ewi v-
otes only one sense that has the maximum co. 
In past research, the approach used by Yang et 
al. (2006) was based on the Thayer?s model (1989), 
which divided emotions into 4 categories.  In sen-
timent analysis research, such as Read?s study 
(2006), a polarity classifier separated instances into 
positive and negative classes.  In our experiments, 
we not only adopt fine-grain classification, but also 
coarse-grain classification.  We first select 40 
emoticons as a category set, and also adopt the 
Thayer?s model to divide the emoticons into 4 
quadrants of the emotion space.  As shown in Fig-
ure 3, the top-right side collects the emotions that 
are more positive and energetic and the bottom-left 
side is more negative and silent.  A polarity classi-
fier uses the right side as positive and the left side 
as negative. 
6 Evaluation 
Table 3 shows the performance under various 
combinations of lexicons, emotion categories and 
classification methods.  ?Hit #? stands for the 
number of correctly-answered instances. The base-
line represents the precision of predicting the ma-
jority category, such as ?happy? or ?positive?, as 
the answer.  The baseline method?s precision in-
creases as the number of emotion classes decreases.  
The upper bound recall indicates the upper limit on 
the fraction of the 307,751 instances solvable by 
the corresponding method and thus reflects the 
limitation of the method.  The closer a method?s 
actual recall is to the upper bound recall, the better 
the method.  For example, at most 40,855 instances 
(14.90%) can be answered using Method 1 in 
combination with Lexicon A.  But the actual recall 
is 4.55% only, meaning that Method 1?s recall is 
more than 10% behind its upper bound.  Methods 
which have a larger set of candidate answers have 
higher upper bound recalls, because the probability 
that the correct answer is in their set of candidate 
answers is greater. 
Experiment results show that all methods utiliz-
ing Lexicon A have performance figures lower 
than the baseline, so Lexicon A is not useful.  In 
contrast, Lexicon B, which provides a larger col-
lection of vocabularies and emotion senses, outper-
forms Lexicon A and the baseline.  Although 
Method 3 has the smallest candidate answer set 
and thus has the smallest upper bound recall, it 
outperforms the other two methods in most cases.  
Method 2 achieves better precisions when using 
?? (ha1ha1) ?hah hah?  
Sense 1. (laughing) ? co: 25154.50 
e.g., ??...  ???????~ 
        ?hah hah?  I am getting lucky~? 
Sense 2. (big grin) ? co: 2667.11 
e.g., ??????????~??  
        ?I only memorized vowels today~ haha ? 
?? (ke3wu4) ?darn? 
Sense 1. (angry) ? co: 2797.82 
e.g., ??????...??  
        ?What's the hacker doing... darn it ? 
Sense 2. (phbbbbt) ? co: 619.24 
e.g., ???????  
        ?Damn those aliens ? 
Figure 2. Some Example Words in a Lexicon. 
 
Arousal (energetic) 
 
 
               
                       
                                                                  Valence 
(negative)                                            (positive) 
 
        
       
 
(silent) 
unassigned:  
Figure 3. Emoticons on Thayer?s model. 
135
Thayer?s emotion categories.  Method 1 treats the 
vote to every sense equally.  Hence, it loses some 
differentiation abilities.  Method 1 performs the 
best in the first case (Lexicon A, 40 classes). 
We can also apply machine learning to the data-
set to train a high-precision classification model.  
To experiment with this idea, we adopt LIBSVM 
(Fan et al, 2005) as the SVM kernel to deal with 
the binary polarity classification problem.  The 
SVM classifier chooses top k (k = 25, 50, 75, and 
100) emotion words as features.  Since the SVM 
classifier uses a small feature set, there are testing 
instances which do not contain any features seen 
previously by the SVM classifier.  To deal with 
this problem, we use the class prediction from 
Method 3 for any testing instances without any 
features that the SVM classifier can recognize.  In 
Table 4, the SVM classifier employing 25 features 
has the highest precision.  On the other hand, the 
SVM classifier employing 50 features has the 
highest F measure when used in conjunction with 
Method 3. 
7 Conclusion and Future Work 
Our methods for building an emotional lexicon 
utilize emoticons from blog articles collaboratively 
contributed by bloggers.  Since thousands of blog 
articles are created everyday, we expect the set of 
emotional expressions to keep expanding.  In the 
experiments, the method of employing each emo-
tion word to vote only one emotion category 
achieves the best performance in both fine-grain 
and coarse-grain classification. 
Acknowledgment 
Research of this paper was partially supported by 
Excellent Research Projects of National Taiwan 
University, under the contract of 95R0062-AE00-
02.  We thank Yahoo! Taiwan Inc. for providing 
the dataset for researches. 
References 
Corinna Cortes and V. Vapnik. 1995. Support-Vector 
Network. Machine Learning, 20:273?297. 
Rong-En Fan, Pai-Hsuen Chen and Chih-Jen Lin. 2005. 
Working Set Selection Using Second Order Informa-
tion for Training Support Vector Machines. Journal 
of Machine Learning Research, 6:1889?1918. 
Gilad Mishne. 2005. Experiments with Mood Classifi-
cation in Blog Posts. Proceedings of 1st Workshop on 
Stylistic Analysis of Text for Information Access. 
Jonathon Read. 2005. Using Emotions to Reduce De-
pendency in Machine Learning Techniques for Sen-
timent Classification. Proceedings of the ACL Stu-
dent Research Workshop, 43-48. 
Robert E. Thayer. 1989. The Biopsychology of Mood 
and Arousal, Oxford University Press. 
Changhua Yang and Hsin-Hsi Chen. 2006. A Study of 
Emotion Classification Using Blog Articles. Pro-
ceedings of Conference on Computational Linguistics 
and Speech Processing, 253-269. 
Yi-Hsuan Yang, Chia-Chu Liu, and Homer H. Chen. 
2006. Music Emotion Classification: A Fuzzy Ap-
proach. Proceedings of ACM Multimedia, 81-84. 
Chung-Hsien Wu, Ze-Jing Chuang, and Yu-Chung Lin. 
2006. Emotion Recognition from Text Using Seman-
tic Labels and Separable Mixture Models. ACM 
Transactions on Asian Language Information Proc-
essing, 5(2):165-182. 
Table 3. Evaluation Results. 
Method 1 (M1) Method 2 (M2) Method 3 (M3) 
 Baseline Upp. R. Hit # Prec. Reca. Upp. R. Hit # Prec. Reca. Upp. R. Hit # Prec. Reca. 
Lexicon A 
40 classes 8.04% 14.90% 14,009 4.86% 4.55% 14.90% 9,392 3.26% 3.05% 6.49% 13,929 4.83% 4.52% 
Lexicon A 
Thayer 38.38% 48.70% 90,332 32.46% 29.35% 48.70% 64,689 23.25% 21.02% 35.94% 93,285 33.53% 30.31% 
Lexicon A 
Polarity 63.49% 60.74% 150,946 54.25% 49.05% 60.74% 120,237 43.21% 39.07% 54.97% 153,292 55.09% 49.81% 
Lexicon B 
40 classes 8.04% 73.18% 45,075 15.65% 14.65% 73.18% 43,637 15.15% 14.18% 27.89% 45,604 15.83% 14.81% 
Lexicon B 
Thayer 38.38% 89.11% 104,094 37.40% 33.82% 89.11% 118,392 42.55% 38.47% 63.74% 110,904 39.86% 36.04% 
Lexicon B 
Polarity 63.49% 91.12% 192,653 69.24% 62.60% 91.12% 188,434 67.72% 61.23% 81.92% 195,190 70.15% 63.42% 
Upp. R. ? upper bound recall; Prec. ? precision; Reca. ? recall 
                          Table 4. SVM  Performance. 
Method Upp. R. Hit # Prec. Reca. F 
Lexicon B M3 81.92% 195,190 70.15% 63.42% 66.62% 
SVM 25 features 15.80% 38,651 79.49% 12.56% 21.69% 
SVM 50 features 26.27% 62,999 77.93% 20.47% 32.42% 
SVM 75 features 36.74% 84,638 74.86% 27.50% 40.23% 
SVM 100 features 45.49% 101,934 72.81% 33.12% 45.53% 
 (Svm-25 + M3) 90.41% 196,147 70.05% 63.73% 66.74% 
 (Svm-50 + M3) 90.41% 195,835 70.37% 63.64% 66.83% 
(Svm-75 + M3) 90.41% 195,229 70.16% 63.44% 66.63% 
 (Svm-100 + M3) 90.41% 195,054 70.01% 63.38% 66.53% 
                                F = 2?(Precision?Recall)/(Precision+Recall) 
136
Sense-Tagging Chinese Corpus 
Hsin-Hsi Chen 
Department ofComputer Science and 
Information Engineering 
Natioual Taiwan University 
Taipei, TAIWAN 
hh_chen@csie.ntu .edu.tw 
Clii-Ching Lin 
Department ofComputer Science and 
Information Engineering 
National Taiwan University 
Taipei, TAIWAN 
cclin@nlg2.csie.ntu.edu.tw 
Abstract 
Contextual information and the mapping 
from WordNet synsets to Cilin sense tags 
deal with word sense disambiguation. The 
average performance is 63.36% when small 
categories are used, and 1, 2 and 3 
candidates are proposed for low, middle and 
high ambiguous words. The performance 
of tagging unknown words is 34.35%, which 
is much better than that of baseline mode. 
The sense tagger achieves the performance 
of 76.04%, when unambiguous, ambiguous, 
and unknown words are tagged. 
1 Introduction 
Tagging task, which adds lexical, syntactic or 
semantic information to raw text, makes 
materials more valuable. The researches on 
part of speech (POS) tagging have been a long 
history, and achieve very good results. Many 
POS-tagged corpora are available. The 
accuracy for POS-tagging is in the range of 95% 
to 97% 1 . In contrast, although the researches 
on word sense disambiguation (WSD) are also 
very early (Kelly and Stone, 1975), large-scale 
sense-tagged corpus is relatively few. In 
English, only some sense-tagged corpora such as 
HECTOR (Atkins, 1993), DSO (Ng and Lee, 
1996), SEMCOR (Fellbaum, 1997), and 
SENSEVAL (Kilgarriff, 1998) are available. 
For evaluating word sense disarnbiguation 
systems, the first SENSEVAL (Kilgarriff and 
Rosenzweig, 2000) reports that the performance 
for a fine-grained word sense disambiguation 
task is at around 75 %. 
1 The pelrforlnancg includes tagging wnzmbiguous 
words. Marslmll (1987) reported that the 
performance of CLAWS tagger is 94%. 
Approximately 65% of words were tagged 
nnambiguously, and the disambigualion program 
achieved better than 80% success on the ambiguous 
words. 
Tagging accuracy depends on several issues 
(Manning and Schutze, 1999), e.g., the amount 
of training data, the granularity of the tagging set, 
the occurrences of unknown words, and so on. 
Three approaches have been proposed for WSD, 
including dictionary/thesaurus-based pproach, 
supervised learning, and unsupervised learning. 
The major differences are what kinds of 
resources are used, i.e., dictionary versus text 
corpus, and sense-tagged corpus versus 
untagged eorpns. A good survey refers to the 
paper Ode and Veronis, 1998). Compared with 
English, Chinese does not have large-scale 
sense-tagged corpus. The widely available 
corpus is Academic Sinica Balanced Corpus 
abbreviated as ASBC hereafter (I-Iuang and 
Chen, 1995), which is a POS-tagged corpus. 
Thus, a computer-aided tool to sense-tag 
Chinese corpus is indispensable. 
This paper presents a sense tagger for 
Mandarin Chinese. It is organized as follows. 
Section 2 discusses the degree of polysemy in 
Mandarin Chinese from several viewpoints. 
Section 3 presents WSD algorithms for tagging 
ambiguous words and unknown words. 
Section 4 shows our experimental results. 
Finally, Section 5 concludes the remarks. 
2 Degree of Polysemy in Mandarin Chinese 
The degree of polysemy is defined as the 
average number of senses of words. We adopt 
tagging set from tong2yi4ei2ci21in2 (~ ~ ~q ~'\] 
~hk) abbreviated as Cilin (Mei, et al, 1982). It 
is composed of 12 large categories, 94 middle 
categories, and 1,428 small categories. 
Small categories (more fine granularity) are 
used to compute the distribution of word senses. 
Besides Cilin, ASBC is employed to count 
frequency of a word. Total 28,321 word types 
appear both in Cilin and in ASBC corpus. 
Here a word type corresponds toa dictionary 
Table 1. The Distribution of Word Senses 
Low Ambiguity 
Degree #Word Types 
2 4261 (71.95%) 
3 948 (16.01%) 
4 - 344 (5.81%) 
Sum 5553 (93.77%) Sum 330 (5.57%) 
Total Word Types 5922 
Middle Ambiguity High Ambiguity 
Degree #Word Types Degree #Word Types Degree #Word Types 
5 186 (3.14%) 9 14 (0.24%) 14 
6 77 (1.30%) 10 8 (0.14%) 15 
7 42 (0.71%) 11 3 (0.05%) 17 
8 25 (0.42%) 12 4 (0.07%) 18 
13 5 (0.08%) 20 
Sum 
1 (0.02%) 
1 (0.02%) 
1 (0.02%) 
1 (0.02%) 
1 (0.02%) 
39 (0.66%) 
Table 2. The Distribution of Word Senses with Consideration of POS 
2 
Low 3 
4 
5 
6 Middle 
7 
8 
9 
l l  
High 12 
13 
19 
Total Word 
Types 
N 
1441 (81.05%1 
238 (13.39% 
55 (3.09% 1
26 (1.46%1 
V A 
1056(71.79%) 
238 (16.18%) 
99 (6.73%) 
580 (79.67%) 
115 (15.80%) 
20 (2.75%) 
41 (2.79%) 9 (1.24%) 
12 (0.67% 1 13 (0.88%) 2 (0.27%)i 
3 (0.17%) 13 (0.88%) 2 (0.27%)I 
2 (0.11%) 6 (0.40%) 
1 (0.06%) 
728 1778 
1 (0.07%) 
1 (0.07%) 
1 (0.07%) 
1 (0.07%) 
1 (0.07%) 
14711 
F 
14 (77.78%) 
4 (22.22%) 
18 
K 
101 (73.72%) 
25 (18.25%) 
7 (5.11%) 
3 (2.19%) 
1 (0.73%)i 
137 
entry. Of these, 5,922 words are polysemous, 
i.e., they have more than one sense. Table 1 
lists the statistics. We divide the ambiguity 
degree into three levels according to the number 
of senses of a word. It includes low (2-4), 
middle (5-8), and high ambiguity (>8). The 
statistics shows that 93.77% of word types 
belong to the class of low ambiguity. 
We further consider POS when computing 
the distribution of word senses. Table 2 shows 
the statistics. N, V, A, F, and K denote nouns, 
verbs, adjectives, numerals, and auxiliaries 
(adverbs), respectively. We can find most of 
words belong to the class of low ambiguity no 
matter which POSes they are. Besides, the 
ambiguity is decreased when POS is considered. 
The number of polysemous words is down to 
4,132. For A and K, the number of senses is 
no more than 7, and the percentages in the class 
of  low degrees are 98.22% and 97.08%, 
respectively. For N and V, there are some high 
ambiguous words. In particular, the verb (6 ,  
da3) has 19 senses 2. The percentages in the 
class of low degrees are 97.53% and 94.70%, 
respectively. 
Then, the ffi'equency of word types is 
considered. ASBC corpus is used to compute 
the occurrences of  word types. Table 3 fists 
the statistics. A word token is an occurrence of 
a type in the corpus. On the average, the words 
of low, middle and high ambiguity appear 
205.96, 1926.65, and 4480.28 times, 
respectively. Table 1 shows 93.77% of 
polysemous words belong to the class of low 
ambiguity, but Table 3 illustrates they only 
2 The word (~, da3) has 20 senses. Besides verb 
usage, it also functions as art auxiliary. 
Table 3. The Distribution of Word Senses with Consideration of Frequencies 
Low Ambiguity Middle Ambiguity High Ambiguity 
Types I Tokens I #Tokens/ 
#Types 
5553 1143686 205.96 
93.77% 58.52% 
Types I Tokens 
330 635796 
5.57% 32.53% 
#Tokens/ 
#Types 
1926.65 
Types 
39 
0.66% 
Tokens \] #Tokens/ 
#Ty  
174731 4480.28 
8.94% 
Table 4. The Distribution of Word Senses andFrequencies with Consideration of POS, 
'~uency  Low Middle High Sum Percentage Ambiguity ~ 
Types (C) 3112 ! 734 
? ~ 70131 230955 Low Tokens (A) I 22.54 
A/C 314.65 
Types (C) 421 62 
Middle Tokens (A) i 1905 14667 
A/C 45.36 236.56 
Types (C) 0 2 
High Tokens (A) 0 843 
A/C 
Types (C) 3154 
Sum Tokens (A) 72036 
0 421.5 
A/C 22.84 
Types (C) 76.33% 
Tokens (A) 5.94% % 
798 
147 
735819 
5005.57 
29 
153307 
5286.45 
4847 
1211.75 
3993 
1036905 
259.68 
133 
169879 
1277.29 
5690 
180 
948.33 
4132 
246465 i 893973 i 1212474 
308.851 4966.52 i 
19.31%! 4.36% I 
20.33%i 73.73%i 
96.64~ 
85.52?A 
3.22"A 
14.01% 
0.15~ 
0.479 
occupy 58.52% of tokens in ASBC corpus. 
Table 4 summarizes the distribution of 
word senses and frequencies. Low frequency 
denotes the number of occurrences less than 100, 
middle frequency denotes the number of 
occurrences between 100 and 1000, and high 
frequency denotes the number of occurrences 
more than 1000. Rows C and A in Table 4 
denote number of word types and word tokens, 
respectively. The last column denotes 
percentage for each ambiguity degree. For 
example, the percentage of word types with low 
ambiguity is 96.64% (i.e., 3993/4132). This 
table shows the following two phenomena: 
(1) POS information reduces the degree of 
ambiguities. Total 8.94% of word tokens are 
high ambiguous in Table 3. It decreases to 
0.47% in Table 4. 
(2) High ambiguous words tend to be high 
frequent. From the row of low ambiguity, 
there are 3,112 low-frequent words. They 
occur 70,131 times in ASBC corpus. 
Comparatively, there are only 881 middle- or 
high-frequent words, but they occur 966,774 
times. That is, 23.67% of word types are 
middle- or high-frequent words, and they 
occupy 94.06% of word tokens. From the row 
of high ambiguity, there are only a few words, 
but they occur frequently in the ASBC corpus. 
It shows that semantic tagging is a ehallengeable 
problem in Mandarin Chinese. 
3 Semantic Tagging 
3.1 Tagging Unambiguous Words 
In the semantic tagging, the small categories are 
selected. We postulate that he sense definition 
for each word in Cilin is complete. That is, a 
word that has only one sense in Cilin is called an 
unambiguous word or a monosemous word. If 
POS information is also considered, a word may 
be unambiguous under a specific POS. 
Because we do not have a semantically tagged 
corpus for training, we try to acquire the context 
for each semantic tag strutting from the 
unambiguous words. 
ASBC corpus is the target we study. At 
the first stage, only those words that are 
unambiguous in Cilin, and also appear in ASBC 
corpus are tagged~ Figure 1 shows this cease. 
Unambiguous Words 
A S B ~ ~ ~  
Figure 1. Tagging Unambiguous Words 
An unambiguous word (and hence its sense 
tag) is characterized bythe words surrounding it.
The window size is set to 6, and stop words are 
removed. A list of stop words is trained from 
ASBC corpus. The words of POSes Neu (~ 
?~q), DE (~,  .~., ~,~-, ~) ,  SHI (~,.), FW (J'l '~ 
~) ,  C (i~l~j~q), T (~l~h~q), and I (~*~q)  
are regarded as stop words. A sense tag Ctag 
is in terms of a vector (wl, w2, ..., wn), where n 
is the vocabulary size and wi is a weight of word 
cw. The weight can be determined by the 
following two ways. 
(1) MI metric (Church, etal., 1989) 
34l (Ctag ,ew ) = 
P (Ctag, cw) 
log 2 P(Ctag )P(cw) = 
f (Ctag , ew ) ? 
l?g2 f (Ctag ) f (ew ) x zv 
where P(Ctag) is the probability of Crag, 
P(cw) is the probability of cw, 
P(Ctag, cw) is the cooccurrence 
probability of Crag and cw, 
J(Ctag) is the frequency of Ctag, 
.?ew) is the frequency of cw, 
~Ctag, cw) is the cooccurrence 
frequency of Ctag and cw, and 
N is total number of words in the 
corpus. 
(2) EM metric (Ballesteros and Croft, 1998) 
em(Ctag, cw)= 
( f(Ctag, cw)- En(Ctag, cw) 0 max f(Ctag)+ f(cw) " ) 
FEn (Ctag , cw )= f (Ctag ) f (cw ) 
N 
3.2 Tagging Ambiguous Words 
At the second stage, we deal with those words 
that have more than one sense in the Cilin. 
Figure 2 shows the words we consider. 
Unambiguous Words 
i  ilin 
Ambiguous Words 
Figure 2. Tagging Ambiguous Words 
The approach we adopted on semantic 
tagging rests on an underlying assumption: each 
sense has a characteristic ontext that is 
different from the context of all the other senses. 
In addition, all words expressing the same sense 
share the same characteristic context. We will 
apply the information trained at the first stage to 
selecting the best sense tag from the candidates 
of each ambiguous word. Recall that a vector 
corresponds to a sense tag. We employ the 
similar way specified in Section 3.1 to identify 
the context vector of an ambiguous word. A 
cosine formula shown as foUows measures the 
similarity between a sense vector and a context 
vector, where w and v are a sense vector and a 
context vector, respectively. The sense tag of 
the highest similarity score is chosen. 
W oV cos (w, v)--IwIIvl 
We retrain the sense vector for each sense tag 
after the unambiguous words are resolved. 
3.3 Tagging Unknown Words 
Those words that appear in ASBC corpus, but 
are not gathered in Cilin are called unknown 
words. All the 1,428 sense tags are the 
possible candidates. Intuitively, the algorithm 
in Section 3.2 can be applied directly to select a 
sense tag from the 1,428 candidates. However, 
the candidate set is very large. Here we adopt 
outside evidences from the mapping among 
WordNet synsets (Fellbaum, 1998) and Cflin 
10 
Cw 
f synll 
~w"  | syn12 Mapping Table 
r ewl / " \[ among 
\[ \[ syn2, WordNet 
r------.t% I" ~ J syn22 
" '  ~ew2 " \ ]  ~ synsets and 
~Figure  3. Flow of Semantic Tagging 
Candidate List 
~Ctagl TM 
Crag2 
Ctag3 
I 
I 
I 
sense tags to narrow down the candidate set. 
Figure 3 summarizes the flow of our algorithm. 
It is illuslrated as follows. 
(1) Find all the English translations of an 
unknown Chinese word by looking up a 
Chinese-English dictionary. 
(2) Find all the symets of the English 
translations by looking up WordNet. We do 
not resolve translation ambiguity and target 
polysemy at these two steps, thus the retrieved 
symets may cover more senses than that of the 
original Chinese word. 
(3) Transform the synsets back to Cilin sense 
tags by looking up a mapping table. How the 
mapping table is set up will be discussed in 
Section 3.3. I. 
(4) Select a sense tag from the candidates 
proposed at step (3) by using the WSD in 
Section 3.2. 
Figure 4 shows the unknown words we deal 
with at this stage. Those words that are not 
gathered in our Chineso-English dictionary are 
not considered, so that only parts of unknown 
words are resolve. In other words, thore 
remain words without sense tags. 
Unambiguous Words 
Unknown "~ 
Words Ambiguous Words 
Figure 4. Tagging Unknown Words 
3.3.1 Mapping SynSets to Cilin Sense Tags 
At first, we put unambiguous words (specified 
in Section 3.1) into WordNet by looking up a 
Chinese-English dictionary. Although these 
words do not have translation ambiguity, the 
corresponding English translation may have 
target polysemy problem. In other words, the 
English translation may cover irrelevant senses 
besides the correct one. The following 
algorithm will find the most similar syuseet with 
Chinese sense tag. 
(1) If the English translation corresponds to 
only one symet, this symet is the solution. 
(2) If the English translation corresponds to 
more than one synset, POS is considered: 
(a) If the Chinese sense tag belongs to one of 
categories A-D in Cilin (i.e., a noun sense), 
and there is only one noun synset, then the 
synset is adopted. Otherwise, we translate 
the context vector of the Chinese sense into 
English, compare it with vectors of the 
synsets, and select he most similar synset. 
(b) If the Chinese sense tag belongs to one of 
categories F-J in Cilin (i.e., a verb sense), 
we try to find a verb syuset in the similar 
way as (a). If it fails, we try noun and 
adjective synsets instead. 
(c) If the Chinese sense tag bdongs to category 
E in Olin (i.e., an adjective sense), we try 
adjective, adverb, noun and verb symets in 
sequence. 
Off) If the Chinese sense tag belongs to category 
K in Cilin (i.e., an adverb sense), only 
adverb syasets are considered. 
Next, we consider the ambiguous words. 
Chinese-English dictionary lookup finds all the 
English translations. WordNet search coneets 
11 
the synset candidates for the translations. 
Some synsets are selected and regarded as the 
mapping of the Cilin sense tag. Here the 
problems of translation ambiguity and target 
polysemy must be faced. In other words, not 
all English translations cover the Cilin sense. 
Because the goal is to find a mapping table 
between WordNet synsets and Cflin sense tags, 
we neglect the problem of translation ambiguity 
and follow the method in the previous paragraph 
to choose the most similar synsets. 
During mapping, English translations of a 
word may not be found in the Chinese-English 
dictionary, and WordNet may not gather the 
English translations even dictionary look-up is 
successful. Thus, only 1,328 of 1,428 Cilin 
tags are mapped to WordNet synsets. From the 
other view, there remains some WordNet 
synsets that do not correspond toany Cilin sense 
tags. Let such a synset be Si. We follow the 
relational pointers like hypernym, hyponym, 
similar, derived, antonym, or participle to 
collect the neighboring synsets denoted by Sj. 
The following method selects suitable Cflin 
tag(s) for Si. 
(1) IfSj is the only one syuset that has been 
mapped to Cilin tags, we choose a Cilin 
tag and map Si to it. 
(2) If there exists more than one Sj (say, Sjl, 
Sj2, ..., S~) that has been mapped to 
Cilin tags, we choose the Cilin tags that 
more synsets map to. 
The above method is called a more restrictive 
scheme. An alternative method (called less 
restrictive method) is: all the Cilin tags that the 
neighboring synsets map to are selected. If 
Cilin tags cannot be found from neighboring 
synsets, we extend the range one more, and 
repeat the selection procedure again until all the 
syuseets are considered. 
4 Experiments 
4.1 Test Materials 
We sample documents of different categories 
from ASBC corpus, including philosophy (10%), 
science (10%), society (35%), art (5%), life 
(20%) and literary (20%). There are 35,921 
words in the test corpus. Research associates 
tag this corpus manually. At first, they mark 
up the ambiguous words by looking up the Cilin 
dictionary. Next, they tag the unknown words. 
A list of candidates i proposed by looking up 
the mapping table. Because the mapping table 
may have errors, the annotators assign a tag 
"none" when they cannot choose a solution from 
the proposed candidates. Total 435 of 1,979 
words are tagged with "none" with the more 
restrictive method. In contrast, only 346 words 
are labeled with "none" with the less restrictive 
method. The tag mapper achieves 82.52% of 
performance approximately. 
4.2 Tagging Ambiguous Words 
Table 5 shows the performance of tagging 
ambiguous words. MI defined in Section 3.1 is 
used. Total 11,101 words are tagged. The 
performance of tagging low, middle, and high 
ambiguous words are 62.60%, 31.36%, and 
27.00%, respectively. Table 6 shows that the 
performance is improved, in particular, the 
classes of middle- and high- ambiguity, when 
EM (defined in Section 3.1) is used. The 
overall performance is increased from 49.55% 
to 52.85%. 
In the previous experiments, only one sense is 
reported for each word. If we report more than 
one sense for middle and high ambiguous words, 
the performance is improved. Table 7 shows 
that the first 2 and 3 candidates are selected. 
From the diagonal of this table, the performance 
for tagging low ambiguity (2-4), middle 
ambiguity (5-8) and high ambiguity (>8) is 
similar (i.e., 63.98%, 60.92% and 67.95%) when 
1 candidate, 2 candidates, and 3 candidates are 
proposed, respectively. In this case, 7,034 of 
11,101 words are tagged correctly. That is, the 
performance is 63.36%. 
In the next experiment, we adopt middle 
categories (i.e., 94 categories) rather than the 
above small categories (i.e., 1428 categories). 
Table 8 shows that the overall performance is 
improved by 11.05%. It also lists the results 
with the combinations of first-n and middle 
categories. Under the middle categories and 
1-3 proposed candidates, the performance for 
tagging low, middle and high ambiguous words 
are 71.02%, 73.88%, and 75.94%, respectively. 
Total 8,033 of 11,101 words are tagged 
correctly. In other words, the performance is 
72.36%. 
12 
Table 5. Performance of Tagging Ambi\[ 
"''--....Ambiguity 
Word Tokem~ Low Middle 
Total Tokens 6601 3511 
Correct Tokens 4132 1101 
Correct Rate 62.60% 31.36% 
aous Words using MI 
High , 
989 
267 
27.00% 
Summary 
11101 
5500 
49.55% 
Table 
Total Tokens 
aous Words using EM 
High 
6. Performance of Tagsing Ambig 
Low Middle " 
63.98% 37.99% 
6601 3511 989 
Correct Tokens 4223 1334 310 
Correct Rate 31.34% 
Summary 
11101 
5867 
52.85% 
-""---~Ambiguity 
First-n 
1 
Table 7. Performance of Tagging usin t
Low Middle 
63.98% 37.99% 
60.92% 
71.35% 
the Firs t-n and E M  
High Middle and High 
31.34% 36.53% 
53.99% 55.40% 
67.95% 70.60% 
Table 8. Performance of Ta, 
F'~st-~cotegoh~s--.-.-......J Low 
2 
Small 63.98% 
;ging using First-n and Middle Cate~ 
Middle High 
31.34% 37.99% 
Middle 71.02% 56.19% 43178% 53.47% 
Small 60.92% 53.99% 59.40% 
Middle 73.88% 72.09% 
Small 71.35% 
79.27% Middle 
65.72% 
67.95% 
75.94% 
ories , . ,  
Middle and High 
36.53% 
70.60% 
78.53% 
4.3 Tagging Unknown Words 
There are 1,979 unknown words in our test 
corpus. Total 1,663 words have been tagged 
manually. In the experiments, we consider the 
effects from training corpus and mapping table. 
Table 9 shows the performance. M1 and P1 
employ more restrictive mapping table, while 
M2 and P2 adopt less restrictive mapping table. 
M1 and M2 use the training result in Section 3.1 
(i.e., unambiguous words), while P1 and P2 
utilize the training result in Section 3.2 (i.e., 
unambiguous and ambiguous words). In the 
baseline model, all 1428 Cilin tags are the 
candidates of unknown words. The 
performance is worse. On the average, the 
precision is 1.22%. M1 is the best because 
more restrictive mapping table reduces the 
possibility of mapping errors. This table also 
lists the perforrnanee of each category. It 
meets our expectation, i.e., tagging verb is 
harder than tagging other categories. Next we 
use POS to improve the performance. POS 
narrows down the number of candidates, o that 
the overall performance is enhanced from 
27.13%% to 34.35%%. 
In summary, we consider the overall 
performance of tagging our sample data. 
Recall that there are 35,921 words in the test 
corpus. Except the stop words that are not 
tagged by the sense tagger, there remain 13,586 
unambiguous words, 11,101 ambiguous words, 
and 1,633 unknown words for tagging. From 
Tables 6 and 9, we know 5,867 unambiguous 
words and 561 unknown words are tagged 
correctly. The sense tagger achieves the 
performance of76.04%. 
5. Conclusion 
This paper analyzes the polysemy degree in 
Mandarin Chinese. We consider the 
distribution of word senses from POS and 
frequency. Under the Cilin small categories, 
23.67% of word types in ASBC corpus are 
13 
Categories 
All 
#Tokens 
1633 
Table 9. 
Correct 
Performance of TaB: ~ 
Baseline 
20 
M1 
443 
ng Unknown Words 
M2 
395 
24.19% 
P1 
438 
26.82% 
P2 
396 
24.25% 
MI(POS) 
56i 
34.35% Preci~on 1.22% 27.13% 
Correct 11 255 228 255 231 320 
N 858 
Preci~on 1.28% 29.72% 26.57% 29.72% 26.92% 37.30% 
Correct 5 144 124 137 120 167 
0.81% 
25.00% 
3.19% 
619 
58 
23.26% 
8.62% 
25.00% 
38 
40.43% 
V 
A 
Precision 
Correct 
20.03% 
8.62% 
25.00% 
37 
39.36 
Precision 
Correct 
Prec~smn 
22.13?A 
8.62% 
25.00% 
40 
42.55 
Correct 
Pr~ismn 
F 
19.39eA 
8.620A 
26.98~ 
28 
K 94 
48.28~ 
1 4 
25.00% 100.00% 
39 42 
41.49 44.68~ 
middle or high frequent words, but they occupy 
94.06% of word tokens. We adopt contextual 
information and mapping from WordNet synsets 
to Cilin sense tags to deal with this 
challengeable problem. The performances for 
tagging low, middle and high ambiguous words 
are 63.98%0, 60.92%, and 67.95% when small 
proposed. Comparatively, the performances 
categories are used and 1-3 candidates are 
71.02%, 73.88%, and 75.94% by using middle 
categories. The performance of tagging 
unknown words is 34.35%. It is worse than 
that of tagging ambiguous words, but is much 
better than that of the baseline mode. The 
overall performance is the sense tagger is 
76.04%. Although sense tagging does not 
achieve the performance of POS tagging, the 
sense tagger proposed in  this paper is still a 
useful computer-aided tool to reduce the human 
cost on tagging a large-scale corpus. 
References 
Atkinn, S. (1993) "Tools for Computer-Aided 
Lexicography: the Hector Project," Acta 
Linguistica Hungarica, 41, pp. 5-72. 
Ballesteros, L. and Croft, W.B. (1998) "Resolving 
Ambiguity for Cress-Language Information 
Retrieval," Proceedings of the 21st Annual 
lnternational A CM SIGIR Conference, pp. 64-71. 
Church, K.W., et al (1989) "Parsing, Word 
Associations and Typical Predicate-Argument 
Relations." Proceedings of International 
Workshop on Parsing Technologies, pp. 389-398. 
Huang, C.R_ and Chen, K.L (1995) "Academic 
Sinica Balanced Corpus," Technical Report 
95-02/98-04, Academic Sinica, Taipei, Taiwan. 
Fellbaum, C. editor (1998) WardNet: An Electronic 
Lexical Database, MIT Press, Cambridge, Mass. 
Ide, N. and Veronis, J. (1998) "Word Sense 
Disambiguation: The State of Art," 
Computational Linguistics, 24( 1 ), pp. 1--40. 
Kelly, E. and Stone, P. (1975) Computer Recognition 
of English Word Senses, North-Holland, 
Amsterdam. 
Kilgarriff, A. (1998) "SENSEVAL: An Exercise in 
Evalnafiqg Word Sense Disnmbiguation 
Program~," Proceedings of First International 
Conference on Language Resources and 
Evaluation, Granada, pp. 581-588. 
Kilgarriff, A. and Rosenzweig, J. (2000) '~,nglish 
SENSEVAL: Report and Results," Proceedings 
of Second International Conference on Language 
Resources and Evaluation. 
Manning, C.D. and Schutze, I-L (1999) Foundations 
of Statistical Natural Language Processing, MIT 
Press, Cambridge, Mass. 
\]Vlarshall, I. (1987) "Tag Selection using Probabilistic 
Methods," in Roger Garside, Geoffrey Leech and 
GeotErey Sampson (editors), The Computational 
Analysis of English, Longman~ pp. 42-56. 
Mei, J.; et al (1982) tong2yi4ci2ci21in2. Shanghai 
Dictionary Press. 
Ng, I-LT. and Lee, I-LB. (1996) "Integrating Multiple 
Knowledge Sources to Disambiguate Word Sense: 
An Exemplar-Based Approach," Proceedings of 
34th Annual Meeting of Association for 
Computational Linguistics, pp. 40--47. 
14 
W  e i-     
    	   
  
         
        	  


           	  
 

  
   

    
   
  
        Enhancing Performance of Protein Name Recognizers Using Collocation 
Wen-Juan Hou 
Department of Computer Science 
and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
wjhou@nlg.csie.ntu.edu.tw 
Hsin-Hsi Chen 
Department of Computer Science 
and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
hh_chen@csie.ntu.edu.tw 
 
 
Abstract 
Named entity recognition is a fundamental task in 
biological relationship mining.  This paper 
employs protein collocates extracted from a 
biological corpus to enhance the performance of 
protein name recognizers.  Yapex and KeX are 
taken as examples.  The precision of Yapex is 
increased from 70.90% to 81.94% at the low 
expense of recall rate (i.e., only decrease 2.39%) 
when collocates are incorporated.  We also 
integrate the results proposed by Yapex and KeX, 
and employs collocates to filter the merged results.  
Because the candidates suggested by these two 
systems may be inconsistent, i.e., overlap in partial, 
one of them is considered as a basis.  The 
experiments show that Yapex-based integration is 
better than KeX-based integration. 
1 Introduction 
Named entities are basic constituents in a 
document.  Recognizing named entities is a 
fundamental step for document understanding.  In 
a famous message understanding competition 
MUC (Darpa, 1998), named entities extraction, 
including organizations, people, and locations, 
along with date/time expressions and monetary and 
percentage expressions, is one of the evaluation 
tasks.  Several approaches have been proposed to 
capture these types of terms.  For example, 
corpus-based methods are employed to extract 
Chinese personal names, and rule-based methods 
are used to extract Chinese date/time expressions 
and monetary and percentage expressions (Chen 
and Lee, 1996; Chen, et al, 1998).  Corpus-based 
approach is adopted because a large personal name 
database is available for training.  In contrast, 
rules which have good coverage exist for date/time 
expressions, so the rule-based approach is adopted. 
In the past, named entities extraction mainly 
focuses on general domains.  Recently, large 
amount of scientific documents has been published, 
in particular for biomedical domains.  Several 
attempts have been made to mine knowledge from 
biomedical documents (Hirschman, et al, 2002).  
One of their goals is to construct a knowledge base 
automatically and to find new information 
embedded in documents (Craven and Kumlien, 
1999).  Similar information extraction works have 
been explored on this domain.  Named entities 
like protein names, gene names, drug names, 
disease names, and so on, were recognized (Collier, 
et al, 2000; Fukuda, et al, 1998; Olsson, et al, 
2002; Rindflesch, et al, 2000).  Besides, the 
relationships among these entities, e.g., 
protein-protein, protein-gene, drug-gene, 
drug-disease, etc., were extracted (Blaschke, et al, 
1999; Frideman, et al, 2001; Hou and Chen, 2002; 
Marcotte, et al, 2001; Ng and Wong, 1999; Park, 
et al, 2001; Rindflesch, et al, 2000; Thomas, et al, 
2000; Wong, 2001). 
Collocation denotes two or more words having 
strong relationships (Manning and Schutze, 1999).  
The related technologies have been applied to 
terminological extraction, natural language 
generation, parsing, and so on.  This paper deals 
with a special collocation in biological domain ? 
say, protein collocation.  We will find out those 
keywords that co-occur with protein names by 
using statistical methods.  Such terms, which are 
called collocates of proteins hereafter, will be 
considered as restrictions in protein name 
extraction.  To improve the precision rate at the 
low expense of recall rate is the main theme of this 
approach. 
The rest of the paper is organized as follows.  
The protein name recognizers used in this study are 
introduced in Section 2.  The collocation method 
we adopted is shown in Section 3.  The filtering 
and integration strategies are explained in Sections 
4 and 5, respectively.  Finally, Section 6 
concludes the remarks and lists some future works. 
2 
3 
Protein Name Recognizers 
The detection of protein names presents a 
challenging task because of their variant structural 
characteristics, their resemblance to regular noun 
phrases and their similarity with other kinds of 
biological substances.  Previous approaches on 
biological named entities extraction can be 
classified into two types ? say, rule-based (Fukuda, 
et al, 1998; Humphreys, et al, 2000; Olsson, et al, 
2002) and corpus-based (Collier, et al, 2000).  
KeX developed by Fukuda, et al (1998) and 
Yapex developed by Olsson, et al (2002) were 
based on handcrafted rules for extracting protein 
names.  Collier, et al (2000) trained a Hidden 
Markov Model with a small corpus of 100 
MEDLINE abstracts to extract names of gene and 
gene products. 
Different taggers have their specific features.  
KeX was evaluated by using 30 abstracts on SH3 
domain and 50 abstracts on signal transduction, 
and achieved 94.70% precision and 98.84% recall.  
Yapex was applied to a test corpus of 101 abstracts.   
Of these, 48 documents were queried from protein 
binding and interaction, and 53 documents were 
randomly chosen from GENIA corpus.  The 
performance of tagging protein names is 67.8% 
precision and 66.4% recall.  While the same test 
corpus was applied to KeX, it got 40.4% precision 
and 41.1% recall.  It reveals that each tagger has 
its own characteristics.  Changing the domain 
may result in the variant performance.  
Consequently, how to select the correct molecular 
entities proposed from the existing taggers is an 
interesting issue. 
Statistical Methods for Collocation 
The overall flow of our method is shown in Figure 
1.  To extract protein collocates, we need a corpus 
in which protein names have been tagged.  Thus, 
we prepare a tagged biological corpus by looking 
up the protein lexicon in the first step.  Then, 
common stop words are removed and the 
stemming procedure is applied to gather and group 
more informative words.  Next, the collocation 
values of proteins and their surrounding words are 
calculated.  Finally, we use these values to tell 
which neighbouring words are the desired 
collocates.  The major modules are specified in 
detail in the following subsections. 
 
 
 Protein 
lexicon 
Tag the raw material  
 
 
 Preprocessing 
1. Remove stopwords 
2. Stem 
Stop w
ord
list  
 
 
 
 Calculate collocation value 
 
 
 Extract significant protein 
collocates  
 
 
 
   
Figure 1. Flow of Mining Protein Collocates 
3.1 Step 1: Tagging the Corpus 
On the one hand, to calculate the collocation 
values of words with proteins from a corpus, it is 
necessary to recognize protein names at first.  On 
the other hand, the goal of this paper deals with 
performance issue of protein name tagging.  
Hence, preparing a protein name tagged corpus and 
developing a high performance protein name 
tagger seem to be a chicken-egg problem.  
Because the corpus developed in the first step is 
used to extract the contextual information of 
proteins, a completely tagged corpus is not 
necessary at the first step.  Dictionary-based 
approach for name tagging, i.e., full pattern 
matching between the dictionary entries and the 
words in the corpus, is simple.  The major 
argument is its coverage.  Those protein names 
which are not listed in the dictionary, but appear in 
the corpus will not be recognized.  Thus this 
approach only produces a partial-tagged corpus, 
but it is enough to acquire contextual information 
for latter use. 
3.2 Step 2: Preprocessing 
3.2.1 Step 2.1: Exclusion of Stopwords 
Stopwords are common English words (such as 
preposition ?in? and article ?the?) that frequently 
appear in the text but are not helpful in 
discriminating special classes.  Because they are 
distributed largely in the corpus, they should be 
filtered out.  The stopword list in this study was 
collected with reference to the stoplists of Fox 
(1992), but the words also appearing in the protein 
lexicon are removed.  For example, ?of? is a 
constituent of the protein name ?capsid of the 
lumazine?, so that ?of? is excluded from the 
stoplist.  Finally, 387 stopwords were used. 
3.2.2 Step 2.2: Stemming 
Stemming is a procedure of transforming an 
inflected form to its root form.  For example, 
?inhibited? and ?inhibition? will be mapped into 
the root form ?inhibit? after stemming.  
Stemming can group the same word semantics and 
reflect more information around the proteins. 
3.3 Step 3: Computing Collocation Statistics 
The collocates of proteins are those terms that 
often co-occur with protein names in the corpus.  
In this step, we calculate three collocation statistics 
to find the significant terms around proteins. 
Frequency 
The collocates are selected by frequency.  In 
order to gather more flexible relationships, here we 
define a collocation window that has five words on 
each side of protein names.  And then collocation 
bigrams at a distance are captured.  In general, 
more occurrences in the collocation windows are 
preferred, but the standard criteria for frequencies 
are not acknowledged.  Hence, other collocation 
models are also considered. 
Mean and Variance 
The mean value of collocations can indicate how 
far collocates are typically located from protein 
names.  Furthermore, variance shows the 
deviation from the mean.  The standard deviation 
of value zero indicates that the collocates and the 
protein names always occur at exactly the same 
distance equal to the mean value.  If the standard 
deviation is low, two words usually occur at about 
the same distance, i.e., near the mean value.  If 
the standard deviation is high, then the collocates 
and the protein names occur at random distance. 
t-test Model 
When the values of mean and variance have been 
computed, it is necessary to know if two words do 
not co-occur by chance.  Moreover, we also have 
to know if the standard deviation is low enough.  
In other words, we have to set a threshold in the 
above approach.  To get the statistical confidence 
that two words have a collocation relationship, 
t-test hypothesis testing is adopted. 
The t-value for each word i is formulated as 
follows: 
Ns
uxt
i
ii
i
/2
?=  
Where 
N = 4n - 15, 
N
countnx ii _= , 
)1(2 iii pps ??= , 
ncountnp ii /_= , 
iproteini ppu ?= , and 
proteinp  is the probability of protein. 
When ? (confidence level) is equal to 0.005, the 
value of t is 2.576.  In the t-test model, if the 
t-value is larger than 2.576, the word is regarded as 
a good collocate of protein with 99.5% confidence. 
3.4 Step 4: Extraction of Collocates 
We applied the above procedure to a corpus 
downloaded from the PASTA website in Sheffield 
University with 1,514 MEDLINE abstracts 
[http://www.dcs.shef.ac.uk/nlp/pasta].  Of the 
4,782 different stemmed words appearing in the 
collocation windows, there are 541 collocations 
generated in Step 3.  The collocates are not 
tagged with parts of speech, so that the output may 
contain nouns, prepositions, numbers, verbs, etc. 
The collocates extracted in a corpus cannot only 
serve as conditions of protein names, but also 
facilitate the relationship discovery between 
proteins.  From the past papers on the extraction 
of the biological information, such as Blaschke, et 
al. (1999), Ng, et al (1999), and Ono, et al (2001) 
etc., verbs are the major targets.  This is because 
many of the subjects and the objects related to 
these verbs are names of genes or proteins.  To 
assure that the collocates selected in Step 3 are 
verbs, we assign parts of speech to these words.  
Appendix A lists the collocates and their 
variations. 
4 Filtering Strategies 
For protein name recognition, rule-based systems 
and dictionary-based systems are usually 
complementary.  Rule-based systems can 
recognize those protein names not listed in a 
dictionary, but some false entities may also pass at 
the same time.  Dictionary-based systems can 
recognize those proteins in a dictionary, but the 
coverage is its major deficiency.  In this section, 
we will employ collocates of proteins mined earlier 
to help identify the molecular entities.  Yapex 
system (Olsson et al, 2002) is adopted to propose 
candidates, and collocates are served as restrictions 
to filter out less possible protein names. 
The following filtering strategies are proposed.  
Assume the candidate set M0 is the output 
generated by Yapex. 
z M1: For each candidate in M0, check if a 
collocate is found in its collocation window.  
If yes, tag the candidate as a protein name.  
Otherwise, discard it. 
z M2: Some of the collocates may be 
substrings of protein names.  We relax the 
restriction in M1 as follows.  If a 
collocate appears in the candidate or in the 
collocation window of the candidate, then 
tag the candidate as a protein name; 
otherwise, discard it. 
z M3: Some protein names may appear more 
than once in a document.  They may not 
always co-occur with some collocate in 
each occurrence.  In other words, the 
protein candidate and some collocates may 
co-occur in the first occurrence, the second 
occurrence, or even the last occurrence.  
We revise M1 and M2 as follows to 
capture this phenomenon.  During 
checking if there exists a collocate 
co-occurring with a protein candidate, the 
candidate without any collocate is kept 
undecidable instead of definite no.  After 
all the protein names are examined, those 
undecidable candidates may be considered 
as protein names when one of their 
co-occurrences containing any collocate.  
In other words, as long as a candidate has 
been confirmed once, it is assumed to be a 
protein throughout.  In this way, there are 
two filtering alternatives M31 and M32 
from M1 and M2, respectively. 
To get more objective evaluation, we utilized 
another corpus of 101 abstracts used by Yapex 
[http://www.sics.se/humle/projects/prothalt].  
Using the test corpus and answer keys supported in 
Yapex project, the evaluation results on filtering 
strategies are listed in Table 1. 
 
Table 1.  Evaluation on Filtering Strategies 
 Precision Recall F-score 
M0 70.90% 69.53% 70.22% 
M1 79.18% 56.10% 67.64% 
M2 79.29% 56.66% 67.98% 
M31 81.97% 66.84% 74.41% 
M32 81.94% 67.14% 74.54% 
 
Compared with the baseline model M0, the 
precision rates of all the four models using 
collocates were improved more than 8%.  The 
recall rates of M1 and M2 decreased about 13%. 
Thus, the overall F-scores of M1 and M2 
decreased about 2% compared to M0.  In contrast, 
if the decision of tagging was deferred until all the 
information were considered, then the recall rate 
decreased only 2% and the overall F-scores of M31 
and M32 increased 4% relative to M0.  The best 
one, M32, improved the precision rate from 
70.90% to 81.94%, and the F-score from 70.22% 
to 74.54%.  That meets our expectation, i.e., to 
enhance the precision rate, but not to reduce the 
significant recall rate. 
5 Integration Strategies 
Now we consider how to improve the recall rates. 
Integration strategies based on a hybrid concept are 
introduced.  The basic idea is that different 
protein name taggers have their own specific 
features such that they can recognize some tagging 
objects according to their rules or recognition 
methods.  Among the proposed protein names by 
different recognizers, there may exist some 
overlaps and some differences.  In other words, a 
protein name recognizer may tag a protein name 
that another recognizer cannot identify, or both of 
them may accept certain common proteins.  The 
integration strategies are used to select correct 
protein names proposed by multiple recognizers.  
In this study, we made experiments on Yapex and 
KeX because they are freely available on the web. 
Because protein candidates are proposed by two 
named entity extractors independently, they may 
be totally separated, totally overlap, overlapped in 
between, overlapped in the beginning, and 
overlapped in the end.  Figure 2 demonstrates 
these five cases. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
The integration strategies shown as follows 
combine the results from two sources. 
z When the protein names produced from 
two recognizers are totally separated (i.e., 
type A), retain them as the protein 
candidates.  This integration strategy 
postulates that one protein name 
recognizer may extract some proteins that 
another one cannot identify. 
z When the protein names produced from 
two recognizers are exactly the same (i.e., 
type B), retain them as the protein 
candidates.  Because both taggers accept 
the same protein names, there must exist 
some special features that fit protein 
names. 
z When the protein names tagged by two 
taggers have partial overlap (i.e., types C, 
D and E), two additional integration 
strategies are employed, i.e., Yapex-based 
and KeX-based strategies.  In the former 
strategy, we adopt protein names tagged 
by Yapex as candidates and discard the 
ones produced by KeX.  In contrast, the 
names tagged by KeX are kept in the latter 
strategy.  The integration strategy is 
made because each recognizer has its own 
characteristics, and we do not know which 
one is performed better in advance. 
Type A: totally separated 
The above integration strategies put together all 
the possible protein candidates except the 
ambiguous cases (i.e., types C, D and E).  That 
tends to increase the recall rate.  To avoid 
decreasing the precision rate, we also employ the 
collocates mentioned in Section 3 to filter out the 
less possible protein candidates.  Furthermore, to 
objectively evaluate the performance of the 
proposed collocates, we employ the same 
strategies to the same test corpus with some terms 
suggested by human experts.  Total 48 verbal 
keywords which were used to find the pathway of 
proteins are used and listed in Appendix B. 
Type B: totally overlap 
Type C: overlapped in between 
Type D: overlapped in the beginning 
Four sets of experiments were designed as 
follows for Yapex- and KeX-based integration 
strategies, respectively. 
Type E: overlapped in the end 
(1)YA and KA: Use the collocates automatically 
extracted in Section 3 to filter out the candidates as 
described in Section 4. 
(2)YB and KB: Use the terms suggested by 
human experts for the filtering strategies. 
Figure 2. Candidates Proposed by Two Systems 
(3)YA-C and KA-C: If Yapex and KeX 
recommend the same protein names (i.e., type B), 
regard them as protein names without 
consideration of collocates.  Otherwise, use the 
collocates proposed in this study to make filtering. 
(4)YB-C and KB-C: Similar to (3) except that 
the collocates are replaced by the terms suggested 
by human experts. 
The experimental results are listed in Tables 2 
and 3.  The tendency M32>M31>M2>M1 is still 
kept in the new experiments.  The strategy of 
delaying the decision until clear evidence is found 
is workable.  The performances of YA, YA-C, KA, 
and KA-C are better than the performances of the 
corresponding models (i.e., YB, YB-C, KB, and  
 
Table 2. Evaluation Results on Yapex-based 
Integration Strategy 
YA Precision Recall F-score 
M0 61.98% 77.52% 69.75% 
M1 64.97% 62.82% 63.90% 
M2 65.02% 63.53% 64.28% 
M31 65.94% 74.26% 70.10% 
M32 65.90% 74.62% 70.26% 
YB    
M1 66.79% 44.30% 55.55% 
M2 66.79% 44.81% 55.80% 
M31 70.20% 65.06% 67.63% 
M32 70.19% 65.51% 67.85% 
YA-C    
M1 65.76% 69.18% 67.47% 
M2 65.88% 69.84% 67.86% 
M31 65.39% 75.43% 70.41% 
M32 65.38% 75.69% 70.54% 
YB-C    
M1 68.92% 58.09% 63.51% 
M2 68.78% 58.49% 63.64% 
M31 69.07% 69.08% 69.13% 
M32 69.07% 69.63% 69.35% 
 
Table 3. Evaluation Results on KeX-based 
Integration Strategy 
KA Precision Recall F-score 
M0 60.43% 70.60% 65.52% 
M1 63.82% 56.61% 60.22% 
M2 63.52% 57.22% 60.37% 
M31 64.39% 65.56% 64.98% 
M32 64.03% 65.92% 64.98% 
KB    
M1 67.56% 41.20% 54.38% 
M2 66.99% 41.71% 54.35% 
M31 69.57% 55.70% 61.64% 
M32 69.25% 56.26% 62.76% 
KA-C    
M1 64.72% 63.17% 63.95% 
M2 64.44% 63.68% 64.06% 
M31 63.83% 66.79% 65.31% 
M32 63.49% 67.04% 65.27% 
KB-C    
M1 69.57% 55.60% 62.59% 
M2 69.15% 56.10% 64.06% 
M31 68.36% 60.22% 64.29% 
M32 68.09% 60.78% 64.44% 
KB-C).  It shows that the set of collocates 
proposed by our system is more complete than the 
set of terms suggested by human experts.  
Compared with the recall rate of M0 in Table 1 
(i.e., 69.53%), the recall rates of both Yapex- and 
KeX-based integration are increased, i.e., 77.52% 
and 70.60%, respectively.  That matches our 
expectation.  However, the precision rates are 
decreased more than the increase of recall rates.  
In particular, the F-score of KeX-based integration 
strategy is 4.70% worse than that of the baseline 
M0.  It shows that KeX performed not well in this 
test set, so it cannot recommend good candidates in 
the integration stage.  Moreover, the F-scores of 
M31 and M32 of YA and YA-C are better than that 
of M0 in Table 1.  It reveals that Yapex 
performed better in this test corpus, so that we can 
enhance the performance by both the filtering and 
integration strategies.  Nevertheless, the models 
in Tables 2 and 3 still cannot compete to M32 in 
Table 1.  The reason may be some heuristic rules 
used in Yapex are modified from KeX (Olsson et 
al., 2002). 
6 Concluding Remarks 
This paper shows a fully automatic way of mining 
collocates from scientific text in the protein 
domain, and employs them to improve the 
performance of protein name recognition 
successfully.  The same approach can be extended 
to other domains like gene, DNA, RNA, drugs, and 
so on.  The collocates extracted from a domain 
corpus are also important keywords for pathway 
discovery, so that a systematic way from basic 
named entities finding to complex relationships 
discovery can be established. 
Applying filtering strategy only demonstrates 
better performance than applying both filtering and 
integration strategies together in this paper.  One 
of the possible reasons is that the adopted systems 
are similar, i.e., both systems are rule-based, and 
some heuristic steps used in one system are 
inherited from another.  The effects of combining 
different types of protein name taggers, e.g., 
rule-based and corpus-based, will be investigated 
in the future. 
Acknowledgements 
Part of research results was supported by National 
Science Council under the contract 
NSC-91-2213-E-002-088.  We also thank Dr. 
George Demetriou in the Department of the 
Computer Science of the University of Sheffield, 
who kindly supported the resources in this work. 
References 
Blaschke, C., Andrade, M.A., Ouzounis, C. and 
Valencia, A. (1999) ?Automatic Extraction of 
Biological Information from Scientific Text: 
Protein-Protein Interactions,? Proceedings of 7th 
International Conference on Intelligent Systems for 
Molecular Biology, pp. 60-67. 
Chen, H.H. and Lee, J.C. (1996) ?Identification and 
Classification of Proper Nouns in Chinese Texts,? 
Proceedings of 16th International Conference on 
Computational Linguistics, pp. 222-229. 
Chen, H.H.; Ding, Y.W. and Tsai, S.C. (1998) ?Named 
Entity Extraction for Information Retrieval,? 
Computer Processing of Oriental Languages, 
Special Issue on Information Retrieval on Oriental 
Languages, 12(1), 1998, pp. 75-85. 
Collier, N., Park, H.S., Ogata, N., Tateishi, Y., Nobata, 
C. and Ohta, T. (1999) ?The GENIA project: 
Corpus-based Knowledge Acquisition and 
Information Extraction from Genome Research 
Papers,? Proceedings of the Annual Meeting of the 
European Chapter of the Association for 
Computational Linguistics (EACL?99), June. 
Collier, N., Nobata, C. and Tsujii J.I. (2000) ?Extracting 
the Names of Genes and Gene Products with a 
Hidden Markov Model,? Proceedings of 18th 
International Conference on Computational 
Linguistics, pp. 201-207. 
Craven, M. and Kumlien, J. (1999) ?Constructing 
Biological Knowledge Bases by Extracting 
Information from Text Sources, Proceedings of 7th 
International Conference on Intelligent Systems for 
Molecular Biology, pp. 77-86. 
DARPA (1998) Proceedings of 7th Message 
Understanding Conference. 
Fox, C. Lexical Analysis and Stoplists. In Information 
Retrieval: Data Structures and Algorithms, Frakes, 
W. B. and Baeza-Yates, R., ed., Prentice Hall, 
102-130, 1992. 
Friedman, C., Kra, P., Yu, H., Krauthammer, M. and 
Rzhetsky, A. (2001) ?GENIES: A Natural Language 
Processing System for the Extraction of Molecular 
Pathways from Journal Articles,? Bioinformatics, 
17(S1), pp. 74-82. 
Fukuda, K., Tsunoda, T., Tamura, A., and Takagi, T. 
(1998) ?Toward Information Extraction: Identifying 
Protein Names from Biological Papers,? 
Proceedings of Pacific Symposium on Biocomputing, 
pp. 707-718. 
Hirschman, L., Park, J.C., Tsujii, J., Wong, L. and Wu, 
C.H. (2002) ?Accomplishments and Challenges in 
Literature Data mining for Biology,? Bioinformatics, 
18(12), pp. 1553-1561. 
Hou, W.J. and Chen, H.H. (2002) ?Extracting Biological 
Keywords from Scientific Text,? Proceedings of 13th 
International Conference on Genome Informatics, 
pp. 571-573. 
Humphreys, K., Demetriou, G. and Gaizauskas, R. 
(2000) ?Two Applications of Information Extraction 
to Biological Science Journal Articles: Enzyme 
Interactions and Protein Structures,? Proceedings of 
Pacific Symposium on Biocomputing, 5, pp. 
502-513. 
Manning, C.D. and Schutze, H. (1999) Foundations of 
Statistical Natural Language Processing, The MIT 
Press. 
Marcotte, E.M., Xenarios, I. and Eisenberd, D. (2001) 
?Mining Literature for Protein-protein Interactions,? 
Bioinformatics, 17(4), pp. 359-363. 
Ng, S.-K. and Wong, M. (1999) ?Toward Routine 
Automatic Pathway Discovery from On-line 
Scientific Text Abstracts,? Proceedings of 10th 
International Conference on Genome Informatics, 
pp. 104-112. 
Olsson, F., Eriksson, G., Franzen, K., Asker, L. and 
Liden P. (2002) ?Notions of Correctness when 
Evaluating Protein Name Taggers,? Proceedings of 
the 19th International Conference on Computational 
Linguistics, pp. 765-771. 
Ono, T., Hishigaki, H., Tanigami, A., and Takagi, T. 
?Automated Extraction of Information on 
Protein-Protein Interactions from the Biological 
Literature,? Bioinformatics, 17(2), pp.155-161. 
Park, J.C., Kim, H.S., and Kim, J.J. (2001) 
?Bidirectional Incremental Parsing for Automatic 
Pathway Identification with Combinatory Categorial 
Grammar,? Proceedings of Pacific Symposium on 
Biocomputing, 6, pp. 396-407. 
Rindflesch, T.C., Tanabe, L., Weinstein, J.N. and Hunter, 
L. (2000) ?EDGAR: Extraction of Drugs, Genes, 
and Relations from Biomedical Literature,? 
Proceedings of Pacific Symposium on Biocomputing, 
5, pp. 517-528. 
Thomas, J., Milward, D., Ouzounis, C., Pulman, S., and 
Carroll, M. (2000) ?Automatic Extraction of Protein 
Interactions from Scientific Abstracts,? Proceedings 
of Pacific Symposium on Biocomputing, 5, pp. 
538-549. 
Wong, L. (2001) ?PIES, a Protein Interaction Extraction 
System,? Proceedings of Pacific Symposium on 
Biocomputing, 6, pp. 520-531. 
Appendix A. Collocates mined from corpus 
act (-, -ed, -ing, -ion, -ive, -ivities, -ivity, -s), 
activat (-e, -ed, -es, -ing, -ion, -or) , adopt (-,ed, -s), 
affect (-, -ed, -ing, -s), allow (-, -ed, -s), analy (-sed, 
-ses, -sis, -zed, -zing), appear (-, -s), arrange (-d, 
-ment), assembl (-ing, -y), associat (-e, -ed, -ion), 
bas (-e, -ed, -is), belong (-, -ing, -s), bind (-, -ing, 
-s) / bound, bond (-, -ed, -ing, -s), bridge (-, -d, -s), 
calculat (-ed, -ion), called, carr (-ied, -ier, -ies), 
cataly (-sed, -ses, -stic, -ze, -zed, -zes, -zing), cause 
(-, -d, -s), center (-, -ed) / centre (-, -s), chang (-e, 
-ed, -es, -ing), characteriz (-ation, -e, -ed, -es, -ing), 
charg (-e, -ed), class (-, -es, -ified, -ifying), cleav 
(-e, -ed, -es, -ing), clos (-e, -ed, -ing), coil (-, -ed), 
compar (-e, -ed, -ing, -ison, -isons), complex (-, -ed, 
-es), composed, compris (-es, -ing), conclu (-de, 
-ded, -sion, -sions), conserved, consist (-, -ed, -ent, 
-ing, -s), constitut (-e, -ed, -es), contact (-, -s), 
contain (-, -ed, -ing, -s), coordinat (-e, -ed, -es, 
-ion), correlat (-e, -ed), correspond (-, -ing), crystal 
(-, -lize, -lized, -lizes, -s), cycl (-e, -es, -ing), define 
(-d, -s), demonstrat (-e, -ed, -es, -ing), depend (-, 
-ent, -ing), derived, describe (-, -d), design (-, -ed, 
-ing), detail (-, -ed, -s), determin (-ation, -ations, -e, 
-ed, -es, -ing), differ (-ence, -ences, -s), diffract 
(-ing, -ion), digest (-ed, -s), dimer (-, -ic, -ization, 
-ize), direct (-, -ed, -s), discuss (-, -ed), display (-, 
-s), disrupt (-, -ed, -ing, -s), effect (-, -s), encod (-e, 
-ed, -ing), enhanc (-e, -ed, -er, -es, -ing), exhibit (-, 
-ed, -s), exist (-, -s), explain (-, -ed, -ing, -s), 
express (-ed, -ing), extend (-, -ed), facilitat (-e, -es, 
-ing), finding / found, fold (-, -ed, -ing, -s), form (-, 
-ed, -ing, -s), function (-, -al, -ing, -s), groove (-, 
-s), hydroly (-sis, -zed, -zes), identif (-ied, -ies, -y), 
implicat (-e, -ed, -ions), inactiv (-ated, -ates, -e), 
includ (-ed, -es, -ing), indicat (-e, -ed, -es, -ing), 
induc (-e, -ed, -es, -ing), inhibit (-, -ed, -ing, -ion, 
-or, -ors, -s), initiat (-ed, -es), insert (-, -ed, -ing), 
interact (-, -ing, -ion, -ions, -s), involv (-e, -ed, -es, 
-ing), isolated, lack (-, -s), lead (-, -ing, -s), ligand 
(-, -ed, -s), like, link (-, -ed, -ing), located, loop (-, 
-ing, -s), mediat (-e, -ed, -es, -ing), model (-, -ed, 
-ing, -s), modul (-ate, -ates, -ating, -e, -es), mutat 
(-ed, -ions), observ (-e, -ed), obtain (-, -ed), occup 
(-ied, -ies), occur (-, -red, -s), organiz (-ation, -ed), 
oxidiz (-ed, -ing), phosphorylate (-d, -s), play (-, 
-s), position (-, -ed, -ing, -s), predict (-, -ed, -ing), 
presen (-ce, -ted, -ting), produc (-e, -ed, -es, -ing), 
promot (-e, -er, -es, -ing), proposed, proton (-, 
-ated, -s), provid (-e, -ed, -es, -ing), purif (-ied, -y), 
react (-, -ion, -tive, -s), recogni (-tion, -zed, -zes, 
-ing), reduc (-ed, -es, -ing, -tase, -tion), refined, 
regulat (-e, -ed, -es, -ing, -ion, -ory), relat (-ed, -es, 
-ive), repeat (-, -ed, -s), replaced, report (-, -ed), 
represent (-, -ed, -ing, -s), requir (-e, -ed, -es, -ing), 
resembl (-e, -ed, -es, -ing), resol (-ution, -ve), 
result (-, -ed, -ing, -s), reveal (-, -ed, -s), select (-ed, 
-ive, -ively), sequence (-, -d, -s), serve (-, -s), shape 
(-, -d), share (-, -d, -s), show (-, -n, -s), signal (-, 
-ing, , -ling, -s), sol (-ution, -ved), stabili (sed, -ty, 
-ze, -zed, -zes, -zing), stimulat (-e, -ed, -es, -ion, 
-ory), strain (-, -s), strand (-, -ed, -s), structur (-al, 
-ally, -e, -ed, -es), stud (-ied, -ies, -y, -ying), 
substitut (-e, -es, -ion, -ions), substrate (-, -s), 
suggest (-, -ed, -ing, -ion, -s), support (-, -ing, -s), 
switch (-, -es), synthesi (-s, -ze, -zed), target (-, -ed, 
-ing, -s), transfer (-, -red), transport (-, -s), 
understand (-, -ing) / understood, unexpected, us 
(-e, -ing) 
Appendix B. Terms suggested by an expert 
accompan (-ied, -ies, -y, -ying), activat (-e, -ed, -es, 
-ing, -ion, -or, -ors, -ory), affect (-, -ed, -ing, -s), 
aggregat (-e, -ed, -es, -ing, -ion), assembl (-e, -ed, 
-es, -ing, -y), associat (-e, -ed, -es, -ing, -ion), 
attract (-, -ed, -ing, -ion, -s), bind (-, -ing, -s) / 
bound, catalys (-e, -ed, -es, -ing, -tic), catalyz (-e, 
-ed, -es, -ing), cluster (-, -ed, -ing, -s), communicat 
(-e, -ed, -es, -ing, -ion), complex (-, -ed, -es, -ing), 
construct (-, -ed, -ing, -ion, -s), control (-, -ed, -ing, 
-led, -ling, -s), cooperat (-e, -ed, -es, -ing, -ion, -or, 
-ors), correlat (-e, -ed, -es, -ing, -ion), coupl (-e, 
-ed, -es, -ing), crosslink (-, -ed, -ing, -s), 
deglycosylat (-e, -ed, -es, -ing, -ion, -ory), 
demethylat (-e, -ed, -es, -ing, -ion, -ory), 
dephosphorylat (-e, -ed, -es, -ing, -ion, -ory), effect 
(-, -ed, -ing, -s), eliminat (-e, -ed, -es, -ing, -ion), 
enabl (-e, -ed, -es, -ing), enhanc (-e, -ed, -er, -es, 
-ing), glycosylat (-e, -ed, -es, -ing, -ion, -ory), 
group (-, -ed, -ing, -s), help (-, -ed, -ing, -s), hinder 
(-, -ed, -ing, -s), inactivat (-e, -ed, -es, -ing, -ion, 
-or, -ors, -ory), inhibit (-, -ed, -ing, -ion, -or, -ors, 
-ory, -s), integrat (-e, -ed, -es, -ing, -ion), interact (-, 
-ed, -ing, -ion, -s), link (-, -ed, -ing, -s), methylat 
(-e, -ed, -es, -ing, -ion), obstacl (-e, -ed, -es, -ing), 
participat (-e, -ed, -es, -ing, -ion), phosphorylat (-e, 
-ed, -es, -ing, -ion, -ory), prim (-e, -ed, -es, -ing), 
process (-, -ed, -es, -ing), react (-, -ed, -ing, -ion, 
-or, -ors, -ory, s), regulat (-e, -ed, -es, -ing, 
-ion, ,-or, -ory), relat (-e, -ed, -es, -ing, -ion), signal 
(-, -ed, -ing, , -led, -ling, -s), stimulat (-e, -ed, -es, 
-ing, -ion, ,-or, -ory), suppress (-, -ed, -es, -ing, 
-ion), transduc (-e, -ed, -es, -ing, -tion, ,-tor, -tory), 
trigger (-, -ed, -ing, -s) 
Learning Formulation and Transformation Rules for  
Multilingual Named Entities 
Hsin-Hsi Chen Changhua Yang Ying Lin 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, TAIWAN, 106 
{hh_chen, d91013, b88034}@csie.ntu.edu.tw 
Abstract 
This paper investigates three multilingual 
named entity corpora, including named 
people, named locations and named 
organizations.  Frequency-based 
approaches with and without dictionary 
are proposed to extract formulation rules 
of named entities for individual languages, 
and transformation rules for mapping 
among languages.  We consider the issues 
of abbreviation and compound keyword at 
a distance.  Keywords specify not only the 
types of named entities, but also tell out 
which parts of a named entity should be 
meaning-translated and which part should 
be phoneme-transliterated.  An 
application of the results on cross 
language information retrieval is also 
shown. 
1 Introduction 
Named entities are major components of a 
document.  Capturing named entities is a 
fundamental task to understanding documents 
(MUC, 1998).  Several approaches have been 
proposed to recognize these types of terms.  For 
example, corpus-based methods are employed to 
extract Chinese personal names, and rule-based 
methods are used to extract Chinese date/time 
expressions and monetary and percentage 
expressions (Chen and Lee, 1996; Chen, Ding and 
Tsai, 1998).  In the past, named entity extraction 
mainly focuses on general domains and is 
employed to various applications such as 
information retrieval (Chen, Ding and Tsai, 1998), 
question-answering (Lin, et al, 2001), and so on.  
Recently, several attempts have been extended to 
mine knowledge from biomedical documents 
(Hirschman, et al, 2002). 
Most of the previous approaches dealt with 
monolingual named entity extraction.  Chen et al 
(1998) extended it to cross-language information 
retrieval.  A grapheme-based model was proposed 
to compute the similarity between Chinese 
transliteration name and English name.  Lin and 
Chen (2000) further classified the works into two 
directions ? say, forward transliteration (Wan and 
Verspoor, 1998) and backward transliteration 
(Chen et al, 1998; Knight and Graehl, 1998), and 
proposed a phoneme-based model.  Lin and Chen 
(2002) employed a machine learning approach to 
determine phonetic similarity scores for machine 
transliteration.  AI-Onaizan and Knight (2002) 
investigated the translation of Arabic named 
entities to English using monolingual and bilingual 
resources. 
The past works on multilingual named entities 
emphasizes on the transliteration issues.  However, 
the transformation between named entities in 
different languages is not transliteration only.  The 
mapping may be a combination of meaning 
translation and/or phoneme transliteration.  The 
following five English-Chinese examples show 
this issue.  The symbol A ? B denotes a foreign 
name A is translated and/or transliterated into a 
Chinese name B. 
 
(s1) Victoria Fall  
? ?????? (wei duo li ya pu bu) 
(s2) Little Rocky Mountains 
? ????? (xiao luo ji shan mo) 
(s3) Great Salt Lake ? ??? (da yan hu) 
(s4) Kenmare ? ??? (kang mei er) 
(s5) East Chicago ? ???? (dong zhi jia ge) 
 
Example (s1) shows a name part (i.e., Victoria) 
and a keyword part (i.e., Fall) of a named location 
are transliterated and translated into ?????? 
(wei duo li ya) and ???? (pu bu), respectively.  
In Example (s2), the keyword part (i.e., Mountains) 
is still translated, i.e., ???? (shan mo), however, 
some part of name is translated (i.e., Little ? ??? 
(xiao)) and another part is transliterated (i.e., 
Rocky ? ???? (luo ji)).  Example (s3) shows an 
extreme case.  All the three words are translated 
(i.e., Great ? ??? (da)), Salt ? ??? (yan), Lake 
? ??? (hu)).  Examples (s4) and (s5) show two 
location names without keywords.  The former is 
transliterated and the latter is a combination of 
transliteration and translation. 
Which part is translated and which part is 
transliterated depends on the type of named entities.  
For example, personal names tend to be 
transliterated.  For a location name, name part and 
keyword part are usually transliterated and 
translated, respectively.  The organization names 
are totally different.  Most of constituents are 
translated.  Besides the issue of the named entity 
types, different language pairs have different 
transformation rules.  German named entity has 
decompounding problem when it is 
translated/transliterated, e.g., Bundesbahn ? ??
????? (lian bang tie lu ju) and Bundesbank ? 
?????? (lian bang yin hang). 
This paper will study the issues of languages 
and named entity types on the choices of 
translation and transliteration.  We focus on three 
more challenging named entities only, i.e., named 
people, named locations and named organizations.  
Three phrase-aligned corpora will be adopted ? say, 
a multilingual personal name corpus and a 
multilingual organization name corpus compiled 
by Central News Agency (abbreviated CNA 
personal name and organization corpora hereafter), 
and a multilingual location name corpus compiled 
by National Institute for Compilation and 
Translation of Taiwan (abbreviated NICT location 
name corpus hereafter).  We will extract 
transliteration/translation rules from these 
multilingual named corpora.  This paper is 
organized as follows.  Section 2 introduces the 
corpora used.  Section 3 shows how to extract 
formulation rules and the transformation rules.  
Section 4 analyzes the results.  Section 5 
demonstrates the application of the extracted rules 
on cross language information retrieval.  Section 6 
concludes the remarks. 
2 Multilingual Named Entity Corpora 
NICT location name corpus which was developed 
by Ministry of Education of Taiwan in 1995 
collected 19,385 foreign location names.  Each 
entry consists of three parts, including foreign 
location name, Chinese transliteration/translation 
name, and country name, e.g., (Victoria Fall, ??
?????? (wei duo li ya pu bu), South Africa), 
(Little Rocky Mountains, ??????? (xiao luo 
ji shan mo), USA), etc.  The foreign location 
names are in English alphabet.  Some location 
names denoting the same city have more than one 
form like Firenze and Florence for a famous Italian 
city.  The former is an Italian name and the latter is 
its English name.  They correspond to two 
different transliterations in Chinese, respectively, 
i.e., ????? (fei leng cui) and ?????? (fo 
luo lun si).  The pronunciation of the foreign 
names in NICT corpus is based on Webster?s New 
Geographic Dictionary.  The foreign name itself 
may be a transliteration name.  A Japanese city is 
transliterated in English alphabet, but its 
corresponding translation name is in Kanji (Hanzi 
in Japanese).  It is hard to capture their 
relationships except dictionary lookup, so that 
Japanese location name is out of our discussion.  
We employ the country field to select the 
translation/transliteration pairs that we will deal 
with in this paper.  Table 1 summarizes the 
statistics of NICT corpus based on country tags. 
 
Table 1. Statistics of NICT Corpus 
Country Frequency Percentage Country Frequency Percentage
USA 3,012 15.5% Korea 574 3.0%
UK 1,073 5.5% Brazil 433 2.2%
Russia 961 5.0% German 395 2.0%
Japan 796 4.1% Italy 379 2.0%
Canada 692 3.6% Spain 370 1.9%
France 679 3.5% Mexico 324 1.7%
India 679 3.5% Others 8,413 43.5%
Australia 603 3.1% Total 19,385 100%
 
CNA personal name and organization corpora 
are used by news reporters to unify the name 
transliteration/translation in news stories.  There 
are 50,586 pairs of foreign personal names and 
Chinese transliteration/translation in persona name 
corpus.  Different from NICT corpus, there do not 
exist clear cues to identify the nationality of named 
people.  Thus, we could not exclude the Japanese 
names like ?Hayakawa? and the corresponding 
name ??? ? (zao chuan) from our discussion 
automatically.  There are 14,658 named 
organizations in CNA corpus.  Some organization 
names are tagged with the country names to which 
they belong.  For example, ?Aachen Technical 
University? ? ?????? (ya ken ji shu da 
xue) (Germany).  But not all the organization 
names have such country tags.  Comparatively, 
organization names are longer than the other two 
named entities.  Table 2 shows the statistics of 
NICT organization name corpus.  FL denotes the 
length of foreign names in words, CL denotes the 
length of Chinese names in characters, and Count 
denotes the number of foreign names of the 
specified length. 
3 Rule Mining 
3.1 Frequency-Based Approach with a 
Bilingual Dictionary 
We postulate that a transliterated term is usually an 
unknown word, i.e., not listed in a lexicon and a 
translated term often appears in a lexicon.  Under 
this postulation, a translated term occurs more 
often in a corpus, and comparatively, a 
transliterated term only appears very few. 
A simple frequency-based method will 
compute the frequencies of terms and use them to 
tell out the transliteration and translation parts in a 
named entity.  Because Chinese has segmentation 
problem, we start the frequency computation from 
the foreign name part in a multilingual named 
entity corpus.  The method is sketched as follows. 
(1) Compute the word frequencies of each 
word in the foreign name list. 
(2) Keep those words that appear more than a 
threshold and appear in a common foreign 
dictionary (e.g., an English dictionary).  These 
words form candidates of simple keywords. 
(3) Examine the foreign word list again.  
 
Table 2. Statistics of CNA Organization Corpus 
FL Count CL FL Count CL FL Count CL
1 1,773 4.73 7 425 9.94 13 10 14.20 
2 3,622 4.98 8 223 10.50 14 6 12.00 
3 3,751 6.30 9 122 10.98 15 5 17.00 
4 2,406 7.28 10 53 11.57 16 2 14.50 
5 1,434 8.27 11 32 13.41 18 1 9.00 
6 775 8.97 12 17 12.35 20 1 15.00 
 
Those word strings that are composed of simple 
keyword candidates are candidates of compound 
keywords.  We find out the compound keyword set 
by using collocation metric by selecting the most 
frequently occurring compounds through the well-
known elimination of prepositions. 
(4)Because the experimental corpus is aligned, 
we can cluster the Chinese name list based on 
foreign keywords.  For each Chinese name cluster, 
we try to identify the Chinese keyword sets.  Here 
a bilingual dictionary may be consulted. 
The above algorithm extracts foreign/Chinese 
keyword sets from a multilingual named entity 
corpus.  In the meantime, formulation rules for 
foreign names and Chinese counterparts are mined.  
A complete foreign name and a complete Chinese 
name are mapped into name-keyword combination.  
By the way, which method, translation or 
transliteration, is used is also determined. 
Take NICT location name corpus as an 
example.  The terms of frequencies greater than 20 
include River (?, he), Island (?, dao), Lake (?, 
hu), Mountain (?, shan), Bay (?, wan), Mountain 
(?, feng), Peak (?, feng), Islands (??, qun dao), 
Mountains (??, shan mo), Cape (?, jiao), City 
(?, cheng), Range (?, ling), Peninsula (??, ban 
dao), Point (?, jiao), Strait (??, hai xia), River 
(?, chuan), Gulf (?, wan), Cape (?, jia), Pass 
(?? , shan kou), Plateau (?? , gao yuan), 
Headland (?, jia), Harbor (?, gang), Sea (?, hai), 
Promontory (?, jia), and Hills (??, qui ling).  
On the one hand, a foreign location keyword, e.g., 
?Mountain?, may correspond to two Chinese 
location keywords, e.g., ?? ? (shan) and ?? ? 
(feng).  On the other hand, the same Chinese 
location keyword ??? (feng) can be translated into 
two English location keywords ?Mountain? and 
?Peak?. 
Similarly, suffix and prefix for organization 
names can be extracted from CNA organization 
name corpus.  Some high frequent keywords are 
shown as follows. 
(1) Suffix 
Party (?, dang), Association (??, xie 
hui), University (??, da xue), Co. (??, gong 
si), Committee (???, wei yuan hui), Company 
(??, gong si), Bank (??, yia hang), etc. 
(2) Prefix 
International (??, guo ji), World (??, 
shi jie), American (??, mei guo), National (??, 
quan guo), Japan (??, ri ben), National (??, 
guo jia), Asian (??, ya zhou), etc. 
3.2 Keyword Extraction without a Bilingual 
Dictionary 
At the step (4) of the algorithm in Section 3.1, a 
bilingual dictionary is required.  Because 
abbreviation is common adopted in translation, 
dictionary-based approach is hard to capture this 
phenomenon.  A named organization ?World 
Taiwanese Association? which is translated into 
????? (shi tai hui) is a typical example.  The 
term ?World? is translated into an abbreviated term 
??? (shi) rather than a complete term ???? (shi 
jie).  Here another approach without dictionary is 
proposed.  Suppose there are M pairs of (foreign 
name, Chinese name) in a multilingual named 
entity corpus.  The jth pair, 1 ? j ? M, is denoted by 
{Ej, Cj}, where Ej is a foreign named entity, and Cj 
is a Chinese named entity.  Then some Chinese 
segment c ? Cj should be associated with some 
foreign segment e ?  Ej.  Consider the following 
examples. 
 
(s6) Aletschhorn Mountain ? ?????? 
(s7) Catalan Mountain ? ????  
(s8) Cook Strait ? ????  
(s9) Dover, Strait of ?????  
 
We will align ??? (shan) and ???? (hai xia) to 
Mountain and Strait, respectively, from these 
examples. 
We further decompose the named entities.  If 
a named entity Ej comprises m words w1?w2?wm, 
then a candidate segment ep, q is defined as wp ? wq, 
where 1 ? p ? q ? m.  If a Chinese named entity Cj 
has n syllables s1?s2?sn, then a candidate segment 
cx, y is defined as sx ? sy, where 1 ? p ? q ? n.  
Theoretically, we can get 
2
)1(
2
)1( +?+ nnmm pairs of 
{ep, q, cx, y} from {Ej, Cj}.  We then group the pairs 
collected from the multilingual named entity list 
and count the frequency for each occurrence.  
Those pairs with higher frequency denote 
significant segment pairs.  In the above examples, 
both the two pairs {Mountain, ??? (shan)} and 
{Strait, ???? (hai xia)} appear twice, while the 
other pairs appear only once. 
All the pairs {e, c} whose frequency > 2 are 
kept.  Two issues have to be addressed.  The first is: 
redundancy which may exist in the pairs of 
segments should be eliminated carefully.  If a pair 
{e, s1 s2 ? st} occurs k times, then the frequency 
of t?(t+1)/2 substrings (1 ? u ? v ? t) is at least k.  
The second is: e may be translated to more than 
one synonym, which has the same prefix, suffix, or 
infix.  In examples (s10) and (s11), ?Association? 
may be translated into ???? (xie hui) and ???
?? (lian yi hui), where ??? (hui) is a common 
suffix of these two translation equivalents, so that 
its frequency is more than the translation 
equivalents. 
 
(s10) World Trade Association ? ?????? 
(s11) North Europe Chinese Association ?  
??????? 
 
These two issues may be mixed together to make 
this problem more challengeable. 
A metric to deal with the above issues is 
proposed.  The concept is borrowed from tf?idf 
scheme in information retrieval to measure the 
alignment of each foreign segment and the possible 
Chinese translation segments.  Assume there are N 
foreign segments.  Term frequency (tf) of a 
Chinese translation segment ci in e denotes the 
number of occurrences of ci in e.  Document 
frequency (df) of ci is the number of foreign 
segments that ci is translated to.  We prefer to the 
Chinese translation segment that occur frequently 
in a specific foreign segment, but rarely in the 
remainder of foreign segments.  Besides, we also 
prefer the longer Chinese segment, so that the 
length of a Chinese segment, i.e., |ci|, is also 
considered.   
=}),({ icescore  
)1|(|log)(}),({ 2 +?? iii ccidfcef   (1) 
}),{(max
}),({}),({
jj
i
i cetf
cetfcef =   (2) 
)
)(
(log)( 2
i
i cdf
Ncidf = ,   (3) 
For some e, the corresponding Chinese segment c 
is obtained by equation (4). 
}),({maxarg i
c
cescorec
i
=   (4) 
In this way, we can produce a ranking list of pairs 
of (foreign segment, Chinese segment), which 
form multilingual keyword pairs. 
3.3 Extraction of Transformation Rules 
We apply the keyword pairs extracted in the last 
section to the original named entity list.  In (s6)-
(s9), (mountain, ? (shan)) and (strait, ?? (hai 
xia)) are significant keyword pairs.  We replace the 
non-keywords of Ej and Cj with patterns ? and ?, 
respectively, get the following rules. 
 
(s6?) ? mountain ? ? ? 
(s7?) ? mountain ? ? ? 
(s8?) ? Strait ? ??? 
(s9?) ?, Strait of ? ??? 
 
(s6?) and (s7?) can be grouped into a rule.  As a 
result, a set of transformation rules can be 
formulated.  From these examples, Chinese 
location name keyword tends to be located in the 
rightmost and the remaining part is a transliterated 
name.  On the counterpart, foreign location name 
keyword tends to be either located in the rightmost, 
or permuted by some prepositions, comma, and the 
transliterating part. 
3.4 Extraction of Keywords at a Distance 
The algorithm proposed in Section 3.2 can deal 
with single keywords and connected compound 
keywords.  Now we will extend it to keywords at a 
distance.  Consider examples (s12)-(s15) at first. 
 
(s12) American Podiatric medical Association  
? ???????? 
(s13) American Public Health Association 
? ???????? 
(s14) American Society for Industrial Security 
? ???????? 
 (s15) American Society of Newspaper Editors  
? ????????? 
 
(s12) and (s13) show that an English compound 
keyword is separated and so is its corresponding 
Chinese counterpart.  In contrast, the English 
compound keyword is connected in (s14) and (s15), 
but the corresponding Chinese translation is 
separated.  The phenomenon appears quite often in 
the translation of organization names. 
We introduce a symbol ? to cope with the 
distance issue.  The original algorithm is modified 
as follows.  A candidate segment cp, q is defined as 
a string that begins with sp and ends with sq.  Each 
syllable from sp-1 to sq-1 can be replaced by ?.  
Therefore, both ep, q and cx, y are extended to 2(p-q-1), 
and 2(x-y-1) instances, respectively.  For example, 
the following shows some additional instances for 
?American Civil Liberties Union?. 
 
?American ? Liberties Union? 
?American Civil ? Union? 
?American ? Union? 
 
The scoring method, i.e., formulas (1)-(4), is still 
applicable for the new algorithm.  Nevertheless, 
the complexity is different.  The complexity of the 
original algorithm is O(m2n2), but the complexity 
of the algorithm here is O(2m2n), where m is the 
word count for a foreign named entity and n is the 
character count for a Chinese named entity. 
The mining procedure is performed only 
once, and the mined rules are employed in an 
application without being recomputed.  Thus, the 
running time is not the major concern of this paper.  
Besides, the N is bounded in a reasonable small 
number because the length of a named entity is 
always rather shorter than that of a sentence.  Table 
2 shows that 93.88% of foreign names in CNA 
organization name corpus consist of less than 7 
words. 
4 Experimental Results 
The algorithm in Section 3.2 was performed on 
NICT location name corpus, and CNA personal 
name and organization corpora.  With this 
algorithm, we can produce a ranking list of pairs of 
(foreign segment, Chinese segment), which form 
multilingual keyword pairs.  Individual foreign 
segments and Chinese segments are regarded as 
formulation rules for foreign languages and 
Chinese, respectively.  When both the two  
Table 3. Learning Statistics 
 NICT LOC CNA ORG CNA PER
# of records in corpus 18,922 14,658 50,586
# of records for learning 5,714 12,885 100 
Vocabulary size 18,220 11,542 50,315
# of keyword pairs 122 5,229 12 
# of transformation rules 230   
# of successful records 4,262   
 
segments are considered together, they form a 
transformation rule.  Table 3 summarizes the 
results using the frequency-based approach without 
dictionary.  For named locations, there are 18,922 
records, of which, only 5714 records consist of 
more than one foreign word.  In other words, 
13,208 named locations are single words, and they 
are unique, so that we cannot extract keywords 
from these words.  Total 122 keyword pairs are 
identified.  We classify these keyword pairs into 
the following types: 
 
(1) Meaning translation 
Total 69 keywords belong to this type.  It 
occupies 56.56%.  They are further 
classified into three subtypes. 
(a) common location keywords 
 Besides the English location 
keywords mentioned in Section 3.1, 
some location keywords in other 
languages are also captured, including 
Bir ? ? (jing), Ain ? ? (quan), 
Bahr ? ? (he), Cerro ? ? (shan), 
etc. 
(b) direction (e.g., Low  ? ?  (xia), 
Central ? ? (zhong), East  ? ? 
(dong), etc.), size (e.g., Big ? ? 
(da)), length (e.g, Long ? ? 
(zhang)), color (e.g., Black ? ? 
(hei), Blue ? ? (lan), etc.) 
(c) the specificity of place or area such as 
Crystal ? ?? (jie jing), Diamond 
? ?? (zuan shi), etc.  
(2) Phoneme transliteration keywords 
Some morphemes are transliterated such as 
el ? ? (la), Dera ? ?? (de la), Monte  
? ?? (meng te), Los ? ?? (luo si), 
Le ? ? (le), and so on.  Besides, some 
common transliteration names are also 
regarded as keywords, e.g., Elizabeth ? 
???? (yi li sha bai), Edward ? ??? 
(ai de hua), etc.  Total 39 terms belong to 
this type.  It occupies 31.97%. 
(3) Some keywords in type (1) are 
transliterated.  For example, Bay ? ? 
(Bay), Beach ? ?? (bi qi), mountain ? 
?? (meng tan), Little ? ?? (li te), etc.  
Total 14 keywords (11.48%) are extracted. 
Total 230 transformation rules are mined from 
the NICT location corpus.  On the average, a 
keyword pair corresponds to 1.89 transformation 
rules.  Consider a keyword pair mountain ? ? 
(shan) as an example.  Four transformation rules 
shown as follows are learned, where ? and ? 
denote keywords for foreign language and Chinese, 
respectively; ? is a Chinese transliteration of a 
foreign fragment ?; the number enclosed in 
parentheses denotes frequency the rule is applied. 
(1) ?? ? ?? (234) 
(2) ?, ? ? ?? (45) 
(3) ?, ?? ? ?? (1) 
(4) ??? ? ?? (1) 
When we apply the 230 transformation rules back 
to the 5,714 named locations, we can tell out which 
part is transliterated and which part is translated 
from 4,262 named locations.  It confirms our 
postulation that a named location is composed of 
two parts, i.e., one is translated and the other one is 
transliterated. 
Comparatively, there are 50,586 personal 
names in CNA personal names, but only 100 
named people are composed of more than one 
word.  The number of keywords extracted is only a 
few.  They are listed below. 
De ? ? (dai), La ? ? (la), De La ? ?? 
(dai la), Van Der ? ?? (fan de), Du ? ? (du), 
David ? ?? (da wei), Khan ? ? (han), Del ? 
? (dai), Le ? ? (le), Van Den ? ?? (fan 
deng), Di ? ? (di) 
It shows that personal names tend to be 
transliterated and the CNA personal name corpus 
is suitable for training the similarity scores among 
phonetic characters (Lin and Chen, 2002). 
Finally, we consider the named organizations. 
There are 14,658 records in CNA organization 
corpus.  Total 12,885 organization names are 
composed of more than one word.  The percentage, 
87.90%, is the highest among these three corpora.  
Besides that, 5,229 keyword pairs are extracted.  
Most of the keyword pairs are meaning translated.  
This set is also the largest among the three corpora.  
Thus, the keyword pairs are too small and too large 
to find suitable transformation rules for personal 
names and organization names, respectively.  
Although the original idea of our algorithm is 
universal for languages, it should be modified 
slightly for some specific languages.  The 
following takes German as examples.  German 
words have cases and genders.  Most of German 
words are compound.  Consider examples (s16)-
(s19). 
 
(s16) Neue Osnabruecker ? ???????
(s17) Neues Deutschland ? ??? 
(s18) Bundesbahn ? ????? 
(s19) Bundesbank ? ???? 
 
The first two examples show the German adjective 
Neu (New) has different suffixes such as ?-e? and 
?-es? according to the case and gender of the noun.  
The last two examples suggest that morphological 
analysis for decompounding the words into 
meaningful segments is necessary before our 
algorithm. 
 
5 Application on CLIR 
Cross language information retrieval (CLIR) 
facilitates using queries in one language to access 
documents in another.  Because named entities are 
key components of a document, they are usually 
targets that users are interested in.  Figure 1 shows 
an application of the extracted formulation rules 
and transformation rules on Chinese-Foreign CLIR.  
For each document in the Foreign collection, 
named entities are recognized and classified by 
using formulation rules.  They form important 
indices for the related documents.  When a Chinese 
query is issued, the system extracts the possible 
Chinese named entities according to Chinese 
formulation rules.  If keywords are specified in a 
query, we know the structure and the type of the 
named entity.  The lexical structure tells us which 
part is translated and which part is transliterated.   
The backward transliteration method proposed 
by Lin and Chen (2000, 2002) was followed to 
select the most similar English named entity and 
the related documents at the same time.  In Lin and 
Chen?s approach, both Chinese name and English 
candidates will be transformed into a canonical 
form in terms of International Phonetic Alphabets.  
Similarity computation among Chinese query term 
and English candidates are done on phoneme level.  
Figure 1.  A Chinese-Foreign CLIR System 
Foreign 
Document 
Collection 
Query 
Translation/ 
Transliteration 
Information 
Retrieval 
System 
Relevant
Documents
Named Entity
Extractor
Transliteration 
Knowledge 
Bilingual 
Dictionary 
Chinese Query
Chinese-Foreign
Transformation
Rules
Chinese
Formulation
Rules
Foreign
Formulation
Rules
Rule 
Miner 
Multi-Lingual 
Named Entity 
Corpora 
Named Entity
Extractor
That is an expensive operation.  Hopefully, the 
type of Chinese named entity will help to narrow 
down the number of candidate.   
6 Conclusion and Remarks 
This paper proposes corpus-based approaches to 
extract the formulation rules and the translation/ 
transliteration rules among multilingual named 
entities.  Simple frequency-based method identifies 
keywords of named entities for individual 
languages and their correspondence.  The modified 
tf?idf scheme deals with the issues of abbreviation 
and compound keyword at a distance. 
Since the corpora are already phrase-aligned, 
the mined rules cover at least a significant number 
of instances.  That is, they seem to be significant, 
but further evaluation is needed.  Two types of 
evaluation are being conducted, i.e., direct and 
indirect approaches.  In the former, we will 
partition the corpora into two parts, one for 
training and the other one for testing.  In the latter, 
we are integrating our method in a cross language 
information retrieval system.  Given a query 
consisting of Chinese named entity, the Chinese 
formulation rules will tell us its type and lexical 
structures.  The transformation rules show which 
parts should be translated and transliterated.  Our 
previous works on phoneme transliteration is 
integrated.  The transformation result may be 
submitted to an information retrieval system to 
access documents in another language.  In the 
ongoing evaluation, the test bed is supported by 
CLEF (2003).  The result will be reported in 
CLEF2003 after evaluation by CLEF organizer.  
Further applications will be explored in the future 
and the methodology will be extended to other 
types of named entities. 
 
References 
Al-Onaizan, Yaser and Knight, Kevin (2002) 
?Translating Named Entities Using Monolingual and 
Bilingual Resources,? Proceedings of 41st Annual 
Meeting of Association for Computational Linguistics, 
2002, pp. 400-408. 
Chen, Hsin-Hsi and Lee, Jen-Chang (1996) 
?Identification and Classification of Proper Nouns in 
Chinese Texts,? Proceedings of 16th International 
Conference on Computational Linguistics, 1996, pp. 
222-229. 
Chen, Hsin-Hsi; Ding, Yung-Wei and Tsai, Shih-Chung 
(1998) ?Named Entity Extraction for Information 
Retrieval,? Computer Processing of Oriental 
Languages, Special Issue on Information Retrieval 
on Oriental Languages, 12(1), 1998, pp. 75-85. 
Chen, Hsin-Hsi et al (1998) ?Proper Name Translation 
in Cross-Language Information Retrieval,? 
Proceedings of 17th COLING and 36th ACL, pp. 232-
236. 
CLEF (2003) Cross-Language Retrieval in Image 
Collections, Pilot Experiments, 2003. 
Hirschman, L.; Park, J.C.; Tsujii, J.; Wong, L. and Wu, 
C.H. (2002) ?Accomplishments and Challenges in 
Literature Data mining for Biology,? Bioinformatics, 
18(12), pp. 1553-1561. 
Knight, Kevin and Graehl, Jonathan (1998) ?Machine 
Transliteration,? Computational Linguistics, 24(4), 
pp. 599-612. 
Lin, Chuan-Jie; Chen, Hsin-Hsi; et al (2001) ?Open 
Domain Question Answering on Heterogeneous 
Data,? Proceedings of ACL Workshop on Human 
Language Technology and Knowledge Management, 
2001, pp. 79-85. 
Lin, Wei-Hao and Chen, Hsin-Hsi (2000) ?Similarity 
Measure in Backward Transliteration between 
Different Character Sets and Its Application to 
CLIR,? Proceedings of Research on Computational 
Linguistics Conference XIII, pp. 79-113. 
Lin, Wei-Hao and Chen, Hsin-Hsi (2002) ?Backward 
Machine Transliteration by Learning Phonetic 
Similarity,? Proceedings of 6th Conference on 
Natural Language Learning, 2002. 
MUC (1998) Proceedings of 7th Message 
Understanding Conference, 1998, 
http://www.itl.nist.gov/iaui/894.02/related_projects/
muc/index.html. 
Wan, Stephen and Verspoor, Cornelia Maria (1998) 
?Automatic English-Chinese Name Transliteration 
for Development of Multilingual Resources,? 
Proceedings of 17th COLING and 36th ACL, pp. 
1352-1356. 
 
Event Clustering on Streaming News  
Using Co-Reference Chains and Event Words 
June-Jei Kuo 
Department of Computer Science and 
Information Engineering 
National Taiwan University, Taipei, Taiwan
jjkuo@nlg.csie.ntu.edu.tw 
Hsin-Hsi Chen 
Department of Computer Science and 
Information Engineering  
National Taiwan University, Taipei, Taiwan 
hh_chen@csie.ntu.edu.tw 
 
Abstract 
Event clustering on streaming news aims to 
group documents by events automatically.  
This paper employs co-reference chains to 
extract the most representative sentences, and 
then uses them to select the most informative 
features for clustering.  Due to the long span 
of events, a fixed threshold approach prohibits 
the latter documents to be clustered and thus 
decreases the performance.  A dynamic 
threshold using time decay function and 
spanning window is proposed.  Besides the 
noun phrases in co-reference chains, event 
words in each sentence are also introduced to 
improve the related performance.  Two models 
are proposed.  The experimental results show 
that both event words and co-reference chains 
are useful on event clustering. 
1 Introduction 
News, which is an important information source, 
is reported anytime and anywhere, and is 
disseminated across geographic barriers through 
Internet.  Detecting the occurrences of new events 
and tracking the processes of the events (Allan, 
Carbonell, and Yamron, 2002) are useful for 
decision-making in this fast-changing network era.  
Event clustering automatically groups documents 
by events that are specified in the documents in a 
temporal order.  The research issues behind event 
clustering include: how many features can be used 
to determine event clusters, which cue patterns can 
be employed to relate news stories in the same 
event, how the clustering strategies affect the 
clustering performance using retrospective data or 
on-line data, how the time factor affects clustering 
performance, and how multilingual data is 
clustered. 
Chen and Ku (2002) considered named entities, 
other nouns and verbs as cue patterns to relate 
news stories describing the same event.  A 
centroid-based approach with a two-threshold 
scheme determines relevance (irrelevance) 
between a news story and a topic cluster.  A least-
recently-used removal strategy models the time 
factor in such a way that older and unimportant 
terms will have no effect on clustering.  Chen, Kuo 
and Su (2003) touched on event clustering in 
multilingual multi-document summarization.  They 
showed that translation after clustering is better 
than translation before clustering, and translation 
deferred to sentence clustering, which reduces the 
propagation of translation errors, is most promising. 
Fukumoto and Suzuki (2000) proposed concepts 
of topic words and event words for event tracking.  
They introduced more semantic approach for 
feature selection than the approach of parts of 
speech.  Wong, Kuo and Chen (2001) employed 
these concepts to select informative words for 
headline generation, and to rank the extracted 
sentences in multi-document summarization (Kuo, 
Wong, Lin, and Chen, 2002).   
Bagga and Baldwin (1998) proposed entity-
based cross-document co-referencing which uses 
co-reference chains of each document to generate 
its summary and then use the summary rather than 
the whole article to select informative words to be 
the features of the document.  Azzam, Humphreys, 
and Gaizauskas (1999) proposed a primitive model 
for text summarization using co-reference chains 
as well.  Silber and McCoy (2002) proposed a text 
summarization model using lexical chains and 
showed that proper nouns and anaphora resolution 
is indispensable. 
The two semantics-based feature selection 
approaches, i.e., co-reference chains and event 
words, are complementary in some sense.  The 
former denotes equivalence classes of noun 
phrases, and the latter considers both nominal and 
verbal features, which appear across paragraphs.  
This paper will employ both co-reference chains 
and event words for temporal event clustering.  An 
event clustering system using co-reference chains 
is described in Section 2.  The evaluation method 
and the related experimental results are described 
in Section 3.  The event words are introduced and 
discussed in Section 4.  Section 5 proposes a 
summation model and a two-level model, 
respectively for event clustering using both co-
reference chains and event words.  Section 6 
concludes the remarks. 
2 Event Clustering using Co-Reference 
Chains 
A co-reference chain in a document denotes an 
equivalence class of noun phrases. (Cardie and 
Wagstaff, 1999)  A co-reference resolution 
procedure is first to find all the possible NP 
candidates.  It includes word segmentation, named 
entity extraction, part of speech tagging, and noun 
phrase chunking.  Then the candidates are 
partitioned into equivalence classes using the 
attributes such as word/phrase itself, parts of 
speech of head nouns, named entities, positions in 
a document, numbers (singular, plural, or 
unknown), pronouns, gender (female, male, or 
unknown), and semantics of head nouns.  As the 
best F-measure of automatic co-reference 
resolution in English documents in MUC-7 was 
61.8% (MUC, 1998), a corpus hand-tagged with 
named entities, and co-reference chains are 
prepared and employed to examine the real effects 
of co-reference chains in event clustering r. 
Headlines of a news story can be regarded as its 
short summary.  That is, the words in the headline 
represent the content of a document in some sense.  
The co-reference chains that are initiated by the 
words in the headlines are assumed to have higher 
weights.  A sentence which contains any words in 
a given co-reference chain is said to ?cover? that 
chain.  Those sentences which cover more co-
reference chains contain more information, and are 
selected to represent a document.  Each sentence in 
a document is ranked according to the number of 
co-reference chains that it covers.  Five scores 
shown below are computed.  Sentences are sorted 
by the five scores in sequence and the sentences of 
the highest score are selected.  The selection 
procedure is repeated until the designated number 
of sentences, e.g., 4 in this paper, is obtained. 
(1) For each sentence that is not selected, count 
the number of noun co-reference chains from 
the headline, which are covered by this 
sentence and have not been covered by the 
previously selected sentences. 
(2) For each sentence that is not selected, count 
the number of noun co-reference chains from 
the headline, which are covered by this 
sentence, and add the count to the number of 
verbal terms in this sentence which also appear 
in the headline. 
(3) For each sentence that is not selected, count 
the number of noun co-reference chains, which 
are covered by this sentence and have not been 
covered by the previously selected sentences. 
(4) For each sentence that is not selected, count 
the number of noun co-reference chains, which 
are covered by this sentence, and add the count 
to the number of verbal terms in this sentence 
which also appear in the headline. 
(5) The position of a sentence 
Score 1 only considers nominal features.  
Comparatively, Score 2 considers both nominal 
and verbal features together.  Both scores are 
initiated by headlines.  Scores 3 and 4 consider all 
the co-reference chains no matter whether these 
chains are initiated by headlines or not.  These two 
scores ranks those sentences of the same scores 1 
and 2.  Besides, they can assign scores to news 
stories without headlines.  Scores 1 and 3 are 
recomputed in the iteration.  Finally, since news 
stories tend to contain more information in the 
leading paragraphs, Score 5 determines which 
sentence will be selected according to position of 
sentences, when sentences are of the same scores 
(1)-(4).  The smaller the position number of a 
sentence is, the more it will be preferred. 
The sentences extracted from a document form a 
summary for this document.  It is in terms of a 
term vector with weights defined below.  It is a 
normalized TF-IDF. 
22
2
2
1
log
inii
j
ij
ij
sss
df
Ntf
w +???++
?
=   (1) 
where tfij is frequency of term tj in summary i, N 
is total number of summaries in the 
collection being examined, dfj is number 
of summaries that term tj occurs, and sij 
denotes the TF-IDF value of term tj in 
summary i. 
A single-pass complete link clustering algorithm 
incrementally divides the documents into several 
event clusters.  We compute the similarities of the 
summary of an incoming news story with each 
summary in a cluster.  Let V1 and V2 be the vectors 
for the two summaries extracted from documents 
D1 and D2.  The similarity between V1 and V2 is 
computed as follows. 
??
?
==
?
=
m
j j
n
j j
common
jj
ww
ww
VVSim
1
2
21
2
1
 t term
21
21
j),(  (2) 
If all the similarities are larger than a fixed 
threshold, the news story is assigned to the cluster.  
Otherwise, it forms a new cluster itself.  Life span 
is a typical phenomenon for an event.  It may be 
very long.  Figure 1 shows the life span of an air 
crash event is more than 100 days.  To tackle the 
long life span of an event, a dynamic threshold 
(d_th) shown below is introduced, where th is an 
initial threshold.  In other words, the earlier the 
documents are put in a cluster, the smaller their 
thresholds are.  Assume the published day of 
document D2 is later than that of document D1. 
th
)/w_sizedist(D
)/w_sizedist(DDDthd ?+
+=
1
1),(_
2
1
21  (3) 
where dist (day distance) denotes the number of 
days away from the day at which the 
event happens, and w_size (window size) 
keeps the threshold unchanged within the 
same window. 
Moreover, we use square root function to 
prevent the dynamic threshold from downgrading 
too fast. 
3 Test Collection 
In our experiment, we used the knowledge base 
provided by the United Daily News 
(http://udndata.com/), which has collected 
6,270,000 Chinese news articles from 6 Taiwan 
local newspaper companies since 1975/1/1.  To 
prepare a test corpus, we first set the topic to be 
?????? (Air Accident of China Airlines), 
and the range of searching date from 2002/5/26 to 
2002/9/4 (stopping all rescue activities).  Total 964 
related news articles, which have published date, 
news source, headline and content, respectively, 
are returned from search engine.  All are in SGML 
format.  After reading those news articles, we 
deleted 5 news articles which have headlines but 
without any content.  The average length of a news 
article is 15.6 sentences.  Figure 1 depicts the 
distribution of the document number within the 
event life span, where the x-axis denotes the day 
from the start of the year.  For example, ?146? 
denotes the day of ?2002/5/26?, which is the 146th 
day of year 2002.   
Then, we identify thirteen focus events, e.g., 
rescue status.  Meanwhile, two annotators are 
asked to read all the 959 news articles and classify 
these articles into 13 events.  If a news article can 
not be classified, the article is marked as ?other? 
type.  A news article which reports more than one 
event may be classified into more than one event 
cluster.  We compare the classification results of 
annotators and consider those consistent results as 
our answer set.  Table 1 shows the distribution of 
the 13 focus events. 
Event Name Number of 
Documents
Fly right negotiation between 
Taiwan and Hong Kong 
20 
Cause of air accident 57 
Confirmation of air accident 6 
Influence on stock market 27 
Influence on insurance fee 11 
Influence on China Airline 8 
Influence on Peng-Hu 
archipelagoes 
26 
Punishment for persons in charge 10 
News reporting 18 
Wreckage found 28 
Remains found 57 
Rescue status 65 
Solatium 34 
Other 664 
Table 1: Focus Events 
We adopt the metric used in Topic Detection 
and Tracking (Fiscus and Doddington, 2002).  The 
evaluation is based on miss and false alarm rates.  
Both miss and false alarm are penalties.  They can 
0
20
40
60
80
100
120
140
160
145 155 165 175 185 195 205 215 225 235 245
Day
nu
mb
er 
of 
do
cs 
/ d
ay
Figure 1. Event Evolution of China Airlines Air Accident (2002/5/26 ~ 2002/9/4) 
measure more accurately the behavior of users who 
try to retrieve news stories.  If miss or false alarm 
is too high, users will not be satisfied with the 
clustering results.  The performance is 
characterized by a detection cost, , in terms of 
the probability of miss and false alarm: 
detC
ettnonFAFAettMissMissDet PPCPPCC argarg ???+??= (4) 
where  and  are costs of a miss and a 
false alarm, respectively,  and  
are the conditional probabilities of a miss 
and a false alarm, and  and 
MissC FAC
MissP FAP
ettP arg( )ettettnon PP argarg 1?=?  are the prior target 
probabilities. 
Manmatha, Feng and Allan (2002) indicated that 
the standard TDT cost function used for all 
evaluations in TDT is C , 
when C
FAMiss PP 098.002.0det +=
Miss, CFA and Ptarget are set to 1, 0.1 and 0.02, 
respectively.  The less the detection cost is, the 
higher the performance is. 
For comparison, the centroid-based approach 
and single pass clustering is regarded as a baseline 
model.  Conventional TF-IDF scheme selects 20 
features for each incoming news articles and each 
cluster uses 30 features to be its centroid.  
Whenever an article is assigned to a cluster, the 30 
words of the higher TF-IDFs are regarded as the 
new centroid of that cluster.  The experimental 
results with various thresholds are shown in Table 
2.  The best result is 0.012990 when the threshold 
is set to 0.05. 
Fixed Threshold Cdet
0.01 0.024644 
0.05 0.012990 
0.10 0.013736 
0.15 0.014331 
0.20 0.015480 
0.25 0.015962 
Table 2: Detection Costs Using Centroid Approach 
Kuo, Wong, Lin and Chen (2002) indicated that 
near 26% of compression rate is suitable for a 
normal reader in multi-document summarization.  
Recall that the average length of a news story is 
15.6 sentences.  Following their postulation, total 4 
sentences, i.e., 16/4, are selected using co-
reference chains.  Table 3 shows the detection cost 
with various threshold settings.  We found that the 
best result could be obtained using threshold 0.05, 
however, it was lower than the result of baseline 
(i.e., 0.013137 > 0.012990). 
Next, we study the effects of dynamic thresholds.  
Three dynamic threshold functions are 
experimented under the window size 1.  A linear 
decay approach removes the square root function 
in Formula (3).  A slow decay approach adds a 
constant (0.05) to Formula (3) to keep the 
minimum threshold to be 0.05 and degrades the 
threshold slowly.  Table 4 shows that Formula (3) 
obtained the best result, and the dynamic threshold 
approach is better than the baseline model. 
Fixed Threshold Cdet
0.01 0.015960 
0.05 0.013137 
0.10 0.015309 
0.15 0.016507 
0.20 0.016736 
0.25 0.017360 
Table 3. Detection Costs Using Co-Reference 
Chains 
Function 
Type 
Linear 
decaying 
Formula 
(3) 
Slow 
Decaying
Cdet 0.013196 0.012657 0.016344
Table 4. Detection Costs with Various Dynamic 
Threshold Functions (Initial Threshold = 0.05) 
Additionally, we evaluate the effect of the 
window size.  Table 5 shows the results using 
various window sizes in Formula (3).  The best 
detection cost, i.e., 0.012647, is achieved under 
window size 2.  It also shows the efficiency of 
dynamic threshold and window size. 
Window 
size 
1 2 3 4 
Cdet 0.012657 0.012647 0.012809 0.012942
Table 5. Detection Costs with Various Window 
Sizes Using Formula (3) (Initial Threshold = 0.05) 
4 Event Clustering Using Event Words 
The co-reference chains in the above approach 
considered those features, such as person name, 
organization name, location, temporal expression 
and number expression.  However, the important 
words ?black box? or ?rescue? in an air crash event 
are never shown in any co-reference chain.  This 
section introduces the concepts of event words.  
Topic and event words were applied to topic 
tracking successfully (Fukumoto and Suzuki, 
2000).  The basic hypothesis is that an event word 
associated with a news article appears across 
paragraphs, but a topic word does not.  In contrast, 
a topic word frequently appears across all news 
documents.  Because the goal of event clustering is 
to extract all the events associated with a topic, 
those documents belonging to the same topic, e.g., 
China Airlines Air Accident, always have the 
similar topic words like ?China Airlines?, ?flight 
611?, ?air accident?, ?Pen-Hu?, ?Taiwan strait?, 
?rescue boats?, etc.  Topic words seem to have no 
help in event clustering.  Comparatively, each 
news article has different event words, e.g., 
?emergency command center?, ?set up?, 
?17:10PM?, ?CKS airport?, ?Commander Lin?, 
?stock market?, ?body recovery?, and so on.  
Extracting such keywords is useful to understand 
the events, and distinguish one document from 
another. 
The postulation by Fukumoto and Suzuki (2002) 
is that the domain dependency among words is a 
key clue to distinguish a topic and an event.  This 
can be captured by dispersion value and deviation 
value.  The former tells if a word appears across 
paragraphs (documents), and the latter tells if a 
word appears frequently.  Event words are 
extracted by using these two values.  Formula (5) 
defines a weight of term t in the i-th story. 
tijj
it
it Ns
N
TFsMax
TFsWs log
)(
?=    (5) 
where TFsit denotes term frequency of term t in 
the i-th story, N is total number of stories, 
and Nst is the number of stories where 
term t occurs. 
Besides term weight in story level, Wpit defines 
a weight of term t in the i-th paragraph.  Formulas 
(6) and (7) define dispersion value and deviation 
value, respectively. 
m
meanWs
DispS
m
i tit
t
?= ?= 1 2)(    (6) 
?? +??=
t
tit
it DispS
meanWsDevS )(    (7) 
Where, meant is average weight of term t in 
story level.  Similarly, DispPt and DevPjt are 
defined in the paragraph level.  The dispersion 
value of term t in the story level denotes how 
frequently term t appears across m stories.  The 
deviation value of term t in the i-th story denotes 
how frequently it appears in a particular story.  
Coefficients ? and ? are used to adjust the number 
of event words.  In our experiments, 20 event 
words are extracted for each document.  In such a 
case, (?, ?) is set to (10, 50) in story level and set 
to (10, 25) in paragraph level, respectively. 
Formula (8) shows that term t frequently appears 
across paragraphs rather than stories.  Formula (9) 
shows that term t frequently appears in the i-th 
story rather than paragraph Pj.  An event word is 
extracted if it satisfies both formulas (8) and (9). 
                   tt DispSDispP <    (8) 
ijj S Psuch that  P allfor                   ?< itjt DevSDevP  (9) 
Below shows the event clustering using event 
words only.  At first, we extract the event words of 
each news article using the whole news collection.  
For each sentence, we then compute the number of 
event words in it.  After sorting all sentences, the 
designated number of sentences are extracted 
according to their number of event words.  In the 
experiments, we use different window sizes to 
study the change of detection cost after introducing 
event words.  Table 6 shows the experimental 
results under the same threshold (0.005) and test 
collection mentioned in Section 3. 
Window 
size 
1 2 3 4 
Cdet 0.011918 0.011842 0.011747 0.011923
Table 6. Detection Costs with Event Words and 
Various Window Sizes 
The results in Table 6 are much better than those 
in Table 5, because inclusion of event words 
selects more informative or representative 
sentences or paragraphs.  The more informative 
feature words documents have, the more 
effectively documents of one event can be 
distinguished from those of another.  In other 
words, the similarities of documents among 
different events become smaller, so that the 
documents cannot be assigned to the same cluster 
easily under the higher threshold, and the best 
performance is shifted from window size 2 to 
window size 3. 
5 Event Clustering Using Both Co-reference 
Chains and Event Words 
According to the above experimental results, it is 
evident that either co-reference chains or event 
words are useful for event clustering on streaming 
news.  As co-reference chains and event words are 
complementary in some sense, we further examine 
the effect on event clustering using both of them.  
Thus, two models called summation model and 
two-level model, respectively, are proposed.  The 
summation model is used to observe the 
summation effect using both the co-reference 
chains and the event words on event clustering.  
On the other hand, the two-level model is used to 
observe the interaction between co-reference 
chains and event words. 
5.1 Summation Model 
In summation model, we simply add the scores 
for both co-reference chains and event words, 
which are described above respectively to be the 
score for each sentence in the news document.  At 
first, we extract the event words of each news 
article using the whole news collection described 
in Section 3.  For each sentence, we then compute 
the number of event words in it, and add this count 
to the number of co-reference chains it covers. The 
iterative procedure specified in Section 2 extracts 
the designated number of sentences according to 
the number of event words and co-reference chains. 
Table 7 summarizes the experimental results 
under the same test collection mentioned in 
Section 3.  The experiments of summation model 
show that the best detection cost is 0.011603.  
Comparing the best result with those in Tables 5 
and 6, the detection costs are decreased 9% and 2%, 
respectively. 
Window 
size 
1 2 3 4 
Cdet 0.112233 0.011603 0.013109 0.013109
Table 7. Detection Costs Using Summation Model 
5.2 Two-level model 
By comparing the experimental results described 
in Section 3 and 4, we noticed that the event word 
factor seems more important than the co-reference 
factor on event clustering of news document.  
Moreover, from the summation model we only 
know that both factors are useful on event 
clustering.  In order to make clear which factor is 
more important during event clustering of news 
documents, a two-level model is designed in such a 
way that the co-reference chains or the event words 
are used separately rather than simultaneously.  For 
example, we use the score function and the 
sentence selection algorithm described in Section 3 
first, when there is a tie during sentence selection.  
Then we use the score function described in 
Section 4 to decide which sentence is selected from 
those candidate sentences, and vice versa.  Thus, 
two alternatives are considered.  Type 1 model 
uses the event words sentence selection algorithm 
described in Section 4 to select the representative 
sentences from each document, the co-reference 
chains are used to solve the tie issue.  In contrast, 
type 2 model uses the co-reference chains sentence 
selection algorithm described in Section 3 to select 
the representative sentences for each documents 
and use event words to solve the tie issue.  Table 8 
shows the experimental result under the same test 
collection as described in previous sections. 
Window
size 
2 3 4 5 
Type 1 0.012116 0.011987 0.011662 0.012266
Type 2 0.012789 0.012674 0.012854 0.012941
Table 8. Detection Costs Using Two level Models 
The performance of type 1 outperforms that of 
type 2.  This result conforms to those shown in 
Table 5 and Table 6.  We can say that the effect of 
event words is better than the co-reference chains 
in event clustering.  Furthermore, the best score of 
type 1 is also better than the best score of Table 6.  
Thus, the introduction of co-reference chains can 
really improve the performance of event clustering 
using event words.  On the other hand, the 
introduction of event words in type 2 does not have 
such an effect.  Moreover, to further examine the 
use of co-reference chain information and the 
event words in event clustering, a more elaborate 
combination, e.g., using mutual information or 
entropy, of the two approaches is needed. 
6 Concluding Remarks 
This paper presented an approach for event 
clustering on streaming news based on both co-
reference chains and event words.  The 
experimental results using event words only 
outperform the results using the co-reference 
chains only.  Nevertheless, as to the combination 
of co-reference chains and event words in event 
clustering, the experimental results show that the 
introduction of co-reference chains can improve 
the performance of event clustering using event 
words much.  To model the temporal behavior of 
event clustering of streaming news, a dynamic 
threshold setting using time decay function and 
spanning window size is proposed.  The 
experimental results, using TDT?s evaluation 
metric ? say, detection cost, show that the dynamic 
threshold is useful.  . 
We believe that the improvement of multi-
document co-reference resolution will have great 
impact on temporal event clustering.  In order to 
further improve our performance in even clustering 
on streaming news, there are still future works 
needed to be studied: 
(1) In order to verify the significance of the 
experimental results, statistical test is needed.  
(2) Instead of hand-tagging method, we will 
introduce automatic co-reference resolution 
tools to create large scale test corpus and 
conduct large scale experiments. 
(3) When the length of document is variable, the 
fixed number of representative sentences may 
lose many important sentences to degrade the 
performance of event clustering. The dynamic 
number of representative sentences for each 
document according to its length is introduced.  
(4) As the news stories are reported incrementally 
instead of being given totally in the on-line 
event clustering, the computation of event 
words is an important issue. 
(5) Apply the extracted sentences for each 
document to generate event-based short 
summary. 
References  
Allan, James; Carbonell, Jaime; and Yamron, 
Jonathan (Eds) (2002) Topic Detection and 
Tracking: Event-Based Information Organization, 
Kluwer. 
Azzam, S.; Humphreys, K; and Gaizauskas, R. 
(1999) ?Using Coreference Chains for Text 
Summarization,? Proceedings of the ACL 
Workshop on Coreference and Its Applications, 
Maryland. 
Bagga, A. and Baldwin, B. (1998) ?Entity-Based 
Cross-Document Coreferencing Using the 
Vector Space Model,? Proceedings of the 36th 
Annual Meeting of ACL and the 17th 
International Conference on Computational 
Linguistics. 
Cardie, Claire and Wagstaff, Kiri (1999) ?Noun 
Phrase Co-reference as Clustering,? Proceeding 
of the Joint Coreference on EMNLP and VLC 
Chen, Hsin-Hsi and Ku, Lun-Wei (2002) ?An NLP 
& IR Approach to Topic Detection,? Topic 
Detection and Tracking: Event-Based 
Information Organization, James Allan, Jaime 
Carbonell, and Jonathan Yamron (Editors), 
Kluwer, pp. 243-264. 
Chen, Hsin-Hsi; Kuo, June-Jei and Su, Tsei-Chun 
(2003) ?Clustering and Visualization in a Multi-
Lingual Multi-Document Summarization 
System,? Proceedings of 25th European 
Conference on Information Retrieval Research, 
Lecture Notes in Computer Science, LNCS 2633, 
pp. 266-280. 
Fiscus, Jonathan G. and Doddington, George R. 
(2002) ?Topic Detection and Tracking 
Evaluation Overview,? Topic Detection and 
Tracking: Event-Based Information Organization, 
James Allan, Jaime Carbonell, and Jonathan 
Yamron (Eds), Kluwer, pp. 17-32. 
Fukumoto, F. and Suzuki, Y. (2000) ?Event 
Tracking based on Domain Dependency,? 
Proceedings of the 23rd ACM SIGIR 2000 
Conference, pp. 57-64 
Kuo, June-Jei; Wong, Hung-Chia; Lin, Chuan-Jie 
and Chen, Hsin-Hsi (2002) ?Multi-Document 
Summarization Using Informative Words and Its 
Evaluation with a QA System,? Proceedings of 
The Third International Conference on 
Intelligent Text Processing and Computational 
Linguistics, Lecture Notes in Computer Science, 
LNCS 2276, pp. 391-401. 
Manmatha, R.; Feng, A. and Allan, James (2002) 
?A Critical Examination of TDT?s Cost 
Function,? Proceedings of the 25th ACM SIGIR 
Conference, pp. 403-404. 
MUC (1998) Proceedings of 7th Message 
Understanding Conference, Fairfax, VA, 29 
April - 1 May, 1998, 
http://www.itl.nist.gov/iaui/894.02/ 
related_projects/muc/ index.html. 
Silber, H. Gregory and McCoy, Kathleen F. (2002) 
?Eficiently Computed Lexical Chains As an 
Intermediate Representation for Automatic Text 
Summarization.? Journal of Association for 
Computational Linguistics, Vol.28, No.4, pp. 
487-496. 
Wong, Hong-Jia; Kuo, June-Jei and Chen, Hsin-
Hsi (2001) ?Headline Generation for Summaries 
from Multiple Online Sources.? Proceedings of 
6th Natural Language Processing Pacific Rim 
Symposium, November 27-29 2001, Tokyo, 
Japan, pp. 653-660. 
Support Vector Machine Approach to Extracting  
Gene References into Function from Biological Documents 
Chih Lee, Wen-Juan Hou and Hsin-Hsi Chen 
Natural Language Processing Laboratory 
Department of Computer Science and Information Engineering 
National Taiwan University 
1 Roosevelt Road, Section 4, Taipei, Taiwan, 106 
{clee, wjhou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw 
 
Abstract 
In the biological domain, extracting newly 
discovered functional features from the 
massive literature is a major challenging issue.  
To automatically annotate Gene References 
into Function (GeneRIF) in a new literature is 
the main goal of this paper.  We tried to find 
GRIF words in a training corpus, and then 
applied these informative words to annotate the 
GeneRIFs in abstracts with several different 
weighting schemes.  The experiments showed 
that the Classic Dice score is at most 50.18%, 
when the weighting schemes proposed in the 
paper (Hou et al, 2003) were adopted.  In 
contrast, after employing Support Vector 
Machines (SVMs) and the definition of classes 
proposed by Jelier et al (2003), the score 
greatly improved to 56.86% for Classic Dice 
(CD).  Adopting the same features, SVMs 
demonstrated advantage over the Na?ve Bayes 
Classifier.  Finally, the combination of the 
former two models attained a score of 59.51% 
for CD. 
1 Introduction 
Text Retrieval Conference (TREC) has been 
dedicated to information retrieval and information 
extraction for years.  TREC 2003 introduced a new 
track called Genomics Track (Hersh and 
Bhupatiraju, 2003) to address the information 
retrieval and information extraction issues in the 
biomedical domain.  For the information extraction 
part, the goal was to automatically reproduce the 
Gene Reference into Function (GeneRIF) resource 
in the LocusLink database (Pruitt et al, 2000.) 
GeneRIF associated with a gene is a sentence 
describing the function of that gene, and is currently 
manually generated. 
This paper presents the post-conference work on 
the information extraction task (i.e., secondary task).  
In the official runs, our system (Hou et al, 2003) 
adopted several weighting schemes (described in 
Section 3.2) to deal with this problem.  However, 
we failed to beat the simple baseline approach, 
which always picks the title of a publication as the 
candidate GeneRIF.  Bhalotia et al (2003) 
converted this task into a binary classification 
problem and trained a Na?ve  Bayes classifier with 
kernels, achieving 53.04% for CD.  In their work, 
the title and last sentence of an abstract were 
concatenated and features were then extracted from 
the resulting string.  Jelier et al (2003) observed the 
distribution of target GeneRIFs in 9 sentence 
positions and converted this task into a 9-class 
classification problem, attaining 57.83% for CD.  
Both works indicated that the sentence position is 
of great importance.  We therefore modified our 
system to incorporate the position information with 
the help of SVMs and we also investigated the 
capability of SVMs versus Na?ve  Bayes on this 
problem. 
The rest of this paper is organized as follows.  
Section 2 presents the architecture of our extracting 
procedure.  The basic idea and the experimental 
methods in this study are introduced in Section 3.  
Section 4 shows the results and makes some 
discussions.  Finally, Section 5 concludes the 
remarks and lists some future works. 
2 Architecture Overview 
A complete annotation system may be done at two 
stages, including (1) extraction of molecular 
function for a gene from a publication and (2) 
alignment of this function with a GO term.  Figure 
1 shows an example.  The left part is an MEDLINE 
abstract with the function description highlighted.  
The middle part is the corresponding GeneRIF.  
The matching words are in bold, and the similar 
words are underlined.  The right part is the GO 
annotation.  This figure shows a possible solution of 
maintaining the knowledge bases and ontology 
using natural language processing technology.  We 
addressed automation of the first stage in this paper. 
The overall architecture is shown in Figure 2.  
First, we constructed a training corpus in such a 
way that GeneRIFs were collected from LocusLink 
and the corresponding abstracts were retrieved from 
54
MEDLINE.  ?GRIF words? and their weights were 
derived from the training corpus.  Then Support 
Vector Machines were trained using the derived 
corpus.  Given a new abstract, a sentence is selected 
from the abstract to be the candidate GeneRIF. 
3 Methods  
We adopted several weighting schemes to locate the 
GeneRIF sentence in an abstract in the official runs 
(Hou et al, 2003).  Inspired by the work by Jelier et 
al. (2003), we incorporated their definition of 
classes into our weighting schemes, converting this 
task into a classification problem using SVMs as 
the classifier.  We ran SVMs on both sets of 
features proposed by Hou et al (2003) and Jelier et 
al. (2003), respectively.  Finally, all the features 
were combined and some feature selection methods 
were applied to train the classifier. 
3.1 Training and test material preparation 
Since GeneRIFs are often cited verbatim from 
abstracts, we decided to reproduce the GeneRIF by 
selecting one sentence in the abstract.  Therefore, 
for each abstract in our training corpus, the sentence 
most similar to the GeneRIF was labelled as the 
GeneRIF sentence using Classic Dice coefficient as 
similarity measure.  Totally, 259,244 abstracts were 
used, excluding the abstracts for testing.  The test 
data for evaluation are the 139 abstracts used in 
TREC 2003 Genomics track. 
3.2 GRIF words extraction and weighting 
scheme  
We called the matched words between GeneRIF 
and the selected sentence as GRIF words in this 
paper.  GRIF words represent the favorite 
vocabulary that human experts use to describe gene 
functions.  After stop word removal and stemming 
operation, 10,506 GRIF words were extracted. 
In our previous work (Hou et al, 2003), we first 
generated the weight for each GRIF word.  Given 
an abstract, the score of each sentence is the sum of 
weights of all the GRIF words in this sentence.  
Finally, the sentence with the highest score is 
selected as the  candidate GeneRIF.  This method is 
denoted as OUR weighting scheme, and several 
heuristic weighting schemes were investigated.  
Here, we only present the weighting scheme used in 
SVMs classification.  The weighting scheme is as 
follows. For GRIF word i, the number of 
occurrence Gin  in all the GeneRIF sentences and the 
number of occurrence Ain  in all the abstracts were 
computed and AiGi nn /  was assigned to GRIF word i 
as its weight. 
3.3 Classification 
3.3.1 Class definition and feature extraction 
The distribution of GeneRIF sentences showed that 
the position of a sentence in an abstract is an 
important clue to where the answer sentence is.  
Jelier et al (2003) considered only the title, the first 
three and the last five sentences, achieving the best 
performance in TREC official runs.  Their Na?ve 
Bayes model is as follows.  An abstract a is 
assigned a class vj by calculating vNB: 
 
Existing 
GeneRIFs 
on 
LocusLink 
Corresponding 
Medline 
Abstracts  
GRIF Word 
Extractor 
Weighted GRIF 
Words 
Generating 
Training Data 
Training SVMs 
New 
Abstract 
GeneRIF 
 Sentence Locator  
Candidate 
GeneRIF 
 
Figure 2: Architecture of Extracting Candidate GeneRIF 
Figure 1: An Example of Complete Annotation from a Literature to Gene Ontology 
 
extraction 
alignm
ent 
The Bcl10 gene was recently isolated 
from the breakpoint region of 
t(1;14)(p22;q32) in mucosa-associated 
lymphoid tissue (MALT) lymphomas. 
Somatic mutations of Bcl10 were found 
in not only t(1;14)-bearing MALT 
lymphomas, but also a wide range of 
other tumors. ? ? Our results strongly 
suggest that somatic mutations  of Bcl10 
are extremely rare in malignant 
cartilaginous tumors  and do not 
commonly contribute to their molecular 
pathogenesis. 
PMID: 11836626 
MEDLINE abstract 
Mutations, 
relatively 
common in 
lymphomas, 
are extremely 
rare in 
malignant 
cartilaginous 
tumors. 
GeneRIF 
l GO:0005515  
term: protein binding 
definition: Interacting selectively with any protein, or 
protein complex (a complex of two or more proteins that 
may include other nonprotein molecules). 
l GO:0008181  
term: tumor suppressor 
l GO:0006917  
term: induction of apoptosis 
l GO:0005622 
term: intracellular 
l GO:0016329  
term: apoptosis regulator activity 
definition: The function held by products which directly 
regulate any step in the process of apoptosis. 
l GO:0045786  
term: negative regulation of cell cycle 
GO annotation 
55
  CD MUD MBD MBDP 
1 Jelier (Sentence-wise bag of words + Na?ve  Bayes) 57.83% 59.63% 46.75% 49.11% 
2 Sentence-wise bag of words + SVMs 58.92% 61.46% 47.86% 50.84% 
3 OUR Weighting scheme 50.18% 46.71% 33.47% 38.83% 
4 OUR Weighting scheme + SVMs 56.86% 58.81% 45.08% 48.10% 
5 Combined 59.51% 62.16% 48.17% 51.25% 
6 Combined + gene/protein names 57.59% 59.95% 46.69% 49.68% 
7 Combined + BWRatio feature selection 57.59% 59.90% 47.11% 50.08% 
8 Combined + Graphical feature selection 58.81% 61.09% 47.98% 50.92% 
9 Optimal Classifier 67.60% 70.74% 59.28% 62.09% 
10 Baseline 50.47% 52.60% 34.82% 37.91% 
Table 2: Comparison of performances on the 139 abstracts 
,
,argmax ( ) ( | )
j a i
NB j k i j
v V i S k W
v P v P w v
? ? ?
= ?? ?
 
where vj is one of the nine positions aforementioned, 
S is the set of 9 sentence positions, Wa,i is the set of 
all word positions in sentence i in abstract a, wk,i is 
the occurrence of the normalized word at position k 
in sentence i and V is the set of 9 classes. 
We, therefore, represented each abstract by a 
feature vector composed of the scores of 9 
sentences.  Furthermore, with a list of our 10,506 
GRIF words at hand, we also computed the 
occurrences of these words in each sentence, given 
an abstract.  Each abstract is then represented by the 
number of occurrences of these words in the 9 
sentences respectively, i.e., the feature vector is 
94,554 in length.  Classification based on this type 
of features is denoted the sentence-wise bag of 
words model in the rest of this paper.  Combining 
these two models, we got totally 94,563 features. 
Since we are extracting sentences discussing gene 
functions, it?s reasonable to expect gene or protein 
names in the GeneRIF sentence.  Therefore, we 
employed Yapex (Olsson et al, 2002) and 
GAPSCORE (Chang et al, 2004) protein/gene 
name detectors to count the number of protein/gene 
names in each of the 9 sentences, resulting in 
94,581 features.  
3.3.2 Training SVMs 
The whole process related to SVM was done via 
LIBSVM ? A Library for Support Vector Machines 
(Hsu et al, 2003).  Radial basis kernel was adopted 
based on our previous experience.  However, 
further verification showed that the combined 
model with either linear or polynomial kernel only 
slightly surpassed the baseline, attaining 50.67% for 
CD.  In order to get the best-performing classifier, 
we tuned two parameters, C and gamma.  They are 
the penalty coefficient in optimization and a 
parameter for the radial basis kernel, respectively.  
Four-fold cross validation accuracy was used to 
select the best parameter pair. 
3.3.3 Picking up the answe r sentence  
Test instances were first fed to the classifier to get 
the predicted positions of GeneRIF sentences.  In 
case that the predicted position doesn?t have a 
sentence, which would happen when the abstract 
doesn?t have enough sentences, the sentence with 
the highest score is picked for the weighting 
scheme and the combined model, otherwise the title 
is picked for the sentence-wise bag of words model. 
4 Results and Discussions  
The performance measures are based on Dice 
coefficient, which calculates the overlap between 
the candidate GeneRIF and actual GeneRIF.  
Classic Dice (CD) is the classic Dice formula using 
a common stop word list and the Porter stemming 
algorithm.  Due to lack of space, we referred you to 
the Genomics track overview for the other three 
modifications of CD (Hersh and Bhupatiraju, 2003). 
The evaluation results are shown in Table 2.  The 
1st row shows the official run of Jelier?s team, the 
first place in the official runs.  The 2nd row shows 
the performance when the Na?ve Bayes classifier 
adopted by Jelier is replaced with SVMs.  The 3rd 
row is the performance of our weighting scheme 
without a classifier.  The 4th row then lists the 
performance when our weighting scheme is 
combined with SVMs.  The 5th row is the result 
when our weighting scheme and the sentence-wise 
bag of words model are combined together.  The 6th 
row is the result when two gene/protein name 
detectors are incorporated into the combined model.  
The next two rows were obtained after two feature 
selection methods were applied.  The 9th row shows 
the performance when the classifier always 
proposes a sentence most similar to the actual 
GeneRIF.  The last row lists the baseline, i.e., title 
is always picked. 
A comparative study on text categorization 
(Joachims, 1998) showed that SVMs outperform 
other classification methods, such as Na?ve  Bayes, 
C4.5, and k-NN.  The reasons would be that SVMs 
are capable of handling large feature space, text 
categorization has few irrelevant features, and 
document vectors are sparse.  The comparison 
56
between SVMs and the Na?ve  Bayes classifier again 
demonstrated the superiority of SVMs in text 
categorization (rows 1, 2). 
The performance greatly improved after 
introducing position information (rows 3, 4), 
showing the sentence position plays an important 
role in locating the GeneRIF sentence.  The 2% 
difference between rows 2 and 4 indicates that the 
features under sentence-wise bag of words model 
are more informative than those under our 
weighting scheme.  However, with only 9 features, 
our weighting scheme with SVMs performed fairly 
well.  Comparing the performance before and after 
combining our weighting scheme and the sentence-
wise bag of words model (rows 2, 5 and rows 4, 5), 
we can infer from the performance differences that 
both models provide mutually exclusive 
information in the combined model.  The result 
shown in row 6 indicates that the information of 
gene/protein name occurrences did not help identify 
the GeneRIF sentences in these 139 test abstracts. 
We performed feature selection on the combined 
model to reduce the dimension of feature space.  
There were two methods applied: a supervised 
heuristic method (denoted as BWRatio feature 
selection in Table 2) (S. Dutoit et al, 2002) and 
another unsupervised method (denoted as Graphical 
feature selection in Table 2) (Chang et al, 2002).  
The number of features was then reduced to about 
4,000 for both methods.  Unfortunately, the 
performance did not improve after either method 
was applied.  This may be attributed to over-fitting 
training data, because the cross-validation 
accuracies are indeed higher than those without 
feature selection.  The result may also imply there 
are little irrelevant features in this case. 
5 Conclusion and Future work 
This paper proposed an automatic approach to 
locate the GeneRIF sentence in an abstract with the 
assistance of SVMs, reducing the human effort in 
updating and maintaining the GeneRIF field in the 
LocusLink database. 
We have to admit that the 139 abstracts provided 
in TREC 2003 are too few to verify the 
performance among models, and the results based 
on these 139 abstracts may be slightly biased.  Our 
next step would aim at measuring the cross-
validation performances using Dice coefficient. 
The syntactic  information is worth exploring, 
since the sentences describing gene functions may 
share some common structural patterns.  Moreover, 
how the weighting scheme affects the performance 
is also very interesting.  We are currently trying to 
obtain a weighting scheme that can best distinguish 
GeneRIF sentence from non-GeneRIF sentence 
without classifiers. 
References  
G. Bhalotia, P.I. Nakov, A.S. Schwartz, and M.A. 
Hearst. 2003. BioText Team Report for the TREC 
2003 Genomics Track. TREC 2003 work notes: 
158-166. 
Y.C. I. Chang, H. Hsu and L.Y. Chou. 2002. 
Graphical Features Selection Method. Intelligent 
Data Engineering and Automated Learning, 
Edited by H. Yin, N. Allinson, R. Freeman, J. 
Keane, and S. Hubband. 
J.T. Chang, H. Schutze, R.B. Altman. 2004. 
GAPSCORE: finding gene and protein names one 
word at a time. Bioinformatics, 20(2):216-225. 
S. Dutoit, Y.H. Yang, M.J. Callow and T.P. Speed. 
2002. Statistical methods for identifying 
differentially expressed genes in replicated cDNA 
microarray experiments. J. Amer. Statis. Assoc. 
97:77-86. 
W. Hersh and Ravi Teja Bhupatiraju. 2003. TREC 
Genomics Track Overview. TREC 2003 work 
notes. 
W.J. Hou, C.Y. Teng, C. Lee and H.H. Chen. 2003. 
SVM Approach to GeneRIF Annotation. 
Proceedings of TREC 2003. 
C.W. Hsu, C.C Chang and C.J. Lin. 2003. A 
Practical Guide to Support Vector Classification.  
http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.ht
ml. 
R. Jelier, M. Schuemie, C.V.E. Eijk, M. Weeber, 
E.V. Mulligen, B. Schijvenaars, B. Mons and J. 
Kors. 2003. Searching for geneRIFs: concept-
based query expansion and Bayes classification. 
TREC 2003 work notes: 167-174. 
T. Joachims. 1998. Text Categorization with 
Support Vector Machines: Learning with Many 
Relevant Features. Proceedings of ECML-98, 
137-142. 
F. Olsson, G. Eriksson, K. Franz?n, L. Asker and P. 
Lid?n. 2002. Notions of Correctness when 
Evaluating Protein Name Taggers. Proceedings of 
the 19th International Conference on 
Computational Linguistics 2002, 765-771. 
K.D. Pruitt, K.S. Katz, H. Sicotte and D.R. Maglott. 
2000. Introducing RefSeq and LocusLink: 
Curated Human Genome Resources at the NCBI. 
Trends Genet, 16(1):44-47. 
T. Sekimizu, H.S. Park and J. Tsujji. 1998. 
Identifying the Interaction Between Genes and 
Gene Products Based on Frequently Seen Verbs 
in Medline Abstracts. Genome Information, 9:62-
71 
57
Annotating Multiple Types of Biomedical Entities:  
A Single Word Classification Approach 
Chih Lee, Wen-Juan Hou and Hsin-Hsi Chen 
Natural Language Processing Laboratory 
Department of Computer Science and Information Engineering 
National Taiwan University  
1 Roosevelt Road, Section 4, Taipei, Taiwan, 106 
{clee, wjhou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw 
 
Abstract 
Named entity recognition is a fundamental 
task in biomedical data mining.  Multiple -class 
annotation is more challenging than single -
class annotation.  In this paper, we took a 
single word classification approach to dealing 
with the multiple -class annotation problem 
using Support Vector Machines (SVMs).  
Word attributes, results of existing 
gene/protein name taggers, context, and other 
information are important features for 
classification.  During training, the size of 
training data and the distribution of named 
entities are considered.  The preliminary 
results showed that the approach might be 
feasible when more training data is used to 
alleviate the data imbalance problem. 
1 Introduction 
The volumn of on-line material in the biomedical 
field has been growing steadily for more than 20 
years.  Several attempts have been made to mine 
knowledge from biomedical documents, such as 
identifying gene/protein names, recognizing 
protein interactions, and capturing specific 
relations in databases.  Among these, named entity 
recognition is a fundamental step to mine 
knowledge from biological articles. 
Previous approaches on biological named entity 
extraction can be classified into two types ? rule-
based (Fukuda et al, 1998; Olsson et al, 2002; 
Tanabe and Wilbur, 2002) and corpus-based 
(Collier et al, 2000; Chang et al, 2004).  Yapex 
(Olsson et al, 2002) implemented some heuristic 
steps described by Fukuda, et al, and applied 
filters and knowledge bases to remove false alarms.  
Syntactic information obtained from the parser was 
incorporated as well.  GAPSCORE (Chang et al, 
2004) scored words on the basis of statistical 
models that quantified their appearance, 
morphology and context.  The models includes 
Naive Bayes (Manning and Schutze, 1999), 
Maximum Entropy (Ratnaparkhi, 1998) and 
Support Vector Machines (Burges, 1998).  
GAPSCORE also used Brill?s tagger (Brill, 1994) 
to get the POS tag to filter out some words that are 
clearly not gene or protein names.  Efforts have 
been made (Hou and Chen, 2002, 2003; Tsuruoka 
and Tsujii, 2003) to improve the performance.  The 
nature of classification makes it possible to 
integrate existing approaches by extracting good 
features from them.  Several works employing 
SVM classifier have been done (Kazama et al, 
2002; Lee et al, 2003; Takeuchi and Collier, 2003; 
Yamamoto et al, 2003), and will be discussed 
further in the rest of this paper. 
Collocation denotes two or more words having 
strong relationships (Manning and Schutze, 1999).  
Hou and Chen (2003) showed that protein/gene 
collocates are capable of assisting existing 
protein/gene taggers.  In this paper, we addressed 
this task as a multi-class classification problem 
with SVMs and extended the idea of collocation to 
generate features at word and pattern level in our 
method.  Existing protein/gene recognizers were 
used to perform feature extraction as well. 
The rest of this paper is organized as follows.  
The methods used in this study are introduced in 
Section 2.  The experimental results are shown and 
discussed in Section 3.  Finally, Section 4 
concludes the remarks and lists some future works. 
2 Methods  
Most of the works in the past on recognizing 
named entities in the biomedical domain focused 
on identifying a single type of entities like protein 
and/or gene names.  It is obviously more 
challenging to annotate multiple types of named 
entities simultaneously.  Intuitively, one can 
develop a specific recognizer for each type of 
named entities, run the recognizers one by one to 
annotate all types of named entities, and merge the 
results.  The problem results from the boundary 
decision and the annotation conflicts.  Instead of 
constructing five individual recognizers, we 
regarded the multiple -class annotation as a 
classification problem, and tried to learn a 
80
classifier capable of identifying all the five types of 
named entities. 
Before classification, we have to decide the unit 
of classification.  Since it is difficult to correctly 
mark the boundary of a name to be identified, the 
simplest way is to consider an individual word as 
an instance and assign a type to it.  After the type 
assignment, continuous words of the same type 
will be marked as a complete named entity of that 
type.  The feature extraction process will be 
described in the following subsections. 
2.1 Feature Extraction 
The first step in classification is to extract 
informative and useful features to represent an 
instance to be classified.  In our work, one word is 
represented by the attributes carried per se, the 
attributes contributed by two surrounding words, 
and other contextual information.  The details are 
as follows. 
2.1.1 Word Attributes 
The word ?attribute? is sometimes used 
interchangeably with ?feature?, but in this article 
they denote two different concepts.  Features are 
those used to represent a classification instance, 
and the information enclosed in the features is not 
necessarily contributed by the word itself.  
Attributes are defined to be the information that 
can be derived from the word alone in this paper. 
The attributes assigned to each word are whether 
it is part of a gene/protein name, whether it is part 
of a species name, whether it is part of a tissue 
name, whether it is a stop word, whether it is a 
number, whether it is punctuation, and the part of 
speech of this word.  Instead of using a lexicon for 
gene/protein name annotation, we employed two 
gene/protein name taggers, Yapex and 
GAPSCORE, to do this job.  As for part of speech 
tagging, Brill?s part of speech tagger was adopted. 
2.1.2 Context Information Preparation 
Contextual information has been shown helpful in 
annotating gene/protein names, and therefore two 
strategies for extracting contextual information at 
different levels are used.  One is the usual practice 
at a word level, and the other is at a pattern level.  
Since the training data released in the beginning 
does not define the abstract boundary, we have to 
assume that sentences are independent of each 
other, and the contextual information extraction 
was thus limited to be within a sentence. 
For contextual information extraction at word 
level (Hou and Chen, 2003), collocates along with 
4 statistics including frequency, the average and 
standard error of distance between word and entity 
and t-test score, were extracted.  The frequency 
and t-test score were normalized to [0, 1].  Five 
lists of collocates were obtained for cell-line, cell-
type, DNA, RNA, and protein, respectively. 
As for contextual information extraction at 
pattern level, we first gathered a list of words 
constituting a specific type of named entities.  
Then a hierarchical clustering with cutoff threshold 
was performed on the words. Edit distance was 
adopted as the measure of dissimilarity (see Figure 
1). Afterwards, common substrings were obtained 
to form the list of patterns.  With a list of patterns 
at hand, we estimated the pattern distribution, the 
occurrence frequencies at and around the current 
position, given the type of word at the current 
position.  Figure 2 showed an example of the 
estimated distribution.  The average KL-
Divergence between any two distributions was 
computed to discriminate the power of each pattern.  
The formula is as follows: 
1 1,
1 ( || )
( 1)
n n
i j
i j j i
D p p
n n = = ?- ? ? , where pi and pj 
are the distributions of a pattern given the word at 
position 0 being type i and j, respectively. 
 
Figure 1: Example of common substring extraction 
 
Figure 2: Pattern distributions given the type of 
word at position 0 
2.2 Constructing Training Data 
For each word in a sentence, the attributes of the 
word and the two adjacent words are put into the 
feature vector.  Then, the left five and the right five 
words are searched for previously extracted 
collocates.  The 15 variables thus added are shown 
below. 
5
5, 0
( | )i
i i
Freq w type
=- ?
?
 
5
5, 0
_ ( | )i
i i
t test score w type
=- ?
-?  
81
5, ,
5, 0
??( | , )
i iw type w type
i i
f i m s
=- ?
? , where f is the pdf of 
normal distribution, type is one of the five types, wi 
denotes the surrounding words,
,
?
itypew
m and 
,
?
itypew
s are 
the maximum likelihood estimates of mean and 
standard deviation for wi given the type. Next, the 
left three and right three words along with the 
current word are searched for patterns, adding 6 
variables to the feature vector. 
3
3
Prob ( | )
wi
p
i p P
i type
=- ?
? ? , where type is one of the 
six types including ?O?, 
iwP is the set of patterns 
matching wi, Prob p  denotes the pmf for pattern p.  
Finally, the type of the previous word is added to 
the feature vector, mimicking the concept of a 
stochastic model. 
2.3 Classification 
Support Vector Machines classification with radial 
basis kernel was adopted in this task, and the 
package LIBSVM ? A Library for Support Vector 
Machines (Hsu et al, 2003) was used for training 
and prediction. The penalty coefficient C in 
optimization and gamma in kernel function were 
tuned using a script provided in this package. 
The constructed training data contains 492,551 
instances, which is too large for training.  Also, the 
training data is extremely unbalanced (see Table 1) 
and this is a known problem in SVMs 
classification.  Therefore, we performed stratified 
sampling to form a smaller and balanced data set 
for training. 
Type # of instances (words) 
cell-type 15,466 
DNA 25,307 
cell-line 11,217 
RNA 2,481 
protein 55,117 
O 382,963 
Table 1: Number of instances for each type 
3 Results and Discussion 
Since there is a huge amount of training instances 
and we do not have enough time to tune the 
parameters and train a model with all the training 
instances available, we first randomly selected one 
tenth and one fourth of the complete training data.  
The results, as we expected, showed that model 
trained with more instances performed better (see 
Table 2).  However, we noticed that the 
performances vary among the 6 types and one of 
the possible causes is the imbalance of training 
data among classes (see Table 1). Therefore we 
decided to balance the training data. 
First, the training data was constructed to 
comprise equal number of instances from each 
class.  However, it didn?t perform well and lots of 
type ?O? words were misclassified, indicating that 
using only less than 1% of type ?O? training 
instances is not sufficient to train a good model.  
Thus two more models were trained to see if the 
performance can be enhanced.  One model has 
slightly more type ?O? instances than the equally 
balanced one, and the other model has the ratio 
among classes being 4:8:4:1:8:16.  The results 
showed increase in recall but drop in precision. 
Kazama et al (2002) addressed the data 
imbalance problem and sped up the training 
process by splitting the type ?O? instances into sub-
classes using part-of-speech information.  However, 
we missed their work while we were doing this 
task, and hence didn?t have the chance to use and 
extend this idea. 
After carefully examining the classification 
results, we found that many of the ?DNA? 
instances were classified as ?protein? and many of 
the ?protein? instances were classified as ?DNA?.  
For example, 904 out of 2,845 ?DNA? instances 
were categorized as ?protein? under ?model 1/4?.  
The reason may be that Yapex and GAPSCORE do 
not distinguish gene name from protein names.  
Even humans don?t do very well at this 
(Krauthammer et al, 2002). 
We originally planned to verify the contribution 
of each type of features. For example, how much 
noise was introduced by using existing taggers 
instead of lexicons. This would have helped gain 
more insights into the proposed features. 
4 Conclusion and Future work 
This paper presented the preliminary results of our 
study.  We introduced the use of existing taggers 
and presented a way to collect common substrings 
shared by entities.  Due to lack of time, the models 
were not well tuned against the two parameters ? C 
and gamma, influencing the capabilities of the 
models.  Further, not all of the training instances 
provided were used to train the model, and it will 
be interesting and worthwhile to investigate.  How 
to deal with data imbalance is another important 
issue.  By solving this problem, further evaluation 
of feature effectiveness would be facilitated.  We 
believe there is much left for our approach to 
improve and it may perform better if more time is 
given. 
82
References  
E. Brill. 1994. Some Advances in Transformation-
Based Part of Speech Tagging. Proceedings of 
the National Conference on Artificial 
Intelligence. AAAI Press; 722-727. 
C. Burges. 1998. A Tutorial on Support Vector 
Machines for Pattern Recognition. Data Mining 
and Knowledge Discovery, 2: 121-167. 
J.T. Chang, H. Schutze and R.B. Altman. 2004. 
GAPSCORE: Finding Gene and Protein Names 
One Word at a Time. Bioinformatics, 20(2): 216-
225. 
N. Collier, C. Nobata and J.I. Tsujii. 2000. 
Extracting the Names of Genes and Gene 
Products with a Hidden Markov Model. 
Proceedings of 18 th International Conference on 
Computational Linguistics, 201-207. 
K. Fukuda, T. Tsunoda, A. Tamura and T. Takagi. 
1998. Toward Information Extraction: 
Identifying Protein Names from Biological 
Papers. Proceedings of Pacific Symposium on 
Biocomputing, 707-718. 
W.J. Hou and H.H. Chen 2002. Extracting 
Biological Keywords from Scientific Text. 
Proceedings of 13 th International Conference on 
Genome Informatics; 571-573. 
W.J. Hou and H.H. Chen. 2003. Enhancing 
Performance of Protein Name Recognizers 
Using Collocation. Proceedings of the ACL 2003 
Workshop on NLP in Biomedicine, 25-32. 
C.W. Hsu, C.C Chang and C.J. Lin. 2003. A 
Practical Guide to Support Vector Classification. 
http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.h
tml. 
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. 
Tuning Support Vector Machines for Biomedical 
Named Entity Recognition. Proceedings of the 
ACL 2002 workshop on NLP in the Biomedical 
Domain , 1-8. 
M. Krauthammer, P. Kra, I. Iossifov, S.M. Gomez, 
G. Hripcsak, V. Hatzivassiloglou, C. Friedman 
and A. Rzhetsky. 2002. Of truth and pathways: 
chasing bits of information through myriads of 
articles. Bioinformatics, 18(sup.1):S249-S257. 
K.J. Lee, Y.S. Hwang and H.C. Rim. 2003. Two-
Phase Biomedical NE Recognition based on 
SVMs. Proceedings of the ACL 2003 Workshop 
on NLP in Biomedicine, 33-40. 
C.D. Manning and H. Schutze. 1999. Foundations 
of Statistical Natural Language Processing. MIT 
Press. 
F. Olsson, G. Eriksson, K. Franzen, L. Asker and P. 
Liden. 2002. Notions of Correctness when 
Evaluating Protein Name Taggers. Proceedings 
of the 19th International Conference on 
Computational Linguistics, 765-771. 
A. Ratnaparkrhi. 1998. Maximum Entropy Models 
for Natural Language Ambiguity Resolution. 
PhD Thesis, University of Pennsylvania. 
K. Takeuchi and N. Collier. 2003. Bio-Medical 
Entity Extraction using Support Vector 
Machines. Proceedings of the ACL 2003 
workshop on NLP in Biomedicine, 57-64. 
L. Tanabe and W.J. Wilbur. 2002. Tagging Gene 
and Protein Names in Biomedical Text. 
Bioimformatics, 18(8) : 1124-1132. 
Y. Tsuruoka and J. Tsujii. 2003. Boosting 
Precision and Recall of Dictionary-based Protein 
Name Recognition. Proceedings of the ACL 
2003 Workshop on NLP in Biomedicine, 41-48. 
K. Yamamoto, T. Kudo, A. Konagaya and Y. 
Matsumoto. 2003. Protein Name Tagging for 
Biomedical Annotation in Text. Proceedings of 
the ACL 2003 workshop on NLP in Biomedicine, 
65-72.
 Model 1/10 Model 1/4    
 Recall Prec. F-score Recall Prec. F-score Recall Prec. F-score 
Full (Object) 0.4756 0.4399 0.4571 0.5080 0.4759 0.4914    
Full (protein) 0.5846 0.4392 0.5016 0.6213 0.4614 0.5296    
Full (cell-line) 0.2420 0.2909 0.2642 0.2820 0.3341 0.3059    
Full (DNA) 0.2784 0.3249 0.2998 0.2888 0.4479 0.3512    
Full (cell-type) 0.3863 0.5752 0.4622 0.4196 0.6115 0.4977    
Full (RNA) 0.0085 0.1000 0.0156 0.0000 0.0000 0.0000    
 Model balanced equally Model slightly more ?O? Model 4:8:4:1:8:16 
Full (Object) 0.1480 0.0990 0.1186 0.1512 0.1002 0.1206 0.5036 0.3936 0.4419 
Full (protein) 0.1451 0.1533 0.1491 0.1458 0.1527 0.1492 0.5629 0.4280 0.4863 
Full (cell-line) 0.1580 0.0651 0.0922 0.2280 0.0319 0.0560 0.4060 0.2261 0.2904 
Full (DNA) 0.1326 0.0466 0.0690 0.1591 0.0582 0.0852 0.3759 0.2457 0.2972 
Full (cell-type) 0.1650 0.1375 0.1500 0.1494 0.1908 0.1676 0.4701 0.4900 0.4798 
Full (RNA) 0.0932 0.0067 0.0126 0.0169 0.0075 0.0104 0.0593 0.1148 0.0782 
Table 2: Performance of each model (only FULL is shown) 
83
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 279?289, Dublin, Ireland, August 23-29 2014.
Chinese Word Ordering Errors Detection and Correction 
for Non-Native Chinese Language Learners 
 
 
Shuk-Man Cheng, Chi-Hsin Yu, Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University, Taipei, Taiwan 
{smcheng,jsyu}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw 
 
  
 
Abstract 
Word Ordering Errors (WOEs) are the most frequent type of grammatical errors at sentence 
level for non-native Chinese language learners. Learners taking Chinese as a foreign language 
often place character(s) in the wrong places in sentences, and that results in wrong word(s) or 
ungrammatical sentences. Besides, there are no clear word boundaries in Chinese sentences. 
That makes WOEs detection and correction more challenging. In this paper, we propose 
methods to detect and correct WOEs in Chinese sentences. Conditional random fields (CRFs) 
based WOEs detection models identify the sentence segments containing WOEs. Segment 
point-wise mutual information (PMI), inter-segment PMI difference, language model, tag of 
the previous segment, and CRF bigram template are explored. Words in the segments contain-
ing WOEs are reordered to generate candidates that may have correct word orderings.  Rank-
ing SVM based models rank the candidates and suggests the most proper corrections. Train-
ing and testing sets are selected from HSK dynamic composition corpus created by Beijing 
Language and Culture University. Besides the HSK WOE dataset, Google Chinese Web 5-
gram corpus is used to learn features for WOEs detection and correction. The best model 
achieves an accuracy of 0.834 for detecting WOEs in sentence segments. On the average, the 
correct word orderings are ranked 4.8 among 184.48 candidates. 
1 Introduction 
Detection and correction of grammatical errors are practical for many applications such as document 
editing and language learning. Non-native language learners usually encounter problems in learning a 
new foreign language and are prone to generate ungrammatical sentences. Sentences with various 
types of errors are written by language learners of different backgrounds. In the HSK corpus, which 
contains compositions of students from different countries who study Chinese in Beijing Language 
and Culture University (http://nlp.blcu.edu.cn/online-systems/hsk-language-lib-indexing-system.html), 
there are 35,884 errors at sentence level. The top 10 error types and their occurrences are listed below: 
Word Ordering Errors (WOE) (8,515), Missing Component (Adverb) (3,244), Missing Component 
(Predicate) (3,018), Grammatical Error (?Is ? DE?) (2,629), Missing Component (Subject) (2,405), 
Missing Component (Head Noun) (2364), Grammatical Error (?Is? sentence) (1,427), Redundant 
Component (Predicate) (1,130), Uncompleted Sentence (1,052), and Redundant Component (Adverb) 
(1,051). WOEs are the most frequent type of errors (Yu and Chen, 2012). 
The types of WOEs in Chinese are different from those in English. A Chinese character has its own 
meaning in text, while individual characters are meaningless in English. Learners taking Chinese as a 
foreign language often place character(s) in the wrong places in sentences, and that results in wrong 
word(s) or ungrammatical sentences. Besides, there are no clear word boundaries in Chinese sentences.  
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
279
Word segmentation is fundamental in Chinese language processing (Huang and Zhao, 2007). WOEs 
may result in wrong segmentation. That may make WOEs detection and correction more challenging. 
This paper aims at identifying the positions of WOEs in the text written by non-native Chinese lan-
guage learners, and proposes candidates to correct the errors. It is organized as follows. Section 2 sur-
veys the related work. Section 3 gives an overview of the study. Section 4 introduces the dataset used 
for training and testing. Sections 5 and 6 propose models to detect and correct Chinese WOEs, respec-
tively. Section 7 concludes this study and propose some future work. 
2 Related Work 
There are only a few researches on the topic of detection and correction of WOEs in Chinese language 
until now. We survey the related work from the four aspects: (1) grammatical errors made by non-
native Chinese learners, (2) word ordering errors in Chinese language, (3) computer processing of 
grammatical errors in Chinese language, and (4) grammatical error correction in other languages.  
Leacock et al. (2014) give thorough surveys in automated grammatical error detection for language 
learners. Error types, available corpora, evaluation methods, and approaches for different types of er-
rors are specified. Several shared tasks on grammatical error correction in English have been orga-
nized in recent years, including HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012) 
and CoNLL 2013 (Ng et al., 2013). Different types of grammatical errors are focused: (1) HOO 2011: 
article and preposition errors, (2) HOO 2012: determiner and preposition errors, and (3) CoNLL 2013: 
article or determiner errors, preposition errors, noun number errors, verb form errors, and subject-verb 
agreement errors. In Chinese, spelling check evaluation was held at SIGHAN Bake-off 2013 (Wu et 
al., 2013). However, none of the above evaluations deals with word ordering errors. 
Wang (2011) focuses on the Chinese teaching for native English-speaking students. He shows the 
most frequent grammatical errors made by foreigners are missing components, word orderings and 
sentence structures. One major learning problem of foreign learners is the influence of negative trans-
fer of mother tongue. Lin (2011) studies the biased errors of word order in Chinese written by foreign 
students in the HSK corpus. Sun (2011) compares the word orderings between English and Chinese to 
figure out the differences in sentence structures. Yu and Chen (2012) propose classifiers to detect sen-
tences containing WOEs, but they do not deal with where WOEs are and how to correct them. 
Wagner et al. (2007) deal with common grammatical errors in English. They consider frequencies 
of POS n-grams and the outputs of parsers as features. Gamon et al. (2009) identify and correct errors 
made by non-native English writers. They first detect article and preposition errors, and then apply 
different techniques to correct each type of errors. Huang et al. (2010) propose a correction rule ex-
traction model trained from 310,956 sets of erroneous and corrected pairwise sentences. Some studies 
related to word orderings are specific to the topic of pre-processing or post-processing of statistical 
machine translation, such as Galley and Manning (2008), Setiawan et al. (2009), and DeNero and 
Uszkoreit (2011). 
The major contributions of this paper cover the following aspects: (1) application aspect: detecting 
and correcting a common type of Chinese written errors of foreign learners with HSK corpus; (2) lan-
guage aspect: considering the effects of words and segments in Chinese sentences; and (3) resource 
aspect: exploring the feasibility of using a Chinese web n-gram corpus in WOE detection/correction. 
3 Overview of a Chinese Word Ordering Detection and Correction System 
Figure 1 sketches an overview of our Chinese WOE detection and correction system. It is composed of 
three major parts, including dataset preparation, WOE detection, and WOE correction. At first, a cor-
pus is prepared. Sentences containing WOEs are selected from the corpus and corrected by two Chi-
nese native speakers. This corpus will be used for training and testing. Then, a sentence is segmented 
into a sequence of words, and chunked into several segments based on punctuation marks. Regarding 
words and segments as fundamental units reduce the number of reordering and limit the reordering 
scope. The segments containing WOEs are identified by using CRF-based models. Finally, the candi-
dates are generated by reordering and ranked by Ranking SVM-based models. To examine the per-
formance of WOE correction, two datasets, Cans and Csys, consisting of error segments labelled by hu-
man and detected by our system, respectively, are employed. 
 
280
 Figure 1: Overview of word ordering error detection and correction. 
 
The example shown below demonstrates the major steps.  This sentence is composed of three seg-
ments.  The second segment contains a WOE, i.e., ????????? (Graduated college this 
summer). The correct sentence should be  ????????? (Graduated from college this sum-
mer).  
(1) Reduce the number of reordering units in a sentence by using word segmentation. 
? ? ??? ? ?? ?? ?? ? ?? ? ?? ? ?? ?
( I  / am /Wang Daan/  , /this         /summer  /graduated/le  /college   /,     /now       /look for/job    /.) 
(2) Chunk a sentence into segments by punctuation marks. 
? ? ??? ? ?? ?? ?? ? ?? ? ?? ? ?? ?
(3) Detect the possible segments containing WOEs in a sentence by CRF-based methods. 
? ? ??? ? ?? ?? ?? ? ?? ? ?? ? ?? ?
(4) Reorder words in an erroneous segment and generate candidates. 
? ? ??? ? ?? ?? ?? ?? ? ? ?? ? ?? ?
  ? 
? ? ??? ? ?? ?? ?? ?? ? ? ?? ? ?? ?
(5) Rank candidates and suggest correct word ordering by Ranking SVM-based methods. 
? ? ??? ? ?? ?? ?? ?? ? ? ?? ? ?? ?
? 
281
4 A Word Ordering Errors (WOEs) Corpus 
HSK dynamic composition corpus created by Beijing Language and Culture University is adopted.  It 
contains the Chinese composition articles written by non-native Chinese learners.  There are 11,569 
articles and 4.24 million characters in 29 composition topics.  Composition articles are scanned into 
text and annotated with tags of error types ranging from character level, word level, sentence level, to 
discourse level.  There are 35,884 errors at sentence level, and WOEs are the most frequent type at this 
level.  Total 8,515 sentences are annotated with WOEs.  We filter out sentences with multiple error 
types and remove duplicate sentences. Total 1,150 error sentences with WOEs remain for this study. 
Two Chinese native speakers are asked to correct the 1,150 sentences.  Only reordering operation is 
allowed during correction.  A dataset composed of 1,150 sets of original sentence S and its two correc-
tions A1 and A2 is formed for training and testing in the experiments.  A1 may be different from A2.  
The following shows an example.  Without context, either A1 or A2 is acceptable. 
S:   ???????????????? 
      (She we encouraged to study music and foreign languages.) 
A1: ???????????????? 
      (We encouraged her to study music and foreign languages.) 
A2: ???????????????? 
      (She encouraged us to study music and foreign languages.) 
In some cases, A1 and/or A2 may be equal to S.  That is, the annotators may think S is correct.  That 
may happen when context is not available.  Finally, 327 of 1,150 sets contain different corrections.  
Both A1 and A2 are equal to S in 27 sets. Total 47 sentences corrected by one annotator are the same 
as the original sentences, and total 65 sentences corrected by another annotator are the same as the 
original sentences.  This corpus is available at http://nlg.csie.ntu.edu.tw/nlpresource/woe_corpus/. 
Figure 2 shows the Damerau Levenshtein distance between the original sentences S and the correc-
tions A1 and A2.  It counts the minimum number of operations needed to transform a source string 
into a target one.  Here the operation is the transposition of two adjacent characters.  Total 823 sets of 
A1 and A2 have a distance of 0.  It means 71.5% of sentences have the same corrections by the two 
Chinese native speakers.  The distances between S and A1 are similar to those between S and A2. To-
tal 850 sets of original sentences and the corrections have a distance below 10 characters and 1,014 
sets of sentences have a distance below 20.  We can also observe that the number of sentences with 
even distances is larger than that of sentences with odd distances because most of the Chinese words 
are composed of two characters. 
 
 Figure 2: Transposition distance among the original sentences and two corrections. 
282
5 Detection of Word Ordering Errors 
This section first defines the fundamental units for error detection, then introduces the error detection 
models along with their features, and finally presents and discusses the experimental results. 
5.1 Fundamental Units for Reordering 
Permutation is an intuitive way to find out the correct orderings, but its cost is very high. Unrestrictive 
permutation will generate too many candidates to be acceptable in computation time.  What units to be 
reordered in what range under what condition has be considered. Chinese is different from English in 
that characters are the smallest meaningful units, and there are no clear word boundaries.  Computa-
tion cost and segmentation performance is a trade-off to select character or word as a reordering unit. 
On the one hand, using words as the reordering units will reduce the number of candidates generated. 
On the other hand, word segmentation results will affect the performance of WOE detection and cor-
rection.  The following two examples show that reordering the words cannot generate the correct an-
swers. In these two examples, a word in the original sentence (S) is segmented into two words in the 
correct sentence (A). These words are underlined. Because a word is regarded as a unit for reordering, 
the correct sentence cannot be generated by word reordering only in these two cases. 
S:  ? / ?? / ??? / ?? /? 
      (He / teach to / students / English / .) 
A: ? / ? / ??? / ? / ?? / ? 
      (He / for / students / teach / English / .) 
S:  ?? / ? / ?? / ? / ?? / ? / ??? 
      (Recently / I / start to / learn / China / ?s / cooking cuisine.) 
A: ?? / ? / ?? / ? / ? / ?? / ? / ?? 
      (Recently / I / start to / learn / cooking / China / ?s /cuisine.) 
Total 76 sets of sentences belong to such cases. They occupy 6% of the experimental dataset. Consid-
ering the benefits of words, we still adopt words as reordering units in the following experiments.   
To prevent reordering all the words in the original sentences, we further divide a sentence into seg-
ments based on comma, caesura mark, semi-colon, colon, exclamation mark, question mark, and full 
stop. Sentence segments containing WOEs will be detected and words will be reordered within the 
segments to generate the candidates for correction.  In our dataset, there are only 31 sets of sentences 
(i.e., 2.7%) with WOEs across segments.  The following shows two examples. The underlined words 
are moved to other segments. 
S: ??????????????????? 
   (In fact, when I am still working, I am not honest.) 
A: ??????????????????? 
   (In fact, when I am working, I am still not honest.) 
S: ??????????????????? 
    (Therefore we have absolute guide work experience, we do not need retraining.) 
A: ??????????????????? 
    (We have absolute guide work experience, therefore we do not need retraining.) 
In summary, the upper bound of the correction performance would be 91.3%.  That is, 6%+2.7% of 
sentences cannot be resolved. 
5.2 Word Ordering Errors Detection Models 
Conditional random fields (CRFs) (Lafferty, 2001) are used to implement the WOE detection in sen-
tence segments. Segments with WOEs are labelled with answer tags before training. The original sen-
tence S written by non-native Chinese learner is compared with the annotated correct sentence A. 
Characters are compared from the start and the end of sentences, respectively. The positions are 
marked ERRstart and ERRend once the characters are different. All words within ERRstart and ERRend are 
marked ERRrange. The longest common subsequence (LCS) within ERRrange of S and ERRrange of A are 
excluded from ERRrange and the remaining words are marked ERRwords.  Figure 3 shows an example.  
We use BIO encoding (Ramshaw and Marcus, 1995) to label segments with WOEs. Segments contain-
283
ing words in ERRwords are defined to be segments with WOEs. The leftmost segment with WOEs is 
tagged B, and the following segment with WOEs are tagged I. Those segments without WOEs are 
tagged O. 
 
 Figure 3: An example for ERRrange and ERRwords. 
 
Table 1 lists the distribution of B, I and O segments. Recall that two Chinese native speakers are 
asked to correct the 1,150 sentences, thus we have two sets of B-I-O tagging.   
 
Tagging? B Tag I Tag O Tag Total 
Statistics? #Segments Percentage #Segments Percentage #Segments Percentage Segments
Annotator 1 1111 40.6% 53 1.9% 1572 57.5% 2736 
Annotator 2 1097 40.1% 59 2.2% 1580 57.7% 2736 
Table 1: Distribution of B, I, and O segments. 
 
Five features are proposed as follows for CRF training. Google Chinese Web 5-gram corpus (Liu, 
Yang and Lin, 2010) is adopted to get the frequencies of Chinese words for fPMI, fDiff and fLM. 
(1) Segment Pointwise Mutual Information (fPMI) 
PMI(Segi) defined below measures the coherence of a segment Segi by calculating PMI of all 
word bigrams in Segi. To avoid the bias from different lengths, the sum of PMI of all word bi-
grams is divided by n-1 for normalization, where n denotes the segment length. The segment 
PMI values are partitioned into intervals by equal frequency discretization. Feature fPMI of the 
segment Segi reflects the label of the interval to which PMI(Segi) belongs. 
 
(2) Inter-segment PMI Difference (fDiff) 
Feature fDiff captures the PMI difference between two segments Segj-1 and Segj. It aims to meas-
ure the coherence between segments. The feature setting is also based on equal frequency dis-
cretization.  
(3) Language Model (fLM) 
Feature fLM uses bigram language model to measure the log probability of the words in a seg-
ment defined below. Labels of interval are also determined by equal frequency discretization. 
 
(4) Tag of the previous segment (fTag) 
Feature fTag reflects the tag B, I or O of the previous segment. 
(5) CRF bigram template (fB) 
Feature fB is a bigram template given by SGD-CRF tool1.  Bigram template combines the tags of 
the previous segment and current segment, and generates T*T*N feature functions, where T is 
number of tags and N is number of strings expanded with a macro. 
                                                 
1 http://leon.bottou.org/projects/sgd 
284
5.3 Results and Discussion 
WOE detection models will annotate the segments of a sentence with labels B, I or O. These labels 
will determine which segments may contain WOEs. In the experiments, we use 5-fold cross-validation 
to evaluate the proposed models. Performance for detecting WOEs is measured at the segment and the 
sentence levels, respectively. The metrics at the segment level are defined as follows.  Here set nota-
tion is adopted. The symbol |S| denotes the number of elements in the set S which is derived by the 
logical formula after vertical bar. TAGpred(SEG) and TAGans(SEG) mean the labels of segment SEG 
tagged by WOE detection model and human, respectively.  The symbol m denotes total number of 
segments in the test set. 
 
 
 
 
  
The metrics at the sentence level are defined as follows: 
	
 
 
Accuracy and F1-score measure whether the models can find out segments with WOEs. Correcta-
ble Rate of sentences measures whether it is possible that the candidates of the correct word order can 
be generated by the WOE correction models. If a segment without WOEs is misjudged to be erroneous, 
the word order still has a chance to be kept by the WOE correction models. However, if a segment 
with WOEs is misjudged to be correct, words in the misjudged segment will not be reordered in the 
correction part because the error correction module is not triggered.  A sentence is said to be ?correct-
able? if no segments in it are misjudged as ?correct?. The ratio of the ?correctable? sentences is con-
sidered as a metric at the sentence level.  
Table 2 shows the performance of WOE detection. Five models are compared. We regard tagging 
all the segments with the labels B and O respectively as two baselines. Clearly, the recall at the seg-
ment level and the correctable rate at the sentence level are 1 by the all-tag-B baseline.  However, its 
accuracy at the segment and the sentence levels are low. The all-tag-O baseline has better accuracy at 
the segment level than the all-tag-B baseline, but has very bad F1-score, i.e., 0. The proposed models 
are much better than the two baselines. Among the feature combinations, fPMI fDiff fTag fB show the best 
performance. The accuracy at the segment level is 0.834, and the correctable rate is 0.883.  The best 
detection result will be sent for further correction.  
 
Model Segment Sentence Accuracy Recall Precision F1-Score Accuracy Correctable Rate
Baseline (all tag B) 0.404 1.000 0.424 0.595 0.271 1.000 
Baseline (all tag O) 0.576 0.000 0.000 0.000 0.074 0.074 
fPMI fLM fTag fB 0.830  0.781 0.802  0.791 0.787  0.862  
fPMI fDiff fTag fB 0.834  0.795 0.805  0.800  0.788  0.883  
fPMI fDiff fLM fTag fB 0.831  0.769 0.823  0.795  0.777  0.850  
Table 2: Performance of word ordering error detection 
285
6 Correction of Word Ordering Errors 
This section deals with generating and ranking candidates to correct WOEs. Two datasets, Cans and Csys, 
are explored in the experiments. We evaluate the optimal performance of the WOE correction models 
with the Cans dataset, and evaluate WOE detection and correction together with the Csys dataset. 
6.1 Candidate Generation 
Instead of direct permutation, we consider three strategies shown as follows to correct the error sen-
tences. The complexity of generating candidates by permutation is O(n!). The complexity of using 
these three strategies decreases to O(n2). 
(1) Reorder single unit (Rsingle) 
Rsingle strategy reorders only one reordering unit (i.e., a word) to n-1 positions within a 
segment containing n words. Total (n-1)2 candidates can be generated by this strategy. The 
following shows an example. 
S:  ?? / ?? / ? 
     (Today / school / go to) 
A: ?? / ? / ?? 
 (Today / go to / school) 
(2) Reorder bi-word (Rbi-word) 
Rbi-word is similar to Rsingle, but two reordering units are merged into a new word before re-
ordering. Because n-1 bi-words can be generated in a segment and n-2 positions are avail-
able for each merged bi-word, (n-1)(n-2) candidates are generated by Rbi-word. The follow-
ing shows an example. 
  S:  ?/?/??/??/?/?/?? 
                      (before / already / one / company / employ / me / work) 
A:  ??/??/?/?/?/?/?? 
          (one / company / before / already / employ / me / work) 
(3) Reorder tri-word (Rtri-word) 
Rtri-word works similarly to Rbi-word, but three reordering units are merged before reordering. 
Total (n-2)(n-3) candidates are generated by Rtri-word. The following shows an example. 
S: ?/??/??/?/??/?/?/?/??? 
           (I / need / working / (de) / experience / in / your / (de) / company.) 
A: ?/??/?/?/?/??/??/?/??? 
          (I / need / in / your / (de) / company / working / (de) / experience.) 
Table 3 shows the recall rate of each candidate generation strategy.  With the Cans dataset, correct 
word ordering can be generated for 85.8% of the original sentences by fusing Rsingle, Rbi-word and Rtri-word. 
The candidates generated by using the Csys dataset cover 69.7% of the correct word orderings. The dif-
ference would probably be due to the error propagation of word ordering error detection specified in 
Section 5.3. Furthermore, 6% of correct word orderings are unable to be generated by using the reor-
dering units due to the word segmentation issue as mentioned in Section 5.1. We can also find that 
72.3% of sentences with WOEs can be corrected by the Rsingle strategy using the Cans dataset. It means 
most of the WOEs made by non-native Chinese learners can be corrected by moving only one word. 
 
Strategy\Dataset Cans Csys 
Rsingle  0.723 0.577 
Rbi-word 0.365 0.308 
Rtri-word 0.239 0.217 
Rsingle ? Rbi-word ? Rtri-word 0.858 0.697 
Table 3: Recall of candidate generation strategies 
6.2 Candidate Ranking 
We use Ranking SVM (Joachims, 2002) for candidates ranking. Because WOEs may produce abnor-
mal POS sequence, POS bigrams and POS trigrams are considered as features for Ranking SVM. We 
286
use a k-tuple feature vector for each candidate sentence, where k is the number of features. In each di-
mension, binary weight is assigned: 1 if the feature exists in a candidate, and 0 otherwise. Score for 
each candidate is assigned by a binary classifier: 1 if the candidate is the same as either of the annotat-
ed corrections, and 0 otherwise. 
6.3 Results and Discussion 
Mean Reciprocal Rank (MRR) defined below is used for performance evaluation. The reciprocal rank 
is the multiplicative inverse of the rank of the first correct answer. MRR is the mean of reciprocal rank 
for all sentences S, value from 0 to 1. The larger MRR means the correct answer more closes to the top 
ranking. 
 
Percentage of answers having rank 1 is another metric. Five-fold cross-validation is used for training 
and testing. In the Cans and Csys datasets, 182.03 and 184.48 candidates are proposed by the approach 
of fusing the results of Rsingle, Rbi-word, and Rtri-word on the average.  Experimental results are listed in 
Table 4. The proposed candidate ranking method achieves an MRR of 0.270 in the Cans dataset. It 
means the correct candidates are ranked 3.7 on the average. In contrast, the MRR by using the Csys da-
taset is 0.208. It means the correct candidates are ranked 4.8 on the average when error detection and 
correction are performed in pipelining. 
 
Metric\Dataset Cans Csys 
MRR 0.270 0.208 
% of rank 1 0.195 0.144 
Table 4: Performance of candidate ranking 
 
There are some major types of errors shown as follows in WOE correction. 
(1) Word ordering errors across segments 
Section 5.1 mentions there are 31 sets of sentences (i.e., 2.7%) with WOEs across segments.  
Our algorithm cannot capture such kinds of sentences. 
(2) Propagation errors from candidate generation 
Table 3 shows the recall of word ordering error detection using the Cans dataset is 0.858. Be-
sides, 6% of sentences mentioned in Section 5.1 cannot be reordered to correct word ordering 
due to word segmentation issue. 
(3) Limitation of our models 
In the fused n-gram models, only one n-gram can be moved. It reduces the number of candi-
dates to be generated, but some types of reorderings are missed.  An example is shown as fol-
lows.  The 2-gram?? / ? (was born in) and the unigram? (on) have to be exchanged. 
S?? / ?? / ? / 1968? 10? 25? / ? / ???? 
           (I / was born / in / 25 October 1968 / on / Vienna.) 
A?? / ? / 1968? 10? 25? / ?? / ? / ???? 
         (I / on / 25 October 1968 / was born / in / Vienna.) 
7 Conclusion 
In this paper, we consider words as the reordering units in WOE detection and correction. Sentences 
are chunked into segments based on punctuation marks and the CRF technique is used to detect seg-
ments that possibly contain WOEs. The best error detection model achieves an accuracy of 0.834. 
Three reordering strategies are further proposed to generate candidates with correct word ordering and 
reduce the numerous number of candidates generated by permutation. If the segments containing 
WOEs are known, 85.8% of correct sentences can be generated by our approach. Finally, Ranking 
SVM orders the generated candidates based on POS bigrams and POS trigrams features, and achieves 
an MRR of 0.270 when all erroneous segments are given and an MRR of 0.208 when both detection 
and correction modules are considered. 
287
Using words as the reordering unit reduces the cost to generate numerous candidates, but 6% of sen-
tences are unable to reorder due to the word segmentation issue. How to balance the trade-off has to be 
investigated further. In the candidate ranking, selection of proper weights for POS bigram and trigram 
features may improve the ranking performance. Since the corpus of WOEs in Chinese is still in a lim-
ited size, expanding the related corpus for further research is also indispensable. 
Acknowledgements 
This research was partially supported by National Taiwan University and Ministry of Science and 
Technology, Taiwan under grants 103R890858, 101-2221-E-002-195-MY3 and 102-2221-E-002-103-
MY3.  We are also very thankful to the anonymous reviewers for their helpful comments to revise this 
paper. 
References 
Robert Dale, Ilya Anisimoff and George Narroway. 2012. HOO 2012: A Report on the Preposition and Deter-
miner Error Correction Shared Task. In Proceedings of The 7th Workshop on the Innovative Use of NLP for 
Building Educational Applications, pages 54?62, Montre?al, Canada. 
Robert Dale and Adam Kilgarriff. 2011. Helping Our Own: The HOO 2011 Pilot Shared Task. In Proceedings of 
the 13th European Workshop on Natural Language Generation (ENLG), pages 242?249, Nancy, France. 
John DeNero and Jakob Uszkoreit. 2011. Inducing Sentence Structure from Parallel Corpora for Reordering. In 
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 193?203, 
Edinburgh, Scotland, UK. 
Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Mod-
el. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 
848?856, Honolulu. 
Michael Gamon, Claudia Leacock, Chris Brockett, William B. Dolan, Jianfeng Gao, Dmitriy Belenko, and Alex-
andre Klementiev. 2009. Using Statistical Techniques and Web Search to Correct ESL Errors. CALICO Jour-
nal, 26(3):491?511. 
An-Ta Huang, Tsung-Ting Kuo, Ying-Chun Lai, and Shou-De Lin. 2010. Discovering Correction Rules for Auto 
Editing. Computational Linguistics and Chinese Language Processing, 15(3-4):219-236. 
Chang-ning Huang and Hai Zhao. 2007. Chinese Word Segmentation: A Decade Review. Journal of Chinese 
Information Processing, 21(3):8-19. 
Thorsten Joachims. 2002. Optimizing Search Engines using Clickthrough Data. In Proceedings of the Eighth 
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 133-142, Edmon-
ton, Alberta, Canada. 
John Lafferty, Andrew McCallum, and Fernando C.N. Pereira. 2001. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labelling Sequence Data. In Proceedings of the 18th International Conference on 
Machine Learning (ICML 2001), pages 282-289, San Francisco, CA, USA. 
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2014. Automated Grammatical Error 
Detection for Language Learners. 2nd Edition. Morgan and Claypool Publishers. 
Jia-Na Lin. 2011. Analysis on the Biased Errors of Word Order in Written Expression of Foreign Students. Mas-
ter Thesis. Soochow University. 
Fang Liu, Meng Yang, Dekang Lin. 2010. Chinese Web 5-gram Version 1. Linguistic Data Consortium, Phila-
delphia. http://catalog.ldc.upenn.edu/LDC2010T06. 
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-2013 
Shared Task on Grammatical Error Correction. In Proceedings of the Seventeenth Conference on Computa-
tional Natural Language Learning: Shared Task, pages 1?12, Sofia, Bulgaria. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text Chunking Using Transformation-based Learning. In 
Proceedings of Third Workshop on Very Large Corpora. Pages 82-94. 
Hendra Setiawan, Min-Yen Kan, Haizhou Li, and Philip Resnik. 2009. Topological Ordering of Function Words 
in Hierarchical Phrase-based Translation. In Proceedings of the 47th Annual Meeting of the ACL and the 4th 
IJCNLP of the AFNLP, pages 324?332, Suntec, Singapore. 
288
Li-Li Sun. 2011. Comparison of Chinese and English Word Ordering and Suggestion of Chinese Teaching for 
Foreign Learners. Master Thesis. Heilongjiang University. 
Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2007. A Comparative Evaluation of Deep and Shal-
low Approaches to the Automatic Detection of Common Grammatical Errors. In Proceedings of the 2007 
Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-
guage Learning, pages 112?121, Prague, Czech Republic. 
Zhuo Wang. 2011. A Study on the Teaching of Unique Syntactic Pattern in Modern Chinese for Native English-
Speaking Students. Master Thesis.  Northeast Normal University. 
Shih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee. 2013. Chinese Spelling Check Evaluation at SIGHAN Bake-
off 2013. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing (SIGHAN-7), 
pages 35?42, Nagoya, Japan. 
Chi-Hsin Yu and Hsin-Hsi Chen. 2012. Detecting Word Ordering Errors in Chinese Sentences for Learning Chi-
nese as a Foreign Language. In Proceedings of the 24th International Conference on Computational Linguis-
tics, pages 3003-3018, Mumbai, India. 
289
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 632?643, Dublin, Ireland, August 23-29 2014.
 Interpretation of Chinese Discourse Connectives  
for Explicit Discourse Relation Recognition 
 
 
Hen-Hsen Huang, Tai-Wei Chang, Huan-Yuan Chen, and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan 
{hhhuang, twchang}@nlg.csie.ntu.edu.tw; 
{b00902057, hhchen}@ntu.edu.tw 
 
  
 
Abstract 
This paper addresses the specific features of Chinese discourse connectives, including types 
(word-pair and single-word), linking directions (forward and backward linking), positions and 
ambiguous degrees, and discusses how they affect the discourse relation recognition. A semi-
supervised learning method is proposed to learn the probability distributions of discourse func-
tions of connectives from a small labeled dataset and a big unlabeled dataset. The statistics 
learned from the dataset demonstrates some interesting linguistic phenomena such as connec-
tive synonyms sharing similar distributions, multiple discourse functions of connectives, and 
couple-linking elements providing strong clues for discourse relation resolution.  
1 Introduction 
Discourse relation labeling determines how two discourse units cohere to each other. A discourse unit 
may be a clause, a sentence, or a group of sentences. The labeled relation has many potential applica-
tions. Coherence is considered as a metric to evaluate the essay writing by essay scorer (Lin et al., 
2011). Discourse relations are used to order sentences in an event in a summarization system (Der-
czynski and Gaizauskas, 2013). Sentiment transition of two clausal arguments is identified based on 
their discourse relation in sentiment analysis (Hutchinson, 2004; Zhou et al., 2011; Wang et al., 2012; 
Huang et al., 2013).  
The pioneer research of discourse has been established by Hobbs (1985), Polanyi (1988), Hovy and 
Maier (1992), and Asher and Lascarides (1995). Various discourse relation types have been defined in 
the frameworks such as Sanders et al. (1992), Hovy and Maier (1992), RST-DT (Carlson et al., 2002), 
Wolf and Gibson (2005), and PDTB (Prasad et al., 2008). Temporal, Contingency, Comparison, and 
Expansion, the four classes on the top level of PDTB sense hierarchy, are common used in the dis-
course relation labeling tasks. When two arguments are temporally related, they form a Temporal rela-
tion. The Contingency relation talks about the situation that the event in one argument casually affects 
the event in the other argument. Comparison is used to show the difference between two arguments. 
The last one relation, Expansion, is the most common. An Expansion relation either expands the in-
formation for one argument in the other one or continues the narrative flow. 
In the recent years, discourse relation recognition has been studied for different languages (Afan-
tenos et al., 2012, Cartoni et al., 2013). In explicit English discourse relation labeling tasks, the accu-
racy of the approach using just the connectives is already quite high, 93.67%, and incorporating the 
syntactic features raises performance to 94.15% (Pitler and Nenkova, 2009). In our previous work, we 
investigate Chinese intra-sentential relation detection and show an accuracy of 81.63% and an F-score 
of 71.11% in the two-way classification (Contingency vs. Comparison relations) when connectives are 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 
632
introduced as features (Huang and Chen, 2012a). We also report an accuracy of 27.10% and an F-
score of 24.27% in the four-way inter-sentential relation classification when only connectives are used 
(Huang and Chen, 2011). Sporleder and Lascarides (2008) point out some English connectives are of-
ten ambiguous between multiple discourse relations or between discourse and non-discourse usage, 
and Roze et al. (2010) report the ambiguity of French connectives. This issue also occurs in Chinese. 
Zhou et al. (2012) propose a framework to identify the ambiguous Chinese discourse connectives, and 
report an F-score of 74.81% in the four-way classification at the intra-sentential level. 
The above discourse relation labeling tasks are done on the datasets of different size for different 
languages at the intra-/inter-sentential levels, thus the results cannot be compared directly. However, 
these works show a tendency: discourse connectives are useful clues for explicit discourse relation 
recognition, and the uses of Chinese connectives in discourse relation labeling are more challenging 
than those of English connectives. In comparison with English, the connectives in Chinese are more 
and their parts of speech are diverse. There are 100 English explicit connectives annotated in the 
PDTB 2.0. In Chinese, the linguists report a list of 808 discourse connectives (Cheng and Tian, 1989; 
Cheng, 2006). In addition, the Chinese discourse connectives have a variety of parts-of-speech. For 
example, ?? (ji? sh?, suppose) is a verb and listed as a discourse connective of the Contingency rela-
tion. 
The following examples address some specific features of Chinese discourse connectives. On the 
one hand, the two words, ???? (su? r?n, although) and ???? (d?n sh?, but), which form a word-pair 
connective, appear in the two discourse units shown in (S1), respectively. These two units demonstrate 
a Comparison relation. On the other hand, ???? (su? r?n, although) and ???? (d?n sh?, but) can ap-
pear individually as single-word connectives shown in (S2)-(S6). The two discourse units have differ-
ent discourse relations when the single-word connectives appear at different positions, i.e., (S2): Com-
parison, (S3): Comparison, (S4): Expansion, (S5): Comparison, and (S6): Expansion. Furthermore, 
the short word ??? (?r) can be an individual connective, which is interpreted as ???? (and), ???? 
(but), or ???? (thus), and serves as functions of Expansion, Comparison, and Contingency, respec-
tively. In addition, it can be linked with ???? (su? r?n, although) and ???? (y?n w?i, because) to be 
word-pair connectives, which are interpreted as Comparison and Contingency functions in (S7) and 
(S8), respectively. These examples demonstrate word-pair connectives composed of a same word and 
other words may have different discourse functions, so does the same single-word connective at dif-
ferent positions. 
 
(S1) ????????????????(Although Tom is smart, he doesn?t study hard.) 
(S2) ??????????????(Although Tom is smart, he doesn?t study hard.) 
(S3) ??????????????(He sweated a lot, although he went only a few miles.) 
(S4) ????????????????????(I'll read, even if I really feel spider terrible.) 
(S5) ??????????????(Tom is smart, but he doesn?t study hard.) 
(S6) ?????????????(But in Paris, he gave up studying medicine.) 
(S7) ??????????????(Although you did not say, I knew that smell.) 
(S8) ??????????????(Because he came home late, he was scolded by his mother.)  
 
In this paper, we investigate special features of Chinese discourse connectives and apply the results 
to discourse relation labeling. A semi-supervised learning algorithm is proposed to estimate the proba-
bility distribution of the discourse functions of each connective. We address the issue of ambiguity 
between multiple discourse relations of Chinese connectives. The ambiguity between discourse and 
non-discourse usages is not our focus in this paper. This paper is organized as follows. Section 2 anal-
yses the types of Chinese connectives and their forward/backward linking properties. Section 3 pre-
sents a semi-supervised method to deal with the probability distributions of discourse functions of 
Chinese connectives and discourse relation labeling. The experimental results are shown and discussed. 
In Section 4, we further introduce the discourse relation labeler to annotate 302,293 unlabeled sen-
tences and analyze the linguistic phenomena of discourse connectives. We conclude this work in Sec-
tion 5. 
633
2 Types of Discourse Connectives 
From the surface form, there are three kinds of linking elements in Chinese (Li and Thompson, 1981): 
forward-linking elements, backward-linking elements, and couple-linking elements. Discourse con-
nectives are such kinds of linking elements. A discourse unit containing a forward-linking (backward-
linking) element is linked with its next (previous) discourse unit. A couple-linking element is a pair of 
words that exist in two discourse units (Chen, 1994).  
Figure 1 shows connectives and their linking direction. The word-pair connective ???...??? 
(su? r?n?d?n sh?, although?but) in (S1) is a couple-linking element. A single-word connective may 
function as a forward-linking element and/or a backward-linking element. It may be a word appearing 
in a word-pair connective, e.g., ???? (su? r?n, although), or a word existing individually, e.g., ???? 
(y? j?, and). A single-word connective which is the first (the second) word of a word-pair connective 
may function as a forward-linking (backward-linking) element. The single-word connective ???? 
(su? r?n, although) in (S2) is a typical example. It keeps the major discourse function, i.e., Comparison, 
of the word-pair connective that it belongs when it appears in the first discourse unit. In contrast, it 
may become ambiguous when its position is reversed from the first to the second (i.e., S3 and S4). It 
may link to the previous or the next discourse units. S5 and S6 have the similar behaviors. The single-
word ???? (d?n sh?, but) in (S5) shows a backward-linking. In (S6), it is shifted to the first position 
and becomes ambiguous. It may be linked to the previous, or to the next discourse units. The correct 
interpretation depends on the context. These phenomena show a single-word connective may have dif-
ferent senses when it is not at its original position. 
 
  
   
 
  Figure 1: Examples for forward linkging and backward linking. 
 
In this study, we collect 808 discourse connectives based on Cheng and Tian (1989), Cheng (2006), 
and Lu (2007). The discourse connective lexicon contains 319 single-word and 489 word-pair connec-
tives. Initially, each connective is associated with only one discourse function manually by linguists. 
634
For example, the word-pair connective, ???...??? (su? r?n?d?n sh?, although?but), is assigned a 
Comparison function. The assignment is one-to-one mapping, thus it cannot capture the complete dis-
course functions of Chinese connectives. Table 1 shows an overview of the discourse connective lexi-
con. In this lexicon, Expansion is the majority, and Comparison is the minority. The percentages of 
Contingency and Expansion are close. Temporal is the third largest discourse function. Intuitively, the 
discourse connective lexicon cannot cover all their senses. To learn the probability distribution of the 
discourse functions of a connective needs a large-scale discourse corpus. Compared with RST-DT 
(Carlson et al., 2002) and PDTB (Prasad et al., 2008), Chinese discourse corpora are not publicly 
available (Zhou and Xue, 2012; Huang and Chen, 2012b). 
 
Discourse Function Number of Connectives Examples of Single-Word and Word-Pair Discourse Connectives 
Temporal 151 (18.69%) ?? (ji? zhe, then), ??...?? (zu? ch??xi?n z?i, first...now) 
Contingency 261 (32.30%) ?? (y?n w?i, because), ?...? (r??z?, if ... then) 
Comparison 87 (10.77%) ?? (j? sh?, even if), ???? (j?n gu?n?d?n, although?but)  
Expansion 309 (38.24%) ?? (l?ng w?i, besides), ????? (b? j?n??r qi?, not only?but also) 
Table 1: A Chinese discourse connective lexicon. 
3 Learning Discourse Functions of Connectives 
This section proposes a semi-supervised learning method to learn the interpretation of discourse con-
nectives from an incomplete and sparse dataset. 
3.1 A Semi-Supervised Learning Algorithm 
Given a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation 
classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expan-
sion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1?c2, 
where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. 
Each discourse unit is mapped into a representation. Various features from different linguistic levels 
have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 
2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particu-
lar on the probability distributions of the discourse functions and the positions of connectives. 
  
Length. This feature includes the word counts of ds1 and ds2. 
Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation 
includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is 
dropped from the features because it is always a comma.   
Words. The bags of words in ds1 and ds2 are considered.  
Hypernym. The bags of hypernyms of the words in ds1 and ds2 are considered. A Chinese thesau-
rus, Tongyici Cilin1, is consulted. The categorization scheme at the fourth level is adopted. 
Shared Word. The number of words shared in ds1 and ds2 is considered as a feature. 
Collocated Word. Collocated words are word pairs mined from the training set. The first and the 
second words of a pair come from ds1 and ds2, respectively. 
POS. The bags of parts of speech in ds1 and ds2 are considered. 
Polarity. Polarity and discourse relation may be related (Huang et al., 2013; Zhou, et al., 2011). 
For example, a Comparison relation implies its two discourse units are contrasting, and some contrasts 
are presented with different polarities. We estimate the polarity of ds1 and ds2 by a lexicon-based ap-
proach. The polarity score and the existence of negation are taken as features. 
Discourse Connective. A discourse connective c is represented as a probability distribution of dis-
course functions denoted by a quadruple (P(c,temporal), P(c,contingency), P(c,comparison), P(c,expansion)), where 
P(c,temporal), P(c,contingency), P(c,comparison), and P(c,expansion) indicate the probabilities of the four discourse func-
tions of c, such that P(c,temporal)+P(c,contingency)+P(c,comparison)+P(c,expansion)=1. Section 3.3 shows how we as-
sign the probabilities to each connective in different experimental settings.    
Position. The linguistic phenomena discussed in Section 2 show a single-word connective at dif-
ferent position may play different discourse function. Thus, the position of c is considered as a feature. 
                                                 
1 http://ir.hit.edu.cn/ 
635
Because the number of Chinese connectives is large (e.g., 808 Chinese connectives in our lexicon) 
and the large-scale labeled Chinese discourse corpus is not available, how to learn the probability dis-
tribution is a challenging issue. This paper proposes a semi-supervised learning method as follows. Its 
pseudo code is shown in Algorithm 1. 
 
(1) Train a 4-way discourse relation classifier drc with the training set and LIBSVM (Chang and 
Lin, 2011). 
(2) Initialize probability distributions of unknown connectives in the test set (see experiments). 
(3) Use drc to label all the instances in the test set. 
(4) Compute the new probability distribution of discourse functions of each connective based on 
the labeled results in the current run. Maximum likelihood estimation is adopted. 
(5) Repeat (3) and (4) until the number of label changes between two successive runs is below 1%. 
 
Algorithm 1. Probability Estimation for the Discourse Functions of Connectives 
Input:  
D={Temporal, Contingency, Comparison, Expansion}: a set of discourse relations and discourse 
functions for argument pairs and discourse connectives, 
C={c1, c2, ?, cn}: a set of n discourse connectives, 
S={s1, s2, ?, sp}: a set of p labeled argument-pairs [sa1, sa2] containing connective c?CS?C, each 
with a label d?D, where CS is a set of connectives appearing in S, 
T={t1, t2, ?, tq}: a set of q unlabeled argument-pairs [ta1, ta2] containing connective c?CT?C, where 
CT is a set of connectives appearing in T. 
Output:  
Q={q1, q2, ?, qn}: a probability distribution qi for connective ci?C. 
Method: 
1. Initialization 
1) Train a classifier drc using S. 
2) Initialize the probability distribution with equal weight, (0.25, 0.25, 0.25, 0.25), for connec-
tive c ? CT-CS, and build Q(0). 
3) i ? 0 
2. Relation labeling 
For each t ? T, estimate the probabilities of four discourse relations, P(t,temporal), P(t,contingency), 
P(t.comparison), and P(t.expansion), using the classifier drc with Q
(i). 
3. Updating the probability distribution 
1) For each c ? C, compute the average probability of each discourse relation among the argu-
ment-pairs containing c in T:  
P(c,tempora)l ? Average of P(t,temporal) for all t containing c in T. 
P(c,contingency) ? Average of P(t,contingency) for all t containing c in T. 
P(c,comparison) ? Average of P(t,comparison) for all t containing c in T. 
P(c,expansion) ? Average of P(t,expansion) for all t containing c in T. 
2) Form a new Q(i+1) 
3) i ? i+1 
4. Repeat steps 2-3 until the ratio of the number of label changes by previous and current runs is less 
than 1%. 
5. Q ? Q(i) 
 
3.2 Experimental Setup 
For the corpus study of discourse connectives and discourse relations, we refer to a public available 
Chinese Web POS tagged corpus (Yu et al., 2012). This Chinese POS-tagged corpus is developed 
based on the ClueWeb09 dataset (CMU, 2009), where Chinese material is the second largest.  To cap-
ture the discourse functions of individual connectives more accurately, the following three criteria are 
used to sample sentences: 
 
1. A sentence should contain only two clauses. 
2. A sentence should contain exact one discourse connective. 
636
3. The lengths of both clauses in a sentence are no more than 20 Chinese characters.  
 
Total 7,601 sentences composed of two discourse units linked by a connective are sampled from a 
public available Chinese Web POS tagged corpus (Yu et al., 2012). Each sentence is annotated with a 
most likely discourse relation selected from {Comparison, Contingency, Comparison, Expansion} by 
three annotators guided by an instruction manual. The majority is taken as the ground truth. A mentor 
is involved to make a final decision for the tie conditions. The inter-agreement among the annotators is 
0.41 in Fleiss? Kappa values, which is a moderate agreement. The discourse category with the lowest 
inter-annotation agreement is Temporal, which annotators usually confuse with Expansion. It shows 
the difficulty to distinguish Temporal and Expansion even by human. Table 2 shows the statistics of 
the corpus. More than 50% of pairs are annotated with Expansion relation. The second largest group is 
Contingency relation. The percentages of Temporal and Comparison relations are near. Only 359 con-
nectives appear in the corpus. That reflects the incompleteness issue. 
 
Discourse Relation # Instances Percentage 
Temporal 846 11.13% 
Contingency 1,594 20.97% 
Comparison 926 12.18% 
Expansion 4,235 55.72% 
Table 2: Statistics of the experimental discourse corpus. 
 
This Chinese discourse corpus is used for training and testing. We set up the experiments to simu-
late the scenario of estimating the probability distributions of discourse functions of the unknown con-
nectives based on the information in the training set. We evaluate the experimental results by 5-fold 
cross-validation. To ensure the discourse connectives appearing in the test set are mutual exclusive of 
those connectives in the training set, we split the discourse connectives into 5 mutual exclusive sets 
and split all the 7,601 sentences into 5 folds according to the 5 sets of discourse connectives.  
The kernel of our SVM classifier is the radial basis function. The two parameters, cost c and gamma 
g, are optimized by the grid-search algorithm within the range c ? {2-5, 2-3, 2-1, ?, 215} and g ? {2-15, 
2-13, 2-11, ?, 23}.  
3.3 Results and Discussions 
To demonstrate the performance of our proposed semi-supervised learning methods, the following five 
models are experimented and compared. 
 
M0:  Label the relation between two discourse units linked by a connective c based on the c?s dis-
course function defined in the connective lexicon. M0 is considered as a baseline model. 
M1: Train a 4-way discourse relation classifier drc with the training set, then initialize the function 
probability distributions of the unknown connectives to (0.25, 0.25, 0.25, 0.25), and finally la-
bel all the pairs of discourse units by the classifier drc. M1 is a supervised-learning method. 
M2: M2 model is similar to M1 model except that the probability distribution (p(c,temporal), p(c,contingency), 
p(c,comparison), p(c,expansion)) of an unknown connective is initialized based on its setting in the con-
nective lexicon. The probability of the unique function is set to 1, and the others are set to 0. 
M3: M3 is a semi-supervised learning method. In testing, the function probability distributions of 
the unknown connectives are initialized to (0.25, 0.25, 0.25, 0.25). Discourse relation labeling 
and probability distribution updating are done iteratively. Finally, all the test instances are la-
beled, and probability distributions of discourse functions are learned for all test connectives. 
M4: M4 is similar to M3 except that the initial probability distributions are set based on the connec-
tive lexicon. 
 
Table 3 compares the performances of these five models. The average tendency is 
M4>M3>M2>M1>M0. It shows the proposed two semi-supervised learning methods are significantly 
better than the baseline model M0 and the two supervised-learning methods M1 and M2 at p=0.001. 
The best model is M4, but the performance differences between M3 and M4 are not significant. It 
demonstrates that both the two initial assignments, i.e., equal-weight assignment and lexicon-based 
637
assignment, are effective. If a connective is not listed in the lexicon due to its coverage, we can still 
derive its probability distribution starting from the equal-weight approach. 
We further examine the individual performance of each discourse relation. Comparing M1 and M3, 
the semi-supervised classifier (M3) outperforms the supervised classifier (M1) in all three metrics in 
all the four relations except recall and F-score in the Temporal relation. Because more than one half of 
the pairs of discourse units annotated with Temporal relation whose discourse connectives have Ex-
pansion function in the connective lexicon, some discourse-units of Temporal relation are misclassi-
fied as Expansion relation. That is why the recall is dropped by 8.22% in M3. The precisions of all the 
four relations are increased. In particular, the precisions of Temporal, Contingency, and Comparison 
gain more than 10%. The overall F-score is increased 6.61%. 
Moreover, M4 is better than M2 in F-score for all the relations. In particular, the precisions of Tem-
poral, Contingency, and Comparison recognition by M4 are greatly increased. In other words, the 
boosting algorithm tends to correct those instances that are originally misclassified into the Expansion 
relation. The t-test also confirms M4 has a significant improvement over M2 at p=0.001. 
The semi-supervised algorithm learns the probability distributions of discourse functions of the un-
known connectives from the test instances, so that their size may affect the performance. Figure 2 ana-
lyzes how the number of test instances of a connective affects the performance. Each point (x, y) in 
this figure denote a connective, where x is its total occurrences in the test set, and y is its F-score in 
Figure 2(a) and its precision/recall in Figure 2(b). We can find (1) many connectives have good per-
formance, (2) connectives containing more test instances demonstrate better performance, and (3) 
connectives containing fewer instances are sensitive to the evaluation. We treat the probability distri-
bution of discourse functions of each connective as a vector of four real numbers and compute the co-
sine similarity among the distributions of connectives derived by the connective lexicon, human anno-
tators, and our best model M4. When the 114 connectives containing more than 10 instances are 
counted, the average cosine similarity between our model and human is 0.940, and the average cosine 
similarity between the connective lexicon and human is 0.767. 
 
Metric Model Temporal Contingency Comparison Expansion Average 
 M0 0.3933 0.7124 0.5092 0.7364 0.6656 
 M1 0.5618 0.6005 0.5982 0.7147 0.6595 
Precision M2 0.5024 0.7038 0.5332 0.7529 0.6879 
 M3 0.6682 0.7652 0.7749 0.7254 0.7334 
 M4 0.6708 0.7773 0.7869 0.7373 0.7344 
 M0 0.3757 0.6014 0.6588 0.7389 0.6600 
 M1 0.5371 0.5098 0.4154 0.8114 0.6694 
Recall M2 0.4808 0.5808 0.6207 0.7578 0.6731 
 M3 0.4549 0.5387 0.5065 0.9015 0.7276 
 M4 0.4480 0.5803 0.5821 0.8985 0.7299 
 M0 0.3843 0.6522 0.5744 0.7376 0.6606 
 M1 0.5492 0.5515 0.4903 0.7600 0.6644 
F-score M2 0.4913 0.6364 0.5736 0.7553 0.6805 
 M3 0.5413 0.6323 0.6126 0.8039 0.7305 
 M4 0.5372 0.6645 0.6691 0.8099 0.7322 
Table 3: Performance comparisons among models. 
 
       
            (a) F-Score                                                      (b) Precision/Recall 
Figure 2: Effects of the number of test instances for each connective on relation labeling. 
638
4 Further Analyses on a Big Dataset 
We further apply the best model (M4) to predict the probability distributions of discourse functions of 
connectives on a big dataset. For each discourse connective c, up to 500 sentences composed of two 
discourse units linked by c are randomly selected from the Chinese Web POS tagged corpus (Yu et al., 
2012). The limitation of 500 is set to reduce the imbalance among the discourse connectives. Some 
connectives appear quite often in the dataset, e.g., the connective ??? (y?, also). Some connectives 
appear less than 500 times, e.g., ??????? (qi?n w?n?b? r?n, must...otherwise) occurs only 212 
times. Finally, total 302,293 sentences are extracted and predicted. Because the dataset is very large, it 
is not easy to evaluate each pair of discourse units. We examine the linguistic phenomena instead. A 
lexicon of the probability distributions of connectives estimated by M4 is available at 
http://nlg.csie.ntu.edu.tw/ntu-discourse/. 
We sort the discourse connectives by the ratios of their largest relations. In this way, the top connec-
tives in this order almost contain one relation. They can be considered to be less ambiguous. The top 
ten connectives which appear 500 times are shown in Table 4. Note the bracket notation [ds1, ds2] de-
notes the discourse units where connectives appear. The discourse function defined in the discourse 
connective lexicon specified in Section 2 is marked in bold. The probabilities of the major discourse 
function of these connectives are larger than 0.89. The distribution is consistent with the human as-
signment except the last connective ???...??? (ch? f?i...b? r?n, unless...otherwise), which is as-
signed to Contingency in the lexicon. This connective denotes a negated cause-effect relation between 
ds1 and ds2 in which ds2 is the effect when ds1 is not satisfied. In such a case, ds1 and ds2 show clear 
contrast, so that it is reasonable to label this connective with a higher probability of the Comparison 
relation. There are two groups of synonyms in the list: (1) ???...??? (su? r?n?b? gu?, alt-
hough?but) and ???...??? (su? r?n?k? sh?, although?but), and (2) ????? (ji?n y?n zh?, in 
short) and ?????? (ji?n ?r y?n zh?, in short). Table 4 shows that synonyms share similar distribu-
tions. The cosine similarities of their probability distributions are 0.99996 and 0.99952, respectively.  
The probability of each discourse function of each connective c is the average of the probabilities 
estimated by the classifier, thus the distributions reported by our model is not completely identical to 
the empirical distribution. For example, all the instances containing the connective ???...??? (su? 
r?n?b? gu?, although?but) are labeled with the major discourse function Expansion, but the esti-
mated probability of Expansion of this connective is 93.47%. 
We also sort the discourse connectives by the ratio of their second largest relations. In this manner, 
the top connectives in this order may have two major discourse functions. In other words, they are 
ambiguous. Table 5 shows the top ten estimated ambiguous discourse connectives. It is interesting that 
Expansion is one of the two major discourse functions, and the other one shown in bold is the dis-
course function defined in the connective lexicon. The discourse connectives ????? (j?n ji? zhe, 
then), ???? (xi?n z?i, now), ???? (w?i l?i, in the future), and ???? (zh?ng y?, finally), which 
are defined to have Temporal function in the lexicon, frequently occur in the discourse units with Ex-
pansion relation. The estimated distribution of the connective ??? (?r, and; but; thus) is consistent 
with the human interpretation, i.e., it has multiple discourse functions.  
Chinese single-word connectives are usually put together with other words to form word-pair con-
nectives. Tables 6 and 7 show examples for ???? (su? r?n, although) and ???? (su? y?, so),  
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[???, ?] ([in short, ?]) 2.78 2.08 1.67 93.47 
[??, ??] ([although, but]) 0.77 1.80 92.70 4.74 
[???, ?] ([in other words, ?]) 3.63 2.82 1.53 92.02 
[??, ??] ([although, but]) 0.93 2.11 91.58 5.37 
[??, ??] ([since, therefore]) 1.41 91.07 0.97 6.55 
[???, ] ([after all, ?]) 3.17 3.95 2.97 89.91 
[?, ???] ([?, after all]) 3.13 4.34 2.84 89.69 
[????, ] ([in short, ?]) 5.07 3.20 2.25 89.48 
[??, ??] ([or, or]) 3.94 4.51 2.16 89.39 
[??, ??] ([unless, otherwise]) 1.04 3.71 89.33 5.93 
Table 4: Top 10 less-ambiguous connectives estimated by using a big dataset. 
639
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[???, ?] ([then, ?]) 48.71 5.12 1.70 44.46 
[?, ??] ([?, even though]) 3.93 5.23 46.48 44.36 
[??, ?] ([now, ?]) 44.31 7.42 3.42 44.85 
[?, ??] ([?, although]) 3.60 3.68 44.17 48.55 
[??, ?] ([so that, ?]) 3.96 49.83 2.05 44.16 
[??, ??] ([now, in the future]) 47.05 6.41 3.10 43.44 
[??, ?] ([only, then]) 4.34 43.30 9.33 43.03 
[??, ?] ([in the future, ?]) 48.21 6.15 2.85 42.79 
[?, ?] ([?, and; but; thus])  3.72 6.13 42.78 47.37 
[?, ??] ([?, finally]) 42.39 6.13 2.99 48.49 
Table 5: Some ambiguous connectives estimated by using a big dataset. 
 
respectively. The former is often connected with a word in the second discourse unit to form a couple-
linking, while the latter is connected with a word in the first one. We can find word-pair connectives 
are less ambiguous than single-word connectives in different probabilities. The former (????, su? 
r?n, although) tends to have Comparison function. When the word-pair connectives are shorten to sin-
gle-word connectives, the probability to have Comparison function becomes lower. The connective 
???? (su? r?n, although) in the first argument still has probability 0.7639 to have Comparison func-
tion. When ???? (su? r?n, although) is moved to the second argument, the probability to serve as 
Comparison function is decreased to 0.4417, which is even lower than that of Expansion function. It 
shows that couple-linking elements provide strong clue to determine discourse relation. Besides, a sin-
gle-word connective has some tendency to function as either forward linking or backward linking. For 
example, ???? (su? r?n, although) is a forward-linking element. Normally, it will link the first dis-
course unit containing it with the second one. When it appears in the second discourse unit, it becomes 
ambiguous. The connectives containing ???? (su? y?, so) have the similar effects. It tends to be a 
backward linking element, so its companion appears in the first discourse unit. Its probability to have 
Contingency function decreases from a word-pair connective to a single-word connective. When it 
appears in the first discourse unit, it may link to the previous sentence at the inter-sentential level.  
Some Chinese short words like ??? (?r) is often a part of word-pair connectives. Table 8 shows 10 
words which are often connected with ??? (?r) to form word-pair connectives. The word-pair connec-
tives tend to have one major function. When the word-pair connective is ?abbreviated? to a single- 
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[??,??] ([although, but]) 0.77 1.80 92.70 4.74 
[??,??] ([although, but]) 0.93 2.11 91.58 5.37 
[??,??] ([while, however]) 1.04 2.03 90.76 6.17 
[??,??] ([although, but]) 1.14 2.62 88.49 7.74 
[??,?] ([although, but]) 1.48 2.89 87.54 8.09 
[??,?] ([although, still]) 2.70 3.43 85.20 8.68 
[??,?] ([although, still]) 3.06 4.10 81.03 11.81 
[??,?] ([although, while]) 2.86 5.09 79.23 12.82 
[??,??] ([although, still]) 3.68 5.70 77.23 13.39 
[??,??] ([although, still]) 3.51 8.54 75.26 12.69 
[??,?] ([although, still]) 4.24 3.71 74.58 17.47 
[??, ?] ([although, ?]) 3.46 5.28 76.39 14.87 
[?, ??] ([?, although]) 3.60 3.68 44.17 48.55 
Table 6: Effects of single-word and word-pair connectives containing ???? (su? r?n, although). 
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[??, ??] ([because, so]) 1.64 85.25 1.77 11.35 
[?, ??] ([because, so]) 2.26 83.20 1.82 12.72 
[??, ??] ([because, so]) 2.69 78.03 2.35 16.93 
[??, ??] ([since, so]) 1.68 67.32 6.37 24.63 
[?, ??] ([?, so]) 2.82 50.67 5.29 41.22 
[??, ?] ([so, ?]) 5.71 50.61 2.50 41.18 
Table 7: Effects of single-word and word-pair connectives containing ???? (so). 
640
word connective, it becomes ambiguous. The discourse function depends on which word-pair connec-
tive it is mapped. The determination relies on contextual information. 
Table 9 further shows the effects of positions of single-word connectives. The major discourse func-
tion of the first 7 sets of connectives is changed when the connectives are shifted from the first dis-
course unit to the second one. In contrast, the last 3 sets of connectives keep their major discourse 
function no matter whether they are placed in the first or the second discourse unit. The only differ-
ence is the probability to serve as the major discourse function is changed. For example, the probabil-
ity of the connective ????? (zh? b? gu?, only; just; merely) to have Comparison function is in-
creased from 0.6920 to 0.8501 when it is shifted from the first discourse unit to the second one. 
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[??, ?] ([not only, but]) 2.19 4.13 4.92 88.76 
[??, ?] ([not only, but]) 2.41 4.56 10.13 82.89 
[??, ?] ([not only, but]) 3.20 5.14 10.55 81.11 
[??, ?] ([since, but]) 3.99 13.87 13.42 68.72 
[??, ?] ([of course, while]) 1.16 2.76 80.82 15.24 
[??, ?] ([although, while]) 2.86 5.09 79.23 12.82 
[??, ?] ([although, while]) 2.76 43.61 79.16 13.71 
[??, ?] ([because, so]) 2.02 79.01 2.16 16.81 
[?, ?] ([because, so]) 3.21 71.03 2.28 23.49 
[??, ?] ([because, so]) 3.11 49.12 7.52 40.26 
[?, ?] ([?, and; but; thus]) 3.71 6.13 42.78 47.37 
[?, ?] ([and; but; thus, ?]) 5.47 8.55 17.00 68.98 
Table 8: Effects of single-word and word-pair connectives containing ??? (and, but, so). 
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[??, ?] ([therefore, ?]) 6.26 64.30 1.66 27.77 
[?, ??] ([?, therefore]) 3.54 28.32 5.15 62.99 
[??, ?] ([as long as, ?]) 2.68 66.02 5.33 25.98 
[?, ??] ([?, as long as]) 2.57 5.49 4.23 87.71 
[??, ?] ([if, ?]) 3.51 57.15 7.47 31.87 
[?, ??] ([?, if]) 3.31 5.21 5.33 86.16 
[??, ?] ([however, ?]) 8.17 9.20 23.12 59.51 
[?, ??] ([?, however]) 2.26 2.39 80.97 14.38 
[??, ?] ([but, ?]) 8.56 7.72 20.87 62.86 
[?, ??] ([?, but]) 2.32 2.90 75.76 19.02 
[??, ?] ([even though, ?]) 3.55 5.04 75.65 15.75 
[?, ??] ([?, even though]) 3.93 5.23 46.48 44.36 
[??, ?] ([now, ?]) 44.31 7.42 3.42 44.85 
[?, ??] ([?, now]) 8.03 2.88 3.60 85.49 
[?, ?] ([and, ?]) 7.14 8.43 3.14 81.29 
[?, ?] ([?, and]) 4.62 3.79 2.38 89.22 
[??, ?] ([as well as, ?]) 4.83 9.88 2.69 82.60 
[?, ??] ([?, as well as]) 4.20 4.29 2.33 89.18 
[???, ?] ([merely, ?]) 3.54 4.76 69.20 22.50 
[?, ???] ([?, merely]) 1.48 2.00 85.01 11.50 
Table 9: Effects of positions of single-word connectives. 
5 Conclusion 
In this paper, we address the issue of the ambiguous discourse functions of Chinese connectives in 
discourse relation labeling and propose a semi-supervised learning method to estimate the probability 
distribution of discourse functions of connectives. We examine the constructions of Chinese connec-
tives and their effects on the discourse relation recognition. The proposed approach learns the proba-
bility distributions of discourse functions of Chinese connectives from a small labeled dataset and a 
big unlabeled dataset. The results reflect many interesting linguistic phenomena. We compare the am-
biguity degrees of single-word and word-pair connectives, and show the effects of the positions of sin-
gle-word connectives on the discourse functions. The discourse relation recognizer integrating the 
641
probability distributions and contextual information significantly outperforms the approaches without 
the knowledge.  
This methodology can be extended to estimate the probability distribution of discourse functions of 
connectives on much finer relation categories. In the current experiments, we focus on explicit dis-
course relation recognition. The 302,293 labeled sentences in Section 4 can be regarded as a training 
corpus for implicit discourse relation recognition. Those labeled sentences composed of unambiguous 
connectives will be sampled from the reference corpus for training an implicit discourse relation 
recognition system. Furthermore, how to employ the learned probability distributions to deal with dis-
course units containing multiple connectives will be investigated. In the future, we will tell out the dis-
course connective and non-discourse connective uses of words and explore their interpretations on the 
discourse relation recognition. Besides, we will make use of the probability distributions to the relation 
labeling on more than two clauses and further extend the methodology to experiments at the inter-
sentence level. 
Acknowledgements 
This research was partially supported by Ministry of Science and Technology, Taiwan, under the 
grants 101-2221-E-002-195-MY3 and 102-2221-E-002-103-MY3, and 2012 Google Research Award.  
We are also very thankful to the anonymous reviewers for their helpful comments to revise this paper. 
References 
Stergos Afantenos, Nicholas Asher, Farah Benamara, Myriam Bras, C?cile Fabre, Mai Ho-dac, Anne Le Dra-
oulec, Philippe Muller, Marie-Paule P?ry-Woodley, Laurent Pr?vot, Josette Rebeyrolle, Ludovic Tanguy, Ma-
rianne Vergez-Couret, and Laure Vieu. 2012. An Empirical Resource for Discovering Cognitive Principles of 
Discourse Organisation: the ANNODIS Corpus. In Proceedings of the18th International Conference on Lan-
guage Resources and Evaluation (LREC 2012), pages 2727-2734, Istanbul, Turkey. 
Nicholas Asher and Alex Lascarides. 1995. Lexical Disambiguation in a Discourse Context. Journal of Seman-
tics, 12(1):69-108, Oxford University Press. 
Lynn Carlson, Daniel Marcu, and Mary E. Okurowski. 2002. RST Discourse Treebank. Linguistic Data Consor-
tium, Philadelphia. 
Bruno Cartoni, Sandrine Zufferey, and Thomas Meyer. 2013. Annotating the Meaning of Discourse Connectives 
by Looking at their Translation: The Translation Spotting Technique. Dialogue and Discourse, 4(2):65-86. 
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A Library for Support Vector Machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1-27:27. 
Hsin-Hsi Chen. 1994. The Contextual Analysis of Chinese Sentences with Punctuation Marks. Literal and Lin-
guistic Computing, 9(4):281-289. 
Shou-Yi Cheng. 2006. Corpus-Based Coherence Relation Tagging in Chinese Discourse. Master Thesis, Na-
tional Chiao Tung University, Hsinchu, Taiwan. 
Xianghui Cheng and Xiaolin Tian. 1989. Xian dai Han yu (????), San lian shu dian (????), Hong 
Kong. 
CMU 2009. ClueWeb09, http://lemurproject.org/clue-web09.php/ 
Leon Derczynski and Robert Gaizauskas. 2013. Temporal Signals Help Label Temporal Relations. In Proceed-
ings of the 51st Annual Meeting of the Association for Computational Linguistics, Volume 2: Short Papers, 
pages 645-650, Sofia, Bulgaria. 
Jerry R. Hobbs. 1985. On the Coherence and Structure of Discourse, Report No. CSLI-85-37, Center for the 
Study of Language and Information, Stanford University. http://www.isi.edu/~hobbs/ocsd.pdf 
Eduard H. Hovy and Elisabeth Maier. 1992. Parsimonious or Profligate: How Many and Which Discourse Struc-
ture Relations? No. ISI/RR-93-373. Information Sciences Institute, University of Southern California, Marina 
del Rey. 
Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese Discourse Relation Recognition. In Proceedings of the 5th 
International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 1442-1446, Chiang 
Mai, Thailand. 
642
Hen-Hsen Huang and Hsin-Hsi Chen. 2012a. Contingency and Comparison Relation Labeling and Structure 
Prediction in Chinese Sentences. In Proceedings of the 13th Annual Meeting of the Special Interest Group on 
Discourse and Dialogue (SIGDIAL 2012), pages 261-269, Seoul, South Korea. 
Hen-Hsen Huang and Hsin-Hsi Chen. 2012b. An Annotation System for Development of Chinese Discourse 
Corpus. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012) 
Demonstration Papers, pages 223-230, Mumbai, India. 
Hen-Hsen Huang, Chi-Hsin Yu, Tai-Wei Chang, Cong-Kai Lin, and Hsin-Hsi Chen. 2013. Analyses of the As-
sociation between Discourse Relation and Sentiment Polarity with a Chinese Human-Annotated Corpus. In 
Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 70-78, So-
fia, Bulgaria. 
Ben Hutchinson. 2004. Acquiring the Meaning of Discourse Markers. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics (ACL 2004), pages 684-691, Barcelona, Spain. 
Charles N. Li, Sandra A. Thompson. 1981. Mandarin Chinese: A Functional Reference Grammar. University of 
California Press. 
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically Evaluating Text Coherence Using Discourse 
Relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 
2011), pages 997-1006, Portland, Oregon, USA. 
Shuxiang Lu. 2007. Eight Hundred Words of The Contemporary Chinese (Xian dai Han yu Ba bai Ci). China 
Social Sciences Press. 
Emily Pitler and Ani Nenkova. 2009. Using Syntax to Disambiguate Explicit Discourse Connectives in Text. In 
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13-16, Suntec, Singapore. 
Livia Polanyi. 1988. A Formal Model of the Structure of Discourse. Journal of Pragmatics, 12(5-6):601-638. 
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 
2008. The Penn Discourse TreeBank 2.0. In Proceedings of the 6th Language Resources and Evaluation Con-
ference (LREC 2008), pages 2961-2968, Marrakech, Morocco. 
Charlotte Roze, Laurence Danlos, and Philippe Muller. 2010. LEXCONN: a French Lexicon of Discourse Con-
nectives. In Proceedings of the 8th International Workshop on Multidisciplinary Approaches to Discourse 
(MAD 2010), Moissac.  
Ted J. M. Sanders, Wilbert P. M. Spooren, and Leo G. M. Noordman. 1992. Toward a Taxonomy of Coherence 
Relations. Discourse Processes, 15(1):1-35. 
Caroline Sporleder and Alex Lascarides. 2008. Using Automatically Labelled Examples to Classify Rhetorical 
Relations: A Critical Assessment. Natural Language Engineering, 14(3):369-416, Cambridge University 
Press. 
Fei Wang, Yunfang Wu, and Likun Qiu. 2012. Exploiting Discourse Relations for Sentiment Analysis. In Pro-
ceedings of the 24th International Conference on Computational Linguistics (COLING 2012), Posters, pages 
1311-1320, Mumbai, India. 
Florian Wolf and Edward Gibson. 2005. Representing Discourse Coherence: A Corpus-Based Study. Computa-
tional Linguistics, 31(2):249-287. 
Chi-Hsin Yu, Yi-jie Tang and Hsin-Hsi Chen. 2012. Development of a Web-scale Chinese Word N-gram Corpus 
with Parts of Speech Information. In Proceedings the 8th International Conference on Language Resources 
and Evaluation (LREC 2012), pages 320-324, Istanbul, Turkey. 
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei and Kam-Fai Wong. 2011. Unsupervised Discovery of Dis-
course Relations for Eliminating Intra-sentence Polarity Ambiguities. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing (EMNLP 2011), pages 162-171, Edinburgh, UK. 
Lanjun Zhou, Wei Gao, Binyang Li, Zhongyu Wei and Kam-Fai Wong. 2012. Cross-lingual Identification of 
Ambiguous Discourse Connectives for Resource-Poor Language. In Proceedings of the 24th International 
Conference on Computational Linguistics (COLING 2012), pages 1409-1418, Mumbai, India. 
Yuping Zhou and Nianwen Xue. 2012. PDTB-style Discourse Annotation of Chinese Text. In Proceedings of the 
50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 67-77, Jeju Island, 
Korea. 
643
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1269?1278, Dublin, Ireland, August 23-29 2014.
Chinese Irony Corpus Construction and Ironic Structure Analysis 
 
 
Yi-jie Tang and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University, Taipei, Taiwan  
tangyj@nlg.csie.ntu.edu.tw,hhchen@ntu.edu.tw
 
  
 
 Abstract 
Non-literal expression recognition is a challenging task in natural language processing. An ironic expression 
implies the opposite of the literal meaning, causing problems in opinion mining and sentiment analysis. In this 
paper, ironic messages are collected from microblogs to form an irony corpus based on the use of emoticons, 
linguistic forms, and sentiment polarity. Five linguistic patterns are mined by using the proposed bootstrapping 
approach. We also analyze the linguistic structure and elements used to convey irony. Based on our observations, 
ironic words/phrases and contextual information are the necessary elements in irony, while the contextual infor-
mation can be hidden in linguistic forms. A rhetorical element, which is optional in irony, can also be used to 
help strengthen the effects and understandability of an ironic expression. The ironic elements in each instance of 
our irony corpus are labelled based on this structure. This corpus can be used to study the usage of ironic expres-
sions and the identification of ironic elements, and thus improve the performance of irony recognition. 
1 Introduction 
Dealing with non-literal meaning is a challenging task in natural language processing. Linguistic con-
text and background knowledge are required to interpret non-literal utterances properly. An ironic ex-
pression, where the meaning is the opposite of what is literally expressed, is one of the indirect and 
non-literal linguistic forms that cannot be easily processed and detected. One cannot capture the real 
meanings of opinions and sentiments expressed in a document or conversation if irony is not taken 
into account. 
The challenges of irony processing involve the following issues: (1) No comprehensive irony cor-
pus is available. (2) Irony analysis is related to semantics, pragmatics and discourse studies, which are 
the most challenging in natural language processing. (3) Contextual information and background 
knowledge are necessary, but they are hard to obtain and process. (4) Non-linguistic or non-verbal fac-
tors, e.g., intonations, gestures and talking speed in speech, and spaces, punctuations and typography 
in writing, have to be considered. 
This paper focuses on irony corpus construction, ironic pattern mining, and ironic structure analysis. 
Messages were collected from a microblogging platform based on emoticons, and ironic messages and 
patterns were extracted to build an irony corpus. The structure of ironic expressions and the clarifica-
tion of the uses of ironic elements were also analyzed. Labels representing the ironic elements are 
added to each message in the irony corpus. To the best of our knowledge, this is the first Chinese irony 
corpus available for research. 
This paper is organized as follows. Section 2 surveys the related work. Section 3 proposes a meth-
odology to construct an irony corpus. Section 4 presents the patterns mined from the corpus. Section 5 
discusses the results of ironic expressions collected from a different type of corpus. Section 6 makes 
the error analysis. Section 7 analyzes linguistic structure of Chinese irony. Section 8 concludes the 
remarks. 
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/ 
1269
2 Related Work 
Sarcasm and irony have been studied by linguistics and cognitive scientists (Giora and Fein, 1999; 
Gibbs and Colston, 2007) for years, but there has been no concrete claim on the linguistic structure of 
irony. Some studies have started focusing on the processing of sarcasm and irony recently, but it is 
still not clear whether sarcasm and irony differ significantly or represent the same concept.  
The research of non-literal expression identification has drawn attention in recent years. Katz and 
Giesbrecht (2006) use meaning vectors for literal and non-literal expression classification. Li and 
Sporleder (2010) focus on distinguishing literal and non-literal usages of idioms. 
Filatova (2012) uses crowdsourcing to generate an irony and sarcasm corpus. Veale and Hao (2010) 
construct a corpus of ironic similes using the wildcarded query ?as * as a *? on a search engine. Da-
vidov et al. (2010) collect messages from Twitter and product reviews from Amazon.com using the 
Mechanical Turk service. The #sarcasm hashtag is used as ground truth, and a k-nearest neighbor 
strategy is used for classification. Gonz?lez-Ib??ez et al. (2011) also make use of hashtags in Twitter 
as labels to build a sarcasm corpus. In their study, both human classification and automatic classifica-
tion achieve low accuracy in sarcasm detection. Reyes et al. (2012) analyze humor and irony based on 
the user-generated tags, such as ?#humor? and ?#irony?, in twitter. Lukin and Walker (2013) use a 
bootstrapping method to improve the performance of the classifiers for identifying sarcastic and nasty 
utterances in online dialogues. 
The hashtag-based approaches are not always suitable for irony corpus construction for all the lan-
guages. As of March 9, 2014, only 113 messages are found to contain the hashtag #?? (#irony) in 
Weibo, the largest Chinese language microblogging platform. This paper differs from the previous 
work in that we employ negative emoticons and positive words as clues to capture the irony. The lin-
guistic patterns mined from the irony corpus can be used to detect if a sentence is ironic.  
3 Irony Corpus Generation from Microblogs 
This section introduces a bootstrapping methodology to construct an irony corpus and mine irony pat-
terns. While Lukin and Walker (2013) also used a bootstrapping method to improve sarcasm and nas-
tiness classifiers, this paper, in contrast, focuses on irony pattern mining and corpus construction. 
3.1 An Emotion-Tagged Corpus 
The traditional definition of verbal irony is adopted, where the speaker says something that seems to 
be the opposite of what they mean (Gibbs and Colston, 2007). Under this definition, texts annotated 
with polarity information that expresses the actual meaning should be collected, and the literal mean-
ings of words in the texts should be identified. If any disagreement exists between the actual meaning 
and literal meaning, then we say the text contains irony. 
Nowadays, emoticons are used quite often in social media to express the feelings of the posters. The 
tagged emoticons specify their actual meanings in some sense. Based on this idea, messages were col-
lected from Plurk1, a microblogging platform similar to Twitter. It lets users post messages limited to 
140 characters, and allows them to use graphical emoticons in their messages.  
It was assumed that these emoticons can represent the poster?s sentiments, and, therefore, be re-
garded as sentiment labels of the messages. Among 35 emoticons, 23 are categorized into positive, and 
12 are categorized into negative. Collected messages are dated from Jun 21, 2008 to Nov 7, 2009, and 
all of them are in Traditional Chinese. 
On the other hand, the literal meanings of the posted messages need to be known. Many sentiment 
analysis algorithms (Liu, 2012) can be explored. A lexicon-based approach was adopted. The NTU 
Sentiment Dictionary, or NTUSD (Ku and Chen, 2007), was employed to determine the sentiment of a 
word. This dictionary provides 21,056 positive and 22,751 negative words. Most of these words are in 
Traditional Chinese.  
                                                 
1 http://www.plurk.com 
1270
3.2 Candidates Extraction 
Possible irony messages were extracted from the Plurk corpus by using NTUSD. Since the typical so-
cial function of irony is expressing negative meaning with positive words, as mentioned in Gibbs and 
Colston (2007), focus was directed on those messages with negative emoticons and positive words. A 
total of 3,178,372 messages was found containing at least one negative emoticon. Among them, 
304,754 messages with at least one positive word are found and form an irony candidate dataset. 
Discourse relation determines how two discourse units cohere to each other. Sentiment transition of 
two clausal arguments is identified based on their discourse relation (Zhou et al., 2011; Wang et al., 
2012; Huang et al., 2013). In the sentence ?he is nice but not attractive,? positive opinion in the begin-
ning is transformed to a negative one by the discourse connective ?but.? Both the positive word ?nice? 
and the negative phrase ?not attractive? are used literally. Thus, it was necessary to filter out messages 
containing such connectives.  
Messages are removed only when the positive word occurs earlier than the discourse connectives 
with a comparison function, due to Chinese grammatical structure. The Chinese discourse connectives 
used here include ???, ????, ????, ????, ???? (all the above are equivalent to the English 
word but), ???? (however), ??? (comparatively), ???? (unfortunately), ???? (contrarily), ??
?? (oppositely), and ???? (on the contrary). A total of 254,836 messages remains after this process.  
3.3 Pattern Mining 
Although irony can be used without any customary linguistic patterns, some ironic expressions do ex-
hibit specific forms of language use. Colston and O?Brien (2000) suggest that both irony and hyperbo-
le create contrasts between expected and ensuing events. It was assumed that exaggerated expressions 
could be used with irony to strengthen the effects of the speech act. In the expression??????
?? (I am really and extremely lucky!), the adverbs really and extremely are used to strengthen the 
ironic effect. Thus, combinations of degree adverb phrases and a positive adjective are used as patterns 
to find possible irony expressions automatically in the candidate dataset. 
Not all degree adverbs in Chinese are used because some of them are mostly used in formal texts 
and not frequently present in microblogs. The degree adverb phrases used here include the combina-
tions of the adverbs ??? (h?i), ??? (y?), ???? (w?im?an), ??? (k?) and ???? (truly) and the de-
gree adverbs ??? (really), ??? (extremely) and ???? (very).  
The following bootstrapping procedure was used to find more patterns. 
(1) Which patterns should be used is decided. At the very beginning of the bootstrapping procedure, 
the [degree adverb + positive adjective] pattern mentioned above is used. 
(2) Messages containing the patterns in step (1) are automatically retrieved from the candidates. 
NTUSD is used to determine sentiment polarity, and CKIP parser is used to get parts of speech2. 
(3) Messages retrieved in step (2) were reviewed by the annotator to decide which of them are 
actually ironic.  
(4) If the annotator finds new irony patterns in the reviewed messages, then the procedure starts 
again from step (1) and uses the patterns to repeat the process. 
This process was repeated for four times. After the fourth iteration, no more new patterns were 
found by the annotator. Finally, 2,825 messages are found to have any of the patterns, and 1,005 of 
them are confirmed to be ironic and make up the NTU Irony Corpus.3 Examples of these patterns and 
ironic messages are shown in Section 4. 
4 Irony Patterns 
All the patterns mined by the approach used in Section 3 are categorized into the following five groups.  
4.1 Degree Adverbs + Positive Adjective 
In this pattern, the following two components must exist: 
 
                                                 
2 http://ckipsvr.iis.sinica.edu.tw. 
3 The NTU Irony Corpus is available at http://nlg.csie.ntu.edu.tw/nlpresource/irony_corpus/. 
1271
(a) Degree adverb phrase + positive adjective phrase 
(b) Negative context 
 
The negative context can occur either before or after the component (a). For example, the following 
expression is used when someone has to wait for a long time to start ordering in a restaurant. In total, 
13.03% of all the messages in the corpus contain this pattern. 
 
(s1) ???????????????? 
I have to wait for half an hour to order. The service is definitely really good. 
 
The underlined expression is the contextual information described in (b), and the double-underlined 
expression is the linguistic form described in (a). In the second clause the adverbs ??? (h?i) and ??? 
(really) are combined to form a degree adverb phrase for intensification or hyperbole. Although the 
positive word good is used, the speaker means the opposite. The first clause indicates why they think 
the service is not good, and, therefore, provides the contextual information. 
4.2 The Use of Positive Adjective with High Intensity 
In this pattern, the following two components must exist: 
 
(a) Positive adjective with high intensity 
(b) Negative context 
 
Specific positive adjectives with high intensity are used to form ironic expressions with or without 
other rhetorical elements. Since the context is negative, the positive adjective is used to express non-
literal meanings. The adjectives we found in the corpus include ???? (great), ????? (remarkable) 
and ???? (genius). Only 2.09% of the messages in the corpus contain this pattern. For example, the 
word great is used in the following message: 
 
(s2) ?? plurk??????????...????????????? 
My Plurk account encountered an unknown error ?again?? This is indeed the greatest 
invention in the century. 
4.3 The Use of Positive Noun with High Intensity 
In this pattern, the following two components must exist: 
 
(a) Positive noun with high intensity 
(b) Negative context 
 
Specific nouns that represent highly positive meanings are also used to express irony. These nouns 
include ???? (superstar), ???? (big gift) and ???? (wonderful state). When they are used with a 
negative context, an ironic expression is formed. This is pattern is not found frequently in the corpus. 
Only 2.00% of the messages in the corpus contain this pattern.  An example is listed below: 
 
(s3) ?????????.......????? 
The big gift I received in the Mid Autumn Festival was?? a lot of fat in my body. 
4.4 The Use of ???? (very good) 
In this pattern, the following two components must exist: 
 
(a) Sentence boundary + ?? + punctuation 
(b) Negative context 
 
1272
A sentence boundary occurs before the word ???? (very good) because there is no subject. Multiple 
punctuations, and particularly exclamation marks and ellipses, can be used after ???? to increase the 
intensity. In the following example, exclamation marks are used: 
 
(s4) ??... ??!! ?????? 
I caught a cold? Very good!! My vacation is gone. 
 
Sometimes this pattern is followed by an exclamation word, such as ??? (a), ??? (ya), and ??? 
(ma). These exclamations, like punctuations, can help strengthen the level of the speaker?s feelings. In 
our irony corpus, this pattern is used in 50.84% of all ironic messages. Obviously, this is a common 
way when people want to express their negative feelings with an ironic expression. 
4.5 ???????? (It?s okay to be worse) 
In this pattern, the following expression must exist: 
 
??? + negative adjective + ?? 
(It is okay to be more + negative adjective) 
 
This pattern literally states that it is okay for something to become worse and is a commonly used pat-
tern to express irony in our corpus. It can be found in 33.53% of the messages in the corpus. In most 
cases, even when no proper contextual information is present, the listener can tell the literal meaning is 
not meant because it violates most people?s inclinations. Thus, the use of this pattern is usually non-
literal and ironic. An example is shown below. 
 
(s5) ?????...??????? 
It's -11?C?It is okay to be colder 
 
A message can contain more than one pattern, causing the sum of the percentages of the above five 
patterns to be greater than 100%. For example, both patterns 4.4 and 4.5 are used in the following 
message: 
 
(s6)??!!!!??????? ?????... 
Very Good!!!! It is okay for me to be more idiotic? 
 
The patterns in Sections 4.4 and 4.5 are mainly based on their linguistic forms and frequently used 
in ironic expressions. We argue that these patterns are more static than the others, and we call them the 
customary patterns. On the other hand, the patterns in Sections 4.1, 4.2 and 4.3 are called non-
customary patterns. 
5 Collecting Ironic Expressions from Blogs 
In order to understand how irony is conveyed in different types of media, we use the methodology and 
mined patterns described in Sections 3 and 4 to collect irony expressions from the Yahoo Kimo Blogs 
corpus. 
5.1 The Yahoo Blog Corpus 
The Yahoo Kimo Blog corpus, referred to the Yahoo corpus in the following sections, contains blog 
articles from November 1, 2005 to August 20, 2007 (Yang, Lin and Chen, 2009). Out of all the posts 
in the dataset, 2,764,202 posts have at least one emoticon. The articles posted in July 2006 are used 
here, and they are divided into 341,932 smaller units by the full stop symbol. All articles are in Tradi-
tional Chinese.  
Since the Plurk platform can be used as an instant messaging system, and readers of the message are 
usually on the author?s friend list, these messages are usually conversational. On the other hand, Ya-
1273
hoo blogs are not limited in length and a blog article itself is not part of the conversation. Thus, the 
blog articles are usually more formal compared to microblog messages. 
Although the articles are separated by a full stop into shorter units, these units are not necessarily 
identical to sentences due to the conventional usage of the Chinese period symbol. They can consist of 
multiple sentences and thus contain a discourse structure, which makes them suitable for this corpus 
study. 
5.2 Extract Ironic Expressions  
A similar approach to the steps described in Section 3.3, is used to collect ironic expressions from the 
Yahoo corpus, but four patterns of irony found in Plurk are used to perform step (1). These patterns, as 
listed below, are adopted because they are the most frequently used ones in our Plurk irony corpus. 
They can also reflect the uses of customary and non-customary irony patterns as the first two patterns 
are customary, and the last two are non-customary. Pattern 1 and Pattern 2 are the same patterns as 
mentioned in Section 4.4 and Section 4.5, respectively. Pattern 3 and Pattern 4 are two forms from the 
pattern described in Section 4.1. Only step (1) to step (3) are performed, and step (4) is bypassed; that 
is, the process is not repeated. 
 
 Pattern 1: 
(a) Sentence boundary + ?? + punctuation 
(b) Negative context 
 Pattern 2: 
  ??? + negative adjective + ?? 
 Pattern 3: 
(a) ?? + positive adjective 
(b) Negative context 
 Pattern 4: 
(a) ?? + positive expression 
(b) Negative context 
5.3 Results and Discussion 
A total of 36 ironic texts is obtained. All the four irony patterns seen in Plurk can be found in Yahoo. 
The final results are shown in Table 1. 
 
 Number of Ironic Expressions Percentage 
Pattern 1 14 38.89% 
Pattern 2 10 27.78% 
Pattern 3 5 13.89% 
Pattern 4 7 19.44% 
Table 1: Ironic texts found for the four Patterns in Yahoo. 
 
The proportions of the four patterns in Plurk and Yahoo are also compared. The percentages are 
calculated by dividing the occurrence of each pattern by the occurrence of all four patterns in the same 
datasets. As can be seen in Figure 1, the proportions of patterns (1) and (2) in Plurk are significantly 
higher than in Yahoo, and the proportions of patterns (3) and (4) in Plurk are significantly lower than 
in Yahoo (p<0.05 according to the t-test). This suggests that patterns (1) and (2) tend to be used in 
informal and conversational texts while patterns (3) and (4) tend to be used in formal articles to 
convey irony. Also, this may suggest that customary patterns are more likely to be used in 
conversations, and authors of formal articles prefer an indirect way to express irony, although more 
data are required for further studies in the future. 
 
 
 
 
 
1274
0.00%
10.00%
20.00%
30.00%
40.00%
50.00%
60.00%
Pattern1 Pattern2 Pattern3 Pattern4
Plurk
Yahoo
 Figure 1: Comparison of the proportion of the four patterns in Plurk and Yahoo. 
6 Error Analysis 
In this section, we analyze why non-ironic messages were retrieved by the automatic processes. The 
1,820 wrong messages specified in Section 3.3 are classified into the following two categories. 
 
(1) Sentiment identification 
Using the patterns to find possible ironic messages involves the correct sentiment identification. 
NTUSD does not cover some new words used on Internet informal conversations. The sentiment of a 
word can also be changed depending on its context. For example, ???? (so strong) is listed as a posi-
tive term in NTUSD. However, it is used to indicate a negative condition in the example (s7). 
 
(s7) ?????????????????? 
 The side effect of the pain reliever was so strong, making me sleep through the whole night. 
 
(2) Opinion targets 
In a Plurk message, even though the message poster is talking about the same topic, more than 
one entity with associated opinions can be present. For example: 
 
(s8) ???????????? 
 The business of our company is running so well. I am so tired. 
 
The poster expresses negative sentiment by using the word ?tired.? Although the positive word ??
?? (very good) is also used, it modifies the word ?business? rather than the poster?s condition. That is, 
the opinion targets of the two words are different, and this causes problems when automatically re-
trieving ironic messages.  
7 Linguistic Structure of Irony 
In this section, the linguistic structure of irony is analyzed based on our observations on the corpus. 
7.1 Ironic Word 
As described, the literal meaning of an ironic word or phrase is opposite to the actual meaning. An 
ironic word/phrase is necessary to separate irony from regular utterances. If the ironic word of an 
utterance is reverted, the speaker?s actual sentiment or intention is reconstructed. 
However, it is not easy to identify the ironic word in an utterance. Sometimes more than one word 
can be an ironic word. In our corpus, 94.93% of the ironic words are adjectives, while others are used 
as adverbs, verbs or nouns. The recognition of ironic word/phrase is a challenging task, but other iron-
ic elements described in Sections 7.2 and 7.3 can be analyzed side by side to help improve the perfor-
mance. 
7.2 Contextual Information 
Contextual information is usually provided as part of ironic utterances to help convey irony. For 
example, the underlined sentence in the following utterance is crucial for irony interpretation: 
 
1275
(s9) ??????????? 
 I was injured. I was really lucky. 
 
Without the first sentence, it is hard to tell if lucky is actually meant. Although a speaker can still 
use ironic words/phrases without providing contextual information, this can be an ineffective way to 
communicate the actual meanings of irony. According to the cooperative principle proposed by Grice 
(1975), the speaker must give enough information in order to enable successful communication and 
implicatures. The four maxims of the cooperative principle include: 
(1) Maxim of Quantity: The speaker should make their contribution as informative as is required. 
Do not make the contribution more informative than is required. 
(2) Maxim of Quality: The speaker should not say what they believe to be false, and should not 
say that for which they lack adequate evidence. 
(3) Maxim of Relation: The speaker should be relevant. 
(4) Maxim of Manner: The speaker should avoid obscurity of expression, avoid ambiguity, be 
brief and be orderly. 
Based on Grice?s maxims, it is assumed enough, correct, relevant, and understandable contextual 
information should be provided with ironic expressions. However, the speaker sometimes assumes the 
listener already knows about the conditions where the irony takes place and has the required 
background knowledge; thus the contextual information is hidden in the ironic utterance. 
Four types of context can be used to interpret irony: 
(1) Linguistic context: The linguistic context refers to the words that are expressed before and/or 
after the irony words in a sentence or discourse. It is easier to obtain and analyze than the other 
three types of context. 
(2) Physical context: Physical context refers to what is actually present and/or happening in the 
environment or circumstance where the conversation is taking place. It is also related to the 
timing. In online conversations, participants are not usually in the same location, but they can 
be aware of the same ongoing events and situations. It is not necessary for the speaker to 
provide physical context information if they assume the objects or situations are noticeable to 
the listeners.  
(3) Epistemic context: The background knowledge shared by the participants in a conversion can 
also be used to interpret the irony. This type of context does not change over time. For example, 
people know rocks are hard, so they can understand the expression the bed is as soft as a rock is 
not literal. 
(4) Social context: Social relationship can be important for expressing and interpreting irony, 
especially in online messages.  
We argue that at least one type of contextual information must exist, but it can be hidden if the 
speaker thinks the listener is already aware of it. Physical, epistemic and social context can be hidden, 
while linguistic contextual information must be present. 
7.3 Rhetoric 
As shown in Section 4, degree adverbs, punctuations and exclamations can be used to convey irony. 
Some of them can even be repeated to intensify the effects. These elements increase contradiction and 
strengthen the degree of negative opinions. Unlike ironic words and context, rhetoric elements are not 
necessary to convey irony. 
Liebrecht et al. (2013) call the words used to strengthen evaluative utterances intensifiers. In their 
experiments, non-hyperbolic sarcastic messages often contain an explicit marker on Twitter. They ar-
gue that sarcasm is often signaled by hyperbolic words, including intensifiers and exclamations, and 
sarcastic utterances with hyperbolic words are easier to identify by listeners/readers than sarcastic ut-
terances without hyperbolic words. It can be seen that adverbs, adjectives, punctuations and exclama-
tions with high intensity observed in our irony patterns have very similar effects. 
Among the 113 messages containing the #?? (#irony) hashtag in Weibo, which was mentioned in 
Section 2, 83.19% do not exhibit hyperbole or uses of intensifiers. This observation is similar to the 
argument suggested in Liebrecht et al. (2013) and is one of the reasons why the hashtag is not suitable 
1276
for the irony pattern mining task in this study. In comparison, this methodology helps find more clues 
of irony that can be seen from their linguistic forms. 
7.4 Corpus Labeling 
To increase the usefulness of the corpus, ironic element tags are added to each message. An example 
is shown in Figure 2. 
Figure 2: An example message with ironic element tags. 
 
As can be seen in the example, ??? (good) is the word that is used in the opposite way, so it is 
marked with the ironic word/phrase label <ironic>. The preceding sentence states what actually hap-
pened, and is marked with the label <context>. The message poster also uses the degree adverb ??? 
(extremely) and used the exclamation ??? (ba, a sentence-final partical). These two words are marked 
with the <rhetoric> label. The sentiment polarity marks of the ironic word and contextual information, 
shown as either pos or neg, are also added.  
8 Conclusion 
In this paper, five types of irony patterns are mined, and an irony corpus is constructed based on 
linguistic forms and sentiment classification. Four verbal forms in Plurk and Yahoo were further 
examined. The former platform restricts short text conversation, and the latter platform allows for the 
long text description. The experimental results show that the customary forms tend to be used in 
informal and conversational texts while the non-customary forms tend to be used in formal articles to 
convey irony. The three basic elements that form a successful ironic speech act were also analyzed. 
These elements, including the words/phrases with reversed meanings, contextual information and 
rhetorical words, should be identified first in order to properly process ironic expressions and perform 
linguistic analysis. In the mined patterns, it was found that hyperbole was frequently present. In future 
work, we will explore other opinion mining and sentiment analysis algorithms, and focus on automatic 
recognition of hyperbole and the ironic elements. 
Acknowledgements 
This research was partially supported by National Taiwan University under grant 103R890858.  We 
are also very thankful to the anonymous reviewers for their helpful comments to revise this paper. 
References 
Herbert L. Colston and Jennifer O'Brien. 2000. Contrast of Kind Versus Contrast of Magnitude: the 
Pragmatic Accomplishments of Irony and Hyperbole. Discourse and Processes, 30(3):179-199. 
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-Supervised Recognition of Sarcastic Sen-
tences in Twitter and Amazon, In Proceedings of the Fourteenth Conference on Computational 
Natural Language Learning (CoNLL-2010), pages 107-116, Uppsala, Sweden. 
Elena Filatova. 2012. Irony and Sarcasm: Corpus Generation and Analysis Using Crowdsourcing. In 
Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC 
2012), pages 392-298, Istanbul, Turkey. 
Raymond W. Gibbs and Herbert L. Colston. 2007. Irony in Language and Thought. Lawrence Erl-
baum Associates, New York. 
<context sentiment="pos">????????????</context>???<rhetoric>??
</rhetoric><ironic sentiment="neg">?</ ironic>?<rhetoric>?</rhetoric>. 
 
English translation: 
<context sentiment="pos">The book I just bought has fallen apart.</context> The quality is <rheto-
ric>just extremely</rhetoric> <ironic sentiment="neg">good</ ironic>le<rhetoric>ba</rhetoric>. 
1277
Rachel Giora and Ofer Fein. 1999. Irony: Context and Salience. Metaphor and Symbol, 14:241-257. 
Roberto Gonz?lez-Ib??ez, Smaranda Muresan, and Nina Wacholder. 2011. Identifying Sarcasm in 
Twitter: A Closer Look. In Proceedings of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Short Papers, pages 581-586, Portland, Oregon, USA. 
H. P. Grice. 1975. Logic and Conversation. In P. Cole and J. J. Morgan, eds. Syntax and Semantics, 3: 
Speech Acts. New York: Academic Press. 
Hen-Hsen Huang, Chi-Hsin Yu, Tai-Wei Chang, Cong-Kai Lin and Hsin-Hsi Chen. 2013. Analyses of 
the Association between Discourse Relation and Sentiment Polarity with a Chinese Human-
Annotated Corpus. In Proceedings of ACL 2013 7th Linguistic Annotation Workshop & Interopera-
bility with Discourse, pages 70-78, Sofia, Bulgaria. 
Graham Katz and Eugenie Giesbrecht. 2006. Automatic Identification of Non-compositional Multi-
word Expressions Using Latent Semantic Analysis. In Proceedings of the ACL/COLING-06 Work-
shop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 12-19, 
Sydney, Australia. 
Lun-Wei Ku and Hsin-Hsi Chen. 2007. Mining Opinions from the Web: Beyond Relevance Retrieval. 
Journal of American Society for Information Science and Technology, Special Issue on Mining Web 
Resources for Enhancing Information Retrieval, 58(12):1838-1850.  
Linlin Li and Caroline Sporleder. 2010. Linguistic Cues for Distinguishing Literal and Non-Literal 
Usages. In Proceedings of 23rd International Conference on Computational Linguistics (COLING 
2010), Poster Volume, pages 683-691, Beijing, China. 
Christine Liebrecht, Florian Kunneman, and Antal van den Bosch. 2013. The Perfect Solution for De-
tecting Sarcasm in Tweets #not. In Proceedings of the 4th Workshop on Computational Approaches 
to Subjectivity, Sentiment and Social Media Analysis, pages 29-37, Atlanta, Georgia. 
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language 
Technologies, Morgan & Claypool Publishers. 
Stephanie Lukin and Marilyn Walker. 2013. Really? Well. Apparently Bootstrapping Improves the 
Performance of Sarcasm and Nastiness Classifiers for Online Dialogue. In Proceedings of the 
Workshop on Language Analysis in Social Media, pages 30-40, Atlanta, Georgia. 
Antonio Reyes, Paolo Rosso, and Davide Buscaldi. 2012. From Humor Recognition to Irony Detec-
tion: The Figurative Language of Social Media. Data & Knowledge Engineering, 74:1-12. 
Tony Veale and Yanfen Hao. 2010. Detecting Ironic Intent in Creative Comparisons. In Proceedings 
of the 19th European Conference on Artificial Intelligence (ECAI 2010), pages 765-770, Lisbon, 
Portugal. 
Fei Wang, Yunfang Wu, and Likun Qiu. 2012. Exploiting Discourse Relations for Sentiment Analysis. 
In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012): 
Posters, pages 1311-1320, Mumbai, India. 
Changhua Yang, Kevin Lin, and Hsin-Hsi Chen 2009. Writer Meets Reader: Emotion Analysis of So-
cial Media from both the Writer?s and Reader?s Perspectives. In Proceedings of the 2009 
IEEE/WIC/ACM International Conference on Web Intelligence (WI 2009), pages 287-290, Milan, 
Italy. 
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei, and Kam-Fai Wong. 2011. Unsupervised Discov-
ery of Discourse Relations for Eliminating Intro-Sentence Polarity Ambiguities. In Proceedings of 
the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011), pages 
162-171, Edinburgh, Scotland, UK. 
 
1278
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 67?70, Dublin, Ireland, August 23-29 2014.
 A Sentence Judgment System for Grammatical Error Detection 
 
Lung-Hao Lee 1,2, Liang-Chih Yu3,4, Kuei-Ching Lee1,2,  
Yuen-Hsien Tseng1, Li-Ping Chang5, Hsin-Hsi Chen2 
1Information Technology Center, National Taiwan Normal University 
2Dept. of Computer Science and Information Engineering, National Taiwan University 
3Dept. of Information Management, Yuen Ze University 
4Innovation Center for Big Data and Digital Convergence, Yuen Ze University 
5Mandarin Training Center, National Taiwan Normal University 
lcyu@saturn.yzu.edu.tw, {lhlee, johnlee, lchang, 
samtseng}@ntnu.edu.tw, hhchen@ntu.edu.tw 
  
 
Abstract 
This study develops a sentence judgment system using both rule-based and n-gram statistical 
methods to detect grammatical errors in Chinese sentences. The rule-based method provides 
142 rules developed by linguistic experts to identify potential rule violations in input sentences. 
The n-gram statistical method relies on the n-gram scores of both correct and incorrect training 
sentences to determine the correctness of the input sentences, providing learners with im-
proved understanding of linguistic rules and n-gram frequencies. 
1 Introduction 
China?s growing global influence has prompted a surge of interest in learning Chinese as a foreign 
language (CFL), and this trend is expected to continue. This has driven an increase in demand for au-
tomated IT-based tools designed to assist CFL learners in mastering the language, including so-called 
MOOCs (Massive Open Online Courses) which allows huge numbers of learners to simultaneously 
access instructional opportunities and resources. This, in turn, has driven demand for automatic proof-
reading techniques to help instructors review and respond to the large volume of assignments and tests 
submitted by enrolled learners. 
However, whereas many computer-assisted learning tools have been developed for use by students 
of English as a Foreign Language (EFL), support for CFL learners is relatively sparse, especially in 
terms of tools designed to automatically detect and correct Chinese grammatical errors. For example, 
while Microsoft Word has integrated robust English spelling and grammar checking functions for 
years, such tools for Chinese are still quite primitive. In contrast to the plethora of research related to 
EFL learning, relatively few studies have focused on grammar checking for CFL learners. Wu et al. 
(2010) proposed relative position and parse template language models to detect Chinese errors written 
by US learner. Yu and Chen (2012) proposed a classifier to detect word-ordering errors in Chinese 
sentences from the HSK dynamic composition corpus. Chang et al. (2012) proposed a penalized prob-
abilistic First-Order Inductive Learning (pFOIL) algorithm for error diagnosis. In summary, although 
there are many approaches and tools to help EFL learners, the research problem described above for 
CFL learning is still under-explored. In addition, no common platform is available to compare differ-
ent approaches and to promote the study of this important issue. 
This study develops a sentence judgment system using both rule-based and n-gram statistical meth-
ods to detect grammatical errors in sentences written by CFL learners. Learners can input Chinese sen-
tences into the proposed system to check for possible grammatical errors. The rule-based method uses 
a set of rules developed by linguistic experts to identify potential rule violations in input sentences. 
 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
67
The n-gr
tences to
proved u
can also 
assignme
2 A S
Figure 1
http://sjf
shown in
part-of-s
grammat
ods dete
informat
ence, as 
?
( I
The ru
(towards
frequenc
detail the
2.1 Pr
Chinese 
Languag
nese wo
usually s
corpus-b
Ma, 200
ed word
????
?POS:W
words, t
lexicon a
POS tag 
 
am statistica
 determine 
nderstandin
be incorpora
nts and test
entence Ju
 shows the
.itc.ntnu.edu
 the upper 
peech taggi
ical error de
ct grammatic
ion, the exp
shown in the
     ?     ??
      from   he
le-based me
)) cannot be
y of the big
 pre-process
e-processin
is written w
e Processing
rd segmente
uffers from 
ased learnin
2). This is fo
s with parts-
??? (Ob
ord? sequen
he translatio
nd therefore
?SHI? is a ta
l method re
the correctn
g of both lin
ted into onl
s. 
dgement S
 user interf
.tw/demo/. L
part of Fig. 
ng, and then
tection. Fina
al errors. O
lanation of t
 bottom par
    ?     ?
re    go   tow
thod shows
 used after a
ram ?? ?
ing, rule-ba
g 
ithout word 
 (NLP) task
rs are gener
the unknow
g method is 
llowed by a
of-speech (T
ama is the p
ce  shown 
n of a forei
 is extracted
g to represen
Figure 1.
lies on the n
ess of the in
guistic rules 
ine CFL MO
ystem 
ace of the 
earners can
1. Each inp
 passed to 
lly, an inpu
therwise, it w
he matched 
t of Fig. 1. F
         ? 
ads  north. )
a rule violat
 verb (e.g., ?
? (go toward
sed method, 
boundaries.
s, texts mus
ally trained 
n word (i.e.,
used to merg
 reliable and
sai and Che
resident of 
as follows: 
gn proper na
 by the unk
t the be-ver
Screenshot 
-gram scor
put sentence
and n-gram 
OC platform
sentence ju
 submit sing
ut sentence 
both the ru
t sentence w
ill be mark
rules and n-g
or instance, 
 
ion is detec
?? (go)). T
s) is relativ
and n-gram 
 As a result,
t undergo au
by an input 
 the out-of-v
e unknown
 cost-effecti
n, 2004). F
the USA). 
 Nb:???
me ????
nown word 
b ???. 
of the senten
es of both c
s. The syste
frequencies
s to help as
dgment sys
le or multip
is pre-proce
le-based an
ill be marke
ed as correc
ram frequen
the followin
ted and expl
he n-gram fr
e low. The f
statistical m
 prior to the
tomatic wo
lexicon and
ocabulary, o
 words to tac
ve POS-tagg
or example, 
It was segm
  SHI:?  N
? (Obama) 
detection me
ce judgemen
orrect and in
m helps lea
. In addition,
sess and/or 
tem, which 
le sentences
ssed for wo
d n-gram st
d as incorre
t ( ). In ad
cies are als
g sentence i
ains that a p
equencies al
ollowing su
ethod. 
 implementa
rd segmenta
 probability 
r OOV) pro
kle the OOV
ing method 
take the Ch
ented and ta
c:??  Na
is not likely
chanism. In
t system. 
correct train
rners develo
 the propose
score the nu
can be acc
 through th
rd segmenta
atistical met
ct ( ) if bo
dition to the
o presented 
s marked as 
reposition (e
so shows th
bsections de
tion of mos
tion. Autom
models. Ho
blem. In this
 problem (C
to label the 
inese senten
gged in the
:??. Amo
 to be inclu
 this case, th
ing sen-
p an im-
d system 
mbers of 
essed at 
e textbox 
tion and 
hods for 
th meth-
 decision 
for refer-
incorrect: 
.g., ??? 
at the the 
scribe in 
t Natural 
atic Chi-
wever, it 
 study, a 
hen and 
segment-
ce ???
 form of  
ng these 
ded in a 
e special 
 
68
2.2 Rule-based Linguistic Analysis 
Several symbols are used to represent the syntactic rules to facilitate the detection of errors embedded 
in Chinese sentences written by CFL learners: (1) ?*? is a wild card, with ?Nh*? denoting all subordi-
nate tags of ?Nh?, e.g., ?Nhaa,? ?Nhab,? ?Nhac,? ?Nhb,? and ?Nhc?. (2) ?-? means an exclusion from 
the previous representation, with ?N*-Nab-Nbc? indicating that the corresponding word should be any 
noun (N*) excluding countable entity nouns (Nab) and surnames (Nbc). (3) ?/? means an alternative 
(i.e., ?or?), where the expression ???/??/??? (some/these/those) indicates that one of these 
three words satisfies the rule. (4) The rule mx{W1 W2} denotes the mutual exclusivity of the two 
words W1 and W2. (5) ?<? denotes the follow-by condition, where the expression ?Nhb  <  Nep? 
means the POS-tag ?Nep? follows the tag ?Nhb? that can exist several words ahead of the ?Nep?. 
Using such rule symbols, we manually constructed syntactic rules to cover errors that frequently oc-
cur in sentences written by CFL learners. We adopted the ?Analysis of 900 Common Erroneous Sam-
ples of Chinese Sentences? (Cheng, 1997) as the development set to handcraft the linguistic rules with 
syntactic information. If an input sentence satisfies any syntactic rule, the system will report the input 
as suspected of containing grammatical errors, creating a useful tool for autonomous CFL learners.  
2.3 N-gram Statistical Analysis 
Language modeling approaches to grammatical error detection are usually based on a score (log prob-
ability) output by an n-gram model trained on a large corpus. A sentence with grammatical errors usu-
ally has a low n-gram score. However, choosing an appropriate threshold to determine whether a sen-
tence is correct is still a nontrivial task. Therefore, this study proposes the use of n-gram scores of cor-
rect and incorrect sentences to build the respective correct and incorrect statistical models for gram-
matical error detection. That is, a given sentence is denoted as incorrect (i.e., having grammatical er-
rors) if its probability score output by the statistical model of incorrect sentences (i.e., the incorrect 
model) is greater than that of correct sentences (i.e., the correct model).  
To build the incorrect and correct statistical models, a total of 19,080 sentences with grammatical 
errors were extracted from the HSK dynamic composition corpus. These sentences were then manual-
ly corrected. An n-gram (n= 2 and 3) language model was then built from the Sinica corpus released 
by the Association for Computational Linguistics and Chinese Language Processing (ACLCLP) using 
the SRILM toolkit (Stolcke, 2002). The trained language model was used to assign an n?gram score 
for each correct and incorrect sentence, which were then used to build the respective correct and incor-
rect models based on a normal probability density function (Manning and Sch?tze, 1999). Both mod-
els can then be used to evaluate each test sentence by transforming its n-gram score into a probability 
score to determine whether the sentence is correct or not. 
3 Performance Evaluation 
The test set included 880 sentences with grammatical errors generated by CSL learners in the NCKU 
Chinese Language Center, and the corresponding 880 manually corrected sentences. For the rule-
based approach, a total of 142 rules were developed to identify incorrect sentences. For the n-gram 
statistical approach, both bi-gram and tri-gram language models were used for the correct and incor-
rect statistical models. In addition to precision, recall, and F1, the false positive rate (FPR) was defined 
as the number of correct sentences incorrectly identified as incorrect sentences divided by the total 
number of correct sentences in the test set. 
Table 1 shows the comparative results of the rule-based and n-gram statistical approaches to gram-
matical error detection. The results show that the rule-based approach achieved high precision, low 
recall and low FPR. Conversely, the n-gram-based approach yielded low precision, high recall and 
high FPR. In addition, the tri-gram model outperformed the bi-gram model for all metrics. Given the 
different results yielded by the rule-based and n-gram statistical approaches, we present different com-
binations of these two methods for comparison. The ?OR? combination means that a given sentence is 
identified as incorrect by only one of the methods, while the ?AND? combination means that a given 
sentence is identified as incorrect by both methods. The results show that the ?OR? combination yield-
ed better recall than the individual methods, and the ?AND? combination yielded better precision and 
FPR than the individual methods. Thus, the choice of methods may depend on application require-
ments or preferences 
69
Method Precision Recall F1 False Positive Rate 
Rule 0.857 0.224 0.356 0.038 
2-gram 0.555 0.751 0.638 0.603 
3-gram 0.585 0.838 0.689 0.595 
Rule OR 2-gram 0.500 1.000 0.667 1.000 
Rule OR 3-gram 0.502 1.000 0.668 0.993 
Rule AND 2-gram 0.924 0.083 0.153 0.007 
Rule AND 3-gram 0.924 0.083 0.153 0.007 
Table 1. Comparative results of the rule-based and n-gram statistical approaches. 
 
Many learner corpora exist for EFL for use in machine learning, including the International Corpus 
of Learner English (ICLE) and Cambridge Learner Corpus (CLC). But collecting a representative 
sample of authentic errors from CFL learners poses a challenge. In addition, English and Chinese 
grammars are markedly different. In contrast to syntax-oriented English language, Chinese is dis-
course-oriented, with meaning often expressed in several clauses to make a complete sentence. These 
characteristics make syntactic parsing difficult, due to long dependency between words in a clause or 
across clauses in a sentence. These difficulties constrain system performance.  
4 Conclusions  
This study presents a sentence judgment system developed using both rule-based and n-gram statisti-
cal methods to detect grammatical errors in sentences written by CFL learners. The system not only 
alerts learners to potential grammatical errors in their input sentences, but also helps them learn about 
linguistic rules and n-gram frequencies. The major contributions of this work include: (a) demonstrat-
ingg the feasibility of detecting grammatical errors in sentences written by CFL learners, (b) develop-
ing a system to facilitate autonomous learning among CFL learners and (c) collecting real grammatical 
errors  from CFL learners for the construction of a Chinese learner corpus. 
Acknowledgments 
This research was partially supported by Ministry of Science and Technology, Taiwan under the grant 
NSC102-2221-E-155-029-MY3, NSC 102-2221-E-002-103-MY3, and the "Aim for the Top Universi-
ty Project" sponsored by the Ministry of Education, Taiwan.  
Reference 
Andreas Stolcke. 2002. SRILM ? An extensible language modeling toolkit. Proceedings of ICSLP?02, pages 
901-904. 
Chi-Hsin Yu and Hsin-Hsi Chen. 2012. Detecting word ordering errors in Chinese sentences for learning Chi-
nese as a foreign language. Proceedings of COLING?12, pages 3003-3018. 
Christopher D. Manning and Hinrich Sch?tze. 1999. Foundations of Statistical Natural Language Processing. 
MIT Press. Cambridge, MA.  
Chung-Hsien Wu, Chao-Hung Liu, Matthew Harris and Liang-Chih Yu. 2010. Sentence correction incorporating 
relative position and parse template language model. IEEE Transactions on Audio, Speech, and Language 
Processing, 18(6):1170-1181. 
Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown word extraction for Chinese documents. Proceedings of 
COLING?02, pages 169-175. 
M. Cheng. 1997. Analysis of 900 Common Erroneous Samples of Chinese Sentences - for Chinese Learners 
from English Speaking Countries (in Chinese). Beijing, CN: Sinolingua. 
Ru-Ying Chang, Chung-Hsien Wu, and Philips K. Prasetyo. 2012. Error diagnosis of Chinese sentences using 
inductive learning algorithm and decomposition-based testing mechanism. ACM Transactions on Asian Lan-
guage Information Processing, 11(1):Article 3. 
Yu-Fang Tsai and Keh-Jiann Chen. 2004. Reliable and cost-effective pos-tagging. International Journal of 
Computational Linguistics and Chinese Language Processing, 9(1):83-96. 
70
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1474?1480,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Leveraging Effective Query Modeling Techniques  
for Speech Recognition and Summarization 
 
 Kuan-Yu Chen*?,  Shih-Hung Liu*, Berlin Chen#, Ea-Ee Jan+,  
Hsin-Min Wang*, Wen-Lian Hsu*, and Hsin-Hsi Chen? 
*Institute of Information Science, Academia Sinica, Taiwan 
?National Taiwan University, Taiwan 
#National Taiwan Normal University, Taiwan 
+IBM Thomas J. Watson Research Center, USA 
{kychen, journey, whm, hsu}@iis.sinica.edu.tw, 
berlin@ntnu.edu.tw, hhchen@csie.ntu.edu.tw, ejan@us.ibm.com 
 
Abstract 
Statistical language modeling (LM) that 
purports to quantify the acceptability of a 
given piece of text has long been an in-
teresting yet challenging research area. In 
particular, language modeling for infor-
mation retrieval (IR) has enjoyed re-
markable empirical success; one emerg-
ing stream of the LM approach for IR is 
to employ the pseudo-relevance feedback 
process to enhance the representation of 
an input query so as to improve retrieval 
effectiveness. This paper presents a con-
tinuation of such a general line of re-
search and the main contribution is three-
fold. First, we propose a principled 
framework which can unify the relation-
ships among several widely-used query 
modeling formulations. Second, on top of 
the successfully developed framework, 
we propose an extended query modeling 
formulation by incorporating critical que-
ry-specific information cues to guide the 
model estimation. Third, we further adopt 
and formalize such a framework to the 
speech recognition and summarization 
tasks. A series of empirical experiments 
reveal the feasibility of such an LM 
framework and the performance merits of 
the deduced models on these two tasks. 
1 Introduction 
Along with the rapidly growing popularity of the 
Internet and the ubiquity of social web commu-
nications, tremendous volumes of multimedia 
contents, such as broadcast radio and television 
programs, digital libraries and so on, are made 
available to the public. Research on multimedia 
content understanding and organization has wit-
nessed a booming interest over the past decade. 
By virtue of the developed techniques, a variety 
of functionalities were created to help distill im-
portant content from multimedia collections, or 
provide locations of important speech segments 
in a video accompanied with their corresponding 
transcripts, for users to listen to or to digest. Sta-
tistical language modeling (LM) (Jelinek, 1999; 
Jurafsky and Martin, 2008; Zhai, 2008), which 
manages to quantify the acceptability of a given 
word sequence in a natural language or capture 
the statistical characteristics of a given piece of 
text, has been proved to offer both efficient and 
effective modeling abilities in many practical 
applications of natural language processing and 
speech recognition (Ponte and Croft, 1998; Jelin-
ek, 1999; Huang, et al., 2001; Zhai and Lafferty, 
2001a; Jurafsky and Martin, 2008; Furui et al., 
2012; Liu and Hakkani-Tur, 2011). 
The LM approach was first introduced for the 
information retrieval (IR) problems in the late 
1990s, indicating very good potential, and was 
subsequently extended in a wide array of follow-
up studies. One typical realization of the LM ap-
proach for IR is to access the degree of relevance 
between a query and a document by computing 
the likelihood of the query generated by the doc-
ument (usually referred to as the query-
likelihood approach) (Zhai, 2008; Baeza-Yates 
and Ribeiro-Neto, 2011). A document is deemed 
to be relevant to a given query if the correspond-
ing document model is more likely to generate 
the query. On the other hand, the Kullback-
Leibler divergence measure (denoted by KLM 
for short hereafter), which quantifies the degree 
of relevance between a document and a query 
from a more rigorous information-theoretic per-
spective, has been proposed (Lafferty and Zhai, 
2001; Zhai and Lafferty, 2001b; Baeza-Yates and 
Ribeiro-Neto, 2011). KLM not only can be 
thought as a natural generalization of the query-
likelihood approach, but also has the additional 
merit of being able to accommodate extra infor-
mation cues to improve the performance of doc-
ument ranking. For example, a main challenge 
facing such a measure is that since a given query 
usually consists of few words, the true infor-
mation need is hard to be inferred from the sur-
face statistics of a query. As such, one emerging 
stream of thought for KLM is to employ the 
1474
pseudo-relevance feedback process to construct 
an enhanced query model (or representation) so 
as to achieve better retrieval effectiveness (Hi-
emstra et al., 2004; Lv and Zhai, 2009; Carpineto 
and Romano, 2012; Lee and Croft, 2013). 
Following this line of research, the major con-
tribution of this paper is three-fold: 1) we ana-
lyze several widely-used query models and then 
propose a principled framework to unify the rela-
tionships among them; 2) on top of the success-
fully developed query models, we propose an 
extended modeling formulation by incorporating 
additional query-specific information cues to 
guide the model estimation; 3) we explore a nov-
el use of these query models by adapting them to 
the speech recognition and summarization tasks. 
As we will see, a series of experiments indeed 
demonstrate the effectiveness of the proposed 
models on these two tasks. 
2 Language Modeling Framework 
2.1 Kullback-Leibler Divergence Measure 
A promising realization of the LM approach to 
IR is the Kullback-Leibler divergence measure 
(KLM), which determines the degree of rele-
vance between a document and a query from a 
rigorous information-theoretic perspective. Two 
different language models are involved in KLM: 
one for the document and the other for the query. 
The divergence of the document model with re-
spect to the query model is defined by  
.)|( )|(log)|()||(KL ? ?? Vw DwP QPQwPDQ
  (1)  
KLM not only can be thought as a natural gener-
alization of the traditional query-likelihood ap-
proach (Yi and Allan, 2009; Baeza-Yates and 
Ribeiro-Neto, 2011), but also has the additional 
merit of being able to accommodate extra infor-
mation cues to improve the estimation of its 
component models in a systematic way for better 
document ranking (Zhai, 2008).  
Due to that a query usually consists of only a 
few words, the true query model P(w|Q)
 
might 
not be accurately estimated by the simple ML 
estimator (Jelinek, 1991). There are several stud-
ies devoted to estimating a more accurate query 
modeling, saying that it can be approached with 
the pseudo-relevance feedback process (Lavren-
ko and Croft, 2001; Zhai and Lafferty, 2001b). 
However, the success depends largely on the as-
sumption that the set of top-ranked documents, 
DTop={D1,D2,...,Dr,...}, obtained from an initial 
round of retrieval, are relevant and can be used to 
estimate a more accurate query language model. 
2.2 Relevance Modeling  
Under the notion of relevance modeling (RM, 
often referred to as RM-1), each query Q is as-
sumed to be associated with an unknown rele-
vance class RQ, and documents that are relevant 
to the semantic content expressed in query are 
samples drawn from the relevance class RQ. 
Since there is no prior knowledge about RQ, we 
may use the top-ranked documents DTop to ap-
proximate the relevance class RQ. The corre-
sponding relevance model can be estimated using 
the following equation (Lavrenko and Croft, 
2001; Lavrenko, 2004): 
.)|()(
)|()|()(  )|(RM ? ? ??
? ? ??
???
?
?????
??
ToprD
ToprD
Qw rr
Qw rrr
DwPDP
DwPDwPDPQwP
D
D
(2) 
2.3 Simple Mixture Model 
Another perspective of estimating an accurate 
query model with the top-ranked documents is 
the simple mixture model (SMM), which as-
sumes that words in DTop are drawn from a two-
component mixture model: 1) One component is 
the query-specific topic model PSMM(w|Q), and 2) 
the other is a generic background model 
P(w|BG). By doing so, the SMM model 
PSMM(w|Q) can be estimated by maximizing the 
likelihood over all the top-ranked documents 
(Zhai and Lafferty, 2001b; Tao and Zhai, 2006): 
? ? ,)|()1()|( ),(SMM?? ?? ????? Topr rD Vw DwcBGwPQwPL D ??
(3) 
where ?  is a pre-defined weighting parameter 
used to control the degree of reliance between 
PSMM(w|Q) and P(w|BG). This estimation will 
enable more specific words to receive more 
probability mass, thereby leading to a more dis-
criminative query model PSMM(w|Q). 
Although the SMM modeling aims to extract 
extra word usage cues for enhanced query mod-
eling, it may confront two intrinsic problems. 
One is the extraction of word usage cues from 
DTop is not guided by the original query. The oth-
er is that the mixing coefficient ?  is fixed across 
all top-ranked documents albeit that different 
documents would potentially contribute different 
amounts of word usage cues to the enhanced 
query model. To mitigate these two problems, 
the regularized simple mixture model has been 
proposed and can be estimated by maximizing 
the likelihood function (Tao and Zhai, 2006; Dil-
lon and Collins-Thompson, 2010) 
? ? ,)|()1()|(    
)|(
),(
RSMM
)|(
RSMM
?
?
?
?
?
?
?
????
??
Topr
r
rrD Vw
Dwc
DD
Vw
QwP
BGwPQwP
QwPL
D
??
?
(4) 
where   is a weighting factor indicating the con-
fidence on the prior information. 
3 The Proposed Modeling Framework 
3.1 Fundamentals 
It is obvious that the major difference among the 
1475
representative query models mentioned above is 
how to capitalize on the set of top-ranked docu-
ments and the original query. Several subtle rela-
tionships can be deduced through the following 
in-depth analysis. First, a direct inspiration of the 
LM-based query reformulation framework can 
be drawn from the celebrated Rocchio?s formula-
tion, while the former can be viewed as a proba-
bilistic counterpart of the latter (Robertson, 1990; 
Ponte and Croft, 1998; Baeza-Yates and Ribeiro-
Neto, 2011). Second, after some mathematical 
manipulation, the formulation of the RM model 
(c.f. Eq. (2)) can be rewritten as 
.)()|(
)()|()|(  )|(RM ? ? ???? ????? ToprD ToprD rr
rrr DPDQP
DPDQPDwPQwP D D
(5) 
It becomes evident that the RM model is com-
posed by mixing a set of document models 
P(w|Dr). As such, the RM model bears a close 
resemblance to the Rocchio?s formulation. Fur-
thermore, based on Eq. (5), we can recast the 
estimation of the RM model as an optimization 
problem, and the likelihood (or objective) func-
tion is formulated as 
1)|( ..
,)|()|(
),(
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Topr
Topr
D
r
Vw
Qwc
D
rr
QDPts
QDPDwPL
D
D
  (6) 
where the document models P(w|Dr) are known 
in advance; the conditional probability P(Dr|Q) 
of each document Dr is unknown and leave to be 
estimated. Finally, a principled framework can 
be obtained to unify all of these query models, 
including RM (c.f. Eq. (6)), SMM (c.f. Eq. (3)) 
and RSMM (c.f. Eq. (4))), by using a generalized 
objective likelihood function: 
1)( ..
,)()|(
),(
?
???
?
???
?
?
?
?
?
?
?
?
?
?
M
E M
r
i
i
r
M
r
Vw E
Ewc
M
rr
MPts
MPMwPL  (7) 
where E represents a set of observations which 
we want to maximize their likelihood, and M 
denotes a set of mixture components.  
3.2 Query-specific Mixture Modeling 
The SMM model and the RSMM model are in-
tended to extract useful word usage cues from 
DTop, which are not only relevant to the original 
query Q but also external to those already cap-
tured by the generic background model. Howev-
er, we argue in this paper that the ?generic in-
formation? should be carefully crafted for each 
query due mainly to the fact that users? infor-
mation needs may be very diverse from one an-
other. To crystallize the idea, a query-specific 
background model PQ(w|BG) for each query Q 
can be derived from DTop directly. Another con-
sideration is that since the original query model 
P(w|Q) cannot be accurately estimated, it thus 
may not necessarily be the best choice for use in 
defining a conjugate Dirichlet prior for the en-
hanced query model to be estimated. We propose 
to use the RM model as a prior to guide the esti-
mation of the enhanced query model. The en-
hanced query model is termed query-specific 
mixture model (QMM), and its corresponding 
training objective function can be expressed as 
? ??
?
?
?
?
?
?
????
??
Topr
r
rrD Vw
DwcQDD
Vw
QwP
BGwPQwP
QwPL
D
.)|()1()|(    
)|(
),(QMM
)|(QMM RM
??
?
 (8) 
4 Applications 
4.1 Speech Recognition 
Language modeling is a critical and integral 
component in any large vocabulary continuous 
speech recognition (LVCSR) system (Huang et 
al., 2001; Jurafsky and Martin, 2008; Furui et al., 
2012). More concretely, the role of language 
modeling in LVCSR can be interpreted as calcu-
lating the conditional probability P(w|H), in 
which H is a search history, usually expressed as 
a sequence of words H=h1, h2,?, hL, and w is 
one of its possible immediately succeeding 
words. Once the various aforementioned query 
modeling methods are applied to speech recogni-
tion, for a search history H, we can conceptually 
regard it as a query and each of its immediately 
succeeding words w as a (single-word) document. 
Then, we may leverage an IR procedure that 
takes H as a query and poses it to a retrieval sys-
tem to obtain a set of top-ranked documents from 
a contemporaneous (or in-domain) corpus. Final-
ly, the enhanced query model (that is P(w|H) in 
speech recognition) can be estimated by RM, 
SMM, RSMM or QM , and further combined 
with the background n-gram (e.g., trigram) lan-
guage model to form an adaptive language model 
to guide the speech recognition process. 
4.2 Speech Summarization 
On the other hand, extractive speech summariza-
tion aims at producing a concise summary by 
selecting salient sentences or paragraphs from 
the original spoken document according to a pre-
defined target summarization ratio (Carbonell 
and Goldstein, 1998; Mani and Maybury, 1999; 
Nenkova and McKeown, 2011; Liu and 
Hakkani-Tur, 2011). Intuitively, this task could 
be framed as an ad-hoc IR problem, where the 
spoken document is treated as an information 
need and each sentence of the document is re-
garded as a candidate information unit to be re-
trieved according to its relevance to the infor-
mation need. Therefore, KLM can be used to 
quantify how close the document D and one of 
its sentences S are: the closer the sentence model 
P(w|S) to the document model P(w|D), the more 
1476
likely the sentence would be part of the summary. 
Due to that each sentence S of a spoken docu-
ment D to be summarized usually consists of 
only a few words, the corresponding sentence 
model P(w|S) might not be appropriately esti-
mated by the ML estimation. To alleviate the 
deficiency, we can leverage the merit of the 
above query modeling techniques to estimate an 
accurate sentence model for each sentence to 
enhance the summarization performance. 
5 Experimental Setup 
The speech corpus consists of about 196 hours of 
Mandarin broadcast news collected by the Aca-
demia Sinica and the Public Television Service 
Foundation of Taiwan between November 2001 
and April 2003 (Wang et al., 2005), which is 
publicly available and has been segmented into 
separate stories and transcribed manually. Each 
story contains the speech of one studio anchor, as 
well as several field reporters and interviewees. 
A subset of 25-hour speech data compiled during 
November 2001 to December 2002 was used to 
bootstrap the acoustic model training. The vo-
cabulary size is about 72 thousand words. The 
background language model was estimated from 
a background text corpus consisting of 170 mil-
lion Chinese characters collected from the Chi-
nese Gigaword Corpus released by LDC. 
The dataset for use in the speech recognition 
experiments is compiled by a subset of 3-hour 
speech data from the corpus within 2003 (1.5 
hours for development and 1.5 hours for test). 
The contemporaneous (in-domain) text corpus 
used for training the various LM adaptation 
methods was collected between 2001 and 2003 
from the corpus (excluding the test set), which 
consists of one million Chinese characters of the 
orthographic broadcast news transcripts. In this 
paper, all the LM adaptation experiments were 
performed in word graph rescoring. The associ-
ated word graphs of the speech data were built 
beforehand with a typical LVCSR system (Ort-
manns et al., 1997; Young et al., 2006). 
In addition, the summarization task also em-
ploys the same broadcast news corpus as well. A 
subset of 205 broadcast news documents com-
piled between November 2001 and August 2002 
was reserved for the summarization experiments 
(185 for development and 20 for test). A subset 
of about 100,000 text news documents, compiled 
during the same period as the documents to be 
summarized, was employed to estimate the relat-
ed summarization models compared in this paper. 
We adopted three variants of the widely-used 
ROUGE metric (i.e., ROUGE-1, ROGUE-2 and 
ROUGE-L) for the assessment of summarization 
performance (Lin, 2003). The summarization 
ratio, defined as the ratio of the number of words 
in the automatic (or manual) summary to that in 
the reference transcript of a spoken document, 
was set to 10% in this research. 
6 Experimental Results 
In the first part of experiments, we evaluate the 
effectiveness of the various query models applied 
to the speech recognition task. The correspond-
ing results with respect to different numbers of 
top-ranked documents being used for estimating 
their component models are shown in Table 1. 
Also worth mentioning is that the baseline sys-
tem with the background trigram language model, 
which was trained with the SRILM toolkit 
(Stolcke, 2005) and Good-Turing smoothing 
(Jelinek, 1999), results in a Chinese character 
error rate (CER) of 20.08% on the test set. Con-
sulting Table 1 we notice two particularities. One 
is that there is more fluctuation in the CER re-
sults of SMM than in those of RM. The reason 
might be that, for SMM, the extraction of rele-
vance information from the top-ranked docu-
ments is conducted with no involvement of the 
test utterance (i.e., the query; or its correspond-
ing search histories), as elaborated earlier in Sec-
tion 2. When too many feedback documents are 
being used, there would be a concern for SMM 
to be distracted from being able to appropriate 
model the test utterance, which is probably 
caused by some dominant distracting (or irrele-
vant) feedback documents. The other interesting 
observation is that RSMM only achieves a com-
parable (even worse) result when compared to 
SMM. A possible reason is that the prior con-
straint of the RSMM may contain too much 
noisy information so as to bias the model estima-
tion. Furthermore, it is evident that the proposed 
QMM is the best-performing method among all 
the query models compared in the paper. Alt-
hough the improvements made by QMM are not 
as pronounced as expected, we believe that 
QMM has demonstrated its potential to be ap-
plied to other related applications. On the other 
hand, we compare the various query models with 
two well-practiced language models, namely the 
cache model (Cache) (Kuhn and Mori, 1990; 
Jelinek et al., 1991) and the latent Dirichlet allo-
cation (LDA) (Liu and Liu, 2007; Tam and 
Schultz, 2005). The CER results of these two 
models are also shown in Table 1, respectively. 
For the cache model, bigram cache was used 
since it can yield better results than the unigram 
and trigram cache models in our experiments. It 
is worthy to notice that the LDA model was 
trained with the entire set of contemporaneous 
text document collection (c.f. Section 4), while 
all of the query models explored in the paper 
were estimated based on a subset of the corpus 
selected by an initial round of retrieval. The re-
sults reveal that most of these query models can 
achieve superior performance over the two con-
ventional language models. 
1477
In the second part of experiments, we evaluate 
the utilities of the various query models as ap-
plied to the speech summarization task. At the 
outset, we assess the performance level of the 
baseline KLM method by comparison with two 
well-practiced unsupervised methods, viz. the 
vector space model (VSM) (Gong and Liu, 2001), 
and its extension, maximal marginal relevance 
(MMR) (Carbonell and Goldstein, 1998). The 
corresponding results are shown in Table 2 and 
can be aligned with several related literature re-
views. By looking at the results, we find that 
KLM outperforms VSM by a large margin, con-
firming the applicability of the language model-
ing framework for speech summarization. Fur-
thermore, MMR that presents an extension of 
VSM performs on par with KLM for the text 
summarization task (TD) and exhibits superior 
performance over KLM for the speech summari-
zation task (SD). We now turn to evaluate the 
effectiveness of the various query models (viz. 
RM, SMM, RSMM and QMM) in conjunction 
with the pseudo-relevance feedback process for 
enhancing the sentence model involved in the 
KLM method. The corresponding results are also 
shown in Table 2. Two noteworthy observations 
can be drawn from Table 2. One is that all these 
query models can considerably improve the 
summarization performance of the KLM method, 
which corroborates the advantage of using them 
for enhanced sentence representations. The other 
is that QMM is the best-performing one among 
all the formulations studied in this paper for both 
the TD and SD cases.  
Going one step further, we explore to use extra 
prosodic features that are deemed complemen-
tary to the LM cue provided by QMM for speech 
summarization. To this end, a support vector ma-
chine (SVM) based summarization model is 
trained to integrate a set of 28 commonly-used 
prosodic features (Liu and Hakkani-Tur, 2011) 
for representing each spoken sentence, since 
SVM is arguably one of the state-of-the-art su-
pervised methods that can make use of a diversi-
ty of indicative features for text or speech sum-
marization (Xie and Liu, 2010; Chen et al., 
2013). The sentence ranking scores derived by 
QMM and SVM are in turn integrated through a 
simple log-linear combination. The correspond-
ing results are shown in Table 2, demonstrating 
consistent improvements with respect to all the 
three variants of the ROUGE metric as compared 
to that using either QMM or SVM in isolation. 
We also investigate using SVM to additionally 
integrate a richer set of lexical and relevance fea-
tures to complement QMM and further enhance 
the summarization effectiveness. However, due 
to space limitation, we omit the details here. As a 
side note, there is a sizable gap between the TD 
and SD cases, indicating room for further im-
provements. We may seek remedies, such as ro-
bust indexing schemes, to compensate for imper-
fect speech recognition. 
7 Conclusion and Outlook 
In this paper, we have presented a systematic and 
thorough analysis of a few well-practiced query 
models for IR and extended their novel applica-
bility to speech recognition and summarization in 
a principled way. Furthermore, we have pro-
posed an extension of this research line by intro-
ducing query-specific mixture modeling; the util-
ities of the deduced model have been extensively 
compared with several existing query models. As 
to future work, we would like to investigate 
jointly integrating proximity and other different 
kinds of relevance and lexical/semantic infor-
mation cues into the process of feedback docu-
ment selection so as to improve the empirical 
effectiveness of such query modeling.  
Acknowledgements 
This research is supported in part by the ?Aim 
for the Top University Project? of National Tai-
wan Normal University (NTNU), sponsored by 
the Ministry of Education, Taiwan, and by the 
Ministry of Science and Technology, Taiwan, 
under Grants MOST 103-2221-E-003-016-MY2, 
NSC 101-2221-E-003-024-MY3, NSC 102-
2221-E-003-014-, NSC 101-2511-S-003-057-
MY3, NSC 101-2511-S-003-047-MY3 and NSC 
103-2911-I-003-301. 
  
Table 1. The speech recognition results (in CER 
(%)) achieved by various language models along 
with different numbers of latent topics/pseudo-
relevance feedback documents. 
 16 32 64 128 
Baseline 20.08 
Cache 19.86 
LDA 19.29 19.30 19.28 19.15 
RM 19.26 19.26 19.26 19.26 
SMM 19.19 19.00 19.14 19.10 
RSMM 19.18 19.14 19.15 19.19 
QMM 19.05 18.97 19.00 18.99 
Table 2. The summarization results (in F-scores) 
achieved by various language models along with 
text and spoken documents. 
 
Text Documents (TD) Spoken Documents (SD) 
ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L 
VSM 0.347 0.228 0.290 0.342 0.189 0.287 
MMR 0.407 0.294 0.358 0.381 0.226 0.331 
KLM 0.411 0.298 0.361 0.364 0.210 0.307 
RM 0.453 0.335 0.403 0.382 0.239 0.331 
SMM 0.439 0.320 0.388 0.383 0.229 0.327 
RSMM 0.472 0.365 0.423 0.381 0.235 0.329 
QMM 0.486 0.382 0.435 0.395 0.256 0.349 
SVM 0.441 0.334 0.396 0.370 0.222 0.326 
QMM+
SVM 
0.492 0.395 0.448 0.398 0.261 0.358 
 
 
 
 
1478
References 
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 
2011. Modern information retrieval: the con-
cepts and technology behind search, ACM 
Press. 
David M. Blei, Andrew Y. Ng, and Michael I. 
Jordan. 2003. Latent dirichlet allocation. 
Journal of Machine Learning Research, 
pp.993?1022. 
David M. Blei and John Lafferty. 2009. Topic 
models. In A. Srivastava and M. Sahami, 
(eds.), Text Mining: Theory and Applications. 
Taylor and Francis.  
Jaime Carbonell and Jade Goldstein. 1998. The 
use of MMR, diversitybased reranking for 
reordering documents and producing sum-
maries. In Proc. SIGIR, pp. 335?336. 
Claudio Carpineto and Giovanni Romano. 2012. 
A survey of automatic query expansion in in-
formation retrieval. ACM Computing Surveys, 
vol. 44, pp.1?56. 
Stephane Clinchant and Eric Gaussier. 2013. A 
theoretical analysis of pseudo-relevance 
feedback models. In Proc. ICTIR. 
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and 
Stephen Robertson. 2008. Selecting good 
expansion terms for pseudo-relevance feed-
back. In Proc. SIGIR, pp. 243?250. 
Berlin Chen, Shih-Hsiang Lin, Yu-Mei Chang, 
and Jia-Wen Liu. 2013. Extractive speech 
summarization using evaluation metric-
related training criteria. Information Pro-
cessing & Management, 49(1), pp. 1cess 
Arthur P. Dempster, Nan M. Laird, and Donald 
B. Rubin. 1977. Maximum likelihood from 
incomplete data via the EM algorithm. Jour-
nal of Royal Statistical Society B, 39(1), pp. 
1?38. 
Joshua V. Dillon and Kevyn Collins-Thompson. 
2010. A unified optimization framework for 
robust pseudo-relevance feedback algorithms. 
In Proc. CIKM, pp. 1069?1078. 
Sadaoki Furui, Li Deng, Mark Gales, Hermann 
Ney, and Keiichi Tokuda. 2012. Fundamen-
tal technologies in modern speech recogni-
tion. IEEE Signal Processing Magazine, 
29(6), pp. 16?17. 
Yihong Gong and Xin Liu. 2001. Generic text 
summarization using relevance measure and 
latent semantic analysis. In Proc. SIGIR, pp. 
19?25. 
Djoerd Hiemstra, Stephen Robertson, and Hugo 
Zaragoza. 2004. Parsimonious language 
models for information retrieval. In Proc. 
SIGIR, pp. 178?185. 
Thomas Hofmann. 1999. Probabilistic latent se-
mantic indexing. In Proc. SIGIR, pp. 50?57.  
Thomas Hofmann. 2001. Unsupervised learning 
by probabilistic latent semantic analysis. 
Machine Learning, 42, pp. 177?196.  
Xuedong Huang, Alex Acero, and Hsiao-Wuen 
Hon. 2001. Spoken language processing: a 
guide to theory, algorithm, and system de-
velopment. Prentice Hall PTR, Upper Saddle 
River, NJ, USA. 
Frederick Jelinek, Bernard Merialdo, Salim Rou-
kos, and M. Strauss. 1991. A dynamic lan-
guage model for speech recognition. In Proc. 
the DARPA workshop on speech and natural 
language, pp. 293?295. 
Frederick Jelinek. 1999. Statistical methods for 
speech recognition. MIT Press. 
Daniel Jurafsky and James H. Martin. 2008. 
Speech and language processing. Prentice 
Hall PTR, Upper Saddle River, NJ, USA. 
Roland Kuhn and Renato D. Mori. 1990. A 
cache-based natural language model for 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
12(6), pp. 570?583. 
Solomon Kullback and Richard A. Leibler. 1951. 
On information and sufficiency. The Annals 
of Mathematical Statistics, 22(1), pp. 79?86. 
Chin-Yew Lin. 2003. ROUGE: Recall-oriented 
Understudy for Gisting Evaluation. Availa-
ble: http://haydn.isi.edu/ROUGE/. 
Feifan Liu and Yang Liu. 2007. Unsupervised 
language model adaptation incorporating 
named entity information. In Proc. ACL, pp. 
672?769. 
Yang Liu and Dilek Hakkani-Tur. 2011. Speech 
summarization. Chapter 13 in Spoken Lan-
guage Understanding: Systems for Extract-
ing Semantic Information from Speech, G. 
Tur and R. D. Mori (Eds), New York: Wiley. 
John Lafferty and Chengxiang Zhai. 2001. Doc-
ument language models, query models, and 
risk minimization for information retrieval. 
In Proc. SIGIR, pp. 111?119. 
Victor Lavrenko and W. Bruce Croft. 2001. Rel-
evance-based language models. In Proc. 
SIGIR, pp. 120?127. 
Victor Lavrenko. 2004. A Generative Theory of 
Relevance. PhD thesis, University of Massa-
chusetts, Amherst. 
1479
Shasha Xie and Yang Liu. 2010. Improving su-
pervised learning for meeting summarization 
using sampling and regression. Computer 
Speech & Language, 24(3), pp. 495?514. 
Yuanhua Lv and Chengxiang Zhai. 2009. A 
comparative study of methods for estimating 
query language models with pseudo feed-
back. In Proc. CIKM, pp. 1895?1898. 
Yuanhua Lv and Chengxiang Zhai. 2010. Posi-
tional relevance model for pseudo-relevance 
feedback. In Proc. SIGIR, pp. 579?586. 
Kyung Soon Lee, W. Bruce Croft, and James 
Allan. 2008. A cluster-based resampling 
method for pseudo-relevance feedback. In 
Proc. SIGIR, pp. 235?242. 
Kyung Soon Lee and W. Bruce Croft. 2013. A 
deterministic resampling method using over-
lapping document clusters for pseudo-
relevance feedback. Inf. Process. Manage. 
49(4), pp. 792?806. 
Inderjeet Mani and Mark T. Maybury (Eds.). 
1999. Advances in automatic text summari-
zation. Cambridge, MA: MIT Press. 
Ani Nenkova and Kathleen McKeown. 2011. 
Automatic summarization. Foundations and 
Trends in Information Retrieval, 5(2?3), pp. 
103?233. 
Stefan Ortmanns, Hermann Ney, and Xavier Au-
bert. 1997. A word graph algorithm for large 
vocabulary continuous speech recognition. 
Computer Speech and Language, pp. 43?72. 
Jay M. Ponte and W. Bruce Croft. 1998. A lan-
guage modeling approach to information re-
trieval. In Proc. SIGIR, pp. 275?281. 
Stephen E. Robertson. 1990. On term selection 
for query expansion. Journal of Documenta-
tion, 46(4), pp. 359?364. 
Andreas Stolcke. 2005. SRILM - An extensible 
language modeling toolkit. In Proc. INTER-
SPEECH, pp.901?904. 
Tao Tao and Chengxiang Zhai. 2006. Regular-
ized estimation of mixture models for robust 
pseudo-relevance feedback. In Proc. SIGIR, 
pp. 162?169. 
Yik-Cheung Tam and Tanja Schultz. 2005. Dy-
namic language model adaptation using vari-
ational Bayes inference. In Proc. INTER-
SPEECH, pp. 5?8. 
Xuanhui Wang, Hui Fang, and Chengxiang Zhai. 
2008. A study of methods for negative rele-
vance feedback. In Proc. SIGIR, pp. 219?226. 
Hsin-Min Wang, Berlin Chen, Jen-Wei Kuo, and 
Shih-Sian Cheng. 2005. MATBN: A Manda-
rin Chinese broadcast news corpus. Interna-
tional Journal of Computational Linguistics 
& Chinese Language Processing, 10(2), pp. 
219?236. 
Xing Yi and James Allan. 2009. A comparative 
study of utilizing topic models for infor-
mation retrieval. In Proc. ECIR, pp. 29?41. 
Steve Young, Dan Kershaw, Julian Odell, Dave 
Ollason, Valtcho Valtchev, and Phil Wood-
land. 2006. The HTK book version 3.4. 
Cambridge University Press. 
Chengxiang Zhai and John Lafferty. 2001a. A 
study of smoothing methods for language 
models applied to ad hoc information re-
trieval. In Proc. SIGIR, pp. 334?342.  
Chengxiang Zhai and John Lafferty. 2001b. 
Model-based feedback in the language mod-
eling approach to information retrieval. In 
Proc. CIKM, pp. 403?410. 
Chengxiang Zhai. 2008. Statistical language 
models for information retrieval: a critical 
review. Foundations and Trends in Infor-
mation Retrieval, 2 (3), pp. 137?213. 
Yi Zhang, Jamie Callan, and Thomas Minka. 
2002. Novelty and redundancy detection in 
adaptive filtering. In Proc. SIGIR, pp. 81?88.  
1480
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 12?16,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Chinese Open Relation Extraction for Knowledge Acquisition Yuen-Hsien Tseng1, Lung-Hao Lee1,2, Shu-Yen Lin1, Bo-Shun Liao1,  Mei-Jun Liu1, Hsin-Hsi Chen2, Oren Etzioni3, Anthony Fader4   1Information Technology Center, National Taiwan Normal University  2Dept. of Computer Science and Information Engineering, National Taiwan University 3Allen Institute for Artificial Intelligence, Seattle, WA 4Dept. of Computer Science and Engineering, University of Washington  {samtseng, lhlee, sylin, skylock, meijun}@ntnu.edu.tw, hhchen@ntu.edu.tw, OrenE@allenai.org, afader@cs.washington.edu  Abstract 
This study presents the Chinese Open Relation Extraction (CORE) system that is able to extract entity-relation triples from Chinese free texts based on a series of NLP techniques, i.e., word segmentation, POS tagging, syntactic parsing, and extraction rules. We employ the proposed CORE techniques to extract more than 13 million entity-relations for an open domain question answering application. To our best knowledge, CORE is the first Chinese Open IE system for knowledge acquisition.  1 Introduction  Traditional Information Extraction (IE) involves human intervention of handcrafted rules or tagged examples as the input for machine learning to recognize the assertion of a particular relationship between two entities in texts (Riloff, 1996; Soderland, 1999). Although machine learning helps enumerate potential relation patterns for extraction, this approach is often limited to extracting the relation sets that are predefined. In addition, traditional IE has focused on satisfying pre-specified requests from small homogeneous corpora, leaving the question open whether it can scale up to massive and heterogeneous corpora such as the Web (Banko and Etzioni, 2008; Etzioni et al., 2008, 2011). Open IE, a new domain-independent knowledge discovery paradigm that extracts a diverse set of relations without requiring any relation-specific human inputs and a pre-specified vocabulary, is especially suited to 
massive text corpora, where target relations are unknown in advance. Several Open IE systems, such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), ReVerb (Fader et al., 2011), and OLLIE (Mausam et al., 2012) achieve promising performance in open relation extraction on English sentences. However, application of these systems poses challenges to those languages that are very different from English, such as Chinese, as grammatical functions in English and Chinese are realized in markedly different ways. It is not sure whether those techniques for English still work for Chinese. This issue motivates us to extend the state-of-the-art Open IE systems to extract relations from Chinese texts. The relatively rich morpho-syntactic marking system of English (e.g., verbal inflection, nominal case, clausal markers) makes the syntactic roles of many words detectable from their surface forms. A tensed verb in English, for example, generally indicates its main verb status of a clause. The pinning down of the main verb in a Chinese clause, on the other hand, must rely on other linguistic cues such as word context due to the lack of tense markers. In contrast to the syntax-oriented English language, Chinese is discourse-oriented and rich in ellipsis ? meaning is often construable in the absence of explicit linguistic devices such that many obligatory grammatical categories (e.g., pronouns and BE verbs) can be elided in Chinese.  For example, the three Chinese sentences ???????? (?Apples nutritious?), ????????? ? (?Apples are nutritious?), and ???????? 
12
(?Apples are rich in nutrition?) are semantically synonymous sentences, but the first one, which lacks an overt verb, is used far more often than the other two. Presumably, an adequate multilingual IE system must take into account those intrinsic differences between languages. This paper introduces the Chinese Open Relation Extraction (CORE) system, which utilizes a series of NLP techniques to extract relations embedded in Chinese sentences. Given a Chinese text as the input, CORE employs word segmentation, part-of-speech (POS) tagging, and syntactic parsing, to automatically annotate the Chinese sentences. Based on this rich information, the input sentences are chunked and the entity-relation triples are extracted. Our evaluation shows the effectiveness of CORE, and its deficiency as well. 2 Related Work TextRunner (Banko et al., 2007) was the first Open IE system, which trains a Na?ve Bayes classifier with POS and NP-chunk features to extract relationships between entities. The subsequent work showed that employing the classifiers capable of modeling the sequential information inherited in the texts, like linear-chain CRF (Banko and Etzioni, 2008) and Markov Logic Network (Zhu et al., 2009), can result in better extraction performance. The WOE system (Wu and Weld, 2010) adopted Wikipedia as the training source for their extractor. Experimental results indicated that parsed dependency features lead to further improvements over TextRunner.  ReVerb (Fader et al., 2011) introduced another approach by identifying first a verb-centered relational phrase that satisfies their pre-defined syntactic and lexical constraints, and then split the input sentence into an Argument-Verb-Argument triple. This approach involves only POS tagging for English and ?regular expression?-like matching. As such, it is suitable for large corpora, and likely to be applicable to Chinese.  
For multilingual open IE, Gamallo et al. (2012) adopts a rule-based dependency parser to extract relations represented in English, Spanish, Portuguese, and Galician. For each parsed sentence, they separate each verbal clause and then identify each one?s verb participants, including their functions: subject, direct object, attribute, and prepositional complements. A set of rules is then applied on the clause constituents to extract the target triples. For Chinese open IE, we adopt a similar general approach. The main differences are the processing steps specific to Chinese language. 3 Chinese Open Relation Extraction This section describes the components of CORE. Not requiring any predefined vocabulary, CORE?s sole input is a Chinese corpus and its output is an extracted set of relational tuples. The system consists of three key modules, i.e., word segmentation and POS tagging, syntactic parsing, and entity-relation triple extraction, which are introduced as follows: Chinese is generally written without word boundaries. As a result, prior to the implementation of most NLP tasks, texts must undergo automatic word segmentation. Automatic Chinese word segmenters are generally trained by an input lexicon and probability models. However, it usually suffers from the unknown word (i.e., the out-of-vocabulary, or OOV) problem. In CORE, a corpus-based learning method to merge the unknown words is adopted to tackle the OOV problem (Chen and Ma, 2002). This is followed by a reliable and cost-effective POS-tagging method to label the segmented words with part-of-speeches (Tsai and Chen, 2004). Take the Chinese sentence ?????????? (?Edison invented the light bulb?) for instance. It was segmented and tagged as follows: ???/Nb  ??/VC  ?/Di  ??/Na. Among these words, the translation of a foreign proper name ????? (?Edison?) is not likely to be included in a lexicon and therefore is extracted by the unknown word detection method. In this case, 
13
the special POS tag ?Di? is a tag to represent a verb?s tense when its character ??? follows immediately after its precedent verb. The complete set of part-of-speech tags is defined in the technical report (CKIP, 1993). In the above sentence, ?? ? could represent a complete different meaning if it is associate with other character, such as ???? meaning ?understand?. Therefore, ????????? ? (?Edison invented a cure?) would be segmented incorrectly once ?? ? is associated with its following character, instead of its precedent word. We adopt CKIP, the best-performing parser in the bakeoff of SIGHAN 2012 (Tseng et al., 2012), to do syntactic structure analysis. The CKIP solution re-estimates the context-dependent probability for Chinese parsing and improves the performance of probabilistic context-free grammar (Hsieh et al., 2012). For the example sentence above, ????/Nb? and ??? /Na? were annotated as two nominal phrases (i.e., ?NP?), and ???/VC  ?/Di? was annotated as a verbal phrase (i.e., ?VP?). CKIP parser also adopts dependency decision-making and example-based approaches to label the semantic role ?Head?, showing the status of a word or a phrase as the pivotal constituent of a sentence (You and Chen, 2004). CORE adopts the head-driven principle to identify the main relation in a given sentence (Huang et al., 2000). Firstly, a relation is defined by both the ?Head?-labeled verb and the other words in the syntactic chunk headed by the verb. Secondly, the noun phrases preceding/preceded by the relational chunk are regarded as the candidates of the head?s arguments. Finally, the entity-relation triple is identified in the form of (entity1, relation, entity2). Regarding the example sentence described above, the triple (???/Edison, ???/invented, ??/light bulb) is extracted by this approach. Figure 1 shows the parsed tree of a Chinese sentence for the relation extraction by CORE. The Chinese sentence ???????????
????????? (?Democrats on the House Budget Committee released a report on Monday?) is the manual translation of one of the English sentences evaluated by ReVerb (Fader et al., 2011). The first step of CORE involves word-segmentation and POS-tagging, thus returning eight word/POS pairs: ??/Nc, ??/Na, ???/Nc, ?/DE, ???/Nb, ???/Nd, ??/VE, ?? /Na. Next, ???? /Nd ?? /VE? is identified as the verbal phrase that heads the sentence. This verbal phrase is regarded as the center of a potential relation. The two noun phrases before and after the verbal phrase, i.e., the NP ??? ?? ??? ? ???? and NP ???? are regarded as the entities that complete the relation. A potential entity-relation-entity triple (i.e., ??????????? / ????? / ??, ?Democrats on the House Budget Committee / on Monday released / a report?) is extracted accordingly. This triple is chunked from its original sentence fully automatically. Finally, a filtering process, which retains ?Head?-labeled words only, can be applied to strain out from each component of this triple the most prominent word: ???? / ?? / ??? (?Democrats / released / report?). 
 Figure 1: The parsed tree of a Chinese sentence. 4 Experiments and Evaluation We adopted the same test set released by ReVerb for performance evaluation. The test set consists of 500 English sentences randomly sampled from the Web and were annotated using a pooling method. To obtain ?gold standard? relation triples in Chinese, the 500 test sentences were manually translated from English to Chinese by a 
14
trained native Chinese speaker and verified by another. Additionally, two other native Chinese speakers annotated the relation triples for each Chinese sentence. In total, 716 Chinese entity-relation triples with an agreement score of 0.79 between the two annotators were obtained and regarded as gold standard.  Performance evaluation of CORE was conducted based on: 1) exact match; and 2) relation-only match. For exact match, each component of the extracted triple must be identical with the gold standard. For relation-only match, the extracted triple is regarded as a correct case if an extracted relation agreed with the relation of the gold standard.  Without another Chinese Open IE system for performance comparison, we compared CORE with a modification of ReVerb system capable of handling Chinese sentences. The modification of ReVerb?s verb-driven regular expression matching was kept to a minimum to deal with language-specific processing. As such, ReVerb remains mostly the same as its English counterpart so that a bilingual (Chinese/English) Open IE system can be easily implemented. Table 1 shows the experimental results. Our CORE system obviously performs better than ReVerb when recall is considered for both exact and relation-only match. The results suggest that utilizing more sophisticated NLP techniques is effective to extract relations without any specific human intervention. In addition, there is a slight decrease in the precision of exact match for CORE. This reveals that ReVerb?s original syntactic and lexical constraints are also useful to identify the arguments and their relationship precisely. In summary, CORE achieved relatively promising F1 scores. These results imply that CORE method is more suitable for Chinese open relation extraction. 
Chinese Open IE Precision Recall F1 Exact Match ReVerb 0.5820 0.0987 0.1688 CORE 0.5579 0.3291 0.4140 Relation Only ReVerb 0.8361 0.1425 0.2435 CORE 0.8463 0.5000 0.6286 Table 1: Performance evaluation on Chinese Open IE. 
We also analyzed the errors made by the CORE model. Almost all the errors resulted from incorrect parsing. Enhancing the parsing effectiveness is most likely to improve the performance of CORE. The relatively low recall rate also indicates that CORE misses many types of relation expression. Ellipsis and flexibility in Chinese syntax are so difficult not only to fail the parser, but also the extraction attempts to bypass the parsing errors. To demonstrate the applicability of CORE, we implement a Chinese Question-Answering (QA) system based on two million news articles from 2002 to 2009 published by the United Daily News Group (udn.com/NEWS). CORE extracted more than 13 million unique entity-relation triples from this corpus. These extracted relations are useful for knowledge acquisition. Take the question ????????? ? (?What is originated from China??) as an example, the relation is automatically identified as ?? ? (?originate?) that heads the following entity ??? ? (?China?). Our open QA system then searched the triples and returned the first entity as the answers. In addition to the obvious answer ???? (?Chinese medicine?), which is usually considered as common-sense knowledge, we also obtained those that are less known, such as the traditional Japanese food ???? (?natto?) and the musical instrument ????? (?accordion?). 5 Conclusions This work demonstrates the feasibility of extracting relations from Chinese corpus without the input of any predefined vocabulary to IE systems. This work is the first to explore Chinese open relation extraction to our best knowledge.  Acknowledgments 
This research was partially supported by National Science Council, Taiwan under grant NSC102-2221-E-002-103-MY3, and the ?Aim for the Top University Project? of National Taiwan Normal University, sponsored by the Ministry of Education, Taiwan. 
15
References  Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. Proceedings of EMNLP?11, pages 1535-1545. Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhao-Ming Gao, and Kuang-Yu Chen. 2000. Sinina Treebank: design criteria, annotation guidelines, and on-line interface. Proceedings of SIGHAN?00, pages 29-37. Chinese Knowledge Information Processing (CKIP) Group. 1993. Categorical analysis of Chinese. ACLCLP Technical Report # 93-05, Academia Sinica.  Fei Wu and Daniel S. Weld. 2010. Open information extraction using Wikipedia. Proceedings of ACL?10, pages 118-127. Jia-Ming You, and Keh-Jiann Chen. 2004. Automatic semantic role assignment for a tree structure.  In Proceedings of SIGHAN?04, pages 1-8. Jun Zhu, Zaiqing Nie, Xiaojiang Lium Bo Zhang, and Ji-Rong Wen. 2009. StatSnowball: a statistical approach to extracting entity relationships. In Proceedings of WWW?09, pages 101-110. Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown word extraction for Chinese documents. In Proceedings of COLING?02, pages 169-175. Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. Proceedings of IJCAI?07, pages 2670-2676. 
Michele Banko, and Oren Etzioni. 2008. The tradeoffs between open and traditional relation extraction. Proceedings of ACL?08, pages 28-26.   Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam. 2011. Open information extraction: the second generation. In Proceedings of IJCAI?11, pages 3-10. Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008. Open information extraction from the web. Communications of the ACM, 51(12):68-74. Pablo Gamallo, Marcos Garcia, and Santiago Fern?ndez-Lanza. 2012. Dependency-based open information extraction. In Proceedings of ROBUS-UNSUP?12, pages 10-18.  Elleen Riloff. 1996. Automatically constructing extraction patterns from untagged text. In Proceedings of AAAI?96, pages 1044-1049.  Stephen Soderland. 1999. Learning information extraction rules for semi-structured and free text.  Machine Learning, 34(1-3):233-272. Yu-Ming Hsieh, Ming-Hong Bai, Jason S. Chang, and Keh-Jiann Chen. 2012. Improving PCFG Chinese Parsing with Context-Dependent Probability Re-estimation. Proceedings of CLP?12, pages 216-221. Yu-Fang Tsai, and Keh-Jiann Chen. 2004. Reliable and cost-effective pos-tagging. International Journal of Computational Linguistics and Chinese Language Processing, 9(1):83-96. Yuen-Hsien Tseng, Lung-Hao Lee, and Liang-Chih Yu 2012. Traditional Chinese parsing evaluation at SIGHAN Bake-offs 2012. Proceedings of CLP?12, pages 199-205.   
16
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 446?450,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling Human Inference Process for  
Textual Entailment Recognition 
 
Hen-Hsen Huang Kai-Chun Chang Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan 
{hhhuang, kcchang}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw 
 
 
Abstract 
This paper aims at understanding what hu-
man think in textual entailment (TE) recogni-
tion process and modeling their thinking pro-
cess to deal with this problem. We first ana-
lyze a labeled RTE-5 test set and find that the 
negative entailment phenomena are very ef-
fective features for TE recognition. Then, a 
method is proposed to extract this kind of 
phenomena from text-hypothesis pairs auto-
matically. We evaluate the performance of 
using the negative entailment phenomena on 
both the English RTE-5 dataset and Chinese 
NTCIR-9 RITE dataset, and conclude the 
same findings. 
1 Introduction 
Textual Entailment (TE) is a directional relation-
ship between pairs of text expressions, text (T) 
and hypothesis (H). If human would agree that 
the meaning of H can be inferred from the mean-
ing of T, we say that T entails H (Dagan et al, 
2006). The researches on textual entailment have 
attracted much attention in recent years due to its 
potential applications (Androutsopoulos and Ma-
lakasiotis, 2010). Recognizing Textual Entail-
ment (RTE) (Bentivogli, et al, 2011), a series of 
evaluations on the developments of English TE 
recognition technologies, have been held seven 
times up to 2011. In the meanwhile, TE recogni-
tion technologies in other languages are also un-
derway (Shima, et al, 2011).   
Sammons, et al, (2010) propose an evaluation 
metric to examine the characteristics of a TE 
recognition system. They annotate text-
hypothesis pairs selected from the RTE-5 test set 
with a series of linguistic phenomena required in 
the human inference process. The RTE systems 
are evaluated by the new indicators, such as how 
many T-H pairs annotated with a particular phe-
nomenon can be correctly recognized. The indi-
cators can tell developers which systems are bet-
ter to deal with T-H pairs with the appearance of 
which phenomenon. That would give developers 
a direction to enhance their RTE systems. 
Such linguistic phenomena are thought as im-
portant in the human inference process by anno-
tators. In this paper, we use this valuable re-
source from a different aspect. We aim at know-
ing the ultimate performance of TE recognition 
systems which embody human knowledge in the 
inference process. The experiments show five 
negative entailment phenomena are strong fea-
tures for TE recognition, and this finding con-
firms the previous study of Vanderwende et al 
(2006). We propose a method to acquire the lin-
guistic phenomena automatically and use them in 
TE recognition.  
This paper is organized as follows. In Section 
2, we introduce linguistic phenomena used by 
annotators in the inference process and point out 
five significant negative entailment phenomena. 
Section 3 proposes a method to extract them 
from T-H pairs automatically, and discuss their 
effects on TE recognition. In Section 4, we ex-
tend the methodology to the BC (binary class 
subtask) dataset distributed by NTCIR-9 RITE 
task (Shima, et al, 2011) and discuss their ef-
fects on TE recognition in Chinese. Section 5 
concludes the remarks. 
2 Human Inference Process in TE 
We regard the human annotated phenomena as 
features in recognizing the binary entailment re-
lation between the given T-H pairs, i.e., EN-
TAILMENT and NO ENTAILMENT. Total 210 
T-H pairs are chosen from the RTE-5 test set by 
Sammons et al (2010), and total 39 linguistic 
phenomena divided into the 5 aspects, including 
knowledge domains, hypothesis structures, infer-
ence phenomena, negative entailment phenome-
446
na, and knowledge resources, are annotated on 
the selected dataset. 
2.1 Five aspects as features 
We train SVM classifiers to evaluate the perfor-
mances of the five aspects of phenomena as fea-
tures for TE recognition. LIBSVM RBF kernel 
(Chang and Lin, 2011) is adopted to develop 
classifiers with the parameters tuned by grid 
search. The experiments are done with 10-fold 
cross validation. 
For the dataset of Sammons et al (2010), two 
annotators are involved in labeling the above 39 
linguistic phenomena on the T-H pairs. They 
may agree or disagree in the annotation. In the 
experiments, we consider the effects of their 
agreement. Table 1 shows the results. Five as-
pects are first regarded as individual features, 
and are then merged together. Schemes ?Annota-
tor A? and ?Annotator B? mean the phenomena 
labelled by annotator A and annotator B are used 
as features respectively.  The ?A AND B? 
scheme, a strict criterion, denotes a phenomenon 
exists in a T-H pair only if both annotators agree 
with its appearance. In contrast, the ?A OR B? 
scheme, a looser criterion, denotes a phenome-
non exists in a T-H pair if at least one annotator 
marks its appearance. 
We can see that the aspect of negative entail-
ment phenomena is the most significant feature 
among the five aspects. With only 9 phenomena 
in this aspect, the SVM classifier achieves accu-
racy above 90% no matter which labeling 
schemes are adopted. Comparatively, the best 
accuracy in RTE-5 task is 73.5% (Iftene and 
Moruz, 2009). In negative entailment phenomena 
aspect, the ?A OR B? scheme achieves the best 
accuracy. In the following experiments, we adopt 
this labeling scheme. 
2.2 Negative entailment phenomena 
There is a large gap between using negative en-
tailment phenomena and using the second effec-
tive features (i.e., inference phenomena). Moreo-
ver, using the negative entailment phenomena as 
features only is even better than using all the 39 
linguistic phenomena. We further analyze which 
negative entailment phenomena are more signifi-
cant. 
There are nine linguistic phenomena in the as-
pect of negative entailment. We take each phe-
nomenon as a single feature to do the two-way 
textual entailment recognition. The ?A OR B? 
scheme is applied. Table 2 shows the experi-
mental results. 
 Annotator A Annotator B A AND B A OR B 
Knowledge  
Domains 
50.95% 52.38% 52.38% 50.95% 
Hypothesis  
Structures 
50.95% 51.90% 50.95% 51.90% 
Inference  
Phenomena 
74.29% 72.38% 72.86% 74.76% 
Negative  
Entailment  
Phenomena 
97.14% 95.71% 92.38% 97.62% 
Knowledge  
Resources 
69.05% 69.52% 67.62% 69.52% 
ALL  97.14% 92.20% 90.48% 97.14% 
Table 1: Accuracy of recognizing binary TE rela-
tion with the five aspects as features. 
 
Phenomenon ID Negative entailment  
Phenomenon  
Accuracy 
0 Named Entity mismatch 60.95% 
1 Numeric Quantity mismatch 54.76% 
2 Disconnected argument 55.24% 
3 Disconnected relation 57.62% 
4 Exclusive argument 61.90% 
5 Exclusive relation 56.67% 
6 Missing modifier 56.19% 
7 Missing argument 69.52% 
8 Missing relation 68.57% 
Table 2: Accuracy of recognizing TE relation 
with individual negative entailment phenomena. 
 
The 1st column is phenomenon ID, the 2nd col-
umn is the phenomenon, and the 3rd column is 
the accuracy of using the phenomenon in the bi-
nary classification. Comparing with the best ac-
curacy 97.62% shown in Table 1, the highest 
accuracy in Table 2 is 69.52%, when missing 
argument is adopted. It shows that each phenom-
enon is suitable for some T-H pairs, and merging 
all negative entailment phenomena together 
achieves the best performance.  
We consider all possible combinations of 
these 9 negative entailment phenomena, i.e., 
  
 +?+   
  =511 feature settings, and use each 
feature setting to do 2-way entailment relation 
recognition by LIBSVM. The notation   
  de-
notes a set of 
  
(   )   
 feature settings, each with 
n features.  
The model using all nine phenomena achieves 
the best accuracy of 97.62%. Examining the 
combination sets, we find phenomena IDs 3, 4, 5, 
7 and 8 appear quite often in the top 4 feature 
settings of each combination set. In fact, this set-
ting achieves an accuracy of 95.24%, which is 
the best performance in   
  combination set. On 
the one hand, adding more phenomena into (3, 4, 
5, 7, 8) setting does not have much performance 
difference.  
In the above experiments, we do all the anal-
yses on the corpus annotated with linguistic phe-
nomena by human. We aim at knowing the ulti-
447
mate performance of TE recognition systems 
embodying human knowledge in the inference. 
The human knowledge in the inference cannot be 
captured by TE recognition systems fully correct-
ly. In the later experiments, we explore the five 
critical features, (3, 4, 5, 7, 8), and examine how 
the performance is affected if they are extracted 
automatically. 
3 Negative Entailment Phenomena Ex-
traction 
The experimental results in Section 2.2 show that 
disconnected relation, exclusive argument, ex-
clusive relation, missing argument, and missing 
relation are significant. We follow the definitions 
of Sammons et al (2010) and show them as fol-
lows. 
(a) Disconnected Relation. The arguments and 
the relations in Hypothesis (H) are all matched 
by counterparts in Text (T). None of the argu-
ments in T is connected to the matching relation. 
(b) Exclusive Argument. There is a relation 
common to both the hypothesis and the text, but 
one argument is matched in a way that makes H 
contradict T. 
(c) Exclusive Relation. There are two or more 
arguments in the hypothesis that are also related 
in the text, but by a relation that means H contra-
dicts T. 
(d) Missing Argument. Entailment fails be-
cause an argument in the Hypothesis is not pre-
sent in the Text, either explicitly or implicitly. 
(e) Missing Relation. Entailment fails because 
a relation in the Hypothesis is not present in the 
Text, either explicitly or implicitly. 
To model the annotator?s inference process, 
we must first determine the arguments and the 
relations existing in T and H, and then align the 
arguments and relations in H to the related ones 
in T. It is easy for human to find the important 
parts in a text description in the inference process, 
but it is challenging for a machine to determine 
what words are important and what are not, and 
to detect the boundary of arguments and relations. 
Moreover, two arguments (relations) of strong 
semantic relatedness are not always literally 
identical.  
In the following, a method is proposed to ex-
tract the phenomena from T-H pairs automatical-
ly. Before extraction, the English T-H pairs are 
pre-processed by numerical character transfor-
mation, POS tagging, and dependency parsing 
with Stanford Parser (Marneffe, et al, 2006; 
Levy and Manning, 2003), and stemming with 
NLTK (Bird, 2006). 
3.1 A feature extraction method 
Given a T-H pair, we first extract 4 sets of noun 
phrases based on their POS tags, including {noun 
in H}, {named entity (nnp) in H}, {compound 
noun (cnn) in T}, and {compound noun (cnn) in 
H}.  Then, we extract 2 sets of relations, includ-
ing {relation in H} and {relation in T}, where 
each relation in the sets is in a form of Predi-
cate(Argument1, Argument2).  Some typical ex-
amples of relations are verb(subject, object) for 
verb phrases, neg(A, B) for negations, num(Noun, 
number) for numeric modifier, and tmod(C, tem-
poral argument) for temporal modifier. A predi-
cate has only 2 arguments in this representation. 
Thus, a di-transitive verb is in terms of two rela-
tions. 
Instead of measuring the relatedness of T-H 
pairs by comparing T and H on the predicate-
argument structure (Wang and Zhang, 2009), our 
method tries to find the five negative entailment 
phenomena based on the similar representation. 
Each of the five negative entailment phenomena 
is extracted as follows according to their defini-
tions. To reduce the error propagation which may 
be arisen from the parsing errors, we directly 
match those nouns and named entities appearing 
in H to the text T. Furthermore, we introduce 
WordNet to align arguments in H to T. 
(a) Disconnected Relation. If (1) for each a ? 
{noun in H}?{nnp in H}?{cnn in H}, we can 
find a ? T too, and (2) for each r1=h(a1,a2) ? 
{relation in H}, we can find a relation r2=h(a3,a4) 
? {relation in T} with the same header h, but 
with different arguments, i.e., a3?a1 and a4?a2, 
then we say the T-H pair has the ?Disconnected 
Relation?  phenomenon. 
(b) Exclusive Argument. If there exist a rela-
tion r1=h(a1,a2)?{relation in H}, and a relation 
r2=h(a3,a4)?{relation in T} where both relations 
have the same header h, but either the pair (a1,a3) 
or the pair (a2,a4) is an antonym by looking up 
WordNet, then we say the T-H pair has the ?Ex-
clusive Argument? phenomenon.   
(c) Exclusive Relation. If there exist a relation 
r1=h1(a1,a2)?{relation in T}, and a relation 
r2=h2(a1,a2)?{relation in H} where both relations 
have the same arguments, but h1 and h2 have the 
opposite meanings by consulting WordNet, then 
we say that the T-H pair has the ?Exclusive Rela-
tion? phenomenon. 
448
(d) Missing Argument. For each argument a1 
?{noun in H}?{nnp in H}?{cnn in H}, if there 
does not exist an argument a2?T such that a1=a2, 
then we say that the T-H pair has ?Missing Ar-
gument? phenomenon. 
(e) Missing Relation. For each relation 
r1=h1(a1,a2)?{relation in H}, if there does not 
exist a relation r2=h2(a3,a4)?{relation in T} such 
that h1=h2, then we say that the T-H pair has 
?Missing Relation? phenomenon. 
3.2 Experiments and discussion 
The following two datasets are used in English 
TE recognition experiments. 
(a) 210 pairs from part of RTE-5 test set. The 
210 T-H pairs are annotated with the linguistic 
phenomena by human annotators.  They are se-
lected from the 600 pairs in RTE-5 test set, in-
cluding 51% ENTAILMENT and 49% NO EN-
TAILMENT. 
(b) 600 pairs of RTE-5 test set. The original 
RTE-5 test set, including 50% ENTAILMENT 
and 50% NO ENTAILMENT.  
Table 3 shows the performances of TE recog-
nition. The ?Machine-annotated? and the ?Hu-
man-annotated? columns denote that the phe-
nomena annotated by machine and human are 
used in the evaluation respectively. Using ?Hu-
man-annotated? phenomena can be seen as the 
upper-bound of the experiments. The perfor-
mance of using machine-annotated features in 
210-pair and 600-pair datasets is 52.38% and 
59.17% respectively. 
Though the performance of using the phenom-
ena extracted automatically by machine is not 
comparable to that of using the human annotated 
ones, the accuracy achieved by using only 5 fea-
tures (59.17%) is just a little lower than the aver-
age accuracy of all runs in RTE-5 formal runs 
(60.36%) (Bentivogli, et al, 2009). It shows that 
the significant phenomena are really effective in 
dealing with entailment recognition. If we can 
improve the performance of the automatic phe-
nomena extraction, it may make a great progress 
on the textual entailment. 
 
Phenomena 210 pairs 600 pairs 
Machine- 
annotated 
Human- 
annotated 
Machine- 
annotated 
Disconnected Relation 50.95% 57.62% 54.17% 
Exclusive Argument 50.95% 61.90% 55.67% 
Exclusive Relation 50.95% 56.67% 51.33% 
Missing Argument 53.81% 69.52% 56.17% 
Missing Relation 50.95% 68.57% 52.83% 
All 52.38% 95.24% 59.17% 
Table 3: Accuracy of textual entailment recogni-
tion using the extracted phenomena as features. 
4 Negative Entailment Phenomena in 
Chinese RITE Dataset 
To make sure if negative entailment phenomena 
exist in other languages, we apply the methodol-
ogies in Sections 2 and 3 to the RITE dataset in 
NTCIR-9. We annotate all the 9 negative entail-
ment phenomena on Chinese T-H pairs according 
to the definitions by Sammons et al (2010) and 
analyze the effects of various combinations of 
the phenomena on the new annotated Chinese 
data. The accuracy of using all the 9 phenomena 
as features (i.e.,   
  setting) is 91.11%. It shows 
the same tendency as the analyses on English 
data. The significant negative entailment phe-
nomena on Chinese data, i.e., (3, 4, 5, 7, 8), are 
also identical to those on English data. The mod-
el using only 5 phenomena achieves an accuracy 
of 90.78%, which is very close to the perfor-
mance using all phenomena.  
We also classify the entailment relation using 
the phenomena extracted automatically by the 
similar method shown in Section 3.1, and get a 
similar result. The accuracy achieved by using 
the five automatically extracted phenomena as 
features is 57.11%, and the average accuracy of 
all runs in NTCIR-9 RITE task is 59.36% (Shima, 
et al, 2011). Compared to the other methods us-
ing a lot of features, only a small number of bi-
nary features are used in our method. Those ob-
servations establish what we can call a useful 
baseline for TE recognition. 
5 Conclusion 
In this paper we conclude that the negative en-
tailment phenomena have a great effect in deal-
ing with TE recognition. Systems with human 
annotated knowledge achieve very good perfor-
mance. Experimental results show that not only 
can it be applied to the English TE problem, but 
also has the similar effect on the Chinese TE 
recognition. Though the automatic extraction of 
the negative entailment phenomena still needs a 
lot of efforts, it gives us a new direction to deal 
with the TE problem.  
The fundamental issues such as determining 
the boundary of the arguments and the relations, 
finding the implicit arguments and relations, ver-
ifying the antonyms of arguments and relations, 
and determining their alignments need to be fur-
ther examined to extract correct negative entail-
ment phenomena. Besides, learning-based ap-
proaches to extract phenomena and multi-class 
TE recognition will be explored in the future.  
449
Acknowledgments 
 
This research was partially supported by Excel-
lent Research Projects of National Taiwan Uni-
versity under contract 102R890858 and 2012 
Google Research Award.  
References 
Ion Androutsopoulos and Prodromos Malakasiotis. 
2010. A Survey of Paraphrasing and Textual En-
tailment Methods. Journal of Artificial Intelligence 
Research, 38:135-187.  
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang 
Dang, and Danilo Giampiccolo. 2011. The seventh 
PASCAL recognizing textual entailment challenge. 
In Proceedings of the 2011 Text Analysis 
Conference (TAC 2011), Gaithersburg, Maryland, 
USA.. 
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, 
Danilo Giampiccolo, and Bernardo Magnini. 2009. 
The fifth PASCAL recognizing textual entailment 
challenge. In Proceedings of the 2009 Text 
Analysis Conference (TAC 2009), Gaithersburg, 
Maryland, USA. 
Steven Bird. 2006. NLTK: the natural language 
toolkit. In Proceedings of the 21st International 
Conference on Computational Linguistics and 44th 
Annual Meeting of the Association for Computa-
tional Linguistics (COLING-ACL 2006), pages 69-
72. 
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: 
a Library for Support Vector Machines. ACM 
Transactions on Intelligent Systems and Technolo-
gy, 2:27:1-27:27. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Ido Dagan, Oren Glickman, and Bernardo Magnini. 
2006. The PASCAL Recognising Textual Entail-
ment Challenge.  Lecture Notes in Computer Sci-
ence, 3944:177-190. 
Adrian Iftene and Mihai Alex Moruz. 2009. UAIC 
Participation at RTE5. In Proceedings of the 2009 
Text Analysis Conference (TAC 2009), 
Gaithersburg, Maryland, USA. 
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank? 
In Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics (ACL 
2003), pages 439-446. 
Marie-Catherine de Marneffe, Bill MacCartney, and 
Christopher D. Manning. 2006. Generating typed 
dependency parses from phrase structure parses. In 
The Fifth International Conference on Language 
Resources and Evaluation (LREC 2006), pages 
449-454. 
Mark Sammons, V.G.Vinod Vydiswaran, and Dan 
Roth. 2010. Ask not what textual entailment can do 
for you... In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2010), pages 1199-1208, Uppsala, Swe-
den. 
Hideki Shima, Hiroshi Kanayama, Cheng-Wei Lee, 
Chuan-Jie Lin, Teruko Mitamura, Yusuke Miyao,  
Shuming Shi, and Koichi Takeda. 2011. Overview 
of NTCIR-9 RITE: Recognizing inference in text. 
In Proceedings of the NTCIR-9 Workshop Meeting, 
Tokyo, Japan. 
Lucy Vanderwende, Arul Menezes, and Rion Snow. 
2006. Microsoft Research at RTE-2: Syntactic 
Contributions in the Entailment Task: an imple-
mentation. In Proceedings of the Second PASCAL 
Challenges Workshop. 
Rui Wang and Yi Zhang. 2009. Recognizing Textual 
Relatedness with Predicate-Argument Structures. 
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pag-
es 784?792, Singapore.  
 
450
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 103?108,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
FAdR: A System for Recognizing False Online Advertisements 
  Yi-jie Tang and Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University, Taipei, Taiwan tangyj@nlg.csie.ntu.edu.tw;hhchen@ntu.edu.tw     Abstract 
More and more product information, in-cluding advertisements and user reviews, are presented to Internet users nowadays. Some of the information is false, mislead-ing or overstated, which can cause seri-ousness and needs to be identified. Au-thorities, advertisers, website owners and consumers all have the needs to detect such statements. In this paper, we propose a False Advertisements Recognition sys-tem called FAdR by using one-class and binary classification models. Illegal adver-tising lists made public by a government and product descriptions from a shopping website are obtained for training and test-ing. The results show that the binary SVM models can achieve the highest perfor-mance when unigrams with the weighting of log relative frequency ratios are used as features. Comparatively, the benefit of the one-class classification models is the ad-justable rejection rate parameter, which can be changed to suit different applica-tions. Verb phrases more likely to intro-duce overstated information are obtained by mining the datasets. These phrases help find problematic wordings in the advertis-ing texts. 1 Introduction As online commerce and advertising keep grow-ing, more and more consumers depend on infor-mation on the Internet to make purchasing deci-sions. This kind of information includes online advertisements posted by businesses, and discus-sions or reviews generated by users. However, false statements can also be presented to con-sumers. For example, some companies hire peo-ple to post fake product reviews in an attempt to 
promote their own products or reduce competi-tors? reputations (Ott et al., 2011). It is referred to as deceptive opinion spamming and explored in recent researches (Ott et al., 2011; Mukherjee et al., 2012; Mukherjee et al., 2013; Fei et al., 2013). False statements and exaggerated content can also be seen in online advertisements. These statements can also be regarded as opinion spams, while the authors, that is, the advertisers, can be more easily identified. Yeh (2014) report-ed the top two types of illegal advertisements on the web, TV and broadcast are food (62.61%) and cosmetic (24.26%). Of the dissemination media, the web is the major source of false ad-vertisements. Most inappropriate food-related advertisements contain overstated health claims. The medical effects and cure claims may also appear in cosmetic advertising. As a result, ad-vertising regulations are enforced in many coun-tries to protect consumers from fraudulent and misleading information. False, overstated or mis-leading information and mentions of curative effects can be prohibited by the authorities (FTC, 2000; DOH, 2009; CFIA, 2010). To regulate online advertising, the authorities need to review a large number of advertisements and determine their legality, which is cost- and time-consuming. Advertisers also need to know the legality of their advertisements to avoid vio-lating advertising laws. This becomes especially important when every Internet user can be an advertiser if s/he posts messages related to any product announcement, promotion, or sales. Website owners that accept advertisements have to present appropriate advertisement contents to users and avoid legal issues. Even Internet users should also identify false advertisements in order not to be misled. Thus, the recognition of false, misleading or overstated information is an emerging task.  This paper presents a False Advertisements Recognition system called FAdR, and take two 
103
major sources of illegal advertisements on the web, i.e., food and cosmetic advertising, as ex-amples. Section 2 surveys the related work. Sec-tion 3 introduces the datasets used in the experi-ments. Section 4 presents classification models and shows their performance. Section 5 mines the overstated phrases. Section 6 demonstrates the uses of FAdR system with screenshot. Both sentence and document levels are considered. 2 Related Work Gokhman et al. (2012) collected data from the Internet and explored methods to construct a gold standard corpus for ?deception? studies. Ott et al. (2011) studied methods to detect ?disrup-tive opinion spams.? Unlike conventional adver-tising spams, these fake opinions look authentic and are used to mislead users. Mukherjee et al. (2013) used reviewer?s behavioral footprints to detect spammer. As they pointed out, one of the largest problems to solve this issue is that there is no appropriate datasets for fake and non-fake reviews. Previous online advertising research mostly focuses on bidding, matching or recommendation of advertisements on websites. Ghosh et al. (2009) studied bidding strategies for advertise-ment allocations. Huang et al. (2008) proposed an advertisement recommendation method by classifying instant messages into the Yahoo cate-gories. Scaiano and Inkpen (2011) used Wikipe-dia for negative keyphrase generation to hide advertisements that users are not interested in. This paper, in contrast, focuses on identifying false statements in online advertisements with classification models. 3 Datasets We use the illegal advertising lists and state-ments made public by the Taipei City Govern-ment1 as the illegal advertising datasets. The con-tents of the government data are split into sen-tences by colon, period, question mark and ex-clamation mark. Two types of datasets are built for illegal food and cosmetic advertising, named FOOD_ILLEGAL and COS_ILLEGAL, respec-tively. Some illegal sentences in the illegal food advertising dataset are shown below: (1)  ?????????? Reduces waste produced by metabolism process. (2)  ????????                                                 1 http://www.health.gov.tw/Default.aspx?tabid=295 
Stops insomnia and pain. (3)  ?????? Cures hypertension. In the government website, the authority does not regularly announce legal advertising data. We adopt one-class classifiers with only illegal data for this scenario, as shown in Section 4.1. To experiment on binary classifiers, we collect product descriptions from a shopping website2 and verify their legality manually to construct the legal advertising datasets. The legal food and cosmetic adverting datasets are named FOOD_LEGAL and COS_LEGAL, respectively. The numbers of the sentences in FOOD_LEGAL, FOOD_ILLEGAL, COS_LEGAL, and COS_ILLEGAL are 5,059, 7,033, 10,520, and 11,381, respectively. 4 Classification Models One-class Na?ve Bayes and Bagging classifiers, and binary classifiers based on Na?ve Bayes and SVM models are implemented.  4.1 One-Class Classifiers  We adopt the OneClassClassifier module (Hempstalk et al., 2008) in the WEKA machine learning tool to train one-class classifiers with illegal statements only. The OneClassClassifier module provides a rejection rate parameter for adjusting the threshold between target and non-target instances.  The target class, which corre-sponds to the illegal class in this study, is the single class used to train the classifier.  Higher rejection rate means that more legal statements will be preferred, but illegal statements may be still incorrectly classified into legal ones. Na?ve Bayes and Bagging classifiers are chosen be-cause they achieve best performance among the algorithms we have explored in this experiment. Each instance in the dataset, i.e., a sentence, is represented by a word vector (w1, w2, ?, w1000), where wi is a binary value indicating whether a word occurs in the sentence or not. The vocabu-lary is selected from the illegal advertising da-tasets. To properly filter out common words, we count top 1,000 frequent words in the Sinica Balanced Corpus of Modern Chinese3 and re-move them from the vocabulary. The remaining top 1,000 words are used for vector representation. Total 532 illegal statements provided by the Department of Health form the training set. An                                                 2 http://www.7net.com.tw 3 http://app.sinica.edu.tw/kiwi/mkiwi/ 
104
illegal and a legal advertising dataset make up the test set. The former consists of 317 illegal sentences from Taipei City Government?s lists, and the latter contains 203 legal statement exam-ples from the Department of Health. Table 1 shows the accuracies of Na?ve Bayes and Bagging classifiers in the food dataset. The rejection rates from 0.7 to 0.8 are preferable for most applications, because they result in higher accuracy for legal statement classification while not significantly reducing the performance of illegal statement detection.  Using the 0.7 rejec-tion rate produces high performance for the ille-gal class while 0.8 rejection rate does better for the legal class.  The actual choice of rejection rate depends on the demands of users.  For an advertiser, it is important to avoid all possible problematic statements.  Thus, a lower rejection rate will be more suitable.  If the system is used by the authorities, a rejection rate higher than 0.7 may be preferable because they don?t misjudge too many legal advertisements.  Rejection rate 0.4 0.5 0.6 0.7 0.8 0.9 Na?ve Bayes Illegal 85.33% 82.39% 79.01% 74.49% 68.17% 59.14% Legal 31.07% 39.81% 53.40% 63.11% 72.82% 86.41% 
Bagging Illegal 92.78% 88.49% 84.65% 74.94% 69.07% 0.23% Legal 3.88% 17.48% 27.18% 65.72% 82.52% 99.77% Table 1: Accuracies of Classifiers in Different Rejec-tion Rates. 4.2 Binary Classifiers  We use FOOD_LEGAL and FOOD_ILLEGAL datasets, and COS_LEGAL and COS_ILLEGAL datasets to build binary classifiers for food and cosmetic advertising classification, respectively. Na?ve Bayes classifiers and SVM classifiers im-plemented with libSVM (Chang & Lin, 2011) are adopted. Ten-fold cross validation is used for the training and testing tasks. Total 1,000 highly fre-quent words are selected in the same way as in Section 4.1 to form a word-based unigram fea-ture set.  Two weighting schemes are considered. In the binary weighting, each sentence is represented by a word vector (w1, w2, ?, w1000), where wi is a binary value indicating whether a word occurs in the sentence or not.  In the weighting of log rela-tive frequency ratio, we follow the idea of collo-cation mining (Damerau, 1993). Relative fre-quency ratio between two datasets has been shown to be useful to discover collocations that are characteristic of a dataset when compared to the other dataset. It has been successfully applied to mine sentiment words from microblog and to 
model reader/writer emotion transition (Tang and Chen, 2011, 2012). The log relative frequency ratio (logRF) is defined formally as follows. Given two datasets A and B, the log relative frequency ratio for each wi?A?B is computed with the following formula. logRFAB (wi ) = log fA (wi )| A |fB (wi )| B |  logRFAB(wi) is a log ratio of relative frequen-cies of word wi in A and B, fA(wi) and fB(wi) are frequencies of wi in A and in B, respectively, and |A| and |B| are total words in A and in B, respec-tively. logRF values are used to estimate the dis-tribution of the words in datasets A and B. If wi has higher relative frequency in A than in B, then logRFAB(wi)>0, and vice versa. In our experi-ments, logRF is used to present each unigram?s distribution in the legal and illegal datasets, re-placing the binary value for a unigram feature. Tables 2 and 3 show the results of the classifi-cation models with different combinations of feature sets. When logRF is combined with Uni-gram, the accuracy is significantly improved in both the food and cosmetic datasets. We can also see that the performance of all FOOD models are higher than equivalent COS models. Possible reasons may be that the effects of cosmetics are related to body appearance, and inappropriate cure claims are also related to body improvement and appearance changes. There can be some overlaps between the words used in legal and illegal cosmetic advertising.   Classification Mod-els ? Na?ve Bayes SVM Illegal vs. Legal ? Features ? Illegal Legal Illegal Legal Unigram 92.59% 85.06% 89.46% 88.00% Unigram + logRF 94.32% 86.37% 94.70% 91.68% Table 2: Classification Accuracies for FOOD Datasets.  Classification Mod-els ? Na?ve Bayes SVM Illegal vs. Legal ? Features ? Illegal Legal Illegal Legal Unigram 86.48% 77.63% 82.47% 82.36% Unigram + logRF 88.20% 83.06% 88.46% 83.41% Table 3: Classification Accuracies for COS Datasets. 5 Overstated Phrase Mining Since the authority focuses on health claims in advertising, almost all illegal statements an-nounced by the government include an action related to health improvement and a name that refers to diseases or body conditions. Thus, we can observe that most of the illegal statements 
105
recognized and forbidden by the authority con-tain a health-related verb phrase consisting of a transitive verb and an object. These illegal adver-tising verb phrases can be mined from the da-tasets for the government?s and advertisers? ref-erence. We can also use these verb phrases to help the users of our system understand possible reasons why the sentences in advertisements are labeled as illegal. We propose a mining method based on log relative frequency ratio, which is described in Section 4.2. We compute logRFAB(wi) to obtain the words that are most likely to be used in ille-gal advertising. We identify transitive verbs and nouns in the word list based on POS tagging re-sults generated by the CKIP parser4, and then use them to examine if a verb phrase is presented in a sentence. Total 979 verb phrases are mined from the FOOD datasets, and 2,302 from the COS da-taset. Table 4 shows some examples.  Dataset Illegal advertising verb phrases Transitive verb Object noun 
FOOD 
?? (improve) ?? (physical condition) ?? (inactivate) ?? (bacteria) ?? (decompose) ??? (cholesterol) 
COS 
?? (purify) ?? (body) ?? (ease) ?? (pain) ?? (cure) ?? (acne vulgaris) Table 4: Example illegal verb phrases mined from the FOOD and COS datasets. 6 System Architecture The FAdR system is composed of pre-processing (Pre-Processor), recognition (Recog-nizer), and explanation (Explainer) modules. Figure 1 shows the overall system architecture. 6.1 Pre-processing Module Our classification models are sentence-based, so the main purpose of the Pre-processor in the sys-tem is detecting sentence boundaries. Four types of punctuations, including period, colon, excla-mation, and question mark, are used to segment a document into sentences. Line breaks are also regarded as a sentence boundary marker because                                                 4 http://ckipsvr.iis.sinica.edu.tw 
many advertisements in Chinese put sentences in separate lines and do not include any punctua-tion. Sentences with less than three characters or more than 80 characters are ignored. Word segmentation is performed by using the CKIP segmenter, which is an online service and can be accessed through the TCP socket. Seg-mented data will be represented by the corre-sponding feature sets based on classification model and converted to a format that the Recog-nizer can read as input.   
Recognizer Classification Models
Advertising Document
Sentence 
Segmenter
Word
Segmenter
Format 
Converter Feature Sets
Explainer
Advertising document
with sentence-based
legality labels and
explanations.
Pre-Processor
  Figure 1. System architecture of FAdR 6.2 Recognition Module All processed sentences are sent from the Pre-Processor to the Recognizer for legality identifi-cation.  Since our training tasks are done in WEKA, we can use the model files generated by WEKA for implementing the Recognizer. The Recogniz-er loads the pre-trained SVM models for food and cosmetic advertising classification, and then uses them for labeling the incoming sentences. For the One-Class models, the model files are pre-generated by training with different rejection rates from 0.4 to 0.9. When the user adjusts the threshold, the Recognizer chooses the corre-sponding model to perform illegal sentences identification. 
106
6.3 Explanation Module To give users more information on the possible reasons why the advertising contents are consid-ered illegal, the Explainer uses the illegal verb phrase list, which is discussed in Section 5, to extract the problematic words from the input sen-tences. If the verb and the object noun in a verb phrase from the list both occur in an illegal sen-tence, then the verb phrase will be shown besides the recognition results in the user interface. 6.4 User Interface Users can copy and paste the advertising con-tents to be recognized to the text field, or upload a document to the system. It usually takes less than 10 seconds on our server to process a doc-ument with 200 characters, so the system is suit-able to quickly process a large amount of data. If the users choose to use the one-class mod-els, they can adjust the threshold value to fit dif-ferent needs and receive useful results. Lowering the value can find as many problematic sentences as possible, but more legal sentences can also be misjudged.  Increasing the value can avoid wrongly labeling legal sentences as illegal, but more illegal sentences can be missed.  Figure 2 shows a system screenshot. The recognition results of a food advertisement with 11 sentences are demonstrated. Sentences la-belled as illegal are highlighted in red. Verb phrases possibly causing illegality are listed in grey colour for illegal sentences. The number of all sentences, the number of illegal sentences, and the final score are shown at the bottom. The correct score of an advertisement is defined as the number of correct sentences divided by total sentences in this advertisement. The sample ad-vertisement used in Figure 2 and its English translation are shown as follows.  <A food advertisement> ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????    (The leading brand for Japanese tea. The first tea product combining three kinds of natural colour-ings in Taiwan. Can improve immunity. Can re-lieve stress. Can strengthen resistance to disease.  Can increase antibodies in your body. It is mild and not irritative. Good for daily use. Can pre-vent body cells from being harmed by free radi-
cals. Can strengthen immunity. It is healthy and tasty, and brings no body burden.)  
  Figure 2: Screenshot for Illegal Sentence Recognition 7 Conclusion Detecting false information on the Internet has become an important issue for users and organi-zations. In this paper, we present two types of classification methods to identify overstated sen-tences in online advertisements and build a false online advertisements recognition system FAdR. The recognition on both document and sentence levels is addressed in the demonstration. In the binary models, using combinations of unigrams and the log relative frequency ratio as features can achieve highest performance. On the other hand, the one-class models can be used to build a system that is adjustable by users for dif-ferent application domains. The authorities or website owners can use a rejection rate of 0.7 or 0.8 to highlight most seri-ous illegal advertisements. An advertisement 
107
with a score lower than 0.5 means it may critical-ly violate the regulations, and need to be regard-ed as illegal advertising. Since not all advertise-ment posters are professional advertisers, they may need detailed information on the legality of every sentence. The illegal verb phrases found in a sentence provide clues to the advertiser. The system is also useful for consumers, as they can check if the advertisement contents can be trust-ed before making a purchase decision. As future work, we will extend the methodol-ogy presented in this study to handle other types of advertisements and the materials in other lan-guages.  We will also investigate what linguistic patterns can be used to mine the overstated phrases in different languages. Acknowledgments This research was partially supported by Nation-al Taiwan University and Ministry of Science and Technology, Taiwan under 103R890858 and 102-2221-E-002-103-MY3. References  Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: a Library for Support Vector Machines. Available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. CFIA. 2010. Advertising Requirements. Canadian Food Inspection Agency. Available at http://www.inspection.gc.ca/english/fssa/labeti/advpube.shtml. Fred J. Damerau. 1993. Generating and Evaluating Domain-Oriented Multi-Word Terms from Text. Information Processing and Management, 29:433-477. DOH. 2009. Legal and Illegal Advertising Statements for Cosmetic Regulations. Department of Health of Taiwan. Available at http://www.doh.gov.tw/ufile/doc/0980305527.pdf. Geli Fei, Arjun Mukherjee, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. 2013. Exploiting Burstiness in Reviews for Review Spammer Detection. In Proceedings of the Interna-tional AAAI Conference on Weblogs and Social Media (ICWSM-2013), 175-184. FTC. 2000. Advertising and Marketing on the Inter-net: Rules of the Road, Bureau of Consumer Pro-tection. Federal Trade Commission, September 2000. Available at http://business.ftc.gov/sites/default/files/pdf/bus28-advertising-and-marketing-internet-rules-road.pdf. Stephanie Gokhman, Jeff Hancock, Poornima Prabhu, Myle Ott, and Claire Cardie. 2012. In Search of a 
Gold Standard in Studies of Deception. In Pro-ceedings of the EACL 2012 Workshop on Compu-tational Approaches to Deception Detection, 23?30. Arpita Ghosh, Preston McAfee, Kishore Papineni, and Sergei Vassilvitskii. 2009. Bidding for Representa-tive Allocations for Display Advertising. CoRR, abs/0910-0880, 2009. Hung-Chi Huang, Ming-Shun Lin and Hsin-Hsi Chen. 2008. Analysis of Intention in Dialogues Using Category Trees and Its Application to Advertise-ment Recommendation. In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP 2008), 625-630. Kathryn Hempstalk, Eibe Frank, and Ian H. Witten. 2008. One-Class Classification by Combining Density and Class Probability Estimation. In Pro-ceedings of the 12th European Conference on Principles and Practice of Knowledge Discovery in Databases and 19th European Conference on Ma-chine Learning, 505-519. Arjun Mukherjee, Bing Liu, and Natalie Glance. 2012. Spotting Fake Reviewer Groups in Consum-er Reviews. In Proceedings of the International World Wide Web Conference (WWW 2012), 191-200. Arjun Mukherjee, Abhinav Kumar, Bing Liu, Junhui Wang, Meichun Hsu, Malu Castellanos, and Rid-dhiman Ghosh. 2013. Spotting Opinion Spammers using Behavioral Footprints. In Proceedings of SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2013), 632-640. Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Hancock. 2011. Finding Deceptive Opinion Spam by Any Stretch of the Imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, 309?319.  M. Scaiano and D. Inkpen. 2011. Finding Negative Key Phrases for Internet Advertising Campaigns Using Wikipedia. In Recent Advances in Natural Language Processing (RANLP 2011), 648?653. Yi-jie Tang and Hsin-Hsi Chen. 2011. Emotion Mod-eling from Writer/Reader Perspectives Using a Mi-croblog Dataset. In Proceedings of IJCNLP Work-shop on Sentiment Analysis where AI Meets Psy-chology, 11-19.  Yi-jie Tang and Hsin-Hsi Chen. 2012. Mining Senti-ment Words from Microblogs for Predicting Writ-er-Reader Emotion Transition. In Proceedings of the 8th International Conference on Language Re-sources and Evaluation (LREC 2012), 1226-1229. Ming-kung Yeh. 2014. Weekly Food and Drug Safety. No. 440, February, Food and Drug Administration, Taiwan. Available at http://www.fda.gov.tw/TC/PublishOther.aspx. 
108
Classical Chinese Sentence Segmentation 
Hen-Hsen Huang?, Chuen-Tsai Sun? and Hsin-Hsi Chen? 
?Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan 
?Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan 
hhhuang@nlg.csie.ntu.edu.tw ctsun@cis.nctu.edu.tw hhchen@csie.ntu.edu.tw 
Abstract 
Sentence segmentation is a fundamental 
issue in Classical Chinese language 
processing. To facilitate reading and 
processing of the raw Classical Chinese 
data, we propose a statistical method to 
split unstructured Classical Chinese text 
into smaller pieces such as sentences and 
clauses. The segmenter based on the 
conditional random field (CRF) model is 
tested under different tagging schemes 
and various features including n-gram, 
jump, word class, and phonetic informa-
tion. We evaluated our method on four 
datasets from several eras (i.e., from the 
5th century BCE to the 19th century). 
Our CRF segmenter achieves an F-score 
of 83.34% and can be applied on a varie-
ty of data from different eras. 
1 Introduction 
Chinese word segmentation is a well-known and 
widely studied problem in Chinese language 
processing. In Classical Chinese processing, sen-
tence segmentation is an even more vexing issue. 
Unlike English and other western languages, 
there is no delimiter marking the end of the word 
in Chinese. Moreover, not only is there a lack of 
delimiters between the words, almost all pre-
20th century Chinese is written without any 
punctuation marks. Figure 1 shows photocopies 
of printed and hand written documents from the 
19th century. Within any given paragraph, the 
Chinese characters are printed as evenly spaced 
characters, with nothing to separate words from 
words, phrases from phrases, and sentences from 
sentences. Thus, inside a paragraph, explicit 
boundaries of sentences and clauses are lacking. 
In order to understand the structure, readers of 
Classical Chinese have to manually identify 
these boundaries during the reading. This 
process is called Classical Chinese sentence 
segmentation, or Judo (??). 
For example, the opening lines of the Daoist 
classic Zhuangzi originally lacked segmentation: 
 
?/north ?/ocean ?/have?/fish ?/it ?/name 
?/is ?/Kun (a kind of big fish) ?/Kun ?/of 
?/big ?/not ?/know ?/how ?/thousand ?
/mile  ?/exclamation 
 
The meaning of the text is hard to interpret 
without segmentation. Below is the identical text 
as segmented by a human being. It is clearly 
more readable.  
 
????/in the north ocean there is a fish 
???? /its name is Kun 
???/the size of the Kun 
??????/I don?t know how many  
thousand miles the fish is 
 
However, sentence segmentation in Classical 
Chinese is not a trivial problem. Classical Chi-
nese sentence segmentation, like Chinese word 
segmentation, is inherently ambiguous. Individ-
uals generally perform sentence segmentation in 
instinctive ways. To identify the boundaries of 
sentences and clauses, they primarily rely on 
their experience and sense of the language rather 
than on a systematic procedure. It is thus diffi-
cult to construct a set of rules or practical proce-
dures to specify the segmentation of the infinite 
variety of Classical Chinese sentences. 
 
Figure 1. A Printed Page (Left) and a Hand Written Manuscript (Right) from the 19th Century. 
 
Because of the importance of sentence seg-
mentation, beginning in the 20th century, some 
editions of the Chinese classics have been labor-
intensively segmented and marked with modern 
punctuation. However, innumerable documents 
in Classical Chinese from the centuries of Chi-
nese history remain to be segmented. To aid in 
processing these documents, we propose an au-
tomated Classical Chinese sentence segmenta-
tion approach that enables completion of seg-
mentation tasks quickly and accurately. To con-
struct the sentence segmenter for Classical Chi-
nese, the popular sequence tagging models, con-
ditional random field (CRF) (Lafferty et al, 
2001), are adopted in this study. 
The rest of this paper is organized as follows. 
First, we describe the Classical Chinese sentence 
segmentation problem in Section 2. In Section 3, 
we review the relevant literature, including sen-
tence boundary detection (SBD) and Chinese 
word segmentation. In Section 4, we introduce 
the tagging schemes along with the features, and 
show how the sentence segmentation problem 
can be transformed into a sequence tagging 
problem and decoded with CRFs. In Section 5, 
the experimental setup and data are described. In 
Section 6, we report the experimental results and 
discuss the properties and the challenges of the 
Classical Chinese sentence segmentation prob-
lem. Finally, we conclude the remarks in Section 
7. 
2 Problem Description 
The outcomes of Classical Chinese sentence 
 
segmentation are not well-defined in linguistics 
at present. In general, the results of segmentation 
consist of sentences, clauses, and phrases. For 
instance, in the segmented sentence ???? / 
??? / ?????????, ????? (?the 
mists on the mountains like wild horses?) and 
????? (?the dust in the air?) are phrases, and ?
????????? (?the living creatures blow 
their breaths at each other?) is a clause. A 
sentence such as ?????????? (?I do 
not believe it because it is ridiculous.?) is a short 
sentence itself, and does not require any 
segmentation. For a given text, there is no strict 
rule to determine at which level the 
segmentation should be performed. For instance, 
the opening lines of the Daoist classic Daodejing 
is ?????????????? (?The way 
that can be spoken is not the eternal way. The 
name that can be given is not the eternal name.?) 
which is usually segmented as ???? / ??? 
/ ??? / ????, but may also be segmented 
as ?? / ?? / ??? / ? / ?? / ????. 
Either segmentation is reasonable. 
In this paper, we do not distinguish among the 
three levels of segmentation. Instead, our system 
learns directly from the human-segmented cor-
pus. After training, our system will be adapted to 
perform human-like segmentation automatically. 
Further, we do not distinguish the various out-
comes of Classical Chinese sentence segmenta-
tion. Instead, for the sake of convenience, every 
product of the segmentation process is termed 
?clause? in the following sections. 
  
 
3 Related Work 
Besides Classical Chinese, sentence boundary 
detection (SBD) is also an issue in English and 
other western languages. SBD in written texts 
and speech represents quite different problems. 
For written text, the SBD task is to distinguish 
periods used as the end-of-sentence indicator 
(full stop) from other usages, such as parts of 
abbreviations and decimal points. By contrast, 
the task of SBD in speech is closely related to 
the task of Classical Chinese sentence segmenta-
tion. In speech processing, the outcome of 
speech recognizers is a sequence of words, in 
which the punctuation marks are absence, and 
the sentence boundaries are thus lacking. To re-
cover the syntactic structure of the original 
speech, SBD is required. 
Like Classical Chinese sentence segmentation, 
the task of SBD in speech is to determine which 
of the inter-word boundaries in the stream of 
words should be marked as end-of-sentence, and 
then to divide the entire word sequence into in-
dividual sentences. Empirical methods are com-
monly employed to deal with this problem. Such 
methods involve many different sequence labe-
ling models including HMMs (Shriberg et al, 
2000), maximum entropy (Maxent) models (Liu 
et al, 2004), and CRFs (Liu et al, 2005). 
Among these, a CRF model used in Liu et al
(2005) offered the lowest error rate.  
Chinese word segmentation is a problem 
closely related to Classical Chinese sentence 
segmentation. The former identifies the bounda-
ries of the words in a given text, while the latter 
identifies the boundaries of the sentences, claus-
es, and phrases. In contrast to sentences and 
clauses, the length of Chinese words is shorter, 
and the variety of Chinese words is more limited. 
Despite the minor unknown words, most of the 
frequent words can be handled with a dictionary 
predefined by Chinese language experts or ex-
tracted from the corpus automatically. However, 
it is impossible to maintain a dictionary of the 
infinite number of sentences and clauses. For 
these reasons, the Classical Chinese sentence 
segmentation problem is more challenging. 
Methods of Chinese word segmentation can 
be mainly classified into heuristic rule-based 
approaches, statistical machine learning ap-
proaches, and hybrid approaches. Hybrid ap-
proaches combine the advantages of heuristic 
and statistical approaches to achieve better re-
sults (Gao et al, 2003; Xue, 2003; Peng et al, 
2004). 
Xue (2003) transformed the Chinese word 
segmentation problem into a tagging problem. 
For a given sequence of Chinese characters, the 
author applies a Maxent tagger to assign each 
character one of four positions-of-character 
(POC) tags, and then coverts the tagged se-
quence into a segmented sequence. The four 
POC tags used in Xue (2003) denote the posi-
tions of characters within a word. For example, 
the first character of a word is tagged ?left 
boundary?, the last character of a word is tagged 
?right boundary?, the middle character of a word 
is tagged ?middle?, and a single character that 
forms a word by itself is tagged ?single-
character-word?. Once the given sequence is 
tagged, the boundaries of words are also re-
vealed, and the task of segmentation becomes 
straightforward. However, the Maxent models 
used in Xue (2003) suffer from an inherent the 
label bias problem. Peng et al(2004) uses the 
CRFs to address this issue. The tags used in 
Peng et al(2004) are of only two types, ?start? 
and ?non-start?, in which the ?start? tag denotes 
the first character of a word, and the characters 
in other positions are given the ?non-start? tag. 
The closest previous works to Classic Chinese 
sentence segmentation are Huang (2008) and 
Zhang et al (2009). Huang combined the Xue?s 
tagging scheme (i.e., 4-tag set) and CRFs to ad-
dress the Classical Chinese sentence segmenta-
tion problem and reported an F-score of 80.96% 
averaged over various datasets. A similar work 
by Zhang et al reported an F-score of 71.42%.  
4 Methods 
Conditional random field is our tagging model, 
and the implementation is CrfSgd 1.31 provided 
by L?on Bottou. As denoted by the tool name, 
the parameters in this implementation are opti-
mized using Stochastic Gradient Descent (SGD) 
which convergences much faster than the com-
mon optimization algorithms such as L-BFGS 
and conjugate gradient (Vishwanathan, et al, 
2006). To construct the sentence segmenter on 
                                                 
1 http://leon.bottou.org/projects/sgd 
CRF, the tagging scheme and the feature func-
tions play the crucial roles.  
4.1 Tagging Schemes 
In the previous works (Huang, 2008; Zhang et 
al., 2009), POC tags used in Chinese word seg-
mentation (Xue, 2003) are converted to denote 
the positions of characters within a clause. The 
4-tag set is redefined as L (?the left boundary of 
a clause?), R (?the right boundary of a clause?), 
M (?the middle character of a clause?), and S (?a 
single character forming a clause?). For example, 
the sentence ???????????????
???? should be tagged as follows. 
 
?/L ?/M ?/M ?/R ?/L ?/M ?/M ?/R ?
/L ?/M ?/R ?/L ?/M ?/M ?/M ?/M ?/R 
 
We can easily split the sentence into clauses by 
making a break after each character tagged R 
and S and obtain the final outcome ????? / 
???? / ??? / ???????. 
In this work, more tagging schemes are expe-
rimented. The basic tagging scheme for segmen-
tation is 2-tag set in which only two types of tags, 
?start? and ?non-start?, are used to label the se-
quence. The segmented fragments (clauses) for 
sentence segmentation are usually much longer 
than those for word segmentation. Thus, we add 
more middle states into the 4-tag set to model 
the nature of long fragments. The Markov chain 
of our tagging scheme is shown in Figure 2, 
where L2, L3, ?, Lk are the additional states to 
extend Xue?s 4-tag set. In our experiments, vari-
ous k values are tested. If the k value is 1, the 
scheme is identical to the one used in the two 
previous works (Zhang et al, 2009; Huang, 
2008). The 2-tag set, 4-tag set, 5-tag set and their 
corresponding examples are listed in Table 1. 
With the tagging scheme, the Classical Chinese 
sentence segmentation task is transformed into a 
sequence labeling or tagging task. 
 
4.2 Features 
Due to the flexibility of the feature function in-
terface provided by CRFs, we apply various fea-
ture conjunctions. Besides the n-gram character 
patterns, the phonetic information and the part-  
 
Figure 2. Markov Chain of Our Tagging Scheme.  
 
 
Tag set Tags Example 
2-tag S: Start ??????? 
N: Non-Start ??????? 
4-tag 
(k=1) 
L1: Left-end ??????? 
M: Middle ??????? 
R: Right-end ??????? 
S: Single ? / ???? 
5-tag 
(k=2) 
L1: Left-end ??????? 
L2: Left-2nd ??????? 
M: Middle ??????? 
R: Right-end ??????? 
S: Single ? / ???? 
Table 1. Examples of Tag Sets. 
 
of-speech (POS) are also included. The pronun-
ciation of each Chinese character is labeled in 
three ways. The first one is Mandarin Phonetic 
Symbols (MPS), also known as Bopomofo, 
which is a phonetic system for Modern Chinese. 
The initial/final/tone of each character can be 
obtained from its MPS label.  
However, Chinese pronunciation varies in the 
thousands of years, and the pronunciation of 
Modern Chinese is much different from the 
Classical Chinese. For this reason, two Ancient 
Chinese phonetic systems, Fanqie (??) and 
Guangyun (??), are applied to label the cha-
racters. The pronunciation of a target character is 
represented by two characters in the Fanqie sys-
tem. The first character indicates the initial of 
the target character, and the second character 
indicates the combination of the final and the 
tone. The Guangyun system is in a similar man-
ner with a smaller phonetic symbol set. There 
are 8,157 characters in our phonetic dictionary 
and the statistics are shown in Table 2. 
The POS information is also considered. It is 
difficult to construct a Classical Chinese POS 
System #Initials #Finals #Tones 
MPS 21 36 5 
Fanqie 403 1,054 
Guangyun 43 203 
Table 2. Phonetic System Statistics. 
  
POS # Characters Examples 
Beginning 60 ?, ?, ? 
Middle 50 ?, ? 
End 45 ?, ?, ?, ?  
Interjection 20 ?, ?, ?, ? 
Table 3. Four Types of POS. 
 
tagger at this moment. Instead, we collected 
three types of particles that are usually placed at 
the beginning, at the middle, and at the end of 
Classical Chinese clauses. In addition, the inter-
jections which are usually used at the end of 
clauses are also collected. Some examples are 
given in Table 3. The five feature sets and the 
feature templates are shown in Table 4. 
5 Experiments 
There are three major sets of experiments.  In the 
1st set of experiments, we test different tagging 
schemes for Classical Chinese sentence segmen-
tation. In the 2nd set of experiments, all kinds of 
feature sets and their combinations are tested. 
The performances of the first two sets of expe-
riments are evaluated by 10-fold cross-validation 
on four datasets which cross both eras and con-
texts. In the 3rd set of experiments, we train the 
system on one dataset, and test it on the others. 
In last part of the experiments, the generality of 
the datasets and the toughness of our system are 
tested (Peng et al, 2004). The cut-off threshold 
for the features is set to 2 for all the experiments. 
In other words, the features occur only once in 
the training set will be ignored. The other op-
tions of CrfSgd remain default. 
5.1 Datasets 
The datasets used in the evaluation are collected 
from the corpora of the Pre-Qin and Han Dynas-
ties (the 5th century BCE to the 1st century BCE) 
and the Qing Dynasty (the 17th century CE to 
the 20th century CE). Chinese in the 19th cen-
tury is fairly different from Chinese in the era 
before 0 CE. In ancient Chinese, the syntax is 
much simpler, the sentences are shorter, and the 
words are largely composed of a single character. 
Those are unlike later and more modern Chinese, 
where word segmentation is a serious issue. 
Given these properties, the task of segmenting 
 
Feature Set Template Function 
Character ?? ,?2 ? ? ? 2 Unigrams 
????+1 ,?2 ? ? ? 1 Bigrams 
????+1??+2,?2 ? ? ? 0 Trigrams 
????+2 ,?2 ? ? ? 0 Jumps 
POS ???_?(?0)  Current character serves as a clause-beginning particle. 
???_?(?0) Current character serves as a clause-middle particle. 
???_?(?0) Current character serves as a clause-end particle. 
???_?(?0) Current character serves as an interjection. 
MPS ?_?(?0) The initial of current character in MPS. 
?_?(?0) The final of current character in MPS. 
?_?(?0) The tone of current character in MPS. 
?_?(??1)?_?(??1)?_?(?0) The connection between successive characters. 
Fanqie ?_?(?0) The initial of current character in Fanqie. 
?_?(?0) The final and the tone of current character in Fanqie. 
?_?(??1)?_?(?0) The connection between successive characters. 
Guangyun ?_?(?0) The initial of the current character in Guangyun. 
?_?(?0) The final and the tone of current character in Guan-
gyun. 
?_?(??1)?_?(?0) The connection between successive characters. 
Table 4. Feature Templates. 
Corpus Author Era #  of data 
entries 
# of  
characters 
Size of cha-
racter set 
Average # of  
characters/clause 
Zuozhuan Zuo Qiuming 500 BCE 3,381 195,983 3,238 4.145 
Zhuangzi Zhuangzi 300 BCE 1,128 65,165 2,936 5.183 
Shiji Qian Sima 100 BCE 4,778 503,890 4,788 5.049 
Qing Documents Qing Dynasty  
Officials 
19th  
century 
1,000 111,739 3,147 7.199 
Table 5. Datasets and Statistics. 
 
ancient Chinese sentences is easier than that of 
segmenting later Chinese ones. Thus, we col-
lected texts from the pre-Qin and Han period, 
and from the late Qing Dynasty closer to the 
present, to show that our system can handle 
Classical Chinese as it has evolved across a span 
of two thousand years. 
A summary of the four datasets is listed in 
Table 5. The Zuozhuan is one of earliest histori-
cal works, recording events of China in the 
Spring and Autumn Period (from 722 BCE to 
481 BCE). The book Zhuangzi was named after 
its semi-legendary author, the Daoist philoso-
pher Zhuangzi, who lived around the 4th century 
BCE. The book consists of stories and fables, in 
which the philosophy of the Dao is propounded. 
The Shiji, known in English as The Records of 
the Grand Historian, was written by Qian Sima 
in the 1st century BCE. It narrates Chinese histo-
ry from 2600 BCE to 100 BCE. The Shiji is not 
only an extremely long book of more than 
500,000 characters, but also the chief historical 
work of ancient China, exerting an enormous 
influence on subsequent Chinese literature and 
historiography. 
The three ancient works are the most impor-
tant classics of Chinese literature. We fetched 
well-segmented electronic editions of these 
works from the online database of the Institute 
of History and philology of the Academia Sinica, 
Taiwan.2 Each work was partitioned into para-
graphs forming a single data entry, which acted 
as the basic unit of training and testing. The da-
taset of Qing documents is selected from the 
Qing Palace Memorials (??) related to Taiwan 
written in the 19th century. These documents 
were kindly provided by the Taiwan History 
Digital Library and have also been human-
segmented and stored on electronic media (Chen 
et al, 2007). We randomly selected 1,000 para-
graphs from them as our dataset. 
                                                 
2http://hanji.sinica.edu.tw 
5.2 Evaluation Metrics 
For Classical Chinese sentence segmentation, we 
define the precision P as the ratio of the bounda-
ries of clauses which are correctly segmented to 
all segmented boundaries, the recall R as the ra-
tio of correctly segmented boundaries to all ref-
erence boundaries, and the score F as the har-
monic mean of precision and recall: 
 
? =
? ? ? ? 2
? + ?
 
 
Dataset Precision Recall F-Score 
Zuozhuan 100% 32.80% 42.73% 
Zhuangzi 100% 19.84% 29.83% 
Shiji 100% 14.11% 20.63% 
Qing Doc. 100% 33.08% 41.42% 
Average 100% 24.96% 33.65% 
Table 6. Performance of Majority-Class Baseline. 
 
Tag Set Precision Recall F-Score 
2-tag set 85.00% 82.16% 82.92% 
4-tag set 85.11% 82.13% 82.95% 
5-tag set 85.26% 82.36% 83.18% 
7-tag set 84.47% 82.18% 82.74% 
Baseline 100% 24.96% 33.65% 
Table 7. Comparison between Tagging Schemes. 
 
Features Precision Recall F-Score 
Character 85.26% 82.36% 83.18% 
POS 61.04% 40.35% 43.93% 
MPS 65.31% 54.00% 56.31% 
Fanqie 80.96% 76.80% 77.95% 
Guangyun 73.11% 69.13% 69.59% 
POS + 
Fanqie 
81.07% 74.91% 76.77% 
Character 
+ Fanqie 
85.43% 82.52% 83.34% 
Character 
+ POS + 
Fanqie 
85.67% 81.70% 82.98% 
Table 8. Comparison between Feature Sets. 
Dataset Precision Recall F-Score 
Zuozhuan 92.83% 91.56% 91.79% 
Zhuangzi 81.02% 78.87% 79.34% 
Shiji 80.79% 78.10% 78.99% 
Qing Doc. 87.07% 81.53% 83.24% 
Average 85.43% 82.52% 83.34% 
Table 9. Performance on Four Datasets. 
6 Results 
Our baseline is a majority-class tagger which 
always regards the whole paragraph as a single 
sentence (i.e., never segments). In Table 6, the 
performance of the baseline is given. In the 1st 
set of experiments, four tagging schemes are 
tested while the feature set is Character. The re-
sults are shown in Table 7. In the table, each of 
the precision, the recall, and the F-score are av-
eraged over the four datasets for each scheme. 
The results show that the CRF with the 5-tag set 
is superior to the 4-tag set used in previous 
works. However, the performance is degraded 
when the k is larger.  
In the 2nd set of experiments, the tag scheme 
is fixed to the 5-tag set and a number of feature 
set combinations are tested. The results are 
shown in Table 8. The performance of MPS is 
significantly inferior to the other two phonetic 
systems. As expected, the pronunciation of Clas-
sical Chinese is much different from that of 
Modern Chinese, thus the Ancient Chinese pho-
netic systems are more suitable for this work. 
The Fanqie has a surprisingly performance close 
to the Character. However, performance of the 
combination of Character and Fanqie is similar 
to the performance of Character only model. 
This result indicates that the phonetic informa-
tion is an important clue to Classical Chinese 
sentence segmentation but such information is 
mostly already covered by the characters. Be-
sides, the simple POS features do not help a lot. 
The higher precision and the lower recall of the 
POS features show that the particles such as ?/
?/?/? is indeed a clue to segmentation, but 
does not catch enough cases. 
The best performance comes from the com-
bination of Character and Fanqie with the 5-tag 
set. We use this configuration as our final tagger. 
The performances of our tagger for each dataset 
are given in Table 9. The result shows that our 
tagger achieves fairly good performance on the 
Zuozhuan segmentation, while obtaining accept-
able performance overall. Because the 19th cen-
tury Chinese is more complex than ancient Chi-
nese, what we had assumed was that segmenta-
tion of the Qing documents would more difficult. 
However, the results indicate that our assump-
tion does not seem to be true. Our tagger per-
forms the sentence segmentation on the Qing 
documents well, even better than on the Zhuang-
zi and on the Shiji. The issues of longer clauses 
and word segmentation described earlier in this 
paper do not significantly affect the performance 
of our system. 
In the last experiments, our system is trained 
and tested on different datasets, and the results 
are presented in Table 10, where the training 
datasets are in the rows and the test datasets are 
in the columns, and the F-scores of the segmen-
tation performance are shown in the inner entries. 
As expected, the results of segmentation tasks 
across datasets are significantly poorer than the 
segmentation in the first two experiments. 
These results indicate that our system main-
tains its performance on a test dataset differing 
from the training dataset, but the difference in 
written eras between the test dataset and training 
dataset cannot be very large. Among all datasets, 
Shiji is the best training dataset. As training on 
Shiji and testing on the two other ancient corpo-
ra Zuozhuan and Zhuangzi, the performances of 
our CRF segmenter are not bad. 
 
Training Set Testing Set  
Zuozhuan Zhuangzi Shiji Qing doc. Average 
Zuozhuan  72.04% 59.12% 38.85% 56.67% 
Zhuangzi 63.70%  52.51% 42.75% 52.99% 
Shiji 76.27% 75.46%  44.11% 65.28% 
Qing doc. 52.68% 53.13% 42.61%  49.47% 
Average 64.22% 66.88% 51.41% 41.90%  
Table 10. F-score of Segmentation cross the Datasets.
7 Conclusion 
Our Classical Chinese sentence segmentation is 
important for many applications such as text 
mining, information retrieval, corpora research, 
and digital archiving. To aid in processing such 
kind of data, an automatic sentence segmenta-
tion system is proposed. Different tagging 
schemes and various features are introduced and 
tested. Our system was evaluated using three 
sets of experiments. Five main results are de-
rived. First, the CRF segmenter achieves an F-
score of 91.79% in the best case and 83.34% in 
overall performance. Second, a little longer tag-
ging scheme improves the performance. Third, 
the phonetic information, especially sourced 
from Fanqie, is an important clue for Classical 
Chinese sentence segmentation and may be use-
ful in the related tasks. Fourth, our method per-
forms well on data from various eras. In the ex-
periments, texts from both 500 BCE and the 
19th century were well-segmented. Last, the 
CRF segmenter maintains a certain level of per-
formance in situations which the test data and 
the training data differ in authors, genres, and 
written styles, but eras in which they were pro-
duced are sufficiently close. 
References 
Chen, Szu-Pei, Jieh Hsiang, Hsieh-Chang Tu, and 
Micha Wu. 2007. On Building a Full-Text Digital 
Library of Historical Documents. In Proceedings 
of the 10th International Conference on Asian 
Digital Libraries, Lecture Notes in Computer 
Science, Springer-Verlag 4822:49-60. 
Gao, Jianfeng, Mu Li, and Chang-Ning Huang. 2003. 
Improved Source-Channel Models for Chinese 
Word Segmentation. In Proceedings of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics, 272-279. 
Huang, Hen-Hsen. 2008. Classical Chinese Sentence 
Division by Sequence Labeling Approaches. Mas-
ter?s Thesis, National Chiao Tung University, 
Hsinchu, Taiwan. 
Lafferty, John, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmentation and Labeling Se-
quence Data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, 282-289. 
Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, and 
Mary Harper. 2004. Comparing and Combining 
Generative and Posterior Probability Models: 
Some Advances in Sentence Boundary Detection 
in Speech. In Proceedings of the Conference on 
Empirical Methods in Natural Language 
Processing. 
Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, and 
Mary Harper. 2005. Using Conditional Random 
Fields for Sentence Boundary Detection in Speech. 
In Proceedings of the 43rd Annual Meeting of the 
Association for Computational Linguistics, 451-
458. Ann Arbor, Mich., USA. 
Peng, Fuchun, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese Segmentation and New Word 
Detection using Conditional Random Fields. In 
Proceedings of the 20th International Conference 
on Computational Linguistics, 562-568. 
Shriberg, Elizabeth, Andreas Stolcke, Dilek Hakkani-
T?r, and G?khan T?r. 2000. Prosody-Based Au-
tomatic Segmentation of Speech into Sentences 
and Topics. Speech Communication, 32(1-2):127-
154. 
Vishwanathan, S. V. N., Nicol N. Schraudolph, Mark 
W. Schmidt, and Kevin P. Murphy. 2006. Accele-
rated training of conditional random fields with 
stochastic gradient methods. In Proceedings of the 
23th International Conference on Machine Learn-
ing, 969?976. ACM Press, New York, USA. 
Xue, Nianwen. 2003. Chinese Word Segmentation as 
Character Tagging. Computational Linguistics and 
Chinese Language Processing, 8(1):29-48. 
Zhang, Hel, Wang Xiao-dong, Yang Jian-yu, and 
Zhou Wei-dong. 2009. Method of Sentence Seg-
mentation and Punctuating for Ancient Chinese. 
Application Research of Computers, 26(9):3326-
3329. 
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 261?269,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Contingency and Comparison Relation Labeling and Structure Prediction in Chinese Sentences 
  Hen-Hsen Huang Hsin-Hsi Chen Department of Computer Science and Department of Computer Science and Information Engineering, Information Engineering, National Taiwan University, Taipei, Taiwan National Taiwan University, Taipei, Taiwan hhhuang@nlg.csie.ntu.edu.tw hhchen@csie.ntu.edu.tw      Abstract 
Unlike in English, the sentence boundaries in Chinese are fuzzy and not well-defined. As a result, Chinese sentences tend to be long and consist of complex discourse relations. In this paper, we focus on two important relations, Contingency and Comparison, which occur often inside a sentence. We construct a moderate-sized corpus for the investigation of intra-sentential relations and propose models to label the relation structure. A learning based model is evaluated with various features. Experimental results show our model achieves accuracies of 81.63% in the task of relation labeling and 74.8% in the task of relation structure prediction.  1 Introduction Discourse relation labeling has attracted much attention in recent years due to its potential applications such as opinion mining, question answering, etc. The release of the Penn Discourse Treebank (Joshi and Webber, 2004; Prasad et al, 2008) has advanced the development of English discourse relation recognition (Lin et al, 2009; Pitler et al, 2009; Pitler and Nenkova, 2009; Wang et al, 2010). For Chinese, a discourse corpus is not publicly available yet. Thus, the research on Chinese discourse relation recognition is relatively rare. Most notably, Xue (2005) annotated discourse 
connectives in the Chinese Treebank. Our previous work labeled four types of relations, including temporal, contingency, comparison and expansion, between two successive sentences, and reported an accuracy of 88.28% and an F-score of 62.88% (Huang and Chen, 2011). The major issue of our work is the determination of discourse boundaries. Each Chinese sentence is always treated as one of the two arguments in their annotation and many instances of the Contingency and the Comparison remain uncaught. As suggested by the Penn Discourse Treebank annotation guidelines, an argument is possibly some clauses in a sentence, a sentence, or several successive sentences. In Chinese, the Contingency and the Comparison relations are likely to occur within a sentence. Thus, a lot of the Contingency relations and the Comparison relations are missing from annotation in the corpus used in our previous work, and the classification performance for these two relations, especially the Contingency relation, is especially poor (Huang and Chen, 2011). In contrast to Chinese inter-sentential discourse relation detection (Huang and Chen, 2011) and the study of English coherence evaluation (Lin et al, 2011), this paper focuses on the Contingency relation and the Comparison relations that occur inside a sentence. In Chinese, the relations usually occur in the sentences which contain many clauses. For example, two relations occur in sample (S1).  (S1) ?????????????????????????????  (?Although the 
261
management office tried to make the Yangmingshan area a more natural environment as the long-term garden of Taipei?)???????????????  (?But due to the two-day weekend and the improved economic conditions?)???????????????????? (?The issues of tourists parking, garbage, and other indirect effects become more serious?)?  In (S1), the long sentence consists of three clauses, and such a Chinese sentence is expressed as multiple short sentences in English. Figure 1 shows that a Comparison relation occurs between the first clause and the last two clauses, and a Contingency relation occurs between the second clause and the third clause. An explicit paired discourse marker ? (although) ? ? (but) denotes a Comparison relation in (S1), where the first clause is the first argument of this relation, and the second and the third clauses are the second argument of this relation. In addition, an implicit Contingency relation also occurs between the second and the third clauses. The second clause is the cause argument of this Contingency relation, and the third clause is its effect. It shows a nested relation, which makes relation labeling and relation structure determination challenging. In Chinese, an explicit discourse marker does not always uniquely identify the existence of a particular discourse relation. In sample (S2), a discourse marker ?  ?moreover? appears, but neither Contingency nor Comparison relation exists between the two clauses. The discourse marker ? has many meanings. Here, It has the meaning of ?and? or ?moreover?, which indicates an Expansion relation. In other usages, it may have the meaning of ?but? or ?however?, which indicates a Comparison relation.  (S2) ???????????????????????? (?Moreover, the progress of mainland is more impressive due to its economic openness for the last 10 years.?)  Note that the relation structure of a sentence cannot be exactly derived from the parse tree of the sentence. Shown in Figure 2 is the structure of sample (S3) based on the syntactic tree generated by the Stanford parser. However, it is clear that the 
correct structure of (S3) is the one shown in Figure 3.  (S3) ????????????????? (?Although women only appear in the pictures?)?? ? ? ? ? ? ? ?  (?The contribution of women?)?????????????? (?Will be another major focus in textbooks in the future?)?  This shows that the Stanford parser does not capture the information that the last two clauses form a unit, which in turn is one of the two arguments of a Comparison relation. In this work, we investigate intra-sentential relation detection in Chinese. Given a Chinese sentence, our model will predict if Contingency or Comparison relations exist, and determine their relation structure. In Section 2, the development of a corpus annotated with Contingency and Comparison relations is presented. The methods and the features are proposed in Section 3. In Section 4, the experimental results are shown and discussed. Finally, Section 5 concludes this paper. 
 Figure 1: Relation structure of sample (S1). 
 Figure 2: Structure of sample (S3) based on the syntactic tree generated by the Stanford parser.  
 Figure 3: Correct structure of sample (S3) 
262
2 Dataset  The corpus is based on the Sinica Treebank (Huang et al, 2000). A Total of 81 articles are randomly selected from the Sino and Travel sets. All the sentences that consist of two, three, and four clauses are extracted for relation and structure labeling by native Chinese speakers. A web-based system is developed for annotation. The annotation scheme is designed as follows. An annotator first signs in to the annotation system, and a list of sentences that are assigned to the annotator are given. The annotator labels the sentences one by one in the system. A sentence is split into clauses along commas, and all of its feasible binary tree structures are shown in the interface. The annotator decides if a Contingency/Comparison relation occurs in this sentence. The sentence will be marked as ?Nil? if no relation is found. If there is at least one relation in this sentence, the annotator then chooses the best tree structure of the relations, and the second page is shown. The previously chosen tree structure is presented again, and at this time the annotator has to assign a suitable relation type to each internal node of the tree structure. The relation type includes Contingency ????, Comparison ????, and Nil. For example, in sample (S4), its three internal nodes are annotated with three relation types as shown in Figure 4.  (S4) ?????????? (?Even without the sense of mission of the heritage?)????????????  (?In order to seek better treatments?)?????????????????? (?These medical workers will be driven crossing domain areas?)?????? (?To find resources?)?  The number of feasible relation structures of a sentence may be very large depending on the number of clauses. For a sentence with n clauses, the number of its feasible structures is given as the recursive function f(n) as follows, and the number of its feasible relation structures is 3???? ? .  
? ? = 1, ? = 1? ? ? ? ?(?)?????? , ? > 1 
 Figure 4: Relation structure of sample (S4).  Explicit/ implicit Relations 2-Clause 3-Clause 4-Clause Total % Explicit Both 0 5 6 11 0.89% Contingency 59 72 45 176 14.31% Comparison 41 57 22 120 9.76% Nil 269 249 169 687 55.85% Implicit Both 0 0 0 0 0.00% Contingency 11 8 0 19 1.54% Comparison 6 0 0 6 0.49% Nil 125 56 4 211 17.15% All  511 447 272 1,230 100.00% Table 1: Statistics of the dataset.  For a two-clause sentence, there are only one tree structure and three possible relation tags (Contingency, Comparison, and Nil) for the only one internal node, the root. For a three-clause sentence, there are two candidate tree structures and nine combinations of the relation tags. For a four-clause sentence, there are five candidate tree structures and 27 combinations of the relation tags. There are theoretically 3, 18, and 135 feasible relation structures for the two-, three-, and four- clause sentences, respectively, though only 49 types of relations structures are observed in the dataset. Each sentence is shown to three annotators, and the majority is taken as the ground-truth. The Fleiss-Kappa of the inter-annotator agreement is 0.44 (moderate agreement). A final decider is involved to break ties. The statistics of our corpus are shown in Table 1. The explicit data are those sentences which have at least one discourse marker. The rest of the data are implicit. A total of 11 explicit sentences which contain both Contingency and Comparison relations form complex sentence compositions. The implicit samples are relatively rare. 3 Methods To predict the intra-sentential relations and structures, two learning algorithms, the modern implementation of the decision tree algorithm, 
263
C5.01, and the support vector machine, SVMlight2, are applied. The linguistic features are the crucial part in the learning-based approaches. Various features from different linguistic levels are evaluated in the experiments as shown below. Word: The bags of words in each clause. The Stanford Chinese word segmenter3 is applied to all the sentences to tokenize the Chinese words. In addition, the first word and the last word in each clause are extracted as distinguished features. POS: The bags of parts of speech (POS) of the words in each clause are also taken as features. All the sentences in the dataset are sent to the Stanford parser4 that parses a sentence from a surface form into a syntactic tree, labels POS for each word, and generates all the dependencies among the words. In addition, the POS tags of the first word and the last word in each clause are extracted as distinguished features. Length: Several length features are considered, including the number of clauses in the sentence and the number of words for each clause in the sentence. Connective: In English, some words/phrases called connectives are used as discourse markers. For example, the phrase ?due to? is a typical connective that indicates a Contingency relation, and the word ?however? is a connective that indicates a Comparison relation. Similar to the connectives in English, various words and word pair patterns are usually used as discourse markers in Chinese. A dictionary that contains several types of discourse markers is used. The statistics of the connective dictionary and samples are listed in Table 2. An intra-sentential phrase pair indicates a relation which occurs only inside a sentence. In other words, a relation occurs when the two phrases of an intra-sentential pair exist in the same sentence no matter whether they are in the same clause or not. In contrast, an inter-sentential connective indicates a relation that can occur across neighboring sentences. Some connectives belong to both intra-sentential and inter-sentential types. Each connective in each clause is detected and marked with its corresponding type. For example, the phrase ??
                                                            1 http://www.rulequest.com/see5-unix.html 2 http://svmlight.joachims.org/ 3 http://nlp.stanford.edu/software/segmenter.shtml 4 http://nlp.stanford.edu/software/lex-parser.shtml 
? ?In contrast? will be marked as a connective that belongs to Comparison relation. The number of types and scopes of the connectives in a sentence are used as features. Dependency: The dependencies among all words in a sentence are used as features. The Stanford parser generates dependency pairs from the sentence. A dependency pair consists of two arguments, i.e., the governor and the dependent, and their types. We are interested in those dependency pairs that are across two clauses. That is, the two arguments of a pair are from different clauses. In our assumption, the clauses have a closer connection if some dependencies occur between them. All such dependency pairs and their types are extracted and counted. Structure: Recent research work reported improved performance using syntactic information for English discourse relation detection. In the work of Pilter and Nenkova (2009), the categories of a tree node, its parent, its left sibling, and its right sibling are taken as features. In the work of Wang et al (2010), the entire paragraph is parsed    Relation Type  # Samples Temporal Single Phrase 41 ?? ?now? ?? ?after? 
Intra-Sent Phrase Pair 80 ??...? ?Then...again? ??...? ?At first...ever? 
Inter-Sent Phrase Pair 30 ??...?? ?Initially...Later? ??...??? ?At first...Then? 
Contingency Single Phrase 62 ???? ?As a result? ?? ?If? 
Intra-Sent Phrase Pair 180 ??...? ?If ... then? ??...? ?Whether ...? 
Inter-Sent Phrase Pair 14 ??...?? ?Since... It seems? ??...?? ?Fortunately... otherwise? 
Comparison Single Phrase 34 ??? ?In contrast? ?? ?Unexpectedly? 
Intra-Sent Phrase Pair 38 ??...? ?Even ... but? ??...? ?Although...still? 
Inter-Sent Phrase Pair 15 ??...?? ?Although... In fact? ??...?? ?Although... However? 
Expansion Single Phrase 182 ???? ?in addition? ?? ?moreover? 
Intra-Sent Phrase Pair 106 ??...?? ?Not only...but also? ??...?? ?or...or? 
Inter-Sent Phrase Pair 26 ??...?? ?Firstly...Secondly? ??...?? ?Since...Furthermore? Table 2: Statistics of connectives (discourse markers). 
264
 Figure 5: The upper three level sub-tree of (S1) and the punctuation sub-tree of (S1).  as a syntactic tree, and three levels of tree expansions are extracted as structured syntactic features. To capture syntactic structure, we get the syntactic tree for each sentence using the Stanford parser, and extract the sub-tree of the upper three levels, which represents the fundamental composition of this sentence. In addition, all the paths from the root to each punctuation node in a sentence are extracted. From the paths, the depth of each comma node is counted, and the common parent node of every adjacent clause is also extracted. For example, the upper three level sub-tree of the syntactic tree of (S1) is shown in Figure 5. In addition, the sub-tree in the dotted line forms the structure of the punctuations in the (S1).  Polarity: A Comparison relation implies its two arguments are contrasting, and some contrasts are presented with different polarities in the two arguments. For example, sample (S5) is a case of Comparison.  (S5)  ???????????????????????????????????? (?Despite such favorable natural environment, man-made disasters still make the Khmer people unfortunate to suffer from the pain of war.?)  The first clause in (S5) is positive (?favorable natural environment?), while the last two clauses are negative (?unfortunate to suffer from the pain of war?). Besides the connectives ?? ?despite? and ??  ?still?, the opposing polarity values between the first and the last two clauses is also a strong clue to the existence of a Comparison 
relation. In addition, the same polarity of the last two clauses is also a hint that no Comparison relation occurs between them. To capture polarity information, we estimate the polarity of each clause and detect the negations from the clause. The polarity score is a real number estimated by a sentiment dictionary-based algorithm. For each clause, the polarity score, and the existence of negation are taken as features. 4 Experiments and Discussion 4.1   Experimental Results All the models in the experiments are evaluated by 5-fold cross-validation. The metrics are accuracies and macro-averaged F-scores. The t-test is used for significance testing. We firstly examine our model for the task of two-way classification. In this task, binary classifiers are trained to predict the existence of Contingency and Comparison relations in a given sentence. For meaningful comparison, a majority classifier is used as a baseline model, which always predicts the majority class. In the dataset, 72.6% of the sentences involve neither Contingency nor Comparison. Thus, the major class is ?Nil?, and the accuracy and the F-score of the baseline model is 72.6% and 42.06%, respectively. The experimental results for the two-way classification task are shown in Table 3. In the table, the symbol ? denotes the lowest accuracy which has a significant improvement over the baseline at p=0.05 for the two models. The symbol ? denotes the adding of a single feature yields a significant improvement for the model at p=0.005. The performance of the decision tree and the SVM are similar in terms of accuracy and F-score. Overall, the decision tree model achieves better accuracies. In the two-way classification task, the decision tree model with only the Word feature achieves an accuracy of 76.75%, which is significantly better than the baseline at p=0.05. For both the decision tree and the SVM, Connective is the most useful feature: performance is significantly improved with the addition of Connective.  Besides the binary classification task, we extend our model to tackle the task of finer classification. In the second task, four-way classifiers are trained  
265
  Decision Tree SVM Features Accuracy F-Score Accuracy F-Score Word ?76.75%  58.94% 72.36% 56.54% +POS  77.15% 61.72% 72.28%  60.53% +Length 77.15%  61.72%  72.60% 61.09% +Connective ?81.63%  71.11% ?78.05% 69.17% +Dependency 81.14% 70.79% 77.80% 68.79% +Structure  81.30%  70.78%  ?77.48% 69.08% +Polarity  81.30%  70.78% 77.64% 69.09% Table 3: Performance of the two-way classification.    Decision Tree SVM Features Accuracy F-Score Accuracy F-Score Word ?76.50%  34.72% 73.58% 31.54% +POS  76.99% 36.77% 72.52%  34.44% +Length  76.99%  36.77% 72.36% 34.54% +Connective  79.84%  44.08% ?77.89%  45.26% +Dependency 79.92% 44.47% ?77.07% 44.42% +Structure   79.92%   44.47% 77.15% 44.69% +Polarity  79.92%  44.47% 77.40% 44.80% Table 4: Performance of the four-way classification.   Decision Tree SVM Features Accuracy F-Score Accuracy F-Score Word 73.66%   3.00% 70.00% 3.62% +POS  73.66% 3.00% 69.84% 4.29% +Length 73.66%  3.00% 70.00% 5.08% +Connective  74.80%  4.90% 74.39% 7.66% +Dependency  74.72% 4.61% 72.60% 5.60% +Structure  74.72%  4.61% 73.01% 5.49% +Polarity  74.72%  4.61% 72.76% 5.23% Table 5: Performance of the 49-way classification.   Task Explicit Implicit Accuracy F-score Accuracy F-score 2-way 77.97% 69.26% 88.98% 50.64% 4-way 76.06% 42.54% 88.98% 31.39% 49-way 71.33% 4.88% 89.41% 1.92% Table 6: Performances for explicit cases and implicit cases.  to predict a given sentence with four classes: existence of Contingency relations only, existence of Comparison relations only, existence of Both relations, and Nil. The experimental results of the four-way classification task are shown in Table 4. Consistent with the results of the two-way classification task, the addition of Connective to the SVM yields a significant improvement at p=0.005. The performance between the decision tree and the SVM is still similar, but the SVM achieves a slightly better F-score of 45.26% in comparison with the best F-score of 44.47% achieved by the decision tree. 
We further extend our model to predict the full relation structure of a given sentence as shown in Figure 1 and Figure 4. This is a 49-way classification task because there are 49 types of the full relation structures in the dataset. Not only as many as 49-ways, 72.6% of instances belong to the Nil relation, which yields an unbalanced classification problem. The experimental results are shown in Table 5. In the most challenging case, the SVM achieves a better F-score of 7.66% in comparison with the F-score of 4.90% achieved by the decision tree. Connective is still the most helpful feature. Comparing the F-scores of the SVM in the three tasks with the F-scores of the decision tree, it shows that the SVM performs better for predicting finer classes. 4.2 Explicit versus Implicit We compare the performances between the explicit instances and the implicit instances for the three tasks with the decision tree model trained on all features.  The results are shown in Table 6. The higher accuracies and the lower F-scores of the implicit cases are due to the fact that the classifier tends to predict the sentences as Nil when no connective is found, and most implicit samples are Nil. For example, the relation of Contingency in implicit sample (S6) should be inferred from the meaning of ?? ?brought?.  (S6) ??????????????????????????(?The unique geographical environment, it really brought the infinite wealth to this hundred-year port.?)  In addition, some informal/spoken phrases are useful clues for predicting the relations, but they are not present in our connective dictionary. For example, the phrase ? ?  ?if? implies a Contingency relation in (S7). This issue can be addressed by using a larger connective dictionary that contains informal and spoken phrases.  (S7) ??????????????????????? (?If you want to backpacking, how about an organized tour??)    We regard an instance as explicit if there is at least one connective in the sentence. However, many explicit instances are still not easy to label 
266
even with the connectives. As a result, predicting explicit samples is much more challenging than the task of recognizing explicit discourse relations in English. One reason is the ambiguous usage of connectives as shown in (S2). The following sentence depicts another issue. The word ?? ?however? in (S8) is a connective used as a marker of an inter-sentential relation. That is, the entire sentence is one of the arguments of an inter-sentential Comparison relation, but it does not contain any intra-sentential relation inside the sentence itself.   (S8) ????????????????????????(?However, Fu Wu Kang, who speaks fluent Chinese, openly criticizes this opinion.?)  The fact that connectives possess multiple senses is one of the important reasons for their misclassification. This issue can be addressed by employing contextual information such as the neighboring sentences. 4.3 Number of Clauses We compare the performance among the 2-clause instances, the 3-clause instances, and the 4-clause instances for the three tasks with the decision tree model trained on all the features. The accuracies (A) and F-scores (F) are reported in Table 7.  Comparing the two-way classification and the four-way classification tasks, the performance of the longer instances decreases a little in relation labeling. Although sentence complexity increases with length, a longer sentence provides more information at the same time. In the 49-way classification, the model should predict the sentence structure and the relation tags from the 49 candidate classes. The performances are greatly decreased because the feasible classes are substantially increased along with the number of clauses.  4.4 Contingency versus Comparison The confusion matrix of the decision tree model trained on all features for the four-way classification is shown in Table 8. Each row represents the samples in an actual class, while each column of the matrix represents the samples in a predicted class. The precision (P), recall (R), 
  Task 2-Clause 3-Clause 4-Clause A (%) F (%) A (%) F (%) A (%) F (%) 2-way 81.80 66.39 78.52 70.32 79.41 69.32 4-way 79.84 49.98 75.62 42.64 80.88 46.73 49-way 80.23 29.62 70.02 9.56 69.85 2.25 Table 7: Performances of clauses of different lengths.  Actual Class Predicted Class Performance Cont. Comp. Both Nil P (%) R (%) F (%) Cont. 61 3 0 131 81.33 31.28 45.19 Comp. 3 40 0 83 74.07 31.75 44.44 Both 2 4 0 5 0 0 0 Nil 9 7 0 882 80.11 98.22 88.24 Table 8: Confusion matrix of the best model in the 4-way classification.  Feature instance Category Usages The first token in the third clause is the word? ?but; however? Word 100% The first token in the second clause is the word ? ?but; however? Word 99% The first token in the third clause is a single connective of Contingency Connective 98% The first token in the first clause is the word ?? ?because; due to? Word 96% There is at least one word ?? ?in order to avoid? in the entire sentence Word 95% The first token in the second clause is the word ? ?moreover; while; but? Word 94% The first token in the third clause is a single connective of Comparison Connective 93% The second clause contains a single connective of Contingency Connective 92% The first token in the second clause is a single connective of Contingency Connective 91% The first clause contains a single connective of Contingency Connective 90% Table 9: Instances of the top ten useful features for the decision tree model  and F-score (F) for each class are provided on the right side of the table. The class Both is too small to train the model, thus our model does not correctly predict the samples in the Both class. The confusion matrix shows that the confusions between the classes Contingency and Comparison are very rare. The major issue is to distinguish Contingency and Comparison from the largest class, Nil. The lower recall of the Contingency and Comparison relations also show that our model tends to predict the instances as the largest class. 4.5 Features The top ten useful feature instances reported by the decision tree model in the 49-way classification are shown in Table 9. Word and Connective provide useful information for the classification. Moreover, 
267
seven of the ten feature instances are about the word or the connective category of the first token in each clause. This result shows that it is crucial to employ the information of the first token in each clause as distinguished features. Certain words, for example, ? ?but; however?, ?? ?because; due to?, and ? ?moreover; while; but? are especially useful for deciding the relations. For this reason, labeling these words carefully is necessary. All the synonyms for each of these words should be clustered and assigned the same category. In addition, a dedicated extractor should be involved in accurately fetching these words from the sentence in order to reduce tokenization errors introduced by the Chinese word segmenter.  The advanced features such as Dependency, Structure, and Polarity are not helpful as expected. One possible reason is that the training data is still not enough to model the complex features. In such a case, the surface features are even more useful. Sample (S1) shows an interesting case of the use of polarity information. The first clause of (S1) is positive (?????????????????????????  ?tried to make the Yangmingshan area a more natural state as the long-term garden of Taipei?), the second clause of (S1) is also positive (??????????????  ?the two-day weekend and the improved economic conditions.?), while the last clause of (S1) is negative (???????????????????  ?the issues of tourists parking, garbage, and other indirect effects?). The polarity of the last clause is opposite to those of the second clause, but they do not form a Comparison relation. Instead, a Contingency relation occurs between the last two clauses. Likewise, the polarities of the first and second clauses are both positive, but a Comparison relation occurs after the first clause. In fact, we realize that this is a complex case after performing an in-depth analysis. Because the last clause plays the role of effect in the Contingency relation, the negative polarity of the last clause makes the last two clauses form a negative polarity. For this reason, a Comparison relation occurs between the first argument with positive polarity and the second argument (i.e., the last two clauses) with negative polarity without a doubt. The polarity diagram of sample (S1) is shown in Figure 6.  
 
 Figure 6: Polarity diagram of (S1).  Overall, the interaction among structure, relation, and polarity is complicated. The surface polarity information we extract by using the sentiment dictionary-based algorithm does not capture such complexity well. A dedicated structure-sensitive polarity tagger will be utilized in future work. 5 Conclusion and Future Work In this paper, we addressed the problem of intra-sentential Contingency and Comparison relation detection in Chinese. This is a challenging task because Chinese sentences tend to be very long and therefore contain more clauses. To tackle this problem, we constructed a moderate-sized corpus and proposed a learning-based approach that achieves accuracies of 81.63%, 79.92%, and 74.80% and F-scores of 71.11%, 45.26%, and 7.66% in the two-way, the four-way, and the 49-way classification tasks, respectively. From the experiments, we found that performance could be significantly improved by adding the Connective feature. The next step is to enlarge the connective dictionary automatically by a text mining approach, in particular with those informal connectives, in order to boost performance. The advanced features such as Dependency, Structure, and Polarity are not as helpful as expected due to the small size of the corpus. In future work, we plan to construct a large Chinese discourse Treebank based on the methodology proposed in Section 2 and release the corpus to the public. Naturally, the intra-sentential relations are important cues for discourse relation detection at the inter-sentential level. How to integrate cues from these two levels will be investigated. Besides, relation labeling and structure prediction are tackled at the same time with the same learning algorithm in this study. We will explore different methods to tackle the two problems separately to reduce the complexity.  
268
References Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhao-ming Gao, and Kuang-Yu Chen. 2000. Sinica Treebank: Design Criteria, Annotation Guidelines, and On-line Interface. In Proceedings of 2nd Chinese Language Processing Workshop (Held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, ACL-2000), pages 29-37. Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese Discourse Relation Recognition. In Proceedings of 5th International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 1442-1446. Aravind Joshi and Bonnie L. Webber. 2004. The Penn Discourse Treebank. In Proceedings of the Language and Resources and Evaluation Conference, Lisbon. Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing Implicit Discourse Relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), Singapore.  Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.  Automatically Evaluating Text Coherence Using Discourse Relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011), pages 997-1006. Emily Pitler and Ani Nenkova. 2009. Using Syntax to Disambiguate Explicit Discourse Connectives in Text. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13-1, Singapore. Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic Sense Prediction for Implicit Discourse Relations in Text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP 2009), Singapore. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Treebank 2.0. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC). WenTing Wang, Jian Su, and Chew Lim Tan. 2010. Kernel Based Discourse Relation Recognition with Temporal Ordering Information. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, July. 
Nianwen Xue. 2005. Annotating Discourse Connectives in the Chinese Treebank. In Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 84-91. 
269
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 70?78,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Analyses of the Association between Discourse Relation and Sentiment Polarity with a Chinese Human-Annotated Corpus 
 Hen-Hsen Huang Chi-Hsin Yu Tai-Wei Chang Cong-Kai Lin Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University, Taipei, Taiwan {hhhuang, jsyu, twchang, cklin}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw     Abstract 
Discourse relation may entail sentiment in-formation. In this work, we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annota-tion, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Finally, we highlight some language phenomena and give some remarks. 1 Introduction A discourse relation indicates how two argu-ments (i.e., elementary discourse units) cohere to each other. Various discourse relations were de-fined according to different taxonomy (Carlson and Marcu, 2001; Carlson et al, 2002; Prasad et al, 2008). In the work of the Penn Discourse Treebank 2.0 annotation, Prasad et al (2008) labeled four grammatical classes of connectives in English, including subordinating conjunctions, coordinating conjunctions, adverbial connectives, and implicit connectives. Besides, the sense of each connective was also tagged. They defined three levels of sense hierarchy for the connec-tives. The four classes on the top level are Tem-poral, Contingency, Comparison, and Expansion.  There are explicit and implicit uses of dis-course relations. An explicit discourse relation indicates the arguments are connected with an overt discourse marker (i.e., connective). A con-nective joins two discourse units such as phrases, clauses, or sentences together. For example, the word however is a common connective that indi-cates a Comparison relation between two argu-ments. The sense of a discourse marker denotes how its two arguments cohere. In other words, a 
discourse marker presents the relation of its two arguments. In other cases, discourse marker is absent from an implicit relation. However, readers can still infer the relation from its argument pair. To re-solve implicit discourse relations, i.e., without the information from discourse markers, is more challenging (Lin et al, 2009; Zhou et al, 2010).   Hutchinson (2004) pointed out the properties of a discourse marker from three dimensions, including polarity, veridicality, and type. The polarity of a discourse marker indicates the sen-timent transition of its two arguments. Veridi-cality, the second dimension of a discourse marker, specifies whether both the two argu-ments are true or not. Type, similar to the sense which is annotated in the PDTB, is the third di-mension of a discourse marker.  Our previous work (Huang and Chen, 2012a; Huang and Chen, 2012b) addressed the interac-tion between the sentiment polarity and the dis-course structure in Chinese. Consider (S1), which consists of three clauses and forms a nest-ed discourse structure shown in Figure 1.  (S1) ??????????????????????????????? (Although the management office tried to make the Yang-mingshan area a more natural environment as the long-term garden of Taipei)??????????????? (but due to the two-day weekend and the improved economic conditions)??????????????????? (the is-sues of tourist parking, garbage, and other indi-rect effects become more serious)?  The second and the third clauses form a Con-tingency relation with a sentiment polarity transi-tion from Positive to Negative. Furthermore, 
70
 Figure 1: Discourse structure and sentiment po-larities of (S1).  these two clauses also constitute one of the ar-guments of a Positive-Negative Comparison rela-tion. As the PDTB 2.0 annotation manual sug-gests (Prasad, et al, 2007), a Comparison rela-tion is established to emphasize the differences between two arguments. Therefore, it is expected that the two arguments of a Comparison relation are relatively likely to have the opposing polarity states (i.e., Positive-Negative or Negative-Positive). On the other hand, the two arguments of an Expansion relation are relatively likely to belong to the same polarity states (e.g., Positive-Positive or Neutral-Neutral).  Discourse relation recognition (Hernault et al, 2010; Soricut and Marcu, 2003) and sentiment analysis (Pang and Lee, 2008) have attracted much attention recently. Due to the limitation of the resources, the research on Chinese discourse relation analysis is relatively rare. In our previ-ous work, we annotated a collection of Chinese discourse corpora, namely NTU Chinese Dis-course Resources (http://nlg.csie.ntu.edu.tw/ntu-discourse/), for inter-sentential and intra-sentential discourse relation recognition (Huang and Chen, 2011; Huang and Chen, 2012a). How-ever, no sentiment information is labeled in these corpora. In another work (Huang and Chen, 2012b), we proposed an annotation scheme to construct a Chinese discourse corpus with rich information including sentiment polarities, but the corpus is still under construction due to its complexity. Zhou and Xue (2012) did PDTB-style Chinese discourse corpus annotation, but the corpus is also not available yet. In this paper, we annotate a moderate-sized Chinese corpus with the information of discourse relations and sentiment polarities. Total 7,638 sentences are sampled from the ClueWeb09. We review the results of annotation and analyze some language phenomena found in the corpus.  The rest of this paper is organized as follows. In Section 2, we introduce the ClueWeb corpus 
and a dictionary of Chinese discourse markers. In Section 3, the criteria to sample instances and the annotation scheme are shown. We analyze the language phenomena found in the annotated data and discuss the correlation between discourse relations and sentiment polarities in Section 4. Finally, we conclude the remarks in Section 5.  2 Linguistic Resources The PDTB is a popular dataset used in the Eng-lish discourse research. In contrast, no Chinese discourse corpus is publicly available at present. To construct a Chinese discourse corpus, we sample instances from a huge Chinese corpus (Yu et al, 2012). This corpus was developed based on the ClueWeb09 dataset, where Chinese material is the second largest. It contains a total of 9,598,430,559 POS-tagged sentences in 172,298,866 documents.   In this paper, only the explicit discourse rela-tions are concerned. A dictionary of discourse markers is consulted to extract the instances of explicit discourse relations from the ClueWeb. This Chinese discourse marker dictionary is de-veloped based on Cheng and Tian (1989), Cheng (2006) and Lu (2007). Table 1 shows an over-view of the discourse marker dictionary. It con-tains 808 words and word pairs mapped into the PDTB four top-level classes (Cheng and Tian, 1989; Wolf and Gibson, 2005). Besides the types of discourse relations, we further classify the markers into three groups of scopes shown in the second column, including Single word, Intra-sentential, and Inter-sentential, according to their grammatical usages. The Single word group con-tains those individual words used as discourse markers. The Intra-sentential group contains pairs of words that occur inside the same senten-ce and denote a discourse relation. Here, a Chi-nese sentence is defined as a sequence of succes-sive words that is ended by a period, a question mark, or an exclamation mark. The clauses of a sentence are delimited by commas. The Inter-sentential discourse markers are similar to the Intra-sentential ones, but the two words of a pair individually appear in different sentences. Some discourse markers can be used as both Inter-sentential and Intra-sentential. In this work, the Inter-sentential only discourse markers are ex-cluded because we only concern the discourse relation occurring within a sentence. The third column lists the number of discourse markers for each scope under each PDTB class, and the fourth column gives some examples. 
71
PDTB Class Scope # Markers Examples 
Expansion 
Single word 177 ?? (besides), ?? (or), ?? (not only), ?? (such as) Intra-sentential 106 ??????? (on the one hand ... on the other hand), ????? (not ... but), ???? (not only ... also) Inter-sentential 26 ????? (first ... second), ???? (or ... perhaps), ????? (not only ... not only) 
Temporal 
Single word 41 ?? (then) Intra-sentential 80 ????? (first ... finally) Inter-sentential 30 ????? (first ... now) 
Comparison 
Single word 34 ?? (even if) Intra-sentential 38 ???? (although ... but) Inter-sentential 15 ????? (in spite of ... in fact) 
Contingency 
Single word 67 ?? (because), ? (if), ?? (suppose), ?? (in order to avoid) Intra-sentential 180 ??? (because ... then), ??? (if then), ??? (any ... can) Inter-sentential 14 ????? (since ... then), ????? (at least ... otherwise) Table 1: Overview of a Chinese discourse marker dictionary. 3 Annotation Based on the Chinese part of the ClueWeb09 (Yu et al, 2012), we sample a moderate-sized data with some criteria and annotate them with the information of discourse relations and sentiment polarities. 3.1 Sampling a reliable dataset Discourse relations may be explicit or implicit, and a sentence may contain more than one dis-course marker. Multiple discourse relations oc-curring in a sentence will make the annotation more complex. In this work, we focus on the cor-relation between discourse relations and senti-ment polarity. To get a reliable dataset for analy-sis, we sample sentences based on the following three criteria. 1.  A sentence should contain only two clauses. 2. A sentence should contain exact one dis-course marker shown in the Chinese discourse marker dictionary. We match the discourse marker on the word level. For the Single word markers, the marker can appear in either of the clauses. For the pairwise markers, the first word should appear in the first clause, and the second word should appear in the second one. 3. The lengths of both clauses in a sentence are no more than 20 Chinese characters.  As shown in Figure 1, the sentiment polarity determination is more challenging when more than one discourse relation is involved in a sen-tence. In order to facilitate the analysis, we focus on those sentences that contain exact one dis-
course marker. The limitation of clause length is also applied to avoid the noise from implicit dis-course relation. Based on a preliminary statistics, we find that most clauses in the Chinese part of the ClueWeb (Yu et al, 2012) are no longer than 20 Chinese characters shown in Figure 2. 
 Figure 2: Length distribution in the ClueWeb. 3.2 Annotation scheme Using the criteria described in Section 3.1, total 7,638 instances are randomly selected from the ClueWeb, and 87 native speakers annotate these instances. Each instance is shown to three anno-tators. The annotator labels the polarities of the first clause, the second clause, and the whole in-stance with Negative, Neutral, and Positive. In addition, the discourse relation between the two clauses is also labeled with Temporal, Contin-gency, Comparison, and Expansion. For each target sentence, the annotation is based on the information from the sentence only. The sen-tences are not given to annotators. Finally, the majority of each label is taken. For example, the 
72
polarity p1 of the first clause in the instance (S2) is labeled as Positive, the polarity p2 of the sec-ond clause is labeled as Negative, the resulting polarity pw of the whole sentence is also labeled as Negative, and the discourse relation between the two clauses is labeled as Comparison.    (S2) ???????????????????? (Although French brand cars share more than half of the domestic market share)?????????? (but the market share con-tinued to shrink)? The inter-agreements of p1, p2, pw, and dis-course relation among annotators are 0.49, 0.50, 0.47, and 0.41 in Fleiss? Kappa values, respec-tively (all are moderate agreement). The result-ing corpus is publicly available on the website of NTU Chinese Discourse Resources1.  4 Results and Discussion To investigate the corpus annotated with dis-course relation and sentiment polarity, we firstly give an overview of results with respect to these two types of linguistic phenomena. And then, the most frequent discourse markers for each class of discourse relations are discussed. Finally, we reorganize the results to several aspects and dis-cuss the association between discourse relations and sentiment polarities.  4.1 Overview of the annotated corpus The distribution of the discourse relations versus the polarities of whole sentence (pw) is shown in Table 2. Compared to the distributions of dis-course relations in the Penn Discourse Treebank (Prasad et al, 2008) shown in Table 3, the ex-plicit Chinese discourse corpus is more similar to the whole English corpus. The instances of Ex-pansion form the largest set among four dis-course relation classes. In Chinese, the instances of Expansion are even more. Temporal is the most infrequent relation which has close fre-quencies in both corpora. The different charac-teristic is the frequency of Comparison relation. In our Chinese corpus, the frequency of Compar-ison relation is about half of that in the PDTB.  In Table 2, the symbol ? is used to highlight the relatively major polarity of each relation. The symbol ? is marked when the polarity is the ma-jority (i.e., with a frequency greater than 50%). Near half (49.11%) of the instances belong to Neutral. Neutral statements are major in Tem-                                                1 http://nlg.csie.ntu.edu.tw/ntu-discourse/  
poral and Expansion classes. On the other hand, Comparison is the relation which is most in-volved in expressing sentiment, negative senti-ment in particular. Contingency is second to Comparison in expressing sentiment. The distribution of the discourse relations ver-sus (p1, p2), the sentiment polarity transitions be-tween two clauses, is shown in Table 4. Neutral-Neutral is the most frequent polarity transition in all relations. More than half of the Temporal in-stances are Neutral-Neutral. The reason may be that the Temporal relations are usually used in the sentences that describe the objective facts of the past, present, or the future. In such sentences, the sentiments are relatively rare. On the other hand, the sentences of Comparison and Contin-gency occur more in the critical and analytical scenarios. Although the most frequent transition of Com-parison is also Neutral-Neutral (23.14%), the other three types of transitions, Positive-Negative, Neutral-Negative, and Negative-Positive, have close frequencies of 22.71%, 16.90%, and 15.72%, respectively. Moreover, Negative polar-ity is involved in all these three transitions in one of their clauses. The relations between p1, p2, and pw are also interesting. Table 5 shows the top 10 most fre-quent correlations of the polarities (p1, p2, pw) of the first clause, the second clause, and the whole sentence. On the one hand, it is not surprising that most instances belong to (Neutral, Neutral, Neutral). On the other hand, it is worthy of not-ing that p2 and pw are identical in the top eight types of combinations in Table 5. In other words, the resulting sentiment polarity of a two-clause sentence is mostly consistent with the polarity of   Relation # % Neu  (%) Pos  (%) Neg  (%) Temporal 849 11.12 ?60.66 22.38 16.96 Contingency 1,598 20.92 ?44.74 26.97 28.29 Comparison 929 12.16 33.37 27.88 ?38.75 Expansion 4,262 55.80 ?51.88 31.75 16.38 Overall 7,638 100.00 ?49.11 29.24 21.65 Table 2: Distribution of discourse relations vs. polarities of whole sentences.  Relation Only Explicit Cases Total # % # % Temporal 3,612 18.88 4,650 12.71 Contingency 3,581 18.72 8,042 21.98 Comparison 5,516 28.83 8,394 22.94 Expansion 6,424 33.58 15,506 42.38 Overall 19,133 100.00 36,592 100.00 Table 3: Distribution of discourse relations in the Penn Discourse TreeBank 2.0. 
73
PDTB Class # Distribution of each type of sentiment polarity transition (p1, p2) (%) Neu Neu Pos Neu Neg Neu Neu Pos Pos Pos Neg Pos Neu Neg Pos Neg Neg Neg Temporal 849 ?57.01 1.53 2.12 16.37 3.53 2.36 12.72 1.06 3.30 Contingency 1,598 ?35.42 3.69 5.88 13.70 10.45 2.32 11.64 1.81 15.08 Comparison 929 ?23.14 2.69 2.48 8.61 3.12 15.72 16.90 22.71 4.63 Expansion 4,262 ?48.33 2.86 1.92 14.24 16.19 0.59 7.86 0.63 7.37 Overall 7,638 ?43.53 2.87 2.84 13.68 11.99 2.99 10.29 3.61 8.20 Table 4: Distribution of discourse relations vs. types of sentiment transitions.  p1 p2 pw Occurrences Neutral Neutral Neutral 3,268 Neutral Positive Positive 945 Positive Positive Positive 908 Neutral Negative Negative 706 Negative Negative Negative 614 Positive Negative Negative 204 Negative Positive Positive 199 Negative Neutral Neutral 125 Positive Neutral Positive 121 Neutral Positive Neutral 99 Table 5: Most frequent (p1, p2, pw) combinations.   p1 = pw p1 ? pw Total p2 = pw 62.71% 29.79% 92.50% p2 ? pw 5.51% 1.99% 7.50% Total 68.22% 31.78% 100.00% Table 6: Correlations between (p1,pw) and (p2,pw).  the second clause. Table 6 shows the correlations of sentiment polarities between clauses and the whole sentence. Total 92.50% of instances be-long to the case (p2 = pw), where the polarity of the second clause is identical to the polarity of the whole sentence. In Chinese writing, putting the important part of a sentence at the end of the sentence is very common.  4.2 Frequent discourse markers The top discourse markers in our Chinese corpus are shown in Table 7. For each PDTB class, the five most frequent discourse markers are listed. In each row of the table, its number of occur-rences and the distribution of its nine sentiment polarity transitions are given. Note that there are three polarities, i.e., positive, neutral, and nega-tive. The relatively major sentiment polarity tran-sition of each discourser maker is labeled with the symbol ?. The symbol ? is marked when the sentiment polarity is the majority, i.e., its ratio is greater than 50%. Some discourse markers are the top markers in more than one discourse relation such as ? (also) and ? (still). In the discourse marker dictionary, the word ?  (also) is defined as a discourse 
marker of the Expansion relation. However, this word is frequent in the instances of all the four relations. In different relations, the distributions of the sentiment transitions of this word differ. In other words, the word ?  (also), which is a common word in Chinese, is not only used as a discourse marker for emphasizing the Expansion relation, but also has various senses in other us-ages.  For instance, the word ? in (S3) is a dis-course marker to denote an Expansion relation, but it is a particle in (S4). In fact, (S4) is an in-stance of the implicit Contingency relation. We ignore all of instances of the word ? (also) in the following analysis since it is an outlier. (S3) ??????????? (This is an af-firmation of our work)??????????????(and also our encouragement and mo-tivation)? (S4) ??????? (The mind cannot be open to forward progress)???????? (the world becomes narrow)? The word ? (still) is another ambiguous dis-course marker. Besides the Expansion relation defined in the dictionary, it is sometimes used to denote the Temporal relation, especially in the negation context, e.g., ?? (not yet). The two frequent discourse markers of the Contingency relation, ?? (due to) and ?? (because) share the similar sense, and their dis-tributions of sentiment polarity transitions are more consistent than the other markers of the Contingency relation.  The most frequent discourse marker of the Comparison class is ? (but). The other two dis-course markers ? (but) and ?? (but) share the similar sense, however, their polarity distribu-tions differ significantly. Compared to the more general marker ?  (but), the second frequent marker ? (but) is bolder and more critical. (S5) is an example of the marker ? (but). As shown in our data, the marker ? (but) is likely to high-light the negative sentences. 
74
PDTB  Class Discourse Markers # Distribution of each type of sentiment polarity transition (%) Neu Neu Pos Neu Neg Neu Neu Pos Pos Pos Neg Pos Neu Neg Pos Neg Neg Neg Temporal ?? (and then) in Arg1 69 ?50.72 1.45 2.90 15.94 5.80 2.90 8.70 4.35 7.25 ? (also) in Arg2 50 ?44.00 2.00 2.00 18.00 6.00 0.00 20.00 0.00 8.00 ? (again) in Arg2 49 ?71.43 0.00 0.00 12.24 2.04 0.00 10.20 4.08 0.00 ? (still) in Arg2 46 ?58.70 0.00 0.00 10.87 8.70 0.00 17.39 0.00 4.35 ? (again) in Arg2 38 ?78.95 2.63 0.00 10.53 0.00 0.00 2.63 0.00 5.26 Contingency ?? (if) in Arg1 190 ?42.63 4.21 11.58 14.21 3.68 3.16 10.53 1.05 8.95 ?? (due to) in Arg1 82 ?31.71 2.44 2.44 4.88 18.29 3.66 13.41 1.22 21.95 ? (also) in Arg2 77 20.78 0.00 1.30 20.78 19.48 0.00 11.69 2.60 ?23.38 ?? (because ) in Arg1 70 ?28.57 4.29 7.14 7.14 10.00 2.86 18.57 4.29 17.14 ?? (in order to) in Arg1 62 ?50.00 14.52 1.61 6.45 9.68 1.61 8.06 6.45 1.61 Comparison ? (but) in Arg2 176 21.59 4.55 2.84 4.55 3.41 16.48 15.91 ?28.98 1.70 ? (but) in Arg2 85 11.76 0.00 2.35 4.71 1.18 10.59 22.35 ?42.35 4.71 ? (however) in Arg2 77 ?46.75 5.19 0.00 5.19 1.30 3.90 10.39 22.08 5.19 ? (also) in Arg2 44 ?31.82 0.00 2.27 6.82 15.91 13.64 18.18 2.27 9.09 ?? (but) in Arg2 44 15.91 4.55 0.00 0.00 2.27 25.00 11.36 ?40.91 0.00 Expansion ? (also) in Arg2 603 ?43.62 1.66 1.49 15.26 19.07 1.00 7.79 0.33 9.78 ? (still) in Arg2 231 ?50.65 2.60 0.87 11.26 14.72 0.87 9.96 0.43 8.66 ? (say) in Arg1 206 ?48.54 2.43 0.49 18.45 9.22 0.00 16.50 0.49 3.88 ? (and) in Arg2 191 ?54.45 3.14 0.52 10.47 25.65 0.00 4.19 0.00 1.57 ? (also) in Arg1 159 ?37.11 7.55 3.14 11.95 25.16 0.63 3.77 0.63 10.06 Table 7. Five most frequent discourse makers of each PDTB class in our corpus.   (S5) ???????????  (The new type of crime is so startling)????????????(but had never been disclosed before solved)? The other discourser marker ?? (but) is an emphasized version of the marker ? (but) so that it is more likely used in the stronger polarity transitions such as Positive-Negative and Nega-tive-Positive. In addition, the sense of the marker ? (however) is also similar to the sense of ? (but), but it is more frequent to be used in the neutral situations. These linguistic phenomena show that the synonyms may have different sen-timent usages in the real world. 4.3 Association between discourse relation and sentiment polarity To analyze the data at a higher level, we reor-ganize the sentiment transitions into several tran-sition categories from four aspects. The details are shown in Table 8. The first aspect is Polarity Tendency, which classifies the transitions into three categories, including Positive-Tendency, Neutral, and Negative-Tendency. This aspect reflects the overall polarity of both arguments. The Negative-Positive transition is considered as Positive-Tendency because the emphasis of a Chinese sentence is usually placed in the last clause. Similarly, the Positive-Negative transition is considered as Negative-Tendency. The second aspect is Polarity Change, which indicates if the polarities of both arguments are opposite. Only Negative-Positive and Positive-Negative are re-garded as Opposite. All the rest transitions are 
treated as NonOpposite. The third aspect is Di-rection, which captures the movement from the first clause to the second one. To-Positive stands for the transitions in which the polarity of the second clause is more positive than that of the first clause. On the other hand, To-Negative stands for the transitions in which the polarity of the second clause is less positive than that of the first clause. Equal stands for the cases in which the polarities of both clauses are identical. The last aspect is Negativity, which regards the polar-ity of an argument as binary values, i.e., Negative and NonNegative. In this way, we re-classify the nine-way sentiment polarity transitions into four transitions. In other words, both the polarity states Neutral and Positive are merged into one state NonNegative in this aspect. Such a binary scheme is also used in some related work, in which the negative polarity is distinguished and the rest are considered Positive (Kim and Hovy, 2004; Devitt and Ahmad, 2007). For each type of each aspect, five discourse markers that occur more than 10 times in the dataset and have the highest ratio of the corresponding type are listed in the fifth column of Table 8 as significant dis-course markers.  We analyze the annotations according to the four aspects, and the results are shown in Table 9. The chi-squared test is used to test the dependen-cy between the PDTB classes of discourse mark-ers and each aspect of sentiment transitions. The results show that no matter whether the senti-ment polarity transitions are categorized into Po-larity Tendency, Polarity Change, Direction, or Negativity, the classes of discourse relations are 
75
significantly dependent on the sentiment polari-ties of the arguments at p=0.001. In the aspect of Polarity Tendency, the ratios of Neutral in the Temporal and Expansion rela-tions are 57.01% and 48.33%, respectively, which are definitely higher than those of Contin-gency and Comparison relations. In other words, the two arguments of Contingency and Compari-son relations are less likely to be neutral. The ratio of Negative-Tendency of the Comparison relation is 46.72%. It confirms the Comparison relation is likely to be involved in negative statements. As shown in Table 8, three of the five significant discourse markers of Negative-Tendency are the synonyms of ? (but), which are discourse markers of  the Comparison rela-tion. The other two markers, ?? (otherwise) and ? (because), are discourse markers of the Contingency relation. Like the word otherwise in English, ?? (otherwise) is used for introducing what bad scenario will happen if something is not done. The marker ? (because) is not only a significant discourse marker of the category Negative-Tendency, but also a significant marker 
of Negative-Negative from the aspect of Negativ-ity. From the real data, we find this marker is often used in bad cause-and-effect statements. (S6) is an example. The usage of the other dis-course marker ??  (because), which is a syno-nyms of ? (because), is more general.  (S6) ????????? (Because the tow-el is without sunlight for a long time)?????????? (it is easy to breed bacteria and fungi)? The ratio of Opposite of Comparison relation from the aspect of Polarity Change is 38.43%. Although it is not as high as expected, it is the highest among the four PDTB classes and much higher than those of three other classes. Com-pared to the other classes, Comparison is most likely to have a pair of opposite arguments. Four of the five significant discourse markers of Opposite in Table 8 are the synonyms of ? (but). Expansion relation has the highest ratio of NonOpposite. This matches our expectation that the Expansion relation is used to concatenate several events which have similar properties   Aspect  Transition Category Sentiment polarity transitions Explanation Significant Discourse Markers Polarity Tendency Positive-Tendency Pos-Neu, Neu-Pos, Pos-Pos, Neg-Pos The two arguments present an overall positive polarity. ?? ...?  (not only... also), ?? (finally) , ?...? (now that... ), ?? ...?  (as long as... ), ?? (recently) Neutral Neu-Neu Both arguments are neutral.  ?? (and then), ?? (hence) , ?? (at the end), ? (so) , ?? (as well as) Negative-Tendency Pos-Neg, Neg-Neu, Neu-Neg, Neg-Neg The two arguments present an overall negative polarity. ??  (otherwise), ?  (but), ?? (but), ?? (but), ? (because) Polarity Change Opposite Neg-Pos, Pos-Neg The polarities of both arguments are opposite. ?? (but), ??...? (although...) ,  ? (but), ? (but), ?? (but)  NonOpposite Neu-Neu, Pos-Neu, Neg-Neu, Neu-Pos, Pos-Pos, Neu-Neg, Neg-Neg The polarities of both arguments are not opposite. ? (or), ? (as), ?? (moreover), ??...? (if ... may), ?? (say) Direction To-Positive Neg-Neu, Neg-Pos, Neu-Pos The second argument is less negative than the first one. ? ?  (finally), ? ? ... ? (although...) , ?? (recently), ?? ...?  (as long as...) , ?? (seem...) Equal Neg-Neg, Neu-Neu, Pos-Pos Both arguments are the same polarity value. ??...? (Not only... even), ?? (at the end), ?? (in addition), ? (so),  ?...? (now that...) To-Negative Pos-Neu, Pos-Neg, Neu-Neg The second argument is less positive than the first one. ? (but), ?? (but), ?? (but), ?? (otherwise), ??...? (even if...) Negativity NonNegative- NonNegative Neu-Neu, Neu-Pos, Pos-Neu, Pos-Pos Both arguments are not negative.  ??  (as well as), ??  (in the future),  ?? (in order to), ?? (in addition), ?? (and then) NonNegative- Negative Neu-Neg, Pos-Neg The first argument is not negative while the second argument is negative. ?  (but), ??  (otherwise), ?? (but), ??...?  (even if...), ?? (but) Negative-NonNegative Neg-Neu, Neg-Pos The first argument is negative while the second argument is not negative. ??...? (although...), ?? (but), ?? (but), ?? (finally), ? (but) Negative-Negative Neg-Neg Both arguments are negative. ?? (even), ? (but), ? (because), ??...? (if... may), ?? (but) 
76
Table 8: Aspects of sentiment transition. PDTB Class # Polarity Tendency (%) Polarity Change (%) Direction(%) Negativity (%) Pos Tend Neutral Neg Tend Oppo Non Oppo To Pos Eq. To Neg NonNeg-NonNeg NonNeg-Neg Neg-NonNeg Neg-Neg Tem 849 23.79 57.01 19.20 3.42 96.58 20.85 63.84 15.31 78.45 13.78 4.48 3.30 Con 1,598 30.16 35.42 34.42 4.13 95.87 21.90 60.95 17.15 63.27 13.45 8.20 15.08 Com 929 30.14 23.14 46.72 38.43 61.57 26.80 30.89 42.30 37.57 39.61 18.19 4.63 Exp 4,262 33.88 48.33 17.79 1.22 98.78 16.75 71.89 11.36 81.63 8.49 2.51 7.37 Table 9: Statistics of sentiment transition for each PDTB class over the corpus annotated by human.  from certain perspective. The ratio of To-Negative of Comparison rela-tion from the aspect of Direction in Table 9 is 42.30%, which is significantly higher than the ratios of To-Negative of the other classes. This also confirms the Comparison relation is likely to be used to express critical opinions. Further-more, the ratio of Equal of Comparison relations is much lower than those of other classes. This result shows the Comparison relation is more involved in sentiment polarity transitions. The Negativity aspect in Table 9 also shows the NonNegative-Negative is more likely to hap-pen than the Negative-NonNegative in all rela-tions. This statistics reflects a particular phenom-enon ?good words ahead? in Chinese.  That is, speakers tend to express a negative opinion after kind words. The sentiment polarity flips in the instances of the two categories Negative-NonNegative and NonNegative-Negative. However, the significant discourse markers of the two categories are very different. In spite of the general marker ?? (but), the discourse markers ? (but), ?? (oth-erwise), ??...? (even if...), and ?? (but) are often used in NonNegative-Negative, which usu-ally results a negative remark. On the other hand, the discourse markers??...? (although...), ?? (but), ?? (finally), and ? (but) are often used in Negative-NonNegative, which usually results a positive remark. For example, the dis-course marker ?? (finally), which is a dis-course marker of the Temporal relation, is usu-ally used when an event successfully accom-plished after twists and turns such as (S7). (S7) ???????????????	 (Domestic mobile phone giant Ningbo Bird after many tribulations)??????????? (finally successfully fought in the automotive industry)? 
5 Conclusion To investigate the discourse relation and the sen-timent polarity of Chinese discourse markers, we construct a moderate-sized corpus based on the Chinese part of ClueWeb09. In this paper, our annotation scheme and the analysis of the anno-tation results are shown. Total 7,638 instances are annotated by native speakers. The discourse relation distribution of the annotated data is comparable to the distribution of the well-known English discourse corpus PDTB 2.0. Through the data analysis, we validate certain human intui-tions in Chinese language. Near half of instances are in neutral sentiment while the Comparison relation is most likely to be involved in negative sentiment. Furthermore, the high sentiment de-pendency between the last clause and the whole sentence is validated in the data. The data shows the significant association be-tween the discourse relation and the sentiment polarity. The arguments of a Comparison rela-tion or a Contingency relation are more likely to be involved in expressing sentiment. Moreover, the Comparison relation often occurs in the sen-tences with sentiment polarity transitions, and frequently occurs in the instances with the nega-tive sentiment. On the other hand, the arguments of the Temporal and the Expansion relations are relatively objective. The behavior of word choice between synonyms is also observed in the data. Each synonym of a sense may have its own us-age in expressing sentiment. This paper points out the ambiguities of the discourse markers in Chinese.  That is, a marker may suggest more than one discourse relation. Besides, words may have both the functions of discourse connectives and non-discourse ones in their surface forms. These two issues make the interpretation of Chinese discourse markers more challenging. Determination of their correct uses and disambiguation of their discourse functions will be investigated in the future.  
77
Acknowledgments 
This research was partially supported by Excel-lent Research Projects of National Taiwan Uni-versity under contract 102R890858 and 2012 Google Research Award. References Lynn Carlson and Daniel Marcu. 2001. Discourse Tagging Reference Manual.   http://www.isi.edu/~marcu/discourse/tagging-ref-manual.pdf Lynn Carlson, Daniel Marcu, and Mary Ellen Oku-rowski. 2002. RST Discourse Treebank. Linguistic Data Consortium, Philadelphia.  Shou-Yi Cheng. 2006. Corpus-Based Coherence Re-lation Tagging in Chinese Discourse. Master?s Thesis, National Chiao Tung University, Hsinchu, Taiwan. Xianghui Cheng and Xiaolin Tian. 1989. Xian dai Han yu (????), San lian shu dian (????), Hong Kong. Ann Devitt and Khurshid Ahmad. 2007. Sentiment polarity identification in financial news: a cohe-sion-based approach. In Proceedings of the 45th Annual Meeting of the Association of Computa-tional Linguistics (ACL 2007), pages 984-991, Pra-gue, Czech Republic. Hugo Hernault, Helmut Prendinger, David A. duVerle, and Mitsuru Ishizuka. 2010. HILDA: A Discourse Parser Using Support Vector Machine Classifica-tion. Dialogue and Discourse, 1(3): 1-33. Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese discourse relation recognition. In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011). pages 1442-1446, Chiang Mai, Thailand. Hen-Hsen Huang and Hsin-Hsi Chen. 2012a. Contin-gency and comparison relation labeling and struc-ture prediction in Chinese sentences. In Proceed-ings of the 13th Annual Meeting of the Special In-terest Group on Discourse and Dialogue (SIGDI-AL 2012), pages 261-269, Seoul, South Korea. Hen-Hsen Huang and Hsin-Hsi Chen. 2012b. An An-notation System for Development of Chinese Dis-course Corpus. In Proceedings of the 24th Interna-tional Conference on Computational Linguistics (COLING 2012): Demonstration Papers, pages 223-230, Mumbai, India. Ben Hutchinson. 2004. Acquiring the meaning of dis-course markers. In Proceedings of the 42nd Annual Meeting of the Association for Computational Lin-guistics (ACL 2004), pages 684-691, Barcelona, Spain. 
Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04), pages 1367-1373, Ge-neva, Switzerland. Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), pages 343-351. Shuxiang Lu. 2007. Eight Hundred Words of The Contemporary Chinese (Xian dai Han yu Ba bai Ci), China Social Sciences Press. Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in In-formation Retrieval, 2(1-2): 1-135. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2007. The Penn Discourse Tree-bank 2.0 Annotation Manual. The PDTB Research Group. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Tree-Bank 2.0. In Proceedings of the 6th Language Re-sources and Evaluation Conference (LREC 2008), pages 2961-2968, Marrakech, Morocco. Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical in-formation. In Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Lin-guistics (HLT/NAACL 2003), pages 149-156, Ed-monton, Canada. Florian Wolf and Edward Gibson. 2005. Representing Discourse Coherence: A Corpus-Based Analysis. Computational Linguistics, 31(2): 249-287. Chi-Hsin Yu, Yi-jie Tang and Hsin-Hsi Chen. 2012. Development of a web-scale Chinese word N-gram corpus with parts of speech information. In Pro-ceedings the 8th International Conference on Lan-guage Resources and Evaluation (LREC 2012), pages 320-324, Istanbul, Turkey. Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su, and Chew Lim Tan. 2010. Predicting discourse connectives for implicit discourse relation recogni-tion. In Proceedings of the 23rd International Con-ference on Computational Linguistics (COLING 2010): Posters, pages 1507-1514. Yuping Zhou and Nianwen Xue. 2012. PDTB-style discourse annotation of Chinese text. In Proceed-ings the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 69-77, Jeju, South Korea. 
78
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 117?122,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Uses of Monolingual In-Domain Corpora for Cross-Domain  
Adaptation with Hybrid MT Approaches 
 
 
An-Chang Hsieh, Hen-Hsen Huang and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan 
{achsieh,hhhuang}@nlg.csie.ntu.edu.tw;hhchen@ntu.edu.tw 
 
  
 
Abstract 
Resource limitation is challenging for cross-
domain adaption. This paper employs patterns 
identified from a monolingual in-domain cor-
pus and patterns learned from the post-edited 
translation results, and translation model as 
well as language model learned from pseudo 
bilingual corpora produced by a baseline MT 
system. The adaptation from a government 
document domain to a medical record 
domain shows the rules mined from the 
monolingual in-domain corpus are useful, 
and the effect of using the selected pseudo 
bilingual corpus is significant.   
1 Introduction 
Bilingual dictionary and corpus are important 
resources for MT applications. They are used for 
lexical choice and model construction. However, 
not all resources are available in bilingual forms 
in each domain. For example, medical records 
are in English only in some countries. In such a 
case, only bilingual dictionary and monolingual 
corpus is available. Lack of bilingual corpus 
makes domain adaptation more challenging.  
A number of adaptation approaches (Civera 
and Juan, 2007; Foster and Kuhn 2007; Foster et al, 
2010, Matsoukas et al, 2009; Zhao et al, 2004) 
have been proposed. They address the reliability 
of a model in a new domain and count the do-
main similarities between a model and the in-
domain development data. The domain relevance 
in different granularities including words, 
phrases, sentences, documents and corpora are 
considered. Ueffing et al (2007) propose semi-
supervised methods which use monolingual data 
in source language to improve translation per-
formance. Schwenk (2008) present lightly-
supervised training to generate additional train-
ing data from the translation results of monolin-
gual data. To deal with the resource-poor issue, 
Bertoldi and Federico (2009) generate a pseudo 
bilingual corpus from the monolingual in-domain 
corpus, and then train a translation model from 
the pseudo bilingual corpus.   
Besides counting similarities and generating 
pseudo bilingual in-domain corpus, text simplifi-
cation (Zhu et al, 2010; Woodsend and Lapata, 
2011; Wubben et al, 2012) is another direction. 
Simplifying a source language text makes the 
translation easier in a background MT system. 
Chen et al (2012a) propose a method to simplify 
a sentence before MT and to restore the transla-
tion of the simplified part after MT. They focus 
on the treatments of input text only, but do not 
consider how to adapt the background MT to the 
specific domain. The translation performance 
depends on the coverage of the simplification 
rules and the quality of the background system. 
This paper adopts the simplification-
translation-restoration methodology (Chen et al, 
2012a), but emphasizes on how to update bilin-
gual translation rules, translation model and lan-
guage model, which are two kernels of rule-
based and statistics-based MT systems, respec-
tively. This paper is organized as follows. Sec-
tion 2 specifies the proposed hybrid MT ap-
proaches to resource-limited domains. The char-
acteristics of available resources including their 
types, their linguality, their belonging domains, 
and their belonging languages are analyzed and 
their uses in translation rule mining and model 
construction are presented. Section 3 discusses 
how to adapt an MT system from a government 
document domain to a medical record domain. 
The experimental setups reflect various settings. 
Section 4 concludes the remarks. 
 
117
 Figure 1: Hybrid MT Approaches 
 
2 Hybrid MT Approaches 
Figure 1 sketches the overall picture of our pro-
posed hybrid MT approaches. A resource is rep-
resented in terms of its linguality, domain, lan-
guage, and type, where MO/BI denotes mono-
lingual/bilingual, ID/OD denotes in-domain/out-
domain, and SL/TL denotes source lan-
guage/target language. For example, an MO-ID-
SL corpus and an MO-ID-TL corpus mean mon-
olingual in-domain corpora in source and in tar-
get languages, respectively. Similarly, a BI-OD 
corpus and a BI-ID dictionary denote a bilingual 
out-domain corpus, and a bilingual in-domain 
dictionary, respectively.   
Resources may be provided by some organi-
zations such as LDC, or collected from hetero-
geneous resources. The MO-ID-SL/TL corpus, 
the BI-OD corpus, and the BI-ID dictionary be-
long to this type. Besides, some outputs generat-
ed by the baseline MT systems are regarded as 
other kinds of resources for enhancing the pro-
posed methods incrementally. Initial translation 
results, selected translation results, and post-
edited translation results, which form pseudo 
bilingual in-domain corpora, belong to this type.   
The following subsections first describe the 
baseline systems with the original resources and 
then specify the advanced systems with the gen-
erated resources. 
2.1 A baseline translation system 
In an extreme case, only a bilingual out-domain 
corpus, a monolingual in-domain corpus in 
source/target language, a bilingual in-domain 
dictionary and a monolingual in-domain thesau-
rus in source language are available. The bilin-
gual out-domain corpus is used to train transla-
tion and language models by Moses. They form 
a background out-domain translation system. 
A pattern miner is used to capture the written 
styles in the monolingual in-domain corpus in 
source language. A monolingual in-domain the-
saurus in source language is looked up to extract 
the class (sense) information of words. Mono-
lingual patterns are mined by counting frequent 
word/class n-grams. Then, the bilingual in-
domain dictionary is introduced to formulate 
translation rules based on the mined monolin-
gual patterns. Here in-domain experts may be 
involved in reviewing the bilingual rules. The 
human cost will affect the number of translation 
rules formulated and thus its coverage. 
The baseline translation system is composed 
of four major steps shown as follows. (1) and (2) 
are pre-processing steps before kernel MT, and 
(4) is a post-processing step after kernel MT. 
(1) Identifying and translating in-domain 
segments from an input sentence by using 
translation rules. 
118
(2) Simplifying the input sentence by replac-
ing the in-domain segments as follows. 
(a) If an in-domain segment is a term in 
the bilingual in-domain dictionary, 
we find a related term (i.e., hypernym 
or synonym) in the in-domain thesau-
rus which has relatively more occur-
rences in the background SMT sys-
tem to replace the term. 
(b) If an in-domain segment is a noun 
phrase, we keep its head only, and 
find a related term of the head as (a). 
(c) If an in-domain segment is a verb 
phrase composed of a verb and a 
noun phrase, we keep the verb and 
simplify the noun phrase as (b). 
(d) If an in-domain segment is a verb 
phrase composed of a verb and a 
prepositional phrase, we keep the 
verb and remove the prepositional 
phrase if it is optional. If the preposi-
tional phrase is mandatory, it is kept 
and simplified as (e). 
(e) If an in-domain segment is a preposi-
tional phrase, we keep the preposition 
and simplify the noun phrase as (b). 
(f) If an in-domain segment is a clause, 
we simplify its children recursively as 
(a)-(e). 
(3) Translating the simplified source sentence 
by using the out-domain background MT 
system. 
(4) Restoring the results of the bilingual in-
domain segments translated in (1) back to 
the translation results generated in (3).  
The restoration is based on the internal 
alignment between the source and the tar-
get sentences. 
2.2 Incremental learning 
There are several alternatives to update the base-
line translation system incrementally. The first 
consideration is the in-domain translation rules.  
They are formed semi-automatically by domain 
experts.  The cost of domain experts results that 
only small portion of n-gram patterns along with 
the corresponding translation are generated. The 
post-editing results suggests more translation 
rules and they are fed back to revise the baseline 
translation system. 
The second consideration is translation model 
and language model in the Moses. In an ideal 
case, the complete monolingual in-domain cor-
pus in source language is translated by the base-
line translation system, then the results are post-
edited by domain experts, and finally the com-
plete post-edited bilingual corpus is fed back to 
revise both translation model and language 
model. However, the post-editing cost by do-
main experts is high. Only some samples of the 
initial translation are edited by domain experts.  
On the one hand, the sampled post-edited in-
domain corpus in target language is used to re-
vise the language model.  On the other hand, the 
in-domain bilingual translation result before 
post-editing is used to revise the translation 
model and the language model. Size and transla-
tion quality are two factors to be considered. We 
will explore the effect of different size of imper-
fect in-domain translation results on refining the 
baseline MT system.  Moreover, a selection 
strategy, e.g., only those translation results com-
pletely in target language are considered, is in-
troduced to sample ?relatively more accurate? 
bilingual translation results. 
In the above incremental learning, translation 
rules, translation model and language model are 
revised individually.  The third consideration is 
to merge some refinements together and exam-
ine their effects on the translation performance. 
3 Cross-Domain Adaptation  
To evaluate the feasibility of the proposed hy-
brid MT approaches, we adapt an English-
Chinese machine translation system from a gov-
ernment document domain to a medical record 
domain. The linguistic resources are described 
first and then the experimental results. 
3.1 Resource description 
Hong Kong parallel text (LDC2004T08), which 
contains official records, law codes, and press 
releases of the Legislative Council, the Depart-
ment of Justice, and the Information Services 
Department of the HKSAR, respectively, and 
UN Chinese-English Parallel Text collection 
(LDC2004E12) is used to train the translation 
model. These two corpora contain total 6.8M 
sentences. The Chinese counterpart of the above 
parallel corpus and the Central News Agency 
part of the Tagged Chinese Gigaword 
(LDC2007T03) are used to train trigram lan-
guage model. These two corpora contain total 
18.8M sentences. The trained models are used in 
Step (3) of the baseline translation system. 
Besides the out-domain corpora for the devel-
opment of translation model and language model, 
we select 60,448 English medical records (1.8M 
sentences) from National Taiwan University 
119
Hospital (NTUH) to learn the n-gram patterns. 
Metathesaurus of the Unified Medical Language 
System (UMLS) provides medical classes of in-
domain words. A bilingual medical domain dic-
tionary composed of 71,687 pairs is collected. 
Total 7.2M word/class 2-grams~5-grams are 
identified. After parsing, there remain 57.2K 
linguistic patterns. A higher order pattern may 
be composed of two lower order patterns. Keep-
ing the covering patterns and ruling out the cov-
ered ones further reduce the size of the extracted 
patterns. The remaining 40.1K patterns are 
translated by dictionary look-up.  Because of the 
high cost of medical record domain experts (i.e., 
physicians), only a small portion is verified. Fi-
nally, 981 translation rules are formulated.  They 
are used in Step (1) of the baseline MT system. 
The detail rule mining and human correction 
process please refer to Chen et al (2012b). 
We further sample 2.1M and 1.1M sentences 
from NTUH medical record datasets, translate 
them by the baseline MT system, and get 2.1M- 
and 1.1M-pseudo bilingual in-domain corpora. 
We will experiment the effects of the corpus size. 
On the other hand, we apply the selection strate-
gy to select 0.95M ?good? translation from 
2.1M-pseudo bilingual in-domain corpus.  Fur-
thermore, some other 1,004 sentences are post-
edited by the domain experts. They are used to 
learn the advanced MT systems. 
To evaluate the baseline and the advanced 
MT systems, we sample 1,000 sentences differ-
ent from the above corpora as the test data, and 
translate them manually as the ground truth.  
3.2 Results and discussion 
Table 1 lists the methods along with the re-
sources they used. B is the baseline MT system. 
Most patterns appearing in the 57.2K learned n-
grams mentioned in Section 3.1 are not reviewed 
by physicians due to their cost. Part of these un-
reviewed patterns may occur in the post-edited 
data. They will be further introduced into M1. In 
the experiments, patterns appearing at least two 
times in the post-edited result are integrated into 
M1. Total 422 new patterns are identified. 
Translation model and language model in M1 is 
the same as those in baseline system.   
In M2-M6, the translation rules are the same 
as those in baseline MT system, only translation 
model and/or language model are re-trained. In 
 
 Translation Rules Translation Model Language Model Tuning Data 
B 981 bilingual translation rules 6.8M government domain bilingual 
sentences 
18.8M government/news domain 
Chinese sentences 
1000 government domain 
bilingual sentences 
M1 981 bilingual translation rules + 
422 mined  rules from post-
editing 
6.8M government domain bilingual 
sentences 
18.8M government/news domain 
Chinese sentences 
200 post-edited medical 
domain sentences 
M2 981 bilingual translation rules 6.8M government domain bilingual 
sentences 
804 post-edited Chinese sentences 200 post-edited medical 
domain sentences 
M3 981 bilingual translation rules 6.8M government domain bilingual 
sentences 
30,000 Chinese sentences selected 
from medical literature 
200 post-edited medical 
domain sentences 
M4 981 bilingual translation rules 1.1M pseudo medical domain bilingual 
sentences generated by M1 
1.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M5 981 bilingual translation rules 2.1M pseudo medical domain bilingual 
sentences generated by M1 
2.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M6 981 bilingual translation rules 0.95M selected pseudo medical do-
main bilingual sentences generated by 
M1 
0.95M selected pseudo medical do-
main Chinese sentences generated by 
M1 
200 post-edited medical 
domain sentences 
M12 981 bilingual translation rules + 
422 mined  rules from post-
editing 
6.8M government domain bilingual 
sentences 
804 post-edited Chinese sentences 200 post-edited medical 
domain sentences 
M13 981 bilingual translation rules + 
422 mined  rules from post-
editing 
6.8M government domain bilingual 
sentences 
30,000 medical domain Chinese sen-
tences 
200 post-edited medical 
domain sentences 
M14 981 bilingual translation rules + 
422 mined  rules from post-
editing 
1.1M pseudo medical domain bilingual 
sentences generated by M1 
1.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M15 981 bilingual translation rules + 
422 mined  rules from post-
editing 
2.1M pseudo medical domain bilingual 
sentences generated by M1 
2.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M16 981 bilingual translation rules + 
422 mined  rules from post-
editing 
0.95M selected pseudo medical do-
main bilingual sentences generated by 
M1 
0.95M selected pseudo medical do-
main Chinese sentences generated by 
M1 
200 post-edited medical 
domain sentences 
Table 1: Resources used in each hybrid MT method 
 
Method Bleu Method Bleu Method Bleu Method Bleu Method Bleu Method Bleu 
B 28.04 M2 39.45 M3 32.03 M4 34.86 M5 35.09 M6 40.48 
M1 39.72 M12 39.72 M13 32.85 M14 35.11 M15 35.52 M16 40.71 
Table 2: BLEU of each hybrid MT method 
120
M2, 804 post-edited sentences are used to train a 
new language model, without changing the 
translation model. In M3, paper abstracts in 
medical domain are used to derive a new lan-
guage model. M4, M5 and M6 are similar except 
that different sizes of corpora are used.  M4 and 
M5 use 1.1M and 2.1M sentences, respectively, 
while M6 uses 0.95M sentences chosen by using 
the selection strategy. M12-M16 are combina-
tions of M1 and M2-M6, respectively. Transla-
tion rules, translation model and language model 
are refined by using different resources. Total 
200 of the 1,004 post-edited sentences are se-
lected to tune the parameters of Moses in the 
advanced methods. 
Table 2 shows the BLEU of various MT 
methods. The BLEU of the MT system without 
employing simplification-translation-restoration 
methodology (Chen et al, 2012a) is 15.24. Ap-
parently, the method B, which employs the 
methodology, achieves the BLEU 28.04 and is 
much better than the original system. All the 
enhanced systems are significantly better than 
the baseline system B by t-test (p<0.05). Com-
paring M1 and M12-M16 with the correspond-
ing systems, we can find that introducing the 
mined patterns has positive effects. M1 is even 
much better than B. Although the number of the 
post-edited sentences is small, M2 and M12 
show such a resource has the strongest effects. 
The results of M3 and M13 depict that 30,000 
sentences selected from medical literature are 
not quite useful for medical record translation. 
Comparing M4 and M5, we can find larger 
pseudo corpus is useful.  M6 shows using the 
selected pseudo subset performs much better. 
Comparing the top 4 methods, the best method, 
M16, is significantly better than M12 and M1 
(p<0.05), but is not different from M6 signifi-
cantly (p=0.1662). 
We further analyze the translation results of 
the best methods M6 and M16 from two per-
spectives.  On the one hand, we show how the 
mined rules improve the translation. The follow-
ing list some examples for reference.  The un-
derlined parts are translated correctly by new 
mined patterns in M16. 
(1) Example: Stenting was done from distal 
IVC through left common iliac vein to ex-
ternal iliac vein. 
M6: ????? ? ? ?? ???? 
?? ? ????? ? ???? ? 
M16: ?? ????? ? ?? ???
? ?? ? ????? ? ???? ? 
(2) Example: We shifted the antibiotic to 
cefazolin. 
M6: ?? ? ??? ???? ? 
M16: ?? ? ??? ?? ? ???
? ? 
(3) Example: Enhancement of right side pleu-
ral, and mild pericardial effusion was not-
ed . 
M6: ?? ?? ? ?? ?? ? ? ?? 
? ??   ? ???? ? 
M16: ?? ? ? ?? ? ?? ???? 
? ??? ? 
On the other hand, we touch on which factors 
affect the translation performance of M16. Three 
factors including word ordering errors, word 
sense disambiguation errors and OOV (out-of-
vocabulary) errors are addressed as follows.  
The erroneous parts are underlined. 
(1) Ordering errors 
Example: Antibiotics were discontinued 
after 8 days of treatment. 
M16: ??? ?? ? 8? ? ?? ? 
Analysis: The correct translation result is 
?8 ? ? ?? ? ??? ????The 
current patterns are 2-5 grams, so that the 
longer patterns cannot be captured. 
(2) Word sense disambiguation errors 
Example: After tracheostomy, he was 
transferred to our ward for post operation 
care. 
M16: ????? ? ? ? ? ??? ?
? ?? ? ?? ?? ?? ? 
Analysis: The correct translation of ?post 
operation care? should be ??????.  
However, the 1,004 post-edited sentences 
are still not large enough to cover the pos-
sible patterns. Incremental update will in-
troduce more patterns and may decrease 
the number of translation errors. 
(3) OOV errors 
Example: Transcatheter intravenous uro-
kinase therapy was started on 1/11 for 24 
hours infusion. 
M16: transcatheter ?? ??? ? 1/11 
?? ?? ?? 24 ?? ?? ? 
Analysis: The word ?transcatheter? is an 
OOV. Its translation should be???".   
4 Conclusion 
This paper considers different types of resources 
in cross-domain MT adaptation. Several meth-
ods are proposed to integrate the mined transla-
121
tion rules, translation model and language model. 
The adaptation experiments show that the rules 
mined from the monolingual in-domain corpus 
are useful, and the effect of using the selected 
pseudo bilingual corpus is significant. 
Several issues such as word ordering errors, 
word sense disambiguation errors, and OOV 
errors still remain for further investigation in the 
future. 
 
Acknowledgments 
This work was partially supported by National 
Science Council (Taiwan) and Excellent Re-
search Projects of National Taiwan University 
under contracts NSC101-2221-E-002-195-MY3 
and 102R890858. We are very thankful to Na-
tional Taiwan University Hospital for providing 
NTUH the medical record dataset. 
References  
N. Bertoldi and M. Federico. 2009. Domain adapta-
tion for statistical machine translation with mono-
lingual resources. In Proceedings of the Fourth 
Workshop on Statistical Machine Translation, 
pages 182?189. 
H.B. Chen, H.H. Huang, H.H. Chen and C.T. Tan. 
2012a. A simplification-translation-restoration 
framework for cross-domain SMT applications. In 
Proceedings of COLING 2012, pages 545?560. 
H.B. Chen, H.H. Huang, J. Tjiu, C.Ti. Tan and H.H. 
Chen. 2012b. A statistical medical summary trans-
lation system. In Proceedings of 2012 ACM 
SIGHIT International Health Informatics Sympo-
sium, pages. 101-110. 
J. Civera and A. Juan. 2007. Domain adaptation in 
statistical machine translation with mixture model-
ing. In Proceedings of the Second Workshop on 
Statistical Machine Translation, pages 177?180. 
G. Foster and R. Kuhn. 2007. Mixture-model adapta-
tion for SMT. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 
128?135. 
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrimina-
tive instance weighting for domain adaptation in 
statistical machine translation. In Proceedings of 
EMNLP 2010, pages 451?459. 
S. Matsoukas, A.I. Rosti, and B. Zhang. 2009. Dis-
criminative corpus weight estimation for machine 
translation. In Proceedings of EMNLP 2009, pages 
708?717. 
H. Schwenk. 2008. Investigations on large-scale 
lightly-supervised training. In Proceedings of 
IWSLT 2008, pages 182?189. 
N. Ueffing, G. Haffari and A. Sarkar. 2007. Trans-
ductive learning for statistical machine translation. 
In Proceedings of the 45th Annual Meeting of the 
Association of Computational Linguistics, pages 
25?32, 
K. Woodsend and M. Lapata. 2011. Learning to sim-
plify sentences with quasi-synchronous grammar 
and integer programming. In Proceedings of 
EMNLP 2011, pages 409?420. 
S. Wubben and  A. van den Bosch, and  E. Krahmer. 
2012. Sentence simplification by monolingual ma-
chine translation. In Proceedings of ACL 2012, 
pages 1015?1024. 
B. Zhao, M. Eck, M. and S. Vogel. 2004. Language 
model adaptation for statistical machine translation 
via structured query models. In Proceedings of 
COLING 2004, pages 411?417. 
Z. Zhu, D. Bernhard, and I. Gurevych. 2010. A mon-
olingual tree-based translation model for sentence 
simplification. In Proceedings of COLING 2010, 
pages 1353?1361. 
122
