Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 837?846, Prague, June 2007. c?2007 Association for Computational Linguistics 
Extracting Data Records from Unstructured Biomedical Full Text 
Donghui Feng       Gully Burns       Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, burns, hovy}@isi.edu 
 
 
Abstract 
In this paper, we address the problem of 
extracting data records and their attributes 
from unstructured biomedical full text. 
There has been little effort reported on this 
in the research community. We argue that 
semantics is important for record extraction 
or finer-grained language processing tasks. 
We derive a data record template including 
semantic language models from unstruc-
tured text and represent them with a dis-
course level Conditional Random Fields 
(CRF) model. We evaluate the approach 
from the perspective of Information Extrac-
tion and achieve significant improvements 
on system performance compared with 
other baseline systems. 
1 Introduction 
The discovery and extraction of specific types of 
information, and its (re)structuring and storage into 
databases, are critical tasks for data mining, 
knowledge acquisition, and information integration 
from large corpora or heterogeneous resources 
(e.g., Muslea et al, 2001; Arasu and Garcia-
Molina, 2003). For example, webpages of products 
on Amazon may contain a list of data records such 
as books, watches, and electronics. Automatic 
extraction of individual records will facilitate the 
access and management of data resources. 
Most current approaches address this problem 
for structured or semi-structured text, for instance, 
from XML format files or lists and/or tabular data 
records on webpages (e.g., Liu et al, 2003; Zhu et 
al., 2006). The techniques applied rely strongly on 
the analysis of document structure derived from 
the webpage?s html tags (e.g., the DOM tree 
model). 
Regarding unstructured text, most Information 
Extraction (IE) work has focused on named entities 
(people, organizations, places, etc.). Such IE treats 
each extracted element as a separate record. Much 
less work has focused on the case where several 
related pieces of information have to be extracted 
to jointly comprise a single data record. In this 
work, it is usually assumed that there is only one 
record for each document (e.g., Kristjannson et al, 
2004). Almost no work tries to extract multiple 
data records from a single document. Multiple data 
records can be scattered across the narrative in free 
text. The problem becomes much harder as there 
are no explicit boundaries between data records 
and no heavily indicative format features (like html 
tags) to utilize. 
With the exponential increase of unstructured 
text resources (e.g., digitalized publications, papers 
and/or technical reports), knowledge needs have 
made it a necessity to explore this problem. For 
example, biomedical papers contain numerous ex-
periments and findings. But the large volume and 
rate of publication have made it infeasible to read 
through the articles and manually identify data re-
cords and attributes. 
We present a study to extract data records and 
attributes from the biomedical research literature. 
This is part of an effort to develop a Knowledge 
Base Management System to benefit neuroscience 
research. Specifically we are interested in knowl-
edge of various aspects (attributes) of Tract-tracing 
Experiments (TTE) (data records) in neuroscience. 
The goal of TTE experiments is to chart the inter-
connectivity of the brain by injecting tracer chemi-
cals into a region of the brain and identifying cor-
responding labeled regions where the tracer is 
837
  
Figure 1. An example of data records and attributes in a research article. 
taken up and transported to (Burns et al, 2007). 
To extract data records from the research litera-
ture, we need to solve two sub-problems: discover-
ing individual attributes of records and grouping 
them into one or more individual records, each re-
cord representing one TTE experiment. Each at-
tribute may contain a list of words or phrases and 
each record may contain a list of attributes.  
Listing each sentence from top to bottom, we 
call the first problem the Horizontal Problem (HP) 
and the second the Vertical Problem (VP). Figure 
1 provides an example of a TTE research article 
with colored fragments representing attributes and 
dashed frames representing data records. For in-
stance, the third dashed frame represents one ex-
periment record having three attributes with corre-
sponding biological interpretations: ?no labeled 
cells?, ?the DCN?, and ?the contralateral AVCN?. 
We view the HP and VP problems as two se-
quential labeling problems and describe our ap-
proach using two-level Conditional Random Fields 
(CRF) (Lafferty et al, 2001) models to extract data 
records and their attributes.  
The HP problem (finding individual attribute 
values) is solved using a sentence-level CRF label-
ing model that integrates a rich set of linguistic 
features. For the VP problem, we apply a dis-
course-level CRF model to identify individual ex-
periments (data records). This model utilizes deep 
semantic knowledge from the HP results (attribute 
labels within sentences) together with semantic 
language models and achieves significant im-
provements over baseline systems.  
This paper mainly focuses on the VP problem, 
since linguistic features for the HP problem is the 
general IE topic of much past research (e.g., Peng 
and McCallum, 2004). We apply various feature 
combinations to learn the most suitable and indica-
tive linguistic features. 
The remainder of this paper is organized as fol-
lows: in the next section we discuss related work. 
Following that, we present the approach to extract 
data records in Section 3. We give extensive ex-
perimental evaluations in Section 4 and conclude 
in Section 5. 
2 Related Work 
As mentioned, data record extraction has been 
extensively studied for structured and semi-
structured resources (e.g., Muslea et al, 2001; 
Arasu and Garcia-Molina, 2003; Liu et al, 2003; 
Zhu et al, 2006). Most of those approaches rely on 
the analysis of document structure (reflected in, for 
example, html tags), from which record templates 
are derived. However, this approach does not apply 
to unstructured text. The reason lies in the 
difficulty of representing a data record template in 
free text without formatting tags and integrating it 
838
 into a learning system. We show how to address 
this problem by deriving data record templates 
through language analysis and representing them 
with a discourse level CRF model. 
Given the problem of identifying one or more 
records in free text, it is natural to turn toward text 
segmentation. The Natural Language Processing 
(NLP) community has come up with various 
solutions towards topic-based text segmentation 
(e.g., Hearst, 1994; Choi, 2000; Malioutov and 
Barzilay, 2006). Most unsupervised text 
segmentation approaches work under optimization 
criteria to maximize the intra-segment similarity 
and minimize the inter-segment similarity based on 
word distribution statistics. However, this 
approach cannot be applied directly to data record 
extraction. A careful study of our corpus shows 
that data records share many words and phrases 
and are not distinguishable based on word 
similairties. In other words, different experiments 
(records) always belong to the same topic and there 
is no way to segment them using standard topic 
segmentation techniques (even if one views the 
problem as a finer-level segmentation than 
traditional text segmentation). In addition, most 
text segmentation approaches require a 
prespecified number of segments, which in our 
domain cannot be provided. 
(Wick et al, 2006) report extracting database re-
cords by learning record field compatibility. How-
ever, in our case, the field compatibility is hard to 
distinguish even by a human expert. Cluster-based 
or pairwise field similarity measures do not apply 
to our corpora without complex knowledge reason-
ing. Most of Wick et al?s data (faculty and stu-
dent?s homepages) contains one record. 
In addition, as explained below, we have found 
that surface word statistics alone are not sufficient 
to derive data record templates for extraction. 
Some (limited) form of semantic understanding of 
text is necessary. We therefore first perform some  
sentence level extraction (following the HP 
problem) and then integrate semantic labels and 
semantic language model features into a discourse 
level CRF model to represent the template for 
extracting data records in the future. 
Recently an increasing number of research ef-
forts on text mining and IE have used CRF models 
(e.g., Peng and McCallum, 2004). The CRF model 
provides a compact way to integrate different types 
of features when sequential labeling is important. 
Recent work includes improved model variants 
(e.g., Jiao et al, 2006; Okanohara et al, 2006) and 
applications such as web data extraction (Pinto et 
al., 2003), scientific citation extraction (Peng and 
McCallum, 2004), and word alignment (Blunsom 
and Cohn, 2006). But none of them have used 
CRFs for discourse level data record extraction. 
We use a CRF model to represent a data record 
template and integrate various knowledge as CRF 
features. Instead of traditional work on the sen-
tence level, our focus here is on the discourse level. 
As this has not been carefully explored, we ex-
periment with various selected features. 
For the biomedical domain, our work will facili-
tate biomedical research by supporting the con-
struction of Knowledge Base Management Sys-
tems (e.g., Stephan et al, 2001; Hahn et al, 2002; 
Burns and Cheng, 2006). Unlike the well-studied 
problem of relation extraction from biomedical 
text, our work focuses on grouping extracted at-
tributes across sentences into meaningful data re-
cords. TTE experiment is only one of many ex-
perimental types in biology. Our work can be gen-
eralized to many different types of data records to 
facilitate biology research. 
In the next section, we present our approach to 
extracting data records. 
3 Extracting Data Records 
Inspired by the idea of Noun Phrase (NP) chunking 
in a single sentence, we view the data records 
extraction problem as discourse chunking from a 
sequence of sentences using a sequential labeling 
CRF model. 
3.1 Sequential Labeling Model: CRF 
The CRF model addresses the problem of labeling 
sequential tokens while relaxing the strong 
independence assumptions of Hidden Markov 
Models (HMMs) and avoiding the presence of 
label bias from having few successor states. For 
each current state, we obtain the conditional 
probability of its output states given previously 
assigned values of input states. For most language 
processing tasks, this model is simply a linear-
chain Markov Random Fields model. 
In typical labeling processes using CRFs each 
token is viewed as a labeling unit. For our prob-
lem, we process each input document 
),...,,( 21 nsssD =  as a sequence of individual sen-
839
 tences, with a corresponding labeling sequence of 
labels, ),...,,( 21 nlllL = , so that each sentence corre-
sponds to only one label. In our problem, each data 
record corresponds to a distinct TTE experiment. 
Similar to NP chunking, we define three labels for 
sentences, ?B_REC? (beginning of record), 
?I_REC? (inside record), and ?O? (other). The de-
fault label ?O? indicates that this sentence is be-
yond our concern. 
The CRF model is trained to maximize the 
probability of )|( DLP , that is, given an input 
document D, we find the most probable labeling 
sequence L. The decision rule for this procedure is: 
)|(maxarg? DLPL
L
=                                        (1) 
A CRF model of the two sequences is character-
ized by a set of feature functions kf and their corre-
sponding weights k? . As in Markov fields, the 
conditional probability )|( DLP  can be computed 
using Equation 2. 
??
???
???=
= ?
T
t k
ttkk
S
tDllf
Z
DLP
1
1 ),,,(*exp
1
)|( ?        (2) 
where ),,,( 1 tDllf ttk ? is a feature function, represent-
ing either the state transition feature ),,( 1 Dllf ttk ?  or 
the feature of output state ),( Dlf tk given the input 
sequence. All these feature functions are user-
defined boolean functions. 
CRF works under the framework of supervised 
learning, which requires a pre-labeled training set 
to learn and optimize system parameters to maxi-
mize the probability or its log format. Equipped 
with this model, we investigate how to apply it and 
prepare features accordingly. 
3.2 Feature Preparation 
The CRF model provides a compact, unified 
framework to integrate features. However, unlike 
sentence-level processing, where features are very 
intuitive and circumscribed, it is not obvious what 
features are most indicative for our problem. We 
therefore explore three categories of features for 
discourse level chunking. 
3.2.1 Semantic Attribute Labels 
Most text segmentation approaches compute 
surface word similarity scores in given corpora 
without semantic analysis. However, in our case, 
data records have very similar characteristics and 
share most of the words. They are not 
distinguishable just from an analysis of surface 
word statistics. We have to understand the 
semantics before we can make decisions about data 
record extraction.  
In our case, we care about the four types of at-
tributes of each data record (one TTE experiment). 
Table 1 gives the definitions of the four attributes 
for each data record. 
Name Description 
injectionLocation the named brain region where the injection was made. 
tracerChemical the tracer chemical used. 
labelingLocation the region/location where the labeling was found. 
labelingDescription 
a description of labeling, in-
cluding label density or label 
type. 
Table 1. Attributes of data records (a TTE experiment). 
To obtain this semantic attributes information of 
individual sentences (the HP problem), we first 
apply another sentence-level CRF model to label 
each sentence. We consider five categories of fea-
tures based on language analysis. Table 2 shows 
the features for each category. 
Name Feature Description 
TOPOGRAPHY Is word topog-
raphic? 
BRAIN_REGION Is word a region 
name? 
TRACER Is word a tracer 
chemical? 
DENSITY Is word a den-
sity term? 
Lexicon 
Knowledge 
LABELING_TYPE Does word de-
note a labeling 
type? 
Surface 
Word 
Word Current word 
Context    
Window 
CONT-INJ If current word 
is within a win-
dow of injection 
context 
Prev-word Previous word Window 
Words Next-word Next word 
Root-form Root form of 
the word if dif-
ferent 
Gov-verb The governing 
verb 
Subject The sentence 
subject  
Dependency 
Features 
Object The sentence 
object 
Table 2. The features for labeling words. 
840
 a. Lexicon knowledge. We used names of brain 
structures taken from brain atlases (Swanson, 
2004), standard terms to denote neuro-
anatomical topographical relationships (e.g., 
?rostral?), the name or abbreviation of the 
tracer chemical used (e.g., ?PHAL?), and 
commonsense descriptions for descriptions of 
the labeling (e.g., ?dense?, ?light?).  
b. Surface and window word. The current 
word and the words around are important in-
dicators of the most probable label. 
c. Context window. The TTE is a description of 
the inject-label-findings process. Whenever a 
word having a root form of ?injection? or 
?deposit? appears, we generate a context 
window and all the words falling into this 
window are assigned a feature of ?CONT-
INJ?.  
d. Dependency features. We apply a depend-
ency parser MiniPar (Lin, 1998) to parse each 
sentence, and then derive four types of fea-
tures from the parsing result. These features 
are (a) root form of every word, (b) the sub-
ject within the sentence, (c) the object within 
the sentence, and (d) the governing verbs. 
The labeling system assigns a label for every to-
ken in each sentence. We achieved the best per-
formance with an F-score of 0.79 (based on a pre-
cision of 0.80 and a recall of 0.78). This is not the 
focus of this paper. Please refer to our previous 
work (Burns et al, 2007) for details. 
 
 
 
 
 
 
 
Figure 2. An example of semantic attribute labels. 
With the sentence-level understanding of each 
sentence, we obtain the semantic attribute labels 
for the data records. Figure 2 gives an example 
sentence with semantic attribute labels. Here 
<tracerChemical>, <labelingLocation>, and <la-
belingDescription> are recognized by the system, 
and the attribute names will be used as features for 
this sentence. 
3.2.2 Semantic Language Model 
Since text narratives might adhere to logical ways 
of expressing facts, language models for each sen-
tence will also provide good features to extract 
data records. However, in biomedical research arti-
cles many of the technical words/phrases used in 
the narrative are repeated across experiments, mak-
ing the surface word language model of little use in 
deriving generalized data record templates. Con-
sidering this, we replace in each sentence the la-
beled fragments with their attribute labels and then 
derive semantic language models from that format. 
By ?semantic language model? we therefore mean 
a combination of semantic labels and surface 
words.  
For example, in the sentence shown in Figure 2, 
we have the semantic language model trigrams 
location-of-<tracerChemical>, sites-in-
<injectionLocation>, and <labelingDescription>-
followed-the. In addition, we also query WordNet 
for the root form of each word to generalize the 
semantic language models. This for example pro-
duces the semantic language model trigrams site-
in-<injectionLocation> and <labelingDescription>-
follow-the. 
We believe the collected semantic language 
models represent an inherent structure of unstruc-
tured data records. By integrating them as features 
with a CRF model, we expect to represent data re-
cord templates and use the learned model to extract 
new data records.  
However, it is not clear what semantic language 
models are most indicative and useful. A bag-of-
words (language models) approach may bring 
much noise in. We show below a comparison of 
regular language models and semantic language 
models in evaluations.  
3.2.3 Layout and Word Heuristics 
The previous two categories of features come from 
the discovery of semantic components of sentences 
and their narrative form word analysis. When in-
terviewing the neuroscience expert annotator, we 
learned that some layout and word level heuristics 
may also help to delineate individual data records. 
Table 3 gives the two types of heuristic features. 
When a sentence contains heuristic words, it 
will be assigned to a word heuristic feature. If the 
sentence is at the boundary of a paragraph, it will 
be assigned a layout heuristic feature, namely the 
first or the last sentence in the paragraph.  
<SENT FILE="1995-360-213-ns.xml" INDEX= "63"> 
Regardless of the precise location of <tracerChemical> 
PHAL </tracerChemical> injection sites in <injectionLo-
cation> the MEA </injectionLocation> , <labelingDe-
scription> labeled axons </labelingDescription> followed 
the same basic routes . 
</SENT> 
841
 Name Feature Descrip-tion 
EXP_B_WORD 
INJECT 
CASE 
EXPERIMENT 
APPLICATION 
DEPOSIT 
PLACEMENT 
INTRODUCTION 
Heuristic 
words for 
beginning 
of an ex-
periment 
descrip-
tion 
POS_IN_PARA FIRST_IN_PARA 
LAST_IN_PARA 
Position of 
the sen-
tence in 
the para-
graph 
Table 3. The heuristic features. 
4 Empirical Evaluation 
To evaluate the effectiveness and performance of 
our technique, we conducted extensive experi-
ments to measure the data record extraction ap-
proach. 
4.1 Experimental Setup 
We used the machine learning package MALLET 
(McCallum, 2002) to conduct the CRF model 
training and labeling. 
We have obtained the digital publications of 
9474 Journal of Comparative Neurology (JCN)1 
articles from 1982 to 2005. We have converted the 
PDF format into plain text, maintaining paragraph 
breaks (some errors still occur though).  A simple 
heuristic based approach identifies semantic sec-
tions of the paper (e.g, Introduction, Results, Dis-
cussion). As most experimental descriptions appear 
in the Results section, we only process the Results 
section. A neuroscience expert manually annotated 
the data records in the Results section of 58 re-
search articles. The total number of sentences in 
the Results section of the 58 files is 6630 (averag-
ing 114.3 sentences per article). 
 Training Set Testing Set 
Docs 39 19 
Data Records 249 133 
Table 4. Experiment configuration. 
We randomly divided this material into training 
and testing sets under a 2:1 ratio, giving 39 docu-
ments in the training set and 19 in the testing set. 
                                                 
1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248 
Table 4 gives the numbers of documents and data 
records in the training and the testing set. 
4.2 Evaluation Metrics 
To evaluate data record extraction, we notice it is 
not fair to strictly evaluate the boundaries of data 
records because this does not penalize the near-
miss and false positive of data records in a reason-
able way; sentences near a boundary that contain 
no relevant record information can be included or 
omitted without affecting the results. Hence the 
standard Pk (Beeferman et al, 1997) and WinDiff 
(Pevzner and Hearst, 2002) measures for text seg-
mentation are not so suitable for our task. 
As we are concerned with the usefulness of 
knowledge in extracted data records, we instead 
evaluate from the perspective of IE. We measure 
system performance on the quality of the extracted 
data records. For each extracted data record, it will 
be aligned to one of the data records in the gold 
standard using the ?dominance rule? (if the data 
record can be aligned to multiple records in the 
gold standard, it will be aligned to the one with 
highest overlap). Then we evaluate the precision, 
recall, and F1 scores of extracted units of the data 
record. The units are the attributes in data records. 
system by the units extracted  theof #
unitscorrect   # of
precision =   (3) 
standard gold in the units  theof #
 unitscorrect   # of
recall =                (4) 
ecallrprecision
recall*precision
F +=
*2
1                                    (5) 
These measures provide an indication of the 
completeness and correctness of each extracted 
record (experiment). We also measure the number 
of distinct records extracted, compared with the 
gold standard as appearing in the document. 
4.3 Experiment Results 
To fully compare the effectiveness of our semantic 
analysis functionality, we evaluated system per-
formance for all the following systems:  
TextTiling (TT): To compare with text segmen-
tation techniques, we use TextTiling (Hearst, 1994) 
with default parameters as the first baseline sys-
tem. 
Random Guess (RG): In order to demonstrate 
the data balance of all the possible labels in the 
testing set, we also use another baseline system 
with random decisions for each sentence.  
842
 Domain Heuristics (DH): In a regular TTE ex-
periment, only one tracer chemical will typically 
be used. Given this heuristic, we assume each data 
record contains one tracer chemical. In this system, 
we first locate sentences with identified trace 
chemicals, and then we greedily expand backward 
and forward until another new tracer chemical ap-
pears or no other attribute is included. 
Surface Text (ST): To measure the effective-
ness of the semantic analysis (attribute labels and 
semantic language models), the ST system utilizes 
only standard surface word language models and 
heuristic features. 
Semantic Analysis (SEM): The SEM system 
uses all the semantic features available (including 
identified attributes and semantic language models) 
and two heuristic features. 
Table 5 shows the final performance of these 
different systems. The second column provides the 
numbers of extracted data records. In this task, a 
larger number does not necessarily mean a better 
system, as a system might produce too many false 
positives. The remaining three columns represent 
the precision, recall, and F1 scores, averaged over 
all data records. With our approach, the system 
performance is significantly improved compared 
with other systems. System TT fails in this task as 
it only outputs the full document as one single re-
cord. 
 # of     
Records 
Prec. Rec. F1 
TT 19 0.3861 1.0 0.5571 
RG 758 0.6331 0.0913 0.1595 
DH 162 0.6703 0.4902 0.5663 
ST 82 0.8182 0.8339 0.8260 
SEM 72 0.8505 0.9258 0.8865 
Table 5. System performance. 
To investigate how plain text language models 
and semantic language models affect system per-
formance, we also experimented with all the lan-
guage models. Table 6 shows comparisons of three 
types of language models. Systems with semantic 
analysis always work better than those with only 
surface text analysis. Without semantic analysis, 
unigram features work better than bigram and tri-
gram features. This matches our intuition: without 
generalizing to semantic language models, higher 
order language models will be relatively sparse and 
contain much noise. However, when taking into 
account the semantic features, we found that bi-
gram and trigram semantic language model fea-
tures outperformed unigrams. They are especially 
important in boosting the recall scores as they cap-
ture more generalized information when derived. 
Unigram (%) Bigram (%) Trigram (%)  
Prec/Rec/F1 Prec/Rec/F1 Prec/Rec/F1 
ST 81.8/83.4/82.6 69.1/88.4/77.6 57.9/88.8/70.1 
SEM 85.1/86.6/85.6 85.1/92.6/88.7 82.2/92.7/87.1 
Table 6. Language model comparisons. 
As an example, Table 7 gives a list of high qual-
ity bigram semantic language models ranked by 
their information gains based on the training data. 
through_<labelingLocation> rat_no 
<labelingDescription>_be of_<tracerChemical> 
<labelingLocation>_( <tracerChemical>_be 
<tracerChemical>_injection be_inject 
into_<injectionLocation> be_center 
<labelingDescription>_from inject_with 
<tracerChemical>_in injection_of 
in_<labelingLocation> in_experiment 
Table 7. An example list of top-ranked bigrams. 
The main difficulty for data record extraction 
from unstructured text lies in deriving and repre-
senting a template for future extraction. We actu-
ally take advantage of CRF and represent the tem-
plate with a CRF model.  
Each data record is measured with precision, re-
call, and F1 scores. Figure 3 depicts the distribu-
tion of extracted data records according to these 
measures in the best system. 
Distribution
0
5
10
15
20
25
30
35
40
45
50
55
60
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Performance
# 
of
 e
xt
ra
ct
ed
 r
ec
or
ds
Prec
Rec
F1
 
Figure 3. Data records performance distribution. 
The results are encouraging, especially given the 
complexity and flexibility of data record descrip-
tions in the unstructured text. In Figure 3, Axis X 
843
 represents the value interval for precision, recall, 
and F1, and Axis Y represents the number of ex-
tracted records with their corresponding values. 
For example, 57 records have recall scores falling 
into [0.9, 1.0].  
Figure 4 gives an example alignment between 
system result and the gold standard. Each record is 
represented by a range of sentences. The numbers 
following each record in the system result are indi-
vidual data record?s precision and recall scores. 
          System                                   Gold 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. An example of record extraction in one doc. 
This is a real example from the testing set. For 
records R1, R3, and R6, the system can extract the 
exact sentences contained. For record R2 and R5, 
although they do not exactly match at the sentence 
level, the extracted record contains the entire re-
quired set of attributes as in the gold standard.  
4.4 Error Analysis and Discussion 
When we investigated the errors, we found that 
sometimes the extracted data records combined 
two or more smaller gold standard records, or vice 
versa. As shown in Figure 4, extracted records R4 
and R7 are both combinations of records in the 
gold standard. This is partially due to the granular-
ity definition problem. Authors may mention sev-
eral approaches/symptoms to one type of experi-
ment for a single purpose. In this case, it is almost 
infeasible to have annotators strictly agree on 
granularity and thus to teach the system to acquire 
this knowledge. For example, in the gold standard, 
the annotator annotated three successive sentences 
as three separate records but the system output 
those as only one data record. In this extreme case, 
it is too hard to expect the system to perform well. 
In our approach, the semantic attribute labels 
and semantic language models require the result of 
the initial sentence-level labeling, which has an F-
score of 0.79. The error may propagate into the 
data record extraction procedure and lower overall 
system performance. 
In our current experiments, we also assume all 
the attributes within one segment belong to one 
record. However, the situation of embedded data 
records will make this problem harder. For exam-
ple, authors sometimes compare the current ex-
periment with other approaches in referenced pa-
pers. In this case, those attributes should be ex-
cluded from the records. We need to invent rules or 
constraints to filter them out. When such reference 
occurs at experiment boundaries, it brings higher 
risk for correct results.  
It is a very hard problem to extract from unstruc-
tured text neat structured records. The annotators 
sometimes employ background knowledge or rea-
soning when performing manual extraction; such 
knowledge cannot today be easily modeled and 
integrated into learning systems.  
In our study, we also compared some feature se-
lection approaches. Similar to (Yang and Pedersen, 
1997), we tried Feature Instance Frequency, Mu-
tual Information, Information Gain, and CHI-
square test. But we eventually found that the sys-
tem including all the features worked best, and 
with all the other configurations unchanged, fea-
ture instance frequency worked at almost the same 
level as other complex measures such as mutual 
information and information gain.  
5 Conclusion and Future Work 
In this paper, we explored the problem of extract-
ing data records from unstructured text. The lack 
of structure makes it difficult to derive meaningful 
objects and their values without resorting to deeper 
language analysis techniques. We derived indica-
tive linguistic features to represent data record 
templates in free text, using a two-pass approach in 
which the second pass used the IE labels derived 
from the first to compose attributes into coherent 
data records. We evaluated the results from an IE 
perspective and reported potential problems of er-
ror generation. 
? 
R1:S12~S29 (1.0/1.0) 
? 
R2: S31~S41 (1.0/1.0) 
 
R3: S42~S52 (1.0/1.0) 
? 
R4: S56~S73 
(0.517/1.0) 
? 
R5: S75~S88 (1.0/1.0) 
? 
R6: S91~S106(1.0/1.0) 
? 
R7: S108~S118 
(0.523/1.0)  
? 
? 
R1': S12~S29 
? 
R2': S31~S40  
? 
R3': S42~S52 
? 
R4': S56~S63 
? 
R5': S65~S73 
R6': S74~S88 
.. 
R7': S91~S106 
? 
R8': S108~S114 
R9': S115~S118 
? 
844
 For the future, we plan to explore additional fea-
ture types and feature selection strategies to deter-
mine what is ?good? for unstructured record tem-
plates to improve our results. More effort will also 
be put into the sentence-level analysis to reduce 
error propagations. In addition, ontology based 
knowledge inference strategies might be useful to 
validate attributes in single record and in turn help 
data record extraction. The last thing under our 
direction is to explore new models if applicable.  
We hope this thought-provoking problem will 
attract more attention from the community. In the 
future, we plan to make our corpus available to the 
community. The solution to this problem will 
highly affect the access of knowledge in large scale 
unstructured text corpora. 
Acknowledgements 
The work was supported in part by an ISI seed 
funding, and in part by a grant from the National 
Library of Medicine (RO1 LM07061). The authors 
want to thank Feng Pan for his helpful suggestions 
with the manuscript. We would also like to thank 
the anonymous reviewers for their valuable com-
ments. 
References 
Arasu, A., and Garcia-Molina, H. 2003. Extracting 
structured data from web pages. In Proc. of SIMOD-
2003.  
Beeferman, D., Berger, A., and Lafferty, J. 1997. Text 
segmentation using exponential models. In Proc. of 
EMNLP-1997.  
Blunsom, P. and Cohn, T. 2006. Discriminative word 
alignment with conditional random fields. In Proc. of 
ACL-2006.  
Brazma, A., et al, 2001. Minimum information about a 
microarray experiment (MIAME)-toward standards 
for microarray data. Nat Genet, 29(4): p. 365-71.  
Burns, G.A. and Cheng, W.-C. 2006. Tools for knowl-
edge acquisition within the NeuroScholar system and 
their application to anatomical tract-tracing data. In 
Journal of Biomedical Discovery and Collaboration.  
Burns, G., Feng, D., and Hovy, E.H. 2007. Intelligent 
Approaches to Mining the Primary Research Litera-
ture: Techniques, Systems, and Examples. Book 
Chapter in Computational Intelligence in Bioinfor-
matics, Springer-Verlag, Germany. 
Choi, F. Y. Y. 2000. Advances in domain independent 
linear text segmentation. In Proc. of NAACL-2000.  
Hahn, U., Romacher, M., and Schulz, S. 2002. Creating 
knowledge repositories from biomedical reports the 
MEDSYNDIKATE text mining system. In Proc. of 
PSB-2002. 
Hearst, M. 1994. Multi-paragraph segmentation of ex-
pository text. In Proc. of ACL-1994.  
Jiao, F., Wang, S., Lee, C., Greiner, R., and 
Schuurmans, D. 2006. Semi-supervised conditional 
random fields for improved sequence segmentation 
and labeling. In Proc. of ACL-2006.  
Kristjannson, T., Culotta, A. Viola, P., and McCallum, 
2004. A. Interactive information extraction with con-
strained conditional random fields. In Proc. of AAAI-
2004. 
Lafferty, J., McCallum, A. and Pereira, F. 2001 Condi-
tional Random Fields: probabilistic models for seg-
menting and labeling Sequence Data. In Proc. of 
ICML-2001. 
Lin, D. 1998. Dependency-based evaluation of MINI-
PAR. In Proc. of Workshop on the Evaluation of 
Parsing Systems.  
Liu, B., Grossman, R., and Zhai, Y. 2003. Mining data 
records in web pages. In Proc. of SIGKDD-2003.  
Malioutov, I. and Barzilay, R. 2006. Minimum cut 
model for spoken lecture segmentation. In Proc. of 
ACL-2006.  
McCallum, A.K. 2002. MALLET: A Machine Learning 
for Language Toolkit. http://mallet.cs.umass.edu. 
Muslea, I., Minton, S., and Knoblock, C.A. 2001. 
Hierarchical wrapper induction for semistructured 
information sources. Autonomous Agents and Multi-
Agent Systems 4:93-114. 
Okanohara, D., Miyao, Y., Tsuruoka, Y., and Tsujii, J. 
2006. Improving the scalability of semi-markov con-
ditional random fields for named entity recognition. 
In Proc. of ACL-2006.  
Peng, F. and McCallum, A. 2004. Accurate information 
extraction from research papers using conditional 
random fields. In Proc. of HLT-NAACL-2004.  
Pevzner, L., and Hearst, M. 2002. A Critique and Im-
provement of an Evaluation Metric for Text Segmen-
tation. Computational Linguistics. 
Pinto, D., A. McCallum, X. Wei, and W.B. Croft. 2003. 
Table Extraction Using Conditional Random Fields. 
In Proc. of SIGIR-2003.  
845
 Stephan, K.E. et al, 2001. Advanced database method-
ology for the Collation of Connectivity data on the 
Macaque brain (CoCoMac). Philos Trans R Soc Lond 
B Biol Sci, 356(1412).  
Swanson, L.W. 2004. Brain Maps: Structure of the Rat 
Brain. 3rd edition, Elsevier Academic Press.  
Wick, M., Culotta, A., and McCallum, A. 2006. Learn-
ing field compatibilities to extract database records 
from unstructured text. In Proc. of EMNLP-2006. 
Yang, Y., and Pedersen, J. 1997. A comparative study 
on feature selection in text categorization. In Proc. of 
ICML-1997, pp. 412-420.  
Zhu, J., Nie, Z., Wen, J., Zhang, B., and Ma, W. 2006. 
Simultaneous record detection and attribute labeling 
in web data extraction. In Proc. of KDD-2006. 
846
Towards Automated Semantic Analysis on Biomedical Research Articles
 
Donghui Feng         Gully Burns         Jingbo Zhu         Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, burns, jingboz, hovy}@isi.edu 
 
Abstract 
In this paper, we present an empirical 
study on adapting Conditional Random 
Fields (CRF) models to conduct semantic 
analysis on biomedical articles using ac-
tive learning. We explore uncertainty-
based active learning with the CRF model 
to dynamically select the most informa-
tive training examples. This abridges the 
power of the supervised methods and ex-
pensive human annotation cost. 
1 Introduction 
Researchers have experienced an increasing need 
for automated/semi-automated knowledge acquisi-
tion from the research literature. This situation is 
especially serious in the biomedical domain where 
the number of individual facts that need to be 
memorized is very high. 
Many successful information extraction (IE) 
systems, work in a supervised fashion, requiring 
human annotations for training. However, human 
annotations are either too expensive or not always 
available and this has become a bottleneck to de-
veloping supervised IE methods to new domains. 
Fortunately, active learning systems design 
strategies to select the most informative training 
examples. This process can achieve certain levels 
of performance faster and reduce human annota-
tion (e.g., Thompson et al, 1999; Shen et al, 2004). 
In this paper, we present an empirical study on 
adapting CRF model to conduct semantic analysis 
on biomedical research literature. We integrate an 
uncertainty-based active learning framework with 
the CRF model to dynamically select the most in-
formative training examples and reduce human 
annotation cost. A systematic study with exhaus-
tive experimental evaluations shows that it can 
achieve satisfactory performance on biomedical 
data while requiring less human annotation. 
Unlike direct estimation on target individuals in 
traditional active learning, we use two heuristic 
certainty scores, peer comparison certainty and set 
comparison certainty, to indirectly estimate se-
quences labeling quality in CRF models. 
We partition biomedical research literature by 
experimental types. In this paper, our goal is to 
analyze various aspects of useful knowledge about 
tract-tracing experiments (TTE). This type of ex-
periments has prompted the development of sev-
eral curated databases but they have only partial 
coverage of the available literature (e.g., Stephan et 
al., 2001). 
2 Related Work 
Knowledge Base Management Systems allow 
individual users to construct personalized 
repositories of knowledge statements based on 
their own interaction with the research literature 
(Stephan et al, 2001; Burns and Cheng, 2006). But 
this process of data entry and curation is manual. 
Current approaches on biomedical text mining (e.g., 
Srinivas et al, 2005; OKanohara et al, 2006) tend 
to address the tasks of named entity recognition or 
relation extraction, and our goal is more complex: 
to extract computational representations of the 
minimum information in a given experiment type. 
Pattern-based IE approaches employ seed data 
to learn useful patterns to pinpoint required fields 
values (e.g. Ravichandran and Hovy, 2002; Mann 
and Yarowsky, 2005; Feng et al, 2006). However, 
this only works if the data corpus is rich enough to 
learn variant surface patterns and does not neces-
sarily generalize to more complex situations, such 
as our domain problem. Within biomedical articles, 
sentences tend to be long and the prose structure 
tends to be more complex than newsprint. 
871
The CRF model (Lafferty et al, 2001) provides 
a compact way to integrate different types of fea-
tures for sequential labeling problems. Reported 
work includes improved model variants (e.g., Jiao 
et al, 2006) and applications such as web data ex-
traction (Pinto et al, 2003), scientific citation ex-
traction (Peng and McCallum, 2004), word align-
ment (Blunsom and Cohn, 2006), and discourse-
level chunking (Feng et al, 2007). 
Pool-based active learning was first successfully 
applied to language processing on text classifica-
tion (Lewis and Gale, 1994; McCallum and Nigam, 
1998; Tong and Koller, 2000). It was also gradu-
ally applied to NLP tasks, such as information ex-
traction (Thompson et al, 1999); semantic parsing 
(Thompson et al, 1999); statistical parsing (Tang 
et al, 2002); NER (Shen et al, 2004); and Word 
Sense Disambiguation (Chen et al, 2006). In this 
paper, we use CRF models to perform a more com-
plex task on the primary TTE experimental results 
and adapt it to process new biomedical data. 
3 Semantic Analysis with CRF Model 
3.1 What knowledge is of interest? 
The goal of TTE is to chart the interconnectivity of 
the brain by injecting tracer chemicals into a region 
of the brain and then identifying corresponding 
labeled regions where the tracer is transported to. 
A typical TTE paper may report experiments about 
one or many labeled regions.  
Name Description 
injectionLocation the named brain region where the injection was made. 
tracerChemical the tracer chemical used. 
labelingLocation the region/location where the labeling was found. 
labelingDescription a description of labeling, den-sity or label type. 
Table 1. Minimum knowledge schema for a TTE. 
 
 
 
 
 
 
 
 
 
 
Figure 1. An extraction example of TTE description. 
In order to construct the minimum information 
required to interpret a TTE, we consider a set of 
specific components as shown in Table 1. 
Figure 1 gives an example of description of a 
complete TTE in a single sentence. In the research 
articles, this information is usually spread over 
many such sentences.  
3.2 CRF Labeling 
We use a plain text sentence for input and attempt 
to label each token with a field label. In addition to 
the four pre-defined fields, a default label, ?O?, is 
used to denote tokens beyond our concern.  
In this task, we consider five types of features 
based on language analysis as shown in Table 2. 
Name Feature Description 
TOPOGRAPHY Is word topog-
raphic? 
BRAIN_REGION Is word a region 
name? 
TRACER Is word a tracer 
chemical? 
DENSITY Is word a density 
term? 
Lexical 
Knowledge 
LABELING_TYPE Does word denote 
a labeling type? 
Surface Word Word Current word 
Context    
Window 
CONT_INJ If current word if 
within a window 
of injection con-
text 
Prev-word Previous word Window 
Words Next-word Next word 
Root-form Root form of the 
word if different 
Gov-verb The governing 
verb 
Subj The sentence 
subject  
Dependency 
Features 
Obj The sentence 
object 
Table 2. The features for system labeling. 
Lexical Knowledge. We define lexical items rep-
resenting different aspects of prior knowledge. To 
this end we use names of brain structures taken 
from brain atlases, standard terms to denote neuro-
anatomical topographical spatial relationships, and 
common sense words for labeling descriptions. We 
collect five separate lexicons as shown in Table 3. 
Lexicons # of terms # of words 
BRAIN_REGION 1123 5536 
DENSITY 8 10 
LABELING_TYPE 9 13 
TRACER 30 30 
TOPOGRAPHY 9 36 
Total 1179 5625 
Table 3. The five lexicons. 
The NY injection ( Fig . 9B ) encompassed  
 
tracerChemical 
most of the pons and was very dense in  
 
injectionLocation 
the region of the MLF. 
 
labelingLocation 
872
Surface word. The word token is an important 
indicator of the probable label for itself.  
Context Window. The TTE is a description of the 
inject-label-findings context. Whenever we find a 
word with a root form of ?injection? or ?deposit?, 
we generate a context window around this word 
and all the words falling into this window are as-
signed a feature of ?CON_INJ?. This means when 
labeling these words the system should consider 
the very current context. 
Window Words. We also use all the words occur-
ring in the window around the current word. We 
set the window size to only include the previous 
and following words (window size = 1).  
Dependency Features. To untangle word relation-
ships within each sentence, we apply the depend-
ency parser MiniPar (Lin, 1998) to parse each sen-
tence, and then derive four types of features. These 
features are (a) root form of word, (b) the subject 
in the sentence, (c) the object in the sentence, and 
(d) the governing verb for each word. 
4 Uncertainty-based Active Learning 
Active learning was initially introduced for 
classification tasks. The intuition is to always add 
the most informative examples to the training set to 
improve the system as much as possible.  
We apply an uncertainty/certainty score-based 
approach. Unlike traditional classification tasks, 
where disagreement or uncertainty is easy to obtain 
on target individuals, information extraction tasks 
in our problem take a whole sequence of tokens 
that might include several slots as processing units. 
We therefore need to make decisions on whether a 
full sequence should be returned for labeling. 
Estimations on confidence for single segments 
in the CRF model have been proposed by (Culotta 
and McCallum, 2004; Kristjannson et al, 2004). 
However as every processing unit in the data set is 
at the sentence level and we make decisions at the 
sentence level to train better sequential labeling 
models, we define heuristic scores at the sentence 
level.  
Symons et al (2006) presents multi-criterion for 
active learning with CRF models, but our motiva-
tion is from a different perspective. The labeling 
result for every sentence corresponds to a decoding 
path in the state transition network. Inspired by the 
decoding and re-ranking approaches in statistical 
machine translation, we use two heuristic scores to 
measure the degree of correctness of the top label-
ing path, namely, peer comparison certainty and 
set comparison certainty. 
Suppose a sentence S includes n words/tokens 
and a labeling path at position m in the ranked N-
best list is represented by ),...,,( 110 ?= nm lllL . Then 
the probability of this labeling path is represented 
by )( mLP , and we have the following two equa-
tions to define the peer comparison certainty 
score, )(SScore peer  and set comparison certainty 
score, )(SScoreset : 
)(
)(
)(
2
1
LP
LP
SScorepeer =                                      (1) 
?
=
=
N
k
k
set
LP
LP
SScore
1
1
)(
)(
)(                                    (2) 
For peer comparison certainty (Eq. 1), we calcu-
late the ratio of the top-scoring labeling path prob-
ability to the second labeling path probability. A 
high ratio means there is a big jump from the top 
labeling path to the second one. The higher the ra-
tio score, the higher the relative degree of correct-
ness for the top labeling path, giving system higher 
confidence for those with higher peer comparison 
certainty scores. Sentences with lowest certainty 
score will be sent to the oracle for manual labeling. 
In the labeling path space, if a labeling path is 
strong enough, its probability score should domi-
nate all the other path scores. In Equation 2, we 
compute the set comparison certainty score by con-
sidering the portion of the probability of the path in 
the overall N-best labeling path space. A large 
value means the top path dominates all the other 
labeling paths together giving the system a higher 
confidence on the current path over others. 
We start with a seed training set including k la-
beled sentences. We then train a CRF model with 
the training data and use it to label unlabeled data. 
The results are compared based on the certainty 
scores and those sentences with the lowest cer-
tainty scores are sent to an oracle for human label-
ing. The new labeled sentences are then added to 
the training set for next iteration.  
5 Experimental Results 
We first investigated how the active learning steps 
could help for the task. Second, we evaluated how 
the CRF labeling system worked with different sets 
of features. We finally applied the model to new 
873
biomedical articles and examined its performance 
on one of its subsets. 
5.1 Experimental Setup 
We have obtained 9474 Journal of Comparative 
Neurology (JCN)1 articles from 1982 to 2005. For 
sentence labeling, we collected 21 TTE articles 
from the JCN corpus. They were converted from 
PDF files to XML files, and all of the article sec-
tions were identified using a simple rule-based ap-
proach. As most of the meaningful descriptions of 
TTEs appear in the Results section, we only proc-
essed the Results section. The 21 files in total in-
clude 2009 sentences, in which 1029 sentences are 
meaningful descriptions for TTEs and 980 sen-
tences are not related to TTEs.  
We randomly split the sentences into a training 
pool and a testing pool, under a ratio 2:1. The 
training pool includes 1338 sentences, with 685 of 
them related to TTEs, while 653 not. Testing was 
based on meaningful sentences in the testing pool. 
Table 4 gives the configurations in the data pools. 
 # of        
Related 
Sentences  
# of        
Unrelated 
Sentences 
Sum 
Training Pool 685 653 1338 
Testing Pool 344 327 671 
Sum 1029 980 2009 
Table 4. Training and testing pool configurations. 
5.2 Evaluation Metrics 
As the label ?O? dominates the data set (70% out 
of all tokens), a simple accuracy score would pro-
vide an inappropriate high score for a baseline sys-
tem that always chooses ?O?. We used Precision, 
Recall, and F_Score to evaluate only meaningful 
labels. 
5.3 How well does active learning work? 
For the active learning procedure, we initially se-
lected a set of seed sentences related to TTEs from 
the training pool. At every step we trained a CRF 
model and labeled sentences in the rest of the train-
ing pool. As described in section 4, those with the 
lowest rank on certainty scores were selected. If 
they are related to a TTE, human annotation will 
be added to the training set. Otherwise, the system 
will keep on selecting sentences until it finds 
enough related sentences. 
                                                 
1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248 
People have found active learning in batch mode 
is more efficient, as in some cases a single addi-
tional training example will not improve a classi-
fier/system that much. In our task, we chose the 
bottom k related sentences with the lowest cer-
tainty scores. We conducted various experiments 
for k = 2, 5, and 10. We also compared experi-
ments with passive learning, where at every step 
the new k related sentences were randomly se-
lected from the corpus. Figures 2, 3, and 4 give the 
learning curves for precision, recall, and F_Scores 
when k = 10. 
 
Figure 2. Learning curve for Precision. 
 
Figure 3. Learning curve for Recall. 
 
Figure 4. Learning curve for F_Score. 
From these figures, we can see active learning 
approaches required fewer training examples to 
achieve the same level of performance. As we it-
eratively added new labeled sentences into the 
training set, the precision scores of active learning 
were steadily better than that of passive learning as 
the uncertain examples were added to strengthen 
874
existing labels. However, the recall curve is 
slightly different. Before some point, the recall 
score of passive learning was a little better than 
active learning. The reason is that examples se-
lected by active learning are mainly used to foster 
existing labels but have relatively weaker im-
provements for new labels, while passive learning 
has the freedom to add new knowledge for new 
labels and improve recall scores faster. As we keep 
on using more examples, the active learning 
catches up with and overtakes passive learning on 
recall score. 
These experiments demonstrate that under the 
framework of active learning, examples needed to 
train a CRF model can be greatly reduced and 
therefore make it feasible to adapt to other domains. 
5.4 How well does CRF labeling work? 
As we added selected annotated sentences, the sys-
tem performance kept improving. We investigated 
system performance at the final step when all the 
related sentences in the training pool are selected 
into the training set. The testing set alo only in-
cludes the related sentences. This results in 685 
training sentences and 344 testing sentences. 
To establish a baseline for our labeling task, we 
simply scanned every sentence for words or 
phrases from each lexicon. If the term was present, 
then we labeled the word based on the lexicon in 
which it appeared. If words appeared in multiple 
lexicons, we assigned labels randomly. 
System Features Prec. Recall F_Score 
Baseline 0.4067 0.1761 0.2458 
Lexicon 0.5998 0.3734 0.4602 
Lexicon                   
+ Surface Words 
0.7663 0.7302 0.7478 
Lexicon                   
+ Surface Words     
+ Context Window 
0.7717 0.7279 0.7491 
Lexicon + Surface 
Words + Context 
Window + Window 
Words 
0.8076 0.7451 0.7751 
Lexicon + Surface 
Words + Context 
Window + Window 
Words + Depend-
ency Features  
0.7991 0.7828 0.7909 
Table 5. Precision, Recall, and F_Score for labeling. 
We tried exhaustive feature combinations. Table 
5 shows system performance with different feature 
combinations. All systems performed significantly 
higher than the baseline. The sole use of lexicon 
knowledge produced poor performance, and the 
inclusion of surface words produced significant 
improvement. The use of window words boosted 
precision and recall. The performance with all the 
features generated an F_score of 0.7909. 
We explored how system performance reflects 
different labels. Figure 5 and 6 depict the detailed 
distribution of system labeling from the perspec-
tive of precision and recall respectively for the sys-
tem with the best performance. Most errors oc-
curred in the confusion of injectionLocation and 
labelingLocation, or of the meaningful labels and 
?O?. 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
injLoc labelDesp labelLoc tracer
O
injLoc
labelDesp
labelLoc
tracer
 
Figure 5. Precision confusion matrix distribution. 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
injLoc labelDesp labelLoc tracer
O
injLoc
labelDesp
labelLoc
tracer
 
Figure 6. Recall confusion matrix distribution. 
The worst performance occurred for files that 
distinguish themselves from others by using fairly 
different writing styles. We believe given more 
training data with different writing styles, the sys-
tem could achieve a better overall performance. 
5.5 On New Biomedical Data 
Under this active learning framework, we have 
shown a CRF model can be trained with less anno-
tation cost than using traditional passive learning. 
We adapted the trained CRF model to new bio-
medical research articles. 
Out of the 9474 collected JCN articles, more 
than 230 research articles are on TTEs. The whole 
processing time for each document varies from 20 
seconds to 90 seconds. We sent the new system-
labeled files back to a biomedical knowledge ex-
pert for manual annotation. The time to correct one 
automatically labeled document is dramatically 
reduced, around 1/3 of that spent on raw text. 
We processed 214 new research articles and ex-
amined a subset including 16 articles. We evalu-
875
ated it in two aspects: the overall performance and 
the performance averaged at the document level. 
Table 6 gives the performance on the whole new 
subset and that averaged on 16 documents. The 
performance is a little bit lower than reported in 
the previous section as the new document set might 
include different styles of documents. We exam-
ined system performance at each document. Figure 
7 gives the detailed evaluation for each of the 16 
documents. The average F_Score of the document 
level is around 74%. For those documents with 
reasonable TTE description, the system can 
achieve an F_Score of 87%. The bad documents 
had a different description style and usually mixed 
the TTE descriptions with general discussion.  
 Prec. Recall F_Score 
Overall 0.7683 0.7155 0.7410 
Averaged per Doc. 0.7686 0.7209 0.7418 
Table 6. Performance on the whole new subset and                 
the averaged performance per document. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
11 4 12 0 6 2 14 7 8 15 3 1 13 10 9 5 Doc No.
Precision 
Recall 
F-score
 
Figure 7. System performance per document. 
6 Conclusions and Future Work 
In this paper, we explored adapting a supervised 
CRF model for semantic analysis on biomedical 
articles using an active learning framework. It 
abridges the power of the supervised approach and 
expensive human costs. We are also investigating 
the use of other certainty measures, such as aver-
aged field confidence scores over each sentence. 
In the long run we wish to generalize the frame-
work to be able to mine other types of experiments 
within the biomedical research literature and im-
pact research in those domains. 
References 
Blunsom, P. and Cohn, T. 2006. Discriminative word align-
ment with conditional random fields. In ACL-2006. 
Burns, G.A. and Cheng, W.C. 2006. Tools for knowledge 
acquisition within the NeuroScholar system and their ap-
plication to anatomical tract-tracing data. In Journal of 
Biomedical Discovery and Collaboration. 
Chen, J., Schein, A., Ungar, L., and Palmer, M. 2006. An em-
pirical study of the behavior of active learning for word 
sense disambiguation. In Proc. of HLT-NAACL 2006.  
Culotta, A. and McCallum, A. 2004. Confidence estimation 
for information extraction. In HLT-NAACL-2004, short pa-
pers. 
Feng, D., Burns, G., and Hovy, E.H. 2007. Extracting data 
records from unstructured biomedical full text. In 
Proc. of EMNLP-CONLL-2007. 
Feng, D., Ravichandran, D., and Hovy, E.H. 2006. Mining and 
re-ranking for answering biographical queries on the web. 
In Proc. of AAAI-2006. 
Jiao, F., Wang, S., Lee, C., Greiner, R., and Schuurmans, D. 
2006. Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In Proc. of 
ACL-2006. 
Kristjannson, T., Culotta, A., Viola, P., and McCallum, A. 
2004. Interactive information extraction with constrained 
conditional random fields. In Proc. of AAAI-2004.  
Lafferty, J., McCallum, A., and Pereira, F. 2001. Conditional 
random fields: probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML-2001. 
Lewis, D.D. and Gale, W.A. 1994. A sequential algorithm for 
training text classifiers. In Proc. of SIGIR-1994. 
Lin, D. 1998. Dependency-based evaluation of MINIPAR. In 
Workshop on the Evaluation of Parsing Systems. 
Mann, G.S. and Yarowsky, D. 2005. Multi-field information 
extraction and cross-document fusion. In Proc. of ACL-
2005. 
McCallum, A.K. 2002. MALLET: a machine Learning for 
language toolkit. http://mallet.cs.umass.edu.  
McCallum, A. and Nigam, K. 1998. Employing EM in pool-
based active learning for text classification. In Proc. of 
ICML-98.  
OKanohara, D., Miyao, Y., Tsuruoka, Y., and Tsujii, J. 2006. 
Improving the scalability of semi-markov conditional ran-
dom fields for named entity recognition. In ACL-2006. 
Peng, F. and McCallum, A. 2004. Accurate information ex-
traction from research papers using conditional random 
fields. In Proc. of HLT-NAACL-2004. 
Pinto, D., McCallum, A., Wei, X., and Croft, W.B. 2003. Ta-
ble extraction using conditional random fields. In SIGIR-
2003. 
Ravichandran, D. and Hovy, E.H. 2002. Learning surface text 
patterns for a question answering system. In ACL-2002.  
Shen, D., Zhang, J., Su, J., Zhou, G., and Tan, C.L. 2004. 
Multi-criteria-based active learning for named entity rec-
ognition. In Proc. of ACL-2004. 
Srinivas, et al, 2005. Comparison of vector space model 
methodologies to reconcile cross-species neuroanatomical 
concepts. Neuroinformatics, 3(2). 
Stephan, K.E., et al, 2001. Advanced database methodology 
for the Collation of Connectivity data on the Macaque 
brain (CoCoMac). Philos Trans R Soc Lond B Biol Sci. 
Symons et al, 2006. Multi-Criterion Active Learning in Con-
ditional Random Fields.  In ICTAI-2006. 
Tang, M., Luo, X., and Roukos, S. 2002. Active learning for 
statistical natural language parsing. In ACL-2002. 
Thompson, C.A., Califf, M.E., and Mooney, R.J. 1999. Active 
learning for natural language parsing and information ex-
traction. In Proc. of ICML-99. 
Tong, S. and Koller, D. 2000. Support vector machine active 
learning with applications to text classification. In Proc. of 
ICML-2000. 
876
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 120?121,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Adaptive Information Extraction for Complex Biomedical Tasks 
 
Donghui Feng            Gully Burns            Eduard Hovy 
Information Sciences Insitute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, burns, hovy}@isi.edu 
Abstract 
Biomedical information extraction tasks are of-
ten more complex and contain uncertainty at 
each step during problem solving processes. We 
present an adaptive information extraction 
framework and demonstrate how to explore un-
certainty using feedback integration. 
1 Adaptive Information Extraction 
Biomedical information extraction (IE) tasks are 
often more complex and contain uncertainty at each 
step during problem solving processes.  
When in the first place the desired information is 
not easy to define and to annotate (even by humans), 
iterative IE cycles are to be expected. There might 
be gaps between the domain knowledge representa-
tion and computer processing ability. Domain 
knowledge might be hard to represent in a clear 
format easy for computers to process. Computer sci-
entists may need time to understand the inherent 
characteristics of domain problems so as to find ef-
fective approaches to solve them. All these issues 
mandate a more expressive IE process.  
In these situations, the traditional, straightfor-
ward, and one-pass problem-solving procedure, con-
sisting of definition-learning-testing, is no longer 
adequate for the solution.  
 
Figure 1. Adaptive information extraction. 
For more complex tasks requiring iterative cycles, 
an adaptive and extended IE framework has not yet 
been fully defined although variants have been ex-
plored. We describe an adaptive IE framework to 
characterize the activities involved in complex IE 
tasks. Figure 1 depicts the adaptive information ex-
traction framework.  
This procedure emphasizes one important adap-
tive step between the learning and application 
phases. If the IE result is not adequate, some adapta-
tions are required:  
Our study focuses on extracting tract-tracing ex-
periments (Swanson, 2004) from neuroscience arti-
cles. The goal of tract-tracing experiment is to chart 
the interconnectivity of the brain by injecting tracer 
chemicals into a region of the brain and then identi-
fying corresponding labeled regions where the tracer 
is transported to (Burns et al, 2007). Our work is 
performed in the context of NeuroScholar1, a project 
that aims to develop a Knowledge Base Manage-
ment System to benefit neuroscience research.  
We show how this new framework evolves to 
meet the demands of the more complex scenario of 
biomedical text mining. 
2 Feedback Integration 
This task requires finding the knowledge describing 
one or more experiments within an article as well as 
identifying desired fields within individual sen-
tences. Significant complexity arises from the pres-
ence of a variable number of records (experiments) 
in a single research article --- anywhere from one to 
many. 
 
Table 1. An example tract-tracing experiment. 
Table 1 provides an example of a tract-tracing ex-
periment. In this experiment, when the tracer was 
injected into the injection location ?the contralateral 
AVCN?, ?no labeled cells? was found in the label-
ing location ?the DCN?. 
For sentence level fields labeling, the perform-
ance of F1 score is around 0.79 (Feng et al, 2008). 
                                                          
1 http://www.neuroscholar.org/ 
120
We here show how the adaptive information extrac-
tion framework is applied to labeling individual sen-
tences. Please see Feng et al (2007) for the details 
of segmenting data records. 
2.1 Choosing Learning Approach via F1 
A natural way to label sentences is to obtain (by 
hand or learning) patterns characterizing each field 
(Feng et al, 2006; Ravichandran and Hovy, 2002). 
We tried to annotate field values for the biomedical 
data, but we found few intuitive clues that rich sur-
face text patterns could be learned with this corpus.  
This insight, Feedback F1, caused us to give up 
the idea of learning surface text patterns as usual, 
and switch to the Conditional Random Fields (CRF) 
(Lafferty et al, 2001) for labeling sentences instead. 
In contrast to fixed-order patterns, the CRF model 
provides a compact way to integrate different types 
of features for sequential labeling problems and can 
reach state-of-the-art level performance. 
2.2 Determining Knowledge Schema via F2 
In the first place, it is not clear what granularity of 
knowledge/information can be extracted from text 
and whether the knowledge representation is suitable 
for computer processing. We tried a series of ap-
proaches, using different levels of granularity and 
description, in order to obtain formulation suitable 
for IE. Figure 2 represents the evolution of the 
knowledge schema in our repeated activities.  
 
Figure 2. Knowledge schema evolution. 
 
Figure 3. System performance at stage 1 and 2. 
We initially started with the schema in the left-
most column but our pilot study showed that some 
fields, for example, ?label_type?, had too many 
variations in text description, making it very hard for 
CRF to learn clues about it. We then switched to the 
second schema but ended up seeing that the field 
?injectionSpread? needed more domain knowledge 
and was therefore not able to be learned by the sys-
tems. The last column is the final schema after those 
pilot studies. Figure 3 shows system performance 
(overall and the worst field) corresponding to the 
first and the second representation schemas. 
2.3 Exploring Features via F3 
To train CRF sentence labeling systems, it is vital to 
decide what features to use and how to prepare those 
features. Through the cycle of Feedback F3, we ex-
plored five categories of features and their combina-
tions to determine the best features for optimal 
system performance. Table 2 shows system per-
formance with different feature combinations.  
System Features Prec. Recall F_Score 
Baseline 0.4067 0.1761 0.2458 
Lexicon 0.5998 0.3734 0.4602 
Lexicon                   
+ Surface Words 
0.7663 0.7302 0.7478 
Lexicon                   
+ Surface Words     
+ Context Window 
0.7717 0.7279 0.7491 
Lexicon + Surface 
Words + Context 
Window + Window 
Words 
0.8076 0.7451 0.7751 
Lexicon + Surface 
Words + Context 
Window + Window 
Words + Depend-
ency Features  
0.7991 0.7828 0.7909 
Table 2. Precision, Recall, and F_Score for labeling. 
Please see Feng et al (2008) for the details of the 
sentence level extraction and feature preparation,  
3 Conclusions 
In this paper, we have shown an adaptive informa-
tion extraction framework for complex biomedical 
tasks. Using the iterative development cycle, we 
have been able to explore uncertainty at different 
levels using feedback integration.  
References  
Burns, G., Feng, D., and Hovy, E.H. 2007. Intelligent Approaches to 
Mining the Primary Research Literature: Techniques, Systems, and 
Examples. Book Chapter in Computational Intelligence in Bioinfor-
matics, Springer-Verlag, Germany. 
Feng, D., Burns, G., and Hovy, E.H. 2007. Extracting Data Records 
from Unstructured Biomedical Full Text. In Proc. of EMNLP 2007.  
Feng, D., Burns, G., Zhu, J., and Hovy, E.H. 2008. Towards Automated 
Semantic Analysis on Biomedical Research Articles. In Proc. of 
IJCNLP-2008. Poster Paper.  
Feng, D., Ravichandran, D., and Hovy, E.H. 2006. Mining and re-
ranking for answering biographical queries on the web. In Proc. of 
AAAI-2006. pp. 1283-1288. 
Lafferty, J., McCallum, A. and Pereira, F. 2001. Conditional random 
fields: probabilistic models for segmenting and labeling sequence 
data. In Proc. of ICML-2001. 
Ravichandran, D. and Hovy, E.H. 2002. Learning surface text patterns 
for a question answering system. In Proceedings of ACL-2002. 
Swanson, L.W. 2004. Brain maps: structure of the rat brain. 3rd edition, 
Elsevier Academic Press. 
121
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 46?55,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
The Role of Information Extraction in the Design of a Document Triage
Application for Biocuration
Sandeep Pokkunuri
School of Computing
University of Utah
Salt Lake City, UT
sandeepp@cs.utah.edu
Cartic Ramakrishnan
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA
cartic@isi.edu
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT
riloff@cs.utah.edu
Eduard Hovy
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA
hovy@isi.edu
Gully APC Burns
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA
burns@isi.edu
Abstract
Traditionally, automated triage of papers is
performed using lexical (unigram, bigram,
and sometimes trigram) features. This pa-
per explores the use of information extrac-
tion (IE) techniques to create richer linguistic
features than traditional bag-of-words models.
Our classifier includes lexico-syntactic pat-
terns and more-complex features that repre-
sent a pattern coupled with its extracted noun,
represented both as a lexical term and as a
semantic category. Our experimental results
show that the IE-based features can improve
performance over unigram and bigram fea-
tures alone. We present intrinsic evaluation
results of full-text document classification ex-
periments to determine automatically whether
a paper should be considered of interest to
biologists at the Mouse Genome Informatics
(MGI) system at the Jackson Laboratories. We
also further discuss issues relating to design
and deployment of our classifiers as an ap-
plication to support scientific knowledge cu-
ration at MGI.
1 Introduction
A long-standing promise of Biomedical Natural
Language Processing is to accelerate the process of
literature-based ?biocuration?, where published in-
formation must be carefully and appropriately trans-
lated into the knowledge architecture of a biomed-
ical database. Typically, biocuration is a manual
activity, performed by specialists with expertise in
both biomedicine and the computational represen-
tation of the target database. It is widely acknowl-
edged as a vital lynch-pin of biomedical informatics
(Bourne and McEntyre, 2006).
A key step in biocuration is the initial triage of
documents in order to direct to specialists only the
documents appropriate for them. This classifica-
tion (Cohen and Hersh, 2006)(Hersh W, 2005) can
be followed by a step in which desired information
is extracted and appropriately standardized and for-
malized for entry into the database. Both these steps
can be enhanced by suitably powerful Natural Lan-
guage Processing (NLP) technology. In this paper,
we address text mining as a step within the broader
context of developing both infrastructure and tools
for biocuration support within the Mouse Genome
Informatics (MGI) system at the Jackson Labora-
tories. We previously identified ?document triage?
as a crucial bottleneck (Ramakrishnan et al, 2010)
within MGI?s biocuration workflow.
Our research explores the use of information ex-
traction (IE) techniques to create richer linguis-
tic features than traditional bag-of-words models.
These features are employed by a classifier to per-
form the triage step. The features include lexico-
syntactic patterns as well as more-complex features,
such as a pattern coupled with its extracted noun,
where the noun is represented both as a lexical term
and by its semantic category. Our experimental re-
sults show that the IE-based enhanced features can
improve performance over unigram and bigram fea-
tures alone.
46
Evaluating the performance of BioNLP tools is
not trivial. So-called intrinsic metrics measure the
performance of a tool against some gold standard of
performance, while extrinsic ones (Alex et al, 2008)
measure how much the overall biocuration process
is benefited. Such metrics necessarily involve the
deployment of the software in-house for testing
by biocurators, and require a large-scale software-
engineering infrastructure effort. In this paper, we
present intrinsic evaluation results of full-text doc-
ument classification experiments to determine auto-
matically whether a paper should be considered of
interest to MGI curators. We plan in-house deploy-
ment and extrinsic evaluation in near-term work.
Our work should be considered as the first step of
a broader process within which (a) the features used
in this particular classification approach will be re-
engineered so that they may be dynamically recre-
ated in any new domain by a reusable component,
(b) this component is deployed into reusable infras-
tructure that also includes document-, annotation-
and feature-storage capabilities that support scaling
and reuse, and (c) the overall functionality can then
be delivered as a software application to biocurators
themselves for extrinsic evaluation in any domain
they choose. Within the ?SciKnowMine? project, we
are constructing such a framework (Ramakrishnan et
al., 2010), and this work reported here forms a pro-
totype component that we plan to incorporate into
a live application. We describe the underlying NLP
research here, and provide context for the work by
describing the overall design and implementation of
the SciKnowMine infrastructure.
1.1 Motivation
MGI?s biocurators use very specific guidelines for
triage that continuously evolve. These guidelines
are tailored to specific subcategories within MGI?s
triage task (phenotype, Gene Ontology1 (GO) term,
gene expression, tumor biology and chromosomal
location mapping). They help biocurators decide
whether a paper is relevant to one or more subcat-
egories. As an example, consider the guideline for
the phenotype category shown in Table 1.
This example makes clear that it is not sufficient
to match on relevant words like ?transgene? alone.
1http://www.geneontology.org/
?Select paper
If: it is about transgenes where a gene from any
species is inserted in mice and this results in
a phenotype.
Except: if the paper uses transgenes to
examine promoter function?.
Table 1: Sample triage guideline used by MGI biocura-
tors
To identify a paper as being ?within-scope? or ?out-
of-scope? requires that a biocurator understand the
context of the experiment described in the paper.
To check this we examined two sample papers; one
that matches the precondition of the above rule and
another that matches its exception. The first paper
(Sjo?gren et al, 2009) is about a transgene inser-
tion causing a pheotype and is a positive example
of the category phenotype, while the second paper
(Bouatia-Naji et al, 2010) is about the use of trans-
genes to study promoter function and is a negative
example for the same category.
Inspection of the negative-example paper illus-
trates the following issues concerning the language
used: (1) This paper is about transgene-use in study-
ing promoter function. Understanding this requires
the following background knowledge: (a) the two
genes mentioned in the title are transgenes; (b) the
phrase ?elevation of fasting glucose levels? in the ti-
tle represents an up-regulation phenotype event. (2)
Note that the word ?transgene? never occurs in the
entire negative-example paper. This suggests that
recognizing that a paper involves the use of trans-
genes requires annotation of domain-specific enti-
ties and a richer representation than that offered by
a simple bag-of-words model.
Similar inspection of the positive-example paper
reveals that (3) the paper contains experimental ev-
idence showing the phenotype resulting from the
transgene insertion. (4) The ?Materials and Meth-
ods? section of the positive-example paper clearly
identifies the construction of the transgene and the
?Results? section describes the development of the
transgenic mouse model used in the study. (3)
and (4) above suggest that domain knowledge about
complex biological phenomena (events) such as
phenotype and experimental protocol may be help-
ful for the triage task.
47
Together, points (1)?(4) suggest that different
sections of a paper contain additional important
context-specific clues. The example highlights the
complex nature of the triage task facing the MGI
biocurators. At present, this level of nuanced ?un-
derstanding? of content semantics is extremely hard
for machines to replicate. Nonetheless, merely treat-
ing the papers as a bag-of-words is unlikely to make
nuanced distinctions between positive and negative
examples with the level of precision and recall re-
quired in MGI?s triage task.
In this paper we therefore describe: (1) the design
and performance of a classifier that is enriched with
three types of features, all derived from informa-
tion extraction: (a) lexico-syntactic patterns, (b) pat-
terns coupled with lexical extractions, and (c) pat-
terns coupled with semantic extractions. We com-
pare the enriched classifier against classifiers that
use only unigram and bigram features; (2) the de-
sign of a biocuration application for MGI along with
the first prototype system where we emphasize the
infrastructure necessary to support the engineering
of domain-specific features of the kind described
in the examples above. Our application is based
on Unstructured Information Management Architec-
ture (UIMA) (Ferrucci and Lally, 2004), which is
a pipeline-based framework for the development of
software systems that analyze large volumes of un-
structured information.
2 Information Extraction for Triage
Classification
In this section, we present the information extraction
techniques that we used as the basis for our IE-based
features, and we describe the three types of IE fea-
tures that we incorporated into the triage classifier.
2.1 Information Extraction Techniques
Information extraction (IE) includes a variety of
techniques for extracting factual information from
text. We focus on pattern-based IE methods
that were originally designed for event extrac-
tion. Event extraction systems identify the role
fillers associated with events. For example, con-
sider the task of extracting information from dis-
ease outbreak reports, such as ProMed-mail arti-
cles (http://www.promedmail.org/). In contrast to a
named entity recognizer, which should identify all
mentions of diseases and people, an event extraction
system should only extract the diseases involved in
an outbreak incident and the people who were the
victims. Other mentions of diseases (e.g., in histori-
cal discussions) or people (e.g., doctors or scientists)
should be discarded.
We utilized the Sundance/AutoSlog software
package (Riloff and Phillips, 2004), which is freely
available for research. Sundance is an information
extraction engine that applies lexico-syntactic pat-
terns to extract noun phrases from specific linguistic
contexts. Sundance performs its own syntactic anal-
ysis, which includes morphological analysis, shal-
low parsing, clause segmentation, and syntactic role
assignment (i.e., identifying subjects and direct ob-
jects of verb phrases). Sundance labels verb phrases
with respect to active/passive voice, which is im-
portant for event role labelling. For example, ?Tom
Smith was diagnosed with bird flu? means that Tom
Smith is a victim, but ?Tom Smith diagnosed the el-
derly man with bird flu? means that the elderly man
is the victim.
Sundance?s information extraction engine can ap-
ply lexico-syntactic patterns to extract noun phrases
that participate in syntactic relations. Each pat-
tern represents a linguistic expression, and extracts
a noun phrase (NP) argument from one of three syn-
tactic positions: Subject, Direct Object, or Prepo-
sitional Phrase. Patterns may be defined manu-
ally, or they can be generated by the AutoSlog pat-
tern generator (Riloff, 1993), which automatically
generates patterns from a domain-specific text cor-
pus. AutoSlog uses 17 syntactic ?templates? that are
matched against the text. Lexico-syntactic patterns
are generated by instantiating the matching words in
the text with the syntactic template. For example,
five of AutoSlog?s syntactic templates are shown in
Table 2:
(a) <SUBJ> PassVP
(b) PassVP Prep <NP>
(c) <SUBJ> ActVP
(d) ActVP Prep <NP>
(e) Subject PassVP Prep <NP>
Table 2: Five example syntactic templates (PassVP
means passive voice verb phrase, ActVP means active
voice verb phrase)
48
Pattern (a) matches any verb phrase (VP) in a pas-
sive voice construction and extracts the Subject of
the VP. Pattern (b) matches passive voice VPs that
are followed by a prepositional phrase. The NP
in the prepositional phrase is extracted. Pattern (c)
matches any active voice VP and extracts its Subject,
while Pattern (d) matches active voice VPs followed
by a prepositional phrase. Pattern (e) is a more com-
plex pattern that requires a specific Subject2, passive
voice VP, and a prepositional phrase. We applied the
AutoSlog pattern generator to our corpus (described
in Section 3.1) to exhaustively generate every pat-
tern that occurs in the corpus.
As an example, consider the following sentence,
taken from an article in PLoS Genetics:
USP14 is endogenously expressed in
HEK293 cells and in kidney tissue derived
from wt mice.
<SUBJ> PassVP(expressed)
<SUBJ> ActVP(derived)
PassVP(expressed) Prep(in) <NP>
ActVP(derived) Prep(from) <NP>
Subject(USP14) PassVP(expressed) Prep(in) <NP>
Table 3: Lexico-syntactic patterns for the PLoS Genetics
sentence shown above.
AutoSlog generates five patterns from this sen-
tence, which are shown in Table 3:
The first pattern matches passive voice instances
of the verb ?expressed?, and the second pattern
matches active voice instances of the verb ?de-
rived?.3 These patterns rely on syntactic analysis,
so they will match any syntactically appropriate con-
struction. For example, the first pattern would match
?was expressed?, ?were expressed?, ?have been ex-
pressed? and ?was very clearly expressed?. The third
and fourth patterns represent the same two VPs but
also require the presence of a specific prepositional
phrase. The prepositional phrase does not need to
be adjacent to the VP, so long as it is attached to
the VP syntactically. The last pattern is very spe-
cific and will only match passive voice instances of
2Only the head nouns must match.
3Actually, the second clause is in reduced passive voice (i.e.,
tissue that was derived from mice), but the parser misidentifies
it as an active voice construction.
?expressed? that also have a Subject with a particular
head noun (?USP14?) and an attached prepositional
phrase with the preposition ?in?.
The example sentence contains four noun phrases,
which are underlined. When the patterns generated
by AutoSlog are applied to the sentence, they pro-
duce the following NP extractions (shown in bold-
face in Table 4):
<USP14> PassVP(expressed)
<kidney tissue> ActVP(derived)
PassVP(expressed) Prep(in) <HEK293 cells>
ActVP(derived) Prep(from) <wt mice>
Subject(USP14) PassVP(expressed) Prep(in) <HEK293
cells>
Table 4: Noun phrase extractions produced by Sundance
for the sample sentence.
In the next section, we explain how we use the in-
formation extraction system to produce rich linguis-
tic features for our triage classifier.
2.2 IE Pattern Features
For the triage classification task, we experimented
with four types of IE-based features: Patterns, Lexi-
cal Extractions, and Semantic Extractions.
The Pattern features are the lexico-syntactic IE
patterns. Intuitively, each pattern represents a phrase
or expression that could potentially capture contexts
associated with mouse genomics better than isolated
words (unigrams). We ran the AutoSlog pattern gen-
erator over the training set to exhaustively generate
every pattern that appeared in the corpus. We then
defined one feature for each pattern and gave it a
binary feature value (i.e., 1 if the pattern occurred
anywhere in the document, 0 otherwise).
We also created features that capture not just the
pattern expression, but also its argument. The Lex-
ical Extraction features represent a pattern paired
with the head noun of its extracted noun phrase.
Table 5 shows the Lexical Extraction features that
would be generated for the sample sentence shown
earlier. Our hypothesis was that these features could
help to distinguish between contexts where an activ-
ity is relevant (or irrelevant) to MGI because of the
combination of an activity and its argument.
The Lexical Extraction features are very specific,
requiring the presence of multiple terms. So we
49
PassVP(expressed), USP14
ActVP(derived), tissue
PassVP(expressed) Prep(in), cells
ActVP(derived) Prep(from), mice
Subject(USP14) PassVP(expressed) Prep(in), cells
Table 5: Lexical Extraction features
also experimented with generalizing the extracted
nouns by replacing them with a semantic category.
To generate a semantic dictionary for the mouse ge-
nomics domain, we used the Basilisk bootstrapping
algorithm (Thelen and Riloff, 2002). Basilisk has
been used previously to create semantic lexicons for
terrorist events (Thelen and Riloff, 2002) and senti-
ment analysis (Riloff et al, 2003), and recent work
has shown good results for bioNLP domains using
similar bootstrapping algorithms (McIntosh, 2010;
McIntosh and Curran, 2009).
As input, Basilisk requires a domain-specific text
corpus (unannotated) and a handful of seed nouns
for each semantic category to be learned. A boot-
strapping algorithm then iteratively hypothesizes ad-
ditional words that belong to each semantic cat-
egory based on their association with the seed
words in pattern contexts. The output is a lexicon
of nouns paired with their corresponding semantic
class. (e.g., liver : BODY PART).
We used Basilisk to create a lexicon for eight se-
mantic categories associated with mouse genomics:
BIOLOGICAL PROCESS, BODY PART, CELL TYPE,
CELLULAR LOCATION, BIOLOGICAL SUBSTANCE,
EXPERIMENTAL REAGENT, RESEARCH SUBJECT,
TUMOR. To choose the seed nouns, we parsed
the training corpus, ranked all of the nouns by fre-
quency4, and selected the 10 most frequent, unam-
biguous nouns belonging to each semantic category.
The seed words that we used for each semantic cat-
egory are shown in Table 6.
Finally, we defined Semantic Extraction features
as a pair consisting of a pattern coupled with the
semantic category of the noun that it extracted. If
the noun was not present in the semantic lexicons,
then no feature was created. The Basilisk-generated
lexicons are not perfect, so some entries will be in-
correct. But our hope was that replacing the lexical
terms with semantic categories might help the clas-
4We only used nouns that occurred as the head of a NP.
BIOLOGICAL PROCESS: expression, ac-
tivity, activation, development, function,
production, differentiation, regulation, re-
duction, proliferation
BODY PART: brain, muscle, thymus, cor-
tex, retina, skin, spleen, heart, lung, pan-
creas
CELL TYPE: neurons, macrophages, thy-
mocytes, splenocytes, fibroblasts, lym-
phocytes, oocytes, monocytes, hepato-
cytes, spermatocytes
CELLULAR LOCATION: receptor, nu-
clei, axons, chromosome, membrane, nu-
cleus, chromatin, peroxisome, mitochon-
dria, cilia
BIOLOGICAL SUBSTANCE: antibody,
lysates, kinase, cytokines, peptide, anti-
gen, insulin, ligands, peptides, enzyme
EXPERIMENTAL REAGENT: buffer,
primers, glucose, acid, nacl, water, saline,
ethanol, reagents, paraffin
RESEARCH SUBJECT: mice, embryos,
animals, mouse, mutants, patients, litter-
mates, females, males, individuals
TUMOR: tumors, tumor, lymphomas,
tumours, carcinomas, malignancies,
melanoma, adenocarcinomas, gliomas,
sarcoma
Table 6: Seed words given to Basilisk
sifier learn more general associations. For exam-
ple, ?PassVP(expressed) Prep(in), CELLULAR LO-
CATION? will apply much more broadly than the
corresponding lexical extraction with just one spe-
cific cellular location (e.g., ?mitochondria?).
Information extraction patterns and their argu-
ments have been used for text classification in pre-
vious work (Riloff and Lehnert, 1994; Riloff and
Lorenzen, 1999), but the patterns and arguments
were represented separately and the semantic fea-
tures came from a hand-crafted dictionary. In con-
trast, our work couples each pattern with its ex-
tracted argument as a single feature, uses an auto-
matically generated semantic lexicon, and is the first
application of these techniques to the biocuration
triage task.
50
3 Results
3.1 Data Set
For our experiments in this paper we use articles
within the PubMed Central (PMC) Open Access
Subset5. From this subset we select all articles that
are published in journals of interest to biocurators
at MGI. This results in a total of 14,827 documents
out of which 981 have been selected manually by
MGI biocurators as relevant (referred to as IN docu-
ments). This leaves 13,846 that are presumably out
of scope (referred to as OUT documents), although
it was not guaranteed that all of them had been man-
ually reviewed so some relevant documents could be
included as well. (We plan eventually to present to
the biocurators those papers not included by them
but nonetheless selected by our tools as IN with
high confidence, for possible reclassification. Such
changes will improve the system?s evaluated score.)
As preprocessing for the NLP tools, we split
the input text into sentences using the Lin-
gua::EN::Sentence perl package. We trimmed non-
alpha-numerics attached before and after words.
We also removed stop words using the Lin-
gua::EN::StopWords package.
3.2 Classifier
We used SVM Light6(Joachims, 1999) for all of our
experiments. We used a linear kernel and a tol-
erance value of 0.1 for QP solver termination. In
preliminary experiments, we observed that the cost
factor (C value) made a big difference in perfor-
mance. In SVMs, the cost factor represents the
importance of penalizing errors on the training in-
stances in comparison to the complexity (general-
ization) of the model. We observed that higher val-
ues of C produced increased recall, though at the ex-
pense of some precision. We used a tuning set to
experiment with different values of C, trying a wide
range of powers of 2. We found that C=1024 gen-
erally produced the best balance of recall and preci-
sion, so we used that value throughout our experi-
ments.
5http://www.ncbi.nlm.nih.gov/pmc/about/
openftlist.html
6http://svmlight.joachims.org/
3.3 Experiments
We randomly partitioned our text corpus into 5 sub-
sets of 2,965 documents each.7 We used the first 4
subsets as the training set, and reserved the fifth sub-
set as a blind test set.
In preliminary experiments, we found that the
classifiers consistently benefitted from feature se-
lection when we discarded low-frequency features.
This helps to keep the classifier from overfitting to
the training data. For each type of feature, we set
a frequency threshold ? and discarded any features
that occurred fewer than ? times in the training set.
We chose these ? values empirically by performing
4-fold cross-validation on the training set. We eval-
uated ? values ranging from 1 to 50, and chose the
value that produced the highest F score. The ? val-
ues that were selected are: 7 for unigrams, 50 for
bigrams, 35 for patterns, 50 for lexical extractions,
and 5 for semantic extractions.
Finally, we trained an SVM classifier on the en-
tire training set and evaluated the classifier on the
test set. We computed Precision (P), Recall (R), and
the F score, which is the harmonic mean of preci-
sion and recall. Precision and recall were equally
weighted, so this is sometimes called an F1 score.
Table 7 shows the results obtained by using each
of the features in isolation. The lexical extraction
features are shown as ?lexExts? and the semantic ex-
traction features are shown as ?semExts?. We also
experimented with using a hybrid extraction fea-
ture, ?hybridExts?, which replaced a lexical extrac-
tion noun with its semantic category when one was
available but left the noun as the extraction term
when no semantic category was known.
Table 7 shows that the bigram features produced
the best Recall (65.87%) and F-Score (74.05%),
while the hybrid extraction features produced the
best Precision (85.52%) but could not match the bi-
grams in terms of recall. This is not surprising be-
cause the extraction features on their own are quite
specific, often requiring 3-4 words to match.
Next, we experimented with adding the IE-based
features to the bigram features to allow the classifier
to choose among both feature sets and get the best
of both worlds. Combining bigrams with IE-based
7Our 5-way random split left 2 documents aside, which we
ignored for our experiments.
51
Feature P R F
unigrams 79.75 60.58 68.85
bigrams 84.57 65.87 74.05
patterns 78.98 59.62 67.95
lexExts 76.54 59.62 67.03
semExts 72.39 46.63 56.73
hybridExts 85.52 59.62 70.25
bigrams + patterns 84.87 62.02 71.67
bigrams + lexExts 85.28 66.83 74.93
bigrams + semExts 85.43 62.02 71.87
bigrams + hybridExts 87.10 64.90 74.38
Table 7: Triage classifier performance using different sets
of features.
features did in fact yield the best results. Using bi-
grams and lexical extraction features achieved both
the highest recall (66.83%) and the highest F score
(74.93%). In terms of overall F score, we see a rela-
tively modest gain of about 1% by adding the lexical
extraction features to the bigram features, which is
primarily due to the 1% gain in recall.
However, precision is of paramount importance
for many applications because users don?t want to
wade through incorrect predictions. So it is worth
noting that adding the hybrid extraction features to
the bigram features produced a 2.5% increase in pre-
cision (84.57% ? 87.10%) with just a 1% drop in
recall. This recall/precision trade-off is likely to be
worthwhile for many real-world application settings,
including biocuration.
4 Biocuration Application for MGI
Developing an application that supports MGI biocu-
rators necessitates an application design that mini-
mally alters existing curation workflows while main-
taining high classification F-scores (intrinsic mea-
sures) and speeding up the curation process (extrin-
sic measures). We seek improvements with respect
to intrinsic measures by engineering context-specific
features and seek extrinsic evaluations by instru-
menting the deployed triage application to record us-
age statistics that serve as input to extrinsic evalua-
tion measures.
4.1 Software Architecture
As stated earlier, one of our major goals is to build,
deploy, and extrinsically evaluate an NLP-assisted
curation application (Alex et al, 2008) for triage at
MGI. By definition, an extrinsic evaluation of our
triage application requires its deployment and sub-
sequent tuning to obtain optimal performance with
respect to extrinsic evaluation criteria. We antici-
pate that features, learning parameters, and training
data distributions may all need to be adjusted during
a tuning process. Cognizant of these future needs,
we have designed the SciKnowMine system so as
to integrate the various components and algorithms
using the UIMA infrastructure. Figure 1 shows a
schematic of SciKnowMine?s overall architecture.
4.1.1 Building configurable & reusable UIMA
pipelines
The experiments we have presented in this paper
have been conducted using third party implementa-
tions of a variety of algorithms implemented on a
wide variety of platforms. We use SVMLight to
train a triage classifier on features that were pro-
duced by AutoSlog and Sundance on sentences iden-
tified by the perl package Lingua::EN::Sentence.
Each of these types of components has either been
reimplemented or wrapped as a component reusable
in UIMA pipelines within the SciKnowMine in-
frastructure. We hope that building such a li-
brary of reusable components will help galvanize the
BioNLP community towards standardization of an
interoperable and open-access set of NLP compo-
nents. Such a standardization effort is likely to lower
the barrier-of-entry for NLP researchers interested in
applying their algorithms to knowledge engineering
problems in Biology (such as biocuration).
4.1.2 Storage infrastructure for annotations &
features
As we develop richer section-specific and
context-specific features we anticipate the need for
provenance pertaining to classification decisions for
a given paper. We have therefore built an Annotation
Store and a Feature Store collectively referred to as
the Classification Metadata Store8 in Figure 1. Fig-
ure 1 also shows parallel pre-processing populating
the annotation store. We are working on develop-
ing parallel UIMA pipelines that extract expensive
(resource & time intensive) features (such as depen-
8Our classification metadata store has been implemented us-
ing Solr http://lucene.apache.org/solr/
52
dency parses).The annotation store holds features
produced by pre-processing pipelines. The annota-
tion store has been designed to support query-based
composition of feature sets specific to a classifica-
tion run. These feature sets can be asserted to the
feature store and reused later by any pipeline. This
design provides us with the flexibility necessary to
experiment with a wide variety of features and tune
our classifiers in response to feedback from biocura-
tors.
5 Discussions & Conclusions
In this paper we have argued the need for richer se-
mantic features for the MGI biocuration task. Our
results show that simple lexical and semantic fea-
tures used to augment bigram features can yield
higher classification performance with respect to in-
trinsic metrics (such as F-Score). It is noteworthy
that using a hybrid of lexical and semantic features
results in the highest precision of 87%.
In our motivating example, we have proposed
the need for sectional-zoning of articles and have
demonstrated that certain zones like the ?Materi-
als and Methods? section can contain contextual
features that might increase classification perfor-
mance. It is clear from the samples of MGI man-
ual classification guidelines that biocurators do, in
fact, use zone-specific features in triage. It there-
fore seems likely that section specific feature ex-
traction might result in better classification perfor-
mance in the triage task. Our preliminary analysis of
the MGI biocuration guidelines suggests that exper-
imental procedures described in the ?Materials and
Methods? seem to be a good source of triage clues.
We therefore propose to investigate zone and context
specific features and the explicit use of domain mod-
els of experimental procedure as features for docu-
ment triage.
We have also identified infrastructure needs aris-
ing within the construction of a biocuration applica-
tion. In response we have constructed preliminary
versions of metadata stores and UIMA pipelines to
support MGI?s biocuration. Our next step is to de-
ploy a prototype assisted-curation application that
uses a classifier trained on the best performing fea-
tures discussed in this paper. This application will
be instrumented to record usage statistics for use in
extrinsic evaluations (Alex et al, 2008). We hope
that construction on such an application will also
engender the creation of an open environment for
NLP scientists to apply their algorithms to biomedi-
cal corpora in addressing biomedical knowledge en-
gineering challenges.
6 Acknowledgements
This research is funded by the U.S. National Sci-
ence Foundation under grant #0849977 for the
SciKnowMine project (http://sciknowmine.
isi.edu/). We wish to acknowledge Kevin Co-
hen for helping us collect the seed terms for Basilisk
and Karin Verspoor for discussions regarding feature
engineering.
References
[Alex et al2008] Beatrice Alex, Claire Grover, Barry
Haddow, Mijail Kabadjov, Ewan Klein, Michael
Matthews, Stuart Roebuck, Richard Tobin, and Xin-
glong Wang. 2008. Assisted curation: does text min-
ing really help? Pacific Symposium On Biocomputing,
567:556?567.
[Bouatia-Naji et al2010] Nabila Bouatia-Naji, Ame?lie
Bonnefond, Devin A Baerenwald, Marion Marchand,
Marco Bugliani, Piero Marchetti, Franc?ois Pattou,
Richard L Printz, Brian P Flemming, Obi C Umu-
nakwe, Nicholas L Conley, Martine Vaxillaire, Olivier
Lantieri, Beverley Balkau, Michel Marre, Claire Le?vy-
Marchal, Paul Elliott, Marjo-Riitta Jarvelin, David
Meyre, Christian Dina, James K Oeser, Philippe
Froguel, and Richard M O?Brien. 2010. Genetic and
functional assessment of the role of the rs13431652-
A and rs573225-A alleles in the G6PC2 promoter that
are strongly associated with elevated fasting glucose
levels. Diabetes, 59(10):2662?2671.
[Bourne and McEntyre2006] Philip E Bourne and Jo-
hanna McEntyre. 2006. Biocurators: Contributors to
the World of Science. PLoS Computational Biology,
2(10):1.
[Cohen and Hersh2006] Aaron M Cohen and William R
Hersh. 2006. The TREC 2004 genomics track cate-
gorization task: classifying full text biomedical docu-
ments. Journal of Biomedical Discovery and Collab-
oration, 1:4.
[Ferrucci and Lally2004] D Ferrucci and A Lally. 2004.
Building an example application with the Unstructured
Information Management Architecture. IBM Systems
Journal, 43(3):455?475.
53
Citation
Store
Feature
Store 
(token trigrams, 
stemmed bigrams)
Document
Store
MGI
training
Corpus
Digital Library
Parallel
pre-processing pipelines
Classification Metadata Store
Annotation Store 
(token, Sundance 
patterns, parse trees,)
Classifier training
pipeline
trained triage 
classification 
model
Ranked Triage ResultsDigital Library
MGI Biocuration Application
Newly 
published
papers
Figure 1: Design schematic of the MGI biocuration application. The components of the application are: (A) Digital
Library composed of a citation store and document store. (B) Pre-processing UIMA pipelines which are a mecha-
nism to pre-extract standard features such as parse trees, tokenizations etc. (C) Classification Metadata Store which
is composed of an Annotation Store for the pre-extracted standard features from (B), and a Feature Store to hold de-
rived features constructed from the standard ones in the Annotation Store. (D) Classifier training pipeline. (E) MGI
Biocuration Application.
[Hersh W2005] Yang J Bhupatiraju RT Roberts P M.
Hearst M Hersh W, Cohen AM. 2005. TREC 2005
genomics track overview. In The Fourteenth Text Re-
trieval Conference.
[Joachims1999] Thorsten Joachims. 1999. Making
Large-Scale SVM Learning Practical. Advances in
Kernel Methods Support Vector Learning, pages 169?
184.
[McIntosh and Curran2009] T. McIntosh and J. Curran.
2009. Reducing Semantic Drift with Bagging and
Distributional Similarity. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics.
[McIntosh2010] Tara McIntosh. 2010. Unsupervised dis-
covery of negative categories in lexicon bootstrapping.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, number Oc-
tober, pages 356?365. Association for Computational
Linguistics.
[Ramakrishnan et al2010] Cartic Ramakrishnan, William
A Baumgartner Jr, Judith A Blake, Gully A P C
Burns, K Bretonnel Cohen, Harold Drabkin, Janan
Eppig, Eduard Hovy, Chun-Nan Hsu, Lawrence E
Hunter, Tommy Ingulfsen, Hiroaki Rocky Onda,
Sandeep Pokkunuri, Ellen Riloff, and Karin Verspoor.
2010. Building the Scientific Knowledge Mine ( Sci-
KnowMine 1 ): a community- driven framework for
text mining tools in direct service to biocuration. In
proceeding of Workshop ?New Challenges for NLP
Frameworks? collocated with The seventh interna-
tional conference on Language Resources and Eval-
uation (LREC) 2010.
[Riloff and Lehnert1994] E. Riloff and W. Lehnert. 1994.
Information Extraction as a Basis for High-Precision
Text Classification. ACM Transactions on Information
54
Systems, 12(3):296?333, July.
[Riloff and Lorenzen1999] E. Riloff and J. Lorenzen.
1999. Extraction-based text categorization: Generat-
ing domain-specific role relationships automatically.
In Tomek Strzalkowski, editor, Natural Language In-
formation Retrieval. Kluwer Academic Publishers.
[Riloff and Phillips2004] E. Riloff and W. Phillips. 2004.
An Introduction to the Sundance and AutoSlog Sys-
tems. Technical Report UUCS-04-015, School of
Computing, University of Utah.
[Riloff et al2003] E. Riloff, J. Wiebe, and T. Wilson.
2003. Learning Subjective Nouns using Extraction
Pattern Bootstrapping. In Proceedings of the Seventh
Conference on Natural Language Learning (CoNLL-
2003), pages 25?32.
[Riloff1993] E. Riloff. 1993. Automatically Construct-
ing a Dictionary for Information Extraction Tasks. In
Proceedings of the 11th National Conference on Arti-
ficial Intelligence.
[Sjo?gren et al2009] Klara Sjo?gren, Marie Lagerquist,
Sofia Moverare-Skrtic, Niklas Andersson, Sara H
Windahl, Charlotte Swanson, Subburaman Mohan,
Matti Poutanen, and Claes Ohlsson. 2009. Elevated
aromatase expression in osteoblasts leads to increased
bone mass without systemic adverse effects. Journal
of bone and mineral research the official journal of
the American Society for Bone and Mineral Research,
24(7):1263?1270.
[Thelen and Riloff2002] M. Thelen and E. Riloff. 2002.
A Bootstrapping Method for Learning Semantic Lexi-
cons Using Extraction Pa ttern Contexts. In Proceed-
ings of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 214?221.
55
