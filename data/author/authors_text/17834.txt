Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411?1416,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Walk-based Semantically Enriched Tree Kernel
Over Distributed Word Representations
Shashank Srivastava1 Dirk Hovy2 Eduard Hovy1
(1) Carnegie Mellon University, Pittsburgh
(2) Center for Language Technology, University of Copenhagen, Denmark
{ssrivastava,hovy}@cmu.edu, mail@dirkhovy.com
Abstract
In this paper, we propose a walk-based graph
kernel that generalizes the notion of tree-
kernels to continuous spaces. Our proposed
approach subsumes a general framework for
word-similarity, and in particular, provides a
flexible way to incorporate distributed repre-
sentations. Using vector representations, such
an approach captures both distributional se-
mantic similarities among words as well as the
structural relations between them (encoded as
the structure of the parse tree). We show an ef-
ficient formulation to compute this kernel us-
ing simple matrix operations. We present our
results on three diverse NLP tasks, showing
state-of-the-art results.
1 Introduction
Capturing semantic similarity between sentences
is a fundamental issue in NLP, with applications in
a wide range of tasks. Previously, tree kernels based
on common substructures have been used to model
similarity between parse trees (Collins and Duffy,
2002; Moschitti, 2004; Moschitti, 2006b). These
kernels encode a high number of latent syntactic
features within a concise representation, and com-
pute the similarity between two parse trees based
on the matching of node-labels (words, POS tags,
etc.), as well as the overlap of tree structures. While
this is sufficient to capture syntactic similarity, it
does not capture semantic similarity very well, even
when using discrete semantic types as node labels.
This constrains the utility of many traditional
tree kernels in two ways: i) two sentences that
are syntactically identical, but have no semantic
similarity can receive a high matching score (see
Table 1, top) while ii) two sentences with only local
syntactic overlap, but high semantic similarity can
receive low scores (see Table 1, bottom).
tree pairs semantic syntactic score
?
?
high
?
?
low
love
we toys
crush
they puppies
kissed
she cat
gave
she
her
kiss
a
friend
feline
green little
her
Table 1: Traditional tree kernels do not capture se-
mantic similarity
In contrast, distributional vector representations
of words have been successful in capturing fine-
grained semantics, but lack syntactic knowledge.
Resources such as Wordnet, dictionaries and on-
tologies that encode different semantic perspectives
can also provide additional knowledge infusion.
In this paper, we describe a generic walk-based
graph kernel for dependency parse trees that sub-
sumes general notions of word-similarity, while
focusing on vector representations of words to
capture lexical semantics. Through a convolutional
framework, our approach takes into account the
distributional semantic similarities between words
in a sentence as well as the structure of the parse
tree. Our main contributions are:
1. We present a new graph kernel for NLP that ex-
tends to distributed word representations, and
diverse word similarity measures.
2. Our proposed approach provides a flexible
framework for incorporating both syntax and
semantics of sentence level constructions.
3. Our generic kernel shows state-of-the-art per-
formance on three eclectic NLP tasks.
1411
2 Related Work
Tree kernels in NLP Tree kernels have been ex-
tensively used to capture syntactic information about
parse trees in tasks such as parsing (Collins and
Duffy, 2002), NER (Wang et al, 2010; Cumby and
Roth, 2003), SRL (Moschitti et al, 2008) and rela-
tion extraction (Qian et al, 2008). These kernels are
based on the paradigm that parse trees are similar if
they contain many common substructures, consist-
ing of nodes with identical labels (Vishwanathan and
Smola, 2003; Collins and Duffy, 2002). Moschitti
(2006a) proposed a partial tree kernel that adds flex-
ibility in matching tree substructures. Croce et al
(2011) introduce a lexical semantic tree kernel that
incorporates continuous similarity values between
node labels, albeit with a different focus than ours
and would not match words with different POS. This
would miss the similarity of ?feline friend? and ?cat?
in our examples, as it requires matching the adjective
?feline? with ?cat?, and verb ?kissed? with ?kiss?.
Walk based kernels Kernels for structured data
derive from the seminal Convolution Kernel for-
malism by Haussler (1999) for designing kernels
for structured objects through local decompositions.
Our proposed kernel for parse trees is most closely
associated with the random walk-based kernels de-
fined by Gartner et al (2003) and Kashima et al
(2003). The walk-based graph kernels proposed by
Gartner et al (2003) count the common walks be-
tween two input graphs, using the adjacency matrix
of the product graph. This work extends to graphs
with a finite set of edge and node labels by appro-
priately modifying the adjacency matrix. Our kernel
differs from these kernels in two significant ways: (i)
Our method extends beyond label matching to con-
tinuous similarity metrics (this conforms with the
very general formalism for graph kernels in Vish-
wanathan et al (2010)). (ii) Rather than using the
adjacency matrix to model edge-strengths, we mod-
ify the product graph and the corresponding adja-
cency matrix to model node similarities.
3 Vector Tree Kernels
In this section, we describe our kernel and an al-
gorithm to compute it as a simple matrix multiplica-
tion formulation.
3.1 Kernel description
The similarity kernel K between two dependency
trees can be defined as:
K(T1, T2) =
?
h1?T1,h2?T2
len(h1)=len(h2)
k(h1, h2)
where the summation is over pairs of equal length
walks h1 and h2 on the trees T1 and T2 respec-
tively. The similarity between two n length walks,
k(h1, h2), is in turn given by the pairwise similari-
ties of the corresponding nodes vih in the respective
walks, measured via the node similarity kernel ?:
k(h1, h2) =
n?
i:1
?(vh1i , v
h2
i )
In the context of parse trees, nodes vh1i and v
h2
i cor-
respond to words in the two parse trees, and thus can
often be conveniently represented as vectors over
distributional/dependency contexts. The vector rep-
resentation allows us several choices for the node
kernel function ?. In particular, we consider:
1. Gaussian : ?(v1, v2) = exp
(
? ?v1?v2?
2
2?2
)
2. Positive-Linear: ?(v1, v2) = max(vT1 v2, 0)
3. Sigmoid: ?(v1, v2) =
(
1 + tanh(?vT1 v2)
)
/2
We note that the kernels above take strictly non-
negative values in [0, 1] (assuming word vector rep-
resentations are normalized). Non-negativity is nec-
essary, since we define the walk kernel to be the
product of the individual kernels. As walk kernels
are products of individual node-kernels, bounded-
ness by 1 ensures that the kernel contribution does
not grow arbitrarily for longer length walks.
The kernel function K puts a high similarity
weight between parse trees if they contain com-
mon walks with semantically similar words in corre-
sponding positions. Apart from the Gaussian kernel,
the other two kernels are based on the dot-product
of the word vector representations. We observe that
the positive-linear kernel defined above is not a Mer-
cer kernel, since the max operation makes it non-
positive semidefinite (PSD). However, this formu-
lation has desirable properties, most significant be-
ing that all walks with one or more node-pair mis-
matches are strictly penalized and add no score to
1412
the tree-kernel. This is a more selective condition
than the other two kernels, where mediocre walk
combinations could also add small contributions to
the score. The sigmoid kernel is also non-PSD, but
is known to work well empirically (Boughorbel et
al., 2005). We also observe while the summation in
the kernel is over equal length walks, the formalism
can allow comparisons over different length paths by
including self-loops at nodes in the tree.
With a notion of similarity between words that
defines the local node kernels, we need computa-
tional machinery to enumerate all pairs of walks
between two trees, and compute the summation
over products in the kernel K(T1, T2) efficiently.
We now show a convenient way to compute this as
a matrix geometric series.
3.2 Matrix Formulation for Kernel
Computation
Walk-based kernels compute the number of com-
mon walks using the adjacency matrix of the prod-
uct graph (Gartner et al, 2003). In our case, this
computation is complicated by the fact that instead
of counting common walks, we need to compute a
product of node-similarities for each walk. Since
we compute similarity scores over nodes, rather than
edges, the product for a walk of length n involves
n+ 1 factors.
However, we can still compute the tree kernel K
as a simple sum of matrix products. Given two trees
T (V,E) and T ?(V ?, E?), we define a modified prod-
uct graph G(Vp, Ep) with an additional ghost node
u added to the vertex set. The vertex and edge sets
for the modified product graph are given as:
Vp := {(vi1, vj1
?) : vi1 ? V, vj1
? ? V ?} ? u
Ep := {((vi1, vj1
?), (vi2, vj2
?)) : (vi1, vi2) ? E,
(vj1
?, vj2
?)) ? E?}
?
{(u, (vi1, vj1
?)) : vi1 ? V, vj1
? ? V ?}
The modified product graph thus has additional
edges connecting u to all other nodes. In our for-
mulation, u now serves as a starting location for all
random walks on G, and a k + 1 length walk of G
corresponds to a pair of k length walks on T and T ?.
We now define the weighted adjacency matrixW for
G, which incorporates the local node kernels.
W(vi1,vj1?),(vi2,vj2?) =
{
0 : ((vi1,vj1?),(vi2,vj2?)) /? Ep
?(vi2, vj2?) : otherwise
Wu,(vi1,vj1?) = ?(vi1, vj1
?)
W(v,u) = 0 ? v ? Vp
There is a straightforward bijective mapping from
walks on G starting from u to pairs of walks on T
and T ?. Restricting ourselves to the case when the
first node of a k + 1 length walk is u, the next k
steps allow us to efficiently compute the products of
the node similarities along the k nodes in the corre-
sponding k length walks in T and T ?. Given this ad-
jacency matrix for G, the sum of values of k length
walk kernels is given by the uth row of the (k+1)th
exponent of the weighted adjacency matrix (denoted
asW k+1). This corresponds to k+1 length walks on
G starting from u and ending at any node. Specif-
ically, Wu,(vi,v?j) corresponds to the sum of similar-
ities of all common walks of length n in T and T ?
that end in vi in T and v?j in T
?. The kernel K for
walks upto length N can now be calculated as :
K(T, T ?) =
|Vp|?
i
Su,i
where
S = W +W 2 + ...WN+1
We note that in out formulation, longer walks are
naturally discounted, since they involve products of
more factors (generally all less than unity).
The above kernel provides a similarity measure
between any two pairs of dependency parse-trees.
Depending on whether we consider directional re-
lations in the parse tree, the edge set Ep changes,
while the procedure for the kernel computation re-
mains the same. Finally, to avoid larger trees yield-
ing larger values for the kernel, we normalize the
kernel by the number of edges in the product graph.
4 Experiments
We evaluate the Vector Tree Kernel (VTK) on
three NLP tasks. We create dependency trees using
the FANSE parser (Tratz and Hovy, 2011), and
use distribution-based SENNA word embeddings
by Collobert et al (2011) as word representations.
These embeddings provide low-dimensional vector
1413
representations of words, while encoding distribu-
tional semantic characteristics. We use LibSVM for
classification. For sake of brevity, we only report
results for the best performing kernel.
We first consider the Cornell Sentence Polarity
dataset by Pang and Lee (2005). The task is to
identify the polarity of a given sentence. The
data consists of 5331 sentences from positive and
negative movie reviews. Many phrases denoting
sentiments are lexically ambiguous (cf. ?terribly
entertaining? vs ?terribly written?), so simple lexi-
cal approaches are not expected to work well here,
while syntactic context could help disambiguation.
Next, we try our approach on the MSR paraphrase
corpus. The data contains a training set of 4077
pairs of sentences, annotated as paraphrases and
non-paraphrases, and a test-set of 1726 sentence
pairs. Each instance consists of a pair of sentences,
so the VTK cannot be directly used by a kernel
machine for classification. Instead, we generate
16 kernel values based for each pair on different
parameter settings of the kernel, and feed these as
features to a linear SVM.
We finally look at the annotated Metaphor corpus
by (Hovy et al, 2013). The dataset consists of sen-
tences with specified target phrases. The task here is
to classify the target use as literal or metaphorical.
We focus on target phrases by upweighting walks
that pass through target nodes. This is done by
simply multiplying the corresponding entries in the
adjacency matrix by a constant factor.
5 Results
5.1 Sentence Polarity Dataset
Prec Rec F1 Acc
Albornoz et al0.63 ? ? 0.63
WNA+synsets 0.61 ? ? 0.61
WNA 0.53 ? ? 0.51
DSM 0.54 0.55 0.55 0.54
SSTK 0.49 0.48 0.48 0.49
VTK 0.65 0.58 0.62 0.67
Table 2: Results on Sentence Polarity dataset
On the polarity data set, Vector Tree Kernel
(VTK) significantly outperforms the state-of-the-art
method by Carrillo de Albornoz et al (2010), who
use a hybrid model incorporating databases of af-
fective lexicons, and also explicitly model the ef-
fect of negation and quantifiers (see Table 2). Lex-
ical approaches using pairwise semantic similarity
of SENNA embeddings (DSM), as well as Word-
net Affective Database-based (WNA) labels perform
poorly (Carrillo de Albornoz et al, 2010), showing
the importance of syntax for this particular problem.
On the other hand, a syntactic tree kernel (SSTK)
that ignores distributional semantic similarity be-
tween words, fails as expected.
5.2 MSR Paraphrase Dataset
Prec Rec F1 Acc
BASE 0.72 0.86 0.79 0.69
Zhang et al0.74 0.88 0.81 0.72
Qiu et al0.73 0.93 0.82 0.72
Malakasiotis 0.74 0.94 0.83 0.74
Finch 0.77 0.90 0.83 0.75
VTK 0.72 0.95 0.82 0.72
Table 3: Results on MSR Paraphrase corpus
On the MSR paraphrase corpus, VTK performs
competitively against state-of-the-art-methods. We
expected paraphrasing to be challenging to our
method, since it can involve little syntactic overlap.
However, data analysis reveals that the corpus gener-
ally contains sentence pairs with high syntactic sim-
ilarity. Results for this task are encouraging since
ours is a general approach, while other systems use
multiple task-specific features like semantic role la-
bels, active-passive voice conversion, and synonymy
resolution. In the future, incorporating such features
to VTK should further improve results for this task .
5.3 Metaphor Identification
Acc P R F1
CRF 0.69 0.74 0.50 0.59
SVM+DSM 0.70 0.63 0.80 0.71
SSTK 0.75 0.70 0.80 0.75
VTK 0.76 0.67 0.87 0.76
Table 4: Results on Metaphor dataset
On the Metaphor corpus, VTK improves the pre-
vious score by Hovy et al (2013), whose approach
uses an conjunction of lexical and syntactic tree ker-
nels (Moschitti, 2006b), and distributional vectors.
VTK identified several templates of metaphor usage
such as ?warm heart? and ?cold shoulder?. We look
towards approaches for automatedly mining such
metaphor patterns from a corpus.
6 Conclusion
We present a general formalism for walk-based
kernels to evaluate similarity of dependency trees.
1414
Our method generalizes tree kernels to take dis-
tributed representations of nodes as input, and cap-
ture both lexical semantics and syntactic structures
of parse trees. Our approach has tunable parame-
ters to look for larger or smaller syntactic constructs.
Our experiments shows state-of-the-art performance
on three diverse NLP tasks. The approach can gen-
eralize to any task involving structural and local sim-
ilarity, and arbitrary node similarity measures.
References
Sabri Boughorbel, Jean-Philippe Tarel, and Nozha Bouje-
maa. 2005. Conditionally positive definite kernels for
svm based image recognition. In ICME, pages 113?
116.
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gerva?s. 2010. A hybrid approach to emotional sen-
tence polarity and intensity classification. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 153?161. Associa-
tion for Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association for
computational linguistics, pages 263?270. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1034?1046. Association for
Computational Linguistics.
Chad Cumby and Dan Roth. 2003. On kernel methods
for relational learning. In In Proc. of the International
Conference on Machine Learning, pages 107?114.
Andrew Finch. 2005. Using machine translation evalu-
ation techniques to determine sentence-level semantic
equivalence. In In IWP2005.
Thomas Gartner, Peter Flach, and Stefan Wrobel. 2003.
On graph kernels: Hardness results and efficient al-
ternatives. In Proceedings of the Annual Conference
on Computational Learning Theory, pages 129?143.
Springer.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical Report Technical Report UCS-
CRL-99-10, UC Santa Cruz.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Proceed-
ings of NAACL HLT, Meta4NLP Workshop.
Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi.
2003. Marginalized kernels between labeled graphs.
In Proceedings of the Twentieth International Con-
ference on Machine Learning, pages 321?328. AAAI
Press.
Prodromos Malakasiotis. 2009. Paraphrase recognition
using machine learning to combine similarity mea-
sures. In Proceedings of the ACL-IJCNLP 2009 Stu-
dent Research Workshop, ACLstudent ?09, pages 27?
35, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 335?es. Association for
Computational Linguistics.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318?329.
Springer.
Alessandro Moschitti. 2006b. Making Tree Kernels
Practical for Natural Language Learning. In In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 697?704. Association for Computational Lin-
guistics.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 18?26, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 142?149.
1415
Stephen Tratz and Eduard Hovy. 2011. A fast, accu-
rate, non-projective, semantically-enriched parser. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?11, pages
1257?1268, Stroudsburg, PA, USA. Association for
Computational Linguistics.
S. V. N. Vishwanathan and Alexander J. Smola. 2003.
Fast kernels for string and tree matching. In Advances
In Neural Information Processing Systems 15, pages
569?576. MIT Press.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kon-
dor, and Karsten M. Borgwardt. 2010. Graph kernels.
J. Mach. Learn. Res., 99:1201?1242, August.
Xinglong Wang, Jun?ichi Tsujii, and Sophia Ananiadou.
2010. Disambiguating the species of biomedical
named entities using natural language parsers. Bioin-
formatics, 26(5):661?667.
Yitao Zhang and Jon Patrick. 2005. Paraphrase identi-
fication by text canonicalization. In In Proceedings
of the Australasian Language Technology Workshop
2005.
1416
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 467?473,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Structured Distributional Semantic Model for Event Co-reference
Kartik Goyal? Sujay Kumar Jauhar? Huiying Li?
Mrinmaya Sachan? Shashank Srivastava? Eduard Hovy
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{kartikgo,sjauhar,huiyingl,mrinmays,shashans,hovy}@cs.cmu.edu
Abstract
In this paper we present a novel ap-
proach to modelling distributional seman-
tics that represents meaning as distribu-
tions over relations in syntactic neighbor-
hoods. We argue that our model approxi-
mates meaning in compositional configu-
rations more effectively than standard dis-
tributional vectors or bag-of-words mod-
els. We test our hypothesis on the problem
of judging event coreferentiality, which in-
volves compositional interactions in the
predicate-argument structure of sentences,
and demonstrate that our model outper-
forms both state-of-the-art window-based
word embeddings as well as simple ap-
proaches to compositional semantics pre-
viously employed in the literature.
1 Introduction
Distributional Semantic Models (DSM) are popu-
lar in computational semantics. DSMs are based
on the hypothesis that the meaning of a word or
phrase can be effectively captured by the distribu-
tion of words in its neighborhood. They have been
successfully used in a variety of NLP tasks includ-
ing information retrieval (Manning et al, 2008),
question answering (Tellex et al, 2003), word-
sense discrimination (Sch?tze, 1998) and disam-
biguation (McCarthy et al, 2004), semantic sim-
ilarity computation (Wong and Raghavan, 1984;
McCarthy and Carroll, 2003) and selectional pref-
erence modeling (Erk, 2007).
A shortcoming of DSMs is that they ignore the
syntax within the context, thereby reducing the
distribution to a bag of words. Composing the
?*Equally contributing authors
distributions for ?Lincoln?, ?Booth?, and ?killed?
gives the same result regardless of whether the in-
put is ?Booth killed Lincoln? or ?Lincoln killed
Booth?. But as suggested by Pantel and Lin (2000)
and others, modeling the distribution over prefer-
ential attachments for each syntactic relation sep-
arately yields greater expressive power. Thus, to
remedy the bag-of-words failing, we extend the
generic DSM model to several relation-specific
distributions over syntactic neighborhoods. In
other words, one can think of the Structured DSM
(SDSM) representation of a word/phrase as sev-
eral vectors defined over the same vocabulary,
each vector representing the word?s selectional
preferences for its various syntactic arguments.
We argue that this representation not only cap-
tures individual word semantics more effectively
than the standard DSM, but is also better able to
express the semantics of compositional units. We
prove this on the task of judging event coreference.
Experimental results indicate that our model
achieves greater predictive accuracy on the task
than models that employ weaker forms of compo-
sition, as well as a baseline that relies on state-
of-the-art window based word embeddings. This
suggests that our formalism holds the potential of
greater expressive power in problems that involve
underlying semantic compositionality.
2 Related Work
Next, we relate and contrast our work to prior re-
search in the fields of Distributional Vector Space
Models, Semantic Compositionality and Event
Co-reference Resolution.
2.1 DSMs and Compositionality
The underlying idea that ?a word is characterized
by the company it keeps? was expressed by Firth
467
(1957). Several works have defined approaches to
modelling context-word distributions anchored on
a target word, topic, or sentence position. Collec-
tively these approaches are called Distributional
Semantic Models (DSMs).
While DSMs have been very successful on a va-
riety of tasks, they are not an effective model of
semantics as they lack properties such as compo-
sitionality or the ability to handle operators such
as negation. In order to model a stronger form of
semantics, there has been a recent surge in stud-
ies that phrase the problem of DSM composition-
ality as one of vector composition. These tech-
niques derive the meaning of the combination of
two words a and b by a single vector c = f(a, b).
Mitchell and Lapata (2008) propose a framework
to define the composition c = f(a, b, r,K) where
r is the relation between a and b, and K is the
additional knowledge used to define composition.
While this framework is quite general, the actual
models considered in the literature tend to disre-
gard K and r and mostly perform component-wise
addition and multiplication, with slight variations,
of the two vectors. To the best of our knowledge
the formulation of composition we propose is the
first to account for both K and r within this com-
positional framework.
Dinu and Lapata (2010) and S?aghdha and Ko-
rhonen (2011) introduced a probabilistic model
to represent word meanings by a latent variable
model. Subsequently, other high-dimensional ex-
tensions by Rudolph and Giesbrecht (2010), Ba-
roni and Zamparelli (2010) and Grefenstette et
al. (2011), regression models by Guevara (2010),
and recursive neural network based solutions by
Socher et al (2012) and Collobert et al (2011)
have been proposed. However, these models do
not efficiently account for structure.
Pantel and Lin (2000) and Erk and Pad? (2008)
attempt to include syntactic context in distribu-
tional models. A quasi-compositional approach
was attempted in Thater et al (2010) by a com-
bination of first and second order context vectors.
But they do not explicitly construct phrase-level
meaning from words which limits their applicabil-
ity to real world problems. Furthermore, we also
include structure into our method of composition.
Prior work in structure aware methods to the best
of our knowledge are (Weisman et al, 2012) and
(Baroni and Lenci, 2010). However, these meth-
ods do not explicitly model composition.
2.2 Event Co-reference Resolution
While automated resolution of entity coreference
has been an actively researched area (Haghighi
and Klein, 2009; Stoyanov et al, 2009; Raghu-
nathan et al, 2010), there has been relatively lit-
tle work on event coreference resolution. Lee
et al (2012) perform joint cross-document entity
and event coreference resolution using the two-
way feedback between events and their arguments.
We, on the other hand, attempt a slightly different
problem of making co-referentiality judgements
on event-coreference candidate pairs.
3 Structured Distributional Semantics
In this paper, we propose an approach to incorpo-
rate structure into distributional semantics (more
details in Goyal et al (2013)). The word distribu-
tions drawn from the context defined by a set of
relations anchored on the target word (or phrase)
form a set of vectors, namely a matrix for the tar-
get word. One axis of the matrix runs over all
the relations and the other axis is over the distri-
butional word vocabulary. The cells store word
counts (or PMI scores, or other measures of word
association). Note that collapsing the rows of the
matrix provides the standard dependency based
distributional representation.
3.1 Building Representation: The PropStore
To build a lexicon of SDSM matrices for a given
vocabulary we first construct a proposition knowl-
edge base (the PropStore) created by parsing the
Simple English Wikipedia. Dependency arcs are
stored as 3-tuples of the form ?w1, r, w2?, denot-
ing an occurrence of words w1, word w2 related
by r. We also store sentence indices for triples
as this allows us to achieve an intuitive technique
to achieve compositionality. In addition to the
words? surface-forms, the PropStore also stores
their POS tags, lemmas, and Wordnet supersenses.
This helps to generalize our representation when
surface-form distributions are sparse.
The PropStore can be used to query for the ex-
pectations of words, supersenses, relations, etc.,
around a given word. In the example in Figure 1,
the query (SST(W1) = verb.consumption, ?, dobj)
i.e. ?what is consumed? might return expectations
[pasta:1, spaghetti:1, mice:1 . . . ]. Relations and
POS tags are obtained using a dependency parser
Tratz and Hovy (2011), supersense tags using sst-
light Ciaramita and Altun (2006), and lemmas us-
468
Figure 1: Sample sentences & triples
ing Wordnet Fellbaum (1998).
3.2 Mimicking Compositionality
For representing intermediate multi-word phrases,
we extend the above word-relation matrix symbol-
ism in a bottom-up fashion using the PropStore.
The combination hinges on the intuition that when
lexical units combine to form a larger syntactically
connected phrase, the representation of the phrase
is given by its own distributional neighborhood
within the embedded parse tree. The distributional
neighborhood of the net phrase can be computed
using the PropStore given syntactic relations an-
chored on its parts. For the example in Figure
1, we can compose SST(w1) = Noun.person and
Lemma(W1) = eat appearing together with a nsubj
relation to obtain expectations around ?people eat?
yielding [pasta:1, spaghetti:1 . . . ] for the object
relation, [room:2, restaurant:1 . . .] for the location
relation, etc. Larger phrasal queries can be built to
answer queries like ?What do people in China eat
with??, ?What do cows do??, etc. All of this helps
us to account for both relation r and knowledge K
obtained from the PropStore within the composi-
tional framework c = f(a, b, r,K).
The general outline to obtain a composition
of two words is given in Algorithm 1, which
returns the distributional expectation around the
composed unit. Note that the entire algorithm can
conveniently be written in the form of database
queries to our PropStore.
Algorithm 1 ComposePair(w1, r, w2)
M1 ? queryMatrix(w1) (1)
M2 ? queryMatrix(w2) (2)
SentIDs?M1(r) ?M2(r) (3)
return ((M1? SentIDs) ? (M2? SentIDs)) (4)
For the example ?noun.person nsubj eat?, steps
(1) and (2) involve querying the PropStore for the
individual tokens, noun.person and eat. Let the re-
sulting matrices be M1 and M2, respectively. In
step (3), SentIDs (sentences where the two words
appear with the specified relation) are obtained by
taking the intersection between the nsubj compo-
nent vectors of the two matrices M1 and M2. In
step (4), the entries of the original matrices M1
and M2 are intersected with this list of common
SentIDs. Finally, the resulting matrix for the com-
position of the two words is simply the union of
all the relationwise intersected sentence IDs. Intu-
itively, through this procedure, we have computed
the expectation around the words w1 and w2 when
they are connected by the relation ?r?.
Similar to the two-word composition process,
given a parse subtree T of a phrase, we obtain
its matrix representation of empirical counts over
word-relation contexts (described in Algorithm 2).
Let the E = {e1 . . . en} be the set of edges in T ,
ei = (wi1, ri, wi2)?i = 1 . . . n.
Algorithm 2 ComposePhrase(T )
SentIDs? All Sentences in corpus
for i = 1? n do
Mi1 ? queryMatrix(wi1)
Mi2 ? queryMatrix(wi2)
SentIDs? SentIDs ?(M1(ri) ?M2(ri))
end for
return ((M11? SentIDs) ? (M12? SentIDs)
? ? ? ? (Mn1? SentIDs) ? (Mn2? SentIDs))
The phrase representations becomes sparser as
phrase length increases. For this study, we restrict
phrasal query length to a maximum of three words.
3.3 Event Coreferentiality
Given the SDSM formulation and assuming no
sparsity constraints, it is possible to calculate
469
SDSM matrices for composed concepts. However,
are these correct? Intuitively, if they truly capture
semantics, the two SDSM matrix representations
for ?Booth assassinated Lincoln? and ?Booth shot
Lincoln with a gun" should be (almost) the same.
To test this hypothesis we turn to the task of pre-
dicting whether two event mentions are coreferent
or not, even if their surface forms differ. It may be
noted that this task is different from the task of full
event coreference and hence is not directly compa-
rable to previous experimental results in the liter-
ature. Two mentions generally refer to the same
event when their respective actions, agents, pa-
tients, locations, and times are (almost) the same.
Given the non-compositional nature of determin-
ing equality of locations and times, we represent
each event mention by a triple E = (e, a, p) for
the event, agent, and patient.
In our corpus, most event mentions are verbs.
However, when nominalized events are encoun-
tered, we replace them by their verbal forms. We
use SRL Collobert et al (2011) to determine the
agent and patient arguments of an event mention.
When SRL fails to determine either role, its empir-
ical substitutes are obtained by querying the Prop-
Store for the most likely word expectations for
the role. It may be noted that the SDSM repre-
sentation relies on syntactic dependancy relations.
Hence, to bridge the gap between these relations
and the composition of semantic role participants
of event mentions we empirically determine those
syntactic relations which most strongly co-occur
with the semantic relations connecting events,
agents and patients. The triple (e, a, p) is thus the
composition of the triples (a, relationsetagent, e)
and (p, relationsetpatient, e), and hence a com-
plex object. To determine equality of this complex
composed representation we generate three levels
of progressively simplified event constituents for
comparison:
Level 1: Full Composition:
Mfull = ComposePhrase(e, a, p).
Level 2: Partial Composition:
Mpart:EA = ComposePair(e, r, a)
Mpart:EP = ComposePair(e, r, p).
Level 3: No Composition:
ME = queryMatrix(e)
MA = queryMatrix(a)
MP = queryMatrix(p)
To judge coreference between
events E1 and E2, we compute pair-
wise similarities Sim(M1full,M2full),
Sim(M1part:EA,M2part:EA), etc., for each
level of the composed triple representation. Fur-
thermore, we vary the computation of similarity
by considering different levels of granularity
(lemma, SST), various choices of distance
metric (Euclidean, Cityblock, Cosine), and
score normalization techniques (Row-wise, Full,
Column-collapsed). This results in 159 similarity-
based features for every pair of events, which are
used to train a classifier to decide conference.
4 Experiments
We evaluate our method on two datasets and com-
pare it against four baselines, two of which use
window based distributional vectors and two that
employ weaker forms of composition.
4.1 Datasets
IC Event Coreference Corpus: The dataset
(Hovy et al, 2013), drawn from 100 news articles
about violent events, contains manually created
annotations for 2214 pairs of co-referent and non-
coreferent events each. Where available, events?
semantic role-fillers for agent and patient are an-
notated as well. When missing, empirical substi-
tutes were obtained by querying the PropStore for
the preferred word attachments.
EventCorefBank (ECB) corpus: This corpus
(Bejan and Harabagiu, 2010) of 482 documents
from Google News is clustered into 45 topics,
with event coreference chains annotated over each
topic. The event mentions are enriched with se-
mantic roles to obtain the canonical event struc-
ture described above. Positive instances are ob-
tained by taking pairwise event mentions within
each chain, and negative instances are generated
from pairwise event mentions across chains, but
within the same topic. This results in 11039 posi-
tive instances and 33459 negative instances.
4.2 Baselines
To establish the efficacy of our model, we compare
SDSM against a purely window-based baseline
(DSM) trained on the same corpus. In our exper-
iments we set a window size of seven words. We
also compare SDSM against the window-based
embeddings trained using a recursive neural net-
work (SENNA) (Collobert et al, 2011) on both
datsets. SENNA embeddings are state-of-the-art
for many NLP tasks. The second baseline uses
470
IC Corpus ECB Corpus
Prec Rec F-1 Acc Prec Rec F-1 Acc
SDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843
Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791
DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830
MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831
AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834
Table 1: Cross-validation Performance on IC and ECB dataset
SENNA to generate level 3 similarity features for
events? individual words (agent, patient and ac-
tion). As our final set of baselines, we extend two
simple techniques proposed by (Mitchell and Lap-
ata, 2008) that use element-wise addition and mul-
tiplication operators to perform composition. We
extend it to our matrix representation and build
two baselines AVC (element-wise addition) and
MVC (element-wise multiplication).
4.3 Discussion
Among common classifiers, decision-trees (J48)
yielded best results in our experiments. Table 1
summarizes our results on both datasets.
The results reveal that the SDSM model con-
sistently outperforms DSM, SENNA embeddings,
and the MVC and AVC models, both in terms
of F-1 score and accuracy. The IC corpus com-
prises of domain specific texts, resulting in high
lexical overlap between event mentions. Hence,
the scores on the IC corpus are consistently higher
than those on the ECB corpus.
The improvements over DSM and SENNA em-
beddings, support our hypothesis that syntax lends
greater expressive power to distributional seman-
tics in compositional configurations. Furthermore,
the increase in predictive accuracy over MVC and
AVC shows that our formulation of composition
of two words based on the relation binding them
yields a stronger form of compositionality than
simple additive and multiplicative models.
Next, we perform an ablation study to deter-
mine the most predictive features for the task of
event coreferentiality. The forward selection pro-
cedure reveals that the most informative attributes
are the level 2 compositional features involving
the agent and the action, as well as their individ-
ual level 3 features. This corresponds to the in-
tuition that the agent and the action are the prin-
cipal determiners for identifying events. Features
involving the patient and level 1 features are least
useful. This is probably because features involv-
ing full composition are sparse, and not as likely
to provide statistically significant evidence. This
may change as our PropStore grows in size.
5 Conclusion and Future Work
We outlined an approach that introduces structure
into distributed semantic representations gives us
an ability to compare the identity of two repre-
sentations derived from supposedly semantically
identical phrases with different surface realiza-
tions. We employed the task of event coreference
to validate our representation and achieved sig-
nificantly higher predictive accuracy than several
baselines.
In the future, we would like to extend our model
to other semantic tasks such as paraphrase detec-
tion, lexical substitution and recognizing textual
entailment. We would also like to replace our syn-
tactic relations to semantic relations and explore
various ways of dimensionality reduction to solve
this problem.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper. This
work was supported in part by the following
grants: NSF grant IIS-1143703, NSF award IIS-
1147810, DARPA grant FA87501220342.
References
Marco Baroni and Alessandro Lenci. 2010. Distri-
butional memory: A general framework for corpus-
based semantics. Comput. Linguist., 36(4):673?721,
December.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
471
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 594?602, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 999888:2493?2537,
November.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10, pages
1162?1172, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 897?906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
Kartik. Goyal, Sujay Kumar Jauhar, Mrinmaya Sachan,
Shashank Srivastava, Huiying Li, and Eduard Hovy.
2013. A structured distributional semantic model
: Integrating structure with semantics. In Proceed-
ings of the 1st Continuous Vector Space Models and
their Compositionality Workshop at the conference
of ACL 2013.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Proceedings of the
Ninth International Conference on Computational
Semantics, IWCS ?11, pages 125?134, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, GEMS ?10, pages 33?37, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ?09, pages 1152?
1161, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
E.H. Hovy, T. Mitamura, M.F. Verdejo, J. Araki, and
A. Philpot. 2013. Events are not simple: Iden-
tity, non-identity, and quasi-identity. In Proceedings
of the 1st Events Workshop at the conference of the
HLT-NAACL 2013.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Comput.
Linguist., 29(4):639?654, December.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, ACL ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244.
Patrick Pantel and Dekang Lin. 2000. Word-for-word
glossing with contextually similar words. In Pro-
ceedings of the 1st North American chapter of the
Association for Computational Linguistics confer-
ence, NAACL 2000, pages 78?85, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
472
pages 492?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 907?916, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hinrich Sch?tze. 1998. Automatic word sense dis-
crimination. Comput. Linguist., 24(1):97?123.
Diarmuid ? S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1047?1057, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 1201?1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ?09, pages 656?664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In SIGIR, pages 41?47.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?10, pages
948?957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1257?1268, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido
Dagan. 2012. Learning verb inference rules from
linguistically-motivated evidence. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 194?204, Stroudsburg, PA, USA. Association
for Computational Linguistics.
S. K. M. Wong and Vijay V. Raghavan. 1984. Vector
space model of information retrieval: a reevaluation.
In Proceedings of the 7th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?84, pages 167?185,
Swinton, UK. British Computer Society.
473
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 634?643,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Vector space semantics with frequency-driven motifs
Shashank Srivastava
Carnegie Mellon University
Pittsburgh, PA 15217
ssrivastava@cmu.edu
Eduard Hovy
Carnegie Mellon University
Pittsburgh, PA 15217
hovy@cmu.edu
Abstract
Traditional models of distributional se-
mantics suffer from computational issues
such as data sparsity for individual lex-
emes and complexities of modeling se-
mantic composition when dealing with
structures larger than single lexical items.
In this work, we present a frequency-
driven paradigm for robust distributional
semantics in terms of semantically cohe-
sive lineal constituents, or motifs. The
framework subsumes issues such as dif-
ferential compositional as well as non-
compositional behavior of phrasal con-
situents, and circumvents some problems
of data sparsity by design. We design
a segmentation model to optimally par-
tition a sentence into lineal constituents,
which can be used to define distributional
contexts that are less noisy, semantically
more interpretable, and linguistically dis-
ambiguated. Hellinger PCA embeddings
learnt using the framework show competi-
tive results on empirical tasks.
1 Introduction
Meaning in language is a confluence of experien-
tially acquired semantics of words or multi-word
phrases, and their semantic composition to create
new meanings. For instance, successfully inter-
preting a sentence such as
The old senator kicked the bucket.
requires the knowledge that the semantic conno-
tations of ?kicking the bucket? as a unit are the
same as those for ?dying?. Short of explicit su-
pervision, such semantic mappings must be in-
ferred by a new language speaker through induc-
tive mechanisms operating on observed linguis-
tic usage. This perspective of acquired meaning
aligns with the ?meaning is usage? adage, conso-
nant with Wittgenstein?s view of semantics. At
the same time, the ability to adaptively commu-
nicate elaborate meanings can only be conciled
through Frege?s principle of compositionality, i.e.,
meanings of larger linguistic constructs can be
derived from the meanings of individual compo-
nents, modulated by their syntactic interrelations.
Indeed, most linguistic usage appears composi-
tional. This is supported by the fact even with
very limited vocabulary, children and non-native
speakers can often communicate surprisingly ef-
fectively.
It can be argued that to be sustainable, induc-
tive aspects of meaning must be recurrent enough
to be learnable by new users. That is, a non-
compositional phrase such as ?kick the bucket? is
likely to persist in common parlance only if it is
frequently used with its associated semantic map-
ping. If a usage-driven meaning of a motif is not
recurrent enough, learning this mapping is inef-
ficient in two ways. First, the sparseness of ob-
servations would severely limit accurate inductive
acquisition by new observers. Second, the value
of learning a very infrequent semantic mapping
is likely marginal. This motivates the need for
a frequency-driven view of lexical semantics. In
particular, such a perspective can be especially
advantageous for distributional semantics for rea-
sons we outline below.
Distributional semantic models (DSMs) that
represent words as distributions over neighbouring
contexts have been particularly effective in captur-
ing fine-grained lexical semantics (Turney et al,
2010). Such models have engendered improve-
ments in diverse applications such as selectional
preference modeling (Erk, 2007), word-sense dis-
crimination (McCarthy and Carroll, 2003), auto-
matic dictionary building (Curran, 2003), and in-
formation retrieval (Manning et al, 2008). How-
ever, while conventional DSMs consider colloca-
634
With the bad press in wake of the financial crisis, businesses are leaving our shores .
crisis: <bad, businesses, financial, leaving, press, shores, wake>
financial crisis: <bad press, businesses, in wake of, leaving our shores>
Table 1: Meaning representation by conventional DSMs vs notional ideal
tion strengths (through counts and PMI scores) of
word neighbourhoods, they disregard much of the
regularity in human language. Most significantly,
word tokens that act as latent dimensions are of-
ten derived from arbitrary tokenization. The ex-
ample given in Table 1 succinctly describes this.
The first row in the table shows a representation
of the meaning of the token ?crisis? that a conven-
tional DSM might extract from the given sentence
after stopword removal. While helpful, the repre-
sentation seems unsatisfying since words such as
?press?, ?wake? and ?shores? seem to have little to
do with a crisis. From a semantic perspective, a
representation similar to the second is more valu-
able: not only does it represent a semantic map-
ping for a more specific meaning, but the latent di-
mensions of the representation have are less noisy
(e.g., while ?wake? is semantically ambiguous, its
surrounding context in ?in wake of? disambiguates
it) and more intuitive in regards of semantic in-
terepretability. This is the overarching theme of
this work: we present a frequency driven paradigm
for extending distributional semantics to phrasal
and sentential levels in terms of such semantically
cohesive, recurrent lexical units or motifs.
We propose to identify such semantically
cohesive motifs in terms of features inspired
from frequency-characteristics, linguistic idiosyn-
crasies, and shallow syntactic analysis; and ex-
plore both supervised and semi-supervised mod-
els to optimally segment a sentence into such mo-
tifs. Through exploiting regularities in language
usage, the framework can efficiently account for
both compositional and non-compositional word
usage, while avoiding the issue of data-sparsity by
design. Our principal contributions in this paper
are:
? We present a framework for extending dis-
tributional semantics to learn semantic repre-
sentations of both words and phrases in terms
of recurrent motifs, rather than arbitrary word
tokens
? We present a simple model to segment a sen-
tence into such motifs using a feature-set
drawing from frequency statistics, informa-
tion theory, linguistic theories and shallow
syntactic analysis
? Word and phrasal representations learnt
through the approach outperform conven-
tional DSM representations on empirical
tasks
This paper is organized as follows: In Sec-
tion 2, we briefly review related work in the do-
main of compositional distributional semantics,
and motivate our formulation. Section 3 describes
our methodology, which consists of a frequency-
driven segmentation model to partition text into
semantically meaningful recurring lineal-subunits,
a representation learning framework for learning
new semantic embeddings based on this segmen-
tation, and an approach to use such embeddings in
downstream applications. We present experiments
and empirical evaluations for our method in Sec-
tion 4. Finally, we conclude in Section 5 with a
summary of our principal findings, and a discus-
sion of possible directions for future work.
2 Related Work
While DSMs have been valuable in representing
semantics of single words, approaches to extend
them to represent the semantics of phrases and
sentences has met with only marginal success.
While there is considerable variety in approaches
and formulations, existing approaches for phrasal
level and sentential semantics can broadly be par-
titioned into two categories.
2.1 Compositional approaches
These have aimed at using semantic representa-
tions for individual words to learn semantic rep-
resentations for larger linguistic structures. These
methods implicitly make an assumption of com-
positionality, and often include explicit computa-
tional models of compositionality. Notable among
such models are the additive and multiplicative
models of composition by Mitchell and Lapata
(2008), Grefenstette et al (2010), Baroni and
635
Zamparelli?s (2010) model that differentially mod-
els content and function words for semantic com-
position, and Goyal et al?s SDSM model (2013)
that incorporates syntactic roles to model seman-
tic composition. Notable among the most effec-
tive distributional representations are the recent
deep-learning approaches by Socher et al (2012),
that model vector composition through non-linear
transformations. While word embeddings and lan-
guage models from such methods have been use-
ful for tasks such as relation classification, polarity
detection, event coreference and parsing; much of
existing literature on composition is based on ab-
stract linguistic theory and conjecture, and there
is little evidence to support that learnt represen-
tations for larger linguistic units correspond to
their semantic meanings. While works such as
the SDSM model suffer from the problem of spar-
sity in composing structures beyond bigrams and
trigrams, methods such as Mitchell and Lapata
(2008)and (Socher et al, 2012) and Grefenstette
and Sadrzadeh (2011) are restricted by signifi-
cant model biases in representing semantic com-
position by generic algebraic operations. Finally,
the assumption that semantic meanings for sen-
tences could have representations similar to those
for smaller individual tokens is in some sense un-
intuitive, and not supported by linguistic or seman-
tic theories.
2.2 Tree kernels
Tree Kernel methods have gained popularity in
the last decade for capturing syntactic information
in the structure of parse trees (Collins and Duffy,
2002; Moschitti, 2006). Instead of procuring ex-
plicit representations, the kernel paradigm directly
focuses on the larger goal of quantifying semantic
similarity of larger linguistic units. Structural ker-
nels for NLP are based on matching substructures
within two parse trees , consisting of word-nodes
with similar labels. These methods have been use-
ful for eclectic tasks such as parsing, NER, se-
mantic role labeling, and sentiment analysis. Re-
cent approaches such as by Croce et al (2011)
and Srivastava et al (2013) have attempted to pro-
vide formulations to incorporate semantics into
tree kernels through the use of distributional word
vectors at the individual word-nodes. While this
framework is attractive in the lack of assumptions
on representation that it makes, the use of distri-
butional embeddings for individual tokens means
that it suffers from the same shortcomings as de-
scribed for the example in Table 1, and hence these
methods model semantic relations between word-
nodes very weakly. Figure 1 shows an example of
the shortcomings of this general approach.
Figure 1: Tokenwise syntactic and semantic simi-
larities don?t imply sentential semantic similarity
While the two sentences in consideration have
near-identical syntax and could be argued to have
semantically aligned words in similar positions,
the semantics of the complete sentences are widely
divergent. Specifically, the ?bag of words? as-
sumption in tree kernels doesn?t suffice for these
lexemes, and a stronger semantic model is needed
to capture phrasal semantics as well as diverging
inter-word relations such as in ?coffee table? and
?water table?. Our hypothesis is that a model that
can even weakly identify recurrent motifs such as
?water table? or ?breaking a fall? would be help-
ful in building more effective semantic represen-
tations. A significant advantage of a frequency
driven view is that it makes the concern of com-
positionality of recurrent phrases immaterial. If a
motif occurs frequently enough in common par-
lance, its semantics could be captured with distri-
butional models irrespective of whether its associ-
ated semantics are compositional or acquired.
2.3 Identifying multi-word expressions
Several approaches have focused on supervised
identification of multi-word expressions (MWEs)
through statistical (Pecina, 2008; Villavicencio et
al., 2007) and linguistically motivated (Piao et al,
2005) techniques. More recently, hybrid methods
based on both statistical as well as linguistic fea-
tures have been popular (Tsvetkov and Wintner,
2011). Ramisch et al (2008) demonstrate that
adding part-of-speech tags to frequency counts
substantially improves performance. Other meth-
ods have attempted to exploit morphological, syn-
tactic and semantic characteristics of MWEs. In
636
particular, approaches such as Bannard (2007) use
syntactic rigidity to characterize MWEs. While
existing work has focused on the classification
task of categorizing a phrasal constituent as a
MWE or a non-MWE, the general ideas of most
of these works are in line with our current frame-
work, and the feature-set for our motif segmen-
tation model is designed to subsume most of
these ideas. It is worthwhile to point out that
the task of motif segmentation is slightly differ-
ent from MWE identification. Specifically, the
onus on recurrent occurrences means that non-
decomposibility is not an essential consideration
for a word to be considered a motif. In line with
the proposed paradigm, typical MWEs such as
?shoot the breeze?, ?sour note? and ?hot dog? would
be considered valid lineal motifs.
1
In addition,
even decomposable recurrent lineal phrases such
as ?love story?, ?federal government?, and ?mil-
lions of people? are marked as meaningful recur-
rent motifs. Finally, and least interestingly, we
include common named entities such as ?United
States? and ?Java Virtual Machine? within the am-
bit of motifs.
3 Method
In this section, we define our frequency-driven
framework for distributional semantics in detail.
As just described above, our definition for motifs
is less specific than MWEs. With such a working
definition, contiguous motifs are likely to make
distributional representations less noisy and also
assist in disambiguating context. Also, the lack of
specificity ensures that such motifs are common
enough to meaningfully influence distributional
representation beyond single tokens. A method
towards frequency-driven distributional semantics
could involve the following principal components:
3.1 Linear segmentation model
The segmentation model forms the core of the
framework. Ideally, it fragments a given sen-
tence into non-overlapping, semantically mean-
ingful, empirically frequent contiguous sub-units
or motifs. The model accounts for possible seg-
mentations of a sentence into potential motifs, and
prefers recurrent and cohesive motifs through fea-
tures that capture frequency-based and statistical
1
We note that since we take motifs as lineal units,
the current method doesn?t subsume several common non-
contiguous MWEs such as ?let off? in ?let him off?.
features, as well as linguistic idiosyncracies. This
is accomplished using a very simple linear chain
model and a rich feature set consisting of a combi-
nation of frequency-driven, information theoretic
and linguistically motivated features.
Let an observed sentence be denoted by x, with
the individual tokens x
i
denoting the i?th token in
the sentence. The segmentation model is a chain
LVM (latent variable model) that aims to maxi-
mize a linear objective defined by:
J =
?
i
w
i
f
i
(y
k
, y
k?1
,x)
where f
i
are arbitrary Markov features that can
depend on segments (potential motifs) of the ob-
served sentence x, and contiguous latent states.
The features are chosen so as to best represent
frequency-based, statistical as well as linguistic
considerations for treating a segment as an ag-
glutinative unit, or a motif. In specific, these
features could encode characteristics such as fre-
quency statistics, collocation strengths and syn-
tactic distinctness, or inflectional rigidity of the
considered segments; described in detail in Sec-
tion 3.2. The model is an instantiation of a sim-
ple featurized HMM, and the weighted sum of fea-
tures corresponding to a segment is cognate with
an affinity score for the ?stickiness? of the segment,
i.e., the affinity for the segment to be treated as
holistic unit or a single motif.
We also associate a penalizing cost for each non
unary-motif to avoid aggressive agglutination of
tokens. In particular, for an ngram occurrence to
be considered a motif, the marginal contribution
due to the affinity of the prospective motif should
at minimum exceed this penalty. The weights for
the affinity functions as well as these penalties are
learnt from data using full as well as partial anno-
tations. The latent state-variables y
k
denotes the
membership of the token x
k
to a unary or a larger
motif; and the state-sequence collectively gives
the segmentation of the sentence. An individual
state-variable y
k
encodes a pairing of the size of
the encompassing ngram motif, and the position
of the word x
k
within it. For instance, y
k
= T
3
denotes that the token x
k
is the final position in a
trigram motif.
3.1.1 Inference of optimal segmentation
If the optimal weights w
i
are known, inference
for the best motif segmentation can be performed
637
in linear time (in the number of tokens) follow-
ing the generalized Viterbi algorithm. A slightly
modified version of Viterbi could also be used to
find segmentations that are constrained to agree
with some given motif boundaries, but can seg-
ment other parts of the sentence optimally under
these constraints. This is necessary for the sce-
nario of semi-supervised learning of weights with
partially annotated sentences, as described later.
3.2 Learning motif affinities and penalties
We briefly discuss data-driven learning of weights
for features that define the motif affinity scores
and penalties. We describe learning of the model
parameters with fully annotated training data, as
well as an approach for learning motif segmenta-
tion that requires only partial supervision.
Supervised learning: In the supervised case, op-
timal state sequences y
(k)
are fully observed for
the training set. For this purpose, we created a
dataset of 1000 sentences from the Simple En-
glish Wikipedia and the Gigaword Corpus, and
manually annotated it with motif boundaries us-
ing BRAT (Stenetorp et al, 2012). In this case,
learning can follow the online structured percep-
tron learning procedure by Collins (2002), where
weights updates for the k?th training example
(x
(k)
,y
(k)
) are given as:
w
i
? w
i
+ ?(f
i
(x
(k)
,y
(k)
)? f
i
(x
(k)
,y
?
))
Here y
?
= Decode(x
(k)
,w) is the optimal
Viterbi decoding using the current estimates of
the weights. Updates are run for a large number
of iterations until the change in objective drops
below a threshold, and the learning rate ? is
adaptively modified as described in Collins et al
Implicitly, the weight learning algorithm can be
seen as a gradient descent procedure minimizing
the difference between the scores of highest
scoring (Viterbi) state sequences, and the label
state sequences.
Semi-supervised learning: In the semi-
supervised case, the labels y
(k)
i
are known
only for some of the tokens in x
(k)
. This is a
commonplace scenario, where a part of a sentence
has clear motif-boundaries, whereas the rest of the
sentence is not annotated. For accumulating such
data, we looked for occurrences of 2500 expres-
sions from the WikiMWE dataset in sentences
from the combined Simple English Wikipedia
and Gigaword corpora. The query expressions in
the retrieved sentences were marked with motif
boundaries, while the remaining tokens in the
sentences were left unannotated.
While the Viterbi algorithm can be used for tag-
ging optimal state-sequences given the weights,
the structured perceptron can learn optimal model
weights given gold-standard sequence labels.
Hence, in this case, we use a variation of the hard
EM algorithm for learning. The algorithm pro-
ceeds as follows: in the E-step, we use the current
values of weights to compute hard-expectations,
i.e., the best scoring Viterbi sequences among
those consistent with the observed state labels. In
the M-step, we take the decoded state-sequences
in the E-step as observed, and run perceptron
learning to update feature weightsw
i
. Pseudocode
of the learning algorithm for the partially labeled
case is given in Algorithm 1.
Algorithm 1
1: Input: Partially labeled data D = {(x, y)
i
}
2: Output: Weights w
3: Initialization: Set w
i
randomly, ?i
4: for i : 1 to maxIter do
5: Decode D with current w to find optimal
Viterbi paths that agree with (partial) ground
truths.
6: Run Structured Perceptron algorithm with de-
coded tag-sequences to update weights w
7: end for
8: return w
The semi-supervised approach enables incor-
poration of significantly more training data. In
particular, this method could be used in conjunc-
tion with a supervised approach. This would in-
volve initializing the weights prior to the semi-
supervised procedure with the weights from the
supervised learning model, so as to seed the semi-
supervised approach with reasonable model, and
use the partially annotated data to fine-tune the su-
pervised model. The sequential approach, akin to
annealing weights, can efficiently utilize both full
and partial annotations.
3.2.1 Feature engineering
In this section, we describe the principal features
used in the segmentation model
Transitional features and penalties:
? Transitional features f
trans
(y
i?1
, y
i
) =
638
Iy
i?1
,y
i
2
describing the transitional affinities
of state pairs. Since our state definitions pre-
clude certain transitions (such as from state
T
2
to T
1
), these weights are initialized to??
to expedite training.
? N-gram penalties: f
ngram
We define a
penalty for tagging each non-unary motif as
described before. For a motif to be tagged,
the improvement in objective score should at
least exceed the corresponding penalty. e.g.,
f
qgram
(y
i
) = I
y
i
=Q
4
denotes the penalty for
tagging a tetragram.
3
Frequency-based, information theoretic, and POS
features:
? Absolute and log-normalized motif frequen-
cies f
ngram
(x
i?n+1
, ...x
i?1
, x
i
, y
i
). This
feature is associated with a particular token-
sequence and ngram-tag, and takes the
value of the motif-frequency if the motif
token-sequence matches the feature token-
sequence, and is marked as with a match-
ing tag. e.g., f
bgram
(x
i?1
= love, x
i
=
story, y
i
= B
2
).
? Absolute and log-normalized motif frequen-
cies for a particular POS-sequence. This
feature is associated with a particular POS-
tag sequence and ngram-tag, and takes the
value of the motif-frequency if the motif
token-sequence gets a matching tag, and is
marked as with a matching ngram tag. e.g.,
f
bgram
(p
i?1
= V B, p
i
= NN, y
i
= B
2
).
? Medians and maxima of pairwise collocation
statistics for tokens for a particular size of
ngram motifs: we use the following statis-
tics: pointwise mutual information, Chi-
square statistic, and conditional probability.
We also used POS sensitive versions of these,
which performed much better than plain ver-
sions in our evaluations.
? Histogram counts of inflectional forms of to-
ken sequence for the corresponding ngram
motif and POS sequence: this features takes
the value of the count of inflectional forms
of an ngram that account for 90% of occur-
rences of all inflectional forms.
2
Here, I denotes the indicator function
3
It is straightforward to preclude partial n-gram annota-
tions near sentence boundaries with prohibitive penalties.
? Entropies of histogram distributions of inflec-
tional variants (described above).
? Features encoding syntactic rigidity: ratios
and log-ratios of frequencies of an ngram mo-
tif and variations by replacing a token using
near synonyms from its synset.
Additionally, a few feature for the segmenta-
tions model contained minor orthographic features
based on word shape (length and capitalization
patterns). Also, all numbers, URLs, and cur-
rency symbols were normalized to the special NU-
MERIC, URL, and CURRENCY tokens respec-
tively. Finally, a gazetteer feature checked for oc-
currences of motifs in a gazetteer of named enti-
ties.
3.3 Representation learning
With the segmentation model described in the pre-
vious section, we process text from the English Gi-
gaword corpus and the Simple English Wikipedia
to partition sentences into motifs. Since the seg-
mentation model accounts for the contexts of the
entire sentence in determining motifs, different in-
stances of the same token could evoke different
meaning representations. Consider the following
sentences tagged by the segmentation model, that
would correspond to different representations of
the token ?remains?: once as a standalone motif,
and once as part of an encompassing bigram motif
(?remains classified?).
Hog prices have declined sharply , while the
cost of corn remains relatively high.
Even with the release of such documents, ques-
tions are not answered, since only the agency
knows what remains classified
Given constituent motifs of each sentence in the
data, we can now define neighbourhood distribu-
tions for unary or phrasal motifs in terms of other
motifs (as envisioned in Table 1). In our experi-
ments, we use a window-length of 5 adjoining mo-
tifs on either side to define the neighbourhood of
a constituent. Naturally, in the presence of multi-
word motifs, the neighbourhood boundary could
be more extended than in a conventional DSM.
With such neighbourhood contexts, the distri-
butional paradigm posits that semantic similarity
between a pair of motifs can be given by a sense
of ?distance? between the two distributions. Most
popularly, traditional measures of vector distance
639
such as the cosine similarity, Euclidean distance
and City-block distance have been used in sev-
eral distributional approaches. Additionally, sev-
eral distance measures between discrete distribu-
tions exist in statistical literature, most famously
the Kullback Leibler divergence, Bhattacharyya
distance and the Hellinger distance. Recent work
(Lebret and Lebret, 2013) has shown that the
Hellinger distance is an especially effective mea-
sure in learning distributional embeddings, with
Hellinger PCA being much more computationally
inexpensive than neural language modeling ap-
proaches, while performing much better than stan-
dard PCA, and competitive with the state-of-the-
art in downstream evaluations. Hence, we use the
Hellinger measure between neighbourhood motif
distributions in learning representations.
The Hellinger distance between two categorical
distributions P = (p
1
...p
k
) and Q = (q
1
...q
k
) is
defined as:
H(P,Q) =
1
?
2
?
?
?
?
k
?
i=1
(
?
p
i
?
?
q
i
)
2
=
1
?
2
?
?
?
?
P ?
?
Q
?
?
?
2
The Hellinger measure has intuitively desir-
able properties: specifically, it can be seen
as the Euclidean distance between the square-
roots transformed distributions, where both vec-
tors
?
P and
?
Q are length-normalized under the
same(Euclidean) norm. Finally, we perform SVD
on the motif similarity matrix (with size of the or-
der of the total vocabulary in the corpus), and re-
tain the first k principal eigenvectors to obtain low-
dimensional vector representations that are more
convenient to work with. In our preliminary ex-
periments, we found that k = 300 gave quanti-
tatively good results, with marginal change with
added dimensionality. We use this setting for all
our experiments.
4 Experiments
In this section, we describe some experimental
evaluations and findings for our approach. We first
quantitatively and qualitatively analyze the perfor-
mance of the segmentation model, and then evalu-
ate the distributional motif representations learnt
by the model through two downstream applica-
tions.
4.1 Motif segmentation
In an evaluation of the motif segmentations model
within the perspective of our framework, we be-
lieve that exact correspondence to human judg-
ment is unrealistic, since guiding principles for
defining motifs, such as semantic cohesion, are
hard to define and only serve as working princi-
ples. However, for purposes of relative compar-
ison, we quantitatively evaluate the performance
of the motif segmentation models on the fully an-
notated dataset. For this experiment, the gold-
annotated corpus was split into a training and test
sets in a 9:1 proportion. A small fraction of the
training split was set apart for development and
validation. For this evaluation, we considered a
motif boundary as correct only for an exact match,
i.e., when both its boundaries (left and right) were
correctly predicted. Also, since a majority of mo-
tifs are unary tokens, including them into consider-
ation artificially boosts the accuracy, whereas we
are more interested in the prediction of larger n-
gram tokens. Hence we report results on the per-
formance on only non-unary motifs.
P R F
Rule-based baseline 0.85 0.10 0.18
Supervised 0.62 0.28 0.39
Semi-supervised 0.30 0.17 0.22
Supervised + annealing 0.69 0.38 0.49
Table 2: Results for motif segmentations
Table 2 shows the performance of the segmen-
tation model with the three proposed learning ap-
proaches described earlier. For a baseline, we
consider a rule-based model that simply learns all
ngram segmentations seen in the training data, and
marks any occurrence of a matching token se-
quence as a motif; without taking neighbouring
context into account. We observe that this model
has a very high precision (since many token se-
quences marked as motifs would recur in simi-
lar contexts, and would thus have the same mo-
tif boundaries). However, the rule-based method
has a very row recall due to lack of generaliza-
tion capabilities. We see that while all three learn-
ing algorithms perform better than the baseline,
the performance of the purely unsupervised sys-
tem is inferior to supervised approaches. This is
not unexpected: the supervision provided to the
model is very weak due to a lack of negative ex-
amples (which leads to spurious motif taggings,
640
While men often (openly or privately) sympathized with Prince Charles when the princess went public
about her rotten marriage , women cheered her on.
The healthcare initiative has become a White elephant for the federal government.
Chirac and Juppe have made a bad situation worse by seeking to meet Maastricht criteria not by
cutting spending, but by raising taxes still further.
Now , say Vatican observers , Pope John Paul II wants to show the world that many church members
did resist the Third Reich and paid the price.
Table 3: Examples of output from sentence segmentation model
leading to a low precision), as well as no examples
of transitions between adjacent motifs (to learn
transitional weights and penalties). The super-
vised model expectedly outperforms both the rule-
based and the semi-supervised systems. However,
the supervised learning model with subsequent an-
nealing outperforms the supervised model in terms
of both precision and recall; showing the utility of
the semi-supervised method when seeded with a
good initial model, and the additive value of par-
tially labeled data.
Qualitative analysis of motif-segmented sen-
tences shows that our designed feature-set is effec-
tive and helpful in identifying semantically cohe-
sive ngrams. Table 3 provides four examples. The
first example correctly identifies ?went public?,
while missing out on the potential motif ?cheered
her on?. In general, these examples illustrate that
the model can identify idiomatic and idiosyncratic
themes as well as commonly recurrent ngrams (in
the second example, the model picks out ?has be-
come? which is highly recurrent, but doesn?t have
the semantic cohesiveness of some of the other
motifs). In particular, consider the second exam-
ple, where the model picks ?white elephant? as a
motif. In such cases, the disambiguating influence
of context incorporated by the motif is apparent.
Elephant White elephant
tusks expensive
trunk spend
african biggest
white the project
indian very high
baby multibillion dollar
The above table shows some of the top results
for the unary token ?elephant? by frequency, and
frequent unary and non-unary motifs for the mo-
tif ?white elephant? retrieved by the segmentation
model.
4.2 Distributional representations
For evaluating distributional representations for
motifs (in terms of other motifs) learnt by the
framework, we test these representations in two
downstream tasks: sentence polarity classification
and metaphor detection. For sentence polarity, we
consider the Cornell Sentence Polarity corpus by
Pang and Lee (2005), where the task is to classify
the polarity of a sentence as positive or negative.
The data consists of 10662 sentences from movie
reviews that have been annotated as either posi-
tive or negative. For composing the motifs repre-
sentations to get judgments on semantic similarity
of sentences, we use our recent Vector Tree Ker-
nel approach The VTK approach defines a convo-
lutional kernel over graphs defined by the depen-
dency parses of sentences, using a vector repre-
sentation at each graph node that representing a
single lexical token. For our purposes, we mod-
ify the approach to merge the nodes of all tokens
that constitute a motif occurrence, and use the mo-
tif representation as the vector associated with the
node. Table 4 shows results for the sentence polar-
ity task.
P R F1
DSM 0.56 0.50 0.53
AVM 0.55 0.53 0.54
MVM 0.55 0.49 0.52
VTK 0.65 0.58 0.62
VTK + MotifDSM 0.66 0.60 0.63
Table 4: Results for Sentence Polarity detection
For this task, the motif based distributional em-
beddings vastly outperform a conventional distri-
butional model (DSM) based on token distribu-
tions, as well as additive (AVM) and multiplica-
tive (MVM) models of vector compositionality, as
641
proposed by Lapata et al The model is compet-
itive with the state-of-the-art VTK (Srivastava et
al., 2013) that uses the SENNA neural embeddings
by Collobert et al (2011).
P R F1
CRF 0.74 0.50 0.59
SVM+DSM 0.63 0.80 0.71
VTK+ SENNA 0.67 0.87 0.76
VTK+ MotifDSM 0.70 0.87 0.78
Table 5: Results for Metaphor identification
On the metaphor detection task, we use the
Metaphor dataset (Hovy et al, 2013). The data
consists of sentences with defined phrases, and
the task consists of identifying the linguistic use
in these phrases as metaphorical or literal. For
this task, the motif based model is expected to
perform well as common metaphorical usage is
generally through idiosyncratic MWEs, which the
motif based models is specially geared to capture
through the features of the segmentation model.
For this task, we again use the VTK formalism
for combining vector representations of the indi-
vidual motifs. Table 5 shows that the motif-based
DSM does better than discriminative models such
as CRFs and SVMs, and also slightly improves on
the VTK kernel with distributional embeddings.
5 Conclusion
We have presented a new frequency-driven frame-
work for distributional semantics of not only lex-
ical items but also longer cohesive motifs. The
theme of this work is a general paradigm of seek-
ing motifs that are recurrent in common parlance,
are semantically coherent, and are possibly non-
compositional. Such a framework for distribu-
tional models avoids the issue of data sparsity
in learning of representations for larger linguis-
tic structures. The approach depends on drawing
features from frequency statistics, statistical cor-
relations, and linguistic theories; and this work
provides a computational framework to jointly
model recurrence and semantic cohesiveness of
motifs through compositional penalties and affin-
ity scores in a data driven way.
While being deliberately vague in our work-
ing definition of motifs, we have presented simple
efficient formulations to extract such motifs that
uses both annotated as well as partially unanno-
tated data. The qualitative and quantitative analyis
of results from our preliminary motif segmenta-
tion model indicate that such motifs can help to
disambiguate contexts of single tokens, and pro-
vide cleaner, more interpretable representations.
Finally, we obtain motif representations in form
of low-dimensional vector-space embeddings, and
our experimental findings indicate value of the
learnt representations in downstream applications.
We believe that the approach has considerable the-
oretical as well as practical merits, and provides a
simple and clean formulation for modeling phrasal
and sentential semantics.
In particular, we believe that ours is the first
method that can invoke different meaning repre-
sentations for a token depending on textual context
of the sentence. The flexibility of having separate
representations to model different semantic senses
has considerable valuable, as compared with ex-
tant approaches that assign a single representation
to each token, and are hence constrained to con-
flate several semantic senses into a common repre-
sentation. The approach also elegantly deals with
the problematic issue of differential compositional
and non-compositional usage of words. Future
work can focus on a more thorough quantitative
evaluation of the paradigm, as well as extension to
model non-contiguous motifs.
References
Colin Bannard. 2007. A measure of syntactic flexibil-
ity for automatically identifying multiword expres-
sions in corpora. In Proceedings of the Workshop
on a Broader Perspective on Multiword Expressions,
pages 1?8. Association for Computational Linguis-
tics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association
for computational linguistics, pages 263?270. Asso-
ciation for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1?8.
Association for Computational Linguistics.
642
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1034?1046. Asso-
ciation for Computational Linguistics.
James Richard Curran. 2003. From Distributional to
Semantic Similarity. Ph.D. thesis, Institute for Com-
municating and Collaborative Systems School of In-
formatics University of Edinburgh.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In ACL.
Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-
maya Sachan, Shashank Srivastava, and Eduard
Hovy. 2013. A structured distributional semantic
model: Integrating structure with semantics. ACL
2013, page 20.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394?1404. Asso-
ciation for Computational Linguistics.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2010.
Concrete sentence spaces for compositional dis-
tributional models of meaning. arXiv preprint
arXiv:1101.0309.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. Meta4NLP
2013, page 52.
R?emi Lebret and Ronan Lebret. 2013. Word
emdeddings through hellinger pca. arXiv preprint
arXiv:1312.5542.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL, pages
236?244.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318?329.
Springer.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In ACL.
Pavel Pecina. 2008. A machine learning approach to
multiword expression extraction. In Proceedings of
the LREC MWE 2008 Workshop, pages 54?57. Cite-
seer.
Scott Songlin Piao, Paul Rayson, Dawn Archer, and
Tony McEnery. 2005. Comparing and combining
a semantic tagger and a statistical tool for mwe ex-
traction. Computer Speech & Language, 19(4):378?
397.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and
Aline Villavicencio. 2008. An evaluation of meth-
ods for the extraction of multiword expressions.
In Proceedings of the LREC Workshop-Towards
a Shared Task for Multiword Expressions (MWE
2008), pages 50?53.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Shashank Srivastava, Dirk Hovy, and Eduard H. Hovy.
2013. A walk-based semantically enriched tree
kernel over distributed word representations. In
EMNLP, pages 1411?1416.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. Brat: a web-based tool for nlp-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102?107. Association for Computational Lin-
guistics.
Yulia Tsvetkov and Shuly Wintner. 2011. Identifica-
tion of multi-word expressions by combining mul-
tiple linguistic information sources. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 836?845. Association
for Computational Linguistics.
Peter D Turney, Patrick Pantel, et al 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In EMNLP-
CoNLL, pages 1034?1043.
643
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20?29,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
A Structured Distributional Semantic Model : Integrating Structure with
Semantics
Kartik Goyal? Sujay Kumar Jauhar? Huiying Li?
Mrinmaya Sachan? Shashank Srivastava? Eduard Hovy
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{kartikgo,sjauhar,huiyingl,mrinmays,shashans,hovy}@cs.cmu.edu
Abstract
In this paper we present a novel approach
(SDSM) that incorporates structure in dis-
tributional semantics. SDSM represents
meaning as relation specific distributions
over syntactic neighborhoods. We em-
pirically show that the model can effec-
tively represent the semantics of single
words and provides significant advantages
when dealing with phrasal units that in-
volve word composition. In particular, we
demonstrate that our model outperforms
both state-of-the-art window-based word
embeddings as well as simple approaches
for composing distributional semantic rep-
resentations on an artificial task of verb
sense disambiguation and a real-world ap-
plication of judging event coreference.
1 Introduction
With the advent of statistical methods for NLP,
Distributional Semantic Models (DSMs) have
emerged as powerful method for representing
word semantics. In particular, the distributional
vector formalism, which represents meaning by a
distribution over neighboring words, has gained
the most popularity.
DSMs are widely used in information re-
trieval (Manning et al, 2008), question answer-
ing (Tellex et al, 2003), semantic similarity com-
putation (Wong and Raghavan, 1984; McCarthy
and Carroll, 2003), automated dictionary building
(Curran, 2003), automated essay grading (Lan-
dauer and Dutnais, 1997), word-sense discrimina-
tion and disambiguation (McCarthy et al, 2004;
?*Equally contributing authors
Sch?tze, 1998), selectional preference model-
ing (Erk, 2007) and identification of translation
equivalents (Hjelm, 2007).
Systems that use DSMs implicitly make a bag
of words assumption: that the meaning of a phrase
can be reasonably estimated from the meaning of
its constituents. However, semantics in natural
language is a compositional phenomenon, encom-
passing interactions between syntactic structures,
and the meaning of lexical constituents. It fol-
lows that the DSM formalism lends itself poorly
to composition since it implicitly disregards syn-
tactic structure. For instance, the distributions for
?Lincoln?, ?Booth?, and ?killed? when merged
produce the same result regardless of whether the
input is ?Booth killed Lincoln? or ?Lincoln killed
Booth?. As suggested by Pantel and Lin (2000)
and others, modeling the distribution over prefer-
ential attachments for each syntactic relation sep-
arately can yield greater expressive power.
Attempts have been made to model linguistic
composition of individual word vectors (Mitchell
and Lapata, 2009), as well as remedy the inher-
ent failings of the standard distributional approach
(Erk and Pad?, 2008). The results show vary-
ing degrees of efficacy, but have largely failed to
model deeper lexical semantics or compositional
expectations of words and word combinations.
In this paper we propose an extension to the
traditional DSM model that explicitly preserves
structural information and permits the approxima-
tion of distributional expectation over dependency
relations. We extend the generic DSM model by
representing a word as distributions over relation-
specific syntactic neighborhoods. One can think
of the Structured DSM (SDSM) representation
of a word/phrase as several vectors defined over
the same vocabulary, each vector representing the
20
word?s selectional preferences for a different syn-
tactic argument. We argue that this represen-
tation captures individual word semantics effec-
tively, and is better able to express the semantics
of composed units.
The overarching theme of our framework of
evaluation is to explore the semantic space of the
SDSM. We do this by measuring its ability to dis-
criminate between varying surface forms of the
same underlying concept. We perform the follow-
ing set of experiments to evaluate its expressive
power, and conclude the following:
1. Experiments with single words on similar-
ity scoring and substitute selection: SDSM
performs at par with window-based distribu-
tional vectors.
2. Experiments with phrasal units on two-word
composition: state-of-the-art results are pro-
duced on the dataset from Mitchell and Lap-
ata (2008) in terms of correlation with human
judgment.
3. Experiments with larger structures on the
task of judging event coreferentiality: SDSM
shows superior performance over state-of-
the-art window-based word embeddings, and
simple models for composing distributional
semantic representations.
2 Related Work
Distributional Semantic Models are based on the
intuition that ?a word is characterized by the com-
pany it keeps? (Firth, 1957). While DSMs have
been very successful on a variety of NLP tasks,
they are generally considered inappropriate for
deeper semantics because they lack the ability to
model composition, modifiers or negation.
Recently, there has been a surge in studies to
model a stronger form of semantics by phrasing
the problem of DSM compositionality as one of
vector composition. These techniques derive the
meaning of the combination of two words a and
b by a single vector c = f(a, b). Mitchell and
Lapata (2008) propose a framework to define the
composition c = f(a, b, r,K) where r is the re-
lation between a and b, and K is the additional
knowledge used to define composition.
While the framework is quite general, most
models in the literature tend to disregard K and
r and are generally restricted to component-wise
addition and multiplication on the vectors to be
composed, with slight variations. Dinu and Lap-
ata (2010) and S?aghdha and Korhonen (2011) in-
troduced a probabilistic model to represent word
meanings by a latent variable model. Subse-
quently, other high-dimensional extensions by
Rudolph and Giesbrecht (2010), Baroni and Zam-
parelli (2010) and Grefenstette et al (2011), re-
gression models by Guevara (2010), and recursive
neural network based solutions by Socher et al
(2012) and Collobert et al (2011) have been pro-
posed.
Pantel and Lin (2000) and Erk and Pad? (2008)
attempted to include syntactic context in distri-
butional models. However, their approaches do
not explicitly construct phrase-level meaning from
words which limits their applicability to real world
problems. A quasi-compositional approach was
also attempted in Thater et al (2010) by a system-
atic combination of first and second order context
vectors. To the best of our knowledge the formu-
lation of composition we propose is the first to ac-
count for K and r within the general framework
of composition c = f(a, b, r,K).
3 Structured Distributional Semantics
In this section, we describe our Structured Distri-
butional Semantic framework in detail. We first
build a large knowledge base from sample english
texts and use it to represent basic lexical units.
Next, we describe a technique to obtain the repre-
sentation for larger units by composing their con-
stituents.
3.1 The PropStore
To build a lexicon of SDSM representations for
a given vocabulary we construct a proposition
knowledge base (the PropStore) by processing the
text of Simple English Wikipedia through a de-
pendency parser. Dependency arcs are stored as
3-tuples of the form ?w1, r, w2?, denoting occur-
rences of words w1 and word w2 related by the
syntactic dependency r. We also store sentence
identifiers for each triple for reasons described
later. In addition to the words? surface-forms, the
PropStore also stores their POS tags, lemmas, and
Wordnet supersenses.
The PropStore can be used to query for pre-
ferred expectations of words, supersenses, re-
lations, etc., around a given word. In the
example in Figure 1, the query (SST(W1)
21
Figure 1: Sample sentences & triples
= verb.consumption, ?, dobj) i.e., ?what is
consumed?, might return expectations [pasta:1,
spaghetti:1, mice:1 . . . ]. In our implementation,
the relations and POS tags are obtained using the
Fanseparser (Tratz and Hovy, 2011), supersense
tags using sst-light (Ciaramita and Altun, 2006),
and lemmas are obtained from Wordnet (Miller,
1995).
3.2 Building the Representation
Next, we describe a method to represent lexical
entries as structured distributional matrices using
the PropStore.
The canonical form of a concept C (word,
phrase etc.) in the SDSM framework is a matrix
MC , whose entry MCij is a list of sentence identi-
fiers obtained by querying the PropStore for con-
texts in which C appears in the syntactic neigh-
borhood of the word j linked by the dependency
relation i. As with other distributional models in
the literature, the content of a cell is the frequency
of co-occurrence of its concept and word under the
given relational constraint.
This canonical matrix form can be interpreted
in several different ways. Each interpretation is
based on a different normalization scheme.
1. Row Norm: Each row of the matrix is inter-
preted as a distribution over words that attach
to the target concept with the given depen-
dency relation.
MCij =
Mij
?jMij
?i
2. Full Norm: The entire matrix is interpreted
as a distribution over the word-relation pairs
which can attach to the target concept.
MCij =
Mij
?i,jMij
?i, j
Figure 2: Mimicking composition of two words
3. Collapsed Vector Norm: The columns of
the matrix are collapsed to form a standard
normalized distributional vector trained on
dependency relations rather than sliding win-
dows.
MCj =
?iMij
?i,jMij
?j
3.3 Mimicking Compositionality
For representing intermediate multi-word phrases,
we extend the above word-relation matrix sym-
bolism in a bottom-up fashion. The combina-
tion hinges on the intuition that when lexical units
combine to form a larger syntactically connected
phrase, the representation of the phrase is given
by its own distributional neighborhood within the
embedded parse tree. The distributional neighbor-
hood of the net phrase can be computed using the
PropStore given syntactic relations anchored on its
parts. For the example in Figure 1, we can com-
pose SST(w1) = Noun.person and Lemma(W1)
= eat with relation ?nsubj? to obtain expectations
around ?people eat? yielding [pasta:1, spaghetti:1
. . . ] for the object relation ([dining room:2, restau-
rant:1 . . .] for the location relation, etc.) (See Fig-
ure 2). Larger phrasal queries can be built to an-
swer questions like ?What do people in China eat
with??, ?What do cows do??, etc. All of this helps
22
us to account for both relation r and knowledgeK
obtained from the PropStore within the composi-
tional framework c = f(a, b, r,K).
The general outline to obtain a composition of
two words is given in Algorithm 1. Here, we
first determine the sentence indices where the two
words w1 and w2 occur with relation r. Then,
we return the expectations around the two words
within these sentences. Note that the entire algo-
rithm can conveniently be written in the form of
database queries to our PropStore.
Algorithm 1 ComposePair(w1, r, w2)
M1 ? queryMatrix(w1)
M2 ? queryMatrix(w2)
SentIDs?M1(r) ?M2(r)
return ((M1? SentIDs) ? (M2? SentIDs))
Similar to the two-word composition process,
given a parse subtree T of a phrase, we obtain
its matrix representation of empirical counts over
word-relation contexts. This procedure is de-
scribed in Algorithm 2. Let the E = {e1 . . . en}
be the set of edges in T , ei = (wi1, ri, wi2)?i =
1 . . . n.
Algorithm 2 ComposePhrase(T )
SentIDs? All Sentences in corpus
for i = 1? n do
Mi1 ? queryMatrix(wi1)
Mi2 ? queryMatrix(wi2)
SentIDs? SentIDs ?(M1(ri) ?M2(ri))
end for
return ((M11? SentIDs) ? (M12? SentIDs)
? ? ? ? (Mn1? SentIDs) ? (Mn2? SentIDs))
3.4 Tackling Sparsity
The SDSM model reflects syntactic properties of
language through preferential filler constraints.
But by distributing counts over a set of relations
the resultant SDSM representation is compara-
tively much sparser than the DSM representation
for the same word. In this section we present some
ways to address this problem.
3.4.1 Sparse Back-off
The first technique to tackle sparsity is to back
off to progressively more general levels of lin-
guistic granularity when sparse matrix represen-
tations for words or compositional units are en-
countered or when the word or unit is not in the
lexicon. For example, the composition ?Balthazar
eats? cannot be directly computed if the named en-
tity ?Balthazar? does not occur in the PropStore?s
knowledge base. In this case, a query for a su-
persense substitute ? ?Noun.person eat? ? can be
issued instead. When supersenses themselves fail
to provide numerically significant distributions for
words or word combinations, a second back-off
step involves querying for POS tags. With coarser
levels of linguistic representation, the expressive
power of the distributions becomes diluted. But
this is often necessary to handle rare words. Note
that this is an issue with DSMs too.
3.4.2 Densification
In addition to the back-off method, we also pro-
pose a secondary method for ?densifying? distri-
butions. A concept?s distribution is modified by
using words encountered in its syntactic neighbor-
hood to infer counts for other semantically similar
words. In other terms, given the matrix represen-
tation of a concept, densification seeks to popu-
late its null columns (which each represent a word-
dimension in the structured distributional context)
with values weighted by their scaled similarities to
words (or effectively word-dimensions) that actu-
ally occur in the syntactic neighborhood.
For example, suppose the word ?play? had an
?nsubj? preferential vector that contained the fol-
lowing counts: [cat:4 ; Jane:2]. One might then
populate the column for ?dog? in this vector with
a count proportional to its similarity to the word
cat (say 0.8), thus resulting in the vector [cat:4 ;
Jane:2 ; dog:3.2]. These counts could just as well
be probability values or PMI associations (suitably
normalized). In this manner, the k most similar
word-dimensions can be densified for each word
that actually occurs in a syntactic context. As with
sparse back-off, there is an inherent trade-off be-
tween the degree of densification k and the expres-
sive power of the resulting representation.
3.4.3 Dimensionality Reduction
The final method tackles the problem of sparsity
by reducing the representation to a dense low-
dimensional word embedding using singular value
decomposition (SVD). In a typical term-document
matrix, SVD finds a low-dimensional approxima-
tion of the original matrix where columns become
latent concepts while similarity structure between
rows are preserved. The PropStore, as described in
Section 3.1, is an order-3 tensor with w1, w2 and
23
rel as its three axes. We explore the following two
possibilities to perform dimensionality reduction
using SVD.
Word-word matrix SVD. In this experiment,
we preserve the axes w1 and w2 and ignore the re-
lational information. Following the SVD regime (
W = U?V T ) where ? is a square diagonal ma-
trix of k largest singular values, and U and V are
m? k and n? k matrices respectively. We adopt
matrixU as the compacted concept representation.
Tensor SVD. To remedy the relation-agnostic
nature of the word-word SVD matrix represen-
tation, we use tensor SVD (Vasilescu and Ter-
zopoulos, 2002) to preserve the structural infor-
mation. The mode-n vectors of an order-N tensor
A?RI1?I2?...?IN are the In-dimensional vectors
obtained from A by varying index in while keep-
ing other indices fixed. The matrix formed by all
the mode-n vectors is a mode-n flattening of the
tensor. To obtain the compact representations of
concepts we thus first apply mode w1 flattening
and then perform SVD on the resulting tensor.
4 Single Word Evaluation
In this section we describe experiments and re-
sults for judging the expressive power of the struc-
tured distributional representation for individual
words. We use a similarity scoring task and a lexi-
cal substitute selection task for the purpose of this
evaluation. We compare the SDSM representa-
tion to standard window-based distributional vec-
tors trained on the same corpus (Simple English
Wikipedia). We also experiment with different
normalization techniques outlined in Section 3.2,
which effectively lead to structured distributional
representations with distinct interpretations.
We experimented with various similarity met-
rics and found that the normalized cityblock dis-
tance metric provides the most stable results.
CityBlock(X,Y ) =
ArcTan(d(X,Y ))
d(X,Y )
d(X,Y ) =
1
|R|
?
r?R
d(Xr, Yr)
Results in the rest of this section are thus reported
using the normalized cityblock metric. We also
report experimental results for the two methods
of alleviating sparsity discussed in Section 3.4,
namely, densification and SVD.
4.1 Similarity Scoring
On this task, the different semantic representations
were used to compute similarity scores between
two (out of context) words. We used a dataset
from Finkelstein et al (2002) for our experiments.
It consists of 353 pairs of words along with an av-
eraged similarity score on a scale of 1.0 to 10.0
obtained from 13?16 human judges.
4.2 Lexical Substitute Selection
In the second task, the same set of semantic repre-
sentations was used to produce a similarity rank-
ing on the Turney (2002) ESL dataset. This dataset
comprises 50 words that appear in a context (we
discarded the context in this experiment), along
with 4 candidate lexical substitutions. We eval-
uate the semantic representations on the basis of
their ability to discriminate the top-ranked candi-
date.1
4.3 Results and Discussion
Table 1 summarizes the results for the window-
based baseline and each of the structured distri-
butional representations on both tasks. It shows
that our representations for single words are com-
petitive with window based distributional vectors.
Densification in certain conditions improves our
results, but no consistent pattern is discernible.
This can be attributed to the trade-off between the
gain from generalization and the noise introduced
by semantic drift.
Hence we resort to dimensionality reduction as
an additional method of reducing sparsity. Table
2 gives correlation scores on the Finkelstein et al
(2002) dataset when SVD is performed on the rep-
resentations, as described in Section 3.4.3. We
give results when 100 and 500 principal compo-
nents are preserved for both SVD techniques.
These experiments suggest that though afflicted
by sparsity, the proposed structured distributional
paradigm is competitive with window-based dis-
tributional vectors. In the following sections we
show that that the framework provides consid-
erably greater power for modeling composition
when dealing with units consisting of more than
one word.
1While we are aware of the standard lexical substitution
corpus from McCarthy and Navigli (2007) we chose the one
mentioned above for its basic vocabulary, lower dependence
on context, and simpler evaluation framework.
24
Model Finklestein (Corr.) ESL (% Acc.)
DSM 0.283 0.247
Collapsed 0.260 0.178
FullNorm 0.282 0.192
RowNorm 0.236 0.264
Densified RowNorm 0.259 0.267
Table 1: Single Word Evaluation
Model Correlation
matSVD100 0.207
matSVD500 0.221
tenSVD100 0.267
tenSVD500 0.315
Table 2: Finklestein: Correlation using SVD
5 Verb Sense Disambiguation using
Composition
In this section, we examine how well our model
performs composition on a pair of words. We
derive the compositional semantic representations
for word pairs from the M&L dataset (Mitchell
and Lapata, 2008) and compare our performance
with M&L?s additive and multiplicative models of
composition.
5.1 Dataset
The M&L dataset consists of polysemous intransi-
tive verb and subject pairs that co-occur at least 50
times in the BNC corpus. Additionally two land-
mark words are given for every polysemous verb,
each corresponding to one of its senses. The sub-
ject nouns provide contextual disambiguation for
the senses of the verb. For each [subject, verb,
landmark] tuple, a human assigned score on a 7-
point scale is provided, indicating the compatibil-
ity of the landmark with the reference verb-subj
pair. For example, for the pair ?gun bomb?, land-
mark ?thunder? is more similar to the verb than
landmark ?prosper?. The corpus contains 120 tu-
ples and altogether 3600 human judgments. Re-
liability of the human ratings is examined by cal-
culating inter-annotator Spearman?s ? correlation
coefficient.
5.2 Experiment procedure
For each tuple in the dataset, we derive the com-
posed word-pair matrix for the reference verb-subj
pair based on the algorithm described in Section
3.3 and query the single-word matrix for the land-
mark word. A few modifications are made to ad-
just the algorithm for the current task:
1. In our formulation, the dependency relation
needs to be specified in order to compose
a pair of words. Hence, we determine the
five most frequent relations between w1 and
w2 by querying the PropStore. We then use
the algorithm in Section 3.3 to compose the
verb-subj word pair using these relations, re-
sulting in five composed representations.
2. The word pairs in M&L corpus are ex-
tracted from a parsed version of the BNC cor-
pus, while our PropStore is built on Simple
Wikipedia texts, whose vocabulary is signif-
icantly different from that of the BNC cor-
pus. This causes null returns in our PropStore
queries, in which case we back-off to retriev-
ing results for super-sense tags of both the
words. Finally, the composed matrix and the
landmark matrix are compared against each
other by different matrix distance measures,
which results in a similarity score. For a [sub-
ject, verb, landmark] tuple, we average the
similarity scores yielded by the relations ob-
tained in 1.
The Spearman Correlation ? between our sim-
ilarity ratings and the ones assigned by human
judges is computed over all the tuples. Follow-
ing M&L?s experiments, the inter-annotator agree-
ment correlation coefficient serves an upper bound
on the task.
5.3 Results and Discussion
As in Section 4, we choose the cityblock mea-
sure as the similarity metric of choice. Table 3
shows the evaluation results for two word compo-
sition. Except for row normalization, both forms
of normalization in the structured distributional
paradigm show significant improvement over the
results reported by M&L. The results are statisti-
cally significant at p-value = 0.004 and 0.001 for
Full Norm and Collapsed Vector Norm, respec-
tively.
Model ?
M&L combined 0.19
Row Norm 0.134
Full Norm 0.289
Collapsed Vector Norm 0.259
UpperBound 0.40
Table 3: Two Word Composition Evaluation
These results validate our hypothesis that the in-
tegration of structure into distributional semantics
25
as well as our framing of word composition to-
gether outperform window-based representations
under simplistic models of composition such as
addition and multiplication. This finding is further
re-enforced in the following experiments on event
coreferentiality judgment.
6 Event Coreference Judgment
Given the SDSM formulation and assuming no
sparsity constraints, it is possible to calculate
SDSM matrices for composed concepts. However,
are these correct? Intuitively, if they truly capture
semantics, the two SDSM matrix representations
for ?Booth assassinated Lincoln? and ?Booth shot
Lincoln with a gun" should be (almost) the same.
To test this hypothesis we turn to the task of pre-
dicting whether two event mentions are coreferent
or not, even if their surface forms differ.
While automated resolution of entity coref-
erence has been an actively researched area
(Haghighi and Klein, 2009; Stoyanov et al, 2009;
Raghunathan et al, 2010), there has been rela-
tively little work on event coreference resolution.
Lee et al (2012) perform joint cross-document
entity and event coreference resolution using the
two-way feedback between events and their argu-
ments.
In this paper, however, we only consider coref-
erentiality between pairs of events. Formally,
two event mentions generally refer to the same
event when their respective actions, agents, pa-
tients, locations, and times are (almost) the same.
Given the non-compositional nature of determin-
ing equality of locations and times, we represent
each event mention by a triple E = (e, a, p) for
the event, agent, and patient.
While linguistic theory of argument realiza-
tion is a debated research area (Levin and Rap-
paport Hovav, 2005; Goldberg, 2005), it is com-
monly believed that event structure (Moens and
Steedman, 1988) centralizes on the predicate,
which governs and selects its role arguments
(Jackendoff, 1987). In the corpora we use for
our experiments, most event mentions are verbs.
However, when nominalized events are encoun-
tered, we replace them by their verbal forms. We
use SRL Collobert et al (2011) to determine the
agent and patient arguments of an event mention.
When SRL fails to determine either role, its empir-
ical substitutes are obtained by querying the Prop-
Store for the most likely word expectations for the
role. The triple (e, a, p) is thus the composition
of the triples (a, relagent, e) and (p, relpatient, e),
and hence a complex object. To determine equal-
ity of this complex composed representation we
generate three levels of progressively simplified
event constituents for comparison:
Level 1: Full Composition:
Mfull = ComposePhrase(e, a, p).
Level 2: Partial Composition:
Mpart:EA = ComposePair(e, r, a)
Mpart:EP = ComposePair(e, r, p).
Level 3: No Composition:
ME = queryMatrix(e)
MA = queryMatrix(a)
MP = queryMatrix(p).
To judge coreference between
events E1 and E2, we compute pair-
wise similarities Sim(M1full,M2full),
Sim(M1part:EA,M2part:EA), etc., for each
level of the composed triple representation. Fur-
thermore, we vary the computation of similarity
by considering different levels of granularity
(lemma, SST), various choices of distance metric
(Euclidean, Cityblock, Cosine), and score nor-
malization techniques (Row-wise, Full, Column
collapsed). This results in 159 similarity-based
features for every pair of events, which are used
to train a classifier to make a binary decision for
coreferentiality.
6.1 Datasets
We evaluate our method on two datasets and com-
pare it against four baselines, two of which use
window based distributional vectors and two that
employ weaker forms of composition.
IC Event Coreference Corpus: The dataset
(citation suppressed), drawn from 100 news arti-
cles about violent events, contains manually cre-
ated annotations for 2214 pairs of co-referent
and non-coreferent events each. Where available,
events? semantic role-fillers for agent and patient
are annotated as well. When missing, empirical
substitutes were obtained by querying the Prop-
Store for the preferred word attachments.
EventCorefBank (ECB) corpus: This corpus
(Bejan and Harabagiu, 2010) of 482 documents
from Google News is clustered into 45 topics,
with event coreference chains annotated over each
topic. The event mentions are enriched with se-
mantic roles to obtain the canonical event struc-
ture described above. Positive instances are ob-
26
IC Corpus ECB Corpus
Prec Rec F-1 Acc Prec Rec F-1 Acc
SDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843
Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791
DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830
MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831
AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834
Table 4: Cross-validation Performance on IC and ECB dataset
tained by taking pairwise event mentions within
each chain, and negative instances are generated
from pairwise event mentions across chains, but
within the same topic. This results in 11039 posi-
tive instances and 33459 negative instances.
6.2 Baselines:
To establish the efficacy of our model, we com-
pare SDSM against a purely window-based base-
line (DSM) trained on the same corpus. In our ex-
periments we set a window size of three words to
either side of the target. We also compare SDSM
against the window-based embeddings trained us-
ing a recursive neural network (SENNA) (Col-
lobert et al, 2011) on both datsets. SENNA em-
beddings are state-of-the-art for many NLP tasks.
The second baseline uses SENNA to generate
level 3 similarity features for events? individual
words (agent, patient and action). As our final
set of baselines, we extend two simple techniques
proposed by Mitchell and Lapata (2008) that use
element-wise addition and multiplication opera-
tors to perform composition. The two baselines
thus obtained are AVC (element-wise addition)
and MVC (element-wise multiplication).
6.3 Results and Discussion:
We experimented with a number of common clas-
sifiers, and selected decision-trees (J48) as they
give the best classification accuracy. Table 4 sum-
marizes our results on both datasets.
The results reveal that the SDSM model con-
sistently outperforms DSM, SENNA embeddings,
and the MVC and AVC models, both in terms
of F-1 score and accuracy. The IC corpus com-
prises of domain specific texts, resulting in high
lexical overlap between event mentions. Hence,
the scores on the IC corpus are consistently higher
than those on the ECB corpus.
The improvements over DSM and SENNA em-
beddings, support our hypothesis that syntax lends
greater expressive power to distributional seman-
tics in compositional configurations. Furthermore,
the increase in predictive accuracy over MVC and
AVC shows that our formulation of composition
of two words based on the relation binding them
yields a stronger form of composition than simple
additive and multiplicative models.
Next, we perform an ablation study to deter-
mine the most predictive features for the task of
determining event coreferentiality. The forward
selection procedure reveals that the most informa-
tive attributes are the level 2 compositional fea-
tures involving the agent and the action, as well as
their individual level 3 features. This corresponds
to the intuition that the agent and the action are the
principal determiners for identifying events. Fea-
tures involving the patient and level 1 features are
least useful. The latter involves full composition,
resulting in sparse representations and hence have
low predictive power.
7 Conclusion and Future Work
In this paper we outlined an approach that intro-
duces structure into distributional semantics. We
presented a method to compose distributional rep-
resentations of individual units into larger com-
posed structures. We tested the efficacy of our
model on several evaluation tasks. Our model?s
performance is competitive for tasks dealing with
semantic similarity of individual words, even
though it suffers from the problem of sparsity.
Additionally, it outperforms window-based ap-
proaches on tasks involving semantic composi-
tion. In future work we hope to extend this for-
malism to other semantic tasks like paraphrase de-
tection and recognizing textual entailment.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper. This
work was supported in part by the following
grants: NSF grant IIS-1143703, NSF award IIS-
1147810, DARPA grant FA87501220342.
27
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 594?602, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 999888:2493?2537,
November.
James Richard Curran. 2003. From distributional to
semantic similarity. Technical report.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10, pages
1162?1172, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 897?906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The con-
cept revisited. In ACM Transactions on Information
Systems, volume 20, pages 116?131, January.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
Adele E. Goldberg. 2005. Argument Realization: Cog-
nitive Grouping and Theoretical Extensions.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Proceedings of the
Ninth International Conference on Computational
Semantics, IWCS ?11, pages 125?134, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, GEMS ?10, pages 33?37, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ?09, pages 1152?
1161, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Hans Hjelm. 2007. Identifying cross language term
equivalents using statistical machine translation and
distributional association measures. In Proceedings
of NODALIDA, pages 97?104. Citeseer.
Ray Jackendoff. 1987. The status of thematic roles in
linguistic theory. Linguistic Inquiry, 18(3):369?411.
Thomas K Landauer and Susan T. Dutnais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
pages 211?240.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment Realization. Cambridge University Press.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Comput.
Linguist., 29(4):639?654, December.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task.
In Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007), Prague,
Czech Republic, pages 48?53.
28
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, ACL ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41, Novem-
ber.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings
of ACL-08: HLT, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 1 - Volume
1, EMNLP ?09, pages 430?439, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational lin-
guistics, 14(2):15?28.
Patrick Pantel and Dekang Lin. 2000. Word-for-word
glossing with contextually similar words. In Pro-
ceedings of the 1st North American chapter of the
Association for Computational Linguistics confer-
ence, NAACL 2000, pages 78?85, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 492?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 907?916, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hinrich Sch?tze. 1998. Automatic word sense dis-
crimination. Comput. Linguist., 24(1):97?123.
Diarmuid ? S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1047?1057, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 1201?1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ?09, pages 656?664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In SIGIR, pages 41?47.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?10, pages
948?957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1257?1268, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Peter D. Turney. 2002. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. CoRR.
M. Alex O. Vasilescu and Demetri Terzopoulos. 2002.
Multilinear analysis of image ensembles: Tensor-
faces. In In Proceedings of the European Confer-
ence on Computer Vision, pages 447?460.
S. K. M. Wong and Vijay V. Raghavan. 1984. Vector
space model of information retrieval: a reevaluation.
In Proceedings of the 7th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?84, pages 167?185,
Swinton, UK. British Computer Society.
29
