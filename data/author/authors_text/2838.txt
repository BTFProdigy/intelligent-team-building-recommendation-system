187
188
189
190
Generalized Hebbian Algorithm for Incremental Singular Value
Decomposition in Natural Language Processing
Genevieve Gorrell
Department of Computer and Information Science
Linko?ping University
581 83 LINKO?PING
Sweden
gengo@ida.liu.se
Abstract
An algorithm based on the Generalized
Hebbian Algorithm is described that
allows the singular value decomposition
of a dataset to be learned based on
single observation pairs presented seri-
ally. The algorithm has minimal mem-
ory requirements, and is therefore in-
teresting in the natural language do-
main, where very large datasets are of-
ten used, and datasets quickly become
intractable. The technique is demon-
strated on the task of learning word
and letter bigram pairs from text.
1 Introduction
Dimensionality reduction techniques are of
great relevance within the field of natural lan-
guage processing. A persistent problem within
language processing is the over-specificity of
language, and the sparsity of data. Corpus-
based techniques depend on a sufficiency of
examples in order to model human language
use, but the Zipfian nature of frequency be-
haviour in language means that this approach
has diminishing returns with corpus size. In
short, there are a large number of ways to say
the same thing, and no matter how large your
corpus is, you will never cover all the things
that might reasonably be said. Language is
often too rich for the task being performed;
for example it can be difficult to establish that
two documents are discussing the same topic.
Likewise no matter how much data your sys-
tem has seen during training, it will invari-
ably see something new at run-time in a do-
main of any complexity. Any approach to au-
tomatic natural language processing will en-
counter this problem on several levels, creat-
ing a need for techniques which compensate
for this.
Imagine we have a set of data stored as a
matrix. Techniques based on eigen decomposi-
tion allow such a matrix to be transformed into
a set of orthogonal vectors, each with an asso-
ciated ?strength?, or eigenvalue. This trans-
formation allows the data contained in the ma-
trix to be compressed; by discarding the less
significant vectors (dimensions) the matrix can
be approximated with fewer numbers. This
is what is meant by dimensionality reduction.
The technique is guaranteed to return the clos-
est (least squared error) approximation possi-
ble for a given number of numbers (Golub and
Reinsch, 1970). In certain domains, however,
the technique has even greater significance. It
is effectively forcing the data through a bot-
tleneck; requiring it to describe itself using
an impoverished construct set. This can al-
low the critical underlying features to reveal
themselves. In language, for example, these
features might be semantic constructs. It can
also improve the data, in the case that the de-
tail is noise, or richness not relevant to the
task.
Singular value decomposition (SVD) is a
near relative of eigen decomposition, appro-
priate to domains where input is asymmetri-
cal. The best known application of singular
value decomposition within natural language
processing is Latent Semantic Analysis (Deer-
wester et al, 1990). Latent Semantic Analysis
(LSA) allows passages of text to be compared
to each other in a reduced-dimensionality se-
mantic space, based on the words they contain.
97
The technique has been successfully applied to
information retrieval, where the overspecificity
of language is particularly problematic; text
searches often miss relevant documents where
different vocabulary has been chosen in the
search terms to that used in the document (for
example, the user searches on ?eigen decom-
position? and fails to retrieve documents on
factor analysis). LSA has also been applied in
language modelling (Bellegarda, 2000), where
it has been used to incorporate long-span se-
mantic dependencies.
Much research has been done on optimis-
ing eigen decomposition algorithms, and the
extent to which they can be optimised de-
pends on the area of application. Most natu-
ral language problems involve sparse matrices,
since there are many words in a natural lan-
guage and the great majority do not appear in,
for example, any one document. Domains in
which matrices are less sparse lend themselves
to such techniques as Golub-Kahan-Reinsch
(Golub and Reinsch, 1970) and Jacobi-like ap-
proaches. Techniques such as those described
in (Berry, 1992) are more appropriate in the
natural language domain.
Optimisation is an important way to in-
crease the applicability of eigen and singu-
lar value decomposition. Designing algorithms
that accommodate different requirements is
another. For example, another drawback to
Jacobi-like approaches is that they calculate
all the singular triplets (singular vector pairs
with associated values) simultaneously, which
may not be practical in a situation where only
the top few are required. Consider also that
the methods mentioned so far assume that the
entire matrix is available from the start. There
are many situations in which data may con-
tinue to become available.
(Berry et al, 1995) describe a number of
techniques for including new data in an ex-
isting decomposition. Their techniques apply
to a situation in which SVD has been per-
formed on a collection of data, then new data
becomes available. However, these techniques
are either expensive, or else they are approxi-
mations which degrade in quality over time.
They are useful in the context of updating
an existing batch decomposition with a sec-
ond batch of data, but are less applicable in
the case where data are presented serially, for
example, in the context of a learning system.
Furthermore, there are limits to the size of ma-
trix that can feasibly be processed using batch
decomposition techniques. This is especially
relevant within natural language processing,
where very large corpora are common. Ran-
dom Indexing (Kanerva et al, 2000) provides
a less principled, though very simple and ef-
ficient, alternative to SVD for dimensionality
reduction over large corpora.
This paper describes an approach to singu-
lar value decomposition based on the General-
ized Hebbian Algorithm (Sanger, 1989). GHA
calculates the eigen decomposition of a ma-
trix based on single observations presented se-
rially. The algorithm presented here differs in
that where GHA produces the eigen decom-
position of symmetrical data, our algorithm
produces the singular value decomposition of
asymmetrical data. It allows singular vectors
to be learned from paired inputs presented se-
rially using no more memory than is required
to store the singular vector pairs themselves.
It is therefore relevant in situations where the
size of the dataset makes conventional batch
approaches infeasible. It is also of interest in
the context of adaptivity, since it has the po-
tential to adapt to changing input. The learn-
ing update operation is very cheap computa-
tionally. Assuming a stable vector length, each
update operation takes exactly as long as each
previous one; there is no increase with corpus
size to the speed of the update. Matrix di-
mensions may increase during processing. The
algorithm produces singular vector pairs one
at a time, starting with the most significant,
which means that useful data becomes avail-
able quickly; many standard techniques pro-
duce the entire decomposition simultaneously.
Since it is a learning technique, however, it dif-
fers from what would normally be considered
an incremental technique, in that the algo-
rithm converges on the singular value decom-
position of the dataset, rather than at any one
point having the best solution possible for the
data it has seen so far. The method is poten-
tially most appropriate in situations where the
dataset is very large or unbounded: smaller,
bounded datasets may be more efficiently pro-
cessed by other methods. Furthermore, our
98
approach is limited to cases where the final
matrix is expressible as the linear sum of outer
products of the data vectors. Note in particu-
lar that Latent Semantic Analysis, as usually
implemented, is not an example of this, be-
cause LSA takes the log of the final sums in
each cell (Dumais, 1990). LSA, however, does
not depend on singular value decomposition;
Gorrell and Webb (Gorrell and Webb, 2005)
discuss using eigen decomposition to perform
LSA, and demonstrate LSA using the Gen-
eralized Hebbian Algorithm in its unmodified
form. Sanger (Sanger, 1993) presents similar
work, and future work will involve more de-
tailed comparison of this approach to his.
The next section describes the algorithm.
Section 3 describes implementation in practi-
cal terms. Section 4 illustrates, using word
n-gram and letter n-gram tasks as examples
and section 5 concludes.
2 The Algorithm
This section introduces the Generalized Heb-
bian Algorithm, and shows how the technique
can be adapted to the rectangular matrix form
of singular value decomposition. Eigen decom-
position requires as input a square diagonally-
symmetrical matrix, that is to say, one in
which the cell value at row x, column y is
the same as that at row y, column x. The
kind of data described by such a matrix is
the correlation between data in a particular
space with other data in the same space. For
example, we might wish to describe how of-
ten a particular word appears with a particu-
lar other word. The data therefore are sym-
metrical relations between items in the same
space; word a appears with word b exactly as
often as word b appears with word a. In sin-
gular value decomposition, rectangular input
matrices are handled. Ordered word bigrams
are an example of this; imagine a matrix in
which rows correspond to the first word in a
bigram, and columns to the second. The num-
ber of times that word b appears after word
a is by no means the same as the number
of times that word a appears after word b.
Rows and columns are different spaces; rows
are the space of first words in the bigrams,
and columns are the space of second words.
The singular value decomposition of a rect-
angular data matrix, A, can be presented as;
A = U?V T (1)
where U and V are matrices of orthogonal left
and right singular vectors (columns) respec-
tively, and ? is a diagonal matrix of the cor-
responding singular values. The U and V ma-
trices can be seen as a matched set of orthogo-
nal basis vectors in their corresponding spaces,
while the singular values specify the effective
magnitude of each vector pair. By convention,
these matrices are sorted such that the diag-
onal of ? is monotonically decreasing, and it
is a property of SVD that preserving only the
first (largest) N of these (and hence also only
the first N columns of U and V) provides a
least-squared error, rank-N approximation to
the original matrix A.
Singular Value Decomposition is intimately
related to eigenvalue decomposition in that the
singular vectors, U and V , of the data matrix,
A, are simply the eigenvectors of A ? AT and
AT ? A, respectively, and the singular values,
?, are the square-roots of the corresponding
eigenvalues.
2.1 Generalised Hebbian Algorithm
Oja and Karhunen (Oja and Karhunen, 1985)
demonstrated an incremental solution to find-
ing the first eigenvector from data arriving in
the form of serial data items presented as vec-
tors, and Sanger (Sanger, 1989) later gener-
alized this to finding the first N eigenvectors
with the Generalized Hebbian Algorithm. The
algorithm converges on the exact eigen decom-
position of the data with a probability of one.
The essence of these algorithms is a simple
Hebbian learning rule:
Un(t + 1) = Un(t) + ? ? (UTn ?Aj) ?Aj (2)
Un is the n?th column of U (i.e., the n?th eigen-
vector, see equation 1), ? is the learning rate
and Aj is the j?th column of training matrix
A. t is the timestep. The only modification to
this required in order to extend it to multiple
eigenvectors is that each Un needs to shadow
any lower-ranked Um(m > n) by removing its
projection from the input Aj in order to assure
both orthogonality and an ordered ranking of
99
the resulting eigenvectors. Sanger?s final for-
mulation (Sanger, 1989) is:
cij(t+ 1) = cij(t) + ?(t)(yi(t)xj(t) (3)
?yi(t)
?
k?i
ckj(t)yk(t))
In the above, cij is an individual element in
the current eigenvector, xj is the input vector
and yi is the activation (that is to say, ci.xj ,
the dot product of the input vector with the
ith eigenvector). ? is the learning rate.
To summarise, the formula updates the cur-
rent eigenvector by adding to it the input vec-
tor multiplied by the activation minus the pro-
jection of the input vector on all the eigenvec-
tors so far including the current eigenvector,
multiplied by the activation. Including the
current eigenvector in the projection subtrac-
tion step has the effect of keeping the eigen-
vectors normalised. Note that Sanger includes
an explicit learning rate, ?. The formula can
be varied slightly by not including the current
eigenvector in the projection subtraction step.
In the absence of the autonormalisation influ-
ence, the vector is allowed to grow long. This
has the effect of introducing an implicit learn-
ing rate, since the vector only begins to grow
long when it settles in the right direction, and
since further learning has less impact once the
vector has become long. Weng et al (Weng
et al, 2003) demonstrate the efficacy of this
approach. So, in vector form, assuming C to
be the eigenvector currently being trained, ex-
panding y out and using the implicit learning
rate;
ci = ci.x(x?
?
j<i
(x.cj)cj) (4)
Delta notation is used to describe the update
here, for further readability. The subtracted
element is responsible for removing from the
training update any projection on previous
singular vectors, thereby ensuring orthgonal-
ity. Let us assume for the moment that we
are calculating only the first eigenvector. The
training update, that is, the vector to be added
to the eigenvector, can then be more simply
described as follows, making the next steps
more readable;
c = c.x(x) (5)
2.2 Extension to Paired Data
Let us begin with a simplification of 5:
c = 1ncX(X) (6)
Here, the upper case X is the entire data ma-
trix. n is the number of training items. The
simplification is valid in the case that c is sta-
bilised; a simplification that in our case will
become more valid with time. Extension to
paired data initially appears to present a prob-
lem. As mentioned earlier, the singular vectors
of a rectangular matrix are the eigenvectors
of the matrix multiplied by its transpose, and
the eigenvectors of the transpose of the matrix
multiplied by itself. Running GHA on a non-
square non-symmetrical matrix M, ie. paired
data, would therefore be achievable using stan-
dard GHA as follows:
ca = 1nc
aMMT (MMT ) (7)
cb = 1nc
bMTM(MTM) (8)
In the above, ca and cb are left and right sin-
gular vectors. However, to be able to feed the
algorithm with rows of the matrices MMT
and MTM , we would need to have the en-
tire training corpus available simultaneously,
and square it, which we hoped to avoid. This
makes it impossible to use GHA for singu-
lar value decomposition of serially-presented
paired input in this way without some further
transformation. Equation 1, however, gives:
?ca = cbMT =
?
x
(cb.bx)ax (9)
?cb = caM =
?
x
(ca.ax)bx (10)
Here, ? is the singular value and a and b are
left and right data vectors. The above is valid
in the case that left and right singular vectors
ca and cb have settled (which will become more
accurate over time) and that data vectors a
and b outer-product and sum to M.
100
Inserting 9 and 10 into 7 and 8 allows them
to be reduced as follows:
ca = ?nc
bMTMMT (11)
cb = ?nc
aMMTM (12)
ca = ?
2
n c
aMMT (13)
cb = ?
2
n c
bMTM (14)
ca = ?
3
n c
bMT (15)
cb = ?
3
n c
aM (16)
ca = ?3(cb.b)a (17)
cb = ?3(ca.a)b (18)
This element can then be reinserted into GHA.
To summarise, where GHA dotted the input
with the eigenvector and multiplied the result
by the input vector to form the training up-
date (thereby adding the input vector to the
eigenvector with a length proportional to the
extent to which it reflects the current direc-
tion of the eigenvector) our formulation dots
the right input vector with the right singular
vector and multiplies the left input vector by
this quantity before adding it to the left singu-
lar vector, and vice versa. In this way, the two
sides cross-train each other. Below is the final
modification of GHA extended to cover mul-
tiple vector pairs. The original GHA is given
beneath it for comparison.
cai = cbi .b(a?
?
j<i
(a.caj )caj ) (19)
cbi = cai .a(b?
?
j<i
(b.cbj)cbj) (20)
ci = ci.x(x?
?
j<i
(x.cj)cj) (21)
In equations 6 and 9/10 we introduced approx-
imations that become accurate as the direction
of the singular vectors settles. These approx-
imations will therefore not interfere with the
accuracy of the final result, though they might
interfere with the rate of convergence. The
constant ?3 has been dropped in 19 and 20.
Its relevance is purely with respect to the cal-
culation of the singular value. Recall that in
(Weng et al, 2003) the eigenvalue is calcula-
ble as the average magnitude of the training
update c. In our formulation, according to
17 and 18, the singular value would be c di-
vided by ?3. Dropping the ?3 in 19 and 20
achieves that implicitly; the singular value is
once more the average length of the training
update.
The next section discusses practical aspects
of implementation. The following section illus-
trates usage, with English language word and
letter bigram data as test domains.
3 Implementation
Within the framework of the algorithm out-
lined above, there is still room for some im-
plementation decisions to be made. The naive
implementation can be summarised as follows:
the first datum is used to train the first singu-
lar vector pair; the projection of the first singu-
lar vector pair onto this datum is subtracted
from the datum; the datum is then used to
train the second singular vector pair and so on
for all the vector pairs; ensuing data items are
processed similarly. The main problem with
this approach is as follows. At the beginning
of the training process, the singular vectors are
close to the values they were initialised with,
and far away from the values they will settle
on. The second singular vector pair is trained
on the datum minus its projection onto the
first singular vector pair in order to prevent
the second singular vector pair from becom-
ing the same as the first. But if the first pair
is far away from its eventual direction, then
the second has a chance to move in the direc-
tion that the first will eventually take on. In
fact, all the vectors, such as they can whilst re-
maining orthogonal to each other, will move in
the strongest direction. Then, when the first
pair eventually takes on the right direction,
the others have difficulty recovering, since they
start to receive data that they have very lit-
tle projection on, meaning that they learn very
101
slowly. The problem can be addressed by wait-
ing until each singular vector pair is relatively
stable before beginning to train the next. By
?stable?, we mean that the vector is changing
little in its direction, such as to suggest it is
very close to its target. Measures of stability
might include the average variation in posi-
tion of the endpoint of the (normalised) vector
over a number of training iterations, or simply
length of the (unnormalised) vector, since a
long vector is one that is being reinforced by
the training data, such as it would be if it was
settled on the dominant feature. Termination
criteria might include that a target number
of singular vector pairs have been reached, or
that the last vector is increasing in length only
very slowly.
4 Application
The task of relating linguistic bigrams to each
other, as mentioned earlier, is an example of
a task appropriate to singular value decom-
position, in that the data is paired data, in
which each item is in a different space to the
other. Consider word bigrams, for example.
First word space is in a non-symmetrical re-
lationship to second word space; indeed, the
spaces are not even necessarily of the same di-
mensionality, since there could conceivably be
words in the corpus that never appear in the
first word slot (they might never appear at the
start of a sentence) or in the second word slot
(they might never appear at the end.) So a
matrix containing word counts, in which each
unique first word forms a row and each unique
second word forms a column, will not be a
square symmetrical matrix; the value at row
a, column b, will not be the same as the value
at row b column a, except by coincidence.
The significance of performing dimension-
ality reduction on word bigrams could be
thought of as follows. Language clearly ad-
heres to some extent to a rule system less
rich than the individual instances that form
its surface manifestation. Those rules govern
which words might follow which other words;
although the rule system is more complex and
of a longer range that word bigrams can hope
to illustrate, nonetheless the rule system gov-
erns the surface form of word bigrams, and we
might hope that it would be possible to discern
from word bigrams something of the nature of
the rules. In performing dimensionality reduc-
tion on word bigram data, we force the rules to
describe themselves through a more impover-
ished form than via the collection of instances
that form the training corpus. The hope is
that the resulting simplified description will
be a generalisable system that applies even to
instances not encountered at training time.
On a practical level, the outcome has ap-
plications in automatic language acquisition.
For example, the result might be applicable in
language modelling. Use of the learning algo-
rithm presented in this paper is appropriate
given the very large dimensions of any real-
istic corpus of language; The corpus chosen
for this demonstration is Margaret Mitchell?s
?Gone with the Wind?, which contains 19,296
unique words (421,373 in total), which fully re-
alized as a correlation matrix with, for exam-
ple, 4-byte floats would consume 1.5 gigabytes,
and which in any case, within natural language
processing, would not be considered a particu-
larly large corpus. Results on the word bigram
task are presented in the next section.
Letter bigrams provide a useful contrast-
ing illustration in this context; an input di-
mensionality of 26 allows the result to be
more easily visualised. Practical applications
might include automatic handwriting recogni-
tion, where an estimate of the likelihood of
a particular letter following another would be
useful information. The fact that there are
only twenty-something letters in most western
alphabets though makes the usefulness of the
incremental approach, and indeed, dimension-
ality reduction techniques in general, less ob-
vious in this domain. However, extending the
space to letter trigrams and even four-grams
would change the requirements. Section 4.2
discusses results on a letter bigram task.
4.1 Word Bigram Task
?Gone with the Wind? was presented to the
algorithm as word bigrams. Each word was
mapped to a vector containing all zeros but for
a one in the slot corresponding to the unique
word index assigned to that word. This had
the effect of making input to the algorithm a
normalised vector, and of making word vec-
tors orthogonal to each other. The singular
vector pair?s reaching a combined Euclidean
102
magnitude of 2000 was given as the criterion
for beginning to train the next vector pair, the
reasoning being that since the singular vectors
only start to grow long when they settle in
the approximate right direction and the data
starts to reinforce them, length forms a reason-
able heuristic for deciding if they are settled
enough to begin training the next vector pair.
2000 was chosen ad hoc based on observation
of the behaviour of the algorithm during train-
ing.
The data presented are the words most rep-
resentative of the top two singular vectors,
that is to say, the directions these singular
vectors mostly point in. Table 1 shows the
words with highest scores in the top two vec-
tor pairs. It says that in this vector pair, the
normalised left hand vector projected by 0.513
onto the vector for the word ?of? (or in other
words, these vectors have a dot product of
0.513.) The normalised right hand vector has
a projection of 0.876 onto the word ?the? etc.
This first table shows a left side dominated
by prepositions, with a right side in which
?the? is by far the most important word, but
which also contains many pronouns. The fact
that the first singular vector pair is effectively
about ?the? (the right hand side points far
more in the direction of ?the? than any other
word) reflects its status as the most common
word in the English language. What this result
is saying is that were we to be allowed only one
feature with which to describe word English
bigrams, a feature describing words appear-
ing before ?the? and words behaving similarly
to ?the? would be the best we could choose.
Other very common words in English are also
prominent in this feature.
Table 1: Top words in 1st singular vector pair
Vector 1, Eigenvalue 0.00938
of 0.5125468 the 0.8755944
in 0.49723375 her 0.28781646
and 0.39370865 a 0.23318098
to 0.2748983 his 0.14336193
on 0.21759394 she 0.1128443
at 0.17932475 it 0.06529821
for 0.16905183 he 0.063333265
with 0.16042696 you 0.058997907
from 0.13463423 their 0.05517004
Table 2 puts ?she?, ?he? and ?it? at the
top on the left, and four common verbs on the
right, indicating a pronoun-verb pattern as the
second most dominant feature in the corpus.
Table 2: Top words in 2nd singular vector pair
Vector 2, Eigenvalue 0.00427
she 0.6633538 was 0.58067155
he 0.38005337 had 0.50169927
it 0.30800354 could 0.2315106
and 0.18958427 would 0.17589279
4.2 Letter Bigram Task
Running the algorithm on letter bigrams illus-
trates different properties. Because there are
only 26 letters in the English alphabet, it is
meaningful to examine the entire singular vec-
tor pair. Figure 1 shows the third singular vec-
tor pair derived by running the algorithm on
letter bigrams. The y axis gives the projection
of the vector for the given letter onto the sin-
gular vector. The left singular vector is given
on the left, and the right on the right, that is
to say, the first letter in the bigram is on the
left and the second on the right. The first two
singular vector pairs are dominated by letter
frequency effects, but the third is interesting
because it clearly shows that the method has
identified vowels. It means that the third most
useful feature for determining the likelihood of
letter b following letter a is whether letter a
is a vowel. If letter b is a vowel, letter a is
less likely to be (vowels dominate the nega-
tive end of the right singular vector). (Later
features could introduce subcases where a par-
ticular vowel is likely to follow another partic-
ular vowel, but this result suggests that the
most dominant case is that this does not hap-
pen.) Interestingly, the letter ?h? also appears
at the negative end of the right singular vec-
tor, suggesting that ?h? for the most part does
not follow a vowel in English. Items near zero
(?k?, ?z? etc.) are not strongly represented in
this singular vector pair; it tells us little about
them.
5 Conclusion
An incremental approach to approximating
the singular value decomposition of a cor-
relation matrix has been presented. Use
103
Figure 1: Third Singular Vector Pair on Letter Bigram Task
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
i a
o
e
u
_ q x j z n y f k p g b s d wv l c m
r
t h
n
r t
s ml c d f v wp g b u k x z j q
_ y
a i o
h
e
of the incremental approach means that
singular value decomposition is an option
in situations where data takes the form of
single serially-presented observations from an
unknown matrix. The method is particularly
appropriate in natural language contexts,
where datasets are often too large to be pro-
cessed by traditional methods, and situations
where the dataset is unbounded, for example
in systems that learn through use. The
approach produces preliminary estimations
of the top vectors, meaning that informa-
tion becomes available early in the training
process. By avoiding matrix multiplication,
data of high dimensionality can be processed.
Results of preliminary experiments have been
discussed here on the task of modelling word
and letter bigrams. Future work will include
an evaluation on much larger corpora.
Acknowledgements: The author would like
to thank Brandyn Webb for his contribution,
and the Graduate School of Language Technol-
ogy and Vinnova for their financial support.
References
J. Bellegarda. 2000. Exploiting latent semantic in-
formation in statistical language modeling. Pro-
ceedings of the IEEE, 88:8.
Michael W. Berry, Susan T. Dumais, and Gavin W.
O?Brien. 1995. Using linear algebra for in-
telligent information retrieval. SIAM Review,
34(4):573?595.
R. W. Berry. 1992. Large-scale sparse singular
value computations. The International Journal
of Supercomputer Applications, 6(1):13?49.
Scott C. Deerwester, Susan T. Dumais, Thomas K.
Landauer, George W. Furnas, and Richard A.
Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society of In-
formation Science, 41(6):391?407.
S. Dumais. 1990. Enhancing performance in latent
semantic indexing. TM-ARH-017527 Technical
Report, Bellcore, 1990.
G. H. Golub and C. Reinsch. 1970. Handbook se-
ries linear algebra. singular value decomposition
and least squares solutions. Numerical Mathe-
matics, 14:403?420.
G. Gorrell and B. Webb. 2005. Generalized heb-
bian algorithm for latent semantic analysis. In
Proceedings of Interspeech 2005.
P. Kanerva, J. Kristoferson, and A. Holst. 2000.
Random indexing of text samples for latent se-
mantic analysis. In Proceedings of 22nd Annual
Conference of the Cognitive Science Society.
E. Oja and J. Karhunen. 1985. On stochastic ap-
proximation of the eigenvectors and eigenvalues
of the expectation of a random matrix. J. Math.
Analysis and Application, 106:69?84.
Terence D. Sanger. 1989. Optimal unsupervised
learning in a single-layer linear feedforward neu-
ral network. Neural Networks, 2:459?473.
Terence D. Sanger. 1993. Two iterative algorithms
for computing the singular value decomposition
from input/output samples. NIPS, 6:144?151.
Juyang Weng, Yilu Zhang, and Wey-Shiuan
Hwang. 2003. Candid covariance-free incremen-
tal principal component analysis. IEEE Trans-
actions on Pattern Analysis and Machine Intel-
ligence, 25:8:1034?1040.
104
Statistical Filtering and Subcategorization Frame Acquisition 
Anna Korhonen and  Genev ieve  Gor re l l  
Computer Laboratory, University of Cambridge 
Pembroke Street, Cambridge CB2 3QG, UK 
alk23@cl, cam. ac. uk, genevieve, gorrel l@netdecis ions,  co. uk 
Diana  McCar thy  
School of Cognitive and Computing Sciences 
University of Sussex, Brighton, BN1 9QH, UK 
dianam@cogs, usx. ac.  uk 
Abst rac t  
Research "into the automatic acquisition of 
subcategorization frames (SCFS) from corpora 
is starting to produce large-scale computa- 
tional lexicons which include valuable fre- 
quency information. However, the accuracy 
of the resulting lexicons shows room for im- 
provement. One significant source of error 
lies in the statistical filtering used by some re- 
searchers to remove noise from automatically 
acquired subcategorization frames. In this pa- 
per, we compare three different approaches to 
filtering out spurious hypotheses. Two hy- 
pothesis tests perform poorly, compared to 
filtering frames on the basis of relative fre- 
quency. We discuss reasons for this and con- 
sider directions for future research. 
1 In t roduct ion  
Subcategorization information is vital for suc- 
cessful parsing, however, manual develop- 
ment of large subcategorized lexicons has 
proved difficult because predicates change be- 
haviour between sublanguages, domains and 
over time. Additionally, manually devel- 
oped sucategorization lexicons do not provide 
the relative frequency of different SCFs for a 
given predicate, ssential in a probabilistic ap- 
proach. 
Over the past years acquiring subcatego- 
rization dictionaries from textual corpora has 
become increasingly popular. The different 
approaches (e.g. Brent, !991, 1993; Ushioda 
et al, 1993; Briscoe and Carroll, 1997; Man- 
ning, 1993; Carroll and Rooth, 1998; Gahl, 
1998; Lapata, 1999; Sarkar and Zeman, 2000) 
vary largely according to the methods used 
and the number of SCFS being extracted. Re- 
gardless of this, there is a ceiling on the perfor- 
mance of these systems at around 80% token 
recall 1 
zWhere token recall is the percentage .ofSCF to- 
kens in a sample of manually analysed text that were 
The approaches to extracting SCF informa- 
tion from corpora have frequently employed 
statistical methods for filtering (e.g. Brent, 
1993; Manning 1993; Briscoe and Carroll, 
1997; Lapata, 1999). This has been done to 
remove the noise that arises when dealing with 
naturally occurring data, and from mistakes 
made by the SCF acquisition system, for ex- 
ample, parser errors. 
Filtering is usually done with a hypothe- 
sis test, and frequently with a variation of 
the binomial filter introduced by Brent (1991, 
1993). Hypothesis testing is performed by for- 
mulating a null hypothesis, (H0), which is as- 
sumed true unless there is evidence to the con- 
trary. If there is evidence to the contrary, 
H0 is rejected and the alternative hypothe- 
sis (H1) is accepted. In SCF acquisition, H0 is 
that there is no association between aparticu- 
lar verb (verbj) and a SCF (SCFi), meanwhile 
H1 is that there is such an association. For 
SCF acquisition, the test is one-tailed since H1 
states the direction of the association, a pos- 
itive correlation between verbj and scfi. We 
compare the expected probability of scfi oc- 
curring with verbj if H0 is true, to the ob- 
served probability of co-occurrence obtained 
from the corpus data. If the observed proba- 
bility is greater than the expected probability 
we reject Ho and accept H1, and if not, we 
retain H0. 
Despite the popularity of this method, it 
has been reported as problematic. Accord- 
ing to one account (Briscoe and Carroll, 1997) 
the majority of errors arise because of the sta- 
tistical filtering process, which is reported to 
be particularly unreliable for low frequency 
SCFs (Brent, 1993; Briscoe and Carroll, 1997; 
Manning, 1993; Manning and Schiitze, 1999). 
Lapata (1999) reported that a threshold on 
the relative frequencies produced slightly bet- 
ter results than those achieved with a Brent- 
correctly acquired by the system. 
199 
style binomial filter when establishing SCFs for 
diathesis alternation detection. Lapata deter- 
mined thresholds for each SCF using the fre- 
quency of the SCF in COMLEX Syntax dictio- 
nary (Grishman et al, 1994). 
Adopting the SCF acquisition system of 
Briscoe and Carroll, we have experimented 
with an alternative hypothesis test, the bi- 
nomial log-likelihood ratio (LLR) test (Dun- 
ning, 1993). Sarkar and Zeman (2000) have 
also used this test when filtering SCFs auto- 
matically acquired for Czech. This test has 
been recommended for use in NLP since it 
does not assume a normal distribution, which 
invalidates many other parametric tests for 
use with natural language phenomena. LLR 
can be used in a form (-2logA) which is 
X 2 distributed. Moreover, this asymptote is 
appropriate at quite low frequencies, which 
makes the hypothesis test particularly useful 
when dealing with natural anguage phenom- 
ena, where low frequency events are common- 
place. 
A problem with using hypothesis testing for 
filtering automatically acquired SCFs is ob- 
taining a good estimation of the expected oc- 
currence of scfi with verbj. This is often 
performed using the unconditional distribu- 
tion, that is the probability distribution over 
all SCFS, regardless of the verb. It is as- 
sumed that verbj must occur with scfi sig- 
nificantly more than is expected given this 
estimate. Our paper addresses the problem 
that the conditional distribution, dependent 
on the verb, and unconditional distribution 
are rarely correlated. Therefore statistical fil- 
ters which assume such correlation for H0 will 
be susceptible to error, 
In this paper, we compare the results of 
the Brent style binomial filter of Briscoe and 
Carroll and the LLR filter to a simple method 
which uses a threshold on the relative frequen- 
cies of the verb and SCF combinations. We 
do this within the framework of the Briscoe 
and Carroll SCF acquisition system, which is 
described in section 2.1. The details of the 
two statistical filters are described in section 
2.2, along with the details of the threshold ap- 
plied to the relative frequencies output from 
the SCF acquisition system. The details of the 
experimental evaluation are supplied in sec- 
tion 3. We discuss our findings in section 3.3 
and conclude with directions for future work 
(section 4). 
2 Method  
2.1 F ramework  for SCF  Acquisit ion 
Briscoe and Carroll's (1997) verbal acquisition 
system distinguishes 163 SCFs and returns rel- 
ative frequencies for each SCF found for a given 
predicate. The SCFs are a superset of classes 
found in the Alvey NL Tools (ANLT) dictio- 
nary, Boguraev et al (1987) and the COML~X 
Syntax dictionary, Grishman et al (1994). 
They incorporate information about control 
of predicative arguments, as well as alterna- 
tions such as extraposition and particle move- 
ment. The system employs a shallow parser to 
obtain the subcategorization information. Po- 
tential SCF entries are filtered before the final 
SCF lexicon is produced. The filter is the only 
component of this system which we experi- 
ment with here. The three filtering methods 
which we compare are described below. 
2.2 Fi l ter ing Methods  
2.2.1 B inomia l  Hypothes is  Test 
Briscoe and Carroll (1997) used a binomial 
hypothesis test (BHT) to filter the acquired 
SCFs. They applied BHT as follows. The sys- 
tem recorded the total number of sets of SCF 
cues (n) found for a given predicate, and the 
number of these sets for a given SCF (ra). The 
system estimated the error probability (pe) 
that a cue for a SCF (scfi) occurred with a 
verb which did not take scfi. pe was esti- 
mated in two stages, as shown in equation 1. 
Firstly, the number of verbs which are mem- 
bers of the target SCF in the ANLT dictionary 
were extracted. This number was converted 
to a probability of class membership by divid- 
ing by the total number of verbs in the dic- 
tionary. The complement of this probability 
provided an estimate for the probability of a 
verb not taking scfi. Secondly, this proba- 
bility was multiplied by an estimate for the 
probability of observing the cue for scfi. This 
was estimated using the number of cues for i 
extracted from the Susanue corpus (Sampson, 
1995), divided by the total number of cues. 
pe = (1  - Iverbsl    i  cZass il I eSlc e l, for il (1) 
The probability of an event with probability p
happening exactly rn times out of n attempts 
is given by the following binomial distribution: 
20O 
n~ P(m,n,p) = m!(n-  m)! pro(1 _p)n-m (2) 
The probability of the event happening m or 
more times is: 
= (3) 
k=rn 
Finally, P(m+, n,p e) is the probabil ity that 
m or more occurrences of cues for scfi will oc- 
cur with a verb which is not a member ofscfi, 
given n occurrences of that verb. A threshold 
on this probability, P(m+,n, pe), was set at 
less than or equal to 0.05. This yielded a 95% 
or better confidence that a high enough pro- 
portion of cues for scfi have been observed for 
the verb to be legitimately assigned scfi. 
Other approaches which use a binomial fil- 
ter differ in respect of the calculation of the 
error probability. Brent (1993) estimated the 
error probabilities for each SCF experimen- 
tally from the behaviour of his SCF extrac- 
tor, which detected simple morpho-syntactic 
cues in the corpus data. Manning (1993) in- 
Creased the number of available cues at the ex- 
pense of the reliability of these cues. To main- 
tain high levels of accuracy, Manning applied 
higher bounds on the error probabilities for 
certain cues. These bounds were determined 
experimentally. A similar approach was taken 
by Briscoe, Carroll and Korhonen (1997) in a 
modification to the Briscoe and Carroll sys- 
tem. The overall performance was increased 
by changing the estimates of pe according to 
the performance of the system for the target 
SCF. In the work described here, we use the 
original BHT proposed by Briscoe and Carroll. 
2.2.2 The  B inomia l  Log L ike l ihood  
Rat io  as a S ta t i s t i ca l  F i l te r  
Dunning (1993) demonstrates the benefits of 
the LLR statistic, compared to Pearson's chi- 
squared, on the task of ranking bigram data. 
The binomial log-likelihood ratio test is 
simple to calculate. For each verb and SCF 
combination four counts are required. These 
are the number of times that: 
1. the target verb occurs with the target SCF 
(kl) 
2. the target verb occurs with any other SCF 
(nl - kl) 
3. any other verb occurs with the target SCF 
(k2) 
4. any other verb occurs with any other SCF 
- k2) 
The statistic -21ogA is calculated as follows:- 
log-likelihood = 
where 
2\[logL(pl, kl, nl ) 
+logL(p2, k2, n2) 
-logL(p, kl, nl) 
-logL(p, k2, n2) \] (4) 
logL(p, n, k) = k x logp + (n - k) x log(1 -p )  
and 
kl k2 kl + k2 
P l=- - ,  P2------ ,  P - -  nl n2 nl -4- n2 
The LLR statistic provides a score that re- 
flects the difference in (i) the number of bits 
it takes to describe the observed data, using 
pl = p(SCFIverb ) and p2 = p(SCFl-~verb ), 
and (ii) the number of bits it takes to de- 
scribe the expected ata using the probability 
p = p(scFlany verb). 
The LLR statistic detects differences be- 
tween pl  and p2. The difference could 
potentially be in either direction, but we are 
interested in LLRS where p l  > p2, i.e. where 
there is a positive association between the SCF 
and the verb. For these cases, we compared 
the value of -2logA to the threshold value 
obtained from Pearson's Chi-Squared table, 
to see if it was significant at the 95% level 2. 
2.2.3 Us ing  a Thresho ld  on the  
Re la t ive  Frequenc ies  as a 
Base l ine  
In order to examine the baseline performance 
of this system without employing any notion 
of the significance of the observations, we 
used a threshold on relative frequencies. This 
was done by extracting the SCFS, and rank- 
ing them in the order of the probability of 
their occurrence with the verb. The probabil- 
ities were estimated using a maximum likeli- 
hood estimate (MLE) from the observed rela- 
tive frequencies. A threshold, determined em- 
pirically, was applied to these probability esti- 
mates to filter out the low probability entries 
for each verb. .... 
2See (Gorrell, 1999) for details of this" method. 
201 
3 Eva luat ion  
3.1 Method  
To evaluate the different approaches, we took 
a sample of 10 million words of the BNC cor- 
pus (Leech, 1992). We extracted all sentences 
containing an occurrence of one of fourteen 
verbs 3. The verbs were chosen at random, 
subject to the constraint that they exhibited 
multiple complementation patterns. After the 
extraction process, we retained 3000 citations, 
on average, for each verb. The sentences con- 
taining these verbs were processed by the SCF 
acquisition system, and then we applied the 
three filtering methods described above. We 
also obtained results for a baseline without 
any filtering. 
The results were evaluated against a man- 
ual analysis of corpus data 4. This was ob- 
tained by analysing up to a maximum of 300 
occurrences for each of the 14 test verbs in 
LOB (Garside et al, 1987), Susanne and SEC 
(Taylor and Knowles, 1988) corpora. Follow- 
ing Briscoe and Carroll (1997), we calculated 
precision (percentage of SCFS acquired which 
were also exemplified in the manual analysis) 
and recall (percentage of the SCFs exemplified 
in the manual analysis which were acquired 
automatically). We also combined precision 
and recall into a single measure of overall per- 
formance using the F measure (MA.nniug and 
Schiitze, 1999). 
F = 2.precis ion.  recall (5) 
precision + recall 
3.2 Resu l ts  
Table 1 gives the raw results for the 14 verbs 
using each method. It shows the number of 
true positives (TP), .false positives (FP), and 
.false negatives (FN), as determined accord- 
ing to the manual analysis. The results for 
high frequency SCFs (above 0.01 relative fre- 
quency), medium frequency (between 0.001 
and 0.01) and low frequency (below 0.001) 
SCFs are listed respectively in the second, 
3These verbs were ask, begin, believe, cause, expect, 
find, give, help, like, move, produce, provide, seem, 
swing. 
4The importance of the manual analysis is outlined 
in Briscoe and Carroll (1997). We use the same man- 
ual analysis as Briscoe and Carroll, Le. one from the 
Susanne, LOB, and SEC corpora. A manual analysis of 
the BNC data might produce better results. However, 
since the BNC is a heterogeneous corpus we felt it was 
reasonable to test the data on a different corpus, which 
is also heterogeneous. 
third and fourth columns, and the final col- 
umn includes the total results for all frequency 
ranges. 
Table 2 shows precision and recall for the 14 
verbs and the F measure, which combines pre- 
cision and recall. We also provide the baseline 
results, if all SCFs were accepted. 
From the results given in tables 1 and 2, the 
MLE approach outperformed both hypothesis 
tests. For both BHT and LLR there was an 
increase in FNs at high frequencies, and an 
increase in FPs at medium and low frequen- 
cies, when compared to MLE. The number of 
errors was typically larger for LLR than BHT. 
The hypothesis tests reduced the number of 
FNS at medium and low frequencies, however, 
this was countered by the substantial increase 
in FPs that they gave. While BHT nearly al- 
ways acquired the three most frequent SCFs of 
verbs correctly, LLR tended to reject these. 
While the high number of FNS can be ex- 
plained by reports which have shown LLR to 
be over-conservative (Ribas, 1995; Pedersen, 
1996), the high number of FPs is surprising. 
Although theoretically, the strength of LLR 
lies in its suitability for low frequency data, 
the results displayed in table 1 do not suggest 
that the method performs better than BHT on 
low frequency frames. 
MLE thresholding produced better results 
than the two statistical tests used. Preci- 
sion improved considerably, showing that the 
classes occurring in the data with the high- 
est frequency are often correct. Although MLE 
thresholding clearly makes no attempt to solve 
the sparse data problem, it performs better 
than BHT or LLR overall. MLE is not adept at 
finding low frequency SCFS, however, the other 
methods are problematic in that they wrongly 
accept more than they correctly reject. The 
baseline, of accepting all SCFS, obtained a high 
recall at the expense of precision. 
3.3 D iscuss ion  
Our results indicate that MLE outperforms 
both hypothesis tests. There are two explana- 
tions for this, and these are jointly responsible 
for the results. 
Firstly, the SCF distribution is zipfian, as 
are many distributions concerned with nat- 
ural language (Manning and Schiitze, 1999). 
Figure 1 shows the conditional distribution 
for the verb find. This ~mf~ltered SCF prob- 
ability distribution was obtained from 20 M 
words of BNC data output from the SCF sys- 
202 
High Freq 
TP FP  FN 
BHT 75 29 23 
LLR 66 30 32 
MLE 92 31 6 
Med ium Freq Low Freq 
TP FP  FN TP FP  I FN 
11 37 31 4 23 15 
9 52 33 2 23 17 
0 0 42 0 0 19 
Totals 
TP FP I FN 
m 
90 89 69 
77 105 82 
92 31 67 
Table 1: Raw results for 14 test verbs 
~r31ff.t: Precision % Recall % F measure 
BHT 50.3 56.6 53.3 
LLR 42.3 48.4 45.1 
MLE 74.8 57.8 65.2 
baseline 24.3 83.5 37.6 
Table 2: Precision, Recall, and F measure 
0.1  
0.01 
& 
0.001 
0.0001 
. . . . . . . . .  i . . . . . . . .  
!. 
o 
I , r i i , , i , l  , , i i i i , 
10 100 
rank 
0.1  
0.01 
o.oo~ 
0.01~ 
10  4 
\ 
, , , , t , , r  , i , , i , , ,1  
10 100 
rank 
Figure 1: Hypothesised SCF distribution for 
find 
tern. The unconditional distribution obtained 
from the observed istribution of SCFs in the 
20 M words of BNC is shown in figure 2. The 
figures show SCF rank on the X-axis versus 
SCF frequency on the Y-axis, using logarith- 
mic scales. The line indicates the closest Zipf- 
like power law fit to the data. 
Secondly, the hypothesis tests make the 
false assumption (H0) that the unconditional 
and conditional distributions are correlated. 
The fact that a significant improvement in
performance is made by correcting the prior 
probabilities according to the performance of
the system (Briscoe, Carroll and Korhonen, 
Figure 2: Hypothesised unconditional SCF dis- 
tribution 
1997) suggests the discrepancy between the 
unconditional and the conditional distribu- 
tions. 
We examined the correlation between the 
manual analysis for the 14 verbs, and the 
unconditional distribution of verb types over 
all SCFs estimated from the ANLT using the 
Spearman Rank Correlation Coefficient. The 
results included in table 3 show that only a 
moderate correlation was found averaged over 
all verb types. 
Both LLR and BHT work by comparing the 
observed value of p(scfi\[verbj) to that ex- 
pected by chance. They both use the observed 
203 
\[ Verb Rank  Correlation 
ask 0.10 
begin 0.83 
believe 0.77 
cause 0.19 
expect 
find 
0.45 
0.33 
give 0.06 
help 0.43 
like 0.56 
move 0.53 
produce 0.95 
provide 0.65 
seem 0.16 
swing 
Average 
0.50 
0.47 
Table 3: Rank correlation between the condi- 
tional SCF distributions of the test verbs and 
the unconditional distribution 
value for p(sc.filverbj) from the system's out- 
put, and they both use an estimate for the un- 
conditional probability distribution (p(scfi)) 
for estimating the expected probability. They 
differ in the way that the estimate for the un- 
conditional probability is obtained, and the 
way that it is used in hypothesis testing. 
For  BHT, the null hypothesis i that the ob- 
served value ofp(scfiIverbj) arose by chance, 
because of noise in the data. We estimate the 
probability that the value observed could have 
arisen by chance using p(m+,  n,pe), pe is cal- 
culated using: 
? the SCF acquisition system's raw (until- 
tered) estimate for the unconditional dis- 
tribution, which is obtained from the Su- 
sanne corpus and 
? the ANLT estimate of the unconditional 
distribution of a verb not taking scf~, 
across all SCFs 
For LLR, both the conditional (pl) and un- 
conditional distributions (p2) are estimated 
from the BNC data. The unconditional proba- 
bility distribution uses the occurrence of scfi 
with any verb other than our target. 
The binomial tests look at one point in the 
SCF distribution at a time, for a given verb. 
The expected value is determined using the 
unconditional distribution, on the assumption 
that if the null hypothesis true then this dis- 
tribution will correlate with the conditional 
distribution. However, this is rarely the case. 
Moreover, because of the zipfian nature of 
the distributions, the frequency differences at 
any point can be substantial. In these exper- 
iments, we used one-tailed tests because we 
were looking for cases where there was a pos- 
itive association between the SCF and verb, 
however, in a two-tailed test the null hypoth- 
esis would rarely be accepted, because of the 
substantial differences in the conditional and 
unconditional distributions. 
A large number of false negatives occurred 
for high frequency SCFs because the probabil- 
ity we compared them to was too high. This 
probability was estimated from the combina- 
tion of many verbs genuinely occurring with 
the frame in question, rather than from an es- 
timate of background noise from verbs which 
did not occur with the frame. We did not use 
an estimate from verbs which do not take the 
SCF, since this would require a priori knowl- 
edge about the phenomena that we were en- 
deavouring to acquire automatically. For LLR 
the unconditional probability estimate (p2) 
was high, simply because this SCF was a com- 
mon one, rather than because the data was 
particularly noisy. For BHT, R e was likewise 
too high as the SCF was also common in the 
Susanne data. The ANLT estimate went some- 
way to compensating for this, thus we ob- 
tained fewer false negatives with BHT than 
LLR. 
A large number of false positives occurred 
for low frequency SCFs because the estimate 
for p(scf) was low. This estimate was more 
readily exceeded by the conditional estimate. 
For BHT false positives arose because of the 
low estimate of p(scf) (from Susanne) and 
because the estimate of p(-,SCF) from ANLT 
did not compensate enough for this. For LLR, 
there was no mean~ to compensate for the fact 
that p2 was lower than pl .  
In contrast, MLE did not compare two dis- 
tributions. Simply rejecting the low frequency 
data produced better results overall by avoid- 
ing the false positives with the low frequency 
data, and the false negatives with the high 
frequency data. 
4 Conc lus ion  
This paper explored three possibilities for fil- 
tering out the SCF entries produced by a SCF 
acquisition system. These were (i) a version 
of Brent's binomial filter, commonly used for 
this purpose, (ii) the binomial og-likelihood 
204 
ratio test, recommended for use with low fre- 
quency data and (iii) a simple method using 
a threshold on the MLEs of  the SCFS output 
from the system. Surprisingly, the simple MLE 
thresholding method worked best. The BHT 
and LLR both produced an astounding mlm- 
ber of FPs, particularly at low frequencies. 
Further work on handling low frequency 
data in SCF acquisition is warranted. A non- 
parametric statistical test, such as Fisher's ex- 
act test, recommended by Pedersen (1996), 
might improve on the results obtained using 
parametric tests. However, it seems from our 
experiments hat it would be better to avoid 
hypothesis tests that make use of the uncon- 
ditional distribution. 
One possibility is to put more effort into the 
estimation of pe, and to avoid use of the un- 
conditional distribution for this. In some re- 
cent experiments, we tried optimising the es- 
timates for pe depending on the performance 
of the system for the target SCF, using the 
method proposed by Briscoe, Carroll and Ko- 
rhonen (1997). The estimates of pe were ob- 
tained from a training set separate to the held- 
out BNC data used for testing. Results using 
the new estimates for pe gave an improvement 
of 10% precision and 6% recall, compared to 
the BHT results reported here. Nevertheless, 
the precision result was 14% worse for preci- 
sion than MLE, though there was a 4% im- 
provement in recall, making the overall per- 
formance 3.9 worse than MLE according to the 
F measure. Lapata (1999) also reported that 
a simple relative frequency cut off produced 
slightly better esults than a Brent style BHT. 
If MLE thresholding persistently achieves 
better results, it would be worth investi- 
gating ways of handling the low frequency 
data, such as smoothing, for integration with 
this method. However, more sophisticated 
smoothing methods, which back-off to an un- 
Conditional distribution, will also suffer from 
the lack of correlation between conditional 
and unconditional SCF distributions. Any sta- 
tistical test would work better at low frequen- 
cies than the MLE, since this simply disregards 
all low frequency SCFs. In our experiments, ff 
we had used MLE only for the high frequency 
data, and BHT for medium and low, then over- 
all we would have had 54% precision and 67% 
recall. It certainly seems worth employing hy- 
pothesis tests which do not rely on the un- 
conditional distribution for the low frequency 
SCFS. 
5 Acknowledgements  
We thank Ted Briscoe for many helpful dis- 
cussions and suggestions concerning this work. 
We also acknowledge Yuval Krymolowski for 
useful comments on this paper. 
Re ferences  
Boguraev, B., Briscoe, E., Carroll, J., Carter, 
D. and Grover, C. 1987. The derivation of a 
grammatically-indexed lexicon from the Long- 
man Dictionary of Contemporary English. In 
Proceedings of the 25th Annual Meeting of 
the Association for Computational Linguis- 
tics, Stanford, CA. 193-200. 
Brent, M. 1991. Automatic acquisition of 
subcategorization frames from untagged text. 
In Proceedings of the 29th Annual Meeting 
of the Association for Computational Linguis- 
tics, Berkeley, CA. 209-214. 
Brent, M. 1993. From gra.mmar to lexicon: 
unsupervised learning of lexical syntax. Com- 
putational Linguistics 19.3: 243-262. 
Briscoe, E.J. and J. Carroll 1997. Automatic 
extraction of subcategorization from corpora. 
In Proceedings of the 5th ACL Conf. on Ap- 
plied Nat. Lg. Proc., Washington, DC. 356- 
363. 
Briscoe, E., Carroll, J. and Korhonen, A. 
1997. Automatic extraction of subcategoriza- 
tion frames from corpora - a framework and 
3 experiments. '97 Sparkle WP5 Deliverable, 
available in http://www.ilc.pi.cnr.it/. 
Carroll, G. and Rooth, M. 1998. Valence 
induction with a head-lexicalized PCFG. In 
Proceedings of the 3rd Conference on Empir- 
ical Methods in Natural Language Processing, 
Granada, Spain. 
Dunning, T. 1993. Accurate methods for the 
Statistics of Surprise and Coincidence. Com- 
putational Linguistics 19.1: 61-74. 
Gahl, S. 1998. Automatic extraction of sub- 
corpora based on subcategorization frames 
from a part-of-speech tagged corpus. In Pro- 
ceedings of the COLING-A CL'98, Montreal, 
Canada. 
Garside, R., Leech, G. and Sampson, G. 1987. 
The computational nalysis of English: A 
corpus-based approach. Longman, London. 
Gorrell, G. 1999. Acquiring Subcategorisation 
from Textual Corpora. MPhil dissertation, 
University of Cambridge, UK. 
205 
Grishman, R., Macleod, C. and Meyers, A. 
1994. Comlex syntax: building a computa- 
tional lexicon. In Proceedings of the Interna- 
tional Conference on Computational Linguis- 
tics, COLING-94, Kyoto, Japan. 268-272. 
Lapata, M. 1999. Acquiring lexical gener- 
alizations from corpora: A case study for 
diathesis alternations. In Proceedings of the 
37th Annual Meeting of the Association for 
Computational Linguistics, Maryland. 397- 
404. 
Leech, G. 1992. 100 million words of English: 
the British National Corpus. Language Re- 
search 28(1): 1-13. 
Manning, C. 1993. Automatic acquisition of 
a large subcategorization dictionary from cor- 
pora. In Proceedings of the 31st Annual Meet- 
ing of the Association .for Computational Lin- 
guistics, Columbus, Ohio. 235-242. 
Manning, C. and Schiitze, H. 1999. Founda- 
tions of Statistical Natural Language Process- 
ing. MIT Press, Cambridge MA. 
Pedersen, T. 1996. Fishing for Exactness. In 
Proceedings of the South-Central SAS Users 
Group Conference SCSUG-96, Austin, Texas. 
Ribas, F. 1995. On Acquiring Appropriate Se- 
lectional Restrictions from Corpora Using a 
Semantic Taxonomy. Ph.D thesis, University 
of Catalonia. 
Sampson, G. 1995. English for the computer. 
Oxford University Press, Oxford UK. 
Sarkar, A. and Zeman, D. 2000. Auto- 
matic Extraction of Subcategorization Frames 
for Czech. In Proceedings of the Inter- 
national Conference on Computational Lin- 
guistics, COLING-O0, Saarbrucken, Germany. 
691-697. 
Taylor, L. and Knowles, G. 1988. Manual 
of information to accompany the SEC cor- 
pus: the machine-readable corpus of spoken 
English. University of Lancaster, UK, Ms. 
Ushioda, A., Evans, D., Gibson, T. and 
Waibel, A. 1993. The automatic acquisition of 
frequencies of verb subcategorization frames 
from tagged corpora. In Boguraev, B. and 
Pustejovsky, J. eds. SIGLEX A CL Workshop 
on the Acquisition of Lexieal Knowledge .from 
Text. Columbus, Ohio: 95-106. 
206 
i 
 
	ffPlug and Play Speech Understanding
Manny Rayner, Ian Lewin
& Genevieve Gorrell
netdecisions Ltd
Wellington House,
East Road, Cambridge CB1 1BH, UK
manny.raynerjian.lewinjgenevieve.gorrell
@netdecisions.com
Johan Boye
Telia Research
S-123 86 Farsta, Sweden
johan.boye@trab.se
Abstract
Plug and Play is an increasingly im-
portant concept in system and network
architectures. We introduce and de-
scribe a spoken language dialogue sys-
tem architecture which supports Plug
and Playable networks of objects in its
domain. Each device in the network car-
ries the linguistic and dialogue manage-
ment information which is pertinent to it
and uploads it dynamically to the rele-
vant language processing components in
the spoken language interface. We de-
scribe the current state of our plug and
play demonstrator and discuss theoreti-
cal issues that arise from our work. Plug
and Play forms a central topic for the
DHomme project.
1 Introduction
The notion of Plug and Play nds its most natu-
ral home in the world of networked home devices,
where it oers at least the following two important
properties
 the network of devices is dynamically recon-
gurable as devices are brought online or dis-
appear oine
 zero re-conguration by the user is required
Frameworks for achieving Plug and Play gener-
ally address this by including at least the following
 devices announce themselves on the network
when they are plugged into it (and also dis-
cover the existence of others)
 devices describe their own capabilities, pro-
vide a means for accessing them and can
query and access the capabilities of others
 devices should support, where possible, seam-
less interaction with other devices.
Plug and Play is, not surprisingly, viewed as
a pre-requisite for the commercial success of net-
worked devices in the home. There are already
several promising candidate platforms for achiev-
ing the necessary functionality, including Univer-
sal Plug and Play (UPnP) (Microsoft, 2000) and
Jini (Oaks and Wong, 2000). In this paper, we
address the requirements on spoken dialogue in-
terfaces that arise from a plug and play domain.
We also present the current state of our English
language plug and play demonstrator for control-
ling lamps, dimmers and sensors, previously de-
scribed in (Rayner et al, 2001b). (There is also a
Swedish instantiation).
First, however, we need briey to distinguish
our notion from other notions of plug and play
and recongurability.
The notion of Plug and Play has been used
for dialogue system toolkits in which the various
dierent language processing components them-
selves (e.g. recognition, parsing, generation and
dialogue management) can be plugged in and
out. The most prominent instance of this is
the Darpa Communicator architecture (Goldschen
and Loehr, 1999), which denes interoperability
standards for language processing components.
The intention is simply that researchers and de-
velopers can experiment with systems containing
dierent instantiations of the language process-
ing components. The Communicator Architec-
ture is not designed to address the special require-
ments of a plug and play domain. In fact, the
Communicator architecture does not support the
dynamic re-conguration of language processing
components while the system is running.
At a more general level, simple re-conguration
of spoken language dialogue systems has of course
long been a goal of language engineering. But
such re-conguration is nearly always viewed as
the problem of cross-domain or possibly cross-
language porting, e.g. (Glass, 1999). Once one
has a cinema ticket booking service, for example,
one may examine the eort required for book-
ing train tickets, or for e-shopping in general or
even the \database access" scenario. There are
various toolkits, architectures and methodologies
for rapidly and/or semi-expertly generating new
instances of dialogue systems, e.g. by abstract-
ing away from domain or application dependent
features of particular systems, e.g. (Fraser and
Thornton, 1995; Kolzer, 1999), or `bottom-up' by
aggregation of useful re-congurable components
, e.g. (Sutton et al 1998; Larsson and Traum,
2000). The automated within-domain recongu-
ration required for a plug and play domain, has
not, to our knowledge, been described previously.
Pursuit of plug and play functionality (and its
realization in strong and weak forms - discussed in
section 3) forms a central theme of the DHomme
project.
1
In the rest of this paper, we begin by detail-
ing our concrete Plug and Play scenario - device
control in the home - with an example dialogue
from our demonstrator and an outline of the main
dialogue processing elements. In section 3, we dis-
tinguish strong and weak notions of Plug and Play
and their applicability to spoken language inter-
faces. In section 4, we discuss the strong plug
and play capability we have built into the recogni-
tion, parsing and (context independent) semantic
interpretation system components of our demon-
strator. In section 5, we discuss some future work
for Plug and Play dialogue management. Section
6 contains our conclusions.
2 A Plug and Play Scenario
In this section we present example dialogues from
our current demonstrator and briey outline the
main processing elements. The domain is net-
worked home devices, an area where plug and play
is already in a reasonably advanced state of devel-
opment and speech control looks highly attractive.
2.1 Plug and Play Examples
Figure 1 displays an example dialogue from our
current demonstrator.
In U1, the user asks for the TV to be switched
on. The system reports (S1) that it simply does
not understand the user. (If it possessed \TV"
in its recognition vocabulary and knew something
about the concept of TVs it could have reported
I don't know of any TV). When a TV is plugged
into the network (following U2), the system is able
to understand, and act on the user's repeated re-
quest to switch on the TV. The system reports on
1
This work is supported by EU 5th Framework
project IST-2000-26280 { see Acknowledgments
its action (S3). S4 illustrates another type of \er-
ror" message. When another TV is plugged into
the network, the system must now engage in dis-
ambiguation behaviour whereas previously it had
no need to (S7).
S10 illustrates that, in the absence of dimmable
lights, \Dim" is not understood, and, possibly,
not even recognized. When a dimmable light is
plugged in (or, at least, knowledge of dimmable
lights is plugged in), then a more helpful error
message can be given in S12. Finally, when the
grammar is increased to cover new commands, the
system may begin to make mistakes that it did not
make originally (S13).
2.2 The current demonstrator
Our demonstrator expects devices of three main
types: switchable, dimmable and sensors. Switch-
able devices are binary state devices that can be
set or queried. Dimmable devices have a state
varying on a single scalar dimension which can be
set, changed, or queried. Sensors are similar but
can only be queried.
Formally, these commands and queries are en-
coded by a 4-ary slot-value structure and De-
vice Grammars must generate semantic values
containing these slots. (Not all slots are re-
quired for every utterance, of course.) The
four slots are: op (lled by command or query);
level; change and dir (on or o). In or-
der to identify devices, there are 4 other slots:
device (light, tv . . . ), loc (bathroom, kitchen . . . ),
device-spec (all, the . . . ) and pronoun (it, them
. . . ). For example, Is the light in the hall on?
translates to h op=query dir=on device=light
device-spec=the loc=hall i. Dim everything by
ten percent translates to h op=command dir=o
device-spec=everything change=10 i. Switch
the hall and kitchen lights o translates to h
op=command dir=o level=0 device=light h
loc=kitchen loc=hall ii.
Dialogue interpretation contains three stages.
First, conjunctions in the input (which are treated
as just packed representations) are unpacked into
a set of (7-ary) slot-structures. Secondly, a form of
ellipsis resolution, \sticky defaults", takes place in
which missing slots are lled in from their previ-
ous values. A fragmentary semantic value is sim-
ply pasted over the corresponding parts of the last
one. Thus And in the bathroom translates to h
loc=bathroomi but the other required slots (e.g.
device) are supplied from the previous represen-
tation. Finally reference resolution tries to deter-
mine device identiers for pronouns and denitely
described devices. Currently, devices are identi-
ed only by their location and type so a simple
matching procedure can be used.
Following contextual interpretation, either a
program (a command or sequence of such) or an
`error' condition (e.g. resolution failed to identify
a device) will have been generated. The system
must then execute the program and/or generate
feedback. Knowledge of how to execute these pro-
grams, e.g. that it is an `error' to try to switch on
a light that is already on, and possible feedback
messages are simply hardcoded into the Dialogue
Manager. There is a pre-dened set of feedback
message structures associated with each underly-
ing action and their possible results. Some exam-
ple paraphrases of message structures are \The X
is now on", \The X is already on", \The X is now
at Y percent", \There is no X in the Y".
3 Strong and Weak Plug and Play
In its weakest form, Plug and Play refers only to
the ability to add a device to a network with-
out manual conguration. Knowledge distribu-
tion is not included. Standard Plug and Play for
PC peripherals simply automates the matching up
of physical devices with software device-specic
drivers in the PC. Communication links between
them are established by reserving resources such
as shared memory and interrupt request numbers.
The weak sense is very useful. Users need not
congure their hardware via jumper switches or
software drivers by entering `magic' numbers in
conguration les.
In the strong sense, Plug and Play can refer
also to modular, distributed knowledge. Devices
not only set up network communications but pub-
lish information about themselves over it. Other
devices can obtain and use it. In Jini, for exam-
ple, a new printer device can register its printing
service (and java code for invoking methods on it)
on the network. Then, a word-processing applica-
tion can nd it and congure itself to use it. In
UPnP, devices publish XML descriptions of their
interfaces.
The strong-weak contrast is not a sharp or bi-
nary one. The word-processor might know the
industry agreed printer interface and so display
a greyed-out print button if no printer is net-
worked. When a new type of printer is net-
worked, it might supply additional print options
(e.g. \print colour") that the processor knows
nothing about.
One desirable Plug and Play property in both
strong and weak forms is commutativity, i.e. the
system understands the same commands in the
same way no matter which device is connected
rst. It is less obvious whether disconnecting de-
vice X should be the inverse operation of connect-
ing device X. This seems reasonable in a weak plug
and play system, but in the strong case it would
mean that the recognizer would cease to under-
stand the word \TV" as soon as the TV were dis-
connected. This might be confusing for the user.
The strong and weak senses of plug and play
apply to spoken language dialogue interfaces. In
the weakest sense, the dialogue system might be
entirely pre-congured to deal with all possible
devices and device-combinations. The required
knowledge is already present in the network. Plug
and Play then consists of identifying which partic-
ular devices are currently networked and estab-
lishing communication channels with them. In
the stronger sense, the components of the spoken
language dialogue interface acquire the knowledge
pertinent to particular devices from those devices.
So, as in example S1 above, the speech recognizer
may not have the word \TV" in its vocabulary
until a TV is plugged into the network. The di-
alogue manager may not be capable of uttering
\That device is not dimmable" until a dimmable
device is plugged into the network. A strongly
Plug and Play system may therefore be distin-
guishable from a weaker one by its behaviour in
the absence of certain device specic knowledge.
If the relevant knowledge is present, one cannot be
certain whether it was pre-congured or uploaded
\on demand".
Plug and Play also enforces a certain sort of
modularity on the system. Since devices must de-
clare the information required to update the dia-
logue components, a clear interface is provided for
re-conguring the system for new types of device
as well as a clearer picture of the internal structure
of those dialogue components. Indeed, it is really
just a design choice whether device knowledge is in
fact installed only when the device is plugged in.
One may, for example, choose to optimize recog-
nition performance on the set of devices actually
installed by not loading information about other
devices. Alternatively, one might prefer to rec-
ognize the names of devices not installed so that
helpful error messages can be delivered.
Potentially, each component in a spoken lan-
guage interface (recognizer, parser, interpreter, di-
alogue manager etc.) can be updated by informa-
tion from a device in a Plug and Play domain. Dif-
ferent components might support dierent degrees
of strength of the Plug and Play notion. Further-
more, dierent instantiations of these components
may require very dierent sorts of update. To
take a very simple example, if recognition is car-
ried out by a statistically trained language model,
then updating this with information pertinent to a
particular device will evidently be a signicantly
dierent task from updating a recognizer which
uses a grammar-based language model.
Our current demonstrator program instantiates
a Plug and Play capability for recognition, parsing
and context independent semantic analysis and is
built on top of the Nuance toolkit (Nuance Com-
munications, 1999). The next section discusses
the capability in detail. Section 5 discusses and
makes some proposals for Plug and Play Dialogue
Management.
4 Distributed Grammar
4.1 Introduction
In this section, we will describe how we have ad-
dressed the issues that arise when we attempt to
apply the (strong) Plug and Play scenario to the
tasks of speech recognition and language process-
ing. Each device will provide the knowledge that
the speech interface needs in order to recognise the
new types of utterance relevant to the device in
question, and convert these utterances into well-
formed semantic representations.
Let's start by considering what this means in
practice. There are in fact a whole range of pos-
sible scenarios to consider, depending on how the
speech understanding module is congured. If the
module's construction is simple enough, there may
be no signicant problems involved in extending
it to oer Plug and Play functionality. For ex-
ample, the command vocabulary oered by the
speech interface may just consist of a list of xed
phrases. In this case, Plug and Play speech recog-
nition becomes trivial: each device contributes the
phrases it needs, after which they can be com-
bined into a single grammar. An approach of
this kind fails however to scale up to an interface
which supports complex commands, in particular
commands which combine within the same utter-
ance language referring to two or more dierent
devices. For example, a command may address
several devices at once (\turn on the radio and the
living room light"); alternately, several commands
may be combined into a single utterance (\switch
on the cooker and switch o the microwave"). Our
experience with practical spoken device interfaces
suggests that examples like these are by no means
uncommon.
Another architecture relatively easy to com-
bine with Plug and Play is doing recognition
through a general large-vocabulary recogniser,
and language-processing through device-specic
phrase-spotting (Milward, 2000). The recogniser
stays the same irrespective of how many devices
are connected, so there are by denition no prob-
lems at the level of speech recognition, and it is in
principle possible to support complex commands.
The main drawback, however, is that recognition
quality is markedly inferior compared to a system
in which recognition coverage is limited to the do-
main dened by the current set of devices.
Modern speech interfaces supporting complex
commands are typically specied using a rule-
based grammar formalism dened by a platform
like Nuance (Nuance Communications, 1999) or
SpeechWorks (Inc, 2001). The type of grammar
supported is some subset of full CFG, extended to
include semantic annotations. Grammar rules de-
ne the language model that constrains the recog-
nition process, tuning it to the domain in order to
achieve high performance. (They also supply the
semantic rules that dene the output representa-
tion; we will return to this point later). If we want
to implement an ambitious Plug and Play speech
recognition module within this kind of framework,
we have two top-level goals. On the one hand, we
want to achieve high-quality speech recognition.
At the same time, standard software engineering
considerations suggest that we want to minimize
the overlap between the rule-sets contributed by
each device: ideally, the device will only upload
the specic lexical items relevant to it.
It turns out that our software engineering ob-
jectives conict to some extent with our initial
goal of achieving high-quality speech recognition.
Consider a straightforward solution, in which the
grammatical information contributed by each de-
vice consists purely of lexical entries, i.e. entries
of the form
<Nonterminal> --> <Terminal>
In a CFG-based framework, this implies that we
have a central device-independent CFG grammar,
which denes the other rules which link together
the nonterminals that appear on the left-hand-
sides of the lexical rules. The crucial question is
what these lexical non-terminal symbols will be.
Suppose, for concreteness, that we want our set
of devices to include lights with dimmer switches,
which will among other things accept commands
like \dim the light". We might achieve this by
making the device upload lexical rules of the rough
form
TRANSITIVE_VERB --> dim
NOUN --> light
where the LHSs are conventional grammatical cat-
egories. (We will for the moment skip over the
question of how to represent semantics). The lex-
ical rules might combine with general grammar
rules of the form
COMMAND --> TRANSITIVE_VERB NP
NP --> DET NOUN
DET --> the
This kind of solution is easy to understand, but ex-
perience shows that it leads to poor speech recog-
nition. The problem is that the language model
produced by the grammar is underconstrained: it
will in particular allow any transitive verb to com-
bine with any NP. However, a verb like \dim" will
only combine with a restricted range of possible
NPs, and ideally we would like to capture this
fact. What we really want to do is parameterise
the language model. In the present case, we want
to parameterise the TRANSITIVE VERB \dim" with
the information that it only combines with object
NPs that can be used to refer to dimmable de-
vices. We will parameterise the NP and NOUN
non-terminals similarly. The obvious way to do
this within the bounds of CFG is to specialise the
rules approximately as follows:
COMMAND --> TRANS_DIM_VERB DIMMABLE_NP
DIMMABLE_NP --> DET DIMMABLE_NOUN
TRANS_DIM_VERB --> dim
DIMMABLE_NOUN --> light
DET --> the
Unfortunately, however, this defeats the original
object of the exercise, since the \general" rules
now make reference to the device-specic concept
of dimming. What we want instead is a more
generic treatment, like the following:
COMMAND -->
TRANSITIVE_VERB:[sem_obj_type=T]
NP:[sem_type=T]
NP:[sem_type=T] -->
DET NOUN:[sem_type=T]
DET --> the
TRANSITIVE_VERB:[sem_obj_type=dimmable]
--> dim
NOUN:[sem_type=dimmable] --> light
This kind of parameterisation of a CFG is not
in any way new: it is simply unication gram-
mar (Pullum and Gazdar, 1982; Gazdar et al,
1985). Thus our rst main idea is to raise the
level of abstraction, formulating the device gram-
mar at the level of unication grammars, and
compiling these down into the underlying CFG
representation. There are now a number of sys-
tems which can perform this type of compilation
(Moore, 1998; Kiefer and Krieger, 2000); the ba-
sic methods we use in our system are described
in detail elsewhere (Rayner et al, 2001a). Here,
we focus on the aspects that are required for \dis-
tributed" unication grammars needed for Plug
and Play.
4.2 \Unication grammars meet
object-oriented programming".
Our basic idea is to start with a general device-
independent unication grammar, which imple-
ments the core grammar rules. In our prototype,
there are 34 core rules. Typical examples are
the NP conjunction and PP modications rules,
schematically
NP --> NP CONJ NP
NP --> NP PP
which are likely to occur in connection with any
kind of device. These rules are parameterised by
various features. For example, the set of features
associated with the NP category includes gram-
matical number (singular or plural), WH (plus or
minus) and sortal type (multiple options).
Each individual type of device can extend the
core grammar in one of three possible ways:
New lexical entries A device may add lexical
entries for device-specic words and phrases;
e.g., a device will generally contribute at least
one noun used to refer to it.
New grammar rules A device may add device-
specic rules; e.g., a dimmer switch may in-
clude rules for dimming and brightening, like
\another X percent" or \a bit brighter".
New feature values Least obviously, a device
may extend the range of values that a gram-
matical feature can take (see further below).
For usual software engineering reasons, we nd it
convenient to divide the distributed grammar into
modules; the grammatical knowledge associated
with a device may reside in more than one module.
The grammar in our current demonstrator con-
tains 21 modules, including the \core" grammar
described above. Each device typically requires
between two and ve modules. For example, an
on/o light switch loads three modules: the core
grammar, the general grammar for on/o switch-
able devices, and the grammar specically for
on/o switchable lights. The core grammar, as al-
ready explained, consists of linguistically oriented
device-independent grammar rules. The mod-
ule for on/o switchable devices contains gram-
mar rules specic to on/o switchable behaviour,
which in general make use of the framework es-
tablished by the general grammar. For example,
there are rules of the schematic form
QUESTION -->
is
NP:[sem_type=device]
ON_OFF_PHRASE
PARTICLE_VERB:[particle_type=onoff]
--> switch
Finally, the module for on/o switchable lights is
very small, and just consists of a handful of lexi-
cal entries for nouns like \light", dening these as
nouns referring to on/o switchable devices. The
way in which nouns of this kind can combine is
however dened entirely by the on/o switchable
device grammar and core grammar.
The pattern here turns out to be the usual one:
the grammar appropriate to a device is composed
of a chain of modules, each one depending on the
previous link in the chain and in some way special-
ising it. Structurally, this is similar to the organ-
isation of a piece of normal object-oriented soft-
ware, and we have been interested to discover that
many of the standard concepts of object-oriented
programming carry over naturally to distributed
unication grammars. In the remainder of the sec-
tion, we will expand on this analogy.
If we think in terms of Java or a similar main-
stream OO language, a major grammatical con-
stituent like S, NP or PP has many of the prop-
erties of an OO interface. Grammar rules in one
module can make reference to these constituents,
letting rules in other modules implement their
denition. For example, the temperature sen-
sor grammar module contains a small number of
highly specialised rules, e.g.
QUESTION -->
what is the temperature
PP:[pp_type=location]
QUESTION -->
how many degrees is it
PP:[pp_type=location]
The point to note here is that the temperature
sensor grammar module does not dene the loca-
tive PP construction; this is handled elsewhere,
currently in the core grammar module. The up-
shot is that the temperature sensor module is able
to dene its constructions without worrying about
the exact nature of the locative PP construction.
As a result, we were for instance able to upgrade
the PP rules to include conjoined PPs (thus allow-
ing e.g. \what is the temperature in the kitchen
and the living room") without in any way alter-
ing the grammar rules in the temparature sensor
module
2
2
An ambitious treatment of conjunction might ar-
guably also necessitate changes in the dialogue man-
agement component specic to the temperature sen-
sor device. In the implemented system, conjunction
is uniformly treated as distributive, so \what is the
temperature in the kitchen and the living room" is au-
In order for the scheme to work, the \interfaces"
{ the major categories { naturally need to be well-
dened. In practice, this implies restrictions on
the way we handle three things: the set of syntac-
tic features associated with a category, the range
of possible values (the domain) associated with
each feature, and the semantics of the category.
We consider each of these in turn.
Most obviously, we need to standardise the
feature-set for the category. At present, we de-
ne most major categories in the core grammar
module, to the extent of specifying there the full
range of features associated with each category.
It turns out, however, that it is sometimes desir-
able not to x the domain of a feature in the core
grammar, but rather to allow this domain to be
extended as new modules are added. The issues
that arise here are interesting, and we will discuss
them in some detail.
The problems occur primarily in connection
with features mediating sortal constraints. As
we have already seen in examples above, most
constituents will have at least one sortal fea-
ture, encoding the sortal type of the constituent;
there may also be further features encoding the
sortal types of possible complements and ad-
juncts. For example, the V category has a fea-
ture vtype encoding the sortal type of the V it-
self, a feature obj sem np type encoding the sor-
tal type of a possible direct object, and a feature
vp modifiers type encoding the sortal type of a
possible postverbal modier.
Features like these pose two interrelated prob-
lems. First, the plug and play scenario implies
that we cannot know ahead of time the whole do-
main of a sortal feature. It is always possible that
we will connect a device whose associated gram-
mar module requires denition of a new sortal
type, in order to enforce appropriate constraints in
the language model. The second problem is that
it is still often necessary to dene grammar rules
referring to sortal features before the domains of
these features are known: in particular, the core
module will contain many such rules. Even before
knowing the identity of any specic devices, gen-
eral grammar rules may well want to distinguish
between \device" NPs and \location" NPs. For
example, the general \where-question" rule has
the form
QUESTION --> where is NP
Here, we prefer to constrain the NP so as to make
it refer only to devices, since the system currently
tomatically interpreted as equivalent to \what is the
temperature in the kitchen and what is the tempera-
ture in the living room'.
has no way to interpret a where question referring
to a room, e.g. \where is the bathroom".
We have addressed these issues in a natural way
by adapting the OO-oriented idea of inheritance:
specically, we dene a hierarchy of possible fea-
ture values, allowing one feature value to inherit
from another. In the context of the \where is
NP" rule above, we dene the rule in the core
module; in this module, the sortal NP feature
sem np type may only take the two values device
and location, which we specify with the declara-
tion
3
domain(sem_np_type, [location, device])
This allows us to write the constrained \where is"
rule as
QUESTION -->
where is NP:[sem_np_type=device]
Suppose now that we add modules for both on/o
switchable and dimmable devices; we would like
to make these into distinct sortal types, called
switchable device and dimmable device. We
do this by including the following declarations in
the \switchable" module:
domain(sem_np_type,
[location,
device,
switchable_device])
specialises(switchable_device, device)
and correspondingly in the \dimmable" module:
domain(sem_np_type,
[location,
device,
dimmable_device])
specialises(dimmable_device, device)
When all these declarations are combined at
compile-time, the eect is as follows. The do-
main of the sem np type feature is now the
union of the domains specied by each compo-
nent, and is thus the set flocation, device,
switchable device, dimmable deviceg. Since
switchable device and dimmable device are
the precise values specialising device, the com-
piler systematically replaces the original feature
value device with the disjunction
switchable_device \/ dimmable_device
Thus the \where is" rule now becomes
QUESTION -->
where is
NP:[sem_np_type=switchable_device \/
dimmable_device]
3
We have slightly simplied the form of the decla-
ration for expository purposes.
If new modules are added which further specialise
switchable device, then the rule will again be
adjusted by the compiler so as to include appropri-
ate new elements in the disjunction. The impor-
tant point to notice here is that no change is made
to the original rule denition; in line with nor-
mal OO thinking, the feature domain information
is distributed across several independent modules,
and the changes occur invisibly at compile-time
4
.
We have so far said nothing about how we deal
with semantics, and we conclude the section by
sketching our treatment. In fact, it is not clear
to us that the demands of supporting Plug and
Play greatly aect semantics. If they do, the
most important practical consideration is proba-
bly that plug and play becomes easier to realise
if the semantics are kept simple. We have at any
rate adopted a minimal semantic representation
scheme, and the lack of problems we have experi-
enced with regard to semantics may partly be due
to this.
The annotated CFG grammars produced by our
compiler are in normal Nuance Grammar Speci-
cation Language (GSL) notation, which includes
semantics; unication grammar rules encode se-
mantics using the distinguished feature sem, which
translates into the GSL return construction. So
for example the unication grammar rules
DEVICE_NOUN:[sem=light] --> light
DEVICE_NOUN:[sem=heater] --> heater
translates into the GSL rule
DEVICE_NOUN
[ light {return(light)}
heater {return(heater)}]
Unication grammar rules may contain variables,
translating down into GSL variables; so for exam-
ple,
NP:[sem=[D, N]] -->
DET:[sem=D]
NOUN:[sem=N]
translates into the GSL rule
NP (DET:d NOUN:n) {return(($d $n))}
Our basic semantic representation is a form of fea-
ture/value notation, extended to allow handling
4
Readers familiar with OO methodology may
be disturbed by the fact that the rule appears
to have been attached to the daughter nodes
(switchable device dimmable device, etc), rather
than to the mother device node. We would argue that
the rule is still conceptually attached to the device
node, but that the necessity of eventually realising it
in CFG form implies that it must be compiled in this
way, so that it can later be expanded into a separate
CFG rule for each daughter.
of conjunction. We allow four types of semantic
construction:
 Simple values, e.g. light, heater. Typically
associated with lexical entries.
 Feature/value pairs expressed in list no-
tation, e.g. [device, light], [location,
kitchen]. These are associated with nouns,
adjectives and similar constituents.
 Lists of feature/value pairs, e.g. [[device,
light], [location, kitchen]]. These are
associated with major constituents such as
NP, PP, VP and S.
 Conjunctions of lists of feature/value pairs,
e.g. [and, [[device, light]], [[device,
heater]]] These represent conjoined con-
stituents, e.g. conjoined NPs, PPs and Ss.
This scheme makes it straightforward to write the
semantic parts of grammar rules. Most often, the
rule just concatenates the semantic contributions
of its daughters: thus for example the semantic
features of the nominal PP rule are simply
NP:[sem=concat(Np, Pp)] -->
NP:[sem=Np]
PP:[sem=Pp]
The semantic output of a conjunction rule is typ-
ically the conjunction of its daughters excluding
the conjunction itself, e.g.
NP:[sem=[and, Np1, Np2]] -->
NP:[sem=Np1]
and
NP:[sem=Np2]
5 Future Plug & Play work
In the future, we intend to move to a system
in which all dialogue components can be recon-
gured by devices. For example, in a complete
Plug and Play scenario, the possible device ac-
tions themselves should be declared by devices
perhaps following UPNP standards in which de-
vices publish all interface commands in the form
actionname(arg
1
...arg
i
) plus an internal state
model of a simple vector of values. In this section
we start with some very general observations on
Plug and Play dialogue management and the role
of inference. Then we outline a proposal for a rule
based formalism.
At a very general level of course, indirections
between executable actions and linguistic contents
can arise at several levels: the speech act level
(\It's too warm in here"), the content level (\How
warm is it?"), as well through underdetermination
of contents either through pronominal or ellipti-
cal constructions. At the moment, our pronom-
inal and elliptical resolution methods depend on
very simple `matching' algorithms. In general, one
might at least want some sort of consistency check
between the linguistic properties expressed in an
utterance and those of candidate objects referred
to. One might expect that inferential elements in
contextual interpretation should be strongly Plug
and Play - they will depend, for correctness and
e?ciency, on tailoring to the current objects in
the domain. The research project of uploading
relevant axioms and meaning postulates from a
device to a general purpose inference engine that
can be invoked in contextual resolution looks very
exciting.
Evidently, higher pragmatic relations between
what the user \strictly says" and possible device
operations are also very heavily inference based.
At the moment, we simply encode any neces-
sary inferences directly into the device grammars
and this su?ces to deal with certain simple be-
haviours. However, the requirement to encapsu-
late all device behaviour in a Plug and Play man-
ner imposes a signicant requirement. For ex-
ample, the most natural interaction with a ther-
mometer is, for example, \How warm is it?" or
\What is the temperature?" and not \Query the
thermometer". In our demonstrator, the (gram-
mar derived) semantic values simply reect di-
rectly the relevant device operations: h op=query
device=thermometeri. The strategy supports the
simple natural interactions it is designed to. It
even interacts tolerably well with our ellipsis and
reference resolution methods. \What is the tem-
perature in the hall? And in the living room?"
and \What is the temperature in the hall? What
is it in the living room?" can both be correctly
interpreted. Other interactions are less natural.
The default output when resolution cannot iden-
tify a device N is \I don't know which N you
mean". However, asking for the temperature in a
room with several thermometers should probably
not result in \I don't know which temperature you
mean". It follows that prescribing all behaviour in
a Plug and Play fashion is a signicant constraint.
Indeed, a more general point can be made here.
A problem has arisen because the inference from
service required to service provider has become in-
secure in the presence of other service providers.
In the highly networked homes of the future, more
sophisticated inference may be required just be-
cause service level concepts will predominate over
device level concepts.
5.1 A Rule based formalism
In this section we assume that semantic
values consist of 5-ary slot-structure with
slots device class (dimmable light, TV . . . ),
device attributes (kitchen, blue . . . ), pronoun
(as before) and device-specifier (as before)
and operation. An operation is the action
the user wants carried out, e.g. switch on,
set level(X), where X is a real number (for
dimmable lights), set program(Y) etc. (for Hi-
Fis, TVs), and so on. As in the grammar, device
classes are ordered in a hierarchy in a standard
object-oriented way. Thus \dimmable light" is a
subclass of \dimmable device" and inherits from
it.
For strong plug and play, at least the follow-
ing information must be loaded by a device into
the dialogue manager: the device interface (e.g.
that the \switchable light" class has a switch on
method; the feedback to the user; the update to
the system's device model generated by executing
the command. Clearly, behaviour can also depend
on the current state. Reaction to \switch on the
kitchen light" depends on whether the lamp is o,
on, and whether there is a kitchen light. We write
a rule as command(A,B,C,D) where A is a com-
mand, B is a class of devices for which A is appli-
cable, C is a list of device attributes whose values
must be known in order to execute A, D is a list of
items describing how the system should react on
A. Each item has the following components:
precondition(X) | X is an executable proce-
dure that tests the network state and returns
true or false. If true, the item can `re'.
action(Y) | Y is the procedure to execute
feedback(Z,R) | Z is the feedback for the user
and can depend on R, the return value from
the device operation
upd(W,R) | W describes how the system's
model of the network state should be up-
dated. Also W can depend on R.
For example switching on
a light might be encoded as
command(switch; switchable light; [id = ID]; D)
where D is a list of items in the above form of
which one describes behaviour when the light is
already o thus:
[ precondition(light off(ID));
action(switch on light(ID));
feedback(light now on(ID); success);
feedback(could not switch(ID); error);
upd([dev update(ID; status= 1)]; success);
upd([]; failure) ]
light off and switch on light are procedures
provided by the lamp. The feedback to the user
and the update rules depend on the result of the
switch on light procedure.
6 Conclusion
Applying the idea of plug and play to spoken di-
alogue interfaces poses a number of interesting
and important problems. Since the linguistic and
dialogue management information is distributed
throughout the network, a plug and play system
must update its speech interface whenever a new
device is connected. In this paper, we have fo-
cussed in particular on distributed grammars for
plug and play speech recognition which we have
integrated into our demonstrator system. We have
also examined some issues and described a possi-
ble approach to distributed dialogue management
which we plan to undertake in further work.
Acknowledgments
We are very grateful to our partners in the
DHomme project for discussion of the above ideas
- especially on the importance and role of dier-
ing strengths of Plug and Play. The DHomme
project partners include netdecisions Ltd, SRI In-
ternational, Telia Research AB, and the Universi-
ties of Edinburgh, Gothenburg and Seville.
References
N.M. Fraser and J.H.S. Thornton. 1995. Vocalist:
A robust, portable spoken language dialogue
system for telephone applications. In Proc. of
Eurospeech '95, pages 1947{1950, Madrid.
Gerald Gazdar, Ewan Klein, Georey Pullum,
and Ivan Sag. 1985. Generalized Phrase Struc-
ture Grammar. Harvard University Press, Cam-
bridge, MA.
J.R Glass. 1999. Challenges for spoken dialogue
systems. In Proc. IEEE ASRU Workshop, Key-
stone, CO.
A. Goldschen and D Loehr. 1999. The role of the
darpa communicator architecture as a human
computer interface for distributed simulations.
In 1999 SISO Spring Simulation Interoperabil-
ity Workshop, Orlando, Florida, March 1999.
SpeechWorks Int Inc, 2001. SpeechWorks.
http://www.speechworks.com. As at 31/01/01.
B. Kiefer and H. Krieger. 2000. A context-free
approximation of head-driven phrase structure
grammar. In Proceedings of 6th Int. Workshop
on Parsing Technologies, pages 135{146.
A. Kolzer. 1999. Universal dialogue specication
for conversational systems. In Proceedings of
IJCAI'99 Workshop on Knowledge & Reason-
ing In Practical Dialogue Systems, Stockholm.
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the trindi dialogue
move engine toolkit. Nat.Lang. Engineering, 6.
Microsoft, 2000. Universal Plug and Play Device
Architecture. http://www.upnp.org. Version
1.0, 8 June 2000.
D. Milward. 2000. Distributing representation for
robust interpretation of dialogue utterances. In
Proc. of 38th ACL, Hong Kong, pages 133{141.
R. Moore. 1998. Using natural language knowl-
edge sources in speech recognition. In Proceed-
ings of the NATO Advanced Studies Institute.
Nuance Communications, 1999. Nuance Speech
Recognition System Developer's Manual version
6.2. 1380 Willow Road, Menlo Park, CA 94025.
S. Oaks and H. Wong. 2000. Jini in a Nutshell.
O'Reilly.
Georey K. Pullum and Gerald Gazdar. 1982.
natural languages and context-free languages.
Llinguistics and Philosophy, 4:471{504.
M. Rayner, J. Dowding, and B.A. Hockey. 2001a.
A baseline method for compiling typed unica-
tion grammars into context free language mod-
els. Proc. Eurospeech 2001, Aalborg.
M. Rayner, G. Gorrell, B.A. Hockey, J. Dowding,
and J. Boye. 2001b. Do cfg based language
models need agreement constraints? In Pro-
ceedings of 2nd NAACL, Pittsburgh.
S. Sutton et al 1998. Universal speech tools: The
cslu toolkit. In Proc. ICSLP-98, pages 3221{
3224.
Figure 1 - Example Dialogue
Network status: There is no TV. There is a VCR
in the TV room.
U1 Switch on the television
S1 Sorry, try again
U2 Add new component (this must precede
device installation)
Now, a TV is plugged into the living room. Cur-
rently we simulate this in a simple text interface,
where devices and rooms are chosen from prede-
ned lists. The speech interface is then automati-
cally recongured (this currently takes 29 seconds)
U3 Switch on the television
S3 The TV is now on
U4 Switch on the television
S4 The TV is already on
U5a Add new component
Now, another TV is plugged into the kitchen. No
speech interface reconguration required this time
U5 Switch on the television
S5 The living-room TV is already on
U6 Switch on the VCR
S6 The VCR is now on
U7 Switch on the television
S7 Sorry, I don't know which TV you mean
U8 The television in the kitchen
S8 The kitchen TV is now on
U9a Add new component
Now, on-o lights are installed in the hall and
bathroom. Reconguration time is 27 seconds
U9 Switch on the light in the hall
S9 The hall light is now on
U10 Dim the light to fty percent
S10 Sorry, try again.
U11a Add new component
Now, a dimmable light is installed in the kitchen.
Reconguration time is 39 seconds
U11 Dim the kitchen light to 50 percent
S11 The light in the kitchen is now on at fty
percent
U12 Dim the light in the hall
S12 I'm sorry, the hall light can only be on or
o
U13 Turn on the light in the kitchen (misheard
as \increase the light in the kitchen")
S13 The kitchen light is now on at sixty per-
cent
