Re-evaluating the Role of BLEU in Machine Translation Research
Chris Callison-Burch Miles Osborne Philipp Koehn
School on Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
callison-burch@ed.ac.uk
Abstract
We argue that the machine translation
community is overly reliant on the Bleu
machine translation evaluation metric. We
show that an improved Bleu score is nei-
ther necessary nor sufficient for achieving
an actual improvement in translation qual-
ity, and give two significant counterex-
amples to Bleu?s correlation with human
judgments of quality. This offers new po-
tential for research which was previously
deemed unpromising by an inability to im-
prove upon Bleu scores.
1 Introduction
Over the past five years progress in machine trans-
lation, and to a lesser extent progress in natural
language generation tasks such as summarization,
has been driven by optimizing against n-gram-
based evaluation metrics such as Bleu (Papineni
et al, 2002). The statistical machine translation
community relies on the Bleu metric for the pur-
poses of evaluating incremental system changes
and optimizing systems through minimum er-
ror rate training (Och, 2003). Conference pa-
pers routinely claim improvements in translation
quality by reporting improved Bleu scores, while
neglecting to show any actual example transla-
tions. Workshops commonly compare systems us-
ing Bleu scores, often without confirming these
rankings through manual evaluation. All these
uses of Bleu are predicated on the assumption that
it correlates with human judgments of translation
quality, which has been shown to hold in many
cases (Doddington, 2002; Coughlin, 2003).
However, there is a question as to whether min-
imizing the error rate with respect to Bleu does in-
deed guarantee genuine translation improvements.
If Bleu?s correlation with human judgments has
been overestimated, then the field needs to ask it-
self whether it should continue to be driven by
Bleu to the extent that it currently is. In this
paper we give a number of counterexamples for
Bleu?s correlation with human judgments. We
show that under some circumstances an improve-
ment in Bleu is not sufficient to reflect a genuine
improvement in translation quality, and in other
circumstances that it is not necessary to improve
Bleu in order to achieve a noticeable improvement
in translation quality.
We argue that Bleu is insufficient by showing
that Bleu admits a huge amount of variation for
identically scored hypotheses. Typically there are
millions of variations on a hypothesis translation
that receive the same Bleu score. Because not all
these variations are equally grammatically or se-
mantically plausible there are translations which
have the same Bleu score but a worse human eval-
uation. We further illustrate that in practice a
higher Bleu score is not necessarily indicative of
better translation quality by giving two substantial
examples of Bleu vastly underestimating the trans-
lation quality of systems. Finally, we discuss ap-
propriate uses for Bleu and suggest that for some
research projects it may be preferable to use a fo-
cused, manual evaluation instead.
2 BLEU Detailed
The rationale behind the development of Bleu (Pa-
pineni et al, 2002) is that human evaluation of ma-
chine translation can be time consuming and ex-
pensive. An automatic evaluation metric, on the
other hand, can be used for frequent tasks like
monitoring incremental system changes during de-
velopment, which are seemingly infeasible in a
manual evaluation setting.
The way that Bleu and other automatic evalu-
ation metrics work is to compare the output of a
machine translation system against reference hu-
man translations. Machine translation evaluation
metrics differ from other metrics that use a refer-
ence, like the word error rate metric that is used
249
Orejuela appeared calm as he was led to the
American plane which will take him to Mi-
ami, Florida.
Orejuela appeared calm while being escorted
to the plane that would take him to Miami,
Florida.
Orejuela appeared calm as he was being led
to the American plane that was to carry him
to Miami in Florida.
Orejuela seemed quite calm as he was being
led to the American plane that would take
him to Miami in Florida.
Appeared calm when he was taken to
the American plane, which will to Miami,
Florida.
Table 1: A set of four reference translations, and
a hypothesis translation from the 2005 NIST MT
Evaluation
in speech recognition, because translations have a
degree of variation in terms of word choice and in
terms of variant ordering of some phrases.
Bleu attempts to capture allowable variation in
word choice through the use of multiple reference
translations (as proposed in Thompson (1991)).
In order to overcome the problem of variation in
phrase order, Bleu uses modified n-gram precision
instead of WER?s more strict string edit distance.
Bleu?s n-gram precision is modified to elimi-
nate repetitions that occur across sentences. For
example, even though the bigram ?to Miami? is
repeated across all four reference translations in
Table 1, it is counted only once in a hypothesis
translation. Table 2 shows the n-gram sets created
from the reference translations.
Papineni et al (2002) calculate their modified
precision score, pn, for each n-gram length by
summing over the matches for every hypothesis
sentence S in the complete corpus C as:
pn =
?
S?C
?
ngram?S Countmatched(ngram)
?
S?C
?
ngram?S Count(ngram)
Counting punctuation marks as separate tokens,
the hypothesis translation given in Table 1 has 15
unigram matches, 10 bigram matches, 5 trigram
matches (these are shown in bold in Table 2), and
three 4-gram matches (not shown). The hypoth-
esis translation contains a total of 18 unigrams,
17 bigrams, 16 trigrams, and 15 4-grams. If the
complete corpus consisted of this single sentence
1-grams: American, Florida, Miami, Orejuela, ap-
peared, as, being, calm, carry, escorted, he, him, in, led,
plane, quite, seemed, take, that, the, to, to, to, was , was,
which, while, will, would, ,, .
2-grams: American plane, Florida ., Miami ,, Miami
in, Orejuela appeared, Orejuela seemed, appeared calm,
as he, being escorted, being led, calm as, calm while, carry
him, escorted to, he was, him to, in Florida, led to, plane
that, plane which, quite calm, seemed quite, take him, that
was, that would, the American, the plane, to Miami, to
carry, to the, was being, was led, was to, which will, while
being, will take, would take, , Florida
3-grams: American plane that, American plane which,
Miami , Florida, Miami in Florida, Orejuela appeared
calm, Orejuela seemed quite, appeared calm as, appeared
calmwhile, as he was, being escorted to, being led to, calm
as he, calm while being, carry him to, escorted to the, he
was being, he was led, him to Miami, in Florida ., led to
the, plane that was, plane that would, plane which will,
quite calm as, seemed quite calm, take him to, that was to,
that would take, the American plane, the plane that, to
Miami ,, to Miami in, to carry him, to the American, to
the plane, was being led, was led to, was to carry, which
will take, while being escorted, will take him, would take
him, , Florida .
Table 2: The n-grams extracted from the refer-
ence translations, with matches from the hypoth-
esis translation in bold
then the modified precisions would be p1 = .83,
p2 = .59, p3 = .31, and p4 = .2. Each pn is com-
bined and can be weighted by specifying a weight
wn. In practice each pn is generally assigned an
equal weight.
Because Bleu is precision based, and because
recall is difficult to formulate over multiple refer-
ence translations, a brevity penalty is introduced to
compensate for the possibility of proposing high-
precision hypothesis translations which are too
short. The brevity penalty is calculated as:
BP =
{
1 if c > r
e1?r/c if c ? r
where c is the length of the corpus of hypothesis
translations, and r is the effective reference corpus
length.1
Thus, the Bleu score is calculated as
Bleu = BP ? exp(
N
?
n=1
wn logpn)
A Bleu score can range from 0 to 1, where
higher scores indicate closer matches to the ref-
erence translations, and where a score of 1 is as-
signed to a hypothesis translation which exactly
1The effective reference corpus length is calculated as the
sum of the single reference translation from each set which is
closest to the hypothesis translation.
250
matches one of the reference translations. A score
of 1 is also assigned to a hypothesis translation
which has matches for all its n-grams (up to the
maximum n measured by Bleu) in the clipped ref-
erence n-grams, and which has no brevity penalty.
The primary reason that Bleu is viewed as a use-
ful stand-in for manual evaluation is that it has
been shown to correlate with human judgments of
translation quality. Papineni et al (2002) showed
that Bleu correlated with human judgments in
its rankings of five Chinese-to-English machine
translation systems, and in its ability to distinguish
between human and machine translations. Bleu?s
correlation with human judgments has been fur-
ther tested in the annual NIST Machine Transla-
tion Evaluation exercise wherein Bleu?s rankings
of Arabic-to-English and Chinese-to-English sys-
tems is verified by manual evaluation.
In the next section we discuss theoretical rea-
sons why Bleu may not always correlate with hu-
man judgments.
3 Variations Allowed By BLEU
While Bleu attempts to capture allowable variation
in translation, it goes much further than it should.
In order to allow some amount of variant order in
phrases, Bleu places no explicit constraints on the
order that matching n-grams occur in. To allow
variation in word choice in translation Bleu uses
multiple reference translations, but puts very few
constraints on how n-gram matches can be drawn
from the multiple reference translations. Because
Bleu is underconstrained in these ways, it allows a
tremendous amount of variation ? far beyond what
could reasonably be considered acceptable varia-
tion in translation.
In this section we examine various permutations
and substitutions allowed by Bleu. We show that
for an average hypothesis translation there are mil-
lions of possible variants that would each receive
a similar Bleu score. We argue that because the
number of translations that score the same is so
large, it is unlikely that all of them will be judged
to be identical in quality by human annotators.
This means that it is possible to have items which
receive identical Bleu scores but are judged by hu-
mans to be worse. It is also therefore possible to
have a higher Bleu score without any genuine im-
provement in translation quality. In Sections 3.1
and 3.2 we examine ways of synthetically produc-
ing such variant translations.
3.1 Permuting phrases
One way in which variation can be introduced is
by permuting phrases within a hypothesis trans-
lation. A simple way of estimating a lower bound
on the number of ways that phrases in a hypothesis
translation can be reordered is to examine bigram
mismatches. Phrases that are bracketed by these
bigram mismatch sites can be freely permuted be-
cause reordering a hypothesis translation at these
points will not reduce the number of matching n-
grams and thus will not reduce the overall Bleu
score.
Here we denote bigram mismatches for the hy-
pothesis translation given in Table 1 with vertical
bars:
Appeared calm | when | he was | taken |
to the American plane | , | which will |
to Miami , Florida .
We can randomly produce other hypothesis trans-
lations that have the same Bleu score but are rad-
ically different from each other. Because Bleu
only takes order into account through rewarding
matches of higher order n-grams, a hypothesis
sentence may be freely permuted around these
bigram mismatch sites and without reducing the
Bleu score. Thus:
which will | he was | , | when | taken |
Appeared calm | to the American plane
| to Miami , Florida .
receives an identical score to the hypothesis trans-
lation in Table 1.
If b is the number of bigram matches in a hy-
pothesis translation, and k is its length, then there
are
(k ? b)! (1)
possible ways to generate similarly scored items
using only the words in the hypothesis transla-
tion.2 Thus for the example hypothesis transla-
tion there are at least 40,320 different ways of per-
muting the sentence and receiving a similar Bleu
score. The number of permutations varies with
respect to sentence length and number of bigram
mismatches. Therefore as a hypothesis translation
approaches being an identical match to one of the
reference translations, the amount of variance de-
creases significantly. So, as translations improve
2Note that in some cases randomly permuting the sen-
tence in this way may actually result in a greater number of
n-gram matches; however, one would not expect random per-
mutation to increase the human evaluation.
251
 0
 20
 40
 60
 80
 100
 120
 1  1e+10  1e+20  1e+30  1e+40  1e+50  1e+60  1e+70  1e+80
Se
nt
en
ce
 L
en
gt
h
Number of Permutations
Figure 1: Scatterplot of the length of each trans-
lation against its number of possible permutations
due to bigram mismatches for an entry in the 2005
NIST MT Eval
spurious variation goes down. However, at today?s
levels the amount of variation that Bleu admits is
unacceptably high. Figure 1 gives a scatterplot
of each of the hypothesis translations produced by
the second best Bleu system from the 2005 NIST
MT Evaluation. The number of possible permuta-
tions for some translations is greater than 1073.
3.2 Drawing different items from the
reference set
In addition to the factorial number of ways that
similarly scored Bleu items can be generated
by permuting phrases around bigram mismatch
points, additional variation may be synthesized
by drawing different items from the reference n-
grams. For example, since the hypothesis trans-
lation from Table 1 has a length of 18 with 15
unigram matches, 10 bigram matches, 5 trigram
matches, and three 4-gram matches, we can arti-
ficially construct an identically scored hypothesis
by drawing an identical number of matching n-
grams from the reference translations. Therefore
the far less plausible:
was being led to the | calm as he was |
would take | carry him | seemed quite |
when | taken
would receive the same Bleu score as the hypoth-
esis translation from Table 1, even though human
judges would assign it a much lower score.
This problem is made worse by the fact that
Bleu equally weights all items in the reference
sentences (Babych and Hartley, 2004). There-
fore omitting content-bearing lexical items does
not carry a greater penalty than omitting function
words.
The problem is further exacerbated by Bleu not
having any facilities for matching synonyms or
lexical variants. Therefore words in the hypothesis
that did not appear in the references (such as when
and taken in the hypothesis from Table 1) can be
substituted with arbitrary words because they do
not contribute towards the Bleu score. Under Bleu,
we could just as validly use the words black and
helicopters as we could when and taken.
The lack of recall combined with naive token
identity means that there can be overlap between
similar items in the multiple reference transla-
tions. For example we can produce a translation
which contains both the words carry and take even
though they arise from the same source word. The
chance of problems of this sort being introduced
increases as we add more reference translations.
3.3 Implication: BLEU cannot guarantee
correlation with human judgments
Bleu?s inability to distinguish between randomly
generated variations in translation hints that it may
not correlate with human judgments of translation
quality in some cases. As the number of identi-
cally scored variants goes up, the likelihood that
they would all be judged equally plausible goes
down. This is a theoretical point, and while the
variants are artificially constructed, it does high-
light the fact that Bleu is quite a crude measure-
ment of translation quality.
A number of prominent factors contribute to
Bleu?s crudeness:
? Synonyms and paraphrases are only handled
if they are in the set of multiple reference
translations.
? The scores for words are equally weighted
so missing out on content-bearing material
brings no additional penalty.
? The brevity penalty is a stop-gap measure to
compensate for the fairly serious problem of
not being able to calculate recall.
Each of these failures contributes to an increased
amount of inappropriately indistinguishable trans-
lations in the analysis presented above.
Given that Bleu can theoretically assign equal
scoring to translations of obvious different qual-
ity, it is logical that a higher Bleu score may not
252
Fluency
How do you judge the fluency of this translation?
5 = Flawless English
4 = Good English
3 = Non-native English
2 = Disfluent English
1 = Incomprehensible
Adequacy
How much of the meaning expressed in the refer-
ence translation is also expressed in the hypothesis
translation?
5 = All
4 = Most
3 = Much
2 = Little
1 = None
Table 3: The scales for manually assigned ade-
quacy and fluency scores
necessarily be indicative of a genuine improve-
ment in translation quality. This begs the question
as to whether this is only a theoretical concern or
whether Bleu?s inadequacies can come into play
in practice. In the next section we give two signif-
icant examples that show that Bleu can indeed fail
to correlate with human judgments in practice.
4 Failures in Practice: the 2005 NIST
MT Eval, and Systran v. SMT
The NIST Machine Translation Evaluation exer-
cise has run annually for the past five years as
part of DARPA?s TIDES program. The quality of
Chinese-to-English and Arabic-to-English transla-
tion systems is evaluated both by using Bleu score
and by conducting a manual evaluation. As such,
the NIST MT Eval provides an excellent source
of data that allows Bleu?s correlation with hu-
man judgments to be verified. Last year?s eval-
uation exercise (Lee and Przybocki, 2005) was
startling in that Bleu?s rankings of the Arabic-
English translation systems failed to fully corre-
spond to the manual evaluation. In particular, the
entry that was ranked 1st in the human evaluation
was ranked 6th by Bleu. In this section we exam-
ine Bleu?s failure to correctly rank this entry.
The manual evaluation conducted for the NIST
MT Eval is done by English speakers without ref-
erence to the original Arabic or Chinese docu-
ments. Two judges assigned each sentence in
Iran has already stated that Kharazi?s state-
ments to the conference because of the Jor-
danian King Abdullah II in which he stood
accused Iran of interfering in Iraqi affairs.
n-gram matches: 27 unigrams, 20 bigrams,
15 trigrams, and ten 4-grams
human scores: Adequacy:3,2 Fluency:3,2
Iran already announced that Kharrazi will not
attend the conference because of the state-
ments made by the Jordanian Monarch Ab-
dullah II who has accused Iran of interfering
in Iraqi affairs.
n-gram matches: 24 unigrams, 19 bigrams,
15 trigrams, and 12 4-grams
human scores: Adequacy:5,4 Fluency:5,4
Reference: Iran had already announced
Kharazi would boycott the conference after
Jordan?s King Abdullah II accused Iran of
meddling in Iraq?s affairs.
Table 4: Two hypothesis translations with similar
Bleu scores but different human scores, and one of
four reference translations
the hypothesis translations a subjective 1?5 score
along two axes: adequacy and fluency (LDC,
2005). Table 3 gives the interpretations of the
scores. When first evaluating fluency, the judges
are shown only the hypothesis translation. They
are then shown a reference translation and are
asked to judge the adequacy of the hypothesis sen-
tences.
Table 4 gives a comparison between the output
of the system that was ranked 2nd by Bleu3 (top)
and of the entry that was ranked 6th in Bleu but
1st in the human evaluation (bottom). The exam-
ple is interesting because the number of match-
ing n-grams for the two hypothesis translations
is roughly similar but the human scores are quite
different. The first hypothesis is less adequate
because it fails to indicated that Kharazi is boy-
cotting the conference, and because it inserts the
word stood before accused which makes the Ab-
dullah?s actions less clear. The second hypothe-
sis contains all of the information of the reference,
but uses some synonyms and paraphrases which
would not picked up on by Bleu: will not attend
for would boycott and interfering for meddling.
3The output of the system that was ranked 1st by Bleu is
not publicly available.
253
 2
 2.5
 3
 3.5
 4
 0.38  0.4  0.42  0.44  0.46  0.48  0.5  0.52
H
um
an
 S
co
re
Bleu Score
Adequacy
Correlation
Figure 2: Bleu scores plotted against human judg-
ments of adequacy, with R2 = 0.14 when the out-
lier entry is included
Figures 2 and 3 plot the average human score
for each of the seven NIST entries against its
Bleu score. It is notable that one entry received
a much higher human score than would be antici-
pated from its low Bleu score. The offending en-
try was unusual in that it was not fully automatic
machine translation; instead the entry was aided
by monolingual English speakers selecting among
alternative automatic translations of phrases in the
Arabic source sentences and post-editing the result
(Callison-Burch, 2005). The remaining six entries
were all fully automatic machine translation sys-
tems; in fact, they were all phrase-based statistical
machine translation system that had been trained
on the same parallel corpus and most used Bleu-
based minimum error rate training (Och, 2003) to
optimize the weights of their log linear models?
feature functions (Och and Ney, 2002).
This opens the possibility that in order for Bleu
to be valid only sufficiently similar systems should
be compared with one another. For instance, when
measuring correlation using Pearson?s we get a
very low correlation of R2 = 0.14 when the out-
lier in Figure 2 is included, but a strong R2 = 0.87
when it is excluded. Similarly Figure 3 goes from
R2 = 0.002 to a much stronger R2 = 0.742.
Systems which explore different areas of transla-
tion space may produce output which has differ-
ing characteristics, and might end up in different
regions of the human scores / Bleu score graph.
We investigated this by performing a manual
evaluation comparing the output of two statisti-
cal machine translation systems with a rule-based
machine translation, and seeing whether Bleu cor-
 2
 2.5
 3
 3.5
 4
 0.38  0.4  0.42  0.44  0.46  0.48  0.5  0.52
H
um
an
 S
co
re
Bleu Score
Fluency
Correlation
Figure 3: Bleu scores plotted against human judg-
ments of fluency, with R2 = 0.002 when the out-
lier entry is included
rectly ranked the systems. We used Systran for the
rule-based system, and used the French-English
portion of the Europarl corpus (Koehn, 2005) to
train the SMT systems and to evaluate all three
systems. We built the first phrase-based SMT sys-
tem with the complete set of Europarl data (14-
15 million words per language), and optimized its
feature functions using minimum error rate train-
ing in the standard way (Koehn, 2004). We eval-
uated it and the Systran system with Bleu using
a set of 2,000 held out sentence pairs, using the
same normalization and tokenization schemes on
both systems? output. We then built a number of
SMT systems with various portions of the training
corpus, and selected one that was trained with 164
of the data, which had a Bleu score that was close
to, but still higher than that for the rule-based sys-
tem.
We then performed a manual evaluation where
we had three judges assign fluency and adequacy
ratings for the English translations of 300 French
sentences for each of the three systems. These
scores are plotted against the systems? Bleu scores
in Figure 4. The graph shows that the Bleu score
for the rule-based system (Systran) vastly under-
estimates its actual quality. This serves as another
significant counter-example to Bleu?s correlation
with human judgments of translation quality, and
further increases the concern that Bleu may not be
appropriate for comparing systems which employ
different translation strategies.
254
 2
 2.5
 3
 3.5
 4
 4.5
 0.18  0.2  0.22  0.24  0.26  0.28  0.3
H
um
an
 S
co
re
Bleu Score
Adequacy
Fluency
SMT System 1
SMT System 2
Rule-based System
(Systran)
Figure 4: Bleu scores plotted against human
judgments of fluency and adequacy, showing that
Bleu vastly underestimates the quality of a non-
statistical system
5 Related Work
A number of projects in the past have looked into
ways of extending and improving the Bleu met-
ric. Doddington (2002) suggested changing Bleu?s
weighted geometric average of n-gram matches to
an arithmetic average, and calculating the brevity
penalty in a slightly different manner. Hovy and
Ravichandra (2003) suggested increasing Bleu?s
sensitivity to inappropriate phrase movement by
matching part-of-speech tag sequences against ref-
erence translations in addition to Bleu?s n-gram
matches. Babych and Hartley (2004) extend Bleu
by adding frequency weighting to lexical items
through TF/IDF as a way of placing greater em-
phasis on content-bearing words and phrases.
Two alternative automatic translation evaluation
metrics do a much better job at incorporating re-
call than Bleu does. Melamed et al (2003) for-
mulate a metric which measures translation accu-
racy in terms of precision and recall directly rather
than precision and a brevity penalty. Banerjee and
Lavie (2005) introduce the Meteor metric, which
also incorporates recall on the unigram level and
further provides facilities incorporating stemming,
and WordNet synonyms as a more flexible match.
Lin and Hovy (2003) as well as Soricut and Brill
(2004) present ways of extending the notion of n-
gram co-occurrence statistics over multiple refer-
ences, such as those used in Bleu, to other natural
language generation tasks such as summarization.
Both these approaches potentially suffer from the
same weaknesses that Bleu has in machine trans-
lation evaluation.
Coughlin (2003) performs a large-scale inves-
tigation of Bleu?s correlation with human judg-
ments, and finds one example that fails to corre-
late. Her future work section suggests that she
has preliminary evidence that statistical machine
translation systems receive a higher Bleu score
than their non-n-gram-based counterparts.
6 Conclusions
In this paper we have shown theoretical and prac-
tical evidence that Bleu may not correlate with hu-
man judgment to the degree that it is currently be-
lieved to do. We have shown that Bleu?s rather
coarse model of allowable variation in translation
can mean that an improved Bleu score is not suffi-
cient to reflect a genuine improvement in transla-
tion quality. We have further shown that it is not
necessary to receive a higher Bleu score in order
to be judged to have better translation quality by
human subjects, as illustrated in the 2005 NIST
Machine Translation Evaluation and our experi-
ment manually evaluating Systran and SMT trans-
lations.
What conclusions can we draw from this?
Should we give up on using Bleu entirely? We
think that the advantages of Bleu are still very
strong; automatic evaluation metrics are inexpen-
sive, and do allow many tasks to be performed
that would otherwise be impossible. The impor-
tant thing therefore is to recognize which uses of
Bleu are appropriate and which uses are not.
Appropriate uses for Bleu include tracking
broad, incremental changes to a single system,
comparing systems which employ similar trans-
lation strategies (such as comparing phrase-based
statistical machine translation systems with other
phrase-based statistical machine translation sys-
tems), and using Bleu as an objective function to
optimize the values of parameters such as feature
weights in log linear translation models, until a
better metric has been proposed.
Inappropriate uses for Bleu include comparing
systems which employ radically different strate-
gies (especially comparing phrase-based statistical
machine translation systems against systems that
do not employ similar n-gram-based approaches),
trying to detect improvements for aspects of trans-
lation that are not modeled well by Bleu, and
monitoring improvements that occur infrequently
within a test corpus.
These comments do not apply solely to Bleu.
255
Meteor (Banerjee and Lavie, 2005), Precision and
Recall (Melamed et al, 2003), and other such au-
tomatic metrics may also be affected to a greater
or lesser degree because they are all quite rough
measures of translation similarity, and have inex-
act models of allowable variation in translation.
Finally, that the fact that Bleu?s correlation with
human judgments has been drawn into question
may warrant a re-examination of past work which
failed to show improvements in Bleu. For ex-
ample, work which failed to detect improvements
in translation quality with the integration of word
sense disambiguation (Carpuat and Wu, 2005), or
work which attempted to integrate syntactic infor-
mation but which failed to improve Bleu (Char-
niak et al, 2003; Och et al, 2004) may deserve a
second look with a more targeted manual evalua-
tion.
Acknowledgments
The authors are grateful to Amittai Axelrod,
Frank Keller, Beata Kouchnir, Jean Senellart, and
Matthew Stone for their feedback on drafts of this
paper, and to Systran for providing translations of
the Europarl test set.
References
Bogdan Babych and Anthony Hartley. 2004. Extend-
ing the Bleu MT evaluation method with frequency
weightings. In Proceedings of ACL.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, Michigan.
Chris Callison-Burch. 2005. Linear B system descrip-
tion for the 2005 NIST MT evaluation exercise. In
Proceedings of the NIST 2005 Machine Translation
Evaluation Workshop.
Marine Carpuat and Dekai Wu. 2005. Word sense dis-
ambiguation vs. statistical machine translation. In
Proceedings of ACL.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for machine
translation. In Proceedings of MT Summit IX.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of MT Summit IX.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Human Language Technol-
ogy: Notebook Proceedings, pages 128?132, San
Diego.
Eduard Hovy and Deepak Ravichandra. 2003. Holy
and unholy grails. Panel Discussion at MT Summit
IX.
Philipp Koehn. 2004. Pharaoh: A beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of AMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Audrey Lee and Mark Przybocki. 2005. NIST 2005
machine translation evaluation official results. Of-
ficial release of automatic evaluation scores for all
submissions, August.
Chin-Yew Lin and Ed Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurrence
statistics. In Proceedings of HLT-NAACL.
Dan Melamed, Ryan Green, and Jospeh P. Turian.
2003. Precision and recall of machine translation.
In Proceedings of HLT/NAACL.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical machine
translation. In Proceedings of NAACL-04, Boston.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Radu Soricut and Eric Brill. 2004. A unified frame-
work for automatic evaluation using n-gram co-
occurrence statistics. In Proceedings of ACL.
Henry Thompson. 1991. Automatic evaluation of
translation quality: Outline of methodology and re-
port on pilot experiment. In (ISSCO) Proceedings
of the Evaluators Forum, pages 215?223, Geneva,
Switzerland.
256
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 17?24,
New York, June 2006. c?2006 Association for Computational Linguistics
Improved Statistical Machine Translation Using Paraphrases
Chris Callison-Burch Philipp Koehn Miles Osborne
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
callison-burch@ed.ac.uk
Abstract
Parallel corpora are crucial for training
SMT systems. However, for many lan-
guage pairs they are available only in
very limited quantities. For these lan-
guage pairs a huge portion of phrases en-
countered at run-time will be unknown.
We show how techniques from paraphras-
ing can be used to deal with these oth-
erwise unknown source language phrases.
Our results show that augmenting a state-
of-the-art SMT system with paraphrases
leads to significantly improved coverage
and translation quality. For a training
corpus with 10,000 sentence pairs we in-
crease the coverage of unique test set un-
igrams from 48% to 90%, with more than
half of the newly covered items accurately
translated, as opposed to none in current
approaches.
1 Introduction
As with many other statistical natural language pro-
cessing tasks, statistical machine translation (Brown
et al, 1993) produces high quality results when am-
ple training data is available. This is problematic for
so called ?low density? language pairs which do not
have very large parallel corpora. For example, when
words occur infrequently in a parallel corpus param-
eter estimates for word-level alignments can be in-
accurate, which can in turn lead to inaccurate phrase
translations. Limited amounts of training data can
further lead to a problem of low coverage in that
many phrases encountered at run-time are not ob-
served in the training data and therefore their trans-
lations will not be learned.
Here we address the problem of unknown phrases.
Specifically we show that upon encountering an un-
known source phrase, we can substitute a paraphrase
for it and then proceed using the translation of that
paraphrase. We derive these paraphrases from re-
sources that are external to the parallel corpus that
the translation model is trained from, and we are
able to exploit (potentially more abundant) parallel
corpora from other language pairs to do so.
In this paper we:
? Define a method for incorporating paraphrases
of unseen source phrases into the statistical ma-
chine translation process.
? Show that by translating paraphrases we
achieve a marked improvement in coverage and
translation quality, especially in the case of un-
known words which to date have been left un-
translated.
? Argue that while we observe an improvement
in Bleu score, this metric is particularly poorly
suited to measuring the sort of improvements
that we achieve.
? Present an alternative methodology for targeted
manual evaluation that may be useful in other
research projects.
2 The Problem of Coverage in SMT
Statistical machine translation made considerable
advances in translation quality with the introduc-
tion of phrase-based translation (Marcu and Wong,
2002; Koehn et al, 2003; Och and Ney, 2004). By
17
 0 10
 20 30
 40 50
 60 70
 80 90
 100
 10000  100000  1e+06  1e+07
T
e
s
t
 
S
e
t
 
I
t
e
m
s
 
w
i
t
h
 
T
r
a
n
s
l
a
t
i
o
n
s
 
(
%
)
Training Corpus Size (num words)
unigramsbigramstrigrams4-grams
Figure 1: Percent of unique unigrams, bigrams, tri-
grams, and 4-grams from the Europarl Spanish test
sentences for which translations were learned in in-
creasingly large training corpora
increasing the size of the basic unit of translation,
phrase-based machine translation does away with
many of the problems associated with the original
word-based formulation of statistical machine trans-
lation (Brown et al, 1993). For instance, with multi-
word units less re-ordering needs to occur since lo-
cal dependencies are frequently captured. For exam-
ple, common adjective-noun alternations are mem-
orized. However, since this linguistic information
is not explicitly and generatively encoded in the
model, unseen adjective noun pairs may still be han-
dled incorrectly.
Thus, having observed phrases in the past dramat-
ically increases the chances that they will be trans-
lated correctly in the future. However, for any given
test set, a huge amount of training data has to be ob-
served before translations are learned for a reason-
able percentage of the test phrases. Figure 1 shows
the extent of this problem. For a training corpus
containing 10,000 words translations will have been
learned for only 10% of the unigrams (types, not
tokens). For a training corpus containing 100,000
words this increases to 30%. It is not until nearly
10,000,000 words worth of training data have been
analyzed that translation for more than 90% of the
vocabulary items have been learned. This problem
is obviously compounded for higher-order n-grams
(longer phrases), and for morphologically richer lan-
guages.
encargarnos to ensure, take care, ensure that
garantizar guarantee, ensure, guaranteed, as-
sure, provided
velar ensure, ensuring, safeguard, making
sure
procurar ensure that, try to, ensure, endeavour
to
asegurarnos ensure, secure, make certain
usado used
utilizado used, use, spent, utilized
empleado used, spent, employee
uso use, used, usage
utiliza used, uses, used, being used
utilizar to use, use, used
Table 1: Example of automatically generated para-
phrases for the Spanish words encargarnos and us-
ado along with their English translations which were
automatically learned from the Europarl corpus
2.1 Handling unknown words
Currently most statistical machine translation sys-
tems are simply unable to handle unknown words.
There are two strategies that are generally employed
when an unknown source word is encountered. Ei-
ther the source word is simply omitted when pro-
ducing the translation, or alternatively it is passed
through untranslated, which is a reasonable strategy
if the unknown word happens to be a name (assum-
ing that no transliteration need be done). Neither of
these strategies is satisfying.
2.2 Using paraphrases in SMT
When a system is trained using 10,000 sentence
pairs (roughly 200,000 words) there will be a num-
ber of words and phrases in a test sentence which it
has not learned the translation of. For example, the
Spanish sentence
Es positivo llegar a un acuerdo sobre los
procedimientos, pero debemos encargar-
nos de que este sistema no sea susceptible
de ser usado como arma pol??tica.
may translate as
It is good reach an agreement on proce-
dures, but we must encargarnos that this
system is not susceptible to be usado as
political weapon.
18
what is more, the relevant cost dynamic is completely under control
im ?brigen ist die diesbez?gliche kostenentwicklung v?llig  unter kontrolle
we owe it to the taxpayers to keep in checkthe costs
wir sind es den steuerzahlern die kosten zu habenschuldig  unter kontrolle
Figure 2: Using a bilingual parallel corpus to extract paraphrases
The strategy that we employ for dealing with un-
known source language words is to substitute para-
phrases of those words, and then translate the para-
phrases. Table 1 gives examples of paraphrases and
their translations. If we had learned a translation of
garantizar we could translate it instead of encargar-
nos, and similarly for utilizado instead of usado.
3 Acquiring Paraphrases
Paraphrases are alternative ways of expressing the
same information within one language. The auto-
matic generation of paraphrases has been the focus
of a significant amount of research lately. Many
methods for extracting paraphrases (Barzilay and
McKeown, 2001; Pang et al, 2003) make use of
monolingual parallel corpora, such as multiple trans-
lations of classic French novels into English, or the
multiple reference translations used by many auto-
matic evaluation metrics for machine translation.
Bannard and Callison-Burch (2005) use bilin-
gual parallel corpora to generate paraphrases. Para-
phrases are identified by pivoting through phrases in
another language. The foreign language translations
of an English phrase are identified, all occurrences
of those foreign phrases are found, and all English
phrases that they translate back to are treated as po-
tential paraphrases of the original English phrase.
Figure 2 illustrates how a German phrase can be
used as a point of identification for English para-
phrases in this way.
The method defined in Bannard and Callison-
Burch (2005) has several features that make it an
ideal candidate for incorporation into statistical ma-
chine translation system. Firstly, it can easily be ap-
plied to any language for which we have one or more
parallel corpora. Secondly, it defines a paraphrase
probability, p(e2|e1), which can be incorporated into
the probabilistic framework of SMT.
3.1 Paraphrase probabilities
The paraphrase probability p(e2|e1) is defined
in terms of two translation model probabilities:
p(f |e1), the probability that the original English
phrase e1 translates as a particular phrase f in the
other language, and p(e2|f), the probability that the
candidate paraphrase e2 translates as the foreign lan-
guage phrase. Since e1 can translate as multiple for-
eign language phrases, we marginalize f out:
p(e2|e1) =
?
f
p(f |e1)p(e2|f) (1)
The translation model probabilities can be com-
puted using any standard formulation from phrase-
based machine translation. For example, p(e2|f)
can be calculated straightforwardly using maximum
likelihood estimation by counting how often the
phrases e and f were aligned in the parallel corpus:
p(e2|f) ?
count(e2, f)
?
e2 count(e2, f)
(2)
There is nothing that limits us to estimating para-
phrases probabilities from a single parallel corpus.
We can extend the definition of the paraphrase prob-
ability to include multiple corpora, as follows:
p(e2|e1) ?
?
c?C
?
f in c p(f |e1)p(e2|f)
|C|
(3)
where c is a parallel corpus from a set of paral-
lel corpora C. Thus multiple corpora may be used
19
by summing over all paraphrase probabilities calcu-
lated from a single corpus (as in Equation 1) and
normalized by the number of parallel corpora.
4 Experimental Design
We examined the application of paraphrases to deal
with unknown phrases when translating from Span-
ish and French into English. We used the pub-
licly available Europarl multilingual parallel corpus
(Koehn, 2005) to create six training corpora for the
two language pairs, and used the standard Europarl
development and test sets.
4.1 Baseline
For a baseline system we produced a phrase-based
statistical machine translation system based on the
log-linear formulation described in (Och and Ney,
2002)
e? = argmax
e
p(e|f) (4)
= argmax
e
M?
m=1
?mhm(e, f) (5)
The baseline model had a total of eight feature
functions, hm(e, f): a language model probabil-
ity, a phrase translation probability, a reverse phrase
translation probability, lexical translation probabil-
ity, a reverse lexical translation probability, a word
penalty, a phrase penalty, and a distortion cost. To
set the weights, ?m, we performed minimum error
rate training (Och, 2003) on the development set us-
ing Bleu (Papineni et al, 2002) as the objective func-
tion.
The phrase translation probabilities were deter-
mined using maximum likelihood estimation over
phrases induced from word-level alignments pro-
duced by performing Giza++ training on each of the
three training corpora. We used the Pharaoh beam-
search decoder (Koehn, 2004) to produce the trans-
lations after all of the model parameters had been
set.
When the baseline system encountered unknown
words in the test set, its behavior was simply to re-
produce the foreign word in the translated output.
This is the default behavior for many systems, as
noted in Section 2.1.
4.2 Translation with paraphrases
We extracted all source language (Spanish and
French) phrases up to length 10 from the test and
development sets which did not have translations in
phrase tables that were generated for the three train-
ing corpora. For each of these phrases we gener-
ated a list of paraphrases using all of the parallel cor-
pora from Europarl aside from the Spanish-English
and French-English corpora. We used bitexts be-
tween Spanish and Danish, Dutch, Finnish, French,
German, Italian, Portuguese, and Swedish to gener-
ate our Spanish paraphrases, and did similarly for
the French paraphrases. We manage the parallel
corpora with a suffix array -based data structure
(Callison-Burch et al, 2005). We calculated para-
phrase probabilities using the Bannard and Callison-
Burch (2005) method, summarized in Equation 3.
Source language phrases that included names and
numbers were not paraphrased.
For each paraphrase that had translations in the
phrase table, we added additional entries in the
phrase table containing the original phrase and the
paraphrase?s translations. We augmented the base-
line model by incorporating the paraphrase probabil-
ity into an additional feature function which assigns
values as follows:
h(e, f1) =
?
??
??
p(f2|f1) If phrase table entry (e, f1)
is generated from (e, f2)
1 Otherwise
Just as we did in the baseline system, we performed
minimum error rate training to set the weights of the
nine feature functions in our translation model that
exploits paraphrases.
We tested the usefulness of the paraphrase fea-
ture function by performing an additional experi-
ment where the phrase table was expanded but the
paraphrase probability was omitted.
4.3 Evaluation
We evaluated the efficacy of using paraphrases in
three ways: by calculating the Bleu score for the
translated output, by measuring the increase in cov-
erage when including paraphrases, and through a tar-
geted manual evaluation of the phrasal translations
of unseen phrases to determine how many of the
newly covered phrases were accurately translated.
20
ca
u
s
a
s
Alignment Tool
for
citizens
of
treatment
the
in
inequality
and
discrimination
combats
article
The
reasons
the
therein.
listed
l
a
s
p
o
r
c
i
u
d
a
d
a
n
o
s
l
o
s
d
e
d
e
s
i
g
u
a
l
t
r
a
t
o
e
l
yc
o
m
b
a
t
e
a
r
t
?
c
u
l
o
E
l
e
n
e
n
u
m
e
r
a
d
a
s
m
i
s
m
o
.
e
l
d
i
s
c
r
i
m
i
n
a
c
i
?
n
l
a
Figure 3: Test sentences and reference translations
were manually word-aligned. This allowed us to
equate unseen phrases with their corresponding En-
glish phrase. In this case enumeradas with listed.
Although Bleu is currently the standard metric for
MT evaluation, we believe that it may not meaning-
fully measure translation improvements in our setup.
By substituting a paraphrase for an unknown source
phrase there is a strong chance that its translation
may also be a paraphrase of the equivalent target
language phrase. Bleu relies on exact matches of
n-grams in a reference translation. Thus if our trans-
lation is a paraphrase of the reference, Bleu will fail
to score it correctly.
Because Bleu is potentially insensitive to the type
of changes that we were making to the translations,
we additionally performed a focused manual evalu-
ation (Callison-Burch et al, 2006). To do this, had
bilingual speakers create word-level alignments for
the first 150 and 250 sentence in the Spanish-English
and French-English test corpora, as shown in Figure
3. We were able to use these alignments to extract
the translations of the Spanish and French words that
we were applying our paraphrase method to.
Knowing this correspondence between foreign
phrases and their English counterparts allowed us to
directly analyze whether translations that were be-
ing produced from paraphrases remained faithful to
the meaning of the reference translation. When pro-
The article combats discrimination and inequality
in the treatment of citizens for the reasons listed
therein.
The article combats discrimination and the dif-
ferent treatment of citizens for the reasons men-
tioned in the same.
The article fights against uneven and the treatment
of citizens for the reasons enshrined in the same.
The article is countering discrimination and the
unequal treatment of citizens for the reasons that
in the same.
Figure 4: Judges were asked whether the highlighted
phrase retained the same meaning as the highlighted
phrase in the reference translation (top)
ducing our translations using the Pharaoh decoder
we employed its ?trace? facility, which tells which
source sentence span each target phrase was derived
from. This allowed us to identify which elements
in the machine translated output corresponded to the
paraphrased foreign phrase. We asked a monolin-
gual judge whether the phrases in the machine trans-
lated output had the same meaning as of the refer-
ence phrase. This is illustrated in Figure 4.
In addition to judging the accuracy of 100 phrases
for each of the translated sets, we measured how
much our paraphrase method increased the cover-
age of the translation system. Because we focus
on words that the system was previously unable to
translate, the increase in coverage and the transla-
tion quality of the newly covered phrases are the
two most relevant indicators as to the efficacy of the
method.
5 Results
We produced translations under five conditions for
each of our training corpora: a set of baseline
translations without any additional entries in the
phrase table, a condition where we added the trans-
lations of paraphrases for unseen source words along
with paraphrase probabilities, a condition where we
added the translations of paraphrases of multi-word
phrases along with paraphrase probabilities, and two
additional conditions where we added the transla-
tions of paraphrases of single and multi-word para-
phrase without paraphrase probabilities.
21
Spanish-English French-English
Corpus size 10k 20k 40k 80k 160k 320k 10k 20k 40k 80k 160k 320k
Baseline 22.6 25.0 26.5 26.5 28.7 30.0 21.9 24.3 26.3 27.8 28.8 29.5
Single word 23.1 25.2 26.6 28.0 29.0 30.0 22.7 24.2 26.9 27.7 28.9 29.8
Multi-word 23.3 26.0 27.2 28.0 28.8 29.7 23.7 25.1 27.1 28.5 29.1 29.8
Table 2: Bleu scores for the various training corpora, including baseline results without paraphrasing, results
for only paraphrasing unknown words, and results for paraphrasing any unseen phrase. Corpus size is
measured in sentences.
Corpus size 10k 20k 40k 80k 160k 320k 10k 20k 40k 80k 160k 320k
Single w/o-ff 23.0 25.1 26.7 28.0 29.0 29.9 22.5 24.1 26.0 27.6 28.8 29.6
Multi w/o-ff 20.6 22.6 21.9 24.0 25.4 27.5 19.7 22.1 24.3 25.6 26.0 28.1
Table 3: Bleu scores for the various training corpora, when the paraphrase feature function is not included
5.1 Bleu scores
Table 2 gives the Bleu scores for each of these con-
ditions. We were able to measure a translation im-
provement for all sizes of training corpora, under
both the single word and multi-word conditions, ex-
cept for the largest Spanish-English corpus. For the
single word condition, it would have been surprising
if we had seen a decrease in Bleu score. Because we
are translating words that were previously untrans-
latable it would be unlikely that we could do any
worse. In the worst case we would be replacing one
word that did not occur in the reference translation
with another, and thus have no effect on Bleu.
More interesting is the fact that by paraphrasing
unseen multi-word units we get an increase in qual-
ity above and beyond the single word paraphrases.
These multi-word units may not have been observed
in the training data as a unit, but each of the compo-
nent words may have been. In this case translating
a paraphrase would not be guaranteed to received
an improved or identical Bleu score, as in the single
word case. Thus the improved Bleu score is notable.
Table 3 shows that incorporating the paraphrase
probability into the model?s feature functions plays a
critical role. Without it, the multi-word paraphrases
harm translation performance when compared to the
baseline.
5.2 Manual evaluation
We performed a manual evaluation by judging the
accuracy of phrases for 100 paraphrased translations
from each of the sets using the manual word align-
ments.1 Table 4 gives the percentage of time that
each of the translations of paraphrases were judged
to have the same meaning as the equivalent target
phrase. In the case of the translations of single word
paraphrases for the Spanish accuracy ranged from
just below 50% to just below 70%. This number
is impressive in light of the fact that none of those
items are correctly translated in the baseline model,
which simply inserts the foreign language word. As
with the Bleu scores, the translations of multi-word
paraphrases were judged to be more accurate than
the translations of single word paraphrases.
In performing the manual evaluation we were ad-
ditionally able to determine how often Bleu was ca-
pable of measuring an actual improvement in trans-
lation. For those items judged to have the same
meaning as the gold standard phrases we could
track how many would have contributed to a higher
Bleu score (that is, which of them were exactly
the same as the reference translation phrase, or had
some words in common with the reference trans-
lation phrase). By counting how often a correct
phrase would have contributed to an increased Bleu
score, and how often it would fail to increase the
Bleu score we were able to determine with what fre-
quency Bleu was sensitive to our improvements. We
found that Bleu was insensitive to our translation im-
provements between 60-75% of the time, thus re-
1Note that for the larger training corpora fewer than 100
paraphrases occurred in the first 150 and 250 sentence pairs.
22
Spanish-English French-English
Corpus size 10k 20k 40k 80k 160k 320k 10k 20k 40k 80k 160k 320k
Single word 48% 53% 57% 67%? 33%? 50%? 54% 49% 45% 50% 39%? 21%?
Multi-word 64% 65% 66% 71% 76%? 71%? 60% 67% 63% 58% 65% 42%?
Table 4: Percent of time that the translation of a paraphrase was judged to retain the same meaning as the
corresponding phrase in the gold standard. Starred items had fewer than 100 judgments and should not be
taken as reliable estimates.
Size 1-gram 2-gram 3-gram 4-gram
10k 48% 25% 10% 3%
20k 60% 35% 15% 6%
40k 71% 45% 22% 9%
80k 80% 55% 29% 12%
160k 86% 64% 37% 17%
320k 91% 71% 45% 22%
Table 5: The percent of the unique test set phrases
which have translations in each of the Spanish-
English training corpora prior to paraphrasing
inforcing our belief that it is not an appropriate mea-
sure for translation improvements of this sort.
5.3 Increase in coverage
As illustrated in Figure 1, translation models suffer
from sparse data. When only a very small paral-
lel corpus is available for training, translations are
learned for very few of the unique phrases in a test
set. If we exclude 451 words worth of names, num-
bers, and foreign language text in 2,000 sentences
that comprise the Spanish portion of the Europarl
test set, then the number of unique n-grams in text
are: 7,331 unigrams, 28,890 bigrams, 44,194 tri-
grams, and 48,259 4-grams. Table 5 gives the per-
centage of these which have translations in each of
the three training corpora, if we do not use para-
phrasing.
In contrast after expanding the phrase table using
the translations of paraphrases, the coverage of the
unique test set phrases goes up dramatically (shown
in Table 6). For the first training corpus with 10,000
sentence pairs and roughly 200,000 words of text in
each language, the coverage goes up from less than
50% of the vocabulary items being covered to 90%.
The coverage of unique 4-grams jumps from 3% to
16% ? a level reached only after observing more
Size 1-gram 2-gram 3-gram 4-gram
10k 90% 67% 37% 16%
20k 90% 69% 39% 17%
40k 91% 71% 41% 18%
80k 92% 73% 44% 20%
160k 92% 75% 46% 22%
320k 93% 77% 50% 25%
Table 6: The percent of the unique test set phrases
which have translations in each of the Spanish-
English training corpora after paraphrasing
than 100,000 sentence pairs, or roughly three mil-
lion words of text, without using paraphrases.
6 Related Work
Previous research on trying to overcome data spar-
sity issues in statistical machine translation has
largely focused on introducing morphological anal-
ysis as a way of reducing the number of types ob-
served in a training text. For example, Nissen and
Ney (2004) apply morphological analyzers to En-
glish and German and are able to reduce the amount
of training data needed to reach a certain level
of translation quality. Goldwater and McClosky
(2005) find that stemming Czech and using lemmas
improves the word-to-word correspondences when
training Czech-English alignment models. Koehn
and Knight (2003) show how monolingual texts and
parallel corpora can be used to figure out appropriate
places to split German compounds.
Still other approaches focus on ways of acquiring
data. Resnik and Smith (2003) develop a method
for gathering parallel corpora from the web. Oard
et al (2003) describe various methods employed
for quickly gathering resources to create a machine
translation system for a language with no initial re-
sources.
23
7 Discussion
In this paper we have shown that significant gains in
coverage and translation quality can be had by inte-
grating paraphrases into statistical machine transla-
tion. In effect, paraphrases introduce some amount
of generalization into statistical machine translation.
Whereas before we relied on having observed a par-
ticular word or phrase in the training set in order to
produce a translation of it, we are no longer tied to
having seen every word in advance. We can exploit
knowledge that is external to the translation model
about what words have similar meanings and use
that in the process of translation. This method is
particularly pertinent to small data conditions, which
are plagued by sparse data problems.
In future work, we plan to determine how much
data is required to learn useful paraphrases. The sce-
nario described in this paper was very favorable to
creating high quality paraphrases. The large number
of parallel corpora between Spanish and the other
languages present in the Europarl corpus allowed
us to generate high quality, in domain data. While
this is a realistic scenario, in that many new official
languages have been added to the European Union,
some of which do not yet have extensive parallel cor-
pora, we realize that this may be a slightly idealized
scenario.
Finally, we plan to formalize our targeted manual
evaluation method, in the hopes of creating a eval-
uation methodology for machine translation that is
more thorough and elucidating than Bleu.
Acknowledgments
Thank you to Alexandra Birch and Stephanie Van-
damme for creating the word alignments.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL-2005.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In ACL-2001.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of bleu in ma-
chine translation. In Proceedings of EACL.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Proceedings of EMNLP.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of EACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Sonja Nissen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntatic analysis. Computational Linguistics,
30(2):181?204.
Doug Oard, David Doermann, Bonnie Dorr, Daqing He,
Phillip Resnik, William Byrne, Sanjeeve Khudanpur,
David Yarowsky, Anton Leuski, Philipp Koehn, and
Kevin Knight. 2003. Desperately seeking Cebuano.
In Proceedings of HLT-NAACL.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Philip Resnik and Noah Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29(3):349?
380, September.
24
Statistical Machine Translation with
Word- and Sentence-Aligned Parallel Corpora
Chris Callison-Burch David Talbot Miles Osborne
School on Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
callison-burch@ed.ac.uk
Abstract
The parameters of statistical translation models are
typically estimated from sentence-aligned parallel
corpora. We show that significant improvements in
the alignment and translation quality of such mod-
els can be achieved by additionally including word-
aligned data during training. Incorporating word-
level alignments into the parameter estimation of
the IBM models reduces alignment error rate and
increases the Bleu score when compared to training
the same models only on sentence-aligned data. On
the Verbmobil data set, we attain a 38% reduction
in the alignment error rate and a higher Bleu score
with half as many training examples. We discuss
how varying the ratio of word-aligned to sentence-
aligned data affects the expected performance gain.
1 Introduction
Machine translation systems based on probabilistic
translation models (Brown et al, 1993) are gener-
ally trained using sentence-aligned parallel corpora.
For many language pairs these exist in abundant
quantities. However for new domains or uncommon
language pairs extensive parallel corpora are often
hard to come by.
Two factors could increase the performance of
statistical machine translation for new language
pairs and domains: a reduction in the cost of cre-
ating new training data, and the development of
more efficient methods for exploiting existing train-
ing data. Approaches such as harvesting parallel
corpora from the web (Resnik and Smith, 2003)
address the creation of data. We take the second,
complementary approach. We address the prob-
lem of efficiently exploiting existing parallel cor-
pora by adding explicit word-level alignments be-
tween a number of the sentence pairs in the train-
ing corpus. We modify the standard parameter esti-
mation procedure for IBM Models and HMM vari-
ants so that they can exploit these additional word-
level alignments. Our approach uses both word- and
sentence-level alignments for training material.
In this paper we:
1. Describe how the parameter estimation frame-
work of Brown et al (1993) can be adapted to
incorporate word-level alignments;
2. Report significant improvements in alignment
error rate and translation quality when training
on data with word-level alignments;
3. Demonstrate that the inclusion of word-level
alignments is more effective than using a bilin-
gual dictionary;
4. Show the importance of amplifying the contri-
bution of word-aligned data during parameter
estimation.
This paper shows that word-level alignments im-
prove the parameter estimates for translation mod-
els, which in turn results in improved statistical
translation for languages that do not have large
sentence-aligned parallel corpora.
2 Parameter Estimation Using
Sentence-Aligned Corpora
The task of statistical machine translation is to
choose the source sentence, e, that is the most prob-
able translation of a given sentence, f , in a for-
eign language. Rather than choosing e? that di-
rectly maximizes p(e|f), Brown et al (1993) apply
Bayes? rule and select the source sentence:
e? = argmax
e
p(e)p(f |e). (1)
In this equation p(e) is a language model probabil-
ity and is p(f |e) a translation model probability. A
series of increasingly sophisticated translation mod-
els, referred to as the IBM Models, was defined in
Brown et al (1993).
The translation model, p(f |e) defined as a
marginal probability obtained by summing over
word-level alignments, a, between the source and
target sentences:
p(f |e) =
?
a
p(f ,a|e). (2)
While word-level alignments are a crucial com-
ponent of the IBM models, the model parame-
ters are generally estimated from sentence-aligned
parallel corpora without explicit word-level align-
ment information. The reason for this is that
word-aligned parallel corpora do not generally ex-
ist. Consequently, word level alignments are treated
as hidden variables. To estimate the values of
these hidden variables, the expectation maximiza-
tion (EM) framework for maximum likelihood esti-
mation from incomplete data is used (Dempster et
al., 1977).
The previous section describes how the trans-
lation probability of a given sentence pair is ob-
tained by summing over all alignments p(f |e) =
?
a p(f ,a|e). EM seeks to maximize the marginal
log likelihood, log p(f |e), indirectly by iteratively
maximizing a bound on this term known as the ex-
pected complete log likelihood, ?log p(f ,a|e)?q(a),1
log p(f |e) = log
?
a
p(f ,a|e) (3)
= log
?
a
q(a)
p(f ,a|e)
q(a)
(4)
?
?
a
q(a) log
p(f ,a|e)
q(a)
(5)
= ?log p(f ,a|e)?q(a) + H(q(a))
where the bound in (5) is given by Jensen?s inequal-
ity. By choosing q(a) = p(a|f , e) this bound be-
comes an equality.
This maximization consists of two steps:
? E-step: calculate the posterior probability
under the current model of every permissi-
ble alignment for each sentence pair in the
sentence-aligned training corpus;
? M-step: maximize the expected log like-
lihood under this posterior distribution,
?log p(f ,a|e)?q(a), with respect to the model?s
parameters.
While in standard maximum likelihood estima-
tion events are counted directly to estimate param-
eter settings, in EM we effectively collect frac-
tional counts of events (here permissible alignments
weighted by their posterior probability), and use
these to iteratively update the parameters.
1Here ? ??q(?) denotes an expectation with respect to q(?).
Since only some of the permissible alignments
make sense linguistically, we would like EM to use
the posterior alignment probabilities calculated in
the E-step to weight plausible alignments higher
than the large number of bogus alignments which
are included in the expected complete log likeli-
hood. This in turn should encourage the parame-
ter adjustments made in the M-step to converge to
linguistically plausible values.
Since the number of permissible alignments for
a sentence grows exponentially in the length of the
sentences for the later IBM Models, a large num-
ber of informative example sentence pairs are re-
quired to distinguish between plausible and implau-
sible alignments. Given sufficient data the distinc-
tion occurs because words which are mutual trans-
lations appear together more frequently in aligned
sentences in the corpus.
Given the high number of model parameters and
permissible alignments, however, huge amounts of
data will be required to estimate reasonable transla-
tion models from sentence-aligned data alone.
3 Parameter Estimation Using Word- and
Sentence-Aligned Corpora
As an alternative to collecting a huge amount of
sentence-aligned training data, by annotating some
of our sentence pairs with word-level alignments
we can explicitly provide information to highlight
plausible alignments and thereby help parameters
converge upon reasonable settings with less training
data.
Since word-alignments are inherent in the IBM
translation models it is straightforward to incorpo-
rate this information into the parameter estimation
procedure. For sentence pairs with explicit word-
level alignments marked, fractional counts over all
permissible alignments need not be collected. In-
stead, whole counts are collected for the single hand
annotated alignment for each sentence pair which
has been word-aligned. By doing this the expected
complete log likelihood collapses to a single term,
the complete log likelihood (p(f ,a|e)), and the E-
step is circumvented.
The parameter estimation procedure now in-
volves maximizing the likelihood of data aligned
only at the sentence level and also of data aligned
at the word level. The mixed likelihood function,
M, combines the expected information contained
in the sentence-aligned data with the complete in-
formation contained in the word-aligned data.
M =
Ns?
s=1
(1? ?)?log p(fs,as|es)?q(as)
+
Nw?
w=1
? log p(fw,aw|ew) (6)
Here s and w index the Ns sentence-aligned sen-
tences and Nw word-aligned sentences in our cor-
pora respectively. Thus M combines the expected
complete log likelihood and the complete log likeli-
hood. In order to control the relative contributions
of the sentence-aligned and word-aligned data in
the parameter estimation procedure, we introduce a
mixing weight ? that can take values between 0 and
1.
3.1 The impact of word-level alignments
The impact of word-level alignments on parameter
estimation is closely tied to the structure of the IBM
Models. Since translation and word alignment pa-
rameters are shared between all sentences, the pos-
terior alignment probability of a source-target word
pair in the sentence-aligned section of the corpus
that were aligned in the word-aligned section will
tend to be relatively high.
In this way, the alignments from the word-aligned
data effectively percolate through to the sentence-
aligned data indirectly constraining the E-step of
EM.
3.2 Weighting the contribution of
word-aligned data
By incorporating ?, Equation 6 becomes an interpo-
lation of the expected complete log likelihood pro-
vided by the sentence-aligned data and the complete
log likelihood provided by word-aligned data.
The use of a weight to balance the contributions
of unlabeled and labeled data in maximum like-
lihood estimation was proposed by Nigam et al
(2000). ? quantifies our relative confidence in the
expected statistics and observed statistics estimated
from the sentence- and word-aligned data respec-
tively.
Standard maximum likelihood estimation (MLE)
which weighs all training samples equally, corre-
sponds to an implicit value of lambda equal to the
proportion of word-aligned data in the whole of
the training set: ? = NwNw+Ns . However, having
the total amount of sentence-aligned data be much
larger than the amount of word-aligned data implies
a value of ? close to zero. This means that M can be
maximized while essentially ignoring the likelihood
of the word-aligned data. Since we believe that the
explicit word-alignment information will be highly
effective in distinguishing plausible alignments in
the corpus as a whole, we expect to see benefits by
setting ? to amplify the contribution of the word-
aligned data set particularly when this is a relatively
small portion of the corpus.
4 Experimental Design
To perform our experiments with word-level aligne-
ments we modified GIZA++, an existing and freely
available implementation of the IBM models and
HMM variants (Och and Ney, 2003). Our modifi-
cations involved circumventing the E-step for sen-
tences which had word-level alignments and incor-
porating these observed alignment statistics in the
M-step. The observed and expected statistics were
weighted accordingly by ? and (1? ?) respectively
as were their contributions to the mixed log likeli-
hood.
In order to measure the accuracy of the predic-
tions that the statistical translation models make un-
der our various experimental settings, we choose
the alignment error rate (AER) metric, which is de-
fined in Och and Ney (2003). We also investigated
whether improved AER leads to improved transla-
tion quality. We used the alignments created during
our AER experiments as the input to a phrase-based
decoder. We translated a test set of 350 sentences,
and used the Bleu metric (Papineni et al, 2001) to
automatically evaluate machine translation quality.
We used the Verbmobil German-English parallel
corpus as a source of training data because it has
been used extensively in evaluating statistical trans-
lation and alignment accuracy. This data set comes
with a manually word-aligned set of 350 sentences
which we used as our test set.
Our experiments additionally required a very
large set of word-aligned sentence pairs to be in-
corporated in the training set. Since previous work
has shown that when training on the complete set
of 34,000 sentence pairs an alignment error rate as
low as 6% can be achieved for the Verbmobil data,
we automatically generated a set of alignments for
the entire training data set using the unmodified ver-
sion of GIZA++. We wanted to use automatic align-
ments in lieu of actual hand alignments so that we
would be able to perform experiments using large
data sets. We ran a pilot experiment to test whether
our automatic would produce similar results to man-
ual alignments.
We divided our manual word alignments into
training and test sets and compared the performance
of models trained on human aligned data against
models trained on automatically aligned data. A
Size of training corpus
Model .5k 2k 8k 16k
Model 1 29.64 24.66 22.64 21.68
HMM 18.74 15.63 12.39 12.04
Model 3 26.07 18.64 14.39 13.87
Model 4 20.59 16.05 12.63 12.17
Table 1: Alignment error rates for the various IBM
Models trained with sentence-aligned data
100-fold cross validation showed that manual and
automatic alignments produced AER results that
were similar to each other to within 0.1%.2
Having satisfied ourselves that automatic align-
ment were a sufficient stand-in for manual align-
ments, we performed our main experiments which
fell into the following categories:
1. Verifying that the use of word-aligned data has
an impact on the quality of alignments pre-
dicted by the IBM Models, and comparing the
quality increase to that gained by using a bilin-
gual dictionary in the estimation stage.
2. Evaluating whether improved parameter esti-
mates of alignment quality lead to improved
translation quality.
3. Experimenting with how increasing the ratio of
word-aligned to sentence-aligned data affected
the performance.
4. Experimenting with our ? parameter which al-
lows us to weight the relative contributions
of the word-aligned and sentence-aligned data,
and relating it to the ratio experiments.
5. Showing that improvements to AER and trans-
lation quality held for another corpus.
5 Results
5.1 Improved alignment quality
As a staring point for comparison we trained
GIZA++ using four different sized portions of the
Verbmobil corpus. For each of those portions we
output the most probable alignments of the testing
data for Model 1, the HMM, Model 3, and Model
2Note that we stripped out probable alignments from our
manually produced alignments. Probable alignments are large
blocks of words which the annotator was uncertain of how to
align. The many possible word-to-word translations implied by
the manual alignments led to lower results than with the auto-
matic alignments, which contained fewer word-to-word trans-
lation possibilities.
Size of training corpus
Model .5k 2k 8k 16k
Model 1 21.43 18.04 16.49 16.20
HMM 14.42 10.47 9.09 8.80
Model 3 20.56 13.25 10.82 10.51
Model 4 14.19 10.13 7.87 7.52
Table 2: Alignment error rates for the various IBM
Models trained with word-aligned data
4,3 and evaluated their AERs. Table 1 gives align-
ment error rates when training on 500, 2000, 8000,
and 16000 sentence pairs from Verbmobil corpus
without using any word-aligned training data.
We obtained much better results when incorpo-
rating word-alignments with our mixed likelihood
function. Table 2 shows the results for the differ-
ent corpus sizes, when all of the sentence pairs have
been word-aligned. The best performing model in
the unmodified GIZA++ code was the HMM trained
on 16,000 sentence pairs, which had an alignment
error rate of 12.04%. In our modified code the
best performing model was Model 4 trained on
16,000 sentence pairs (where all the sentence pairs
are word-aligned) with an alignment error rate of
7.52%. The difference in the best performing mod-
els represents a 38% relative reduction in AER. In-
terestingly, we achieve a lower AER than the best
performing unmodified models using a corpus that
is one-eight the size of the sentence-aligned data.
Figure 1 show an example of the improved
alignments that are achieved when using the word
aligned data. The example alignments were held
out sentence pairs that were aligned after training on
500 sentence pairs. The alignments produced when
the training on word-aligned data are dramatically
better than when training on sentence-aligned data.
We contrasted these improvements with the im-
provements that are to be had from incorporating a
bilingual dictionary into the estimation process. For
this experiment we allowed a bilingual dictionary
to constrain which words can act as translations of
each other during the initial estimates of translation
probabilities (as described in Och and Ney (2003)).
As can be seen in Table 3, using a dictionary reduces
the AER when compared to using GIZA++ without
a dictionary, but not as dramatically as integrating
the word-alignments. We further tried combining a
dictionary with our word-alignments but found that
the dictionary results in only very minimal improve-
ments over using word-alignments alone.
3We used the default training schemes for GIZA++, and left
model smoothing parameters at their default settings.
Th
e
n
a
s
s
u
m
e
.
Dann
reserviere
ich
zwei
Einzelzimmer
I
w
i
l
l
r
e
s
e
r
v
e
t
w
o
s
i
n
g
l
e
,
nehme
r
o
o
m
s
, I
ich
mal
an
.
(a) Sentence-aligned
T
h
e
n
a
s
s
u
m
e
.
Dann
reserviere
ich
zwei
Einzelzimmer
I
w
i
l
l
r
e
s
e
r
v
e
t
w
o
s
i
n
g
l
e
,
nehme
r
o
o
m
s
, I
ich
mal
an
.
(b) Word-aligned
T
h
e
n
a
s
s
u
m
e
.
Dann
reserviere
ich
zwei
Einzelzimmer
I
w
i
l
l
r
e
s
e
r
v
e
t
w
o
s
i
n
g
l
e
,
nehme
r
o
o
m
s
, I
ich
mal
an
.
(c) Reference
Figure 1: Example alignments using sentence-aligned training data (a), using word-aligned data (b), and a
reference manual alignment (c)
Size of training corpus
Model .5k 2k 8k 16k
Model 1 23.56 20.75 18.69 18.37
HMM 15.71 12.15 9.91 10.13
Model 3 22.11 16.93 13.78 12.33
Model 4 17.07 13.60 11.49 10.77
Table 3: The improved alignment error rates when
using a dictionary instead of word-aligned data to
constrain word translations
Sentence-aligned Word-aligned
Size AER Bleu AER Bleu
500 20.59 0.211 14.19 0.233
2000 16.05 0.247 10.13 0.260
8000 12.63 0.265 7.87 0.278
16000 12.17 0.270 7.52 0.282
Table 4: Improved AER leads to improved transla-
tion quality
5.2 Improved translation quality
The fact that using word-aligned data in estimat-
ing the parameters for machine translation leads to
better alignments is predictable. A more signifi-
cant result is whether it leads to improved transla-
tion quality. In order to test that our improved pa-
rameter estimates lead to better translation quality,
we used a state-of-the-art phrase-based decoder to
translate a held out set of German sentences into
English. The phrase-based decoder extracts phrases
from the word alignments produced by GIZA++,
and computes translation probabilities based on the
frequency of one phrase being aligned with another
(Koehn et al, 2003). We trained a language model
AER when when
Ratio ? = Standard MLE ? = .9
0.1 11.73 9.40
0.2 10.89 8.66
0.3 10.23 8.13
0.5 8.65 8.19
0.7 8.29 8.03
0.9 7.78 7.78
Table 5: The effect of weighting word-aligned data
more heavily that its proportion in the training data
(corpus size 16000 sentence pairs)
using the 34,000 English sentences from the train-
ing set.
Table 4 shows that using word-aligned data leads
to better translation quality than using sentence-
aligned data. Particularly, significantly less data is
needed to achieve a high Bleu score when using
word alignments. Training on a corpus of 8,000 sen-
tence pairs with word alignments results in a higher
Bleu score than when training on a corpus of 16,000
sentence pairs without word alignments.
5.3 Weighting the word-aligned data
We have seen that using training data consisting
of entirely word-aligned sentence pairs leads to
better alignment accuracy and translation quality.
However, because manually word-aligning sentence
pairs costs more than just using sentence-aligned
data, it is unlikely that we will ever want to label
an entire corpus. Instead we will likely have a rel-
atively small portion of the corpus word aligned.
We want to be sure that this small amount of data
labeled with word alignments does not get over-
whelmed by a larger amount of unlabeled data.
 
0.07
 
0.075 0.08
 
0.085 0.09
 
0.095 0.1
 
0.105 0.11
 
0.115 0.12
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9
 
1
Alignment Error Rate
Lamb
da20% w
ord-a
ligned
50% 
word-
aligne
d
70% 
word-
aligne
d
100%
 word
-align
ed
Figure 2: The effect on AER of varying ? for a train-
ing corpus of 16K sentence pairs with various pro-
portions of word-alignments
Thus we introduced the ? weight into our mixed
likelihood function.
Table 5 compares the natural setting of ? (where
it is proportional to the amount of labeled data in the
corpus) to a value that amplifies the contribution of
the word-aligned data. Figure 2 shows a variety of
values for ?. It shows as ? increases AER decreases.
Placing nearly all the weight onto the word-aligned
data seems to be most effective.4 Note this did not
vary the training data size ? only the relative contri-
butions between sentence- and word-aligned train-
ing material.
5.4 Ratio of word- to sentence-aligned data
We also varied the ratio of word-aligned to
sentence-aligned data, and evaluated the AER and
Bleu scores, and assigned high value to ? (= 0.9).
Figure 3 shows how AER improves as more
word-aligned data is added. Each curve on the graph
represents a corpus size and shows its reduction in
error rate as more word-aligned data is added. For
example, the bottom curve shows the performance
of a corpus of 16,000 sentence pairs which starts
with an AER of just over 12% with no word-aligned
training data and decreases to an AER of 7.5% when
all 16,000 sentence pairs are word-aligned. This
curve essentially levels off after 30% of the data is
word-aligned. This shows that a small amount of
word-aligned data is very useful, and if we wanted
to achieve a low AER, we would only have to label
4,800 examples with their word alignments rather
than the entire corpus.
Figure 4 shows how the Bleu score improves as
more word-aligned data is added. This graph also
4At ? = 1 (not shown in Figure 2) the data that is only
sentence-aligned is ignored, and the AER is therefore higher.
 
0.06
 
0.08 0.1
 
0.12
 
0.14
 
0.16
 
0.18 0.2
 
0.22  
0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Alignment error rate
Ratio
 of wo
rd-alig
ned to
 sente
nce-a
ligned
 data
500 s
enten
ce pa
irs
2000 
sente
nce p
airs
8000 
sente
nce p
airs
16000
 sente
nce p
airs
Figure 3: The effect on AER of varying the ratio of
word-aligned to sentence-aligned data
 
0.2
 
0.21
 
0.22
 
0.23
 
0.24
 
0.25
 
0.26
 
0.27
 
0.28
 
0.29  
0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Bleu Score
Ratio
 of wo
rd-alig
ned to
 sente
nce-a
ligned
 data
500 s
enten
ce pa
irs
2000 
sente
nce p
airs
8000 
sente
nce p
airs
16000
 sente
nce p
airs
Figure 4: The effect on Bleu of varying the ratio of
word-aligned to sentence-aligned data
reinforces the fact that a small amount of word-
aligned data is useful. A corpus of 8,000 sentence
pairs with only 800 of them labeled with word align-
ments achieves a higher Bleu score than a corpus of
16,000 sentence pairs with no word alignments.
5.5 Evaluation using a larger training corpus
We additionally tested whether incorporating word-
level alignments into the estimation improved re-
sults for a larger corpus. We repeated our experi-
ments using the Canadian Hansards French-English
parallel corpus. Figure 6 gives a summary of the im-
provements in AER and Bleu score for that corpus,
when testing on a held out set of 484 hand aligned
sentences.
On the whole, alignment error rates are higher
and Bleu scores are considerably lower for the
Hansards corpus. This is probably due to the dif-
ferences in the corpora. Whereas the Verbmobil
corpus has a small vocabulary (<10,000 per lan-
Sentence-aligned Word-aligned
Size AER Bleu AER Bleu
500 33.65 0.054 25.73 0.064
2000 25.97 0.087 18.57 0.100
8000 19.00 0.115 14.57 0.120
16000 16.59 0.126 13.55 0.128
Table 6: Summary results for AER and translation
quality experiments on Hansards data
guage), the Hansards has ten times that many vocab-
ulary items and has a much longer average sentence
length. This made it more difficult for us to create a
simulated set of hand alignments; we measured the
AER of our simulated alignments at 11.3% (which
compares to 6.5% for our simulated alignments for
the Verbmobil corpus).
Nevertheless, the trend of decreased AER and in-
creased Bleu score still holds. For each size of train-
ing corpus we tested we found better results using
the word-aligned data.
6 Related Work
Och and Ney (2003) is the most extensive analy-
sis to date of how many different factors contribute
towards improved alignments error rates, but the in-
clusion of word-alignments is not considered. Och
and Ney do not give any direct analysis of how
improved word alignments accuracy contributes to-
ward better translation quality as we do here.
Mihalcea and Pedersen (2003) described a shared
task where the goal was to achieve the best AER. A
number of different methods were tried, but none
of them used word-level alignments. Since the best
performing system used an unmodified version of
Giza++, we would expected that our modifed ver-
sion would show enhanced performance. Naturally
this would need to be tested in future work.
Melamed (1998) describes the process of manu-
ally creating a large set of word-level alignments of
sentences in a parallel text.
Nigam et al (2000) described the use of weight
to balance the respective contributions of labeled
and unlabeled data to a mixed likelihood function.
Corduneanu (2002) provides a detailed discussion
of the instability of maximum likelhood solutions
estimated from a mixture of labeled and unlabeled
data.
7 Discussion and Future Work
In this paper we show with the appropriate modifi-
cation of EM significant improvement gains can be
had through labeling word alignments in a bilingual
corpus. Because of this significantly less data is re-
quired to achieve a low alignment error rate or high
Bleu score. This holds even when using noisy word
alignments such as our automatically created set.
One should take our research into account when
trying to efficiently create a statistical machine
translation system for a language pair for which a
parallel corpus is not available. Germann (2001)
describes the cost of building a Tamil-English paral-
lel corpus from scratch, and finds that using profes-
sional translations is prohibitively high. In our ex-
perience it is quicker to manually word-align trans-
lated sentence pairs than to translate a sentence, and
word-level alignment can be done by someone who
might not be fluent enough to produce translations.
It might therefore be possible to achieve a higher
performance at a fraction of the cost by hiring a non-
professional produce word-alignments after a lim-
ited set of sentences have been translated.
We plan to investigate whether it is feasible to
use active learning to select which examples will
be most useful when aligned at the word-level. Sec-
tion 5.4 shows that word-aligning a fraction of sen-
tence pairs in a training corpus, rather than the entire
training corpus can still yield most of the benefits
described in this paper. One would hope that by se-
lectively sampling which sentences are to be manu-
ally word-aligned we would achieve nearly the same
performance as word-aligning the entire corpus.
Acknowledgements
The authors would like to thank Franz Och, Her-
mann Ney, and Richard Zens for providing the
Verbmobil data, and Linear B for providing its
phrase-based decoder.
References
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Adrian Corduneanu. 2002. Stable mixing of complete
and incomplete information. Master?s thesis, Mas-
sachusetts Institute of Technology, February.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(1):1?38, Nov.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang for
the buck can we expect? In ACL 2001 Workshop on
Data-Driven Machine Translation, Toulouse, France,
July 7.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the HLT/NAACL.
I. Dan Melamed. 1998. Manual annotation of trans-
lational equivalence: The blinker project. Cognitive
Science Technical Report 98/07, University of Penn-
sylvania.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Rada Mihalcea and
Ted Pedersen, editors, HLT-NAACL 2003 Workshop:
Building and Using Parallel Texts.
Kamal Nigam, Andrew K. McCallum, Sebastian Thrun,
and Tom M. Mitchell. 2000. Text classification from
labeled and unlabeled documents using EM. Machine
Learning, 39(2/3):103?134.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report
RC22176(W0109-022), IBM.
Philip Resnik and Noah Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29(3):349?
380, September.
Proceedings of the 43rd Annual Meeting of the ACL, pages 255?262,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Scaling Phrase-Based Statistical Machine Translation
to Larger Corpora and Longer Phrases
Chris Callison-Burch Colin Bannard
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
{chris,colin}@linearb.co.uk
Josh Schroeder
Linear B Ltd.
39 B Cumberland Street
Edinburgh EH3 6RA
josh@linearb.co.uk
Abstract
In this paper we describe a novel data
structure for phrase-based statistical ma-
chine translation which allows for the re-
trieval of arbitrarily long phrases while si-
multaneously using less memory than is
required by current decoder implementa-
tions. We detail the computational com-
plexity and average retrieval times for
looking up phrase translations in our suf-
fix array-based data structure. We show
how sampling can be used to reduce the
retrieval time by orders of magnitude with
no loss in translation quality.
1 Introduction
Statistical machine translation (SMT) has an advan-
tage over many other statistical natural language
processing applications in that training data is reg-
ularly produced by other human activity. For some
language pairs very large sets of training data are
now available. The publications of the European
Union and United Nations provide gigbytes of data
between various language pairs which can be eas-
ily mined using a web crawler. The Linguistics
Data Consortium provides an excellent set of off
the shelf Arabic-English and Chinese-English paral-
lel corpora for the annual NIST machine translation
evaluation exercises.
The size of the NIST training data presents a prob-
lem for phrase-based statistical machine translation.
Decoders such as Pharaoh (Koehn, 2004) primarily
use lookup tables for the storage of phrases and their
translations. Since retrieving longer segments of hu-
man translated text generally leads to better trans-
lation quality, participants in the evaluation exer-
cise try to maximize the length of phrases that are
stored in lookup tables. The combination of large
corpora and long phrases means that the table size
can quickly become unwieldy.
A number of groups in the 2004 evaluation exer-
cise indicated problems dealing with the data. Cop-
ing strategies included limiting the length of phrases
to something small, not using the entire training data
set, computing phrases probabilities on disk, and fil-
tering the phrase table down to a manageable size
after the testing set was distributed. We present a
data structure that is easily capable of handling the
largest data sets currently available, and show that it
can be scaled to much larger data sets.
In this paper we:
? Motivate the problem with storing enumerated
phrases in a table by examining the memory re-
quirements of the method for the NIST data set
? Detail the advantages of using long phrases in
SMT, and examine their potential coverage
? Describe a suffix array-based data structure
which allows for the retrieval of translations
of arbitrarily long phrases, and show that it re-
quires far less memory than a table
? Calculate the computational complexity and
average time for retrieving phrases and show
how this can be sped up by orders of magnitude
with no loss in translation accuracy
2 Related Work
Koehn et al (2003) compare a number of differ-
ent approaches to phrase-based statistical machine
255
length num uniq
(mil)
average #
translations
avg trans
length
1 .88 8.322 1.37
2 16.5 1.733 2.35
3 42.6 1.182 3.44
4 58.7 1.065 4.58
5 65.0 1.035 5.75
6 66.4 1.022 6.91
7 65.8 1.015 8.07
8 64.3 1.012 9.23
9 62.2 1.010 10.4
10 59.9 1.010 11.6
Table 1: Statistics about Arabic phrases in the NIST-
2004 large data track.
translation including the joint probability phrase-
based model (Marcu and Wong, 2002) and a vari-
ant on the alignment template approach (Och and
Ney, 2004), and contrast them to the performance of
the word-based IBM Model 4 (Brown et al, 1993).
Most relevant for the work presented in this paper,
they compare the effect on translation quality of us-
ing various lengths of phrases, and the size of the
resulting phrase probability tables.
Tillmann (2003) further examines the relationship
between maximum phrase length, size of the trans-
lation table, and accuracy of translation when in-
ducing block-based phrases from word-level align-
ments. Venugopal et al (2003) and Vogel et al
(2003) present methods for achieving better transla-
tion quality by growing incrementally larger phrases
by combining smaller phrases with overlapping seg-
ments.
3 Scaling to Long Phrases
Table 1 gives statistics about the Arabic-English par-
allel corpus used in the NIST large data track. The
corpus contains 3.75 million sentence pairs, and has
127 million words in English, and 106 million words
in Arabic. The table shows the number of unique
Arabic phrases, and gives the average number of
translations into English and their average length.
Table 2 gives estimates of the size of the lookup
tables needed to store phrases of various lengths,
based on the statistics in Table 1. The number of
unique entries is calculated as the number unique
length entries
(mil)
words
(mil)
memory
(gigs)
including
alignments
1 7.3 10 .1 .11
2 36 111 .68 .82
3 86 412 2.18 2.64
4 149 933 4.59 5.59
5 216 1,645 7.74 9.46
6 284 2,513 11.48 14.07
7 351 3,513 15.70 19.30
8 416 4,628 20.34 25.05
9 479 5,841 25.33 31.26
10 539 7,140 30.62 37.85
Table 2: Estimated size of lookup tables for the
NIST-2004 Arabic-English data
length coverage length coverage
1 93.5% 6 4.70%
2 73.3% 7 2.95%
3 37.1% 8 2.14%
4 15.5% 9 1.99%
5 8.05% 10 1.49%
Table 3: Lengths of phrases from the training data
that occur in the NIST-2004 test set
phrases times the average number of translations.
The number of words in the table is calculated as the
number of unique phrases times the phrase length
plus the number of entries times the average transla-
tion length. The memory is calculated assuming that
each word is represented with a 4 byte integer, that
each entry stores its probability as an 8 byte double
and that each word alignment is stored as a 2 byte
short. Note that the size of the table will vary de-
pending on the phrase extraction technique.
Table 3 gives the percent of the 35,313 word long
test set which can be covered using only phrases of
the specified length or greater. The table shows the
efficacy of using phrases of different lengths. The ta-
ble shows that while the rate of falloff is rapid, there
are still multiple matches of phrases of length 10.
The longest matching phrase was one of length 18.
There is little generalization in current SMT imple-
mentations, and consequently longer phrases gener-
ally lead to better translation quality.
256
3.1 Why use phrases?
Statistical machine translation made considerable
advances in translation quality with the introduction
of phrase-based translation. By increasing the size
of the basic unit of translation, phrase-based ma-
chine translation does away with many of the prob-
lems associated with the original word-based for-
mulation of statistical machine translation (Brown
et al, 1993), in particular:
? The Brown et al (1993) formulation doesn?t
have a direct way of translating phrases; instead
they specify a fertility parameter which is used
to replicate words and translate them individu-
ally.
? With units as small as words, a lot of reordering
has to happen between languages with different
word orders. But the distortion parameter is a
poor explanation of word order.
Phrase-based SMT overcomes the first of these
problems by eliminating the fertility parameter
and directly handling word-to-phrase and phrase-to-
phrase mappings. The second problem is alleviated
through the use of multi-word units which reduce
the dependency on the distortion parameter. Less
word re-ordering need occur since local dependen-
cies are frequently captured. For example, common
adjective-noun alternations are memorized. How-
ever, since this linguistic information is not encoded
in the model, unseen adjective noun pairs may still
be handled incorrectly.
By increasing the length of phrases beyond a
few words, we might hope to capture additional
non-local linguistic phenomena. For example, by
memorizing longer phrases we may correctly learn
case information for nouns commonly selected by
frequently occurring verbs; we may properly han-
dle discontinuous phrases (such as French negation,
some German verb forms, and English verb particle
constructions) that are neglected by current phrase-
based models; and we may by chance capture some
agreement information in coordinated structures.
3.2 Deciding what length of phrase to store
Despite the potential gains from memorizing longer
phrases, the fact remains that as phrases get longer
length coverage length coverage
1 96.3% 6 21.9%
2 94.9% 7 11.2%
3 86.1% 8 6.16%
4 65.6% 9 3.95%
5 40.9% 10 2.90%
Table 4: Coverage using only repeated phrases of
the specified length
there is a decreasing likelihood that they will be re-
peated. Because of the amount of memory required
to store a phrase table, in current implementations a
choice is made as to the maximum length of phrase
to store.
Based on their analysis of the relationship be-
tween translation quality and phrase length, Koehn
et al (2003) suggest limiting phrase length to three
words or less. This is entirely a practical sugges-
tion for keeping the phrase table to a reasonable
size, since they measure minor but incremental im-
provement in translation quality up to their maxi-
mum tested phrase length of seven words.1
Table 4 gives statistics about phrases which oc-
cur more than once in the English section of the Eu-
roparl corpus (Koehn, 2002) which was used in the
Koehn et al (2003) experiments. It shows that the
percentage of words in the corpus that can be cov-
ered by repeated phrases falls off rapidly at length
6, but that even phrases up to length 10 are able to
cover a non-trivial portion of the corpus. This draws
into question the desirability of limiting phrase re-
trieval to length three.
The decision concerning what length of phrases
to store in the phrase table seems to boil down to
a practical consideration: one must weigh the like-
lihood of retrieval against the memory needed to
store longer phrases. We present a data structure
where this is not a consideration. Our suffix array-
based data structure allows the retrieval of arbitrar-
ily long phrases, while simultaneously requiring far
less memory than the standard table-based represen-
tation.
1While the improvements to translation quality reported in
Koehn et al (2003) are minor, their evaluation metric may not
have been especially sensitive to adding longer phrases. They
used the Bleu evaluation metric (Papineni et al, 2002), but
capped the n-gram precision at 4-grams.
257
01
2
3
4
5
6
7
8
9
spain declined to confirm that spain declined to aid morocco
declined to confirm that spain declined to aid morocco
to confirm that spain declined to aid morocco
confirm that spain declined to aid morocco
that spain declined to aid morocco
spain declined to aid morocco
declined to aid morocco
to aid morocco
aid morocco
morocco
spain declined to confirm that spain declined aidto morocco
0 1 2 3 4 5 6 87 9
s[0]
s[1]
s[2]
s[3]
s[4]
s[5]
s[6]
s[7]
s[8]
s[9]
Initialized, unsorted
Suffix Array
Suffixes denoted by s[i]
Corpus
Index of
words:
Figure 1: An initialized, unsorted suffix array for a
very small corpus
4 Suffix Arrays
The suffix array data structure (Manber and Myers,
1990) was introduced as a space-economical way of
creating an index for string searches. The suffix ar-
ray data structure makes it convenient to compute
the frequency and location of any substring or n-
gram in a large corpus. Abstractly, a suffix array is
an alphabetically-sorted list of all suffixes in a cor-
pus, where a suffix is a substring running from each
position in the text to the end. However, rather than
actually storing all suffixes, a suffix array can be
constructed by creating a list of references to each
of the suffixes in a corpus. Figure 1 shows how a
suffix array is initialized for a corpus with one sen-
tence. Each index of a word in the corpus has a cor-
responding place in the suffix array, which is identi-
cal in length to the corpus. Figure 2 shows the final
state of the suffix array, which is as a list of the in-
dices of words in the corpus that corresponds to an
alphabetically sorted list of the suffixes.
The advantages of this representation are that it is
compact and easily searchable. The total size of the
suffix array is a constant amount of memory. Typ-
ically it is stored as an array of integers where the
array is the same length as the corpus. Because it is
organized alphabetically, any phrase can be quickly
located within it using a binary search algorithm.
Yamamoto and Church (2001) show how to use
suffix arrays to calculate a number of statistics that
are interesting in natural language processing appli-
cations. They demonstrate how to calculate term fre-
8
3
6
1
9
5
0
4
7
2
to aid morocco
to confirm that spain declined to aid morocco
morocco
spain declined to aid morocco
declined to confirm that spain declined to aid morocco
declined to aid morocco
confirm that spain declined to aid morocco
aid morocco
that spain declined to aid morocco
spain declined to confirm that spain declined to aid morocco
Sorted
Suffix Array
Suffixes denoted by s[i]
s[0]
s[1]
s[2]
s[3]
s[4]
s[5]
s[6]
s[7]
s[8]
s[9]
Figure 2: A sorted suffix array and its corresponding
suffixes
quency / inverse document frequency (tf / idf) for all
n-grams in very large corpora, as well as how to use
these frequencies to calculate n-grams with high mu-
tual information and residual inverse document fre-
quency. Here we show how to apply suffix arrays to
parallel corpora to calculate phrase translation prob-
abilities.
4.1 Applied to parallel corpora
In order to adapt suffix arrays to be useful for sta-
tistical machine translation we need a data structure
with the following elements:
? A suffix array created from the source language
portion of the corpus, and another created from
the target language portion of the corpus,
? An index that tells us the correspondence be-
tween sentence numbers and positions in the
source and target language corpora,
? An alignment a for each sentence pair in the
parallel corpus, where a is defined as a subset
of the Cartesian product of the word positions
in a sentence e of length I and a sentence f of
length J :
a ? {(i, j) : i = 1...I; j = 1...J}
? A method for extracting the translationally
equivalent phrase for a subphrase given an
aligned sentence pair containing that sub-
phrase.
The total memory usage of the data structure is
thus the size of the source and target corpora, plus
the size of the suffix arrays (identical in length to the
258
corpora), plus the size of the two indexes that cor-
relate sentence positions with word positions, plus
the size of the alignments. Assuming we use ints
to represent words and indices, and shorts to repre-
sent word alignments, we get the following memory
usage:
2 ? num words in source corpus ? sizeof(int)+
2 ? num words in target corpus ? sizeof(int)+
2 ? number sentence pairs ? sizeof(int)+
number of word alignments ? sizeof(short)
The total amount of memory required to store the
NIST Arabic-English data using this data structure
is
2 ? 105,994,774 ? sizeof(int)+
2 ? 127,450,473 ? sizeof(int)+
2 ? 3,758,904 ? sizeof(int)+
92,975,229 ? sizeof(short)
Or just over 2 Gigabytes.
4.2 Calculating phrase translation
probabilities
In order to produce a set of phrase translation prob-
abilities, we need to examine the ways in which
they are calculated. We consider two common ways
of calculating the translation probability: using the
maximum likelihood estimator (MLE) and smooth-
ing the MLE using lexical weighting.
The maximum likelihood estimator for the proba-
bility of a phrase is defined as
p(f? |e?) =
count(f? , e?)
?
f? count(f? , e?)
(1)
Where count(f? , e?) gives the total number of times
the phrase f? was aligned with the phrase e? in the
parallel corpus. We define phrase alignments as fol-
lows. A substring e? consisting of the words at po-
sitions l...m is aligned with the phrase f? by way of
the subalignment
s = a ? {(i, j) : i = l...m, j = 1...J}
The aligned phrase f? is the subphrase in f which
spans from min(j) to max(j) for j|(i, j) ? s.
The procedure for generating the counts that are
used to calculate the MLE probability using our suf-
fix array-based data structures is:
1. Locate all the suffixes in the English suffix ar-
ray which begin with the phrase e?. Since the
suffix array is sorted alphabetically we can eas-
ily find the first occurrence s[k] and the last oc-
currence s[l]. The length of the span in the suf-
fix array l?k+1 indicates the number of occur-
rences of e? in the corpus. Thus the denominator
?
f? count(f? , e?) can be calculated as l ? k + 1.
2. For each of the matching phrases s[i] in the
span s[k]...s[l], look up the value of s[i] which
is the word index w of the suffix in the English
corpus. Look up the sentence number that in-
cludes w, and retrieve the corresponding sen-
tences e and f , and their alignment a.
3. Use a to extract the target phrase f? that aligns
with the phrase e? that we are searching for. In-
crement the count for < f?, e? >.
4. Calculate the probability for each unique
matching phrase f? using the formula in Equa-
tion 1.
A common alternative formulation of the phrase
translation probability is to lexically weight it as fol-
lows:
plw(f? |e?, s) =
n?
i=1
1
|{i|(i, j) ? s}|
?
?(i,j)?s
p(fj |ei)
(2)
Where n is the length of e?.
In order to use lexical weighting we would need
to repeat steps 1-4 above for each word ei in e?. This
would give us the values for p(fj |ei). We would fur-
ther need to retain the subphrase alignment s in or-
der to know the correspondence between the words
(i, j) ? s in the aligned phrases, and the total num-
ber of foreign words that each ei is aligned with
(|{i|(i, j) ? s}|). Since a phrase alignment < f?, e? >
may have multiple possible word-level alignments,
we retain a set of alignments S and take the maxi-
mum:
259
p(f? |e?, S) = p(f? |e?) ? argmax
s?S
plw(f? |e?, s) (3)
Thus our suffix array-based data structure can be
used straightforwardly to look up all aligned trans-
lations for a given phrase and calculate the proba-
bilities on-the-fly. In the next section we turn to
the computational complexity of constructing phrase
translation probabilities in this way.
5 Computational Complexity
Computational complexity is relevant because there
is a speed-memory tradeoff when adopting our data
structure. What we gained in memory efficiency
may be rendered useless if the time it takes to cal-
culate phrase translation probabilities is unreason-
ably long. The computational complexity of looking
up items in a hash table, as is done in current table-
based data structures, is extremely fast. Looking up
a single phrase can be done in unit time, O(1).
The computational complexity of our method has
the following components:
? The complexity of finding all occurrences of
the phrase in the suffix array
? The complexity of retrieving the associated
aligned sentence pairs given the positions of the
phrase in the corpus
? The complexity of extracting all aligned
phrases using our phrase extraction algorithm
? The complexity of calculating the probabilities
given the aligned phrases
The methods we use to execute each of these, and
their complexities are as follow:
? Since the array is sorted, finding all occur-
rences of the English phrase is extremely fast.
We can do two binary searches: one to find the
first occurrence of the phrase and a second to
find the last. The computational complexity is
therefore bounded by O(2 log(n)) where n is
the length of the corpus.
? We use a similar method to look up the sen-
tences ei and fi and word-level alignment ai
phrase freq O time (ms)
respect for the
dead
3 80 24
since the end of
the cold war
19 240 136
the parliament 1291 4391 1117
of the 290921 682550 218369
Table 5: Examples of O and calculation times for
phrases of different frequencies
that are associated with the position wi in the
corpus of each phrase occurrence e?i. The com-
plexity is O(k ? 2 log(m)) where k is the num-
ber of occurrences of e? and m is the number of
sentence pairs in the parallel corpus.
? The complexity of extracting the aligned phrase
for a single occurrence of e?i is O(2 log(|ai|) to
get the subphrase alignment si, since we store
the alignments in a sorted array. The complex-
ity of then getting f?i from si is O(length(f?i)).
? The complexity of summing over all aligned
phrases and simultaneously calculating their
probabilities is O(k).
Thus we have a total complexity of:
O(2 log(n) + k ? 2 log(m) (4)
+
e?1...e?k?
ai,f?i|e?i
(2 log(|ai|) + length(f?i)) + k) (5)
for the MLE estimation of the translation probabil-
ities for a single phrase. The complexity is domi-
nated by the k terms in the equation, when the num-
ber of occurrences of the phrase in the corpus is
high. Phrases with high frequency may cause exces-
sively long retrieval time. This problem is exacer-
bated when we shift to a lexically weighted calcula-
tion of the phrase translation probability. The com-
plexity will be multiplied across each of the compo-
nent words in the phrase, and the component words
themselves will be more frequent than the phrase.
Table 5 shows example times for calculating the
translation probabilities for a number of phrases. For
frequent phrases like of the these times get unaccept-
ably long. While our data structure is perfect for
260
overcoming the problems associated with storing the
translations of long, infrequently occurring phrases,
it in a way introduces the converse problem. It has
a clear disadvantage in the amount of time it takes
to retrieve commonly occurring phrases. In the next
section we examine the use of sampling to speed up
the calculation of translation probabilities for very
frequent phrases.
6 Sampling
Rather than compute the phrase translation proba-
bilities by examining the hundreds of thousands of
occurrences of common phrases, we instead sam-
ple from a small subset of the occurrences. It is
unlikely that we need to extract the translations of
all occurrences of a high frequency phrase in order
to get a good approximation of their probabilities.
We instead cap the number of occurrences that we
consider, and thus give a maximum bound on k in
Equation 5.
In order to determine the effect of different lev-
els of sampling, we compare the translation quality
against cumulative retrieval time for calculating the
phrase translation probabilities for all subphrases in
an evaluation set. We translated a held out set of
430 German sentences with 50 words or less into
English. The test sentences were drawn from the
01/17/00 proceedings of the Europarl corpus. The
remainder of the corpus (1 million sentences) was
used as training data to calculate the phrase trans-
lation probabilities. We calculated the translation
quality using Bleu?s modified n-gram precision met-
ric (Papineni et al, 2002) for n-grams of up to length
four. The framework that we used to calculate the
translation probabilities was similar to that detailed
in Koehn et al (2003). That is:
e? = argmax
eI1
p(eI1|f
I
1) (6)
= argmax
eI1
pLM (e
I
1) ? (7)
I?
i=1
p(f?i|e?i)d(ai ? bi?1)plw(f?i|e?i,a) (8)
Where pLM is a language model probability and d is
a distortion probability which penalizes movement.
Table 6 gives a comparison of the translation qual-
ity under different levels of sampling. While the ac-
sample size time quality
unlimited 6279 sec .290
50000 1051 sec .289
10000 336 sec .291
5000 201 sec .289
1000 60 sec .288
500 35 sec .288
100 10 sec .288
Table 6: A comparison of retrieval times and trans-
lation quality when the number of translations is
capped at various sample sizes
curacy fluctuates very slightly it essentially remains
uniformly high for all levels of sampling. There are
a number of possible reasons for the fact that the
quality does not decrease:
? The probability estimates under sampling are
sufficiently good that the most probable trans-
lations remain unchanged,
? The interaction with the language model prob-
ability rules out the few misestimated probabil-
ities, or
? The decoder tends to select longer or less fre-
quent phrases which are not affected by the
sampling.
While the translation quality remains essentially
unchanged, the cumulative time that it takes to cal-
culate the translation probabilities for all subphrases
in the 430 sentence test set decreases radically. The
total time drops by orders of magnitude from an hour
and a half without sampling down to a mere 10 sec-
onds with a cavalier amount of sampling. This sug-
gests that the data structure is suitable for deployed
SMT systems and that no additional caching need
be done to compensate for the structure?s computa-
tional complexity.
7 Discussion
The paper has presented a super-efficient data struc-
ture for phrase-based statistical machine translation.
We have shown that current table-based methods are
unwieldily when used in conjunction with large data
sets and long phrases. We have contrasted this with
our suffix array-based data structure which provides
261
a very compact way of storing large data sets while
simultaneously allowing the retrieval of arbitrarily
long phrases.
For the NIST-2004 Arabic-English data set,
which is among the largest currently assembled for
statistical machine translation, our representation
uses a very manageable 2 gigabytes of memory. This
is less than is needed to store a table containing
phrases with a maximum of three words, and is ten
times less than the memory required to store a table
with phrases of length eight.
We have further demonstrated that while compu-
tational complexity can make the retrieval of trans-
lation of frequent phrases slow, the use of sampling
is an extremely effective countermeasure to this.
We demonstrated that calculating phrase translation
probabilities from sets of 100 occurrences or less re-
sults in nearly no decrease in translation quality.
The implications of the data structure presented
in this paper are significant. The compact rep-
resentation will allow us to easily scale to paral-
lel corpora consisting of billions of words of text,
and the retrieval of arbitrarily long phrases will al-
low experiments with alternative decoding strate-
gies. These facts in combination allow for an even
greater exploitation of training data in statistical ma-
chine translation.
References
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished
Draft.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Udi Manber and Gene Myers. 1990. Suffix arrays:
A new method for on-line string searches. In The
First Annual ACM-SIAM Symposium on Dicrete Algo-
rithms, pages 319?327.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?450, De-
cember.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proceed-
ings of EMNLP.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical machine translation sys-
tem. In Proceedings of MT Summit 9.
Mikio Yamamoto and Kenneth Church. 2001. Using suf-
fix arrays to compute term frequency and document
frequency for all substrings in a corpus. Compuata-
tional Linguistics, 27(1):1?30.
262
Proceedings of the 43rd Annual Meeting of the ACL, pages 597?604,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Paraphrasing with Bilingual Parallel Corpora
Colin Bannard Chris Callison-Burch
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
{c.j.bannard, callison-burch}@ed.ac.uk
Abstract
Previous work has used monolingual par-
allel corpora to extract and generate para-
phrases. We show that this task can be
done using bilingual parallel corpora, a
much more commonly available resource.
Using alignment techniques from phrase-
based statistical machine translation, we
show how paraphrases in one language
can be identified using a phrase in another
language as a pivot. We define a para-
phrase probability that allows paraphrases
extracted from a bilingual parallel corpus
to be ranked using translation probabili-
ties, and show how it can be refined to
take contextual information into account.
We evaluate our paraphrase extraction and
ranking methods using a set of manual
word alignments, and contrast the qual-
ity with paraphrases extracted from auto-
matic alignments.
1 Introduction
Paraphrases are alternative ways of conveying the
same information. Paraphrases are useful in a num-
ber of NLP applications. In natural language gen-
eration the production of paraphrases allows for the
creation of more varied and fluent text (Iordanskaja
et al, 1991). In multidocument summarization the
identification of paraphrases allows information re-
peated across documents to be condensed (McKe-
own et al, 2002). In the automatic evaluation of
machine translation, paraphrases may help to alle-
viate problems presented by the fact that there are
often alternative and equally valid ways of translat-
ing a text (Pang et al, 2003). In question answering,
discovering paraphrased answers may provide addi-
tional evidence that an answer is correct (Ibrahim et
al., 2003).
In this paper we introduce a novel method for ex-
tracting paraphrases that uses bilingual parallel cor-
pora. Past work (Barzilay and McKeown, 2001;
Barzilay and Lee, 2003; Pang et al, 2003; Ibrahim et
al., 2003) has examined the use of monolingual par-
allel corpora for paraphrase extraction. Examples
of monolingual parallel corpora that have been used
are multiple translations of classical French novels
into English, and data created for machine transla-
tion evaluation methods such as Bleu (Papineni et
al., 2002) which use multiple reference translations.
While the results reported for these methods are
impressive, their usefulness is limited by the scarcity
of monolingual parallel corpora. Small data sets
mean a limited number of paraphrases can be ex-
tracted. Furthermore, the narrow range of text gen-
res available for monolingual parallel corpora limits
the range of contexts in which the paraphrases can
be used.
Instead of relying on scarce monolingual parallel
data, our method utilizes the abundance of bilingual
parallel data that is available. This allows us to cre-
ate a much larger inventory of phrases that is appli-
cable to a wider range of texts.
Our method for identifying paraphrases is an
extension of recent work in phrase-based statisti-
cal machine translation (Koehn et al, 2003). The
essence of our method is to align phrases in a bilin-
gual parallel corpus, and equate different English
phrases that are aligned with the same phrase in the
other language. This assumption of similar mean-
597
Emma burst into tears and he tried to comfort
her, saying things to make her smile.
Emma cried, and he tried to console her, adorn-
ing his words with puns.
Figure 1: Using a monolingal parallel corpus to ex-
tract paraphrases
ing when multiple phrases map onto a single for-
eign language phrase is the converse of the assump-
tion made in the word sense disambiguation work of
Diab and Resnik (2002) which posits different word
senses when a single English word maps onto differ-
ent words in the foreign language (we return to this
point in Section 4.4).
The remainder of this paper is as follows: Section
2 contrasts our method for extracting paraphrases
with the monolingual case, and describes how we
rank the extracted paraphrases with a probability
assignment. Section 3 describes our experimental
setup and includes information about how phrases
were selected, how we manually aligned parts of the
bilingual corpus, and how we evaluated the para-
phrases. Section 4 gives the results of our evalua-
tion and gives a number of example paraphrases ex-
tracted with our technique. Section 5 reviews related
work, and Section 6 discusses future directions.
2 Extracting paraphrases
Much previous work on extracting paraphrases
(Barzilay and McKeown, 2001; Barzilay and Lee,
2003; Pang et al, 2003) has focused on finding iden-
tifying contexts within aligned monolingual sen-
tences from which divergent text can be extracted,
and treated as paraphrases. Barzilay and McKeown
(2001) gives the example shown in Figure 1 of how
identical surrounding substrings can be used to ex-
tract the paraphrases of burst into tears as cried and
comfort as console.
While monolingual parallel corpora often have
identical contexts that can be used for identifying
paraphrases, bilingual parallel corpora do not. In-
stead, we use phrases in the other language as piv-
ots: we look at what foreign language phrases the
English translates to, find all occurrences of those
foreign phrases, and then look back at what other
English phrases they translate to. We treat the other
English phrases as potential paraphrases. Figure 2 il-
lustrates how a German phrase can be used as a point
of identification for English paraphrases in this way.
Section 2.1 explains which statistical machine trans-
lation techniques are used to align phrases within
sentence pairs in a bilingual corpus.
A significant difference between the present work
and that employing monolingual parallel corpora, is
that our method frequently extracts more than one
possible paraphrase for each phrase. We assign a
probability to each of the possible paraphrases. This
is a mechanism for ranking paraphrases, which can
be utilized when we come to select the correct para-
phrase for a given context . Section 2.2 explains how
we calculate the probability of a paraphrase.
2.1 Aligning phrase pairs
We use phrase alignments in a parallel corpus as
pivots between English paraphrases. We find these
alignments using recent phrase-based approaches to
statistical machine translation.
The original formulation of statistical machine
translation (Brown et al, 1993) was defined as a
word-based operation. The probability that a foreign
sentence is the translation of an English sentence is
calculated by summing over the probabilities of all
possible word-level alignments, a, between the sen-
tences:
p(f |e) =
?
a
p(f ,a|e)
Thus Brown et al decompose the problem of de-
termining whether a sentence is a good translation
of another into the problem of determining whether
there is a sensible mapping between the words in the
sentences.
More recent approaches to statistical translation
calculate the translation probability using larger
blocks of aligned text. Koehn (2004), Tillmann
(2003), and Vogel et al (2003) describe various
heuristics for extracting phrase alignments from the
Viterbi word-level alignments that are estimated us-
ing Brown et al (1993) models. We use the heuris-
tic for phrase alignment described in Och and Ney
(2003) which aligns phrases by incrementally build-
ing longer phrases from words and phrases which
have adjacent alignment points.1
1Note that while we induce the translations of phrases from
598
what is more, the relevant cost dynamic is completely under control
im ?brigen ist die diesbez?gliche kostenentwicklung v?llig  unter kontrolle
we owe it to the taxpayers to keep in checkthe costs
wir sind es den steuerzahlern die kosten zu habenschuldig  unter kontrolle
Figure 2: Using a bilingual parallel corpus to extract paraphrases
2.2 Assigning probabilities
We define a paraphrase probability p(e2|e1) in terms
of the translation model probabilities p(f |e1), that
the original English phrase e1 translates as a partic-
ular phrase f in the other language, and p(e2|f), that
the candidate paraphrase e2 translates as the foreign
language phrase. Since e1 can translate as multiple
foreign language phrases, we sum over f :
e?2 = arg max
e2 6=e1
p(e2|e1) (1)
= arg max
e2 6=e1
?
f
p(f |e1)p(e2|f) (2)
The translation model probabilities can be com-
puted using any standard formulation from phrase-
based machine translation. For example, p(e|f)
can be calculated straightforwardly using maximum
likelihood estimation by counting how often the
phrases e and f were aligned in the parallel corpus:
p(e|f) =
count(e, f)
?
e count(e, f)
(3)
Note that the paraphrase probability defined in
Equation 2 returns the single best paraphrase, e?2, ir-
respective of the context in which e1 appears. Since
the best paraphrase may vary depending on informa-
tion about the sentence that e1 appears in, we extend
the paraphrase probability to include that sentence
S:
e?2 = arg max
e2 6=e1
p(e2|e1, S) (4)
word-level alignments in this paper, direct estimation of phrasal
translations (Marcu and Wong, 2002) would also suffice for ex-
tracting paraphrases from bilingual corpora.
a million, as far as possible, at work, big business,
carbon dioxide, central america, close to, concen-
trate on, crystal clear, do justice to, driving force,
first half, for the first time, global warming, great
care, green light, hard core, horn of africa, last re-
sort, long ago, long run, military action, military
force, moment of truth, new world, noise pollution,
not to mention, nuclear power, on average, only too,
other than, pick up, president clinton, public trans-
port, quest for, red cross, red tape, socialist party,
sooner or later, step up, task force, turn to, under
control, vocational training, western sahara, world
bank
Table 1: Phrases that were selected to paraphrase
S allows us to re-rank the candidate paraphrases
based on additional contextual information. The ex-
periments in this paper employ one variety of con-
textual information. We include a simple language
model probability, which would additionally rank
e2 based on the probability of the sentence formed
by substiuting e2 for e1 in S. A possible extension
which we do not evaluate might be permitting only
paraphrases that are the same syntactic type as the
original phrase, which we could do by extending the
translation model probabilities to count only phrase
occurrences of that type.
3 Experimental Design
We extracted 46 English phrases to paraphrase
(shown in Table 1), randomly selected from those
multi-word phrases in WordNet which also occured
multiple times in the first 50,000 sentences of our
bilingual corpus. The bilingual corpus that we used
599
Alignment Tool
.
kontrolle
unter
v?llig
kostenentwickl...
diesbez?gliche
die
ist
?brigen
im
.c
o
n
t
r
o
l
u
n
d
e
r
c
o
m
p
l
e
t
e
l
y
i
s
d
y
n
a
m
i
c
c
o
s
t
r
e
l
e
v
a
n
t
t
h
e
,m
o
r
e
i
s
w
h
a
t
(a) Aligning the English phrase to be paraphrased
haben
zu
kontrolle
unter
kosten
die
schuldig
steuerzahlern
den
es
sind
wir
.c
h
e
c
k
i
n
c
o
s
t
s
t
h
e
k
e
e
p
t
o
t
a
x
p
a
y
e
r
s
t
h
e
t
o
i
t
o
w
e
w
e
Alignment Tool
(b) Aligning occurrences of its German translation
Figure 3: Phrases highlighted for manual alignment
was the German-English section of the Europarl cor-
pus, version 2 (Koehn, 2002). We produced auto-
matic alignments for it with the Giza++ toolkit (Och
and Ney, 2003). Because we wanted to test our
method independently of the quality of word align-
ment algorithms, we also developed a gold standard
of word alignments for the set of phrases that we
wanted to paraphrase.
3.1 Manual alignment
The gold standard alignments were created by high-
lighting all occurrences of the English phrase to
paraphrase and manually aligning it with its Ger-
man equivalent by correcting the automatic align-
ment, as shown in Figure 3a. All occurrences of
its German equivalents were then highlighted, and
aligned with their English translations (Figure 3b).
The other words in the sentences were left with their
automatic alignments.
3.2 Paraphrase evaluation
We evaluated the accuracy of each of the para-
phrases that was extracted from the manually
aligned data, as well as the top ranked paraphrases
from the experimental conditions detailed below in
Section 3.3. Because the acccuracy of paraphrases
can vary depending on context, we substituted each
Under control
This situation is in check in terms of security.
This situation is checked in terms of security.
This situation is curbed in terms of security.
This situation is curb in terms of security.
This situation is limit in terms of security.
This situation is slow down in terms of security.
Figure 4: Paraphrases substituted in for the original
phrase
set of candidate paraphrases into between 2?10 sen-
tences which contained the original phrase. Figure 4
shows the paraphrases for under control substituted
into one of the sentences in which it occurred. We
created a total of 289 such evaluation sets, with a
total of 1366 unique sentences created through sub-
stitution.
We had two native English speakers produce
judgments as to whether the new sentences pre-
served the meaning of the original phrase and as to
whether they remained grammatical. Paraphrases
that were judged to preserve both meaning and
grammaticality were considered to be correct, and
examples which failed on either judgment were con-
sidered to be incorrect.
In Figure 4 in check, checked, and curbed were
600
under control checked, curb, curbed, in check, limit, slow down
sooner or later at some point, eventually
military force armed forces, defence, force, forces, military forces, peace-keeping personnel
long ago a little time ago, a long time, a long time ago, a lot of time, a while ago, a while back,
far, for a long time, for some time, for such a long time, long, long period of time, long
term, long time, long while, overdue, some time, some time ago
green light approval, call, go-ahead, indication, message, sign, signal, signals, formal go-ahead
great care a careful approach, greater emphasis, particular attention, special attention, specific
attention, very careful
first half first six months
crystal clear absolutely clear, all clarity, clear, clearly, in great detail, no mistake, no uncertain,
obvious, obviously, particularly clear, perfectly clear, quite clear, quite clearly, quite
explicitly, quite openly, very clear, very clear and comprehensive, very clearly, very
sure, very unclear, very well
carbon dioxide co2
at work at the workplace, employment, held, holding, in the work sphere, operate, organised,
taken place, took place, working
Table 2: Paraphrases extracted from a manually word-aligned parallel corpus
judged to be correct and curb, limit and slow down
were judged to be incorrect. The inter-annotator
agreement for these judgements was measured at
? = 0.605, which is conventionally interpreted as
?good? agreement.
3.3 Experiments
We evaluated the accuracy of top ranked paraphrases
when the paraphrase probability was calculated us-
ing:
1. The manual alignments,
2. The automatic alignments,
3. Automatic alignments produced over multiple
corpora in different languages,
4. All of the above with language model re-
ranking.
5. All of the above with the candidate paraphrases
limited to the same sense as the original phrase.
4 Results
We report the percentage of correct translations (ac-
curacy) for each of these experimental conditions. A
summary of these can be seen in Table 3. This sec-
tion will describe each of the set-ups and the score
reported in more detail.
4.1 Manual alignments
Table 2 gives a set of example paraphrases extracted
from the gold standard alignments. The italicized
paraphrases are those that were assigned the highest
probability by Equation 2, which chooses a single
best paraphrase without regard for context. The 289
sentences created by substituting the italicized para-
phrases in for the original phrase were judged to be
correct an average of 74.9% of the time.
Ignoring the constraint that the new sentences re-
main grammatically correct, these paraphrases were
judged to have the correct meaning 84.7% of the
time. This suggests that the context plays a more
important role with respect to the grammaticality
of substituted paraphrases than with respect to their
meaning.
In order to allow the surrounding words in the sen-
tence to have an influence on which paraphrase was
selected, we re-ranked the paraphrase probabilities
based on a trigram language model trained on the
entire English portion of the Europarl corpus. Para-
phrases were selected from among all those in Table
2, and not constrained to the italicized phrases. In
the case of the paraphrases extracted from the man-
ual word alignments, the language model re-ranking
had virtually no influence, and resulted in a slight
dip in accuracy to 71.7%
601
Paraphrase Prob Paraphrase Prob & LM Correct Meaning
Manual Alignments 74.9 71.7 84.7
Automatic Alignments 48.9 55.3 64.5
Using Multiple Corpora 55.0 57.4 65.4
Word Sense Controlled 57.0 61.9 70.4
Table 3: Paraphrase accuracy and correct meaning for the different data conditions
4.2 Automatic alignments
In this experimental condition paraphrases were ex-
tracted from a set of automatic alignments produced
by running Giza++ over a set of 1,036,000 German-
English sentence pairs (roughly 28,000,000 words in
each language). When the single best paraphrase (ir-
respective of context) was used in place of the orig-
inal phrase in the evaluation sentence the accuracy
reached 48.9% which is quite low compared to the
74.9% of the manually aligned set.
As with the manual alignments it seems that we
are selecting phrases which have the correct mean-
ing but are not grammatical in context. Indeed our
judges thought the meaning of the paraphrases to
be correct in 64.5% of cases. Using a language
model to select the best paraphrase given the con-
text reduces the number of ungrammatical examples
and gives an improvement in quality from 48.9% to
55.3% correct.
These results suggest two things: that improving
the quality of automatic alignments would lead to
more accurate paraphrases, and that there is room
for improvement in limiting the paraphrases by their
context. We address these points below.
4.3 Using multiple corpora
Work in statistical machine translation suggests that,
like many other machine learning problems, perfor-
mance increases as the amount of training data in-
creases. Och and Ney (2003) show that the accuracy
of alignments produced by Giza++ improve as the
size of the training corpus increases.
Since we used the whole of the German-English
section of the Europarl corpus, we could not try
improving the alignments by simply adding more
German-English training data. However, there is
nothing that limits our paraphrase extraction method
to drawing on candidate paraphrases from a sin-
gle target language. We therefore re-formulated the
paraphrase probability to include multiple corpora,
as follows:
e?2 = arg max
e2 6=e1
?
C
?
f in C
p(f |e1)p(e2|f) (5)
where C is a parallel corpus from a set of parallel
corpora.
For this condition we used Giza++ to align
the French-English, Spanish-English, and Italian-
English portions of the Europarl corpus in addition
to the German-English portion, for a total of around
4,000,000 sentence pairs in the training data.
The accuracy of paraphrases extracted over mul-
tiple corpora increased to 55%, and further to 57.4%
when the language model re-ranking was included.
4.4 Controlling for word sense
As mentioned in Section 1, the way that we extract
paraphrases is the converse of the methodology em-
ployed in word sense disambiguation work that uses
parallel corpora (Diab and Resnik, 2002). The as-
sumption made in the word sense disambiguation
work is that if a source language word aligns with
different target language words then those words
may represent different word senses. This can be
observed in the paraphrases for at work in Table 2.
The paraphrases at the workplace, employment, and
in the work sphere are a different sense of the phrase
than operate, held, and holding, and they are aligned
with different German phrases.
When we calculate the paraphrase probability we
sum over different target language phrases. There-
fore the English phrases that are aligned with the dif-
ferent German phrases (which themselves maybe in-
dicative of different word senses) are mingled. Per-
formance may be degraded since paraphrases that
reflect different senses of the original phrase, and
which therefore have a different meaning, are in-
cluded in the same candidate set.
602
We therefore performed an experiment to see
whether improvement could be had by limiting the
candidate paraphrases to be the same sense as the
original phrase in each test sentence. To do this,
we used the fact that our test sentences were drawn
from a parallel corpus. We limited phrases to the
same word sense by constraining the candidate para-
phrases to those that aligned with the same target
language phrase. Our basic paraphrase calculation
was therefore:
p(e2|e1, f) = p(f |e1)p(e2|f) (6)
Using the foreign language phrase to identify the
word sense is obviously not applicable in monolin-
gual settings, but acts as a convenient stand-in for a
proper word sense disambiguation algorithm here.
When word sense is controlled in this way, the
accuracy of the paraphrases extracted from the au-
tomatic alignments raises dramatically from 48.9%
to 57% without language model re-ranking, and fur-
ther to 61.9% when language model re-ranking was
included.
5 Related Work
Barzilay and McKeown (2001) extract both single-
and multiple-word paraphrases from a monolingual
parallel corpus. They co-train a classifier to iden-
tify whether two phrases were paraphrases of each
other based on their surrounding context. Two dis-
advantages of this method are that it requires iden-
tical bounding substrings, and has bias towards sin-
gle words. For an evaluation set of 500 paraphrases,
they report an average precision of 86% at identi-
fying paraphrases out of context, and of 91% when
the paraphrases are substituted into the original con-
text of the aligned sentence. The results of our sys-
tems are not directly comparable, since Barzilay and
McKeown (2001) evaluated their paraphrases with a
different set of criteria (they asked judges whether
to judge paraphrases based on ?approximate con-
ceptual equivalence?). Furthermore, their evaluation
was carried out only by substituting the paraphrase
in for the phrase with the identical context, and not
in for arbitrary occurrences of the original phrase, as
we have done.
Lin and Pantel (2001) use a standard (non-
parallel) monolingual corpus to generate para-
phrases, based on dependancy graphs and distribu-
tional similarity. One strong disadvantage of this
method is that their paraphrases can also have op-
posite meanings.
Ibrahim et al (2003) combine the two approaches:
aligned monolingual corpora and parsing. They
evaluated their system with human judges who were
asked whether the paraphrases were ?roughly inter-
changeable given the genre?, scored an average of
41% on a set of 130 paraphrases, with the judges
all agreeing 75% of the time, and a correlation of
0.66. The shortcomings of this method are that it is
dependent upon parse quality, and is limited by the
rareness of the data.
Pang et al (2003) use parse trees over sentences in
monolingual parallel corpus to identify paraphrases
by grouping similar syntactic constituents. They
use heuristics such as keyword checking to limit
the over-application of this method. Our alignment
method might be an improvement of their heuris-
tics for choosing which constituents ought to be
grouped.
6 Discussion and Future Work
In this paper we have introduced a novel method for
extracting paraphrases, which we believe greatly in-
creases the usefulness of paraphrasing in NLP ap-
plications. The advantages of our method are that
it:
? Produces a ranked list of high quality para-
phrases with associated probabilities, from
which the best paraphrase can be chosen ac-
cording to the target context. We have shown
how a language model can be used to select the
best paraphrase for a particular context from
this list.
? Straightforwardly handles multi-word units.
Whereas for previous approaches the evalua-
tion has been performed over mostly single
word paraphrases, our results are reported ex-
clusively over units of between 2 and 4 words.
? Because we use a much more abundant source
of data, our method can be used for a much
wider range of text genres than previous ap-
proaches, namely any for which parallel data
is available.
603
One crucial thing to note is that we have demon-
strated our paraphrases to be of higher quality when
the alignments used to produce them are improved.
This means that our method will reap the benefits
of research that improvements to automatic align-
ment techniques (Callison-Burch et al, 2004), and
will further improve as more parallel data becomes
available.
In the future we plan to:
? Investigate whether our re-ranking can be fur-
ther improved by using a syntax-based lan-
guage model.
? Formulate a paraphrase probability for senten-
tial paraphrases, and use this to try to identify
paraphrases across documents in order to con-
dense information for multi-document summa-
rization.
? See whether paraphrases can be used to in-
crease coverage for statistical machine trans-
lation when translating into ?low-density? lan-
guages which have small parallel corpora.
Acknowledgments
The authors would like to thank Beatrice Alex,
Marco Kuhlmann, and Josh Schroeder for their valu-
able input as well as their time spent annotating and
contributing to the software.
References
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Chris Callison-Burch, David Talbot, and Miles Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In Proceedings of
ACL.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora. In Proceedings of the Second International
Workshop on Paraphrasing (ACL 2003).
Lidija Iordanskaja, Richard Kittredge, and Alain Polge?re.
1991. Lexical selection and paraphrase in a meaning-
text generation model. In Ce?cile L. Paris, William R.
Swartout, and William C. Mann, editors, Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics. Kluwer Academic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished
Draft.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbia?s Newsblaster. In Pro-
ceedings of the Human Language Technology Confer-
ence.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proceed-
ings of EMNLP.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical machine translation sys-
tem. In Proceedings of MT Summit 9.
604
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177?180,
Prague, June 2007. c?2007 Association for Computational Linguistics 
Moses: Open Source Toolkit for Statistical Machine Translation 
Philipp Koehn 
Hieu Hoang  
Alexandra Birch 
Chris Callison-Burch 
University of Edin-
burgh1 
Marcello Federico 
Nicola Bertoldi 
ITC-irst2 
Brooke Cowan 
Wade Shen 
Christine Moran 
MIT3 
Richard Zens 
RWTH Aachen4 
Chris Dyer 
University of Maryland5 
 
Ond?ej Bojar 
Charles University6 
Alexandra Constantin 
Williams College7 
Evan Herbst 
Cornell8 
1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk. 
2{federico, bertoldi}@itc.it. 3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu. 4 
zens@i6.informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 
07aec_2@williams.edu. 8 evh4@cornell.edu 
 
Abstract 
We describe an open-source toolkit for sta-
tistical machine translation whose novel 
contributions are (a) support for linguisti-
cally motivated factors, (b) confusion net-
work decoding, and (c) efficient data for-
mats for translation models and language 
models. In addition to the SMT decoder, 
the toolkit also includes a wide variety of 
tools for training, tuning and applying the 
system to many translation tasks.  
1 Motivation 
Phrase-based statistical machine translation 
(Koehn et al 2003) has emerged as the dominant 
paradigm in machine translation research. How-
ever, until now, most work in this field has been 
carried out on proprietary and in-house research 
systems. This lack of openness has created a high 
barrier to entry for researchers as many of the 
components required have had to be duplicated. 
This has also hindered effective comparisons of the 
different elements of the systems. 
By providing a free and complete toolkit, we 
hope that this will stimulate the development of the 
field. For this system to be adopted by the commu-
nity, it must demonstrate performance that is com-
parable to the best available systems. Moses has 
shown that it achieves results comparable to the 
most competitive and widely used statistical ma-
chine translation systems in translation quality and 
run-time (Shen et al 2006). It features all the ca-
pabilities of the closed sourced Pharaoh decoder 
(Koehn 2004). 
Apart from providing an open-source toolkit 
for SMT, a further motivation for Moses is to ex-
tend phrase-based translation with factors and con-
fusion network decoding. 
The current phrase-based approach to statisti-
cal machine translation is limited to the mapping of 
small text chunks without any explicit use of lin-
guistic information, be it morphological, syntactic, 
or semantic. These additional sources of informa-
tion have been shown to be valuable when inte-
grated into pre-processing or post-processing steps. 
Moses also integrates confusion network de-
coding, which allows the translation of ambiguous 
input. This enables, for instance, the tighter inte-
gration of speech recognition and machine transla-
tion. Instead of passing along the one-best output 
of the recognizer, a network of different word 
choices may be examined by the machine transla-
tion system. 
Efficient data structures in Moses for the 
memory-intensive translation model and language 
model allow the exploitation of much larger data 
resources with limited hardware. 
177
 2 Toolkit 
The toolkit is a complete out-of-the-box trans-
lation system for academic research. It consists of 
all the components needed to preprocess data, train 
the language models and the translation models. It 
also contains tools for tuning these models using 
minimum error rate training (Och 2003) and evalu-
ating the resulting translations using the BLEU 
score (Papineni et al 2002).  
Moses uses standard external tools for some of 
the tasks to avoid duplication, such as GIZA++ 
(Och and Ney 2003) for word alignments and 
SRILM for language modeling.  Also, since these 
tasks are often CPU intensive, the toolkit has been 
designed to work with Sun Grid Engine parallel 
environment to increase throughput.  
In order to unify the experimental stages, a 
utility has been developed to run repeatable ex-
periments. This uses the tools contained in Moses 
and requires minimal changes to set up and cus-
tomize. 
The toolkit has been hosted and developed un-
der sourceforge.net since inception. Moses has an 
active research community and has reached over 
1000 downloads as of 1st March 2007.  
The main online presence is at  
http://www.statmt.org/moses/ 
where many sources of information about the 
project can be found. Moses was the subject of this 
year?s Johns Hopkins University Workshop on 
Machine Translation (Koehn et al 2006). 
The decoder is the core component of Moses. 
To minimize the learning curve for many research-
ers, the decoder was developed as a drop-in re-
placement for Pharaoh, the popular phrase-based 
decoder. 
In order for the toolkit to be adopted by the 
community, and to make it easy for others to con-
tribute to the project, we kept to the following 
principles when developing the decoder: 
? Accessibility 
? Easy to Maintain 
? Flexibility 
? Easy for distributed team development 
? Portability 
It was developed in C++ for efficiency and fol-
lowed modular, object-oriented design. 
3 Factored Translation Model 
Non-factored SMT typically deals only with 
the surface form of words and has one phrase table, 
as shown in Figure 1. 
i am buying you a green cat
using phrase dictionary:
i
 am buying
you
a
green
cat
je
ach?te
vous
un
vert
chat
a une
je vous ach?te un chat vert
Translate:
 
In factored translation models, the surface 
forms may be augmented with different factors, 
such as POS tags or lemma. This creates a factored 
representation of each word, Figure 2.  
1 1 1 / sing /
                                 
je vous achet un chat
PRO PRO VB ART NN
je vous acheter un chat
st st st present masc masc
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?
1 1 / 1 sing sing
i buy you a cat
PRO VB PRO ART NN
i tobuy you a cat
st st present st
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?  
 
Mapping of source phrases to target phrases 
may be decomposed into several steps. Decompo-
sition of the decoding process into various steps 
means that different factors can be modeled sepa-
rately. Modeling factors in isolation allows for 
flexibility in their application. It can also increase 
accuracy and reduce sparsity by minimizing the 
number dependencies for each step. 
For example, we can decompose translating 
from surface forms to surface forms and lemma, as 
shown in Figure 3. 
Figure 2. Factored translation 
Figure 1. Non-factored translation 
178
  
Figure 3. Example of graph of decoding steps 
By allowing the graph to be user definable, we 
can experiment to find the optimum configuration 
for a given language pair and available data.  
The factors on the source sentence are consid-
ered fixed, therefore, there is no decoding step 
which create source factors from other source fac-
tors. However, Moses can have ambiguous input in 
the form of confusion networks. This input type 
has been used successfully for speech to text 
translation (Shen et al 2006). 
Every factor on the target language can have its 
own language model. Since many factors, like 
lemmas and POS tags, are less sparse than surface 
forms, it is possible to create a higher order lan-
guage models for these factors. This may encour-
age more syntactically correct output. In Figure 3 
we apply two language models, indicated by the 
shaded arrows, one over the words and another 
over the lemmas. Moses is also able to integrate 
factored language models, such as those described 
in (Bilmes and Kirchhoff 2003) and (Axelrod 
2006). 
4 Confusion Network Decoding 
Machine translation input currently takes the 
form of simple sequences of words. However, 
there are increasing demands to integrate machine 
translation technology into larger information 
processing systems with upstream NLP/speech 
processing tools (such as named entity recognizers, 
speech recognizers, morphological analyzers, etc.). 
These upstream processes tend to generate multiple, 
erroneous hypotheses with varying confidence. 
Current MT systems are designed to process only 
one input hypothesis, making them vulnerable to 
errors in the input.  
In experiments with confusion networks, we 
have focused so far on the speech translation case, 
where the input is generated by a speech recog-
nizer. Namely, our goal is to improve performance 
of spoken language translation by better integrating 
speech recognition and machine translation models. 
Translation from speech input is considered more 
difficult than translation from text for several rea-
sons. Spoken language has many styles and genres, 
such as, formal read speech, unplanned speeches, 
interviews, spontaneous conversations; it produces 
less controlled language, presenting more relaxed 
syntax and spontaneous speech phenomena. Fi-
nally, translation of spoken language is prone to 
speech recognition errors, which can possibly cor-
rupt the syntax and the meaning of the input. 
There is also empirical evidence that better 
translations can be obtained from transcriptions of 
the speech recognizer which resulted in lower 
scores. This suggests that improvements can be 
achieved by applying machine translation on a 
large set of transcription hypotheses generated by 
the speech recognizers and by combining scores of 
acoustic models, language models, and translation 
models. 
Recently, approaches have been proposed for 
improving translation quality through the process-
ing of multiple input hypotheses. We have imple-
mented in Moses confusion network decoding as 
discussed in (Bertoldi and Federico 2005), and de-
veloped a simpler translation model and a more 
efficient implementation of the search algorithm. 
Remarkably, the confusion network decoder re-
sulted in an extension of the standard text decoder. 
5 Efficient Data Structures for Transla-
tion Model and Language Models 
With the availability of ever-increasing 
amounts of training data, it has become a challenge 
for machine translation systems to cope with the 
resulting strain on computational resources. Instead 
of simply buying larger machines with, say, 12 GB 
of main memory, the implementation of more effi-
cient data structures in Moses makes it possible to 
exploit larger data resources with limited hardware 
infrastructure. 
A phrase translation table easily takes up giga-
bytes of disk space, but for the translation of a sin-
gle sentence only a tiny fraction of this table is 
needed. Moses implements an efficient representa-
tion of the phrase translation table. Its key proper-
ties are a prefix tree structure for source words and 
on demand loading, i.e. only the fraction of the 
phrase table that is needed to translate a sentence is 
loaded into the working memory of the decoder. 
179
 For the Chinese-English NIST  task, the mem-
ory requirement of the phrase table is reduced from 
1.7 gigabytes to less than 20 mega bytes, with no 
loss in translation quality and speed (Zens and Ney 
2007). 
The other large data resource for statistical ma-
chine translation is the language model. Almost 
unlimited text resources can be collected from the 
Internet and used as training data for language 
modeling. This results in language models that are 
too large to easily fit into memory. 
The Moses system implements a data structure 
for language models that is more efficient than the 
canonical SRILM (Stolcke 2002) implementation 
used in most systems. The language model on disk 
is also converted into this binary format, resulting 
in a minimal loading time during start-up of the 
decoder.  
An even more compact representation of the 
language model is the result of the quantization of 
the word prediction and back-off probabilities of 
the language model. Instead of representing these 
probabilities with 4 byte or 8 byte floats, they are 
sorted into bins, resulting in (typically) 256 bins 
which can be referenced with a single 1 byte index. 
This quantized language model, albeit being less 
accurate, has only minimal impact on translation 
performance (Federico and Bertoldi 2006). 
6 Conclusion and Future Work 
This paper has presented a suite of open-source 
tools which we believe will be of value to the MT 
research community. 
We have also described a new SMT decoder 
which can incorporate some linguistic features in a 
consistent and flexible framework. This new direc-
tion in research opens up many possibilities and 
issues that require further research and experimen-
tation. Initial results show the potential benefit of 
factors for statistical machine translation, (Koehn 
et al 2006) and (Koehn and Hoang 2007). 
References 
Axelrod, Amittai. "Factored Language Model for Sta-
tistical Machine Translation." MRes Thesis. 
Edinburgh University, 2006. 
Bertoldi, Nicola, and Marcello Federico. "A New De-
coder for Spoken Language Translation Based 
on Confusion Networks." Automatic Speech 
Recognition and Understanding Workshop 
(ASRU), 2005. 
Bilmes, Jeff A, and Katrin Kirchhoff. "Factored Lan-
guage Models and Generalized Parallel Back-
off." HLT/NACCL, 2003. 
Koehn, Philipp. "Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation 
Models." AMTA, 2004. 
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola 
Bertoldi, Ondrej Bojar, Chris Callison-Burch, 
Brooke Cowan, Chris Dyer, Hieu Hoang, 
Richard Zens, Alexandra Constantin, Christine 
Corbett Moran, and Evan Herbst. "Open 
Source Toolkit for Statistical Machine Transla-
tion". Report of the 2006 Summer Workshop at 
Johns Hopkins University, 2006. 
Koehn, Philipp, and Hieu Hoang. "Factored Translation 
Models." EMNLP, 2007. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
"Statistical Phrase-Based Translation." 
HLT/NAACL, 2003. 
Och, Franz Josef. "Minimum Error Rate Training for 
Statistical Machine Translation." ACL, 2003. 
Och, Franz Josef, and Hermann Ney. "A Systematic 
Comparison of Various Statistical Alignment 
Models." Computational Linguistics 29.1 
(2003): 19-51. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. "BLEU: A Method for Automatic 
Evaluation of Machine Translation." ACL, 
2002. 
Shen, Wade, Richard Zens, Nicola Bertoldi, and 
Marcello Federico. "The JHU Workshop 2006 
Iwslt System." International Workshop on Spo-
ken Language Translation, 2006. 
Stolcke, Andreas. "SRILM an Extensible Language 
Modeling Toolkit." Intl. Conf. on Spoken Lan-
guage Processing, 2002. 
Zens, Richard, and Hermann Ney. "Efficient Phrase-
Table Representation for Machine Translation 
with Applications to Online MT and Speech 
Recognition." HLT/NAACL, 2007. 
180
Bootstrapping Parallel Corpora
Chris Callison-Burch
School of Informatics
University of Edinburgh
callison-burch@ed.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Abstract
We present two methods for the automatic cre-
ation of parallel corpora. Whereas previous
work into the automatic construction of parallel
corpora has focused on harvesting them from
the web, we examine the use of existing paral-
lel corpora to bootstrap data for new language
pairs. First, we extend existing parallel cor-
pora using co-training, wherein machine trans-
lations are selectively added to training corpora
with multiple source texts. Retraining transla-
tion models yields modest improvements. Sec-
ond, we simulate the creation of training data
for a language pair for which a parallel corpus
is not available. Starting with no human trans-
lations from German to English we produce a
German to English translation model with 45%
accuracy using parallel corpora in other lan-
guages. This suggests the method may be use-
ful in the creation of parallel corpora for lan-
guages with scarce resources.
1 Introduction
Statistical translation models (such as those formulated in
Brown et al (1993)) are trained from bilingual sentence-
aligned texts. The bilingual data used for constructing
translation models is often gathered from government
documents produced in multiple languages. For exam-
ple, the Candide system (Berger et al, 1994) was trained
on ten years? worth of Canadian Parliament proceed-
ings, which consists of 2.87 million parallel sentences
in French and English. While the Candide system was
widely regarded as successful, its success is not indica-
tive of the potential for statistical translation between ar-
bitrary language pairs. The reason for this is that collec-
tions of parallel texts as large as the Canadian Hansards
are rare.
Al-Onaizan et al (2000) explains in simple terms the
reasons that using large amounts of training data en-
sures translation quality: if a program sees a partic-
ular word or phrase one thousand times during train-
ing, it is more likely to learn a correct translation than
if sees it ten times, or once, or never. Increasing the
amount of training material therefore leads to improved
quality. This is illustrated in Figure 1, which plots
translation accuracy (measured as 100 minus word er-
ror rate) for French?English, German?English, and
Spanish?English translation models trained on incre-
mentally larger parallel corpora. The quality of the
translations produced by each system increases over the
100,000 training items, and the graph suggests the the
trend would continue if more data were added. Notice
that the rate of improvement is slow: after 90,000 manu-
ally provided training sentences pairs, we only see a 4-6%
change in performance. Sufficient performance for sta-
tistical models may therefore only come when we have
access to many millions of aligned sentences.
One approach that has been proposed to address the
problem of limited training data is to harvest the web for
bilingual texts (Resnik, 1998). The STRAND method au-
tomatically gathers web pages that are potential transla-
tions of each other by looking for documents in one lan-
guage which have links whose text contains the name of
another language. For example, if an English web page
had a link with the text ?Espan?ol? or ?en Espan?ol? then
the page linked to is treated as a candidate translation of
the English page. Further checks verify the plausibility
of its being a translation (Smith, 2002).
Instead of attempting to gather new translations from
the web, we describe an alternate method for automat-
ically creating parallel corpora. Specifically, we exam-
ine the use of existing translations as a resource to boot-
strap more training data, and to create data for new lan-
guage pairs. We generate translation models from exist-
ing data and use them to produce translations of new sen-
4446485052545658606264 1000
0
20000
30000
40000
50000
60000
70000
80000
90000
10000
0
Accuracy (100 - Word Error Rate)
Trainin
g Corp
us Siz
e (numb
er of se
ntence p
airs)
Germa
n
Frenc
h
Spanis
h
Figure 1: Translation accuracy plotted against training
corpus size
tences. Incorporating this machine-created parallel data
to the original set, and retraining the translation models
improves the translation accuracy. To perform the retrain-
ing we use co-training (Blum and Mitchell, 1998; Abney,
2002) which is a weakly supervised learning technique
that relies on having distinct views of the items being
classified. The views that we employ for co-training are
multiple source documents.
Section 2 motivates the use of weakly supervised learn-
ing, and introduces co-training for machine translation.
Section 3 reports our experimental results. One experi-
ment shows that co-training can modestly benefit trans-
lation systems trained from similarly sized corpora. A
second experiment shows that co-training can have a dra-
matic benefit when the size of initial training corpora are
mismatched. This suggests that co-training for statisti-
cal machine translation is especially useful for languages
with impoverished training corpora. Section 4 discusses
the implications of our experiments, and discusses ways
which our methods might be used more practically.
2 Co-training for Statistical Machine
Translation
Most statistical natural language processing tasks use su-
pervised machine learning, meaning that they require
training data that contains examples that have been an-
notated with some sort of labels. Two conflicting factors
make this reliance on annotated training data a problem:
? The accuracy of machine learning improves as more
data is available (as we have shown for statistical
machine translation in Figure 1).
? Annotated training data usually has some cost asso-
ciated with its creation. This cost can often be sub-
stantial, as with the Penn Treebank (Marcus et al,
1993).
There has recently been considerable interest in weakly
supervised learning within the statistical NLP commu-
nity. The goal of weakly supervised learning is to reduce
the cost of creating new annotated corpora by (semi-) au-
tomating the process.
Co-training is a weakly supervised learning techniques
which uses an initially small amount of human labeled
data to automatically bootstrap larger sets of machine la-
beled training data. In co-training implementations mul-
tiple learners are used to label new examples and re-
trained on some of each other?s labeled examples. The
use of multiple learners increases the chance that use-
ful information will be added; an example which is eas-
ily labeled by one learner may be difficult for the other
and therefore adding the confidently labeled example will
provide information in the next round of training.
Self-training is a weakly supervised method in which
a single learner retrains on the labels that it applies to
unlabeled data itself. We describe its application to
machine translation in order to clarify how co-training
would work. In self-training a translation model would be
trained for a language pair, say German?English, from
a German-English parallel corpus. It would then produce
English translations for a set of German sentences. The
machine translated German-English sentences would be
added to the initial bilingual corpus, and the translation
model would be retrained.
Co-training for machine translation is slightly more
complicated. Rather than using a single translation
model to translate a monolingual corpus, it uses mul-
tiple translation models to translate a bi- or multi-
lingual corpus. For example, translation models could
be trained for German?English, French?English and
Spanish?English from appropriate bilingual corpora,
and then used to translate a German-French-Spanish par-
allel corpus into English. Since there are three candidate
English translations for each sentence alignment, the best
translation out of the three can be selected and used to
retrain the models. The process is illustrated in Figure 2.
Co-training thus automatically increases the size of
parallel corpora. There are a number of reasons why
machine translated items added during co-training can be
useful in the next round of training:
? vocabulary acquisition ? One problem that arises
from having a small training corpus is incomplete
word coverage. Without a word occurring in its
training corpus it is unlikely that a translation model
will produce a reasonable translation of it. Because
the initial training corpora can come from different
sources, a collection of translation models will be
more likely to have encountered a word before. This
Maison bleu Casa azulblaues Haus
???
Blue
maison
blaues
House
Blue house
2
Maison bleu Casa azulblaues Haus
Blue house
Blue
maison
blaues
Haus
Blue house
3
French German
Spanish
English target
4
French
some english sentence
some french sentenc
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
German English
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
Spanish English
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
1
English
French
some english sentence
some french sentenc
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
English
Maison
bleu
Blue
house
+
Spanish
some english sentence
some french sentenc
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
English
Casa azul
Blue
house
+
blaues
Haus
Blue
house
+
German
some english sentence
some french sentenc
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
English
Figure 2: Co-training using German, French, and Spanish sources to produce English machine translations
leads to vocabulary acquisition during co-training.
? coping with morphology ? The problem mentioned
above is further exacerbated by the fact that most
current statistical translation formulations have an
incomplete treatment of morphology. This would be
a problem if the training data for a Spanish transla-
tion model contained the masculine form of a adjec-
tive, but not the feminine. Because languages vary
in how they use morphology (some languages have
grammatical gender whereas others don?t) one lan-
guage?s translation model might have the translation
of a particular word form whereas another?s would
not. Thus co-training can increase the inventory of
word forms and reduce the problem that morphol-
ogy poses to simple statistical translation models.
? improved word order ? A significant source of er-
rors in statistical machine translation is the word re-
ordering problem (Och et al, 1999). The word or-
der between related languages is often similar while
word order between distant language may differ sig-
nificantly. By including more examples through co-
training with related languages, the translation mod-
els for distant languages will better learn word order
mappings to the target language.
In all these cases the diversity afforded by multiple trans-
lation models increases the chances that the machine
translated sentences added to the initial bilingual corpora
will be accurate. Our co-training algorithm allows many
source languages to be used.
3 Experimental Results
In order to conduct co-training experiments we first
needed to assemble appropriate corpora. The corpus used
in our experiments was assembled from the data used in
the (Och and Ney, 2001) multiple source translation pa-
per. The data was gathered from the Bulletin of the Eu-
ropean Union which is published on the Internet in the
eleven official languages of the European Union. We
used a subset of the data to create a multi-lingual cor-
pus, aligning sentences between French, Spanish, Ger-
man, Italian and Portuguese (Simard, 1999). Addition-
ally we created bilingual corpora between English and
each of the five languages using sentences that were not
included in the multi-lingual corpus.
Och and Ney (2001) used the data to find a transla-
tion that was most probable given multiple source strings.
Och and Ney found that multi-source translations using
two source languages reduced word error rate when com-
pared to using source strings from a single language.
For multi-source translations using source strings in six
languages a greater reduction in word error rate was
achieved. Our work is similar in spirit, although instead
of using multi-source translation at the time of transla-
tion, we integrate it into the training stage. Whereas
Och and Ney use multiple source strings to improve the
quality of one translation only, our co-training method at-
tempts to improve the accuracy of all translation models
by bootstrapping more training data from multiple source
documents.
3.1 Software
The software that we used to train the statistical mod-
els and to produce the translations was GIZA++ (Och
and Ney, 2000), the CMU-Cambridge Language Model-
ing Toolkit (Clarkson and Rosenfeld, 1997), and the ISI
ReWrite Decoder. The sizes of the language models used
in each experiment were fixed throughout, in order to en-
sure that any gains that were made were not due to the
trivial reason of the language model improving (which
could be done by building a larger monolingual corpus of
the target language).
The experiments that we conducted used GIZA++ to
produce IBM Model 4 translation models. It should be
observed, however, that our co-training algorithm is en-
tirely general and may be applied to any formulation of
statistical machine translation which relies on parallel
Round Number
Translation Pair 0 1 2 3
French?English 55.2 56.3 57.0 55.5
Spanish?English 57.2 57.8 57.6 56.9
German?English 45.1 46.3 47.4 47.6
Italian?English 53.8 54.0 53.6 53.5
Portuguese?Eng 55.2 55.2 55.7 54.3
Table 1: Co-training results over three rounds
corpora for its training data.
3.2 Evaluation
The performance of translation models was evaluated us-
ing a held-out set of 1,000 sentences in each language,
with reference translations into English. Each translation
model was used to produce translation of these sentences
and the machine translations were compared to the ref-
erence human translations using word error rate (WER).
The results are reported in terms of increasing accuracy,
rather than decreasing error. We define accuracy as 100
minus WER.
Other evaluation metrics such as position independent
WER or the Bleu method (Papineni et al, 2001) could
have been used. While WER may not be the best measure
of translation quality, it is sufficient to track performance
improvements in the following experiments.
3.3 Co-training
Table 1 gives the result of co-training using the most
accurate translation from the candidate translations pro-
duced by five translation models. Each translation model
was initially trained on bilingual corpora consisting of
around 20,000 human translated sentences. These trans-
lation models were used to translate 63,000 sentences, of
which the top 10,000 were selected for the first round.
At the next round 53,000 sentences were translated and
the top 10,000 sentences were selected for the second
round. The final candidate pool contained 43,000 trans-
lations and again the top 10,000 were selected. The table
indicates that gains may be had from co-training. Each
of the translation models improves over its initial training
size at some point in the co-training. The German to En-
glish translation model improves the most ? exhibiting a
2.5% improvement in accuracy.
The table further indicates that co-training for ma-
chine translation suffers the same problem reported in
Pierce and Cardie (2001): gains above the accuracy of
the initial corpus are achieved, but decline as after a cer-
tain number of machine translations are added to the
training set. This could be due in part to the manner
in items are selected for each round. Because the best
translations are transferred from the candidate pool to the
2727.52828.52929.530 100
00
15000
20000
25000
30000
35000
40000
Accuracy (100 - Word Error Rate)
Trainin
g Corp
us Siz
e (numb
er of se
ntence p
airs)Coachin
g of G
erman
Figure 3: ?Coaching? of German to English by a French
to English translation model
43.84444.244.444.644.84545.2 100
000
15000
0
20000
0
25000
0
30000
0
35000
0
40000
0
Accuracy (100 - Word Error Rate)
Trainin
g Corp
us Siz
e (numb
er of se
ntence p
airs)Coachin
g of G
erman
Figure 4: ?Coaching? of German to English by multiple
translation models
training pool at each round the number of ?easy? trans-
lations diminishes over time. Because of this, the av-
erage accuracy of the training corpora decreased with
each round, and the amount of noise being introduced
increased. The accuracy gains from co-training might
extend for additional rounds if the size of the candidate
pool were increased, or if some method were employed
to reduce the amount of noise being introduced.
3.4 Coaching
In order to simulate using co-training for language pairs
without extensive parallel corpora, we experimented with
a variation on co-training for machine translation that
we call ?coaching?. It employs two translation models
of vastly different size. In this case we used a French
to English translation model built from 60,000 human
translated sentences and a German to English translation
model that contained no human translated sentences. The
German-English translation model was meant to repre-
sent a language pair with extremely impoverished paral-
lel corpus. Coaching is therefore a special case of co-
training in that one view (the superior one) never retrains
upon material provided by the other (inferior) view.
A German-English parallel corpus was created by tak-
ing a French-German parallel corpus, translating the
French sentences into English and then aligning the trans-
lations with the German sentences. In this experiment the
machine translations produced by the French?English
translation model were always selected. Figure 3 shows
the performance of the resulting German to English trans-
lation model for various sized machine produced parallel
corpora.
We explored this method further by translating 100,000
sentences with each of the non-German translation mod-
els from the co-training experiment in Section 3.3. The
result was a German-English corpus containing 400,000
sentence pairs. The performance of the resulting model
matches the initial accuracy of the model. Thus machine-
translated corpora achieved equivalent quality to human-
translated corpora after two orders of magnitude more
data was added.
The graphs illustrate that increasing the performance
of translation models may be achievable using machine
translations alone. Rather than the 2.5% improvement
gained in co-training experiments wherein models of sim-
ilar sizes were used, coaching achieves an 18%(+) im-
provement by pairing translation models of radically dif-
ferent sizes.
4 Discussion and Future Work
In this paper we presented two methods for the automatic
creation of additional parallel corpora. Co-training uses a
number of different human translated parallel corpora to
create additional data for each of them, leading to modest
increases in translation quality. Coaching uses existing
resources to create a fully machine translated corpora ?
essentially reverse engineering the knowledge present in
the human translated corpora and transferring that to an-
other language. This has significant implications for the
feasibility of using statistical translation methods for lan-
guage pairs for which extensive parallel corpora do not
exist.
A setting in which this would become extremely use-
ful is if the European Union extends membership to a
new country like Turkey, and wants develop translation
resources for its language. One can imagine that sizable
parallel corpora might be available between Turkish and a
few EU languages like Greek and Italian. However, there
may be no parallel corpora between Turkish and Finnish.
Our methods could exploit existing parallel corpora be-
tween the current EU language and use machine transla-
tions from Greek and Italian in order to create a machine
translation system between Turkish and Finnish.
We plan to extend our work by moving from co-
training and its variants to another weakly supervised
learning method, active learning. Active learning incor-
porates human translations along with machine transla-
tions, which should ensure better resulting quality than
using machine translations alone. It will reduce the cost
of creating a parallel corpus entirely by hand, by selec-
tively and judiciously querying a human translator. In
order to make the most effective use of the human trans-
lator?s time we will be required to design an effective se-
lection algorithm, which is something that was neglected
in our current research. An effective selection algorithm
for active learning will be one which chooses those exam-
ples which will add the most information to the machine
translation system, and therefore minimizes the amount
of time a human needs to spend translating sentences.
References
Steve Abney. 2002. Bootstrapping. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics.
Yaser Al-Onaizan, Ulrich Germann, Ulf Hermjakob,
Kevin Knight, Philipp Koehn, Daniel Marcu, and Ya-
mada Kenji. 2000. Translating with scarce resources.
In Proceedings of the National Conference on Artificial
Intelligence (AAAI).
Adam Berger, Peter Brown, Stephen Della Pietra, Vin-
cent Della Pietra, John Gillett, John Lafferty, Robert
Mercer, Harry Printz, and Lubos Ures. 1994. The
Candide system for machine translation.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the Workshop on Computational Learning The-
ory.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Compuata-
tional Linguistics, 19(2):263?311, June.
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge toolkit.
In ESCA Eurospeech Proceedings.
Mitchell P. Marcus, Beatrice Santori, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19.
Franz Joseph Och and Herman Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447, Hong Kong, October.
Franz Joseph Och and Herman Ney. 2001. Statistical
multi-source translation. In MT Summit 2001, pages
253?258, Santiago de Compostela, Spain, September.
Franz Joseph Och, Christop Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the Joint Confer-
ence of Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora, College Park, Mary-
land, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report,
September.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing.
Philip Resnik. 1998. Parallel strands: A preliminary
investigation into mining the web for bilingual text.
In Third Conference of the Association for Machine
Translation in the Americas.
Michel Simard. 1999. Text-translation alignment:
Aligning three or more versions of a text. In Jean
Veronis, editor, Parallel Text Processing. Kluwer Aca-
demic.
Noah Smith. 2002. From words to corpora: Recognizing
translation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
Philadelphia, Pennsylvania.
Proceedings of the Workshop on Statistical Machine Translation, pages 154?157,
New York City, June 2006. c?2006 Association for Computational Linguistics
Constraining the Phrase-Based, Joint Probability Statistical Translation
Model
Alexandra Birch Chris Callison-Burch Miles Osborne Philipp Koehn
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
a.c.birch-mayne@sms.ed.ac.uk
Abstract
The joint probability model proposed by
Marcu and Wong (2002) provides a strong
probabilistic framework for phrase-based
statistical machine translation (SMT). The
model?s usefulness is, however, limited by
the computational complexity of estimat-
ing parameters at the phrase level. We
present the first model to use word align-
ments for constraining the space of phrasal
alignments searched during Expectation
Maximization (EM) training. Constrain-
ing the joint model improves performance,
showing results that are very close to state-
of-the-art phrase-based models. It also al-
lows it to scale up to larger corpora and
therefore be more widely applicable.
1 Introduction
Machine translation is a hard problem because of
the highly complex, irregular and diverse nature
of natural languages. It is impossible to accurately
model all the linguistic rules that shape the trans-
lation process, and therefore a principled approach
uses statistical methods to make optimal decisions
given incomplete data.
The original IBM Models (Brown et al, 1993)
learn word-to-word alignment probabilities which
makes it computationally feasible to estimate
model parameters from large amounts of train-
ing data. Phrase-based SMT models, such as the
alignment template model (Och, 2003), improve
on word-based models because phrases provide
local context which leads to better lexical choice
and more reliable local reordering. However, most
phrase-based models extract their phrase pairs
from previously word-aligned corpora using ad-
hoc heuristics. These models perform no search
for optimal phrasal alignments. Even though this
is an efficient strategy, it is a departure from the
rigorous statistical framework of the IBM Models.
Marcu and Wong (2002) proposed the joint
probability model which directly estimates the
phrase translation probabilities from the corpus in
a theoretically governed way. This model neither
relies on potentially sub-optimal word alignments
nor on heuristics for phrase extraction. Instead, it
searches the phrasal alignment space, simultane-
ously learning translation lexicons for both words
and phrases. The joint model has been shown to
outperform standard models on restricted data sets
such as the small data track for Chinese-English in
the 2004 NIST MT Evaluation (Przybocki, 2004).
However, considering all possible phrases and
all their possible alignments vastly increases the
computational complexity of the joint model when
compared to its word-based counterpart. In this
paper, we propose a method of constraining the
search space of the joint model to areas where
most of the unpromising phrasal alignments are
eliminated and yet as many potentially useful
alignments as possible are still explored. The
joint model is constrained to phrasal alignments
which do not contradict a set high confidence word
alignments for each sentence. These high con-
fidence alignments could incorporate information
from both statistical and linguistic sources. In this
paper we use the points of high confidence from
the intersection of the bi-directional Viterbi word
alignments to constrain the model, increasing per-
formance and decreasing complexity.
154
2 Translation Models
2.1 Standard Phrase-based Model
Most phrase-based translation models (Och, 2003;
Koehn et al, 2003; Vogel et al, 2003) rely on
a pre-existing set of word-based alignments from
which they induce their parameters. In this project
we use the model described by Koehn et al (2003)
which extracts its phrase alignments from a corpus
that has been word aligned. From now on we re-
fer to this phrase-based translation model as the
standard model. The standard model decomposes
the foreign input sentence F into a sequence of
I phrases f1, . . . , f I . Each foreign phrase fi is
translated to an English phrase ei using the prob-
ability distribution ?(f i|ei). English phrases may
be reordered using a relative distortion probability.
This model performs no search for optimal
phrase pairs. Instead, it extracts phrase pairs
(f i, ei) in the following manner. First, it uses the
IBM Models to learn the most likely word-level
Viterbi alignments for English to Foreign and For-
eign to English. It then uses a heuristic to recon-
cile the two alignments, starting from the points
of high confidence in the intersection of the two
Viterbi alignments and growing towards the points
in the union. Points from the union are selected if
they are adjacent to points from the intersection
and their words are previously unaligned.
Phrases are then extracted by selecting phrase
pairs which are ?consistent? with the symmetrized
alignment, which means that all words within the
source language phrase are only aligned to the
words of the target language phrase and vice versa.
Finally the phrase translation probability distribu-
tion is estimated using the relative frequencies of
the extracted phrase pairs.
This approach to phrase extraction means that
phrasal alignments are locked into the sym-
metrized alignment. This is problematic because
the symmetrization process will grow an align-
ment based on arbitrary decisions about adjacent
words and because word alignments inadequately
represent the real dependencies between transla-
tions.
2.2 Joint Probability Model
The joint model (Marcu and Wong, 2002), does
not rely on a pre-existing set of word-level align-
ments. Like the IBM Models, it uses EM to align
and estimate the probabilities for sub-sentential
units in a parallel corpus. Unlike the IBM Mod-
els, it does not constrain the alignments to being
single words.
The joint model creates phrases from words and
commonly occurring sequences of words. A con-
cept, ci, is defined as a pair of aligned phrases
< ei, f i >. A set of concepts which completely
covers the sentence pair is denoted by C. Phrases
are restricted to being sequences of words which
occur above a certain frequency in the corpus.
Commonly occurring phrases are more likely to
lead to the creation of useful phrase pairs, and
without this restriction the search space would be
much larger.
The probability of a sentence and its translation
is the sum of all possible alignments C, each of
which is defined as the product of the probability
of all individual concepts:
p(F,E) =
?
C?C
?
<ei,f i>?C
p(< ei, f i >) (1)
The model is trained by initializing the trans-
lation table using Stirling numbers of the second
kind to efficiently estimate p(< ei, f i >) by cal-
culating the proportion of alignments which con-
tain p(< ei, f i >) compared to the total number
of alignments in the sentence (Marcu and Wong,
2002). EM is then performed by first discovering
an initial phrasal alignments using a greedy algo-
rithm similar to the competitive linking algorithm
(Melamed, 1997). The highest probability phrase
pairs are iteratively selected until all phrases are
are linked. Then hill-climbing is performed by
searching once for each iteration for all merges,
splits, moves and swaps that improve the proba-
bility of the initial phrasal alignment. Fractional
counts are collected for all alignments visited.
Training the IBM models is computationally
challenging, but the joint model is much more de-
manding. Considering all possible segmentations
of phrases and all their possible alignments vastly
increases the number of possible alignments that
can be formed between two sentences. This num-
ber is exponential with relation to the length of the
shorter sentence.
3 Constraining the Joint Model
The joint model requires a strategy for restricting
the search for phrasal alignments to areas of the
alignment space which contain most of the proba-
bility mass. We propose a method which examines
155
phrase pairs that are consistent with a set of high
confidence word alignments defined for the sen-
tence. The set of alignments are taken from the in-
tersection of the bi-directional Viterbi alignments.
This strategy for extracting phrase pairs is simi-
lar to that of the standard phrase-based model and
the definition of ?consistent? is the same. How-
ever, the constrained joint model does not lock
the search into a heuristically derived symmetrized
alignment. Joint model phrases must also occur
above a certain frequency in the corpus to be con-
sidered.
The constraints on the model are binding during
the initialization phase of training. During EM,
inconsistent phrase pairs are given a small, non-
zero probability and are thus not considered un-
less unaligned words remain after linking together
high probability phrase pairs. All words must be
aligned, there is no NULL alignment like in the
IBM models.
By using the IBM Models to constrain the joint
model, we are searching areas in the phrasal align-
ment space where both models overlap. We com-
bine the advantage of prior knowledge about likely
word alignments with the ability to perform a
probabilistic search around them.
4 Experiments
All data and software used was from the NAACL
2006 Statistical Machine Translation workshop
unless otherwise indicated.
4.1 Constraints
The unconstrained joint model becomes in-
tractable with very small amounts of training data.
On a machine with 2 Gb of memory, we were
only able to train 10,000 sentences of the German-
English Europarl corpora. Beyond this, pruning is
required to keep the model in memory during EM.
Table 1 shows that the application of the word con-
straints considerably reduces the size of the space
of phrasal alignments that is searched. It also im-
proves the BLEU score of the model, by guiding it
to explore the more promising areas of the search
space.
4.2 Scalability
Even though the constrained joint model reduces
complexity, pruning is still needed in order to scale
up to larger corpora. After the initialization phase
of the training, all phrase pairs with counts less
Unconstrained Constrained
No. Concepts 6,178k 1,457k
BLEU 19.93 22.13
Time(min) 299 169
Table 1. The impact of constraining the joint model
trained on 10,000 sentences of the German-English
Europarl corpora and tested with the Europarl test set
used in Koehn et al (2003)
than 10 million times that of the phrase pair with
the highest count, are pruned from the phrase ta-
ble. The model is also parallelized in order to
speed up training.
The translation models are included within a
log-linear model (Och and Ney, 2002) which al-
lows a weighted combination of features func-
tions. For the comparison of the basic systems
in Table 2 only three features were used for both
the joint and the standard model: p(e|f), p(f |e)
and the language model, and they were given equal
weights.
The results in Table 2 show that the joint model
is capable of training on large data sets, with a
reasonable performance compared to the standard
model. However, here it seems that the standard
model has a slight advantage. This is almost cer-
tainly related to the fact that the joint model results
in a much smaller phrase table. Pruning eliminates
many phrase pairs, but further investigations indi-
cate that this has little impact on BLEU scores.
BLEU Size
Joint Model 25.49 2.28
Standard Model 26.15 19.04
Table 2. Basic system comparisons: BLEU scores
and model size in millions of phrase pairs for Spanish-
English
The results in Table 3 compare the joint and the
standard model with more features. Apart from
including all Pharaoh?s default features, we use
two new features for both the standard and joint
models: a 5-gram language model and a lexical-
ized reordering model as described in Koehn et al
(2005). The weights of the feature functions, or
model components, are set by minimum error rate
training provided by David Chiang from the Uni-
versity of Maryland.
On smaller data sets (Koehn et al, 2003) the
joint model shows performance comparable to the
standard model, however the joint model does
not reach the level of performance of the stan-
156
EN-ES ES-EN
Joint
3-gram, dl4 20.51 26.64
5-gram, dl6 26.34 27.17
+ lex. reordering 26.82 27.80
Standard Model
5-gram, dl6
+ lex. reordering 31.18 31.86
Table 3. Bleu scores for the joint model and the stan-
dard model showing the effect of the 5-gram language
model, distortion length of 6 (dl) and the addition of
lexical reordering for the English-Spanish and Spanish-
English tasks.
dard model for this larger data set. This could
be due to the fact that the joint model results in
a much smaller phrase table. During EM only
phrase pairs that occur in an alignment visited dur-
ing hill-climbing are retained. Only a very small
proportion of the alignment space can be searched
and this reduces the chances of finding optimum
parameters. The small number of alignments vis-
ited would lead to data sparseness and over-fitting.
Another factor could be efficiency trade-offs like
the fast but not optimal competitive linking search
for phrasal alignments.
4.3 German-English submission
We also submitted a German-English system using
the standard approach to phrase extraction. The
purpose of this submission was to validate the syn-
tactic reordering method that we previously pro-
posed (Collins et al, 2005). We parse the Ger-
man training and test corpus and reorder it accord-
ing to a set of manually devised rules. Then, we
use our phrase-based system with standard phrase-
extraction, lexicalized reordering, lexical scoring,
5-gram LM, and the Pharaoh decoder.
On the development test set, the syntactic re-
ordering improved performance from 26.86 to
27.70. The best submission in last year?s shared
task achieved a score of 24.77 on this set.
5 Conclusion
We presented the first attempt at creating a system-
atic framework which uses word alignment con-
straints to guide phrase-based EM training. This
shows competitive results, to within 0.66 BLEU
points for the basic systems, suggesting that a
rigorous probabilistic framework is preferable to
heuristics for extracting phrase pairs and their
probabilities.
By introducing constraints to the alignment
space we can reduce the complexity of the joint
model and increase its performance, allowing it to
train on larger corpora and making the model more
widely applicable.
For the future, the joint model would benefit
from lexical weighting like that used in the stan-
dard model (Koehn et al, 2003). Using IBM
Model 1 to extract a lexical alignment weight for
each phrase pair would decrease the impact of data
sparseness, and other kinds smoothing techniques
will be investigated. Better search algorithms for
Viterbi phrasal alignments during EM would in-
crease the number and quality of model parame-
ters.
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005.
Clause restructuring for statistical machine translation. In
Proceedings of ACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL, pages 127?133.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne,
and Chris Callison-Burch. 2005. Edinburgh system de-
scription. In IWSLT Speech Translation Evaluation.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine translation.
In Proceedings of EMNLP.
Dan Melamed. 1997. A word-to-word model of translational
equivalence. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In ACL.
Franz Josef Och. 2003. Statistical Machine Translation:
From Single-Word Models to Alignment Templates. Ph.D.
thesis, RWTH Aachen Department of Computer Science,
Aachen, Germany.
Mark Przybocki. 2004. NIST 2004 machine translation eval-
uation results. Confidential e-mail to workshop partici-
pants, May.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,
Ashish Venugopal, Bing Zhao, and Alex Waibel. 2003.
The CMU statistical machine translation system. In Ma-
chine Translation Summit.
157
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 97?104
Manchester, August 2008
ParaMetric: An Automatic Evaluation Metric for Paraphrasing
Chris Callison-Burch
Center for Speech and Language Processing
Johns Hopkins University
3400 N. Charles St.
Baltimore, MD 21218
Trevor Cohn Mirella Lapata
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Abstract
We present ParaMetric, an automatic eval-
uation metric for data-driven approaches to
paraphrasing. ParaMetric provides an ob-
jective measure of quality using a collec-
tion of multiple translations whose para-
phrases have been manually annotated.
ParaMetric calculates precision and recall
scores by comparing the paraphrases dis-
covered by automatic paraphrasing tech-
niques against gold standard alignments of
words and phrases within equivalent sen-
tences. We report scores for several estab-
lished paraphrasing techniques.
1 Introduction
Paraphrasing is useful in a variety of natural lan-
guage processing applications including natural
language generation, question answering, multi-
document summarization and machine translation
evaluation. These applications require paraphrases
for a wide variety of domains and language us-
age. Therefore building hand-crafted lexical re-
sources such as WordNet (Miller, 1990) would be
far too laborious. As such, a number of data-driven
approaches to paraphrasing have been developed
(Lin and Pantel, 2001; Barzilay and McKeown,
2001; Barzilay and Lee, 2003; Pang et al, 2003;
Quirk et al, 2004; Bannard and Callison-Burch,
2005). Despite this spate of research, no objective
evaluation metric has been proposed.
In absence of a repeatable automatic evaluation,
the quality of these paraphrasing techniques was
gauged using subjective manual evaluations. Sec-
tion 2 gives a survey of the various evaluation
methodologies used in previous research. It has
not been possible to directly compare paraphrasing
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
techniques, because each one was evaluated using
its own idiosyncratic experimental design. More-
over, because these evaluations were performed
manually, they are difficult to replicate.
We introduce an automatic evaluation metric,
called ParaMetric, which uses paraphrasing tech-
niques to be compared and enables an evaluation
to be easily repeated in subsequent research. Para-
Metric utilizes data sets which have been annotated
with paraphrases. ParaMetric compares automatic
paraphrases against reference paraphrases.
In this paper we:
? Present a novel automatic evaluation metric
for data-driven paraphrasing methods;
? Describe how manual alignments are cre-
ated by annotating correspondences between
words in multiple translations;
? Show how phrase extraction heuristics from
statistical machine translation can be used to
enumerate paraphrases from the alignments;
? Report ParaMetric scores for a number of ex-
isting paraphrasing methods.
2 Related Work
No consensus has been reached with respect to the
proper methodology to use when evaluating para-
phrase quality. This section reviews past methods
for paraphrase evaluation.
Researchers usually present the quality of their
automatic paraphrasing technique in terms of a
subjective manual evaluation. These have used
a variety of criteria. For example, Barzilay
and McKeown (2001) evaluated their paraphrases
by asking judges whether paraphrases were ?ap-
proximately conceptually equivalent.? Ibrahim
et al (2003) asked judges whether their para-
phrases were ?roughly interchangeable given the
genre.? Bannard and Callison-Burch (2005) re-
placed phrases with paraphrases in a number of
97
sentences and asked judges whether the substi-
tutions ?preserved meaning and remained gram-
matical.? These subjective evaluations are rather
vaguely defined and not easy to reproduce.
Others evaluate paraphrases in terms of whether
they improve performance on particular tasks.
Callison-Burch et al (2006b) measure improve-
ments in translation quality in terms of Bleu score
(Papineni et al, 2002) and in terms of subjective
human evaluation when paraphrases are integrated
into a statistical machine translation system. Lin
and Pantel (2001) manually judge whether a para-
phrase might be used to answer questions from the
TREC question-answering track. To date, no one
has used task-based evaluation to compare differ-
ent paraphrasing methods. Even if such an eval-
uation were performed, it is unclear whether the
results would hold for a different task. Because of
this, we strive for a general evaluation rather than
a task-specific one.
Dolan et al (2004) create a set of manual word
alignments between pairs of English sentences.
We create a similar type of data, as described in
Section 4. Dolan et al use heuristics to draw pairs
of English sentences from a comparable corpus
of newswire articles, and treat these as potential
paraphrases. In some cases these sentence pairs
are good examples of paraphrases, and in some
cases they are not. Our data differs because it
is drawn from multiple translations of the same
foreign sentences. Barzilay (2003) suggested that
multiple translations of the same foreign source
text were a perfect source for ?naturally occur-
ring paraphrases? because they are samples of text
which convey the same meaning but are produced
by different writers. That being said, it may be
possible to use Dolan et als data toward a similar
end. Cohn et al (to appear) compares the use of
the multiple translation corpus with the MSR cor-
pus for this task.
The work described here is similar to work in
summarization evaluation. For example, in the
Pyramid Method (Nenkova et al, 2007) content
units that are similar across human-generated sum-
maries are hand-aligned. These can have alter-
native wordings, and are manually grouped. The
idea of capturing these and building a resource for
evaluating summaries is in the same spirit as our
methodology.
3 Challenges for Evaluating Paraphrases
Automatically
There are several problems inherent to automati-
cally evaluating paraphrases. First and foremost,
developing an exhaustive list of paraphrases for
any given phrase is difficult. Lin and Pantel (2001)
illustrate the difficulties that people have generat-
ing a complete list of paraphrases, reporting that
they missed many examples generated by a sys-
tem that were subsequently judged to be correct. If
a list of reference paraphrases is incomplete, then
using it to calculate precision will give inaccurate
numbers. Precision will be falsely low if the sys-
tem produces correct paraphrases which are not in
the reference list. Additionally, recall is indeter-
minable because there is no way of knowing how
many correct paraphrases exist.
There are further impediments to automatically
evaluating paraphrases. Even if we were able to
come up with a reasonably exhaustive list of para-
phrases for a phrase, the acceptability of each para-
phrase would vary depending on the context of
the original phrase (Szpektor et al, 2007). While
lexical and phrasal paraphrases can be evaluated
by comparing them against a list of known para-
phrases (perhaps customized for particular con-
texts), this cannot be naturally done for struc-
tural paraphrases which may transform whole sen-
tences.
We attempt to resolve these problems by hav-
ing annotators indicate correspondences in pairs
of equivalent sentences. Rather than having peo-
ple enumerate paraphrases, we asked that they per-
form the simper task of aligning paraphrases. Af-
ter developing these manual ?gold standard align-
ments? we can gauge how well different automatic
paraphrases are at aligning paraphrases within
equivalent sentences. By evaluating the perfor-
mance of paraphrasing techniques at alignment,
rather than at matching a list of reference para-
phrases, we obviate the need to have a complete
set of paraphrases.
We describe how sets of reference paraphrases
can be extracted from the gold standard align-
ments. While these sets will obviously be frag-
mentary, we attempt to make them more complete
by aligning groups of equivalent sentences rather
than only pairs. The paraphrase sets that we extract
are appropriate for the particular contexts. More-
over they may potentially be used to study struc-
tural paraphrases, although we do not examine that
98
an
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
.
resign
to
him
want
others
while
,
him
impeach
to
propose
people
some
a
n
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
.
resignation
his
tender
to
him
want
who
those
and
him
impeaching
propose
who
those
are
there
.
voluntarily
office
leave
to
him
want
some
and
him
against
indictment
an
proposing
are
some
a
n
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
Figure 1: Pairs of English sentences were aligned
by hand. Black squares indicate paraphrase corre-
spondences.
in this paper.
4 Manually Aligning Paraphrases
We asked monolingual English speakers to align
corresponding words and phrases across pairs of
equivalent English sentences. The English sen-
tences were equivalent because they were transla-
tions of the same foreign language text created by
different human translators. Our annotators were
instructed to align parts of the sentences which
had the same meaning. Annotators were asked
to prefer smaller one-to-one word alignments, but
were allowed to create one-to-many and many-to-
many alignments where appropriate. They were
given a set of annotation guidelines covering spe-
cial cases such as repetition, pronouns, genitives,
phrasal verbs and omissions (Callison-Burch et al,
2006a). The manual correspondences are treated
as gold standard alignments.
We use a corpus that contains eleven En-
glish translations of Chinese newswire documents,
which were commissioned from different transla-
tion agencies by the Linguistics Data Consortium
1
.
The data was created for the Bleu machine trans-
lation evaluation metric (Papineni et al, 2002),
which uses multiple translations as a way of cap-
turing allowable variation in translation. Whereas
the Bleu metric requires no further information,
our method requires a one-off annotation to explic-
itly show which parts of the multiple translations
constitute paraphrases.
The rationale behind using a corpus with eleven
translations was that a greater number of transla-
tions would likely result in a greater number of
paraphrases for each phrase. Figure 1 shows the
alignments that were created between one sen-
tence and three of its ten corresponding transla-
tions. Table 1 gives a list of non-identical words
and phrases that can be paired by way of the word
alignments. These are the basic paraphrases con-
tained within the three sentence pairs. Each phrase
has up to three paraphrases. The maximum num-
ber of paraphrases for a given span in each sen-
tence is bounded by the number of equivalent sen-
tences that it is paired with.
In addition to these basic paraphrases, longer
paraphrases can also be obtained using the heuris-
tic presented in Och and Ney (2004) for extract-
ing phrase pairs (PP) from word alignments A, be-
tween a foreign sentence f
J
1
and an English sen-
1
See LDC catalog number 2002T01.
99
some some people, there are those who
want propose, are proposing
to impeach an indictment against, impeach-
ing
and while
others some, those who
expect want
step down resign, leave office voluntarily,
tender his resignation
Table 1: Non-identical words and phrases which
are identified as being in correspondence by the
alignments in Figure 1.
tence e
I
1
:
PP (f
J
1
, e
I
1
, A) = {(f
j+m
j
, e
i+n
i
) :
?(i
?
, j
?
) ? A : j ? j
?
? j + m ? i ? i
?
? i + n
??(i
?
, j
?
) ? A : j ? j
?
? j + m? ? i ? i
?
? i + n}
When we apply the phrase extraction heuris-
tic to aligned English sentences, we add the con-
straint f
j+m
j
6= e
i+n
i
to exclude phrases that are
identical. This heuristic would allow ?some peo-
ple propose to impeach him,? ?some are proposing
an indictment against him,? and ?there are those
who propose impeaching him? to be extracted
as paraphrases of ?some want to impeach him.?
The heuristic extracts a total of 142 non-identical
phrase pairs from the three sentences given in Fig-
ure 1.
For the results reported in this paper, annotators
aligned 50 groups of 10 pairs of equivalent sen-
tences, for a total of 500 sentence pairs. These
were assembled by pairing the first of the LDC
translations with the other ten (i.e. 1-2, 1-3, 1-4,
..., 1-11). The choice of pairing one sentence with
the others instead of doing all pairwise combina-
tions was made simply because the latter would
not seem to add much information. However, the
choice of using the first translator as the key was
arbitrary.
Annotators corrected a set of automatic word
alignments that were created using Giza++ (Och
and Ney, 2003), which was trained on a total of
109,230 sentence pairs created from all pairwise
combinations of the eleven translations of 993 Chi-
nese sentences.
The average amount of time spent on each of
the sentence pairs was 77 seconds, with just over
eleven hours spent to annotate all 500 sentence
pairs. Although each sentence pair in our data
set was annotated by a single annotator, Cohn et
al. (to appear) analyzed the inter-annotator agree-
ment for randomly selected phrase pairs from the
same corpus, and found inter-annotator agreement
of
?
C = 0.85 over the aligned words and
?
C = 0.63
over the alignments between basic phrase pairs,
where
?
C is measure of inter-rater agreement in the
style of Kupper and Hafner (1989).
5 ParaMetric Scores
We can exploit the manually aligned data to com-
pute scores in two different fashions. First, we
can calculate how well an automatic paraphrasing
technique is able to align the paraphrases in a sen-
tence pair. Second, we can calculate the lower-
bound on precision for a paraphrasing technique
and its relative recall by enumerating the para-
phrases from each of the sentence groups. The first
of these score types does not require groups of sen-
tences, only pairs.
We calculate alignment accuracy by comparing
the manual alignments for the sentence pairs in the
test corpus with the alignments that the automatic
paraphrasing techniques produce for the same
sentence pairs. We enumerate all non-identical
phrase pairs within the manually word-aligned
sentence pairs and within the automatically word
aligned sentence pairs using PP . We calculate the
precision and recall of the alignments by taking
the intersection of the paraphrases extracted from
the manual alignments M , and the paraphrases
extracted from a system?s alignments S:
Align
Prec
=
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
, S) ? PP (e
1
, e
2
,M)|
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
, S)|
Align
Recall
=
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
, S) ? PP (e
1
, e
2
,M)|
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
,M)|
Where e
1
, e
2
are pairs of English sentence from
the test corpus.
Measuring a paraphrasing method?s perfor-
mance on the task of aligning the paraphrases is
somewhat different than what most paraphrasing
methods do. Most methods produce a list of para-
phrases for a given input phrase, drawing from
a large set of rules or a corpus larger than our
small test set. We therefore also attempt to mea-
sure precision and recall by comparing the set of
100
paraphrases that method M produces for phrase p
that occurs in sentence s. We denote this set as
para
M
(p, s), where s is an optional argument for
methods that constrain their paraphrases based on
context.
Our reference sets of paraphrases are generated
in a per group fashion. We enumerate the reference
paraphrases for phrase p in sentence s in group G
as
para
REF
(p
1
, s
1
, G) =
{p
2
: ?(p
1
, p
2
) ?
?
<s
1
,s
2
,A>?G
PP (s
1
, s
2
, A)}
The maximum size of this set is the number of
sentence pairs in G. Because this set of reference
paraphrases is incomplete, we can only calculate
a lower bound on the precision of a paraphrasing
method and its recall relative to the reference
paraphrases. We call these LB-Precision and
Rel-Recall and calculate them as follows:
LB-Precision =
?
<s,G>?C
?
p?s
|para
M
(p, s) ? para
REF
(p
1
, s,G)|
|para
M
(p, s)|
Rel-Recall =
?
<s,G>?C
?
p?s
|para
M
(p, s) ? para
REF
(p
1
, s,G)|
|para
REF
(p
1
, s,G)|
For these metrics we require the test corpus
C to be a held-out set and restrict the automatic
paraphrasing techniques from drawing paraphrases
from it. The idea is instead to see how well these
techniques are able to draw paraphrases from the
other sources of data which they would normally
use.
6 Paraphrasing Techniques
There are a number of established methods for
extracting paraphrases from data. We describe
the following methods in this section and evaluate
them in the next:
? Pang et al (2003) used syntactic alignment to
merge parse trees of multiple translations,
? Quirk et al (2004) treated paraphrasing as
monolingual statistical machine translation,
? Bannard and Callison-Burch (2005) used
bilingual parallel corpora to extract para-
phrases.
S
NP VP
NN
persons
AUX
were
CD
12
VP
VB
killed
S
NP VP
NN
people
VB
died
CD
twelve
VB
NP VP
CD NN
12
twelve
people
persons
...
were
...
died
...
killed
AUX VP
BEG END
12
twelve
people
persons
died
were
killed
Tree 1 Tree 2
+
Parse Forest
Word Lattice
Merge
Linearize
Figure 2: Pang et al (2003) created word graphs
by merging parse trees. Paths with the same start
and end nodes are treated as paraphrases.
Pang et al (2003) use multiple translations to
learn paraphrases using a syntax-based alignment
algorithm, illustrated in Figure 2. Parse trees were
merged into forests by grouping constituents of the
same type (for example, the two NPs and two VPs
are grouped). Parse forests were mapped onto fi-
nite state word graphs by creating alternative paths
for every group of merged nodes. Different paths
within the resulting word lattice are treated as para-
phrases of each other. For example, in the word lat-
tice in Figure 2, people were killed, persons died,
persons were killed, and people died are all possi-
ble paraphrases of each other.
Quirk et al (2004) treated paraphrasing as
?monolingual statistical machine translation.?
They created a ?parallel corpus? containing pairs
of English sentences by drawing sentences with a
low edit distance from news articles that were writ-
ten about the same topic on the same date, but pub-
lished by different newspapers. They formulated
the problem of paraphrasing in probabilistic terms
in the same way it had been defined in the statisti-
cal machine translation literature:
e?
2
= argmax
e
2
p(e
2
|e
1
)
= argmax
e
2
p(e
1
|e
2
)p(e
2
)
101
I do not believe in mutilating dead bodies
 
cad?veresno soy partidaria mutilarde
cad?veres de inmigrantes ilegales ahogados a la playatantosarrojaEl mar ...
corpsesSo many of drowned illegals get washed up on beaches ...
Figure 3: Bannard and Callison-Burch (2005) ex-
tracted paraphrases by equating English phrases
that share a common translation.
Where p(e
1
|e
2
) is estimated by training word
alignment models over the ?parallel corpus? as in
the IBM Models (Brown et al, 1993), and phrase
translations are extracted from word alignments as
in the Alignment Template Model (Och, 2002).
Bannard and Callison-Burch (2005) also used
techniques from statistical machine translation to
identify paraphrases. Rather than drawing pairs
of English sentences from a comparable corpus,
Bannard and Callison-Burch (2005) used bilingual
parallel corpora. They identified English para-
phrases by pivoting through phrases in another lan-
guage. They located foreign language translations
of an English phrase, and treated the other En-
glish translations of those foreign phrases as poten-
tial paraphrases. Figure 3 illustrates how a Span-
ish phrase can be used as a point of identifica-
tion for English paraphrases in this way. Bannard
and Callison-Burch (2005) defined a paraphrase
probability p(e
2
|e
1
) in terms of the translation
model probabilities p(f |e
1
) and p(e
2
|f). Since e
1
can translate as multiple foreign language phrases,
they sum over f , and since multiple parallel cor-
pora can be used they summed over each parallel
corpus C:
e?
2
= arg max
e
2
6=e
1
p(e
2
|e
1
)
? arg max
e
2
6=e
1
?
C
?
f in C
p(f |e
1
)p(e
2
|f)
7 Comparing Paraphrasing Techniques
with ParaMetric
7.1 Training data for word alignments
In order to calculate Align
Prec
and Align
Recall
for the different paraphrasing techniques, we had
them automatically align the 500 manually aligned
sentence pairs in our test sets.
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
Align
Prec
.62 .65 .73
Align
Recall
.11 .10 .46
LB-Precision .14 .33 .68
Rel-Recall .07 .03 .01
Table 2: Summary results for scoring the different
paraphrasing techniques using our proposed auto-
matic evaluations.
Bo Pang provided syntactic alignments for the
500 sentence pairs. The word lattices combine the
groups of sentences. When measuring alignment
quality, we took pains to try to limit the extracted
phrase pairs to those which occurred in each sen-
tence pair, but we acknowledge that our methodol-
ogy may be flawed.
We created training data for the monolingual
statistical machine translation method using all
pairwise combination of eleven English transla-
tions in LDC2002T01. All combinations of the
eleven translations of the 993 sentences in that
corpus resulted in 109,230 sentence pairs with
3,266,769 words on each side. We used this data
to train an alignment model, and applied it to the
500 sentence pairs in our test set.
We used the parallel corpus method to align
each pair of English sentences by creating interme-
diate alignments through their Chinese source sen-
tences. The bilingual word alignment model was
trained on a Chinese-English parallel corpus from
the NIST MT Evaluation consisting of 40 million
words. This was used to align the 550 Chinese-
English sentence pairs constructed from the test
set.
7.2 Training data for precision and recall
Each of the paraphrasing methods generated para-
phrases for LB-Precision and Rel-Recall us-
ing larger training sets of data than for the align-
ments. For the syntax-based alignment method,
we excluded the 50 word lattices corresponding
to the test set. We used the remaining 849 lat-
tices for the LDC multiple translation corpus.
For the monolingual statistical machine transla-
tion method, we downloaded the Microsoft Re-
search Paraphrase Phrase Table, which contained
paraphrases for nearly 9 million phrases, gener-
102
ated from the method described in Quirk et al
(2004). For the parallel corpus method, we de-
rived paraphrases from the entire Europarl corpus,
which contains parallel corpora between English
and 10 other languages, with approximately 30
million words per language. We limited both the
Quirk et al (2004) and the Bannard and Callison-
Burch (2005) paraphrases to those with a probabil-
ity greater than or equal to 1%.
7.3 Results
Table 2 gives a summary of how each of the para-
phrasing techniques scored using the four different
automatic metrics. The precision of their align-
ments was in the same ballpark, with each para-
phrasing method reaching above 60%. The mono-
lingual SMT method vastly outstripped the others
in terms of recall and therefore seems to be the
best on the simplified task of aligning paraphrases
within pairs of equivalent sentences.
For the task of generating paraphrases from un-
restricted resources, the monolingual SMT method
again had the highest precision, although time
time its recall was quite low. The 500 manually
aligned sentence pairs contained 14,078 unique
paraphrases for phrases of 5 words or less. The
monolingual SMT method only posited 230 para-
phrases with 156 of them being correct. By con-
trast, the syntactic alignment method posited 1,213
with 399 correct, and the parallel corpus method
posited 6,914 with 998 correct. Since the refer-
ence lists are incomplete by their very nature, the
LB-Precision score gives a lower-bound on the
precision, and the Rel-Recall gives recall only
with respect to the partial list of paraphrases.
Table 3 gives the performance of the differ-
ent paraphrasing techniques for different phrase
lengths.
8 Conclusions
In this paper we defined a number of automatic
scores for data-driven approaches to paraphrasing,
which we collectively dub ?ParaMetric?. We dis-
cussed the inherent difficulties in automatically as-
sessing paraphrase quality. These are due primar-
ily to the fact that it is exceedingly difficult to
create an exhaustive list of paraphrases. To ad-
dress this problem, we introduce an artificial task
of aligning paraphrases within pairs of equivalent
English sentences, which guarantees accurate pre-
cision and recall numbers. In order to measure
alignment quality, we create a set of gold standard
alignments. While the creation of this data does
require some effort, it seems to be a manageable
amount, and the inter-annotator agreement seems
reasonable.
Since alignment is not perfectly matched with
what we would like automatic paraphrasing tech-
niques to do, we also use the gold standard align-
ment data to measure a lower bound on the preci-
sion of a method?s paraphrases, as well as its recall
relative to the limited set of paraphrases. Future
studies should examine how well these scores rank
different paraphrasing methods when compared to
human judgments. Follow up work should inves-
tigate the number of equivalent English sentences
that are required for reasonably complete lists of
paraphrases. In this work we aligned sets of eleven
different English sentences, but we acknowledge
that such a data set is rare and might make it dif-
ficult to port this method to other domains or lan-
guages.
The goal of this work is to develop a set of
scores that both allows different paraphrasing tech-
niques to be compared objectively and provides an
easily repeatable method for automatically evalu-
ating paraphrases. This has hitherto not been pos-
sible. The availability of an objective, automatic
evaluation metric for paraphrasing has the poten-
tial to impact research in the area in a number of
ways. It not only allows for the comparison of dif-
ferent approaches to paraphrasing, as shown in this
paper, but also provides a way to tune the parame-
ters of a single system in order to optimize its qual-
ity.
Acknowledgments
The authors are grateful to Bo Pang for providing
the word lattices from her method, to Stefan Rie-
zler for his comments on an early draft of this pa-
per, and to Michelle Bland for proofreading. This
work was supported by the National Science Foun-
dation under Grant No. 0713448. The views and
findings are the authors? alone.
References
Bannard, Colin and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL-2005), Ann Ar-
bor, Michigan.
Barzilay, Regina and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
103
Align
Prec
Align
Recall
LB-Precision Rel-Recall
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
Length = 1 .54 .48 .64 .24 .18 .56 .15 .25 .59 .20 .16 .02
Length ? 2 .56 .56 .69 .19 .13 .52 .15 .31 .66 .18 .10 .03
Length ? 3 .59 .60 .71 .14 .12 .49 .15 .32 .66 .13 .06 .02
Length ? 4 .60 .63 .72 .12 .11 .48 .14 .33 .68 .09 .04 .01
Length ? 5 .62 .65 .73 .11 .10 .46 .14 .33 .68 .07 .03 .01
Table 3: Results for paraphrases of continuous subphrases of various lengths.
ing multiple-sequence alignment. In Proceedings of
HLT/NAACL-2003, Edmonton, Alberta.
Barzilay, Regina and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2001).
Barzilay, Regina. 2003. Information Fusion for Mutli-
document Summarization: Paraphrasing and Gener-
ation. Ph.D. thesis, Columbia University, New York.
Brown, Peter, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer. 1993. The mathematics
of machine translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?311, June.
Callison-Burch, Chris, Trevor Cohn, and Mirella Lap-
ata. 2006a. Annotation guidelines for paraphrase
alignment. Tech report, University of Edinburgh.
Callison-Burch, Chris, Philipp Koehn, and Miles Os-
borne. 2006b. Improved statistical machine
translation using paraphrases. In Proceedings of
HLT/NAACL-2006, New York, New York.
Cohn, Trevor, Chris Callison-Burch, and Mirella Lap-
ata. to appear. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Compu-
tational Linguistics.
Dolan, Bill, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics.
Ibrahim, Ali, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proceedings of the Second Inter-
national Workshop on Paraphrasing (ACL 2003).
Kupper, Lawrence L. and Kerry B. Hafner. 1989. On
assessing interrater agreement for multiple attribute
responses. Biometrics, 45(3):957?967.
Lin, Dekang and Patrick Pantel. 2001. Discovery of
inference rules from text. Natural Language Engi-
neering, 7(3):343?360.
Miller, George A. 1990. Wordnet: An on-line lexical
database. Special Issue of the International Journal
of Lexicography, 3(4).
Nenkova, Ani, Rebecca Passonneau, and Kathleen
McKeown. 2007. The pyramid method: incorporat-
ing human content selection variation in summariza-
tion evaluation. ACM Transactions on Speech and
Language Processing, 4(2).
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Och, Franz Josef. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen Department of
Computer Science, Aachen, Germany.
Pang, Bo, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences.
In Proceedings of HLT/NAACL-2003, Edmonton,
Alberta.
Papineni, Kishore, Salim Roukos, ToddWard, andWei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2002), Philadelphia, Penn-
sylvania.
Quirk, Chris, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2004), Barcelona, Spain.
Szpektor, Idan, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
2007), Prague, Czech Republic.
104
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 196?205,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Syntactic Constraints on Paraphrases Extracted from Parallel Corpora
Chris Callison-Burch
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, Maryland
ccb cs jhu edu
Abstract
We improve the quality of paraphrases ex-
tracted from parallel corpora by requiring that
phrases and their paraphrases be the same syn-
tactic type. This is achieved by parsing the En-
glish side of a parallel corpus and altering the
phrase extraction algorithm to extract phrase
labels alongside bilingual phrase pairs. In or-
der to retain broad coverage of non-constituent
phrases, complex syntactic labels are intro-
duced. A manual evaluation indicates a 19%
absolute improvement in paraphrase quality
over the baseline method.
1 Introduction
Paraphrases are alternative ways of expressing the
same information. Being able to identify or gen-
erate paraphrases automatically is useful in a wide
range of natural language applications. Recent work
has shown how paraphrases can improve question
answering through query expansion (Riezler et al,
2007), automatic evaluation of translation and sum-
marization by modeling alternative lexicalization
(Kauchak and Barzilay, 2006; Zhou et al, 2006;
Owczarzak et al, 2006), and machine translation
both by dealing with out of vocabulary words and
phrases (Callison-Burch et al, 2006) and by expand-
ing the set of reference translations for minimum er-
ror rate training (Madnani et al, 2007). While all ap-
plications require the preservation of meaning when
a phrase is replaced by its paraphrase, some addi-
tionally require the resulting sentence to be gram-
matical.
In this paper we examine the effectiveness of
placing syntactic constraints on a commonly used
paraphrasing technique that extracts paraphrases
from parallel corpora (Bannard and Callison-Burch,
2005). The paraphrasing technique employs various
aspects of phrase-based statistical machine transla-
tion including phrase extraction heuristics to obtain
bilingual phrase pairs from word alignments. En-
glish phrases are considered to be potential para-
phrases of each other if they share a common for-
eign language phrase among their translations. Mul-
tiple paraphrases are frequently extracted for each
phrase and can be ranked using a paraphrase proba-
bility based on phrase translation probabilities.
We find that the quality of the paraphrases that
are generated in this fashion improves significantly
when they are required to be the same syntactic type
as the phrase that they are paraphrasing. This con-
straint:
? Eliminates a trivial but pervasive error that
arises from the interaction of unaligned words
with phrase extraction heuristics.
? Refines the results for phrases that can take on
different syntactic labels.
? Applies both to phrases which are linguistically
coherent and to arbitrary sequences of words.
? Results in much more grammatical output
when phrases are replaced with their para-
phrases.
A thorough manual evaluation of the refined para-
phrasing technique finds a 19% absolute improve-
196
ment in the number of paraphrases that are judged
to be correct.
This paper is structured as follows: Section 2
describes related work in syntactic constraints on
phrase-based SMT and work utilizing syntax in
paraphrase discovery. Section 3 details the prob-
lems with extracting paraphrases from parallel cor-
pora and our improvements to the technique. Sec-
tion 4 describes our experimental design and evalu-
ation methodology. Section 5 gives the results of our
experiments, and Section 6 discusses their implica-
tions.
2 Related work
A number of research efforts have focused on em-
ploying syntactic constraints in statistical machine
translation. Wu (1997) introduced the inversion
transduction grammar formalism which treats trans-
lation as a process of parallel parsing of the source
and target language via a synchronized grammar.
The synchronized grammar places constraints on
which words can be aligned across bilingual sen-
tence pairs. To achieve computational efficiency, the
original proposal used only a single non-terminal la-
bel rather than a linguistic grammar.
Subsequent work used more articulated parses
to improve alignment quality by applying cohesion
constraints (Fox, 2002; Lin and Cherry, 2002). If
two English phrases are in disjoint subtrees in the
parse, then the phrasal cohesion constraint prevents
them from being aligned to overlapping sequences
in the foreign sentence. Other recent work has incor-
porated constituent and dependency subtrees into the
translation rules used by phrase-based systems (Gal-
ley et al, 2004; Quirk et al, 2005). Phrase-based
rules have also been replaced with synchronous con-
text free grammars (Chiang, 2005) and with tree
fragments (Huang and Knight, 2006).
A number of techniques for generating para-
phrases have employed syntactic information, either
in the process of extracting paraphrases from mono-
lingual texts or in the extracted patterns themselves.
Lin and Pantel (2001) derived paraphrases based
on the distributional similarity of paths in depen-
dency trees. Barzilay and McKeown (2001) incor-
porated part-of-speech information and other mor-
phosyntactic clues into their co-training algorithm.
They extracted paraphrase patterns that incorporate
this information. Ibrahim et al (2003) generated
structural paraphrases capable of capturing long-
distance dependencies. Pang et al (2003) employed
a syntax-based algorithm to align equivalent English
sentences by merging corresponding nodes in parse
trees and compressing them down into a word lat-
tice.
Perhaps the most closely related work is a recent
extension to Bannard and Callison-Burch?s para-
phrasing method. Zhao et al (2008b) extended the
method so that it is capable of generating richer
paraphrase patterns that include part-of-speech slots,
rather than simple lexical and phrasal paraphrases.
For example, they extracted patterns such as con-
sider NN ? take NN into consideration. To ac-
complish this, Zhao el al. used dependency parses
on the English side of the parallel corpus. Their
work differs from the work presented in this paper
because their syntactic constraints applied to slots
within paraphrase patters, and our constraints apply
to the paraphrases themselves.
3 Paraphrasing with parallel corpora
Bannard and Callison-Burch (2005) extract para-
phrases from bilingual parallel corpora. They give
a probabilistic formation of paraphrasing which nat-
urally falls out of the fact that they use techniques
from phrase-based statistical machine translation:
e?2 = argmax
e2:e2 6=e1
p(e2|e1) (1)
where
p(e2|e1) =
?
f
p(f |e1)p(e2|f, e1) (2)
?
?
f
p(f |e1)p(e2|f) (3)
Phrase translation probabilities p(f |e1) and p(e2|f)
are commonly calculated using maximum likelihood
estimation (Koehn et al, 2003):
p(f |e) =
count(e, f)
?
f count(e, f)
(4)
where the counts are collected by enumerating all
bilingual phrase pairs that are consistent with the
197
conseguido
.opportunitiesequalcreatetofailedhasprojecteuropeanthe
oportunidadesdeigualdadlahanoeuropeoproyectoel
Figure 1: The interaction of the phrase extraction heuristic with unaligned English words means that the Spanish
phrase la igualdad aligns with equal, create equal, and to create equal.
word alignments for sentence pairs in a bilingual
parallel corpus. Various phrase extraction heuristics
are possible. Och and Ney (2004) defined consistent
bilingual phrase pairs as follows:
BP (fJ1 , e
I
1, A) = {(f
j+m
j , e
i+n
i ) :
?(i?, j?) ? A : j ? j? ? j +m? i ? i? ? i+ n
??(i?, j?) ? A : j ? j? ? j +m? ? i ? i? ? i+ n}
where fJ1 is a foreign sentence, e
I
1 is an English sen-
tence and A is a set of word alignment points.
The heuristic allows unaligned words to be in-
cluded at the boundaries of the source or target lan-
guage phrases. For example, when enumerating the
consistent phrase pairs for the sentence pair given in
Figure 1, la igualdad would align not only to equal,
but also to create equal, and to create equal. In SMT
these alternative translations are ranked by the trans-
lation probabilities and other feature functions dur-
ing decoding.
The interaction between the phrase extraction
heuristic and unaligned words results in an unde-
sirable effect for paraphrasing. By Bannard and
Callison-Burch?s definition, equal, create equal, and
to create equal would be considered paraphrases be-
cause they are aligned to the same foreign phrase.
Tables 1 and 2 show how sub- and super-phrases can
creep into the paraphrases: equal can be paraphrased
as equal rights and create equal can be paraphrased
as equal. Obviously when e2 is substituted for e1 the
resulting sentence will generally be ungrammatical.
The first case could result in equal equal rights, and
the second would drop the verb.
This problem is pervasive. To test its extent we at-
tempted to generate paraphrases for 900,000 phrases
using Bannard and Callison-Burch?s method trained
on the Europarl corpora (as described in Section 4).
It generated a total of 3.7 million paraphrases for
equal
equal .35 equally .02
same .07 the .02
equality .03 fair .01
equals .02 equal rights .01
Table 1: The baseline method?s paraphrases of equal and
their probabilities (excluding items with p < .01).
create equal
create equal .42 same .03
equal .06 created .02
to create a .05 conditions .02
create .04 playing .02
to create equality .03 creating .01
Table 2: The baseline?s paraphrases of create equal. Most
are clearly bad, and the most probable e2 6= e1 is a sub-
string of e1.
400,000 phrases in the list.1 We observed that 34%
of the paraphrases (excluding the phrase itself) were
super- or sub-strings of the original phrase. The
most probable paraphrase was a super- or sub-string
of the phrase 73% of the time.
There are a number of strategies that might be
adopted to alleviate this problem:
? Bannard and Callison-Burch (2005) rank their
paraphrases with a language model when the
paraphrases are substituted into a sentence.
? Bannard and Callison-Burch (2005) sum over
multiple parallel corpora C to reduce the prob-
lems associated with systematic errors in the
1The remaining 500,000 phrases could not be paraphrased
either because e2 6= e1 or because they were not consistently
aligned to any foreign phrases.
198
word alignments in one language pair:
e?2 = argmax
e2
?
c?C
?
f
p(f |e1)p(e2|f) (5)
? We could change the phrase extraction heuris-
tic?s treatment of unaligned words, or we could
attempt to ensure that we have fewer unaligned
items in our word alignments.
? The paraphrase criterion could be changed
from being e2 6= e1 to specifying that e2 is not
sub- or super-string of e1.
In this paper we adopt a different strategy. The
essence of our strategy is to constrain paraphrases
to be the same syntactic type as the phrases that they
are paraphrasing. Syntactic constraints can apply in
two places: during phrase extraction and when sub-
stituting paraphrases into sentences. These are de-
scribed in sections 3.1 and 3.2.
3.1 Syntactic constraints on phrase extraction
When we apply syntactic constraints to the phrase
extraction heuristic, we change how bilingual phrase
pairs are enumerated and how the component proba-
bilities of the paraphrase probability are calculated.
We use the syntactic type s of e1 in a refined ver-
sion of the paraphrase probability:
e?2 = argmax
e2:e2 6=e1?s(e2)=s(e1)
p(e2|e1, s(e1)) (6)
where p(e2|e1, s(e1)) can be approximated as:
?
c?C
?
f p(f |e1, s(e1))p(e2|f, s(e1))
|C|
(7)
We define a new phrase extraction algorithm that op-
erates on an English parse tree P along with foreign
sentence fJ1 , English sentence e
I
1, and word align-
ment A. We dub this SBP for syntactic bilingual
phrases:
SBP (fJ1 , e
I
1, A, P ) = {(f
j+m
j , e
i+n
i , s(e
i+n
i )) :
?(i?, j?) ? A : j ? j? ? j +m? i ? i? ? i+ n
??(i?, j?) ? A : j ? j? ? j +m? ? i ? i? ? i+ n
?? subtree ? P with label s spanning words (i, i+ n)}
equal
JJ equal .60 similar .02
same .14 equivalent .01
fair .02
ADJP equal .79 the same .01
necessary .02 equal in law .01
similar .02 equivalent .01
identical .02
Table 3: Syntactically constrained paraphrases for equal
when it is labeled as an adjective or adjectival phrase.
The SBP phrase extraction algorithm produces tu-
ples containing a foreign phrase, an English phrase
and a syntactic label (f, e, s). After enumerating
these for all phrase pairs in a parallel corpus, we can
calculate p(f |e1, s(e1)) and p(e2|f, s(e1)) as:
p(f |e1, s(e1)) =
count(f, e1, s(e1))
?
f count(f, e1, s(e1))
p(e2|f, s(e1)) =
count(f, e2, s(e1))
?
e2 count(f, e2, s(e1))
By redefining the probabilities in this way we parti-
tion the space of possible paraphrases by their syn-
tactic categories.
In order to enumerate all phrase pairs with their
syntactic labels we need to parse the English side of
the parallel corpus (but not the foreign side). This
limits the potential applicability of our refined para-
phrasing method to languages which have parsers.
Table 3 gives an example of the refined para-
phrases for equal when it occurs as an adjective or
adjectival phrase. Note that most of the paraphrases
that were possible under the baseline model (Table
1) are now excluded. We no longer get the noun
equality, the verb equals, the adverb equally, the de-
termier the or the NP equal rights. The paraphrases
seem to be higher quality, especially if one considers
their fidelity when they replace the original phrase in
the context of some sentence.
We tested the rate of paraphrases that were sub-
and super-strings when we constrain paraphrases
based on non-terminal nodes in parse trees. The
percent of the best paraphrases being substrings
dropped from 73% to 24%, and the overall percent
of paraphrases subsuming or being subsumed by the
original phrase dropped from 34% to 12%. How-
ever, the number of phrases for which we were able
199
SBARQ
WHADVP
WRB
How
SQ
VBP
do
NP
PRP
we
VP
VB
create
NP
JJ
equal
NNS
rights
.
?
Figure 2: In addition to extracting phrases that are domi-
nated by a node in the parse tree, we also generate labels
for non-syntactic constituents. Three labels are possible
for create equal.
to generated paraphrases dropped from 400,000 to
90,000, since we limited ourselves to phrases that
were valid syntactic constituents. The number of
unique paraphrases dropped from several million to
800,000.
The fact that we are able to produce paraphrases
for a much smaller set of phrases is a downside to
using syntactic constraints as we have initially pro-
posed. It means that we would not be able to gen-
erate paraphrases for phrases such as create equal.
Many NLP tasks, such as SMT, which could benefit
from paraphrases require broad coverage and may
need to paraphrases for phrases which are not syn-
tactic constituents.
Complex syntactic labels
To generate paraphrases for a wider set of phrases,
we change our phrase extraction heuristic again so
that it produces phrase pairs for arbitrary spans in
the sentence, including spans that aren?t syntactic
constituents. We assign every span in a sentence a
syntactic label using CCG-style notation (Steedman,
1999), which gives a syntactic role with elements
missing on the left and/or right hand sides.
SBP (fJ1 , e
I
1, A, P ) = {(f
j+m
j , e
i+n
i , s) :
?(i?, j?) ? A : j ? j? ? j +m? i ? i? ? i+ n
??(i?, j?) ? A : j ? j? ? j +m? ? i ? i? ? i+ n
??s ? CCG-labels(ei+ni , P )}
The function CCG-labels describes the set of CCG-
labels for the phrase spanning positions i to i+ n in
create equal
VP/(NP/NNS) create equal .92
creating equal .08
VP/(NP/NNS) PP create equal .96
promote equal .03
establish fair .01
VP/(NP/NNS) PP PP create equal .80
creating equal .10
provide equal .06
create genuinely fair .04
VP/(NP/(NP/NN) PP) create equal .83
create a level playing .17
VP/(NP/(NP/NNS) PP) create equal .83
creating equal .17
Table 4: Paraphrases and syntactic labels for the non-
constituent phrase create equal.
a parse tree P . It generates three complex syntactic
labels for the non-syntactic constituent phrase create
equal in the parse tree given in Figure 2:
1. VP/(NP/NNS) ? This label corresponds to the in-
nermost circle. It indicates that create equal is
a verb phrase missing a noun phrase to its right.
That noun phrase in turn missing a plural noun
(NNS) to its right.
2. SQ\VBP NP/(VP/(NP/NNS)) ? This label corre-
sponds to the middle circle. It indicates that
create equal is an SQ missing a VBP and a NP
to its left, and the complex VP to its right.
3. SBARQ\WHADVP (SQ\VBP NP/(VP/(NP/NNS)))/. ?
This label corresponds to the outermost cir-
cle. It indicates that create equal is an SBARQ
missing a WHADVP and the complex SQ to its
left, and a punctuation mark to its right.
We can use these complex labels instead of atomic
non-terminal symbols to handle non-constituent
phrases. For example, Table 4 shows the para-
phrases and syntactic labels that are generated for
the non-constituent phrase create equal. The para-
phrases are significantly better than the paraphrases
generated for the phrase by the baseline method (re-
fer back to Table 2).
The labels shown in the figure are a fraction of
those that can be derived for the phrase in the paral-
lel corpus. Each of these corresponds to a different
200
syntactic context, and each has its own set of associ-
ated paraphrases.
We increase the number of phrases that are para-
phrasable from the 90,000 in our initial definition
of SBP to 250,000 when we use complex CCG la-
bels. The number of unique paraphrases increases
from 800,000 to 3.5 million, which is nearly as
many paraphrases that were produced by the base-
line method for the sample.
3.2 Syntactic constraints when substituting
paraphrases into a test sentence
In addition to applying syntactic constraints to our
phrase extraction algorithm, we can also apply them
when we substitute a paraphrase into a sentence. To
do so, we limit the paraphrases to be the same syn-
tactic type as the phrase that it is replacing, based on
the syntactic labels that are derived from the phrase
tree for a test sentence. Since each phrase normally
has a set of different CCG labels (instead of a sin-
gle non-termal symbol) we need a way of choosing
which label to use when applying the constraint.
There are several different possibilities for choos-
ing among labels. We could simultaneously choose
the best paraphrase and the best label for the phrase
in the parse tree of the test sentence:
e?2 = argmax
e2:e2 6=e1
argmax
s?CCG-labels(e1,P )
p(e2|e1, s) (8)
Alternately, we could average over all of the labels
that are generated for the phrase in the parse tree:
e?2 = argmax
e2:e2 6=e1
?
s?CCG-labels(e1,P )
p(e2|e1, s) (9)
The potential drawback of using Equations 8 and
9 is that the CCG labels for a particular sentence sig-
nificantly reduces the paraphrases that can be used.
For instance, VP/(NP/NNS) is the only label for the
paraphrases in Table 4 that is compatible with the
parse tree given in Figure 2.
Because the CCG labels for a given sentence are
so specific, many times there are no matches. There-
fore we also investigated a looser constraint. We
choose the highest probability paraphrase with any
label (i.e. the set of labels extracted from all parse
trees in our parallel corpus):
e?2 = argmax
e2:e2 6=e1
argmax
s???T in CCCG-labels(e1,T )
p(e2|e1, s) (10)
Equation 10 only applies syntactic constraints dur-
ing phrase extraction and ignores them during sub-
stitution.
In our experiments, we evaluate the quality of the
paraphrases that are generated using Equations 8, 9
and 10. We compare their quality against the Ban-
nard and Callison-Burch (2005) baseline.
4 Experimental design
We conducted a manual evaluation to evaluate para-
phrase quality. We evaluated whether paraphrases
retained the meaning of their original phrases and
whether they remained grammatical when they re-
placed the original phrase in a sentence.
4.1 Training materials
Our paraphrase model was trained using the Eu-
roparl corpus (Koehn, 2005). We used ten par-
allel corpora between English and (each of) Dan-
ish, Dutch, Finnish, French, German, Greek, Ital-
ian, Portuguese, Spanish, and Swedish, with approx-
imately 30 million words per language for a total of
315 million English words. Automatic word align-
ments were created for these using Giza++ (Och and
Ney, 2003). The English side of each parallel corpus
was parsed using the Bikel parser (Bikel, 2002). A
total of 1.6 million unique sentences were parsed.
A trigram language model was trained on these En-
glish sentences using the SRI language modeling
toolkit (Stolcke, 2002).
The paraphrase model and language model for the
Bannard and Callison-Burch (2005) baseline were
trained on the same data to ensure a fair comparison.
4.2 Test phrases
The test set was the English portion of test sets
used in the shared translation task of the ACL-
2007 Workshop on Statistical Machine Translation
(Callison-Burch et al, 2007). The test sentences
were also parsed with the Bikel parser.
The phrases to be evaluated were selected such
that there was an even balance of phrase lengths
(from one word long up to five words long), with
half of the phrases being valid syntactic constituents
and half being arbitrary sequences of words. 410
phrases were selected at random for evaluation. 30
items were excluded from our results subsequent
to evaluation on the grounds that they consisted
201
solely of punctuation and stop words like determin-
ers, prepositions and pronouns. This left a total of
380 unique phrases.
4.3 Experimental conditions
We produced paraphrases under the following eight
conditions:
1. Baseline ? The paraphrase probability defined
by Bannard and Callison-Burch (2005). Calcu-
lated over multiple parallel corpora as given in
Equation 5. Note that under this condition the
best paraphrase is the same for each occurrence
of the phrase irrespective of which sentence it
occurs in.
2. Baseline + LM ? The paraphrase probability
(as above) combined with the language model
probability calculated for the sentence with the
phrase replaced with the paraphrase.
3. Extraction Constraints ? This condition se-
lected the best paraphrase according to Equa-
tion 10. It chooses the single best paraphrase
over all labels. Conditions 3 and 5 only apply
the syntactic constraints at the phrase extraction
stage, and do not require that the paraphrase
have the same syntactic label as the phrase in
the sentence that it is being subtituted into.
4. Extraction Constraints + LM ? As above, but
the paraphrases are also ranked with a language
model probability.
5. Substitution Constraints ? This condition
corresponds to Equation 8, which selects the
highest probability paraphrase which matches
at least one of the syntactic labels of the phrase
in the test sentence. Conditions 5?8 apply the
syntactic constraints both and the phrase ex-
traction and at the substitution stages.
6. Syntactic Constraints + LM ? As above, but
including a language model probability as well.
7. Averaged Substitution Constraints ? This
condition corresponds to Equation 9, which av-
erages over all of the syntactic labels for the
phrase in the sentence, instead of choosing the
single one which maximizes the probability.
MEANING
5 All of the meaning of the original phrase is re-
tained, and nothing is added
4 The meaning of the original phrase is retained, al-
though some additional information may be added
but does not transform the meaning
3 The meaning of the original phrase is retained, al-
though some information may be deleted without
too great a loss in the meaning
2 Substantial amount of the meaning is different
1 The paraphrase doesn?t mean anything close to
the original phrase
GRAMMAR
5 The sentence with the paraphrase inserted is per-
fectly grammatical
4 The sentence is grammatical, but might sound
slightly awkward
3 The sentence has an agreement error (such as be-
tween its subject and verb, or between a plural
noun and singular determiner)
2 The sentence has multiple errors or omits words
that would be required to make it grammatical
1 The sentence is totally ungrammatical
Table 5: Annotators rated paraphrases along two 5-point
scales.
8. Averaged Substitution Constraints + LM ?
As above, but including a language model
probability.
4.4 Manual evaluation
We evaluated the paraphrase quality through a sub-
stitution test. We retrieved a number of sentences
which contained each test phrase and substituted the
phrase with automatically-generated paraphrases.
Annotators judged whether the paraphrases had the
same meaning as the original and whether the re-
sulting sentences were grammatical. They assigned
two values to each sentence using the 5-point scales
given in Table 5. We considered an item to have
the same meaning if it was assigned a score of 3 or
greater, and to be grammatical if it was assigned a
score of 4 or 5.
We evaluated several instances of a phrase when
it occurred multiple times in the test corpus,
since paraphrase quality can vary based on context
(Szpektor et al, 2007). There were an average of
3.1 instances for each phrase, with a maximum of
6. There were a total of 1,195 sentences that para-
202
phrases were substituted into, with a total of 8,422
judgements collected. Note that 7 different para-
phrases were judged on average for every instance.
This is because annotators judged paraphrases for
eight conditions, and because we collected judg-
ments for the 5-best paraphrases for many of the
conditions.
We measured inter-annotator agreement with the
Kappa statistic (Carletta, 1996) using the 1,391
items that two annotators scored in common. The
two annotators assigned the same absolute score
47% of the time. If we consider chance agreement to
be 20% for 5-point scales, then K = 0.33, which is
commonly interpreted as ?fair? (Landis and Koch,
1977). If we instead measure agreement in terms
of how often the annotators both judged an item to
be above or below the thresholds that we set, then
their rate of agreement was 80%. In this case chance
agreement would be 50%, so K = 0.61, which is
?substantial?.
4.5 Data and code
In order to allow other researchers to recreate our re-
sults or extend our work, we have prepared the fol-
lowing materials for download2:
? The complete set of paraphrases generated for
the test set. This includes the 3.7 million para-
phrases generated by the baseline method and
the 3.5 million paraphrases generated with syn-
tactic constraints.
? The code that we used to produce these para-
phrases and the complete data sets (including
all 10 word-aligned parallel corpora along with
their English parses), so that researchers can
extract paraphrases for new sets of phrases.
? The manual judgments about paraphrase qual-
ity. These may be useful as development ma-
terial for setting the weights of a log-linear for-
mulation of paraphrasing, as suggested in Zhao
et al (2008a).
5 Results
Table 6 summarizes the results of the manual eval-
uation. We can observe a strong trend in the syn-
tactically constrained approaches performing better
2Available from http://cs.jhu.edu/?ccb/.
correct correct both
meaning grammar correct
Baseline .56 .35 .30
Baseline+LM .46 .44 .36
Extraction Constraints .62 .57 .46
Extraction Const+LM .60 .65 .50
Substitution Constraints .60 .60 .50
Substitution Const+LM .61 .68 .54
Avg Substitution Const .62 .61 .51
Avg Substit Const+LM .61 .68 .55
Table 6: The results of the manual evaluation for each
of the eight conditions. Correct meaning is the percent of
time that a condition was assigned a 3, 4, or 5, and correct
grammar is the percent of time that it was given a 4 or 5,
using the scales from Table 5.
than the baseline. They retain the correct meaning
more often (ranging from 4% to up to 15%). They
are judged to be grammatical far more frequently
(up to 26% more often without the language model,
and 24% with the language model) . They perform
nearly 20% better when both meaning and grammat-
icality are used as criteria.3
Another trend that can be observed is that incor-
porating a language model probability tends to result
in more grammatical output (a 7?9% increase), but
meaning suffers as a result in some cases. When
the LM is applied there is a drop of 12% in correct
meaning for the baseline, but only a slight dip of 1-
2% for the syntactically-constrained phrases.
Note that for the conditions where the paraphrases
were required to have the same syntactic type as the
phrase in the parse tree, there was a reduction in the
number of paraphrases that could be applied. For
the first two conditions, paraphrases were posited for
1194 sentences, conditions 3 and 4 could be applied
to 1142 of those sentences, but conditions 5?8 could
only be applied to 876 sentences. The substitution
constraints reduce coverage to 73% of the test sen-
tences. Given that the extraction constraints have
better coverage and nearly identical performance on
3Our results show a significantly lower score for the base-
line than reported in Bannard and Callison-Burch (2005). This
is potentially due to the facts that in this work we evaluated
on out-of-domain news commentary data, and we randomly se-
lected phrases. In the pervious work the test phrases were drawn
from WordNet, and they were evaluated solely on in-domain
European parliament data.
203
the meaning criterion, they might be more suitable
in some circumstances.
6 Conclusion
In this paper we have presented a novel refinement
to paraphrasing with bilingual parallel corpora. We
illustrated that a significantly higher performance
can be achieved by constraining paraphrases to have
the same syntactic type as the original phrase. A
thorough manual evaluation found an absolute im-
provement in quality of 19% using strict criteria
about paraphrase accuracy when comparing against
a strong baseline. The syntactically enhanced para-
phrases are judged to be grammatically correct over
two thirds of the time, as opposed to the baseline
method which was grammatically correct under half
of the time.
This paper proposed constraints on paraphrases at
two stages: when deriving them from parsed paral-
lel corpora and when substituting them into parsed
test sentences. These constraints produce para-
phrases that are better than the baseline and which
are less commonly affected by problems due to un-
aligned words. Furthermore, by introducing com-
plex syntactic labels instead of solely relying on
non-terminal symbols in the parse trees, we are able
to keep the broad coverage of the baseline method.
Syntactic constraints significantly improve the
quality of this paraphrasing method, and their use
opens the question about whether analogous con-
straints can be usefully applied to paraphrases gen-
erated from purely monolingual corpora. Our im-
provements to the extraction of paraphrases from
parallel corpora suggests that it may be usefully ap-
plied to other NLP applications, such as generation,
which require grammatical output.
Acknowledgments
Thanks go to Sally Blatz, Emily Hinchcliff and
Michelle Bland for conducting the manual evalua-
tion and to Michelle Bland and Omar Zaidan for
proofreading and commenting on a draft of this pa-
per.
This work was supported by the National Science
Foundation under Grant No. 0713448. The views
and findings are the author?s alone.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Dan Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of HLT.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of HLT/NAACL.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL.
Bryant Huang and Kevin Knight. 2006. Relabeling syn-
tax trees to improve syntax-based machine translation
quality. In Proceedings of HLT/NAACL.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora. In Proceedings of the Second International
Workshop on Paraphrasing (ACL 2003).
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit,
Phuket, Thailand.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Dekang Lin and Colin Cherry. 2002. Word align-
ment with cohesion constraint. In Proceedings of
HLT/NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343?360.
204
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parame-
ter tuning in statistical machine translation. In Pro-
ceedings of the ACL Workshop on Statistical Machine
Translation.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual bitext-derived
paraphrases in automatic MT evaluation. In Proceed-
ings of the SMT Workshop at HLT-NAACL.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of ACL.
Mark Steedman. 1999. Alternative quantier scope in ccg.
In Proceedings of ACL.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, Denver,
Colorado, September.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of ACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008a. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot approach for extracting paraphrase
patterns from bilingual corpora. In Proceedings of
ACL/HLT.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proceedings of EMNLP.
205
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 52?61,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Feasibility of Human-in-the-loop Minimum Error Rate Training
Omar F. Zaidan and Chris Callison-Burch
Dept. of Computer Science, Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,ccb}@cs.jhu.edu
Abstract
Minimum error rate training (MERT) in-
volves choosing parameter values for a
machine translation (MT) system that
maximize performance on a tuning set as
measured by an automatic evaluation met-
ric, such as BLEU. The method is best
when the system will eventually be eval-
uated using the same metric, but in reality,
most MT evaluations have a human-based
component. Although performing MERT
with a human-based metric seems like a
daunting task, we describe a new metric,
RYPT, which takes human judgments into
account, but only requires human input to
build a database that can be reused over
and over again, hence eliminating the need
for human input at tuning time. In this
investigative study, we analyze the diver-
sity (or lack thereof) of the candidates pro-
duced during MERT, we describe how this
redundancy can be used to our advantage,
and show that RYPT is a better predictor of
translation quality than BLEU.
1 Introduction
Many state-of-the-art machine translation (MT)
systems over the past few years (Och and Ney,
2002; Koehn et al, 2003; Chiang, 2007; Koehn
et al, 2007; Li et al, 2009) rely on several mod-
els to evaluate the ?goodness? of a given candidate
translation in the target language. The MT system
proceeds by searching for the highest-scoring can-
didate translation, as scored by the different model
components, and returns that candidate as the hy-
pothesis translation. Each of these models need
not be a probabilistic model, and instead corre-
sponds to a feature that is a function of a (can-
didate translation,foreign sentence) pair.
Treated as a log-linear model, we need to as-
sign a weight for each of the features. Och (2003)
shows that setting those weights should take into
account the evaluation metric by which the MT
system will eventually be judged. This is achieved
by choosing the weights so as to maximize the per-
formance of the MT system on a development set,
as measured by that evaluation metric. The other
insight of Och?s work is that there exists an ef-
ficient algorithm to find such weights. This pro-
cess has come to be known as the MERT phase
(for Minimum Error Rate Training) in training
pipelines of MT systems.
A problem arises if the performance of the sys-
tem is not judged by an automatic evaluation met-
ric such as BLEU or TER, but instead through
an evaluation process involving a human. The
GALE evaluation, for instance, judges the quality
of systems as measured by human-targeted TER
(HTER), which computes the edit distance be-
tween the system?s output and a version of the
output post-edited by a human. The IWSLT and
WMT workshops also have a manual evaluation
component, as does the NIST Evaluation, in the
form of adequacy and fluency (LDC, 2005).
In theory, one could imagine trying to optimize
a metric like HTER during the MERT phase, but
that would require the availability of an HTER au-
tomatic scorer, which, by definition, does not ex-
ist. If done manually, the scoring of thousands of
candidates produced during MERT would literally
take weeks, and cost a large sum of money. For
these reasons, researchers resort to optimizing an
automatic metric (almost always BLEU) as a proxy
for human judgment.
As daunting as such a task seems for any
human-based metric, we describe a new metric,
RYPT, that takes human judgment into accout
when scoring candidates, but takes advantage of
the redundancy in the candidates produced dur-
ing MERT. In this investigative study, we describe
how this redundancy can be used to our advantage
to eliminate the need to involve a human at any
52
time except when building a database of reusable
judgments, and furthermore show that RYPT is a
better predictor of translation quality than BLEU,
making it an excellent candidate for MERT tun-
ing.
The paper is organized as follows. We start by
describing the core idea of MERT before intro-
ducing our new metric, RYPT, and describing the
data collection effort we undertook to collect the
needed human judgments. We analyze a MERT
run optimizing BLEU to quantify the level of re-
dundancy in the candidate set, and also provide
an extensive analysis of the collected judgments,
before describing a set of experiments showing
RYPT is a better predictor of translation quality
than BLEU. Following a discussion of our findings,
we briefly review related work, before pointing out
future directions and summarizing.
2 Och?s Line Search Method
A common approach to translating a source sen-
tence f in a foreign language is to select the can-
didate translation e that maximizes the posterior
probability:
Pr(e | f)
def
=
exp(s
?
(e, f))
?
e
?
exp(s
?
(e
?
, f))
.
This defines Pr(e | f) using a log-linear model
that associates a sentence pair (e, f) with a fea-
ture vector ?(e, f) = {?
1
(e, f), ..., ?
M
(e, f)},
and assigns a score
s
?
(e, f)
def
= ? ? ?(e, f) =
M
?
m=1
?
m
?
m
(e, f)
for that sentence pair, with the feature weights
? = {?
1
, ..., ?
M
} being the parameters of the
model. Therefore, the system selects the transla-
tion e?:
e? = argmax
e
Pr(e | f) = argmax
e
s
?
(e, f). (1)
Och (2003) provides evidence that ? should be
chosen by optimizing an objective function basd
on the evaluation metric of interest, rather than
likelihood. Since the error surface is not smooth,
and a grid search is too expensive, Och suggests an
alternative, efficient, line optimization approach.
Assume we are performing a line optimiza-
tion along the d
th
dimension. Consider a for-
eign sentence f , and let the candidate set for f
be {e
1
, ..., e
K
}. Recall from (1) that the 1-best
candidate at a given ? is the one with maxi-
mum
?
M
m=1
?
m
?
m
(e
k
, f). We can rewrite the
sum as ?
d
?
d
(e
k
, f) +
?
m 6=d
?
m
?
m
(e
k
, f). The
second term is constant with respect to ?
d
, and
so is ?
d
(e
k
, f). Renaming those two quantities
offest
?
(e
k
) and slope(e
k
), we get
s
?
(e
k
, f) = slope(e
k
)?
d
+ offset
?
(e
k
).
Therefore, if we plot the score for a candidate
translation vs. ?
d
, that candidate will be repre-
sented by a line. If we plot the lines for all candi-
dates (Figure 1), then the upper envelope of these
lines indicates the best candidate at any value for
?
d
.
Therefore, the objective function is piece-wise
linear across any of the M dimensions
1
, mean-
ing we only need to evaluate it at the ?critical?
points corresponding to line intersection points.
Furthermore, we only need to calculate the suffi-
cient statistics once, at the smallest critical point,
and then simply adjust the sufficient statistics to
reflect changes in the set of 1-best candidates.
2.1 The BLEU Metric
The metric most often used with MERT is BLEU
(Papineni et al, 2002), where the score of a candi-
date c against a reference translation r is:
BLEU = BP (len(c), len(r))?exp(
4
?
n=1
1
4
log p
n
),
where p
n
is the n-gram precision
2
and BP is a
brevity penalty meant to penalize short outputs, to
discourage improving precision at the expense of
recall.
There are several compelling reasons to opti-
mize to BLEU. It is the most widely reported met-
ric in MT research, and has been shown to cor-
relate well with human judgment (Papineni et al,
2002; Coughlin, 2003). But BLEU is also partic-
ularly suitable for MERT, because it can be com-
puted quite efficiently, and its sufficient statistics
are decomposable, as required by MERT.
3,4
1
Or, in fact, along any linear combination of the M di-
mensions.
2
Modifed precision, to be precise, based on clipped n-
gram counts.
3
Note that for the sufficient statistics to be decomposable,
the metric itself need not be ? this is in fact the case with
BLEU.
4
Strictly speaking, the sufficient statistics need not be de-
53
e1 e21 2
e1 e21 4
e1 e22 4
e1 e23 1
e1 2
e2 1
e1 3
e1 1 e1 4 e2 2
e2 3
e2 4
1: 
[6,
10
]
2: 
[1,
10
]
3: 
[3,
10
]
4: 
[4,
10
]
e1 e1 e1 e1
1: 
[3,
15
]
2: 
[8,
15
]
3: 
[9,
15
]
4: 
[2,
15
]
e2 e2 e2 e2
e1 e23 3
score(e,f
1
) TERscore(e,f
2
)
0.4
8
[12
,25
]
0.2
4
[6,
25
]
0.5
6
[14
,25
] 0
.32[8,
25
]
0.1
2
[3,
25
]
? d ? d? d
? ???
+
TE
R 
su
ff.
 st
at
s f
or
 
ca
nd
ida
te
s. 
Th
e 
SS
 
fo
r 
m
ea
n 
6 
ed
its
 
ar
e 
ne
ed
ed
 to
 m
at
ch
 
a 
10
-w
or
d 
re
fe
re
nc
e.
e1 1
Oc
h?s
 m
et
ho
d 
ap
pli
ed
 t
o 
a 
se
t 
of
 t
wo
 
fo
re
ign
 s
en
te
nc
es
. C
an
did
at
es
 c
or
re
sp
on
d 
to
 li
ne
s, 
an
d 
en
ve
lop
es
 o
f t
op
-m
os
t l
in
es
co
rre
sp
on
d 
to
 a
rg
m
ax
in 
Eq
. 1
. T
he
 s
et
 o
f 
1-
be
st 
ca
nd
ida
te
s 
an
d 
th
e 
er
ro
r 
m
et
ric
 
(T
ER
) 
ch
an
ge
 
on
ly 
at
 
fo
ur
 
cr
itic
al 
? d
va
lue
s. 
Nu
m
be
rs
 (
)
 in
 s
qu
ar
e 
br
ac
ke
ts 
ar
e 
th
e 
ov
er
all
 s
uf
fic
ien
t s
ta
tis
tic
s 
(S
S)
 fo
r 
TE
R,
 a
nd
 a
re
 th
e 
su
m
 o
f S
S 
fo
r i
nd
ivi
du
al 
1-
be
st 
ca
nd
ida
te
s 
(
). 
Th
is 
su
m
 is
 o
nly
 
do
ne
 o
nc
e 
to
 o
bt
ain
 [
14
,2
5]
, 
an
d 
th
en
 
sim
ply
 a
dju
ste
d 
ap
pr
op
ria
te
ly 
to
 r
ef
lec
t 
ch
an
ge
(s
) i
n 
1-
be
st 
ca
nd
ida
te
s.
Figure 1: Och?s method applied to a set of two foreign sentences. This figure is essentially a visualization
of equation (1). We show here sufficient statistics for TER for simplicity, since there are only 2 of them,
but the metric optimized in MERT is usually BLEU.
In spite of these advantages, recent work has
pointed out a number of problematic aspects of
BLEU that should cause one to pause and recon-
sider the reliance on it. Chiang et al (2008) in-
vestigate several weaknesses in BLEU and show
there are realistic scenraios where the BLEU score
should not be trusted, and in fact behaves in a
counter-intuitive manner. Furthermore, Callison-
Burch et al (2006) point out that it is not always
appropriate to use BLEU to compare systems to
each other. In particular, the quality of rule-based
systems is usually underestimated by BLEU.
All this raises doubts regarding BLEU?s ade-
quacy as a proxy for human judgment, which is
a particularly important issue in the context of set-
ting parameters during the MERT phase. But what
is the alternative?
2.2 (Non-)Applicability of Och?s Method to
Human Metrics
In principle, MERT is applicable to any evalua-
tion metric, including HTER, as long as its suffi-
cient statistics are decomposable.
4
In practice, of
course, the method requires the evaluation of thou-
sands of candidate translations. Whereas this is
composable in MERT, as they can be recalculated at each crit-
ical point. However, this would slow down the optimization
process quite a bit, since one cannot traverse the dimension
by simply adjusting the sufficient statistics to reflect changes
in 1-best candidates.
not a problem with a metric like BLEU, for which
automatic (and fast) scorers are available, such an
evaluation with a human metric would require a
large amount of effort and money, meaning that
a single MERT run would take weeks to com-
plete, and would cost thousands of dollars. As-
sume a single candidate string takes 10 seconds
to post-edit, at a cost of $0.10. Even with such
an (overly) optimistic estimate, scoring 100 candi-
dates for each of 1000 sentences would take 35 8-
hour work days and cost $10,000. The cost would
further grow linearly with the number of MERT it-
erations and the n-best list size. On the other hand,
optimizing for BLEU takes on the order of minutes
per iteration, and costs nothing.
2.3 The RYPT Metric
We suggest here a new metric that combines the
best of both worlds, in that it is based on human
judgment, but that is a viable metric to be used in
the MERT phase. The key to the feasiblity is the
reliance on a database of human judgment rather
than immendiate feedback for each candidate, and
so human feedback is only needed once, and the
collected human judgments can be reused over and
over again by an automatic scorer.
The basic idea is to reward syntactic con-
stituents in the source sentence that get algned
to ?acceptable? translations in the candidate sen-
54
S{0-11}
S{0-10} X{11-11}
S{0-7} X{8-10}
X{8-8}
X{10-10}
X{0-7}
X{4-6}X{0-2}
X{1-1} X{4-4}
official forecasts    are  based on only 3 per cent reported   ,   bloomberg    .
offizielle prognosen sind von nur 3 prozent ausgegangen  ,  meldete bloomberg .
Y
N
Y
Y
Y Y Y
N
N
Y Y Y
N
Y
ROOT
Y NY
Y
Label Y indicates 
forecasts deemed 
acceptable translation 
of prognosen.
Range 0-2 indicates 
coverage of the first 
3 words in the 
source sentence.
Figure 2: The source parse tree (top) and the can-
didate derivation tree (bottom). Nodes in the parse
tree with a thick border correspond to the frontier
node set with maxLen = 4. The human annota-
tor only sees the portion surrounded by the dashed
rectangle, including the highlighting (though ex-
cluding the word alignment links).
tence, and penalize constituents that do not. For
instance, consider the source-candidate sentence
pair of Figure 2. To evaluate the candidate transla-
tion, the source parse tree is first obtained (Dubey,
2005), and each subtree is matched with a sub-
string in the candidate string. If the source sub-
string covered by this subtree is translated into an
acceptable substring in the candidate, that node
gets a YES label. Otherwise, the node gets a NO
label.
The metric we propose is taken to be the ratio of
YES nodes in the parse tree (or RYPT). The candi-
date in Figure 2, for instance, would get a RYPT
score of 13/18 = 0.72.
To justify its use as a proxy for HTER-like met-
rics, we need to demonstrate that this metric corre-
lates well with human judgment. But it is also im-
portant to show that we can obtain the YES/NO la-
bel assignments in an efficient and affordable man-
ner. At first glance, this seems to require a human
to provide judgments for each candidate, much
like with HTER. But we describe in the next sec-
tion strategies that minimize the number of judg-
ments we need to actually collect.
3 Collecting Human Judgments
The first assumption we make to minimize the
number of human judgments, is that once we
have a judgment for a source-candidate substring
pair, that same judgment can be used across all
candidates for this source sentence. In other
words, we build a database for each source sen-
tence, which consists of <source substring,target
substring,judgment> entries. For a given source
substring, multiple entries exist, each with a dif-
ferent target candidate substring. The judgment
field is one of YES, NO, and NOT SURE.
Note that the entries do not store the full can-
didate string, since we reuse a judgment across
all the candidates of that source sentence. For in-
stance, if we collect the judgment:
<der patient,the patient,YES>
from the sentence pair:
der patient wurde isoliert .
the patient was isolated .
then this would apply to any candidate translation
of this source sentence. And so all of the following
substrings are labeled YES as well:
the patient isolated .
the patient was in isolation .
the patient has been isolated .
Similarly, if we collect the judgment:
<der patient,of the patient,NO>
from the sentence pair:
der patient wurde isoliert .
of the patient was isolated .
then this would apply to any candidate translation
of the source, and the following substrings are la-
beled NO as well:
of the patient isolated .
of the patient was in isolation .
of the patient has been isolated .
The strategy of using judgments across candi-
dates reduces the amount of labels we need to col-
lect, but evaluating a candidate translation for the
source sentence of Figure 2 would still require ob-
taining 18 labels, one for each node in the parse
tree. Instead of querying a human for each one
55
of those nodes, it is quite reasonable to percolate
existing labels up and down the parse tree: if a
node is labeled NO, this likely means that all its
ancestors would also be labeled NO, and if a node
is labeled YES, this likely means that all its de-
scendents whould also be labeled YES.
While those two strategies (using judgments
across candidates, and percolating labels up and
down the tree) are only approximations for the true
labels, employing them considerably reduces the
amount of data we need to collect.
3.1 Obtaining Source-to-Candidate
Alignments
How do we determine which segment of the can-
didate sentence aligns to a given source segment?
Given a word alignment between the source and
the candidate, we take the target substring to con-
tain any word aligned with at least one word in
the source segment. One could run an aligner (e.g.
GIZA++) on the two sentences to obtain the word
alignment, but we take a different approach.
We use Joshua (Li et al, 2009), in our experi-
ments. Joshua is a hierarchical parsing-based MT
system, and it can be instructed to produce deriva-
tion trees instead of the candidate sentence string
itself. Furthermore, each node in the derivation
tree is associated with the two indices in the source
sentence that indicate the segment corresponding
to this derivation subtree (the numbers indicated
in curly brackets in Figure 2).
Using this information, we are able to recover
most of the phrasal alignments. There are other
phrasal alignments that can be deduced from
the structure of the tree indirectly, by system-
atically discarding source words that are part
of another phrasal alignment. For instance,
in Figure 2, one can observe the alignment
(offizielle,prognosen,sind)?(official,forecasts,are)
and the alignment (prognosen)?(forecasts) to
deduce (offizielle,sind)?(official,are).
Although some of the phrasal alignment are
one-to-one mappings, many of them are many-
to-many. By construction, any deduced many-to-
many mapping has occurred in the training paral-
lel corpus at least once. And so we recover the
individual word alignments by consulting the par-
allel corpus from which the grammar rules were
extracted (which requires maintaining the word
alignments obtained prior to rule extraction).
5
5
We incorporated our implementation of the source-
We emphasize here that our recovery of word
alignment from phrasal alignment is independent
from the hierarchical and parsing-based nature of
the Joshua system. And so the alignment approach
we suggest here can be applied to a different MT
system as well, as long as that system provides
phrasal alignment along with the output. In partic-
ular, a phrase-based system such as Moses can be
modified in a straightforward manner to provide
phrasal alignments, and then apply our method.
4 Data Collection
We chose the WMT08 German-English news
dataset to work with, and since this is an investiga-
tive study of a novel approach, we collected judg-
ments for a subset of 250 source sentences from
the development set for the set of candidate sen-
tences produced in the last iteration of a MERT
run optimizing BLEU on the full 2051-sentence de-
velopment set. The MT system we used is Joshua
(Li et al, 2009), a software package that comes
complete with a grammar extraction module and a
MERT module, in addition to the decoder itself.
What segments of the source should be chosen
to be judged? We already indicated that we limit
ourselves, by definition of RYPT, to segments that
are covered exactly by a subtree in the source parse
tree. This has a couple of nice advantages: it al-
lows us to present an annotator with a high num-
ber of alternatives judged simulataneously (since
the annotator is shown a source segment and sev-
eral candidates, not just one), and this probably
also makes judging them easier ? it is reasonable
to assume that strings corresponding to syntactic
constituents are easier to process by a human.
Our query selection strategy attempts to max-
imize the amount of YES/NO percolation that
would take place. We therefore ensure that for any
2 queries, the corresponding source segments do
not overlap: such overlap indicates that one sub-
tree is completely contained within the other. Hav-
ing both queries (in the same batch) might be re-
dundant if we use the above percolation procedure.
The idea is to select source segments so that
they fully cover the entire source sentence, but
have no overlap amongst them. In one extreme,
each query would correspond to an entire parse
tree. This is not ideal since the overwhelming ma-
jority of the judgments will most likely be NO,
candidate aligner into the Joshua software as a new
aligner package.
56
which does not help identify where the problem
is. In the other extreme, each query would corre-
spond to a subtree rooted at a preterminal. This is
also not ideal, since it would place too much em-
phasis on translations of unigrams.
So we need a middle ground. We select a
maximum-source-length maxLen to indicate how
long we?re willing to let source segments be. Then
we start at the root of the parse tree, and prop-
agate a ?frontier? node set down the parse tree,
to end up with a set of nodes that fully cover the
source sentence, have no overlap amongst them,
and with each covering no more than maxLen
source words. For instance, with maxLen set to
4, the frontier set of Figure 2 are the nodes with
a thick border. An algorithmic description is pro-
vided in Algorithm 1.
Algorithm 1 Constructing the frontier node set for
a parse tree.
Input: A source parse tree T rooted at ROOT, and
a maximum source length maxLen.
Return: A nonempty set frontierSet, con-
taining a subset of the nodes in T .
1. Initialize frontierSet to the empty set.
2. Initialize currNodes to {ROOT}.
3. while currNodes is not empty do
4. Initialize newNodes to the empty set.
5. for each node N in currNodes do
6. if N covers ? maxLen source words
then
7. Add N to frontierSet.
8. else
9. Add children of N to newNodes.
10. end if
11. end for
12. Set currNodes = newNodes
13. end while
14. Return frontierSet.
This would ensure that our queries cover be-
tween 1 and maxLen source words, and ensures
they do not overlap, which would allow us to take
full advantage of the downward-YES and upward-
NO percolation. We set maxLen = 4 based on a
pilot study of 10 source sentences and their candi-
dates, having observed that longer segments tend
to always be labeled as NO, and shorter segments
tend to be so deep down the parse tree.
4.1 Amazon Mechanical Turk
We use the infrastructure of Amazon?s Mechan-
ical Turk (AMT)
6
to collect the labels. AMT is
a virtual marketplace that allows ?requesters? to
create and post tasks to be completed by ?work-
ers? around the world. To create the tasks (called
Human Intelligence Tasks, or HITs), a requester
supplies an HTML template along with a comma-
separated-values database, and AMT automati-
cally creates the HITs and makes them available to
workers. The queries are displayed as an HTML
page (based on the provided HTML template),
with the user indicating the label (YES, NO, or NOT
SURE) by selecting the appropriate radio button.
The instructions read, in part:
7
You are shown a ?source? German
sentence with a highlighted segment,
followed by several candidate trans-
lations with corresponding highlighted
segments. Your task is to decide if each
highlighted English segment is an ac-
ceptable translation of the highlighted
German segment.
In each HIT, the worker is shown up to 10 al-
ternative translations of a highlighted source seg-
ment, with each itself highlighted within a full
candidate string in which it appears. To aid the
worker in the task, they are also shown the ref-
erence translation, with a highlighted portion that
corresponds to the source segment, deduced using
word alignments obtained with GIZA++.
8
4.2 Cost of Data Collection
The total number of HITs created was 3873,
with the reward for completing a HIT depend-
ing on how many alternative translations are being
judged. On average, each HIT cost 2.1 cents and
involved judging 3.39 alternatives. 115 distinct
workers put in a total of 30.82 hours over a pe-
riod of about 4 days. On average, a label required
8.4 seconds to determine (i.e. at a rate of 426 la-
bels per hour). The total cost was $81.44: $21.43
for Amazon?s commission, $53.47 for wages, and
6
AMT?s website: http://www.mturk.com.
7
Template and full instructions can be viewed at http:
//cs.jhu.edu/
?
ozaidan/hmert.
8
These alignments are not always precise, and we do note
that fact in the instructions. We also deliberately highlight the
reference substring in a different color to make it clear that
workers should judge a candidate substring primarily based
on the source substring, not the reference substring.
57
$6.54 for bonuses
9
, for a cost per label of 0.62
cents (i.e. at a rate of 161.32 labels per dol-
lar). Excluding Amazon?s commission, the effec-
tive hourly ?wage? was $1.95.
5 Experimental Results and Analysis
By limiting our queries to source segments corre-
sponding to frontier nodes with maxLen = 4, we
obtain a total of 3601 subtrees across the 250 sen-
tences, for an average of 14.4 per sentence. On
average, each subtree has 3.65 alternative trans-
lations. Only about 4.8% of the judgments were
returned as NOT SURE (or, occasionally, blank),
with the rest split into 35.1% YES judgments and
60.1% NO judgments.
The coverage we get before percolating labels
up and down the trees is 39.4% of the nodes, in-
creasing to a coverage of 72.9% after percolation.
This is quite good, considering we only do a sin-
gle data collection pass, and considering that about
10% of the subtrees do not align to candidate sub-
strings to begin with (e.g. single source words that
lack a word alignment into the candidate string).
The main question, of course, is whether or not
those labels allow us to calculate a RYPT score
that is reliably correlated with human judgment.
We designed an experiment to compare the predic-
tive power of RYPT vs. BLEU. Given the candidate
set of a source sentence, we rerank the candidate
set according to RYPT and extract the top-1 can-
didate, and we rerank the candidate set according
to BLEU, and extract the top-1 candidate. We then
present the two candidates to human judges, and
ask them to choose the one that is a more adequate
translation. For reliability, we collect 3 judgments
per sentence pair comparison, instead of just 1.
The results show that RYPT significantly outper-
forms BLEU when it comes to predicting human
preference, with its choice prevailing in 46.1%
of judgments vs. 36.0% for BLEU, with 17.9%
judged to be of equal quality (left half of Ta-
ble 1). This advantage is especially true when the
judgments are grouped by sentence, and we ex-
amine cases of strong agreement among the three
annotators (Table 2): whereas BLEU?s candidate
is strongly preferred in 32 of the candidate pairs
(bottom 2 rows), RYPT?s candidate is strongly pre-
ferred in about double that number: 60 candidate
9
We would review the collected labels and give a 20%
reward for good workers to encourage them to come back
and complete more HITs.
pairs (top 2 rows).
This is quite a remarkable result, given that
BLEU, by definition, selects a candidate that has
significant overlap with the reference shown to the
annotators to aid in their decision-making. This
means that BLEU has an inherent advantage in
comparisons where both candidates are more or
less of equal quality, since annotators are encour-
aged (in the instructions) to make a choice even if
the two candidates seem of be of equal quality at
first glance. Pressed to make such a choice, the
annotator is likely to select the candidate that su-
perficially ?looks? more like the reference to be the
?better? of the two candidates. That candidate will
most likely be the BLEU-selected one.
To test this hypothesis, we repeated the experi-
ment without showing the annotators the reference
translations, and limited data collection to work-
ers living in Germany, making judgments based
only on the source sentences. (We only collected
one judgment per source sentence, since German
workers on AMT are in short supply.)
As expected, the difference is even more pro-
nounced: human judges prefer the RYPT-selected
candidate 45.2% of the time, while BLEU?s can-
didate is preferred only 29.2% of the time, with
25.6% judged to be of equal quality (right half
of Table 1). Our hypothesis is further supported
by the fact that most of the gain of the ?equal-
quality? category comes from BLEU, which loses
6.8 percentage points, whereas RYPT?s share re-
mains largely intact, losing less than a single per-
centage point.
5.1 Analysis of Data Collection
Recall that we minimize data collection by per-
forming label percolation and by employing a
frontier node set selection strategy. While the re-
sults just presented indicate those strategies pro-
vide a good approximation of some ?true? RYPT
score, label percolation was a strategy based pri-
marily on intuition, and choosing maxLen = 4
for frontier set construction was based on examin-
ing a limited amount of preliminary data.
Therefore, and in addition to encouraging em-
pricial results, we felt a more rigorous quantitative
analysis was in order, especially with future, more
ambitious annotation projects on the horizon. To
this end, we collected a complete set of judgments
for 50 source sentences and their candidates. That
is, we generated a query for each and every node
58
References shown; References not shown;
unrestricted restricted to DE workers
Preferred candidate # judgments % judgments # judgments % judgments
Top-1 by RYPT 346 46.1 113 45.2
Top-1 by BLEU 270 36.0 73 29.2
Neither 134 17.9 64 25.6
Total 750 100.0 250 100.0
Table 1: Ranking comparison results. The left half corresponds to the experiment (open to all workers)
where the English reference was shown, whereas the right half corresponds to the experiment (open only
to workers living in Germany) where the English reference was not shown.
Aggregate # sentences % sentences Aggregate # sentences % sentences
RYPT +3 45 18.0
RYPT +2 15 6.0 RYPT +any 120 48.0
RYPT +1 60 24.0
? 0 42 16.8 ? 0 42 16.8
BLEU +1 55 22.0
BLEU +2 5 2.0 BLEU +any 88 35.2
BLEU +3 28 11.2
Total 250 100.0 Total 250 100.0
Table 2: Ranking comparison results, grouped by sentence. This table corresponds to the left half of
Table 1. 3 judgments were collected for each comparison, with the ?aggregate? for a comparison calcu-
lated from these 3 judgments. For instance, an aggregate of ?RYPT +3? means all 3 judgments favored
RYPT?s choice, and ?RYPT +1? means one more judgment favored RYPT than did BLEU.
in the source parse tree, instead of limiting our-
selves to a frontier node set. (Though we did limit
the length of a source segment to be ? 7 words.)
This would allow us to judge the validity of label
percolation, and under different maxLen values.
Furthermore, we collected multiple judgments
for each query in order to minimize the effet of
bad/random annotations. For each of 5580 gen-
erated queries, we collected five judgments, for a
total of 27,900 judgments.
10
As before, the anno-
tator would pick one of YES, NO, and NOT SURE.
First, collecting multiple judgments allowed us
to investigate inter-annotator agreement. In 68.9%
of the queries, at least 4 of the 5 annotators chose
the same label, signifying a high degree of inter-
annotator agreement. This is especially encourag-
ing considering that we identified about 15% of
the HITs as being of poor quality, and blocked the
respective annotators from doing further HITs.
11
We then examine the applicability and validity
10
For a given query, the five collected judgments are from
five different annotators, since AMT ensures an annotator is
never shown the same HIT twice.
11
It is especially easy to identify (and then block) such an-
notators when they submit a relatively large number of HITs,
since inspecting some of their annotations would indicate
they are answering randomly and/or inconsistently.
of label percolation. For each of 7 different values
for Algorithm 1?s maxLen, we ignore all but la-
bels that would be requested under that maxLen
value, and percolate the labels up and down the
tree. In Figure 3 we plot the coverage before and
after percolation (middle two curves), and observe
expansion in coverage across different values of
maxLen, peaking at about +33% for maxLen= 4
and 5, with most of the benefit coming from YES
percolation (bottom two curves).
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
0 1 2 3 4 5 6 7
maxLen
Coverage before percolation
Coverage after percolation
Percolation accuracy
? due to perc. of YES
? due to perc. of NO
Figure 3: Label percolation under different
maxLen values. The bottom two curves are the
breakdown of the difference between the middle
two. Accuracy is measured against majority votes.
59
We also measure the accuracy of labels deduced
from percolation (top curve of Figure 3). We de-
fine a percolated label to be correct if it matches
the label given by a majority vote over the col-
lected labels for that particular node. We find that
accuracy at low maxLen values is significantly
lower than at higer values (e.g. 72.6% vs. 84.1%
for 1 vs. 4). This means a middle value such as 3
or 4 is optimal. Higher values could be suitable if
we wish to emphasize translation fluency.
6 Related Work
Nie?en et al (2000) is an early work that also con-
structs a database of translations and judgments.
There, a source sentence is stored along with all
the translations that have already been manually
judged, along with their scores. They utilize this
database to carry out ?semi-automatic? evaluation
in a fast and convenient fashion thanks to tool they
developed with a user-friendly GUI.
In their annual evaluation, the WMT work-
shop has effectively conducted manual evaluation
of submitted systems over the past few years by
distributing the work across tens of volunteers,
though they relied on a self-designed online por-
tal. On the other hand, Snow et al (2008) illus-
trate how AMT can be used to collect data in a
?fast and cheap? fashion, for a number of NLP
tasks, such as word sense disambiguation. They
go a step further and model the behavior of their
annotators to reduce annotator bias. This was pos-
sible as they collect multiple judgments for each
query from multiple annotators.
The question of how to design an automatic
metric that best approximates human judgment
has received a lot of attention lately. NIST started
organizing the Metrics for Machine Translation
Challenge (MetricsMATR) in 2008, with the aim
of developing automatic evaluation metrics that
correlate highly with human judgment of transla-
tion quality. The latest WMT workshop (Callison-
Burch et al, 2009) also conducted a full assess-
ment of how well a suite of automatic metrics cor-
relate with human judgment.
7 Future Work
This pilot study has demonstrated the feasibility
of collecting a large number of human judgments,
and has shown that the RYPT metric is better than
BLEU at picking out the best translation. The
next step is to run a complete MERT run. This
will involve collecting data for thousands of al-
ternative translations for several hundreds source
sentences. Based on our analysis, this it should
be cost-effective to solicit these judgments using
AMT. After training MERT using RYPT as an ob-
jective function the, the next logical step would be
to compare two outputs of a system. One output
would have parameters optimized to BLEU and the
other to RYPT. The hope is that the RYPT-trained
system would be better under the final HTER eval-
uation than the BLEU-trained system.
We are also investigating a probabilistic ap-
proach to percolating the labels up and down the
tree, whereby the label of a node is treated as a
random variable, and inference is performed based
on values of the other observed nodes, as well as
properties of the source/candidate segment. Cast
this way, a probabilistic approach is actually quite
appealing, and one could use collected data to
train a prediction model (such as a Markov ran-
dom field).
8 Summary
We propose a human-based metric, RYPT, that is
quite feasible to optimize using MERT, relying on
the redundancy in the candidate set, and collect-
ing judgments using Amazon?s Mechanical Turk
infrastructure. We show this could be done in a
quite cost-effective manner, and produces data of
good quality. We show the effectiveness of the
metric by illustrating that it is a better predictor of
human judgment of translation quality than BLEU,
the most commonly used metric in MT. We show
this is the case even with a modest amount of data
that does not cover the entirety of all parse trees,
on which the metric is dependent. The collected
data represents a database that can be reused over
and over again, hence limiting human feedback to
the initial phase only.
Acknowledgments
This research was supported by the EuroMatrix-
Plus project funded by the European Commission
(7th Framework Programme), by the Defense Ad-
vanced Research Projects Agency?s GALE pro-
gram under Contract No. HR0011-06-2-0001, and
the US National Science Foundation under grant
IIS-0713448. The views and findings are the au-
thors? alone.
60
References
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In Proceedings of EACL,
pages 249?256.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008. Decomposability of trans-
lation metrics for improved evaluation and efficient
algorithms. In Proceedings of EMNLP, pages 610?
619.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of MT Summit IX.
Amit Dubey. 2005. What to do when lexicaliza-
tion fails: parsing German with suffix analysis and
smoothing. In Proceedings of ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL, Demo and Poster Ses-
sions, pages 177?180.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research.
In Proceedings of LREC.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL,
pages 295?302.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Poukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? but is it
good? evaluating non-expert annotations for natural
language tasks. In Proceedings of EMNLP, pages
254?263.
61
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286?295,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Fast, Cheap, and Creative: Evaluating Translation Quality
Using Amazon?s Mechanical Turk
Chris Callison-Burch
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, Maryland
ccb cs jhu edu
Abstract
Manual evaluation of translation quality is
generally thought to be excessively time
consuming and expensive. We explore a
fast and inexpensive way of doing it using
Amazon?s Mechanical Turk to pay small
sums to a large number of non-expert an-
notators. For $10 we redundantly recre-
ate judgments from a WMT08 transla-
tion task. We find that when combined
non-expert judgments have a high-level of
agreement with the existing gold-standard
judgments of machine translation quality,
and correlate more strongly with expert
judgments than Bleu does. We go on to
show that Mechanical Turk can be used to
calculate human-mediated translation edit
rate (HTER), to conduct reading compre-
hension experiments with machine trans-
lation, and to create high quality reference
translations.
1 Introduction
Conventional wisdom holds that manual evalua-
tion of machine translation is too time-consuming
and expensive to conduct. Instead, researchers
routinely use automatic metrics like Bleu (Pap-
ineni et al, 2002) as the sole evidence of im-
provement to translation quality. Automatic met-
rics have been criticized for a variety of reasons
(Babych and Hartley, 2004; Callison-Burch et al,
2006; Chiang et al, 2008), and it is clear that
they only loosely approximate human judgments.
Therefore, having people evaluate translation out-
put would be preferable, if it were more practical.
In this paper we demonstrate that the manual
evaluation of translation quality is not as expensive
or as time consuming as generally thought. We
use Amazon?s Mechanical Turk, an online labor
market that is designed to pay people small sums
of money to complete human intelligence tests ?
tasks that are difficult for computers but easy for
people. We show that:
? Non-expert annotators produce judgments
that are very similar to experts and that have
a stronger correlation than Bleu.
? Mechanical Turk can be used for complex
tasks like human-mediated translation edit
rate (HTER) and creating multiple reference
translations.
? Evaluating translation quality through read-
ing comprehension, which is rarely done, can
be easily accomplished through creative use
of Mechanical Turk.
2 Related work
Snow et al (2008) examined the accuracy of la-
bels created using Mechanical Turk for a variety
of natural language processing tasks. These tasks
included word sense disambiguation, word simi-
larity, textual entailment, and temporal ordering
of events, but not machine translation. Snow et
al. measured the quality of non-expert annotations
by comparing them against labels that had been
previously created by expert annotators. They re-
port inter-annotator agreement between expert and
non-expert annotators, and show that the average
of many non-experts converges on performance of
a single expert for many of their tasks.
Although it is not common for manual evalu-
ation results to be reported in conference papers,
several large-scale manual evaluations of machine
translation quality take place annually. These in-
clude public forums like the NIST MT Evalu-
ation Workshop, IWSLT and WMT, as well as
the project-specific Go/No Go evaluations for the
DARPA GALE program. Various types of human
judgments are used. NIST collects 5-point fluency
and adequacy scores (LDC, 2005), IWSLT and
286
WMT collect relative rankings (Callison-Burch et
al., 2008; Paul, 2006), and DARPA evaluates us-
ing HTER (Snover et al, 2006). The details of
these are provided later in the paper. Public eval-
uation campaigns provide a ready source of gold-
standard data that non-expert annotations can be
compared to.
3 Mechanical Turk
Amazon describes its Mechanical Turk web ser-
vice
1
as artificial artificial intelligence. The name
and tag line refer to a historical hoax from the 18th
century where an automaton appeared to be able to
beat human opponents at chess using a clockwork
mechanism, but was, in fact, controlled by a per-
son hiding inside the machine. The Mechanical
Turk web site provides a way to pay people small
amounts of money to perform tasks that are sim-
ple for humans but difficult for computers. Exam-
ples of these Human Intelligence Tasks (or HITs)
range from labeling images to moderating blog
comments to providing feedback on relevance of
results for a search query.
Anyone with an Amazon account can either
submit HITs or work on HITs that were submit-
ted by others. Workers are sometimes referred to
as ?Turkers? and people designing the HITs are
?Requesters.? Requesters can specify the amount
that they will pay for each item that is completed.
Payments are frequently as low as $0.01. Turkers
are free to select whichever HITs interest them.
Amazon provides three mechanisms to help en-
sure quality: First, Requesters can have each HIT
be completed by multiple Turkers, which allows
higher quality labels to be selected, for instance,
by taking the majority label. Second, the Re-
quester can require that all workers meet a particu-
lar set of qualications, such as sufficient accuracy
on a small test set or a minimum percentage of
previously accepted submissions. Finally, the Re-
quester has the option of rejecting the work of in-
dividual workers, in which case they are not paid.
The level of good-faith participation by Turkers
is surprisingly high, given the generally small na-
ture of the payment.
2
For complex undertakings
like creating data for NLP tasks, Turkers do not
1
http://www.mturk.com/
2
For an analysis of the demographics of Turk-
ers and why they participate, see: http://
behind-the-enemy-lines.blogspot.com/
2008/03/mechanical-turk-demographics.
html
have a specialized background in the subject, so
there is an obvious tradeoff between hiring indi-
viduals from this non-expert labor pool and seek-
ing out annotators who have a particular expertise.
4 Experts versus non-experts
We use Mechanical Turk as an inexpensive way
of evaluating machine translation. In this section,
we measure the level of agreement between ex-
pert and non-expert judgments of translation qual-
ity. To do so, we recreate an existing set of gold-
standard judgments of machine translation quality
taken from the Workshop on Statistical Machine
Translation (WMT), which conducts an annual
large-scale human evaluation of machine transla-
tion quality. The experts who produced the gold-
standard judgments are computational linguists
who develop machine translation systems.
We recreated all judgments from the WMT08
German-English News translation task. The out-
put of the 11 different machine translation systems
that participated in this task was scored by ranking
translated sentences relative to each other. To col-
lect judgements, we reproduced the WMT08 web
interface in Mechanical Turk and provided these
instructions:
Evaluate machine translation quality Rank each transla-
tion from Best to Worst relative to the other choices (ties are
allowed). If you do not know the source language then you
can read the reference translation, which was created by a
professional human translator.
The web interface displaced 5 different machine
translations of the same source sentence, and had
radio buttons to rate them.
Turkers were paid a grand total of $9.75 to
complete nearly 1,000 HITs. These HITs ex-
actly replicated the 200 screens worth of expert
judgments that were collected for the WMT08
German-English News translation task, with each
screen being completed by five different Turkers.
The Turkers were shown a source sentence, a ref-
erence translation, and translations from five MT
systems. They were asked to rank the translations
relative to each other, assigning scores from best
to worst and allowing ties.
We evaluate non-expert Turker judges by mea-
suring their inter-annotator agreement with the
WMT08 expert judges, and by comparing the cor-
relation coefficient across the rankings of the ma-
chine translation systems produced by the two sets
of judges.
287
Sentence-
level ranking 
(2360 items 
compared)
Choose 1 2 3 4 5
run 1
run 2
run 3
run 4
run 5
run 6
run 7
run 8
run 9
run 10
Expert-Expert 
Agreement 
(56 items 
compared)
Constituent-
level ranking 
(4960 items 
compared)
Choose
run 1
run 2
run 3
run 4
run 5
run 6
run 7
run 8
run 9
run 10
Expert-Expert 
Agreement 
(339 items 
compared)
Constituent-
yes/no (4702 
items 
compared)
Choose
run 1
run 2
run 3
run 4
run 5
run 6
run 7
run 8
run 9
run 10
Expert-Expert 
Agreement 
(294 items 
compared)
44.0% 51.6% 55.0% 56.8% 57.6%  perl analyze.perl sentence-ranking-Batch_25317_result.csv | cat - ../wmt08-human-judgments.csv.expert_annotator_id | grep German-English | grep -v Europarl | grep ",RANK,"| perl ../process_judgements.pl
39.5% 51.9% 53.8% 57.0%
42.5% 51.6% 54.3% 57.5%
39.0% 53.9% 56.7% 57.6%
40.6% 51.9% 56.6% 57.1%
43.4% 49.0% 55.0% 56.2%
41.9% 52.7% 55.6% 57.6%
40.0% 47.9% 55.1% 56.6%
40.5% 49.6% 56.8% 57.6%
41.6% 47.3% 54.2% 58.4%
41.3% 50.7% 55.3% 57.2% 57.6%
41.1% 50.9% 54.9% 56.8% 57.8% <--weighted with non-experts
40.9% 50.6% 53.9% 57.1% 57.8% <--weighted with non-experts (only against same size choose)
41.5% 44.6% 47.9% 51.4% 53.0% <--- without weighting
57.8% 57.8% 57.8% 57.8% 57.8%
<--reported in WMT08 paper
cat ../wmt08-human-judgments.csv | grep German-English | grep -v Europarl | grep ",RANK," | perl ../process_judgements.pl
1 2 3 4 5
55.8% 54.8% 55.1% 53.2% 53.4% perl analyze.perl constituent-ranking-Batch_25553_result.csv.itemIDs_fixed | cat - ../wmt08-human-judgments.csv.expert_annotator_id | grep German-English | grep -v Europarl | grep ",CONSTITUENT," | perl ../process_judgements.pl
55.8% 54.8% 55.1% 53.2% 53.4%
64.0% 64.0% 64.0% 64.0% 64.0%  ../wmt08-human-judgments.csv | grep German-English | grep -v Europarl | grep ",CONSTITUENT," | perl ../process_judgements.pl
1 2 3 4 5
67.5% perl analyze-yes-no.perl constituent-yes-no-Batch_40756_result.csv.itemIDs_fixed | cat - ../wmt08-human-judgments.csv.expert_annotator_id | grep German-English | grep -v Europarl | grep ",CONSTITUENT_ACCEPT," | perl ../process_judgements.pl
67.5%
68.0% 68.0% 68.0% 68.0% 68.0% cat ../wmt08-human-judgments.csv | grep German-English | grep -v Europarl | grep ",CONSTITUENT_ACCEPT," | perl ../process_judgements.pl
40%
45%
50%
55%
60%
1 2 3 4 5
Non-expert agreement with experts
Number of non-experts voting
Unweighted
Weighted by non-experts 
Weighted by experts
Expert v. Expert Agreement
Figure 1: Agreement on ranking translated
sentences increases as more non-experts vote.
Weighting non-experts? votes based on agreement
with either experts or other non-expert increases
it up further. Five weighted non-experts reach the
top line agreement between experts.
Combining ranked judgments Each item is re-
dundantly judged by five non-experts. We would
like to combine of their judgments into a single
judgment. Combining ranked judgments it is more
complicated than taking simple majority vote. We
use techniques from preference voting, in which
voters rank a group of candidates in order of pref-
erence. To create an ordering from the the ranks
assigned to the systems by multiple Turkers, we
use Schulze?s method (Schulze, 2003). It is guar-
anteed to correctly pick the winner that is pre-
ferred pairwise over the other candidates. It fur-
ther allows a complete ranking of candidates to be
constructed, making it a suitable method for com-
bining ranked judgments.
Figure 1 shows the effect of combining non-
experts judgments on their agreement with ex-
perts. Agreement is measured by examining each
pair of translated sentence and counting when two
annotators both indicated that A > B, A < B,
or A = B. Chance agreement is
1
3
. The top line
indicates the inter-annotator agreement between
WMT08 expert annotators, who agreed with each
other 58% of the time. When we have only a sin-
gle non-expert annotator?s judgment for each item,
the agreement with experts is only 41%. As we
increase the number of non-experts to five, their
agreement with experts improves to 53%, if their
Turker num HITs agreement
A88B8YGW
NRCDE
A19P0XAP
EMYVA4
AB1UOP54
VJ302
AHGV18N1
TW0U0
A17TQVV46
QFOPR
A2KOG6YH
XOH6NC
A35JEX1CT
HJ33Q
A38NBJV89
Z0LZX
AYMF2DS6
36OWV
A2CGL84Z8
W7SBS
A37CX0VE
G3TG23
A1036VFVD
H85DY
A2Y4KSQZ
JTFGGN
A35Y70MA3
4AFQ6
A2XUF8U8
TMOZ15
AA0KYO0D
ZS6WI
A339F49S4
6I0EC
A20YJNU6Y
X5DMS
A1XH9D9A
4NHG4
A1A6PCJW
CD65GU
A3VK3AVK
Z6WE90
A1LZZD7E
C0CUR5
AY1636IGK
CLHY
ACPRBLND
ZYK62
A2MLF2ME
MTX888
AMXXRYD
C28BY8
A2AJJ7A2V
LJR8V
AVIKREHT
NVTR5
AP914VPXZ
EVUW
AANG4NIT
PGM9A
A3JG7O1D
F5P7TI
A3IP9RZLIV
3UVJ
A2IINUWLH
XF4L7
A1O9BHBJ
KVI5PS
A1EVIR5BC
W38EG
A18B5QL32
8E02A
A16YNOXG
4YX7W0
AZSX83KO
OREJ0
AYWWEG6
5YVGCI
AS65HZZ5T
2B0U
A3MBIJON2
PHBNC
A37JQWRR
V7IS37
A2HBWOM
2DGNTCF
A24ZRABM
VOF68V
A1FM4EOZ
WGF6VV
A19X9J18Q
19NGY
AYUEYT9T
ZJDGU
AN7O7OLS
9ND8K
A8H56XB9K
7DB5
A3FGNLFD
OSH8W7
A36EOEBV
BUTO85
A2U5YY5E2
KPTP2
A2SB2BW2
W4T4WW
A20J9VGW
RY6KCP
A1OCUUH
OWY8B52
A1MBZ6LM
7JZKP7
A1E8EDB7
UZK38D
AUAZYGJW
M6JLK
A523O8LQ
K0C6E
A3V6V5LSC
4PN8H
A3OHQRF1
MDQ99B
A3MVGVHE
2HCERO
A3LSSQO6
UWSKFR
A3DOUTFZ
2DG53L
A2WLPO1V
YCAEQF
A2LYIHZLR
YIKPQ
A2HNP1YL
1IBFMU
A2CGQY6J
ZGGLYP
A2B8DTCHI
9U902
A2A4OZEX
98MVZS
A25ZA4IBIE
ZGNC
A235PZXM
LBGA4Q
A1T4TJIPC
OQOGP
A1SXPREF
GUADVY
A1MI7U9VJ
HRIPK
A1KS77MY
C4GGZ3
A1JQN7G6
S8158G
A169X9KQI
7BMA3
A15A3FUIS
ZDPI4
A14LPCJ1O
1773B
190 0.32321429
70 0.5710754
66 0.24120603
39 0.29553903
39 0.3436853
38 0.39960239
34 0.58924731
30 0.41605839
25 0.32698413
25 0.30027548
20 0.375
17 0.60888889
15 0.34259259
13 0.43842365
13 0.44970414
11 0.38922156
11 0.6
10 0.39751553
10 0.57236842
10 0.5
9 0.60759494
9 0.66666667
8 0.61151079
8 0.67346939
8 0.61983471
7 0.43220339
7 0.44615385
6 0.60683761
6 0.44859813
6 0.52941176
6 0.63265306
6 0.60833333
6 0.52136752
6 0.59183673
6 0.5
5 0.50943396
5 0.55905512
4 0.60526316
4 0.67708333
4 0.58
4 0.63218391
4 0.51041667
4 0.54545455
4 0.55660377
4 0.50515464
4 0.46938776
3 0.68131868
3 0.62790698
3 0.65853659
3 0.61842105
3 0.59210526
3 0.52631579
3 0.65384615
3 0.59210526
3 0.55714286
3 0.55263158
3 0.58571429
2 0.65151515
2 0.625
2 0.57352941
2 0.65151515
2 0.62121212
2 0.59090909
2 0.60294118
2 0.62121212
2 0.63636364
2 0.54545455
2 0.65151515
2 0.63768116
2 0.64705882
2 0.57575758
2 0.59090909
2 0.60606061
2 0.65151515
2 0.70512821
2 0.54545455
2 0.66666667
2 0.67948718
2 0.64473684
2 0.63235294
0
0.2
0.4
0.6
0.8
0 50 100 150 200
Agreement of individual Turkers v # HITS
Number of HITs completed
Turker Num HITs Agreement
AHHIWLEV
WDLLL
A2T20OFL
M0TLDD
A3A5BBI7T
HNYAM
A3CS8NKS
64RQEK
A189OYGO
EA9SE1
ATFFJKQ1
CSJRS
ATZVHF3D
2AH3U
A2XIZGKT9
7P8OQ
A2LWJW1H
UBILFO
A38UZKXY
E10BN7
AAGC1LGV
UHGFM
A23V9FCS
ERQFF4
AVWJPWU
EE3U4Q
A3L4VHAI7
BMHC5
A13M4NKIO
XIIYL
A3FV4AOL7
N1JBS
A1XH9D9A
4NHG4
A2HNP1YL
1IBFMU
A2S44DG2
KJIR39
A15ENPH8
2MTLJF
AIG9DJSR
WGIY0
A1RXZQVZ
XHYGB9
A3UQJGXM
MOUVJQ
AIQ1I6ODSI
O56
A1CC4Z6E6
08UHR
A2VO7B7F
XDHM47
AH52LWLX
YY5KT
AB4WMS5
WKULFP
AUAZYGJW
M6JLK
A3J51GPLV
UTEVT
A2B032AIP
4I6HH
A2A4OZEX
98MVZS
A3R5BMMS
0J77E4
A2V2IA1P5
MWP9H
A2O7MKGT
GDB2O5
A1HUS55R
Z2IO4Q
AG1TKYUK
S97EV
A3U6F7Q77
IZXIO
AVUVW19F
TPZX6
A3QKCDF
WGN3QKL
A3KEBTLZF
JVKBG
A2HNJBV33
OV5EZ
A1IER46T8
7OQSH
A1HAIFNO
PYYZIJ
A1FU5C6U
S2L0J0
ARXKQFSE
KTYID
A3TY1HAD
H82YAP
A3RN8UHU
A2H8YD
A3NPGGTT
ZXNZUL
300 0.39726734
200 0.68883878
157 0.63821138
151 0.721513
141 0.73357106
104 0.66446062
92 0.63443223
83 0.7
74 0.57422222
67 0.70319241
48 0.56639248
37 0.73545706
36 0.70944993
36 0.728739
31 0.7250384
29 0.64666667
28 0.70786517
22 0.72791519
18 0.55384615
15 0.71369295
13 0.57941834
13 0.6719457
12 0.69392523
11 0.69638554
11 0.59701493
10 0.61463415
9 0.60913706
9 0.71907216
8 0.66219839
8 0.60742706
8 0.67875648
8 0.70866142
7 0.62303665
7 0.69518717
6 0.72965879
6 0.64347826
4 0.68674699
4 0.69325153
3 0.66878981
3 0.66049383
3 0.68012422
3 0.68152866
3 0.7
3 0.67405063
3 0.67924528
2 0.67763158
2 0.67763158
2 0.69871795
2 0.67105263
0
0.2
0.4
0.6
0.8
0 75 150 225 300
Agreement of Individuals on Yes No
Number of HITs completed
Figure 2: The agreement of individual Turkers
with the experts. The most prolific Turker per-
formed barely above chance, indicating random
clicking. This suggests that users who contribute
more tend to have lower quality.
votes are counted equally.
Weighting votes Not all Turkers are created
equal. The quality of their works varies. Fig-
ure 2 shows the agreement of individual Turkers
with expert annotators, plotted against the num-
ber of HITs they completed. The figure shows
that their agreement varies considerably, and that
Turker who completed the most judgments was
among the worst performing.
To avoid letting careless annotators drag down
results, we experimented with weighted voting.
We weighted votes in two ways:
? Votes were weighted by measuring agree-
ment with experts on the 10 initial judgments
made. This would be equivalent to giving
Turkers a pretest on gold standard data and
then calibrating their contribution based on
how well they performed.
? Votes were weighted based on how often one
Turker agreed with the rest of the Turkers
over the whole data set. This does not re-
quire any gold standard calibration data. It
goes beyond simple voting, because it looks
at a Turker?s performance over the entire set,
rather than on an item-by-item basis.
Figure 1 shows that these weighting mechanisms
perform similarly well. For this task, deriving
weights from agreement with other non-experts
is as effective as deriving weights from experts.
Moreover, by weighting the votes of five Turkers,
288
E-all E0 E0.o E1 E1.o E2 E2.o E3 E3.o E4 E4.o
E-all
E0
E0.o
E1
E1.o
E2
E2.o
E3
E3.o
E4
E4.o
N.ew.1.0
N.ew.1.1
N.ew.1.2
N.ew.1.3
N.ew.1.4
N.ew.1.5
N.ew.1.6
N.ew.1.7
N.ew.1.8
N.ew.1.9
N.ew.2.0
N.ew.2.1
N.ew.2.2
N.ew.2.3
N.ew.2.4
N.ew.2.5
N.ew.2.6
N.ew.2.7
N.ew.2.8
N.ew.2.9
N.ew.3.0
N.ew.3.1
N.ew.3.2
N.ew.3.3
N.ew.3.4
N.ew.3.5
N.ew.3.6
N.ew.3.7
N.ew.3.8
N.ew.3.9
N.ew.4.0
N.ew.4.1
N.ew.4.2
N.ew.4.3
N.ew.4.4
N.ew.4.5
N.ew.4.6
N.ew.4.7
N.ew.4.8
N.ew.4.9
N.ew.5.0
N.nw.1.0
N.nw.1.1
N.nw.1.2
N.nw.1.3
N.nw.1.4
N.nw.1.5
N.nw.1.6
N.nw.1.7
N.nw.1.8
N.nw.1.9
N.nw.2.0
N.nw.2.1
N.nw.2.2
N.nw.2.3
N.nw.2.4
N.nw.2.5
N.nw.2.6
N.nw.2.7
N.nw.2.8
N.nw.2.9
N.nw.3.0
N.nw.3.1
N.nw.3.2
N.nw.3.3
N.nw.3.4
N.nw.3.5
N.nw.3.6
N.nw.3.7
N.nw.3.8
N.nw.3.9
N.nw.4.0
N.nw.4.1
N.nw.4.2
N.nw.4.3
N.nw.4.4
N.nw.4.5
N.nw.4.6
N.nw.4.7
N.nw.4.8
N.nw.4.9
N.nw.5.0
N.uw.1.0
N.uw.1.1
N.uw.1.2
N.uw.1.3
N.uw.1.4
N.uw.1.5
N.uw.1.6
N.uw.1.7
N.uw.1.8
N.uw.1.9
N.uw.2.0
N.uw.2.1
N.uw.2.2
N.uw.2.3
N.uw.2.4
N.uw.2.5
N.uw.2.6
N.uw.2.7
N.uw.2.8
N.uw.2.9
N.uw.3.0
N.uw.3.1
N.uw.3.2
N.uw.3.3
N.uw.3.4
N.uw.3.5
N.uw.3.6
N.uw.3.7
N.uw.3.8
N.uw.3.9
N.uw.4.0
N.uw.4.1
N.uw.4.2
N.uw.4.3
N.uw.4.4
N.uw.4.5
N.uw.4.6
N.uw.4.7
N.uw.4.8
N.uw.4.9
N.uw.5.0
bleu
1 0.957 0.989 0.879 0.993 0.893 0.989 0.864 0.989 0.857 0.979
0.957 1 0.921 0.929 0.932 0.857 0.957 0.786 0.975 0.779 0.964
0.989 0.921 1 0.854 0.996 0.893 0.979 0.871 0.975 0.889 0.968
0.879 0.929 0.854 1 0.857 0.693 0.900 0.739 0.904 0.654 0.886
0.993 0.932 0.996 0.857 1 0.896 0.982 0.861 0.979 0.896 0.964
0.893 0.857 0.893 0.693 0.896 1 0.846 0.768 0.907 0.829 0.918
0.989 0.957 0.979 0.900 0.982 0.846 1 0.868 0.975 0.839 0.961
0.864 0.786 0.871 0.739 0.861 0.768 0.868 1 0.836 0.671 0.857
0.989 0.975 0.975 0.904 0.979 0.907 0.975 0.836 1 0.846 0.989
0.857 0.779 0.889 0.654 0.896 0.829 0.839 0.671 0.846 1 0.818
0.979 0.964 0.968 0.886 0.964 0.918 0.961 0.857 0.989 0.818 1
0.800 0.746 0.793 0.757 0.811 0.711 0.821 0.707 0.793 0.689 0.757
0.700 0.639 0.721 0.625 0.739 0.725 0.714 0.732 0.679 0.636 0.675
0.789 0.725 0.811 0.814 0.807 0.689 0.789 0.679 0.789 0.664 0.782
0.779 0.664 0.829 0.668 0.825 0.757 0.779 0.739 0.757 0.779 0.764
0.743 0.671 0.789 0.714 0.782 0.671 0.761 0.786 0.732 0.725 0.757
0.871 0.904 0.850 0.832 0.861 0.825 0.886 0.746 0.879 0.721 0.886
0.875 0.800 0.893 0.800 0.889 0.800 0.871 0.893 0.857 0.711 0.875
0.664 0.604 0.668 0.679 0.682 0.554 0.693 0.732 0.654 0.564 0.643
0.900 0.893 0.904 0.871 0.911 0.771 0.936 0.854 0.900 0.811 0.889
0.786 0.739 0.793 0.657 0.814 0.846 0.754 0.564 0.804 0.850 0.782 0.7679090909
0.804 0.743 0.829 0.771 0.825 0.786 0.793 0.704 0.825 0.736 0.821 0.0853592028
0.807 0.771 0.825 0.832 0.839 0.707 0.825 0.696 0.811 0.757 0.786
0.668 0.582 0.696 0.621 0.693 0.671 0.646 0.654 0.679 0.636 0.700
0.818 0.793 0.836 0.832 0.832 0.775 0.821 0.832 0.836 0.668 0.854
0.786 0.700 0.800 0.739 0.804 0.707 0.796 0.761 0.768 0.643 0.761
0.704 0.600 0.757 0.704 0.739 0.604 0.718 0.668 0.696 0.682 0.704
0.696 0.675 0.725 0.721 0.732 0.689 0.714 0.643 0.707 0.661 0.718
0.825 0.732 0.861 0.768 0.854 0.711 0.850 0.779 0.804 0.761 0.804
0.821 0.729 0.836 0.764 0.829 0.711 0.804 0.779 0.818 0.718 0.829
0.807 0.729 0.846 0.739 0.850 0.804 0.793 0.671 0.811 0.811 0.800 0.7546
0.836 0.746 0.861 0.739 0.868 0.796 0.821 0.718 0.836 0.829 0.821 0.0673480893
0.832 0.761 0.864 0.729 0.875 0.825 0.821 0.675 0.832 0.879 0.818
0.775 0.704 0.811 0.729 0.814 0.764 0.775 0.704 0.786 0.764 0.775
0.714 0.661 0.757 0.711 0.754 0.732 0.711 0.632 0.739 0.714 0.743
0.796 0.704 0.832 0.682 0.839 0.789 0.800 0.754 0.771 0.764 0.771
0.739 0.618 0.779 0.639 0.782 0.700 0.736 0.679 0.711 0.729 0.711
0.721 0.636 0.771 0.707 0.761 0.664 0.729 0.757 0.729 0.711 0.739
0.764 0.700 0.804 0.739 0.800 0.696 0.782 0.779 0.764 0.739 0.775
0.743 0.693 0.779 0.743 0.782 0.743 0.739 0.625 0.764 0.736 0.754
0.800 0.700 0.829 0.729 0.832 0.704 0.811 0.682 0.779 0.775 0.764 0.7559
0.743 0.654 0.796 0.704 0.793 0.707 0.754 0.664 0.736 0.761 0.732 0.0562407913
0.718 0.632 0.771 0.675 0.768 0.700 0.729 0.661 0.714 0.746 0.718
0.843 0.775 0.875 0.757 0.886 0.814 0.843 0.700 0.843 0.875 0.821
0.786 0.689 0.836 0.721 0.832 0.729 0.796 0.686 0.768 0.793 0.764
0.714 0.611 0.768 0.661 0.764 0.675 0.725 0.732 0.696 0.711 0.707
0.739 0.661 0.782 0.711 0.786 0.711 0.743 0.632 0.743 0.761 0.729
0.775 0.707 0.804 0.743 0.807 0.682 0.800 0.654 0.761 0.757 0.754
0.754 0.693 0.804 0.743 0.796 0.736 0.750 0.618 0.768 0.779 0.771
0.796 0.711 0.850 0.746 0.836 0.746 0.796 0.750 0.800 0.800 0.814
0.739 0.636 0.793 0.671 0.789 0.707 0.750 0.700 0.721 0.746 0.725 0.7468
0.0574048555
0.743 0.654 0.796 0.704 0.793 0.707 0.754 0.664 0.736 0.761 0.732 0.7312727273
0.546 0.504 0.571 0.546 0.564 0.539 0.568 0.425 0.543 0.536 0.568
0.771 0.682 0.814 0.721 0.804 0.689 0.793 0.711 0.768 0.764 0.764
0.900 0.875 0.889 0.814 0.882 0.889 0.868 0.904 0.914 0.679 0.946
0.686 0.711 0.661 0.739 0.686 0.643 0.718 0.596 0.704 0.557 0.689
0.736 0.721 0.768 0.789 0.754 0.700 0.750 0.718 0.757 0.629 0.793
0.854 0.832 0.868 0.868 0.864 0.786 0.864 0.782 0.879 0.729 0.875
0.871 0.825 0.864 0.789 0.861 0.721 0.889 0.782 0.861 0.775 0.864
0.746 0.686 0.736 0.654 0.764 0.682 0.739 0.532 0.743 0.782 0.686
0.807 0.746 0.796 0.775 0.814 0.696 0.811 0.679 0.779 0.643 0.757
0.704 0.661 0.704 0.711 0.707 0.564 0.754 0.579 0.693 0.632 0.668 0.7362636364
0.821 0.761 0.857 0.804 0.854 0.754 0.839 0.725 0.825 0.771 0.814 0.1071247817
0.782 0.704 0.825 0.725 0.821 0.754 0.786 0.686 0.789 0.800 0.786
0.829 0.746 0.839 0.786 0.850 0.689 0.839 0.696 0.807 0.761 0.786
0.596 0.500 0.657 0.618 0.646 0.550 0.618 0.664 0.600 0.604 0.596
0.800 0.768 0.839 0.857 0.821 0.718 0.811 0.743 0.825 0.707 0.836
0.714 0.679 0.750 0.729 0.761 0.679 0.739 0.604 0.714 0.729 0.704
0.689 0.629 0.682 0.604 0.707 0.704 0.679 0.489 0.686 0.671 0.654
0.743 0.671 0.775 0.689 0.779 0.729 0.754 0.829 0.729 0.650 0.743
0.811 0.732 0.854 0.796 0.839 0.679 0.839 0.775 0.789 0.732 0.804
0.800 0.718 0.811 0.729 0.814 0.768 0.775 0.668 0.786 0.675 0.793 0.7396272727
0.846 0.754 0.850 0.750 0.861 0.779 0.829 0.782 0.832 0.725 0.821 0.0794354922
0.739 0.646 0.775 0.679 0.779 0.675 0.757 0.679 0.721 0.736 0.718
0.700 0.607 0.743 0.661 0.739 0.675 0.700 0.718 0.707 0.696 0.711
0.764 0.671 0.782 0.704 0.789 0.714 0.761 0.746 0.739 0.636 0.743
0.864 0.839 0.879 0.864 0.889 0.779 0.882 0.746 0.864 0.775 0.850
0.768 0.686 0.793 0.732 0.800 0.661 0.793 0.779 0.746 0.704 0.743
0.757 0.664 0.782 0.736 0.771 0.639 0.775 0.764 0.739 0.625 0.754
0.836 0.757 0.846 0.789 0.854 0.707 0.846 0.768 0.825 0.757 0.811
0.729 0.707 0.754 0.761 0.757 0.704 0.736 0.564 0.754 0.718 0.750
0.832 0.761 0.861 0.768 0.868 0.779 0.836 0.725 0.836 0.832 0.818 0.7593363636
0.825 0.711 0.879 0.725 0.864 0.775 0.814 0.793 0.811 0.807 0.829 0.0655079346
0.718 0.636 0.768 0.711 0.761 0.664 0.739 0.714 0.714 0.689 0.718
0.786 0.707 0.836 0.750 0.821 0.704 0.804 0.757 0.775 0.761 0.796
0.764 0.664 0.818 0.693 0.807 0.714 0.775 0.761 0.750 0.761 0.768
0.693 0.611 0.750 0.704 0.739 0.632 0.707 0.639 0.693 0.707 0.700
0.789 0.729 0.825 0.779 0.832 0.714 0.807 0.668 0.782 0.779 0.764
0.725 0.625 0.786 0.689 0.771 0.664 0.736 0.721 0.711 0.725 0.732
0.814 0.732 0.861 0.746 0.854 0.761 0.825 0.739 0.811 0.821 0.814
0.768 0.675 0.818 0.700 0.814 0.739 0.779 0.725 0.761 0.775 0.761
0.743 0.643 0.800 0.650 0.793 0.736 0.754 0.704 0.729 0.775 0.739 0.7502181818
0.0566353833
0.739 0.646 0.793 0.682 0.789 0.736 0.739 0.682 0.739 0.761 0.739 0.7313636364
0.811 0.711 0.846 0.746 0.839 0.714 0.821 0.739 0.800 0.775 0.796
0.729 0.679 0.736 0.761 0.725 0.625 0.750 0.657 0.711 0.507 0.721
0.739 0.643 0.761 0.700 0.757 0.586 0.775 0.846 0.707 0.625 0.714
0.879 0.832 0.861 0.786 0.879 0.854 0.839 0.711 0.879 0.739 0.857
0.682 0.625 0.725 0.700 0.711 0.668 0.679 0.693 0.714 0.661 0.732
0.711 0.650 0.757 0.671 0.732 0.764 0.675 0.604 0.736 0.679 0.779
0.854 0.754 0.875 0.796 0.864 0.739 0.839 0.879 0.836 0.679 0.850
0.800 0.736 0.836 0.782 0.821 0.746 0.796 0.707 0.829 0.761 0.814
0.886 0.829 0.907 0.804 0.893 0.775 0.893 0.832 0.893 0.850 0.896
0.764 0.696 0.800 0.718 0.789 0.782 0.732 0.750 0.779 0.689 0.814 0.7625454545
0.825 0.750 0.868 0.757 0.861 0.804 0.811 0.682 0.836 0.832 0.832 0.0784769745
0.879 0.804 0.911 0.793 0.907 0.793 0.871 0.764 0.857 0.839 0.871
0.771 0.689 0.818 0.743 0.814 0.707 0.786 0.661 0.768 0.779 0.757
0.804 0.686 0.854 0.693 0.843 0.764 0.800 0.814 0.779 0.757 0.800
0.761 0.671 0.807 0.704 0.804 0.739 0.764 0.704 0.764 0.768 0.761
0.764 0.711 0.796 0.768 0.789 0.721 0.754 0.739 0.789 0.718 0.804
0.879 0.804 0.907 0.800 0.914 0.811 0.868 0.736 0.875 0.879 0.861
0.804 0.711 0.846 0.786 0.832 0.646 0.821 0.750 0.786 0.761 0.793
0.843 0.768 0.886 0.821 0.879 0.721 0.854 0.768 0.836 0.821 0.836
0.714 0.625 0.761 0.718 0.754 0.643 0.711 0.704 0.718 0.696 0.725 0.7846
0.832 0.789 0.854 0.793 0.861 0.804 0.814 0.757 0.850 0.804 0.854 0.0639298928
0.807 0.775 0.818 0.829 0.825 0.668 0.832 0.661 0.807 0.757 0.796
0.729 0.675 0.754 0.761 0.757 0.632 0.761 0.650 0.721 0.650 0.711
0.746 0.625 0.796 0.679 0.786 0.671 0.746 0.782 0.714 0.686 0.739
0.771 0.650 0.811 0.679 0.800 0.704 0.761 0.800 0.739 0.682 0.771
0.886 0.825 0.914 0.825 0.918 0.829 0.882 0.757 0.893 0.864 0.879
0.779 0.696 0.821 0.761 0.814 0.679 0.793 0.793 0.775 0.746 0.782
0.857 0.775 0.889 0.761 0.896 0.821 0.846 0.718 0.854 0.868 0.839
0.875 0.775 0.918 0.779 0.911 0.800 0.864 0.768 0.861 0.861 0.864
0.754 0.675 0.789 0.704 0.796 0.729 0.757 0.704 0.757 0.761 0.746 0.7819
0.789 0.664 0.839 0.671 0.829 0.768 0.771 0.775 0.775 0.771 0.793 0.0698718289
0.829 0.736 0.875 0.761 0.864 0.746 0.846 0.818 0.811 0.779 0.821
0.832 0.729 0.861 0.771 0.850 0.725 0.821 0.782 0.825 0.750 0.832
0.871 0.771 0.914 0.782 0.907 0.775 0.871 0.775 0.850 0.850 0.854
0.850 0.754 0.896 0.754 0.889 0.814 0.839 0.739 0.846 0.854 0.846
0.725 0.621 0.775 0.721 0.761 0.618 0.736 0.686 0.714 0.693 0.721
0.832 0.743 0.879 0.757 0.871 0.771 0.843 0.768 0.825 0.825 0.825
0.796 0.693 0.843 0.718 0.832 0.711 0.811 0.768 0.775 0.775 0.789
0.775 0.671 0.804 0.707 0.811 0.704 0.764 0.629 0.757 0.764 0.743
0.818 0.707 0.850 0.725 0.843 0.714 0.818 0.768 0.796 0.775 0.807 0.7852818182
0.0622027068
0.818 0.725 0.864 0.732 0.857 0.757 0.829 0.750 0.804 0.818 0.811 0.7968181818
0.321 0.328 0.292 0.353 0.263 0.149 0.374 0.353 0.306 0.135 0.349
Choose
1 2 3 4 5
unweighted 0.7625454545 0.7846 0.7819 0.7852818182 0.7968181818
non-expert weights0.7362636364 0.7625454545 0.7593363636 0.7502181818 0.7313636364
expert weights 0.7679090909 0.7546 0.7559 0.7468 0.7313636364
1 2 3 4 5
unweighted
expert 
weights
non-expert
expert-expert
BLEU
0.7625454545 0.7846 0.7819 0.7852818182 0.7968181818
0.7679090909 0.7546 0.7559 0.7468 0.7313636364
0.7362636364 0.7625454545 0.7593363636 0.7502181818 0.7313636364
0.778 0.778 0.778 0.778 0.778
0.293 0.293 0.293 0.293 0.293
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 2 3 4 5
Correlation with Expert ranking of systems
Unweighted Non-expert weights
Expert-Expert BLEU
0
0.225
0.45
0.675
0.9
u
n
w
e
i
g
h
t
e
d
e
x
p
e
r
t
 
w
e
i
g
h
t
s
n
o
n
-
e
x
p
e
r
t
e
x
p
e
r
t
-
e
x
p
e
r
t
B
L
E
U
Spearman?s correlation
Figure 3: Correlation with experts? ranking of sys-
tems. All of the different ways of combining the
non-expert judgments perform at the upper bound
of expert-expert correlation. All correlate more
strongly than Bleu.
we are able to achieve the same rate of agreement
with experts as they achieve with each other.
Correlation when ranking systems In addi-
tion to measuring agreement with experts at
the sentence-level, we also compare non-expert
system-level rankings with experts. Following
Callison-Burch et al (2008), we assigned a score
to each of the 11 MT systems based on how of-
ten its translations were judged to be better than or
equal to any other system. These scores were used
to rank systems and we measured Spearman?s ?
against the system-level ranking produced by ex-
perts.
Figure 3 shows how well the non-expert rank-
ings correlate with expert rankings. An up-
per bound is indicated by the expert-expert bar.
This was created using a five-fold cross valida-
tion where we used 20% of the expert judgments
to rank the systems and measured the correlation
against the rankings produced by the other 80%
of the judgments. This gave a ? of 0.78. All ways
of combining the non-expert judgments resulted in
nearly identical correlation, and all produced cor-
relation within the range of with what we would
experts to.
The rankings produced using Mechanical Turk
had a much stronger correlation with the WMT08
expert rankings than the Blue score did. It should
be noted that the WMT08 data set does not have
multiple reference translations. If multiple ref-
erences were used that Bleu would likely have
stronger correlation. However, it is clear that the
cost of hiring professional translators to create
multiple references for the 2000 sentence test set
would be much greater than the $10 cost of col-
lecting manual judgments on Mechanical Turk.
5 Feasibility of more complex evaluations
In this section we report on a number of cre-
ative uses of Mechanical Turk to do more so-
phisticated tasks. We give evidence that Turkers
can create high quality translations for some lan-
guages, which would make creating multiple ref-
erence translations for Bleu less costly than using
professional translators. We report on experiments
evaluating translation quality with HTER and with
reading comprehension tests.
5.1 Creating multiple reference translations
In addition to evaluating machine translation qual-
ity, we also investigated the possibility of using
Mechanical Turk to create additional reference
translations for use with automatic metrics like
Bleu. Before trying this, we were skeptical that
Turkers would have sufficient language skills to
produce translations. Our translation HIT had the
following instructions:
Translate these sentences Your task is to translate 10 sen-
tences into English. Please make sure that your English
translation:
? Is faithful to the original in both meaning and style
? Is grammatical, fluent, and natural-sounding English
? Does not add or delete information from the original
text
? Does not contain any spelling errors
When creating your translation, please:
? Do not use any machine translation systems
? You may look up a word on wordreference.com if you
do not know its translation
Afterwards, we?ll ask you a few quick questions about your
language abilities.
We solicited translations for 50 sentences in
French, German, Spanish, Chinese and Urdu, and
designed the HIT so that five Turkers would trans-
late each sentence.
Filtering machine translation Upon inspecting
the Turker?s translations it became clear that many
had ignored the instructions, and had simply cut-
and-paste machine translation rather then translat-
ing the text themselves. We therefore set up a sec-
ond HIT to filter these out. After receiving the
289
Topline MTurk MT Topline MTurk MT
Spanish
German
French
Chinese
Urdu
0.54 0.50 0.41 Spanish STDDEV 0.07 0.03 0.02
0.54 0.48 0.33 German STDDEV 0.07 0.02 0.02
0.54 0.33 0.25 French STDDEV 0.07 0.02 0.01
0.54 0.52 0.24 Chinese STDDEV 0.07 0.02 0.00
0.32 0.21 0.14 Urdu STDDEV 0.02 0.02 0.01
0
0.10
0.20
0.30
0.40
0.50
0.60
Spanish German French Chinese
                                    Bleu scores of professional translators, Mechanical Turk, and MT
1 LDC translator v. other LDC translators Mechanical Turk v. other LDC MT v. other LDC
0
0.10
0.20
0.30
0.40
0.50
0.60
Urdu
Figure 4: Bleu scores quantifying the quality of Turkers? translations. The chart shows the average Bleu
score when one LDC translator is compared against the other 10 translators (or the other 2 translators in
the case of Urdu). This gives an upper bound on the expected quality. The Turkers? translation quality
falls within a standard deviation of LDC translators for Spanish, German and Chinese. For all languages,
Turkers produce significantly better translations than an online machine translation system.
translations, we had a second group of Turkers
clean the results.
Detect machine translation Please use two online machine
translation systems to translate the text into English, and then
copy-and-paste the translations into the boxes below. Finally,
look at a list of translations below and click on the ones that
look like they came from the online translation services.
We automatically excluded Turkers whose transla-
tions were flagged 30% of the time or more.
Quality of Turkers? translations Our 50 sen-
tence test sets were selected so that we could com-
pare the translations created by Turkers to transla-
tions commissioned by the Linguistics Data Con-
sortium. For the Chinese, French, Spanish, and
German translations we used the the Multiple-
Translation Chinese Corpus.
3
This corpus has
11 reference human translations for each Chinese
source sentence. We had bilingual graduate stu-
dents translate the first 50 English sentences of
that corpus into French, German and Spanish, so
that we could re-use the multiple English reference
translations. The Urdu sentences were taken from
the NIST MT Eval 2008 Urdu-English Test Set
4
which includes three distinct English translations
for every Urdu source sentence.
Figure 4 shows the Turker?s translation quality
in terms of the Bleu metric. To establish an upper
bound on expected quality, we determined what
3
LDC catalog number LDC2002T01
4
LDC catalog number LDC2009E11
the Bleu score would be for a professional trans-
lator when measured against other professionals.
We calculated a Bleu score for each of the 11
LDC translators using the other 10 translators as
the reference set. The average Bleu score for
LDC2002T01 was 0.54, with a standard deviation
of 0.07. The average Bleu for the Urdu test set is
lower because it has fewer reference translations.
To measure the Turkers? translation quality, we
randomly selected translations of each sentence
from Turkers who passed the Detect MT HIT, and
compared them against the same sets of 10 ref-
erence translations that the LDC translators were
compared against. We randomly sampled the
Turkers 10 times, and calculated averages and
standard deviations for each source language. Fig-
ure 4 the Bleu scores for the Turkers? translations
of Spanish, German and Chinese are within the
range of the LDC translators. For all languages,
the quality is significantly higher than an online
machine translation system. We used Yahoo?s Ba-
belfish for Spanish, German, French and Chinese,
5
was likely and Babylon for Urdu.
Demographics We collected demographic in-
formation about the Turkers who completed the
translation task. We asked how long they had spo-
ken the source language, how long they had spo-
5
We also compared against Google Translate, but ex-
cluded the results since its average Bleu score was better than
the LDC translators, likely because the test data was used to
train Google?s statistical system.
290
Spanish
Native lang English (7 people), Spanish (2), English-Spanish bilingual, Portuguese English, Hindi
Country USA (7 people), Mexico (3), Brazil, USA (2)
Spanish level 30+ years (2 people), 15 years (2), 6 years, 2 years (2), whole life (4) 18 years, 4 years
English level 15 years (3), whole life (9) whole life , 15 years
German
Native lang German (3), Turkish (2), Italian, Danish, English, Norwegian, Hindi Marathi, Tamil, Hindi, English
Country Germany (3), USA, Italy, China, Denmark, Turkey, Norway, India USA (2), India (2)
German level 20 years (2), 10 years (3), 5 years (2), 2 years, whole life (3) 10 years, 1 year (2)
English level 20+ years (4), 10-20 years (5) whole life whole life (2), 15-20 years (2)
French
Native lang English (9 people), Portuguese, Hindi English (2)
Country USA (6), Israel, Singapore, UK, Brazil, India USA (2)
French level 20+ years (4 people), 8-12 years (4), 5 years (2), 2 years 10 years, 1 years, 6 years
English level whole life (9), 20 years, 15 years whole life (2),
Chinese
Native lang Hindi (2) English (3) Hindi, Marathi, Tamil
Country India (2) India (3), USA (3)
Chinese level 2 years, 1 year 3 years, 2 years, none
English level 18 years, 20+ years 16 years, whole life (2)
Urdu
Native lang Urdu (6 people) Tamil (2), Hindi, Telugu
Country Pakistan (3), Bahrain, India, Saudi Arabia India (4)
Urdu level whole life (6 people) 2 years, 1 year, never (2)
English level 20+ years (5), 15 years (2), 10 years 10+ years (5), 5 years
Table 1: Self-reported demographic information from Turkers who completed the translation HIT. The
statistics on the left are for people who appeared to do the task honestly. The statistics on the right are
for people who appeared to be using MT (marked as using it 20% or more in the Detect MT HIT).
ken English, what their native language was, and
where they lived. Table 1 gives their replies.
Cost and speed We paid Turkers $0.10 to trans-
late each sentence, and $0.006 to detect whether a
sentence was machine translated. The cost is low
enough that we could create a multiple reference
set quite cheaply; it would cost less than $1,000 to
create 4 reference translations for 2000 sentences.
The time it took for the 250 translations to be
completed for each language varied. It took less
than 4 hours for Spanish, 20 hours for French, 22.5
hours for German, 2 days for Chinese, and nearly
4 days for Urdu.
5.2 HTER
Human-mediated translation edit rate (HTER)
is the official evaluation metric of the DARPA
GALE program. The evaluation is conducted an-
nually by the Linguistics Data Consortium, and
it is used to determine whether the teams partic-
ipating the program have met that year?s bench-
marks. These evaluations are used as a ?Go / No
Go? determinant of whether teams will continue
to receive funding. Thus, each team have a strong
incentive to get as good a result as possible under
the metric.
Each of the three GALE teams encompasses
multiple sites and each has a collection of ma-
chine translation systems. A general strategy em-
ployed by all teams is to perform system combi-
nation over these systems to produce a synthetic
translation that is better than the sum of its parts
(Matusov et al, 2006; Rosti et al, 2007). The con-
tribution of each component system is weighted
by the expectation that it will produce good out-
put. To our knowledge, none of the teams perform
their own HTER evaluations in order to set these
weights.
We evaluated the feasibility of using Mechan-
ical Turk to perform HTER. We simplified the
official GALE post-editing guidelines (NIST and
LDC, 2007). We provided these instructions:
Edit Machine Translation Your task is to edit the machine
translation making as few changes as possible so that it
matches the meaning of the human translation and is good
English. Please follow these guidelines:
? Change the machine translation so that it has the same
meaning as the human translation.
? Make the machine translation into intelligible English.
? Use as few edits as possible.
? Do not insert or delete punctuation simply to follow
traditional rules about what is ?proper.?
? Please do not copy-and-paste the human translation
into the machine translation.
291
Number of editors
System 0 1 2 3 4 5
google.fr-en .44 .29 .24 .22 .20 .19
google.de-en .48 .34 .30 .28 .25 .24
rbmt5.de-en .53 .41 .33 .28 .27 .25
geneva.de-en .65 .56 .50 .48 .45 .45
tromble.de-en .77 .75 .74 .73 .71 .70
Table 2: HTER scores for five MT systems. The
edit rate decreases as the number of editors in-
creases from zero (where HTER is simply the TER
score between the MT output and the reference
translation) and five.
We displayed 10 sentences from a news article. In
one column was the reference English translation,
in the other column were text boxes containing
the MT output to be edited. To minimize the edit
rate, we collected edits from five different Turkers
for every machine translated segment. We verified
these with a second HIT were we prompted Turk-
ers to:
Judge edited translations First, read the reference human
translation. After that judge the edited machine translation
using two criteria:
? Does the edited translation have the same meaning as
the reference human translation?
? Is it acceptable English? Some small errors are OK, so
long as its still understandable.
For the final score, we choose the edited segment
which passed the criteria and which minimized the
edit distance to the unedited machine translation
output. If none of the five edits was deemed to be
acceptable, then we used the edit distance between
the MT and the reference.
Setup We evaluated five machine translation
systems using HTER. These systems were se-
lected from WMT09 (Callison-Burch et al, 2009).
We wanted a spread in quality, so we took the top
two and bottom two systems from the German-
English task, and the top system from the French-
English task (which significantly outperformed
everything else). Based on the results of the
WMT09 evaluation we would expect the see the
following ranking from the least edits to the most
edits: google.fr-en, google.de-en, rbmt5.de-en,
geneva.de-en and tromble.de-en.
Results Table 2 gives the HTER scores for the
five systems. Their ranking is as predicted, indi-
cating that the editing is working as expected. The
table reports averaged scores when the five anno-
tators are subsampled. This gives a sense of how
much each additional editor is able to minimize
the score for each system. The difference between
the TER score with zero editors, and the HTER
five editors is greatest for the rmbt5 system, which
has a delta of .29 and is smallest for jhu-tromble
with .07.
5.3 Reading comprehension
One interesting technique for evaluating machine
translation quality is through reading comprehen-
sion questions about automatically translated text.
The quality of machine translation systems can be
quantified based on how many questions are an-
swered correctly.
Jones et al (2005) evaluated translation quality
using a reading comprehension test the Defense
Language Proficiency Test (DLPT), which is ad-
ministered to military translators. The DLPT con-
tains a collection of foreign articles of varying lev-
els of difficulties, and a set of short answer ques-
tions. Jones et alused the Arabic DLPT to do a
study of machine translation quality, by automat-
ically translating the Arabic documents into En-
glish and seeing how many human subjects could
successfully pass the exam.
The advantage of this type of evaluation is that
the results have a natural interpretation. They indi-
cate how understandable the output of a machine
translation system is better than Bleu does, and
better than other manual evaluation like the rela-
tive ranking. Despite this advantage, evaluating
MT through reading comprehension hasn?t caught
on, due to the difficulty of administering it and due
to the fact that the DLPT or similar tests are not
publicly available.
We conducted a reading comprehension evalua-
tion using Mechanical Turk. Instead of simply ad-
ministering the test on Mechanical Turk, we used
it for all aspects from test creation to answer grad-
ing. Our procedure was as follows:
Test creation We posted human translations of
foreign news articles, and ask Tukers to write three
questions and provide sample answers. We gave
simple instructions on what qualifies as a good
reading comprehension question.
Reading comprehension test Please read the short news-
paper article, and then write three reading comprehension
questions about it, giving sample answers for each of your
questions. Good reading comprehension questions:
292
? Ask about why something happened or why someone
did something.
? Ask about relationships between people or things.
? Should be answerable in a few words.
Poor reading comprehension questions:
? Ask about numbers or dates.
? Only require a yes/no answer.
Question selection We posted the questions for
each article back to Mechanical Turk, and asked
other Turkers to vote on whether each question
was a good and to indicate if it was redundant with
any other questions in the set. We sorted questions
to maximize the votes and minimized redundan-
cies using a simple perl script, which discarded
questions below a threshold, and eliminated all re-
dundancies.
Taking the test We posted machine translated
versions of the foreign articles along with the
questions, and had Turkers answer them. We en-
sured that no one would see multiple translations
of the same article.
Answer questions about a machine translated text You will
answer questions about an article that has been automat-
ically translated from another language into English. The
translation contains many errors, but the goal is to see how
understandable it is. Please do your best to guess at the right
answers to the questions. Please:
? Read through the automatically translated article.
? Answer the questions listed below, using just a few
words.
? Give your best guess at the answers, even if the trans-
lation is hard to understand.
? Don?t use any other information to answer the ques-
tions.
Grading the answers We aggregated the
answers and used Mechanical Turk to grade
them. We showed the human translation of the
article, one question, the sample answer, and
displayed all answers to it. After the Turkers
graded the answers, we calculated the percentage
of questions that were answered correctly for each
system.
Turkers created 90 questions for 10 articles, which
were subsequently filtered down to 47 good ques-
tions, ranging from 3?6 questions per article. 25
Turkers answered questions about each translated
article. To avoid them answering the questions
multiple times, we randomly selected which sys-
tem?s translation was shown to them. Each sys-
tem?s translation was displayed an average of 5
System % Correct Answers
reference 0.94
google.fr-en 0.85
google.de-en 0.80
rbmt5.de-en 0.77
geneva.de-en 0.63
jhu-tromble.de-en 0.50
Table 3: The results of evaluating the MT output
using a reading comprehension test
times per article. As a control, we had three Turk-
ers answer the reading comprehension questions
using the reference translation.
Table 3 gives the percent of questions that were
correctly answered using each of the different sys-
tems? outputs and using the reference translation.
The ranking is exactly what we would expect,
based on the HTER scores and on the human eval-
uation of the systems in WMT09. This again
helps to validate that the reading comprehension
methodology. The scores are more interpretable
than Blue scores and than the WMT09 relative
rankings, since it gives an indication of how un-
derstandable the MT output is.
Appendix A shows some sample questions and
answers for an article.
6 Conclusions
Mechanical Turk is an inexpensive way of gather-
ing human judgments and annotations for a wide
variety of tasks. In this paper we demonstrate
that it is feasible to perform manual evaluations
of machine translation quality using the web ser-
vice. The low cost of the non-expert labor found
on Mechanical Turk is cheap enough to collect re-
dundant annotations, which can be utilized to en-
sure translation quality. By combining the judg-
ments of many non-experts we are able to achieve
the equivalent quality of experts.
The suggests that manual evaluation of trans-
lation quality could be straightforwardly done to
validate performance improvements reported in
conference papers, or even for mundane tasks
like tracking incremental system updates. This
challenges the conventional wisdom which has
long held that automatic metrics must be used
since manual evaluation is too costly and time-
consuming.
We have shown that Mechanical Turk can be
used creatively to produce quite interesting things.
293
We showed how a reading comprehension test
could be created, administered, and graded, with
only very minimal intervention.
We believe that it is feasible to use Mechanical
Turk for a wide variety of other machine translated
tasks like creating word alignments for sentence
pairs, verifying the accuracy of document- and
sentence-alignments, performing non-simulated
active learning experiments for statistical machine
translation, even collecting training data for low
resource languages like Urdu.
The cost of using Mechanical Turk is low
enough that we might consider attempting
quixotic things like human-in-the-loop minimum
error rate training (Zaidan and Callison-Burch,
2009), or doubling the amount of training data
available for Urdu.
Acknowledgments
This research was supported by the EuroMatrix-
Plus project funded by the European Commission,
and by the US National Science Foundation under
grant IIS-0713448. The views and findings are the
author?s alone.
A Example reading comprehension
questions
Actress Heather Locklear arrested for driving under the
influence of drugs
The actress Heather Locklear, Amanda on the popular se-
ries Melrose Place, was arrested this weekend in Santa Bar-
bara (California) after driving under the influence of drugs. A
witness saw her performing inappropriate maneuvers while
trying to take her car out of a parking space in Montecito, as
revealed to People magazine by a spokesman for the Califor-
nian Highway Police. The witness stated that around 4.30pm
Ms. Locklear ?hit the accelerator very roughly, making ex-
cessive noise and trying to take the car out from the park-
ing space with abrupt back and forth maneuvers. While re-
versing, she passed several times in front of his sunglasses.?
Shortly after, the witness, who at first, apparently had not rec-
ognized the actress, saw Ms. Locklear stopping in a nearby
street and leaving the vehicle.
It was this person who alerted the emergency services, be-
cause ?he was concerned about Ms. Locklear?s life.? When
the patrol arrived, the police found the actress sitting inside
her car, which was partially blocking the road. ?She seemed
confused,? so the policemen took her to a specialized centre
for drugs and alcohol and submitted her a test. According to a
spokesman for the police, the actress was cooperative and ex-
cessive alcohol was ruled out from the beginning, even if ?as
the officers initially observed, we believe Ms. Locklear was
under the influences drugs.? Ms. Locklear was arrested under
suspicion of driving under the influence of some - unspecified
substance, and imprisoned in the local jail at 7.00pm, to be re-
leased some hours later. Two months ago, Ms. Locklear was
released from a specialist clinic in Arizona where she was
treated after an episode of anxiety and depression.
4 questions were selected
? Why did the bystander call emergency services?
He was concerned for Ms. Locklear?s life.
? Why was Heather Locklear arrested in Santa Barbara?
Because she was driving under the influence of drugs
? Where did the witness see her acting abnormally?
Pulling out of parking in Montecito
? Where was Ms. Locklear two months ago?
She was at a specialist clinic in Arizona.
5 questions were excluded as being redundant
? What was Heather Locklear arrested for?
Driving under the influence of drugs
? Where was she taken for testing?
A specialized centre for drugs and alcohol
? Why was Heather Locklear arrested?
She was arested on suspicion of driving under the in-
fluence of drugs.
? Why did the policemen lead her to a specialized centre
for drugs and alcohol
Because she seemed confused.
? For what was she cured for two months ago?
She was cured for anxiety and depression.
Answers to Where was Ms. Locklear two months ago?
that were judged to be correct:
Arizona hospital for treatment of depression; at a treat-
mend clinic in Arizona; in the Arizona clinic being treated
for nervous breakdown; a clinic in Arizona; Arizona, un-
der treatment for depression; She was a patient in a clinic
in Arizona undergoing treatment for anxiety and depression;
In an Arizona mental health facility ; A clinic in Arizona.;
In a clinic being treated for anxiety and depression.; at an
Arizona clinic
These answers were judged to be incorrect: Locklear
was retired in Arizona; Arizona; Arizona; in Arizona;
Ms.Locklaer were laid off after a treatment out of the clinic
in Arizona.
References
Bogdan Babych and Anthony Hartley. 2004. Extend-
ing the Bleu MT evaluation method with frequency
weightings. In Proceedings of ACL.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In Proceedings of EACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation (WMT09), March.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008. Decomposability of trans-
lation metrics for improved evaluation and efficient
algorithms. In Proceedings of EMNLP.
294
Douglas Jones, Wade Shen, Neil Granoien, Martha
Herzog, and Clifford Weinstein. 2005. Measuring
translation quality by testing English speakers with
a new defense language proficiency test for Arabic.
In Proceedings of the 2005 International Conference
on Intelligence Analysis.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation for multiple
machine translation systems using enhanced hypoth-
esis alignment. In Proceedings of EACL.
NIST and LDC. 2007. Post editing guidelines for
GALE machine translation evaluation. Guidelines
developed by the National Institute of Standards and
Technology (NIST), and the Linguistic Data Consor-
tium (LDC).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Michael Paul. 2006. Overview of the IWSLT 2006
evaluation campaign. In Proceedings of Interna-
tional Workshop on Spoken Language Translation.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple
machine translation systems. In Proceedings of
HLT/NAACL.
Markus Schulze. 2003. A new monotonic and clone-
independent single-winner election method. Voting
Matters, (17), October.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of EMNLP.
Omar F. Zaidan and Chris Callison-Burch. 2009. Fea-
sibility of human-in-the-loop minimum error rate
training. In Proceedings of EMNLP.
295
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 381?390,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Improved Statistical Machine Translation
Using Monolingually-Derived Paraphrases
Yuval Marton,
?
Chris Callison-Burch,
?
and Philip Resnik
?
?
Department of Linguistics and the CLIP Lab
at the Institute for Advanced Computer Studies (UMIACS)
University of Maryland College Park, MD 20742-7505, USA
{ymarton,resnik}@umiacs.umd.edu
?
Computer Science Department, Johns Hopkins University
3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218
ccb@cs.jhu.edu
Abstract
Untranslated words still constitute a ma-
jor problem for Statistical Machine Trans-
lation (SMT), and current SMT systems
are limited by the quantity of parallel
training texts. Augmenting the training
data with paraphrases generated by pivot-
ing through other languages alleviates this
problem, especially for the so-called ?low
density? languages. But pivoting requires
additional parallel texts. We address this
problem by deriving paraphrases monolin-
gually, using distributional semantic simi-
larity measures, thus providing access to
larger training resources, such as compa-
rable and unrelated monolingual corpora.
We present what is to our knowledge the
first successful integration of a colloca-
tional approach to untranslated words with
an end-to-end, state of the art SMT sys-
tem demonstrating significant translation
improvements in a low-resource setting.
1 Introduction
Phrase-based systems, flat and hierarchical alike
(Koehn et al, 2003; Koehn, 2004b; Koehn et al,
2007; Chiang, 2005; Chiang, 2007), have achieved
a much better translation coverage than word-
based ones (Brown et al, 1993), but untranslated
words remain a major problem in SMT. For ex-
ample, according to Callison-Burch et al (2006),
a SMT system with a training corpus of 10,000
words learned only 10% of the vocabulary; the
same system learned about 30% with a training
corpus of 100,000 words; and even with a large
training corpus of nearly 10,000,000 words it only
reached about 90% coverage of the source vocab-
ulary. Coverage of higher order n-gram levels is
even harder. This problem plays a major part in re-
ducing machine translation quality, as reflected by
both automatic measures such as BLEU (Papineni
et al, 2002) and human judgment tests. Improving
translation coverage accurately is therefore impor-
tant for SMT systems.
The first solution that might come to mind is
to use larger parallel training corpora. However,
current state-of-the-art SMT systems cannot learn
from non-aligned corpora, while sentence-aligned
parallel corpora (bitexts) are a limited resource
(See Section 2 for discussion of automatically-
compiled bitexts). Another direction might be
to make use of non-parallel corpora for training.
However, this requires developing techniques to
extract alignments or translations from them, and
in a sufficiently fast, memory-efficient, and scal-
able manner. One approach that can, in princi-
ple, better exploit both alignments from bitexts
and make use of non-parallel corpora is the dis-
tributional collocational approach, e.g., as used by
Fung and Yee (1998) and Rapp (1999). However,
the systems described there are not easily scalable,
and require pre-computation of a very large col-
location counts matrix. Related attempts propose
generating bitexts from comparable and ?quasi-
comparable? bilingual texts by iteratively boot-
strapping documents, sentences, and words (Fung
and Cheung, 2004), or by using a maximum
entropy classifier (Munteanu and Marcu, 2005).
Alignment accuracy remains a challenge for them.
Recent work has proposed augmenting the
training data with paraphrases generated by pivot-
ing through other languages (Callison-Burch et al,
2006; Madnani et al, 2007). This indeed allevi-
ates the vocabulary coverage problem, especially
for the so-called ?low density? languages. How-
ever, these approaches still require bitexts where
381
one side contains the original source language.
The paradigm described in this paper involves
constructing monolingual distributional profiles
(DPs; a.k.a. word association profiles, or co-
occurrence vectors) of out-of-vocabulary words
and phrases in the source language; then, gener-
ating paraphrase candidates from phrases that co-
occur in similar contexts, and assigning them sim-
ilarity scores. The highest ranking paraphrases
are used to augment the translation phrase table.
The table augmentation idea is similar to Callison-
Burch et al?s (Callison-Burch et al, 2006), but
our proposed paradigm does not require using a
limited resource such as parallel texts in order
to generate paraphrases. Moreover, our proposed
paradigm can, in principle, achieve large-scale ac-
quisition of paraphrases with high semantic simi-
larity. However, using parallel training texts in
pivoting techniques offers the potential advantage
of implicit translational knowledge, in the form
of sentence alignments, while our approach is un-
guided in this respect. Therefore, we conducted
experiments to find out how these relative advan-
tages play out. We present here, to our knowledge
for the first time, positive results of integrating dis-
tributional monolingually-derived paraphrases in
an end-to-end state-of-the-art SMT system.
In the rest of this paper we discuss related work
in Section 2, describe the distributional hypothesis
and distributional profiles in Section 3, and present
the monolingually-derived paraphrase generation
system in Section 4. We report our experiments
and results in Section 5, and conclude by dis-
cussing the implications and future research direc-
tions in Section 6.
2 Related Work
This is not the first to attempt to ameliorate the
out-of-vocabulary (OOV) words problem in sta-
tistical machine translation, and other natural lan-
guage processing tasks. This work is most closely
related to that of Callison-Burch et al (2006),
who also translate source-side paraphrases of the
OOV phrases. There, paraphrases are generated
from bitexts of various language pairs, by ?pivot-
ing?: translating the OOV phrases to an additional
language (or languages) and back to the source
language. The quality of these paraphrases is es-
timated by marginalizing translation probabilities
to and from the additional language side(s) e, as
follows: p(f
2
|f
1
) =
?
e
p(e|f
1
)p(f
2
|e). A ma-
jor disadvantage of their approach is that it relies
on the availability of parallel corpora in other lan-
guages. While this works for English and many
European languages, it is far less likely to help
when translating from other source languages, for
which bitexts are scarce or non-existent. Also,
the pivoting approach is inherently noisy (in both
the paraphrase candidates? correct sense, and their
translational likelihood), and it is likely to fare
poorly with out-of-domain translation. One ad-
vantage of the bitext-dependent pivoting approach
is the use of the additional human knowledge that
is encapsulated in the parallel sentence alignment.
However, we argue that the ability to use much
larger resources for paraphrasing should trump the
human knowledge advantage.
More recently, Callison-Burch (2008) has im-
proved performance of this pivoting technique by
imposing syntactic constraints on the paraphrases.
The limitation of such an approach is the reliance
on a good parser (in addition to reliance on bi-
texts), but a good parser is not available in all
languages, especially not in resource-poor lan-
guages. Another approach using a pivoting tech-
nique augments the human reference translation
with paraphrases, creating additional translation
?references? (Madnani et al, 2007). Both ap-
proaches have shown gains in BLEU score.
Barzilay and McKeown (2001) extract para-
phrases from a monolingual parallel corpus, con-
taining multiple translations of the same source.
In addition to the parallel corpus usage limitations
described above, this technique is further limited
by the small size of such materials, which are even
scarcer than the resources in the pivoting case.
Dolan et al (2004) explore generating para-
phrases by edit-distance and headlines of time-
and topic-clustered news articles; they do not ad-
dress the OOV problem directly, as their focus
is sentence-level paraphrases; although they use
a standard SMT measure, alignment error rate
(AER), they only report results of the alignment
quality, and not of an end-to-end SMT system.
Much of the previous research largely focused on
morphological analysis in order to reduce type
sparseness; Callison-Burch et al (2006) list some
of the influential work in that direction.
Work that relies on the distributional hypoth-
esis using bilingual comparable corpora (with-
out the need for bitexts), typically uses a seed
lexicon for ?bridging? source language phrases
382
with their target languages paraphrases (Fung and
Yee, 1998; Rapp, 1999; Diab and Finch, 2000).
This approach is sometimes viewed as, or com-
bined with, an information retrieval (IR) approach,
and normalizes strength-of-association measures
(see Section 3) with IR-related measures such as
TF/IDF (Fung and Yee, 1998). To date, reported
implementations suffer from scalability issues, as
they pre-compute and hold in memory a huge col-
location matrix; we know of no report of using this
approach in an end-to-end SMT system.
Another approach aiming to reduce OOV rate
concentrates on increasing parallel training set
size without using more dedicated human transla-
tion (Resnik and Smith, 2003; Oard et al, 2003).
3 Collocational Profiles
The distributional hypothesis and distribu-
tional profiles. Natural language processing
(NLP) applications that assume the distributional
hypothesis (Harris, 1940; Firth, 1957) typically
keep track of word co-occurrences in distribu-
tional profiles (a.k.a. collocation vectors, or con-
text vectors). Each distributional profile DP
u
(for some word u) keeps counts of co-occurrence
of u with all words within a usually fixed dis-
tance from each of its occurrences (a sliding win-
dow) in some training corpus. More advanced pro-
files keep ?strength of association? (SoA) infor-
mation between u and each of the co-occurring
words, which is calculated from the counts of u,
the counts of the other word, their co-occurrence
count, and the count of all words in the corpus
(corpus size). The information on the other words
with respect to u is typically kept in a vector whose
dimensions correspond to all words in the training
corpus. This is described in Equation (1), where
V is the training corpus vocabulary:
DP
u
= {< w
i
, SoA(u,w
i
) > |u,w
i
? V }
for all i s.t. 1 ? i ? |V |
(1)
Semantic similarity between words u and v can
be estimated by calculating the similarity (vector
distance) between their profiles. Slightly more for-
mally, the distributional hypothesis assumes that
if we had access to the hypothetical true (psycho-
linguistic) semantic similarity function over word
pairs, semsim(u, v), then
?u, v, w ? V,
[semsim(u, v) > semsim(u,w)] =?
[psim(DP
u
, DP
v
) > psim(DP
u
, DP
w
)],
(2)
where V is the language vocabulary, DP
word
is
the distributional profile of word, and psim() is
a 2-place vector similarity function (all further
described below). Paraphrasing and other NLP
applications that are based on the distributional
hypothesis assume entailment in the reverse di-
rection: the right-hand-side of Formula (2) (pro-
file/vector similarity) entails the left-hand-side
(semantic similarity).
The sliding window and word association (SoA)
measures. Some researchers count positional
collocations in a sliding window, i.e., the co-
counts and SoA measures are calculated per rel-
ative position (e.g., for some word/token u, po-
sition 1 is the token immediately after u; posi-
tion -2 is the token preceding the token that pre-
cedes u) (Rapp, 1999); other researchers use non-
positional (which we dub here flat) collocations,
meaning, they count all token occurrences within
the sliding window, regardless of their positions
in it relative to u (McDonald, 2000; Mohammad
and Hirst, 2006). We use here flat collocations
in a 6-token sliding window. Beside simple co-
occurrence counts within sliding windows, other
SoA measures include functions based on TF/IDF
(Fung and Yee, 1998), mutual information (PMI)
(Lin, 1998), conditional probabilities (Schuetze
and Pedersen, 1997), chi-square test, and the log-
likelihood ratio (Dunning, 1993).
Profile similarity measures. A profile similar-
ity function psim(DP
u
, DP
v
) is typically defined
as a two-place function, taking vectors as argu-
ments, each vector representing a distributional
profile of some word u and v, respectively, and
whose cells contain the SoA of u (or v) with each
word (?collocate?) in the known vocabulary. Sim-
ilarity can be (and have been) estimated in several
ways, e.g., the cosine coefficient, the Jaccard co-
efficient, the Dice coefficient, and the City-Block
measure. The formula for the cosine function for
similarity measure is given in Eq. (3):
383
psim(DP
u
, DP
v
)
= cos(DP
u
, DP
v
)
=
?
w
i
?V
SoA(u,w
i
)SoA(v, w
i
)
?
?
w
i
?V
SoA(u,w
i
)
2
?
?
w
i
?V
SoA(v, w
i
)
2
(3)
In principle, any SoA can be used with any
profile similarity measure. However, in practice,
only some SoA/similarity measure combinations
do well, and finding the best combination is still
more art than science. Some successful combina-
tions are cos
CP
(Schuetze and Pedersen, 1997),
Lin
PMI
(Lin, 1998), City
LL
(Rapp, 1999), and
Jensen?Shannon divergence of conditional prob-
abilities (JSD
CP
). We use here cosine of log-
likelihood vectors (McDonald, 2000).
Phrasal distributional profiles. Word DPs can
be generalized to phrasal DPs, simply by count-
ing words that co-occur within a sliding window
around the target phrase?s occurrences (i.e., count-
ing occurrences of words up to 6 words before
or after the target phrase). For example, when
building a DP for the target phrase counting words
in the previous sentence, then simply is in rela-
tive position -2, and sliding is in relative posi-
tion 5. Searching for similar phrasal DPs poses
an additional challenge over the word DP case
(see Section 4), but there is no additional diffi-
culty in building the phrasal profile itself as de-
scribed above. In preliminary experiments we
found no gain in using phrasal collocates (i.e.,
count how many times a phrase of more than one
word co-occurs in a sliding window around the tar-
get word/phrase).
4 Searching and Scoring Phrasal
Paraphrases
The system design is as follows: upon receiv-
ing OOV phrase phr, build distributional profile
DP
phr
. Next, gather contexts: for each occur-
rence of phr, keep surrounding (left and right)
context L__R. For each such context, gather para-
phrase candidates X which occur between L and
R in other locations in the training corpus, i.e.,
all X such that LXR occur in the corpus. Fi-
nally, rank all candidates X , by building distribu-
tional profile DP
X
and measuring profile similar-
ity between DP
X
and DP
phr
, for each X . Output
k-best candidates above a certain similarity score
threshold. The rest of this section describes this
system in more detail.
Build phrasal profile DP
phr
. Build a profile of
all word collocates, as described in Section 3. Use
sliding window of size MaxPos = 6. If phr
is very frequent (above some threshold of t oc-
currences), uniformly sample only t occurrences,
multiplying the gathered co-counts by factor of
count(phr)/t. We set t = 10000.
Gather context. The challenge in choosing the
relevant context is this: if it is very short and/or
very frequent (e.g., ?the __ is?), then it might not
be very informative, in the sense that many words
can appear in that context (in this example, practi-
cally any noun); however, if it is too long (too spe-
cific), then it might not occur enough times else-
where (or not at all) in the training corpus. There-
fore, to balance between these two extremes, we
use the following heuristics. Start small: Start
with setting the left part of the context L to be a
single word/token to the left of phrase phr. If it
is stoplisted, append the next word to the left (now
having a bigram left context instead of a unigram),
and repeat until the left context is not in the sto-
plist. Repeat similarly for R, the context to the
right of phr. Add the resulting L__R context to
a context list. We stoplist ?promiscuous? words,
i.e., those that have more than StoplistThreshold
collocates in the training corpus, using the above
MaxPos parameter value. We also stoplist bi-
grams which occur more than t times and com-
prise solely from stoplisted unigrams.
Gather candidates. For each gathered context
in the context list, gather all paraphrase candidate
phrases X that connect left hand side context L
with right hand side context R, i.e., gather all X
such that the sequence LXR occurs in the corpus.
In practice, to keep search complexity low, limit
X to be up to length MaxPhraseLen. Also, to
further speed up runtime, we uniformly sample the
context occurrences.
Rank candidates. For each candidate X ,
build distributional profile DP
X
, and evaluate
psim(DP
phr
, DP
X
).
Output k-best candidates. Output k-best para-
phrase candidates for phrase phr, in descending
order of similarity. We set k = 20. Filter out para-
phrases with score less than minScore.
384
5 Experiment
We examined the application of the system?s para-
phrases to handling unknown phrases when trans-
lating from English into Chinese (E2C) and from
Spanish into English (S2E). For all baselines we
used the phrase-based statistical machine transla-
tion system Moses (Koehn et al, 2007), with the
default model features, weighted in a log-linear
framework (Och and Ney, 2002). Feature weights
were set with minimum error rate training (Och,
2003) on a development set using BLEU (Papineni
et al, 2002) as the objective function. Test re-
sults were evaluated using BLEU and TER (Snover
et al, 2005). The phrase translation probabili-
ties were determined using maximum likelihood
estimation over phrases induced from word-level
alignments produced by performing Giza++ train-
ing (Och and Ney, 2000) on both source and tar-
get sides of the parallel training sets. When the
baseline system encountered unknown words in
the test set, its behavior was simply to reproduce
the foreign word in the translated output.
The paraphrase-augmented systems were iden-
tical to the corresponding baseline system, with
the exception of additional (paraphrase-based)
translation rules, and additional feature(s). Simi-
larly to Callison-Burch et al (2006), we added the
following feature:
h(e, f) =
8
>
>
>
<
>
>
>
:
psim(DP
f
?
, DP
f
) If phrase table entry (e, f)
is generated from (e, f
?
)
using monolingually-
derived paraphrases.
1 Otherwise,
(4)
Note that it is possible to construct a new trans-
lation rule from f to e via more than one pair of
source-side phrase and its paraphrase; e.g., if f
1
is a paraphrase of f , and so is f
2
, and both f
1
, f
2
translate to the same e, then both lead to the con-
struction of the new rule translating f to e, but
with potentially different feature scores.
In order to eliminate this duplicity and lever-
age over these alternate paths which can be used
to increase our confidence level in the new rule,
we did the following: For each paraphrase f
of some source-side phrases f
i
, with respec-
tive similarity scores sim(f
i
, f), we calculated
an aggregate score asim with a ?quasi-online-
updating? method as follows: asim
i
= (1 ?
asim
i?1
)sim(f
i
, f), where asim
0
= 0. The ag-
gregate score asim is updated in an ?online? fash-
ion with each pair f
i
, f as they are processed, but
only the final asim
k
score is used, after all k pairs
have been processed. Simple arithmetics can show
that this method is insensitive to the order in which
the paraphrases are processed. We only augment
the phrase table with a single rule from f to e,
and in it are the feature values of the phrase f
i
for
which the score sim(f
i
, f) was the highest.
5.1 English-to-Chinese Translation
For the English-Chinese (E2C) baseline system,
we trained on the LCD Sinorama and FBIS
tests (LCD2005T10 and LCD2003E14), and seg-
mented the Chinese side with the Stanford Seg-
menter (Tseng et al, 2005). After tokenization
and filtering, this bitext contained 231,586 lines
(6.4M + 5.1M tokens). We trained a trigram lan-
guage model on the Chinese side. We then split the
bitext to 32 even slices, and constructed a reduced
set of about 29,000 lines (sentences) by using only
every eighth slice. The purpose of creating this
subset model was to simulate a resource-poor lan-
guage. See Table 1.
Set # Tokens Source+Target
E2C 29K 0.8 + 0.6
E2C Full 6.4 + 5.1
bnc+apw 187
S2E 10K 0.3 + 0.3
S2E 20K 0.6 + 0.6
S2E 80K 2.3 + 2.3
wmt09 84
wmt09+acquis 139
wmt09+acquis+afp 402
Table 1: Training set sizes (million tokens).
For development, we used the Chinese-English
NIST MT 2005 evaluation set, taking one of the
English references as source, and the Chinese
source as a single reference translation. We tested
the system using the English-Chinese NIST MT
evaluation 2008 test set with its four reference
translations.
We augmented the E2C baseline models with
paraphrases generated as described above, train-
ing on the British National Corpus (BNC)
v3 (Burnard, 2000) and the first 3 million lines
of the English Gigaword v2 APW, totaling 187M
terms after tokenization, and number and punc-
tuation removal. We generated paraphrases for
phrases up to six tokens in length, and used an ar-
385
bitrary similarity threshold of minScore = 0.3.
We experimented with three variants: adding a
single additional feature for all paraphrases (1-
6grams); using only paraphrases of unigrams
(1grams); and adding two features, one only sen-
sitive to unigrams, and the other only to the rest
(1 + 2-6grams). All features had the same de-
sign as described in Section 5, each had an asso-
ciated weight (as all other features), and all fea-
ture weights in each system, including the base-
line, were tuned using a separate minimum error
rate training for each system.
Results are shown in Table 2. For the E2C sys-
tems, for which we had four reference translations
for the test set, we used shortest reference length,
and used the NIST-provided script to split the out-
put words to Chinese characters before evaluation.
Statistical significance for the BLEU results were
calculated using Koehn?s (Koehn, 2004) pair-wise
bootstrapping test with 95% confidence interval.
On the E2C 29,000-line subset, the augmented
system had a significant 1.7 BLEU points gain over
its baseline. On the full size model, results were
negative. Note that our E2C full size baseline
is reasonably strong: Its character-based BLEU
score is slightly higher than the JHU-UMD sys-
tem that participated in the NIST 2008 MT evalua-
tion (constrained training track), although we used
a subset of that system?s training materials, and
a smaller language model. Results there ranged
from 15.69 to 30.38 BLEU (ignoring a seeming
outlier of 3.93).
5.2 Spanish-to-English Translation
In order to to permit a more direct comparison
with the pivoting technique, we also experimented
with Spanish to English (S2E) translation, fol-
lowing Callison-Burch et al (2006). For base-
line we used the Spanish and English sides of
the Europarl multilingual parallel corpus (Koehn,
2005), with the standard training, development,
and test sets. We created training subset models
of 10,000, 20,000, and 80,000 aligned sentences,
as described in Callison-Burch et al (2006). For
better comparison with their pivoting system, we
used the same 5-gram language model, develop-
ment and test sets: For development, we used the
Europarl dev2006 Spanish and English sides, and
for testing we used the Europarl 2006 test set.
We trained the Spanish paraphrase generation
system on the Spanish corpora available from
dataset E2C model BLEU TER
29k baseline 15.21 90.354
29k 1grams 16.87*** 90.370
29k 1-6grams 16.54*** 90.376
29k 1 + 2-6grams 16.88*** 90.349
Full baseline 22.17 90.398
Full 1grams 21.64*** 90.459
Full 1-6grams 21.75 90.421
Full 1 + 2-6grams 21.39*** 90.433
Table 2: E2C Results: character-based BLEU and
TER scores. All models have one additional fea-
ture over baseline, except for the "1 + 2-6" mod-
els that have one feature for unigrams and an-
other feature for bigrams to 6-grams. Paraphrases
with score < .3 were filtered out. *** = sig-
nificance test over baseline with p < 0.0001,
using Koehn?s (2004) pair-wise bootstrap resam-
pling test for BLEU with 95% confidence interval.
Paraphrase Score
Source: deal
agreement 0.56
accord 0.53
talks 0.45
contract 0.42
peace deal 0.33
merger 0.32
agreement is 0.30
Source: fall
rise 0.87
slip 0.82
tumbled today 0.68
fell today 0.67
tumble 0.65
fall tokyo ap stock prices fell 0.56
are mixed 0.54
Source: to provide any other
to give any 0.74
to give further 0.70
to provide any 0.68
to give any other 0.62
to provide further 0.61
to provide other 0.53
to reveal any 0.52
to provide any further 0.48
to disclose any 0.47
to publicly discuss the 0.43
Source: we have a situation that
uncontroversial question about our 0.66
obviously with the developments this morning 0.65
community staffing of community centres 0.64
perhaps we are getting rather impatient 0.63
er around the inner edge 0.60
interested in going to the topics 0.60
and that is the day that 0.60
as a as a final point 0.59
left which it may still have 0.56
Table 3: English paraphrases from E2C 29K-
bitext systems.
386
the EACL 2009 Fourth Workshop on Statistical
Machine Translation:
1
the Spanish side of the
Europarl-v4, news training 2008, and news com-
mentary 2009. We also re-trained adding the JRC-
Acquis-v3 corpus
2
to the paraphrase training set,
and then adding also the LDC Spanish Gigaword
(LDC2006T12) and truncating the resulting cor-
pus after the first 150M lines. We lowercased
these training sets, tokenized and removed punc-
tuation marks and numbers, and this resulted in
training set sizes as detailed in Table 1. We gen-
erated paraphrases for phrases up to four tokens
in length, and used two arbitrary similarity thresh-
olds of minScore = 0.3 (as in the E2C experi-
ments), and 0.6, for enforcing only higher preci-
sion paraphrasing.
We experimented with these variants: a single
feature for all paraphrase (1-4grams); using only
paraphrases of unigrams (1grams); and using two
features: one only sensitive to unigrams and bi-
grams, and the other to the rest (1-2 + 3-4grams).
Results are shown in Table 4. We used BLEU
over lowercased outputs to evaluate all S2E sys-
tems, and Koehn?s significance test as above.
On the S2E 10,000-line subset, both the 1grams
and 1-4grams models achieved significant gains of
.4 BLEU points over the baseline. We concluded
from a manual evaluation of the 10,000-line mod-
els that the two major weaknesses of the baseline
system were (not surprisingly) number of untrans-
lated (OOV) words / phrases, followed by number
of superfluous words / phrases.
On the larger subset models, no system sig-
nificantly outperformed the baseline. Note that
our S2E baselines? scores are higher than those
of Callison-Burch et al (2006), since we evaluate
lowercased outputs, instead of recased ones.
6 Discussion and Future Work
We have shown that monolingually-derived para-
phrases, based on distributional semantic similar-
ity measures over a source-language corpus, can
improve the performance of statistical machine
translation (SMT) systems. Our proposed method
has the advantage of not relying on bitexts in order
to generate the paraphrases, and therefore gives
access to large amounts of monolingual training
data, for which creating bitexts of equivalent size
is generally unfeasible. We haven?t trained our
1
http://www.statmt.org/wmt09
2
http://wt.jrc.it/lt/Acquis
system on nearly as large a corpus as it can han-
dle, and indeed we see this as a natural next step.
Results support the assumption that a larger
monolingual paraphrase training set yields bet-
ter paraphrases: our S2E 1-4grams model per-
formed significantly better than baseline when us-
ing wmt09+aquis for paraphrasing, but when only
using wmt09, the model had a smaller advantage
that did not reach significance. However, for the
S2E 1grams model, there was a slight decrease in
performance when switching paraphrasing corpus
from wmt09+aquis to wmt09+aquis+afp. This ef-
fect might be due to the genre or unbalanced con-
tent of the additional corpus, or perhaps it is the
case that in this corpus size, paraphrases of higher-
level ngrams benefitted from the additional text
much more than paraphrases of unigrams did. The
two rightmost columns in Table 5 show that al-
though Spanish monolingual paraphrases for the
unigram baile improve when using the larger cor-
pus, (e.g., danza and un balie become the third and
fourth top candidates, pushing much worse candi-
dates far down the list), the two top paraphrase
candidates remained unchanged. However, for
the 4gram a favor del informe, antonymous can-
didates, which are bad and misleading for trans-
lation, are pushed down from the top first and
third spots by synonymous, better candidates. Ta-
ble 3 contains additional examples of good and
bad top paraphrase candidates, also in English.
Paraphrases of phrases seem to be of lower qual-
ity than those of unigrams, as can be seen at the
bottom of the table.
These results also show that our method is es-
pecially useful in settings involving low-density
languages or special domains: The smaller sub-
set models, emulating a resource-poor language
situation, show higher gains than larger models
(which are supersets of the smaller subset models),
when augmented with paraphrases derived from
the same paraphrase training set. This was vali-
dated in two very different language pairs: English
to Chinese, and Spanish to English. We believe
that larger monolingual training sets for paraphras-
ing can help languages with richer resources, and
we intend to explore this too.
Although the gains in the Spanish-English sub-
sets are somewhat smaller than the pivoting tech-
nique reported in Callison-Burch et al (2006),
e.g., .7 BLEU for the 10k subset, we take these
results as a proof of concept that can yield better
387
bitext mono.corp. features minScore BLEU TER
10k (baseline) ? ? 23.78 62.382
10k wmt09 1-4grams .6 23.81
10k wmt09 1-2+3-4gr .6 23.92 62.202
10k wmt09+aquis 1-4grams .6 24.13*** 61.739
10k wmt09+aquis 1grams .6 24.11 61.979
20k (baseline) ? ? 24.68 62.333
20k wmt09+aquis 1-4grams .6 24.75 61.528
80k (baseline) ? ? 27.89 57.977
80k wmt09+aquis 1-4grams .6 27.82 57.906
10k wmt09+aquis 1grams .3 24.11 61.979
10k wmt09+aquis+afp 1grams .3 23.97 61.974
20k wmt09+aquis+afp 1grams .3 24.77 61.276
80k wmt09+aquis+afp 1grams .3 27.84*** 57.781
Table 4: S2E Results: Lowercase BLEU and TER. Paraphrases with score < minScore were filtered out.
*** = significance test over baseline with p < 0.0001, using Koehn?s (2004) pair-wise bootstrap test for
BLEU with 95% confidence interval.
pivot wmt09+acquis wmt09+acquis+afp
Source: baile
danza el baile el baile
bailar baile y baile y
a de david palomar y la danza
dans viejo como quien se acomoda una un baile
empresa por juli?n estrada el tercero de teatro
coro al baile a la baloncesto el cine
Source: a favor del informe
a favor de este informe en contra del informe favor del informe
favor del informe a favor de este informe en contra del informe
el informe en contra de este informe a favor de este informe
a favor a favor de la resoluci?n en contra de este informe
por el informe a favor de esta resoluci?n en contra de la resoluci?n
al informe a favor del informe del se?or a favor del informe del sr.
su a favor del informe del sr. en contra del informe del sr.
del informe en contra de la propuesta a favor del excelente informe
de este informe contra el informe a favor del informe deprez
Table 5: Comparison of Spanish paraphrases: by pivoting, and by two monolingual corpora. Ordered
from best to worst score.
system example
source cuando escucho las distintas intervenciones , creo que quienes afirman que deber?amos analizar
nuestras prioridades y limitar el n?mero de objetivos que queremos conseguir , est?n en lo cierto .
reference when i listen to the various comments made , i find myself agreeing with those who recommend
that we take a look at our priorities and then limit the number of aims we want to achieve
baseline escucho when the various speeches, i believe that those who afirman that we should our
environmental limitar priorities and the number of objectives we want to achieve, are in this way.
pivoting (MW) when i can hear the various speeches , i believe that those people that we should look at our
priorities and to limit the number of objectives we want to achieve , are in fact .
wmt09+acquis escucho when the various speeches, i believe that those who claiming that we should environmental
.1-4grams limitar our priorities and the number of objectives we want to achieve, are on the way.
wmt09+acquis escucho when the various speeches, i believe that those who considered that we should our
.1grams environmental priorities and reducing the number of objectives we want to achieve, are on the way.
wmt09+acquis+afp escucho when the various speeches, i believe that those who say that we should our environmental
.1grams priorities and reduce the number of objectives we want to achieve, are on the way.
Table 6: S2E translation examples on 10k-bitext systems. Some translation differences are in bold.
388
gains with larger monolingual training sets. Pivot-
ing techniques (translating back and forth) rely on
limited resources (bitexts), and are subject to shifts
in meaning due to their inherent double transla-
tion step. In contrast, large monolingual resources
are relatively easy to collect, and our system in-
volves only a single translation/paraphrasing step
per target phrase. Table 5 also shows an exemplar
comparison with the pivoting paraphrases used in
Callison-Burch et al (2006). It seems that the piv-
oting paraphrases might suffer more from having
frequent function words as top candidates, which
might be a by-product of their alignment ?promis-
cuity?. However, the top antonymous candidate
problem seems to mainly plague the monolin-
gual distributional paraphrases (but improves with
larger corpora). See also Table 6.
The paraphrase quality remains an issue with
this method (as with all other paraphrasing meth-
ods). Some possible ways of improving it, be-
sides using larger corpora, are: using syntactic in-
formation (Callison-Burch, 2008), using semantic
knowledge such as thesaurus or WordNet to per-
form word sense disambiguation (WSD) (Resnik,
1999; Mohammad and Hirst, 2006), improving
the similarity measure, and refining the similarity
threshold. We would like to explore ways of incor-
porating syntactic knowledge that do not sacrifice
coverage as much as in Callison-Burch (2008); in-
corporating semantic knowledge to disambiguate
phrasal senses; using context to help sense disam-
biguation (Erk and Pad?, 2008); and optimizing
the similarity threshold for use in SMT, for exam-
ple on a held-out dataset: too high a threshold re-
duces coverage, while too low a threshold results
in bad paraphrases and translation.
The method presented here is quite general, and
therefore different similarity measures, including
other corpus-based ones, can be plugged in to gen-
erate paraphrases. We are looking into using DPs
with word-sense disambiguation: Since it has been
shown that similarity is often judged by the se-
mantic distance of the closest senses of the two
target words (Mohammad and Hirst, 2006), and
that paraphrases generated this way are likely to
be of higher quality (Marton et al, 2009), hence
it is also likely that the overall performance of an
SMT system using them will also improve further.
One potential advantage of using bitexts for
paraphrase generation is the usage of implicit hu-
man knowledge, i.e., sentence alignments. The
concern that not using this knowledge would turn
out detrimental to the performance of SMT sys-
tems augmented by paraphrases as described here
was largely put to rest, as our method improved
the tested subset SMT systems? quality.
Acknowledgments
Many thanks to Chris Dyer for his help with
the E2C set, and to Adam Lopez for his imple-
mentation of pattern matching with Suffix Ar-
ray. This research was partially supported by
the GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001 and NSF award 0838801, by the Euro-
MatrixPlus project funded by the European Com-
mission, and by the US National Science Foun-
dation under grant IIS-0713448. The views and
findings are the authors? alone.
References
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of ACL-2001.
P.F. Brown, S.A.D. Pietra, V.J.D. Pietra, and R.L. Mer-
cer. 1993. The mathematics of statistical machine
translation. Computational Linguistics, 19(2):263?
313.
Lou Burnard. 2000. Reference Guide for the British
National Corpus. Oxford University Computing
Services, Oxford, England, world edition edition.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings NAACL-
2006.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP 2008, Waikiki, Hawai?i.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL-05, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Mona Diab and Steve Finch. 2000. A statistical word-
level translation model for comparable corpora. In
Proceedings of the Conference on Content-Based
Multimedia Information Access (RIAO).
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics of the Association for
Computational Linguistics, Geneva, Switzerland.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
389
Katrin Erk and Sebastian Pad?. 2008. A struc-
tured vector space model for word meaning in con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2086), pages 897?906, Honolulu, HI.
John R. Firth. 1957. A synopsis of linguistic theory
1930
?
U55. Studies in Linguistic Analysis, (special
volume of the Philological Society):1?32. Distribu-
tional Hypothesis.
Pascale Fung and Percy Cheung. 2004. Multi-
level bootstrapping for extracting parallel sentences
from a quasi-comparable corpus. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1051, Geneva, Switzerland. Asso-
ciation for Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, com-
parable texts. In Proceedings of COLING-ACL98,
pages 414?420, Montreal, Canada.
Zellig S. Harris. 1940. Review of louis h. gray, foun-
dations of language (new york: Macmillan, 1939).
Language, 16(3):216
?
U231.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), demonstration
session, Prague, Czech Republic.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Philipp Koehn. 2004b. Pharaoh: A beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of AMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, pages
296?304, San Francisco, CA.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parame-
ter tuning in statistical machine translation. In Pro-
ceedings of the ACL Workshop on Statistical Ma-
chine Translation.
Yuval Marton, Saif Mohammad, and Philip Resnik.
2009. Estimating semantic distance using soft se-
mantic constraints in knowledge-source / corpus hy-
brid models. In Procedings of EMNLP, Singapore.
S. McDonald. 2000. Environmental determinants of
lexical processing effort. Ph.D. thesis, University of
Edinburgh.
Saif Mohammad and Graeme Hirst. 2006. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2006), Sydney, Australia.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504.
Doug Oard, David Doermann, Bonnie Dorr, Daqing
He, Phillip Resnik, William Byrne, Sanjeeve Khu-
danpur, David Yarowsky, Anton Leuski, Philipp
Koehn, and Kevin Knight. 2003. Desperately seek-
ing cebuano. In Proceedings of HLT-NAACL.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the ACL, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, John
Henderson, and Florence Reeder. 2002. Corpus-
based comprehensive and diagnostic MT evaluation:
Initial Arabic, Chinese, French, and Spanish results.
In Proceedings of the ACL Human Language Tech-
nology Conference, pages 124?127, San Diego, CA.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th Annual Confer-
ence of the Association for Computational Linguis-
tics., pages 519?525.
Philip Resnik and Noah Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Philip Resnik. 1999. Semantic similarity in a taxon-
omy: An information-based measure and its appli-
cation to problems of ambiguity in natural language.
Journal of Artificial Intelligence Research (JAIR),
11:95?130.
Hinrich Schuetze and Jan O. Pedersen. 1997. A
cooccurrence-based thesaurus and two applications
to information retreival. Information Processing
and Management, 33(3):307
?
U318.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
John Makhoul, Linnea Micciulla, and Ralph
Weischedel. 2005. A study of translation error rate
with targeted human annotation. Technical Report
LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-
58, University of Maryland, July, 2005.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Process-
ing.
390
Constructing Corpora for the Development
and Evaluation of Paraphrase Systems
Trevor Cohn?
University of Edinburgh
Chris Callison-Burch??
Johns Hopkins University
Mirella Lapata?
University of Edinburgh
Automatic paraphrasing is an important component in many natural language processing tasks.
In this article we present a new parallel corpus with paraphrase annotations. We adopt a defini-
tion of paraphrase based on word alignments and show that it yields high inter-annotator agree-
ment. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is
appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed
in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1)
and also in developing linguistically rich paraphrase models based on syntactic structure.
1. Introduction
The ability to paraphrase text automatically carries much practical import for many
NLP applications ranging from summarization (Barzilay 2003; Zhou et al 2006) to
question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine
translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising
that recent years have witnessed increasing interest in the acquisition of paraphrases
from real world corpora. These are most often monolingual corpora containing parallel
translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and
Marcu 2003). Truly bilingual corpora consisting of documents and their translations have
also been used to acquire paraphrases (Bannard and Callison-Burch 2005; Callison-
Burch 2007) as well as comparable corpora such as collections of articles produced
by two different newswire agencies about the same events (Barzilay and Elhadad
2003).
Although paraphrase induction algorithms differ in many respects?for example,
the acquired paraphrases often vary in granularity as they can be lexical (fighting, battle)
or structural (last week?s fighting, the battle last week), and are represented as words or
? School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk.
?? Center for Speech and Language Processing, Johns Hopkins University, Baltimore, MD, 21218.
E-mail: ccb@cs.jhu.edu.
? School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk.
Submission received: 10 September 2007; revised submission received: 8 February 2008; accepted for
publication: 26 March 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
syntax trees?they all rely on some form of alignment for extracting paraphrase pairs.
In its simplest form, the alignment can range over individual words, as is often done
in machine translation (Quirk, Brockett, and Dolan 2004). In other cases, the alignments
range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay
and Lee 2003).
The obtained paraphrases are typically evaluated via human judgments. Para-
phrase pairs are presented to judges who are asked to decide whether they are seman-
tically equivalent, that is, whether they can be generally substituted for one another in
the same context without great information loss (Barzilay and Lee 2003; Barzilay and
McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In
some cases the automatically acquired paraphrases are compared against manually gen-
erated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance
increase for a specific application, such as machine translation (Callison-Burch, Koehn,
and Osborne 2006).
Unfortunately, manually evaluating paraphrases in this way has at least three draw-
backs. First, it is infeasible to perform frequent evaluations when assessing incremental
system changes or tuning system parameters. Second, it is difficult to replicate results
presented in previous work because there is no standard corpus, and no standard evalu-
ation methodology. Consequently comparisons across systems are few and far between.
The third drawback concerns the evaluation studies themselves, which primarily focus
on precision. Recall is almost never evaluated directly in the literature. And this is
for a good reason: There is no guarantee that participants will identify the same set
of paraphrases as each other or with a computational model. The problem relates to
the nature of the paraphrasing task, which has so far eluded formal definition (see
the discussion in Barzilay [2003]). Such a definition is not so crucial when assessing
precision, because subjects are asked to rate the paraphrases without actually having to
identify them. However, recall might be measured with respect to some set of ?gold-
standard? paraphrases which will have to be collected according to some concrete
definition.
In this article we present a resource that could potentially be used to address
these problems. Specifically, we create a monolingual parallel corpus with human
paraphrase annotations. Our working definition of paraphrase is based on word and
phrase1 alignments between semantically equivalent sentences. Other definitions are
possible, for instance we could have asked our annotators to identify all constituents
that are more or less meaning preserving in our parallel corpus. We chose to work
with alignments for two reasons. First, the notion of alignment appears to be central
in paraphrasing?most existing paraphrase induction algorithms rely on alignments
either implicitly or explicitly for identifying paraphrase units. Secondly, research in
machine translation, where several gold-standard alignment corpora have been created,
shows that word alignments can be identified reliably by annotators (Melamed 1998;
Och andNey 2000b;Mihalcea and Pedersen 2003;Martin,Mihalcea, and Pedersen 2005).
We therefore create word alignments similar to those observed in machine transla-
tion, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links
between words. Alignment blocks larger than one-to-one are used to specify phrase
correspondences.
1 Our definition of the term phrase follows the SMT literature. It refers to any contiguous sequence of
words, whether it is a syntactic constituent or not. See Section 2 for details.
598
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
In the following section we explain how our corpus was created and summarize our
annotation guidelines. Section 3 gives the details of an agreement study, demonstrating
that our annotators can identify and align paraphrases reliably. We measure agreement
using alignment overlap measures from the SMT literature, and also introduce a novel
agreement statistic for non-enumerable labeling spaces. Section 4 illustrates how the
corpus can be used in paraphrase research, for example, as a test set for evaluating
the output of automatic systems or as a training set for the development of paraphrase
systems. Discussion of our results concludes the article.
2. Corpus Creation and Annotation
Our corpus was compiled from three data sources that have been previously used for
paraphrase induction (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003;
Dolan, Quirk, and Brockett 2004): the Multiple-Translation Chinese (MTC) corpus,
Jules Verne?s Twenty Thousand Leagues Under the Sea novel (Leagues), and the Microsoft
Research (MSR) paraphrase corpus. These are monolingual parallel corpora, aligned at
the sentence level. Both source and target sentences are in English, and express the same
content using different surface forms.
The MTC corpus contains news stories from three sources of journalistic Mandarin
Chinese text.2 These stories were translated into English by 11 translation agencies.
Because the majority of the translators were non-native English speakers, occasionally
translations contain syntactic or grammatical errors and are not entirely fluent. After
inspection, we identified four translators with consistently fluent English, and used
their sentences for our corpus. The Leagues corpus contains two English translations
of the French novel Twenty Thousand Leagues Under the Sea. The corpus was created
by Tagyoung Chung and manually aligned at the paragraph level.3 In order to obtain
sentence level paraphrase pairs, we sampled from the subset of one-to-one sentence
alignments. The MSR corpus was harvested automatically from online news sources.4
The obtained sentence pairs were further submitted to judges who rated them as being
semantically equivalent or not (Dolan, Quirk, and Brockett 2004). We only used seman-
tically equivalent pairs. The sentence pairs were filtered for length (? 50) and length
ratio (? 1 : 9 between the shorter and longer sentence). This was necessary to prune out
incorrectly aligned sentences.
We randomly sampled 300 sentence pairs from each corpus (900 in total). Of these,
300 pairs (100 per corpus) were first annotated by two coders to assess inter-annotator
agreement. The remaining 600 sentence pairs were split into two distinct sets, each
consisting of 300 sentences (100 per corpus), and were annotated by a single coder.
Each coder annotated the same amount of data. In addition, we obtained a trial set
of 50 sentences from the MTC corpus which was used for familiarizing our annotators
with the paraphrase alignment task (this set does not form part of the corpus). In sum,
we obtained paraphrase annotations for 900 sentence pairs, 300 of which are doubly
annotated.
To speed up the annotation process, the data sources were first aligned automati-
cally and then hand-corrected.We usedGiza++ (Och andNey 2003), a publicly available
2 The corpus is made available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1.
3 The corpus can be downloaded from http://www.isi.edu/?knight/.
4 The corpus is available at http://research.microsoft.com/research/downloads/Details/607D14D9-
20CD-47E3-85BC-A2F65CD28042/Details.aspx.
599
Computational Linguistics Volume 34, Number 4
implementation of the IBM word alignment models (Brown et al 1993). Giza++ was
trained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pair-
ings of English translations as training instances. This resulted in 55 =
11?(11?1)
2 training
pairs per sentence and a total of 54, 615 training pairs. In addition, we augmented the
training data with a word-identity lexicon, as proposed by Quirk, Brockett, and Dolan
(2004). This follows standard practice in SMT where entries from a bilingual dictionary
are added to the training set (Och and Ney 2000a), except in our case the ?dictionary?
is monolingual and specifies that each word type can be paraphrased as itself. This is
necessary in order to inform Giza++ about word identity.
A common problem with automatic word alignments is that they are asymmetric:
one source word can only be aligned to one target word, whereas one target word can
be aligned to multiple source words. In SMT, word alignments are typically predicted
in both directions: source-to-target and target-to-source. These two alignments are then
merged (symmetrized) to produce the final alignment (Koehn, Och, and Marcu 2003).
Symmetrization improves the alignment quality compared to that of a single directional
model, while also allowing a greater range of alignment types (i.e., some many-to-
one, one-to-many, and many-to-many alignments can be produced). Analogously, we
obtained word alignments in both directions6 which we subsequently merged by taking
their intersection. This resulted in a high precision and low recall alignment.
Our annotators (two linguistics graduates) were given pairs of sentences and asked
to show which parts of these were in correspondence by aligning them on a word-by-
word basis.7 Our definition of alignment was fairly general (Och andNey 2003): Given a
source string X = x1, . . . , xN and a target string Y = y1, . . . , yM, an alignmentA between
two word strings is the subset of the Cartesian product of the word positions:
A ? {(n,m) : n = 1, . . . ,N;m = 1, . . . ,M} (1)
We did not provide a formal definition of what constitutes a correspondence. As a
rule of thumb, annotators were told to align words or phrases x ? y in two sentences
(X,Y) whenever the words x could be substituted for y in Y, or vice versa. This relation-
ship should hold within the context of the sentence pair in question: the relation x ? y
need not hold in general contexts. Trivially this definition allowed for identical word
pairs.
Following common practice (Och, Tillmann, and Ney 1999; Och and Ney 2003;
Daume? III and Marcu 2004), we distinguished between sure (S) and possible (P) align-
ments, where S ? P. The intuition here is that sure alignments are clear-cut decisions
and typical of genuinely substitutable words or phrases, whereas possible alignments
flag a correspondence that has slightly divergent syntax or semantics. Annotators were
encouraged to produce sure alignments. They were also instructed to prefer smaller
alignments whenever possible, but were allowed to create larger block alignments.
Smaller alignments were generally used to indicate lexical correspondences, whereas
block alignments were reserved for non-compositional phrase pairs (e.g., idiomatic
expressions) or simply expressions with radically different syntax or vocabulary. In
5 The IBM alignment models require a large amount of parallel data to yield reliable alignments. We
therefore selected the MTC for training purposes as it was the largest of our parallel corpora.
6 We used five iterations for each of Model 1, Model 2, and the HMMmodel.
7 The annotation was conducted using a Web-based alignment tool available at
http://demo.linearb.co.uk/paraphrases/.
600
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Figure 1
Manual alignment between two sentence pairs from the MTC corpus, displayed as a grid. Black
squares represent sure alignment, gray squares represent possible alignment.
cases where information in one sentence was not present in the other, the annotators
were asked to leave this information unaligned.
Finally, annotators were given a list of heuristics to help them decide how to
make alignments in cases of ambiguity. These heuristics handled the alignment of
named entities (e.g., George Bush) and definite descriptions (e.g., the president), tenses
(e.g., had been and shall be), noun phrases with mismatching determiners (e.g., a man
and the man), verb complexes (e.g., was developed and had been developed), phrasal
verbs (e.g., take up and accept), genitives (e.g., Bush?s infrequent speeches and the infre-
quent speeches by Bush), pronouns, repetitions, typographic errors, and approximate
correspondences. For more details, we refer the interested reader to our annotation
guidelines.8
Figure 1 shows the alignment for two sentence pairs from the MTC corpus. The
first pair (Australia is concerned with the issue of carbon dioxide emissions. ? The problem
of greenhouse gases has attracted Australia?s attention.) contains examples of word-to-
word (the ? The; issue ? problem; of ? of ; Australia ? Australia) and many-to-many
alignments (carbon dioxide emissions ? greenhouse gases). Importantly, we do not use
a large many-to-many block for Australia is concerned with and has attracted Australia?s
attention because it is possible to decompose the two phrases into smaller alignments.
The second sentence pair illustrates a possible alignment (could have very long term
effects? was of profound significance) indicated by the gray squares. Possible alignments
are used here because the two phrases only loosely correspond to each other. Possible
alignments are also used to mark significant changes in syntax where the words denote
a similar concept: for example, in cases where two words have the same stem but are
8 Both the corpus and the annotation guidelines can be found at: http://homepages.inf.ed.ac.uk/
tcohn/paraphrase corpus.html.
601
Computational Linguistics Volume 34, Number 4
expressed with different parts of speech, (e.g., co-operative ? cooperation) or when two
verbs are used that are not synonyms (e.g., this is also? this also marks).
3. Human Agreement
As mentioned in the previous section, 300 sentence pairs (100 pairs from each sub-
corpus) were doubly annotated, in order to measure inter-annotator agreement. Here,
we treat one annotator as gold-standard (reference) andmeasure the extent to which the
other annotator deviates from this reference.
Word-Based Measures. The standard technique for evaluating word alignments is to
represent them as a set of links (i.e., pairs of words) and compare them against gold-
standard alignments. The quality of an alignmentA (defined in Equation (1)) compared
to reference alignment B can be then computed using standard recall, precision, and
F1 measures (Och and Ney 2003):
Precision =
|AS ? BP|
|AS|
Recall =
|AP ? BS|
|BS|
F1 = 2 ? Precision ? Recall
Precision+ Recall
(2)
where the subscripts S and P denote sure and possible word alignments, respectively.
Note that both precision and recall are asymmetric in that they compare sets of possible
and sure alignments. This is designed to be maximally generous: sure predictions
which are present in the reference as possibles are not penalized in precision, and the
converse applies for recall. We adopt Fraser and Marcu (2007)?s definition of F1, an
F-measure between precision and recall over the sure and possibles. They argue that
it is a better alternative to the commonly used Alignment Error Rate (AER), which
does not sufficiently penalize unbalanced precision and recall.9 As our corpus is mono-
lingual, in order to avoid artificial score inflation, we limit the precision and recall
calculations to consider only pairs of non-identical words (and phrases, as discussed
subsequently).
To give an example, consider the sentence pairs in Figure 2, whose alignments have
been produced by the two annotators A (left) and B (right). Table 1 shows the individual
word alignments for each annotator and their type (sure or possible). In order to mea-
sure F1, we must first estimate Precision and Recall (see Equation (2)). Treating annota-
tor B as the gold standard, |AS| = 4, |BS| = 5, |AS ? BP| = 4, and |AP ? BS| = 4. This
results in a precision of 44 = 1, a recall of
4
5 , and F1 of
2?1?0.8
1+0.8 = 0.89. Note that we ignore
alignments over identical words (i.e., discussed ? discussed, the ? the, and ? and,
. ? .).
Phrase-Based Measures. The given definitions are all word-based; however, our annota-
tors, and several paraphrasing models, create correspondences not only between words
but also between phrases. To take this into account, we also evaluate these measures
over larger blocks (similar to Ayan and Dorr [2006]). Specifically, we extract phrase
pairs from the alignments produced by our annotators using a modified version of the
standard SMT phrase extraction heuristic (Och, Tillmann, and Ney 1999). The heuristic
9 Fraser and Marcu (2007) also argue for an unbalanced F-measure to bias towards recall. This is shown to
correlate better with translation quality. For paraphrasing it is not clear if such a bias would be beneficial.
602
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Figure 2
Sample sentence pair showing the word alignments from two annotators.
extracts all phrase pairs consistent with the word alignment. These include phrase pairs
whose words are aligned to each other or nothing, but not to words outside the phrase
boundaries.10 The phrase extraction heuristic creates masses of phrase pairs, many of
which are of dubious quality. This is often due to the inclusion of unaligned words or
simply to the extraction of overly-large phrase pairs which might be better decomposed
into smaller units. For our purposes we wish to be maximally conservative in how we
process the data, and therefore we do not extract phrase pairs with unaligned words on
their boundaries.
Figure 3 illustrates the types of phrase pairs our extraction heuristic permits. Here,
the pair and reached ? and arrived at is consistent with the word alignment. In contrast,
the pair and reached ? and arrived isn?t; there is an alignment outside the hypothetical
phrase boundary which is not accounted for (reached is also aligned to at). The phrase
pair and reached an ? and arrived at is consistent with the word alignment; however it
has an unaligned word (i.e., an) on the phrase boundary, which we disallow.
Our phrase extraction procedure distinguishes between two types of phrase pairs:
atomic, that is, the smallest possible phrase pairs, and composite, which can be created
by combining smaller phrase pairs. For example, the phrase pair and reached ? and
arrived at in Figure 3 is composite, as it can be decomposed into and ? and and
reached ? arrived at. Table 2 shows the atomic and composite phrase pairs extracted
from the possible alignments produced by annotators A and B for the sentence pair in
Figure 2.
We compute recall, precision, and F1 over the phrase pairs extracted from the word
alignments as follows:
Precision =
|Apatom ? B
p|
|Apatom|
Recall =
|Ap ? Bpatom|
|Bpatom|
F1 = 2 ? Precision ? Recall
Precision+ Recall
(3)
10 The term phrase is not used here in the linguistic sense; many extracted phrases will not be constituents.
603
Computational Linguistics Volume 34, Number 4
Table 1
Single word pairs specified by the word alignments from Figure 2, for two annotators, A and B.
The column entries specify the alignment type for each annotator, either sure (S) or possible (P).
Dashes indicate that the word pair was not predicted by the annotator. Italics denote lexically
identical word pairs.
Word alignments A B
they ? both ? P
they ? parties P P
discussed ? discussed S S
the ? the S S
aspects ? specific P ?
in ? specific P P
detail ? specific P P
aspects ? issues P S
in ? issues P ?
detail ? issues P ?
and ? and S S
reached ? arrived S S
reached ? at ? S
an ? a S S
extensive ? general S P
agreement ? consensus S S
. ? . S S
Figure 3
Validity of phrase pairs according to the phrase extraction heuristic. Only the leftmost phrase
pair is valid. The others are inconsistent with the alignment or have an unaligned word on a
boundary, respectively, indicated by a cross.
where Ap and Bp are the predicted and reference phrase pairs, respectively, and
the atom subscript denotes the subset of atomic phrase pairs, Apatom ? A
p. As shown
in Equation (3) we measure precision and recall between atomic phrase pairs and
the full space of atomic and composite phrase pairs. This ensures that we do not
multiply reward composite phrase pair combinations,11 while also not unduly pe-
nalizing non-matching phrase pairs which are composed of atomic phrase pairs in
11 This contrasts with Ayan and Dorr (2006), who use all phrase pairs up to a given size, and therefore
might multiply count phrase pairs.
604
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Table 2
Phrase pairs are specified by the word alignments from Figure 2, using the possible alignments.
The entire set of atomic phrase pairs for either annotator (labeled A or B) and a selection of the
remaining 57 composite phrase pairs are shown. The italics denote lexically identical phrase pairs.
?This phrase pair is atomic in A but composite in B.
Atomic phrase pairs A B
they ? parties P ?
they ? both parties ? P
discussed ? discussed S S
the ? the S S
aspects in detail ? specific issues P ?
in detail ? specific ? P
aspects ? issues ? S
and ? and S S
reached ? arrived S S
reached ? arrived at ? S
reached an ? arrived at a S P?
an ? a S S
extensive ? general S P
agreement ? consensus S S
. ? . S S
Composite phrase pairs A B
they discussed ? both parties discussed ? P
they discussed ? parties discussed P ?
they discussed the ? both parties discussed the ? P
they discussed the ? parties discussed the P ?
they ... reached an ? both parties ... arrived at a P ?
the aspects in detail ? the specific issues P P
reached an extensive ? arrived at a general S S
extensive agreement . ? general consensus . S S
...
the reference. Returning to the example in Table 2, with annotator B as the gold
standard, |Apatom| = 7, |B
p
atom| = 8, |A
p
atom ? B
p| = 5, and |Ap ? Bpatom| = 4. Consequently,
precision= 57 = 0.71, recall=
4
8 = 0.50, and F1=
2?0.71?0.50
0.71+0.50 = 0.59. Again we ignore
identical phrase pairs.
A potential caveat here concerns the quality of the atomic phrase pairs, which are
automatically induced and may not correspond to linguistic intuition. To evaluate this,
we had two annotators review a random sample of 166 atomic phrase pairs drawn from
the MTC corpus (sure), classifying each phrase pair as correct, incorrect, or uncertain
given the sentence pair as context. From this set, 73% were deemed correct, 22% un-
certain, and 5% incorrect.12 Annotators agreed in their decisions 75% of the time (using
the Kappa13 statistic, their agreement is 0.61). This confirms that the phrase-extraction
process produces reliable phrase pairs from our word-aligned data (although we cannot
claim that it is exhaustive).
12 Taking a more conservative position by limiting the proportion of unaligned words within the phrase
pair improves these figures monotonically to 90% correct and 0% incorrect (fully aligned phrase pairs).
13 This Kappa is computed over three nominal categories (correct, incorrect, and uncertain) and should not
be confused with the agreement measure we develop in the following section for phrase pairs.
605
Computational Linguistics Volume 34, Number 4
Chance-Corrected Agreement. Besides precision and recall, inter-annotator agreement is
commonly measured using the Kappa statistic (Cohen 1960). Thus is a desirable mea-
sure because it is adjusted for agreement due purely to chance:
? =
Pr(A)? Pr(E)
1? Pr(E)
(4)
where Pr(A) is the proportion of times two coders14 agree, corrected by Pr(E), the
proportion of times we would expect them to agree by chance.
Kappa is a suitable agreement measure for nominal data. An example would be a
classification task, where two coders must assign n linguistic instances (e.g., sentences
or words) into one of m categories. Given this situation, it would be possible for each
coder to assign each instance to the same category. Kappa allows us to quantify whether
the coders agree with each other about the category membership of each instance. It is
relatively straightforward to estimate Pr(A)?it is the proportion of instances on which
the two coders agree. Pr(E) requires a model of what would happen if the coders were
to assign categories randomly. Under the assumption that coders r1 and r2 are indepen-
dent, the chance of them agreeing on the jth category is the product of each of them
assigning an instance to that category: Pr(Cj|r1) Pr(Cj|r2). Chance agreement is then the
sum of this product across all categories: Pr(E) =
m
?
j=1
Pr(Cj|r1) Pr(Cj|r2). The literature
describes two different methods for estimating Pr(Cj|ri). Either a separate distribution
is estimated for each coder (Cohen 1960) or the same distribution for all coders (Scott
1955; Fleiss 1971; Siegel and Castellan 1988).We refer the interested reader to Di Eugenio
and Glass (2004) and Artstein and Poesio (2008) for a more detailed discussion.
Unfortunately, Kappa is not universally suited to every categorization task. A prime
example is structured labeling problems that allow a wide variety of output categories.
Importantly, the number and type of categories is not fixed in advance and can vary
from instance to instance. In parsing, annotators are given a sentence for which they
must specify a tree, of which there is an exponential number in the sentence length. Sim-
ilarly, in our case the space of possible alignments for a sentence pair is also exponential
in the input sentence lengths. Considering these annotations as nominal variables is
inappropriate.
Besides, alignments are only an intermediate representation that we have used to
facilitate the annotation of paraphrases. Ideally, we would like to measure agreement
over the set of phrase pairs which are specified by our annotators (via the word align-
ments), not the alignment matrices themselves.
Kupper and Hafner (1989) present an alternative measure similar to Kappa that is
especially designed for sets of variables:
C? =
??? ?0
1? ?0
, (5)
where ?? =
I
?
i=1
|Ai ? Bi|
min(|Ai|, |Bi|)
, and ?0 =
1
Ik
?
i
min(|Ai|, |Bi|)
14 Kappa has been extended to more than two coders (Fleiss 1971; Bartko and Carpenter 1976). For
simplicity?s sake our discussion and subsequent examples involve two coders. Also note that we use the
term coder instead of the more common rater. This is because in our task the annotators must identify
(a.k.a. code) the paraphrases rather than rate them.
606
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Here, Ai and Bi are the coders? predictions on sentence pair i from our corpus of I
sentence pairs. Each prediction is a subset of the full space of k items. Expression (5)
measures the agreement (or concordance) between coders A and B and follows the
general form of Kappa from Equation (4), which is defined analogously with Pr(A) and
Pr(E) taking the roles of ?? and ?0, but with different definitions.
Kupper and Hafner (1989) developed their agreement measure with medical diag-
nostic tasks in mind. For example, two physicians classify subjects into k = 3 diagnostic
categories and wish to find out whether they agree in their diagnoses. Here, each coder
must decidewhich (possibly empty) subset from k categories best describes each subject.
The size of k is thus invariant with the instance under consideration. This is not true
in our case, where k will vary across sentence pairs as sentences of different lengths
license different numbers of phrase pairs. More critically, the formulation in Equa-
tion (5) assumes that items in the set are independent: All subsets of the same car-
dinality as k are equally likely, and no combination is impossible. This independence
assumption is inappropriate for the paraphrase annotation task. The phrase extraction
heuristic allows each contiguous span in a sentence to be aligned to either zero or one
span in the other sentence; that is, nominating a phrase pair precludes the choice of
many other possible phrase pairs. Consequently relatively few of the subsets of the
full set of possible phrase pairs are valid. Formally, an alignment can specify only
O(N2) phrase pairs from a total set of k = O(N4) possible phrase pairs. This disparity in
magnitudes leads to increasingly underestimated ?? for larger N, namely, limN?? ?0 =
limN??O(N
2)/O(N4) = 0. The end result is an overestimate of C? on longer sentences.
For these reasons, we adapt the method of Kupper and Hafner (1989) to account for
our highly interdependent item sets. We use C? from Equation (5) as our agreement sta-
tistic defined over sets of atomic phrase pairs, that is, A = Apatom,B = B
p
atom. We redefine
?0 as follows:
?0 =
1
I
I
?
i=1
?
Apatom
?
Bpatom
Pr(Apatom) Pr(B
p
atom)
|Apatom ? B
p
atom|
min(|Apatom|, |B
p
atom|)
(6)
where Apatom and B
p
atom range over the sets of atomic phrase pairs licensed by sentence
pair i, and Pr(Apatom) and Pr(B
p
atom) are priors over these sets for each annotator. A conse-
quence of dropping the independence assumptions is that calculating ?0 is considerably
more difficult.
While it may be possible to calculate ?0 analytically, this gets increasingly compli-
cated for larger phrase pairs or with an expressive prior. For the sake of flexibility we
estimate ?0 using Monte Carlo sampling. Specifically, we approximate the full sum by
drawing samples from a prior distribution over sets of phrase pairs for each of our
annotators (Pr(Apatom) and Pr(B
p
atom) in Equation (6)). These samples are then compared
using the intersection metric. This is repeated many times and the results are then
averaged. More formally:
??0 =
1
I
I
?
i=1
1
J
J
?
j=1
|Apatom
( j)
? Bpatom
( j)
|
min(|Apatom
( j)
|, |Bpatom
( j)
|)
(7)
where for each sentence pair, i, we draw J samples of pairs of sets of phrase pairs,
(Apatom,B
p
atom). We use J = 1, 000, which is ample to give reliable estimates. So far we have
607
Computational Linguistics Volume 34, Number 4
not defined how we sample valid sets of phrase pairs. This is done via the word align-
ments. Recall that the annotators start out with alignments from an automatic word-
aligner. Firstly, we develop a distribution to predict how often an annotator changes a
cell from the initial alignment matrix. We model the number of changes made with a
binomial distribution, that is, each local change is assumed independent and has a fixed
probability, Pr(edit|r,Ni,Mi) where r is the coder andNi andMi are the sentence lengths.
This distribution is fit to each annotator?s predictions using a linear function over the
combined length of two sentences. Next we sample word alignments. Each sample
starts with the automatic alignment, and each cell is changed with probability Pr(edit).
These changes are binary, swapping alignments for non-alignments and vice versa.
Finally, the phrase-extraction heuristic is run over the alignment matrix to produce a
set of phrase pairs. This is done for each annotator, A and B, after which we have a
sample, (Apatom,B
p
atom). Each sample is then fed into Equation (7). Admittedly, this is not
themost accurate prior, as annotators are not just randomly changing the alignment, but
instead are influenced by the content expressed by the sentence pair and other factors
such as syntactic complexity. However, this prior produces estimates for ??0 which are
several orders of magnitude larger than those using Kupper and Hafner?s model of ?0
in Equation (5).
We now illustrate the process of measuring chance-corrected agreement, C?, with
respect to the example in Figure 2. Here, |Apatom| = 7, |B
p
atom| = 8, |A
p
atom ? B
p
atom| = 4, and
therefore ?? = 47 = 0.571. For this sentence our annotators edited eight and nine align-
ment cells, respectively, of the initial alignment matrix. This translates into Pr(edit|r =
A) = 812?13 = 5.13% and Pr(edit|r = B) = 5.77%. Given these priors, we run the Monte
Carlo sampling process from Equation (7), which results in ??0 = 0.147. Combining the
agreement estimate, ??, and chance correction estimate, ??0, using Equation (6) results in
C? = 0.571?0.1471?0.147 = 0.497.
Now, imagine a hypothetical case where ?? = 47 = 0.571 (i.e., the agreement is the
same as before), annotator B edits nine alignment cells, but annotator A chooses not
to make any edits. This leads to an increased estimate of ??0 = 0.259 and a decreased
C? = 0.442. If both annotators were not to make any edits, ??0 = 1 and C? = ??. Interest-
ingly, at the other extreme when Pr(edit|r = A) = Pr(edit|r = B) = 1, agreement is also
perfect, ??0 = 1 and C? = ??. This is because only one phrase pair can be extracted
which consists of the two full sentences.
Results. Tables 3 and 4 display agreement statistics on our three corpora using precision,
recall, F1, and C?. Specifically, we estimate C? by aggregating ?? and ??0 into corpus-
level estimates. Table 3 shows agreement scores for individual words, whereas Table 4
shows agreement for phrase pairs. In both cases the agreement is computed over non-
identical word and phrase pairs which are more likely to correspond to paraphrases.
The agreement figures are broken down into possible (Poss) and sure alignments (Sure)
for precision and recall.
When agreement is measured over words, our annotators obtain high F1 on all
three corpora (MTC, Leagues, and News). Recall on Possibles seems worse on the
News corpus when compared to MTC or Leagues. This is to be expected because this
corpus was automatically harvested from the Web, and some of its instances may not
be representative examples of paraphrases. For example, it is common for one sentence
to provide considerably more details than the other, despite the fact that both describe
the same event. The annotators in turn have difficulty deciding whether such instances
are valid paraphrases. The C? scores for the three corpora are in the same ballpark.
608
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Table 3
Inter-annotator agreement using precision, recall, F1, and C?; the agreement is measured over
words.
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.79 0.59 Prec 0.85 0.73 Prec 0.78 0.55
Rec 0.77 0.73 Rec 0.74 0.75 Rec 0.57 0.70
F1 0.76 F1 0.79 F1 0.74
C? 0.85 C? 0.87 C? 0.89
Table 4
Inter-annotator agreement using precision, recall, F1, and C?; the agreement is measured over
atomic phrase pairs.
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.77 0.67 Prec 0.74 0.72 Prec 0.72 0.68
Rec 0.77 0.66 Rec 0.77 0.73 Rec 0.69 0.81
F1 0.71 F1 0.74 F1 0.76
C? 0.63 C? 0.62 C? 0.53
Interestingly, C? is highest on the News corpus, whereas F1 is lowest. Whereas precision
and recall are normalized by the number of predictions from annotators A and B,
respectively, C? is normalized by the minimum number of predictions between the two.
Therefore, when the predictions are highly divergent, C? will paint a rosier picture than
F1 (which is the combination of precision and recall). This indeed seems to be the case
for the News corpus, where precision and recall have a higher spread in comparison to
the other two corpora (see the Poss column in Table 3).
Agreement scores tend to be lower when taking phrases into account (see Table 4).
This is expected because annotators are faced with a more complex task; they must
generally make more decisions: for example, determining the phrase boundaries and
how to align their constituent words. An exception to this trend is the News corpus
where the F1 is higher for phrase pairs than for individual word pairs. This is due to the
fact that there are many similar sentence pairs in this data. These have many identical
words and a few different words. The differences are often in a clump (e.g., person
names, verb phrases), rather than distributed throughout the sentence. The annotators
tend to block align these and there is a large scope for disagreement.Whereas estimating
agreement over words heavily penalizes block differences, when phrases are taken
into account in the F1 measure, these are treated more leniently. Note that C? is not
so lenient, as it measures agreement over the sets of atomic phrase pairs rather than
between atomic and composite phrase pairs in the F1 measure. This means that under
C?, choosing different granularities of phrases will be penalized, but would not have
been under the F1 measure.
In Figure 4we show how C? varies with sentence length for our three corpora. Specif-
ically, we plot observed agreement ??, chance agreement ?0, and C? against sentence pairs
609
Computational Linguistics Volume 34, Number 4
Figure 4
Agreement statistics plotted against sentence length for the three sub-corpora. Each group of
three columns correspond to ??, ??0, and C?, respectively. The statistics were measured over
non-identical phrase pairs using all phrase pairs, atomic and composite.
Table 5
Agreement between automatic Giza++ predicted word alignments and our manually corrected
alignments, measured over atomic phrase pairs.
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.58 0.55 Prec 0.63 0.60 Prec 0.63 0.65
Rec 0.42 0.49 Rec 0.39 0.47 Rec 0.50 0.64
F1 0.53 F1 0.54 F1 0.63
binned by (the shorter) sentence length. In all cases we observe that chance agreement
is substantially lower than observed agreement for all sentence lengths. We also see that
C? tends to be higher for shorter sentences. Differences in C? across sentence lengths are
mostly of small magnitude across all three corpora. This indicates that disagreements
may be due to other factors, besides sentence length.
Unfortunately, there are no comparable annotation studies that would allow us
to gauge the quality of the obtained agreements. The use of precision, recall, and F1
is widespread in SMT, but these measures evaluate automatic alignments against a
gold standard, rather than the agreement between two or more annotators (but see
Melamed [1998] for an exception). Nevertheless, we would expect the humans to agree
more with each other than with Giza++, given that the latter produces many erroneous
word alignments and is not specifically tuned to the paraphrasing task. Table 5 shows
agreement between one annotator and Giza++ for atomic phrase pairs.15 We obtained
similar results for the other annotator and with the word-based measures. As can be
seen, human?Giza++ agreement is much lower than human?human agreement on all
three corpora (compare Tables 5 and 4). Taken together the results in Tables 3?5 show
a substantial level of agreement, thus indicating that our definition of paraphrases via
word alignments can yield reliable annotations. In the following section we discuss how
our corpus can be usefully employed in the study of paraphrasing.
15 Note that we cannot meaningfully measure C? for this data because the Giza++ predictions are already
being used to estimate ?0 in our formulation. Consequently, P(A) = P(B) and C? is zero.
610
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
4. Experiments
Our annotated corpus can be used in a number of ways to help paraphrase research:
for example, to inform the linguistic analysis of paraphrases, as a training set for the
development of discriminative paraphrase systems, and as a test set for the automatic
evaluation of computational models. Here, we briefly demonstrate some of these uses.
Paraphrase Modeling.Much previous research has focused on lexical paraphrases (but see
Lin and Pantel [2001] and Pang, Knight, and Marcu [2003] for exceptions). We argue
that our corpus should support a richer range of structural (syntactic) paraphrases.
To demonstrate this we have extracted paraphrase rules from our annotations using
the grammar induction algorithm from Cohn and Lapata (2007). Briefly, the algorithm
extracts tree pairs from word-aligned text by choosing aligned constituents in a pair of
equivalent sentences. These pairs are then generalized by factoring out aligned subtrees,
thereby resulting in synchronous grammar rules (Aho and Ullman 1969) with variable
nodes.
We parsed the MTC corpus with Bikel?s (2002) parser and extracted synchronous
rules from the gold-standard alignments. A sample of these rules are shown in Figure 5.
Here we see three lexical paraphrases, followed by five structural paraphrases. In
example 4, also is replaced with moreover and is moved to the start of the sentence from
the pre-verbal position. Examples 5?8 show various reordering operations, where the
boxed numbers indicate correspondences between non-terminals in the two sides of the
rules.
The synchronous rules in Figure 5 provide insight into the process of paraphrasing
at the syntactic level, and also a practical means for developing algorithms for para-
phrase generation?a task which has received little attention to date. For instance, we
could envisage a paraphrase model that transforms parse trees of an input sentence
into parse trees that represent a sentential paraphrase of that sentence. Our corpus can
be used to learn this mapping using discriminative methods (Cowan, Kuc?erova?, and
Collins 2006; Cohn and Lapata 2007).
Evaluation Set. As mentioned in Section 1, it is currently difficult to compare competing
approaches due to the effort involved in eliciting manual judgments of paraphrase
output. Our corpus could fill the role of a gold-standard test set, allowing for automatic
evaluation techniques.
Developing measures for automatic paraphrase evaluation is outside the scope of
this article. Nevertheless, we illustrate how the corpus can be used for this purpose.
For example we could easily measure the precision and recall of an automatic system
Figure 5
Synchronous grammar rules extracted from the MTC corpus.
611
Computational Linguistics Volume 34, Number 4
against our annotations. Computing precision and recall for an individual system is not
perhaps the most meaningful test, considering the large potential for paraphrasing in
a given sentence pair. A better evaluation strategy would include a comparison across
many systems on the same corpus. We could then rank these systems without, however,
paying so much attention to the absolute precision and recall values. We expect these
comparisons to yield relatively low numbers for many reasons. First and foremost the
task is hard, as shown by our inter-annotator agreement figures in Tables 3 and 4.
Secondly, there may be valid paraphrases that the systems identify but are not listed
in our gold standard. Thirdly, systems may have different biases, for example, towards
producing more lexical or syntactic paraphrases, but our comparison would not take
this into account. Despite all these considerations, we believe that comparison against
our corpus would treat these systems on an equal footing against the same materials
while factoring out nonessential degrees of freedom inherent in human elicitation stud-
ies (e.g., attention span, task familiarity, background).
We evaluated the performance of two systems against our corpus. Our first system
is simply Giza++ trained on the 55, 615 sentence pairs described in Section 4. The
second system uses a co-training-based paraphrase extraction algorithm (Barzilay and
McKeown 2001). It was also trained on the MTC part 1 corpus, on the same data set
used for Giza++, with its default parameters. For each system, we filtered the predicted
paraphrases to just those which match part of a sentence pair in the test set. These
paraphrases were then compared to the sure phrase pairs extracted from our manually
aligned corpus. Giza++?s precision is 55% and recall 49% (see Table 5). The co-training
system obtained a precision of 30% and recall of 16%. To confirm the accuracy of
the precision estimate, we performed a human evaluation on a sample of 48 of the
predicted paraphrases which were treated as errors. Of these, 63% were confirmed as
being incorrect and only 20%were acceptable (the remaining were uncertain). The inter-
annotator agreement in Table 4 can be used as an upper bound for precision and recall
(precision for Sure phrase pairs is 67% and recall 66%). These results seem to suggest
that a hypothetical paraphrase extractor based on automatic word alignments would
obtain performance superior to the co-training approach. However, we must bear in
mind that the co-training system is highly parametrized and was not specifically tuned
to our data set.
5. Conclusions
In this article we have presented a human-annotated paraphrase corpus and argued
that it can be usefully employed for the evaluation and modeling of paraphrases. We
have defined paraphrases as word alignments in a corpus containing pairs of equivalent
sentences and shown that these can be reliably identified by annotators. In measur-
ing agreement, we used the standard measures of precision, recall, and F1, but also
proposed a novel formulation of chance-corrected agreement for word (and phrase)
alignments. Beyond alignment, our formulation could be applied to other structured
tasks including parsing and sequence labeling.
The uses of the corpus are many and varied. It can serve as a test set for eval-
uating the precision and recall of paraphrase induction systems trained on parallel
monolingual corpora. The corpus could be further used to develop new evaluation
metrics for paraphrase acquisition or novel paraphrasing models. An exciting avenue
for future research concerns paraphrase prediction, that is, determiningwhen and how to
paraphrase single sentence input. Because our corpus contains paraphrase annotations
at the sentence level, it could provide a natural test-bed for prediction algorithms.
612
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Acknowledgments
The authors acknowledge the support of the
EPSRC (Cohn, grant GR/T04557/01;
Lapata, grant GR/T04540/01), the National
Science Foundation (Callison-Burch, grant
IIS-071344), and the EuroMatrix project
(Callison-Burch) funded by the European
Commission (6th Framework Programme).
We are grateful to our annotators Vasilis
Karaiskos and Tom Segler. Thanks to Regina
Barzilay for providing us the output of her
system on our data and to the anonymous
referees whose feedback helped to
substantially improve the present article.
References
Aho, A. V. and J. D. Ullman. 1969. Syntax
directed translations and the pushdown
assembler. Journal of Compute System
Sciences, 3(1):37?56.
Artstein, Ron and Massimo Poesio. 2008.
Inter-coder agreement for Computational
Linguistics. Computational Linguistics.
Ayan, Necip Fazil and Bonnie J. Dorr. 2006.
Going beyond AER: An extensive analysis
of word alignments and their impact on
MT. In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 9?16,
Sydney.
Bannard, Colin and Chris Callison-Burch.
2005. Paraphrasing with bilingual parallel
corpora. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 597?604, Ann Arbor, MI.
Bartko, John J. and William T. Carpenter.
1976. On the methods and theory of
reliability. Journal of Nervous and Mental
Disease, 163(5):307?317.
Barzilay, Regina. 2003. Information Fusion for
Multi-Document Summarization:
Paraphrasing and Generation. Ph.D. thesis,
Columbia University, New York, NY.
Barzilay, Regina and Noemie Elhadad.
2003. Sentence alignment for monolingual
comparable corpora. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages 25?32,
Sapporo.
Barzilay, Regina and Lillian Lee. 2003.
Learning to paraphrase: An unsupervised
approach using multiple-sequence
alignment. In Proceedings of the Human
Language Technology Conference and the
Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 16?23, Edmonton.
Barzilay, Regina and Kathy McKeown. 2001.
Extracting paraphrases from a parallel
corpus. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics, pages 50?57, Toulouse.
Bikel, Daniel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing
engine. In Proceedings of the Human
Language Technology Conference,
pages 24?27, San Diego, CA.
Brown, Peter F., Stephen A. Della-Pietra,
Vincent J. Della-Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Callison-Burch, Chris. 2007. Paraphrasing and
Translation. Ph.D. thesis, University of
Edinburgh, Edinburgh, Scotland.
Callison-Burch, Chris, Philipp Koehn, and
Miles Osborne. 2006. Improved statistical
machine translation using paraphrases. In
Proceedings of the Human Language
Technology Conference and Annual Meeting of
the North American Chapter of the Association
for Computational Linguistics, pages 17?24,
New York, NY.
Cohen, J. 1960. A coefficient of agreement for
nominal scales. Educational and
Psychological Measurement, 20:37?46.
Cohn, Trevor and Mirella Lapata. 2007. Large
margin synchronous generation and its
application to sentence compression. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing and
on Computational Natural Language
Learning, pages 73?82, Prague.
Cowan, Brooke, Ivona Kuc?erova?, and
Michael Collins. 2006. A discriminative
model for tree-to-tree translation. In
Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 232?241, Sydney.
Daume? III, Hal and Daniel Marcu.
2004. A phrase-based HMM approach
to document/abstract alignment.
In Proceedings of the 2004 Conference
on Empirical Methods in Natural
Language Processing, pages 119?126,
Barcelona.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
Dolan, William, Chris Quirk, and Chris
Brockett. 2004. Unsupervised construction
of large paraphrase corpora: Exploiting
massively parallel news sources. In
Proceedings of the 20th International
Conference on Computational Linguistics,
pages 350?356, Geneva.
613
Computational Linguistics Volume 34, Number 4
Duboue, Pablo and Jennifer Chu-Carroll.
2006. Answering the question you wish
they had asked: The impact of
paraphrasing for question answering.
In Proceedings of the Human Language
Technology Conference of the NAACL,
Companion Volume: Short Papers,
pages 33?36, New York, NY.
Fleiss, Joseph L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Fraser, Alexander and Daniel Marcu. 2007.
Measuring word alignment quality for
statistical machine translation.
Computational Linguistics, 33(3):293?303.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Human Language Technology
Conference and Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 48?54,
Edmonton.
Kupper, Lawrence L. and Kerry B. Hafner.
1989. On assessing interrater agreement
for multiple attribute responses. Biometrics,
45(3):957?967.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7(4):342?360.
Martin, Joel, Rada Mihalcea, and Ted
Pedersen. 2005. Word alignment for
languages with scarce resources. In
Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 67?74,
Ann Arbor, MI.
Melamed, I. Dan. 1998. Manual annotation
of translational equivalence: The Blinker
project. IRCS Technical Report #98-07,
University of Pennsylvania,
Philadelphia, PA.
Mihalcea, Rada and Ted Pedersen. 2003. An
evaluation exercise for word alignment. In
Proceedings of the HLT-NAACL Workshop on
Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond,
pages 1?6, Edmonton.
Och, Franz Josef and Hermann Ney. 2000a. A
comparison of alignment models for
statistical machine translation. In
Proceedings of the 18th International
Conference on Computational Linguistics,
pages 1086?1090, Saarbru?cken.
Och, Franz Josef and Hermann Ney.
2000b. Improved statistical alignment
models. In Proceedings of the 38th
Annual Meeting of the Association for
Computational Linguistics, pages 440?447,
Hong Kong.
Och, Franz Josef and Hermann Ney. 2003. A
systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?52.
Och, Franz Josef, Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora,
pages 20?28, College Park, MD.
Pang, Bo, Kevin Knight, and Daniel Marcu.
2003. Syntax-based alignment of multiple
translations: Extracting paraphrases and
generating new sentences. In Proceedings of
the Human Language Technology Conference
and the Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 181?188,
Edmonton.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 142?149, Barcelona.
Scott, William A. 1955. Reliability of content
analysis: The case of nominal scale. Public
Opinion Quarterly, 19:127?141.
Siegel, Sidney and N. John Castellan. 1988.
Non Parametric Statistics for the Behavioral
Sciences. McGraw-Hill, New York.
Zhou, Liang, Chin-Yew Lin, Dragos Stefan
Munteanu, and Eduard Hovy. 2006.
Paraeval: Using paraphrases to
evaluate summaries automatically. In
Proceedings of the Human Language
Technology Conference, pages 447?454,
New York, NY.
614
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25?28,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
Demonstration of Joshua: An Open Source Toolkit
for Parsing-based Machine Translation
?
Zhifei Li, Chris Callison-Burch, Chris Dyer
?
, Juri Ganitkevitch
+
, Sanjeev Khudanpur,
Lane Schwartz
?
, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University
? Computational Linguistics and Information Processing Lab, University of Maryland
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University
? Natural Language Processing Lab, University of Minnesota
Abstract
We describe Joshua (Li et al, 2009a)
1
,
an open source toolkit for statistical ma-
chine translation. Joshua implements all
of the algorithms required for transla-
tion via synchronous context free gram-
mars (SCFGs): chart-parsing, n-gram lan-
guage model integration, beam- and cube-
pruning, and k-best extraction. The toolkit
also implements suffix-array grammar ex-
traction and minimum error rate training.
It uses parallel and distributed computing
techniques for scalability. We also pro-
vide a demonstration outline for illustrat-
ing the toolkit?s features to potential users,
whether they be newcomers to the field
or power users interested in extending the
toolkit.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high barrier
to entry for other researchers, and makes experi-
ments difficult to duplicate and compare. In this
paper, we describe Joshua, a Java-based general-
purpose open source toolkit for parsing-based ma-
chine translation, serving the same role as Moses
(Koehn et al, 2007) does for regular phrase-based
machine translation.
?
This research was supported in part by the Defense Ad-
vanced Research Projects Agency?s GALE program under
Contract No. HR0011-06-2-0001 and the National Science
Foundation under grants No. 0713448 and 0840112. The
views and findings are the authors? alone.
1
Please cite Li et al (2009a) if you use Joshua in your
research, and not this demonstration description paper.
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: Joshua?s codebase consists of
a separate Java package for each major aspect
of functionality. This way, researchers can focus
on a single package of their choosing. Fuur-
thermore, extensible components are defined by
Java interfaces to minimize unintended inter-
actions and unseen dependencies, a common hin-
drance to extensibility in large projects. Where
there is a clear point of departure for research,
a basic implementation of each interface is
provided as an abstract class to minimize
work necessary for extensions.
End-to-end Cohesion: An MT pipeline con-
sists of many diverse components, often designed
by separate groups that have different file formats
and interaction requirements. This leads to a large
number of scripts for format conversion and to
facilitate interaction between the components, re-
sulting in untenable and non-portable projects, and
hindering repeatability of experiments. Joshua, on
the other hand, integrates the critical components
of an MT pipeline seamlessly. Still, each compo-
nent can be used as a stand-alone tool that does not
rely on the rest of the toolkit.
Scalability: Joshua, especially the decoder, is
scalable to large models and data sets. For ex-
ample, the parsing and pruning algorithms are im-
plemented with dynamic programming strategies
and efficient data structures. We also utilize suffix-
array grammar extraction, parallel/distributed de-
coding, and bloom filter language models.
Joshua offers state-of-the-art quality, having
been ranked 4th out of 16 systems in the French-
English task of the 2009 WMT evaluation, both in
automatic (Table 1) and human evaluation.
25
System BLEU-4
google 31.14
lium 26.89
dcu 26.86
joshua 26.52
uka 25.96
limsi 25.51
uedin 25.44
rwth 24.89
cmu-statxfer 23.65
Table 1: BLEU scores for top primary systems on
the WMT-09 French-English Task from Callison-
Burch et al (2009), who also provide human eval-
uation results.
2.1 Joshua Toolkit Features
Here is a short description of Joshua?s main fea-
tures, described in more detail in Li et al (2009a):
? Training Corpus Sub-sampling: We sup-
port inducing a grammar from a subset
of the training data, that consists of sen-
tences needed to translate a particular test
set. To accomplish this, we make use of the
method proposed by Kishore Papineni (per-
sonal communication), outlined in further de-
tail in (Li et al, 2009a). The method achieves
a 90% reduction in training corpus size while
maintaining state-of-the-art performance.
? Suffix-array Grammar Extraction: Gram-
mars extracted from large training corpora
are often far too large to fit into available
memory. Instead, we follow Callison-Burch
et al (2005) and Lopez (2007), and use a
source language suffix array to extract only
rules that will actually be used in translating
a particular test set. Direct access to the suffix
array is incorporated into the decoder, allow-
ing rule extraction to be performed for each
input sentence individually, but it can also be
executed as a standalone pre-processing step.
? Grammar formalism: Our decoder as-
sumes a probabilistic synchronous context-
free grammar (SCFG). It handles SCFGs
of the kind extracted by Hiero (Chiang,
2007), but is easily extensible to more gen-
eral SCFGs (as in Galley et al (2006)) and
closely related formalisms like synchronous
tree substitution grammars (Eisner, 2003).
? Pruning: We incorporate beam- and cube-
pruning (Chiang, 2007) to make decoding
feasible for large SCFGs.
? k-best extraction: Given a source sentence,
the chart-parsing algorithm produces a hy-
pergraph representing an exponential num-
ber of derivation hypotheses. We implement
the extraction algorithm of Huang and Chi-
ang (2005) to extract the k most likely deriva-
tions from the hypergraph.
? Oracle Extraction: Even within the large
set of translations represented by a hyper-
graph, some desired translations (e.g. the ref-
erences) may not be contained due to pruning
or inherent modeling deficiency. We imple-
ment an efficient dynamic programming al-
gorithm (Li and Khudanpur, 2009) for find-
ing the oracle translations, which are most
similar to the desired translations, as mea-
sured by a metric such as BLEU.
? Parallel and distributed decoding: We
support parallel decoding and a distributed
language model that exploit multi-core and
multi-processor architectures and distributed
computing (Li and Khudanpur, 2008).
? Language Models: We implement three lo-
cal n-gram language models: a straightfor-
ward implementation of the n-gram scoring
function in Java, capable of reading stan-
dard ARPA backoff n-gram models; a na-
tive code bridge that allows the decoder to
use the SRILM toolkit to read and score n-
grams
2
; and finally a Bloom Filter implemen-
tation following Talbot and Osborne (2007).
? Minimum Error Rate Training: Joshua?s
MERT module optimizes parameter weights
so as to maximize performance on a develop-
ment set as measured by an automatic evalu-
ation metric, such as BLEU. The optimization
consists of a series of line-optimizations us-
ing the efficient method of Och (2003). More
details on the MERT method and the imple-
mentation can be found in Zaidan (2009).
3
2
The first implementation allows users to easily try the
Joshua toolkit without installing SRILM. However, users
should note that the basic Java LM implementation is not as
scalable as the SRILM native bridge code.
3
The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
26
? Variational Decoding: spurious ambiguity
causes the probability of an output string
among to be split among many derivations.
The goodness of a string is measured by
the total probability of its derivations, which
means that finding the best output string is
computationally intractable. The standard
Viterbi approximation is based on the most
probable derivation, but we also implement
a variational approximation, which considers
all the derivations but still allows tractable
decoding (Li et al, 2009b).
3 Demonstration Outline
The purpose of the demonstration is 4-fold: 1) to
give newcomers to the field of statistical machine
translation an idea of the state-of-the-art; 2) to
show actual, live, end-to-end operation of the sys-
tem, highlighting its main components, targeting
potential users; 3) to illustrate, through visual aids,
the underlying algorithms, for those interested in
the technical details; and 4) to explain how those
components can be extended, for potential power
users who want to be familiar with the code itself.
The first component of the demonstration will
be an interactive user interface, where arbitrary
user input in a source language is entered into a
web form and then translated into a target lan-
guage by the system. This component specifically
targets newcomers to SMT, and demonstrates the
current state of the art in the field. We will have
trained multiple systems (for multiple language
pairs), hosted on a remote server, which will be
queried with the sample source sentences.
Potential users of the system would be inter-
ested in seeing an actual operation of the system,
in a similar fashion to what they would observe
on their own machines when using the toolkit. For
this purpose, we will demonstrate three main mod-
ules of the toolkit: the rule extraction module, the
MERT module, and the decoding module. Each
module will have a separate terminal window ex-
ecuting it, hence demonstrating both the module?s
expected output as well as its speed of operation.
In addition to demonstrating the functionality
of each module, we will also provide accompa-
nying visual aids that illustrate the underlying al-
gorithms and the technical operational details. We
will provide visualization of the search graph and
(Software and documentation at: http://cs.jhu.edu/
?
ozaidan/zmert.)
the 1-best derivation, which would illustrate the
functionality of the decoder, as well as alterna-
tive translations for phrases of the source sentence,
and where they were learned in the parallel cor-
pus, illustrating the functionality of the grammar
rule extraction. For the MERT module, we will
provide figures that illustrate Och?s efficient line
search method.
4 Demonstration Requirements
The different components of the demonstration
will be spread across at most 3 machines (Fig-
ure 1): one for the live ?instant translation? user
interface, one for demonstrating the different com-
ponents of the system and algorithmic visualiza-
tions, and one designated for technical discussion
of the code. We will provide the machines our-
selves and ensure the proper software is installed
and configured. However, we are requesting that
large LCD monitors be made available, if possi-
ble, since that would allow more space to demon-
strate the different components with clarity than
our laptop displays would provide. We will also
require Internet connectivity for the live demon-
stration, in order to gain access to remote servers
where trained models will be hosted.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
27
We will rely on 3 workstations: 
one for the instant translation 
demo, where arbitrary input is 
translated from/to a language pair 
of choice (top); one for runtime 
demonstration of the system, with 
a terminal window for each of the 
three main components of the 
systems, as well as visual aids, 
such as derivation trees (left); and 
one (not shown) designated for 
technical discussion of the code.
Remote server 
hosting trained 
translation models
JHU
Grammar extraction
Decoder
M
E
R
T
Figure 1: Proposed setup of our demonstration. When this paper is viewed as a PDF, the reader may
zoom in further to see more details.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March. As-
sociation for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In Proceedings of ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
28
Proceedings of the Second Workshop on Statistical Machine Translation, pages 136?158,
Prague, June 2007. c?2007 Association for Computational Linguistics
(Meta-) Evaluation of Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb clsp jhu edu
Cameron Fordyce
CELCT
fordyce celct it
Philipp Koehn
University of Edinburgh
pkoehn inf ed ac uk
Christof Monz
Queen Mary, University of London
christof dcs qmul ac uk
Josh Schroeder
University of Edinburgh
j schroeder ed ac uk
Abstract
This paper evaluates the translation quality
of machine translation systems for 8 lan-
guage pairs: translating French, German,
Spanish, and Czech to English and back.
We carried out an extensive human evalua-
tion which allowed us not only to rank the
different MT systems, but also to perform
higher-level analysis of the evaluation pro-
cess. We measured timing and intra- and
inter-annotator agreement for three types of
subjective evaluation. We measured the cor-
relation of automatic evaluation metrics with
human judgments. This meta-evaluation re-
veals surprising facts about the most com-
monly used methodologies.
1 Introduction
This paper presents the results for the shared trans-
lation task of the 2007 ACL Workshop on Statistical
Machine Translation. The goals of this paper are
twofold: First, we evaluate the shared task entries
in order to determine which systems produce trans-
lations with the highest quality. Second, we analyze
the evaluation measures themselves in order to try to
determine ?best practices? when evaluating machine
translation research.
Previous ACL Workshops on Machine Transla-
tion were more limited in scope (Koehn and Monz,
2005; Koehn and Monz, 2006). The 2005 workshop
evaluated translation quality only in terms of Bleu
score. The 2006 workshop additionally included a
limited manual evaluation in the style of NIST ma-
chine translation evaluation workshop. Here we ap-
ply eleven different automatic evaluation metrics,
and conduct three different types of manual evalu-
ation.
Beyond examining the quality of translations pro-
duced by various systems, we were interested in ex-
amining the following questions about evaluation
methodologies: How consistent are people when
they judge translation quality? To what extent do
they agree with other annotators? Can we im-
prove human evaluation? Which automatic evalu-
ation metrics correlate most strongly with human
judgments of translation quality?
This paper is organized as follows:
? Section 2 gives an overview of the shared task.
It describes the training and test data, reviews
the baseline system, and lists the groups that
participated in the task.
? Section 3 describes the manual evaluation. We
performed three types of evaluation: scoring
with five point scales, relative ranking of trans-
lations of sentences, and ranking of translations
of phrases.
? Section 4 lists the eleven different automatic
evaluation metrics which were also used to
score the shared task submissions.
? Section 5 presents the results of the shared task,
giving scores for each of the systems in each of
the different conditions.
? Section 6 provides an evaluation of the dif-
ferent types of evaluation, giving intra- and
136
inter-annotator agreement figures for the man-
ual evaluation, and correlation numbers for the
automatic metrics.
2 Shared task overview
This year?s shared task changed in some aspects
from last year?s:
? We gave preference to the manual evaluation of
system output in the ranking of systems. Man-
ual evaluation was done by the volunteers from
participating groups and others. Additionally,
there were three modalities of manual evalua-
tion.
? Automatic metrics were also used to rank the
systems. In total eleven metrics were applied,
and their correlation with the manual scores
was measured.
? As in 2006, translation was from English, and
into English. English was again paired with
German, French, and Spanish. We additionally
included Czech (which was fitting given the lo-
cation of the WS).
Similar to the IWSLT International Workshop on
Spoken Language Translation (Eck and Hori, 2005;
Paul, 2006), and the NIST Machine Translation
Evaluation Workshop (Lee, 2006) we provide the
shared task participants with a common set of train-
ing and test data for all language pairs. The major
part of data comes from current and upcoming full
releases of the Europarl data set (Koehn, 2005).
2.1 Description of the Data
The data used in this year?s shared task was similar
to the data used in last year?s shared task. This year?s
data included training and development sets for the
News Commentary data, which was the surprise out-
of-domain test set last year.
The majority of the training data for the Spanish,
French, and German tasks was drawn from a new
version of the Europarl multilingual corpus. Addi-
tional training data was taken from the News Com-
mentary corpus. Czech language resources were
drawn from the News Commentary data. Additional
resources for Czech came from the CzEng Paral-
lel Corpus (Bojar and Z?abokrtsky?, 2006). Overall,
there are over 30 million words of training data per
language from the Europarl corpus and 1 million
words from the News Commentary corpus. Figure 1
provides some statistics about the corpora used this
year.
2.2 Baseline system
To lower the barrier of entrance to the competition,
we provided a complete baseline MT system, along
with data resources. To summarize, we provided:
? sentence-aligned training corpora
? development and dev-test sets
? language models trained for each language
? an open source decoder for phrase-based SMT
called Moses (Koehn et al, 2006), which re-
places the Pharaoh decoder (Koehn, 2004)
? a training script to build models for Moses
The performance of this baseline system is similar
to the best submissions in last year?s shared task.
2.3 Test Data
The test data was again drawn from a segment of
the Europarl corpus from the fourth quarter of 2000,
which is excluded from the training data. Partici-
pants were also provided with three sets of parallel
text to be used for system development and tuning.
In addition to the Europarl test set, we also col-
lected editorials from the Project Syndicate web-
site1, which are published in all the five languages
of the shared task. We aligned the texts at a sentence
level across all five languages, resulting in 2,007
sentences per language. For statistics on this test set,
refer to Figure 1.
The News Commentary test set differs from the
Europarl data in various ways. The text type are ed-
itorials instead of speech transcripts. The domain is
general politics, economics and science. However, it
is also mostly political content (even if not focused
on the internal workings of the European Union) and
opinion.
2.4 Participants
We received submissions from 15 groups from 14
institutions, as listed in Table 1. This is a slight
1http://www.project-syndicate.com/
137
Europarl Training corpus
Spanish? English French? English German? English
Sentences 1,259,914 1,288,901 1,264,825
Foreign words 33,159,337 33,176,243 29,582,157
English words 31,813,692 32,615,285 31,929,435
Distinct foreign words 345,944 344,287 510,544
Distinct English words 266,976 268,718 250,295
News Commentary Training corpus
Spanish? English French? English German? English Czech? English
Sentences 51,613 43,194 59,975 57797
Foreign words 1,263,067 1,028,672 1,297,673 1,083,122
English words 1,076,273 906,593 1,238,274 1,188,006
Distinct foreign words 84,303 68,214 115,589 142,146
Distinct English words 70,755 63,568 76,419 74,042
Language model data
English Spanish French German
Sentence 1,407,285 1,431,614 1,435,027 1,478,428
Words 34,539,822 36,426,542 35,595,199 32,356,475
Distinct words 280,546 385,796 361,205 558,377
Europarl test set
English Spanish French German
Sentences 2,000
Words 53,531 55,380 53,981 49,259
Distinct words 8,558 10,451 10,186 11,106
News Commentary test set
English Spanish French German Czech
Sentences 2,007
Words 43,767 50,771 49,820 45,075 39,002
Distinct words 10,002 10,948 11,244 12,322 15,245
Figure 1: Properties of the training and test sets used in the shared task. The training data is drawn from the
Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple
languages.
138
ID Participant
cmu-uka Carnegie Mellon University, USA (Paulik et al, 2007)
cmu-syntax Carnegie Mellon University, USA (Zollmann et al, 2007)
cu Charles University, Czech Republic (Bojar, 2007)
limsi LIMSI-CNRS, France (Schwenk, 2007)
liu University of Linko?ping, Sweden(Holmqvist et al, 2007)
nrc National Research Council, Canada (Ueffing et al, 2007)
pct a commercial MT provider from the Czech Republic
saar Saarland University & DFKI, Germany (Chen et al, 2007)
systran SYSTRAN, France & U. Edinburgh, UK (Dugast et al, 2007)
systran-nrc National Research Council, Canada (Simard et al, 2007)
ucb University of California at Berkeley, USA (Nakov and Hearst, 2007)
uedin University of Edinburgh, UK (Koehn and Schroeder, 2007)
umd University of Maryland, USA (Dyer, 2007)
upc University of Catalonia, Spain (Costa-Jussa` and Fonollosa, 2007)
upv University of Valencia, Spain (Civera and Juan, 2007)
Table 1: Participants in the shared task. Not all groups participated in all translation directions.
increase over last year?s shared task where submis-
sions were received from 14 groups from 11 insti-
tutions. Of the 11 groups that participated in last
year?s shared task, 6 groups returned this year.
This year, most of these groups follow a phrase-
based statistical approach to machine translation.
However, several groups submitted results from sys-
tems that followed a hybrid approach.
While building a machine translation system is a
serious undertaking we hope to attract more new-
comers to the field by keeping the barrier of entry
as low as possible. The creation of parallel corpora
such as the Europarl, the CzEng, and the News Com-
mentary corpora should help in this direction by pro-
viding freely available language resources for build-
ing systems. The creation of an open source baseline
system should also go a long way towards achieving
this goal.
For more on the participating systems, please re-
fer to the respective system description in the pro-
ceedings of the workshop.
3 Human evaluation
We evaluated the shared task submissions using both
manual evaluation and automatic metrics. While
automatic measures are an invaluable tool for the
day-to-day development of machine translation sys-
tems, they are an imperfect substitute for human
assessment of translation quality. Manual evalua-
tion is time consuming and expensive to perform,
so comprehensive comparisons of multiple systems
are rare. For our manual evaluation we distributed
the workload across a number of people, including
participants in the shared task, interested volunteers,
and a small number of paid annotators. More than
100 people participated in the manual evaluation,
with 75 of those people putting in at least an hour?s
worth of effort. A total of 330 hours of labor was in-
vested, nearly doubling last year?s all-volunteer ef-
fort which yielded 180 hours of effort.
Beyond simply ranking the shared task submis-
sions, we had a number of scientific goals for the
manual evaluation. Firstly, we wanted to collect
data which could be used to assess how well au-
tomatic metrics correlate with human judgments.
Secondly, we wanted to examine different types of
manual evaluation and assess which was the best.
A number of criteria could be adopted for choos-
ing among different types of manual evaluation: the
ease with which people are able to perform the task,
their agreement with other annotators, their reliabil-
ity when asked to repeat judgments, or the number
of judgments which can be collected in a fixed time
period.
There are a range of possibilities for how human
139
evaluation of machine translation can be done. For
instance, it can be evaluated with reading compre-
hension tests (Jones et al, 2005), or by assigning
subjective scores to the translations of individual
sentences (LDC, 2005). We examined three differ-
ent ways of manually evaluating machine translation
quality:
? Assigning scores based on five point adequacy
and fluency scales
? Ranking translated sentences relative to each
other
? Ranking the translations of syntactic con-
stituents drawn from the source sentence
3.1 Fluency and adequacy
The most widely used methodology when manually
evaluatingMT is to assign values from two five point
scales representing fluency and adequacy. These
scales were developed for the annual NIST Machine
Translation Evaluation Workshop by the Linguistics
Data Consortium (LDC, 2005).
The five point scale for adequacy indicates how
much of the meaning expressed in the reference
translation is also expressed in a hypothesis trans-
lation:
5 = All
4 = Most
3 = Much
2 = Little
1 = None
The second five point scale indicates how fluent
the translation is. When translating into English the
values correspond to:
5 = Flawless English
4 = Good English
3 = Non-native English
2 = Disfluent English
1 = Incomprehensible
Separate scales for fluency and adequacy were
developed under the assumption that a translation
might be disfluent but contain all the information
from the source. However, in principle it seems that
people have a hard time separating these two as-
pects of translation. The high correlation between
people?s fluency and adequacy scores (given in Ta-
bles 17 and 18) indicate that the distinction might be
false.
?
people
's
Iraq
to
services
basic
other
and
,
care
health
,
food
provide
cannot
it
if
occupation
its
sustain
US
the
Can
?k
?
n
n
e
n
a
n
b
i
e
t
e
n
D
i
e
n
s
t
l
e
i
s
t
u
n
g
e
n
g
r
u
n
d
l
e
g
e
n
d
e
a
n
d
e
r
e
u
n
d
G
e
s
u
n
d
h
e
i
t
s
f
?
r
s
o
r
g
e
,N
a
h
r
u
n
g
n
i
c
h
t
V
o
l
k
i
r
a
k
i
s
c
h
e
n
d
e
m
s
i
e
w
e
n
n
,U
S
A
d
i
e
K
?
n
n
e
n
a
u
f
r
e
c
h
t
e
r
h
a
l
t
e
n
B
e
s
e
t
z
u
n
g
 
 
i
h
r
e
R
e
f
e
r
e
n
c
e
 
t
r
a
n
s
l
a
t
i
o
n
NP
NP
NP
VP
NP
VP
S
S
CNP
NP
Constituents selected 
for evaluation
Target phrases
highlighted via
word alignments
Parsed source
sentence
Figure 2: In constituent-based evaluation, the source
sentence was parsed, and automatically aligned with
the reference translation and systems? translations
Another problem with the scores is that there are
no clear guidelines on how to assign values to trans-
lations. No instructions are given to evaluators in
terms of how to quantify meaning, or how many
grammatical errors (or what sort) separates the dif-
ferent levels of fluency. Because of this many judges
either develop their own rules of thumb, or use the
scales as relative rather than absolute. These are
borne out in our analysis of inter-annotator agree-
ment in Section 6.
3.2 Ranking translations of sentences
Because fluency and adequacy were seemingly diffi-
cult things for judges to agree on, and because many
people from last year?s workshop seemed to be using
them as a way of ranking translations, we decided to
try a separate evaluation where people were simply
140
asked to rank translations. The instructions for this
task were:
Rank each whole sentence translation
from Best to Worst relative to the other
choices (ties are allowed).
These instructions were just as minimal as for flu-
ency and adequacy, but the task was considerably
simplified. Rather than having to assign each trans-
lation a value along an arbitrary scale, people simply
had to compare different translations of a single sen-
tence and rank them.
3.3 Ranking translations of syntactic
constituents
In addition to having judges rank the translations
of whole sentences, we also conducted a pilot
study of a new type of evaluation methodology,
which we call constituent-based evaluation. In our
constituent-based evaluation we parsed the source
language sentence, selected constituents from the
tree, and had people judge the translations of those
syntactic phrases. In order to draw judges? attention
to these regions, we highlighted the selected source
phrases and the corresponding phrases in the transla-
tions. The corresponding phrases in the translations
were located via automatic word alignments.
Figure 2 illustrates the constituent based evalu-
ation when applied to a German source sentence.
The German source sentence is parsed, and vari-
ous phrases are selected for evaluation. Word align-
ments are created between the source sentence and
the reference translation (shown), and the source
sentence and each of the system translations (not
shown). We parsed the test sentences for each of
the languages aside from Czech. We used Cowan
and Collins (2005)?s parser for Spanish, Arun and
Keller (2005)?s for French, Dubey (2005)?s for Ger-
man, and Bikel (2002)?s for English.
The word alignments were created with Giza++
(Och and Ney, 2003) applied to a parallel corpus
containing 200,000 sentence pairs of the training
data, plus sets of 4,007 sentence pairs created by
pairing the test sentences with the reference transla-
tions, and the test sentences paired with each of the
system translations. The phrases in the translations
were located using techniques from phrase-based
statistical machine translation which extract phrase
pairs fromword alignments (Koehn et al, 2003; Och
and Ney, 2004). Because the word-alignments were
created automatically, and because the phrase ex-
traction is heuristic, the phrases that were selected
may not exactly correspond to the translations of the
selected source phrase. We noted this in the instruc-
tions to judges:
Rank each constituent translation from
Best to Worst relative to the other choices
(ties are allowed). Grade only the high-
lighted part of each translation.
Please note that segments are selected au-
tomatically, and they should be taken as
an approximate guide. They might in-
clude extra words that are not in the actual
alignment, or miss words on either end.
The criteria that we used to select which con-
stituents were to be evaluated were:
? The constituent could not be the whole source
sentence
? The constituent had to be longer three words,
and be no longer than 15 words
? The constituent had to have a corresponding
phrase with a consistent word alignment in
each of the translations
The final criterion helped reduce the number of
alignment errors.
3.4 Collecting judgments
We collected judgments using a web-based tool.
Shared task participants were each asked to judge
200 sets of sentences. The sets consisted of 5 sys-
tem outputs, as shown in Figure 3. The judges
were presented with batches of each type of eval-
uation. We presented them with five screens of ade-
quacy/fluency scores, five screens of sentence rank-
ings, and ten screens of constituent rankings. The
order of the types of evaluation were randomized.
In order to measure intra-annotator agreement
10% of the items were repeated and evaluated twice
by each judge. In order to measure inter-annotator
agreement 40% of the items were randomly drawn
from a common pool that was shared across all
141
http://www.statmt.org/wmt07/shared-task/judge/do_task.php
WMT07 Manual Evaluation
Rank Segments
You have judged 25 sentences for WMT07 German-English News Corpus, 190 sentences total taking 64.9 seconds per sentence.
Source: K?nnen die USA ihre Besetzung aufrechterhalten, wenn sie dem irakischen Volk nicht Nahrung, Gesundheitsf?rsorge und andere 
grundlegende Dienstleistungen anbieten k?nnen?
Reference: Can the US sustain its occupation if it cannot provide food, health care, and other basic services to Iraq's people?
Translation Rank
The United States can maintain its employment when it the Iraqi people not food, health care and other 
basic services on offer?.
1
Worst
2 3 4 5
Best
The US can maintain its occupation, if they cannot offer the Iraqi people food, health care and other basic 
services?
1
Worst
2 3 4 5
Best
Can the US their occupation sustained if it to the Iraqi people not food, health care and other basic 
services can offer?
1
Worst
2 3 4 5
Best
Can the United States maintain their occupation, if the Iraqi people do not food, health care and other 
basic services can offer?
1
Worst
2 3 4 5
Best
The United States is maintained, if the Iraqi people, not food, health care and other basic services can 
offer?
1
Worst
2 3 4 5
Best
Annotator: ccb Task: WMT07 German-English News Corpus
Instructions: 
Rank each constituent translation from Best to Worst relative to the other choices (ties are allowed). Grade 
only the highlighted part of each translation.
Please note that segments are selected automatically, and they should be taken as an approximate guide. 
They might include extra words on either end that are not in the actual alignment, or miss words.
 
Figure 3: For each of the types of evaluation, judges were shown screens containing up to five different
system translations, along with the source sentence and reference translation.
annotators so that we would have items that were
judged by multiple annotators.
Judges were allowed to select whichever data set
they wanted, and to evaluate translations into what-
ever languages they were proficient in. Shared task
participants were excluded from judging their own
systems.
Table 2 gives a summary of the number of judg-
ments that we collected for translations of individ-
ual sentences. Since we had 14 translation tasks and
four different types of scores, there were 55 differ-
ent conditions.2 In total we collected over 81,000
judgments. Despite the large number of conditions
we managed to collect more than 1,000 judgments
for most of them. This provides a rich source of data
for analyzing the quality of translations produced by
different systems, the different types of human eval-
uation, and the correlation of automatic metrics with
human judgments.3
2We did not perform a constituent-based evaluation for
Czech to English because we did not have a syntactic parser
for Czech. We considered adapting our method to use Bojar
(2004)?s dependency parser for Czech, but did not have the time.
3The judgment data along with all system translations are
available at http://www.statmt.org/wmt07/
4 Automatic evaluation
The past two ACL workshops on machine trans-
lation used Bleu as the sole automatic measure of
translation quality. Bleu was used exclusively since
it is the most widely used metric in the field and
has been shown to correlate with human judgments
of translation quality in many instances (Dodding-
ton, 2002; Coughlin, 2003; Przybocki, 2004). How-
ever, recent work suggests that Bleu?s correlation
with human judgments may not be as strong as pre-
viously thought (Callison-Burch et al, 2006). The
results of last year?s workshop further suggested that
Bleu systematically underestimated the quality of
rule-based machine translation systems (Koehn and
Monz, 2006).
We used the manual evaluation data as a means of
testing the correlation of a range of automatic met-
rics in addition to Bleu. In total we used eleven
different automatic evaluation measures to rank the
shared task submissions. They are:
? Meteor (Banerjee and Lavie, 2005)?Meteor
measures precision and recall of unigrams
when comparing a hypothesis translation
142
Language Pair Test Set Adequacy Fluency Rank Constituent
English-German Europarl 1,416 1,418 1,419 2,626
News Commentary 1,412 1,413 1,412 2,755
German-English Europarl 1,525 1,521 1,514 2,999
News Commentary 1,626 1,620 1,601 3,084
English-Spanish Europarl 1,000 1,003 1,064 1,001
News Commentary 1,272 1,272 1,238 1,595
Spanish-English Europarl 1,174 1,175 1,224 1,898
News Commentary 947 949 922 1,339
English-French Europarl 773 772 769 1,456
News Commentary 729 735 728 1,313
French-English Europarl 834 833 830 1,641
News Commentary 1,041 1,045 1,035 2,036
English-Czech News Commentary 2,303 2,304 2,331 3,968
Czech-English News Commentary 1,711 1,711 1,733 0
Totals 17,763 17,771 17,820 27,711
Table 2: The number of items that were judged for each task during the manual evaluation
against a reference. It flexibly matches words
using stemming and WordNet synonyms. Its
flexible matching was extended to French,
Spanish, German and Czech for this workshop
(Lavie and Agarwal, 2007).
? Bleu (Papineni et al, 2002)?Bleu is currently
the de facto standard in machine translation
evaluation. It calculates n-gram precision and
a brevity penalty, and can make use of multi-
ple reference translations as a way of capturing
some of the allowable variation in translation.
We use a single reference translation in our ex-
periments.
? GTM (Melamed et al, 2003)?GTM general-
izes precision, recall, and F-measure to mea-
sure overlap between strings, rather than over-
lap between bags of items. An ?exponent? pa-
rameter which controls the relative importance
of word order. A value of 1.0 reduces GTM to
ordinary unigram overlap, with higher values
emphasizing order.4
? Translation Error Rate (Snover et al, 2006)?
4The GTM scores presented here are an F-measure with a
weight of 0.1, which counts recall at 10x the level of precision.
The exponent is set at 1.2, which puts a mild preference towards
items with words in the correct order. These parameters could
be optimized empirically for better results.
TER calculates the number of edits required to
change a hypothesis translation into a reference
translation. The possible edits in TER include
insertion, deletion, and substitution of single
words, and an edit which moves sequences of
contiguous words.
? ParaEval precision and ParaEval recall (Zhou
et al, 2006)?ParaEval matches hypothesis and
reference translations using paraphrases that
are extracted from parallel corpora in an unsu-
pervised fashion (Bannard and Callison-Burch,
2005). It calculates precision and recall using a
unigram counting strategy.
? Dependency overlap (Amigo? et al, 2006)?
This metric uses dependency trees for the hy-
pothesis and reference translations, by comput-
ing the average overlap between words in the
two trees which are dominated by grammatical
relationships of the same type.
? Semantic role overlap (Gime?nez and Ma`rquez,
2007)?This metric calculates the lexical over-
lap between semantic roles (i.e., semantic argu-
ments or adjuncts) of the same type in the the
hypothesis and reference translations. It uni-
formly averages lexical overlap over all seman-
tic role types.
143
? Word Error Rate over verbs (Popovic and Ney,
2007)?WER? creates a new reference and a
new hypothesis for each POS class by extract-
ing all words belonging to this class, and then
to calculate the standardWER.We show results
for this metric over verbs.
? Maximum correlation training on adequacy and
on fluency (Liu and Gildea, 2007)?a lin-
ear combination of different evaluation metrics
(Bleu, Meteor, Rouge, WER, and stochastic it-
erative alignment) with weights set to maxi-
mize Pearson?s correlation with adequacy and
fluency judgments. Weights were trained on
WMT-06 data.
The scores produced by these are given in the ta-
bles at the end of the paper, and described in Sec-
tion 5. We measured the correlation of the automatic
evaluation metrics with the different types of human
judgments on 12 data conditions, and report these in
Section 6.
5 Shared task results
The results of the human evaluation are given in Ta-
bles 9, 10, 11 and 12. Each of those tables present
four scores:
? FLUENCY and ADEQUACY are normalized ver-
sions of the five point scores described in Sec-
tion 3.1. The tables report an average of the
normalized scores.5
? RANK is the average number of times that a
system was judged to be better than any other
system in the sentence ranking evaluation de-
scribed in Section 3.2.
? CONSTITUENT is the average number of times
that a system was judged to be better than any
other system in the constituent-based evalua-
tion described in Section 3.3.
There was reasonably strong agreement between
these four measures at which of the entries was the
best in each data condition. There was complete
5Since different annotators can vary widely in how they as-
sign fluency and adequacy scores, we normalized these scores
on a per-judge basis using the method suggested by Blatz et al
(2003) in Chapter 5, page 97.
SYSTRAN (systran) 32%
University of Edinburgh (uedin) 20%
University of Catalonia (upc) 15%
LIMSI-CNRS (limsi) 13%
University of Maryland (umd) 5%
National Research Council of Canada?s
joint entry with SYSTRAN (systran-nrc)
5%
Commercial Czech-English system (pct) 5%
University of Valencia (upv) 2%
Charles University (cu) 2%
Table 3: The proportion of time that participants?
entries were top-ranked in the human evaluation
University of Edinburgh (uedin) 41%
University of Catalonia (upc) 12%
LIMSI-CNRS (limsi) 12%
University of Maryland (umd) 9%
Charles University (cu) 4%
Carnegie Mellon University (cmu-syntax) 4%
Carnegie Mellon University (cmu-uka) 4%
University of California at Berkeley (ucb) 3%
National Research Council?s joint entry
with SYSTRAN (systran-nrc)
2%
SYSTRAN (systran) 2%
Saarland University (saar) 0.8%
Table 4: The proportion of time that participants?
entries were top-ranked by the automatic evaluation
metrics
agreement between them in 5 of the 14 conditions,
and agreement between at least three of them in 10
of the 14 cases.
Table 3 gives a summary of how often differ-
ent participants? entries were ranked #1 by any of
the four human evaluation measures. SYSTRAN?s
entries were ranked the best most often, followed
by University of Edinburgh, University of Catalonia
and LIMSI-CNRS.
The following systems were the best perform-
ing for the different language pairs: SYSTRAN
was ranked the highest in German-English, Uni-
versity of Catalonia was ranked the highest in
Spanish-English, LIMSI-CNRS was ranked high-
est in French-English, and the University of Mary-
land and a commercial system were the highest for
144
Evaluation type P (A) P (E) K
Fluency (absolute) .400 .2 .250
Adequacy (absolute) .380 .2 .226
Fluency (relative) .520 .333 .281
Adequacy (relative) .538 .333 .307
Sentence ranking .582 .333 .373
Constituent ranking .693 .333 .540
Constituent ranking .712 .333 .566
(w/identical constituents)
Table 5: Kappa coefficient values representing the
inter-annotator agreement for the different types of
manual evaluation
Czech-English.
While we consider the human evaluation to be
primary, it is also interesting to see how the en-
tries were ranked by the various automatic evalua-
tion metrics. The complete set of results for the auto-
matic evaluation are presented in Tables 13, 14, 15,
and 16. An aggregate summary is provided in Table
4. The automatic evaluation metrics strongly favor
the University of Edinburgh, which garners 41% of
the top-ranked entries (which is partially due to the
fact it was entered in every language pair). Signif-
icantly, the automatic metrics disprefer SYSTRAN,
which was strongly favored in the human evaluation.
6 Meta-evaluation
In addition to evaluating the translation quality of
the shared task entries, we also performed a ?meta-
evaluation? of our evaluation methodologies.
6.1 Inter- and Intra-annotator agreement
We measured pairwise agreement among annotators
using the kappa coefficient (K) which is widely used
in computational linguistics for measuring agree-
ment in category judgments (Carletta, 1996). It is
defined as
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. We define chance
agreement for fluency and adequacy as 15 , since they
are based on five point scales, and for ranking as 13
Evaluation type P (A) P (E) K
Fluency (absolute) .630 .2 .537
Adequacy (absolute) .574 .2 .468
Fluency (relative) .690 .333 .535
Adequacy (relative) .696 .333 .544
Sentence ranking .749 .333 .623
Constituent ranking .825 .333 .738
Constituent ranking .842 .333 .762
(w/identical constituents)
Table 6: Kappa coefficient values for intra-annotator
agreement for the different types of manual evalua-
tion
since there are three possible out comes when rank-
ing the output of a pair of systems: A > B, A = B,
A < B.
For inter-annotator agreement we calculated
P (A) for fluency and adequacy by examining all
items that were annotated by two or more annota-
tors, and calculating the proportion of time they as-
signed identical scores to the same items. For the
ranking tasks we calculated P (A) by examining all
pairs of systems which had been judged by two or
more judges, and calculated the proportion of time
that they agreed that A > B, A = B, or A < B.
For intra-annotator agreement we did similarly, but
gathered items that were annotated on multiple oc-
casions by a single annotator.
Table 5 gives K values for inter-annotator agree-
ment, and Table 6 gives K values for intra-annoator
agreement. These give an indication of how often
different judges agree, and how often single judges
are consistent for repeated judgments, respectively.
The interpretation of Kappa varies, but according to
Landis and Koch (1977) 0??.2 is slight, .21??.4
is fair, .41??.6 is moderate, .61??.8 is substantial
and the rest almost perfect.
The K values for fluency and adequacy should
give us pause about using these metrics in the fu-
ture. When we analyzed them as they are intended to
be?scores classifying the translations of sentences
into different types?the inter-annotator agreement
was barely considered fair, and the intra-annotator
agreement was only moderate. Even when we re-
assessed fluency and adequacy as relative ranks the
agreements increased only minimally.
145
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 0.08
 0  10  20  30  40  50  60
n
u
m
 
s
e
n
t
e
n
c
e
s
 
t
a
k
i
n
g
 
t
h
i
s
 
l
o
n
g
 
(
%
)
time to judge one sentence (seconds)
constituent ranksentence rankfluency+adequacy scoring
Figure 4: Distributions of the amount of time it took
to judge single sentences for the three types of man-
ual evaluation
The agreement on the other two types of man-
ual evaluation that we introduced were considerably
better. The both the sentence and constituent ranking
had moderate inter-annotator agreement and sub-
stantial intra-annotator agreement. Because the con-
stituent ranking examined the translations of short
phrases, often times all systems produced the same
translations. Since these trivially increased agree-
ment (since they would always be equally ranked)
we also evaluated the inter- and intra-annotator
agreement when those items were excluded. The
agreement remained very high for constituent-based
evaluation.
6.2 Timing
We used the web interface to collect timing infor-
mation. The server recorded the time when a set of
sentences was given to a judge and the time when
the judge returned the sentences. We divided the
time that it took to do a set by the number of sen-
tences in the set. The average amount of time that it
took to assign fluency and adequacy to a single sen-
tence was 26 seconds.6 The average amount of time
it took to rank a sentence in a set was 20 seconds.
The average amount of time it took to rank a high-
lighted constituent was 11 seconds. Figure 4 shows
the distribution of times for these tasks.
6Sets which took longer than 5 minutes were excluded from
these calculations, because there was a strong chance that anno-
tators were interrupted while completing the task.
These timing figures are promising because they
indicate that the tasks which the annotators were the
most reliable on (constituent ranking and sentence
ranking) were also much quicker to complete than
the ones that they were unreliable on (assigning flu-
ency and adequacy scores). This suggests that flu-
ency and adequacy should be replaced with ranking
tasks in future evaluation exercises.
6.3 Correlation between automatic metrics and
human judgments
To measure the correlation of the automatic metrics
with the human judgments of translation quality we
used Spearman?s rank correlation coefficient ?. We
opted for Spearman rather than Pearson because it
makes fewer assumptions about the data. Impor-
tantly, it can be applied to ordinal data (such as the
fluency and adequacy scales). Spearman?s rank cor-
relation coefficient is equivalent to Pearson correla-
tion on ranks.
After the raw scores that were assigned to systems
by an automatic metric and by one of our manual
evaluation techniques have been converted to ranks,
we can calculate ? using the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher value for ? is
making predictions that are more similar to the hu-
man judgments than an automatic evaluation metric
with a lower ?.
Table 17 reports ? for the metrics which were
used to evaluate translations into English.7. Table
7 summarizes the results by averaging the correla-
tion numbers by equally weighting each of the data
conditions. The table ranks the automatic evalua-
tion metrics based on how well they correlated with
human judgments. While these are based on a rela-
tively few number of items, and while we have not
performed any tests to determine whether the dif-
ferences in ? are statistically significant, the results
7The Czech-English conditions were excluded since there
were so few systems
146
are nevertheless interesting, since three metrics have
higher correlation than Bleu:
? Semantic role overlap (Gime?nez and Ma`rquez,
2007), which makes its debut in the proceed-
ings of this workshop
? ParaEval measuring recall (Zhou et al, 2006),
which has a model of allowable variation in
translation that uses automatically generated
paraphrases (Callison-Burch, 2007)
? Meteor (Banerjee and Lavie, 2005) which also
allows variation by introducing synonyms and
by flexibly matches words using stemming.
Tables 18 and 8 report ? for the six metrics which
were used to evaluate translations into the other lan-
guages. Here we find that Bleu and TER are the
closest to human judgments, but that overall the cor-
relations are much lower than for translations into
English.
7 Conclusions
Similar to last year?s workshop we carried out an ex-
tensive manual and automatic evaluation of machine
translation performance for translating from four
European languages into English, and vice versa.
This year we substantially increased the number of
automatic evaluation metrics and were also able to
nearly double the efforts of producing the human
judgments.
There were substantial differences in the results
results of the human and automatic evaluations. We
take the human judgments to be authoritative, and
used them to evaluate the automatic metrics. We
measured correlation using Spearman?s coefficient
and found that three less frequently used metrics
were stronger predictors of human judgments than
Bleu. They were: semantic role overlap (newly in-
troduced in this workshop) ParaEval-recall and Me-
teor.
Although we do not claim that our observations
are indisputably conclusive, they again indicate that
the choice of automatic metric can have a signifi-
cant impact on comparing systems. Understanding
the exact causes of those differences still remains an
important issue for future research.
metric A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
O
V
E
R
A
L
L
Semantic
role overlap
.774 .839 .803 .741 .789
ParaEval-
Recall
.712 .742 .768 .798 .755
Meteor .701 .719 .745 .669 .709
Bleu .690 .722 .672 .602 .671
1-TER .607 .538 .520 .514 .644
Max adequ-
correlation
.651 .657 .659 .534 .626
Max fluency
correlation
.644 .653 .656 .512 .616
GTM .655 .674 .616 .495 .610
Dependency
overlap
.639 .644 .601 .512 .599
ParaEval-
Precision
.639 .654 .610 .491 .598
1-WER of
verbs
.378 .422 .431 .297 .382
Table 7: Average corrections for the different auto-
matic metrics when they are used to evaluate trans-
lations into English
metric A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
O
V
E
R
A
L
L
Bleu .657 .445 .352 .409 .466
1-TER .589 .419 .361 .380 .437
Max fluency
correlation
.534 .419 .368 .400 .430
Max adequ-
correlation
.498 .414 .385 .409 .426
Meteor .490 .356 .279 .304 .357
1-WER of
verbs
.371 .304 .359 .359 .348
Table 8: Average corrections for the different auto-
matic metrics when they are used to evaluate trans-
lations into the other languages
147
This year?s evaluation also measured the agree-
ment between human assessors by computing the
Kappa coefficient. One striking observation is
that inter-annotator agreement for fluency and ad-
equacy can be called ?fair? at best. On the other
hand, comparing systems by ranking them manually
(constituents or entire sentences), resulted in much
higher inter-annotator agreement.
Acknowledgments
This work was supported in part by the EuroMa-
trix project funded by the European Commission
(6th Framework Programme), and in part by the
GALE program of the US Defense Advanced Re-
search Projects Agency, Contract No. HR0011-06-
C-0022.
We are grateful to Jesu?s Gime?nez, Dan Melamed,
Maja Popvic, Ding Liu, Liang Zhou, and Abhaya
Agarwal for scoring the entries with their automatic
evaluation metrics. Thanks to Brooke Cowan for
parsing the Spanish test sentences, to Josh Albrecht
for his script for normalizing fluency and adequacy
on a per judge basis, and to Dan Melamed, Rebecca
Hwa, Alon Lavie, Colin Bannard andMirella Lapata
for their advice about statistical tests.
References
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of COLING-ACL06.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of ACL.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, Michigan.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL-2005.
Dan Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of HLT.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. CLSP Summer Workshop Final
Report WS2003, Johns Hopkins University.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2006. CzEng:
Czech-English Parallel Corpus, Release version 0.5.
Prague Bulletin of Mathematical Linguistics, 86.
Ondr?ej Bojar. 2004. Problems of inducing large
coverage constraint-based dependency grammar for
Czech. In Constraint Solving and Language Process-
ing, CSLP 2004, volume LNAI 3438. Springer.
Ondr?ej Bojar. 2007. English-to-Czech factored machine
translation. In Proceedings of the ACL-2007 Work-
shop on Statistical Machine Translation (WMT-07),
Prague.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In Proceedings of EACL.
Chris Callison-Burch. 2007. Paraphrasing and Transla-
tion. Ph.D. thesis, University of Edinburgh, Scotland.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison. 2007.
Multi-engine machine translation with an open-source
decoder for statistical machine translation. In Pro-
ceedings of the ACL-2007 Workshop on Statistical Ma-
chine Translation (WMT-07), Prague.
Jorge Civera and Alfons Juan. 2007. Domain adaptation
in statistical machine translation with mixture mod-
elling. In Proceedings of the ACL-2007 Workshop on
Statistical Machine Translation (WMT-07), Prague.
Marta R. Costa-Jussa` and Jose? A.R. Fonollosa. 2007.
Analysis of statistical and morphological classes to
generate weighted reordering hypotheses on a statisti-
cal machine translation system. In Proceedings of the
ACL-2007 Workshop on Statistical Machine Transla-
tion (WMT-07), Prague.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality. In
Proceedings of MT Summit IX.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of Spanish. In
Proceedings of EMNLP 2005.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Human Language Technology: Notebook
Proceedings, pages 128?132, San Diego.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In Proceedings of ACL.
148
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of the ACL-2007
Workshop on Statistical Machine Translation (WMT-
07), Prague.
Christopher J. Dyer. 2007. The ?noisier channel?: trans-
lation from morphologically complex languages. In
Proceedings of the ACL-2007 Workshop on Statistical
Machine Translation (WMT-07), Prague.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proceedings of
International Workshop on Spoken Language Transla-
tion.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguistic fea-
tures for automatic evaluation of heterogenous mt sys-
tems. In Proceedings of ACL Workshop on Statistical
Machine Translation.
Maria Holmqvist, Sara Stymne, and Lars Ahrenberg.
2007. Getting to know Moses: Initial experiments
on German-English factored translation. In Proceed-
ings of the ACL-2007 Workshop on Statistical Machine
Translation (WMT-07), Prague.
Douglas Jones, Wade Shen, Neil Granoien, Martha Her-
zog, and Clifford Weinstein. 2005. Measuring trans-
lation quality by testing english speakers with a new
defense language proficiency test for arabic. In Pro-
ceedings of the 2005 International Conference on In-
telligence Analysis.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between European lan-
guages. In Proceedings of ACL 2005 Workshop on
Parallel Text Translation.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the ACL-2007 Workshop on
Statistical Machine Translation (WMT-07), Prague.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris
Callison-Burch, Alexandra Constantin, Brooke
Cowan, Chris Dyer, Marcello Federico, Evan Herbst,
Hieu Hoang, Christine Moran, Wade Shen, and
Richard Zens. 2006. Factored translation models.
CLSP Summer Workshop Final Report WS-2006,
Johns Hopkins University.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159?174.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels
of correlation with human judgments. In Proceedings
of the Workshop on Statistical Machine Translation,
Prague, June. Association for Computational Linguis-
tics.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Audrey Lee. 2006. NIST 2006 machine translation eval-
uation official results. Official release of automatic
evaluation scores for all submissions, November.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In Proceedings of NAACL.
Dan Melamed, Ryan Green, and Jospeh P. Turian. 2003.
Precision and recall of machine translation. In Pro-
ceedings of HLT/NAACL.
Preslav Nakov and Marti Hearst. 2007. UCB system de-
scription for the WMT 2007 shared task. In Proceed-
ings of the ACL-2007 Workshop on Statistical Machine
Translation (WMT-07), Prague.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Michael Paul. 2006. Overview of the IWSLT 2006
evaluation campaign. In Proceedings of International
Workshop on Spoken Language Translation.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja
Hildebrand, and Stephan Vogel. 2007. The ISL
phrase-based MT system for the 2007 ACL Workshop
on Statistical Machine Translation. In Proceedings of
the ACL-2007 Workshop on Statistical Machine Trans-
lation (WMT-07), Prague.
149
Maja Popovic and Hermann Ney. 2007. Word error rates:
Decomposition over POS classes and applications for
error analysis. In Proceedings of ACL Workshop on
Statistical Machine Translation.
Mark Przybocki. 2004. NIST 2004 machine translation
evaluation results. Confidential e-mail to workshop
participants, May.
Holger Schwenk. 2007. Building a statistical machine
translation system for French using the Europarl cor-
pus. In Proceedings of the ACL-2007 Workshop on
Statistical Machine Translation (WMT-07), Prague.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and
Roland Kuhn. 2007. Rule-based translation with sta-
tistical phrase-based post-editing. In Proceedings of
the ACL-2007 Workshop on Statistical Machine Trans-
lation (WMT-07), Prague.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Statistical Machine
Translation in the Americas.
Nicola Ueffing, Michel Simard, Samuel Larkin, and
Howard Johnson. 2007. NRC?s PORTAGE system for
WMT 2007. In Proceedings of the ACL-2007 Work-
shop on Statistical Machine Translation (WMT-07),
Prague.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proceedings of EMNLP.
Andreas Zollmann, Ashish Venugopal, Matthias Paulik,
and Stephan Vogel. 2007. The syntax augmented MT
(SAMT) system for the shared task in the 2007 ACL
Workshop on Statistical Machine Translation. In Pro-
ceedings of the ACL-2007 Workshop on Statistical Ma-
chine Translation (WMT-07), Prague.
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
German-English Europarl
cmu-uka 0.511 0.496 0.395 0.206
liu 0.541 0.55 0.415 0.234
nrc 0.474 0.459 0.354 0.214
saar 0.334 0.404 0.119 0.104
systran 0.562 0.594 0.530 0.302
uedin 0.53 0.554 0.43 0.187
upc 0.534 0.533 0.384 0.214
German-English News Corpus
nrc 0.459 0.429 0.325 0.245
saar 0.278 0.341 0.108 0.125
systran 0.552 0.56 0.563 0.344
uedin 0.508 0.536 0.485 0.332
upc 0.536 0.512 0.476 0.330
English-German Europarl
cmu-uka 0.557 0.508 0.416 0.333
nrc 0.534 0.511 0.328 0.321
saar 0.369 0.383 0.172 0.196
systran 0.543 0.525 0.511 0.295
uedin 0.569 0.576 0.389 0.350
upc 0.565 0.522 0.438 0.3
English-German News Corpus
nrc 0.453 0.4 0.437 0.340
saar 0.186 0.273 0.108 0.121
systran 0.542 0.556 0.582 0.351
ucb 0.415 0.403 0.332 0.289
uedin 0.472 0.445 0.455 0.303
upc 0.505 0.475 0.377 0.349
Table 9: Human evaluation for German-English sub-
missions
150
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
Spanish-English Europarl
cmu-syntax 0.552 0.568 0.478 0.152
cmu-uka 0.557 0.564 0.392 0.139
nrc 0.477 0.489 0.382 0.143
saar 0.328 0.336 0.126 0.075
systran 0.525 0.566 0.453 0.156
uedin 0.593 0.610 0.419 0.14
upc 0.587 0.604 0.5 0.188
upv 0.562 0.573 0.326 0.154
Spanish-English News Corpus
cmu-uka 0.522 0.495 0.41 0.213
nrc 0.479 0.464 0.334 0.243
saar 0.446 0.46 0.246 0.198
systran 0.525 0.503 0.453 0.22
uedin 0.546 0.534 0.48 0.268
upc 0.566 0.543 0.537 0.312
upv 0.435 0.459 0.295 0.151
English-Spanish Europarl
cmu-uka 0.563 0.581 0.391 0.23
nrc 0.546 0.548 0.323 0.22
systran 0.495 0.482 0.329 0.224
uedin 0.586 0.638 0.468 0.225
upc 0.584 0.578 0.444 0.239
upv 0.573 0.587 0.406 0.246
English-Spanish News Corpus
cmu-uka 0.51 0.492 0.45 0.277
nrc 0.408 0.392 0.367 0.224
systran 0.501 0.507 0.481 0.352
ucb 0.449 0.414 0.390 0.307
uedin 0.429 0.419 0.389 0.266
upc 0.51 0.488 0.404 0.311
upv 0.405 0.418 0.250 0.217
Table 10: Human evaluation for Spanish-English
submissions
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
French-English Europarl
limsi 0.634 0.618 0.458 0.290
nrc 0.553 0.551 0.404 0.253
saar 0.384 0.447 0.176 0.157
systran 0.494 0.484 0.286 0.202
systran-nrc 0.604 0.6 0.503 0.267
uedin 0.616 0.635 0.514 0.283
upc 0.616 0.619 0.448 0.267
French-English News Corpus
limsi 0.575 0.596 0.494 0.312
nrc 0.472 0.442 0.306 0.241
saar 0.280 0.372 0.183 0.159
systran 0.553 0.534 0.469 0.288
systran-nrc 0.513 0.49 0.464 0.290
uedin 0.556 0.586 0.493 0.306
upc 0.576 0.587 0.493 0.291
English-French Europarl
limsi 0.635 0.627 0.505 0.259
nrc 0.517 0.518 0.359 0.206
saar 0.398 0.448 0.155 0.139
systran 0.574 0.526 0.353 0.179
systran-nrc 0.575 0.58 0.512 0.225
uedin 0.620 0.608 0.485 0.273
upc 0.599 0.566 0.45 0.256
English-French News Corpus
limsi 0.537 0.495 0.44 0.363
nrc 0.481 0.484 0.372 0.324
saar 0.243 0.276 0.086 0.121
systran 0.536 0.546 0.634 0.440
systran-nrc 0.557 0.572 0.485 0.287
ucb 0.401 0.391 0.316 0.245
uedin 0.466 0.447 0.485 0.375
upc 0.509 0.469 0.437 0.326
Table 11: Human evaluation for French-English
submissions
151
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
Czech-English News Corpus
cu 0.468 0.478 0.362 ?
pct 0.418 0.388 0.220 ?
uedin 0.458 0.471 0.353 ?
umd 0.550 0.592 0.627 ?
English-Czech News Corpus
cu 0.523 0.510 0.405 0.440
pct 0.542 0.541 0.499 0.381
uedin 0.449 0.433 0.249 0.258
Table 12: Human evaluation for Czech-English sub-
missions
152
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
A
L
L
PA
R
A
E
V
A
L
-P
R
E
C
IS
IO
N
D
E
P
E
N
D
E
N
C
Y
-O
V
E
R
L
A
P
S
E
M
A
N
T
IC
-R
O
L
E
-O
V
E
R
L
A
P
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
E
N
C
Y
M
A
X
-C
O
R
R
-A
D
E
Q
U
A
C
Y
German-English Europarl
cmu-uka 0.559 0.247 0.326 0.455 0.528 0.531 0.259 0.182 0.848 1.91 1.910
liu 0.559 0.263 0.329 0.460 0.537 0.535 0.276 0.197 0.846 1.91 1.910
nrc 0.551 0.253 0.324 0.454 0.528 0.532 0.263 0.185 0.848 1.88 1.88
saar 0.477 0.198 0.313 0.447 0.44 0.527 0.228 0.157 0.846 1.76 1.710
systran 0.560 0.268 0.342 0.463 0.543 0.541 0.261 0.21 0.849 1.91 1.91
systran-2 0.501 0.154 0.238 0.376 0.462 0.448 0.237 0.154 ? 1.71 1.73
uedin 0.56 0.277 0.319 0.480 0.536 0.562 0.298 0.217 0.855 1.96 1.940
upc 0.541 0.250 0.343 0.470 0.506 0.551 0.27 0.193 0.846 1.89 1.88
German-English News Corpus
nrc 0.563 0.221 0.333 0.454 0.514 0.514 0.246 0.157 0.868 1.920 1.91
saar 0.454 0.159 0.288 0.413 0.405 0.467 0.193 0.120 0.86 1.700 1.64
systran 0.570 0.200 0.275 0.418 0.531 0.472 0.274 0.18 0.858 1.910 1.93
systran-2 0.556 0.169 0.238 0.397 0.511 0.446 0.258 0.163 ? 1.86 1.88
uedin 0.577 0.242 0.339 0.459 0.534 0.524 0.287 0.181 0.871 1.98 1.970
upc 0.575 0.233 0.339 0.455 0.527 0.516 0.265 0.171 0.865 1.96 1.96
English-German Europarl
cmu-uka 0.268 0.189 0.251 ? ? ? ? ? 0.884 1.66 1.63
nrc 0.272 0.185 0.221 ? ? ? ? ? 0.882 1.660 1.630
saar 0.239 0.174 0.237 ? ? ? ? ? 0.881 1.61 1.56
systran 0.198 0.123 0.178 ? ? ? ? ? 0.866 1.46 1.42
uedin 0.277 0.201 0.273 ? ? ? ? ? 0.889 1.690 1.66
upc 0.266 0.177 0.195 ? ? ? ? ? 0.88 1.640 1.62
English-German News Corpus
nrc 0.257 0.157 0.25 ? ? ? ? ? 0.891 1.590 1.560
saar 0.162 0.098 0.212 ? ? ? ? ? 0.881 1.400 1.310
systran 0.223 0.143 0.266 ? ? ? ? ? 0.887 1.55 1.500
ucb 0.256 0.156 0.249 ? ? ? ? ? 0.889 1.59 1.56
ucb-2 0.252 0.152 0.229 ? ? ? ? ? ? 1.57 1.55
uedin 0.266 0.166 0.266 ? ? ? ? ? 0.891 1.600 1.58
upc 0.256 0.167 0.266 ? ? ? ? ? 0.89 1.590 1.56
Table 13: Automatic evaluation scores for German-English submissions
153
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
PA
R
A
E
V
A
L
-P
R
E
C
D
E
P
E
N
D
E
N
C
Y
S
E
M
A
N
T
IC
-R
O
L
E
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
Spanish-English Europarl
cmu-syntax 0.602 0.323 0.414 0.499 0.59 0.588 0.338 0.254 0.866 2.10 2.090
cmu-syntax-2 0.603 0.321 0.408 0.494 0.593 0.584 0.336 0.249 ? 2.09 2.09
cmu-uka 0.597 0.32 0.42 0.501 0.581 0.595 0.336 0.247 0.867 2.09 2.080
nrc 0.596 0.313 0.402 0.484 0.581 0.581 0.321 0.227 0.867 2.04 2.04
saar 0.542 0.245 0.32 0.432 0.531 0.511 0.272 0.198 0.854 1.870 1.870
systran 0.593 0.290 0.364 0.469 0.586 0.550 0.321 0.238 0.858 2.02 2.03
systran-2 0.535 0.202 0.288 0.406 0.524 0.49 0.263 0.187 ? 1.81 1.84
uedin 0.6 0.324 0.414 0.499 0.584 0.589 0.339 0.252 0.868 2.09 2.080
upc 0.600 0.322 0.407 0.492 0.593 0.583 0.334 0.253 0.865 2.08 2.08
upv 0.594 0.315 0.400 0.493 0.582 0.581 0.329 0.249 0.865 2.060 2.06
Spanish-English News Corpus
cmu-uka 0.64 0.299 0.428 0.497 0.617 0.575 0.339 0.246 0.89 2.17 2.17
cmu-uka-2 0.64 0.297 0.427 0.496 0.616 0.574 0.339 0.246 ? 2.17 2.17
nrc 0.641 0.299 0.434 0.499 0.615 0.584 0.329 0.238 0.892 2.160 2.160
saar 0.607 0.244 0.338 0.447 0.587 0.512 0.303 0.208 0.879 2.04 2.05
systran 0.628 0.259 0.35 0.453 0.611 0.523 0.325 0.221 0.877 2.08 2.10
systran-2 0.61 0.233 0.321 0.438 0.602 0.506 0.311 0.209 ? 2.020 2.050
uedin 0.661 0.327 0.457 0.512 0.634 0.595 0.363 0.264 0.893 2.25 2.24
upc 0.654 0.346 0.480 0.528 0.629 0.616 0.363 0.265 0.895 2.240 2.23
upv 0.638 0.283 0.403 0.485 0.614 0.562 0.334 0.234 0.887 2.15 2.140
English-Spanish Europarl
cmu-uka 0.333 0.311 0.389 ? ? ? ? ? 0.889 1.98 2.00
nrc 0.322 0.299 0.376 ? ? ? ? ? 0.886 1.92 1.940
systran 0.269 0.212 0.301 ? ? ? ? ? 0.878 1.730 1.760
uedin 0.33 0.316 0.399 ? ? ? ? ? 0.891 1.980 1.990
upc 0.327 0.312 0.393 ? ? ? ? ? 0.89 1.960 1.98
upv 0.323 0.304 0.379 ? ? ? ? ? 0.887 1.95 1.97
English-Spanish News Corpus
cmu-uka 0.368 0.327 0.469 ? ? ? ? ? 0.903 2.070 2.090
cmu-uka-2 0.355 0.306 0.461 ? ? ? ? ? ? 2.04 2.060
nrc 0.362 0.311 0.448 ? ? ? ? ? 0.904 2.04 2.060
systran 0.335 0.281 0.439 ? ? ? ? ? 0.906 1.970 2.010
ucb 0.374 0.331 0.464 ? ? ? ? ? ? 2.09 2.11
ucb-2 0.375 0.325 0.456 ? ? ? ? ? ? 2.09 2.110
ucb-3 0.372 0.324 0.457 ? ? ? ? ? ? 2.08 2.10
uedin 0.361 0.322 0.479 ? ? ? ? ? 0.907 2.08 2.09
upc 0.361 0.328 0.467 ? ? ? ? ? 0.902 2.06 2.08
upv 0.337 0.285 0.432 ? ? ? ? ? 0.900 1.98 2.000
Table 14: Automatic evaluation scores for Spanish-English submissions
154
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
PA
R
A
E
V
A
L
-P
R
E
C
D
E
P
E
N
D
E
N
C
Y
S
E
M
A
N
T
IC
-R
O
L
E
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
French-English Europarl
limsi 0.604 0.332 0.418 0.504 0.589 0.591 0.344 0.259 0.865 2.100 2.10
limsi-2 0.602 0.33 0.417 0.504 0.587 0.592 0.302 0.257 ? 2.05 2.05
nrc 0.594 0.312 0.403 0.488 0.578 0.58 0.324 0.244 0.861 2.05 2.050
saar 0.534 0.249 0.354 0.459 0.512 0.546 0.279 0.202 0.856 1.880 1.88
systran 0.549 0.211 0.308 0.417 0.525 0.501 0.277 0.201 0.849 1.850 1.890
systran-nrc 0.594 0.313 0.404 0.492 0.578 0.580 0.330 0.248 0.862 2.06 2.060
uedin 0.595 0.318 0.424 0.505 0.574 0.599 0.338 0.254 0.865 2.08 2.08
upc 0.6 0.319 0.409 0.495 0.588 0.583 0.337 0.255 0.861 2.08 2.080
French-English News Corpus
limsi 0.595 0.279 0.405 0.478 0.563 0.555 0.289 0.235 0.875 2.030 2.020
nrc 0.587 0.257 0.389 0.470 0.557 0.546 0.301 0.22 0.876 2.020 2.020
saar 0.503 0.206 0.301 0.418 0.475 0.476 0.245 0.169 0.864 1.80 1.78
systran 0.568 0.202 0.28 0.415 0.554 0.472 0.292 0.198 0.866 1.930 1.96
systran-nrc 0.591 0.269 0.398 0.475 0.558 0.547 0.323 0.226 0.875 2.050 2.06
uedin 0.602 0.27 0.392 0.471 0.569 0.545 0.326 0.233 0.875 2.07 2.07
upc 0.596 0.275 0.400 0.476 0.567 0.552 0.322 0.233 0.876 2.06 2.06
English-French Europarl
limsi 0.226 0.306 0.366 ? ? ? ? ? 0.891 1.940 1.96
nrc 0.218 0.294 0.354 ? ? ? ? ? 0.888 1.930 1.96
saar 0.190 0.262 0.333 ? ? ? ? ? 0.892 1.86 1.87
systran 0.179 0.233 0.313 ? ? ? ? ? 0.885 1.79 1.83
systran-nrc 0.220 0.301 0.365 ? ? ? ? ? 0.892 1.940 1.960
uedin 0.207 0.262 0.301 ? ? ? ? ? 0.886 1.930 1.950
upc 0.22 0.299 0.379 ? ? ? ? ? 0.892 1.940 1.960
English-French News Corpus
limsi 0.206 0.255 0.354 ? ? ? ? ? 0.897 1.84 1.87
nrc 0.208 0.257 0.369 ? ? ? ? ? 0.9 1.87 1.900
saar 0.151 0.188 0.308 ? ? ? ? ? 0.896 1.65 1.65
systran 0.199 0.243 0.378 ? ? ? ? ? 0.901 1.860 1.90
systran-nrc 0.23 0.290 0.408 ? ? ? ? ? 0.903 1.940 1.98
ucb 0.201 0.237 0.366 ? ? ? ? ? 0.897 1.830 1.860
uedin 0.197 0.234 0.340 ? ? ? ? ? 0.899 1.87 1.890
upc 0.212 0.263 0.391 ? ? ? ? ? 0.900 1.87 1.90
Table 15: Automatic evaluation scores for French-English submissions
155
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
PA
R
A
E
V
A
L
-P
R
E
C
D
E
P
E
N
D
E
N
C
Y
S
E
M
A
N
T
IC
-R
O
L
E
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
Czech-English News Corpus
cu 0.545 0.215 0.334 0.441 0.502 0.504 0.245 0.165 0.867 1.87 1.88
cu-2 0.558 0.223 0.344 0.447 0.510 0.514 0.254 0.17 ? 1.90 1.910
uedin 0.54 0.217 0.340 0.445 0.497 0.51 0.243 0.160 0.865 1.860 1.870
umd 0.581 0.241 0.355 0.460 0.531 0.526 0.273 0.184 0.868 1.96 1.97
English-Czech News Corpus
cu 0.429 0.134 0.231 ? ? ? ? ? ? 1.580 1.53
cu-2 0.430 0.132 0.219 ? ? ? ? ? ? 1.58 1.520
uedin 0.42 0.119 0.211 ? ? ? ? ? ? 1.550 1.49
Table 16: Automatic evaluation scores for Czech-English submissions
156
ADEQUACY
FLUENCY
RANK
CONSTITUENT
METEOR
BLEU
1-TER
GTM
PARAEVAL-REC
PARAEVAL-PREC
DEPENDENCY
SEMANTIC-ROLE
1-WER-OF-VS
MAX-CORR-FLU
MAX-CORR-ADEQ
G
erm
an-E
nglish
N
ew
s
C
orpus
adequacy
1
0.900
0.900
0.900
0.600
0.300
-0.025
0.300
0.700
0.300
0.700
0.700
-0.300
0.300
0.600
fl
uency
?
1
1.000
1.000
0.700
0.400
-0.025
0.400
0.900
0.400
0.900
0.900
-0.100
0.400
0.700
rank
?
?
1
1.000
0.700
0.400
-0.025
0.400
0.900
0.400
0.900
0.900
-0.100
0.400
0.700
constituent
?
?
?
1
0.700
0.400
-0.025
0.400
0.900
0.400
0.900
0.900
-0.100
0.400
0.700
G
erm
an-E
nglish
E
uroparl
adequacy
1
0.893
0.821
0.750
0.599
0.643
0.787
0.68
0.750
0.643
0.464
0.750
0.206
0.608
0.447
fl
uency
?
1
0.964
0.537
0.778
0.858
0.500
0.821
0.821
0.787
0.571
0.93
0.562
0.821
0.661
rank
?
?
1
0.500
0.902
0.821
0.393
0.714
0.858
0.643
0.464
0.858
0.652
0.893
0.769
constituent
?
?
?
1
0.456
0.464
0.714
0.18
0.750
0.250
0.214
0.43
0.117
0.214
0.126
Spanish-E
nglish
N
ew
s
C
orpus
adequacy
1
1.000
0.964
0.893
0.643
0.68
0.68
0.68
0.68
0.68
0.634
0.714
0.571
0.68
0.68
fl
uency
?
1
0.964
0.893
0.643
0.68
0.68
0.68
0.68
0.68
0.634
0.714
0.571
0.68
0.68
rank
?
?
1
0.858
0.714
0.750
0.750
0.750
0.750
0.750
0.741
0.787
0.608
0.750
0.750
constituent
?
?
?
1
0.787
0.821
0.821
0.821
0.714
0.821
0.599
0.750
0.750
0.714
0.714
Spanish-E
nglish
E
uroparl
adequacy
1
0.93
0.452
0.333
0.596
0.810
0.62
0.690
0.542
0.714
0.762
0.739
0.489
0.638
0.638
fl
uency
?
1
0.571
0.524
0.596
0.787
0.43
0.500
0.732
0.524
0.690
0.810
0.346
0.566
0.566
rank
?
?
1
0.643
0.739
0.596
0.43
0.262
0.923
0.406
0.500
0.739
0.168
0.542
0.542
constituent
?
?
?
1
0.262
0.143
-0.143
-0.143
0.816
-0.094
0.000
0.477
-0.226
0.042
0.042
F
rench-E
nglish
N
ew
s
C
orpus
adequacy
1
0.964
0.964
0.858
0.787
0.750
0.68
0.68
0.787
0.571
0.321
0.787
0.456
0.68
0.554
fl
uency
?
1
1.000
0.93
0.750
0.787
0.714
0.714
0.750
0.608
0.214
0.858
0.367
0.608
0.482
rank
?
?
1
0.93
0.750
0.787
0.714
0.714
0.750
0.608
0.214
0.858
0.367
0.608
0.482
constituent
?
?
?
1
0.858
0.858
0.787
0.787
0.858
0.643
0.393
0.964
0.349
0.750
0.661
F
rench-E
nglish
E
uroparl
adequacy
1
0.884
0.778
0.991
0.982
0.956
0.902
0.902
0.812
0.902
0.956
0.956
0.849
0.964
0.991
fl
uency
?
1
0.858
0.893
0.849
0.821
0.93
0.93
0.571
0.93
0.858
0.821
0.787
0.849
0.858
rank
?
?
1
0.821
0.670
0.68
0.858
0.858
0.43
0.858
0.787
0.68
0.893
0.741
0.714
constituent
?
?
?
1
0.956
0.93
0.93
0.93
0.750
0.93
0.964
0.93
0.893
0.956
0.964
Table
17:
C
orrelation
of
the
autom
atic
evaluation
m
etrics
w
ith
the
hum
an
judgm
ents
w
hen
translating
into
E
nglish
157
A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
M
E
T
E
O
R
B
L
E
U
1-
T
E
R
1-
W
E
R
-O
F
-V
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
English-German News Corpus
adequacy 1 0.943 0.83 0.943 0.187 0.43 0.814 0.243 0.33 0.187
fluency ? 1 0.714 0.83 0.100 0.371 0.758 0.100 0.243 0.100
rank ? ? 1 0.771 0.414 0.258 0.671 0.414 0.414 0.414
constituent ? ? ? 1 0.13 0.371 0.671 0.243 0.243 0.13
English-German Europarl
adequacy 1 0.714 0.487 0.714 0.487 0.600 0.314 0.371 0.487 0.487
fluency ? 1 0.543 0.43 0.258 0.200 -0.085 0.03 0.258 0.258
rank ? ? 1 0.03 -0.37 -0.256 -0.543 -0.485 -0.37 -0.37
constituent ? ? ? 1 0.887 0.943 0.658 0.83 0.887 0.887
English-Spanish News Corpus
adequacy 1 0.714 0.771 0.83 0.314 0.658 0.487 0.03 0.314 0.600
fluency ? 1 0.943 0.887 -0.200 0.03 0.143 0.200 -0.085 0.258
rank ? ? 1 0.943 -0.029 0.087 0.258 0.371 -0.029 0.371
constituent ? ? ? 1 -0.143 0.143 0.200 0.314 -0.085 0.258
English-Spanish Europarl
adequacy 1 0.83 0.943 0.543 0.658 0.943 0.943 0.943 0.83 0.658
fluency ? 1 0.771 0.543 0.714 0.771 0.771 0.771 0.83 0.714
rank ? ? 1 0.600 0.600 0.887 0.887 0.887 0.771 0.600
constituent ? ? ? 1 0.43 0.43 0.43 0.43 0.371 0.43
English-French News Corpus
adequacy 1 0.952 0.762 0.452 0.690 0.787 0.690 0.709 0.596 0.686
fluency ? 1 0.810 0.477 0.62 0.739 0.714 0.792 0.62 0.780
rank ? ? 1 0.762 0.239 0.381 0.500 0.757 0.596 0.601
constituent ? ? ? 1 -0.048 0.096 0.143 0.411 0.333 0.304
English-French Europarl
adequacy 1 0.964 0.750 0.93 0.608 0.528 0.287 -0.07 0.652 0.376
fluency ? 1 0.858 0.893 0.643 0.562 0.214 -0.07 0.652 0.376
rank ? ? 1 0.750 0.821 0.76 0.393 0.214 0.830 0.697
constituent ? ? ? 1 0.571 0.473 0.18 -0.07 0.652 0.447
Table 18: Correlation of the automatic evaluation metrics with the human judgments when translating out
of English
158
Proceedings of the Third Workshop on Statistical Machine Translation, pages 70?106,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Further Meta-Evaluation of Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb cs jhu edu
Cameron Fordyce
camfordyce gmail com
Philipp Koehn
University of Edinburgh
pkoehn inf ed ac uk
Christof Monz
Queen Mary, University of London
christof dcs qmul ac uk
Josh Schroeder
University of Edinburgh
j schroeder ed ac uk
Abstract
This paper analyzes the translation qual-
ity of machine translation systems for 10
language pairs translating between Czech,
English, French, German, Hungarian, and
Spanish. We report the translation quality
of over 30 diverse translation systems based
on a large-scale manual evaluation involv-
ing hundreds of hours of effort. We use the
human judgments of the systems to analyze
automatic evaluation metrics for translation
quality, and we report the strength of the cor-
relation with human judgments at both the
system-level and at the sentence-level. We
validate our manual evaluation methodol-
ogy by measuring intra- and inter-annotator
agreement, and collecting timing informa-
tion.
1 Introduction
This paper presents the results the shared tasks of the
2008 ACL Workshop on Statistical Machine Trans-
lation, which builds on two past workshops (Koehn
andMonz, 2006; Callison-Burch et al, 2007). There
were two shared tasks this year: a translation task
which evaluated translation between 10 pairs of Eu-
ropean languages, and an evaluation task which ex-
amines automatic evaluation metrics.
There were a number of differences between this
year?s workshop and last year?s workshop:
? Test set selection ? Instead of creating our test
set by reserving a portion of the training data,
we instead hired translators to translate a set of
newspaper articles from a number of different
sources. This out-of-domain test set contrasts
with the in-domain Europarl test set.
? New language pairs ? We evaluated the qual-
ity of Hungarian-English machine translation.
Hungarian is a challenging language because it
is agglutinative, has many cases and verb con-
jugations, and has freer word order. German-
Spanish was our first language pair that did not
include English, but was not manually evalu-
ated since it attracted minimal participation.
? System combination ? Saarland University
entered a system combination over a number
of rule-based MT systems, and provided their
output, which were also treated as fully fledged
entries in the manual evaluation. Three addi-
tional groups were invited to apply their system
combination algorithms to all systems.
? Refined manual evaluation ? Because last
year?s study indicated that fluency and ade-
quacy judgments were slow and unreliable, we
dropped them from manual evaluation. We re-
placed them with yes/no judgments about the
acceptability of translations of shorter phrases.
? Sentence-level correlation ? In addition to
measuring the correlation of automatic evalu-
ation metrics with human judgments at the sys-
tem level, we also measured how consistent
they were with the human rankings of individ-
ual sentences.
The remainder of this paper is organized as fol-
lows: Section 2 gives an overview of the shared
70
translation task, describing the test sets, the mate-
rials that were provided to participants, and a list of
the groups who participated. Section 3 describes the
manual evaluation of the translations, including in-
formation about the different types of judgments that
were solicited and how much data was collected.
Section 4 presents the results of the manual eval-
uation. Section 5 gives an overview of the shared
evaluation task, describes which automatic metrics
were submitted, and tells how they were evaluated.
Section 6 presents the results of the evaluation task.
Section 7 validates the manual evaluation methodol-
ogy.
2 Overview of the shared translation task
The shared translation task consisted of 10 language
pairs: English to German, German to English, En-
glish to Spanish, Spanish to English, English to
French, French to English, English to Czech, Czech
to English, Hungarian to English, and German to
Spanish. Each language pair had two test sets drawn
from the proceedings of the European parliament, or
from newspaper articles.1
2.1 Test data
The test data for this year?s task differed from previ-
ous years? data. Instead of only reserving a portion
of the training data as the test set, we hired people
to translate news articles that were drawn from a va-
riety of sources during November and December of
2007. We refer to this as the News test set. A total
of 90 articles were selected, 15 each from a variety
of Czech-, English-, French-, German-, Hungarian-
and Spanish-language news sites:2
Hungarian: Napi (3 documents), Index (2),
Origo (5), Ne?pszabadsa?g (2), HVG (2),
Uniospez (1)
Czech: Aktua?lne? (1), iHNed (4), Lidovky (7),
Novinky (3)
French: Liberation (4), Le Figaro (4), Dernieres
Nouvelles (2), Les Echos (3), Canoe (2)
1For Czech news editorials replaced the European parlia-
ment transcripts as the second test set, and for Hungarian the
newspaper articles was the only test set.
2For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
Original source language avg. BLEU
Hungarian 8.8
German 11.0
Czech 15.2
Spanish 17.3
English 17.7
French 18.6
Table 1: Difficulty of the test set parts based on the
original language. For each part, we average BLEU
scores from the Edinburgh systems for 12 language
pairs of the shared task.
Spanish: Cinco Dias (7), ABC.es (3), El Mundo (5)
English: BBC (3), Scotsman (3), Economist (3),
Times (3), New York Times (3)
German: Financial Times Deutschland (3), Su?d-
deutsche Zeitung (3), Welt (3), Frankfurter All-
gemeine Zeitung (3), Spiegel (3)
The translations were created by the members of
EuroMatrix consortium who hired a mix of profes-
sional and non-professional translators. All trans-
lators were fluent or native speakers of both lan-
guages, and all translations were proofread by a na-
tive speaker of the target language. All of the trans-
lations were done directly, and not via an intermedi-
ate language. So for instance, each of the 15 Hun-
garian articles were translated into Czech, English,
French, German and Spanish. The total cost of cre-
ating the 6 test sets consisting of 2,051 sentences
in each language was approximately 17,200 euros
(around 26,500 dollars at current exchange rates, at
slightly more than 10c/word).
Having a test set that is balanced in six differ-
ent source languages and translated across six lan-
guages raises some interesting questions. For in-
stance, is it easier, when the machine translation sys-
tem translates in the same direction as the human
translator? We found no conclusive evidence that
shows this. What is striking, however, that the parts
differ dramatically in difficulty, based on the orig-
inal source language. For instance the Edinburgh
French-English system has a BLEU score of 26.8 on
the part that was originally Spanish, but a score of on
9.7 on the part that was originally Hungarian. For
average scores for each original language, see Ta-
ble 1.
71
In order to remain consistent with previous eval-
uations, we also created a Europarl test set. The
Europarl test data was again drawn from the tran-
scripts of EU parliamentary proceedings from the
fourth quarter of 2000, which is excluded from the
Europarl training data. Our rationale behind invest-
ing a considerable sum to create the News test set
was that we believe that it more accurately repre-
sents the quality of systems? translations than when
we simply hold out a portion of the training data
as the test set, as with the Europarl set. For in-
stance, statistical systems are heavily optimized to
their training data, and do not perform as well on
out-of-domain data (Koehn and Schroeder, 2007).
Having both the News test set and the Europarl test
set alows us to contrast the performance of systems
on in-domain and out-of-domain data, and provides
a fairer comparison between systems trained on the
Europarl corpus and systems that were developed
without it.
2.2 Provided materials
To lower the barrier of entry for newcomers to the
field, we provided a complete baseline MT system,
along with data resources. We provided:
? sentence-aligned training corpora
? language model data
? development and dev-test sets
? Moses open source toolkit for phrase-based sta-
tistical translation (Koehn et al, 2007)
The performance of this baseline system is similar
to the best submissions in last year?s shared task.
The training materials are described in Figure 1.
2.3 Submitted systems
We received submissions from 23 groups from 18
institutions, as listed in Table 2. We also eval-
uated seven additional commercial rule-based MT
systems, bringing the total to 30 systems. This is
a significant increase over last year?s shared task,
where there were submissions from 15 groups from
14 institutions. Of the 15 groups that participated in
last year?s shared task, 11 groups returned this year.
One of the goals of the workshop was to attract sub-
missions from newcomers to the field, and we are
please to have attracted many smaller groups, some
as small as a single graduate student and her adviser.
The 30 submitted systems represent a broad
range of approaches to statistical machine transla-
tion. These include statistical phrase-based and rule-
based (RBMT) systems (which together made up the
bulk of the entries), and also hybrid machine trans-
lation, and statistical tree-based systems. For most
language pairs, we assembled a solid representation
of the state of the art in machine translation.
In addition to individual systems being entered,
this year we also solicited a number of entries which
combined the results of other systems. We invited
researchers at BBN, Carnegie Mellon University,
and the University of Edinburgh to apply their sys-
tem combination algorithms to all of the systems
submitted to shared translation task. We designated
the translations of the Europarl set as the develop-
ment data for combination techniques which weight
each system.3 CMU combined the French-English
systems, BBN combined the French-English and
German-English systems, and Edinburgh submitted
combinations for the French-English and German-
English systems as well as a multi-source system
combination which combined all systems which
translated from any language pair into English for
the News test set. The University of Saarland also
produced a system combination over six commercial
RBMT systems (Eisele et al, 2008). Saarland gra-
ciously provided the output of these systems, which
we manually evaluated alongside all other entries.
For more on the participating systems, please re-
fer to the respective system descriptions in the pro-
ceedings of the workshop.
3 Human evaluation
As with last year?s workshop, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention
that automatic measures are an imperfect substitute
for human assessment of translation quality. There-
fore, rather than select an official automatic eval-
uation metric like the NIST Machine Translation
Workshop does (Przybocki and Peterson, 2008), we
define the manual evaluation to be primary, and use
3Since the performance of systems varied significantly be-
tween the Europarl and News test sets, such weighting might
not be optimal. However this was a level playing field, since
none of the individual systems had development data for the
News set either.
72
Europarl Training Corpus
Spanish? English French? English German? English German? Spanish
Sentences 1,258,778 1,288,074 1,266,520 1,237,537
Words 36,424,186 35,060,653 38,784,144 36,046,219 33,404,503 35,259,758 32,652,649 35,780,165
Distinct words 149,159 96,746 119,437 97,571 301,006 96,802 298,040 148,206
News Commentary Training Corpus
Spanish? English French? English German? English German? Spanish
Sentences 64,308 55,030 72,291 63,312
Words 1,759,972 1,544,633 1,528,159 1,329,940 1,784,456 1,718,561 1,597,152 1,751,215
Distinct words 52,832 38,787 42,385 36,032 84,700 40,553 78,658 52,397
Hunglish Training Corpus CzEng Training Corpus
Hungarian? English
Sentences 1,517,584
Words 26,082,667 31,458,540
Distinct words 717,198 192,901
Czech? English
Sentences 1,096,940
Words 15,336,783 17,909,979
Distinct words 339,683 129,176
Europarl Language Model Data
English Spanish French German
Sentence 1,412,546 1,426,427 1,438,435 1,467,291
Words 34,501,453 36,147,902 35,680,827 32,069,151
Distinct words 100,826 155,579 124,149 314,990
Europarl test set
English Spanish French German
Sentences 2,000
Words 60,185 61,790 64,378 56,624
Distinct words 6,050 7,814 7,361 8,844
News Commentary test set
English Czech
Sentences 2,028
Words 45,520 39,384
Distinct words 7,163 12,570
News Test Set
English Spanish French German Czech Hungarian
Sentences 2,051
Words 43,482 47,155 46,183 41,175 36,359 35,513
Distinct words 7,807 8,973 8,898 10,569 12,732 13,144
Figure 1: Properties of the training and test sets used in the shared task. The training data is drawn from the
Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple
languages. For Czech and Hungarian we use other available parallel corpora. Note that the number of
words is computed based on the provided tokenizer and that the number of distinct words is the based on
lowercased tokens.
73
ID Participant
BBN-COMBO BBN system combination (Rosti et al, 2008)
CMU-COMBO Carnegie Mellon University system combination (Jayaraman and Lavie, 2005)
CMU-GIMPEL Carnegie Mellon University Gimpel (Gimpel and Smith, 2008)
CMU-SMT Carnegie Mellon University SMT (Bach et al, 2008)
CMU-STATXFER Carnegie Mellon University Stat-XFER (Hanneman et al, 2008)
CU-TECTOMT Charles University TectoMT (Zabokrtsky et al, 2008)
CU-BOJAR Charles University Bojar (Bojar and Hajic?, 2008)
CUED Cambridge University (Blackwood et al, 2008)
DCU Dublin City University (Tinsley et al, 2008)
LIMSI LIMSI (De?chelotte et al, 2008)
LIU Linko?ping University (Stymne et al, 2008)
LIUM-SYSTRAN LIUM / Systran (Schwenk et al, 2008)
MLOGIC Morphologic (Nova?k et al, 2008)
PCT a commercial MT provider from the Czech Republic
RBMT1?6 Babelfish, Lingenio, Lucy, OpenLogos, ProMT, SDL (ordering anonymized)
SAAR University of Saarbruecken (Eisele et al, 2008)
SYSTRAN Systran (Dugast et al, 2008)
UCB University of California at Berkeley (Nakov, 2008)
UCL University College London (Wang and Shawe-Taylor, 2008)
UEDIN University of Edinburgh (Koehn et al, 2008)
UEDIN-COMBO University of Edinburgh system combination (Josh Schroeder)
UMD University of Maryland (Dyer, 2007)
UPC Universitat Politecnica de Catalunya, Barcelona (Khalilov et al, 2008)
UW University of Washington (Axelrod et al, 2008)
XEROX Xerox Research Centre Europe (Nikoulina and Dymetman, 2008)
Table 2: Participants in the shared translation task. Not all groups participated in all language pairs.
74
the human judgments to validate automatic metrics.
Manual evaluation is time consuming, and it re-
quires a monumental effort to conduct it on the
scale of our workshop. We distributed the work-
load across a number of people, including shared
task participants, interested volunteers, and a small
number of paid annotators. More than 100 people
participated in the manual evaluation, with 75 peo-
ple putting in more than an hour?s worth of effort,
and 25 putting in more than four hours. A collective
total of 266 hours of labor was invested.
We wanted to ensure that we were using our anno-
tators? time effectively, so we carefully designed the
manual evaluation process. In our analysis of last
year?s manual evaluation we found that the NIST-
style fluency and adequacy scores (LDC, 2005) were
overly time consuming and inconsistent.4 We there-
fore abandoned this method of evaluating the trans-
lations.
We asked people to evaluate the systems? output
in three different ways:
? Ranking translated sentences relative to each
other
? Ranking the translations of syntactic con-
stituents drawn from the source sentence
? Assigning absolute yes or no judgments to the
translations of the syntactic constituents.
The manual evaluation software asked for re-
peated judgments from the same individual, and had
multiple people judge the same item, and logged the
time it took to complete each judgment. This al-
lowed us to measure intra- and inter-annotator agree-
ment, and to analyze the average amount of time it
takes to collect the different kinds of judgments. Our
analysis is presented in Section 7.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rela-
tively intuitive and straightforward task. We there-
fore kept the instructions simple. The instructions
for this task were:
4It took 26 seconds on average to assign fluency and ade-
quacy scores to a single sentence, and the inter-annotator agree-
ment had a Kappa of between .225?.25, meaning that annotators
assigned the same scores to identical sentences less than 40% of
the time.
Rank each whole sentence translation
from Best to Worst relative to the other
choices (ties are allowed).
Ranking several translations at a time is a variant
of force choice judgments where a pair of systems
is presented and an annotator is asked ?Is A better
than B, worse than B, or equal to B.? In our exper-
iments, annotators were shown five translations at a
time, except for the Hungarian and Czech language
pairs where there were fewer than five system sub-
missions. In most cases there were more than 5 sys-
tems submissions. We did not attempt to get a com-
plete ordering over the systems, and instead relied
on random selection and a reasonably large sample
size to make the comparisons fair.
?
people
's
Iraq
to
services
basic
other
and
,
care
health
,
food
provide
cannot
it
if
occupation
its
sustain
US
the
Can
?k
?
n
n
e
n
a
n
b
i
e
t
e
n
D
i
e
n
s
t
l
e
i
s
t
u
n
g
e
n
g
r
u
n
d
l
e
g
e
n
d
e
a
n
d
e
r
e
u
n
d
G
e
s
u
n
d
h
e
i
t
s
f
?
r
s
o
r
g
e
,N
a
h
r
u
n
g
n
i
c
h
t
V
o
l
k
i
r
a
k
i
s
c
h
e
n
d
e
m
s
i
e
w
e
n
n
,U
S
A
d
i
e
K
?
n
n
e
n
a
u
f
r
e
c
h
t
e
r
h
a
l
t
e
n
B
e
s
e
t
z
u
n
g
 
 
i
h
r
e
R
e
f
e
r
e
n
c
e
 
t
r
a
n
s
l
a
t
i
o
n
NP
NP
NP
VP
NP
VP
S
S
CNP
NP
Constituents selected 
for evaluation
Target phrases
highlighted via
word alignments
Parsed source
sentence
Figure 2: In constituent-based evaluation, the source
sentence was parsed, and automatically aligned with
the reference translation and systems? translations
75
Language Pair Test Set Constituent Rank Yes/No Judgments Sentence Ranking
English-German Europarl 2,032 2,034 1,004
News 2,170 2,221 1,115
German-English Europarl 1,705 1,674 819
News 1,938 1,881 1,944
English-Spanish Europarl 1,200 1,247 615
News 1,396 1,398 700
Spanish-English Europarl 1,855 1,921 948
News 2,063 1,939 1,896
English-French Europarl 1,248 1,265 674
News 1,741 1,734 843
French-English Europarl 1,829 1,841 909
News 2,467 2,500 2,671
English-Czech News 2,069 2,070 1,045
Commentary 1,840 1,815 932
Czech-English News 0 0 1,400
Commentary 0 0 1,731
Hungarian-English News 0 0 937
All-English News 0 0 4,868
Totals 25,553 25,540 25,051
Table 3: The number of items that were judged for each task during the manual evaluation. The All-English
judgments were reused in the News task for individual language pairs.
3.2 Ranking translations of syntactic
constituents
We continued the constituent-based evaluation that
we piloted last year, wherein we solicited judgments
about the translations of short phrases within sen-
tences rather than whole sentences. We parsed the
source language sentence, selected syntactic con-
stituents from the tree, and had people judge the
translations of those syntactic phrases. In order to
draw judges? attention to these regions, we high-
lighted the selected source phrases and the corre-
sponding phrases in the translations. The corre-
sponding phrases in the translations were located via
automatic word alignments.
Figure 2 illustrates how the source and reference
phrases are highlighted via automatic word align-
ments. The same is done for sentence and each
of the system translations. The English, French,
German and Spanish test sets were automatically
parsed using high quality parsers for those languages
(Bikel, 2002; Arun and Keller, 2005; Dubey, 2005;
Bick, 2006).
The word alignments were created with Giza++
(Och and Ney, 2003) applied to a parallel corpus
containing the complete Europarl training data, plus
sets of 4,051 sentence pairs created by pairing the
test sentences with the reference translations, and
the test sentences paired with each of the system
translations. The phrases in the translations were
located using standard phrase extraction techniques
(Koehn et al, 2003). Because the word-alignments
were created automatically, and because the phrase
extraction is heuristic, the phrases that were selected
may not exactly correspond to the translations of the
selected source phrase. We noted this in the instruc-
tions to judges:
Rank each constituent translation from
Best to Worst relative to the other choices
(ties are allowed). Grade only the high-
lighted part of each translation.
Please note that segments are selected au-
tomatically, and they should be taken as
an approximate guide. They might in-
clude extra words that are not in the actual
alignment, or miss words on either end.
76
The criteria that we used to select which con-
stituents to evaluate were:
? The constituent could not be the whole source
sentence
? The constituent had to be longer three words,
and be no longer than 15 words
? The constituent had to have a corresponding
phrase with a consistent word alignment in
each of the translations
The final criterion helped reduce the number of
alignment errors, but may have biased the sample
to phrases that are more easily aligned.
3.3 Yes/No judgments for the translations of
syntactic constituents
This year we introduced a variant on the constituent-
based evaluation, where instead of asking judges
to rank the translations of phrases relative to each
other, we asked them to indicate which phrasal trans-
lations were acceptable and which were not.
Decide if the highlighted part of each
translation is acceptable, given the refer-
ence. This should not be a relative judg-
ment against the other system translations.
The instructions also contained the same caveat
about the automatic alignments as above. For each
phrase the judges could click on ?Yes?, ?No?, or
?Not Sure.? The number of times people clicked on
?Not Sure? varied by language pair and task. It was
selected as few as 5% of the time for the English-
Spanish News task to as many as 12.5% for the
Czech-English News task.
3.4 Collecting judgments
We collected judgments using a web-based tool that
presented judges with batches of each type of eval-
uation. We presented them with five screens of sen-
tence rankings, ten screens of constituent rankings,
and ten screen of yes/no judgments. The order of the
types of evaluation were randomized.
In order to measure intra-annotator agreement
10% of the items were repeated and evaluated twice
by each judge. In order to measure inter-annotator
agreement 40% of the items were randomly drawn
from a common pool that was shared across all
annotators so that we would have items that were
judged by multiple annotators.
Judges were allowed to select whichever data set
they wanted, and to evaluate translations into what-
ever languages they were proficient in. Shared task
participants were excluded from judging their own
systems.
In addition to evaluation each language pair indi-
vidually, we also combined all system translations
into English for the News test set, taking advantage
of the fact that our test sets were parallel across all
languages. This allowed us to gather interesting data
about the difficulty of translating from different lan-
guages into English.
Table 3 gives a summary of the number of judg-
ments that we collected for translations of individ-
ual sentences. We evaluated 14 translation tasks
with three different types of judgments for most of
them, for a total of 46 different conditions. In to-
tal we collected over 75,000 judgments. Despite the
large number of conditions we managed to collect
between 1,000?2,000 judgments for the constituent-
based evaluation, and several hundred to several
thousand judgments for the sentence ranking tasks.
4 Translation task results
Tables 4, 5, and 6 summarize the results of the hu-
man evaluation of the quality of the machine trans-
lation systems. Table 4 gives the results for the man-
ual evaluation which ranked the translations of sen-
tences. It shows the average number of times that
systems were judged to be better than or equal to
any other system. Table 5 similarly summarizes
the results for the manual evaluation which ranked
the translations of syntactic constituents. Table 6
shows how many times on average a system?s trans-
lated constituents were judged to be acceptable in
the Yes/No evaluation. The bolded items indicate
the system that performed the best for each task un-
der that particular evaluate metric.
Table 7 summaries the results for the All-English
task that we introduced this year. Appendix C gives
an extremely detailed pairwise comparison between
each of the systems, along with an indication of
whether the differences are statistically significant.
The highest ranking entry for the All-English task
77
was the University of Edinburgh?s system combina-
tion entry. It uses a technique similar to Rosti et
al. (2007) to perform system combination. Like the
other system combination entrants, it was tuned on
the Europarl test set and tested on the News test set,
using systems that submitted entries to both tasks.
The University of Edinburgh?s system combi-
nation went beyond other approaches by combin-
ing output from multiple languages pairs (French-
English, German-English and Spanish-English),
resulting in 37 component systems. Rather
than weighting individual systems, it incorporated
weighted features that indicated which language the
system was originally translating from. This entry
was part of ongoing research in multi-lingual, multi-
source translation. Since there was no official multi-
lingual system combination track, this entry should
be viewed only as a contrastive data point.
We analyzed the All-English judgments to see
which source languages were preferred more often,
thinking that this might be a good indication of how
challenging it is for current MT systems to trans-
late from each of the languages into English. For
this analysis we collapsed all of the entries derived
from one source language into an equivalence class,
and judged them against the others. Therefore, all
French systems were judged against all German sys-
tems, and so on. We found that French systems were
judged to be better than or equal to other systems
69% of the time, Spanish systems 64% of the time,
German systems 47% of the time, Czech systems
39% of the time, and Hungarian systems 29% of the
time.
We performed a similar analysis by collapsing the
RBMT systems into one equivalence class, and the
other systems into another. We evaluated how well
these two classes did on the sentence ranking task
for each language pair and test set, and found that
RBMT was a surprisingly good approach in many
of the conditions. RBMT generally did better on the
News test set and for translations into German, sug-
gesting that SMT?s forte is in test sets where it has
appropriate tuning data and for language pairs with
less reordering than between German and English.
BBN-COMBO
CMU-COMBO
CMU-GIMPEL
CMU-SMT
CMU-STATXFER
CU-BOJAR
CU-TECTOMT
CUED
CUED-CONTR
DCU
LIMSI
LIU
LIUM-SYSTRAN
LIUM-SYS-CONTR
MORPHOLOGIC
PC-TRANSLATOR
RBMT2
RBMT3
RBMT4
RBMT5
RBMT6
SAAR
SAAR-CONTR
SYSTRAN
UCB
UCL
UEDIN
UEDIN-COMBO
UMD
UPC
UW
XEROX
C
zech-E
nglish
C
om
m
entary
.745
.445
.603
.717
C
zech-E
nglish
N
ew
s
.675
.583
.646
E
nglish-C
zech
C
om
m
entary
.714
.488
.663
.486
E
nglish-C
zech
N
ew
s
.634
.494
.715
.502
E
nglish-F
rench
E
uroparl
.791
.775
.416
.608
.263
.436
.744
.786
.444
.766
E
nglish-F
rench
N
ew
s
.667
.655
.602
.780
.734
.657
.511
.573
.545
.317
E
nglish-G
erm
an
E
uroparl
.612
.584
.581
.615
.583
.681
.471
.432
.527
.386
.667
E
nglish-G
erm
an
N
ew
s
.361
.426
.787
.664
.752
.667
.555
.463
.444
E
nglish-S
panish
E
uroparl
.667
.737
.554
.560
.413
.436
.717
.500
.714
.593
.735
E
nglish-S
panish
N
ew
s
.494
.537
.683
.674
.724
.715
.548
.586
.481
.601
F
rench-E
nglish
E
uroparl
.415
.697
.642
.776
.792
.400
.504
.484
.323
.577
.753
.465
.524
.707
F
rench-E
nglish
N
ew
s
.659
.592
.379
.549
.643
.632
.660
.693
.581
.575
.654
.565
.540
.639
.614
.608
G
erm
an-E
nglish
E
uroparl
.364
.485
.614
.627
.596
.610
.543
.537
.677
.416
.679
G
erm
an-E
nglish
N
ew
s
.514
.354
.518
.559
.742
.725
.731
.668
.590
.607
.649
.548
.441
H
ungarian-E
nglish
N
ew
s
.853
.321
S
panish-E
nglish
E
uroparl
.714
.676
.677
.780
.427
.488
.350
.470
.671
.425
.660
.687
S
panish-E
nglish
N
ew
s
.567
.563
.674
.583
.667
.768
.577
.613
.669
.543
.561
.602
Table
4:
S
um
m
ary
results
for
the
sentence
ranking
judgm
ents.
T
he
num
bers
reportthe
percentof
tim
e
thateach
system
w
as
judged
to
be
greater
than
or
equalto
any
other
system
.
B
old
indicates
the
highestscore
for
thattask.
78
C
zech-E
nglish
C
om
m
entary
C
zech-E
nglish
N
ew
s
E
nglish-C
zech
C
om
m
entary
.732
.538
.609
.614
E
nglish-C
zech
N
ew
s
.663
.615
.674
.610
E
nglish-F
rench
E
uroparl
.876
.881
.561
.675
.546
.561
.807
.656
.870
E
nglish-F
rench
N
ew
s
.649
.760
.716
.768
.763
.671
.725
.746
.661
.437
E
nglish-G
erm
an
E
uroparl
.774
.750
.812
.577
.585
.582
.508
.518
.770
.690
.822
E
nglish-G
erm
an
N
ew
s
.649
.570
.720
.682
.748
.602
.563
.610
.556
E
nglish-S
panish
E
uroparl
.825
.855
.561
.592
.458
.573
.849
.592
.818
.775
.790
E
nglish-S
panish
N
ew
s
.721
.694
.694
.754
.570
.644
.696
.653
.625
.595
F
rench-E
nglish
E
uroparl
.626
.907
.854
.906
.917
.523
.648
.697
.517
.783
.865
.713
.741
.894
F
rench-E
nglish
N
ew
s
.506
.745
.787
.801
.765
.780
.652
.655
.726
.615
.640
.735
.773
G
erm
an-E
nglish
E
uroparl
.554
.752
.795
.580
.640
.643
.579
.587
.843
.601
.832
G
erm
an-E
nglish
N
ew
s
.502
.715
.674
.772
.755
.740
.674
.640
.757
.775
.744
H
ungarian-E
nglish
N
ew
s
S
panish-E
nglish
E
uroparl
.847
.846
.868
.854
.455
.561
.469
.567
.893
.646
.865
.870
S
panish-E
nglish
N
ew
s
.715
.760
.818
.739
.644
.608
.699
.700
.760
.706
.758
.763
Table
5:
S
um
m
ary
results
for
the
constituent
ranking
judgm
ents.
T
he
num
bers
report
the
percent
of
tim
e
that
each
system
w
as
judged
to
be
greater
than
or
equalto
any
other
system
.
B
old
indicates
the
highestscore
for
thattask.
BBN-COMBO
CMU-COMBO
CMU-GIMPEL
CMU-SMT
CMU-STATXFER
CU-BOJAR
CU-TECTOMT
CUED
CUED-CONTR
DCU
LIMSI
LIU
LIUM-SYSTRAN
LIUM-SYS-CONTR
MORPHOLOGIC
PC-TRANSLATOR
RBMT2
RBMT3
RBMT4
RBMT5
RBMT6
SAAR
SAAR-CONTR
SYSTRAN
UCB
UCL
UEDIN
UEDIN-COMBO
UMD
UPC
UW
XEROX
C
zech-E
nglish
C
om
m
entary
C
zech-E
nglish
N
ew
s
E
nglish-C
zech
C
om
m
entary
.594
.427
.506
.409
E
nglish-C
zech
N
ew
s
.540
.422
.518
.441
E
nglish-F
rench
E
uroparl
.745
.843
.490
.504
.442
.351
.701
.596
.750
E
nglish-F
rench
N
ew
s
.730
.748
.589
.593
.640
.576
.591
.586
.633
.302
E
nglish-G
erm
an
E
uroparl
.822
.794
.788
.692
.571
.665
.447
.466
.774
.611
.849
E
nglish-G
erm
an
N
ew
s
.559
.494
.689
.689
.750
.553
.598
.544
.518
E
nglish-S
panish
E
uroparl
.804
.872
.582
.598
.635
.600
.806
.714
.888
.903
.785
E
nglish-S
panish
N
ew
s
.459
.532
.638
.759
.599
.623
.639
.568
.493
.366
F
rench-E
nglish
E
uroparl
.612
.833
.876
.886
.891
.535
.620
.712
.540
.717
.860
.811
.734
.910
F
rench-E
nglish
N
ew
s
.554
.736
.788
.805
.789
.696
.628
.640
.762
.663
.638
.701
.718
G
erm
an-E
nglish
E
uroparl
.534
.803
.831
.759
.744
.667
.633
.630
.823
.492
.856
G
erm
an-E
nglish
N
ew
s
.470
.725
.638
.717
.731
.738
.589
.684
.669
.716
.632
H
ungarian-E
nglish
N
ew
s
S
panish-E
nglish
E
uroparl
.882
.857
.853
.902
.648
.562
.590
.550
.869
.730
.879
.857
S
panish-E
nglish
N
ew
s
.635
.638
.694
.675
.610
.651
.594
.635
.697
.635
.622
.707
Table
6:
S
um
m
ary
results
for
the
Yes/N
o
judgm
ents
for
constituenttranslations
judgm
ents.
T
he
num
bers
reportthe
percentof
each
system
?s
transla-
tions
thatw
ere
judged
to
be
acceptable.
B
old
indicates
the
highestscore
for
thattask.
79
UEDIN-COMBOxx .717 SAARfr .584
LIUM-SYSTRAN-Cfr .708 SAAR-Cde .574
RBMT5fr .706 RBMT4de .573
UEDIN-COMBOfr .704 CUEDes .572
LIUM-SYSTRANfr .702 RBMT3de .552
RBMT4es .699 CMU-SMTes .548
LIMSIfr .699 UCBes .547
BBN-COMBOfr .695 LIMSIes .537
SAARes .678 RBMT6de .509
CUED-CONTRASTes .674 RBMT5de .493
CMU-COMBOfr .661 LIMSIde .469
UEDINes .654 LIUde .447
CUEDfr .652 SAARde .445
CUED-CONTRASTfr .638 CMU-STATXFRfr .444
RBMT4fr .637 UMDcz .429
UPCes .633 BBN-COMBOde .407
RBMT3es .628 UEDINde .402
RBMT2de .627 MORPHOLOGIChu .387
SAAR-CONTRASTfr .624 DCUcz .380
UEDINfr .616 UEDIN-COMBOde .327
RBMT6fr .615 UEDINcz .293
RBMT6es .615 CMU-STATXFERde .280
RBMT3fr .612 UEDINhu .188
Table 7: The average number of times that each
system was judged to be better than or equal to all
other systems in the sentence ranking task for the
All-English condition. The subscript indicates the
source language of the system.
5 Shared evaluation task overview
The manual evaluation data provides a rich source
of information beyond simply analyzing the qual-
ity of translations produced by different systems. In
particular, it is especially useful for validating the
automatic metrics which are frequently used by the
machine translation research community. We con-
tinued the shared task which we debuted last year,
by examining how well various automatic metrics
correlate with human judgments.
In addition to examining how well the automatic
evaluation metrics predict human judgments at the
system-level, this year we have also started to mea-
sure their ability to predict sentence-level judg-
ments.
The automatic metrics that were evaluated in this
year?s shared task were the following:
? Bleu (Papineni et al, 2002)?Bleu remains the
de facto standard in machine translation eval-
uation. It calculates n-gram precision and a
brevity penalty, and can make use of multi-
ple reference translations as a way of capturing
some of the allowable variation in translation.
We use a single reference translation in our ex-
periments.
? Meteor (Agarwal and Lavie, 2008)?Meteor
measures precision and recall for unigrams and
applies a fragmentation penalty. It uses flex-
ible word matching based on stemming and
WordNet-synonymy. A number of variants are
investigated here: meteor-baseline and meteor-
ranking are optimized for correlation with ad-
equacy and ranking judgments respectively.
mbleu and mter are Bleu and TER computed
using the flexible matching used in Meteor.
? Gimenez and Marquez (2008) measure over-
lapping grammatical dependency relationships
(DP), semantic roles (SR), and discourse repre-
sentations (DR). The authors further investigate
combining these with other metrics including
TER, Bleu, GTM, Rouge, and Meteor (ULC
and ULCh).
? Popovic and Ney (2007) automatically eval-
uate translation quality by examining se-
quences of parts of speech, rather than
words. They calculate Bleu (posbleu) and
F-measure (pos4gramFmeasure) by matching
part of speech 4grams in a hypothesis transla-
tion against the reference translation.
In addition to the above metrics, which scored
the translations on both the system-level5 and the
sentence-level, there were a number of metrics
which focused on the sentence-level:
? Albrecht and Hwa (2008) use support vector re-
gression to score translations using past WMT
manual assessment data as training examples.
The metric uses features derived from target-
side language models and machine-generated
translations (svm-pseudo-ref) as well as refer-
ence human translations (svm-human-ref).
? Duh (2008) similarly used support vector ma-
chines to predict an ordering over a set of
5We provide the scores assigned to each system by these
metrics in Appendix A.
80
system translations (svm-rank). Features in-
cluded in Duh (2008)?s training were sentence-
level BLEU scores and intra-set ranks com-
puted from the entire set of translations.
? USaar?s evaluation metric (alignment-prob)
uses Giza++ to align outputs of multiple sys-
tems with the corresponding reference transla-
tions, with a bias towards identical one-to-one
alignments through a suitably augmented cor-
pus. The Model4 log probabilities in both di-
rections are added and normalized to a scale
between 0 and 1.
5.1 Measuring system-level correlation
To measure the correlation of the automatic metrics
with the human judgments of translation quality at
the system-level we used Spearman?s rank correla-
tion coefficient ?. We converted the raw scores as-
signed each system into ranks. We assigned a rank-
ing to the systems for each of the three types of man-
ual evaluation based on:
? The percent of time that the sentences it pro-
duced were judged to be better than or equal to
the translations of any other system.
? The percent of time that its constituent transla-
tions were judged to be better than or equal to
the translations of any other system.
? The percent of time that its constituent transla-
tions were judged to be acceptable.
We calculated ? three times for each automatic met-
ric, comparing it to each type of human evaluation.
Since there were no ties ? can be calculated using
the simplified equation:
? = 1 ?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher value for ? is
making predictions that are more similar to the hu-
man judgments than an automatic evaluation metric
with a lower ?.
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
O
V
E
R
A
L
L
meteor-ranking .81 .72 .77 .76
ULCh .68 .79 .82 .76
meteor-baseline .77 .75 .74 .75
posbleu .77 .8 .66 .74
pos4gramFmeasure .75 .62 .82 .73
ULC .66 .67 .84 .72
DR .79 .55 .76 .70
SR .79 .53 .76 .69
DP .57 .79 .65 .67
mbleu .61 .77 .56 .65
mter .47 .72 .68 .62
bleu .61 .59 .44 .54
svm-rank .21 .24 .35 .27
Table 8: Average system-level correlations for the
automatic evaluation metrics on translations into En-
glish
5.2 Measuring consistency at the sentence-level
Measuring sentence-level correlation under our hu-
man evaluation framework was made complicated
by the fact that we abandoned the fluency and ad-
equacy judgments which are intended to be abso-
lute scales. Some previous work has focused on
developing automatic metrics which predict human
ranking at the sentence-level (Kulesza and Shieber,
2004; Albrecht and Hwa, 2007a; Albrecht and Hwa,
2007b). Such work generally used the 5-point flu-
ency and adequacy scales to combine the transla-
tions of all sentences into a single ranked list. This
list could be compared against the scores assigned
by automatic metrics and used to calculate corre-
lation coefficients. We did not gather any absolute
scores and thus cannot compare translations across
different sentences. Given the seemingly unreliable
fluency and adequacy assignments that people make
even for translations of the same sentences, it may
be dubious to assume that their scoring will be reli-
able across sentences.
The data points that we have available consist of a
set of 6,400 human judgments each ranking the out-
put of 5 systems. It?s straightforward to construct a
ranking of each of those 5 systems using the scores
81
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
O
V
E
R
A
L
L
posbleu .57 .78 .80 .72
bleu .54 .79 .6 .64
meteor-ranking .55 .74 .55 .61
meteor-baseline .42 .78 .57 .59
pos4gramFmeasure .37 .49 .79 .55
mter .54 .50 .55 .53
svm-rank .55 .56 .46 .52
mbleu .63 .47 .43 .51
Table 9: Average system-level correlations for the
automatic evaluation metrics on translations into
French, German and Spanish
assigned to their translations of that sentence by the
automatic evaluation metrics. When the automatic
scores have been retrieved, we have 6,400 pairs of
ranked lists containing 5 items. How best to treat
these is an open discussion, and certainly warrants
further thought. It does not seem like a good idea
to calculate ? for each pair of ranked list, because
5 items is an insufficient number to get a reliable
correlation coefficient and its unclear if averaging
over all 6,400 lists would make sense. Furthermore,
many of the human judgments of 5 contained ties,
further complicating matters.
Therefore rather than calculating a correlation co-
efficient at the sentence-level we instead ascertained
how consistent the automatic metrics were with the
human judgments. The way that we calculated con-
sistency was the following: for every pairwise com-
parison of two systems on a single sentence by a per-
son, we counted the automatic metric as being con-
sistent if the relative scores were the same (i.e. the
metric assigned a higher score to the higher ranked
system). We divided this by the total number of pair-
wise comparisons to get a percentage. Because the
systems generally assign real numbers as scores, we
excluded pairs that the human annotators ranked as
ties.
6 Evaluation task results
Tables 8 and 9 report the system-level ? for each au-
tomatic evaluation metric, averaged over all trans-
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
DP .514 .527 .536
DR .500 .511 .530
SR .498 .489 .511
ULC .559 .554 .561
ULCh .562 .542 .542
alignment-prob .517 .538 .535
mbleu .505 .516 .544
meteor-baseline .512 .520 .542
meteor-ranking .512 .517 .539
mter .436 .471 .480
pos4gramFmeasure .495 .517 .52
posbleu .435 .43 .454
svm-human-ref .542 .541 .552
svm-pseudo-ref .538 .538 .543
svm-rank .493 .499 .497
Table 10: The percent of time that each automatic
metric was consistent with human judgments for
translations into English
lations directions into English and out of English6
For the into English direction the Meteor score with
its parameters tuned on adequacy judgments had
the strongest correlation with ranking the transla-
tions of whole sentences. It was tied with the com-
bined method of Gimenez and Marquez (2008) for
the highest correlation over all three types of human
judgments. Bleu was the second to lowest ranked
overall, though this may have been due in part to the
fact that we were using test sets which had only a
single reference translation, since the cost of creat-
ing multiple references was prohibitively expensive
(see Section 2.1).
In the reverse direction, for translations out of En-
glish into the other languages, Bleu does consider-
ably better, placing second overall after the part-of-
speech variant on it proposed by Popovic and Ney
(2007). Yet another variant of Bleu which utilizes
Meteor?s flexible matching has the strongest corre-
lation for sentence-level ranking. Appendix B gives
a break down of the correlations for each of the lan-
6Tables 8 and 9 exclude the Spanish-English News Task,
since it had a negative correlation with most of the automatic
metrics. See Tables 19 and 20.
82
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
mbleu 0.520 0.521 0.52
meteor-baseline 0.514 0.494 0.520
meteor-ranking 0.522 0.501 0.534
mter 0.454 0.441 0.457
pos4gramFmeasure 0.515 0.525 0.512
posbleu 0.436 0.446 0.416
svm-rank 0.514 0.531 0.51
Table 11: The percent of time that each automatic
metric was consistent with human judgments for
translations into other languages
guage pairs and test sets.
Tables 10 and 11 report the consistency of the au-
tomatic evaluation metrics with human judgments
on a sentence-by-sentence basis, rather than on the
system level. For the translations into English the
ULC metric (which itself combines many other met-
rics) had the strongest correlation with human judg-
ments, correctly predicting the human ranking of a
each pair of system translations of a sentence more
than half the time. This is dramatically higher than
the chance baseline, which is not .5, since it must
correctly rank a list of systems rather than a pair. For
the reverse direction meteor-ranking performs very
strongly. The svn-rank which had the lowest over-
all correlation at the system level does the best at
consistently predicting the translations of syntactic
constituents into other languages.
7 Validation and analysis of the manual
evaluation
In addition to scoring the shared task entries, we also
continued on our campaign for improving the pro-
cess of manual evaluation.
7.1 Inter- and Intra-annotator agreement
We measured pairwise agreement among annotators
using the kappa coefficient (K) which is widely used
in computational linguistics for measuring agree-
ment in category judgments (Carletta, 1996). It is
defined as
K =
P (A) ? P (E)
1 ? P (E)
Evaluation type P (A) P (E) K
Sentence ranking .578 .333 .367
Constituent ranking .671 .333 .506
Constituent (w/identicals) .678 .333 .517
Yes/No judgments .821 .5 .642
Yes/No (w/identicals) .825 .5 .649
Table 12: Kappa coefficient values representing the
inter-annotator agreement for the different types of
manual evaluation
Evaluation type P (A) P (E) K
Sentence ranking .691 .333 .537
Constituent ranking .825 .333 .737
Constituent (w/identicals) .832 .333 .748
Yes/No judgments .928 .5 .855
Yes/No (w/identicals) .930 .5 .861
Table 13: Kappa coefficient values for intra-
annotator agreement for the different types of man-
ual evaluation
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. We define chance
agreement for ranking tasks as 13 since there are
three possible outcomes when ranking the output of
a pair of systems: A > B, A = B, A < B, and for
the Yes/No judgments as 12 since we ignored those
items marked ?Not Sure?.
For inter-annotator agreement we calculated
P (A) for the yes/no judgments by examining all
items that were annotated by two or more annota-
tors, and calculating the proportion of time they as-
signed identical scores to the same items. For the
ranking tasks we calculated P (A) by examining all
pairs of systems which had been judged by two or
more judges, and calculated the proportion of time
that they agreed that A > B, A = B, or A < B.
For intra-annotator agreement we did similarly, but
gathered items that were annotated on multiple oc-
casions by a single annotator.
Table 12 givesK values for inter-annotator agree-
ment, and Table 13 gives K values for intra-
annotator agreement. These give an indication of
how often different judges agree, and how often sin-
gle judges are consistent for repeated judgments, re-
83
spectively. The interpretation of Kappa varies, but
according to Landis and Koch (1977), 0?.2 is slight,
.2? .4 is fair, .4? .6 is moderate, .6? .8 is substan-
tial and the rest almost perfect. The inter-annotator
agreement for the sentence ranking task was fair, for
the constituent ranking it was moderate and for the
yes/no judgments it was substantial.7 For the intra-
annotator agreement K indicated that people had
moderate consistency with their previous judgments
on the sentence ranking task, substantial consistency
with their previous constituent ranking judgments,
and nearly perfect consistency with their previous
yes/no judgments.
These K values indicate that people are able to
more reliably make simple yes/no judgments about
the translations of short phrases than they are to
rank phrases or whole sentences. While this is an
interesting observation, we do not recommend do-
ing away with the sentence ranking judgments. The
higher agreement on the constituent-based evalua-
tion may be influenced based on the selection cri-
teria for which phrases were selected for evalua-
tion (see Section 3.2). Additionally, the judgments
of the short phrases are not a great substitute for
sentence-level rankings, at least in the way we col-
lected them. The average correlation coefficient be-
tween the constituent-based judgments with the sen-
tence ranking judgments is only ? = 0.51. Tables
19 and 20 give a detailed break down of the cor-
relation of the different types of human judgments
with each other on each translation task. It may
be possible to select phrases in such a way that the
constituent-based evaluations are a better substitute
for the sentence-based ranking, for instance by se-
lecting more of constituents from each sentence, or
attempting to cover most of the words in each sen-
tence in a phrase-by-phrase manner. This warrants
further investigation. It might also be worthwhile to
refine the instructions given to annotators about how
to rank the translations of sentences to try to improve
their agreement, which is currently lower than we
would like it to be (although it is substantially bet-
ter than the previous fluency and adequacy scores,
7Note that for the constituent-based evaluations we verified
that the high K was not trivially due to identical phrasal trans-
lations. We excluded screens where all five phrasal translations
presented to the annotator were identical, and report both num-
bers.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  10  20  30  40  50  60
p
e
r
c
e
n
t
 
o
f
 
s
e
n
t
e
n
c
e
s
 
t
a
k
i
n
g
 
t
h
i
s
 
l
o
n
g
time to judge one sentence (seconds)
yes/no judgmentsconstituent ranksentence rank
Figure 3: Distributions of the amount of time it took
to judge single sentences for the three types of man-
ual evaluation
which had a K < .25 in last year?s evaluation).
7.2 Timing
We used the web interface to collect timing infor-
mation. The server recorded the time when a set of
sentences was given to a judge and the time when
the judge returned the sentences. It took annotators
an average of 18 seconds per sentence to rank a list
of sentences.8 It took an average of 10 seconds per
sentence for them to rank constituents, and an av-
erage of 8.5 seconds per sentence for them to make
yes/no judgments. Figure 3 shows the distribution
of times for these tasks.
These timing figures indicate that the tasks which
the annotators were the most reliable on (yes/no
judgments and constituent ranking) were also much
quicker to complete than the ones they were less re-
liable on (ranking sentences). Given that they are
faster at judging short phrases, they can do propor-
tionally more of them. For instance, we could collect
211 yes/no judgments in the same amount of time
that it would take us to collect 100 sentence ranking
judgments. However, this is partially offset by the
fact that many of the translations of shorter phrases
are identical, which means that we have to collect
more judgments in order to distinguish between two
systems.
8Sets which took longer than 5 minutes were excluded from
these calculations, because there was a strong chance that anno-
tators were interrupted while completing the task.
84
7.3 The potential for re-usability of human
judgments
One strong advantage of the yes/no judgments over
the ranking judgments is their potential for reuse.
We have invested hundreds of hours worth of effort
evaluating the output of the translation systems sub-
mitted to this year?s workshop and last year?s work-
shop. While the judgments that we collected pro-
vide a wealth of information for developing auto-
matic evaluation metrics, we cannot not re-use them
to evaluate our translation systems after we update
their parameters or change their behavior in anyway.
The reason for this is that altered systems will pro-
duce different translations than the ones that we have
judged, so our relative rankings of sentences will no
longer be applicable. However, the translations of
short phrases are more likely to be repeated than the
translations of whole sentences.
Therefore if we collect a large number of yes/no
judgments for short phrases, we could build up a
database that contains information about what frag-
mentary translations are acceptable for each sen-
tence in our test corpus. When we change our sys-
tem and want to evaluate it, we do not need to man-
ually evaluate those segments that match against the
database, and could instead have people evaluate
only those phrasal translations which are new. Ac-
cumulating these judgments over time would give
a very reliable idea of what alternative translations
were allowable. This would be useful because it
could alleviate the problems associated with Bleu
failing to recognize allowable variation in translation
when multiple reference translations are not avail-
able (Callison-Burch et al, 2006). A large database
of human judgments might also be useful as an
objective function for minimum error rate training
(Och, 2003) or in other system development tasks.
8 Conclusions
Similar to previous editions of this workshop we car-
ried out an extensive manual and automatic evalua-
tion of machine translation performance for trans-
lating from European languages into English, and
vice versa. One important aspect in which this year?s
shared task differed from previous years was the in-
troduction of an additional newswire test set that
was different in nature to the training data. We
also added new language pairs to our evaluation:
Hungarian-English and German-Spanish.
As in previous years we were pleased to notice an
increase in the number of participants. This year we
received submissions from 23 groups from 18 insti-
tutions. In addition, we evaluated seven commercial
rule-based MT systems.
The goal of this shared-task is two-fold: First we
want to compare state-of-the-art machine translation
systems, and secondly we aim to measure to what
extent different evaluation metrics can be used to as-
sess MT quality.
With respect to MT quality we noticed that the in-
troduction of test sets from a different domain did
have an impact on the ranking of systems. We ob-
served that rule-based systems generally did better
on the News test set. Overall, it cannot be con-
cluded that one approach clearly outperforms other
approaches, as systems performed differently on the
various translation tasks. One general observation is
that for the tasks where statistical combination ap-
proaches participated, they tended to score relatively
high, in particular with respect to Bleu.
With respect to measuring the correlation between
automated evaluation metrics and human judgments
we found that using Meteor and ULCh (which uti-
lizes a variety of metrics, including Meteor) resulted
in the highest Spearman correlation scores on aver-
age, when translating into English. When translat-
ing from English into French, German, and Spanish,
Bleu and posbleu resulted in the highest correlations
with human judgments.
Finally, we investigated inter- and intra-annotator
agreement of human judgments using Kappa coef-
ficients. We noticed that ranking whole sentences
results in relatively low Kappa coefficients, mean-
ing that there is only fair agreement between the as-
sessors. Constituent ranking and acceptability judg-
ments on the other hand showmoderate and substan-
tial inter-annotator agreement, respectively. Intra-
annotator agreement was substantial to almost per-
fect, except for the sentence ranking assessment
where agreement was only moderate. Although it
is difficult to draw exact conclusions from this, one
might wonder whether the sentence ranking task is
simply too complex, involving too many aspects ac-
cording to which translations can be ranked.
The huge wealth of the data generated by this
85
workshop, including the human judgments, system
translations and automatic scores, is available at
http://www.statmt.org/wmt08/ for other
researchers to analyze.
Acknowledgments
This work was supported in parts by the EuroMatrix
project funded by the European Commission (6th
Framework Programme), the GALE program of the
US Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022, and the US Na-
tional Science Foundation under grant IIS-0713448.
We are grateful to Abhaya Agarwal, John Hen-
derson, Rebecca Hwa, Alon Lavie, Mark Przybocki,
Stuart Shieber, and David Smith for discussing dif-
ferent possibilities for calculating the sentence-level
correlation of automatic evaluation metrics with hu-
man judgments in absence of absolute scores. Any
errors in design remain the responsibility of the au-
thors.
Thank you to Eckhard Bick for parsing the Span-
ish test set. See http://beta.visl.sdu.dk for
more information about the constraint-based parser.
Thanks to Greg Hanneman and Antti-Veikko Rosti
for applying their system combination algorithms to
our data.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, June. Association for Computational
Linguistics.
Joshua Albrecht and Rebecca Hwa. 2007a. A
re-examination of machine learning approaches for
sentence-level mt evaluation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2007), Prague, Czech Repub-
lic.
Joshua Albrecht and Rebecca Hwa. 2007b. Regres-
sion for sentence-level mt evaluation with pseudo ref-
erences. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
2007), Prague, Czech Republic.
Joshua Albrecht and Rebecca Hwa. 2008. The role of
pseudo references in MT evaluation. In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, pages 187?190, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of ACL.
Amittai Axelrod, Mei Yang, Kevin Duh, and Katrin
Kirchhoff. 2008. The University of Washington ma-
chine translation system for ACL WMT 2008. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 123?126, Columbus, Ohio, June.
Association for Computational Linguistics.
Nguyen Bach, Qin Gao, and Stephan Vogel. 2008. Im-
proving word alignment with language model based
confidence scores. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 151?
154, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Eckhard Bick. 2006. A constraint grammar-based parser
for Spanish. In Proceedings of the 4th Workshop on
Information and Human Language Technology (TIL-
2006), Ribeiro Preto, Brazil.
Dan Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceed-
ings of Second International Conference on Human
Language Technology Research (HLT-02), San Diego,
California.
Graeme Blackwood, Adria` de Gispert, Jamie Brunning,
and William Byrne. 2008. European language transla-
tion with weighted finite state transducers: The CUED
MT system for the 2008 ACL workshop on SMT. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 131?134, Columbus, Ohio,
June. Association for Computational Linguistics.
Ondr?ej Bojar and Jan Hajic?. 2008. Phrase-based and
deep syntactic English-to-Czech statistical machine
translation. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 143?146,
Columbus, Ohio, June. Association for Computational
Linguistics.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006), Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
86
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
He?le`ne Bonneau-Maynard, Olivier Galibert, Jean-Luc
Gauvain, Philippe Langlais, and Franc?ois Yvon. 2008.
Limsi?s statistical translation systems for WMT?08. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 107?110, Columbus, Ohio,
June. Association for Computational Linguistics.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In Proceedings of ACL.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2008.
Can we relearn an RBMT system? In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, pages 175?178, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Kevin Duh. 2008. Ranking vs. regression in ma-
chine translation evaluation. In Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 191?194, Columbus, Ohio, June. Association
for Computational Linguistics.
Christopher J. Dyer. 2007. The ?noisier channel?: trans-
lation from morphologically complex languages. In
Proceedings of the ACL-2007 Workshop on Statistcal
Machine Translation (WMT-07), Prague, Czech Re-
public.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using moses to integrate multiple
rule-based machine translation engines into a hybrid
system. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 179?182, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Jesus Gimenez and Lluis Marquez. 2008. A smorgas-
bord of features for automatic MT evaluation. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 195?198, Columbus, Ohio, June.
Association for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 9?17, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Greg Hanneman, Edmund Huber, Abhaya Agarwal,
Vamshi Ambati, Alok Parlikar, Erik Peterson, and
Alon Lavie. 2008. Statistical transfer systems for
French-English and German-English machine transla-
tion. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 163?166, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proceedings of the 10th Annual Confer-
ence of the European Association for Machine Trans-
lation, pages 143?152, Budapest, Hungary, May.
Maxim Khalilov, Adolfo Herna?ndez H., Marta R. Costa-
jussa`, Josep M. Crego, Carlos A. Henr??quez Q., Pa-
trik Lambert, Jose? A. R. Fonollosa, Jose? B. Marin?o,
and Rafael E. Banchs. 2008. The TALP-UPC Ngram-
based statistical machine translation system for ACL-
WMT 2008. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 127?130,
Columbus, Ohio, June. Association for Computational
Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation, New
York, New York.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the ACL-2007 Workshop on Statist-
cal Machine Translation (WMT-07), Prague, Czech
Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
of the North American chapter of the Association for
Computational Linguistics (HLT/NAACL-2003), Ed-
monton, Alberta.
Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris
Callison-Burch, Alexandra Constantin, Brooke
Cowan, Chris Dyer, Marcello Federico, Evan Herbst,
Hieu Hoang, Christine Moran, Wade Shen, and
Richard Zens. 2007. Open source toolkit for statisti-
cal machine translation: Factored translation models
and confusion network decoding. CLSP Summer
Workshop Final Report WS-2006, Johns Hopkins
University.
Philipp Koehn, Abhishek Arun, and Hieu Hoang. 2008.
Towards better machine translation quality for the
German-English language pairs. In Proceedings of
the Third Workshop on Statistical Machine Transla-
tion, pages 139?142, Columbus, Ohio, June. Associ-
ation for Computational Linguistics.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
87
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation, Baltimore, MD, October 4?6.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159?174.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: Experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 147?150,
Columbus, Ohio, June. Association for Computational
Linguistics.
Vassilina Nikoulina and Marc Dymetman. 2008. Using
syntactic coupling features for discriminating phrase-
based translations (wmt-08 shared translation task). In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 159?162, Columbus, Ohio,
June. Association for Computational Linguistics.
Attila Nova?k, La?szlo? Tihanyi, and Ga?bor Pro?sze?ky. 2008.
The MetaMorpho translation system. In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, pages 111?114, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: Amethod for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL-2002), Philadelphia, Pennsylvania.
Maja Popovic and Hermann Ney. 2007. Word error rates:
Decomposition over POS classes and applications for
error analysis. In Proceedings of ACL Workshop on
Machine Translation, Prague, Czech Republic.
Mark Przybocki and Kay Peterson, editors. 2008. Pro-
ceedings of the 2008 NIST Open Machine Translation
Evaluation Workshop. Arlington, Virginia, March.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system combi-
nation for machine translation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2007), Prague, Czech Repub-
lic.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186, Columbus, Ohio,
June. Association for Computational Linguistics.
Holger Schwenk, Jean-Baptiste Fouet, and Jean Senel-
lart. 2008. First steps towards a general purpose
French/English statistical machine translation system.
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 119?122, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of morphological analysis in transla-
tion between German and English. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 135?138, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
John Tinsley, Yanjun Ma, Sylwia Ozdowska, and Andy
Way. 2008. MaTrEx: The DCU MT system for WMT
2008. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 171?174, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel
regression framework for machine translation: UCL
system description for WMT 2008 shared translation
task. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 155?158, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zdenek Zabokrtsky, Jan Ptacek, and Petr Pajas. 2008.
TectoMT: Highly modular MT system with tectogram-
matics used as transfer layer. In Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 167?170, Columbus, Ohio, June. Association
for Computational Linguistics.
88
A Automatic scores for each system
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
S
V
M
-R
A
N
K
English-Czech News Commentary Task
CU-BOJAR 0.15 0.21 0.43 0.35 0.28 4.57
CU-BOJAR-CONTRAST-1 0.04 0.11 0.32 0.25 0.18 0.90
CU-BOJAR-CONTRAST-2 0.14 0.2 0.42 0.34 0.27 2.86
CU-TECTOMT 0.09 0.15 0.37 0.29 0.23 2.13
PC-TRANSLATOR 0.08 0.14 0.35 0.28 0.19 2.09
UEDIN 0.12 0.18 0.4 0.32 0.25 2.28
English-Czech News Task
CU-BOJAR 0.11 0.18 0.37 0.3 0.18 4.72
CU-BOJAR-CONTRAST-1 0.02 0.10 0.26 0.2 0.12 0.80
CU-BOJAR-CONTRAST-2 0.09 0.16 0.35 0.28 0.15 2.65
CU-TECTOMT 0.06 0.13 0.32 0.25 0.16 2.14
PC-TRANSLATOR 0.08 0.14 0.33 0.26 0.14 2.40
UEDIN 0.08 0.15 0.34 0.27 0.15 2.13
Table 14: Automatic evaluation metric for translations into Czech
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
M
E
T
E
O
R
-R
M
T
E
R
P
O
S
F
4G
-A
M
P
O
S
F
4G
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
English-French News Task
LIMSI 0.2 0.26 0.16 0.34 0.33 0.48 0.44 0.43 9.74
LIUM-SYSTRAN 0.20 0.26 0.16 0.35 0.34 0.49 0.44 0.44 7.38
LIUM-SYSTRAN-CONTRAST 0.20 0.26 0.16 0.35 0.34 0.48 0.44 0.44 7.02
RBMT1 0.13 0.19 0.12 0.28 0.24 0.42 0.37 0.35 5.46
RBMT3 0.17 0.23 0.14 0.31 0.31 0.45 0.4 0.40 5.60
RBMT4 0.19 0.24 0.15 0.33 0.32 0.48 0.43 0.43 6.80
RBMT5 0.17 0.23 0.14 0.32 0.31 0.47 0.42 0.42 6.15
RBMT6 0.16 0.22 0.13 0.32 0.3 0.46 0.40 0.41 5.60
SAAR 0.15 0.22 0.15 0.33 0.28 0.46 0.41 0.42 6.12
SAAR-CONTRAST 0.17 0.23 0.15 0.33 0.30 0.47 0.42 0.41 5.50
UEDIN 0.16 0.23 0.14 0.32 0.32 0.44 0.39 0.38 4.79
XEROX 0.13 0.2 0.12 0.29 0.29 0.41 0.34 0.34 3.91
XEROX-CONTRAST 0.13 0.2 0.12 0.29 0.29 0.41 0.35 0.35 3.86
English-French Europarl Task
LIMSI 0.32 0.36 0.24 0.42 0.44 0.56 0.53 0.53 8.84
LIUM-SYSTRAN 0.32 0.36 0.24 0.42 0.45 0.56 0.53 0.53 7.46
LIUM-SYSTRAN-CONTRAST 0.31 0.36 0.23 0.42 0.44 0.56 0.52 0.53 6.69
RBMT1 0.15 0.20 0.13 0.29 0.26 0.44 0.4 0.37 3.89
RBMT3 0.18 0.24 0.15 0.34 0.33 0.47 0.42 0.43 4.13
RBMT4 0.2 0.25 0.17 0.35 0.35 0.5 0.45 0.45 4.70
RBMT5 0.12 0.16 0.09 0.22 0.06 0.37 0.32 0.32 3.01
RBMT6 0.17 0.23 0.14 0.33 0.32 0.47 0.42 0.42 3.93
SAAR 0.26 0.29 0.21 0.41 0.34 0.53 0.49 0.48 7.75
SAAR-CONTRAST 0.28 0.32 0.23 0.41 0.39 0.55 0.51 0.52 6.45
UCL 0.24 0.28 0.19 0.37 0.41 0.49 0.44 0.42 4.16
UEDIN 0.30 0.35 0.23 0.42 0.43 0.54 0.51 0.51 6.56
Table 15: Automatic evaluation metric for translations into French
89
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
English-German News Task
LIMSI 0.11 0.18 0.19 0.45 0.22 0.36 0.29 0.28 7.83
LIU 0.10 0.17 0.18 0.44 0.24 0.36 0.28 0.27 4.03
RBMT1 0.12 0.18 0.18 0.44 0.22 0.39 0.33 0.32 5.42
RBMT2 0.13 0.19 0.20 0.46 0.24 0.4 0.33 0.33 5.76
RBMT3 0.12 0.18 0.19 0.44 0.24 0.39 0.32 0.32 4.70
RBMT4 0.14 0.19 0.2 0.46 0.25 0.41 0.35 0.34 5.58
RBMT5 0.11 0.17 0.17 0.43 0.21 0.38 0.31 0.31 4.49
RBMT6 0.10 0.16 0.17 0.43 0.2 0.37 0.3 0.29 4.81
SAAR 0.13 0.19 0.19 0.44 0.27 0.38 0.31 0.3 4.04
SAAR-CONTRAST 0.12 0.18 0.18 0.43 0.26 0.37 0.3 0.28 3.71
UEDIN 0.12 0.17 0.18 0.45 0.23 0.37 0.30 0.29 4.37
English-German Europarl Task
CMU-GIMPEL 0.20 0.24 0.27 0.54 0.32 0.43 0.37 0.37 9.54
LIMSI 0.20 0.24 0.27 0.53 0.32 0.43 0.37 0.37 6.97
LIU 0.2 0.24 0.27 0.53 0.32 0.43 0.38 0.37 6.95
RBMT1 0.11 0.16 0.16 0.42 0.19 0.38 0.32 0.32 5.01
RBMT2 0.12 0.17 0.19 0.46 0.21 0.39 0.32 0.31 5.93
RBMT3 0.11 0.16 0.17 0.43 0.21 0.38 0.31 0.30 4.75
RBMT4 0.12 0.17 0.18 0.45 0.22 0.41 0.34 0.33 5.42
RBMT5 0.1 0.14 0.16 0.42 0.19 0.39 0.32 0.31 4.42
RBMT6 0.09 0.14 0.15 0.42 0.18 0.38 0.30 0.29 4.40
SAAR 0.20 0.25 0.26 0.53 0.32 0.43 0.38 0.37 6.67
SAAR-CONTRAST 0.2 0.24 0.26 0.52 0.31 0.43 0.37 0.37 6.35
UCL 0.16 0.20 0.23 0.49 0.31 0.4 0.33 0.31 5.12
UEDIN 0.21 0.25 0.27 0.54 0.32 0.44 0.38 0.38 7.02
English-Spanish News Task
CMU-SMT 0.19 0.24 0.25 0.34 0.32 0.32 0.25 0.26 8.34
LIMSI 0.19 0.25 0.26 0.34 0.34 0.33 0.26 0.26 5.92
RBMT1 0.16 0.22 0.23 0.32 0.30 0.31 0.23 0.23 5.36
RBMT3 0.19 0.24 0.25 0.33 0.34 0.33 0.26 0.26 5.42
RBMT4 0.21 0.26 0.26 0.34 0.35 0.34 0.28 0.28 6.36
RBMT5 0.18 0.24 0.25 0.33 0.32 0.33 0.26 0.26 5.84
RBMT6 0.19 0.24 0.24 0.33 0.33 0.32 0.25 0.26 5.42
SAAR 0.20 0.27 0.26 0.34 0.37 0.34 0.28 0.28 5.04
SAAR-CONTRAST 0.2 0.26 0.25 0.34 0.37 0.34 0.27 0.27 4.86
UCB 0.20 0.26 0.26 0.34 0.34 0.33 0.26 0.27 5.70
UEDIN 0.18 0.25 0.25 0.33 0.35 0.33 0.26 0.26 4.30
UPC 0.18 0.23 0.24 0.32 0.35 0.32 0.25 0.24 3.97
English-Spanish Europarl Task
CMU-SMT 0.32 0.36 0.33 0.42 0.45 0.40 0.35 0.36 0.10
LIMSI 0.31 0.36 0.33 0.42 0.45 0.4 0.35 0.35 7.80
RBMT1 0.16 0.22 0.24 0.32 0.31 0.32 0.25 0.25 4.47
RBMT3 0.20 0.25 0.25 0.34 0.35 0.33 0.27 0.27 4.66
RBMT4 0.21 0.25 0.26 0.34 0.36 0.34 0.28 0.28 4.85
RBMT5 0.18 0.24 0.25 0.34 0.33 0.34 0.27 0.27 5.03
RBMT6 0.18 0.23 0.25 0.33 0.33 0.33 0.26 0.26 4.57
SAAR 0.31 0.35 0.33 0.41 0.44 0.40 0.35 0.35 7.59
SAAR-CONTRAST 0.30 0.34 0.33 0.41 0.44 0.4 0.34 0.35 7.42
UCL 0.25 0.29 0.29 0.37 0.43 0.36 0.29 0.29 4.67
UEDIN 0.32 0.36 0.33 0.42 0.45 0.40 0.35 0.35 7.25
UPC 0.30 0.34 0.32 0.40 0.46 0.4 0.35 0.34 6.18
UW 0.32 0.36 0.33 0.42 0.45 0.40 0.35 0.35 7.36
UW-CONTRAST 0.32 0.35 0.33 0.42 0.45 0.40 0.35 0.36 7.21
Table 16: Automatic evaluation metric for translations into German and Spanish
90
D
P
D
R
S
R
U
L
C
U
L
C
H
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
Spanish-English Europarl Task
CMU-SMT 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 9.72
CUED 0.33 0.43 0.25 0.29 0.33 0.32 0.38 0.59 0.48 0.50 0.51 0.47 0.47 7.41
CUED-CONTRAST 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 7.00
DCU 0.34 0.43 0.25 0.29 0.33 0.32 0.38 0.59 0.48 0.50 0.51 0.47 0.48 6.78
LIMSI 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 6.73
RBMT3 0.26 0.37 0.19 0.22 0.27 0.19 0.26 0.51 0.41 0.36 0.45 0.4 0.39 5.46
RBMT4 0.26 0.37 0.19 0.22 0.27 0.18 0.26 0.52 0.42 0.36 0.45 0.39 0.38 5.57
RBMT5 0.25 0.36 0.18 0.22 0.27 0.18 0.25 0.51 0.41 0.36 0.44 0.39 0.38 4.74
RBMT6 0.24 0.34 0.18 0.21 0.26 0.17 0.25 0.51 0.41 0.36 0.44 0.38 0.37 4.71
SAAR 0.34 0.44 0.26 0.29 0.33 0.32 0.39 0.59 0.48 0.51 0.52 0.49 0.48 6.30
SAAR-CONTRAST 0.33 0.43 0.25 0.28 0.33 0.30 0.37 0.59 0.48 0.47 0.51 0.47 0.46 7.33
UCL 0.29 0.4 0.21 0.25 0.29 0.25 0.32 0.55 0.43 0.47 0.47 0.42 0.4 4.02
UEDIN 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.50 0.52 0.48 0.48 6.61
UPC 0.33 0.43 0.25 0.28 0.33 0.32 0.38 0.59 0.48 0.5 0.52 0.48 0.48 6.82
French-English News Task
BBN-COMBO 0.27 0.37 0.2 0.23 0.28 0.21 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-COMBO 0.26 0.36 0.18 0.22 0.27 0.19 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-COMBO-CONTRAST n/a n/a n/a n/a n/a 0.19 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-STATXFER 0.21 0.32 0.14 0.19 0.23 0.14 0.22 0.48 0.39 0.28 0.38 0.32 0.30 9.91
CMU-STATXFER-CONTRAST 0.21 0.30 0.14 0.18 0.23 0.14 0.21 0.47 0.38 0.26 0.38 0.31 0.29 6.47
CUED 0.25 0.35 0.17 0.21 0.26 0.18 0.27 0.51 0.41 0.37 0.41 0.35 0.34 6.34
CUED-CONTRAST 0.26 0.37 0.18 0.22 0.27 0.19 0.28 0.52 0.42 0.38 0.42 0.37 0.36 6.29
LIMSI 0.26 0.37 0.18 0.22 0.27 0.20 0.28 0.51 0.40 0.40 0.43 0.38 0.37 5.75
LIUM-SYSTRAN 0.27 0.38 0.19 0.23 0.27 0.21 0.29 0.51 0.41 0.41 0.44 0.39 0.38 6.32
LIUM-SYSTRAN-CONTRAST 0.27 0.38 0.19 0.23 0.28 0.21 0.29 0.51 0.41 0.41 0.44 0.39 0.38 5.93
RBMT3 0.24 0.36 0.17 0.21 0.26 0.16 0.24 0.49 0.40 0.29 0.42 0.36 0.34 7.61
RBMT4 0.25 0.37 0.17 0.21 0.26 0.17 0.25 0.49 0.4 0.33 0.42 0.36 0.35 6.17
RBMT5 0.25 0.37 0.18 0.22 0.27 0.18 0.25 0.51 0.41 0.33 0.43 0.37 0.36 6.97
RBMT6 0.24 0.36 0.17 0.21 0.26 0.16 0.24 0.49 0.39 0.30 0.41 0.35 0.34 6.51
SAAR 0.24 0.14 0.17 0.19 0.22 0.15 0.24 0.47 0.37 0.39 0.39 0.32 0.31 3.22
SAAR-CONTRAST 0.26 0.36 0.18 0.22 0.27 0.17 0.27 0.51 0.41 0.36 0.41 0.35 0.35 6.01
UEDIN 0.25 0.36 0.17 0.21 0.26 0.18 0.26 0.51 0.41 0.35 0.42 0.36 0.35 5.97
UEDIN-COMBO 0.26 0.36 0.18 0.23 0.27 n/a n/a n/a n/a n/a n/a n/a n/a n/a
French-English Europarl Task
CMU-STATXFER 0.24 0.34 0.18 0.22 0.26 0.2 0.26 0.52 0.42 0.37 0.42 0.36 0.35 9.85
CMU-STATXFER-CONTRAST 0.25 0.34 0.19 0.22 0.26 0.2 0.26 0.53 0.42 0.38 0.42 0.36 0.35 7.10
CUED 0.34 0.44 0.26 0.29 0.33 0.32 0.38 0.59 0.48 0.50 0.51 0.47 0.47 0.11
CUED-CONTRAST 0.34 0.44 0.26 0.29 0.34 0.32 0.39 0.59 0.48 0.51 0.51 0.47 0.47 9.34
DCU 0.33 0.43 0.25 0.28 0.33 0.31 0.37 0.58 0.47 0.49 0.50 0.46 0.46 9.16
LIMSI 0.34 0.44 0.26 0.29 0.34 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 9.59
LIUM-SYSTRAN 0.35 0.45 0.27 0.3 0.34 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.49 9.75
LIUM-SYSTRAN-CONTRAST 0.34 0.44 0.26 0.29 0.34 0.33 0.39 0.59 0.48 0.50 0.52 0.48 0.48 9.23
RBMT3 0.25 0.36 0.10 0.20 0.24 0.17 0.25 0.51 0.41 0.35 0.43 0.37 0.36 7.36
RBMT4 0.27 0.36 0.19 0.22 0.27 0.18 0.26 0.51 0.41 0.37 0.43 0.38 0.37 5.92
RBMT5 0.27 0.38 0.21 0.23 0.28 0.20 0.28 0.53 0.43 0.4 0.45 0.4 0.39 7.20
RBMT6 0.24 0.35 0.18 0.21 0.26 0.16 0.24 0.5 0.40 0.35 0.42 0.36 0.35 5.96
SAAR 0.32 0.41 0.23 0.27 0.31 0.27 0.33 0.54 0.43 0.49 0.49 0.44 0.41 4.76
SAAR-CONTRAST 0.33 0.43 0.25 0.28 0.33 0.3 0.36 0.58 0.48 0.47 0.51 0.47 0.46 0.10
SYSTRAN 0.3 0.4 0.23 0.26 0.30 0.26 0.34 0.55 0.45 0.46 0.48 0.43 0.43 7.01
UCL 0.3 0.40 0.22 0.26 0.3 0.26 0.32 0.55 0.44 0.47 0.47 0.42 0.41 6.35
UEDIN 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.50 0.52 0.48 0.48 9.41
Table 17: Automatic evaluation metric for translations into English
91
D
P
D
R
S
R
U
L
C
U
L
C
H
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
Czech-English News Commentary Task
DCU 0.25 0.34 0.18 0.22 0.27 0.21 0.29 0.54 0.44 0.42 0.42 0.36 0.36 2.45
SYSTRAN 0.19 0.28 0.12 0.17 0.21 0.15 0.23 0.45 0.36 0.34 0.36 0.29 0.29 0.76
UEDIN 0.24 0.31 0.16 0.21 0.25 0.22 0.30 0.54 0.44 0.43 0.41 0.35 0.35 1.37
UMD 0.26 0.34 0.19 0.23 0.28 0.24 0.33 0.56 0.45 0.49 0.44 0.39 0.38 1.41
Czech-English News Task
DCU 0.19 0.30 0.13 0.17 0.22 0.12 0.22 0.45 0.35 0.32 0.36 0.28 0.28 1.78
UEDIN 0.19 0.28 0.12 0.17 0.21 0.12 0.21 0.44 0.34 0.32 0.35 0.27 0.27 0.65
UMD 0.2 0.29 0.12 0.18 0.22 0.13 0.22 0.44 0.34 0.36 0.36 0.29 0.27 0.52
German-English News Task
BBN-COMBO 0.23 0.34 0.14 0.21 0.25 0.18 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-STATXFER 0.16 0.27 0.09 0.15 0.19 0.11 0.18 0.43 0.34 0.25 0.33 0.25 0.24 7.84
LIMSI 0.22 0.33 0.13 0.19 0.23 0.17 0.25 0.47 0.37 0.36 0.4 0.33 0.32 5.58
LIU 0.21 0.32 0.06 0.18 0.22 0.15 0.24 0.48 0.38 0.33 0.38 0.31 0.31 5.51
RBMT1 0.22 0.33 0.14 0.19 0.23 0.14 0.22 0.44 0.35 0.28 0.37 0.31 0.30 6.13
RBMT2 0.24 0.37 0.17 0.21 0.26 0.15 0.24 0.5 0.40 0.31 0.4 0.33 0.32 7.14
RBMT3 0.24 0.37 0.16 0.21 0.26 0.16 0.24 0.49 0.4 0.32 0.41 0.34 0.34 6.97
RBMT4 0.25 0.38 0.17 0.21 0.27 0.16 0.25 0.50 0.40 0.34 0.41 0.35 0.34 7.03
RBMT5 0.23 0.36 0.15 0.20 0.25 0.15 0.23 0.48 0.39 0.32 0.4 0.33 0.32 5.94
RBMT6 0.22 0.34 0.14 0.19 0.24 0.14 0.22 0.47 0.38 0.31 0.39 0.32 0.31 5.65
SAAR 0.22 0.33 0.14 0.2 0.24 0.15 0.24 0.47 0.37 0.36 0.39 0.32 0.31 4.67
SAAR-CONTRAST 0.24 0.35 0.16 0.21 0.25 0.17 0.26 0.5 0.4 0.36 0.4 0.33 0.33 5.80
SAAR-CONTRAST-2 0.21 0.33 0.14 0.19 0.23 0.15 0.24 0.47 0.37 0.36 0.39 0.32 0.31 4.80
UEDIN 0.23 0.34 0.09 0.19 0.23 0.16 0.25 0.48 0.39 0.35 0.4 0.33 0.33 5.72
German-English Europarl Task
CMU-STATXFER 0.2 0.31 0.12 0.19 0.22 0.17 0.23 0.49 0.39 0.34 0.39 0.32 0.31 7.11
LIMSI 0.28 0.38 0.18 0.24 0.28 0.27 0.33 0.55 0.44 0.43 0.47 0.42 0.42 8.04
LIU 0.28 0.39 0.09 0.23 0.26 0.27 0.33 0.55 0.44 0.44 0.47 0.43 0.43 7.46
RBMT1 0.21 0.3 0.14 0.18 0.22 0.12 0.19 0.42 0.33 0.27 0.36 0.30 0.28 4.61
RBMT2 0.24 0.35 0.16 0.20 0.25 0.14 0.23 0.49 0.39 0.32 0.39 0.33 0.32 5.42
RBMT3 0.24 0.35 0.16 0.20 0.25 0.15 0.23 0.48 0.39 0.32 0.40 0.34 0.33 5.43
RBMT4 0.24 0.36 0.15 0.20 0.25 0.14 0.23 0.49 0.39 0.34 0.41 0.34 0.34 5.11
RBMT5 0.23 0.34 0.15 0.2 0.24 0.14 0.22 0.48 0.38 0.33 0.4 0.33 0.32 4.55
RBMT6 0.22 0.33 0.13 0.18 0.23 0.13 0.21 0.47 0.37 0.31 0.38 0.31 0.31 4.08
SAAR 0.29 0.39 0.19 0.25 0.28 0.27 0.33 0.55 0.44 0.43 0.47 0.42 0.42 7.32
SAAR-CONTRAST 0.28 0.37 0.18 0.24 0.28 0.26 0.32 0.54 0.43 0.43 0.47 0.42 0.42 6.77
UCL 0.24 0.36 0.16 0.22 0.25 0.2 0.25 0.49 0.39 0.41 0.42 0.35 0.32 4.26
UEDIN 0.30 0.41 0.20 0.26 0.3 0.28 0.34 0.56 0.45 0.45 0.48 0.44 0.44 7.96
Spanish-English News Task
CMU-SMT 0.24 0.35 0.17 0.21 0.25 0.18 0.26 0.48 0.38 0.39 0.41 0.35 0.34 8.00
CUED 0.25 0.36 0.17 0.21 0.26 0.19 0.28 0.50 0.40 0.38 0.42 0.36 0.36 6.03
CUED-CONTRAST 0.26 0.37 0.18 0.22 0.27 0.21 0.3 0.52 0.42 0.39 0.44 0.38 0.38 6.27
LIMSI 0.26 0.37 0.18 0.22 0.27 0.20 0.28 0.50 0.4 0.41 0.43 0.38 0.37 4.93
RBMT3 0.25 0.38 0.17 0.22 0.27 0.18 0.26 0.50 0.41 0.32 0.43 0.38 0.36 7.54
RBMT4 0.26 0.38 0.18 0.22 0.27 0.18 0.26 0.51 0.42 0.32 0.44 0.39 0.37 7.81
RBMT5 0.26 0.38 0.08 0.20 0.25 0.2 0.27 0.51 0.42 0.33 0.44 0.38 0.37 6.89
RBMT6 0.25 0.36 0.17 0.21 0.26 0.18 0.25 0.51 0.41 0.33 0.43 0.37 0.36 6.83
SAAR 0.26 0.37 0.19 0.22 0.27 0.19 0.29 0.51 0.41 0.39 0.43 0.37 0.37 5.23
SAAR-CONTRAST 0.26 0.37 0.18 0.22 0.27 0.19 0.28 0.51 0.41 0.37 0.42 0.37 0.36 5.95
UCB 0.25 0.35 0.17 0.21 0.26 0.19 0.27 0.5 0.39 0.39 0.42 0.36 0.35 4.40
UEDIN 0.24 0.35 0.17 0.21 0.26 0.18 0.27 0.50 0.40 0.36 0.41 0.35 0.34 5.07
UEDIN-COMBO 0.27 0.36 0.19 0.23 0.27 n/a n/a n/a n/a n/a n/a n/a n/a n/a
UPC 0.25 0.36 0.17 0.21 0.26 0.19 0.26 0.49 0.39 0.4 0.43 0.37 0.36 4.38
Table 18: Automatic evaluation metric for translations into English
92
B Break down of correlation for each task
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
D
P
D
R
S
R
U
L
C
U
L
C
H
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
All-English News Task
RANK 1 n/a n/a 0.83 0.73 0.83 0.83 0.87 0.71 0.7 0.82 0.79 0.41 0.79 0.8 0.80 0.25
French-English News Task
RANK 1 0.69 0.63 0.92 0.83 0.89 0.90 0.90 0.81 0.80 0.88 0.80 0.57 0.87 0.9 0.9 ?
0.21
CONST ? 1 0.81 0.83 0.52 0.81 0.86 0.81 0.93 0.9 0.76 0.64 0.73 0.69 0.72 0.85 ?
0.52
YES/NO ? ? 1 0.71 0.57 0.76 0.77 0.74 0.79 0.75 0.67 0.59 0.62 0.66 0.67 0.79 ?
0.26
French-English Europarl Task
RANK 1 0.95 0.9 0.94 0.95 0.93 0.95 0.93 0.92 0.90 0.88 0.87 0.92 0.94 0.94 0.91 0.50
CONST ? 1 0.91 0.97 0.97 0.98 0.98 0.97 0.97 0.96 0.97 0.95 0.96 0.97 0.97 0.96 0.56
YES/NO ? ? 1 0.94 0.94 0.94 0.96 0.96 0.96 0.97 0.92 0.93 0.92 0.95 0.95 0.97 0.47
German-English News Task
RANK 1 0.56 0.56 0.85 0.93 0.92 0.85 0.95 0.12 0.09 0.83 0.89 ?
0.11
0.63 0.60 0.58 0.36
CONST ? 1 0.48 0.54 0.48 0.59 0.66 0.57 0.64 0.65 0.61 0.55 0.51 0.57 0.63 0.56 ?
0.02
YES/NO ? ? 1 0.68 0.61 0.69 0.73 0.67 0.60 0.41 0.54 0.56 0.33 0.79 0.83 0.70 0.08
German-English Europarl Task
RANK 1 0.63 0.81 0.76 0.59 0.46 0.57 0.60 0.30 0.39 0.40 0.66 0.25 0.53 0.53 0.64 0.35
CONST ? 1 0.78 0.87 0.92 0.51 0.83 0.86 0.69 0.69 0.76 0.80 0.69 0.88 0.88 0.88 0.61
YES/NO ? ? 1 0.88 0.77 0.48 0.77 0.78 0.66 0.67 0.64 0.86 0.58 0.74 0.74 0.85 0.78
Spanish-English News Task
RANK 1 ?
0.07
0.44 0.75 0.76 0.68 0.71 0.81 0.19 0.01 0.66 0.63 ?
0.12
0.73 0.76 0.66 0.36
CONST ? 1 0.66 ?
0.03
?
0.44
0.29 0.29 0.14 0.45 0.66 ?
0.11
?
0.33
0.77 ?
0.37
?
0.34
0.16 ?
0.58
YES/NO ? ? 1 0.29 0.05 0.73 0.64 0.55 0.48 0.47 0.09 ?
0.11
0.71 0.06 0.1 0.39 ?
0.43
Spanish-English Europarl Task
RANK 1 0.69 0.76 0.78 0.73 0.73 0.8 0.77 0.78 0.79 0.83 0.84 0.77 0.73 0.73 0.80 0.87
CONST ? 1 0.68 0.76 0.77 0.75 0.69 0.73 0.64 0.67 0.64 0.68 0.73 0.78 0.78 0.73 0.56
YES/NO ? ? 1 0.94 0.93 0.95 0.96 0.95 0.98 0.97 0.91 0.91 0.95 0.94 0.94 0.98 0.69
Table 19: Correlation of automatic evaluation metrics with the three types of human judgments for transla-
tion into English
93
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
English-French News Task
RANK 1 0.55 0.48 0.73 0.62 0.3 0.47 0.56 0.69 0.69 0.66 0.72
CONST ? 1 0.35 0.49 0.47 0.39 0.49 0.24 0.59 0.59 0.58 0.45
YES/NO ? ? 1 0.81 0.92 0.71 0.73 0.78 0.73 0.73 0.76 0.76
English-French Europarl Task
RANK 1 0.98 0.88 0.95 0.95 0.95 0.95 0.90 0.97 0.97 0.93 0.93
CONST ? 1 0.94 0.98 0.98 0.98 0.98 0.93 1 1 0.97 0.91
YES/NO ? ? 1 0.97 0.97 0.97 0.97 0.92 0.95 0.95 0.92 0.83
English-German News Task
RANK 1 0.57 0.71 0.58 0.42 0.43 0.13 0.25 0.90 0.90 0.90 0.32
CONST ? 1 0.78 0.75 0.83 0.82 0.55 0.60 0.72 0.72 0.72 0.58
YES/NO ? ? 1 0.62 0.54 0.51 0.36 0.23 0.75 0.75 0.75 0.76
English-German Europarl Task
RANK 1 0.28 0.57 0.36 0.36 0.42 0.39 0.26 0.38 0.38 0.50 0.56
CONST ? 1 0.87 0.88 0.88 0.91 0.90 0.93 0.88 0.88 0.80 0.85
YES/NO ? ? 1 0.89 0.89 0.96 0.96 0.84 0.86 0.86 0.87 0.98
English-Spanish News Task
RANK 1 ?
0.30
0.49 ?
0.04
?
0.47
?
0.25
?
0.29
?
0.33
?
0.19
?
0.19
?
0.07
0.02
CONST ? 1 0.43 0.79 0.61 0.64 0.56 0.2 0.59 0.59 0.55 0.56
YES/NO ? ? 1 0.55 0.41 0.43 0.31 0.13 0.65 0.65 0.72 0.16
English-Spanish Europarl Task
RANK 1 0.90 0.63 0.8 0.83 0.84 0.83 0.73 0.79 0.79 0.76 0.80
CONST ? 1 0.73 0.84 0.86 0.81 0.8 0.74 0.84 0.83 0.84 0.86
YES/NO ? ? 1 0.68 0.75 0.66 0.67 0.90 0.67 0.66 0.73 0.68
Table 20: Correlation of automatic evaluation metrics with the three types of human judgments for transla-
tion into other languages
94
C Pairwise system comparisons by human judges
The following tables show pairwise comparisons between systems for each language pair, test set, and
manual evaluation type. The numbers in each of the tables? cells indicate the percent of that the system in
that column was judged to be better than the system in that row. Bolding indicates the winner of the two
systems. The difference between 100 and the sum of the complimentary cells is the percent of time that the
two systems were judged to be equal.
Because there were so many systems and data conditions the significance of each pairwise comparison
needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differ-
ences (rather than differences that are attributable to chance). In the following tables ? indicates statistical
significance at p <= 0.05 and ? indicates statistical significance at p <= 0.01, according to the Sign Test.
B
B
N
-C
M
B
C
M
U
-C
M
B
C
M
U
-X
F
R
C
U
E
D
C
U
E
D
-C
L
IM
S
I
L
IU
M
-S
Y
S
L
IU
M
-S
Y
S
-C
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
U
E
D
IN
-C
M
B
BBN-CMB 0.32 0.18? 0.21 0.42 0.37 0.29 0.24 0.33 0.48 0.48 0.32 0.29 0.44 0.48 0.21
CMU-CMB 0.50 0.26 0.29 0.42 0.4 0.44 0.48 0.49 0.38 0.45 0.55 0.32 0.34 0.34 0.46
CMU-XFR 0.67? 0.44 0.60? 0.75? 0.58 0.73? 0.62 0.59 0.54 0.77? 0.48 0.54 0.65? 0.71? 0.58
CUED 0.46 0.41 0.20? 0.47 0.56 0.47 0.51? 0.41 0.54 0.57 0.37 0.43 0.61 0.39 0.15
CUED-C 0.27 0.22 0.08? 0.20 0.31 0.54 0.52? 0.32 0.52 0.50 0.31 0.40 0.38 0.30 0.52
LIMSI 0.34 0.4 0.29 0.31 0.41 0.23? 0.52 0.38 0.50 0.39 0.49 0.42 0.32 0.26 0.30
LIUM-SYS 0.37 0.32 0.13? 0.39 0.27 0.60? 0.24 0.44 0.46 0.46 0.33 0.24? 0.25 0.30 0.19
LI-SYS-C 0.40 0.26 0.24 0.20? 0.13? 0.30 0.24 0.44 0.42 0.43 0.35 0.21? 0.30 0.30 0.31
RBMT3 0.46 0.43 0.26 0.38 0.46 0.48 0.39 0.39 0.41 0.44 0.26 0.36 0.50 0.68? 0.44
RBMT4 0.36 0.33 0.31 0.36 0.39 0.35 0.50 0.45 0.45 0.49 0.40 0.35 0.57 0.51 0.53
RBMT5 0.37 0.33 0.12? 0.32 0.33 0.33 0.39 0.46 0.25 0.22 0.21 0.37 0.44 0.49 0.57
RBMT6 0.50 0.33 0.37 0.34 0.50 0.39 0.44 0.50 0.48 0.37 0.55 0.42 0.48 0.41 0.41
SAAR 0.50 0.46 0.37 0.38 0.44 0.52 0.6? 0.54? 0.44 0.53 0.44 0.29 0.34 0.52 0.50
SAAR-C 0.31 0.47 0.23? 0.30 0.24 0.51 0.50 0.47 0.25 0.31 0.33 0.35 0.26 0.47 0.38
UED 0.35 0.37 0.13? 0.39 0.55 0.50 0.50 0.43 0.24? 0.37 0.36 0.41 0.31 0.47 0.36
UED-CMB 0.57 0.36 0.16 0.46 0.38 0.30 0.63 0.39 0.39 0.37 0.35 0.53 0.27 0.48 0.36
> OTHERS 0.43 0.37 0.22 0.34 0.41 0.44 0.45 0.45 0.4 0.42 0.47 0.37 0.34 0.43 0.44 0.42
? OTHERS 0.66 0.59 0.38 0.55 0.64 0.63 0.66 0.69 0.58 0.58 0.65 0.57 0.54 0.64 0.61 0.61
Table 21: Sentence-level ranking for the French-English News Task.
C
M
U
-X
F
R
C
U
E
D
D
C
U
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
S
Y
S
T
R
A
N
U
C
L
U
E
D
IN
CMU-XFR 0.53 0.50 0.74? 0.79? 0.55 0.46 0.50 0.36 0.73 0.92? 0.36 0.44 0.77?
CUED 0.29 0.42 0.29 0.48 0.16? 0.53 0.16? 0.18? 0.18? 0.55 0.06? 0.21 0.38
DCU 0.46 0.29 0.38 0.47 0.37 0.27 0.24 0.29 0.35 0.55 0.18 0.25 0.50
LIMSI 0.11? 0.21 0.44 0.11 0.12? 0.17 0.29 0.05? 0.30 0.32 0.19 0.29 0.33
LIUM-SYS 0.14? 0.16 0.24 0.32 0.06? 0.13 0.22 0.12? 0.14? 0.33 0.20? 0.26 0.32
RBMT3 0.36 0.79? 0.58 0.88? 0.72? 0.40 0.57 0.21 0.67 0.72? 0.50 0.54 0.67
RBMT4 0.50 0.40 0.64 0.67 0.56 0.40 0.42 0.21? 0.52 0.67 0.33 0.47 0.75
RBMT5 0.38 0.79? 0.60 0.57 0.56 0.24 0.42 0.26 0.48 0.72? 0.50 0.46 0.60
RBMT6 0.54 0.79? 0.67 0.77? 0.82? 0.47 0.79? 0.53 0.71? 0.83? 0.56 0.47 0.77?
SAAR 0.27 0.59? 0.57 0.47 0.71? 0.22 0.29 0.48 0.18? 0.50 0.35 0.23 0.50
SAAR-C 0.04? 0.15 0.31 0.39 0.48 0.14? 0.24 0.21? 0.08? 0.21 0.17? 0.20 0.57
SYSTRAN 0.50 0.81? 0.65 0.52 0.64? 0.38 0.62 0.33 0.32 0.41 0.71? 0.56 0.55
UCL 0.31 0.64 0.56 0.57 0.47 0.46 0.40 0.39 0.27 0.55 0.60 0.44 0.47
UED 0.24? 0.43 0.35 0.33 0.42 0.28 0.25 0.33 0.15? 0.29 0.26 0.25 0.27
> OTHERS 0.32 0.50 0.5 0.54 0.55 0.28 0.4 0.35 0.21 0.41 0.59 0.32 0.35 0.55
? OTHERS 0.42 0.7 0.64 0.78 0.79 0.40 0.50 0.48 0.32 0.58 0.75 0.47 0.52 0.71
Table 22: Sentence-level ranking for the French-English Europarl Task.
95
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
X
E
R
O
X
LIMSI 0.29 0.25 0.60? 0.52 0.48 0.13 0.30 0.13? 0.17?
LIUM-SYSTRAN 0.36 0.41 0.51 0.41 0.53 0.22 0.26 0.27 0.04?
RBMT3 0.56 0.34 0.48 0.52 0.40 0.31 0.53 0.37 0.11?
RBMT4 0.13? 0.36 0.31 0.29 0.19? 0.26 0.15? 0.17? 0.09?
RBMT5 0.33 0.35 0.29 0.42 0.26 0.17? 0.32 0.17? 0.12?
RBMT6 0.42 0.38 0.37 0.43? 0.44 0.32 0.32 0.28 0.11?
SAAR 0.56 0.52 0.51 0.56 0.69? 0.41 0.33 0.46 0.3
SAAR-CONTRAST 0.55 0.44 0.33 0.63? 0.56 0.46 0.21 0.41 0.22?
UEDIN 0.48? 0.48 0.41 0.60? 0.65? 0.53 0.41 0.43 0.09?
XEROX 0.63? 0.74? 0.78? 0.74? 0.71? 0.75? 0.44 0.64? 0.63?
> OTHERS 0.44 0.43 0.41 0.54 0.53 0.43 0.28 0.37 0.32 0.13
? OTHERS 0.67 0.66 0.60 0.78 0.73 0.66 0.51 0.57 0.55 0.32
Table 23: Sentence-level ranking for the English-French News Task.
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
C
L
U
E
D
IN
LIMSI 0.23 0.21? 0.32 0.10? 0.15? 0.35 0.27 0.15? 0.17
LIUM-SYSTRAN 0.28 0.39 0.11? 0.21? 0.22 0.40 0.19? 0.15
RBMT3 0.75? 0.59 0.38 0.39 0.49 0.70? 0.81? 0.47 0.81?
RBMT4 0.64 0.36 0.28 0.24? 0.18 0.61 0.48 0.42 0.50
RBMT5 0.85? 0.89? 0.49 0.62? 0.67? 0.78? 0.91? 0.63? 0.93?
RBMT6 0.85? 0.62? 0.26 0.42 0.24? 0.83? 0.82? 0.47 0.68?
SAAR 0.41 0.52 0.17? 0.30 0.11? 0.06? 0.41 0.11? 0.41
SAAR-CONTRAST 0.47 0.40 0.11? 0.26 0.03? 0.06? 0.32 0.27 0.26
UCL 0.80? 0.70? 0.42 0.47 0.22? 0.44 0.71? 0.61 0.78?
UEDIN 0.46 0.41 0.11? 0.33 0.04? 0.15? 0.32 0.36 0.03?
> OTHERS 0.62 0.54 0.26 0.4 0.17 0.27 0.56 0.6 0.32 0.54
? OTHERS 0.79 0.78 0.42 0.61 0.26 0.44 0.74 0.79 0.44 0.77
Table 24: Sentence-level ranking for the English-French Europarl Task.
B
B
N
-C
M
B
C
M
U
-X
F
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
U
E
D
IN
-C
M
B
BBN-COMBO 0.1? 0.22 0.37 0.62? 0.69? 0.74? 0.66? 0.41 0.63? 0.60? 0.35 0.40
CMU-STATXFER 0.71? 0.44 0.54 0.76? 0.79? 0.73? 0.74? 0.80? 0.62? 0.65? 0.54? 0.37
LIMSI 0.44 0.24 0.41 0.67? 0.65? 0.69? 0.54 0.50 0.50 0.63 0.38 0.22
LIU 0.37 0.27 0.34 0.55? 0.56 0.61? 0.50 0.45 0.48 0.56 0.32 0.34
RBMT2 0.21? 0.14? 0.31? 0.20? 0.27 0.43 0.29 0.34 0.30 0.13? 0.25? 0.24?
RBMT3 0.18? 0.13? 0.19? 0.27 0.56 0.37 0.33 0.32 0.29 0.29 0.19? 0.17?
RBMT4 0.22? 0.12? 0.17? 0.18? 0.46 0.51 0.3 0.31 0.18? 0.26? 0.28 0.17?
RBMT5 0.22? 0.12? 0.32 0.36 0.58 0.51 0.40 0.29 0.23? 0.37 0.3 0.28
RBMT6 0.55 0.08? 0.40 0.4 0.51 0.51 0.47 0.51 0.49 0.52 0.22? 0.43
SAAR 0.23? 0.21? 0.40 0.39 0.52 0.50 0.61? 0.53? 0.38 0.50? 0.26? 0.13?
SAAR-CONTRAST 0.23? 0.19? 0.3 0.37 0.71? 0.37 0.60? 0.37 0.33 0.17? 0.48 0.13?
UEDIN 0.23 0.13? 0.38 0.3 0.68? 0.65? 0.55 0.59 0.64? 0.67? 0.38 0.42
UEDIN-COMBO 0.35 0.41 0.59 0.50 0.72? 0.66? 0.83? 0.56 0.52 0.50? 0.67? 0.38
> OTHERS 0.32 0.17 0.34 0.35 0.61 0.56 0.57 0.49 0.45 0.41 0.46 0.33 0.28
? OTHERS 0.51 0.35 0.52 0.56 0.74 0.73 0.73 0.67 0.59 0.61 0.65 0.55 0.44
Table 25: Sentence-level ranking for the German-English News Task.
96
C
M
U
-X
F
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-STATXFER 0.57? 0.77? 0.53 0.71? 0.69? 0.50 0.58 0.82? 0.46 0.75?
LIMSI 0.17? 0.35 0.71? 0.63 0.76? 0.50 0.59 0.52 0.23 0.67?
LIU 0.14? 0.35 0.50 0.29 0.67 0.3 0.42 0.35 0.27 0.57
RBMT2 0.27 0.24? 0.46 0.39 0.33 0.36 0.42 0.50 0.33 0.46
RBMT3 0.23? 0.3 0.57 0.45 0.40 0.31 0.38 0.56 0.32 0.55
RBMT4 0.22? 0.19? 0.29 0.50 0.48 0.39 0.48 0.41 0.32 0.61
RBMT5 0.40 0.40 0.56 0.54 0.57 0.52 0.3 0.48 0.29? 0.54
RBMT6 0.27 0.32 0.48 0.46 0.53 0.44 0.51 0.55 0.36 0.61
SAAR 0.12? 0.19 0.30 0.44 0.41 0.48 0.32 0.42 0.20? 0.40
UCL 0.35 0.54 0.46 0.63 0.61 0.68 0.68? 0.61 0.63? 0.65?
UEDIN 0.22? 0.17? 0.32 0.42 0.42 0.36 0.41 0.27 0.40 0.23?
> OTHERS 0.24 0.32 0.46 0.51 0.51 0.53 0.43 0.43 0.53 0.30 0.58
? OTHERS 0.36 0.49 0.61 0.63 0.6 0.61 0.54 0.54 0.68 0.42 0.68
Table 26: Sentence-level ranking for the German-English Europarl Task.
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
E
D
IN
LIMSI 0.44 0.8? 0.67? 0.81? 0.76? 0.63? 0.53 0.47?
LIU 0.29 0.80? 0.68? 0.81? 0.62? 0.63? 0.25 0.31
RBMT2 0.13? 0.07? 0.35 0.33 0.32? 0.20? 0.17? 0.09?
RBMT3 0.18? 0.27? 0.50 0.52 0.45 0.29? 0.26 0.21?
RBMT4 0.09? 0.12? 0.47 0.30 0.42 0.22? 0.15? 0.17?
RBMT5 0.12? 0.26? 0.59? 0.42 0.40 0.33 0.28 0.24?
RBMT6 0.25? 0.22? 0.6? 0.61? 0.63? 0.50 0.36 0.33
SAAR 0.28 0.63 0.66? 0.56 0.7? 0.62 0.46 0.45
UEDIN 0.24? 0.42 0.75? 0.66? 0.73? 0.68? 0.51 0.36
> OTHERS 0.19 0.28 0.64 0.54 0.61 0.54 0.40 0.3 0.27
? OTHERS 0.36 0.43 0.79 0.66 0.75 0.67 0.56 0.46 0.44
Table 27: Sentence-level ranking for the English-German News Task.
C
M
U
-G
IM
P
E
L
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-GIMPEL 0.29 0.28 0.41 0.49 0.56 0.44 0.24? 0.09? 0.24? 0.52
LIMSI 0.45 0.31 0.48 0.45 0.54 0.40 0.35 0.40 0.29? 0.47
LIU 0.34 0.47 0.56 0.44 0.65? 0.37 0.30 0.31 0.19? 0.50
RBMT2 0.51 0.48 0.41 0.41 0.48 0.22? 0.24? 0.62 0.26? 0.43
RBMT3 0.40 0.50 0.47 0.47 0.60 0.33 0.3? 0.11 0.26? 0.50
RBMT4 0.39 0.37 0.27? 0.41 0.35 0.22? 0.14? 0.25 0.33 0.46
RBMT5 0.49 0.47 0.54 0.64? 0.60 0.64? 0.32 0.47 0.45 0.64?
RBMT6 0.71? 0.50 0.58 0.57? 0.65? 0.74? 0.46 0.41 0.36 0.60
SAAR 0.73? 0.40 0.39 0.39 0.78 0.58 0.47 0.35 0.31 0.50
UCL 0.61? 0.6? 0.67? 0.59? 0.68? 0.64 0.53 0.51 0.62 0.70?
UEDIN 0.25 0.27 0.30 0.52 0.41 0.49 0.26? 0.31 0.25 0.23?
> OTHERS 0.47 0.43 0.43 0.51 0.51 0.59 0.36 0.3 0.37 0.3 0.54
? OTHERS 0.61 0.58 0.58 0.62 0.58 0.68 0.47 0.43 0.53 0.39 0.67
Table 28: Sentence-level ranking for the English-German Europarl Task.
97
C
M
U
-S
M
T
C
U
E
D
C
U
E
D
-C
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.41 0.62? 0.33 0.54? 0.57? 0.42 0.46 0.46 0.29 0.34 0.37
CUED 0.29 0.24 0.27 0.54? 0.76? 0.61? 0.50 0.39 0.46 0.26 0.42
CUED-CONTRAST 0.19? 0.24 0.23 0.47 0.48 0.28 0.41 0.37 0.26 0.26 0.33
LIMSI 0.33 0.30 0.51 0.41 0.56? 0.47 0.41 0.46 0.33 0.37 0.43
RBMT3 0.19? 0.23? 0.37 0.43 0.39 0.28 0.3 0.33 0.39 0.30 0.49
RBMT4 0.19? 0.14? 0.27 0.21? 0.27 0.21? 0.30 0.27 0.17? 0.29? 0.23?
RBMT5 0.37 0.19? 0.56 0.35 0.47 0.57? 0.56 0.43 0.24? 0.35 0.52
RBMT6 0.41 0.30 0.29 0.39 0.43 0.50 0.25 0.46 0.34 0.44 0.46
SAAR 0.29 0.25 0.43 0.32 0.50 0.42 0.33 0.31 0.2? 0.26 0.3
UCB 0.29 0.36 0.52 0.49 0.46 0.61? 0.6? 0.41 0.56? 0.39 0.28
UEDIN 0.39 0.37 0.52 0.30 0.50 0.61? 0.58 0.39 0.46 0.24 0.44
UPC 0.26 0.36 0.47 0.35 0.40 0.59? 0.32 0.42 0.46 0.33 0.41
> OTHERS 0.29 0.28 0.43 0.34 0.45 0.55 0.39 0.40 0.42 0.29 0.34 0.39
? OTHERS 0.57 0.56 0.67 0.58 0.67 0.77 0.58 0.61 0.67 0.54 0.56 0.60
Table 29: Sentence-level ranking for the Spanish-English News Task.
C
M
U
-S
M
T
C
U
E
D
D
C
U
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
CMU-SMT 0.36 0.38 0.37 0.10? 0.20? 0.14? 0.32 0.39 0.22 0.25 0.38
CUED 0.40 0.38 0.53 0.33 0.30 0.30 0.20? 0.32 0.08? 0.36 0.29
DCU 0.34 0.38 0.46 0.32 0.19? 0.26? 0.21? 0.32 0.33 0.25 0.46
LIMSI 0.31 0.30 0.21 0.05? 0.09? 0.15? 0.18? 0.24 0.10? 0.19 0.48
RBMT3 0.83? 0.62 0.58 0.73? 0.56 0.25 0.37 0.60? 0.31 0.66? 0.78?
RBMT4 0.73? 0.54 0.76? 0.74? 0.28 0.38 0.24 0.53 0.29 0.56 0.65?
RBMT5 0.79? 0.55 0.67? 0.75? 0.58 0.57 0.59? 0.70? 0.44 0.71? 0.67
RBMT6 0.52 0.77? 0.66? 0.68? 0.42 0.49 0.18? 0.55 0.41 0.54 0.71
SAAR 0.43 0.42 0.41 0.47 0.20? 0.32 0.17? 0.30 0.22? 0.35 0.32
UCL 0.56 0.71? 0.56 0.70? 0.42 0.57 0.33 0.44 0.59? 0.81? 0.67
UEDIN 0.28 0.46 0.39 0.31 0.29? 0.42 0.25? 0.39 0.35 0.15? 0.40
UPC 0.44 0.39 0.43 0.36 0.07? 0.23? 0.24 0.29 0.27 0.20 0.40
> OTHERS 0.50 0.5 0.49 0.53 0.28 0.36 0.24 0.32 0.44 0.26 0.45 0.51
? OTHERS 0.71 0.68 0.68 0.78 0.43 0.49 0.35 0.47 0.67 0.43 0.66 0.69
Table 30: Sentence-level ranking for the Spanish-English Europarl Task.
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.39 0.57 0.52? 0.62? 0.56? 0.50 0.41 0.42 0.56?
LIMSI 0.42 0.56 0.53 0.63? 0.58 0.32 0.39 0.35 0.35
RBMT3 0.23 0.3 0.34 0.46 0.50 0.39 0.17 0.21? 0.06?
RBMT4 0.25? 0.30 0.47 0.31 0.35 0.38 0.36 0.32 0.19
RBMT5 0.21? 0.20? 0.28 0.42 0.42 0.29? 0.24 0.17? 0.23
RBMT6 0.23? 0.23 0.31 0.41 0.42 0.23? 0.19 0.24? 0.24
SAAR 0.36 0.52 0.39 0.43 0.67? 0.54? 0.36 0.29 0.42
UCB 0.37 0.39 0.52 0.39 0.49 0.52 0.46 0.27 0.25
UEDIN 0.35 0.48 0.62? 0.48 0.64? 0.61? 0.50 0.47 0.53?
UPC 0.11? 0.41 0.63? 0.48 0.50 0.57 0.42 0.63 0.06?
> OTHERS 0.28 0.36 0.47 0.45 0.52 0.51 0.38 0.34 0.27 0.33
? OTHERS 0.49 0.54 0.68 0.67 0.72 0.72 0.55 0.59 0.48 0.60
Table 31: Sentence-level ranking for the English-Spanish News Task.
98
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
U
W
CMU-SMT 0.28 0.47 0.33 0.17? 0.26 0.50 0.25 0.48? 0.44 0.28
LIMSI 0.38 0.19? 0.33 0.16? 0.23 0.33 0.14? 0.14 0.35 0.32
RBMT3 0.42 0.62? 0.42 0.36 0.29 0.54 0.28 0.39 0.50 0.75?
RBMT4 0.46 0.47 0.42 0.19 0.31 0.61 0.50 0.40 0.50 0.57
RBMT5 0.70? 0.64? 0.59 0.48 0.35 0.65? 0.52 0.64 0.61 0.63?
RBMT6 0.63 0.58 0.47 0.56 0.50 0.78? 0.32 0.58 0.33 0.71?
SAAR 0.33 0.40 0.33 0.30 0.23? 0.19? 0.20 0.27 0.24 0.33
UCL 0.46 0.64? 0.41 0.46 0.36 0.41 0.60 0.65? 0.42 0.57?
UEDIN 0.09? 0.29 0.48 0.45 0.28 0.27 0.41 0.19? 0.25 0.17
UPC 0.22 0.40 0.50 0.43 0.28 0.40 0.52 0.26 0.56 0.58
UW 0.44 0.32 0.06? 0.29 0.17? 0.21? 0.33 0.14? 0.33 0.33
> OTHERS 0.43 0.46 0.4 0.4 0.26 0.28 0.53 0.28 0.46 0.4 0.49
? OTHERS 0.67 0.74 0.55 0.56 0.41 0.44 0.72 0.50 0.71 0.59 0.74
Table 32: Sentence-level ranking for the English-Spanish Europarl Task.
DCU UEDIN UMD
DCU 0.26? 0.4
UEDIN 0.37? 0.46?
UMD 0.4 0.31?
> OTHERS 0.38 0.28 0.43
? OTHERS 0.68 0.58 0.65
Table 33: Sentence-level ranking for the Czech-English News Task.
DCU SYSTRAN UEDIN UMD
DCU 0.21? 0.19? 0.37
SYSTRAN 0.59? 0.47? 0.61?
UEDIN 0.42? 0.27? 0.50?
UMD 0.38 0.18? 0.29?
> OTHERS 0.46 0.22 0.31 0.49
? OTHERS 0.75 0.45 0.60 0.72
Table 34: Sentence-level ranking for the Czech-English Commentary Task.
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.32? 0.51? 0.27?
CU-TECTOMT 0.52? 0.58? 0.42
PC-TRANSLATOR 0.35? 0.25? 0.26?
UEDIN 0.5? 0.40 0.59?
> OTHERS 0.45 0.32 0.56 0.32
? OTHERS 0.63 0.49 0.72 0.50
Table 35: Sentence-level ranking for the English-Czech News Task.
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.28? 0.38 0.19?
CU-TECTOMT 0.58? 0.53? 0.43
PC-TRANSLATOR 0.45 0.3? 0.26?
UEDIN 0.60? 0.37 0.56?
> OTHERS 0.54 0.32 0.49 0.29
? OTHERS 0.71 0.49 0.66 0.49
Table 36: Sentence-level ranking for the English-Czech Commentary Task.
99
MLOGIC UEDIN
MORPHOLOGIC 0.15?
UEDIN 0.68?
> OTHERS 0.68 0.15
? OTHERS 0.85 0.32
Table 37: Sentence-level ranking for the Hungarian-English News Task.
C
M
U
-X
F
R
C
U
E
D
C
U
E
D
-C
L
IM
S
I
L
IU
M
-S
Y
S
L
IU
M
-S
Y
S
-C
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
CMU-XFR 0.37 0.49? 0.62? 0.57? 0.61? 0.49 0.49 0.48? 0.41 0.56? 0.39 0.46?
CUED 0.28 0.21 0.30 0.30 0.13 0.28 0.18 0.27 0.28 0.31 0.34 0.18
CUED-C 0.2? 0.11 0.30? 0.19 0.33 0.18? 0.21 0.24 0.2? 0.2? 0.17? 0.24
LIMSI 0.13? 0.20 0.13? 0.27 0.22 0.23 0.24 0.2 0.20? 0.16? 0.23 0.22
LIUM-SYS 0.18? 0.17 0.27 0.17 0.20 0.18? 0.41 0.29 0.24 0.26 0.22 0.26
LI-SYS-C 0.18? 0.28 0.24 0.25 0.07 0.33 0.2? 0.27 0.18? 0.23 0.25 0.19
RBMT3 0.28 0.34 0.52? 0.28 0.40? 0.37 0.27 0.46? 0.27 0.30 0.39 0.34
RBMT4 0.29 0.40 0.34 0.31 0.39 0.43? 0.33 0.34 0.34 0.27 0.41 0.31
RBMT5 0.22? 0.24 0.34 0.3 0.27 0.43 0.14? 0.24 0.13? 0.32 0.32 0.32
RBMT6 0.3 0.41 0.50? 0.39? 0.33 0.58? 0.3 0.33 0.37? 0.33 0.52? 0.37
SAAR 0.27? 0.33 0.43? 0.37? 0.4 0.42 0.41 0.36 0.32 0.41 0.23 0.41
SAAR-C 0.28 0.32 0.38? 0.27 0.27 0.45 0.23 0.21 0.20 0.23? 0.18 0.19
UED 0.19? 0.15 0.20 0.25 0.29 0.19 0.28 0.27 0.19 0.24 0.21 0.26
> OTHERS 0.24 0.27 0.33 0.32 0.32 0.37 0.29 0.28 0.30 0.27 0.29 0.31 0.29
? OTHERS 0.51 0.75 0.79 0.80 0.77 0.78 0.65 0.66 0.73 0.62 0.64 0.74 0.77
Table 38: Constituent ranking for the French-English News Task
C
M
U
-X
F
R
C
U
E
D
D
C
U
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
S
Y
S
T
R
A
N
U
C
L
U
E
D
IN
CMU-XFR 0.42? 0.4? 0.37? 0.54? 0.16? 0.21 0.41 0.23 0.49? 0.42? 0.34 0.45 0.50?
CUED 0.03? 0.13 0.08 0.14 0.13? 0.13? 0.08? 0.05? 0.08 0.04 0.15 0.11 0.07
DCU 0.09? 0.08 0.10 0.12 0.06? 0.20 0.31 0.16? 0.14 0.22 0.13 0.10 0.16
LIMSI 0.1? 0.05 0.19 0.05 0.04? 0.08? 0.19 0.11? 0.18 0.09 0.05? 0.05?
LIUM-SYS 0.03? 0.14 0.19 0.07 0 0.08? 0.03? 0.05? 0.03? 0.09 0.15 0.14 0.08
RBMT3 0.44? 0.61? 0.50? 0.58? 0.56? 0.41? 0.38 0.32 0.37 0.53? 0.44 0.50? 0.58?
RBMT4 0.39 0.44? 0.43 0.45? 0.35? 0.12? 0.31 0.23 0.42 0.39 0.33 0.32 0.35
RBMT5 0.19 0.47? 0.29 0.35 0.37? 0.18 0.17 0.23 0.35 0.33 0.19 0.46 0.40
RBMT6 0.36 0.65? 0.54? 0.48? 0.55? 0.26 0.40 0.50 0.50? 0.52? 0.47? 0.60? 0.44
SAAR 0.07? 0.25 0.24 0.18 0.37? 0.23 0.36 0.23 0.12? 0.12 0.23 0.13 0.37?
SAAR-C 0.09? 0.18 0.12 0.16 0.16 0.09? 0.18 0.2 0.06? 0.12 0.09 0.14 0.15
SYSTRAN 0.34 0.40 0.21 0.38? 0.23 0.25 0.36 0.22 0.15? 0.23 0.28 0.31 0.30?
UCL 0.25 0.34 0.28 0.31? 0.19 0.11? 0.24 0.23 0.11? 0.24 0.31 0.34 0.37?
UED 0.10? 0.10 0.16 0.05 0.08 0.03? 0.15 0.14 0.18 0.07? 0.13 0.07? 0.11?
> OTHERS 0.2 0.32 0.27 0.28 0.28 0.12 0.22 0.25 0.15 0.26 0.27 0.22 0.25 0.28
? OTHERS 0.63 0.91 0.85 0.91 0.92 0.52 0.65 0.7 0.52 0.78 0.87 0.71 0.74 0.89
Table 39: Constituent ranking for the French-English Europarl Task
100
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
X
E
R
O
X
LIMSI 0.27 0.43 0.43 0.29 0.53? 0.32 0.37 0.30 0.14?
LIUM-SYSTRAN 0.09 0.33 0.36 0.18 0.35 0.16? 0.25 0.22 0.13?
RBMT3 0.36 0.33 0.22 0.31 0.28 0.4 0.26 0.26? 0.20?
RBMT4 0.25 0.26 0.30 0.23 0.16? 0.28 0.26 0.24 0.13?
RBMT5 0.31 0.33 0.22 0.28 0.17 0.27 0.25 0.23 0.13?
RBMT6 0.26? 0.30 0.31 0.38? 0.32 0.33 0.36 0.39 0.25?
SAAR 0.32 0.41? 0.35 0.38 0.32 0.28 0.14 0.23 0.11?
SAAR-CONTRAST 0.25 0.26 0.36 0.30 0.33 0.36 0.05 0.22 0.13?
UEDIN 0.29 0.34 0.45? 0.4 0.33 0.40 0.31 0.35 0.13?
XEROX 0.66? 0.55? 0.61? 0.65? 0.58? 0.51? 0.53? 0.57? 0.45?
> OTHERS 0.31 0.34 0.38 0.38 0.33 0.33 0.3 0.31 0.29 0.15
? OTHERS 0.65 0.76 0.72 0.77 0.76 0.67 0.73 0.75 0.66 0.44
Table 40: Constituent ranking for the English-French News Task
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
LIMSI 0.14 0.09? 0.10? 0.24 0.11? 0.13 0.08? 0.12
LIUM-SYSTRAN 0.19? 0.19? 0.15 0.12? 0.06 0.06? 0.09
RBMT3 0.65? 0.59? 0.33 0.43 0.32 0.50? 0.39 0.46?
RBMT4 0.53? 0.47? 0.19 0.27 0.18? 0.33 0.38 0.39
RBMT5 0.48 0.38 0.32 0.48 0.47 0.55? 0.44 0.51?
RBMT6 0.54? 0.49? 0.32 0.41? 0.26 0.52? 0.45 0.58?
SAAR 0.21 0.17 0.23? 0.25 0.21? 0.17? 0.19 0.13
UCL 0.37? 0.33? 0.38 0.35 0.36 0.32 0.34 0.31?
UEDIN 0.12 0.11 0.17? 0.23 0.13? 0.13? 0.07 0.07?
> OTHERS 0.38 0.36 0.25 0.30 0.26 0.24 0.33 0.27 0.34
? OTHERS 0.88 0.88 0.56 0.68 0.55 0.56 0.81 0.66 0.87
Table 41: Constituent ranking for the English-French Europarl Task
C
M
U
-X
F
E
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
CMU-STATXFER 0.47? 0.44 0.52? 0.53? 0.57? 0.49? 0.41 0.49 0.58? 0.49?
LIMSI 0.17? 0.18 0.35 0.34 0.40 0.33 0.43 0.19 0.28 0.19
LIU 0.25 0.3 0.37 0.35 0.44 0.28 0.40 0.21 0.33 0.32?
RBMT2 0.19? 0.26 0.30 0.19 0.32 0.16? 0.20 0.26 0.23 0.21
RBMT3 0.22? 0.36 0.26 0.23 0.24 0.23 0.14? 0.15 0.28 0.29
RBMT4 0.20? 0.35 0.23 0.21 0.24 0.22 0.19? 0.36 0.32 0.31
RBMT5 0.26? 0.28 0.38 0.34? 0.31 0.35 0.26 0.3 0.43? 0.35
RBMT6 0.38 0.37 0.39 0.34 0.44? 0.4? 0.30 0.28 0.26 0.38
SAAR 0.29 0.22 0.37 0.29 0.10 0.28 0.19 0.22 0.26 0.18
SAAR-CONTRAST 0.18? 0.33 0.29 0.19 0.22 0.24 0.15? 0.26 0.18 0.23
UEDIN 0.11? 0.3 0.13? 0.23 0.35 0.3 0.2 0.37 0.30 0.31
> OTHERS 0.22 0.33 0.3 0.31 0.32 0.35 0.25 0.29 0.28 0.33 0.30
? OTHERS 0.50 0.72 0.67 0.77 0.76 0.74 0.67 0.64 0.76 0.78 0.74
Table 42: Constituent ranking for the German-English News Task
101
C
M
U
-X
F
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-STATXFER 0.51? 0.51? 0.38 0.38 0.41 0.37 0.44 0.48? 0.39 0.6?
LIMSI 0.18? 0.22 0.3 0.30 0.23 0.22? 0.32 0.27 0.18? 0.29
LIU 0.14? 0.22 0.26? 0.32 0.22? 0.16? 0.31 0.20 0.08? 0.12
RBMT2 0.38 0.51 0.52? 0.40 0.32 0.25 0.31 0.51 0.40 0.7?
RBMT3 0.32 0.42 0.45 0.28 0.46 0.16 0.20? 0.56? 0.38 0.43
RBMT4 0.32 0.45 0.52? 0.31 0.24 0.13? 0.30 0.49? 0.44 0.48?
RBMT5 0.44 0.57? 0.53? 0.34 0.31 0.43? 0.19 0.54? 0.39 0.54?
RBMT6 0.33 0.51 0.48 0.33 0.47? 0.33 0.33 0.47? 0.42 0.51?
SAAR 0.12? 0.1 0.15 0.26 0.09? 0.19? 0.17? 0.23? 0.11? 0.14
UCL 0.30 0.43? 0.49? 0.40 0.40 0.30 0.41 0.39 0.38? 0.51?
UEDIN 0.11? 0.16 0.12 0.18? 0.25 0.2? 0.18? 0.23? 0.14 0.12?
> OTHERS 0.27 0.40 0.41 0.31 0.32 0.32 0.25 0.3 0.41 0.30 0.44
? OTHERS 0.55 0.75 0.8 0.58 0.64 0.64 0.58 0.59 0.84 0.60 0.83
Table 43: Constituent ranking for the German-English Europarl Task
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
E
D
IN
LIMSI 0.29 0.46 0.45 0.37 0.36 0.29? 0.33 0.22
LIU 0.32 0.53? 0.45? 0.51? 0.5? 0.38 0.31 0.36
RBMT2 0.33 0.32? 0.29 0.29 0.20? 0.25? 0.28 0.28?
RBMT3 0.34 0.3? 0.4 0.33 0.3? 0.34 0.20? 0.27?
RBMT4 0.26 0.25? 0.31 0.3 0.23? 0.23? 0.20? 0.21?
RBMT5 0.46 0.33? 0.55? 0.46? 0.40? 0.32 0.32 0.29?
RBMT6 0.52? 0.40 0.47? 0.44 0.53? 0.40 0.27 0.37
SAAR 0.38 0.3 0.39 0.42? 0.44? 0.40 0.44 0.34
UEDIN 0.30 0.24 0.53? 0.52? 0.51? 0.56? 0.45 0.36
> OTHERS 0.36 0.31 0.46 0.41 0.42 0.37 0.33 0.28 0.29
? OTHERS 0.65 0.57 0.72 0.68 0.75 0.60 0.56 0.61 0.56
Table 44: Constituent ranking for the English-German News Task
C
M
U
-G
IM
P
E
L
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-GIMPEL 0.12 0.27 0.21? 0.30 0.21? 0.27? 0.21? 0.22 0.22 0.23
LIMSI 0.22 0.22 0.34 0.29? 0.29? 0.23? 0.29? 0.2 0.21 0.19
LIU 0.18 0.2 0.20? 0.25? 0.17? 0.16? 0.12? 0.28 0.21 0.18
RBMT2 0.54? 0.41 0.62? 0.28 0.33 0.35 0.28 0.61? 0.43 0.47?
RBMT3 0.47 0.47? 0.47? 0.4 0.33 0.32 0.28 0.56? 0.47 0.48?
RBMT4 0.52? 0.57? 0.52? 0.42 0.32 0.27? 0.28 0.47 0.45 0.39
RBMT5 0.49? 0.57? 0.65? 0.42 0.38 0.48? 0.31 0.76? 0.51 0.52?
RBMT6 0.51? 0.54? 0.60? 0.41 0.39 0.40 0.41 0.51? 0.53? 0.51?
SAAR 0.24 0.29 0.17 0.26? 0.22? 0.25 0.20? 0.21? 0.31 0.12
UCL 0.28 0.32 0.29 0.33 0.38 0.32 0.32 0.29? 0.19 0.30
UEDIN 0.1 0.13 0.22 0.2? 0.18? 0.22 0.21? 0.18? 0.15 0.17
> OTHERS 0.37 0.37 0.42 0.32 0.30 0.31 0.28 0.25 0.39 0.35 0.35
? OTHERS 0.77 0.75 0.81 0.58 0.59 0.58 0.51 0.52 0.77 0.69 0.82
Table 45: Constituent ranking for the English-German Europarl Task
102
C
M
U
-S
M
T
C
U
E
D
C
U
E
D
-C
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.19 0.17 0.26 0.38 0.27 0.45 0.32 0.35 0.27 0.26 0.2
CUED 0.21 0.21 0.24 0.24 0.2 0.34 0.25 0.27 0.18 0.26 0.21
CUED-CONTRAST 0.17 0.08 0.12 0.24 0.23? 0.27 0.25 0.21 0.12 0.11 0.26
LIMSI 0.17 0.25 0.26 0.34 0.18? 0.33 0.33 0.31 0.17 0.26 0.23
RBMT3 0.29 0.31 0.35 0.37 0.21 0.4 0.31 0.32 0.43 0.42 0.52?
RBMT4 0.38 0.34 0.54? 0.47? 0.35 0.24 0.32 0.46? 0.37 0.40 0.53
RBMT5 0.24 0.31 0.40 0.33 0.25 0.18 0.31 0.33 0.32 0.28 0.38
RBMT6 0.33 0.29 0.28 0.33 0.26 0.27 0.16 0.26 0.3 0.39 0.41
SAAR 0.26 0.27 0.33 0.26 0.21 0.12? 0.25 0.24 0.20 0.28 0.20
UCB 0.25 0.30 0.23 0.27 0.31 0.27 0.40 0.34 0.28 0.32 0.26
UEDIN 0.19 0.20 0.19 0.24 0.27 0.33 0.31 0.27 0.21 0.21 0.25
UPC 0.1 0.21 0.17 0.2 0.22? 0.28 0.4 0.24 0.29 0.30 0.2
> OTHERS 0.24 0.25 0.28 0.28 0.28 0.23 0.33 0.29 0.3 0.26 0.3 0.32
? OTHERS 0.72 0.76 0.82 0.74 0.64 0.61 0.7 0.70 0.76 0.71 0.76 0.76
Table 46: Constituent ranking for the Spanish-English News Task
C
M
U
-S
M
T
C
U
E
D
D
C
U
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
CMU-SMT 0.2 0.20 0.1 0.1? 0.18? 0.04? 0.18? 0.16 0.17 0.19 0.19
CUED 0.18 0.13 0.19 0.14? 0.12? 0.1? 0.2? 0.13 0.12? 0.22 0.12
DCU 0.15 0.13 0.11 0.09? 0.10? 0.13? 0.09? 0.19 0.15? 0.14 0.15
LIMSI 0.03 0.15 0.16 0.19? 0.18? 0.15? 0.19? 0.19 0.08? 0.07 0.22
RBMT3 0.7? 0.73? 0.59? 0.49? 0.19 0.36 0.22 0.62? 0.55? 0.68? 0.73?
RBMT4 0.55? 0.62? 0.51? 0.55? 0.23 0.22 0.17 0.56? 0.43 0.56? 0.44?
RBMT5 0.60? 0.61? 0.53? 0.61? 0.32 0.38 0.28 0.63? 0.53 0.7? 0.59?
RBMT6 0.52? 0.48? 0.51? 0.49? 0.23 0.26 0.19 0.49? 0.53? 0.52? 0.50?
SAAR 0.14 0.10 0.12 0.15 0.10? 0.12? 0.05? 0.07? 0.14? 0.05 0.18
UCL 0.38 0.37? 0.46? 0.45? 0.28? 0.32 0.29 0.24? 0.38? 0.38? 0.36
UEDIN 0.06 0.14 0.14 0.18 0.15? 0.16? 0.05? 0.16? 0.15 0.10? 0.21
UPC 0.19 0.12 0.20 0.12 0.07? 0.17? 0.09? 0.14? 0.04 0.17 0.14
> OTHERS 0.32 0.33 0.32 0.32 0.17 0.2 0.15 0.17 0.33 0.28 0.34 0.35
? OTHERS 0.85 0.85 0.87 0.85 0.46 0.56 0.47 0.57 0.89 0.65 0.87 0.87
Table 47: Constituent ranking for the Spanish-English Europarl Task
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.20 0.36 0.37 0.24? 0.36 0.32 0.21 0.17 0.27
LIMSI 0.23 0.4 0.46? 0.33 0.39 0.31 0.23 0.17 0.18
RBMT3 0.33 0.35 0.22 0.19? 0.3 0.31 0.49 0.34 0.22
RBMT4 0.30 0.25? 0.25 0.17? 0.17? 0.24 0.19? 0.34 0.30
RBMT5 0.53? 0.42 0.50? 0.41? 0.35 0.50? 0.44 0.37 0.29
RBMT6 0.36 0.35 0.34 0.39? 0.32 0.35 0.36 0.37 0.38
SAAR 0.33 0.36 0.38 0.28 0.24? 0.38 0.29 0.22? 0.24
UCB 0.32 0.29 0.35 0.54? 0.33 0.45 0.31 0.19 0.29
UEDIN 0.29 0.33 0.36 0.42 0.42 0.39 0.45? 0.30 0.44
UPC 0.36 0.42 0.50 0.49 0.42 0.44 0.51 0.21 0.26
> OTHERS 0.34 0.33 0.38 0.39 0.29 0.35 0.36 0.31 0.27 0.29
? OTHERS 0.72 0.69 0.69 0.75 0.57 0.64 0.7 0.65 0.63 0.6
Table 48: Constituent ranking for the English-Spanish News Task
103
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
U
W
CMU-SMT 0.13 0.10? 0.21? 0.2? 0.2? 0.26 0.22 0.13 0.16 0.14
LIMSI 0.17 0.24 0.16? 0.20? 0.13? 0.21 0.06? 0.09 0.14 0.08
RBMT3 0.64? 0.45 0.24 0.30 0.21 0.57? 0.56 0.58? 0.32 0.58?
RBMT4 0.54? 0.52? 0.42 0.26 0.24 0.50? 0.35 0.43 0.47 0.44
RBMT5 0.61? 0.68? 0.46 0.44 0.37 0.64? 0.50 0.63? 0.62? 0.54
RBMT6 0.57? 0.48? 0.39 0.33 0.25 0.52? 0.33 0.54? 0.46 0.46
SAAR 0.19 0.14 0.07? 0.19? 0.09? 0.14? 0.13? 0.17 0.26 0.18
UCL 0.43 0.46? 0.29 0.37 0.38 0.42 0.49? 0.37? 0.48 0.40
UEDIN 0.15 0.11 0.24? 0.20 0.13? 0.17? 0.30 0.14? 0.20 0.20
UPC 0.26 0.05 0.35 0.25 0.16? 0.23 0.34 0.21 0.23 0.10
UW 0.14 0.14 0.17? 0.22 0.23 0.2 0.32 0.20 0.20 0.35
> OTHERS 0.37 0.32 0.28 0.26 0.22 0.23 0.42 0.27 0.35 0.35 0.33
? OTHERS 0.83 0.86 0.56 0.59 0.46 0.57 0.85 0.59 0.82 0.78 0.79
Table 49: Constituent ranking for the English-Spanish Europarl Task
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.33 0.41 0.28?
CU-TECTOMT 0.37 0.42? 0.36
PC-TRANSLATOR 0.34 0.31? 0.32?
UEDIN 0.37? 0.37 0.43?
> OTHERS 0.36 0.34 0.42 0.32
? OTHERS 0.66 0.62 0.67 0.61
Table 50: Constituent ranking for the English-Czech News Task
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.25? 0.33? 0.22?
CU-TECTOMT 0.50? 0.44? 0.45
PC-TRANSLATOR 0.47? 0.3? 0.40
UEDIN 0.39? 0.37 0.39
> OTHERS 0.45 0.31 0.39 0.36
? OTHERS 0.73 0.54 0.61 0.61
Table 51: Constituent ranking for the English-Czech Commentary Task
104
French?English English?French
Europarl YES NO
CMU-XFR 0.61 0.39
CUED 0.83 0.17
DCU 0.88 0.12
LIMSI 0.89 0.11
LIUM-SYS 0.89 0.11
RBMT3 0.54 0.47
RBMT4 0.62 0.38
RBMT5 0.71 0.29
RBMT6 0.54 0.46
SAAR 0.72 0.28
SAAR-C 0.86 0.14
SYSTRAN 0.81 0.19
UCL 0.73 0.27
UEDIN 0.91 0.09
News YES NO
CMU-XFR 0.55 0.45
CUED 0.74 0.26
CUED-C 0.79 0.21
LIMSI 0.81 0.2
LIUM-SYS 0.79 0.21
LI-SYS-C 0.7 0.30
RBMT3 0.63 0.37
RBMT4 0.64 0.36
RBMT5 0.76 0.24
RBMT6 0.66 0.34
SAAR 0.64 0.36
SAAR-C 0.70 0.3
UEDIN 0.72 0.28
Europarl YES NO
LIMSI 0.75 0.26
LIUM-SYS 0.84 0.16
RBMT3 0.49 0.51
RBMT4 0.50 0.5
RBMT5 0.44 0.56
RBMT6 0.35 0.65
SAAR 0.70 0.3
UCL 0.6 0.40
UEDIN 0.75 0.25
News YES NO
LIMSI 0.73 0.27
LIUM-SYS 0.75 0.25
RBMT3 0.59 0.41
RBMT4 0.59 0.41
RBMT5 0.64 0.36
RBMT6 0.58 0.42
SAAR 0.59 0.41
SAAR-C 0.59 0.41
UEDIN 0.63 0.37
XEROX 0.30 0.7
German?English English?German
Europarl YES NO
CMU-XFER 0.53 0.47
LIMSI 0.80 0.2
LIU 0.83 0.17
RBMT2 0.76 0.24
RBMT3 0.74 0.26
RBMT4 0.67 0.33
RBMT5 0.63 0.37
RBMT6 0.63 0.37
SAAR 0.82 0.18
UCL 0.49 0.51
UEDIN 0.86 0.14
News YES NO
CMU-XFER 0.47 0.53
LIMSI 0.73 0.28
LIU 0.64 0.36
RBMT2 0.72 0.28
RBMT3 0.73 0.27
RBMT4 0.74 0.26
RBMT5 0.59 0.41
RBMT6 0.68 0.32
SAAR 0.67 0.33
SAAR-C 0.72 0.28
UEDIN 0.63 0.37
Europarl YES NO
CMU-GIMPEL 0.82? 0.18
LIMSI 0.79? 0.21
LIU 0.79? 0.21
RBMT2 0.69? 0.31
RBMT3 0.57 0.43
RBMT4 0.67? 0.34
RBMT5 0.45 0.55
RBMT6 0.47 0.53
SAAR 0.77? 0.23
UCL 0.61? 0.39
UEDIN 0.85? 0.15
News YES NO
LIMSI 0.56 0.44
LIU 0.49 0.51
RBMT2 0.69 0.31
RBMT3 0.69 0.31
RBMT4 0.75 0.25
RBMT5 0.55 0.45
RBMT6 0.6 0.40
SAAR 0.54 0.46
UEDIN 0.52 0.48
Spanish?English English?Spanish
Europarl YES NO
CMU-SMT 0.88 0.12
CUED 0.86 0.14
DCU 0.85 0.15
LIMSI 0.90 0.1
RBMT3 0.65 0.35
RBMT4 0.56 0.44
RBMT5 0.59 0.41
RBMT6 0.55 0.45
SAAR 0.87 0.13
UCL 0.73 0.27
UEDIN 0.88 0.12
UPC 0.86 0.14
News YES NO
CMU-SMT 0.64 0.37
CUED 0.64 0.36
CUED-C 0.69 0.31
LIMSI 0.68 0.33
RBMT3 0.61 0.39
RBMT4 0.65 0.35
RBMT5 0.59 0.41
RBMT6 0.64 0.37
SAAR 0.7 0.30
UCB 0.64 0.37
UEDIN 0.62 0.38
UPC 0.71 0.29
Europarl YES NO
CMU-SMT 0.80 0.2
LIMSI 0.87 0.13
RBMT3 0.58 0.42
RBMT4 0.6 0.40
RBMT5 0.64 0.37
RBMT6 0.60 0.40
SAAR 0.81 0.19
UCL 0.71 0.29
UEDIN 0.89 0.11
UPC 0.90 0.1
UW 0.79 0.22
News YES NO
CMU-SMT 0.46 0.54
LIMSI 0.53 0.47
RBMT3 0.64 0.36
RBMT4 0.76 0.24
RBMT5 0.6 0.40
RBMT6 0.62 0.38
SAAR 0.64 0.36
UCB 0.57 0.43
UEDIN 0.49 0.51
UPC 0.37 0.63
English?Czech
Commentary YES NO
CU-BOJAR 0.59 0.41
CU-TECTO 0.43 0.57
PC-TRANS 0.51 0.49
UEDIN 0.41 0.59
News YES NO
CU-BOJAR 0.54 0.46
CU-TECTO 0.42 0.58
PC-TRANS 0.52 0.48
UEDIN 0.44 0.56
Table 52: Yes/No Acceptability of Constituents
105
BBN-CMB-DE
BBN-CMB-FR
CMU-CMB-FR
CMU-SMT-ES
CMU-XFR-DE
CMU-XFR-FR
CUED-C-ES
CUED-C-FR
CUED-ES
CUED-FR
DCU-CZ
LIMSI-DE
LIMSI-ES
LIMSI-FR
LIU-DE
LIUM-S-C-FR
LIUM-SYS-FR
MLOGIC-HU
RBMT2-DE
RBMT3-DE
RBMT3-ES
RBMT3-FR
RBMT4-DE
RBMT4-ES
RBMT4-FR
RBMT5-DE
RBMT5-ES
RBMT5-FR
RBMT6-DE
RBMT6-ES
RBMT6-FR
SAAR-C-DE
SAAR-C-FR
SAAR-DE
SAAR-ES
SAAR-FR
UCB-ES
UED-CMB-DE
UED-CMB-FR
UED-CMB-XX
UED-CZ
UED-DE
UED-ES
UED-FR
UED-HU
UMD-CZ
UPC-ES
B
B
N
-C
M
B
-D
E
.50
.40
1
.20
.50
1
?
.64
1
.73
.31
.69
?
.71
.38
.70
.60
.60
.80
.77
?
.60
.63
.89
?
1
.57
.62
.83
.60
.17
.57
.55
.41
.70
.58
.71
.82
.75
.40
.33
1
.25
.36
.85
?
.50
.40
.60
B
B
N
-C
M
B
-F
R
.38
.14
.38
.09
?
.13
?
.33
.63
.20
.25
.13
.13
.60
.31
.46
.43
.27
.13
.67
.25
.46
.33
.38
.22
.43
.07
?
.33
.42
.50
.36
.25
.46
.40
.06
?
.30
.33
.50
.80
.14
?
.20
.67
.33
.25
.13
.42
C
M
U
-C
M
B
-F
R
.60
.71
.54
.09
?
.60
.29
.13
.57
.33
.23
.33
.33
.46
.44
.58
.40
.20
.54
.27
.50
.67
.11
.14
.44
.11
.25
.60
.09
?
.40
.29
.29
.25
.56
.20
.56
.25
.14
.38
.11
?
.11
.22
.36
.44
C
M
U
-S
M
T-E
S
.50
.31
.50
.17
.75
.46
.64
.43
.25
.54
.60
.83
?
.40
.50
.17
.14
.46
.50
.64
.73
.80
.67
.64
.33
.33
.67
.46
.50
.57
.50
.39
.36
.64
.70
.17
.50
.33
.14
.25
.33
.13
.38
.43
C
M
U
-X
F
R
-D
E
.60
.82
?
.91
?
.50
.78
.56
.89
?
.42
.73
?
.55
.27
.33
.88
?
.57
.73
?
.92
?
.75
.80
?
.82
.75
?
.67
.75
.86
.78
.91
?
.89
?
.79
?
.81
?
.80
.80
?
.67
.90
?
.64
.73
?
.80
.64
.33
1
.83
.11
.20
1
?
.90
?
.33
.50
.85
?
C
M
U
-X
F
R
-F
R
.50
.75
?
.67
.11
.70
.80
?
.88
?
.71
.50
.75
.50
.60
.71
.67
.50
.67
.60
.40
.43
.60
.67
.29
.25
.64
.75
.38
.75
.38
.50
.67
.18
.57
.44
.73
.33
.50
.75
.80
.69
.64
.50
.33
1
C
U
E
D
-C
-E
S
0
.56
.59
.22
.20
.18
.21
.19
0
.29
.15
.47
.14
.39
.50
.25
.39
.36
.43
.46
.33
.31
.56
.50
.07
?
.73
?
.31
.42
.42
.43
.50
.42
.40
.27
.18
.50
.38
.29
.10
?
.22
.33
.43
.33
C
U
E
D
-C
-F
R
.29
.13
.38
.39
.11
?
.10
?
.73
.50
.25
.36
.57
.40
.36
.11
?
.70
?
.60
.40
.58
.36
.56
.20
.39
.50
.60
.10
?
.50
.50
.30
.55
.46
.33
0
.17
.20
.39
.13
.88
0
.29
.39
.36
0
.40
.50
C
U
E
D
-E
S
.80
.29
.18
.25
0
.29
.38
.64
.25
.20
.14
.78
.25
.36
.88
.25
.36
.39
.69
.71
.58
.83
?
.67
.30
.50
.60
.47
.67
.43
.40
.20
.38
.50
.50
.57
.25
.50
.50
.08
?
0
.57
.20
.50
.67
C
U
E
D
-F
R
.18
.25
.22
.43
.09
?
.29
.69
.38
.27
.11
.10
?
.47
.33
.64
.15
.50
.38
.57
.50
.42
.43
.33
.50
.22
.46
.46
.33
.58
.43
.50
.56
.18
.44
.25
.38
.20
.25
0
.33
.13
.44
0
.10
?
.50
D
C
U
-C
Z
.39
.75
.69
.67
.36
.50
.90
?
.46
.58
1
.44
.22
.91
?
.56
.60
.85
?
1
.77
?
.78
.86
.75
.62
.57
.83
?
.30
.55
.80
.67
.77
?
.80
?
.79
?
.50
.33
.80
?
.89
?
.73
.17
.50
.60
.50
.54
.78
.80
.13
.38
.39
L
IM
S
I-D
E
.17
.63
.67
.39
.27
1
.71
.43
.80
.78
.38
.33
.57
.50
.77
?
1
?
.29
.50
.78
.50
.67
.71
.88
.71
?
.33
.57
.89
?
.30
.60
.80
.43
.78
?
.27
.36
.17
.44
1
1
.13
.50
.67
.50
.40
.30
.43
L
IM
S
I-E
S
.08
?
.40
.67
.30
.17
.25
.54
.60
.57
.80
?
.56
.50
.50
.43
.55
.50
.33
.43
.10
?
.67
.50
.63
.39
.69
.29
.75
.50
.29
.60
.82
.63
.20
.22
.55
.33
.29
.25
1
.75
.17
.25
.57
.50
.64
.50
L
IM
S
I-F
R
.14
.38
.18
.08
?
0
.40
.27
.36
.11
.35
.09
?
.29
.25
.23
.63
.30
.25
.38
.56
.36
.44
.22
.25
.50
.10
?
.31
.20
.20
.56
.40
.17
.20
.38
.50
.36
.33
1
.33
.50
.25
.08
?
.27
.24
.25
.29
.50
L
IU
-D
E
.50
.55
.33
.60
.40
.86
.89
?
.38
.44
.22
.25
.57
.54
.73
?
.80
1
.22
.55
.86
.83
.67
.67
.67
.25
.89
?
.71
.50
.60
.58
.60
.60
.60
.75
.71
.67
.33
1
1
.22
.22
.43
.67
.40
.69
L
IU
M
-S-C
-F
R
.20
.29
.17
.50
0
.14
.46
0
.43
.21
.20
.08
?
.18
.13
.09
?
.25
.11
?
.18
.39
.50
.27
.27
.46
.50
.13
?
.31
.55
.33
.46
.50
.42
.25
.33
.59
?
.33
.33
.33
.50
.25
.22
.18
0
.44
.60
L
IU
M
-S-F
R
.20
.36
.20
.67
.08
?
.11
.50
.20
.13
.62
.15
?
0
.38
.50
.20
.25
.14
.42
.36
.17
.43
.13
.60
.30
.25
.33
.52
.25
.18
.43
.39
.29
.20
.44
.16
?
.44
.50
.60
.08
?
.17
?
.23
.17
0
.16
?
.46
M
L
O
G
IC
-H
U
.40
.75
.60
.71
.25
.50
.63
.60
.75
.50
.43
.67
.50
.89
?
.71
.88
.67
.44
.86
?
1
?
.50
.75
.67
.83
.63
.63
.54
.63
.67
.50
.86
.33
.63
.33
.75
1
.40
1
1
.44
.25
.80
R
B
M
T2-D
E
.33
.39
.46
.07
?
.33
.46
.33
.64
.38
.08
?
.50
.36
.50
.33
.55
.58
.13
.17
.67
.38
.38
.70
.55
.22
.46
.46
.46
.43
.17
.10
.42
.43
.67
.29
.33
.40
.40
1
.10
?
.31
.54
.36
.14
.07
?
.56
R
B
M
T3-D
E
.08
?
.75
.64
.50
.18
.20
.64
.64
.31
.43
.22
.11
.80
?
.44
.36
.62
.64
.33
.67
.55
.46
.35
.90
?
.40
.14
.80
?
.40
.38
.38
.60
.25
.53
.44
.31
.56
.63
.17
.80
.60
.22
.55
.60
.17
.20
.67
R
B
M
T3-E
S
.40
.55
.50
.18
.13
?
.50
.36
.22
.23
.50
.14
.42
.33
.50
.14
.50
.83
.44
.33
.27
.39
.64
.50
.50
.36
.33
.31
.27
.46
.33
.09
.43
.23
.50
.46
.29
.75
.75
.25
.25
.36
.50
0
.20
.78
R
B
M
T3-F
R
.25
.58
.22
.27
.33
.29
.27
.40
.29
.33
.13
.33
.43
.50
.17
.55
.50
0
.56
.31
.62
.75
.63
.33
.13
.44
.38
.27
.60
.09
?
.57
.63
.17
.50
.38
.40
1
.11
.25
.73
.50
.25
.53
R
B
M
T4-D
E
.11
?
.50
.78
.20
.40
.67
.62
.25
.14
.15
.29
.13
.78
.25
.73
.75
.50
.55
.18
.22
.43
.50
.57
.29
.25
.50
.63
.42
.33
.25
.67
.58
.50
.60
.67
.14
.38
.50
.75
0
.22
.63
R
B
M
T4-E
S
.44
.57
.22
.14
.17
.38
.38
.08
?
.56
.14
.13
.31
.50
.33
.46
.30
0
.10
.10
?
.50
.38
.56
.29
.43
.25
.33
.38
.25
.25
.46
.33
.14
.09
.25
.33
.11
1
1
0
.09
?
.39
.50
0
.38
.38
R
B
M
T4-F
R
.43
.29
.22
.27
.11
.57
.22
.27
.33
.33
.08
?
.14
?
.23
.33
.22
.25
.60
.50
.46
.30
.50
.47
.57
.41
.67
.46
.58
.54
.43
.38
.36
.63
.14
.86
.36
.25
.60
.39
.50
.50
.33
.20
.38
R
B
M
T5-D
E
.23
.71
?
.67
.50
.09
?
.50
.38
.80
?
.40
.56
.60
.44
.57
.80
?
.63
.75
?
.75
.25
.67
.43
.83
.75
.29
.57
.17
.50
1
?
.13
.67
.53
.40
.71
.20
.67
.47
.38
.86
.25
.27
.25
.70
.67
.20
.38
.50
R
B
M
T5-E
S
.17
.67
.75
.27
.11
?
.27
.73
?
.50
.13
.46
.27
.43
.62
.11
?
.62
.50
.22
.36
.10
?
.43
.44
.43
.63
.46
.36
.50
.29
.44
.17
.57
.27
.29
.25
.60
.11
.33
.60
.67
.44
0
.30
.43
.17
.25
.58
R
B
M
T5-F
R
.30
.42
.10
.11
.14
?
.17
.09
?
.42
.30
.39
.20
.11
?
.33
.50
.21
.27
.29
.15
.40
.67
.38
.71
.33
.17
0
.40
.50
.40
.20
.25
.56
.07
?
.50
.31
.14
.50
.22
.40
.57
.54
.29
R
B
M
T6-D
E
.67
.25
.91
?
.36
.06
?
.50
.54
.70
.47
.53
.33
.50
.57
.70
.40
.67
.58
.25
.39
.38
.69
.73
.50
.50
.46
.50
.43
.50
.13
.50
.46
.62
.50
.46
.50
.46
.40
.33
.80
.29
.20
.57
.50
.29
.86
R
B
M
T6-E
S
.29
.55
.60
.50
.25
.25
.46
.22
.33
.08
?
.40
.20
.44
.55
.64
.38
.29
.63
.40
.30
.30
.25
.57
.33
.22
.60
.63
.67
.64
.42
.38
.67
.71
.46
0
.50
.22
.25
.50
.33
0
.13
.67
R
B
M
T6-F
R
.36
.63
.57
.43
.20
?
.63
.50
.83
.43
.36
.10
?
.18
.40
.17
.43
.50
.39
.67
.30
.39
.73
?
.38
.63
.38
.27
.83
.50
.38
.17
.29
.67
.22
.40
.33
.38
.33
.50
.13
.25
.63
.33
.14
.25
S
A
A
R
-C
-D
E
.41
.55
.67
.30
.25
.50
.50
.54
.60
.40
.14
?
.57
.25
.83
.30
.50
.62
.25
.70
.50
.47
.36
.50
.46
.55
.33
.14
.75
.36
.27
.59
.21
.13
.50
.36
.42
.33
1
.20
.33
.54
.88
.69
0
.44
S
A
A
R
-C
-F
R
.20
.40
.50
.54
0
.33
.50
.33
.60
.44
.38
0
.60
.60
.40
.42
.43
.17
.50
.33
.64
.13
.67
.67
.25
.46
.22
.31
.42
.27
.79
.18
.56
.18
.17
.60
.25
.09
.86
.50
.18
.44
.44
S
A
A
R
-D
E
.33
.77
?
.63
.43
.14
.64
.50
.91
?
.44
.82
.58
.73
.67
.50
.40
.67
.70
.33
.43
.44
.36
.67
.67
.86
.86
.60
.50
.86
?
.29
.56
.44
.53
.73
.33
.83
?
.31
.67
.86
?
.43
.33
.83
?
.56
.17
.78
.55
S
A
A
R
-E
S
.29
.60
.44
.18
.07
?
.29
.40
.67
.17
.44
.10
?
.46
.27
.50
.13
.12
?
.50
.14
.33
.46
.62
.36
.25
.55
.14
.17
.25
.40
.39
.17
.50
.42
.44
.47
.50
.11
.33
.11
?
.33
.39
.18
.08
.33
S
A
A
R
-F
R
.18
.44
.60
.30
.20
.56
.73
.60
.50
.58
0
.50
.33
.64
.29
.42
.63
?
.67
.57
.33
.25
.38
.42
.58
.43
.41
.30
.54
.50
.29
.33
.36
.36
.17
?
.43
.33
.40
.60
.33
.27
.20
.64
?
.31
.25
.58
U
C
B
-E
S
.33
.44
.33
.18
.18
.55
.62
.14
.63
.18
.44
.57
.44
.33
.67
.44
.25
.56
.25
.46
.60
.50
.56
.83
.63
.56
.57
.36
.46
.50
.50
.75
.50
.56
.58
.88
.71
.57
.14
.42
.67
.44
.31
U
E
D
-C
M
B
-D
E
.20
.67
.75
1
.67
.33
1
.88
.75
1
.83
.71
.75
.67
.67
1
.44
.60
.67
.57
1
1
.78
1
.67
.44
1
.60
1
?
1
.67
1
.83
1
.40
.13
.67
.75
?
.25
.50
.40
1
.50
.75
.50
U
E
D
-C
M
B
-F
R
.67
1
.57
.50
.50
.13
.50
.40
.33
.50
.25
.40
.20
.25
.50
.40
.50
.14
.20
.33
.33
1
.67
.40
.33
.33
.29
.38
.33
.33
.50
U
E
D
-C
M
B
-X
X
.50
.67
.25
.13
1
.50
.50
.40
.25
.20
.40
.13
.33
.40
.75
.33
1
.50
.50
.60
.50
0
.67
.56
.14
0
.38
0
.33
.25
U
E
D
-C
Z
.75
.79
?
.89
?
.86
.56
.83
.71
.77
?
.92
?
1
?
.20
.88
.67
.63
.67
1
.69
?
.60
.80
?
.80
.75
.67
.86
.75
?
.62
.46
.56
.67
.71
.78
.88
.67
.64
.43
.78
?
.73
.86
.50
1
1
?
.55
1
?
.80
?
0
.46
.90
?
U
E
D
-D
E
.27
.80
.78
1
.20
.80
?
.71
1
?
.67
.31
.25
.75
.92
?
.56
.67
.83
?
.62
.44
.67
.67
.63
.82
?
.25
.75
1
?
.40
.40
.75
.75
.23
1
.67
.67
.60
.57
.50
1
.36
.64
.67
.11
.25
.67
U
E
D
-E
S
.15
?
.17
.56
.42
0
.31
.56
.46
.50
.88
.11
.33
.29
.46
.29
.67
.69
.31
.36
.46
.27
.50
.54
.33
.30
.60
.43
.29
.50
.38
.13
.14
.17
?
.23
0
.17
.40
.83
.80
0
.27
.46
0
.13
U
E
D
-F
R
.50
.56
.75
.67
0
.18
.56
.50
.14
.44
.20
.17
.42
.53
.22
.64
.58
.44
.57
.30
.50
.38
.17
.42
.50
.22
.57
.31
.38
.44
.42
.23
.40
.33
.46
.39
.33
.75
1
.10
?
.11
.46
.31
.38
U
E
D
-H
U
1
.75
.67
.88
.67
.50
.75
1
?
.80
1
?
.38
.60
.83
.75
1
1
?
.86
?
.50
.86
.75
1
?
.80
1
?
1
?
.80
.80
.83
1
1
1
?
1
.80
.82
.67
1
1
.83
.50
.33
.67
.86
?
.67
1
?
1
.83
.40
U
M
D
-C
Z
.40
.50
.64
.50
.50
.67
.29
.50
.50
.80
?
.25
.40
.27
.71
.60
.44
.79
?
.79
?
.70
.80
.63
.67
.50
.63
.44
.63
.60
.43
.75
.64
.91
?
.56
.11
.54
.67
.44
.25
.67
1
.27
.50
.88
.69
.17
.62
U
P
C
-E
S
.40
.42
.44
.21
.15
?
.44
.38
.25
.38
.54
.43
.30
.50
.23
.40
.55
.33
.33
.22
.47
.13
.50
.50
.38
.25
.43
.14
.33
.63
.56
.44
.36
.53
.25
.39
.50
.50
.50
.10
?
.33
.63
.50
.40
.31
>
O
T
H
E
R
S
.29
.54
.51
.41
.15
.35
.51
.52
.43
.51
.25
.34
.39
.55
.33
.57
.57
.3
.5
.44
.52
.49
.48
.56
.50
.35
.46
.57
.39
.50
.49
.46
.49
.32
.51
.45
.4
.22
.56
.58
.19
.28
.53
.48
.11
.3
.52
?
O
T
H
E
R
S
.41
.7
.66
.55
.28
.44
.67
.64
.57
.65
.38
.47
.54
.7
.45
.71
.70
.39
.63
.55
.63
.61
.57
.7
.64
.49
.60
.71
.51
.62
.62
.57
.62
.45
.68
.58
.55
.33
.70
.72
.29
.40
.65
.62
.19
.43
.63
Table
53:
S
entence-levelranking
for
the
A
ll-E
nglish
N
ew
s
Task.
106
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 1?28,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Findings of the 2009 Workshop on Statistical Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb cs jhu edu
Philipp Koehn
University of Edinburgh
pkoehn inf ed ac uk
Christof Monz
University of Amsterdam
christof science uva nl
Josh Schroeder
University of Edinburgh
j schroeder ed ac uk
Abstract
This paper presents the results of the
WMT09 shared tasks, which included a
translation task, a system combination
task, and an evaluation task. We con-
ducted a large-scale manual evaluation of
87 machine translation systems and 22
system combination entries. We used the
ranking of these systems to measure how
strongly automatic metrics correlate with
human judgments of translation quality,
for more than 20 metrics. We present a
new evaluation technique whereby system
output is edited and judged for correctness.
1 Introduction
This paper presents the results of the shared tasks
of the 2009 EACL Workshop on Statistical Ma-
chine Translation, which builds on three previ-
ous workshops (Koehn and Monz, 2006; Callison-
Burch et al, 2007; Callison-Burch et al, 2008).
There were three shared tasks this year: a transla-
tion task between English and five other European
languages, a task to combine the output of multiple
machine translation systems, and a task to predict
human judgments of translation quality using au-
tomatic evaluation metrics. The performance on
each of these shared task was determined after a
comprehensive human evaluation.
There were a number of differences between
this year?s workshop and last year?s workshop:
? Larger training sets ? In addition to annual
increases in the Europarl corpus, we released
a French-English parallel corpus verging on 1
billion words. We also provided large mono-
lingual training sets for better language mod-
eling of the news translation task.
? Reduced number of conditions ? Previ-
ous workshops had many conditions: 10
language pairs, both in-domain and out-of-
domain translation, and three types of man-
ual evaluation. This year we eliminated
the in-domain Europarl test set and defined
sentence-level ranking as the primary type of
manual evaluation.
? Editing to evaluate translation quality ?
Beyond ranking the output of translation sys-
tems, we evaluated translation quality by hav-
ing people edit the output of systems. Later,
we asked annotators to judge whether those
edited translations were correct when shown
the source and reference translation.
The primary objectives of this workshop are to
evaluate the state of the art in machine transla-
tion, to disseminate common test sets and pub-
lic training data with published performance num-
bers, and to refine evaluation methodologies for
machine translation. All of the data, translations,
and human judgments produced for our workshop
are publicly available.1 We hope they form a
valuable resource for research into statistical ma-
chine translation, system combination, and auto-
matic evaluation of translation quality.
2 Overview of the shared translation and
system combination tasks
The workshop examined translation between En-
glish and five other languages: German, Spanish,
French, Czech, and Hungarian. We created a test
set for each language pair by translating news-
paper articles. We additionally provided training
data and a baseline system.
1http://statmt.org/WMT09/results.html
1
2.1 Test data
The test data for this year?s task was created by
hiring people to translate news articles that were
drawn from a variety of sources during the pe-
riod from the end of September to mid-October
of 2008. A total of 136 articles were selected, in
roughly equal amounts from a variety of Czech,
English, French, German, Hungarian, Italian and
Spanish news sites:2
Hungarian: hvg.hu (10), Napi (2), MNO (4),
Ne?pszabadsa?g (4)
Czech: iHNed.cz (3), iDNES.cz (4), Li-
dovky.cz (3), aktua?lne?.cz (2), Novinky (1)
French: dernieresnouvelles (1), Le Figaro (2),
Les Echos (4), Liberation (4), Le Devoir (9)
Spanish: ABC.es (11), El Mundo (12)
English: BBC (11), New York Times (6), Times
of London (4),
German: Su?ddeutsche Zeitung (3), Frankfurter
Allgemeine Zeitung (3), Spiegel (8), Welt (3)
Italian: ADN Kronos (5), Affari Italiani (2),
ASCA (1), Corriere della Sera (4), Il Sole 24
ORE (1), Il Quotidiano (1), La Republica (8)
Note that Italian translation was not one of this
year?s official translation tasks.
The translations were created by the members
of EuroMatrix consortium who hired a mix of
professional and non-professional translators. All
translators were fluent or native speakers of both
languages. Although we made efforts to proof-
read all translations, many sentences still contain
minor errors and disfluencies. All of the transla-
tions were done directly, and not via an interme-
diate language. For instance, each of the 20 Hun-
garian articles were translated directly into Czech,
English, French, German, Italian and Spanish.
The total cost of creating the test sets consisting
of roughly 80,000 words across 3027 sentences in
seven languages was approximately 31,700 euros
(around 39,800 dollars at current exchange rates,
or slightly more than $0.08/word).
Previous evaluations additionally used test sets
drawn from the Europarl corpus. Our rationale be-
hind discontinuing the use of Europarl as a test set
was that it overly biases towards statistical systems
that were trained on this particular domain, and
2For more details see the XML test files. The docid
tag gives the source and the date for each document in the
test set, and the origlang tag indicates the original source
language.
that European Parliament proceedings were less of
general interest than news stories. We focus on a
single task since the use of multiple test sets in the
past spread our resources too thin, especially in the
manual evaluation.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
train language models, and development sets to
tune parameters. Some statistics about the train-
ing materials are given in Figure 1.
109 word parallel corpus
To create the large French-English parallel cor-
pus, we conducted a targeted web crawl of bilin-
gual web sites. These sites came from a variety of
sources including the Canadian government, the
European Union, the United Nations, and other
international organizations. The crawl yielded on
the order of 40 million files, consisting of more
than 1TB of data. Pairs of translated documents
were identified using a set of simple heuristics to
transform French URLs into English URLs (for in-
stance, by replacing fr with en). Documents that
matched were assumed to be translations of each
other.
All HTML and PDF documents were converted
into plain text, which yielded 2 million French
files paired with their English equivalents. Text
files were split so that they contained one sen-
tence per line and had markers between para-
graphs. They were sentence-aligned in batches of
10,000 document pairs, using a sentence aligner
that incorporates IBM Model 1 probabilities in ad-
dition to sentence lengths (Moore, 2002). The
document-aligned corpus contained 220 million
segments with 2.9 billion words on the French side
and 215 million segments with 2.5 billion words
on the English side. After sentence alignment,
there were 177 million sentence pairs with 2.5 bil-
lion French words and 2.2 billion English words.
The sentence-aligned corpus was cleaned to re-
move sentence pairs which consisted only of num-
bers or paragraph markers, or where the French
and English sentences were identical. The later
step helped eliminate documents that were not
actually translated, which was necessary because
we did not perform language identification. After
cleaning, the parallel corpus contained 105 million
sentence pairs with 2 billion French words and 1.8
billion English words.
2
Europarl Training Corpus
Spanish? English French? English German? English
Sentences 1,411,589 1,428,799 1,418,115
Words 40,067,498 41,042,070 44,692,992 40,067,498 39,516,645 37,431,872
Distinct words 154,971 108,116 129,166 107,733 320,180 104,269
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 74,512 64,223 82,740 79,930
Words 2,052,186 1,799,312 1,831,149 1,560,274 2,051,369 1,977,200 1,733,865 1,891,559
Distinct words 56,578 41,592 46,056 38,821 92,313 43,383 105,280 41,801
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
Hunglish Training Corpus CzEng Training Corpus
Hungarian? English
Sentences 1,517,584
Words 26,114,985 31,467,693
Distinct words 717,198 192,901
Czech? English
Sentences 1,096,940
Words 15,336,783 17,909,979
Distinct words 339,683 129,176
Europarl Language Model Data
English Spanish French German
Sentence 1,658,841 1,607,419 1,676,435 1,713,715
Words 44,983,136 45,382,287 50,577,097 41,457,414
Distinct words 117,577 162,604 138,621 348,197
News Language Model Data
English Spanish French German Czech Hungarian
Sentence 21,232,163 1,626,538 6,722,485 10,193,376 5,116,211 4,209,121
Words 504,094,159 48,392,418 167,204,556 185,639,915 81,743,223 86,538,513
Distinct words 1,141,895 358,664 660,123 1,668,387 929,318 1,313,578
News Test Set
English Spanish French German Czech Hungarian Italian
Sentences 2525
Words 65,595 68,092 72,554 62,699 55,389 54,464 64,906
Distinct words 8,907 10,631 10,609 12,277 15,387 16,167 11,046
News System Combination Development Set
English Spanish French German Czech Hungarian Italian
Sentences 502
Words 11,843 12,499 12,988 11,235 9,997 9,628 11,833
Distinct words 2,940 3,176 3,202 3,471 4,121 4,133 3,318
Figure 1: Statistics for the training and test sets used in the translation task. The number of words is
based on the provided tokenizer and the number of distinct words is the based on lowercased tokens.
3
In addition to cleaning the sentence-aligned par-
allel corpus we also de-duplicated the corpus, re-
moving all sentence pairs that occured more than
once in the parallel corpus. Many of the docu-
ments gathered in our web crawl were duplicates
or near duplicates, and a lot of the text is repeated,
as with web site navigation. We further elimi-
nated sentence pairs that varied from previous sen-
tences by only numbers, which helped eliminate
template web pages such as expense reports. We
used a Bloom Filter (Talbot and Osborne, 2007) to
do de-duplication, so it may have discarded more
sentence pairs than strictly necessary. After de-
duplication, the parallel corpus contained 28 mil-
lion sentence pairs with 0.8 billion French words
and 0.7 billion English words.
Monolingual news corpora
We have crawled the news sources that were the
basis of our test sets (and a few more additional
sources) since August 2007. This allowed us to
assemble large corpora in the target domain to be
mainly used as training data for language mod-
eling. We collected texts from the beginning of
our data collection period to one month before the
test set period, segmented these into sentences and
randomized the order of the sentences to obviate
copyright concerns.
2.3 Baseline system
To lower the barrier of entry for newcomers to the
field, we provided Moses, an open source toolkit
for phrase-based statistical translation (Koehn et
al., 2007). The performance of this baseline sys-
tem is similar to the best submissions in last year?s
shared task. Twelve participating groups used the
Moses toolkit for the development of their system.
2.4 Submitted systems
We received submissions from 22 groups from
20 institutions, as listed in Table 1, a similar
turnout to last year?s shared task. Of the 20
groups that participated with regular system sub-
missions in last year?s shared task, 12 groups re-
turned this year. A major hurdle for many was
a DARPA/GALE evaluation that occurred at the
same time as this shared task.
We also evaluated 7 commercial rule-based MT
systems, and Google?s online statistical machine
translation system. We note that Google did not
submit an entry itself. Its entry was created by
the WMT09 organizers using Google?s online sys-
tem.3 In personal correspondence, Franz Och
clarified that the online system is different from
Google?s research system in that it runs at faster
speeds at the expense of somewhat lower transla-
tion quality. On the other hand, the training data
used by Google is unconstrained, which means
that it may have an advantage compared to the re-
search systems evaluated in this workshop, since
they were trained using only the provided materi-
als.
2.5 System combination
In total, we received 87 primary system submis-
sions along with 42 secondary submissions. These
were made available to participants in the sys-
tem combination shared task. Based on feedback
that we received on last year?s system combina-
tion task, we provided two additional resources to
participants:
? Development set: We reserved 25 articles
to use as a dev set for system combina-
tion (details of the set are given in Table
1). These were translated by all participating
sites, and distributed to system combination
participants along with reference translations.
? n-best translations: We requested n-best
lists from sites whose systems could produce
them. We received 25 100-best lists accom-
panying the primary system submissions, and
5 accompanying the secondary system sub-
missions.
In addition to soliciting system combination en-
tries for each of the language pairs, we treated sys-
tem combination as a way of doing multi-source
translation, following Schroeder et al (2009). For
the multi-source system combination task, we pro-
vided all 46 primary system submissions from any
language into English, along with an additional 32
secondary systems.
Table 2 lists the six participants in the system
combination task.
3 Human evaluation
As with past workshops, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention
3http://translate.google.com
4
ID Participant
CMU-STATXFER Carnegie Mellon University?s statistical transfer system (Hanneman et al, 2009)
COLUMBIA Columbia University (Carpuat, 2009)
CU-BOJAR Charles University Bojar (Bojar et al, 2009)
CU-TECTOMT Charles University Tectogramatical MT (Bojar et al, 2009)
DCU Dublin City University (Du et al, 2009)
EUROTRANXP commercial MT provider from the Czech Republic
GENEVA University of Geneva (Wehrli et al, 2009)
GOOGLE Google?s production system
JHU Johns Hopkins University (Li et al, 2009)
JHU-TROMBLE Johns Hopkins University Tromble (Eisner and Tromble, 2006)
LIMSI LIMSI (Allauzen et al, 2009)
LIU Linko?ping University (Holmqvist et al, 2009)
LIUM-SYSTRAN University of Le Mans / Systran (Schwenk et al, 2009)
MORPHO Morphologic (Nova?k, 2009)
NICT National Institute of Information and Comm. Tech., Japan (Paul et al, 2009)
NUS National University of Singapore (Nakov and Ng, 2009)
PCTRANS commercial MT provider from the Czech Republic
RBMT1-5 commercial systems from Learnout&Houspie, Lingenio, Lucy, PROMT, SDL
RWTH RWTH Aachen (Popovic et al, 2009)
STUTTGART University of Stuttgart (Fraser, 2009)
SYSTRAN Systran (Dugast et al, 2009)
TALP-UPC Universitat Politecnica de Catalunya, Barcelona (R. Fonollosa et al, 2009)
UEDIN University of Edinburgh (Koehn and Haddow, 2009)
UKA University of Karlsruhe (Niehues et al, 2009)
UMD University of Maryland (Dyer et al, 2009)
USAAR University of Saarland (Federmann et al, 2009)
Table 1: Participants in the shared translation task. Not all groups participated in all language pairs.
ID Participant
BBN-COMBO BBN system combination (Rosti et al, 2009)
CMU-COMBO Carnegie Mellon University system combination (Heafield et al, 2009)
CMU-COMBO-HYPOSEL CMU system comb. with hyp. selection (Hildebrand and Vogel, 2009)
DCU-COMBO Dublin City University system combination
RWTH-COMBO RWTH Aachen system combination (Leusch et al, 2009)
USAAR-COMBO University of Saarland system combination (Chen et al, 2009)
Table 2: Participants in the system combination task.
5
Language Pair Sentence Ranking Edited Translations Yes/No Judgments
German-English 3,736 1,271 4,361
English-German 3,700 823 3,854
Spanish-English 2,412 844 2,599
English-Spanish 1,878 278 837
French-English 3,920 1,145 4,491
English-French 1,968 332 1,331
Czech-English 1,590 565 1,071
English-Czech 7,121 2,166 9,460
Hungarian-English 1,426 554 1,309
All-English 4,807 0 0
Multisource-English 2,919 647 2184
Totals 35,786 8,655 31,524
Table 3: The number of items that were judged for each task during the manual evaluation.
that automatic measures are an imperfect substi-
tute for human assessment of translation quality.
Therefore, we define the manual evaluation to be
primary, and use the human judgments to validate
automatic metrics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct it on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partic-
ipants, interested volunteers, and a small number
of paid annotators. More than 160 people partic-
ipated in the manual evaluation, with 100 people
putting in more than an hour?s worth of effort, and
30 putting in more than four hours. A collective
total of 479 hours of labor was invested.
We asked people to evaluate the systems? output
in two different ways:
? Ranking translated sentences relative to each
other. This was our official determinant of
translation quality.
? Editing the output of systems without dis-
playing the source or a reference translation,
and then later judging whether edited transla-
tions were correct.
The total number of judgments collected for the
different modes of annotation is given in Table 3.
In all cases, the output of the various translation
outputs were judged on equal footing; the output
of system combinations was judged alongside that
of the individual system, and the constrained and
unconstrained systems were judged together.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the in-
structions simple:
Rank translations from Best to Worst rel-
ative to the other choices (ties are al-
lowed).
In our the manual evaluation, annotators were
shown at most five translations at a time. For most
language pairs there were more than 5 systems
submissions. We did not attempt to get a com-
plete ordering over the systems, and instead relied
on random selection and a reasonably large sample
size to make the comparisons fair.
Relative ranking is our official evaluation met-
ric. Individual systems and system combinations
are ranked based on how frequently they were
judged to be better than or equal to any other sys-
tem. The results of this are reported in Section 4.
Appendix A provides detailed tables that contain
pairwise comparisons between systems.
3.2 Editing machine translation output
We experimented with a new type of evaluation
this year where we asked judges to edit the output
of MT systems. We did not show judges the refer-
ence translation, which makes our edit-based eval-
uation different than the Human-targeted Trans-
lation Error Rate (HTER) measure used in the
DARPA GALE program (NIST, 2008). Rather
than asking people to make the minimum number
of changes to the MT output in order capture the
same meaning as the reference, we asked them to
6
edit the translation to be as fluent as possible with-
out seeing the reference. Our hope was that this
would reflect people?s understanding of the out-
put.
The instructions that we gave our judges were
the following:
Correct the translation displayed, mak-
ing it as fluent as possible. If no correc-
tions are needed, select ?No corrections
needed.? If you cannot understand the
sentence well enough to correct it, select
?Unable to correct.?
Each translated sentence was shown in isolation
without any additional context. A screenshot is
shown in Figure 2.
Since we wanted to prevent judges from see-
ing the reference before editing the translations,
we split the test set between the sentences used
in the ranking task and the editing task (because
they were being conducted concurrently). More-
over, annotators edited only a single system?s out-
put for one source sentence to ensure that their un-
derstanding of it would not be influenced by an-
other system?s output.
3.3 Judging the acceptability of edited output
Halfway through the manual evaluation period, we
stopped collecting edited translations, and instead
asked annotators to do the following:
Indicate whether the edited transla-
tions represent fully fluent and meaning-
equivalent alternatives to the reference
sentence. The reference is shown with
context, the actual sentence is bold.
In addition to edited translations, unedited items
that were either marked as acceptable or as incom-
prehensible were also shown. Judges gave a sim-
ple yes/no indication to each item. A screenshot is
shown in Figure 3.
3.4 Inter- and Intra-annotator agreement
In order to measure intra-annotator agreement
10% of the items were repeated and evaluated
twice by each judge. In order to measure inter-
annotator agreement 40% of the items were ran-
domly drawn from a common pool that was shared
across all annotators so that we would have items
that were judged by multiple annotators.
INTER-ANNOTATOR AGREEMENT
Evaluation type P (A) P (E) K
Sentence ranking .549 .333 .323
Yes/no to edited output .774 .5 .549
INTRA-ANNOTATOR AGREEMENT
Evaluation type P (A) P (E) K
Sentence ranking .707 .333 .561
Yes/no to edited output .866 .5 .732
Table 4: Inter- and intra-annotator agreement for
the two types of manual evaluation
We measured pairwise agreement among anno-
tators using the kappa coefficient (K) which is de-
fined as
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance.
For inter-annotator agreement we calculated
P (A) for the yes/no judgments by examining all
items that were annotated by two or more anno-
tators, and calculating the proportion of time they
assigned identical scores to the same items. For
the ranking tasks we calculated P (A) by examin-
ing all pairs of systems which had been judged by
two or more judges, and calculated the proportion
of time that they agreed that A > B, A = B, or
A < B. Intra-annotator agreement was computed
similarly, but we gathered items that were anno-
tated on multiple occasions by a single annotator.
Table 4 gives K values for inter-annotator and
intra-annotator agreement. These give an indi-
cation of how often different judges agree, and
how often single judges are consistent for repeated
judgments, respectively. The interpretation of
Kappa varies, but according to Landis and Koch
(1977), 0 ? .2 is slight, .2 ? .4 is fair, .4 ? .6 is
moderate, .6? .8 is substantial and the rest almost
perfect.
Based on these interpretations the agreement for
yes/no judgments is moderate for inter-annotator
agreement and substantial for intra-annotator
agreement, but the inter-annotator agreement for
sentence level ranking is only fair.
We analyzed two possible strategies for improv-
ing inter-annotator agreement on the ranking task:
First, we tried discarding initial judgments to give
7
Edit MT Output
You have judged 19 sentences for WMT09 Multisource-English News Editing, 468 sentences total taking 74.4 seconds per sentence.
Original: They are often linked to other alterations sleep as nightmares, night terrors, the nocturnal enuresis (pee in bed) or the sleepwalking, but it is not 
always the case.
Edit:
Reset Edit
    Edited.
    No corrections needed.
    Unable to correct.
Annotator: ccb Task: WMT09 Multisource-English News Editing
Instructions: 
Correct the translation displayed, making it as fluent as possble. If no corrections are needed, select "No corrections needed." If you cannot understand
the sentence well enough to correct it, select "Unable to correct."
They are often linked to other sleep disorders, such as nightmares, night terrors, the nocturnal enuresis (bedwetting) or sleepwalking, but this is 
not always the case.
http://www.statmt.org/wmt09/judge/do_task.php
WMT09 Manual Evaluation
Figure 2: This screenshot shows an annotator editing the output of a machine translation system.
http://www.statmt.org/wmt09/judge/do_task.php
WMT09 Manual Evaluation
Judge Edited MT Output
You have judged 84 sentences for WMT09 French-English News Edit Acceptance, 459 sentences total taking 64.9 seconds per sentence.
Source: Au m?me moment, les gouvernements belges, hollandais et luxembourgeois ont en parti nationalis? le conglom?rat europ?en financier, Fortis. 
Les analystes de Barclays Capital ont d?clar? que les n?gociations fr?n?tiques de ce week end, conclues avec l'accord de sauvetage" semblent ne pas avoir 
r?ussi ? faire revivre le march?". 
Alors que la situation ?conomique se d?t?riorasse, la demande en mati?res premi?res, p?trole inclus, devrait se ralentir. 
"la prospective d'?quit? globale, de taux d'int?r?t et d'?change des march?s, est devenue incertaine" ont ?crit les analystes de Deutsche Bank dans une 
lettre ? leurs investisseurs." 
"nous pensons que les mati?res premi?res ne pourront ?chapper ? cette contagion. 
Reference: Meanwhile, the Belgian, Dutch and Luxembourg governments partially nationalized the European financial conglomerate Fortis. 
Analysts at Barclays Capital said the frantic weekend negotiations that led to the bailout agreement "appear to have failed to revive market sentiment." 
As the economic situation deteriorates, the demand for commodities, including oil, is expected to slow down. 
"The outlook for global equity, interest rate and exchange rate markets has become increasingly uncertain," analysts at Deutsche Bank wrote in a note to 
investors. 
"We believe commodities will be unable to escape the contagion.
Translation Verdict
While the economic situation is deteriorating, demand for commodities, including oil, should decrease.
Yes No
While the economic situation is deteriorating, the demand for raw materials, including oil, should slow down.
Yes No
Alors que the economic situation deteriorated, the request in rawmaterial enclosed, oil, would have to slow down.
Yes  No
While the financial situation damaged itself, the first matters affected, oil included, should slow down themselves.
Yes  No
While the economic situation is depressed, demand for raw materials, including oil, will be slow.
Yes No
Annotator: ccb Task: WMT09 French-English News Edit Acceptance
Instructions: 
Indicate whether the edited translations represent fully fluent and meaning-equivalent alternatives to the reference sentence. 
The reference is shown with context, the actual sentence is bold.
Figure 3: This screenshot shows an annotator judging the acceptability of edited translations.
8
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0.40
0.41
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.51
0.52
0.53
0.54
0.55
0.56
0.57
0.58
0.59
0.60
0.61
Inter-annotator agreement
Intra-annotator agreement
Proportion of judgments retained
Figure 4: The effect of discarding every annota-
tors? initial judgments, up to the first 50 items
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0.40
0.41
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.51
0.52
0.53
0.54
0.55
0.56
0.57
0.58
0.59
0.60
0.61
Inter-annotator agreement
Intra-annotator agreement
Proportion of judgments retained
Figure 5: The effect of removing annotators with
the lowest agreement, disregarding up to 40 anno-
tators
9
annotators a chance to learn to how to perform
the task. Second, we tried disregarding annota-
tors who have very low agreement with others, by
throwing away judgments for the annotators with
the lowest judgments.
Figures 4 and 5 show how the K values im-
prove for intra- and inter-annotator agreement un-
der these two strategies, and what percentage of
the judgments are retained as more annotators are
removed, or as the initial learning period is made
longer. It seems that the strategy of removing the
worst annotators is the best in terms of improv-
ing inter-annotator K, while retaining most of the
judgments. If we remove the 33 judges with the
worst agreement, we increase the inter-annotator
K from fair to moderate, and still retain 60% of
the data.
For the results presented in the rest of the paper,
we retain all judgments.
4 Translation task results
We used the results of the manual evaluation to
analyze the translation quality of the different sys-
tems that were submitted to the workshop. In our
analysis, we aimed to address the following ques-
tions:
? Which systems produced the best translation
quality for each language pair?
? Did the system combinations produce better
translations than individual systems?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 6 shows best individual systems. We de-
fine the best systems as those which had no other
system that was statistically significantly better
than them under the Sign Test at p ? 0.1.4 Multi-
ple systems are listed for many language pairs be-
cause it was not possible to draw a statistically sig-
nificant difference between the systems. Commer-
cial translation software (including Google, Sys-
tran, Morphologic, PCTrans, Eurotran XP, and
anonymized RBMT providers) did well in each of
the language pairs. Research systems that utilized
4In one case this definition meant that the system that was
ranked the highest overall was not considered to be one of
the best systems. For German-English translation RBMT5
was ranked highest overall, but was statistically significantly
worse than RBMT2.
only the provided data did as well as commercial
vendors in half of the language pairs.
The table also lists the best systems among
those which used only the provided materials.
To determine this decision we excluded uncon-
strained systems which employed significant ex-
ternal resources. Specifically, we ruled out all of
the commercial systems, since Google has access
to significantly greater data sources for its statisti-
cal system, and since the commercial RBMT sys-
tems utilize knowledge sources not available to
other workshop participants. The remaining sys-
tems were research systems that employ statisti-
cal models. We were able to draw distinctions
between half of these for each of the language
pairs. There are some borderline cases, for in-
stance LIMSI only used additional monolingual
training resources, and LIUM/Systran used addi-
tional translation dictionaries as well as additional
monolingual resources.
Table 5 summarizes the performance of the
system combination entries by listing the best
ranked combinations, and by indicating whether
they have a statistically significant difference with
the best individual systems. In general, system
combinations performed as well as the best indi-
vidual systems, but not statistically significantly
better than them. Moreover, it was hard to draw
a distinction between the different system combi-
nation strategies themselves. There are a number
of possibilities as to why we failed to find signifi-
cant differences:
? The number of judgments that we collected
were not sufficient to find a difference. Al-
though we collected several thousand judg-
ments for each language pair, most pairs of
systems were judged together fewer than 100
times.
? It is possible that the best performing indi-
vidual systems were sufficiently better than
the other systems and that it is difficult to im-
prove on them by combining them.
? Individual systems could have been weighted
incorrectly during the development stage,
which could happen if the automatic evalu-
ation metrics scores on the dev set did not
strongly correlate with human judgments.
? The lack of distinction between different
combinations could be due to the fact that
10
Language Pair Best system combinations Entries Significantly different than
best individual systems?
German-English RWTH-COMBO, BBN-COMBO,
CMU-COMBO, USAAR-COMBO
5 BBN-COMBO>GOOGLE, SYSTRAN,
USAAR-COMBO<RMBT2,
no difference for others
English-German USAAR-COMBO 1 worse than 3 best systems
Spanish-English CMU-COMBO, USAAR-COMBO,
BBN-COMBO
3 each better than one of the RBMT
systems, but there was no difference
with GOOGLE, TALP-UPC
English-Spanish USAAR-COMBO 1 no difference
French-English CMU-COMBO-HYPOSEL,
DCU-COMBO, CMU-COMBO
5 no difference
English-French USAAR-COMBO, DCU-COMBO 2 USAAR-COMBO>UKA,
DCU-COMBO>SYSTRAN, LIMSI,
no difference with others
Czech-English CMU-COMBO 2 no difference
Hungarian-English CMU-COMBO-HYPOSEL,
CMU-COMBO
3 both worse than MORPHO
Multisource-English RWTH-COMBO 3 n/a
Table 5: A comparison between the best system combinations and the best individual systems. It was
generally difficult to draw a statistically significant differences between the two groups, and between the
combinations themselves.
there is significant overlap in the strategies
that they employ.
Improved system combination warrants further in-
vestigation. We would suggest collecting addi-
tional judgments, and doing oracle experiments
where the contributions of individual systems are
weighted according to human judgments of their
quality.
Understandability
Our hope is that judging the acceptability of edited
output as discussed in Section 3 gives some indi-
cation of how often a system?s output was under-
standable. Figure 6 gives the percentage of times
that each system?s edited output was judged to
be acceptable (the percentage also factors in in-
stances when judges were unable to improve the
output because it was incomprehensible).
The edited output of the best perform-
ing systems under this evaluation model were
deemed acceptable around 50% of the time
for French-English, English-French, English-
Spanish, German-English, and English-German.
For Spanish-English the edited output of the best
system was acceptable around 40% of the time, for
English-Czech it was 30% and for Czech-English
and Hungarian-English it was around 20%.
This style of manual evaluation is experimental
and should not be taken to be authoritative. Some
caveats about this measure:
? Editing translations without context is diffi-
cult, so the acceptability rate is probably an
underestimate of how understandable a sys-
tem actually is.
? There are several sources of variance that are
difficult to control for: some people are better
at editing, and some sentences are more dif-
ficult to edit. Therefore, variance in the un-
derstandability of systems is difficult to pin
down.
? The acceptability measure does not strongly
correlate with the more established method of
ranking translations relative to each other for
all the language pairs.5
Please also note that the number of corrected
translations per system are very low for some
language pairs, as low as 23 corrected sentences
per system for the language pair English?French.
5The Spearman rank correlation coefficients for how the
two types of manual evaluation rank systems are .67 for de-
en, .67 for fr-en, .06 for es-en, .50 for cz-en, .36 for hu-en,
.65 for en-de, .02 for en-fr, -.6 for en-es, and .94 for en-cz.
11
French?English
625?836 judgments per system
System C? ?others
GOOGLE ? no .76
DCU ? yes .66
LIMSI ? no .65
JHU ? yes .62
UEDIN ? yes .61
UKA yes .61
LIUM-SYSTRAN no .60
RBMT5 no .59
CMU-STATXFER ? yes .58
RBMT1 no .56
USAAR no .55
RBMT3 no .54
RWTH ? yes .52
COLUMBIA yes .50
RBMT4 no .47
GENEVA no .34
English?French
422?517 judgments per system
System C? ?others
LIUM-SYSTRAN ? no .73
GOOGLE ? no .68
UKA ?? yes .66
SYSTRAN ? no .65
RBMT3 ? no .65
DCU ?? yes .65
LIMSI ? no .64
UEDIN ? yes .60
RBMT4 no .59
RWTH yes .58
RBMT5 no .57
RBMT1 no .54
USAAR no .48
GENEVA no .38
Hungarian?English
865?988 judgments per system
System C? ?others
MORPHO ? no .75
UMD ? yes .66
UEDIN yes .45
German?English
651?867 judgments per system
System C? ?others
RBMT5 no .66
USAAR ? no .65
GOOGLE ? no .65
RBMT2 ? no .64
RBMT3 no .64
RBMT4 no .62
STUTTGART ?? yes .61
SYSTRAN ? no .60
UEDIN ? yes .59
UKA ? yes .58
UMD ? yes .56
RBMT1 no .54
LIU ? yes .50
RWTH yes .50
GENEVA no .33
JHU-TROMBLE yes .13
English?German
977?1226 judgments per system
System C? ?others
RBMT2 ? no .66
RBMT3 ? no .64
RBMT5 ? no .64
USAAR no .58
RBMT4 no .58
RBMT1 no .57
GOOGLE no .54
UKA ? yes .54
UEDIN ? yes .51
LIU ? yes .49
RWTH ? yes .48
STUTTGART yes .43
Czech?English
1257?1263 judgments per system
System C? ?others
GOOGLE ? no .75
UEDIN ? yes .57
CU-BOJAR ? yes .51
Spanish?English
613?801 judgments per system
System C? ?others
GOOGLE ? no .70
TALP-UPC ?? yes .59
UEDIN ? yes .56
RBMT1 ? no .55
RBMT3 ? no .55
RBMT5 ? no .55
RBMT4 ? no .53
RWTH ? yes .51
USAAR no .51
NICT yes .37
English?Spanish
632?746 judgments per system
System C? ?others
RBMT3 ? no .66
UEDIN ?? yes .66
GOOGLE ? no .65
RBMT5 ? no .64
RBMT4 no .61
NUS ? yes .59
TALP-UPC yes .58
RWTH yes .51
RBMT1 no .25
USAAR no .48
English?Czech
4626?4784 judgments per system
System C? ?others
PCTRANS ? no .67
EUROTRANXP ? no .67
GOOGLE no .66
CU-BOJAR ? yes .61
UEDIN yes .53
CU-TECTOMT yes .48
Systems are listed in the order of how often their translations were ranked higher than or equal to any
other system. Ties are broken by direct comparison.
C? indicates constrained condition, meaning only using the supplied training data and possibly standard
monolingual linguistic tools (but no additional corpora).
? indicates a win in the category, meaning that no other system is statistically significantly better at
p-level?0.1 in pairwise comparison.
? indicates a constrained win, no other constrained system is statistically better.
For all pairwise comparisons between systems, please check the appendix.
Table 6: Official results for the WMT09 translation task, based on the human evaluation (ranking trans-
lations relative to each other)
12
Given these low numbers, the numbers presented
in Figure 6 should not be read as comparisons be-
tween systems, but rather viewed as indicating the
state of machine translation for different language
pairs.
5 Shared evaluation task overview
In addition to allowing us to analyze the transla-
tion quality of different systems, the data gath-
ered during the manual evaluation is useful for
validating the automatic evaluation metrics. Last
year, NIST began running a similar ?Metrics
for MAchine TRanslation? challenge (Metrics-
MATR), and presented their findings at a work-
shop at AMTA (Przybocki et al, 2008).
In this year?s shared task we evaluated a number
of different automatic metrics:
? Bleu (Papineni et al, 2002)?Bleu remains
the de facto standard in machine translation
evaluation. It calculates n-gram precision and
a brevity penalty, and can make use of multi-
ple reference translations as a way of captur-
ing some of the allowable variation in trans-
lation. We use a single reference translation
in our experiments.
? Meteor (Agarwal and Lavie, 2008)?Meteor
measures precision and recall for unigrams
and applies a fragmentation penalty. It uses
flexible word matching based on stemming
and WordNet-synonymy. meteor-ranking is
optimized for correlation with ranking judg-
ments.
? Translation Error Rate (Snover et al,
2006)?TER calculates the number of ed-
its required to change a hypothesis transla-
tion into a reference translation. The possi-
ble edits in TER include insertion, deletion,
and substitution of single words, and an edit
which moves sequences of contiguous words.
Two variants of TER are also included: TERp
(Snover et al, 2009), a new version which in-
troduces a number of different features, and
(Bleu ? TER)/2, a combination of Bleu and
Translation Edit Rate.
? MaxSim (Chan and Ng, 2008)?MaxSim
calculates a similarity score by comparing
items in the translation against the reference.
Unlike most metrics which do strict match-
ing, MaxSim computes a similarity score
for non-identical items. To find a maxi-
mum weight matching that matches each sys-
tem item to at most one reference item, the
items are then modeled as nodes in a bipar-
tite graph.
? wcd6p4er (Leusch and Ney, 2008)?a mea-
sure based on cder with word-based substitu-
tion costs. Leusch and Ney (2008) also sub-
mitted two contrastive metrics: bleusp4114,
a modified version of BLEU-S (Lin and
Och, 2004), with tuned n-gram weights, and
bleusp, with constant weights. wcd6p4er
is an error measure and bleusp is a quality
score.
? RTE (Pado et al, 2009)?The RTE metric
follows a semantic approach which applies
recent work in rich textual entailment to the
problem of MT evaluation. Its predictions are
based on a regression model over a feature
set adapted from an entailment systems. The
features primarily model alignment quality
and (mis-)matches of syntactic and semantic
structures.
? ULC (Gime?nez and Ma`rquez, 2008)?ULC
is an arithmetic mean over other automatic
metrics. The set of metrics used include
Rouge, Meteor, measures of overlap between
constituent parses, dependency parses, se-
mantic roles, and discourse representations.
The ULC metric had the strongest correlation
with human judgments in WMT08 (Callison-
Burch et al, 2008).
? wpF and wpBleu (Popovic and Ney, 2009) -
These metrics are based on words and part of
speech sequences. wpF is an n-gram based F-
measure which takes into account both word
n-grams and part of speech n-grams. wp-
BLEU is a combnination of the normal Blue
score and a part of speech-based Bleu score.
? SemPOS (Kos and Bojar, 2009) ? the Sem-
POS metric computes overlapping words, as
defined in (Gime?nez and Ma`rquez, 2007),
with respect to their semantic part of speech.
Moreover, it does not use the surface repre-
sentation of words but their underlying forms
obtained from the TectoMT framework.
13
re
f
b
b
n
-
c
g
o
o
g
l
e
c
m
u
-
c
o
m
b
o
c
u
-
b
o
j
a
r
u
e
d
i
n
0.140.160.180.230.250.98
Czech-English
r
e
f
g
o
o
g
l
e
c
m
u
-
s
t
a
t
x
b
b
n
-
c
o
m
b
o
c
o
l
u
m
b
i
a
c
m
u
-
c
o
m
b
o
c
m
u
-
c
o
m
b
o
-
h
y
p
d
c
u
-
c
o
m
b
o
u
k
a
r
w
t
h
r
b
m
t
4
l
i
m
s
i
r
b
m
t
1
l
i
u
m
-
s
y
s
t
r
n
u
s
a
a
r
-
c
o
m
b
o
d
c
u
u
e
d
i
n
j
h
u
r
b
m
t
3
r
b
m
t
5
u
s
a
a
r
g
e
n
e
v
a
0.210.280.280.280.290.300.310.330.330.340.340.340.350.380.390.400.410.410.470.500.520.85
French-English
r
e
f
r
w
t
h
-
c
c
m
u
-
c
o
m
b
o
b
b
n
-
c
o
m
b
o
r
b
m
t
5
u
s
a
a
r
-
c
g
o
o
g
l
e
c
m
u
-
c
m
b
-
h
u
s
a
a
r
r
b
m
t
3
s
t
u
t
t
g
a
r
t
r
b
m
t
4
r
b
m
t
1
u
k
a
r
w
t
h
u
m
d
u
e
d
i
n
r
b
m
t
2
s
y
s
t
r
a
n
l
i
u
g
e
n
e
v
a
j
h
u
-
t
r
o
m
b
l
e
0.030.060.200.210.250.260.260.270.280.300.300.310.310.320.330.340.350.360.370.410.470.83
German-English
r
e
f
u
s
a
a
r
-
c
g
o
o
g
l
e
r
b
m
t
5
r
w
t
h
t
a
l
p
-
u
p
c
u
e
d
i
n
u
s
a
a
r
r
b
m
t
3
n
u
s
r
b
m
t
4
r
b
m
t
1
0.080.100.190.210.270.270.280.320.330.380.520.69
English-Spanish
r
e
f
u
s
a
a
r
-
c
g
o
o
g
l
e
r
b
m
t
5
r
b
m
t
3
b
b
n
-
c
m
b
u
e
d
i
n
t
a
l
p
-
u
p
c
r
b
m
t
1
r
w
t
h
c
m
u
-
c
r
b
m
t
4
n
i
c
t
u
s
a
a
r
0.230.260.270.280.280.280.280.310.310.360.370.380.410.88
Spanish-English
r
e
f
g
o
o
g
l
e
p
c
t
r
a
n
s
e
u
r
o
t
r
a
n
x
p
u
e
d
i
n
c
u
-
b
o
j
a
r
c
u
-
t
e
c
t
o
m
t
0.190.210.230.260.320.320.91
English-Czech
r
e
f
g
o
o
g
l
e
u
k
a
l
i
m
s
i
u
s
a
a
r
-
c
r
b
m
t
3
r
b
m
t
5
u
e
d
i
n
u
s
a
a
r
r
b
m
t
4
l
i
u
m
-
s
y
s
r
w
t
h
d
c
u
-
c
m
b
d
c
u
s
y
s
t
r
a
n
r
b
m
t
1
g
e
n
e
v
a
0.080.100.220.270.300.310.320.340.370.400.400.430.440.450.480.490.79
English-French
r
e
f
m
o
r
p
h
o
c
m
u
-
c
m
b
-
h
c
m
u
-
c
o
m
b
o
u
m
d
u
e
d
i
n
b
b
n
-
c
m
b
0.110.120.150.190.210.220.93
Hungarian-English
r
e
f
r
b
m
t
5
r
b
m
t
3
g
o
o
g
l
e
r
b
m
t
1
r
b
m
t
2
u
s
a
a
r
u
s
a
a
r
-
c
m
b
r
b
m
t
4
r
w
t
h
u
k
a
u
e
d
i
n
s
t
u
t
t
g
a
r
t
l
i
u
0.120.180.190.260.280.310.310.320.330.350.370.420.470.85
English-German
r
e
f
r
w
t
h
b
b
n
c
m
u
0.250.270.320.90
Multsource-English
Figure 6: The percent of time that each system?s edited output was judged to be an acceptable translation.
These numbers also include judgments of the system?s output when it was marked either incomprehen-
sible or acceptable and left unedited. Note that the reference translation was edited alongside the system
outputs. Error bars show one positive and one negative standard deviation for the systems in that lan-
guage pair.
14
5.1 Measuring system-level correlation
We measured the correlation of the automatic met-
rics with the human judgments of translation qual-
ity at the system-level using Spearman?s rank cor-
relation coefficient ?. We converted the raw scores
assigned to each system into ranks. We assigned
a human ranking to the systems based on the per-
cent of time that their translations were judged to
be better than or equal to the translations of any
other system in the manual evaluation.
When there are no ties ? can be calculated using
the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all sys-
tems are ranked in the same order) and ?1 (where
the systems are ranked in the reverse order). Thus
an automatic evaluation metric with a higher abso-
lute value for ? is making predictions that are more
similar to the human judgments than an automatic
evaluation metric with a lower absolute ?.
5.2 Measuring sentence-level consistency
Because the sentence-level judgments collected
in the manual evaluation are relative judgments
rather than absolute judgments, it is not possi-
ble for us to measure correlation at the sentence-
level in the same way that previous work has done
(Kulesza and Shieber, 2004; Albrecht and Hwa,
2007a; Albrecht and Hwa, 2007b).
Rather than calculating a correlation coefficient
at the sentence-level we instead ascertained how
consistent the automatic metrics were with the hu-
man judgments. The way that we calculated con-
sistency was the following: for every pairwise
comparison of two systems on a single sentence by
a person, we counted the automatic metric as being
consistent if the relative scores were the same (i.e.
the metric assigned a higher score to the higher
ranked system). We divided this by the total num-
ber of pairwise comparisons to get a percentage.
Because the systems generally assign real num-
bers as scores, we excluded pairs that the human
annotators ranked as ties.
de
-e
n
(2
1
sy
st
em
s)
fr
-e
n
(2
1
sy
st
em
s)
es
-e
n
(1
3
sy
st
em
s)
cz
-e
n
(5
sy
st
em
s)
hu
-e
n
(6
sy
st
em
s)
A
ve
ra
ge
ulc .78 .92 .86 1 .6 .83
maxsim .76 .91 .98 .7 .66 .8
rte (absolute) .64 .91 .96 .6 .83 .79
meteor-rank .64 .93 .96 .7 .54 .75
rte (pairwise) .76 .59 .78 .8 .83 .75
terp -.72 -.89 -.94 -.7 -.37 -.72
meteor-0.6 .56 .93 .87 .7 .54 .72
meteor-0.7 .55 .93 .86 .7 .26 .66
bleu-ter/2 .38 .88 .78 .9 -.03 .58
nist .41 .87 .75 .9 -.14 .56
wpF .42 .87 .82 1 -.31 .56
ter -.43 -.83 -.84 -.6 -.01 -.54
nist (cased) .42 .83 .75 1 -.31 .54
bleu .41 .88 .79 .6 -.14 .51
bleusp .39 .88 .78 .6 -.09 .51
bleusp4114 .39 .89 .78 .6 -.26 .48
bleu (cased) .4 .86 .8 .6 -.31 .47
wpbleu .43 .86 .8 .7 -.49 .46
wcd6p4er -.41 -.89 -.76 -.6 .43 -.45
Table 7: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation into English.
en
-d
e
(1
3
sy
st
em
s)
en
-f
r
(1
6
sy
st
em
s)
en
-e
s
(1
1
sy
st
em
s)
en
-c
z
(5
sy
st
em
s)
A
ve
ra
ge
terp .03 -.89 -.58 -.4 -.46
ter -.03 -.78 -.5 -.1 -.35
bleusp4114 -.3 .88 .51 .1 .3
bleusp -.3 .87 .51 .1 .29
bleu -.43 .87 .36 .3 .27
bleu (cased) -.45 .87 .35 .3 .27
bleu-ter/2 -.37 .87 .44 .1 .26
wcd6p4er .54 -.89 -.45 -.1 -.22
nist (cased) -.47 .84 .35 .1 .2
nist -.52 .87 .23 .1 .17
wpF -.06 .9 .58 n/a n/a
wpbleu .07 .92 .63 n/a n/a
Table 8: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation out of English.
15
SemPOS .4 BLEUtecto .3
Meteor .4 BLEU .3
GTM(e=0.5)tecto .4 NISTlemma .1
GTM(e=0.5)lemma .4 NIST .1
GTM(e=0.5) .4 BLEUlemma .1
WERtecto .3 WERlemma -.1
TERtecto .3 WER -.1
PERtecto .3 TERlemma -.1
F-measuretecto .3 TER -.1
F-measurelemma .3 PERlemma -.1
F-measure .3 PER -.1
NISTtecto -.3
Table 9: The system-level correlation for auto-
matic metrics ranking five English-Czech systems
6 Evaluation task results
6.1 System-level correlation
Table 7 shows the correlation of automatic met-
rics when they rank systems that are translating
into English. Note that TERp, TER and wcd6p4er
are error metrics, so a negative correlation is bet-
ter for them. The strength of correlation varied for
the different language pairs. The automatic met-
rics were able to rank the French-English systems
reasonably well with correlation coefficients in the
range of .8 and .9. In comparison, metrics per-
formed worse for Hungarian-English, where half
of the systems had negative correlation. The ULC
metric once again had strongest correlation with
human judgments of translation quality. This was
followed closely by MaxSim and RTE, with Me-
teor and TERp doing respectably well in 4th and
5th place. Notably, Bleu and its variants were the
worst performing metrics in this translation direc-
tion.
Table 8 shows correlation for metrics which op-
erated on languages other than English. Most of
the best performing metrics that operate on En-
glish do not work for foreign languages, because
they perform some linguistic analysis or rely on
a resource like WordNet. For translation into for-
eign languages TERp was the best system overall.
The wpBleu and wpF metrics also did extremely
well, performing the best in the language pairs that
they were applied to. wpBleu and wpF were not
applied to Czech because the authors of the met-
ric did not have a Czech tagger. English-German
proved to be the most problematic language pair
to automatically evaluate, with all of the metrics
having a negative correlation except wpBleu and
TER.
Table 9 gives detailed results for how well vari-
ations on a number of automatic metrics do for
the task of ranking five English-Czech systems.6
These systems were submitted by Kos and Bojar
(2009), and they investigate the effects of using
Prague Dependency Treebank annotations during
automatic evaluation. They linearizing the Czech
trees and evaluated either the lemmatized forms of
the Czech (lemma) read off the trees or the Tec-
togrammatical form which retained only lemma-
tized content words (tecto). The table also demon-
strates SemPOS, Meteor, and GTM perform better
on Czech than many other metrics.
6.2 Sentence-level consistency
Tables 10 and 11 show the percent of times that the
metrics? scores were consistent with human rank-
ings of every pair of translated sentences.7 Since
we eliminated sentence pairs that were judged to
be equal, the random baseline for this task is 50%.
Many metrics failed to reach the baseline (includ-
ing most metrics in the out-of-English direction).
This indicates that sentence-level evaluation of
machine translation quality is very difficult. RTE
and ULC again do the best overall for the into-
English direction. They are followed closely by
wpF and wcd6p4er, which considerably improve
their performance over their system-level correla-
tions.
We tried a variant on measuring sentence-level
consistency. Instead of using the scores assigned
to each individual sentence, we used the system-
level score and applied it to every sentence that
was produced by that system. These can be
thought of as a metric?s prior expectation about
how a system should preform, based on their per-
formance on the whole data set. Tables 12 and 13
show that using the system-level scores in place
of the sentence-level scores results in considerably
higher consistency with human judgments. This
suggests an interesting line of research for improv-
ing sentence-level predictions by using the perfor-
mance on a larger data set as a prior.
7 Summary
As in previous editions of this workshop we car-
ried out an extensive manual and automatic eval-
uation of machine translation performance for
translating from European languages into English,
6PCTRANS was excluded from the English-Czech systems
because its SGML file was malformed.
7Not all metrics entered into the sentence-level task.
16
fr
-e
n
(6
26
8
pa
ir
s)
de
-e
n
(6
38
2
pa
ir
s)
es
-e
n
(4
10
6
pa
ir
s)
cz
-e
n
(2
25
1
pa
ir
s)
hu
-e
n
(2
19
3
pa
ir
s)
xx
-e
n
(1
95
2
pa
ir
s)
O
ve
ra
ll
(2
31
52
pa
ir
s)
ulc .55 .56 .51 .50 .51 .51 .54
rte (absolute) .54 .56 .51 .50 .55 .51 .53
wpF .54 .55 .50 .47 .48 .51 .52
wcd6p4er .54 .54 .49 .48 .48 .50 .52
maxsim .53 .55 .49 .47 .50 .49 .52
bleusp .54 .55 .49 .47 .46 .50 .51
bleusp4114 .53 .55 .48 .47 .46 .50 .51
rte (pairwise) .49 .48 .52 .53 .55 .52 .51
terp .52 .53 .48 .46 .45 .48 .50
meteor-0.6 .50 .53 .46 .48 .47 .47 .49
meteor-rank .50 .52 .46 .48 .47 .47 .49
meteor-0.7 .49 .52 .46 .48 .47 .47 .49
ter .48 .47 .43 .41 .40 .42 .45
wpbleu .46 .45 .46 .39 .35 .45 .44
Table 10: Sentence-level consistency of the auto-
matic metrics with human judgments for transla-
tions into English. Italicized numbers fall below
the random-choice baseline.
en
-f
r
(2
96
7
pa
ir
s)
en
-d
e
(6
56
3
pa
ir
s)
en
-e
s
(3
24
9
pa
ir
s)
en
-c
z
(1
12
42
pa
ir
s)
O
ve
ra
ll
(2
40
21
pa
ir
s)
wcd6p4er .57 .47 .52 .49 .50
bleusp4114 .57 .46 .54 .49 .50
bleusp .57 .46 .53 .48 .49
ter .50 .41 .45 .37 .41
terp .51 .39 .48 .27 .36
wpF .57 .46 .54 n/a .51
wpbleu .53 .37 .46 n/a .43
Table 11: Sentence-level consistency of the auto-
matic metrics with human judgments for transla-
tions out of English. Italicized numbers fall below
the random-choice baseline.
fr
-e
n
(6
26
8
pa
ir
s)
de
-e
n
(6
38
2
pa
ir
s)
es
-e
n
(4
10
6
pa
ir
s)
cz
-e
n
(2
25
1
pa
ir
s)
hu
-e
n
(2
19
3
pa
ir
s)
O
ve
ra
ll
(2
12
00
pa
ir
s)
Oracle .61 .63 .59 .61 .67 .62
rte (absolute) .60 .61 .59 .57 .65 .61
ulc .61 .62 .58 .61 .59 .60
maxsim .61 .62 .59 .57 .61 .60
meteor-rank .61 .61 .59 .57 .61 .60
meteor-0.6 .61 .61 .58 .57 .60 .60
rte (pairwise) .56 .61 .57 .59 .64 .59
terp .60 .61 .59 .57 .56 .59
meteor-0.7 .61 .61 .58 .57 .55 .59
ter .60 .59 .57 .55 .51 .58
wpF .60 .59 .57 .61 .46 .58
bleusp .61 .59 .56 .55 .48 .57
bleusp4114 .61 .59 .56 .55 .46 .57
wcd6p4er .61 .59 .57 .55 .44 .57
wpbleu .60 .59 .57 .57 .43 .57
Table 12: Consistency of the automatic met-
rics when their system-level ranks are treated as
sentence-level scores. Oracle shows the consis-
tency of using the system-level human ranks that
are given in Table 6.
en
-f
r
(2
96
7
pa
ir
s)
en
-d
e
(6
56
3
pa
ir
s)
en
-e
s
(3
24
9
pa
ir
s)
en
-c
z
(1
12
42
pa
ir
s)
O
ve
ra
ll
(2
40
21
pa
ir
s)
Oracle .62 .59 .63 .60 .60
terp .62 .50 .59 .53 .54
ter .61 .51 .58 .50 .53
bleusp .62 .48 .59 .50 .52
bleusp4114 .63 .48 .59 .50 .52
wcd6p4er .62 .46 .58 .50 .52
wpbleu .63 .51 .60 n/a .56
wpF .63 .50 .59 n/a .55
Table 13: Consistency of the automatic met-
rics when their system-level ranks are treated as
sentence-level scores. Oracle shows the consis-
tency of using the system-level human ranks that
are given in Table 6.
17
and vice versa.
The number of participants remained stable
compared to last year?s WMT workshop, with
22 groups from 20 institutions participating in
WMT09. This year?s evaluation also included 7
commercial rule-based MT systems and Google?s
online statistical machine translation system.
Compared to previous years, we have simpli-
fied the evaluation conditions by removing the in-
domain vs. out-of-domain distinction focusing on
news translations only. The main reason for this
was eliminating the advantage statistical systems
have with respect to test data that are from the
same domain as the training data.
Analogously to previous years, the main focus
of comparing the quality of different approaches
is on manual evaluation. Here, also, we reduced
the number of dimensions with respect to which
the different systems are compared, with sentence-
level ranking as the primary type of manual eval-
uation. In addition to the direct quality judgments
we also evaluated translation quality by having
people edit the output of systems and have as-
sessors judge the correctness of the edited output.
The degree to which users were able to edit the
translations (without having access to the source
sentence or reference translation) served as a mea-
sure of the overall comprehensibility of the trans-
lation.
Although the inter-annotator agreement in the
sentence-ranking evaluation is only fair (as mea-
sured by the Kappa score), agreement can be im-
proved by removing the first (up to 50) judgments
of each assessor, focusing on the judgments that
were made once the assessors are more familiar
with the task. Inter-annotator agreement with re-
spect to correctness judgments of the edited trans-
lations were higher (moderate), which is proba-
bly due to the simplified evaluation criterion (bi-
nary judgments versus rankings). Inter-annotator
agreement for both conditions can be increased
further by removing the judges with the worst
agreement. Intra-annotator agreement on the other
hand was considerably higher ranging between
moderate and substantial.
In addition to the manual evaluation criteria we
applied a large number of automated metrics to
see how they correlate with the human judgments.
There is considerably variation between the differ-
ent metrics and the language pairs under consid-
eration. As in WMT08, the ULC metric had the
highest overall correlation with human judgments
when translating into English, with MaxSim and
RTE following closely behind. TERp and wpBleu
were best when translating into other languages.
Automatically predicting human judgments at
the sentence-level proved to be quite challeng-
ing with many of the systems performing around
chance. We performed an analysis that showed
that if metrics? system-level scores are used in
place of their scores for individual sentences, that
they do quite a lot better. This suggests that prior
probabilities ought to be integrated into sentence-
level scoring.
All data sets generated by this workshop, in-
cluding the human judgments, system translations
and automatic scores, are publicly available for
other researchers to analyze.8
Acknowledgments
This work was supported in parts by the EuroMa-
trix project funded by the European Commission
(6th Framework Programme), the GALE program
of the US Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022, and
the US National Science Foundation under grant
IIS-0713448.
We are grateful to Holger Schwenk and Preslav
Nakov for pointing out the potential bias in our
method for ranking systems when self-judgments
are excluded. We analyzed the results and found
that this did not hold. We would like to thank
Maja Popovic for sharing thoughts about how to
improve the manual evaluation. Thanks to Cam
Fordyce for helping out with the manual evalua-
tion again this year.
An extremely big thanks to Sebastian Pado for
helping us work through the logic of segment-level
scoring of automatic evaluation metric.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor, M-
BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine trans-
lation output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Joshua Albrecht and Rebecca Hwa. 2007a. A re-
examination of machine learning approaches for
8http://www.statmt.org/wmt09/results.
html
18
sentence-level MT evaluation. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2007), Prague, Czech Re-
public.
Joshua Albrecht and Rebecca Hwa. 2007b. Regres-
sion for sentence-level MT evaluation with pseudo
references. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2007), Prague, Czech Republic.
Alexandre Allauzen, Josep Crego, Aure?lien Max, and
Fran cois Yvon. 2009. LIMSI?s statistical transla-
tion systems for WMT?09. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Mar-
tin Popel, Jan Pta?c?ek, Jan Rous?, and Zdene?k
Z?abokrtsky?. 2009. English-Czech MT in 2008. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation (WMT07), Prague, Czech Repub-
lic.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08), Colmbus, Ohio.
Marine Carpuat. 2009. Toward using morphology
in French-English phrase-based SMT. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, Athens, Greece, March. Association for
Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2008. An automatic
metric for machine translation evaluation based on
maximum similary. In In the Metrics-MATR Work-
shop of AMTA-2008, Honolulu, Hawaii.
Yu Chen, Michael Jellinghaus, Andreas Eisele,
Yi Zhang, Sabine Hunsicker, Silke Theison, Chris-
tian Federmann, and Hans Uszkoreit. 2009. Com-
bining multi-engine translations with moses. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Jinhua Du, Yifan He, Sergio Penkale, and Andy Way.
2009. MATREX: The DCU MT system for WMT
2009. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Lo??c Dugast, Jean Senellart, and Philipp Koehn.
2009. Statistical post editing and dictionary ex-
traction: Systran/Edinburgh submissions for ACL-
WMT2009. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Chris Dyer, Hendra Setiawan, Yuval Marton, and
Philip Resnik. 2009. The University of Mary-
land statistical machine translation system for the
fourth workshop on machine translation. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Jason Eisner and Roy W. Tromble. 2006. Local
search with very large-scale neighborhoods for op-
timal permutations in machine translation. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American chapter of the Associ-
ation for Computational Linguistics (HLT/NAACL-
2006), New York, New York.
Christian Federmann, Silke Theison, Andreas Eisele,
Hans Uszkoreit, Yu Chen, Michael Jellinghaus, and
Sabine Hunsicker. 2009. Translation combina-
tion using factored word substitution. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, Athens, Greece, March. Association for
Computational Linguistics.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of ACL Workshop on
Machine Translation.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. A smor-
gasbord of features for automatic MT evaluation.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198.
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark,
Alok Parlikar, and Alon Lavie. 2009. An
improved statistical transfer system for French-
English machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Kenneth Heafield, Greg Hanneman, and Alon Lavie.
2009. Machine translation system combination
with flexible word ordering. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Almut Silja Hildebrand and Stephan Vogel. 2009.
CMU system combination for WMT?09. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
19
Maria Holmqvist, Sara Stymne, Jody Foo, and Lars
Ahrenberg. 2009. Improving alignment for SMT by
reordering and augmenting the training corpus. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2009. Edin-
burgh?s submission to all tracks of the WMT2009
shared task with reordering and speed improvements
to Moses. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL
2006 Workshop on Statistical Machine Translation,
New York, New York.
Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris
Callison-Burch, Alexandra Constantin, Brooke
Cowan, Chris Dyer, Marcello Federico, Evan
Herbst, Hieu Hoang, Christine Moran, Wade Shen,
and Richard Zens. 2007. Open source toolkit for
statistical machine translation: Factored translation
models and confusion network decoding. CLSP
Summer Workshop Final Report WS-2006, Johns
Hopkins University.
Kamil Kos and Ondr?ej Bojar. 2009. Evaluation of Ma-
chine Translation Metrics for Czech as the Target
Language. Prague Bulletin of Mathematical Lin-
guistics, 92. in print.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-
ing approach to improving sentence-level MT evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation, Baltimore, MD, October 4?6.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Gregor Leusch and Hermann Ney. 2008. BLEUSP,
PINVWER, CDER: Three improved MT evaluation
measures. In In the Metrics-MATR Workshop of
AMTA-2008, Honolulu, Hawaii.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2009. The RWTH system combination system for
WMT 2009. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2004), Barcelona, Spain.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
the 5th Biennial Conference of the Association for
Machine Translation in the Americas (AMTA-2002),
Tiburon, California.
Preslav Nakov and Hwee Tou Ng. 2009. NUS
at WMT09: Domain adaptation experiments for
English-Spanish machine translation of news com-
mentary text. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, Athens,
Greece, March. Association for Computational Lin-
guistics.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
translation system for the EACL-WMT 2009. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
NIST. 2008. Evaluation plan for gale go/no-go phase
3 / phase 3.5 translation evaluations. June 18, 2008.
Attila Nova?k. 2009. Morphologic?s submission for
the WMT 2009 shared task. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Sebastian Pado, Michel Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Machine transla-
tion evaluation with textual entailment features. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2002),
Philadelphia, Pennsylvania.
Michael Paul, Andrew Finch, and Eiichiro Sumita.
2009. NICT@WMT09: Model adaptation and
transliteration for Spanish-English SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Maja Popovic and Hermann Ney. 2009. Syntax-
oriented evaluation measures for machine transla-
tion output. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
20
Maja Popovic, David Vilar, Daniel Stein, Evgeny Ma-
tusov, and Hermann Ney. 2009. The RWTH ma-
chine translation system for WMT 2009. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Mark Przybocki, Kay Peterson, and Se-
bastien Bronsart. 2008. Official results
of the NIST 2008 ?Metrics for MAchine
TRanslation? challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/.
Jose? A. R. Fonollosa, Maxim Khalilov, Marta R. Costa-
jussa?, Jose? B. Marin?o, Carlos A. Henra?quez Q.,
Adolfo Herna?ndez H., and Rafael E. Banchs. 2009.
The TALP-UPC phrase-based translation system
for EACL-WMT 2009. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hy-
pothesis alignment with flexible matching for build-
ing confusion networks: BBN system description
for WMT09 system combination task. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, Athens, Greece, March. Association for
Computational Linguistics.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word lattices for multi-source translation.
In 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Holger Schwenk, Sadaf Abdul Rauf, Loic Barrault, and
Jean Senellart. 2009. SMT and SPE machine trans-
lation systems for WMT?09. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA-2006), Cambridge, Massachusetts.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy,
or HTER? exploring different human judgments
with a tunable MT metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale lms on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), Prague, Czech Repub-
lic.
Eric Wehrli, Luka Nerima, and Yves Scherrer.
2009. Deep linguistic multilingual translation
and bilingual dictionaries. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
21
A Pairwise system comparisons by human judges
Tables 14?24 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row. Bolding indicates the winner of the two systems. The difference between
100 and the sum of the complimentary cells is the percent of time that the two systems were judged to
be equal.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
B Automatic scores
Tables 26 and 25 give the automatic scores for each of the systems.
G
E
N
E
V
A
G
O
O
G
L
E
JH
U
-T
R
O
M
B
L
E
L
IU
R
B
M
T
1
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
S
T
U
T
T
G
A
R
T
S
Y
S
T
R
A
N
U
E
D
IN
U
K
A
U
M
D
U
S
A
A
R
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
R
W
T
H
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
GENEVA .76? .08? .63? .54 .69? .73? .83? .78? .49? .77? .75? .74? .57? .74? .69? .75? .84? .60 .84? .71?
GOOGLE .15? .03? .23? .50 .43 .24? .39 .42 .39 .43 .33 .27? .29? .38 .48 .57? .44 .32 .35 .36
JHU-TROMBLE .75? .90? .77? .81? .84? .91? .94? .88? .79? .83? .83? .93? .89? .92? .90? .94? .90? .95? .91? .83?
LIU .29? .65? .12? .49 .63 .63? .57 .63? .41 .49 .46 .50 .49 .50 .41 .66? .53 .59? .62? .53
RBMT1 .32 .43 .11? .46 .42 .46 .50 .61? .34 .46 .58 .51 .42 .42 .56 .47 .53 .49 .58 .54
RBMT2 .25? .46 .09? .37 .45 .33 .45 .23? .3 .28 .47 .42 .31? .34 .39 .49 .61 .4 .32 .29?
RBMT3 .17? .59? .02? .26? .35 .46 .27 .45 .27 .36 .46 .42 .43 .26? .49 .4 .48 .58 .29 .31
RBMT4 .12? .47 .07? .37 .4 .45 .52 .60? .39 .39 .45 .39 .31? .29? .44 .54 .45 .37 .43 .30
RBMT5 .13? .34 .07? .30? .24? .57? .41 .29? .31 .50 .34 .3 .28? .43 .30 .49 .57 .3 .49 .21
RWTH .21? .55 .10? .41 .49 .55 .46 .46 .60 .44 .57 .48 .51? .41 .56 .64? .54 .56? .74? .59?
STUTTGART .17? .43 .13? .39 .43 .55 .39 .36 .33 .34 .38 .42 .52 .42 .49 .49 .28 .35 .56 .46
SYSTRAN .11? .63 .06? .42 .37 .47 .50 .32 .58 .34 .55 .36 .44 .35 .43 .61? .46 .41 .33 .44
UEDIN .10? .50? .03? .35 .49 .46 .39 .52 .55 .29 .39 .52 .35 .33 .42 .58? .43 .56 .59? .55
UKA .29? .58? .04? .32 .47 .63? .55 .54? .64? .24? .28 .39 .50 .29 .50 .48 .36 .57? .45 .45
UMD .16? .53 .08? .38 .49 .43 .63? .68? .49 .38 .39 .41 .50 .49 .46 .54 .44 .38 .46 .50
USAAR .19? .44 ? .41 .34 .49 .4 .44 .33 .36 .33 .45 .39 .32 .41 .46 .41 .31 .42 .11
BBN-COMBO .14? .31? .06? .26? .44 .44 .48 .36 .38 .23? .35 .26? .29? .34 .36 .37 .32 .23? .38 .32
CMU-COMBO .10? .36 .07? .37 .37 .36 .48 .40 .30 .28 .53 .41 .4 .43 .28 .34 .50 .33 .53 .44
CMU-COMBO-H .3 .46 ? .10? .39 .43 .40 .48 .57 .27? .41 .47 .28 .26? .38 .49 .65? .46 .41 .47
RWTH-COMBO .06? .38 ? .19? .36 .54 .43 .43 .30 .10? .33 .56 .22? .27 .23 .42 .32 .31 .41 .29
USAAR-COMBO .20? .55 .17? .3 .39 .57? .45 .59 .32 .27? .33 .47 .32 .33 .27 .16 .55 .44 .4 .50
> OTHERS .22 .51 .06 .38 .44 .52 .49 .49 .50 .33 .44 .48 .44 .42 .41 .47 .56 .48 .46 .51 .43
>= OTHERS .33 .65 .13 .50 .54 .64 .64 .62 .66 .50 .61 .60 .59 .58 .56 .65 .68 .63 .62 .70 .62
Table 14: Sentence-level ranking for the WMT09 German-English News Task
22
G
O
O
G
L
E
L
IU
R
B
M
T
1
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
S
T
U
T
T
G
A
R
T
U
E
D
IN
U
K
A
U
S
A
A
R
U
S
A
A
R
-C
O
M
B
O
GOOGLE .34? .56 .51 .55? .44 .56? .37 .41 .42 .45 .45 .43
LIU .58? .62? .55? .55? .61? .59? .37 .38 .47 .43 .58? .44
RBMT1 .39 .33? .56? .44 .50? .57? .41 .32? .37? .35? .45 .42
RBMT2 .35 .34? .34? .43 .37? .40 .25? .25? .31? .36? .37? .32?
RBMT3 .31? .35? .41 .35 .37? .41 .24? .25? .33? .43 .49 .36?
RBMT4 .48 .33? .33? .56? .55? .47 .37 .35? .34? .45 .44 .38
RBMT5 .36? .35? .33? .50 .53 .33 .36? .32? .35? .31? .25? .32?
RWTH .51 .46 .50 .60? .65? .51 .60? .38 .47 .48 .52 .54
STUTTGART .50 .47 .62? .65? .64? .57? .62? .46 .52? .54? .66? .53
UEDIN .50 .37 .53? .64? .62? .60? .55? .45 .28? .41 .53 .35
UKA .47 .42 .57? .58? .46 .44 .62? .35 .32? .36 .46 .41
USAAR .46 .36? .46 .55? .42 .42 .48? .42 .28? .39 .44 .41
USAAR-COMBO .37 .45 .54 .55? .55? .53 .61? .39 .40 .39 .46 .52
> OTHERS .44 .38 .48 .55 .53 .47 .54 .37 .33 .39 .42 .48 .41
>= OTHERS .54 .49 .57 .66 .64 .58 .64 .48 .43 .51 .54 .58 .52
Table 15: Sentence-level ranking for the WMT09 English-German News Task
G
O
O
G
L
E
N
IC
T
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
T
A
L
P
-U
P
C
U
E
D
IN
U
S
A
A
R
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
GOOGLE .21? .40 .40 .41 .38 .23? .35 .31? .25? .36 .14 .21
NICT .74? .52 .53 .63? .64? .55? .61? .65? .59? .62? .78? .66?
RBMT1 .56 .40 .34 .44 .46 .35 .48 .42 .42 .57? .52 .54
RBMT3 .40 .39 .40 .34 .36 .42 .4 .55 .50 .57? .48 .62?
RBMT4 .55 .32? .41 .46 .47 .39 .49 .49 .48 .54 .57? .54
RBMT5 .54 .30? .35 .44 .38 .45 .50 .49 .23 .51 .51 .66?
RWTH .64? .29? .50 .53 .53 .49 .42 .46 .43 .44 .51 .58?
TALP-UPC .48 .24? .44 .47 .41 .36 .39 .36 .32? .47 .45 .50
UEDIN .61? .16? .48 .42 .41 .46 .44 .43 .44 .49 .51 .41
USAAR .69? .28? .47 .44 .38 .35 .43 .60? .48 .64? .58? .56?
BBN-COMBO .35 .20? .32? .36? .39 .37 .36 .39 .32 .31? .50 .40
CMU-COMBO .19 .15? .33 .39 .32? .37 .36 .31 .37 .21? .35 .31
USAAR-COMBO .23 .20? .42 .31? .39 .25? .27? .35 .35 .32? .36 .29
> OTHERS .50 .26 .42 .42 .42 .42 .39 .44 .43 .37 .49 .49 .50
>= OTHERS .70 .37 .55 .55 .53 .55 .51 .59 .56 .51 .64 .70 .69
Table 16: Sentence-level ranking for the WMT09 Spanish-English News Task
G
O
O
G
L
E
N
U
S
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
T
A
L
P
-U
P
C
U
E
D
IN
U
S
A
A
R
U
S
A
A
R
-C
O
M
B
O
GOOGLE .39 .21? .49 .36 .48 .34? .39 .33 .36? .21
NUS .50 .11? .62? .51 .51 .35 .25 .47 .36 .43
RBMT1 .76? .80? .79? .79? .83? .64? .76? .80? .67? .64?
RBMT3 .42 .31? .16? .30? .43 .34 .29? .56 .24? .32
RBMT4 .47 .32 .11? .52? .49 .38 .36 .51 .39 .38
RBMT5 .42 .40 .11? .49 .35 .31? .39 .47 .18? .47
RWTH .59? .52 .26? .54 .51 .61? .46 .56? .39 .55?
TALP-UPC .49 .41 .17? .63? .52 .51 .29 .45? .39 .41
UEDIN .50 .32 .17? .36 .37 .46 .30? .29? .32? .36
USAAR .58? .56 .23? .67? .53 .47? .51 .49 .61? .58?
USAAR-COMBO .31 .45 .21? .54 .49 .50 .30? .43 .43 .33?
> OTHERS .50 .45 .17 .56 .47 .53 .38 .42 .52 .37 .43
>= OTHERS .65 .59 .25 .66 .61 .64 .51 .58 .66 .48 .61
Table 17: Sentence-level ranking for the WMT09 English-Spanish News Task
23
C
M
U
-S
T
A
T
X
F
E
R
C
O
L
U
M
B
IA
D
C
U
G
E
N
E
V
A
G
O
O
G
L
E
JH
U
L
IM
S
I
L
IU
M
-S
Y
S
T
R
A
N
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
U
E
D
IN
U
K
A
U
S
A
A
R
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
D
C
U
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
CMU-STATXFER .37 .44 .17? .63? .47 .46 .58? .34 .32 .25? .42 .48 .46 .28 .38 .58? .47 .39 .41 .35
COLUMBIA .56 .56? .37 .71? .48 .56? .35 .45 .28? .38 .42 .41 .33 .58 .50 .64? .52 .64? .71? .58?
DCU .27 .29? .15? .67? .45 .33 .34 .29 .31 .29 .27? .24 .37 .21? .39 .61? .4 .36 .37 .1
GENEVA .76? .54 .73? .71? .65? .73? .62? .66? .76? .46 .79? .57 .74? .72? .67? .69? .52 .71? .67? .64?
GOOGLE .23? .17? .12? .13? .21? .35 .09? .20? .27? .31? .44 .16? .21? .33 .27? .28 .30 .34 .37 .16?
JHU .40 .26 .38 .22? .60? .31 .44 .27 .37 .29? .41 .33 .37 .48 .48 .53 .47 .31 .47 .29
LIMSI .4 .16? .38 .19? .56 .49 .29 .37 .27 .20? .38 .23? .33 .29 .38 .61? .47 .31 .36 .26?
LIUM-SYSTRAN .23? .30 .42 .33? .61? .27 .45 .48 .31 .41 .44 .32 .35 .41 .39 .54? .61? .24 .67? .36
RBMT1 .53 .23 .42 .19? .57? .46 .51 .45 .47 .33 .46 .33 .41 .30 .61 .77? .51 .41 .50 .41
RBMT3 .57 .63? .55 .15? .69? .44 .57 .52 .41 .22? .38 .51 .43 .43 .31 .57? .46 .47 .38 .55
RBMT4 .58? .35 .51 .36 .67? .60? .63? .35 .41 .59? .40 .55 .50 .71? .52? .63? .65? .65? .66? .38
RBMT5 .42 .49 .54? .09? .38 .49 .49 .37 .27 .29 .34 .38 .39 .51 .18 .42 .58 .48 .50 .60?
RWTH .38 .39 .45 .32 .63? .46 .51? .34 .56 .39 .32 .52 .48 .46 .46 .66? .62? .61? .66? .54?
UEDIN .41 .21 .31 .19? .68? .46 .42 .35 .41 .38 .31 .46 .33 .34 .41 .41 .35 .44 .63? .37
UKA .40 .31 .54? .19? .51 .37 .44 .33 .52 .51 .17? .27 .32 .49 .34 .39 .53 .36 .44 .29
USAAR .44 .43 .52 .26? .62? .48 .46 .30 .30 .58 .17? .24 .44 .47 .41 .65? .52 .70? .55 .41
BBN-COMBO .21? .21? .12? .23? .26 .32 .28? .23? .12? .26? .22? .49 .09? .34 .23 .19? .44 .49? .28 .21?
CMU-COMBO .41 .36 .4 .28 .30 .35 .47 .21? .29 .42 .23? .31 .17? .49 .25 .42 .31 .37 .29 .25
CMU-COMBO-H .24 .21? .38 .23? .37 .39 .31 .24 .31 .41 .28? .31 .14? .33 .34 .24? .18? .3 .29 .27
DCU-COMBO .41 .13? .42 .20? .37 .29 .50 .19? .44 .49 .23? .46 .20? .21? .37 .39 .31 .26 .46 .19?
USAAR-COMBO .41 .25? .18 .28? .66? .53 .52? .48 .41 .38 .53 .17? .21? .42 .42 .47 .58? .58 .47 .63?
> OTHERS .40 .31 .41 .23 .56 .43 .46 .36 .37 .41 .30 .40 .33 .41 .40 .40 .50 .47 .46 .49 .36
>= OTHERS .58 .5 .66 .34 .76 .62 .65 .60 .56 .54 .47 .59 .52 .61 .61 .55 .73 .66 .71 .67 .57
Table 18: Sentence-level ranking for the WMT09 French-English News Task
D
C
U
G
E
N
E
V
A
G
O
O
G
L
E
L
IM
S
I
L
IU
M
-S
Y
S
T
R
A
N
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
S
Y
S
T
R
A
N
U
E
D
IN
U
K
A
U
S
A
A
R
D
C
U
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
DCU .12? .39 .47 .44 .33 .44 .27 .45 .24? .49 .24 .46 .26? .39 .33
GENEVA .62? .73? .69? .80? .50? .71? .50? .52? .56? .66? .46? .56? .57 .74? .84?
GOOGLE .46 .15? .28 .42 .26 .44 .26? .34 .29? .44 .24 .32 .29 .36 .32
LIMSI .25 .16? .45 .48 .23? .43 .30 .45 .27 .42 .34 .4 .36 .53? .38
LIUM-SYSTRAN .24 ? .45 .32 .17? .29 .17? .21? .38 .29 .17? .35 .17? .41 .41
RBMT1 .39 .25? .51 .51? .53? .46 .40 .29 .52 .36 .60? .63? .41 .44 .60?
RBMT3 .36 .11? .37 .37 .52 .24 .25? .27? .31 .44 .43 .32 .27? .53 .44
RBMT4 .36 .19? .58? .37 .57? .23 .61? .42 .32 .50 .22 .39 .44 .53 .56?
RBMT5 .41 .17? .53 .39 .61? .38 .58? .30 .41 .52? .41 .48 .13 .54 .60
RWTH .59? .21? .63? .50 .47 .29 .44 .37 .31 .37 .35 .51 .16? .50? .57?
SYSTRAN .35 .20? .33 .39 .38 .40 .22 .29 .26? .44 .47 .33 .32 .60? .45
UEDIN .38 .11? .41 .28 .77? .33? .51 .44 .49 .32 .37 .30 .31 .56 .56?
UKA .36 .09? .46 .4 .45 .23? .50 .39 .29 .29 .47 .26 .19? .41 .56?
USAAR .66? .27 .52 .49 .70? .31 .61? .29 .32 .64? .62 .51 .61? .76? .65?
DCU-COMBO .32 .11? .30 .18? .45 .22 .29 .33 .29 .13? .27? .26 .41 .12? .21
USAAR-COMBO .40 ? .39 .17 .26 .17? .28 .20? .28 .20? .39 .04? .06? .08? .39
> OTHERS .41 .15 .47 .39 .52 .29 .45 .32 .35 .35 .45 .34 .42 .28 .51 .49
>= OTHERS .65 .38 .68 .64 .73 .54 .65 .59 .57 .58 .65 .60 .66 .48 .74 .77
Table 19: Sentence-level ranking for the WMT09 English-French News Task
C
U
-B
O
JA
R
G
O
O
G
L
E
U
E
D
IN
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
CU-BOJAR .54? .44 .45? .52?
GOOGLE .28? .32? .18? .23
UEDIN .38 .51? .38 .45?
BBN-COMBO .31? .39? .32 .38?
CMU-COMBO .28? .29 .27? .24?
> OTHERS .31 .43 .34 .31 .40
>= OTHERS .51 .75 .57 .65 .73
Table 20: Sentence-level ranking for the WMT09 Czech-English News Task
24
C
U
-B
O
JA
R
C
U
-T
E
C
T
O
M
T
E
U
R
O
T
R
A
N
X
P
G
O
O
G
L
E
P
C
T
R
A
N
S
U
E
D
IN
CU-BOJAR .31? .45? .43? .48? .30?
CU-TECTOMT .51? .54? .56? .58? .42?
EUROTRANXP .35? .26? .39 .38 .29?
GOOGLE .31? .30? .42 .43? .26?
PCTRANS .33? .27? .36 .38? .30?
UEDIN .42? .37? .52? .50? .53?
> OTHERS .38 .30 .46 .45 .48 .31
>= OTHERS .61 .48 .67 .66 .67 .53
Table 21: Sentence-level ranking for the WMT09 English-Czech News Task
M
O
R
P
H
O
U
E
D
IN
U
M
D
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
MORPHO .21? .28? .24? .27? .28?
UEDIN .70? .59? .45? .55? .50?
UMD .61? .26? .21? .29 .38
BBN-COMBO .67? .23? .48? .41? .52?
CMU-COMBO .59? .25? .35 .29? .42
CMU-COMBO-HYPOSEL .55? .15? .34 .27? .34
> OTHERS .62 .22 .41 .29 .37 .42
>= OTHERS .75 .45 .66 .54 .62 .68
Table 22: Sentence-level ranking for the WMT09 Hungarian-English News Task
G
O
O
G
L
E
C
Z
G
O
O
G
L
E
E
S
G
O
O
G
L
E
F
R
R
B
M
T
2 D
E
R
B
M
T
3 D
E
R
B
M
T
3 E
S
R
B
M
T
3 F
R
R
B
M
T
5 E
S
R
B
M
T
5 F
R
B
B
N
-C
O
M
B
O
C
Z
B
B
N
-C
O
M
B
O
D
E
B
B
N
-C
O
M
B
O
E
S
B
B
N
-C
O
M
B
O
F
R
B
B
N
-C
O
M
B
O
H
U
B
B
N
-C
O
M
B
O
X
X
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
D
E
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
H
U
C
M
U
-C
O
M
B
O
C
Z
C
M
U
-C
O
M
B
O
H
U
C
M
U
-C
O
M
B
O
X
X
D
C
U
-C
O
M
B
O
F
R
R
W
T
H
-C
O
M
B
O
D
E
R
W
T
H
-C
O
M
B
O
X
X
U
S
A
A
R
-C
O
M
B
O
E
S
GOOGLECZ .61? .54? .47 .52 .51 .47 .61? .42 .38 .52 .55 .54 .11? .51 .48 .34 .49 .32 .53 .52 .50 .59 .53
GOOGLEES .33
? .42 .37 .38 .41 .35 .49 .45 .11? .39 .25 .36 .18? .26? .36 .22? .32 .18? .38 .4 .4 .38 .22
GOOGLEFR .27
? .42 .26? .36 .43 .47 .33 .35 .29? .23? .50 .23 .14? .29? .21? .11? .17? .22? .39 .48 .32 .36 .27
RBMT2DE .33 .49 .61? .41 .43 .25? .52 .38 .33 .41 .4 .55 .20? .66? .62? .18? .55 .35 .35 .58 .54 .61? .57?
RBMT3DE .37 .60 .54 .41 .42 .38 .45 .61 .48 .39 .40 .63? .32 .43 .25? .35 .35 .25? .56 .69? .46 .49 .46
RBMT3ES .34 .52 .46 .51 .54 .43 .36 .38 .30? .54 .41 .47 .25? .50 .42 .26? .43 .27? .52 .57 .47 .46 .26?
RBMT3FR .40 .58 .37 .63? .53 .57 .54 .50 .36 .64? .44 .55 .13? .60 .64? .4 .53 .31 .46 .48 .44 .52 .42
RBMT5ES .29
? .41 .55 .31 .48 .36 .33 .39 .16? .44 .50 .68? .23? .35 .48 .38 .37 .41 .60? .51 .51 .65? .32
RBMT5FR .47 .52 .45 .50 .33 .51 .34 .42 .29 .59 .44 .49 ? .49 .61? .28? .19? .35 .58? .60? .27 .59 .57
BBN-COMBOCZ .41 .74? .65? .55 .44 .67? .56 .80? .46 .46 .58 .70? .22? .73? .63? .32 .38 .48 .65? .72? .66? .70? .58
BBN-COMBODE .39 .54 .58? .41 .49 .44 .31? .44 .28 .49 .49 .52 .16? .52 .36 .22? .38 .33? .41 .68? .34 .52 .56
BBN-COMBOES .38 .40 .41 .43 .47 .55 .46 .25 .51 .31 .43 .44 .20? .50 .42 .30? .32 .29? .36 .62 .47 .44 .38
BBN-COMBOFR .38 .52 .35 .36 .27? .53 .40 .26? .33 .24? .44 .36 .12? .47 .47 .32 .44 .27? .41 .42 .33 .60? .35
BBN-COMBOHU .84? .75? .78? .60? .57 .70? .71? .62? .84? .65? .72? .63? .85? .78? .69? .60? .71? .50 .85? .78? .87? .86? .75?
BBN-COMBOXX .4 .54? .63? .34? .50 .47 .32 .45 .39 .20? .39 .45 .41 .14? .24? .21? .3 .21? .46 .40 .47 .41 .41
CMU-CMB-HYPDE .48 .43 .68? .29? .64? .46 .31? .30 .30? .23? .41 .39 .32 .19? .74? .21? .32 .31 .50 .74? .38 .56? .53
CMU-CMB-HYPHU .63 .75? .78? .70? .55 .63? .46 .58 .59? .50 .61? .70? .59 .13? .68? .69? .65? .39 .75? .71? .82? .80? .68?
CMU-COMBOCZ .32 .59 .81? .36 .50 .46 .41 .50 .60? .28 .54 .52 .47 .20? .55 .56 .26? .13? .55 .69? .57 .66? .55
CMU-COMBOHU .62 .76? .69? .58 .68? .67? .59 .54 .54 .48 .67? .64? .70? .32 .74? .60 .50 .77? .66? .72? .61 .82? .82?
CMU-COMBOXX .4 .50 .33 .51 .37 .43 .44 .29? .24? .32? .56 .43 .39 .13? .39 .39 .16? .30 .32? .39 .4 .46 .4
DCU-COMBOFR .44 .57 .29 .32 .25? .29 .26 .35 .27? .19? .23? .38 .42 .15? .34 .20? .12? .19? .17? .50 .55 .49 .30?
RWTH-COMBODE .41 .43 .52 .37 .39 .53 .50 .35 .53 .25? .40 .47 .54 .10? .47 .41 .07? .38 .30 .53 .38 .56 .49
RWTH-COMBOXX .31 .38 .44 .26? .41 .39 .31 .26? .32 .18? .29 .44 .19? .10? .36 .25? .11? .28? .15? .39 .42 .28 .44
USAAR-COMBOES .37 .37 .54 .21? .4 .58? .39 .47 .31 .32 .34 .28 .55 .11? .38 .38 .20? .38 .18? .44 .67? .43 .44
> OTHERS .41 .54 .54 .43 .45 .49 .41 .44 .44 .32 .46 .46 .50 .16 .51 .45 .26 .40 .29 .52 .57 .48 .55 .47
>= OTHERS .52 .67 .70 .55 .55 .57 .52 .58 .58 .43 .57 .59 .62 .27 .62 .58 .37 .52 .36 .63 .68 .59 .69 .62
Table 23: Sentence-level ranking for the WMT09 All-English News Task
25
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
BBN-COMBO .37 .40?
CMU-COMBO .41 .44?
RWTH-COMBO .32? .34?
> OTHERS .36 .35 .42
>= OTHERS .62 .58 .67
Table 24: Sentence-level ranking for the WMT09 Multisource-English News Task
26
R
A
N
K
B
L
E
U
B
L
E
U
-C
A
S
E
D
B
L
E
U
-T
E
R
B
L
E
U
S
P
B
L
E
U
S
P
41
14
M
A
X
S
IM
M
E
T
E
O
R
-0
.6
M
E
T
E
O
R
-0
.7
M
E
T
E
O
R
-R
A
N
K
IN
G
N
IS
T
N
IS
T
-C
A
S
E
D
R
T
E
-A
B
S
O
L
U
T
E
R
T
E
-P
A
IR
W
IS
E
T
E
R
T
E
R
P
U
L
C
W
C
D
6P
4E
R
W
P
F
W
P
B
L
E
U
German-English News Task
BBN-COMBO 0.68 0.24 0.22 ?0.17 0.29 0.31 0.51 0.55 0.6 0.41 7.08 6.78 0.13 0.1 0.54 0.63 0.31 0.45 0.36 0.31
CMU-COMBO 0.63 0.22 0.21 ?0.19 0.28 0.29 0.49 0.54 0.58 0.4 6.95 6.71 0.12 0.09 0.56 0.66 0.29 0.47 0.35 0.29
CMU-COMBO-HYPOSEL 0.62 0.23 0.21 ?0.19 0.28 0.3 0.49 0.54 0.57 0.4 6.79 6.5 0.11 0.09 0.57 0.66 0.29 0.47 0.35 0.3
GENEVA 0.33 0.1 0.09 ?0.33 0.17 0.18 0.38 0.43 0.44 0.30 4.88 4.65 0.03 0.04 0.71 0.86 0.22 0.58 0.25 0.17
GOOGLE 0.65 0.21 0.20 ?0.2 0.27 0.28 0.48 0.54 0.57 0.39 6.85 6.65 0.11 0.11 0.56 0.65 0.29 0.48 0.35 0.28
JHU-TROMBLE 0.13 0.07 0.06 ?0.38 0.09 0.1 0.34 0.43 0.41 0.29 4.90 4.25 0.02 0.02 0.81 1 0.19 0.61 0.22 0.12
LIU 0.50 0.19 0.18 ?0.22 0.25 0.27 0.46 0.51 0.54 0.38 6.35 6.02 0.06 0.05 0.61 0.72 0.27 0.49 0.33 0.26
RBMT1 0.54 0.14 0.13 ?0.29 0.20 0.21 0.43 0.50 0.53 0.37 5.30 5.07 0.04 0.04 0.67 0.76 0.26 0.55 0.29 0.22
RBMT2 0.64 0.17 0.16 ?0.26 0.23 0.24 0.48 0.52 0.55 0.38 6.06 5.75 0.1 0.12 0.63 0.70 0.29 0.51 0.31 0.24
RBMT3 0.64 0.17 0.16 ?0.25 0.23 0.25 0.48 0.52 0.55 0.38 5.98 5.71 0.09 0.09 0.61 0.68 0.29 0.51 0.32 0.25
RBMT4 0.62 0.16 0.14 ?0.27 0.21 0.23 0.45 0.5 0.52 0.36 5.65 5.36 0.06 0.07 0.65 0.72 0.27 0.52 0.30 0.23
RBMT5 0.66 0.16 0.15 ?0.26 0.22 0.24 0.47 0.51 0.54 0.37 5.76 5.52 0.07 0.06 0.63 0.70 0.28 0.52 0.31 0.24
RWTH 0.50 0.19 0.18 ?0.21 0.25 0.26 0.45 0.50 0.53 0.36 6.44 6.24 0.06 0.03 0.60 0.74 0.27 0.49 0.33 0.26
RWTH-COMBO 0.7 0.23 0.22 ?0.18 0.29 0.30 0.50 0.55 0.59 0.41 7.06 6.81 0.11 0.07 0.54 0.63 0.30 0.46 0.36 0.31
STUTTGART 0.61 0.2 0.18 ?0.22 0.26 0.27 0.48 0.52 0.56 0.38 6.39 6.11 0.1 0.06 0.60 0.69 0.29 0.49 0.33 0.27
SYSTRAN 0.6 0.19 0.17 ?0.22 0.24 0.26 0.47 0.52 0.55 0.38 6.40 6.08 0.08 0.07 0.60 0.71 0.28 0.5 0.33 0.26
UEDIN 0.59 0.20 0.19 ?0.22 0.26 0.27 0.47 0.52 0.55 0.38 6.47 6.24 0.07 0.04 0.61 0.70 0.27 0.49 0.34 0.27
UKA 0.58 0.21 0.2 ?0.20 0.27 0.28 0.47 0.52 0.56 0.38 6.66 6.43 0.08 0.04 0.58 0.69 0.28 0.48 0.34 0.28
UMD 0.56 0.21 0.19 ?0.19 0.26 0.28 0.47 0.52 0.56 0.38 6.74 6.42 0.08 0.04 0.56 0.69 0.28 0.48 0.34 0.27
USAAR 0.65 0.17 0.15 ?0.26 0.23 0.24 0.47 0.51 0.54 0.38 5.89 5.64 0.06 0.05 0.64 0.71 0.28 0.52 0.31 0.24
USAAR-COMBO 0.62 0.17 0.16 ?0.25 0.23 0.24 0.47 0.51 0.55 0.38 5.99 6.85 0.07 0.06 0.64 0.70 0.28 0.51 0.32 0.25
Spanish-English News Task
BBN-COMBO 0.64 0.29 0.27 ?0.13 0.34 0.35 0.53 0.57 0.62 0.43 7.64 7.35 0.16 0.13 0.51 0.61 0.33 0.42 0.4 0.35
CMU-COMBO 0.7 0.28 0.27 ?0.13 0.33 0.35 0.53 0.58 0.62 0.43 7.65 7.46 0.21 0.2 0.51 0.60 0.34 0.42 0.40 0.36
GOOGLE 0.70 0.29 0.28 ?0.13 0.34 0.35 0.53 0.58 0.62 0.43 7.68 7.50 0.23 0.22 0.5 0.59 0.34 0.42 0.41 0.36
NICT 0.37 0.22 0.22 ?0.19 0.27 0.29 0.48 0.54 0.57 0.39 6.91 6.74 0.1 0.1 0.60 0.71 0.3 0.46 0.36 0.3
RBMT1 0.55 0.19 0.18 ?0.24 0.25 0.26 0.49 0.54 0.57 0.40 6.07 5.93 0.11 0.12 0.62 0.69 0.3 0.49 0.34 0.28
RBMT3 0.55 0.20 0.2 ?0.22 0.26 0.27 0.50 0.54 0.58 0.41 6.24 6.08 0.13 0.14 0.60 0.65 0.31 0.48 0.36 0.29
RBMT4 0.53 0.2 0.19 ?0.22 0.25 0.27 0.48 0.53 0.57 0.4 6.20 6.03 0.10 0.11 0.60 0.67 0.3 0.48 0.35 0.28
RBMT5 0.55 0.20 0.2 ?0.22 0.26 0.27 0.5 0.54 0.58 0.40 6.26 6.10 0.12 0.11 0.6 0.65 0.31 0.48 0.36 0.29
RWTH 0.51 0.24 0.23 ?0.16 0.3 0.31 0.49 0.54 0.58 0.4 7.12 6.95 0.11 0.08 0.56 0.68 0.31 0.45 0.37 0.32
TALP-UPC 0.59 0.26 0.25 ?0.15 0.31 0.33 0.51 0.56 0.6 0.41 7.28 7.02 0.13 0.11 0.54 0.64 0.32 0.44 0.38 0.33
UEDIN 0.56 0.26 0.25 ?0.15 0.32 0.33 0.51 0.56 0.60 0.42 7.25 7.04 0.16 0.1 0.55 0.64 0.32 0.43 0.39 0.34
USAAR 0.51 0.2 0.19 ?0.22 0.25 0.27 0.48 0.54 0.57 0.4 6.31 6.14 0.11 0.09 0.62 0.67 0.3 0.48 0.34 0.28
USAAR-COMBO 0.69 0.29 0.27 ?0.13 0.34 0.35 0.53 0.58 0.62 0.43 7.58 7.25 0.20 0.13 0.51 0.6 0.34 0.42 0.4 0.35
French-English News Task
BBN-COMBO 0.73 0.31 0.3 ?0.11 0.36 0.38 0.54 0.59 0.64 0.45 7.88 7.58 0.14 0.12 0.2 0.20 0.36 0.40 0.41 0.37
CMU-COMBO 0.66 0.3 0.29 ?0.12 0.35 0.36 0.53 0.58 0.63 0.44 7.72 7.57 0.15 0.12 0.24 0.26 0.35 0.41 0.41 0.37
CMU-COMBO-HYPOSEL 0.71 0.28 0.26 ?0.14 0.33 0.35 0.53 0.57 0.61 0.43 7.40 7.15 0.1 0.08 0.31 0.33 0.34 0.42 0.4 0.35
CMU-STATXFER 0.58 0.24 0.23 ?0.18 0.29 0.31 0.49 0.54 0.58 0.40 6.89 6.75 0.08 0.07 0.38 0.42 0.31 0.46 0.37 0.32
COLUMBIA 0.50 0.23 0.22 ?0.18 0.29 0.30 0.49 0.54 0.58 0.40 6.85 6.68 0.07 0.07 0.36 0.39 0.31 0.46 0.36 0.31
DCU 0.66 0.27 0.25 ?0.15 0.32 0.34 0.52 0.56 0.61 0.42 7.29 6.94 0.09 0.07 0.32 0.34 0.33 0.43 0.38 0.34
DCU-COMBO 0.67 0.31 0.31 ?0.11 0.36 0.37 0.54 0.59 0.64 0.44 7.84 7.69 0.14 0.12 0.21 0.22 0.35 0.41 0.42 0.38
GENEVA 0.34 0.14 0.14 ?0.29 0.21 0.22 0.43 0.49 0.52 0.36 5.32 5.15 0.05 0.05 0.54 0.52 0.26 0.53 0.29 0.22
GOOGLE 0.76 0.31 0.30 ?0.10 0.36 0.37 0.54 0.58 0.63 0.44 8 7.84 0.17 0.13 0.17 0.2 0.36 0.41 0.42 0.38
JHU 0.62 0.27 0.23 ?0.15 0.32 0.33 0.51 0.56 0.6 0.41 7.23 6.68 0.08 0.05 0.33 0.36 0.32 0.43 0.37 0.32
LIMSI 0.65 0.26 0.25 ?0.16 0.30 0.32 0.51 0.56 0.60 0.42 7.02 6.87 0.09 0.07 0.35 0.36 0.33 0.44 0.38 0.33
LIUM-SYSTRAN 0.60 0.27 0.26 ?0.15 0.32 0.33 0.51 0.56 0.60 0.42 7.26 7.10 0.10 0.06 0.33 0.36 0.33 0.43 0.39 0.35
RBMT1 0.56 0.18 0.18 ?0.25 0.24 0.25 0.48 0.53 0.57 0.4 5.89 5.73 0.07 0.06 0.51 0.45 0.3 0.50 0.34 0.26
RBMT3 0.54 0.2 0.19 ?0.22 0.25 0.27 0.48 0.53 0.56 0.39 6.12 5.96 0.07 0.06 0.45 0.45 0.30 0.49 0.35 0.28
RBMT4 0.47 0.19 0.18 ?0.24 0.24 0.26 0.48 0.52 0.56 0.39 5.97 5.83 0.07 0.06 0.46 0.45 0.3 0.49 0.34 0.27
RBMT5 0.59 0.19 0.19 ?0.24 0.25 0.26 0.49 0.54 0.57 0.40 6.03 5.9 0.09 0.07 0.46 0.43 0.31 0.49 0.35 0.28
RWTH 0.52 0.25 0.24 ?0.16 0.30 0.32 0.5 0.55 0.59 0.40 7.09 6.94 0.07 0.03 0.35 0.39 0.32 0.44 0.38 0.32
UEDIN 0.61 0.25 0.24 ?0.16 0.31 0.32 0.50 0.55 0.59 0.41 7.04 6.85 0.08 0.04 0.35 0.38 0.32 0.44 0.38 0.33
UKA 0.61 0.26 0.25 ?0.15 0.31 0.33 0.51 0.55 0.6 0.41 7.17 7.00 0.08 0.04 0.34 0.37 0.32 0.44 0.38 0.34
USAAR 0.55 0.19 0.18 ?0.24 0.24 0.26 0.48 0.54 0.57 0.4 6.08 5.92 0.07 0.06 0.46 0.44 0.3 0.49 0.34 0.26
USAAR-COMBO 0.57 0.26 0.25 ?0.16 0.31 0.33 0.51 0.55 0.59 0.41 7.13 6.85 0.08 0.02 0.33 0.35 0.32 0.44 0.38 0.33
Czech-English News Task
BBN-COMBO 0.65 0.22 0.20 ?0.19 0.27 0.29 0.47 0.52 0.56 0.39 6.74 6.45 0.24 0.3 0.52 0.60 0.29 0.47 0.34 0.29
CMU-COMBO 0.73 0.22 0.20 ?0.2 0.27 0.29 0.47 0.53 0.57 0.39 6.72 6.46 0.34 0.34 0.53 0.60 0.29 0.47 0.35 0.29
CU-BOJAR 0.51 0.16 0.15 ?0.26 0.22 0.24 0.43 0.5 0.52 0.36 5.84 5.54 0.26 0.28 0.61 0.69 0.26 0.52 0.31 0.24
GOOGLE 0.75 0.21 0.20 ?0.19 0.26 0.28 0.46 0.52 0.55 0.38 6.82 6.61 0.32 0.33 0.53 0.62 0.29 0.47 0.35 0.28
UEDIN 0.57 0.2 0.19 ?0.23 0.25 0.27 0.45 0.50 0.54 0.37 6.2 6 0.22 0.25 0.56 0.63 0.27 0.49 0.33 0.27
Hungarian-English News Task
BBN-COMBO 0.54 0.14 0.13 ?0.29 0.19 0.21 0.38 0.45 0.46 0.32 5.46 5.2 0.16 0.18 0.71 0.83 0.23 0.55 0.27 0.2
CMU-COMBO 0.62 0.14 0.13 ?0.29 0.19 0.21 0.39 0.46 0.47 0.32 5.52 5.24 0.28 0.22 0.71 0.82 0.23 0.55 0.28 0.2
CMU-COMBO-HYPOSEL 0.68 0.14 0.12 ?0.29 0.19 0.21 0.39 0.45 0.46 0.32 5.51 5.16 0.25 0.25 0.71 0.82 0.23 0.55 0.27 0.2
MORPHO 0.75 0.1 0.09 ?0.36 0.15 0.17 0.39 0.45 0.46 0.32 4.75 4.55 0.34 0.49 0.79 0.83 0.23 0.6 0.26 0.17
UEDIN 0.45 0.12 0.11 ?0.32 0.18 0.19 0.37 0.42 0.43 0.30 4.95 4.74 0.12 0.12 0.75 0.87 0.21 0.58 0.27 0.19
UMD 0.66 0.13 0.12 ?0.28 0.18 0.2 0.36 0.44 0.45 0.30 5.41 5.12 0.21 0.13 0.68 0.85 0.22 0.55 0.27 0.18
Table 25: Automatic evaluation metric scores for translations into English
27
R
A
N
K
B
L
E
U
B
L
E
U
-C
A
S
E
D
B
L
E
U
-T
E
R
B
L
E
U
S
P
B
L
E
U
S
P
41
14
N
IS
T
N
IS
T
-C
A
S
E
D
T
E
R
T
E
R
P
W
C
D
6P
4E
R
W
P
F
W
P
B
L
E
U
English-German News Task
GOOGLE 0.54 0.15 0.14 ?0.29 0.20 0.22 5.36 5.25 0.62 0.74 0.54 0.3 0.23
LIU 0.49 0.14 0.13 ?0.29 0.2 0.21 5.35 5.18 0.65 0.78 0.54 0.3 0.23
RBMT1 0.57 0.11 0.11 ?0.32 0.17 0.19 4.69 4.59 0.67 0.81 0.57 0.28 0.21
RBMT2 0.66 0.13 0.13 ?0.30 0.19 0.21 5.08 4.99 0.62 0.75 0.55 0.30 0.23
RBMT3 0.64 0.12 0.12 ?0.29 0.2 0.21 4.8 4.71 0.62 0.76 0.54 0.31 0.25
RBMT4 0.58 0.11 0.10 ?0.33 0.17 0.18 4.66 4.57 0.7 0.84 0.57 0.27 0.2
RBMT5 0.64 0.13 0.12 ?0.3 0.19 0.20 5.03 4.94 0.64 0.79 0.55 0.3 0.23
RWTH 0.48 0.14 0.13 ?0.28 0.2 0.21 5.51 5.41 0.62 0.78 0.53 0.3 0.23
STUTTGART 0.43 0.12 0.12 ?0.31 0.18 0.20 5.06 4.82 0.67 0.79 0.55 0.29 0.21
UEDIN 0.51 0.15 0.15 ?0.27 0.21 0.23 5.53 5.42 0.63 0.77 0.53 0.31 0.24
UKA 0.54 0.15 0.15 ?0.27 0.21 0.22 5.6 5.48 0.62 0.75 0.52 0.31 0.24
USAAR 0.58 0.12 0.11 ?0.33 0.18 0.19 4.83 4.71 0.69 0.8 0.57 0.28 0.21
USAAR-COMBO 0.52 0.16 0.15 ?0.27 0.21 0.23 5.6 5.39 0.62 0.75 0.52 0.31 0.24
English-Spanish News Task
GOOGLE 0.65 0.28 0.27 ?0.15 0.33 0.34 7.27 7.07 0.36 0.42 0.42 0.37 0.31
NUS 0.59 0.25 0.23 ?0.17 0.30 0.31 6.96 6.67 0.48 0.59 0.44 0.34 0.28
RBMT1 0.25 0.15 0.14 ?0.27 0.20 0.22 5.32 5.17 0.55 0.66 0.51 0.24 0.16
RBMT3 0.66 0.18 0.17 ?0.18 0.28 0.3 5.79 5.63 0.49 0.59 0.45 0.33 0.27
RBMT4 0.61 0.21 0.2 ?0.20 0.26 0.28 6.47 6.28 0.52 0.64 0.47 0.31 0.25
RBMT5 0.64 0.22 0.21 ?0.2 0.27 0.29 6.53 6.34 0.52 0.64 0.46 0.32 0.26
RWTH 0.51 0.22 0.21 ?0.18 0.27 0.29 6.83 6.63 0.50 0.65 0.46 0.32 0.26
TALP-UPC 0.58 0.25 0.23 ?0.17 0.3 0.31 6.96 6.69 0.47 0.58 0.44 0.34 0.28
UEDIN 0.66 0.25 0.24 ?0.17 0.30 0.31 6.94 6.73 0.48 0.59 0.44 0.34 0.29
USAAR 0.48 0.20 0.19 ?0.21 0.26 0.27 6.36 6.16 0.54 0.66 0.47 0.30 0.24
USAAR-COMBO 0.61 0.28 0.26 ?0.14 0.33 0.34 7.36 6.97 0.39 0.48 0.42 0.36 0.31
English-French News Task
DCU 0.65 0.24 0.22 ?0.19 0.29 0.30 6.69 6.39 0.63 0.72 0.47 0.38 0.34
DCU-COMBO 0.74 0.28 0.27 ?0.15 0.33 0.34 7.29 7.12 0.58 0.67 0.44 0.42 0.38
GENEVA 0.38 0.15 0.14 ?0.27 0.20 0.22 5.59 5.39 0.68 0.82 0.53 0.32 0.25
GOOGLE 0.68 0.25 0.24 ?0.17 0.30 0.31 6.90 6.71 0.62 0.7 0.46 0.40 0.36
LIMSI 0.64 0.25 0.24 ?0.17 0.3 0.31 6.94 6.77 0.60 0.71 0.46 0.4 0.35
LIUM-SYSTRAN 0.73 0.26 0.24 ?0.17 0.31 0.32 7.02 6.83 0.61 0.71 0.45 0.40 0.36
RBMT1 0.54 0.18 0.17 ?0.23 0.24 0.26 6.12 5.96 0.65 0.76 0.5 0.35 0.29
RBMT3 0.65 0.22 0.20 ?0.20 0.27 0.28 6.48 6.29 0.63 0.72 0.48 0.38 0.33
RBMT4 0.59 0.18 0.17 ?0.24 0.24 0.25 6.02 5.86 0.66 0.77 0.50 0.35 0.3
RBMT5 0.57 0.20 0.19 ?0.21 0.26 0.27 6.31 6.15 0.63 0.74 0.49 0.36 0.31
RWTH 0.58 0.22 0.21 ?0.19 0.27 0.28 6.67 6.51 0.62 0.75 0.48 0.38 0.32
SYSTRAN 0.65 0.23 0.22 ?0.19 0.28 0.29 6.7 6.47 0.63 0.74 0.47 0.39 0.34
UEDIN 0.60 0.24 0.23 ?0.18 0.29 0.30 6.75 6.57 0.62 0.71 0.47 0.39 0.35
UKA 0.66 0.24 0.23 ?0.18 0.29 0.30 6.82 6.65 0.61 0.71 0.46 0.39 0.35
USAAR 0.48 0.19 0.18 ?0.23 0.24 0.26 6.16 5.98 0.66 0.76 0.5 0.34 0.29
USAAR-COMBO 0.77 0.27 0.25 ?0.15 0.32 0.33 7.24 6.93 0.59 0.69 0.44 0.41 0.37
English-Czech News Task
CU-BOJAR 0.61 0.14 0.13 ?0.28 0.21 0.23 5.18 4.96 0.63 0.82 0.01 n/a n/a
CU-TECTOMT 0.48 0.07 0.07 ?0.35 0.14 0.16 4.17 4.03 0.71 0.96 0.01 n/a n/a
EUROTRANXP 0.67 0.1 0.09 ?0.33 0.16 0.18 4.38 4.26 0.7 0.93 0.01 n/a n/a
GOOGLE 0.66 0.14 0.13 ?0.30 0.20 0.22 4.96 4.84 0.66 0.82 0.01 n/a n/a
PCTRANS 0.67 0.09 0.09 ?0.34 0.17 0.18 4.34 4.19 0.71 0.90 0.01 n/a n/a
UEDIN 0.53 0.14 0.13 ?0.29 0.21 0.22 5.04 4.9 0.64 0.84 0.01 n/a n/a
English-Hungarian News Task
MORPHO 0.79 0.08 0.08 ?0.37 0.15 0.16 4.04 3.92 0.83 1 0.6 n/a n/a
UEDIN 0.32 0.1 0.09 ?0.33 0.17 0.18 4.48 4.32 0.78 1 0.56 n/a n/a
Table 26: Automatic evaluation metric scores for translations out of English
28
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135?139,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Joshua: An Open Source Toolkit for Parsing-based Machine Translation
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,+ Sanjeev Khudanpur,
Lane Schwartz,? Wren N. G. Thornton, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe Joshua, an open source
toolkit for statistical machine transla-
tion. Joshua implements all of the algo-
rithms required for synchronous context
free grammars (SCFGs): chart-parsing, n-
gram language model integration, beam-
and cube-pruning, and k-best extraction.
The toolkit also implements suffix-array
grammar extraction and minimum error
rate training. It uses parallel and dis-
tributed computing techniques for scala-
bility. We demonstrate that the toolkit
achieves state of the art translation per-
formance on the WMT09 French-English
translation task.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high bar-
rier to entry for other researchers, and makes ex-
periments difficult to duplicate and compare. In
this paper, we describe Joshua, a general-purpose
open source toolkit for parsing-based machine
translation, serving the same role as Moses (Koehn
et al, 2007) does for regular phrase-based ma-
chine translation.
Our toolkit is written in Java and implements
all the essential algorithms described in Chiang
(2007): chart-parsing, n-gram language model in-
tegration, beam- and cube-pruning, and k-best ex-
traction. The toolkit also implements suffix-array
grammar extraction (Lopez, 2007) and minimum
error rate training (Och, 2003). Additionally, par-
allel and distributed computing techniques are ex-
ploited to make it scalable (Li and Khudanpur,
2008b). We have also made great effort to ensure
that our toolkit is easy to use and to extend.
The toolkit has been used to translate roughly
a million sentences in a parallel corpus for large-
scale discriminative training experiments (Li and
Khudanpur, 2008a). We hope the release of the
toolkit will greatly contribute the progress of the
syntax-based machine translation research.1
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: The Joshua code is organized
into separate packages for each major aspect of
functionality. In this way it is clear which files
contribute to a given functionality and researchers
can focus on a single package without worrying
about the rest of the system. Moreover, to mini-
mize the problems of unintended interactions and
unseen dependencies, which is common hinder-
ance to extensibility in large projects, all exten-
sible components are defined by Java interfaces.
Where there is a clear point of departure for re-
search, a basic implementation of each interface is
provided as an abstract class to minimize the work
necessary for new extensions.
End-to-end Cohesion: There are many compo-
nents to a machine translation pipeline. One of the
great difficulties with current MT pipelines is that
these diverse components are often designed by
separate groups and have different file format and
interaction requirements. This leads to a large in-
vestment in scripts to convert formats and connect
the different components, and often leads to unten-
able and non-portable projects as well as hinder-
1The toolkit can be downloaded at http://www.
sourceforge.net/projects/joshua, and the in-
structions in using the toolkit are at http://cs.jhu.
edu/?ccb/joshua.
135
ing repeatability of experiments. To combat these
issues, the Joshua toolkit integrates most critical
components of the machine translation pipeline.
Moreover, each component can be treated as a
stand-alone tool and does not rely on the rest of
the toolkit we provide.
Scalability: Our third design goal was to en-
sure that the decoder is scalable to large models
and data sets. The parsing and pruning algorithms
are carefully implemented with dynamic program-
ming strategies, and efficient data structures are
used to minimize overhead. Other techniques con-
tributing to scalability includes suffix-array gram-
mar extraction, parallel and distributed decoding,
and bloom filter language models.
Below we give a short description about the
main functions implemented in our Joshua toolkit.
2.1 Training Corpus Sub-sampling
Rather than inducing a grammar from the full par-
allel training data, we made use of a method pro-
posed by Kishore Papineni (personal communica-
tion) to select the subset of the training data con-
sisting of sentences useful for inducing a gram-
mar to translate a particular test set. This method
works as follows: for the development and test
sets that will be translated, every n-gram (up to
length 10) is gathered into a map W and asso-
ciated with an initial count of zero. Proceeding
in order through the training data, for each sen-
tence pair whose source-to-target length ratio is
within one standard deviation of the average, if
any n-gram found in the source sentence is also
found in W with a count of less than k, the sen-
tence is selected. When a sentence is selected, the
count of every n-gram in W that is found in the
source sentence is incremented by the number of
its occurrences in the source sentence. For our
submission, we used k = 20, which resulted in
1.5 million (out of 23 million) sentence pairs be-
ing selected for use as training data. There were
30,037,600 English words and 30,083,927 French
words in the subsampled training corpus.
2.2 Suffix-array Grammar Extraction
Hierarchical phrase-based translation requires a
translation grammar extracted from a parallel cor-
pus, where grammar rules include associated fea-
ture values. In real translation tasks, the grammars
extracted from large training corpora are often far
too large to fit into available memory.
In such tasks, feature calculation is also very ex-
pensive in terms of time required; huge sets of
extracted rules must be sorted in two directions
for relative frequency calculation of such features
as the translation probability p(f |e) and reverse
translation probability p(e|f) (Koehn et al, 2003).
Since the extraction steps must be re-run if any
change is made to the input training data, the time
required can be a major hindrance to researchers,
especially those investigating the effects of tok-
enization or word segmentation.
To alleviate these issues, we extract only a sub-
set of all available rules. Specifically, we follow
Callison-Burch et al (2005; Lopez (2007) and use
a source language suffix array to extract only those
rules which will actually be used in translating a
particular set of test sentences. This results in a
vastly smaller rule set than techniques which ex-
tract all rules from the training set.
The current code requires suffix array rule ex-
traction to be run as a pre-processing step to ex-
tract the rules needed to translate a particular test
set. However, we are currently extending the de-
coder to directly access the suffix array. This will
allow the decoder at runtime to efficiently extract
exactly those rules needed to translate a particu-
lar sentence, without the need for a rule extraction
pre-processing step.
2.3 Decoding Algorithms2
Grammar formalism: Our decoder assumes a
probabilistic synchronous context-free grammar
(SCFG). Currently, it only handles SCFGs of the
kind extracted by Heiro (Chiang, 2007), but is eas-
ily extensible to more general SCFGs (e.g., (Gal-
ley et al, 2006)) and closely related formalisms
like synchronous tree substitution grammars (Eis-
ner, 2003).
Chart parsing: Given a source sentence to de-
code, the decoder generates a one-best or k-best
translations using a CKY algorithm. Specifically,
the decoding algorithm maintains a chart, which
contains an array of cells. Each cell in turn main-
tains a list of proven items. The parsing process
starts with the axioms, and proceeds by applying
the inference rules repeatedly to prove new items
until proving a goal item. Whenever the parser
proves a new item, it adds the item to the appro-
priate chart cell. The item also maintains back-
2More details on the decoding algorithms are provided in
(Li et al, 2009a).
136
pointers to antecedent items, which are used for
k-best extraction.
Pruning: Severe pruning is needed in order to
make the decoding computationally feasible for
SCFGs with large target-language vocabularies.
In our decoder, we incorporate two pruning tech-
niques: beam and cube pruning (Chiang, 2007).
Hypergraphs and k-best extraction: For each
source-language sentence, the chart-parsing algo-
rithm produces a hypergraph, which represents
an exponential set of likely derivation hypotheses.
Using the k-best extraction algorithm (Huang and
Chiang, 2005), we extract the k most likely deriva-
tions from the hypergraph.
Parallel and distributed decoding: We also
implement parallel decoding and a distributed
language model by exploiting multi-core and
multi-processor architectures and distributed com-
puting techniques. More details on these two fea-
tures are provided by Li and Khudanpur (2008b).
2.4 Language Models
In addition to the distributed LM mentioned
above, we implement three local n-gram language
models. Specifically, we first provide a straightfor-
ward implementation of the n-gram scoring func-
tion in Java. This Java implementation is able to
read the standard ARPA backoff n-gram models,
and thus the decoder can be used independently
from the SRILM toolkit.3 We also provide a na-
tive code bridge that allows the decoder to use the
SRILM toolkit to read and score n-grams. This
native implementation is more scalable than the
basic Java LM implementation. We have also im-
plemented a Bloom Filter LM in Joshua, following
Talbot and Osborne (2007).
2.5 Minimum Error Rate Training
Johsua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu. The optimization
consists of a series of line-optimizations along
the dimensions corresponding to the parameters.
The search across a dimension uses the efficient
method of Och (2003). Each iteration of our
MERT implementation consists of multiple weight
3This feature allows users to easily try the Joshua toolkit
without installing the SRILM toolkit and compiling the native
bridge code. However, users should note that the basic Java
LM implementation is not as scalable as the native bridge
code.
updates, each reflecting a greedy selection of the
dimension giving the most gain. Each iteration
also optimizes several random ?intermediate ini-
tial? points in addition to the one surviving from
the previous iteration, as an approximation to per-
forming multiple random restarts. More details on
the MERT method and the implementation can be
found in Zaidan (2009).4
3 WMT-09 Translation Task Results
3.1 Training and Development Data
We assembled a very large French-English train-
ing corpus (Callison-Burch, 2009) by conducting
a web crawl that targted bilingual web sites from
the Canadian government, the European Union,
and various international organizations like the
Amnesty International and the Olympic Commit-
tee. The crawl gathered approximately 40 million
files, consisting of over 1TB of data. We converted
pdf, doc, html, asp, php, etc. files into text, and
preserved the directory structure of the web crawl.
We wrote set of simple heuristics to transform
French URLs onto English URLs, and considered
matching documents to be translations of each
other. This yielded 2 million French documents
paired with their English equivalents. We split the
sentences and paragraphs in these documents, per-
formed sentence-aligned them using software that
IBM Model 1 probabilities into account (Moore,
2002). We filtered and de-duplcated the result-
ing parallel corpus. After discarding 630 thousand
sentence pairs which had more than 100 words,
our final corpus had 21.9 million sentence pairs
with 587,867,024 English words and 714,137,609
French words.
We distributed the corpus to the other WMT09
participants to use in addition to the Europarl
v4 French-English parallel corpus (Koehn, 2005),
which consists of approximately 1.4 million sen-
tence pairs with 39 million English words and 44
million French words. Our translation model was
trained on these corpora using the subsampling de-
scried in Section 2.1.
For language model training, we used the
monolingual news and blog data that was as-
sembled by the University of Edinburgh and dis-
tributed as part of WMT09. This data consisted
4The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
137
of 21.2 million English sentences with half a bil-
lion words. We used SRILM to train a 5-gram
language model using a vocabulary containing the
500,000 most frequent words in this corpus. Note
that we did not use the English side of the parallel
corpus as language model training data.
To tune the system parameters we used News
Test Set from WMT08 (Callison-Burch et al,
2008), which consists of 2,051 sentence pairs
with 43 thousand English words and 46 thou-
sand French words. This is in-domain data that
was gathered from the same news sources as the
WMT09 test set.
3.2 Translation Scores
The translation scores for four different systems
are reported in Table 1.5
Baseline: In this system, we use the GIZA++
toolkit (Och and Ney, 2003), a suffix-array archi-
tecture (Lopez, 2007), the SRILM toolkit (Stol-
cke, 2002), and minimum error rate training (Och,
2003) to obtain word-alignments, a translation
model, language models, and the optimal weights
for combining these models, respectively.
Minimum Bayes Risk Rescoring: In this sys-
tem, we re-ranked the n-best output of our base-
line system using Minimum Bayes Risk (Kumar
and Byrne, 2004). We re-score the top 300 trans-
lations to minimize expected loss under the Bleu
metric.
Deterministic Annealing: In this system, in-
stead of using the regular MERT (Och, 2003)
whose training objective is to minimize the one-
best error, we use the deterministic annealing
training procedure described in Smith and Eisner
(2006), whose objective is to minimize the ex-
pected error (together with the entropy regulariza-
tion technique).
Variational Decoding: Statistical models in
machine translation exhibit spurious ambiguity.
That is, the probability of an output string is split
among many distinct derivations (e.g., trees or
segmentations). In principle, the goodness of a
string is measured by the total probability of its
many derivations. However, finding the best string
(e.g., during decoding) is then computationally in-
tractable. Therefore, most systems use a simple
Viterbi approximation that measures the goodness
5Note that the implementation of the novel techniques
used to produce the non-baseline results is not part of the cur-
rent Joshua release, though we plan to incorporate it in the
next release.
System BLEU-4
Joshua Baseline 25.92
Minimum Bayes Risk Rescoring 26.16
Deterministic Annealing 25.98
Variational Decoding 26.52
Table 1: The uncased BLEU scores on WMT-09
French-English Task. The test set consists of 2525
segments, each with one reference translation.
of a string using only its most probable deriva-
tion. Instead, we develop a variational approxima-
tion, which considers all the derivations but still
allows tractable decoding. More details will be
provided in Li et al (2009b). In this system, we
have used both deterministic annealing (for train-
ing) and variational decoding (for decoding).
4 Conclusions
We have described a scalable toolkit for parsing-
based machine translation. It is written in Java
and implements all the essential algorithms de-
scribed in Chiang (2007) and Li and Khudanpur
(2008b): chart-parsing, n-gram language model
integration, beam- and cube-pruning, and k-best
extraction. The toolkit also implements suffix-
array grammar extraction (Callison-Burch et al,
2005; Lopez, 2007) and minimum error rate train-
ing (Och, 2003). Additionally, parallel and dis-
tributed computing techniques are exploited to
make it scalable. The decoder achieves state of
the art translation performance.
Acknowledgments
This research was supported in part by the Defense
Advanced Research Projects Agency?s GALE pro-
gram under Contract No. HR0011-06-2-0001 and
the National Science Foundation under grants
No. 0713448 and 0840112. The views and find-
ings are the authors? alone.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08).
138
Chris Callison-Burch. 2009. A 109 word parallel cor-
pus. In preparation.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, , and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit,
Phuket, Thailand.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT/NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008a. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In Proceedings of AMTA.
Zhifei Li and Sanjeev Khudanpur. 2008b. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,
and Wren Thornton. 2009a. Decoding in joshua:
Open source, parsing-based machine translation.
The Prague Bulletin of Mathematical Linguistics,
91:47?56.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In preparation.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
AMTA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the ACL/Coling.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, Denver, Colorado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
139
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 129?137,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving Translation Lexicon Induction from Monolingual Corpora via
Dependency Contexts and Part-of-Speech Equivalences
Nikesh Garera, Chris Callison-Burch, David Yarowsky
Department of Computer Science, Johns Hopkins University
Baltimore MD, USA
{ngarera,ccb,yarowsky}@cs.jhu.edu
Abstract
This paper presents novel improvements
to the induction of translation lexicons
from monolingual corpora using multilin-
gual dependency parses. We introduce a
dependency-based context model that in-
corporates long-range dependencies, vari-
able context sizes, and reordering. It pro-
vides a 16% relative improvement over
the baseline approach that uses a fixed
context window of adjacent words. Its
Top 10 accuracy for noun translation is
higher than that of a statistical translation
model trained on a Spanish-English par-
allel corpus containing 100,000 sentence
pairs. We generalize the evaluation to
other word-types, and show that the per-
formance can be increased to 18% rela-
tive by preserving part-of-speech equiva-
lencies during translation.
1 Introduction
Recent trends in machine translation illustrate that
highly accurate word and phrase translations can be
learned automatically given enough parallel training
data (Koehn et al, 2003; Chiang, 2007). However,
large parallel corpora exist for only a small frac-
tion of the world?s languages, leading to a bottleneck
for building translation systems in low-density lan-
guages such as Swahili, Uzbek or Punjabi. While
parallel training data is uncommon for such lan-
guages, more readily available resources include
small translation dictionaries, comparable corpora,
and large amounts of monolingual data.
The marked difference in the availability of
monolingual vs parallel corpora has led several
researchers to develop methods for automatically
learning bilingual lexicons, either by using mono-
lingual corpora (Rapp, 1999; Koehn and Knight,
2002; Schafer and Yarowsky, 2002; Haghighi et al,
2008) or by exploiting the cross-language evidence
of closely related ?bridge? languages that have more
resources (Mann and Yarowsky, 2001).
This paper investigates new ways of learning
translations from monolingual corpora. We extend
the Rapp (1999) model of context vector projection
using a seed lexicon. It is based on the intuition that
translations will have similar lexical context, even in
unrelated corpora. For example, in order to translate
the word ?airplane?, the algorithm builds a context
vector which might contain terms such as ?passen-
gers?, ?runway?, ?airport?, etc. and words in tar-
get language that have their translations (obtained
via seed lexicon) in surrounding context can be con-
sidered as likely translations. We extend the basic
approach by formulating a context model that uses
dependency trees. The use of dependencies has the
following advantages:
? Long distance dependencies allow associated
words to be included in the context vector even
if they fall outside of the fixed-window used in
the baseline model.
? Using relationships like parent and child in-
stead of absolute positions alleviates problems
when projecting vectors between languages
with different word orders.
? It achieves better performance than baseline
context models across the board, and better
performance than statistical translation models
on Top-10 accuracy for noun translation when
trained on identical data.
129
We further show that an extension based on part-
of-speech clustering can give similar accuracy gains
for learning translations of all word-types, deepen-
ing the findings of previous literature which mainly
focused on translating nouns (Rapp, 1999; Koehn
and Knight, 2002; Haghighi et al, 2008).
2 Related Work
The literature on translation lexicon induction for
low-density languages falls in to two broad cate-
gories: 1) Effectively utilizing similarity between
languages by choosing a high-resource ?bridge? lan-
guage for translation (Mann and Yarowsky, 2001;
Schafer and Yarowsky, 2002) and 2) Extracting
noisy clues (such as similar context) from mono-
lingual corpora with help of a seed lexicon (Rapp,
1999; Koehn and Knight, 2002; Schafer and
Yarowsky, 2002, Haghighi et al, 2008). The lat-
ter category is more relevant to this work and is ex-
plained in detail below.
The idea of words with similar meaning having
similar contexts in the same language comes from
the Distributional Hypothesis (Harris, 1985) and
Rapp (1999) was the first to propose using context of
a given word as a clue to its translation. Given a Ger-
man word with an unknown translation, a German
context vector is constructed by counting its sur-
rounding words in a monolingual German corpus.
Using an incomplete bilingual dictionary, the counts
of the German context words with known transla-
tions are projected onto an English vector. The pro-
jected vector for the German word is compared to
the vectors constructed for all English words using
a monolingual English corpus. The English words
with the highest vector similarity are treated as trans-
lation candidates. The original work employed a rel-
atively large bilingual dictionary containing approx-
imately 16,000 words and tested only on a small col-
lection of 100 manually selected nouns.
Koehn and Knight (2002) tested this idea on a
larger test set consisting of the 1000 most frequent
words from a German-English lexicon. They also
incorporated clues such as frequency and ortho-
graphic similarity in addition to context. Schafer
and Yarowsky, (2002) independently proposed us-
ing frequency, orthographic similarity and also
showed improvements using temporal and word-
burstiness similarity measures, in addition to con-
text. Haghighi et al, (2008) made use of contex-
tual and orthographic clues for learning a generative
model from monolingual corpora and a seed lexicon.
All of the aforementioned work defines context
similarity in terms of the adjacent words over a win-
dow of some arbitary size (usually 2 to 4 words), as
initially proposed by Rapp (1999). We show that the
model for surrounding context can be improved by
using dependency information rather than strictly re-
lying on adjacent words, based on the success of de-
pendency trees for monolingual clustering and dis-
ambiguation tasks (Lin and Pantel, 2002; Pado and
Lapata, 2007) and the recent developments in multi-
lingual dependency parsing literature (Buchholz and
Marsi, 2006; Nivre et al, 2007).
We further differentiate ourselves from previous
work by conducting a second evaluation which ex-
amines the accuracy of translating all word types,
rather than just nouns. While the straightforward ap-
plication of context-based model gives a lower over-
all accuracy than nouns alone, we show how learn-
ing a mapping of part-of-speech tagsets between the
source and target language can result in comparable
performance to that of noun translation.
3 Translation by Context Vector
Projection
This section details how translations are discovered
from monolingual corpora through context vector
projection. Section 3.1 defines alternative ways of
modeling context vectors, and including baseline
models and our dependency-based model.
The central idea of Rapp?s method for learning
translations is that of context vector projection and
vector similarity. The goodness of semantic ?fit? of
candidate translations is measured as the vector sim-
ilarity between two words. Those vectors are drawn
from two different languages, so the vector for one
word must first be projected onto the language space
of the other. The algorithm for creating, projecting
and comparing vectors is described below, and illus-
trated in Figure 1.
Algorithm:
1. Extract context vectors:
Given a word in source language, say sw, create
a vector using the surrounding context words
and call this reference source vector rssw for
130
Figure 1: Illustration of (Rapp, 1999) model for translating spanish word ?crecimiento (growth)? via dependency context vectors
extracted from respective monolingual corpora as explained in Section 3.1.2
source word sw. The actual composition of this
vector varies depending on how the surround-
ing context is modeled. The context model is
independent of the algorithm, and various mod-
els are explained in later sections.
2. Project reference source vector:
Project all the source vector words contained in
the projection dictionary onto the vector space
for the target language, retaining the counts
from source corpus. This vector now exists in
the target language space and is called the ref-
erence target vector rtsw . This vector may be
sparse, depending on how complete the bilin-
gual dictionary is, because words without dic-
tionary entires will receive zero counts in the
reference target vector.
3. Rank candidates by vector similarity:
For each word twi in the target language a con-
text vector is created using the target language
monolingual corpora as in Step 1. Compute a
similarity score between the context vector of
twi = ?ci1, ci2, ...., cin? and reference target vec-
tor rtsw = ?r1, r2, ...., rn?. The word with the
maximum similarity score t?wi is chosen as thecandidate translation of sw.
The vector similarity can be computed in a
number of ways. Our setup we used cosine
similarity:
t?wi = argmaxtwi ci1?r1+ci2?r2+....+cin?rn?c2i1+c2i2+...+c2in?r21+r22+...+r2n
Rapp (1999) used l1-norm metric after nor-
malizing the vectors to unit length, Koehn and
Knight (2002) used Spearman rank order cor-
relation, and Schafer and Yarowsky (2002) use
cosine similarity. We found that cosine simi-
larity gave the best results in our experimental
conditions. Other similarity measures may be
used equally well.
3.1 Models of Context
We compared several context models. Empirical re-
sults for their ability to find accurate translations are
given in Section 5.
3.1.1 Baseline model
In the baseline model, the context is computed
using adjacent words as in (Rapp,1999; Koehn
and Knight, 2002; Schafer and Yarowsky, 2002;
Haghighi et al, 2008). Given a word in source lan-
guage, say sw, count all its immediate context words
appearing in a window of four words. The counts
are collected seperately for each position by keeping
track of four seperate vectors for positions -2, -1, +1
and +2. Thus each vector is a sparse vector, having
the # of dimensions as the size of source language
vocabulary. Each dimension is also reweighted by
multiplying the inverse document frequency (IDF)
131
Figure 2: Illustration of using dependency trees to model richer contexts for projection
as in the standard TF.IDF weighting scheme1. These
vectors are then concatenated into a single vector,
having dimension four times the size of the vocabu-
lary. This vector is called the reference source vector
rssw for source word sw.
3.1.2 Modeling context using dependency trees
We use dependency parsing to extend the con-
text model. Our context vectors use contexts derived
from head-words linked by dependency trees instead
of using the immediate adjacent lexical words. The
use of dependency trees for modeling contexts has
been shown to help in monolingual clustering tasks
of finding words with similar meaning (Lin and Pan-
tel, 2002) and we show how they can be effectively
used for translation lexicon induction.
Position Adjacent Dependency
Context Context
-2 para camino
-1 el para
+1 y prosperidad, y, el
+2 la econo?mica
Table 1: Contrasting context words derived from the adjacent
vs dependency models for the above example
The four vectors for positions -1, +1, -2 and +2
in the baseline model get mapped to immediate par-
ent (-1), immediate child (+1), grandparent (-2) and
grandchild (+2). An example of using the depen-
dency tree context is shown in Figure 2, and the de-
pendency context is shown in contrast with the ad-
jacent context in Table 1, showing the selection of
more salient words by using the dependencies.
Note that while we are limiting to four positions
in the tree, it does not imply that only a maximum of
four context words are selected since the word can
have multiple immediate children depending upon
the dependency parse of the sentence. Hence, this
approach allows for a dynamic context size, with the
1In order to compute the IDF, while there were no clear doc-
ument boundaries in our corpus, a virtual document boundary
was created by binning after every 1000 words.
number of context words varying with the number of
children and parents at the two levels.
Another advantage of this method is that it al-
leviates the reordering problem as we use tree po-
sitions (consisting of head-words) as compared to
the adjacent position in the baseline context model.
For example, if the source spanish word to be trans-
lated was ?prosperidad?, then in the example shown
in Figure 2, in case of adjacent context, the con-
text word ?econo?mica? will show up in +1 position
in Spanish and -1 position in English (as adjectives
come before nouns in English) but in case of depen-
dency context, the adjective will be the child of noun
and hence will show up in +1 position in both lan-
guages. Thus, we do not need to use a bag of word
model as in Section 3 in order to avoid learning the
explicit mapping that adjectives and nouns in Span-
ish and English are reversed.
4 Experimental Design
For our initial set of experiments we compared sev-
eral different vector-based context models:
? Adjbow ? A baseline model which used bag of
words model with a fixed window of 4 words,
two on either side of the word to be translated.
? Adjposn ? A second baseline that used a fixed
window of 4 words but which took positional
into account.
? Depbow ? A dependency model which did not
distinguish between grandparent, parent, child
and grandparent relations, analogous to the bag
of words model.
? Depposn ? A dependency model which did in-
clude such relationships, and was analogous to
the position-based baseline.
? Depposn + rev ? The above Depposn model ap-
plied in both directions (Spanish-to-English
and English-to-Spanish) using their sum as the
final translation score.
We contrasted the accuracy of the above methods,
which use monolingual corpora, with a statistical
132
model trained on bilingual parallel corpora. We re-
fer to that model as Mosesen-es-100k, because it was
trained using the Moses toolkit (Koehn et al, 2007).
4.1 Training Data
All context models were trained on a Spanish cor-
pus containing 100,000 sentences with 2.13 million
words and an English corpus containing 100,000
sentences with 2.07 million words. The Spanish cor-
pus was parsed using the MST dependency parser
(McDonald et al, 2005) trained using dependency
trees generated from the the English Penn Treebank
(Marcus et al, 1993) and Spanish CoNLL-X data
(Buchholz and Marsi, 2006).
So that we could directly compare against sta-
tistical translation models, our Spanish and English
monolingual corpora were drawn from the Europarl
parallel corpus (Koehn, 2005). The fact that our
two monolingual corpora are taken from a parallel
corpus ensures that the assumption that similar con-
texts are a good indicator of translation holds. This
assumption underlies in all work of translation lex-
icon induction from comparable monolingual cor-
pora, and here we strongly bias toward that assump-
tion. Despite the bias, the comparison of different
context models holds, since all models are trained
on the same data.
4.2 Evaluation Criterion
The models were evaluated in terms of exact-match
translation accuracy of the 1000 most frequent
nouns in a English-Spanish dictionary. The accuracy
was calculated by counting how many mappings ex-
actly match one of the entries in the dictionary. This
evaluation criterion is similar to the setup used by
Koehn and Knight (2002). We compute the Top N
accuracy in the standard way as the number of Span-
ish test words whose Top N English translation can-
didates contain a lexicon translation entry out of the
total number of Spanish words that can be mapped
correctly using the lexicon entries. Thus if ?crec-
imiento, growth? is the correct mapping based on the
lexicon entries, the translation for ?crecimiento? will
be counted as correct if ?growth? occurs in the Top
N English translation candidates for ?crecimiento?.
Note that the exact-match accuracy is a conser-
vative estimate as it is possible that the algorithm
may propose a reasonable translation for the given
camino
Depposn Cntxt Model Adjbow Cntxt Model
way 0.124 intentions 0.22
solution 0.097 way 0.21
steps 0.094 idea 0.20
path 0.093 thing 0.20
debate 0.085 faith 0.18
account 0.082 steps 0.17
means 0.080 example 0.17
work 0.079 news 0.16
approach 0.074 work 0.16
issue 0.073 attitude 0.15
Table 2: Top 10 translation candidates for the spanish word
?camino (way)? for the best adjacent context model (Adjbow)
and best dependency context model (Depposn). The bold English
terms show the acceptable translations.
Figure 3: Precision/Recall curve showing superior perfor-
mance of dependency context model as compared to adjacent
context at different recall points. Precision is the fraction of
tested Spanish words with Top 1 translation correct and Recall
is fraction of the 1000 Spanish words tested upon.
Spanish word but is marked incorrect if it does not
exist in the lexicon. Because it would be intractable
to compare each projected vector against the vectors
for all possible English words, we limited ourselves
to comparing the projected vector from each Spanish
word against the vectors for the 1000 most frequent
English nouns, following along the lines of previ-
ous work (Koehn and Knight, 2002; Haghighi et al,
2008).
5 Results
Table 3 gives the Top 1 and Top 10 accuracy for
each of the models on their ability to translate Span-
ish nouns into English. Examples of the top 10
translations using the best performing baseline and
dependency-based models are shown in Table 2. The
baseline models Adjposn and Adjbow differ in that the
133
Model AccTop 1 AccTop 10
Adjbow 35.3% 59.8%
Adjposn 20.9% 46.9%
Depbow 41.0% 62.0%
Depposn 41.0% 64.1%
Depposn + rev 42.9% 65.5%
Mosesen-es-100k 56.4% 62.7%
Table 3: Performance of various context-based models
learned from monolingual corpora and phrase-table learned
from parallel corpora on Noun translation.
latter disregards the position information in the con-
text vector and simply uses a bag of words instead.
Table 3 shows that Adjbow gains using this simplifi-
cation. A bag of words vector approach pools counts
together, which helps to reduce data sparsity. In
the position based model the vector is four times as
long. Additionally, the bag of words model can help
when there is local re-ordering between the two lan-
guages. For instance, Spanish adjectives often fol-
low nouns whereas in English the the ordering is
reversed. Thus, one can either learn position map-
pings, that is, position +1 for adjectives in Spanish is
the same as position -1 in English or just add the the
word counts from different positions into one com-
mon vector as considered in the bag of words ap-
proach.
Using dependency trees also alleviates the prob-
lem of position mapping between source and target
language. Table 3 shows the performance using the
dependency based models outperforms the baseline
models substantially. Comparing Depbow to Depposn
shows that ignoring the tree depth and treating it as
a bag of words does not increase the performance.
This contrasts with the baseline models. The de-
pendency positions account for re-ordering automat-
ically. The precision-recall curve in Figure 3 shows
that the dependency-based context performs better
than adjancet context at almost all recall levels.
The Mosesen-es-100k model shows the performance
of the statistical translation model trained on a bilin-
gual parallel corpus. While the system performs best
in Top 1 accuracy, the dependency context-based
model that ignores the sentence alignments surpris-
ingly performs better in case of Top 10 accuracy,
showing substantial promise.
While computing the accuracy using the phrase-
table learned from parallel corpora (Mosesen-es-100k),
the translation probabilities from both directions
(p(es|en) and p(en|es)) were used to rank the can-
didates. We also apply the monolingual context-
based model in the reverse direction (from English
to Spanish) and the row with label Depposn + rev in
Table 3 shows further gains using both directions.
Spanish English Sim Is present
Score in lexicon
sen?ores gentlemen 0.99 NO
xenofobia xenophobia 0.87 YES
diversidad diversity 0.73 YES
chipre cyprus 0.66 YES
mujeres women 0.65 YES
alemania germany 0.65 YES
explotacio?n exploitation 0.63 YES
hombres men 0.62 YES
repu?blica republic 0.60 YES
racismo racism 0.59 YES
comercio commerce 0.58 YES
continente continent 0.53 YES
gobierno government 0.52 YES
israel israel 0.52 YES
francia france 0.52 YES
fundamento certainty 0.51 NO
suecia sweden 0.50 YES
tra?fico space 0.49 NO
televisio?n tv 0.48 YES
francesa portuguese 0.48 NO
Table 4: List of 20 most confident mappings using the de-
pendency context based model for noun translation. Note that
although the first mapping is the correct one, it was not present
in the lexicon used for evaluation and hence is marked as incor-
rect.
6 Further Extensions: Generalizing to
other word types via tagset mapping
Most of the previous literature on this problem fo-
cuses on evaluating on nouns (Rapp, 1999; Koehn
and Knight 2002; Haghighi et al, 2008). However
the vector projection approach is general, and should
be applicable to other word-types as well. We eval-
uated the models with new test set containing 1000
most frequent words (not just nouns) in the English-
Spanish lexicon.
We used the dependency-based context model to
create translations for this new set. The row labeled
Depposn in Table 5 shows that the accuracy on this
set is lower when compared to evaluating only on
nouns. The main reason for lower accuracy is that
closed class words are often the most frequent and
tend to have a wide range of contexts resulting in
reasonable translation for most words include open
class words via the context model. For instance, the
English preposition ?to? appears as the most confi-
dent translation for 147 out of the 1000 Spanish test
134
Figure 4: Illustration of using part-of-speech tag mapping to
restrict candidate space of translations
words and in none (rightly so) after restricting the
translations by part-of-speech categories.
This problem can be greatly reduced by making
use of the intuition that part-of-speech is often pre-
served in translation, thus the space of possible can-
didate translation can be largely reduced based on
the part-of-speech restrictions. For example, a noun
in source language will usually be translated as noun
in target language, determiner will be translated as
determiner and so on. This idea is more clearly il-
lustrated in in Figure 4. We do not impose a hard
restriction but rather compute a ranking based on
the conditional probability of candidate translation?s
part-of-speech tag given source word?s tag.
An interesting problem in using part-of-speech re-
strictions is that corpora in different languages have
been tagged using widely different tagsets and the
following subsection explains this problem in detail:
6.1 Mapping Part-of-Speech tagsets in
different languages
The English tagset was derived from the Penn tree-
bank consisting of 53 tags (including punctuation
markers) and the Spanish tagset was derived from
the Cast3LB dataset consisting of 57 tags but there
is a large difference in the morphological and syn-
tactic features marked by the tagset. For example,
the Spanish tagset as different tags for masculine and
feminine nouns and also has a different tag for coor-
dinated nouns, all of which need to be mapped to the
singular or plural noun category available in English
tagset. Figure 5 shows an illustration of the mapping
problem between the Spanish and English POS tags.
Figure 5: Illustration of mapping Spanish part-of-speech
tagset to English tagset. The tagsets vary greatly in notation and
the morphological/syntactic constituents represented and need
to be mapped first, using the algorithm described in Section 6.1.
We now describe an empirical approach for learn-
ing the mapping between tagsets using the English-
Spanish projection dictionary used in the monolin-
gual context-based models for translation. Given a
small English-Spanish bilingual dictionary and a n-
best list of part-of-speech tags for each word in the
dictionary2, we compute conditional probability of
translating a source word with pos tag sposi to a tar-
get with pos tag tposj as follows:
p(tposj |sposi) =
c(sposi , tposj )
c(sposi) =?
sw?S, tw?T p(sposi |sw) ? p(tposj |tw) ? Idict(sw, tw)?
sw?S p(sposi |sw)
where
? S and T are the source and target vocabulary in
the seed dictionary, with sw and tw being any
of the words in the respective sets.
? p(sposi |sw), p(tposj |tw) are obtained using rel-
ative frequencies in a part-of-speech tagged
corpus in the source/target languages respec-
tively, and are used as soft counts.
? Idict(sw, tw) is the indicator function with
value 1 if the pair (sw, tw) occurs in the seed
dictionary and 0 otherwise.
In essence, the mapping between tagsets is
learned using the known translations from a small
dictionary.
Given a source word sw to translate, its most
likely tag s?pos, and the most likely mapping of this
tag into English t?pos computed as above, the transla-
tion candidates with part-of-speech tag t?pos are con-
sidered for comparison with vector similarity and
2The n-best part-of-speech tag list for any word in the dic-
tionary was derived using the relative frequencies in a part-of-
speech annotated corpora in the respective languages
135
Figure 6: Precision/Recall curve showing superior perfor-
mance of using part-of-speech equivalences for translating all
word-types. Precision is the fraction of tested Spanish words
with Top 1 translation correct and Recall is fraction of the 1000
Spanish words tested upon.
the other candidates with tposj 6= t?pos are discarded
from the candidate space. Figure 4 shows an exam-
ple of restricting the candidate space using POS tags.
Model AccTop 1 AccTop 10
Depposn 35.1% 62.9%
+ POS 41.3% 66.4%
Table 5: Performance of dependency context-based model
along with addition of part-of-speech mapping model on trans-
lating all word-types.
The row labeled +POS in Table 5 shows the part-
of-speech tags provides substantial gain as com-
pared to direct application of dependency context-
based model and is also comparable to the accuracy
obtained evaluating just on nouns in Table 3.
7 Conclusion
This paper presents a novel contribution to the stan-
dard context models used when learning transla-
tion lexicons from monolingual corpora by vector
projection. We show that using contexts based on
dependency parses can provide more salient con-
texts, allow for dynamic context size, and account
for word reordering in the source and target lan-
guage. An exact-match evaluation shows 16% rela-
tive improvement by using a dependency-based con-
text model over the standard approach. Furthermore,
we show that our model, which is trained only on
monolingual corpora, outperforms the standard sta-
Spanish English Sim Is present
Score in lexicon
sen?ores gentlemen 0.99 NO
chipre cyprus 0.66 YES
mujeres women 0.65 YES
alemania germany 0.65 YES
hombres men 0.62 YES
expresar express 0.60 YES
racismo racism 0.59 YES
interior internal 0.55 YES
gobierno government 0.52 YES
francia france 0.52 YES
cultural cultural 0.51 YES
suecia sweden 0.50 YES
fundamento basis 0.48 YES
francesa french 0.48 YES
entre between 0.47 YES
origen origin 0.46 YES
tra?fico traffic 0.45 YES
de of 0.44 YES
social social 0.43 YES
ruego thank 0.43 NO
Table 6: List of 20 most confident mappings using the depen-
dency context with the part-of-speech mapping model translat-
ing all word-types. Note that although the second best mapping
in Table4 for noun-translation is for xenofobia with score 0.87,
xenofobia is not among the 1000 most frequent words (of all
word-types) and thus is not in this test set.
tistical MT approach to learning phrase tables when
trained on the same amount of sentence-aligned par-
allel corpora, when evaluated on Top 10 accuracy.
As a second contribution, we go beyond previ-
ous literature which evaluated only on nouns. We
showed how preserving a word?s part-of-speech in
translation can improve performance. We further
proposed a solution to an interesting sub-problem
encountered on the way. Since part-of-speeech
tagsets are not identical across two languages, we
propose a way of learning their mapping automat-
ically. Restricting candidate space based on this
learned tagset mapping resulted in 18% improve-
ment over the direct application of context-based
model to all word-types.
Dependency trees help improve the context for
translation substantially and their use opens up the
question of how the context can be enriched further
making use of the hidden structure that may provide
clues for a word?s translation. We also believe that
the problem of learning the mapping between tagsets
in two different languages can be used in general for
other NLP tasks making use of projection of words
and its morphological/syntactic properties between
languages.
136
References
S. Buchholz and E. Marsi. 2006. Conll-X shared task
on multilingual dependency parsing. Proceedings of
CoNLL, pages 189?210.
Y. Cao and H. Li. 2002. Base Noun Phrase translation
using web data and the EM algorithm. Proceedings of
COLING-Volume 1, pages 1?7.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
P. Fung and L.Y. Yee. 1998. An IR Approach for
Translating New Words from Nonparallel, Compara-
ble Texts. Proceedings of ACL, 36:414?420.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. Proceedings of ACL-HLT, pages 771?779.
Z. Harris. 1985. Distributional structure. Katz, J. J. (ed.),
The Philosophy of Linguistics, pages 26?47.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. Proceedings of
ACL Workshop on Unsupervised Lexical Acquisition,
pages 9?16.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL-
HLT, pages 48?54.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. Proceedings
of ACL, companian volume, pages 177?180.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. MT Summit X.
D. Lin and P. Pantel. 2002. Discovery of inference rules
for question-answering. Natural Language Engineer-
ing, 7(04):343?360.
G.S. Mann and D. Yarowsky. 2001. Multipath transla-
tion lexicon induction via bridge languages. Proceed-
ings of NAACL, pages 151?158.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the Penn treebank. Computational Linguistics,
19(2):313?330.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. Proceedings of EMNLP-HLT, pages
523?530.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007
shared task on dependency parsing. Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL,
pages 915?932.
S. Pado and M. Lapata. 2007. Dependency-Based Con-
struction of Semantic Space Models. Computational
Linguistics, 33(2):161?199.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
Proceedings of ACL, pages 519?526.
C. Schafer and D. Yarowsky. 2002. Inducing translation
lexicons via diverse similarity measures and bridge
languages. Proceedings of COLING, pages 1?7.
137
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168?1179,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning Sentential Paraphrases from Bilingual Parallel Corpora
for Text-to-Text Generation
Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme
Center for Language and Speech Processing, and HLTCOE
Johns Hopkins University
Abstract
Previous work has shown that high quality
phrasal paraphrases can be extracted from
bilingual parallel corpora. However, it is not
clear whether bitexts are an appropriate re-
source for extracting more sophisticated sen-
tential paraphrases, which are more obviously
learnable from monolingual parallel corpora.
We extend bilingual paraphrase extraction to
syntactic paraphrases and demonstrate its abil-
ity to learn a variety of general paraphrastic
transformations, including passivization, da-
tive shift, and topicalization. We discuss how
our model can be adapted to many text gener-
ation tasks by augmenting its feature set, de-
velopment data, and parameter estimation rou-
tine. We illustrate this adaptation by using
our paraphrase model for the task of sentence
compression and achieve results competitive
with state-of-the-art compression systems.
1 Introduction
Paraphrases are alternative ways of expressing the
same information (Culicover, 1968). Automatically
generating and detecting paraphrases is a crucial as-
pect of many NLP tasks. In multi-document sum-
marization, paraphrase detection is used to collapse
redundancies (Barzilay et al, 1999; Barzilay, 2003).
Paraphrase generation can be used for query expan-
sion in information retrieval and question answer-
ing systems (McKeown, 1979; Anick and Tipirneni,
1999; Ravichandran and Hovy, 2002; Riezler et al,
2007). Paraphrases allow for more flexible matching
of system output against human references for tasks
like machine translation and automatic summariza-
tion (Zhou et al, 2006; Kauchak and Barzilay, 2006;
Madnani et al, 2007; Snover et al, 2010).
Broadly, we can distinguish two forms of para-
phrases: phrasal paraphrases denote a set of surface
text forms with the same meaning:
the committee?s second proposal
the second proposal of the committee
while syntactic paraphrases augment the surface
forms by introducing nonterminals (or slots) that are
annotated with syntactic constraints:
the NP1?s NP2
the NP2 of the NP1
It is evident that the latter have a much higher poten-
tial for generalization and for capturing interesting
paraphrastic transformations.
A variety of different types of corpora (and se-
mantic equivalence cues) have been used to auto-
matically induce paraphrase collections for English
(Madnani and Dorr, 2010). Perhaps the most nat-
ural type of corpus for this task is a monolingual
parallel text, which allows sentential paraphrases to
be extracted since the sentence pairs in such corpora
are perfect paraphrases of each other (Barzilay and
McKeown, 2001; Pang et al, 2003). While rich syn-
tactic paraphrases have been learned from monolin-
gual parallel corpora, they suffer from very limited
data availability and thus have poor coverage.
Other methods obtain paraphrases from raw
monolingual text by relying on distributional simi-
larity (Lin and Pantel, 2001; Bhagat and Ravichan-
dran, 2008). While vast amounts of data are
readily available for these approaches, the distri-
butional similarity signal they use is noisier than
the sentence-level correspondency in parallel cor-
pora and additionally suffers from problems such as
mistaking cousin expressions or antonyms (such as
{boy , girl} or {rise, fall}) for paraphrases.
1168
Abundantly available bilingual parallel corpora
have been shown to address both these issues, ob-
taining paraphrases via a pivoting step over foreign
language phrases (Bannard and Callison-Burch,
2005). The coverage of paraphrase lexica extracted
from bitexts has been shown to outperform that
obtained from other sources (Zhao et al, 2008a).
While there have been efforts pursuing the extrac-
tion of more powerful paraphrases (Madnani et
al., 2007; Callison-Burch, 2008; Cohn and Lapata,
2008; Zhao et al, 2008b), it is not yet clear to what
extent sentential paraphrases can be induced from
bitexts. In this paper we:
? Extend the bilingual pivoting approach to para-
phrase induction to produce rich syntactic para-
phrases.
? Perform a thorough analysis of the types of
paraphrases we obtain and discuss the para-
phrastic transformations we are capable of cap-
turing.
? Describe how training paradigms for syntac-
tic/sentential paraphrase models should be tai-
lored to different text-to-text generation tasks.
? Demonstrate our framework?s suitability for a
variety of text-to-text generation tasks by ob-
taining state-of-the-art results on the example
task of sentence compression.
2 Related Work
Madnani and Dorr (2010) survey a variety of data-
driven paraphrasing techniques, categorizing them
based on the type of data that they use. These
include large monolingual texts (Lin and Pantel,
2001; Szpektor et al, 2004; Bhagat and Ravichan-
dran, 2008), comparable corpora (Barzilay and Lee,
2003; Dolan et al, 2004), monolingual parallel cor-
pora (Barzilay and McKeown, 2001; Pang et al,
2003), and bilingual parallel corpora (Bannard and
Callison-Burch, 2005; Madnani et al, 2007; Zhao et
al., 2008b). We focus on the latter type of data.
Paraphrase extraction using bilingual parallel cor-
pora was proposed by Bannard and Callison-Burch
(2005) who induced paraphrases using techniques
from phrase-based statistical machine translation
(Koehn et al, 2003). After extracting a bilingual
phrase table, English paraphrases are obtained by
pivoting through foreign language phrases. Since
many paraphrases can be extracted for a phrase,
Bannard and Callison-Burch rank them using a para-
phrase probability defined in terms of the translation
model probabilities p(f |e) and p(e|f):
p(e2|e1) =
?
f
p(e2, f |e1) (1)
=
?
f
p(e2|f, e1)p(f |e1) (2)
?
?
f
p(e2|f)p(f |e1). (3)
Several subsequent efforts extended the bilin-
gual pivoting technique, many of which introduced
elements of more contemporary syntax-based ap-
proaches to statistical machine translation. Mad-
nani et al (2007) extended the technique to hier-
archical phrase-based machine translation (Chiang,
2005), which is formally a synchronous context-free
grammar (SCFG) and thus can be thought of as a
paraphrase grammar. The paraphrase grammar can
paraphrase (or ?decode?) input sentences using an
SCFG decoder, like the Hiero, Joshua or cdec MT
systems (Chiang, 2007; Li et al, 2009; Dyer et al,
2010). Like Hiero, Madnani?s model uses just one
nonterminal X instead of linguistic nonterminals.
Three additional efforts incorporated linguistic
syntax. Callison-Burch (2008) introduced syntac-
tic constraints by labeling all phrases and para-
phrases (even non-constituent phrases) with CCG-
inspired slash categories (Steedman and Baldridge,
2011), an approach similar to Zollmann and Venu-
gopal (2006)?s syntax-augmented machine transla-
tion (SAMT). Callison-Burch did not formally de-
fine a synchronous grammar, nor discuss decoding,
since his presentation did not include hierarchical
rules. Cohn and Lapata (2008) used the GHKM
extraction method (Galley et al, 2004), which is
limited to constituent phrases and thus produces
a reasonably small set of syntactic rules. Zhao
et al (2008b) added slots to bilingually extracted
paraphrase patterns that were labeled with part-of-
speech tags, but not larger syntactic constituents.
Before the shift to statistical natural language pro-
cessing, paraphrasing was often treated as syntactic
transformations or by parsing and then generating
1169
from a semantic representation (McKeown, 1979;
Muraki, 1982; Meteer and Shaked, 1988; Shem-
tov, 1996; Yamamoto, 2002). Indeed, some work
generated paraphrases using (non-probabilistic) syn-
chronous grammars (Shieber and Schabes, 1990;
Dras, 1997; Dras, 1999; Kozlowski et al, 2003).
After the rise of statistical machine translation, a
number of its techniques were repurposed for para-
phrasing. These include sentence alignment (Gale
and Church, 1993; Barzilay and Elhadad, 2003),
word alignment and noisy channel decoding (Brown
et al, 1990; Quirk et al, 2004), phrase-based models
(Koehn et al, 2003; Bannard and Callison-Burch,
2005), hierarchical phrase-based models (Chiang,
2005; Madnani et al, 2007), log-linear models and
minimum error rate training (Och, 2003a; Madnani
et al, 2007; Zhao et al, 2008a), and here syntax-
based machine translation (Wu, 1997; Yamada and
Knight, 2001; Melamed, 2004; Quirk et al, 2005).
Beyond cementing the ties between paraphrasing
and syntax-based statistical machine translation, the
novel contributions of our paper are (1) an in-depth
analysis of the types of structural and sentential
paraphrases that can be extracted with bilingual piv-
oting, (2) a discussion of how our English?English
paraphrase grammar should be adapted to specific
text-to-text generation tasks (Zhao et al, 2009) with
(3) a concrete example of the adaptation procedure
for the task of paraphrase-based sentence compres-
sion (Knight and Marcu, 2002; Cohn and Lapata,
2008; Cohn and Lapata, 2009).
3 SCFGs in Translation
The model we use in our paraphrasing approach is
a syntactically informed synchronous context-free
grammar (SCFG). The SCFG formalism (Aho and
Ullman, 1972) was repopularized for statistical ma-
chine translation by Chiang (2005). Formally, a
probabilistic SCFG G is defined by specifying
G = ?N , TS , TT ,R, S?,
whereN is a set of nonterminal symbols, TS and TT
are the source and target language vocabularies, R
is a set of rules and S ? N is the root symbol. The
rules inR take the form:
C ? ??, ?,?, w?,
PP/NN ? mit einer  |  with a
NP ? das leck  |  the leak
VP ?  NP PP/NN detonation zu schliessen  |  closing NP PP/NN blast 
they
VP
VP
PRP VBD NNDTNN
NP NPNP
closing tried the   
S
sie versuchten das zu schliessen
leak
leck
with a   blast
DT IN
PP
VBG
einermit detonation
Figure 1: Synchronous grammar rules for translation are
extracted from sentence pairs in a bixtext which have
been automatically parsed and word-aligned. Extraction
methods vary on whether they extract only minimal rules
for phrases dominated by nodes in the parse tree, or more
complex rules that include non-constituent phrases.
where the rule?s left-hand side C ? N is a nonter-
minal, ? ? (N?TS)? and ? ? (N?TT )? are strings
of terminal and nonterminal symbols with an equal
number of nonterminals cNT (?) = cNT (?) and
?: {1 . . . cNT (?)} ? {1 . . . cNT (?)}
constitutes a one-to-one correspondency function
between the nonterminals in ? and ?. A non-
negative weight w ? 0 is assigned to each rule, re-
flecting the likelihood of the rule.
Rule Extraction Phrase-based approaches to sta-
tistical machine translation (and their successors)
extract pairs of (e, f) phrases from automatically
word-aligned parallel sentences. Och (2003b)
described various heuristics for extracting phrase
alignments from the Viterbi word-level alignments
that are estimated using Brown et al (1993) word-
alignment models.
These phrase extraction heuristics have been ex-
tended so that they extract synchronous grammar
rules (Galley et al, 2004; Chiang, 2005; Zollmann
and Venugopal, 2006; Liu et al, 2006). Most of
these extraction methods require that one side of the
parallel corpus be parsed. This is typically done au-
tomatically with a statistical parser.
Figure 1 shows examples of rules obtained from
a sentence pair. To extract a rule, we first choose a
source side span f like das leck. Then we use phrase
extraction techniques to find target spans e that are
consistent with the word alignment (in this case the
1170
leak is consistent with our f ). The nonterminal sym-
bol that is the left-hand side of the SCFG rule is then
determined by the syntactic constituent that domi-
nates e (in this case NP). To introduce nonterminals
into the right-hand side of the rule, we can apply
rules extracted over sub-phrases of f , synchronously
substituting the corresponding nonterminal symbol
for the sub-phrases on both sides. The synchronous
substitution applied to f and e then yields the corre-
spondency ?.
One significant differentiating factor between the
competing ways of extracting SCFG rules is whether
the extraction method generates rules only for con-
stituent phrases that are dominated by a node in
the parse tree (Galley et al, 2004; Cohn and
Lapata, 2008) or whether they include arbitrary
phrases, including non-constituent phrases (Zoll-
mann and Venugopal, 2006; Callison-Burch, 2008).
We adopt the extraction for all phrases, including
non-constituents, since it allows us to cover a much
greater set of phrases, both in translation and para-
phrasing.
Feature Functions Rather than assigning a single
weight w, we define a set of feature functions ~? =
{?1...?N} that are combined in a log-linear model:
w = ?
N?
i=1
?i log?i. (4)
The weights ~? of these feature functions are set to
maximize some objective function like BLEU (Pap-
ineni et al, 2002) using a procedure called minimum
error rate training (MERT), owing to Och (2003a).
MERT iteratively adjusts the weights until the de-
coder produces output that best matches reference
translations in a development set, according to the
objective function. We will examine appropriate ob-
jective functions for text-to-text generation tasks in
Section 6.2.
Typical features used in statistical machine trans-
lation include phrase translation probabilities (cal-
culated using maximum likelihood estimation over
all phrase pairs enumerable in the parallel cor-
pus), word-for-word lexical translation probabili-
ties (which help to smooth sparser phrase transla-
tion estimates), a ?rule application penalty? (which
governs whether the system prefers fewer longer
they can not be dangerous to the rest of the village
VP/PP
VB+JJ
S
NP
NP/NN
sie k?nnengef?hrlich werdennichtdem rest des dorfes
VP/PP
VB+JJ
S
NP
NP/NN
NP/NN ? dem rest des  |   the rest of the
NP ? NP/NN dorfes  |  NP/NN village
VP/PP ? nicht VB+JJ k?nnen  |  can not VB+JJ
VB+JJ ? gef?hrlich werden  |  be dangerous
S ? sie NP VP/PP  |  they VP/PP to NP
Figure 2: An example derivation produced by a syntactic
machine translation system. Although the synchronous
trees are unlike the derivations found in the Penn Tree-
bank, their yield is a good translation of the German.
phrases or a greater number of shorter phrases), and
a language model probability.
Decoding Given an SCFG and an input source
sentence, the decoder performs a search for the sin-
gle most probable derivation via the CKY algorithm.
In principle the best translation should be the En-
glish sentence e that is the most probable after sum-
ming over all d ? D derivations, since many deriva-
tions yield the same e. In practice, we use a Viterbi
approximation and return the translation that is the
yield of the single best derivation:
e? = arg max
e?Trans(f)
?
d?D(e,f)
p(d, e|f)
? yield(arg max
d?D(e,f)
p(d, e|f)). (5)
Derivations are simply successive applications of the
SCFG rules such as those given in Figure 2.
4 SCFGs in Paraphrasing
Rule Extraction To create a paraphrase grammar
from a translation grammar, we extend the syntac-
tically informed pivot approach of Callison-Burch
(2008) to the SCFG model. For this purpose, we
assume a grammar that translates from a given for-
eign language to English. For each pair of trans-
lation rules where the left-hand side C and foreign
1171
string ? match:
C ? ??, ?1,?1, ~?1?
C ? ??, ?2,?2, ~?2?,
we create a paraphrase rule:
C ? ??1, ?2,?, ~??,
where the nonterminal correspondency relation ?
has been set to reflect the combined nonterminal
alignment:
? = ??11 ? ?2 .
Feature Functions In the computation of the fea-
tures ~? from ~?1 and ~?2 we follow the approximation
in Equation 3, which yields lexical and phrasal para-
phrase probability features. Additionally, we add a
boolean indicator for whether the rule is an iden-
tity paraphrase, ?identity . Another indicator feature,
?reorder , fires if the rule swaps the order of two non-
terminals, which enables us to promote more com-
plex paraphrases that require structural reordering.
Decoding With this, paraphrasing becomes an
English-to-English translation problem which can
be formulated similarly to Equation 5 as:
e?2 ? yield(arg max
d?D(e2,e1)
p(d, e2|e1)).
Figure 3 shows an example derivation produced as a
result of applying our paraphrase rules in the decod-
ing process. Another advantage of using the decoder
from statistical machine translation is that n-gram
language models, which have been shown to be use-
ful in natural language generation (Langkilde and
Knight, 1998), are already well integrated (Huang
and Chiang, 2007).
5 Analysis
A key motivation for the use of syntactic paraphrases
over their phrasal counterparts is their potential to
capture meaning-preserving linguistic transforma-
tions in a more general fashion. A phrasal system is
limited to memorizing fully lexicalized transforma-
tions in its paraphrase table, resulting in poor gener-
alization capabilities. By contrast, a syntactic para-
phrasing system intuitively should be able to address
this issue and learn well-formed and generic patterns
that can be easily applied to unseen data.
twelve cartoons insulting the
prophet
mohammad
CD
NNS
JJ DT
NNP
NP
NP
VP
NP
DT+NNP
12 the prophet mohammad
CD
NNS
JJ DT
NNP
NP
NP
VP
NP
DT+NNP
cartoons offensive
Foreign Pivot PhraseParaphrase Rule
JJ ? offensive  |   insulting
Lexical paraphrase:
NP ? NP that VP  |  NP VP
Reduced relative clause:
NP ? CD of the NNS  |  CD NNS
Partitive construction: 
VP ? are JJ to NP  |  JJ NP
Pred. adjective copula deletion:
JJ -> beleidigend  |  offensive
JJ -> beleidigend  |  insulting
NP -> NP die VP  |  NP VP
NP -> NP die VP  |  NP that VP
NP -> CD der NNS  |  CD of the NNS
NP -> CD der NNS  |  CD NNS
VP ? sind JJ f?r NP  |  are JJ to NP
VP ? sind JJ f?r NP  |  JJ NP
of the that are to
Figure 3: An example of a synchronous paraphrastic
derivation. A few of the rules applied in the parse are
show in the left column, with the pivot phrases that gave
rise to them on the right.
To put this expectation to the test, we investigate
how our grammar captures a number of well-known
paraphrastic transformations.1 Table 1 shows the
transformations along with examples of the generic
grammar rules our system learns to represent them.
When given a transformation to extract a syntactic
paraphrase for, we want to find rules that neither
under- nor over-generalize. This means that, while
replacing the maximum number of syntactic argu-
ments with nonterminals, the rules ideally will both
retain enough lexicalization to serve as sufficient ev-
idence for the applicability of the transformation and
impose constraints on the nonterminals to ensure the
arguments? well-formedness.
The paraphrases implementing the possessive rule
and the dative shift shown in Table 1 are a good
examples of this: the two noun-phrase arguments
to the expressions are abstracted to nonterminals
while each rule?s lexicalization provides an appro-
priate frame of evidence for the transform. This is
important for a good representation of dative shift,
which is a reordering transformation that fully ap-
plies to certain ditransitive verbs while other verbs
are uncommon in one of the forms:
1The data and software used to extract the grammar we draw
these examples from is described in Section 6.5.
1172
Possessive rule NP ? the NN of the NNP | the NNP ?s NNNP ? the NNS 1 made by NNS 2 | the NNS 2?s NNS 1
Dative shift VP ? give NN to NP | give NP the NNVP ? provide NP1 to NP2 | give NP2 NP1
Adv./adj. phrase move S/VP ? ADVP they VBP | they VPB ADVPS ? it is ADJP VP | VP is ADJP
Verb particle shift VP ? VB NP up | VB up NP
Reduced relative clause SBAR/S ? although PRP VBP that | although PRP VBPADJP ? very JJ that S | JJ S
Partitive constructions NP ? CD of the NN | CD NNNP ? all DT\NP | all of the DT\NP
Topicalization S ? NP , VP . | VP , NP .
Passivization SBAR? that NP had VBN | which was VBN by NP
Light verbs VP ? take action ADVP | to act ADVPVP ? TO take a decision PP | TO decide PP
Table 1: A selection of meaning-preserving transformations and hand-picked examples of syntactic paraphrases that
our system extracts capturing these.
give decontamination equipment to Japan
give Japan decontamination equipment
provide decontamination equipment to Japan
? provide Japan decontamination equipment
Note how our system extracts a dative shift rule for
to give and a rule that both shifts and substitutes a
more appropriate verb for to provide.
The use of syntactic nonterminals in our para-
phrase rules to capture complex transforms also
makes it possible to impose constraints on their ap-
plication. For comparison, as Madnani et al (2007)
do not impose any constraints on how the nontermi-
nal X can be realized, their equivalent of the topi-
calization rule would massively overgeneralize:
S ? X1, X2 . | X2, X1 .
Additional examples of transforms our use of syn-
tax allows us to capture are the adverbial phrase
shift and the reduction of a relative clause, as well
as other phenomena listed in Table 1.
Unsurprisingly, syntactic information alone is not
sufficient to capture all transformations. For in-
stance it is hard to extract generic paraphrases for all
instances of passivization, since our syntactic model
currently has no means of representing the morpho-
logical changes that the verb undergoes:
the reactor leaks radiation
radiation is leaking from the reactor .
Still, for cases where the verb?s morphology does
not change, we manage to learn a rule:
the radiation that the reactor had leaked
the radiation which leaked from the reactor .
Another example of a deficiency in our synchronous
grammar models are light verb constructs such as:
to take a walk
to walk .
Here, a noun is transformed into the corresponding
verb ? something our synchronous syntactic CFGs
are not able to capture except through memorization.
Our survey shows that we are able to extract ap-
propriately generic representations for a wide range
of paraphrastic transformations. This is a surpris-
ing result which shows that bilingual parallel cor-
pora can be used to learn sentential paraphrases, and
that they are a viable alternative to other data sources
like monolingual parallel corpora, which more obvi-
ously contain sentential paraphrases, but are scarce.
6 Text-to-Text Applications
The core of many text-to-text generation tasks is
sentential paraphrasing, augmented with specific
constraints or goals. Since our model borrows much
of its machinery from statistical machine translation
? a sentential rewriting problem itself ? it is straight-
forward to use our paraphrase grammars to generate
new sentences using SMT?s decoding and param-
eter optimization techniques. Our framework can
be adapted to many different text-to-text generation
tasks. These could include text simplification, sen-
1173
tence compression, poetry generation, query expan-
sion, transforming declarative sentences into ques-
tions, and deriving hypotheses for textual entail-
ment. Each individual text-to-text application re-
quires that our framework be adapted in several
ways, by specifying:
? A mechanism for extracting synchronous
grammar rules (in this paper we argue that
pivot-based paraphrasing is widely applicable).
? An appropriate set of rule-level features that
capture information pertinent to the task (e.g.
whether a rule simplifies a phrase).
? An appropriate ?objective function? that scores
the output of the model, i.e. a task-specific
equivalent to the BLEU metric in SMT.
? A development set with examples of the sen-
tential transformations that we are modeling.
? Optionally, a way of injecting task-specific
rules that were not extracted automatically.
In the remainder of this section, we illustrate how
our bilingually extracted paraphrases can be adapted
to perform sentence compression, which is the task
of reducing the length of sentence while preserving
its core meaning. Most previous approaches to sen-
tence compression focused only on the deletion of
a subset of words from the sentence (Knight and
Marcu, 2002). Our approach follows Cohn and La-
pata (2008), who expand the task to include substi-
tutions, insertions and reorderings that are automat-
ically learned from parallel texts.
6.1 Feature Design
In Section 4 we discussed phrasal probabilities.
While these help quantify how good a paraphrase
is in general, they do not make any statement on
task-specific things such as the change in language
complexity or text length. To make this information
available to the decoder, we enhance our paraphrases
with four compression-targeted features. We add the
count features csrc and ctgt , indicating the number of
words on either side of the rule as well as two differ-
ence features: cdcount = ctgt ? csrc and the anal-
ogously computed difference in the average word
length in characters, cdavg .
6.2 Objective Function
Given our paraphrasing system?s connection to
SMT, the naive/obvious choice for parameter op-
timization would be to optimize for BLEU over a
set of paraphrases, for instance parallel English ref-
erence translations for a machine translation task
(Madnani et al, 2007). For a candidate C and a ref-
erence R, (with lengths c and r) BLEU is defined as:
BLEUN (C,R)
=
{
e(1?c/r) ? e
?N
n=1 logwnpn if c/r ? 1
e
?N
n=1 logwnpn otherwise ,
where pn is the modified n-gram precision of C
against R, with typically N = 4 and wn = 1N . The
?brevity penalty? term e(1?c/r) is added to prevent
short candidates from achieving perfect scores.
Naively optimizing for BLEU, however, will re-
sult in a trivial paraphrasing system heavily biased
towards producing identity ?paraphrases?. This is
obviously not what we are looking for. Moreover,
BLEU does not provide a mechanism for directly
specifying a per-sentence compression rate, which
is desirable for the compression task.
Instead, we propose PRE?CIS, an objective func-
tion tailored to the text compression task:
PRE?CIS?,?(I, C,R)
=
{
e?(??c/i) ? BLEU(C,R) if c/i ? ?
BLEU(C,R) otherwise
For an input sentence I , an output C and ref-
erence compression R (with lengths i, c and r),
PRE?CIS combines the precision estimate of BLEU
with an additional ?verbosity penalty? that is ap-
plied to compressions that fail to meet a given target
compression rate ?. We rely on the BLEU brevity
penalty to prevent the system from producing overly
aggressive compressions. The scaling term ? deter-
mines how severely we penalize deviations from ?.
In our experiments we use ? = 10.
It is straightforward to find similar adaptations for
other tasks. For text simplification, for instance, the
penalty term can include a readability metric. For
poetry generation we can analogously penalize out-
puts that break the meter (Greene et al, 2010).
6.3 Development Data
To tune the parameters of our paraphrase system for
sentence compression, we need an appropriate cor-
1174
pus of reference compressions. Since our model is
designed to compress by paraphrasing rather than
deletion, the commonly used deletion-based com-
pression data sets like the Ziff-Davis corpus are not
suitable. We have thus created a corpus of com-
pression paraphrases. Beginning with 9570 tuples
of parallel English?English sentences obtained from
multiple reference translations for machine transla-
tion evaluation, we construct a parallel compression
corpus by selecting the longest reference in each tu-
ple as the source sentence and the shortest reference
as the target sentence. We further retain only those
sentence pairs where the compression rate cr falls in
the range 0.5 < cr ? 0.8. From these, we randomly
select 936 sentences for the development set, as well
as 560 sentences for a test set that we use to gauge
the performance of our system.
6.4 Grammar Augmentations
As we discussed in Section 5, the paraphrase gram-
mar we induce is capable of representing a wide va-
riety of transformations. However, the formalism
and extraction method are not explicitly geared to-
wards a compression application. For instance, the
synchronous nature of our grammar does not allow
us to perform deletions of constituents as done by
Cohn and Lapata (2007)?s tree transducers. One way
to extend the grammar?s capabilities towards the re-
quirements of a given task is by injecting additional
rules designed to capture appropriate operations.
For the compression task, this could include
adding rules to delete target-side nonterminals:
JJ ? JJ | ?
This would render the grammar asynchronous and
require adjustments to the decoding process. Al-
ternatively, we can generate rules that specifically
delete particular adjectives from the corpus:
JJ ? superfluous | ? .
In our experiments we evaluate the latter approach
by generating optional deletion rules for all adjec-
tives, adverbs and determiners.
6.5 Experimental Setup
We extracted a paraphrase grammar from the
French?English Europarl corpus (v5). The bitext
was aligned using the Berkeley aligner and the En-
glish side was parsed with the Berkeley parser. We
Grammar # Rules
total 42,353,318
w/o identity 23,641,016
w/o complex constituents 6,439,923
w/o complex const. & identity 5,097,250
Table 2: Number and distribution of rules in our para-
phrase grammar. Note the significant number of identity
paraphrases and rules with complex nonterminal labels.
obtained the initial translation grammar using the
SAMT toolkit (Venugopal and Zollmann, 2009).
The grammars we extract tend to be extremely
large. To keep their size manageable, we only con-
sider translation rules that have been seen more than
3 times and whose translation probability exceeds
10?4 for pivot recombination. Additionally, we only
retain the top 25 most likely paraphrases of each
phrase, ranked by a uniformly weighted combina-
tion of phrasal and lexical paraphrase probabilities.
We tuned the model parameters to our PRE?CIS
objective function, implemented in the Z-MERT
toolkit (Zaidan, 2009). For decoding we used the
Joshua decoder (Li et al, 2010). The language
model used in our paraphraser and the Clarke and
Lapata (2008) baseline system is a Kneser-Ney dis-
counted 5-gram model estimated on the Gigaword
corpus using the SRILM toolkit (Stolcke, 2002).
6.6 Evaluation
To assess the output quality of the resulting sentence
compression system, we compare it to two state-of-
the-art sentence compression systems. Specifically,
we compare against our implementation of Clarke
and Lapata (2008)?s compression model which uses
a series of constraints in an integer linear program-
ming (ILP) solver, and Cohn and Lapata (2007)?s
tree transducer toolkit (T3) which learns a syn-
chronous tree substitution grammar (STSG) from
paired monolingual sentences. Unlike SCFGs, the
STSG formalism allows changes to the tree topol-
ogy. Cohn and Lapata argue that this is a natural
fit for sentence compression, since deletions intro-
duce structural mismatches. We trained the T3 soft-
ware2 on the 936 ?full, compressed? sentence pairs
that comprise our development set. This is equiva-
lent in size to the training corpora that Cohn and La-
pata (2007) used (their training corpora ranged from
2www.dcs.shef.ac.uk/people/T.Cohn/t3/
1175
882?1020 sentence pairs), and has the advantage of
being in-domain with respect to our test set. Both
these systems reported results outperforming previ-
ous systems such as McDonald (2006). To showcase
the value of the adaptations discussed above, we also
compare variants of our paraphrase-based compres-
sion systems: using Hiero instead of syntax, using
syntax with or without compression features, using
an augmented grammar with optional deletion rules.
We solicit human judgments of the compres-
sions along two five-point scales: grammaticality
and meaning. Judges are instructed to decide how
much the meaning from a reference translation is
retained in the compressed sentence, with a score
of 5 indicating that all of the important information
is present, and 1 being that the compression does
not retain any of the original meaning. Similarly, a
grammar score of 5 indicates perfect grammaticality,
and a grammar score of 1 is assigned to sentences
that are entirely ungrammatical. To ensure fairness,
we perform pairwise system comparisons with com-
pression rates strictly tied on the sentence-level. For
any comparison, a sentence is only included in the
computation of average scores if the difference be-
tween both systems? compression rates is < 0.05.3
Table 4 shows a set of pairwise comparisons for
compression rates ? 0.5. We see that going from
a Hiero-based to a syntactic paraphrase grammar
yields a significant improvement in grammatical-
ity. Adding compression-specific features improves
grammaticality even further. Further augmenting the
grammar with deletion rules significantly helps re-
tain the core meaning at compression rates this high,
however compared to the un-augmented syntactic
system grammaticality scores drop. While our ap-
proach significantly outperforms the T3 system, we
are not able to match ILP?s results in grammaticality.
In Table 3 we compare our system to the ILP ap-
proach at a modest compression rate of? 0.8. Here,
we significantly outperform ILP in meaning reten-
tion while achieving comparable results in gram-
maticality. This improvement is significant at p <
0.0001, using the sign test, while the better gram-
maticality score of the ILP system is not statisti-
3Because evaluation quality correlates linearly with com-
pression rate, the community-accepted practice of not compar-
ing based on a closely tied compression rate is potentially sub-
ject to erroneous interpretation (Napoles et al, 2011).
CR Meaning Grammar
Reference 0.73 4.26 4.35
Syntax+Feat. 0.80 3.67 3.38
ILP 0.80 3.50 3.49
Random Deletions 0.50 1.94 1.57
Table 3: Results of the human evaluation on longer com-
pressions: pairwise compression rates (CR), meaning and
grammaticality scores. Bold indicates a statistically sig-
nificance difference at p < 0.05.
CR Meaning Grammar
Hiero 0.56 2.57 2.35
Syntax 0.56 2.76 2.67
Syntax 0.53 2.70 2.49
Syntax+Feat. 0.53 2.71 2.54
Syntax+Feat. 0.54 2.79 2.71
Syntax+Aug. 0.54 2.96 2.52
Syntax+Aug. 0.52 2.87 2.40
ILP 0.52 2.83 3.09
Syntax+Aug. 0.50 2.41 2.20
T3 0.50 2.01 1.93
Table 4: Human evaluation for shorter compressions and
for variations of our paraphrase system. +Feat. includes
the compression features from Section 6.1, +Aug. in-
cludes optional deletion rules from Section 6.4.
cally significant (p < 0.088). These results indi-
cate that, over a variety of compression rates, our
framework for text-to-text generation is performing
as well as or better than specifically tailored state-
of-the-art methods.
Table 5 shows an example sentence drawn from
our test set and the compressions produced by the
different systems. We see that both the paraphrase
and ILP systems produce good quality results, with
the paraphrase system retaining the meaning of the
source sentence more accurately.
7 Conclusion
In this work we introduced a method to learn syntac-
tically informed paraphrases from bilingual parallel
texts. We discussed the expressive power and limita-
tions of our formalism and outlined straightforward
adaptation strategies for applications in text-to-text
generation. We demonstrated when our paraphras-
ing system was adapted to do sentence compression,
it achieved results competitive with state-of-the-art
compression systems with only minimal effort.
1176
Source he also expected that he would have a role in the future at the level of the islamic movementacross the palestinian territories , even if he was not lucky enough to win in the elections .
Reference he expects to have a future role in the islamic movement in the palestinian territories if he isnot successful in the elections .
Syntax+Feat. he also expected that he would have a role in the future of the islamic movement in thepalestinian territories , although he was not lucky enough to win elections .
ILP he also expected that he would have a role at the level of the islamic movement , even if hewas not lucky enough to win in the elections .
Source in this war which has carried on for the last 12 days , around 700 palestinians , which includea large number of women and children , have died .
Reference about 700 palestinians , mostly women and children , have been killed in the israeli offensiveover the last 12 days .
Syntax+Feat. in this war has done for the last 12 days , around 700 palestinians , including women andchildren , died .
ILP in this war which has carried for the days palestinians , which include a number of womenand children died .
Source hala speaks arabic most of the time with her son , taking into consideration that he can speakenglish with others .
Reference hala speaks to her son mostly in arabic , as he can speak english to others .
Syntax+Feat. hala speaks arabic most of the time with her son , considering that he can speak english withothers .
ILP hala speaks arabic most of the time , taking into consideration that he can speak english withothers .
Table 5: Example compressions produced by the two systems in Table 3 for three input sentences from our test data.
Acknowledgments
We would like to thank Trevor Cohn for kindly pro-
viding us with the T3 compression system. This re-
search was supported by the NSF under grant IIS-
0713448. Opinions, interpretations, and conclusions
are the authors? alone.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Peter G. Anick and Suresh Tipirneni. 1999. The para-
phrase search assistant: terminological feedback for
iterative information seeking. In Proceedings of SI-
GIR.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings of
ACL.
Regina Barzilay. 2003. Information Fusion for Mutli-
document Summarization: Paraphrasing and Genera-
tion. Ph.D. thesis, Columbia University, New York.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Peter Brown, John Cocke, Stephen Della Pietra, Vincent
Della Pietra, Frederick Jelinek, Robert Mercer, and
1177
Paul Poossin. 1990. A statistical approach to language
translation. Computational Linguistics, 16(2), June.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Trevor Cohn and Mirella Lapata. 2007. Large margin
synchronous generation and its application to sentence
compression. In Proceedings of EMNLP-CoLing.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research (JAIR), 34:637?674.
Peter W. Culicover. 1968. Paraphrase generation and
information retrieval from stored text. Mechani-
cal Translation and Computational Linguistics, 11(1-
2):78?88.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the COLING.
Mark Dras. 1997. Representing paraphrases using syn-
chronous tree adjoining grammars. In Proceedings of
ACL.
Mark Dras. 1999. Tree Adjoining Grammar and the Re-
luctant Paraphrasing of Text. Ph.D. thesis, Macquarie
University, Australia.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL.
William Gale and Kenneth Church. 1993. A program
for aligning sentences in bilingual corpora. Compu-
atational Linguistics, 19(1):75?90.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL.
Erica Greene, Tugba Bodrumlu, and Kevin Knight. 2010.
Automatic analysis of rhythmic poetry with applica-
tions to generation and translation. In Proceedings of
EMNLP.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
EMNLP.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Raymond Kozlowski, Kathleen McCoy, and K. Vijay-
Shanker. 2003. Generation of single-sentence
paraphrases from predicate/argument structure using
lexico-grammatical resources. In Workshop On Para-
phrasing.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of n-grams in generation. In Workshop On
Natural Language Generation, Ontario, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proceedings of WMT09.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with syn-
tax, semirings, discriminative training and other good-
ies. In Proceedings of WMT10.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343?360.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parameter
tuning in statistical machine translation. In Proceed-
ings of WMT07.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
1178
Kathleen R. McKeown. 1979. Paraphrasing using given
and new information in a question-answer system. In
Proceedings of ACL.
Dan Melamed. 2004. Statistical machine translation by
parsing. In Proceedings of ACL.
Marie W. Meteer and Varda Shaked. 1988. Strategies for
effective paraphrasing. In Proceedings of COLING.
Kazunori Muraki. 1982. On a semantic model for multi-
lingual paraphrasing. In Proceedings of COLING.
Courtney Napoles, Chris Callison-Burch, and Ben-
jamin Van Durme. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Work-
shop on Monolingual Text-To-Text Generation.
Franz Josef Och. 2003a. Minimum error rate training for
statistical machine translation. In Proceedings of ACL.
Franz Josef Och. 2003b. Minimum error rate training in
statistical machine translation. In Proceedings of ACL.
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual bitext-derived
paraphrases in automatic MT evaluation. In Proceed-
ings of WMT06.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
sufrace text patterns for a question answering system.
In Proceedings of ACL.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of ACL.
Hadar Shemtov. 1996. Generation of paraphrases from
ambiguous logical forms. In Proceedings of COLING.
Stuart Shieber and Yves Schabes. 1990. Generation and
synchronous tree-adjoining grammars. In Workshop
On Natural Language Generation.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Mark Steedman and Jason Baldridge. 2011. Combi-
natory categorial grammar. In Non-Transformational
Syntax: Formal and Explicit Models of Grammar.
Wiley-Blackwell.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceeding of the International
Conference on Spoken Language Processing.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP, Pro-
ceedings of EMNLP.
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. Prague Bul-
letin of Mathematical Linguistics, 91.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of ACL.
Kazuhide Yamamoto. 2002. Machine translation by in-
teraction between paraphraser and transfer. In Pro-
ceedings of COLING.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008a. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot approach for extracting paraphrase
patterns from bilingual corpora. In Proceedings of
ACL/HLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of ACL.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of WMT06.
1179
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 590?600,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Semi-Markov Phrase-based Monolingual Alignment
Xuchen Yao and Benjamin Van Durme
Johns Hopkins University
Baltimore, MD, USA
Chris Callison-Burch?
University of Pennsylvania
Philadelphia, PA, USA
Peter Clark
Allen Institute for Artificial Intelligence
Seattle, WA, USA
Abstract
We introduce a novel discriminative model for
phrase-based monolingual alignment using a
semi-Markov CRF. Our model achieves state-
of-the-art alignment accuracy on two phrase-
based alignment datasets (RTE and para-
phrase), while doing significantly better than
other strong baselines in both non-identical
alignment and phrase-only alignment. Addi-
tional experiments highlight the potential ben-
efit of our alignment model to RTE, para-
phrase identification and question answering,
where even a naive application of our model?s
alignment score approaches the state of the art.
1 Introduction
Various NLP tasks can be treated as an alignment
problem: machine translation (aligning words in one
language with words in another language), ques-
tion answering (aligning question words with the an-
swer phrase), textual entailment recognition (align-
ing premise with hypothesis), paraphrase detection
(aligning semantically equivalent words), etc. Even
though most of these tasks involve only a single lan-
guage, alignment research has primarily focused on
the bilingual setting (i.e., machine translation) rather
than monolingual. Moreover, most work has con-
sidered token-based approaches over phrase-based.1
Here we seek to address this imbalance by proposing
better phrase-based models for monolingual word
alignment.
?Performed while faculty at Johns Hopkins University.
1In this paper we use the term token-based alignment for
one-to-one alignment and phrase-based for non one-to-one
alignment, and word alignment in general for both.
Most token-based alignment models can extrin-
sically handle phrase-based alignment to some ex-
tent. For instance, in the case of NYC align-
ing to New York City, the single source word
NYC may align three times separately to the tar-
get words: NYC?New, NYC?York, NYC?City.
Or in the case of identical alignment, New York
City aligning to New York City is simply
New?New, York?York, City?City. How-
ever, it is not as clear how to token-align New York
(as a city) with New York City. The problem is
more prominent when aligning phrasal paraphrases
or multiword expressions, such as pass away and
kick the bucket. This suggests an intrinsi-
cally phrase-based alignment model.
The token aligner jacana-align (Yao et al, 2013a)
has achieved state-of-the-art result on the task of
monolingual alignment, based on previous work of
Blunsom and Cohn (2006). It employs a Conditional
Random Field (Lafferty et al, 2001) to align tokens
from the source sentence to tokens in the target sen-
tence, by treating source tokens as ?observation? and
target tokens as ?hidden states?. However, it is not
designed to handle phrase-based alignment, largely
due to the Markov nature of the underlying model:
a state can only span one token each time, making
it unable to align multiple consecutive tokens (i.e. a
phrase). We extend this model by introducing semi-
Markov states for phrase-based alignment: a state
can instead span multiple consecutive time steps,
thus aligning phrases on the source side. Also, we
merge phrases on the target side to phrasal states,
allowing the model to align phrases on the target
side as well. We evaluate the resulting semi-Markov
590
CRF model on the task of phrase-based alignment,
and then show a basic application in the NLP tasks
of recognizing textual entailment, paraphrase iden-
tification, and question answering sentence ranking.
The final phrase-based aligner is open-source.2
2 Related Work
Most work in monolingual alignment employs de-
pendency tree/graph matching algorithms, includ-
ing tree edit distance (Punyakanok et al, 2004;
Kouylekov and Magnini, 2005; Heilman and Smith,
2010; Yao et al, 2013b), Particle Swarm Optimiza-
tion (Mehdad, 2009), linear regression/classification
models (Chambers et al, 2007; Wang and Manning,
2010), and min-cut (Roth and Frank, 2012). These
works inherently only support token-based align-
ment, with phrase-like alignment achieved by first
merging tokens to phrases as a preprocessing step.
The MANLI aligner (MacCartney et al, 2008)
and its derivations (Thadani and McKeown, 2011;
Thadani et al, 2012) are the first known phrase-
based aligners specifically designed for aligning En-
glish sentence pairs. It applies discriminative per-
ceptron learning with various features and handles
phrase-based alignment of arbitrary phrase lengths.
MANLI suffers from slow decoding time due to its
large search space. This was optimized by Thadani
and McKeown (2011) through Integer Linear Pro-
gramming (ILP), where benefiting from modern ILP
solvers they showed an order-of-magnitude speedup
in decoding. Also, various syntactic constraints can
be easily added, significantly improving exact align-
ment match rate for whole sentence pairs. Besides
the common application of textual entailment and
question answering, monolingual alignment has also
been applied in the field of text generation (Barzilay
and Lee, 2003; Pang et al, 2003).
Word alignment has been more explored in ma-
chine translation. The IBM models (Brown et al,
1993) allow many-to-one alignment and are essen-
tially asymmetric. Phrase-based MT historically
relied on heuristics (Koehn, 2010) to merge two
sets of word alignment in opposite directions to
yield phrasal alignment. Later, researchers explored
non-heuristic phrase-based methods. Among them,
Marcu and Wong (2002) described a joint proba-
2http://code.google.com/p/jacana/
bility model that generates both the source and tar-
get sentences simultaneously. All possible pairs of
phrases in both sentences are enumerated and then
pruned with statistical evidence. Deng and Byrne
(2008) explored token-to-phrase alignment based
on HMM models (Vogel et al, 1996) by explic-
itly modeling the token-to-phrase probability and
phrase lengths. However, the token-to-phrase align-
ment is only in one direction: each target state still
only spans one source word, and thus alignment on
the source side is limited to tokens. Andre?s-Ferrer
and Juan (2009) extended the HMM-based method
to Hidden Semi-Markov Models (HSMM) (Osten-
dorf et al, 1996), allowing phrasal alignments on
the source side. Finally, Bansal et al (2011) unified
the HSMM models with the alignment by agreement
framework (Liang et al, 2006), achieving phrasal
alignment that agreed in both directions.
Despite successful usage of generative semi-
Markov models in bilingual alignment, this has not
been followed with models in discriminative mono-
lingual alignment. Essentially monolingual align-
ment would benefit more from discriminative mod-
els with various feature extractions (just like those
defined in MANLI) than generative models without
any predefined feature (just like how they were used
in bilingual alignment). To combine the strengths of
both semi-Markov models and discriminative train-
ing, we propose to use the semi-Markov Conditional
Random Field (Sarawagi and Cohen, 2004), which
was first used in information extraction to tag con-
tinuous segments of input sequences and outper-
formed conventional CRFs in the task of named en-
tity recognition. We describe this model in the fol-
lowing section.
3 The Alignment Model
Our objective is to define a model that supports
phrase-based alignment of arbitrary phrase length.
In this section we first describe a regular CRF
model that supports one-to-one token-based align-
ment (Blunsom and Cohn, 2006; Yao et al, 2013a),
then extend it to phrase-based alignment with the
semi-Markov model.
591
3.1 Token-based Model
Given a source sentence s of length M , and a target
sentence t of lengthN , the alignment from s to t is a
sequence of target word indices a, where ai?[1,M ] ?
[0, N ]. We specify that when ai = 0, source word si
is aligned to a NULL state, i.e., deleted. This models
a many-to-one alignment from source to target: mul-
tiple source words can be aligned to the same target
word, but not vice versa. One-to-many alignment
can be obtained by running the aligner in the other
direction. The probability of alignment sequence a
conditioned on both s and t is then:
p(a | s, t) =
exp(
?
i,k ?kfk(ai?1, ai, s, t))
Z(s, t)
This assumes a first-order Conditional Random
Field (Lafferty et al, 2001). Since the word align-
ment task is evaluated over F1, instead of directly
optimizing it, we choose a much easier objective
(Gimpel and Smith, 2010) and add a cost function
to the normalizing function Z(s, t) in the denomi-
nator:
Z(s, t) =
?
a?
exp(
?
i,k
?kfk(a?i?1, a?i, s, t)
+cost(ay, a?))
where ay is the true alignments. cost(ay, a?) can
be viewed as special ?features? that encourage de-
coding to be consistent with true labels. It is only
computed during training in the denominator be-
cause in the numerator cost(ay,ay) = 0. Ham-
ming cost is used in practice without learning the
weights (i.e., uniform weights). The more inconsis-
tence there is between ay and a?, the more penalized
is the decoding sequence a? through the cost func-
tion.
3.2 Phrase-based Model
The token-based model supports 1 : 1 alignment.
We first extend it in the direction of ls : 1, where
a target state spans ls words on the source side (ls
source words align to 1 target word). Then we ex-
tend it in the direction of 1 : lt, where lt is the tar-
get phrase length a source word aligns to (1 source
word aligns to lt target words). The final combined
shops areShops closed up for now until March
NULL
closed
temp.
are
Shops
down
shops-are
...-... 7..14
0
1
2
3
4
5
6
closed-down 15
Figure 1: A semi-Markov phrase-based model
example and the desired Viterbi decoding path.
Shaded horizontal circles represent the source
sentence (Shops are closed up for now
until March) and hollow vertical circles repre-
sent the hidden states with state IDs for the target
sentence (Shops are temporarily closed
down). State 0, a NULL state, is designated for dele-
tion. One state (e.g. state 3 and 15) can span multi-
ple consecutive source words (a semi-Markov prop-
erty) for aligning phrases on the source side. States
with an ID larger than the target sentence length
indicate ?phrasal states? (states 6-15 in this exam-
ple), where consecutive target tokens are merged for
aligning phrases on the target side. Combining the
semi-Markov property and phrasal states yields for
instance, a 2?2 alignment between closed up in
the source and closed down in the target.
model supports ls : lt alignment. Throughout this
section we use Figure 1 as an illustrative example,
which shows phrasal alignment between the source
sentence: (Shops are closed up for now
until March) and the target sentence: (Shops
are temporarily closed down).
1 : 1 alignment is a special case of ls : 1 align-
ment where the target side state spans ls = 1 source
word, i.e., at each time step i, the source side word
592
si aligns to one state ai and the next aligned state
ai+1 only depends on the current state ai. This is
the Markovian property of the CRF. When ls > 1,
during the time frame [i, i + ls), all source words
[ai, ai+ls) share the same state ai. Or in other words,
the state ai ?spans? the following ls time steps. The
Markovian property still holds ?outside? the time
frame ls, i.e., ai+ls still only depends on ai, the pre-
vious state ls time steps ago. But ?within? the time
frame ls, the Markovian property does not hold any
more: [ai, ..., ai+ls?1] are essentially the same state
ai. This is the semi-Markov property . States can be
distinguished by this property into two types: semi-
Markovian states and Markovian states.
We have generalized the regular CRF to a semi-
Markov CRF. Now we define it by generalizing the
feature function:
p(a | s, t) =
exp(
?
i,k,ls ?kfk(ai?ls , ai, s, t))
Z(s, t)
At time i, the k-th feature function fk mainly
extracts features from the pair of source words
(si?ls , ..., si] and target word tai (still with a spe-
cial case that ai = 0 marks for deletion). Inference
is still Viterbi-like: except for the fact during maxi-
mization, the Viterbi algorithm not only checks the
previous one time step, but all ls time steps. Sup-
pose the allowed maximal source phrase length is
Ls, define Vi(a | s, t) as the highest score along the
decoding path until time i ending with state a:
Vi(a | s, t) = max
a1,a2,...ai?1
p(a1, a2, . . . , ai = a | s, t)
then the recursive maximization is:
Vi(a | s, t) = max
a?
max
ls=1...Ls
[Vi?ls(a
?
| s, t)
+?i(a
?
, a, ls, s, t)]
with factor:
?i(a
?
, a, ls, s, t) =
?
k
?kfk(a
?
i?ls , ai, s, t)
and the best alignment a can be obtained by back-
tracking the last state aM from VM (aM | s, t).
Training a semi-Markov CRF is very similar to
the inference, except for replacing maximization
with summation. The forward-backward algorithm
should also be used to dynamically compute the nor-
malization function Z(s, t). Compared to regular
CRFs, a semi-Markov CRF has a decoding time
complexity of O(LsMN2), a constant factor Ls
(usually 3 or 4) slower.
To extend from 1 : 1 alignment to 1 : lt alignment
with one source word aligning to lt target words,
we simply explode the state space by Lt times with
Lt the maximal allowed target phrase length. Thus
the states can be represented as an N ? Lt ma-
trix. The state at (j, lt) represents the target phrase
[tj , ..., tj+lt). In this paper we distinguish states by
three types: NULL state (j = 0, lt = 0), token state
(lt = 1) and phrasal state (lt > 1).
To efficiently store and compute these states, we
linearize the two dimensional matrix with a linear
function mapping uniquely between the state ID and
the target phrase offset/span. Suppose the target
phrase tj of length ltj ? [1, Lt] holds a position
ptj ? [1, N ], and the source word si is aligned to
this state (ptj , ltj ), a tuple for (position, span). Then
state ID asi is computed as:
asi(ptj , ltj ) =
{
ptj ltj = 1
N + (ptj ? 1)? Lt + ltj 1 < ltj ? Lt
Assume in Figure 1, Lt = 2, then the state ID for
the phrasal state (5, 2) closed-down with ptj = 5
for the position of word down and ltj = 2 for the
span of 2 words (looking ?backward? from the word
down) is: 5 + (5? 1)? 2 + 2 = 15.
Similarly, given a state id asi , the original target
phrase position and length can be recovered through
integer division and modulation. Thus during decod-
ing, if one output state is 15, we would know that it
uniquely comes from the phrasal state (5,2), repre-
senting the target phrase closed down.
This two dimensional definition of state space ex-
pands the number of states from 1 + N to 1 +
LtN . Thus the decoding complexity becomes
O(M(LtN)2) = O(L2tMN
2) with a usual value
of 3 or 4 for Lt.
Now we have defined separately the ls : 1 model
and the 1 : lt model. We can simply merge them to
593
have an ls : lt alignment model. The semi-Markov
property makes it possible for any target states to
align phrases on the source side, while the two di-
mensional state mapping makes it possible for any
source words to align phrases on the target side. For
instance, in Figure 1, the phrasal state a15 repre-
sents the two-word phrase closed down on the
target side, while still spanning for two words on the
source side, allowing a 2? 2 alignment. State a15 is
phrasal, and at source word position 3 and 4 (span-
ning closed up) it is semi-Markovian. The final
decoding complexity is O(LsL2tMN
2), a factor of
30 ? 60 times slower than the token-based model
(with a typical value of 3 or 4 for Ls and Lt).
In the following we describe features.
3.3 Feature Design
We reused features in the original token-based
model based on string similarity, POS tags, position,
WordNet, distortion and context. Then we used an
additional chunker to mark phrase boundaries only
for feature extraction:
Chunking Features are binary indicators of
whether the phrase types of two phrases match.
Also, we added indicators for mappings between
source phrase types and target phrase types, such as
?vp2np?, meaning that a verb phrase in the source is
mapped to a noun phrase in the target.
Moreover, we introduced the following lexical
features:
PPDB Features (Ganitkevitch et al, 2013) in-
clude various similarity scores derived from a para-
phrase database with 73 million phrasal and 8 mil-
lion lexical paraphrases. Various paraphrase condi-
tional probability was employed. For instance, for
the ADJP/VP phrase pair capable of and able
to, there are the following minus-log probabilities:
p(lhs|e1) = 0.1, p(lhs|e2) = 0.3, p(e1|lhs) = 5.0
p(e1|e2) = 1.3, p(e2|lhs) = 6.7, p(e2|e1) = 2.8
p(e1|e2, lhs) = 0.6, p(e2|e1, lhs) = 2.3
where e1/e2 are the phrase pair, and lhs is the
left hand side syntactic non-terminal symbol. We
did not use the syntactic part (e.g., the NP of
NNS ? the NNS of NP) of PPDB as we did not
make the assumption that the input sentence pairs
were well-formed (and newswire-like) English, or
even of a language with a parser available. Also, for
phrasal alignments, we ruled out those paraphrases
spanning multiple syntactic structures, or of differ-
ent syntactic structures (indicated as [X] in PPDB),
for instance, and crazy? , mad.
Semantic Relatedness Feature is a single scaled
number in [0, 1] from the best performing system
(Han et al, 2013) of the *Sem 2013 Semantic Tex-
tual Similarity (STS) task. We included this fea-
ture mainly to deal with cases where ?related? words
cannot be well measured by either paraphrases or
distributional similarities. For instance, in one align-
ment dataset annotators aligned married with
wife. Adding a few other words as comparison, the
Han et al (2013) system gives the following similar-
ity scores:
married/wife: 0.85
married/husband: 0.84
married/child: 0.10
married/stone: 0.01
Name Phylogeny Feature (Andrews et al, 2012)
is a similarity feature with a string transducer to
model how one name evolves to another. Examples
below show how similar is the name Bill associ-
ated with other names in log probability:
Bill/Bill: -0.8
Bill/Billy: -5.2
Bill/William: -13.6
Bill/Mary: -18.6
Finally, one decision we made during feature
design was not to use any parsing-based features,
with a permissive assumption that the input might
not be well-formed English, or even not complete
sentences (such as fragmented snippets from web
search). The ?deepest? linguistic processing stays at
the level of tagging and chunking, making the model
more easily extendable to other languages.
3.4 Feature Value
In this phrase-based model, the width of a state span
over the source words depends on the competition
between features fired on the phrases as a whole vs.
the consecutive but individual tokens. We found it
critical to assign feature values ?fairly? among to-
kens and phrases to make sure that semi-Markov
states and phrasal states fire up often enough for
phrasal alignments.
594
train test length %align.
MSR06 800 800 29/11 36%
Edinburgh++ 715 305 22/22 78%
Table 1: Statistics of the two manually aligned cor-
pora, divided into training and test in sentence pairs.
The length column shows average lengths of source
and target sentences in a pair. %align. is the per-
centage of aligned tokens.
To illustrate this in a simplified way, take
closed up?closed down in Figure 1, and as-
sume the only feature is the normalized number of
matching tokens in the pair. Then this feature firing
on the following pairs would have values (the nor-
malization factor is the maximal phrase length):
closed?closed 1.0
closed up?closed 0.5
closed up?up 0.5
closed up?closed down 0.5
...?... ...
The desired alignment closed up?closed
down would not have survived the state com-
petition due to its weak feature value. In this
case the model would simply prefer a token align-
ment closed?closed and up?... (probably
NULL).
Thus we upweighted feature values by the max-
imum source or target phrase length to encour-
age phrasal alignments, in this case closed up
?closed down:1.0. Then this alignment would
have a better chance to be picked out with additional
features, such as with the PPDB and Semantic Relat-
edness Features, which are also upweighted by max-
imum phrase lengths.
4 Experiment
4.1 Data Preparation
There are two annotated datasets for training and
testing. MSR063 (Brockett, 2007) has annotated
alignments on the 2006 PASCAL RTE2 develop-
ment and test corpora, with 1600 pairs in total.
3http://www.cs.biu.ac.il/?nlp/files/RTE_
2006_Aligned.zip
1x1 1x2 1x3 2x2 2x3 3x3 more
MSR06 89.2 1.9 0.3 5.7 0.0 1.9 0.8
EDB++ 81.9 3.5 0.8 8.3 0.4 3.0 2.1
Table 2: Percentage of various alignment sizes
(undirectional, e.g., 1x2 and 2x1 are merged) af-
ter synthesizing phrasal alignment from token align-
ment in the training portion of two corpora.
Semantically equivalent words and phrases in the
premise and hypothesis sentences are aligned in a
manner analogous to alignments in statistical ma-
chine translation. This dataset is asymmetric: on
average the premises contain 29 words and the hy-
potheses 11 words. Edinburgh++4 (Thadani et al,
2012) is a revised version of the Edinburgh para-
phrase corpus(Cohn et al, 2008) with sentences
from the following resources: 1. the Multiple-
Translation Chinese corpus; 2. Jules Verne?s novel
Twenty Thousand Leagues Under the Sea. 3. the
Microsoft Research paraphrase corpus (Dolan et al,
2004). The corpus is more balanced and symmetric:
the source and target sentences are both 22 words
long on average. Table 1 shows some statistics.
Both corpora contain mostly token-based align-
ment. For MSR06, MacCartney et al (2008) showed
that setting the allowable phrase size to be greater
than one only increased F1 by 0.2%. For Ed-
inburgh++, the annotation guideline5 explicitly in-
structs to ?prefer smaller alignments whenever pos-
sible?. Statistics shows that single token alignment
counts 96% and 95% of total alignments in these two
corpora separately. With such a heavy imbalance to-
wards only token-based alignment, a phrase-based
aligner would learn feature weights that award token
alignments more than phrasal alignments.
Thus we synthesized phrasal alignments from
continuous monotonic token alignments in these two
corpora. We first ran the OpenNLP chunker through
the corpora. Then for each phrase pair, if each token
in the source phrase is aligned to a token in the tar-
get phrase in a monotonic way, and vice versa, we
4http://www.ling.ohio-state.edu/?scott/
#edinburgh-plusplus
5http://staffwww.dcs.shef.ac.uk/people/
T.Cohn/paraphrase_guidelines.pdf
595
merge these alignments to form one single phrasal
alignment.6 Table 2 lists the percentage of vari-
ous alignment sizes after the merge. Two obser-
vations can be made: first, the portion of phrasal
alignments increases to 10% ? 20% after merging;
second, allowing a maximal phrase length of 3 cov-
ers 98% ? 99% of total alignments, thus a phrase
length larger than 3 would be a bad trade-off for cov-
erage vs speed.
4.2 Baselines and Evaluation Metrics
MacCartney et al (2008) and Yao et al (2013a)
showed that the traditional MT bilingual aligner
GIZA++ (Och and Ney, 2003) presented weak re-
sults on the task of monolingual alignment. Thus
we instead used four other strong baselines:
Meteor (Denkowski and Lavie, 2011): a sys-
tem for evaluating machine translation by aligning
MT output with reference sentences. It is designed
for the task of monolingual alignment and supports
phrasal alignment. We used version 1.4 and default
weights to optimize by maximum accuracy.
MANLI-constraint (Thadani and McKeown,
2011): a re-implemented MANLI system with ILP-
powered decoding for speed and hard syntactic con-
straints to boost exact match rate, with reported
numbers on MSR06.
MANLI-joint (Thadani et al, 2012): an im-
proved version of MANLI-constraint that not only
models phrasal alignments, but also alignments be-
tween dependency arcs, with reported numbers on
the original Edinburgh paraphrase corpus.
jacana-token (Yao et al, 2013a): a token-
based aligner with state-of-the-art performance on
MSR06.
Note that the jacana-token aligner is open-source,
so we were able to re-train it with exactly the
same feature set used by our phrase-based model.
This allows a fair comparison of model performance
(token-based vs. phrase-based). The MANLI* sys-
tems are not available, thus we only reported their
numbers from published papers.
The standard evaluation metrics for alignments
are precision (P), recall (R), F1, and exact matching
6a few examples: two Atlanta-based
companies?two Atlanta companies, the
UK?the UK, the 17-year-old?the teenager,
was held?was held.
rate (E) based on either tokens (two tokens are con-
sidered aligned iff they are aligned) or phrases (two
tokens are considered aligned iff they are contained
within phrases that are aligned). Following Thadani
et al (2012), we only report the results based on
token alignments (which allows a partial credit if
their containing phrases are not aligned), even for
the phrase-based alignment task. The reasoning is
that if a phrase-based aligner is already doing bet-
ter than a token aligner in terms of token alignment
scores, then the difference in terms of phrase align-
ment scores will be even larger. Thus showing the
superiority of token alignment scores is sufficient.
4.3 Implementation and Training
The elements in the phrase-based model: dynamic
state indices, semi-Markov and phrasal states, are
not typically found in standard CRF implementa-
tions. Thus we implemented the phrase-based model
in the Scala programming language, which is fully
interoperable with Java, using one semi-Markov
CRF package7 as a reference. We used the L2 reg-
ularizer and LBFGS for optimization. OpenNLP8
provided the POS tagger and chunker and JWNL9
interfaced with WordNet (Fellbaum, 1998).
4.4 Results
Table 3 gives scores (in bigger fonts) of different
aligners on MSR06 and Edinburgh++ and their cor-
responding phrasal versions. Overall, the token-
based aligner did the best on the original corpora, in
which single token alignment counts more than 95%
of total alignment. The phrase-based aligner did
slightly worse. We think the main reason was that it
output more phrasal alignment, which in turn harms
scores in token-based evaluation (for instance, if the
gold alignment is New?New, York?York, then
the phrasal alignment of New York?New York
would only have half the precision because it inher-
ently also aligns New in the source with York in
the target.). Further investigation showed that on the
Edinburgh++ corpus, over-generated phrase-based
alignment, when evaluated under just token align-
ment, contributed hurting about 1.1% of overall F1,
7http://crf.sf.net
8http://opennlp.apache.org/
9http://jwordnet.sf.net/
596
a gap that would make the phrase aligner (85.9%)
outperform the token aligner (86.4%).
On the phrasal alignment corpora (represented by
MSR06P and EDB++P in Table 3), the phrase-based
aligner did significantly better. Note that the over-
all F1 and exact match rate are still much lower
than those scores obtained from the original corpora,
suggesting that the phrasal corpora present a much
harder task. Furthermore, as a more ?fair? com-
parison between the two aligners, we synthesized
phrasal alignments from the output of the token-
based aligner, just as how the phrased-based corpora
were prepared, then evaluated its performance again.
Still, on the EDB++P corpus, the token aligner was
about 1.6% (current difference is 69.1% vs. 72.8%)
worse than the phrase-based aligner.
Also, we want to emphasize that since the token-
based aligner and the phrase-based aligner shared
exactly the same features and lexical resources, the
performance boost of the phrase-based aligner on
the phrasal corpora results from a better model de-
sign: it is the semi-Markov property and phrasal
states making the phrase-based aligner better.
To further investigate the performance of aligners
with respect to different types of alignment, we di-
vided the scores into those for identical alignments
(such as New?New) and non-identical alignments
(such as wife?spouse), indicated by the sub-
scripts i and n in Table 3. In terms of identical
alignment, most aligners were able to score more
than 90%, but for non-identical alignment there was
noticeable decrease. Still, on the phrasal alignment
corpora, the phrase-based model has a much larger
recall score for non-identical alignment than others.
We also divided scores with respect to token-only
alignment and phrase-only alignment. Due to space
limit, we only show results on synthesized Edin-
burgh++, in Table 4. Meteor and the token aligner
inherently have either very limited or no support for
phrasal alignment, thus they had very low scores
on phrase-only alignment. We then ran the align-
ers in two directions and merged the results with the
?union? MT heuristic to get better phrase support.
But that still did not bring F1p?s up to over 5%.
The phrase-based aligner baseline Meteor did
worse than our aligners. We think there are two rea-
sons: First, Meteor was not trained on these corpora.
Second, Meteor only does strict word, stem, syn-
System
P% R% F1%
E%
Pi/Pn Ri/Rn F1i/F1n
M
S
R
06
(7
8.
6%
) Meteor
82.5 81.2 81.9
15.0
89.9/39.9 97.3/24.6 93.5/30.5
MANLI-cons. 89.5 86.2 87.8 33.0
token
93.6 83.5 88.3
32.1
96.6/77.7 96.9/35.6 96.8/48.8
phrase
92.1 82.8 86.8
29.1
95.7/65.0 95.9/34.7 95.8/45.2
M
S
R
06
P
(5
9.
0%
) Meteor
82.5 68.3 74.7
7.3
89.9/40.1 97.3/8.8 93.5/14.5
token
92.9 66.1 77.2
13.5
95.5/77.5 94.3/11.1 94.9/19.5
phrase
83.5 77.0 80.1
14.3
94.9/55.5 94.2/48.1 94.5/51.5
E
D
B
+
+
(7
5.
2%
) Meteor
88.3 80.5 84.2
12.7
94.0/61.4 97.8/24.1 95.9/34.7
MANLI-jnt* 76.6 83.8 79.2 12.2
token
91.3 82.0 86.4
15.0
96.4/63.9 97.4/36.4 96.9/46.4
phrase
90.4 81.9 85.9
13.7
96.0/57.4 97.8/38.3 96.9/46.0
E
D
B
+
+
P
(5
1.
7%
) Meteor
88.4 60.6 71.9
2.9
94.0/61.9 97.0/6.5 95.5/11.7
token
90.7 55.8 69.1
2.3
96.2/58.6 91.3/7.1 93.7/12.7
phrase
82.3 65.3 72.8
1.6
95.6/60.4 93.1/34.3 94.4/43.8
Table 3: Results on original (mostly token) and phrasal
(P) alignment corpora, where (x%) indicates how much
alignment is identical alignment, such as New?New. E%
stands for exact (perfect) match rate. Subscript i stands
for corresponding scores for ?identical? alignment and n
for ?non-identical?. *: scores of MANLI-joint were for
the original Edinburgh corpus instead of Edinburgh++
(with hand corrections) so it is not a direct comparison.
onym and paraphrase matching but does not use any
string similarity measures; this can be supported by
the large difference between, for instance, F1i and
F1n. In general Meteor did well on identical align-
ment, but not so well on non-identical alignment.
5 Applications
Natural language alignment can be applied to vari-
ous NLP tasks. While how to most effectively apply
597
System
P% R% F1%
E%
Pt/Pp Rt/Rp F1t/F1p
E
D
B
+
+
P
Meteor
88.4 60.6 71.9
2.9
59.5/14.9 90.6/1.1 71.8/2.0
token
90.7 55.8 69.1
2.3
59.4/21.4 85.5/0.9 70.1/1.7
phrase
82.3 65.3 72.8
1.6
73.3/48.0 73.5/44.2 73.4/46.0
Table 4: Same results on the phrasal Edinburgh++ cor-
pus but with scores divided by token-only alignment
(subscript t) and phrase-only alignment (subscript p).
it is another topic, we simply show in this section us-
ing just alignment scores in binary prediction prob-
lems. Specifically, we pick the tasks of recognizing
textual entailment (RTE), paraphrase identification
(PP), and question answering sentence ranking (QA)
described in Heilman and Smith (2010):
RTE: predicting whether a hypothesis can be in-
ferred from the premise, with training data from
RTE-1/2 and RTE-3 dev, and test from RTE-3 test.
PP: predicting whether two sentences are para-
phrases, with training and test data from the MSR
Paraphrase Corpus (Dolan et al, 2004).
QA: predicting whether a sentence contains the
answer to the question, with training data from
TREC-8 to TREC-12 and test data from TREC-13.
For each aligned pair, we can compute a normal-
ized decoding score. Following MacCartney et al
(2008), we select a threshold score and predict true
if the decoding score is above this threshold. For the
tasks of RTE and PP, we tuned this threshold w.r.t
the maximal accuracy on the training set, then re-
ported performance on the test set. For the task of
QA, since the evaluation methods in Mean Average
Precision and Mean Reciprocal Rank only need a
ranked list of answer sentences, and the scores on
the test set are sufficient to provide the ranking, we
did not tune anything on training but instead directly
ran the aligner on the test set. All three tasks shared
the same aligner model trained on the superset of
MSR06 and Edinburgh++. Results are reported in
Table 5. We could not report on Meteor as Meteor
does not explicitly output alignment scores.
We did not expect the aligners to beat any of the
system A% P% R%
de Marneffe et al (2006) 60.5 61.8 60.2
MacCartney and Manning (2008) 64.3 65.5 63.9
Heilman and Smith (2010) 62.8 61.9 71.2
the token aligner 59.1 61.2 55.4
our phrasal aligner 57.6 57.2 68.8
(a) Recognizing Textual Entailment
system A% P% R%
Wan et al (2006) 75.6 77 90
Das and Smith (2009) 73.9 74.9 91.3
Heilman and Smith (2010) 73.2 75.7 87.8
the token aligner 70.0 72.6 88.1
our phrasal aligner 68.1 68.6 95.8
(b) Paraphrase Identification
system MAP MRR
Cui et al (2005) 0.4271 0.5259
Wang et al (2007) 0.6029 0.6852
Heilman and Smith (2010) 0.6091 0.6917
Yao et al (2013b) 0.6307 0.7477
the token aligner 0.5982 0.6582
our phrasal aligner 0.6165 0.7333
(c) Question Answering Sentence Ranking
Table 5: Results (Accuracy, Precision, Recall, Mean
Average Precision, Mean Reciprocal Rank) on the
tasks of RTE, PP and QA.
state-of-the-art result since no sophisticated models
were additionally used but only the alignment score.
Still, the aligners showed competitive performance.
It still follows the pattern from the alignment exper-
iment that the phrasal aligner had higher recall and
lower precision than the token aligner in the task of
RTE and PP. In the QA task, the phrasal aligner per-
formed better than all systems except for the top one.
6 Conclusion
We have introduced a phrase-to-phrase alignment
model based on semi-Markov Conditional Random
Fields. The combination of semi-Markov states and
phrasal states makes phrasal alignment on both the
source and target sides possible. The final phrase-
598
based aligner performed the best on two phrasal
alignment corpora and showed its potential usage
in three NLP tasks. Future work includes aligning
discontinuous (gappy) phrases and integrating align-
ment more closely in NLP applications.
Acknowledgement
We thank Vulcan Inc. for funding this work. We also
thank Jason Smith, Travis Wolfe, Frank Ferraro and
the three anonymous reviewers for their comments
and suggestion.
References
Jesu?s Andre?s-Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-markov approach to machine trans-
lation. In Procedings of European Association for
Machine Translation (EAMT), Barcelona, Spain, May.
European Association for Machine Translation.
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: a generative model of string
variation. In Proceedings of EMNLP 2012.
Mohit Bansal, Chris Quirk, and Robert Moore. 2011.
Gappy phrasal alignment by agreement. In Proceed-
ings of ACL, Portland, Oregon, June.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL, pages
16?23.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of ACL2006, pages 65?72.
Chris Brockett. 2007. Aligning the RTE 2006 corpus.
Technical report, Microsoft Research.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational linguistics, 19(2):263?311.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh, and
Christopher D Manning. 2007. Learning alignments
and leveraging natural logic. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 165?170.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4):597?614, December.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings
of the 28th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, SIGIR ?05, pages 400?407, New York, NY,
USA. ACM.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 468?476, Suntec,
Singapore, August. Association for Computational
Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christo-
pher D Manning. 2006. Learning to distinguish valid
textual entailments. In Second Pascal RTE Challenge
Workshop.
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. Audio, Speech, and Language Processing, IEEE
Transactions on, 16(3):494?507.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of COLING, Stroudsburg, PA, USA.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL-HLT, pages 758?
764.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
margin CRFs: training log-linear models with cost
functions. In NAACL 2010, pages 733?736.
Lushan Han, Abhay Kashyap, Tim Finin, James May-
field, and Jonathan Weese. 2013. UMBC-EBIQUITY-
CORE: Semantic Textual Similarity Systems. In Pro-
ceedings of the Second Joint Conference on Lexical
and Computational Semantics.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL 2010, pages 1011?1019, Los Angeles, Cali-
fornia, June.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Milen Kouylekov and Bernardo Magnini. 2005. Recog-
nizing textual entailment with tree edit distance algo-
rithms. In PASCAL Challenges on RTE, pages 17?20.
599
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL.
Bill MacCartney and Christopher D Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of ACL 2008,
pages 521?528.
Bill MacCartney, Michel Galley, and Christopher D Man-
ning. 2008. A phrase-based alignment model for nat-
ural language inference. In Proceedings of EMNLP,
pages 802?811.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP-2002, pages 133?139.
Yashar Mehdad. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 289?292.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19?51.
Mari Ostendorf, Vassilios V Digalakis, and Owen A Kim-
ball. 1996. From HMM?s to segment models: a uni-
fied view of stochastic modeling for speech recogni-
tion. IEEE Transactions on Speech and Audio Pro-
cessing, 4(5):360?378.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL, pages 102?109.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2004.
Mapping Dependencies Trees: An Application to
Question Answering. In Proceedings of the 8th In-
ternational Symposium on Artificial Intelligence and
Mathematics, Fort Lauderdale, Florida.
Michael Roth and Anette Frank. 2012. Aligning
predicates across monolingual comparable texts using
graph-based clustering. In Proceedings of EMNLP-
CoNLL, pages 171?182, Jeju Island, Korea, July.
Sarawagi Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Advances in Neural Information Processing
Systems, 17:1185?1192.
Kapil Thadani and Kathleen McKeown. 2011. Optimal
and syntactically-informed decoding for monolingual
phrase-based alignment. In Proceedings of ACL short.
Kapil Thadani, Scott Martin, and Michael White. 2012.
A joint phrasal and dependency model for paraphrase
alignment. In Proceedings of COLING 2012: Posters,
pages 1229?1238, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics - Volume 2, COLING ?96, pages
836?841.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.
2006. Using dependency-based features to take the
?para-farce? out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of COLING, pages 1164?1172,
Stroudsburg, PA, USA.
Mengqiu Wang, Noah A. Smith, and Teruko Mitamura.
2007. What is the Jeopardy Model? A Quasi-
Synchronous Grammar for QA. In Proceedings of
EMNLP-CoNLL, pages 22?32, Prague, Czech Repub-
lic, June.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013a. A Lightweight and
High Performance Monolingual Word Aligner. In
Proceedings of ACL 2013 short, Sofia, Bulgaria.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013b. Answer Extraction as
Sequence Tagging with Tree Edit Distance. In Pro-
ceedings of NAACL 2013.
600
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 130?140,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Toward Statistical Machine Translation without Parallel Corpora
Alexandre Klementiev Ann Irvine Chris Callison-Burch David Yarowsky
Center for Language and Speech Processing
Johns Hopkins University
Abstract
We estimate the parameters of a phrase-
based statistical machine translation sys-
tem from monolingual corpora instead of a
bilingual parallel corpus. We extend exist-
ing research on bilingual lexicon induction
to estimate both lexical and phrasal trans-
lation probabilities for MT-scale phrase-
tables. We propose a novel algorithm to es-
timate reordering probabilities from mono-
lingual data. We report translation results
for an end-to-end translation system us-
ing these monolingual features alone. Our
method only requires monolingual corpora
in source and target languages, a small
bilingual dictionary, and a small bitext for
tuning feature weights. In this paper, we ex-
amine an idealization where a phrase-table
is given. We examine the degradation in
translation performance when bilingually
estimated translation probabilities are re-
moved and show that 80%+ of the loss can
be recovered with monolingually estimated
features alone. We further show that our
monolingual features add 1.5 BLEU points
when combined with standard bilingually
estimated phrase table features.
1 Introduction
The parameters of statistical models of transla-
tion are typically estimated from large bilingual
parallel corpora (Brown et al 1993). However,
these resources are not available for most lan-
guage pairs, and they are expensive to produce in
quantities sufficient for building a good transla-
tion system (Germann, 2001). We attempt an en-
tirely different approach; we use cheap and plen-
tiful monolingual resources to induce an end-to-
end statistical machine translation system. In par-
ticular, we extend the long line of work on in-
ducing translation lexicons (beginning with Rapp
(1995)) and propose to use multiple independent
cues present in monolingual texts to estimate lex-
ical and phrasal translation probabilities for large,
MT-scale phrase-tables. We then introduce a
novel algorithm to estimate reordering features
from monolingual data alone, and we report the
performance of a phrase-based statistical model
(Koehn et al 2003) estimated using these mono-
lingual features.
Most of the prior work on lexicon induction
is motivated by the idea that it could be applied
to machine translation but stops short of actu-
ally doing so. Lexicon induction holds the po-
tential to create machine translation systems for
languages which do not have extensive parallel
corpora. Training would only require two large
monolingual corpora and a small bilingual dictio-
nary, if one is available. The idea is that intrin-
sic properties of monolingual data (possibly along
with a handful of bilingual pairs to act as exam-
ple mappings) can provide independent but infor-
mative cues to learn translations because words
(and phrases) behave similarly across languages.
This work is the first attempt to extend and apply
these ideas to an end-to-end machine translation
pipeline. While we make an explicit assumption
that a table of phrasal translations is given a priori,
we induce every other parameter of a full phrase-
based translation system from monolingual data
alone. The contributions of this work are:
? In Section 2.2 we analyze the challenges
of using bilingual lexicon induction for sta-
tistical MT (performance on low frequency
items, and moving from words to phrases).
? In Sections 3.1 and 3.2 we use multiple cues
present in monolingual data to estimate lexi-
cal and phrasal translation scores.
? In Section 3.3 we propose a novel algo-
rithm for estimating phrase reordering fea-
tures from monolingual texts.
? Finally, in Section 5 we systematically drop
feature functions from a phrase table and
then replace them with monolingually es-
timated equivalents, reporting end-to-end
translation quality.
130
2 Background
We begin with a brief overview of the stan-
dard phrase-based statistical machine translation
model. Here, we define the parameters which
we later replace with monolingual alternatives.
We continue with a discussion of bilingual lex-
icon induction; we extend these methods to es-
timate the monolingual parameters in Section 3.
This approach allows us to replace expensive/rare
bilingual parallel training data with two large
monolingual corpora, a small bilingual dictionary,
and ?2,000 sentence bilingual development set,
which are comparatively plentiful/inexpensive.
2.1 Parameters of phrase-based SMT
Statistical machine translation (SMT) was first
formulated as a series of probabilistic mod-
els that learn word-to-word correspondences
from sentence-aligned bilingual parallel corpora
(Brown et al 1993). Current methods, includ-
ing phrase-based (Och, 2002; Koehn et al 2003)
and hierarchical models (Chiang, 2005), typically
start by word-aligning a bilingual parallel cor-
pus (Och and Ney, 2003). They extract multi-
word phrases that are consistent with the Viterbi
word alignments and use these phrases to build
new translations. A variety of parameters are es-
timated using the bitexts. Here we review the pa-
rameters of the standard phrase-based translation
model (Koehn et al 2007). Later we will show
how to estimate them using monolingual texts in-
stead. These parameters are:
? Phrase pairs. Phrase extraction heuristics
(Venugopal et al 2003; Tillmann, 2003;
Och and Ney, 2004) produce a set of phrase
pairs (e, f) that are consistent with the word
alignments. In this paper we assume that the
phrase pairs are given (without any scores),
and we induce every other parameter of the
phrase-based model from monolingual data.
? Phrase translation probabilities. Each
phrase pair has a list of associated fea-
ture functions (FFs). These include phrase
translation probabilities, ?(e|f) and ?(f |e),
which are typically calculated via maximum
likelihood estimation.
? Lexical weighting. Since MLE overestimates
? for phrase pairs with sparse counts, lexi-
cal weighting FFs are used to smooth. Aver-
How
much
should
you
charge
for
your
W
i
e
v
i
e
l
s
o
l
l
t
e
m
a
n
a
u
f
r
g
u
n
d
s
e
i
n
e
s
P
r
o
fi
l
s
i
n
F
a
c
e
b
o
o
k
v
e
r
d
i
e
n
e
n
Facebook
profile
s
d
m
m
m
d
d
Figure 1: The reordering probabilities from the phrase-
based models are estimated from bilingual data by cal-
culating how often in the parallel corpus a phrase pair
(f, e) is orientated with the preceding phrase pair in
the 3 types of orientations (monotone, swapped, and
discontinuous).
age word translation probabilities, w(ei|fj),
are calculated via phrase-pair-internal word
alignments.
? Reordering model. Each phrase pair (e, f)
also has associated reordering parameters,
po(orientation|f, e), which indicate the dis-
tribution of its orientation with respect to the
previously translated phrase. Orientations
are monotone, swap, discontinuous (Tillman,
2004; Kumar and Byrne, 2004), see Figure 1.
? Other features. Other typical features are
n-gram language model scores and a phrase
penalty, which governs whether to use fewer
longer phrases or more shorter phrases.
These are not bilingually estimated, so we
can re-use them directly without modifica-
tion.
The features are combined in a log linear model,
and their weights are set through minimum error
rate training (Och, 2003). We use the same log
linear formulation and MERT but propose alterna-
tives derived directly from monolingual data for
all parameters except for the phrase pairs them-
selves. Our pipeline still requires a small bitext of
approximately 2,000 sentences to use as a devel-
opment set for MERT parameter tuning.
131
2.2 Bilingual lexicon induction for SMT
Bilingual lexicon induction describes the class of
algorithms that attempt to learn translations from
monolingual corpora. Rapp (1995) was the first
to propose using non-parallel texts to learn the
translations of words. Using large, unrelated En-
glish and German corpora (with 163m and 135m
words) and a small German-English bilingual dic-
tionary (with 22k entires), Rapp (1999) demon-
strated that reasonably accurate translations could
be learned for 100 German nouns that were not
contained in the seed bilingual dictionary. His al-
gorithm worked by (1) building a context vector
representing an unknown German word by count-
ing its co-occurrence with all the other words
in the German monolingual corpus, (2) project-
ing this German vector onto the vector space of
English using the seed bilingual dictionary, (3)
calculating the similarity of this sparse projected
vector to vectors for English words that were con-
structed using the English monolingual corpus,
and (4) outputting the English words with the
highest similarity as the most likely translations.
A variety of subsequent work has extended the
original idea either by exploring different mea-
sures of vector similarity (Fung and Yee, 1998)
or by proposing other ways of measuring simi-
larity beyond co-occurence within a context win-
dow. For instance, Schafer and Yarowsky (2002)
demonstrated that word translations tend to co-
occur in time across languages. Koehn and Knight
(2002) used similarity in spelling as another kind
of cue that a pair of words may be translations of
one another. Garera et al(2009) defined context
vectors using dependency relations rather than ad-
jacent words. Bergsma and Van Durme (2011)
used the visual similarity of labeled web images
to learn translations of nouns. Additional related
work on learning translations from monolingual
corpora is discussed in Section 6.
In this paper, we apply bilingual lexicon in-
duction methods to statistical machine translation.
Given the obvious benefits of not having to rely
on scarce bilingual parallel training data, it is sur-
prising that bilingual lexicon induction has not
been used for SMT before now. There are sev-
eral open questions that make its applicability to
SMT uncertain. Previous research on bilingual
lexicon induction learned translations only for a
small number of high frequency words (e.g. 100
l
llll
lll
l
0 100 200 300 400 500 600
0
10
20
30
40
Accu
racy
, %
Corpus Frequency
l Top 1Top 10
Figure 2: Accuracy of single-word translations in-
duced using contextual similarity as a function of the
source word corpus frequency. Accuracy is the pro-
portion of the source words with at least one correct
(bilingual dictionary) translation in the top 1 and top
10 candidate lists.
nouns in Rapp (1995), 1,000 most frequent words
in Koehn and Knight (2002), or 2,000 most fre-
quent nouns in Haghighi et al(2008)). Although
previous work reported high translation accuracy,
it may be misleading to extrapolate the results to
SMT, where it is necessary to translate a much
larger set of words and phrases, including many
low frequency items.
In a preliminary study, we plotted the accuracy
of translations against the frequency of the source
words in the monolingual corpus. Figure 2 shows
the result for translations induced using contex-
tual similarity (defined in Section 3.1). Unsur-
prisingly, frequent terms have a substantially bet-
ter chance of being paired with a correct transla-
tion, with words that only occur once having a low
chance of being translated accurately.1 This prob-
lem is exacerbated when we move to multi-token
phrases. As with phrase translation features esti-
mated from parallel data, longer phrases are more
sparse, making similarity scores less reliable than
for single words.
Another impediment (not addressed in this
paper) for using lexicon induction for SMT is
the number of translations that must be learned.
Learning translations for all words in the source
language requires n2 vector comparisons, since
each word in the source language vocabulary must
1For a description of the experimental setup used to pro-
duce these translations, see Experiment 8 in Section 5.2.
132
s1
s
2
s
3
s
N-1
s
N
?
?
?
t
1
t
2
t
3
t
M-1
t
M
?
?
dict.
project
?
?
?
?
?
?
c
o
m
p
a
r
e
para crecer
to expand
activity of
economic
activity
policy
growth
foreign
economico
tasa
planeta
empleo
extranjero
policy
para crecer
(projected)
ES Context
Vector
Projected ES
Context Vector
EN Context
Vectors
Figure 3: Scoring contextual similarity of phrases:
first, contextual vectors are projected using a small
seed dictionary and then compared with the target lan-
guage candidates.
be compared against the vectors for all words in
the target language vocabulary. The size of the n2
comparisons hugely increases if we compare vec-
tors for multi-word phrases instead of just words.
In this work, we avoid this problem by assuming
that a limited set of phrase pairs is given a pri-
ori (but without scores). By limiting ourselves
to phrases in a phrase table, we vastly limit the
search space of possible translations. This is an
idealization because high quality translations are
guaranteed to be present. However, as our lesion
experiments in Section 5.1 show, a phrase table
without accurate translation probability estimates
is insufficient to produce high quality translations.
We show that lexicon induction methods can be
used to replace bilingual estimation of phrase- and
lexical-translation probabilities, making a signifi-
cant step towards SMT without parallel corpora.
3 Monolingual Parameter Estimation
We use bilingual lexicon induction methods to es-
timate the parameters of a phrase-based transla-
tion model from monolingual data. Instead of
scores estimated from bilingual parallel data, we
make use of cues present in monolingual data to
provide multiple orthogonal estimates of similar-
ity between a pair of phrases.
3.1 Phrasal similarity features
Contextual similarity. We extend the vector
space approach of Rapp (1999) to compute sim-
ilarity between phrases in the source and tar-
get languages. More formally, assume that
(s1, s2, . . . sN ) and (t1, t2, . . . tM ) are (arbitrarily
indexed) source and target vocabularies, respec-
tively. A source phrase f is represented with an
terrorist (en)terrorista (es)
Occ
urre
nces
terrorist (en)riqueza (es)
Occ
urre
nces
Time
Figure 4: Temporal histograms of the English phrase
terrorist, its Spanish translation terrorista, and riqueza
(wealth) collected from monolingual texts spanning a
13 year period. While the correct translation has a
good temporal match, the non-translation riqueza has
a distinctly different signature.
N - and target phrase e with an M -dimensional
vector (see Figure 3). The component values of
the vector representing a phrase correspond to
how often each of the words in that vocabulary
appear within a two word window on either side
of the phrase. These counts are collected using
monolingual corpora. After the values have been
computed, a contextual vector f is projected onto
the English vector space using translations in a
seed bilingual dictionary to map the component
values into their appropriate English vector posi-
tions. This sparse projected vector is compared
to the vectors representing all English phrases e.
Each phrase pair in the phrase table is assigned
a contextual similarity score c(f, e) based on the
similarity between e and the projection of f .
Various means of computing the component
values and vector similarity measures have been
proposed in literature (e.g. Rapp (1999), Fung and
Yee (1998)). Following Fung and Yee (1998), we
compute the value of the k-th component of f ?s
contextual vector as follows:
wk = nf,k ? (log(n/nk) + 1)
where nf,k and nk are the number of times sk ap-
pears in the context of f and in the entire corpus,
and n is the maximum number of occurrences of
any word in the data. Intuitively, the more fre-
quently sk appears with f and the less common
it is in the corpus in general, the higher its com-
ponent value. Similarity between two vectors is
measured as the cosine of the angle between them.
Temporal similarity. In addition to contex-
tual similarity, phrases in two languages may
133
be scored in terms of their temporal similarity
(Schafer and Yarowsky, 2002; Klementiev and
Roth, 2006; Alfonseca et al 2009). The intu-
ition is that news stories in different languages
will tend to discuss the same world events on the
same day. The frequencies of translated phrases
over time give them particular signatures that will
tend to spike on the same dates. For instance, if
the phrase asian tsunami is used frequently dur-
ing a particular time span, the Spanish transla-
tion maremoto asia?tico is likely to also be used
frequently during that time. Figure 4 illustrates
how the temporal distribution of terrorist is more
similar to Spanish terrorista than to other Span-
ish phrases. We calculate the temporal similar-
ity between a pair of phrases t(f, e) using the
method defined by Klementiev and Roth (2006).
We generate a temporal signature for each phrase
by sorting the set of (time-stamped) documents in
the monolingual corpus into a sequence of equally
sized temporal bins and then counting the number
of phrase occurrences in each bin. In our exper-
iments, we set the window size to 1 day, so the
size of temporal signatures is equal to the num-
ber of days spanned by our corpus. We use cosine
distance to compare the normalized temporal sig-
natures for a pair of phrases (f, e).
Topic similarity. Phrases and their translations
are likely to appear in articles written about the
same topic in two languages. Thus, topic or cat-
egory information associated with monolingual
data can also be used to indicate similarity be-
tween a phrase and its candidate translation. In
order to score a pair of phrases, we collect their
topic signatures by counting their occurrences in
each topic and then comparing the resulting vec-
tors. We again use the cosine similarity mea-
sure on the normalized topic signatures. In our
experiments, we use interlingual links between
Wikipedia articles to estimate topic similarity. We
treat each linked article pair as a topic and collect
counts for each phrase across all articles in its cor-
responding language. Thus, the size of a phrase
topic signature is the number of article pairs with
interlingual links in Wikipedia, and each compo-
nent contains the number of times the phrase ap-
pears in (the appropriate side of) the correspond-
ing pair. Our Wikipedia-based topic similarity
feature, w(f, e), is similar in spirit to polylingual
topic models (Mimno et al 2009), but it is scal-
able to full bilingual lexicon induction.
3.2 Lexical similarity features
In addition to the three phrase similarity features
used in our model ? c(f, e), t(f, e) and w(f, e) ?
we include four additional lexical similarity fea-
tures for each of phrase pair. The first three lex-
ical features clex(f, e), tlex(f, e) and wlex(f, e)
are the lexical equivalents of the phrase-level con-
textual, temporal and wikipedia topic similarity
scores. They score the similarity of individual
words within the phrases. To compute these
lexical similarity features, we average similarity
scores over all possible word alignments across
the two phrases. Because individual words are
more frequent than multiword phrases, the accu-
racy of clex, tlex, and wlex tends to be higher than
their phrasal equivalents (this is similar to the ef-
fect observed in Figure 2).
Orthographic / phonetic similarity. The final
lexical similarity feature that we incorporate is
o(f, e), which measures the orthographic similar-
ity between words in a phrase pair. Etymolog-
ically related words often retain similar spelling
across languages with the same writing system,
and low string edit distance sometimes signals
translation equivalency. Berg-Kirkpatrick and
Klein (2011) present methods for learning cor-
respondences between the alphabets of two lan-
guages. We can also extend this idea to language
pairs not sharing the same writing system since
many cognates, borrowed words, and names re-
main phonetically similar. Transliterations can be
generated for tokens in a source phrase (Knight
and Graehl, 1997), with o(f, e) calculating pho-
netic similarity rather than orthographic.
The three phrasal and four lexical similarity
scores are incorporated into the log linear trans-
lation model as feature functions, replacing the
bilingually estimated phrase translation probabil-
ities ? and lexical weighting probabilities w. Our
seven similarity scores are not the only ones that
could be incorporated into the translation model.
Various other similarity scores can be computed
depending on the available monolingual data and
its associated metadata (see, e.g. Schafer and
Yarowsky (2002)).
3.3 Reordering
The remaining component of the phrase-based
SMT model is the reordering model. We
introduce a novel algorithm for estimating
134
Input: Source and target phrases f and e,
Source and target monolingual corpora Cf and Ce,
Phrase table pairs T = {(f (i), e(i))}Ni=1.
Output: Orientation features (pm, ps, pd).
Sf ? sentences containing f in Cf ;
Se ? sentences containing e in Ce;
(Bf ,?,?)? CollectOccurs(f,?Ni=1f
(i), Sf );
(Be, Ae, De)? CollectOccurs(e,?Ni=1e
(i), Se);
cm = cs = cd = 0;
foreach unique f ? in Bf do
foreach translation e? of f ? in T do
cm = cm + #Be (e
?);
cs = cs + #Ae (e
?);
cd = cd + #De (e
?);
c? cm + cs + cd;
return ( cmc ,
cs
c ,
cd
c )
CollectOccurs(r, R, S)
B ? (); A? (); D ? ();
foreach sentence s ? S do
foreach occurrence of phrase r in s do
B ? B + (longest preceding r and in R);
A? A + (longest following r and in R);
D ? D + (longest discontinuous w/ r and in
R);
return (B, A, D);
Figure 5: Algorithm for estimating reordering
probabilities from monolingual data.
po(orientation|f, e) from two monolingual cor-
pora instead a bitext.
Figure 1 illustrates how the phrase pair orienta-
tion statistics are estimated in the standard phrase-
based SMT pipeline. For a phrase pair like (f =
?Profils?, e = ?profile?), we count its orien-
tation with the previously translated phrase pair
(f ? = ?in Facebook?, e? = ?Facebook?) across
all translated sentence pairs in the bitext.
In our pipeline we do not have translated sen-
tence pairs. Instead, we look for monolingual
sentences in the source corpus which contain
the source phrase that we are interested in, like
f = ?Profils?, and at least one other phrase
that we have a translation for, like f ? = ?in
Facebook?. We then look for all target lan-
guage sentences in the target monolingual cor-
pus that contain the translation of f (here e =
?profile?) and any translation of f ?. Figure 6 il-
lustrates that it is possible to find evidence for
po(swapped|Profils, profile), even from the non-
parallel, non-translated sentences drawn from two
independent monolingual corpora. By looking for
foreign sentences containing pairs of adjacent for-
eign phrases (f, f ?) and English sentences con-
D
a
s
A
n
l
e
g
e
n
e
i
n
e
s
P
r
o
fi
l
s
i
n
F
a
c
e
b
o
o
k
i
s
t
e
i
n
f
a
c
h
s
What
does
your
Facebook
profile
reveal
Figure 6: Collecting phrase orientation statistics for
a English-German phrase pair (?profile?, ?Profils?)
from non-parallel sentences (the German sentence
translates as ?Creating a Facebook profile is easy?).
taining their corresponding translations (e, e?), we
are able to increment orientation counts for (f, e)
by looking at whether e and e? are adjacent,
swapped, or discontinuous. The orientations cor-
respond directly to those shown in Figure 1.
One subtly of our method is that shorter and
more frequent phrases (e.g. punctuation) are more
likely to appear in multiple orientations with a
given phrase, and therefore provide poor evi-
dence of reordering. Therefore, we (a) collect
the longest contextual phrases (which also appear
in the phrase table) for reordering feature estima-
tion, and (b) prune the set of sentences so that
we only keep a small set of least frequent contex-
tual phrases (this has the effect of dropping many
function words and punctuation marks and and re-
lying more heavily on multi-word content phrases
to estimate the reordering).2
Our algorithm for learning the reordering pa-
rameters is given in Figure 5. The algorithm
estimates a probability distribution over mono-
tone, swap, and discontinuous orientations (pm,
ps, pd) for a phrase pair (f, e) from two mono-
lingual corpora Cf and Ce. It begins by calling
CollectOccurs to collect the longest match-
ing phrase table phrases that precede f in source
monolingual data (Bf ), as well as those that pre-
cede (Be), follow (Ae), and are discontinuous
(De) with e in the target language data. For each
unique phrase f ? preceding f , we look up transla-
tions in the phrase table T. Next, we count3 how
2The pruning step has an additional benefit of minimizing
the memory needed for orientation feature estimations.
3#L(x) returns the count of object x in list L.
135
Monolingual training corpora
Europarl Gigaword Wikipedia
date range 4/96-10/09 5/94-12/08 n/a
uniq shared dates 829 5,249 n/a
Spanish articles n/a 3,727,954 59,463
English articles n/a 4,862,876 59,463
Spanish lines 1,307,339 22,862,835 2,598,269
English lines 1,307,339 67,341,030 3,630,041
Spanish words 28,248,930 774,813,847 39,738,084
English words 27,335,006 1,827,065,374 61,656,646
Spanish-English phrase table
Phrase pairs 3,093,228
Spanish phrases 89,386
English phrases 926,138
Spanish unigrams 13,216
Avg # translations 98.7
Spanish bigrams 41,426
Avg # translations 31.9
Spanish trigrams 34,744
Avg # translations 13.5
Table 1: Statistics about the monolingual training data and the phrase table that was used in all of the experiments.
many translations e? of f ? appeared before, after
or were discontinuous with e in the target lan-
guage data. Finally, the counts are normalized and
returned. These normalized counts are the values
we use as estimates of po(orientation|f, e).
4 Experimental Setup
We use the Spanish-English language pair to test
our method for estimating the parameters of an
SMT system from monolingual corpora. This al-
lows us to compare our method against the nor-
mal bilingual training procedure. We expect bilin-
gual training to result in higher translation qual-
ity because it is a more direct method for learn-
ing translation probabilities. We systematically
remove different parameters from the standard
phrase-based model, and then replace them with
our monolingual equivalents. Our goal is to re-
cover as much of the loss as possible for each of
the deleted bilingual components.
The standard phrase-based model that we use
as our top-line is the Moses system (Koehn et
al., 2007) trained over the full Europarl v5 par-
allel corpus (Koehn, 2005). With the exception
of maximum phrase length (set to 3 in our ex-
periments), we used default values for all of the
parameters. All experiments use a trigram lan-
guage model trained on the English side of the
Europarl corpus using SRILM with Kneser-Ney
smoothing. To tune feature weights in minimum
error rate training, we use a development bitext
of 2,553 sentence pairs, and we evaluate per-
formance on a test set of 2,525 single-reference
translated newswire articles. These development
and test datasets were distributed in the WMT
shared task (Callison-Burch et al 2010).4 MERT
4Specifcially, news-test2008 plus news-syscomb2009 for
dev and newstest2009 for test.
was re-run for every experiment.
We estimate the parameters of our model from
two sets of monolingual data, detailed in Table 1:
? First, we treat the two sides of the Europarl
parallel corpus as independent, monolingual
corpora. Haghighi et al(2008) also used
this method to show how well translations
could be learned from monolingual corpora
under ideal conditions, where the contextual
and temporal distribution of words in the two
monolingual corpora are nearly identical.
? Next, we estimate the features from truly
monolingual corpora. To estimate the con-
textual and temporal similarity features, we
use the Spanish and English Gigaword cor-
pora.5 These corpora are substantially larger
than the Europarl corpora, providing 27x as
much Spanish and 67x as much English for
contextual similarity, and 6x as many paired
dates for temporal similarity. Topical simi-
larity is estimated using Spanish and English
Wikipedia articles that are paired with inter-
language links.
To project context vectors from Spanish to En-
glish, we use a bilingual dictionary containing en-
tries for 49,795 Spanish words. Note that end-to-
end translation quality is robust to substantially
reducing dictionary size, but we omit these ex-
periments due to space constraints. The con-
text vectors for words and phrases incorporate co-
occurrence counts using a two-word window on
either side.
The title of our paper uses the word towards be-
cause we assume that an inventory of phrase pairs
is given. Future work will explore inducing the
5We use the afp, apw and xin sections of the corpora.
136
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
Figure 7: Much of the loss in BLEU score when bilingually estimated features are removed from a Spanish-
English translation system (experiments 1-4) can be recovered when they are replaced with monolingual equiva-
lents estimated from monolingual Eur parl data (experiments 5-10). The labels indicate how the different types
of parameters are estimated, the first part is for phrase-table features, the second is for reordering probabilities.
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
Figure 8: Performance of monolingual features de-
rived from truly monolingual corpora. Over 82% of
the BLEU score loss can be recovered.
phrase table itself from monolingual texts. Across
all of our experiments, we use the phrase table
that the bilingual model learned from the Europarl
parallel corpus. We keep its phrase pairs, but we
drop all of its scores. Table 1 gives details of the
phrase pairs. In our experiments, we estimated
similarity and reordering scores for more than 3
million phrase pairs. For each source phrase, the
set of possible translations was constrained and
likely to contain good translations. However, the
average number of possible translations was high
(ranging from nearly 100 translations for each un-
igram to 14 for each trigram). These contain a
lot of noise and result in low end-to-end transla-
tion quality without good estimates of translation
quality, as the experiments in Section 5.1 show.
Software. Because many details of our estima-
tion procedures must be omitted for space, we dis-
tribute our full set of code along with scripts for
running our experiments and output translations.
These may be downed from http://www.cs.
jhu.edu/?anni/papers/lowresmt/
5 Experimental Results
Figures 7 and 8 give experimental results. Figure
7 shows the performance of the standard phrase-
based model when each of the bilingually esti-
mated features are removed. It shows how much
of the performance loss can be recovered using
our monolingual features when they are estimated
from the Europarl training corpus but treating
each side as an independent, monolingual cor-
pus. Figure 8 shows the recovery when using truly
monolingual corpora to estimate the parameters.
5.1 Lesion experiments
Experiments 1-4 remove bilingually estimated pa-
rameters from the standard model. For Spanish-
English, the relative contribution of the phrase-
table features (which include the phrase transla-
tion probabilities ? and the lexical weights w) is
greater than the reordering probabilities. When
the reordering probability po(orientation|f, e) is
eliminated and replaced with a simple distance-
based distortion feature that does not require a
bitext to estimate, the score dips only marginally
since word order in English and Spanish is simi-
lar. However, when both the reordering and the
phrase table features are dropped, leaving only
the LM feature and the phrase penalty, the result-
ing translation quality is abysmal, with the score
dropping a total of over 17 BLEU points.
5.2 Adding equivalent monolingual features
estimated using Europarl
Experiments 5-10 show how much our monolin-
gual equivalents could recover when the monolin-
gual corpora are drawn from the two sides of the
bitext. For instance, our algorithm for estimating
137
reordering probabilities from monolingual data (?
/M) adds 5 BLEU points, which is 73% of the po-
tential recovery going from the model (?/?) to the
model with bilingual reordering features (?/B).
Of the temporal, orthographic, and contextual
monolingual features the temporal feature per-
forms the best. Together (M/?), they recover
more than each individually. Combining mono-
lingually estimated reordering and phrase table
features (M/M) yields a total gain of 13.5 BLEU
points, or over 75% of the BLEU score loss that
occurred when we dropped all features from the
phrase table. However, these results use ?mono-
lingual? corpora which have practically identical
phrasal and temporal distributions.
5.3 Estimating features using truly
monolingual corpora
Experiments 12-18 estimate all of the features
from truly monolingual corpora. Our novel al-
gorithm for estimating reordering holds up well
and recovers 69% of the loss, only 0.4 BLEU
points less than when estimated from the Europarl
monolingual texts. The temporal similarity fea-
ture does not perform as well as when it was esti-
mated using Europarl data, but the contextual fea-
ture does. The topic similarity using Wikipedia
performs the strongest of the individual features.
Combining the monolingually estimated re-
ordering features with the monolingually esti-
mated similarity features (M/M) yields a total
gain of 14.8 BLEU points, or over 82% of the
BLEU point loss that occurred when we dropped
all features from the phrase table. This is equiv-
alent to training the standard system on a bi-
text with roughly 60,000 lines or nearly 2 million
words (learning curve omitted for space).
Finally, we supplement the standard bilingually
estimated model parameters with our monolin-
gual features (BM/B), and we see a 1.5 BLEU
point increase over the standard model. There-
fore, our monolingually estimated scores capture
some novel information not contained in the stan-
dard feature set.
6 Additional Related Work
Carbonell et al(2006) described a data-driven
MT system that used no parallel text. It produced
translation lattices using a bilingual dictionary
and scored them using an n-gram language model.
Their method has no notion of translation similar-
ity aside from a bilingual dictionary. Similarly,
Sa?nchez-Cartagena et al(2011) supplement an
SMT phrase table with translation pairs extracted
from a bilingual dictionary and give each a fre-
quency of one for computing translation scores.
Ravi and Knight (2011) treat MT without paral-
lel training data as a decipherment task and learn
a translation model from monolingual text. They
translate corpora of Spanish time expressions and
subtitles, which both have a limited vocabulary,
into English. Their method has not been applied
to broader domains of text.
Most work on learning translations from mono-
lingual texts only examine small numbers of fre-
quent words. Huang et al(2005) and Daume? and
Jagarlamudi (2011) are exceptions that improve
MT by mining translations for OOV items.
A variety of past research has focused on min-
ing parallel or comparable corpora from the web
(Munteanu and Marcu, 2006; Smith et al 2010;
Uszkoreit et al 2010). Others use an existing
SMT system to discover parallel sentences within
independent monolingual texts, and use them to
re-train and enhance the system (Schwenk, 2008;
Chen et al 2008; Schwenk and Senellart, 2009;
Rauf and Schwenk, 2009; Lambert et al 2011).
These are complementary but orthogonal to our
research goals.
7 Conclusion
This paper has demonstrated a novel set of tech-
niques for successfully estimating phrase-based
SMT parameters from monolingual corpora, po-
tentially circumventing the need for large bitexts,
which are expensive to obtain for new languages
and domains. We evaluated the performance of
our algorithms in a full end-to-end translation sys-
tem. Assuming that a bilingual-corpus-derived
phrase table is available, we were able utilize our
monolingually-estimated features to recover over
82% of BLEU loss that resulted from removing
the bilingual-corpus-derived phrase-table proba-
bilities. We also showed that our monolingual fea-
tures add 1.5 BLEU points when combined with
standard bilingually estimated features. Thus our
techniques have stand-alone efficacy when large
bilingual corpora are not available and also make
a significant contribution to combined ensemble
performance when they are.
138
References
Enrique Alfonseca, Massimiliano Ciaramita, and
Keith Hall. 2009. Gazpacho and summer rash:
lexical relationships from temporal patterns of web
search queries. In Proceedings of EMNLP.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-2011), Edinburgh, Scotland, UK.
Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual simi-
larity of labeled web images. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence.
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, Robert Mercer,
and Paul Poossin. 1988. A statistical approach to
language translation. In 12th International Confer-
ence on Computational Linguistics (CoLing-1988).
Peter Brown, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer. 1993. The mathemat-
ics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Workshop on Sta-
tistical Machine Translation.
Jaime Carbonell, Steve Klein, David Miller, Michael
Steinbaum, Tomer Grassiany, and Jochen Frey.
2006. Context-based machine translation. In Pro-
ceedings of AMTA.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for SMT self-
enhancement. In Proceedings of ACL/HLT, pages
157?160.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL.
Hal Daume? and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of ACL/HLT.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of ACL/CoLing.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Thir-
teenth Conference On Computational Natural Lan-
guage Learning (CoNLL-2009), Boulder, Colorado.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang
for the buck can we expect? In ACL 2001 Workshop
on Data-Driven Machine Translation, Toulouse,
France.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
ACL/HLT.
Fei Huang, Ying Zhang, and Stephan Vogel. 2005.
Mining key phrase translations from web corpora.
In Proceedings of EMNLP.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Pro-
ceedings of the ACL/Coling.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of ACL.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the ACL-2007 Demo
and Poster Sessions.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the Machine Translation Summit.
Shankar Kumar and William Byrne. 2004. Local
phrase reordering models for statistical machine
translation. In Proceedings of HLT/NAACL.
Patrik Lambert, Holger Schwenk, Christophe Ser-
van, and Sadaf Abdul-Rauf. 2011. Investigations
on translation model adaptation using monolingual
data. In Proceedings of the Workshop on Statistical
Machine Translation, pages 284?293, Edinburgh,
Scotland, UK.
David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of
EMNLP.
Dragos Stefan Munteanu and Daniel Marcu. 2006.
Extracting parallel sub-sentential fragments from
non-parallel corpora. In Proceedings of the
ACL/Coling.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
139
Franz Joseph Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of ACL.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of ACL.
Sadaf Abdul Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of EACL.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of ACL/HLT.
Vctor M. Sa?nchez-Cartagena, Felipe Sa?nchez-
Martnez, and Juan Antonio Pe?rez-Ortiz. 2011.
Integrating shallow-transfer rules into phrase-based
statistical machine translation. In Proceedings of
the XIII Machine Translation Summit.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of CoNLL.
Holger Schwenk and Jean Senellart. 2009. Transla-
tion model adaptation for an Arabic/French news
translation system by lightly-supervised training. In
MT Summit.
Holger Schwenk. 2008. Investigations on large-scale
lightly-supervised training for statistical machine
translation. In Proceedings of IWSLT.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Proceedings of HLT/NAACL.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT/NAACL.
Christoph Tillmann. 2003. A projection extension al-
gorithm for statistical machine translation. In Pro-
ceedings of EMNLP.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of CoLing.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL.
140
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 192?201,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
PARADIGM: Paraphrase Diagnostics through Grammar Matching
Jonathan Weese and Juri Ganitkevitch
Johns Hopkins University
Chris Callison-Burch
University of Pennsylvania
Abstract
Paraphrase evaluation is typically done ei-
ther manually or through indirect, task-
based evaluation. We introduce an in-
trinsic evaluation PARADIGM which mea-
sures the goodness of paraphrase col-
lections that are represented using syn-
chronous grammars. We formulate two
measures that evaluate these paraphrase
grammars using gold standard sentential
paraphrases drawn from a monolingual
parallel corpus. The first measure calcu-
lates how often a paraphrase grammar is
able to synchronously parse the sentence
pairs in the corpus. The second mea-
sure enumerates paraphrase rules from the
monolingual parallel corpus and calculates
the overlap between this reference para-
phrase collection and the paraphrase re-
source being evaluated. We demonstrate
the use of these evaluation metrics on para-
phrase collections derived from three dif-
ferent data types: multiple translations
of classic French novels, comparable sen-
tence pairs drawn from different newspa-
pers, and bilingual parallel corpora. We
show that PARADIGM correlates with hu-
man judgments more strongly than BLEU
on a task-based evaluation of paraphrase
quality.
1 Introduction
Paraphrases are useful in a wide range of natu-
ral language processing applications. A variety
of data-driven approaches have been proposed to
generate paraphrase resources (see Madnani and
Dorr (2010) for a survey of these methods). Few
objective metrics have been established to evalu-
ate these resources. Instead, paraphrases are typi-
cally evaluated using subjective manual evaluation
or through task-based evaluations.
Different researchers have used different crite-
ria for manual evaluations. For example, Barzilay
and McKeown (2001) evaluated their paraphrases
by asking judges whether paraphrases were ?ap-
proximately conceptually equivalent.? Ibrahim
et al. (2003) asked judges whether their para-
phrases were ?roughly interchangeable given the
genre.? Bannard and Callison-Burch (2005) re-
placed phrases with paraphrases in a number of
sentences and asked judges whether the substitu-
tions ?preserved meaning and remained grammat-
ical.? The results of these subjective evaluations
are not easily reusable.
Other researchers have evaluated their para-
phrases through task-based evaluations. Lin and
Pantel (2001) measured their potential impact on
question-answering. Cohn and Lapata (2007)
evaluate their applicability in the text-to-text gen-
eration task of sentence compression. Zhao et al.
(2009) use them to perform sentence compression
and simplification and to compute sentence simi-
larity. Several researchers have demonstrated that
paraphrases can improve machine translation eval-
uation (c.f. Kauchak and Barzilay (2006), Zhou
et al. (2006), Madnani (2010) and Snover et al.
(2010)).
We introduce an automatic evaluation met-
ric called PARADIGM, PARAphrase DIagnostics
through Grammar Matching. This metric eval-
uates paraphrase collections that are represented
using synchronous grammars. Synchronous tree-
adjoining grammars (STAGs), synchronous tree
substitution grammars (STSGs), and synchronous
context free grammars (SCFGs) are popular for-
malisms for representing paraphrase rules (Dras,
1997; Cohn and Lapata, 2007; Madnani, 2010;
Ganitkevitch et al., 2011). We present two mea-
sures that evaluate these paraphrase grammars us-
ing gold standard sentential paraphrases drawn
from a monolingual parallel corpus, which have
been previously proposed as a good resource
192
for paraphrase evaluation (Callison-Burch et al.,
2008; Cohn et al., 2008).
The first of our two proposed metrics calculates
how often a paraphrase grammar is able to syn-
chronously parse the sentence pairs in a test set.
The second measure enumerates paraphrase rules
from a monolingual parallel corpus and calculates
the overlap between this reference paraphrase col-
lection, and the paraphrase resource being evalu-
ated.
2 Related work and background
The most closely related work is ParaMetric
(Callison-Burch et al., 2008), which is a set of
objective measures for evaluating the quality of
phrase-based paraphrases. ParaMetric extracts a
set of gold-standard phrasal paraphrases from sen-
tential paraphrases that have been manually word-
aligned. The sentential paraphrases used in Para-
Metric were drawn from a data set originally cre-
ated to evaluate machine translation output using
the BLEU metric. Cohn et al. (2008) argue that
these sorts of monolingual parallel corpora are ap-
propriate for evaluating paraphrase systems, be-
cause they are naturally occurring sources of para-
phrases.
Callison-Burch et al. (2008) calculated three
types of metrics in ParaMetric. The manual word
alignments were used to calculate how well an
automatic paraphrasing technique is able to align
the paraphrases in a sentence pair. This measure
is limited to a class of paraphrasing techniques
that perform alignment (like MacCartney et al.
(2008)). Most methods produce a list of para-
phrases for a given input phrase. So Callison-
Burch et al. (2008) calculate two more gener-
ally applicable measures by comparing the para-
phrases in an automatically extracted resource to
gold standard paraphrases extracted via the align-
ments. These allow a lower-bound on precision
and relative recall to be calculated.
Liu et al. (2010) introduce the PEM metric as an
alternative to BLEU, since BLEU prefers iden-
tical paraphrases. PEM uses a second language
as a pivot to judge semantic equivalence. This re-
quires use of some bilingual data. Chen and Dolan
(2011) suggest using BLEU together with their
metric PINC, which uses n-grams to measure lex-
ical difference between paraphrases.
PARADIGM extends the ideas in ParaMetric
from lexical and phrasal paraphrasing techniques
to paraphrasing techniques that also generate syn-
tactic templates, such as Zhao et al. (2008), Cohn
and Lapata (2009), Madnani (2010) and Ganitke-
vitch et al. (2011). Instead of extracting gold stan-
dard paraphrases using techniques from phrase-
based machine translation, we use grammar ex-
traction techniques (Weese et al., 2011) to ex-
tract gold standard paraphrase grammar rules from
ParaMetric?s word-aligned sentential paraphrases.
Using these rules, we calculate the overlap be-
tween a gold standard paraphrase grammar and an
automatically generated paraphrase grammar.
Moreover, like ParaMetric, PARADIGM is able
to do further analysis on a restricted class of para-
phrasing models. In this case, PARADIGM evalu-
ates how well certain models are able to produce
synchronous parses of sentence pairs drawn from
monolingual parallel corpora. PARADIGM?s dif-
ferent metrics are explained in Section 4, but first
we give background on synchronous parsing and
synchronous grammars.
2.1 Synchronous parsing with SCFGs
Synchronous context-free grammars
An SCFG (Lewis and Stearns, 1968; Aho and
Ullman, 1972) is similar to a context-free gram-
mar, except that it generates pairs of strings
in correspondence. Each production rule in an
SCFG rewrites a non-terminal symbol as a pair of
phrases, which may have contain a mix of words
and non-terminals symbols. The grammar is syn-
chronous because both phrases in the pair must
have an identical set of non-terminals (though they
can come in different orders), and corresponding
non-terminals must be rewritten using the same
rule.
Much recent work in MT (and, by extension,
paraphrasing approaches that use MT machinery)
has been focused on choosing an appropriate set of
non-terminal symbols. The Hiero model (Chiang,
2007) used a single non-terminal symbolX . Other
approaches have read symbols from constituent
parses of the training data (Galley et al., 2004;
Galley et al., 2006; Zollmann and Venugopal,
2006). Labels based combinatory categorial gram-
mar (Steedman and Baldridge, 2011) have also
been used (Almaghout et al., 2010; Weese et al.,
2012).
Synchronous parsing
Wu (1997) introduced a parsing algorithm using
a variant of CKY. Dyer recently showed (2010)
193
an
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
.
resign
to
him
want
others
while
,
him
impeach
to
propose
people
some
D
T
NP
V
B
P
V
B
P
R
P
C
C
N
N
S
V
B
P
P
R
P
V
B
P
R
T
VP
VP
S
VP
S
NP
VP
VP
NP
S
VP
S
S
.
Figure 1: PARADIGM extracts lexical, phrasal and
syntactic paraphrases from parsed, word-aligned
sentence pairs.
that the average parse time can be significantly im-
proved by using a two-pass algorithm.
The question of whether a source-reference pair
is reachable under a model must be addressed in
end-to-end discriminative training in MT (Liang
et al., 2006a; Gimpel and Smith, 2012). Auli et
al. (2009) showed that only approximately 30% of
training pairs are reachable under a phrase-based
model. This result is confirmed by our results in
paraphrasing.
3 Paraphrase grammar extraction
Like ParaMetric, PARADIGM extracts gold stan-
dard paraphrases from word-aligned sentential
paraphrases. PARADIGM goes further by parsing
one of the two input sentences, and uses the parse
tree to extract syntactic paraphrase rules, follow-
ing recent advances in syntactic approaches to ma-
chine translation (like Galley et al. (2004), Zoll-
mann and Venugopal (2006), and others). Figure 1
shows an example of a parsed sentence pair. From
that pair it is possible to extract a wide variety
of non-identical paraphrases, which include lexi-
cal paraphrases (single word synonyms), phrasal
paraphrases, and syntactic paraphrases that in-
clude a mix of words and syntactic non-terminal
CC? and while
VBP? want propose
VBP? expect want
DT? some some people
S? him to step down him to resign
VP? step down resign
VP? to step down to resign
VP? want to impeach him propose to impeach him
VP? want VP propose VP
VP? want to impeach PRP propose to impeach PRP
VP? VBP him to step down VBP him to resign
S? PRP to step down PRP to resign
Figure 2: Four examples each of lexical, phrasal,
and syntactic paraphrases that can be extracted
from the sentence pair in Figure 1.
symbols. Figure 2 shows a set of four examples
for each type that can be extracted from Figure 1.
These rules are formulated as SCFG rules,
with a syntactic left-hand nonterminal symbol
and two English right-hand sides representing the
paraphrase. The examples above include non-
terminal symbols that represent whole syntac-
tic constituents. It is also possible to create
more complex non-terminal symbols that describe
CCG-like non-constituent phrases. For example,
we could extract a rule like
S/VP? <NNS want him to, NNS expect him to>
Using constituents only, we are able to ex-
tract 45 paraphrase rules from Figure 1. Adding
CCG-style slashed constituents yields 66 addi-
tional rules.
4 PARADIGM: Evaluating paraphrase
grammars
By considering a paraphrase model as a syn-
chronous context-free grammar, we propose to
measure the model?s goodness using the following
criteria:
1. What percentage of sentential paraphrases
are reachable under the model? That is, given
a collection of sentence pairs (a
i
, b
i
) and an
SCFG G, where each pair of a and b are sen-
tential paraphrases, how many of the pairs are
in the language of G? We evaluate this by
producing a synchronous parse for the pairs,
as shown in Figure 3.
2. Given a collection of gold-standard para-
phrase rules, how many of those paraphrases
exist as rules in G? To calculate this, we
look at the overlap of grammars (described in
194
of the 
twelve cartoons
insulting
mohammad
CD
NNS
JJ
NP
NP
VP
NP
12 the islamic prophet
CD
NNS
JJ
NP
NP
VP
NP
cartoons offensivethat were to sparked riots
violent unrest was caused by
NP
NPVBD
VBD
VP
VP
S
S
Figure 3: We measure the goodness of paraphrase
grammars by determine how often they can be
used to synchronously parse gold-standard sen-
tential paraphrases. Note we do not require the
synchronous derivation to match a gold-standard
parse tree.
Section 4.2 below), examining different cate-
gories of rules and thresholding based on how
frequently the rule was used in the gold stan-
dard data.
These criteria correspond to properties that we
think are desirable in paraphrase models. They
also have the advantage that they do not depend
on human judgments and so can be calculated au-
tomatically.
4.1 Synchronous parse coverage
Paraphrase grammars should be able to explain
sentential paraphrases. For example, Figure
3 shows a sentence pair that is synchronously
parseable by one paraphrase grammar. In general,
we say that the more such sentence pairs that a
paraphrase grammar can synchronously parse, the
better it is.
The synchronous derivation allows us to draw
inferences about parts of the sentence pair that are
in correspondence; for instance, in Figure 3, vi-
olent unrest corresponds to riots and mohammad
corresponds to the islamic prophet.
4.2 Grammar overlap defined
We measure grammar overlap by comparing the
sets of production rules for two different gram-
mars. If the grammars contain rules that are equiv-
alent, the equivalent rules are in the grammars?
overlap.
We consider two types of overlapping, which
we will call strict and non-strict overlap. For strict
overlap, we say that two rules are equivalent if
they are identical, that is, if they have the same
left-hand side non-terminal symbol, their source
sides are identical strings, and their target sides are
identical strings. (This includes identical indexing
on non-terminal symbols on the right hand sides
of the rule.)
To calculate non-strict overlap, we ignore the
identities of non-terminal symbols in the left-hand
and right-hand sides of the rules. That is, two rules
are considered equivalent if they are identical after
all the non-terminal symbols have been replaced
by one equivalent symbol.
For example, in non-strict overlap, the syntactic
rule
NP ? ?N
1
?s N
2
; the N
2
of N
1
?
would match the Hiero rule
X ? ?X
1
?s X
2
; the X
2
of X
1
?
If we are considering two Hiero grammars,
strict and non-strict intersection are the same op-
eration since they only have on non-terminal X .
4.3 Precision lower bound and relative recall
Callison-Burch et al. (2008) use the notion of over-
lap between two paraphrase sets to define two met-
rics, precision lower bound and relative recall.
These are calculated the same way as standard
precision and recall. Relative recall is qualified
as ?relative? because it is calculated on a poten-
tially incomplete set of gold standard paraphrases.
There may exist valid paraphrases that do not oc-
cur in that set. Similarly, only a lower bound on
precision can be calculated because the candidate
set may contain valid paraphrases that do not oc-
cur in the gold standard set.
5 Experiments
5.1 Data
We extracted paraphrase grammars from a vari-
ety of different data sources, including four collec-
tions of sentential paraphrases. These included:
? Multiple translation corpora that were
compiled by the Linguistics Data Consortium
(LDC) for the purposes of evaluating ma-
chine translation quality with the BLEU met-
ric. We collected eight LDC corpora that all
have multiple English translations.
1
1
LDC Catalog numbers LDC2002T01, LDC2005T05,
LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14,
LDC2010T17, and LDC2010T23.
195
sentence total
Corpus pairs words
LDC Multiple Translations 83,284 2,254,707
Classic French Literature 75,106 682,978
MSR Paraphrase Corpus 5,801 219,492
ParaMetric 970 21,944
Table 1: Amount of English?English parallel data.
LDC data has 4 parallel translations per sentence.
Literature data is from Barzilay and McKeown
(2001). MSR data is from Quirk et al. (2004)
and Dolan et al. (2004). ParaMertic data is from
Callison-Burch et al. (2008).
? Classic French Literature that were trans-
lated by different translators, and which were
compiled by Barzilay and McKeown (2001).
? The MSR Paraphrase corpus which con-
sists of sentence pairs drawn from compara-
ble news articles drawn from different web
sites in the same date rate. The sentence pairs
were aligned heuristically aligned and then
manually judged to be paraphrases.
? The ParaMetric data which consists of 900
manually word-aligned sentence pairs col-
lected by Cohn et al. (2008). 300 sentence
pairs were drawn from each of the 3 above
sources. We use this to extract the gold stan-
dard paraphrase grammar.
The size of the data from each source is summa-
rized in Table 1.
For each dataset, after tokenizing and normaliz-
ing, we parsed one sentence in each English pair
using the Berkeley constituency parser (Liang et
al., 2006b). We then obtained word-level align-
ments, either using GIZA++ (Och and Ney, 2000)
or, in the case of ParaMetric, using human annota-
tions.
We used the Thrax grammar extractor (Weese
et al., 2011) to extract Hiero-style and syntactic
SCFGs from the paraphrase data. In the syntac-
tic setting we allowed labeling of rules with ei-
ther constituent labels or CCG-style slashed cat-
egories. The size of the extracted grammars is
shown in Table 2.
We also used version 0.2 of the SCFG-based
paraphrase collection known as the ParaPhrase
DataBase or PPDB (Ganitkevitch et al., 2013).
The PPDB paraphrases were extracted using the
pivoting technique (Bannard and Callison-Burch,
Grammar Rules
LDC Hiero 52,784,462
Lit. Hiero 3,288,546
MSR Hiero 2,456,513
ParaMetric Hiero 584,944
LDC Syntax 23,978,477
Lit. Syntax 715,154
MSR Syntax 406,115
ParaMetric Syntax 317,772
PPDB-v0.2-small 1,292,224
PPDB-v0.2-large 9,456,356
PPDB-v0.2-xl 46,592,161
Table 2: Size of various paraphrase grammars.
Grammar freq. ? 1 freq. ? 2
ParaMetric Syntax 317,772 21,709
LDC Hiero 5,840 (1.8%) 416 (1.9%)
Lit. Hiero 6,152 (1.9%) 359 (1.7%)
MSR Hiero 10,012 (3.2%) 315 (1.5%)
LDC Syntax 48,833 (15.3%) 7,748 (35.6%)
Lit. Syntax 14,431 (4.5%) 1,960 (9.0%)
MSR Syntax 21,197 (6.7%) 2,053 (9.5%)
PPDB-v0.2-small 15,831 (5.0%) 5,673 (26.1%)
PPDB-v0.2-large 31,277 (9.8%) 8,245 (37.9%)
PPDB-v0.2-xl 47,720 (15.0%) 10,049 (46.2%)
Table 3: Size of strict overlap (number of rules and
% of the gold standard) of each grammar with a
syntactic grammar derived from ParaMetric. freq.
? 2 means we first removed all rules that ap-
peared only once from the ParaMetric grammar.
The number in parentheses shows the percentage
of ParaMetric rules that are present in the overlap.
2005) on bilingual parallel corpora containing
over 42 million sentence pairs.
The PPDB release includes a tool for pruning
the grammar to a smaller size by retaining only
high-precision paraphrases. We include PPDB
grammars for several different pruning settings in
our analysis.
5.2 Experimental setup
We calculated our two metrics for each of the
grammars listed in Table 2.
To perform synchronous parsing, we used the
Joshua decoder (Post et al., 2013), which includes
an implementation of Dyer?s two-pass parsing al-
gorithm (2010). After splitting the LDC data into
10 equal pieces, we trained paraphrase models on
nine-tenths of the data and parsed the other tenth.
Grammars trained from other sources (the MSR
corpus, French literature domain, and PPDB) were
also evaluated on the held-out tenth of LDC data.
196
Grammar freq. ? 1 freq. ? 2
ParaMetric Syntax 200,385 20,699
LDC Hiero 41,346 (20.6%) 5,323 (25.8%)
Lit. Hiero 36,873 (18.4%) 4,606 (22.3%)
MSR Hiero 58,970 (29.4%) 6,741 (32.6%)
LDC Syntax 37,231 (11.7%) 5,055 (24.5%)
Lit. Syntax 19,530 (9.7%) 3,121 (15.1%)
MSR Syntax 28,016 (14.0%) 3,564 (17.2%)
PPDB-v0.2-small 13,003 (6.5%) 3,661 (17.7%)
PPDB-v0.2-large 22,431 (11.2%) 4,837 (23.4%)
PPDB-v0.2-xl 31,294 (15.6%) 5,590 (27.0%)
Table 4: Size of non-strict overlap of each gram-
mar with the syntactic grammar derived from
ParaMetric. The number in parentheses shows the
percentage of ParaMetric rules that are present in
the overlap.
Grammar syntactic phrasal lexical
ParaMetric 238,646 73,320 5,806
LDC
Syn
36,375 (15%) 8,806 (12%) 3,652 (62%)
MSR
Syn
7,734 (3%) 11,254 (15%) 2,209 (38%)
PPDB-xl 40,822 (17%) 3,765 (5%) 3,142 (54%)
Table 5: Number of paraphrases of each type
in each grammar?s strict overlap with the syntac-
tic ParaMetric grammar. Numbers in parentheses
show the percentage of ParaMetric rules of each
type.
Note that the LDC data contains 4 independent
translations of each foreign sentence, giving 6 pos-
sible (unordered) paraphrase pairs. We evaluated
coverage in two ways (corresponding to the two
columns in Table 6): first, considering all possible
sentence pairs from the test data, how many were
able to be parsed?
Secondly, if we consider all the English sen-
tences that correspond to one foreign sentence,
how many foreign sentences had at least one pair
of English translations that could be parsed syn-
chronously?
For grammar overlap, we perform both strict
and non-strict calculations (see Section 4.2)
against a syntactic grammar derived from hand-
aligned ParaMetric data.
5.3 Grammar overlap results
In Table 5 we see a breakdown of the types of para-
phrases in the overlap for three of the models. Al-
though the PPDB-xl overlap is much larger than
the other two, about 80% of its rules are syntac-
tic transformations. The LDC and MSR models
have a much larger proportion of phrasal and lexi-
cal rules.
Next we will look at the grammar overlap num-
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 10 100 1k 10k 100k  0 0.04 0.08 0.12 0.16Precision Lower Bound Relative RecallNumber of rules RecallPrec.
Figure 4: Precision lower bound and relative recall
when overlapping different sizes of PPDB with the
syntactic ParaMetric grammar.
bers presented in Table 3 and Table 4.
Note the non-intuitive result that for some
grammars (notably PPDB), the non-strict overlap
is smaller than the strict overlap. This is because
rules with different non-terminals only count once
in the non-strict overlap; for example, in PPDB-
small,
NN?? answer ; reply ?
VB?? answer ; reply ?
count as separate entries when calculating strictly,
but when ignoring non-terminals, they count as
only one type of rule.
The fact that the non-strict overlaps are smaller
means that there must be many rules in PPDB that
are identical except for non-terminal labels.
5.4 Precision and recall results
Figure 4 shows relative recall and precision lower
bound calculated for various sizes of PPDB rela-
tive to the ParaMetric grammar. The x-axis rep-
resents the size of the grammar as we vary from
keeping only the most probable rules to including
less probable ones. Restricting to high probability
rules makes the grammar much smaller, resulting
in higher precision.
5.5 Synchronous parsing results
Table 6 shows the percentage of sentence pairs that
were reachable in a held-out portion of the LDC
multiple-translation data.
We find that a grammar trained on LDC data
vastly outperforms data from any other domain.
This is not surprising ? we shouldn?t expect a
model trained on French literature to be able to
197
Grammar % (all) % (any)
LDC Hiero 9.5 33.0
Lit. Hiero 1.8 9.6
MSR Hiero 1.7 9.2
LDC Syntax 9.1 30.2
Lit. Syntax 2.0 10.7
MSR Syntax 1.9 10.4
PM Syntax 1.7 9.8
PPDB-v0.2-small 1.8 3.3
PPDB-v0.2-large 2.5 4.5
PPDB-v0.2-xl 3.5 6.2
Table 6: Parse coverage on held-out LDC data.
The all column considers every possible sentential
paraphrase in the test set. The any column consid-
ers a sentence parsed if any of its paraphrases was
able to parsed.
handle some of the vocabulary found in news sto-
ries that were originally in Arabic or Chinese.
The PPDB data outperforms both French litera-
ture and MSR models if we look all possible sen-
tence pairs from test data (the column labeled ?all?
in the table). However, when we consider whether
any pair from a set of 4 translations can be trans-
lated, the PPDB models do not do as well. This
implies that PPDB tends to be able to reach many
pairs from the same set of translations, but there
are many translations that it cannot handle at all.
By contrast, the literature- and MSR-trained mod-
els can reach at least one pair from 10% of the
test examples, even though the absolute number
of pairs they can reach is lower.
5.6 Effects of grammar size and choice of
syntactic labels
Table 2 shows that the PPDB-derived grammars
are much larger than the syntactic models derived
from other domains. It may seem surprising that
they should perform worse, but adding more rules
to the grammar just by varying non-terminal labels
isn?t likely to help overall parse coverage. This
suggests a new pruning method: keep only the top
k label variations for each rule type.
If we compare the syntactic models to the Hi-
ero models trained from the same data, we see
that their overall reachability performance is not
very different. This implies that paraphrases can
be annotated with linguistic information without
necessarily hurting their ability to explain partic-
ular sentence pairs. Contrast this result, with, for
example, those of Koehn et al. (2003), showing
that restricting translation models to only syntac-
tic phrases hurts overall translation performance.
The comparable performance between Hiero and
syntactic models seems to hold regardless of do-
main.
6 Correlation with human judgments
To validate PARADIGM, we calculated its correla-
tion with human judgments of paraphrase quality
on the sentence compression text-to-text genera-
tion task, which has been used to evaluate para-
phrase grammars in previous research (Cohn and
Lapata, 2007; Zhao et al., 2009; Ganitkevitch et
al., 2011; Napoles et al., 2011). We created sen-
tence compression systems for five of the para-
phrase grammars described in Section 5.1. We fol-
lowed the methodology outlined by Ganitkevitch
et al. (2011) and did the following:
? Each paraphrase grammar was augmented
with an appropriate set of rule-level features
that capture information pertinent to the task.
In this case, the paraphrase rules were given
two additional features that shows how the
number of words and characters changed af-
ter applying the rule.
? Similarly to how the weights of the mod-
els are set using minimum error rate training
in statistical machine translation, the weights
for each of the paraphrase grammars using
the PRO tuning method (Hopkins and May,
2011).
? Instead of optimizing to the BLEU metric, as
is done in machine translation, we optimized
to PR
?
ECIS, a metric developed for sentence
compression that adapts BLEU so that it in-
cludes a ?verbosity penalty? (Ganitkevitch et
al., 2011) to encourage the compression sys-
tems to produce shorter output.
? We created a development set with sentence
compressions by selecting 1000 pairs of sen-
tences from the multiple translation corpus
where two English translations of the same
foreign sentences differed in each other by a
length ratio of 0.67?0.75.
? We decoded a test set of 1000 sentences us-
ing each of the grammars and its optimized
198
weights with the Joshua decoder (Ganitke-
vitch et al., 2012). The selected in the same
fashion as the dev sentences, so each one had
a human-created reference compression.
We conducted a human evaluation to judge the
meaning and grammaticality of the sentence com-
pressions derived from each paraphrase grammar.
We presented workers on Mechanical Turk with
the input sentence to the compression sentence
(the long sentence), along with 5 shortened out-
puts from our compression systems. To ensure
that workers were producing reliable judgments
we also presented them with a positive control (a
reference compression written by a person) and a
negative controls (a compressed output that was
generated by randomly deleted words). We ex-
cluded judgments from workers who did not per-
form well on the positive and negative controls.
Meaning and grammaticality were scored on
5-point scales where 5 is best. These human
scores were averaged over 2000 judgments (1000
sentences x 2 annotators) for each system. The
systems? outputs were then scored with BLEU,
PR
?
ECIS, and their paraphrase grammars were
scored PARADIGM?s relative recall and precision
lower-bound estimates. For each grammar, we
also calculated the average length of parseable
sentences.
We calculated the correlation between the hu-
man judgements and the automatic scores, using
Spearman?s rank correlation coefficient ?. This
is methodology is the same that is used to quan-
tify the goodness of automatic evaluation metrics
in the machine translation literature (Przybocki et
al., 2008; Callison-Burch et al., 2010). The pos-
sible values of ? range between 1 (where all sys-
tems are ranked in the same order) and ?1 (where
the systems are ranked in the reverse order). Thus
an automatic evaluation metric with a higher abso-
lute value for ? is making predictions that are more
similar to the human judgments than an automatic
evaluation metric with a lower absolute ?.
Table 7 shows that our PARADIGM scores cor-
relate more highly with human judgments than ei-
ther BLEU or PR
?
ECIS for the 5 systems in our eval-
uation. This suggests that it may be a better predic-
tor of the goodness of paraphrase grammars than
MT metrics, when the paraphrase grammars are
used for text-to-text generation tasks.
MEANING GRAMMAR
BLEU -0.7 -0.1
PR
?
ECIS -0.6 +0.2
PINC +0.1 +0.4
PARADIGM
precision
+0.6 +0.1
PARADIGM
recall
+0.1 +0.4
PARADIGM
avg?len
-0.3 +0.4
Table 7: The correlation (Spearman?s ?) of dif-
ferent automatic evaluation metrics with human
judgments of paraphrase quality for the text-to-
text generation task of sentence compression.
7 Summary
We have introduced two new metrics for evaluat-
ing paraphrase grammars, and looked at several
models from a variety of domains. Using these
metrics we can perform a variety of analyses about
SCFG-based paraphrase models:
? Automatically-extracted grammars can parse
a small fraction of held-out data (?30%).
This is comparable to results in MT (Auli et
al., 2009).
? In-domain training data is necessary in or-
der to parse held-out data. A model trained
on newswire data parsed 30% of held-out
newswire sentence pairs, versus to <10% for
literature or parliamentary data.
? SCFGs with syntactic labels perform just as
well as simpler models with a single non-
terminal label.
? Automatically-extracted syntactic grammars
tend to have a reasonable overlap with gram-
mars derived from human-aligned data, in-
cluding more 45% of the gold-standard gram-
mar?s paraphrase rules that occurred at least
twice.
? We showed that PARADIGM more strongly
correlates with human judgments of the
meaning and grammaticality of paraphrases
produced by sentence compression systems
than standard automatic evaluation measures
like BLEU.
PARADIGM will help researchers developing
paraphrase resources to perform similar diagnos-
tics on their models, and quickly evaluate their
systems.
199
Acknowledgements
This material is based on research sponsored by
the NSF under grant IIS-1249516 and DARPA
under agreement number FA8750-13-2-0017 (the
DEFT program). The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes. The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as representing
official policies or endorsements of DARPA or the
U.S. Government.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling. Pren-
tice Hall.
Hala Almaghout, Jie Jiang, and Andy Way. 2010.
CCG augmented hierarchical phrase-based machine
translation. In Proc. of IWSLT.
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proc. WMT.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of ACL.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proc. of ACL.
Chris Callison-Burch, Trevor Cohn, and Mirella Lap-
ata. 2008. ParaMetric: An automatic evaluation
metric for paraphrasing. In Proc. of COLING.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Fourth Workshop
on Statistical Machine Translation (WMT10).
David L. Chen and William Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proc. of ACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of EMNLP-
CoLing.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research (JAIR), 34:637?674.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
putational Linguistics, 34(4).
William Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrases cor-
pora: Exploiting massively parallel news sources. In
Proc. of COLING.
Mark Dras. 1997. Representing paraphrases using
synchronous tree adjoining grammars. In Proceed-
ings of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 516?518,
Madrid, Spain, July. Association for Computational
Linguistics.
Chris Dyer. 2010. Two monolingual parses are bet-
ter than one (synchronous parse). In Proceedings of
HLT/NAACL, pages 263?266. Association for Com-
putational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL 2004: Main Proceedings, pages
273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve Deneefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL, pages 961?968.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In Proceedings
of EMNLP.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, pro, and paraphrases. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 283?291, Montr?eal, Canada, June. As-
sociation for Computational Linguistics.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proc. NAACL.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proc. of NAACL.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proc. of the Second International
Workshop on Paraphrasing.
200
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philip M. Lewis and Richard E. Stearns. 1968.
Syntax-directed transduction. Journal of the ACM,
15(3):465?488.
Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proc. of
ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006b.
Alignment by agreement. In Proceedings of the
Human Language Technology Conference of the
NAACL, Main Conference, pages 104?111, New
York City, USA, June. Association for Computa-
tional Linguistics.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules from text. Natural Language Engi-
neering, 7(3):343?360.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. PEM: a paraphrase evaluation metric exploit-
ing parallel texts. In Proc. of EMNLP.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 802?811, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, Department of Computer Science, University of
Maryland College Park.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 91?97, Portland, Oregon,
June. Association for Computational Linguistics.
Franz Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440?447, Hong Kong,
China, October.
Matt Post, Juri Ganitkevitch, Luke Orland, Jonathan
Weese, Yuan Cao, and Chris Callison-Burch. 2013.
Joshua 5.0: Sparser, better, faster, server. In Proc. of
WMT.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 ?Met-
rics for MAchine TRanslation? challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monlingual machine translation for paraphrase gen-
eration. In Proc. of EMNLP.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Mark Steedman and Jason Baldridge. 2011. Combi-
natory categorial grammar. In Robert Borsley and
Kersti B?orjars, editors, Non-Transformational Syn-
tax. Wiley-Blackwell.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proc. of ACL.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the thrax
grammar extractor. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
478?484, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Jonathan Weese, Chris Callison-Burch, and Adam
Lopez. 2012. Using categorial grammar to label
translation rules. In Proc. of WMT.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In Proceedings of
ACL/HLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of ACL.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using para-
phrases to evaluate summaries automatically. In
Proceedings of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York City,
June. Association for Computational Linguistics.
201
Use of Modality and Negation in
Semantically-Informed Syntactic MT
Kathryn Baker?
U.S. Department of Defense
Michael Bloodgood??
University of Maryland
Bonnie J. Dorr?
University of Maryland
Chris Callison-Burch?
Johns Hopkins University
Nathaniel W. Filardo?
Johns Hopkins University
Christine Piatko?
Johns Hopkins University
Lori Levin||
Carnegie Mellon University
Scott Miller#
BBN Technologies
? U.S. Department of Defense, 9800 Savage Rd., Suite 6811, Fort Meade, MD 20755.
E-mail: kathrynlb@gmail.com.
?? Center for Advanced Study of Language, University of Maryland, 7005 52nd Avenue, College Park, MD
20742. E-mail: meb@umd.edu.
? Department of Computer Science and UMIACS, University of Maryland, AV Williams Building 3153,
College Park, MD 20742. E-mail: bonnie@umiacs.umd.edu.
? Center for Language and Speech Processing, Johns Hopkins University, 3400 N. Charles Street,
Hackerman Hall 320, Baltimore MD 21218. E-mail: {ccb,nwf}@cs.jhu.edu.
? Applied Physics Laboratory, Johns Hopkins University, 11000 Johns Hopkins Rd., Laurel, MD 20723.
E-mail: christine.piatko@jhuapl.edu.
|| Carnegie Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: lsl@cs.cmu.edu.
# BNN Technologies, 10 Moulton Street, Cambridge, MA 02138. E-mail: smiller@bbn.com.
Submission received: 27 March 2011; revised submission received: 28 September 2011; accepted for
publication: 30 November 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
This article describes the resource- and system-building efforts of an 8-week Johns Hopkins
University Human Language Technology Center of Excellence Summer Camp for Applied Lan-
guage Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT). We
describe a new modality/negation (MN) annotation scheme, the creation of a (publicly available)
MN lexicon, and two automated MN taggers that we built using the annotation scheme and
lexicon. Our annotation scheme isolates three components of modality and negation: a trigger
(a word that conveys modality or negation), a target (an action associated with modality or
negation), and a holder (an experiencer of modality). We describe how our MN lexicon was
semi-automatically produced and we demonstrate that a structure-based MN tagger results in
precision around 86% (depending on genre) for tagging of a standard LDC data set.
We apply our MN annotation scheme to statistical machine translation using a syntactic
framework that supports the inclusion of semantic annotations. Syntactic tags enriched with
semantic annotations are assigned to parse trees in the target-language training texts through
a process of tree grafting. Although the focus of our work is modality and negation, the tree
grafting procedure is general and supports other types of semantic information. We exploit this
capability by including named entities, produced by a pre-existing tagger, in addition to the MN
elements produced by the taggers described here. The resulting system significantly outperformed
a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the
NIST 2009 Urdu?English test set. This finding supports the hypothesis that both syntactic and
semantic information can improve translation quality.
1. Introduction
This article describes the resource- and system-building efforts of an 8-week Johns
Hopkins Human Language Technology Center of Excellence Summer Camp for Ap-
plied Language Exploration (SCALE-2009) on Semantically InformedMachine Translation
(SIMT) (Baker et al 2010a, 2010b, 2010c, 2010d). Specifically, we describe our modal-
ity/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two
automated MN taggers that were built using the lexicon and annotation scheme.
Our annotation scheme isolates three components of modality and negation: a
trigger (a word that conveys modality or negation), a target (an action associated with
modality or negation), and a holder (an experiencer of modality). Two examples of MN
tagging are shown in Figure 1.
Note that modality and negation are unified into single MN tags (e.g., the ?Able?
modality tag is combined with ?NOT? to form the ?NOTAble? tag) and also that
Figure 1
Modality/negation tagging examples.
412
Baker et al Modality and Negation in SIMT
MN tags occur in pairs of triggers (e.g., TrigAble and TrigNegation) and targets (e.g.,
TargNOTAble).
We apply our modality and negation mechanism to the problem of Urdu?English
machine translation using a technique that we call tree grafting. This technique incorpo-
rates syntactic labels and semantic annotations in a unified and coherent framework for
implementing semantically informed machine translation. Our framework is not lim-
ited to the semantic annotations produced by the MN taggers that are the subject of this
article and we exploit this capability to additionally include named-entity annotations
produced by a pre-existing tagger. By augmenting hierarchical phrase-based translation
rules with syntactic labels that were extracted from a parsed parallel corpus, and further
augmenting the parse trees with markers for modality, negation, and entities (through
the tree grafting process), we produced a better model for translating Urdu and English.
The resulting system significantly outperformed the linguistically naive baseline Hiero
model, and reached the highest scores yet reported on the NIST 2009 Urdu?English
translation task.
We note that although our largest gains were from syntactic enrichments to the
model, smaller (but significant) gains were achieved by injecting semantic knowledge
into the syntactic paradigm. Verbal semantics (modality and negation) contributed
slightly more gains than nominal semantics (named entities) and their combined gains
were the sum of their individual contributions.
Of course, the limited semantic types we explored (modality, negation, and en-
tities) are only a small piece of the much larger semantic space, but demonstrating
success on these semantic aspects of language, the combination of which has been
unexplored by the statistical machine translation community, bodes well for (larger)
improvements based on the incorporation of other semantic aspects (e.g., relations and
temporal knowledge). Moreover, we believe this syntactic framework to be well suited
for further exploration of the impact of many different types of semantics on the quality
of machine-translation (MT) output. Indeed, it would not have been possible to initiate
the current study without the foundational work that gave rise to a syntactic paradigm
that could support these semantic enrichments.
In the SIMT paradigm, semantic elements (e.g., modality/negation) are identified
in the English portion of a parallel training corpus and projected to the source language
(in our case, Urdu) during a process of syntactic alignment. These semantic elements are
subsequently used in the translation rules that are extracted from the parallel corpus.
The goal of adding them to the translation rules is to constrain the space of possible
translations to more grammatical and more semantically coherent output. We explored
whether including such semantic elements could improve translation output in the face
of sparse training data and few source language annotations. Results were encouraging.
Translation quality, as measured by the Bleu metric (Papineni et al 2002), improved
when the training process for the Joshua machine translation system (Li et al 2009)
used in the SCALE workshop included MN annotation.
We were particularly interested in identifying modalities and negation because
they can be used to characterize events in a variety of automated analytic processes.
Modalities and negation can distinguish realized events from unrealized events, beliefs
from certainties, and can distinguish positive and negative instances of entities and
events. For example, the correct identification and retention of negation in a particular
language?such as a single instance of the word ?not??is very important for a correct
representation of events and likewise for translation.
The next two sections examine related work and the motivation behind the SIMT
approach. Section 4 defines the theoretical framework for ourMN lexicon and automatic
413
Computational Linguistics Volume 38, Number 2
MN taggers. Section 5 presents the MN annotation scheme used by our human annota-
tors and describes the creation of a MN lexicon based on this scheme. Section 6 presents
two types of MN taggers?one that is string-based and one that is structure-based?
and evaluates the effectiveness of the structure-based tagger. Section 7 then presents
implementation details of the semantically informed syntactic system and describes
the results of its application. Finally, Section 8 presents conclusions and future work.
2. Related Work
The development of annotation schemes has become an area of computational lin-
guistics development in its own right, often separate from machine learning applica-
tions. Many projects began as strictly linguistic projects that were later adapted for
computational linguistics. When an annotation scheme is consistent and well devel-
oped, its subsequent application to NLP systems is most effective. For example, the
syntactic annotation of parse trees in the Penn Treebank (Marcus, Marcinkiewicz, and
Santorini 1993) had a tremendous effect on parsing and onNatural Language Processing
in general.
In the case of semantic annotations, each tends to have its unique area of focus.
Although the labeling conventions may differ, a layer of modality annotation over
verb role annotation, for example, can have a complementary effect of providing more
information, rather than being viewed as a competing scheme. We review some of the
major semantic annotation efforts here.
Propbank (Palmer, Gildea, and Kingsbury 2005) is a set of annotations of predicate?
argument structure over parse trees. First annotated as an overlay to the Penn
Treebank, Propbank annotation now exists for other corpora. Propbank annotation aims
to answer the question Who did what to whom? for individual predicates. It is tightly
coupled with the behavior of individual verbs. FrameNet (Baker, Fillmore, and Lowe
1998), a frame-based lexical database that associates each word in the database with
a semantic frame and semantic roles, is also associated with annotations at the lexical
level.WordNet (Fellbaum 1998) is a verywidely used online lexical taxonomywhich has
been developed in numerous languages.WordNet nouns, verbs, adjectives, and adverbs
are organized into synonym sets. PropBank, FrameNet, and WordNet cover the word
senses and argument-taking properties of many modal predicates.
The Prague Dependency Treebank (Hajic? et al 2001; Bo?hmova?, Cinkova?, and
Hajic?ova? 2005) (PDT) is a multi-level system of annotation for texts in Czech and other
languages, with its roots in the Prague school of linguistics. Besides a morphological
layer and an analytical layer, there is a Tectogrammatical layer. The Tectogrammatical
layer includes functional relationships, dependency relations, and co-reference. The
PDT also integrates propositional and extra-propositional meanings in a single anno-
tation framework.
The Penn Discourse Treebank (PDTB) (Webber et al 2003; Prasad et al 2008)
annotates discourse connectives and their arguments over a portion of the Penn
Treebank. Within this framework, senses are annotated for the discourse connectives
in a hierarchical scheme. Relevant to the current work, one type of tag in the scheme is
the Conditional tag, which includes hypothetical, general, unreal present, unreal past,
factual present, and factual past arguments.
The PDTB work is related to that of Wiebe, Wilson, and Cardie (2005) for estab-
lishing the importance of attributing a belief or assertion expressed in text to its agent
(equivalent to the notion of holder in our scheme). The annotation scheme is designed to
capture the expression of opinions and emotions. In the PDTB, each discourse relation
414
Baker et al Modality and Negation in SIMT
and its two arguments are annotated for attribution. The attribute features are the
Source or agent, the Type (assertion propositions, belief propositions, facts, and eventu-
alities), scopal polarity, and determinacy. Scopal polarity is annotated on relations and
their arguments to identify cases when verbs of attribution are negated on the surface
but the negation takes scope over the embedded clause. An example is the sentence
?Having the dividend increases is a supportive element in the market outlook but I don?t
think it?s a main consideration.? Here, the second argument (the clause following but) is
annotated with a ?Neg? marker, meaning ?I think it?s not a main consideration.?
Wilson, Wiebe, and Hoffman (2009) describe the importance of correctly inter-
preting polarity in the context of sentiment analysis, which is the task of identifying
positive and negative opinions, emotions, and evaluations. The authors have estab-
lished a set of features to distinguish between positive and negative polarity and discuss
the importance of correctly analyzing the scope of the negation and the modality (e.g.,
whether the proposition is asserted to be real or not real).
A major annotation effort for temporal and event expressions is the TimeML spec-
ification language, which has been developed in the context of reasoning for question
answering (Saur??, Verhagen, and Pustejovsky 2006). TimeML, which includes modality
annotation on events, is the basis for creating the TimeBank and FactBank corpora
(Pustejovsky et al 2006; Saur?? and Pustejovsky 2009). In FactBank, event mentions are
marked with their degree of factuality.
Recent work incorporating modality annotation includes work on detecting cer-
tainty and uncertainty. Rubin (2007) describes a scheme for five levels of certainty,
referred to as Epistemic modality, in news texts. Annotators identify explicit certainty
markers and also take into account Perspective, Focus, and Time. Focus separates
certainty into facts and opinions, to include attitudes. In our scheme, Focus would be
covered by want and belief modality. Also, separating focus and uncertainty can allow
the annotation of both on one trigger word. Prabhakaran, Rambow, and Diab (2010)
describe a scheme for automatic committed belief tagging. Committed belief indicates
the writer believes the proposition. The authors use a previously annotated corpus of
committed belief, non-committed belief, and not applicable (Diab et al 2009), and derive
features for machine learning from parse trees. The authors desire to combine their
work with FactBank annotation.
The CoNLL-2010 shared task (Farkas et al 2010) was about the detection of cues
for uncertainty and their scope. The task was described as ?hedge detection,? that is,
finding statements which do not or cannot be backed up with facts. Auxiliary verbs
such as may, might, can, and so forth, are one type of hedge cue. The training data for
the shared task included the BioScope corpus (Szarvas et al 2008), which is manually
annotated with negation and speculation cues and their scope, and paragraphs from
Wikipedia possibly containing hedge information. Our scheme also identifies cues in
the form of triggers, but our desired outcome is to cover the full range of modalities
and not just certainty and uncertainty. To identify scope, we use syntactic parse trees,
as was allowed in the CoNLL task.
The textual entailment literature includes modality annotation schemes. Identifying
modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al
(2007) include polarity based rules and negation and modality annotation rules. The
polarity rules are based on an independent polarity lexicon (Nairn, Condorovdi, and
Karttunen 2006). The annotation rules for negation andmodality of predicates are based
on identifying modal verbs, as well as conditional sentences and modal adverbials.
The authors read the modality off parse trees directly using simple structural rules for
modifiers.
415
Computational Linguistics Volume 38, Number 2
Earlier work describing the difficulty of correctly translating modality using ma-
chine translation includes Sigurd and Gawro?nska (1994) andMurata et al (2005). Sigurd
and Gawro?nska (1994) write about rule based frameworks and how using alternate
grammatical constructions such as the passive can improve the rendering of the modal
in the target language. Murata et al (2005) analyze the translation of Japanese into
English by several systems, showing they often render the present incorrectly as the
progressive. The authors trained a support vector machine to specifically handle modal
constructions, whereas our modal annotation approach is a part of a full translation
system.
We now consider other literature, relating to tree-grafting and machine translation.
Our tree-grafting approach builds on a technique used for tree augmentation in Miller
et al (2000), where parse-tree nodes are augmented with semantic categories. In that
earlier work, tree nodes were augmented with relations, whereas we augmented tree
nodes with modality and negation. The parser is subsequently retrained for both
semantic and syntactic processing. The semantic annotations were done manually by
students who were provided a set of guidelines and then merged with the syntactic
trees automatically. In our work we tagged our corpus with entities, modality, and
negation automatically and then grafted them onto the syntactic trees automatically,
for the purpose of training a statistical machine translation system. An added benefit of
the extracted translation rules is that they are capable of producing semantically tagged
Urdu parses, despite the fact that the training data were processed by only an English
parser and tagger.
Related work in syntax-based MT includes that of Huang and Knight (2006), where
a series of syntax rules are applied to a source language string to produce a target
language phrase structure tree. The Penn English Treebank (Marcus, Marcinkiewicz,
and Santorini 1993) is used as the source for the syntactic labels and syntax trees are
relabeled to improve translation quality. In this work, node-internal and node-external
information is used to relabel nodes, similar to earlier work where structural context
was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein
and Manning?s methods include lexicalizing determiners and percent markers, making
more fine-grained verb phrase (VP) categories, and marking the properties of sister
nodes on nodes. All of these labels are derivable from the trees themselves and not
from an auxiliary source. Wang et al (2010) use this type of node splitting in machine
translation and report a small increase in BLEU score.
We use the methods described in Zollmann and Venugopal (2006) and Venugopal,
Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which
requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel
(2007) use generic non-terminal category symbols, as in Chiang (2005), as well as gram-
matical categories from the Stanford parser (Klein and Manning 2003). Their method
for rule induction generalizes to any set of non-terminals. We further refine this process
by adding semantic notations onto the syntactic non-terminals produced by a Penn
Treebank trained parser, thus making the categories more informative.
In the parsing domain, the work of Petrov and Klein (2007) is related to the current
work. In their work, rule splitting and rule merging are applied to refine parse trees
during machine learning. Hierarchical splitting leads to the creation of learned cate-
gories that have linguistic relevance, such as a breakdown of a determiner category into
two subcategories of determiners by number, that is, this and that group together as do
some and these. We augment parse trees by category insertion in cases where a semantic
category is inserted as a node in a parse tree, after the English side of the corpus has
been parsed by a statistical parser.
416
Baker et al Modality and Negation in SIMT
3. SIMTMotivation
As in many of the frameworks described herein, the aim of the SIMT effort was to
provide a generalized framework for representing structured semantic information,
such as modality and negation. Unlike many of the previous semantic annotation efforts
(where the emphasis tends to be on English), however, our approach is designed to
be directly integrated into a translation engine, with the goal of translating highly
divergent language pairs, such as Urdu and English. As such, our choice of annotation
scheme?illustrated in the trigger-target example shown in Figure 1?was based on a
simplified structural representation that is general enough to accommodate divergent
modality/negation phenomena, easy for language experts to follow, and straightfor-
ward to integrate into a tree-grafting mechanism for MT. Our objective is to investigate
whether incorporating this sort of information into machine translation systems could
produce better translations, particularly in settings where only small parallel corpora
are available.
It is informative to look at an example translation to understand the challenges of
translating important semantic elements when working with a low-resource language
pair. Figure 2 shows an example taken from the 2008 NIST Urdu?English translation
task, and illustrates the translation quality of a state-of-the-art Urdu?English system
(prior to the SIMT effort). The small amount of training data for this language pair (see
Figure 2
An example of Urdu?English translation. Shown are an Urdu source document, a reference
translation produced by a professional human translator, and MT output from a phrase-based
model (Moses) without linguistic information, which is representative of state-of-the-art MT
quality before the SIMT effort.
417
Computational Linguistics Volume 38, Number 2
Table 1
The size of the various data sets used for the experiments in this article including the training,
development (dev), incremental test set (devtest), and blind test set (test). The dev/devtest was a
split of the NIST08 Urdu?English test set, and the blind test set was NIST09.
Urdu English
set lines tokens types tokens types
training 202k 1.7M 56k 1.7M 51k
dev 981 21k 4k 19k 4k
devtest 883 22k 4k 19?20k 4k
test 1,792 42k 6k 38?41k 5k
Table 1) results in significantly degraded translation quality compared, for example, to
an Arabic?English system that has more than 100 times the amount of training data.
The output in Figure 2 was produced using Moses (Koehn et al 2007), a state-of-
the-art phrase-based MT system that by default does not incorporate any linguistic
information (e.g., syntax or morphology or transliteration knowledge). As a result,
words that were not directly observed in the bilingual training data were untranslatable.
Names, in particular, are problematic. For example, the lack of translation for Nagaland
and Nagas induces multiple omissions throughout the translated text, thus producing
several instances where the holder of a claim (or belief ) is missing. This is because out-of-
vocabulary words are deleted from the Moses output.
We use syntactic and semantic tags as higher-order symbols inside the translation
rules used by the translation models. Generic symbols in translation rules (i.e., the
non-terminal symbol ?X?) were replaced with structured information at multiple levels
of abstraction, using a tree-grafting approach that we describe subsequently. Figure 3
Figure 3
The evolution of a semantically informed approach to our synchronous context-free grammars.
At the start of the 8 weeks the decoder used translation rules with a single generic non-terminal
symbol. Later syntactic categories were used, and by the end of the workshop the translation
rules included semantic elements such as modalities and negation, as well as named entities.
418
Baker et al Modality and Negation in SIMT
illustrates the evolution of the translation rules that we used, first replacing ?X? with
grammatical categories and then with categories corresponding to semantic units.
The semantic units that we examined in this effort weremodalities and negation (in-
dications that a statement represents something that has/hasn?t taken place or is/isn?t
a belief or an intention) and named entities (such as people or organizations). Other
semantic units, such as relations between entities and events, were not part of this effort
but we believe they could be similarly incorporated into the framework. We chose to
examine semantic units that canonically exhibit two different syntactic types: verbal, in
the case of modality and negation, and nominal, in the case of named entities.
Although used in this effort, named entities were not the focus of our research
efforts in SIMT. Rather, we focused on the development of an annotation scheme
for modality and negation and its use in MT, while relying on a pre-existing hidden
Markov model (HMM)-based tagger derived from Identifinder (Bikel, Schwartz, and
Weischedel 1999) to produce entity tags. Thus, the remainder of this article will focus
on our MN annotation scheme, two MN taggers produced by the effort, and on the
integration of semantics in the SIMT paradigm.
4. Modality and Negation
Modality is an extra-propositional component of meaning. In John may go to NY, the
basic proposition is John go to NY and the word may indicates modality and is called the
trigger in our work. van der Auwera and Amman (2005) define core cases of modality:
John must go to NY (epistemic necessity), John might go to NY (epistemic possibility),
John has to leave NY now (deontic necessity), and John may leave NY now (deontic pos-
sibility). Larreya (2009) defines the core cases slightly differently as root and epistemic.
Root modality in Larreya?s taxonomy includes physical modality (He had to stop. The
road was blocked) and deontic modality (You have to stop). Epistemic modality includes
problematic modality (You must be tired) and implicative modality (You have to be mad to
do that). Many semanticists (Kratzer 1991, von Fintel and Iatridou 2006) define modality
as quantification over possible worlds. John might leave NY means that there exist some
possible worlds in which John leaves NY. Another view of modality relates more to a
speaker?s attitude toward a proposition (McShane, Nirenburg, and Zacharski).
We incorporate negation as an inextricably intertwined component of modality,
using the term ?modality/negation (MN)? to refer to our resources (lexicons) and
processes (taggers). We adopt the view that modality includes several types of attitudes
that a speaker might have (or not have) toward an event or state. From the point of
view of the reader or listener, modality might indicate factivity, evidentiality, or senti-
ment. Factivity is related to whether an event, state, or proposition happened or didn?t
happen. It distinguishes things that happened from things that are desired, planned,
or probable. Evidentiality deals with the source of information and may provide clues
to the reliability of the information. Did the speaker have first-hand knowledge of
what he or she is reporting, or was it hearsay or inferred from indirect evidence?
Sentiment deals with a speaker?s positive or negative feelings toward an event, state,
or proposition.
Our project was limited to modal words and phrases?and their negations?that
are related to factivity. Beyond the core cases of modality, however, we include some
aspects of speaker attitude such as intent and desire. We included these because they
are often not separable from the core cases of modality. For example, He had to go may
include the ideas that someone wanted him to go, that he might not have wanted to go,
419
Computational Linguistics Volume 38, Number 2
that at some point after coercion he intended to go, and that at some point he was able
to go (Larreya 2009).
Our focus was on the eight modalities in Figure 4, where P is a proposition (the
target of the triggering modality) and H is the holder (experiencer or cognizer of the
modality). Some of the eight factivity-related modalities may overlap with sentiment
or evidentiality. For example, want indicates that the proposition it scopes over may
not be a fact (it may just be desired), but it also expresses positive sentiment toward
the proposition it scopes over. We assume that sentiment and evidentiality are covered
under separate coding schemes, and that words like want would have two tags, one for
sentiment and one for factivity.
5. The Modality/Negation Annotation Scheme
The challenge of creating an MN annotation scheme was to deal with the complex
scoping ofmodalities with each other andwith negation, while at the same time creating
a simplified operational procedure that could be followed by language experts without
special training. Here we describe our MN annotation framework, including a set
of linguistic simplifications, and then we present our methodology for creation of a
publicly available MN lexicon. The modality annotation scheme is fully documented in
a set of guidelines that were written with English example sentences (Baker et al 2010c).
The guidelines can be used to derive hand-tagged evaluation data for English and they
also include a section that contains a set of Urdu trigger-word examples.
During the SCALE workshop, some Urdu speakers used the guidelines to annotate
a small corpus of Urdu by hand, which we reserved for future work. The Urdu corpus
could be useful as an evaluation corpus for automatically tagged Urdu, such as one
derived from rule projection in the Urdu?English MT system, a method we describe
further in Section 7. Also, although we did not annotate a very large Urdu corpus, more
data could be manually annotated to train an automatic Urdu tagger in the future.
5.1 Anatomy of Modality/Negation in Sentences
In sentences that express modality, we identify three components: a trigger, a target, and
a holder. The trigger is the word or string of words that expresses modality or negation.
The target is the event, state, or relation over which the modality scopes. The holder is
Figure 4
Eight modalities used for tagging. H = the holder of the modality; P = the proposition over
which the modality has scope.
420
Baker et al Modality and Negation in SIMT
the experiencer or cognizer of themodality. The trigger can be a word such as should, try,
able, likely, or want. It can also be a negative element such as not or n?t. Often, modality
or negation is expressed without a lexical trigger. For a typical declarative sentence
(e.g., John went to NY), the default modality is strong belief when no lexical trigger is
present. Modality can also be expressed constructionally. For example, Requirement can
be expressed in Urdu with a dative subject and infinitive verb followed by a verb that
means to happen or befall.
5.2 Linguistic Simplifications for Efficient Operationalization
Six linguistic simplifications were made for the sake of efficient operationalization of
the annotation task. The first linguistic simplification deals with the scope of modality
and negation. The first given sentence indicates scope of modality over negation. The
second sentence indicates scope of negation over modality:
 He tried not to criticize the president.
 He didn?t try to criticize the president.
The interaction of modality with negation is complex, but was operationalized eas-
ily in the menu of 13 choices shown in Figure 5. First consider the case where negation
scopes over modality. Four of the 13 choices are composites of negation scoping over
modality. For example, the annotators can choose try or not try as two separate modali-
ties. Five modalities (Require, Permit, Want, Firmly Believe, and Believe) do not have a
negated form. For three of these modalities (Want, Firmly Believe, and Believe), this is
because they are often transparent to negation. For example, I do not believe that he left NY
sometimes means the same as I believe he didn?t leave NY. Merging the two is obviously
a simplification, but it saves the annotators from having to make a difficult decision.
Figure 5
Thirteen menu choices for Modality/Negation annotation. H = the holder of the modality;
P = the proposition over which the modality has scope.
421
Computational Linguistics Volume 38, Number 2
The second linguistic simplification is related to a duality in meaning between
require and permit. Not requiring P to be true is similar in meaning to permitting P to
be false. Thus, annotators were instructed to label not require P to be true as Permit P to be
false. Conversely, not Permit P to be truewas labeled as Require P to be false.
After the annotator chooses the modality, the scoping of modality over negation
takes place as a second decision. For example, for the sentence John tried not to go to NY,
the annotator first identifies go as the target of a modality and then chooses try as the
modality. Finally, the annotator chooses false as the polarity of the target.
The third simplification relates to entailments between modalities. Many words
have complex meanings that include components of more than one modality. For ex-
ample, if one managed to do something, one tried to do it and one probably wanted to
do it. Thus, annotators were provided a specificity-ordered modality list as in Figure 5,
andwere asked to choose the first applicable modality. We note that this list corresponds
to two independent ?entailment groupings,? ordered by specificity:
 {requires ? permits}
 {succeeds ? tries ? intends ? is able ? wants}
Inside the entailment groupings, the ordering corresponds to an entailment relation:
For example, succeeds can only occur if tries has occurred. Also, the {requires ? . . . }
entailment grouping is taken to be more specific than (ordered before) the {succeeds ?
. . . } entailment grouping. Moreover, both entailment groupings are taken to be more
specific than believes, which is not in an entailment relation with any of the other
modalities.
The fourth simplification, already mentioned, is that sentences without an overt
trigger word are tagged as firmly believes. This heuristic works reasonably well for the
types of documents we were working with, although one could imagine genres such
as fiction in which many sentences take place in an alternate possible world (imagined,
conditional, or counterfactual) without explicit marking.
The fifth linguistic simplification is that we did not require annotators to mark
nested modalities. For a sentence like He might be able to go to NY the target word go
is marked as ability, but might is not annotated for Belief modality. This decision was
based on time limits on the annotation task; there was not enough time for annotators
to deal with syntactic scoping of modalities over other modalities.
Finally, we did not mark the holder H because of the short time frame for workshop
preparation. We felt that identifying the triggers and targets would be most beneficial
in the context of machine translation.
5.3 The English Modality/Negation Lexicon
Using the given framework, we created an MN lexicon that was incorporated into an
MN tagging scheme to be described in Section 6. Entries in the MN lexicon consist of:
(1) A string of one or more words: for example, should or have need of . (2) A part of
speech for each word: The part of speech helps us avoid irrelevant homophones such as
the noun can. (3) An MN designator: one of the 13 modality/negation cases described
previously. (4) A head word (or trigger): the primary phrasal constituent to cover
cases where an entry is a multi-word unit (e.g., the word hope in hope for). (5) One or
more subcategorization codes derived from the Longman Dictionary of Contemporary
English (LDOCE).
422
Baker et al Modality and Negation in SIMT
We produced the full English MN lexicon semi-automatically. First, we gathered a
small seed list of MN trigger words and phrases from our modality annotation manual
(Baker et al 2010c). Then, we expanded this small list of MN trigger words by running
an on-line search for each of the words, specifically targeting free on-line thesauri (e.g.,
thesaurus.com), to find both synonymous and antonymous words. From these we
manually selected the words we thought triggered modality (or their corresponding
negative variants) and filtered out words that we thought didn?t trigger modality. The
resulting list of MN trigger words and phrases contained about 150 lemmas.
We note that most intransitive (LDOCE) codes were not applicable to modality/
negation constructions. For example, hunger (in the Want modality class) has a modal
reading of ?desire? when combined with the preposition for (as in she hungered for a
promotion), but we do not consider it to be modal when it is used in the somewhat
archaic sentence He hungered, meaning that he did not have enough to eat. Thus the
LDOCE code I associated with the verb hungerwas hand-changed to I-FOR. There were
43 such cases. Once the LDOCE codes were hand-verified (and modified accordingly),
the mapping to subcategorization codes was applied.
The MN lexicon is publicly available at http://www.umiacs.umd.edu/?bonnie/
ModalityLexicon.txt. An example of an entry is given in Figure 6, for the verb need.
6. Automatic Modality/Negation Annotation
An MN tagger produces text or structured text in which modality or negation triggers
and/or targets are identified. Automatic identification of the holders of modalities was
beyond the scope of our project because the holder is often not explicitly stated in the
sentence in which the trigger and target occur. This section describes two types of MN
taggers?one that is string-based and one that is structure-based.
6.1 The String-Based English Modality/Negation Tagger
The string-based tagger operates on text that has been tagged with parts of speech
by a Collins-style statistical parser (Miller et al 1998). The tagger marks spans of
words/phrases that exactly match MN trigger words in the MN lexicon described
previously, and that exactly match the same parts of speech. This tagger identifies the
target of each modality/negation using the heuristic of tagging the next non-auxiliary
verb to the right of the trigger. Spans of words can be tagged multiple times with
different types of triggers and targets.
Figure 6
Modality lexicon entry for need.
423
Computational Linguistics Volume 38, Number 2
We found the string-based MN tagger to produce output that matched about 80%
of the sentence-level tags produced by our structure-based tagger, the results of which
are described next. Although string-based tagging is fast and reasonably accurate in
practice, we opted to focus on the indepth analysis of modality/negation of our SIMT
results using the more accurate structure-based tagger.
6.2 The Structure-Based English Modality/Negation Tagger
The structure-based MN tagger operates on text that has been parsed (Miller et al
1998). We used a version of the parser that produces flattened trees. In particular, the
flattener deletes VP nodes that are immediately dominated by VP or S and noun phrase
(NP) nodes that are immediately dominated by PP or NP. The parsed sentences are
processed by TSurgeon rules. Each TSurgeon rule consists of a pattern and an action.
The pattern matches part of a parse tree and the action alters the parse tree. More
specifically, the pattern finds an MN trigger word and its target and the action inserts
tags such as TrigRequire and TargRequire for triggers and targets for the modality
Require. Figure 7 shows output from the structure-based MN tagger. (Note that the
sentence is disfluent: Pakistan which could not reach semi-final, in a match against South
African team for the fifth position Pakistan defeated South Africa by 41 runs.) The example
shows that could is a trigger for the Ability modality and not is a trigger for negation.
Reach is a target for both Ability and Negation, which means that it is in the category of
?H is not able [to make P true/false]? in our coding scheme. Reach is also a trigger for
the Succeed modality and semi-final is its target.
The TSurgeon patterns are automatically generated from the verb class codes in
the MN lexicon along with a set of 15 templates. Each template covers one situation
such as the following: the target is the subject of the trigger; the target is the direct
object of the trigger; the target heads an infinitival complement of the trigger; the target
is a noun modified by an adjectival trigger, and so on. The verb class codes indicate
Figure 7
Sample output from the structure-based MN tagger.
424
Baker et al Modality and Negation in SIMT
which templates are applicable for each trigger word. For example, a trigger verb in the
transitive class may use two target templates, one in which the trigger is in active voice
and the target is a direct object (need tents) and one in which the trigger is in passive
voice and the target is a subject (tents are needed).
In developing the TSurgeon rules, we first conducted a corpus analysis for 40 of the
most common trigger words in order to identify and debug the most broadly applicable
templates. We then used LDOCE to assign verb classes to the remaining verbal triggers
in the MN lexicon, and we associated one or more debugged templates with each verb
class. In this way, the initial corpus work on a limited number of trigger words was
generalized to a longer list of trigger words. Because the TSurgeon patterns are tailored
to the flattened structures produced by our parser, it is not easily ported to new parser
outputs. The MN lexicon itself is portable, however. Switching parsers would entail
writing new TSurgeon templates, but the trigger words in the MN lexicon would still
be automatically assigned to templates based on their verb classes.
The following example shows an example of a TSurgeon pattern?action pair for a
sentence like They were required to provide tents. The pattern?action pair is intended to
be used after a pre-processing stage in which labels such as ?VoicePassive? and ?AUX?
have been assigned. ?VoicePassive? is inserted by a pre-processing TSurgeon pattern
because, in some cases, the target of a passive modality trigger word is in a different
location from the target of the corresponding active modality trigger word. ?AUX? is
inserted during pre-processing to distinguish auxiliary uses of have and be from their
uses as main verbs. The pattern portion of the pattern?action pair matches a node with
label VB that is not already tagged as a trigger and that is passive and dominates the
string ?required?. The VB node is also a sister to an S node, and the S node dominates a
VB that is not an auxiliary (provide in this case). The action portion of the pattern?action
pair inserts the string ?TargReq? as the second daughter of the second VB and inserts
the string ?TrigReq? as the second daughter of the first VB.
VB=trigger !< /^Trig/ < VoicePassive < required $..
(S < (VB=target !< AUX))
insert (TargReq) >2 target
insert (TrigReq) >2 trigger
Verb-specific patterns such as this one were generalized in order to gain coverage of
the whole modality lexicon. The specific lexical item, required, was replaced with a vari-
able, as were the labels ?TrigReq? and ?TargReq.? The pattern was then given a name,
V3-passive-basic, where V3 is a verb class tag from LDOCE (described in Section 5.3)
for verbs that take infinitive complements. We then looked up the LDOCE verb class
labels for all of the verbs in the modality lexicon. Using this information, we could then
generate a set of new, verb-specific patterns for each V3 verb in the modality lexicon.
6.3 Evaluating the Effectiveness of Structure-Based MN Tagging
We performed amanual inspection of the structure-based tagging output. We calculated
precision by examining 229 instances of modality triggers that were tagged by our
tagger from the English side of the NIST 09 MTEval training sentences. We analyzed
precision in two steps, first checking for the correct syntactic position of the target and
then checking the semantic correctness of the trigger and target. For 192 of the 229
triggers (around 84%), the targets were tagged in the correct syntactic location.
For example, for the sentence A solution must be found to this problem shown in
Figure 8, the word must is a modality trigger word, and the correct target is the first
425
Computational Linguistics Volume 38, Number 2
Figure 8
Example of embedded target head found inside VP must be found.
non-auxiliary verb heading a verb phrase that is contained in the syntactic complement
of must. The syntactic complement of must is the verb phrase be found to this problem.
The syntactic head of that verb phrase, be, is skipped because it is an auxiliary verb. The
correct (embedded) target found is the head of the syntactic complement of be.
The 192 modality instances with structurally correct targets do not all have seman-
tically correct tags. In this example, must is tagged as TrigBelief, where the correct tag
would be TrigRequire. Also, because theMN lexiconwas usedwithout respect to word
sense, words were sometimes erroneously identified as triggers. This includes non-
modal uses of work (work with refugees), reach (reach a destination), and attack (attack
a physical object), in constrast to modal uses of these words: work for peace (effort), reach
a goal (succeed), and attack a problem (effort). Fully correct tagging of modality would
need to include word sense disambiguation.
For 37 of the 229 triggers we examined, a target was not tagged in the correct syn-
tactic position. In 12 of 37 incorrectly tagged instances the targets are inside compound
nouns or coordinate structures (NP or VP), which are not yet handled by the modality
tagger. The remaining 25 of the 37 incorrectly tagged instances had targets that were lost
because the tagger does not yet handle all cases of nested modalities. Nested modalities
occur in sentences like They did not want to succeed in winning where the target words
want and succeed are also modality trigger words. Proper treatment of nested modalities
requires consideration of scope and compositional semantics.
Nesting was treated in two steps. First, the modality tagger marked each word as a
trigger and/or target. In They did not want to succeed in winning, not is marked as a trigger
for negation, want is marked as a target of negation and a trigger of wanting, succeed is
marked as a trigger of succeeding and a target of wanting, and win is marked as a target
of succeeding. The second step in the treatment of nested modalities occurs during tree
grafting, where the meanings of the nested modalities are composed. The tree grafting
program correctly composes some cases of nested modalities. For example, the tag
TrigAble composed with TrigNegation results in the target tag TargNOTAble, as shown
in Figure 9. In other cases, where compositional semantics are not yet accommodated,
the tree grafting program removed target labels from the trees, and those cases were
counted as incorrect for the purpose of this evaluation.
Figure 9
Example of modality composed with negation: TrigAble and TrigNegation combine to form
NOTAble.
426
Baker et al Modality and Negation in SIMT
In the 229 instances that we examined, there were 14 in which a light verb or noun
was the correct syntactic target, but not the correct semantic target. Decision would be
a better target than taken in The decision should be taken on delayed cases on the basis of
merit.We counted sentences with semantically light targets as correct in our evaluation
because our goal was to identify the syntactic head of the target. The semantics of the
target is a general issue, and we often find lexico-syntactic fluff between the trigger and
the most semantically salient target in sentences likeWe succeeded in our goal of winning
the war where ?success in war? is the salient meaning.
With respect to recall, the tagger primarily missed special forms of negation in
noun phrases and prepositional phrases: There was no place to seek shelter; The buildings
should be reconstructed, not with RCC, but with the wood and steel sheets. More complex
constructional and phrasal triggers were also missed: President Pervaiz Musharraf has said
that he will not rest unless the process of rehabilitation is completed. Finally, we discovered
some omissions from our MN lexicon: It is not possible in the middle of winter to re-open
the roads. Further annotation experiments are planned, which will be analyzed to close
such gaps and update the lexicon as appropriate.
Providing a quantitative measure of recall was beyond the scope of this project.
At best we could count instances of sentences containing trigger words that were not
tagged. We are also aware of many cases of modality that were not covered such as
the modal uses of the future tense auxiliary will as in That?ll be John (conjecture), I?ll do
the dishes (volition), He won?t do it (non-volition), and It will accommodate five (ability)
(Larreya 2009). Because of the complexity and subtlety of modality and negation, how-
ever, it would be impractical to count every clause (such as the not rest unless clause
above) that had a nuance of non-factivity.
7. Semantically Informed Syntactic MT
This section describes the incorporation of our structured-based MN tagging into an
Urdu?English machine-translation system using tree grafting for combining syntactic
symbols with semantic categories (e.g., modality/negation). We note that a de facto
Urdu MN tagger resulted from identifying the English MN trigger and target words in
a parallel English?Urdu corpus, and then projecting the trigger and target labels to the
corresponding words in Urdu syntax trees.
7.1 Refinement of Translation Grammars with Semantic Categories
We used synchronous context-free grammars (SCFGs) as the underlying formalism
for our statistical models of translation. SCFGs provide a convenient and theoretically
grounded way of incorporating linguistic information into statistical models of transla-
tion, by specifying grammar rules with syntactic non-terminals in the source and target
languages. We refine the set of non-terminal symbols so that they not only include
syntactic categories, but also semantic categories.
Chiang (2005) re-popularized the use of SCFGs for machine translation, with the
introduction of his hierarchical phrase-based machine translation system, Hiero. Hiero
uses grammars with a single non-terminal symbol ?X? rather than using linguistically
informed non-terminal symbols. When moving to linguistic grammars, we use Syntax
Augmented Machine Translation (SAMT) developed by Venugopal, Zollmann, and
Vogel (2007). In SAMT the ?X? symbols in translation grammars are replaced with
nonterminal categories derived from parse trees that label the English side of the
427
Computational Linguistics Volume 38, Number 2
Figure 10
A sentence on the English side of the bilingual parallel training corpus is parsed with a
syntactic parser, and also tagged with our modality tagger. The tags are then grafted onto
the syntactic parse tree to form new categories like VP-TargNOTAble and VP-TargRequire.
Grafting happens prior to extracting translation rules, which happens normally except for
the use of the augmented trees.
Urdu?English parallel corpus.1 We refine the syntactic categories by combining them
with semantic categories. Recall that this progression was illustrated in Figure 3.
We extracted SCFG grammar rules containing modality, negation, and named enti-
ties using an extraction procedure that requires parse trees for one side of the parallel
corpus. Although it is assumed that these trees are labeled and bracketed in a syntac-
tically motivated fashion, the framework places no specific requirement on the label
inventory. We take advantage of this characteristic by providing the rule extraction
algorithm with augmented parse trees containing syntactic labels that have semantic
annotations grafted onto them so that they additionally express semantic information.
Our strategy for producing semantically grafted parse trees involves three steps:
1. The English sentences in the parallel training data are parsed with a
syntactic parser. In our work, we used the lexicalized probabilistic context
free grammar parser provided by Basis Technology Corporation.
2. The English sentences are MN-tagged by the system described herein and
named-entity-tagged by the Phoenix tagger (Richman and Schone 2008).
3. The modality/negation and entity markers are grafted onto the syntactic
parse trees using a tree-grafting procedure. The grafting procedure was
implemented as part of the SIMT effort. Details are further spelled out in
Section 7.2.
Figure 10 illustrates how modality tags are grafted onto a parse tree. Note that
although we focus the discussion here on the modality and negation, our framework
is general and we were able to incorporate other semantic elements (specifically, named
entities) into the SIMT effort.
Once the semantically grafted trees have been produced for the parallel corpus, the
trees are presented, along with word alignments (produced by the Berkeley aligner),
to the rule extraction software to extract synchronous grammar rules that are both
1 For non-constituent phrases, composite CCG-style categories are used (Steedman 1999).
428
Baker et al Modality and Negation in SIMT
syntactically and semantically informed. These grammar rules are used by the decoder
to produce translations. In our experiments, we used the Joshua decoder (Li et al 2009),
the SAMT grammar extraction software (Venugopal and Zollmann 2009), and special
purpose-built tree-grafting software.
Figure 11 shows example semantic rules that are used by the decoder. The verb
phrase rules are augmented with modality and negation, taken from the semantic
categories listed in Table 2. Because these get marked on the Urdu source as well as
the English translation, semantically enriched grammars also act as very simple named
entity or MN taggers for Urdu. Only entities, modality, and negation that occurred in
the parallel training corpus are marked in the output, however.
7.2 Tree-Grafting Algorithm
The overall scheme of our tree-grafting algorithm is to match semantic tags to syntactic
categories. There are two inputs to the process. Each is derived from a common text
file of sentences. The first input is a list of standoff annotations for the semantically
tagged word sequences in the input sentences, indexed by sentence number. The second
is a list of parse trees for the sentences in Penn Treebank format, indexed by sentence
number.
Table 2 lists the modality/negation types that were produced by the MN tagger. For
example, the sentence The students are able to swim is tagged as The students are ?TrigAble?
to ?TargAble swim?. The distinction between ?Negation? and ?NOT? corresponds to the
difference between negation that is inherently expressed in the triggering lexical item
and negation that is expressed explicitly as a separate lexical item. Thus, I achieved
my goal is tagged ?Succeed? and I did not achieve my goal is tagged as ?NOTSucceed,?
Figure 11
Example translation rules with tags for modality, negation, and entities combined with
syntactic categories.
429
Computational Linguistics Volume 38, Number 2
Table 2
Modality tags with their negated versions. Note that Require and Permit are in a dual relation,
and thus RequireNegation is represented as NOTPermit and PermitNegation is represented
as NOTRequire.
Require NOTRequire
Permit NOTPermit
Succeed NOTSucceed
SucceedNegation NOTSucceedNegation
Effort NOTEffort
EffortNegation NOTEffortNegation
Intend NOTIntend
IntendNegation NOTIntendNegation
Able NOTAble
AbleNegation NOTAbleNegation
Want NOTWant
WantNegation NOTWantNegation
Belief NOTBelief
BeliefNegation NOTBeliefNegation
Firm Belief NOTFirm Belief
Firm BeliefNegation NOTFirm BeliefNegation
Negation
but I failed to win is tagged as ?SucceedNegation,? and I did not fail to win is tagged as
?NOTSucceedNegation.?
The tree-grafting algorithm proceeds as follows. For each tagged sentence, we
iterate over the list of semantic tags. For each semantic tag, there is an associated word
or sequence of words. For example, the modality tag TargAble may tag the word swim.
For each semantically tagged word, we find the parent node in the correspond-
ing syntactic parse tree that dominates that word. For a word sequence, we find and
compare the parent nodes for all of the words. Each node in the syntax tree has a
category label. The following tests are then made and tree grafts applied:
 If there is a single node in the parse tree that dominates all and only the
words with the semantic tag, graft the name of the semantic tag onto
the highest corresponding syntactic constituent in the tree. For example,
in Figure 10, which shows the grafting process for modality tagging,
the semantic tag TargNOTAble that ?hand over? receives is grafted onto
the VB node that dominates all and only the words ?hand over.? Then the
semantic tag TargNOTAble is passed up the tree to the VP node, which is
the highest corresponding syntactic constituent.
 If the semantic tag corresponds to words that are adjacent daughters in
a syntactic constituent, but less than the full constituent, insert a node
dominating those words into the parse tree, as a daughter of the original
syntactic constituent. The name of the semantic tag is grafted onto the new
node and becomes its category label. This is a case of tree augmentation by
node insertion.
 If a syntactic constituent selected for grafting has already been labeled
with a semantic tag, overlay the previous tag with the current tag. We
chose to tag in this manner simply because our system was not set up to
handle the grafting of multiple tags onto a single constituent. An example
430
Baker et al Modality and Negation in SIMT
of this occurs in the sentence ?The Muslims had obtained Pakistan.? If the
NP node dominating Pakistan is grafted with a named entity tag such as
NP-GPE, we overlay this with the NP-TargSucceed tag in a modality
tagging scheme.
 In the case of a word sequence, if the words covered by the semantic tag
fall across two different syntactic constituents, do nothing. This is a case of
crossing brackets.
Our tree-grafting procedure was simplified to accept a single semantic tag per
syntactic tree node as the final result. The algorithm keeps the last tag seen as the tag of
precedence. In practice, we established a precedence ordering for modality/negation
tags over named entity tags by grafting named entity tags first and modality/negation
second. Our intuition was that, in case of a tie, finer-grained verbal categories would be
more helpful to parsing than finer-grained nominal categories.2 In cases where a word
was tagged both as a MN target and a MN trigger, we gave precedence to the target tag.
This is because, although MN targets vary, MN triggers are generally identifiable with
lexical items. Finally, we used the simplified specificity ordering of MN tags described
in Section 5.2 to ensure precedence of more specific tags over more general ones. Table 2
lists the modality/negation types from highest (Require modality) to lowest (Negation)
precedence.3
7.3 SIMT Results
We evaluated our tree grafting approach by performing a series of translation experi-
ments. Each version of our translation systemwas trained on the same bilingual training
data. The bilingual parallel corpus that we used was distributed as part of the 2008
NIST Open Machine Translation Evaluation Workshop.4 The training set contained
88,108 Urdu?English sentence pairs, and a bilingual dictionary with 113,911 entries.
For our development and test sets, we split the NIST MT-08 test set into two portions
(with each document going into either test or dev, and preserving the genre split).
Our test set contained 883 Urdu sentences, each with four translations into English,
and our dev set contained 981 Urdu sentences, each with four reference translations.
To extract a syntactically informed translation model, we parsed the English side of
the training corpus using a Penn Treebank?trained parser (Miller et al 1998). For the
experiments that involved grafting named entities onto the parse trees, we tagged
the English side of the training corpus with the Phoenix tagger (Richman and Schone
2008). We word-aligned the parallel corpus with the Berkeley aligner. All models used a
5-gram language model trained on the English Gigaword corpus (v5) using the SRILM
toolkit with modified KN smoothing. The Hiero translation grammar was extracted
using the Joshua toolkit (Li et al 2009). The other translation grammars were extracted
using the SAMT toolkit (Venugopal and Zollmann 2009).
2 In testing we found that grafting named entities first and MN last yielded a slightly higher BLEU score
than the reverse order.
3 Future work could include exploring additional methods of resolving tag conflicts or combining tag
types on single nodes, for example, by inserting multiple intermediate nodes (effectively using unary
rewrite rules) or by stringing tag names together.
4 http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/.
431
Computational Linguistics Volume 38, Number 2
Figure 12
Results for a range of experiments conducted during the SIMT effort show the score for our
top-performing baseline systems derived from a hierarchical phrase-based model (Hiero).
Substantial improvements obtained when syntax was introduced along with feature functions
(FFs) and further improvements resulted from the addition of semantic elements. The scores
are lowercased BLEU calculated on the held-out devtest set. NE = named entities.
Figure 12 gives the results for a number of experiments conducted during the SIMT
effort.5 The experiments are broken into three groups: baselines, syntax, and semantics.
To contextualize our results we experimented with a number of different baselines
that were composed from two different approaches to statistical machine translation?
phrase-based and hierarchical phrase-based SMT?along with different combinations
of language model sizes and word aligners. Our best-performing baseline was a Hiero
model. The Bleu score for this baseline on the development set was 22.9 Bleu points.
After experimenting with syntactically motivated grammar rules, we conducted
experiments on the effects of incorporating semantic elements (e.g., named entities and
modality/negation) into the translation grammars. In our devtest set our taggers tagged
on average 3.5 named entities per sentence and 0.35 MN markers per sentence. These
were included by grafting modality, negation, and named-entity markers onto the parse
trees. Individually, each of these made modest improvements over the syntactically
informed system alone. Grafting named entities onto the parse trees improved the Bleu
score by 0.2 points. Modality/negation improved it by 0.3 points. Doing both simulta-
neously had an additive effect and resulted in a 0.5 Bleu score improvement over syntax
alone. This improvement was the largest improvement that we got from anything other
than the move from linguistically naive models to syntactically informed models.
We used bootstrap resampling to test whether the differences in Bleu scores were
statistically significant (Koehn 2004). All of the results were a significant improvement
over Hiero (at p ? 0.01). The difference between the syntactic system and the syntactic
system with named entities is not significant (p = 0.38). The differences between the
5 These experiments were conducted on the devtest set, containing 883 Urdu sentences (21,623 Urdu
words) and four reference translations per sentence. The BLEU score for these experiments is measured
on uncased output.
432
Baker et al Modality and Negation in SIMT
syntactic system and the syntactic system with MN, and between the syntactic system
and the syntactic system with both MN and named entities were both significant at
(p ? 0.05).
Figure 13 shows example output from the final SIMT system in comparison to
the pre-SIMT results and the translation produced by a human (reference). An error
analysis of this example output illustrates that SIMT enhancements have resulted in the
elimination of misleading translation output in several cases:
1. pre-SIMT: China had the experience of Pakistan?s first nuclear bomb.
SIMT: China has the first nuclear bomb test.
reference: China has conducted the experiment of Pakistan?s first nuclear bomb.
2. pre-SIMT: the nuclear bomb in 1998 that Pakistan may experience
SIMT: the experience of the atom bomb Pakistan in May 1998
reference: the atom bomb, whose experiment was done in 1998
by Pakistan
3. pre-SIMT: He said that it is also present proof of that Dr. Abdul Qadeer
Khan after the Chinese design
SIMT: He said that there is evidence that Dr. Abdul Qadeer Khan has
also used the Chinese design
reference: He said that the proof to this also exists in that Dr. Abdul
Qadeer Khan used the Chinese design
The article in question pertains to claims by Thomas Reid that China allowed Pakistan
to detonate a nuclear weapon at its test site. In the first example, however, the reader is
potentially misled by the pre-SIMT output to believe that Pakistan launched a nuclear
bomb on China. The SIMT output leaves out the mention of Pakistan, but correctly con-
veys the firm belief that the bomb event is a test (closely resembling the term experiment
in the human reference), not a true bombing event. This is clearly an improvement over
the misleading pre-SIMT output.
In the second example, the pre-SIMT output misleads the reader to believe that
Pakistan is (or will be) attacked, through the use of the phrase may experience, where
may is poorly placed. (We note here that this is a date translation error, i.e., the month
of May should be next to the year 1998, further adding to the potential for confusion.)
Unfortunately, the SIMT output also uses the term experience (rather than experiment,
which is in the human reference), but in this case the month is correctly positioned in
the output, thus eliminating the potential for confusionwith respect to themodality. The
lack of a modal appropriately neutralizes the statement so that it refers to an abstract
event associated with the atom bomb, rather than an attack on the country.
In the third example, where the Chinese design used by Dr. Abdul Qandeer Khan is
argued to be proof of the nuclear testing relationship between Pakistan and China, the
first pre-SIMT output potentially leads the reader to believe that Dr. Abdul Qadeer is
after the Chinese design (not that he actually used it), whereas the SIMT output conveys
the firm belief that the Chinese design has been used by Dr. Abdul Qadeer. This output
very closely matches the human reference.
Note that even in the title of the article, the SIMT system produces much more
coherent English output than that of the linguistically naive system. The figure also
shows improvements due to transliteration, which are described in Irvine et al (2010).
The scores reported in Figure 12 do not include transliteration improvements.
433
Computational Linguistics Volume 38, Number 2
Figure 13
An example of the improvements to Urdu?English translation before and after the SIMT effort.
Output is from the baseline Hiero model, which does not use linguistic information, and from
the final model, which incorporates syntactic and semantic information.
434
Baker et al Modality and Negation in SIMT
8. Conclusions and Future Work
We developed a modality/negation lexicon and a set of automatic MN taggers, one of
which?the structure-based tagger?results in 86% precision for tagging of a standard
LDC data set. The MN tagger has been used to improve machine translation output
by imposing semantic constraints on possible translations in the face of sparse training
data. The tagger is also an important component of a language-understanding module
for a related project.
We have described a technique for translation that shows particular promise
for low-resource languages. We have integrated linguistic knowledge into statistical
machine translation in a unified and coherent framework. We demonstrated that
augmenting hierarchical phrase-based translation rules with semantic labels (through
?grafting?) resulted in a 0.5 Bleu score improvement over syntax alone.
Although our largest gains were from syntactic enrichments to the Hiero model,
demonstrating success on the integration of semantic aspects of language bodes well
for additional improvements based on the incorporation of other semantic aspects. For
example, we hypothesize that incorporating relations and temporal knowledge into
the translation rules would further improve translation quality. The syntactic grafting
framework is well-suited to support the exploration of the impact of many different
types of semantics on MT quality, though in this article we focused on exploring the
impact of modality and negation.
An important future study is one that focuses on demonstrating whether further
improvements in modality/negation identification are likely to lead to further gains in
translation performance. Such a study would benefit from the inclusion of a more de-
tailed manual evaluation to determine if modality and negation is adequately conveyed
in the downstream translations. This work would be additionally enhanced through
experimentation on other language pair(s) and larger corpora.
The work presented here represents the first small steps toward a full integration
of MT and semantics. Efforts underway in DARPA?s GALE program demonstrated the
potential for combining MT and semantics (termed distillation) to answer the informa-
tion needs of monolingual speakers using multilingual sources. Proper recognition of
modalities and negation is crucial for handling those information needs effectively.
In previous work, however, semantic processing proceeded largely independently of
the MT system, operating only on the translated output. Our approach is significantly
different in that it combines syntax, semantics, and MT into a single model, offering
the potential advantages of joint modeling and joint decision-making. It would be
interesting to explore whether the integration of MT with syntax and semantics can be
extended to provide a single-model solution for tasks such as cross-language informa-
tion extraction and question answering, and to evaluate our integrated approach (e.g.,
using GALE distillation metrics).
Acknowledgments
We thank Aaron Phillips for help with
conversion of the output of the entity tagger
for ingest by the tree-grafting program. We
thank Anni Irvine and David Zajic for their
help with experiments on an alternative
Urdu modality/negation tagger based on
projection and training an HMM-based
tagger derived from Identifinder (Bikel,
Schwartz, and Weischedel 1999). For their
helpful ideas and suggestions during the
development of the modality framework,
we are indebted to Mona Diab, Eduard
Hovy, Marge McShane, Teruko Mitamura,
Sergei Nirenburg, Boyan Onyshkevych,
and Owen Rambow. We also thank Basis
Technology Corporation for their generous
contribution of software components to this
work. This work was supported, in part,
by the Johns Hopkins Human Language
435
Computational Linguistics Volume 38, Number 2
Technology Center of Excellence (HLTCOE),
by the National Science Foundation under
grant IIS-0713448, and by BBN Technologies
under GALE DARPA/IPTO contract no.
HR0011-06-C-0022. Any opinions, findings,
and conclusions or recommendations
expressed in this material are those of the
authors and do not necessarily reflect the
views of the sponsor.
References
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of the
36th Annual Meeting of the Association
for Computational Linguistics and
17th International Conference on
Computational Linguistics - Volume 1,
ACL ?98, pages 86?90, Stroudsburg, PA.
Baker, Kathryn, Steven Bethard, Michael
Bloodgood, Ralf Brown, Chris Callison-
Burch, Glen Coppersmith, Bonnie J. Dorr,
Nathaniel W. Filardo, Kendall Giles, Ann
Irvine, Michael Kayser, Lori Levin, Justin
Martineau, James Mayfield, Scott Miller,
Aaron Phillips, Andrew Philpot, Christine
Piatko, Lane Schwartz, and David Zajic.
2010a. Semantically informed machine
translation. Technical Report 002,
Human Language Technology Center of
Excellence, Johns Hopkins University,
Baltimore, MD.
Baker, Kathryn, Michael Bloodgood,
Chris Callison-Burch, Bonnie J. Dorr,
Nathaniel W. Filardo, Lori Levin, Scott
Miller, and Christine Piatko. 2010b.
Semantically-informed machine
translation: A tree-grafting approach.
In Proceedings of The Ninth Biennial
Conference of the Association for Machine
Translation in the Americas, Denver, CO.
Baker, Kathryn, Michael Bloodgood, Mona
Diab, Bonnie J. Dorr, Ed Hovy, Lori Levin,
Marjorie McShane, Teruko Mitamura,
Sergei Nirenburg, Christine Piatko, Owen
Rambow, and Gramm Richardson. 2010c.
SIMT SCALE 2009?Modality annotation
guidelines. Technical Report 004, Human
Language Technology Center of
Excellence, Johns Hopkins University,
Baltimore, MD.
Baker, Kathryn, Michael Bloodgood,
Bonnie J. Dorr, Nathanial W. Filardo,
Lori Levin, and Christine Piatko.
2010d. A modality lexicon and its use
in automatic tagging. In Proceedings of
the Seventh International Conference on
Language Resources and Evaluation
(LREC), pages 1402?1407, Mediterranean
Conference Center, Valletta.
Bar-Haim, Roy, Ido Dagan, Iddo Greental,
and Eyal Shnarch. 2007. Semantic
inference at the lexical-syntactic level.
In Proceedings of the 22nd National
Conference on Artificial intelligence -
Volume 1, pages 871?876, Vancouver,
British Columbia.
Bikel, Daniel M., Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm
that learns what?s in a name.Machine
Learning, 34(1?3):211?231.
Bo?hmova?, Alena, Silvie Cinkova?, and
Eva Hajic?ova?. 2005. A manual for
tectogrammatical layer annotation of the
Prague Dependency Treebank [English
translation]. Technical Report #30, U?FAL
MFF UK, Prague, Czech Republic.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL-2005),
pages 263?270, Ann Arbor, MI.
Diab, Mona T., Lori Levin, Teruko Mitamura,
Owen Rambow, Vinodkumar
Prabhakaran, and Weiwei Guo. 2009.
Committed belief annotation and tagging.
In Proceedings of the Third Linguistic
Annotation Workshop, ACL-IJCNLP ?09,
pages 68?73, Stroudsburg, PA.
Farkas, Richa?rd, Veronika Vincze, Gyo?rgy
Mo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.
2010. The CoNLL-2010 shared task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the Fourteenth Conference on Computational
Natural Language Learning?Shared Task,
CoNLL ?10: Shared Task, pages 1?12,
Stroudsburg, PA.
Fellbaum, Christiane, editor. 1998.WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Hajic?, Jan, Eva Hajic?ova?, Petr Pajas, Jarmila
Panevova?, Petr Sgall, and Barbora Vidova?
Hladka?. 2001. Prague Dependency
Treebank 1.0 (Final Production Label),
UFAL MFF UK, Prague, Czech Republic.
Huang, Bryant and Kevin Knight. 2006.
Relabeling syntax trees to improve
syntax-based machine translation
quality. In HLT-NAACL, New York.
Irvine, Ann, Mike Kayser, Zhifei Li, Wren
Thornton, and Chris Callison-Burch.
2010. Integrating output from specialized
modules in machine translation:
Transliteration in Joshua. Proceedings
of the Human Language Technology
436
Baker et al Modality and Negation in SIMT
and North American Chapter of the
Association for Computational Linguistics,
pages 240?247. The Prague Bulletin of
Mathematical Linguistics, 93:107?116.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo, Japan.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In
Proceedings of EMNLP 2004, pages 388?395,
Barcelona.
Koehn, Philipp, Hieu Hoang, Alexandra
Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine
translation. In Proceedings of the ACL-2007
Demo and Poster Sessions, Prague, Czech
Republic, pages 177?180.
Kratzer, Angelika. 1991. Modality. In
Arnim von Stechow and Dieter, editors,
Semantics: An International Handbook of
Contemporary Research. De Gruyter,
Berlin, pages 639?650.
Larreya, Paul. 2009. Towards a typology of
modality in language. In Raphael Salkie,
Pierre Busuttil, and Johan van der Auwera,
editors,Modality in English: Theory and
Description. Mouton de Gruyter, Paris,
pages 9?30.
Li, Zhifei, Chris Callison-Burch, Chris Dyer,
Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar
Zaidan. 2009. Joshua: An open source
toolkit for parsing-based machine
translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation,
pages 135?139, Athens.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
McShane, Marjorie, Sergei Nirenburg, and
Ron Zacharski. 2004. Mood and modality:
Out of the theory and into the fray. Natural
Language Engineering, 19(1):57?89.
Miller, Scott, Heidi Fox, Lance Ramshaw,
and Ralph Weischedel. 1998. SIFT:
Statistically-derived information from
text. In Seventh Message Understanding
Conference (MUC-7), Washington, DC,
Miller, Scott, Heidi J. Fox, Lance A.
Ramshaw, and Ralph M. Weischedel. 2000.
A novel use of statistical parsing to extract
information from text. In Proceedings of
Applied Natural Language Processing
and the North American Association for
Computational Linguistics, pages 226?233,
Seattle, Washington.
Murata, Masaki, Kiyotaka Uchimoto, Qing
Ma, Toshiyuki Kanamaru, and Hitoshi
Isahara. 2005. Analysis of machine
translation systems? errors in tense,
aspect, and modality. In Proceedings of the
19th Asia-Pacific Conference on Language,
Information and Computing (PACLIC 2005),
Taipei, Taiwan.
Nairn, Rowan, Cleo Condorovdi, and
Lauri Karttunen. 2006. Computing
relative polarity for textual inference.
In Proceedings of the International Workshop
on Inference in Computational Semantics
(ICoS-5), pages 66?76, Buxton, England.
Palmer, Martha, Daniel Gildea, and
Paul Kingsbury. 2005. The Proposition
Bank: An annotated corpus of semantic
roles. Computational Linguistics,
31:71?106.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics (ACL-2002),
pages 311?318, Philadelphia, PA.
Petrov, Slav and Dan Klein. 2007. Learning
and inference for hierarchically split
PCFGs. In Proceedings of the 22nd American
Association for Artificial Intelligence,
pages 1663?1666, Vancouver, British
Columbia, Canada.
Prabhakaran, Vinodkumar, Owen Rambow,
and Mona Diab. 2010. Automatic
committed belief tagging. In Proceedings
of the 23rd International Conference on
Computational Linguistics: Posters, COLING
?10, pages 1014?1022, Beijing, China.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, Livio Robaldo, Aravind
Joshi, and Bonnie Webber. 2008. The Penn
Discourse TreeBank 2.0. In Proceedings of
the Sixth International Language Resources
and Evaluation (LREC?08), pages 28?30,
Marrakech.
Pustejovsky, James, Marc Verhagen, Roser
Saur??, Jessica Littman, Robert Gaizauskas,
Graham Katz, Inderjeet Mani, Robert
Knippen, and Andrea Setzer. 2006.
TimeBank 1.2. Linguistic Data Consortium,
Philadelphia, PA.
Richman, Alexander and Patrick Schone.
2008. Mining wiki resources for
multilingual named entity recognition.
437
Computational Linguistics Volume 38, Number 2
In Proceedings of ACL-08: HLT, pages 1?9,
Columbus, OH.
Rubin, Victoria L. 2007. Stating with
certainty or stating with doubt:
Intercoder reliability results for manual
annotation of epistemically modalized
statements. In Proceedings of the Human
Language Technology and North American
Chapter of the Association for Computational
Linguistics (Short Papers), pages 141?144,
Rochester, NY.
Saur??, Roser and James Pustejovsky. 2009.
FactBank: A corpus annotated with
event factuality. Language Resources and
Evaluation, 43(3):227?268.
Saur??, Roser, Marc Verhagen, and James
Pustejovsky. 2006. Annotating and
recognizing event modality in text.
In Proceedings of the 19th International
Florida Artificial Intelligence Research
Society Conference, pages 333?339,
Melbourne Beach, FL.
Sigurd, Bengt and Barbara Gawro?nska.
1994. Modals as a problem for MT. In
Proceedings of the 15th International
Conference on Computational Linguistics
(COLING) - Volume 1, pages 120?124,
Kyoto, Japan.
Steedman, Mark. 1999. Alternating
quantifier scope in CCG. In Proceedings of
the 37th Annual Meeting of the Association
for Computational Linguistics (ACL),
College Park, MD.
Szarvas, Gyo?rgy, Veronika Vincze, Richa?rd
Farkas, and Ja?nos Csirik. 2008. The
BioScope corpus: Annotation for negation,
uncertainty and their scope in biomedical
texts. In Proceedings of the Workshop on
Current Trends in Biomedical Natural
Language Processing, pages 38?45,
Stroudsburg, PA.
van der Auwera, Johan and Andreas
Ammann. 2005. Overlap between
situational and epistemic modal marking.
In Martin Haspelmath, Matthew S. Dryer,
David Gil, and Bernard Comrie, editors,
World Atlas of Language Structures. Oxford
University Press, New York, chapter 76,
pages 310?313.
Venugopal, Ashish and Andreas Zollmann.
2009. Grammar based statistical MT on
Hadoop: An end-to-end toolkit for large
scale PSCFG based MT. Prague Bulletin of
Mathematical Linguistics, 91:67?78.
Venugopal, Ashish, Andreas Zollmann, and
Stephan Vogel. 2007. An efficient two-pass
approach to synchronous-CFG driven
statistical MT. In Proceedings of the
Human Language Technology Conference
of the North American Chapter of the
Association for Computational Linguistics
(HLT/NAACL-2007), pages 500?507,
Rochester, NY.
von Fintel, Kai and Sabine Iatridou.
2006. How to say ought in foreign: The
composition of weak necessity modals.
In Proceedings of the 6th Workshop on
Formal Linguistics, Florianopolis, Brazil,
August 2006.
Wang, Wei, Jonathan May, Kevin Knight,
and Daniel Marcu. 2010. Re-structuring,
re-labeling, and re-aligning for
syntax-based machine translation.
Computational Linguistics, 36(2):247?277.
Webber, Bonnie, Aravid Joshi, Matthew
Stone, and Alistair Knott. 2003. Anaphora
and discourse structure. Computational
Linguistics, 29:545?587.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions
of opinions and emotions in language.
Language Resources and Evaluation,
39(2?3):165?210.
Wilson, Theresa, Janyce Wiebe, and Paul
Hoffman. 2009. Recognizing contextual
polarity: An exploration of features
for phrase-level sentiment analysis.
Computational Linguistics, 35:399?433.
Zollmann, Andreas and Ashish Venugopal.
2006. Syntax augmented machine
translation via chart parsing. In
Proceedings on the Workshop on Statistical
Machine Translation, pages 138?141,
New York City.
438
Arabic Dialect Identification
Omar F. Zaidan?
Microsoft Research
Chris Callison-Burch??
University of Pennsylvania
The written form of the Arabic language, Modern Standard Arabic (MSA), differs in a non-
trivial manner from the various spoken regional dialects of Arabic?the true ?native languages?
of Arabic speakers. Those dialects, in turn, differ quite a bit from each other. However, due to
MSA?s prevalence in written form, almost all Arabic data sets have predominantly MSA content.
In this article, we describe the creation of a novel Arabic resource with dialect annotations. We
have created a large monolingual data set rich in dialectal Arabic content called the Arabic On-
line Commentary Data set (Zaidan and Callison-Burch 2011). We describe our annotation
effort to identify the dialect level (and dialect itself) in each of more than 100,000 sentences from
the data set by crowdsourcing the annotation task, and delve into interesting annotator behaviors
(like over-identification of one?s own dialect). Using this new annotated data set, we consider
the task of Arabic dialect identification: Given the word sequence forming an Arabic sentence,
determine the variety of Arabic in which it is written. We use the data to train and evaluate
automatic classifiers for dialect identification, and establish that classifiers using dialectal data
significantly and dramatically outperform baselines that use MSA-only data, achieving near-
human classification accuracy. Finally, we apply our classifiers to discover dialectical data from
a large Web crawl consisting of 3.5 million pages mined from on-line Arabic newspapers.
1. Introduction
The Arabic language is a loose term that refers to the many existing varieties of Arabic.
Those varieties include one ?written? form, Modern Standard Arabic (MSA), and many
?spoken? forms, each of which is a regional dialect. MSA is the only variety that
is standardized, regulated, and taught in schools, necessitated by its use in written
communication and formal venues. The regional dialects, used primarily for day-to-
day dealings and spoken communication, remain somewhat absent from written com-
munication compared with MSA. That said, it is certainly possible to produce dialectal
Arabic text, by using the same letters used in MSA and the same (mostly phonetic)
spelling rules of MSA.
? E-mail: ozaidan@gmail.com.
?? Computer and Information Science Department University of Pennsylvania, Levine Hall, room 506,
3330 Walnut Street, Philadelphia, PA 19104. E-mail: ccb@cis.upenn.edu.
Submission received: 12 March 2012; revised version received: 14 March 2012; accepted for publication:
17 April 2013.
doi:10.1162/COLI a 00169
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
One domain of written communication in which both MSA and dialectal Arabic are
commonly used is the on-line domain: Dialectal Arabic has a strong presence in blogs,
forums, chatrooms, and user/reader commentary. Harvesting data from such sources
is a viable option for computational linguists to create large data sets to be used in
statistical learning setups. However, because all Arabic varieties use the same character
set, and furthermore much of the vocabulary is shared among different varieties, it is
not a trivial matter to distinguish and separate the dialects from each other.
In this article, we focus on the problem of Arabic dialect identification. We describe
a large data set that we created by harvesting a large amount of reader commentary
on on-line newspaper content, and describe our annotation effort on a subset of the
harvested data. We crowdsourced an annotation task to obtain sentence-level labels
indicating what proportion of the sentence is dialectal, and which dialect the sentence
is written in. Analysis of the collected labels reveals interesting annotator behavior
patterns and biases, and the data are used to train and evaluate automatic classifiers for
dialect detection and identification. Our approach, which relies on training language
models for the different Arabic varieties, greatly outperforms baselines that use (much
more) MSA-only data: On one of the classification tasks we considered, where human
annotators achieve 88.0% classification accuracy, our approach achieves 85.7% accuracy,
compared with only 66.6% accuracy by a system using MSA-only data.
The article is structured as follows. In Section 2, we provide an introduction to
the various Arabic varieties and corresponding data resources. In Section 3, we intro-
duce the dialect identification problem for Arabic, discussing what makes it a difficult
problem, and what applications would benefit from it. Section 4 provides details about
our annotation set-up, which relied on crowdsourcing the annotation to workers on
Amazon?s Mechanical Turk. By examining the collected labels and their distribution,
we characterize annotator behavior and observe several types of human annotator
biases. We introduce our technique for automatic dialect identification in Section 5.
The technique relies on training separate language models for the different Arabic
varieties, and scoring sentences using these models. In Section 6, we report on a large-
scale Web crawl that we performed to gather a large amount of Arabic text from on-line
newspapers, and apply our classifier on the gathered data. Before concluding, we give
an overview of related work in Section 7.
2. Background: The MSA/Dialect Distinction in Arabic
Although the Arabic language has an official status in over 20 countries and is spoken
by more than 250 million people, the term itself is used rather loosely and refers to
different varieties of the language. Arabic is characterized by an interesting linguistic
dichotomy: the written form of the language, MSA, differs in a non-trivial fashion from
the various spoken varieties of Arabic, each of which is a regional dialect (or a lahjah,
lit. ?accent?; also darjah, lit. ?modern?). MSA is the only variety that is standardized,
regulated, and taught in schools. This is necessitated because of its use in written
communication in formal venues.1 The regional dialects, used primarily for day-to-day
dealings and spoken communication, are not taught formally in schools, and remain
somewhat absent from traditional, and certainly official, written communication.
1 The term MSA is used primarily by linguists and in educational settings. For example, constitutions
of countries where Arabic is an official language simply refer to The Arabic Language, the reference to
the standard form of Arabic being implicit.
172
Zaidan and Callison-Burch Arabic Dialect Identification
Unlike MSA, a regional dialect does not have an explicit written set of grammar
rules regulated by an authoritative organization, but there is certainly a concept of
grammatical and ungrammatical.2 Furthermore, even though they are ?spoken? varieties,
it is certainly possible to produce dialectal Arabic text, by spelling out words using the
same spelling rules used in MSA, which are mostly phonetic.3
There is a reasonable level of mutual intelligibility across the dialects, but the
extent to which a particular individual is able to understand other dialects depends
heavily on that person?s own dialect and their exposure to Arab culture and literature
from outside of their own country. For example, the typical Arabic speaker has little
trouble understanding the Egyptian dialect, thanks in no small part to Egypt?s history
in movie-making and television show production, and their popularity across the Arab
world. On the other hand, the Moroccan dialect, especially in its spoken form, is quite
difficult to understand by a Levantine speaker. Therefore, from a scientific point of
view, the dialects can be considered separate languages in their own right, much like
North Germanic languages (Norwegian/Swedish/Danish) and West Slavic languages
(Czech/Slovak/Polish).4
2.1 The Dialectal Varieties of Arabic
One possible breakdown of regional dialects into main groups is as follows (see
Figure 1):
 Egyptian: The most widely understood dialect, due to a thriving Egyptian
television and movie industry, and Egypt?s highly influential role in the
region for much of the 20th century (Haeri 2003).
 Levantine: A set of dialects that differ somewhat in pronunciation and
intonation, but are largely equivalent in written form; closely related to
Aramaic (Bassiouney 2009).
 Gulf: Folk wisdom holds that Gulf is the closest of the regional dialect to
MSA, perhaps because the current form of MSA evolved from an Arabic
variety originating in the Gulf region. Although there are major differences
between Gulf and MSA, Gulf has notably preserved more of MSA?s verb
conjugation than other varieties have (Versteegh 2001).
2 There exist resources that describe grammars and dictionaries of many Arabic dialects (e.g.,
Abdel-Massih, Abdel-Malek, and Badawi 1979; Badawi and Hinds 1986; Cowell 1964; Erwin 1963;
Ingham 1994; Holes 2004), but these are compiled by individual linguists as one-off efforts, rather
than updated regularly by central regulatory organizations, as is the case with MSA and many
other world languages.
3 Arabic speakers writing in dialectal Arabic mostly follow MSA spelling rules in cases where MSA
is not strictly phonetic as well (e.g., the pronunciation of the definite article Al). Habash, Diab, and
Rambow (2012) have proposed CODA, a Conventional Orthography for Dialectal Arabic, to
standardize the spelling of Arabic dialect computational models.
4 Note that such a view is not widely accepted by Arabic speakers, who hold MSA in high regard. They
consider dialects, including their own, to be simply imperfect, even ?corrupted,? versions of MSA,
rather than separate languages (Suleiman 1994). One exception might be the Egyptian dialect, where a
nationalistic movement gave rise to such phenomena as the Egyptian Wikipedia, with articles written
exclusively in Egyptian, and little, if any, MSA. Another notable exception is the Lebanese poet Said Akl,
who spearheaded an effort to recognize Lebanese as an independent language, and even proposed a
Latin-based Lebanese alphabet.
173
Computational Linguistics Volume 40, Number 1
Figure 1
One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine,
Gulf, and Iraqi. Habash (2010) and Versteegh (2001) give a breakdown along mostly the same
lines. Note that this is a relatively coarse breakdown, and further division of the dialect groups
is possible, especially in large regions such as the Maghreb.
 Iraqi: Sometimes considered to be one of the Gulf dialects, though it has
distinctive features of its own in terms of prepositions, verb conjugation,
and pronunciation (Mitchell 1990).
 Maghrebi: Heavily influenced by the French and Berber languages. The
Western-most varieties could be unintelligible by speakers from other
regions in the Middle East, especially in spoken form. The Maghreb is a
large region with more variation than is seen in other regions such as the
Levant and the Gulf, and could be subdivided further (Mohand 1999).
There are a large number of linguistic differences between MSA and the regional
dialects. Some of those differences do not appear in written form if they are on the level
of short vowels, which are omitted in Arabic text anyway. That said, many differences
manifest themselves textually as well:
 MSA?s morphology is richer than dialects? along some dimensions such
as case and mood. For instance, MSA has a dual form in addition to the
singular and plural forms, whereas the dialects mostly lack the dual
form. Also, MSA has two plural forms, one masculine and one feminine,
whereas many (though not all) dialects often make no such gendered
distinction.5 On the other hand, dialects have a more complex cliticization
system than MSA, allowing for circumfix negation, and for attached
pronouns to act as indirect objects.
 Dialects lack grammatical case, whereas MSA has a complex case system.
In MSA, most cases are expressed with diacritics that are rarely explicitly
written, with the accusative case being a notable exception, as it is
expressed using a suffix (+A) in addition to a diacritic (e.g., on objects
and adverbs).
5 Dialects may preserve the dual form for nouns, but often lack it in verb conjugation and pronouns, using
plural forms instead. The same is true for the gendered plural forms, which exist for many nouns (e.g.,
?teachers? is either m?lmyn [male] or m?lmAt [female]), but not used otherwise as frequently as in MSA.
174
Zaidan and Callison-Burch Arabic Dialect Identification
 There are lexical choice differences in the vocabulary itself. Table 1
gives several examples. Note that these differences go beyond a lack
of orthography standardization.
 Differences in verb conjugation, even when the triliteral root is preserved.
See the lower part of Table 1 for some conjugations of the root s?-r-b
(to drink).
This list, and Table 1, deal with differences that are expressed at the inidividual-
word level. It is important to note that Arabic varieties differ markedly in style and
sentence composition as well. For instance, all varieties of Arabic, MSA, and otherwise,
allow both SVO and VSO word orders, but MSA has a higher incidence of VSO sen-
tences than dialects do (Aoun, Benmamoun, and Sportiche 1994; Shlonsky 1997).
2.2 Existing Arabic Data Sources
Despite the fact that speakers are usually less comfortable communicating in MSA than
in their own dialect, MSA content significantly dominates dialectal content, as MSA
is the variant of choice for formal and official communication. Relatively little printed
material exists in local dialects, such as folkloric literature and some modern poetry,
but the vast majority of published Arabic is in MSA. As a result, MSA?s dominance is
also apparent in data sets available for linguistic research. The problem is somewhat
mitigated in the speech domain, since dialectal data exists in the form of phone conver-
sations and television program recordings, but, in general, dialectal Arabic data sets are
hard to come by.
Table 1
A few examples illustrating similarities and differences across MSA and three Arabic dialects:
Levantine, Gulf, and Egyptian. Even when a word is spelled the same across two or more
varieties, the pronunciation might differ due to differences in short vowels (which are not
spelled out). Also, due to the lack of orthography standardization, and variance in pronunciation
even within a single dialect, some dialectal words could have more than one spelling (e.g.,
Egyptian ?I drink? could be bAs?rb, Levantine ?He drinks? could be bys?rb). (We use the
Habash-Soudi-Buckwalter transliteration scheme to represent Arabic orthography, which maps
each Arabic letter to a single, distinct character. We provide a table with the character mapping
in Appendix A.)
English MSA LEV GLF EGY
Book ktAb ktAb ktAb ktAb
Year sn sn sn sn
Money nqwd mSAry flws flws
Come on! hyA! ylA! ylA! ylA!
I want Aryd bdy Ab?y? ?Ayz
Now AlA?n hlq AlHyn dlwqt
When? mty?? Aymty?? mty?? Amty??
What? mAA? Ays?? ws?? Ayh?
I drink A?s?rb bs?rb As?rb bs?rb
He drinks ys?rb bs?rb ys?rb bys?rb
We drink ns?rb bns?rb ns?rb bns?rb
175
Computational Linguistics Volume 40, Number 1
Figure 2
Two roughly equivalent Arabic sentences, one in MSA and one in Levantine Arabic, translated
by the same MT system (Google Translate) into English. An acceptable translation would be
When will we see this group of criminals undergo trial (or tried)?. The MSA variant is handled well,
whereas the dialectal variant is mostly transliterated.
Figure 3
Two roughly equivalent Arabic sentences, one in MSA and one in Egyptian Arabic, translated
by the same MT system (Google Translate) into English. An acceptable translation would be
What is this that is happening? What is this that I?m seeing?. As in Figure 2, the dialectal variant
is handled quite poorly.
The abundance of MSA data has greatly aided research on computational meth-
ods applied to Arabic, but only the MSA variant of it. For example, a state-of-the-art
Arabic-to-English machine translation system performs quite well when translating
MSA source sentences, but often produces incomprehensible output when the input
is dialectal. For example, most words of the dialectal sentence shown in Figure 2 are
transliterated, whereas an equivalent MSA sentence is handled quite well. The high
transliteration rate is somewhat alarming, as the first two words of the dialectal sentence
are relatively frequent function words: Aymty? means ?when? and rH corresponds to the
modal ?will?.
Figure 3 shows another dialectal sentence, this time in Egyptian, which again causes
the system to produce a poor translation even for frequent words. Case in point, the
system is unable to consistently handle any of Ayh (?what?), Ally (the conjunction ?that?),
or dh (?this?). Granted, it is conceivable that processing dialectal content is more difficult
than MSA, but the main problem is the lack of dialectal training data.6
This is an important point to take into consideration, because the dialects differ to
a large enough extent to warrant treating them as more or less different languages. The
behavior of machine translation systems translating dialectal Arabic when the system
6 In the context of machine translation in particular, additional factors make translating dialectal content
difficult, such as a general mismatch between available training data and the topics that are usually
discussed dialectally.
176
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 4
The output of a Spanish-to-English system when given a Portuguese sentence as input,
compared with the output of a Portuguese-to-English system, which performs well.
The behavior is very similar to that in Figures 2 and 3, namely, the failure to translate
out-of-vocabulary words when there is a language mismatch.
has been trained exclusively on MSA data is similar to the behavior of a Spanish-
to-English MT system when a user inputs a Portuguese sentence. Figure 4 illustrates
how MT systems behave (the analogy is not intended to draw a parallel between the
linguistic differences MSA-dialect and Spanish-Portuguese). The MT system?s behavior
is similar to the Arabic example, in that words that are shared in common between
Spanish and Portuguese are translated, while the Portuguese words that were never
observed in the Spanish training data are left untranslated.
This example illustrates the need for dialectal data to train MT systems to handle
dialectal content properly. A similar scenario would arise with many other NLP tasks,
such as parsing or speech recognition, where dialectal content would be needed in large
quantities for adequate training. A robust dialect identifier could sift through immense
volumes of Arabic text, and separate out dialectal content from MSA content.
2.3 Harvesting Dialect Data from On-line Social Media
One domain of written communication in which MSA and dialectal Arabic are both
commonly used is the on-line domain, because it is more individual-driven and less
institutionalized than other venues. This makes a dialect much more likely to be the
user?s language of choice, and dialectal Arabic has a strong presence in blogs, fo-
rums, chatrooms, and user/reader commentary. Therefore, on-line data is a valuable
resource of dialectal Arabic text, and harvesting this data is a viable option for com-
putational linguists for purposes of creating large data sets to be used in statistical
learning.
We created the Arabic On-line Commentary Data Set (AOC) (Zaidan and Callison-
Burch 2011) a 52M-word monolingual data set by harvesting reader commentary from
the on-line versions of three Arabic newspapers (Table 2). The data is characterized
by the prevalence of dialectal Arabic, alongside MSA, mainly in Levantine, Gulf, and
Egyptian. These correspond to the countries in which the three newspapers are pub-
lished: Al-Ghad is from Jordan, Al-Riyadh is from Saudi Arabia, and Al-Youm Al-Sabe? is
from Egypt.7
Although a significant portion of the AOC?s content is dialectal, there is still a very
large portion of it that is in MSA. (Later analysis in Section 4.2.1 shows dialectal content
is roughly 40%.) In order to take full advantage of the AOC (and other Arabic data sets
7 URLs: www.alghad.com, www.alriyadh.com, and www.youm7.com.
177
Computational Linguistics Volume 40, Number 1
Table 2
A summary of the different components of the AOC data set. Overall, 1.4M comments were
harvested from 86.1K articles, corresponding to 52.1M words.
News Source Al-Ghad Al-Riyadh Al-Youm Al-Sabe? ALL
# articles 6.30K 34.2K 45.7K 86.1K
# comments 26.6K 805K 565K 1.4M
# sentences 63.3K 1,686K 1,384K 3.1M
# words 1.24M 18.8M 32.1M 52.1M
comments/article 4.23 23.56 12.37 16.21
sentences/comment 2.38 2.09 2.45 2.24
words/sentence 19.51 11.14 23.22 16.65
with at least some dialectal content), it is desirable to separate dialectal content from
non-dialectal content automatically. The task of dialect identification (and its automa-
tion) is the focus for the remainder of this article. We next present the task of Arabic
dialect identification, and discuss our effort to create a data set of Arabic sentences with
their dialectal labels. Our annotation effort relied on crowdsourcing the annotation task
to Arabic-speakers on Amazon?s Mechanical Turk service (Section 3).
3. Arabic Dialect Identification
The discussion of the varieties of Arabic and the differences between them gives rise
to the task of automatic dialect identification (DID). In its simplest form, the task is to
build a learner that can, given an Arabic sentence S, determine whether or not S contains
dialectal content. Another form of the task would be to determine in which dialect S was
written, which requires identification at a more fine-grained level.
In many ways, DID is equivalent to language identification. Although language
identification is often considered to be a ?solved problem,? DID is most similar to a
particularly difficult case of language ID, where it is applied to a group of closely related
languages that share a common character set. Given the parallels between DID and
language identification, we investigate standard statistical methods to establish how
difficult the task is. We discuss prior efforts for Arabic DID in Section 7.
3.1 The Difficulty of Arabic DID
Despite the differences illustrated in the previous section, in which we justify treating
the different dialects as separate languages, it is not a trivial matter to automatically
distinguish and separate the dialects from each other. Because all Arabic varieties
use the same character set, and because much of the vocabulary is shared among
different varieties, identifying dialect in a sentence is not simply a matter of, say,
compiling a dialectal dictionary and detecting whether or not a given sentence contains
dialectal words.
This word-level source ambiguity is caused by several factors:
 A dialectal sentence might consist entirely of words that are used across all
Arabic varieties, including MSA. Each of the sentences in Figure 5 consists
178
Zaidan and Callison-Burch Arabic Dialect Identification
of words that are used both in MSA and dialectally, and an MSA-based
dictionary would not (and should not) recognize those words as
out of vocabulary (OOV). Nevertheless, the sentences are heavily
dialectal.
 Some words are used across the varieties with different functions. For
example, Tyb is used dialectally as an interjection, but is an adjective
in MSA. (This is similar to the English usage of okay.)
 Primarily due to the omission of short vowels, a dialectal word might
have the same spelling as an MSA word with an entirely different
meaning, forming pairs of heteronyms. This includes strongly dialectal
words such as dwl and nby: dwl is either Egyptian for these (pronounced
dowl) or the MSA for countries (pronounced duwal); nby is either
the Gulf for we want (pronounced nibi) or the MSA for prophet
(pronounced nabi).
It might not be clear for a non-Arabic speaker what makes certain sentences, such
as those of Figure 5, dialectal, even when none of the individual words are. The answer
lies in the structure of such sentences and the particular word order within them, rather
than the individual words themselves taken in isolation. Figure 6 shows MSA sentences
that express the same meaning as the dialectal sentences from Figure 5. As one could
see, the two versions of any given sentence could share much of the vocabulary, but
in ways that are noticeably different to an Arabic speaker. Furthermore, the differences
would be starker still if the MSA sentences were composed from scratch, rather than
by modifying the dialectal sentences, since the tone might differ substantially when
composing sentences in MSA.
Figure 5
Three sentences that were identified by our annotators as dialectical, even thought they do
not contain individually dialectal words. A word-based OOV-detection approach would
fail to classify these sentences as being dialectal, because all these words could appear
in an MSA corpus. One might argue that a distinction should be drawn between informal
uses of MSA versus dialectical sentences, but annotators consistently classify these sentences
as dialect.
179
Computational Linguistics Volume 40, Number 1
Figure 6
The dialectal sentences of Figure 5, with MSA equivalents.
3.2 Applications of Dialect Identification
Being able to perform automatic DID is interesting from a purely linguistic and experi-
mental point of view. In addition, automatic DID has several useful applications:
 Distinguishing dialectal data from non-dialectal data would aid in creating
a large monolingual dialectal data set, exactly as we would hope to do
with the AOC data set. Such a data set would aid many NLP systems that
deal with dialectal content, for instance, to train a language model for
an Arabic dialect speech recognition system (Novotney, Schwartz, and
Khudanpur 2011). Identifying dialectal content can also aid in creating
parallel data sets for machine translation, with a dialectal source side.
 A user might be interested in content of a specific dialect, or, conversely,
in strictly non-dialectal content. This would be particularly relevant
in fine-tuning and personalizing search engine results, and could
allow for better user-targeted advertising. In the same vein, being
able to recognize dialectal content in user-generated text could aid in
characterizing communicants and their biographic attributes (Garera
and Yarowsky 2009).
180
Zaidan and Callison-Burch Arabic Dialect Identification
 In the context of an application such as machine translation (MT),
identifying dialectal content could be quite helpful. Most MT systems,
when faced with OOV words, either discard the words or make an effort
to transliterate them. If a segment is identified as being dialectal first, the
MT system might instead attempt to find equivalent MSA words, which
are presumably easier to process correctly (e.g., as in Salloum and Habash
[2011] and, to some degree, Habash [2008]). Even for non-OOV words,
identifying dialectal content before translating could be critical to resolve
the heteronym ambiguity of the kind mentioned in Section 3.1.
4. Crowdsourcing Arabic Dialect Annotation
In this section, we discuss crowdsourcing Arabic dialect annotation. We discuss how
we built a data set of Arabic sentences, each of which is labeled with whether or not
it contains dialectal content. The labels include additional details about the level of
dialectal content (i.e., how much dialect there is), and of which type of dialect it is. The
sentences themselves are sampled from the AOC data set, and we observe that about
40% of sentences contain dialectal content, with that percentage varying between 37%
and 48%, depending on the news source.
Collecting annotated data for speech and language applications requires careful
quality control (Callison-Burch and Dredze 2010). We present the annotation interface
and discuss an effective way for quality control that can detect spamming behavior. We
then examine the collected data itself, analyzing annotator behavior, measuring agree-
ment among annotators, and identifying interesting biases exhibited by the annotators.
In Section 5, we use the collected data to train and evaluate statistical models for several
dialect identification tasks.
4.1 Annotation Interface
The annotation interface displayed a group of Arabic sentences, randomly selected from
the AOC. For each sentence, the annotator was instructed to examine the sentence and
make two judgments about its dialectal content: the level of dialectal content, and its
type, if any. The instructions were kept short and simple:
This task is for Arabic speakers who understand the different local Arabic dialects,
and can distinguish them from Fusha8 Arabic.
Below, you will see several Arabic sentences. For each sentence:
1. Tell us how much dialect is in the sentence, and then
2. Tell us which Arabic dialect the writer intends.
The instructions were accompanied by the map of Figure 1, to visually illustrate
the dialect breakdown. Figure 7 shows the annotator interface populated with some
actual examples, with labeling in progress. We also collected self-reported information
such as native Arabic dialect and age (or number of years speaking Arabic for non-
native speakers). The interface also had built-in functionality to detect each annotator?s
geographic location based on their IP address.
8 Fusha is the Arabic word for MSA, pronounced foss-ha.
181
Computational Linguistics Volume 40, Number 1
Figure 7
The interface for the dialect identification task. This example, and the full interface, can be
viewed at the http://bit.ly/eUtiO3.
Of the 3.1M sentences in the AOC, we randomly9 selected a ?small? subset of about
110,000 sentences to be annotated for dialect.
For each sentence shown in the interface, we asked annotators to label which dialect
the segment is written in and the level of dialect in the segment. The dialect labels were
Egyptian, Gulf, Iraqi, Levantine, Maghrebi, other dialect, general dialect (for segments
that could be classified as multiple dialects), dialect but unfamiliar (for sentences that
are clearly dialect, but are written in a dialect that the annotator is not familiar with), no
dialect (for MSA), or not Arabic (for segments written in English or other languages).
Options for the level of dialect included no dialect (for MSA), a small amount of dialect,
an even mix of dialect and MSA, mostly dialect, and not Arabic. For this article we
use only the dialect labels, and not the level of dialect. Zaidan (2012) incorporates
finer-grained labels into an ?annotator rationales? model (Zaidan, Eisner, and Piatko
2007).
The sentences were randomly grouped into sets of 10 sentences each, and when
Workers performed our task, they were shown the 10 sentences of a randomly selected
set on a single HTML page. As a result, each screen contained a mix of sentences across
the three newspapers presented in random order. As control items, each screen had two
additional sentences that were randomly sampled from the article bodies. Such sentences
are almost always in MSA Arabic, and so their expected label is MSA. Any worker
who frequently mislabeled the control sentences with a non-MSA label was considered
a spammer, and their work was rejected. Hence, each screen had twelve sentences in
total.
9 There are far fewer sentences available from Al-Ghad commentary than the other two sources over any
given period of time (third line of Table 2). We have taken this imbalance into account and heavily
oversampled Al-Ghad sentences when choosing sentences to be labeled, to obtain a subset that is more
balanced across the three sources.
182
Zaidan and Callison-Burch Arabic Dialect Identification
We offered a reward of $0.05 per screen (later raised to $0.10), and had each set
redundantly completed by three distinct Workers. The data collection lasted about
4.5 months, during which 33,093 Human Intelligence Task (HIT) Assignments were
completed, corresponding to 330,930 collected labels (excluding control items). The
total cost of annotation was $3,050.52 ($2,773.20 for rewards, and $277.32 for Amazon?s
commission).
4.2 Annotator Behavior
With the aid of the embedded control segments (taken from article bodies) and expected
dialect label distribution, it was possible to spot spamming behavior and reject it. Table 3
shows three examples of workers whose work was rejected on this basis, having clearly
demonstrated they are unable or unwilling to perform the task faithfully. In total,
11.4% of the assignments were rejected on this basis. In the approved assignments,
the embedded MSA control sentence was annotated with the MSA label 94.4% of
the time. In the remainder of this article, we analyze only data from the approved
assignments.
We note here that we only rejected assignments where the annotator?s behavior
was clearly problematic, opting to approve assignments from workers mentioned later in
Section 4.2.3, who exhibit systematic biases in their labels. Although these annotators?
behavior is non-ideal, we cannot assume that they are not working faithfully, and
therefore rejecting their work might not be fully justified. Furthermore, such behavior
might be quite common, and it is worth investigating these biases to benefit future
research.
Table 3
Some statistics over the labels provided by three spammers. Compared with the typical worker
(right-most column), all workers perform terribly on the MSA control items, and also usually fail
to recognize dialectal content in commentary sentences. Other red flags, such as geographic
location and ?identifying? unrepresented dialects, are further proof of the spammy behavior.
A
29
V
7O
G
M
2C
62
05
A
3S
Z
L
M
2N
K
8N
U
O
G
A
8E
F1
I6
C
O
7T
C
U
Typical
MSA in control items 0% 14% 33% >90%
LEV in Al-Ghad 0% 0% 15% 25%
GLF in Al-Riyadh 8% 0% 14% 20%
EGY in Al-Youm Al-Sabe? 5% 0% 27% 33%
Other dialects 56% 0% 28% <1%
Incomplete answers 13% 6% 1% <2%
Worker location Romania Philippines Jordan Middle East
Claimed native dialect Gulf ?Other? Unanswered (Various)
183
Computational Linguistics Volume 40, Number 1
4.2.1 Label Distribution. Overall, 454 annotators participated in the task, 138 of whom
completed at least 10 HITs. Upon examination of the provided labels for the com-
mentary sentences, 40.7% of them indicate some level of dialect, and 57.1% indicate
no dialectal content (Figure 8a). Note that 2.14% of the labels identify a sentence as
being non-Arabic, non-textual, or as being left unanswered. The label breakdown is a
strong confirmation of our initial motivation, which is that a large portion of reader
commentary contains dialectal content.10
Figure 8 also illustrates the following:
 The most common dialectal label within a given news source matches
the dialect of the country of publication. This is not surprising, since the
readership for any newspaper is likely to mostly consist of the local
population of that country. Also, given the newspapers? countries of
publication, there is almost no content that is in a dialect other than
Levantine, Gulf, or Egyptian. For this reason, other dialects such as
Iraqi and Maghrebi, all combined, correspond to less than 0.01% of our
data, and we mostly drop them from further discussion.
 The three news sources vary in the prevalence of dialectal content. The
Egyptian newspaper has a markedly larger percentage of dialectal content
(46.6% of labels) compared with the Saudi newspaper (40.1%) and the
Jordanian newspaper (36.8%).
 A nontrivial amount of labels (5?8%) indicate General dialectal content.
The General label was meant to indicate a sentence that is dialectal but
lacks a strong indication of a particular dialect. Although many of the
provided General labels seem to reflect an intent to express this fact,
there is evidence that some annotators used this category in cases where
choosing the label Not sure would have been more appropriate but
was ignored (see Section 4.2.3).
 Non-Arabic content, although infrequent, is not a rare occurrence in the
Jordanian and Egyptian newspapers, at around 3%. The percentage is
much lower in the Saudi newspaper, at 0.8%. This might reflect the deeper
penetration of the English language (and English-only keyboards) in
Jordan and Egypt compared with Saudi Arabia.
We can associate a label with each segment based on the majority vote over the three
provided labels for that segment. If a sentence has at least two annotators choosing
a dialectal label, we label it as dialect. If it has at least two annotators choosing
the MSA label, we label it as MSA.11 In the remainder of the article, we will report
classification accuracy rates that assume the presence of gold-standard class labels.
Unless otherwise noted, this majority-vote label set is used as the gold-standard in such
experiments.
10 Later analysis in Section 4.2.3 shows that a non-trivial portion of the labels were provided by MSA-biased
annotators, indicating that dialectal content could be even more prevalent than what is initially suggested
by the MSA/dialect label breakdown.
11 A very small percentage of sentences (2%) do not have such agreement; upon inspection these are
typically found to be sentences that are in English, e-mail addresses, romanized Arabic, or simply
random symbols.
184
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 8
The distribution of labels provided by the workers for the dialect identification task, over all
three news sources (a) and over each individual news source (b?d). Al-Ghad is published in
Jordan, Al-Riyadh in Saudi Arabia, and Al-Youm Al-Sabe? in Egypt. Their local readerships are
reflected in the higher proportion of corresponding dialects. Note that this is not a breakdown
on the sentence level, and does not reflect any kind of majority voting. For example, most of
the LEV labels on sentences from the Saudi newspaper are trumped by GLF labels when taking
a majority vote, making the proportion of LEV-majority sentences smaller than what might be
deduced by looking at the label distribution in (c).
In experiments where the dialectal label set is more fine-grained (i.e., LEV, GLF, and
EGY instead of simply dialect), we assign to the dialectal sentence the label corre-
sponding to the news source?s country of publication. That is, dialectal sentences in the
Jordanian (respectively, Saudi, Egyptian) are given the label LEV (respectively, GLF, EGY).
We could have used dialect labels provided by the annotators, but chose to override
those using the likely dialect of the newspaper instead. It turns out that sentences with
an EGY majority, for instance, are extremely unlikely to appear in either the Jordanian or
Saudi newspaper?only around 1% of those sentences have an EGY majority. In the case
of the Saudi newspaper, 9% of all dialectal sentences were originally annotated as LEV
but were transformed to GLF. Our rationales for performing the transformation is that
no context was given for the sentences when they were annotated, and annotators had a
bias towards their own dialect. We provide the original annotations for other researchers
to re-analyze if they wish.
185
Computational Linguistics Volume 40, Number 1
Table 4
The specific-dialect label distribution (given that a dialect label was provided), shown for each
speaker group.
Group size % LEV % GLF % EGY % GNRL % Other dialects
All speakers 454 26.1 27.1 28.8 15.4 2.6
Levantine speakers 181 35.9 28.4 21.2 12.9 1.6
Gulf speakers 32 21.7 29.4 25.6 21.8 1.4
Egyptian speakers 121 25.9 19.1 38.0 10.9 6.1
Iraqi speakers 16 18.9 29.0 23.9 18.2 10.1
Maghrebi speakers 67 20.5 28.0 34.5 12.7 4.3
Other/Unknown 37 17.9 18.8 27.8 31.4 4.1
Even when a sentence would receive a majority-vote label that differs from
the news source?s primary dialect, inspection of such sentences reveals that the
classification was usually unjustified, and reflected a bias towards the annotator?s
native dialect. Case in point: Gulf-speaking annotators were in relatively short supply,
whereas a plurality of annotators spoke Levantine (see Table 4). Later in Section 4.2.3,
we point out that annotators have a native-dialect bias, whereby they are likely to
label a sentence with their native dialect even when the sentence has no evidence of
being written in that particular dialect. This explains why a non-trivial number of LEV
labels were given by annotators to sentences from the Saudi newspaper (Figure 8). In
reality, most of these labels were given by Levantine speakers over-identifying their
own dialect. Even if we were to assign dialect labels based on the (Levantine-biased)
majority votes, Levantine would only cover 3.6% of the sentences from the Saudi
newspaper.12
Therefore, for simplicity, we assume that a dialectal sentence is written in the
dialect corresponding to the sentence?s news source, without having to inspect the
specific dialect labels provided by the annotators. This not only serves to simplify our
experimental set-up, but also contributes to partially reversing the native dialect bias
that we observed.
4.2.2 Annotator Agreement and Performance. The annotators exhibit a decent level of
agreement with regard to whether a segment is dialectal or not, with full agreement (i.e.,
across all three annotators) on 72.2% of the segments regarding this binary dialect/MSA
decision. This corresponds to a kappa value of 0.619 (using the definition of Fleiss
[1971] for multi-rater scenarios), indicating very high agreement.13 The full-agreement
percentage decreases to 56.2% when expanding the classification from a binary decision
to a fine-grained scale that includes individual dialect labels as well. This is still quite
a reasonable result, since the criterion is somewhat strict: It does not include a segment
labeled, say, {Levantine, Levantine, General}, though there is good reason to consider
that annotators are in ?agreement? in such a case.
12 Note that the distributions in Figure 8 are on the label level, not on the sentence level.
13 Although it is difficult to determine the significance of a given kappa value, Landis and Koch (1977)
characterize kappa values above 0.6 to indicate ?substantial agreement? between annotators.
186
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 9
A bubble chart showing workers? MSA and dialect recall. Each data point (or bubble) in
the graph represents one annotator, with the bubble size corresponding to the number of
assignments completed by that annotator.
So how good are humans at the classification task? We examine their classifica-
tion accuracy, dialect recall, and MSA recall. The classification accuracy is measured
over all sentences, both MSA and dialectal. We define dialect (MSA) recall to be the
number of sentences labeled as being dialectal (MSA), over the total number of sen-
tences that have dialectal (MSA) labels based on the majority vote. Overall, human
annotators have a classification accuracy of 90.3%, with dialect recall at 89.0%, and
MSA recall at 91.5%. Those recall rates do vary across annotators, as shown in Fig-
ure 9, causing some accuracy rates to drop as low as 80% or 75%. Of the annota-
tors performing at least five HITs, 89.4% have accuracy rates greater than or equal
to 80%.
Most annotators have both high MSA recall and high dialect recall, with about 70%
of them achieving at least 80% in both MSA and dialect recall. Combined with the
general agreement rate measure, this is indicative that the task is well-defined?it is
unlikely that many people would agree on something that is incorrect.
We note here that the accuracy rate (90.3%) is a slight overestimate of the human
annotators? accuracy rate, by virtue of the construction of the gold labels. Because the
correct labels are based on a majority vote of the annotators? labels themselves, the two
sets are not independent, and an annotator is inherently likely to be correct. A more
informative accuracy rate disregards the case where only two of the three annotators
agreed and the annotator whose accuracy was being evaluated contributed one of those
two votes. In other words, an annotator?s label would be judged against a majority vote
that is independent of that annotator?s label. Under this evaluation set-up, the human
accuracy rate slightly decreases, to 88.0%.
4.2.3 Annotator Bias Types. Examining the submitted labels of individual workers reveals
interesting annotation patterns, and indicates that annotators are quite diverse in their
187
Computational Linguistics Volume 40, Number 1
Table 5
Two annotators with a General label bias, one who uses the label liberally, and one who is more
conservative. Note that in both cases, there is a noticeably smaller percentage of General labels
in the Egyptian newspaper than in the Jordanian and Saudi newspapers.
A
ll
w
or
ke
rs
A
1M
50
U
V
37
A
M
B
Z
3
A
2Z
N
K
1P
Z
O
V
IE
C
D
% General 6.3 12.0 2.3
% General in Al-Ghad 5.2 14.2 3.1
% General in Al-Riyadh 7.7 13.1 2.6
% General in Al-Youm Al-Sabe? 4.9 7.6 1.0
Native dialect (Various) Maghrebi Egyptian
behavior. An annotator can be observed to have one or more of the following bias
types:14
 MSA bias/dialect bias: Figure 9 shows that annotators vary in how willing
they are to label a sentence as being dialectal. Whereas most workers (top
right) exhibit both high MSA and high dialect recall, other annotators have
either a MSA bias (top left) or a dialect bias (bottom right).
 Dialect-specific bias: Many annotators over-identify a particular dialect,
usually their native one. If we group the annotators by their native dialect
and examine their label breakdown (Table 4), we find that Levantine
speakers over-identify sentences as being Levantine, Gulf speakers
over-identify Gulf, and Egyptian speakers over-identify Egyptian. This
holds for speakers of other dialects as well, as they over-identify other
dialects more often than most speakers. Another telling observation
is that Iraqi speakers have a bias for the Gulf dialect, which is quite
similar to Iraqi. Maghrebi speakers have a bias for Egyptian, reflecting
their unfamiliarity with the geographically distant Levantine and
Gulf dialects.
 The General bias: The General label is meant to signify sentences that
cannot be definitively classified as one dialect over another. This is the
case when enough evidence exists that the sentence is not in MSA, but
contains no evidence for a specific dialect. In practice, some annotators
make very little use of this label, even though many sentences warrant
its use, whereas other annotators make extensive use of this label (see,
for example, Table 5). One interesting case is that of annotators whose
General label seems to mean they are unable to identify the dialect,
14 These biases should be differentiated from spammy behavior, which we already can deal with quite
effectively, as explained in Section 4.2.
188
Zaidan and Callison-Burch Arabic Dialect Identification
and a label like Not sure might have been more appropriate. Take the
case of the Maghrebi worker in Table 5, whose General bias is much
more pronounced in the Jordanian and Saudi newspapers. This is
an indication she might have been having difficulty distinguishing
Levantine and Gulf from each other, but that she is familiar with the
Egyptian dialect.
5. Automatic Dialect Identification
From a computational point of view, we can think of dialect identification as language
identification, though with finer-grained distinctions that make it more difficult than
typical language ID. Even languages that share a common character set can be distin-
guished from each other at high accuracy rates using methods as simple as examining
character histograms (Cavnar and Trenkle 1994; Dunning 1994; Souter et al. 1994), and,
as a largely solved problem, the one challenge becomes whether languages can be
identified for very short segments.
Due to the nature and characteristics and high overlap across Arabic dialects, rely-
ing on character histograms alone is ineffective (see Section 5.3.1), and more context
is needed. We will explore higher-order letter models as well as word models, and
determine what factors determine which model is best.
5.1 Smoothed n-Gram Models
Given a sentence S to classify into one of k classes C1, C2, . . . , Ck, we will choose the class
with the maximum conditional probability:
C? = argmax
Ci
P(Ci|S) = argmax
Ci
P(S|Ci) ? P(Ci) (1)
Note that the decision process takes into account the prior distribution of the
classes, which is estimated from the training set. The training set is also used to train
probabilistic models to estimate the probability of S given a particular class. We rely
on training n-gram language models to compute such probabilities, and apply Kneser-
Ney smoothing to these probabilities and also use that technique to assign probability
mass to unseen or OOV items (Chen and Goodman 1998). In language model scoring, a
sentence is typically split into words. We will also consider letter-based models, where
the sentence is split into sequences of characters. Note that letter-based models would
be able to take advantage of clues in the sentence that are not complete words, such as
prefixes or suffixes. This would be useful if the amount of training data is very small,
or if we expect a large domain shift between training and testing, in which case content
words indicative of MSA or dialect might not still be valuable in the new domain.
Although our classification method is based only on language model scoring, and
is thus relatively simple, it is nevertheless very effective. Experimental results in Sec-
tion 5.3 (e.g., Figure 10) indicate that this method yields accuracy rates above 85%,
only slightly behind the human accuracy rate of 88.0% reported in Section 4.2.2.
5.2 Baselines
To properly evaluate classification performance trained on dialectal data, we compare
the language-model classifiers to two baselines that do not use the newly collected data.
189
Computational Linguistics Volume 40, Number 1
Figure 10
Learning curves for the general MSA vs. dialect task, with all three news sources pooled
together. Learning curves for the individual news sources can be found in Figure 11.
The 83% line has no significance, and is provided to ease comparison with Figure 11.
Rather, they use available MSA-only data and attempt to determine how MSA-like a
sentence is.
The first baseline is based on the assumption that a dialectal sentence would contain
a higher percentage of ?non-MSA? words that cannot be found in a large MSA corpus.
To this end, we extracted a vocabulary list from the Arabic Gigaword Corpus, producing
a list of 2.9M word types. Each sentence is given a score that equals the OOV percentage,
and if this percentage exceeds a certain threshold, the sentence is classified as being
dialectal. For each of the cross validation runs in Section 5.3.1, we use the threshold
that yields the optimal accuracy rate on the test set (hence giving this baseline as
much a boost as possible). In our experiments, we found this threshold to be usually
around 10%.
The second approach uses a more fine-grained approach. We train a language model
using MSA-only data, and use it to score a test sentence. Again, if the perplexity exceeds
a certain threshold, the sentence is classified as being dialectal. To take advantage of
domain knowledge, we train this MSA model on the sentences extracted from the article
bodies of the AOC, which corresponds to 43M words of highly relevant content.
5.3 Experimental Results
In this section, we explore using the collected labels to train word- and letter-based
DID systems, and show that they outperform other baselines that do not utilize the
annotated data.
5.3.1 Two-Way, MSA vs. Dialect Classification. We measure classification accuracy at vari-
ous training set sizes, using 10-fold cross validation, for several classification tasks. We
examine the task both as a general MSA vs. dialect task, as well as when restricted
within a particular news source. We train unigram, bigram, and trigram (word-based)
models, as well as unigraph, trigraph, and 5-graph (letter-based) models. Table 6 sum-
marizes the accuracy rates for these models, and includes rates for the baselines that do
not utilize the dialect-annotated data.
Generally, we find that a unigram word model performs best, with a 5-graph model
slightly behind (Figure 11). Bigram and trigram word models seem to suffer from the
sparseness of the data and lag behind, given the large number of parameters they
190
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 11
Learning curves for the MSA vs. dialect task, for each of the three news sources. The 83% line
has no significance, and is provided to ease comparison across the three components, and
with Figure 10.
would need to estimate (and instead resort to smoothing heavily). The letter-based
models, with a significantly smaller vocabulary size, do not suffer from this problem,
and perform well. This is a double-edged sword though, especially for the trigraph
model, as it means the model is less expressive and converges faster.
Overall though, the experiments show a clear superiority of a supervised method,
be it word- or letter-based, over baselines that use existing MSA-only data. Whichever
model we choose (with the exception of the unigraph model), the obtained accuracy
rates show a significant dominance over the baselines.
It is worth noting that a classification error becomes less likely to occur as the length
of the sentence increases (Figure 12). This is not surprising given prior work on the
language identification problem (R?ehu?r?ek and Kolkus 2009; Verma, Lee, and Zakos
2009), which points out that the only ?interesting? aspect of the problem is performance
on short segments. The same is true in the case of dialect identification: a short sentence
191
Computational Linguistics Volume 40, Number 1
Table 6
Accuracy rates (%) on several two-way classification tasks (MSA vs. dialect) for various models.
Models in the top part of the table do not utilize the dialect-annotated data, whereas models in
the bottom part do. (For the latter kind of models, the accuracy rates reported are based on a
training set size of 90% of the available data.)
Model MS
A
vs
.d
ia
le
ct
A
l-
G
ha
d
MS
A
vs
.d
ia
le
ct
(L
ev
an
ti
ne
)
A
l-
R
iy
ad
h
MS
A
vs
.d
ia
le
ct
(G
u
lf
)
A
l-
Yo
um
A
l-
Sa
be
?M
SA
vs
.d
ia
le
ct
(E
gy
p
ti
an
)
Majority Class 58.8 62.5 60.0 51.9
OOV % vs. Gigaword 65.5 65.1 65.3 66.7
MSA LM-scoring 66.6 67.8 66.8 65.2
Letter-based, 1-graph 68.1 69.9 68.0 70.4
Letter-based, 3-graph 83.5 85.1 81.9 86.0
Letter-based, 5-graph 85.0 85.7 81.4 87.0
Word-based, 1-gram 85.7 87.2 83.3 87.9
Word-based, 2-gram 82.8 84.1 80.6 85.9
Word-based, 3-gram 82.5 83.7 80.4 85.6
that contains even a single misleading feature is prone to misclassification, whereas a
long sentence is likely to have other features that help identify the correct class label.15
One could also observe that distinguishing MSA from dialect is a more difficult
task in the Saudi newspaper than in the Jordanian paper, which in turn is harder than
in the Egyptian newspaper. This might be considered evidence that the Gulf dialect
is the closest of the dialects to MSA, and Egyptian is the farthest, in agreement with
the conventional wisdom. Note also that this is not due to the fact that the Saudi
sentences tend to be significantly shorter?the ease of distinguishing Egyptian holds
even at higher sentence lengths, as shown by Figure 12.
5.3.2 Multi-Way, Fine-Grained Classification. The experiments reported earlier focused
on distinguishing MSA from dialect when the news source is known, making it
straightforward to determine which of the Arabic dialects a sentence is written in (once
15 The accuracy curve for the Egyptian newspaper has an outlier for sentence lengths 10?12. Upon
inspection, we found that over 10% of the sentences in that particular length subset were actually
repetitions of a single 12-word sentence. (A disgruntled reader, angry about perceived referee corruption,
essentially bombarded the reader commentary section of several articles with that single sentence.)
This created an artificial overlap between the training and test sets, hence increasing the accuracy
rate beyond what would be reasonably expected due to increased sentence length alone.
192
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 12
Accuracy rates vs. sentence length in the general MSA vs. dialect task. Accuracy rates shown
are for the unigram word model trained on 90% of the data.
the sentence is determined to be dialectal). If the news source is not known, we do not
have the luxury of such a strong prior on the specific Arabic dialect. It is therefore
important to evaluate our approach in a multi-way classifiation scenario, where the
class set is expanded from {MSA, dialect} to {MSA, LEV, GLF, EGY}.
Under this classification set-up, the classification accuracy decreases from 85.7% to
81.0%.16 The drop in performance is not at all surprising, since four-way classification is
inherently more difficult than two-way classification. (Note that the classifier is trained
on exactly the same training data in both scenarios, but with more fine-grained dialectal
labels in the four-way set-up.)
Table 7 is the classifier?s confusion matrix for this four-way set-up, illustrating
when the classifier tends to make mistakes. We note here that most classification errors
on dialectal sentences occur when these sentences are mislabeled as being MSA, not
when they are misidentified as being in some other incorrect dialect. In other words,
dialect?dialect confusion constitutes a smaller proportion of errors than dialect?MSA
confusion. Indeed, if we consider a three-way classification setup on dialectal sentences
alone (LEV vs. GLF vs. EGY), the classifier?s accuracy rate shoots up to 88.4%. This is a
higher accuracy rate than for the general two-way MSA vs. dialect classification (85.7%),
despite involving more classes (three instead of two), and being trained on less data
(0.77M words instead of 1.78M words). This indicates that the dialects deviate from
MSA in various ways, and therefore distinguishing dialects from each other can be done
even more effectively than distinguishing dialect from MSA.
5.3.3 Word and Letter Dialectness. Examining the letter and word distribution in the
corpus provides valuable insight into what features of a sentence are most dialectal.
Let DF(w) denote the dialectness factor of a word w, defined as:
DF(w) def=
f (w|D)
f (w|MSA) =
countD(w)/countD(.)
countMSA(w)/countMSA(.)
(2)
16 For clarity, we report accuracy rates only for the unigram classifier. The patterns from Section 5.3.1 mostly
hold here as well, in terms of how the different n-gram models perform relative to each other.
193
Computational Linguistics Volume 40, Number 1
Table 7
Confusion matrix in the four-way classification setup. Rows correspond to actual labels, and
columns correspond to predicted labels. For instance, 6.7% of MSA sentences were given a GLF
label (first row, third column). Note that entries within a single row sum to 100%.
Class label MSA LEV GLF EGY
MSA Sentences 86.5% 4.2% 6.7% 2.6%
LEV Sentences 20.6% 69.1% 8.6% 1.8%
GLF Sentences 24.2% 2.4% 72.0% 1.4%
EGY Sentences 14.4% 2.2% 4.6% 78.8%
where countD(w) (respectively, countMSA(w)) is the number of times w appeared in the
dialectal (respectively, MSA) sentences, and countD(.) is the total number of words in
those sentences. Hence, DF(w) is simply a ratio measuring how much more likely w is to
appear in a dialectal sentence than in an MSA sentence. Note that the dialectness factor
can be easily computed for letters as well, and can be computed for bigrams/bigraphs,
trigrams/trigraphs, and so forth.
Figure 13 lists, for each news source, the word types with the highest and lowest
dialectness factor. The most dialectal words tend to be function words, and they also
tend to be strong indicators of dialect, judging by their very high DF. On the other hand,
the MSA word group contains several content words, relating mainly to politics and
religion.
One must also take into account the actual frequency of a word, as DF only captures
relative frequencies of dialect/MSA, but does not capture how often the word occurs in
the first place. Figure 14 plots both measures for the words of Al-Ghad newspaper. The
Figure 13
Words with the highest and lowest dialectness factor values in each of the three news sources.
194
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 14
A plot of the most common words in the Al-Ghad sentences, showing each word?s DF and
corpus frequency. The right- and left-most words here also appear in Figure 13. Not every
word from that list appears here though, since some words have counts below 100. For clarity,
not all points display the words they represent.
plot illustrates which words are most important to the classifier: the words that are
farthest away from the point of origin, along both dimensions.
As for letter-based features, many of the longer ones (e.g., 5-graph features) are
essentially the same words important to the unigram word model. The letter-based
models are, however, able to capture some linguistic phenomenon that the word model
is unable to: the suffixes +s? (not in Levantine) and +wn (plural conjugation in Gulf),
and the prefixes H+ (will in Egyptian), bt+ (present tense conjugation in Levantine and
Egyptian), and y+ (present tense conjugation in Gulf).
Figure 15 sheds some light on why even the unigraph model outperforms the
baselines. It picks up on subtle properties of the MSA writing style that are lacking
Figure 15
A plot of the most common letters in the Al-Ghad sentences, showing each letter?s DF and
corpus frequency.
195
Computational Linguistics Volume 40, Number 1
when using dialect. Namely, there is closer attention to following hamza rules (distin-
guishing A, A?, and A? from each other, rather than mapping them all to A), and better
adherence to (properly) using + instead of +h at the end of many words. There is also
a higher tendency to use words containing the letters that are most susceptible to being
transformed when pronounced dialectally: ? (usually pronounced as z), D? (pronounced
as D), and ? (pronounced as t).
On the topic of spelling variation, one might wonder if nomalizing the Arabic
text before training language models might enhance coverage and therefore improve
performance. For instance, would it help to map all forms of the alef hamza to a single
letter, and all instances of  to h, and so on? Our pilot experiments indicated that such
normalization tends to slightly but consistently hurt performance, so we opted to leave
the Arabic text as is. The only type of preprocessing we performed was more on the
?cleanup? side of things rather than computationally motivated normalization, such as
proper conversion of HTML entities (e.g., &quot; to ") and mapping Eastern Arabic
numerals to their European equivalents.
6. Applying DID to a Large-Scale Arabic Web Crawl
We conducted a large-scale Web crawl to gather Arabic text from the on-line versions of
newspapers from various Arabic-speaking countries. The first batch contained 319 on-
line Arabic-language newspapers published in 24 countries. This list was compiled from
http://newspapermap.com/ and http://www.onlinenewspapers.com/, which are Web
sites that show the location and language of newspapers published around the world.
The list contained 55 newspapers from Lebanon, 42 from Egypt, 40 from Saudi Arabia,
26 from Yemen, 26 from Iraq, 18 from Kuwait, 17 from Morocco, 15 from Algeria, 12
from Jordan, and 10 from Syria. The data were gathered from July?Sept 2011.
We mirrored the 319 Web sites using wget, resulting in 20 million individual files
and directories. We identified 3,485,241 files that were likely to contain text by selecting
the extensions htm, html, cmff, asp, pdf, rtf, doc, and docx. We converted these files
to text using xpdf?s pdftotext for PDFs and Apple?s textutil for HTML and Doc files.
When concatenated together, the text files contained 438,940,861 lines (3,452,404,197
words). We performed de-duplication to remove identical lines, after which 18,219,348
lines (1,393,010,506 words) remained.
We used the dialect-annotated data to train a language model for each of the four
Arabic varieties (MSA, LEV, GLF, EGY), as described in the previous section. We used these
models to classify the crawled data, assigning a given sentence the label corresponding
to the language model under which that sentence received the highest score. Table 8
gives the resulting label breakdown. We see that the overwhelming majority of the
sentences are classified as MSA, which comes as no surprise, given the prevalence of
MSA in the newspaper genre. Figure 16 shows some sentences that were given non-
MSA labels by our classifier.
7. Related Work
Habash et al. (2008) presented annotation guidelines for the identification of dialec-
tal content in Arabic content, paying particular attention to cases of code switching.
They present pilot annotation results on a small set of around 1,600 Arabic sentences
(19k words), with both sentence- and word-level dialectness annotations.
196
Zaidan and Callison-Burch Arabic Dialect Identification
Table 8
Predicted label breakdown for the crawled data, over the four varieties of Arabic. All varieties
were given equal priors.
Variety Sentence Count Percentage
MSA 13,102,427 71.9%
LEV 3,636,525 20.0%
GLF 630,726 3.5%
EGY 849,670 4.7%
ALL 18,219,348 100.0%
The Cross Lingual Arabic Blog Alerts (COLABA) project (Diab et al. 2010) is another
large-scale effort to create dialectal Arabic resources (and tools). They too focus on on-
line sources such as blogs and forums, and use information retrieval tasks to measure
their ability to properly process dialectal Arabic content. The COLABA project demon-
strates the importance of using dialectal content when training and designing tools that
deal with dialectal Arabic, and deal quite extensively with resource creation and data
harvesting for dialectal Arabic.
Figure 16
Example sentences from the crawled data set that were predicted to be dialectal, two in each of
the three Arabic dialects.
197
Computational Linguistics Volume 40, Number 1
Chiang et al. (2006) investigate building a parser for Levantine Arabic, without
using any significant amount of dialectal data. They utilize an available Levantine?MSA
lexicon, but no parses of Levantine sentences. Their work illustrates the difficulty of
adapting MSA resources for use in a dialectal domain.
Zbib et al. (2012) show that incorporating dialect training data into a statistical
machine translation system vastly improves the quality of the translation of dialect
sentences when compared to a system trained solely on an MSA-English parallel cor-
pus. When translating Egyptian and Levantine test sets, a dialect Arabic MT system
outperforms a Modern Standard Arabic MT system trained on a 150 million word
Arabic?English parallel corpus?over 100 times the amount of data as their dialect
parallel corpus.
As far as we can tell, no prior dialect identification work exists that is applied to Ara-
bic text. However, Lei and Hansen (2011) and Biadsy, Hirschberg, and Habash (2009) in-
vestigate Arabic dialect identification in the speech domain. Lei and Hansen (2011) build
Gaussian mixture models to identify the same three dialects we consider, and are able
to achieve an accuracy rate of 71.7% using about 10 hours of speech data for training.
Biadsy, Hirschberg, and Habash (2009) utilize a much larger data set (170 hours of
speech data) and take a phone recognition and language modeling approach (Zissman
1996). In a four-way classification task (with Iraqi as a fourth dialect), they achieve a
78.5% accuracy rate. It must be noted that both works use speech data, and that dialect
identification is done on the speaker level, not the sentence level as we do.
8. Conclusion
Social media, like reader commentary on on-line newspapers, is a rich source of dialectal
Arabic that has previously not been studied in detail. We have harvested this type of
resource to create a large data set of informal Arabic that is rich in dialectal content. We
selected a large subset of this data set, and had the sentences in it manually annotated
for dialect. We used the collected labels to train and evaluate automatic classifiers for
dialect identification, and observed interesting linguistic aspects about the task and
annotators? behavior. Using an approach based on language model scoring, we develop
classifiers that significantly outperform baselines that use large amounts of MSA data,
and we approach the accuracy rates exhibited by human annotators.
In addition to n-gram features, one could imagine benefiting from morphological
features of the Arabic text, by incorporating analyses given by automatic analyzers such
as BAMA (Buckwalter 2004), MAGEAD (Habash and Rambow 2006), ADAM (Salloum
and Habash 2011), or CALIMA (Habash, Eskander, and Hawwari 2012). Although the
difference between our presented approach and human annotators was found to be
relatively small, incorporating additional linguistically motivated features might be
pivotal in bridging that final gap.
In future annotation efforts, we hope to solicit more detailed labels about dialectal
content, such as specific annotation for why a certain sentence is dialectal and not MSA:
Is it due to structural differences, dialectal terms, and so forth? We also hope to expand
beyond the three dialects discussed in this article, by including sources from a larger
number of countries.
Given the recent political unrest in the Middle East (2011), another rich source of
dialectal Arabic are Twitter posts (e.g., with the #Egypt tag) and discussions on various
political Facebook groups. Here again, given the topic at hand and the individualistic
nature of the posts, they are very likely to contain a high degree of dialectal data.
198
Zaidan and Callison-Burch Arabic Dialect Identification
Appendix A
The Arabic transliteration scheme used in the article is the Habash-Soudi-Buckwalter
transliteration (HSBT) mapping (Habash, Soudi, and Buckwalter 2007), which extends
the scheme designed by Buckwalter in the 1990s (Buckwalter 2002). Buckwalter?s origi-
nal scheme represents Arabic orthography by designating a single, distinct ASCII char-
acter for each Arabic letter. HSBT uses some non-ASCII characters for better readibility,
but maintains the distinct 1-to-1 mapping.
Figure 17 lists the character mapping used in HSBT. We divide the list into four
sections: vowels, forms of the hamzah (glottal stop), consonants, and pharyngealized
Figure 17
The character mapping used in the HBST scheme. Most mappings are straightforward; a
few non-obvious mappings are highlighted with an arrow (?) next to them. For brevity, the
mappings for short vowels and other diacritics are omitted. Note that we take the view that
? is a pharyngealized glottal stop, which is supported by Gairdner (1925), Al-Ani (1970),
Ka?stner (1981), Thelwall and Sa?Adeddin (1990), and Newman (2002). For completeness,
we indicate its IPA name as well.
199
Computational Linguistics Volume 40, Number 1
consonants. Pharyngealized consonants are ?thickened? versions of other, more familiar
consonants, voiced such that the pharynx or epiglottis is constricted during the articula-
tion of the sound. Those consonants are present in very few languages and are therefore
likely to be unfamiliar to most readers, which is why we place them in a separate
section?there is no real distinction in Arabic between them and other consonants.
HSBT also allows for the expression of short vowels and other Arabic diacritics, but
because those diacritics are only rarely expressed in written (and typed) form, we omit
them for brevity.
Acknowledgments
This research was supported in part by
the DARPA GALE program under contract
no. HR0011-06-2-0001, the DARPA BOLT
program contract no. HR0011-12-C-0014,
the EuroMatrixPlus project funded by the
European Commission (7th Framework
Programme), the Human Language
Technology Center of Excellence, and by
gifts from Google and Microsoft. The views
and findings are the authors? alone. They
do not reflect the official policies or positions
of the Department of Defense or the
U.S. Government. The authors would like
to thank the anonymous reviewers for their
extremely valuable comments on earlier
drafts of this article, and for suggesting
future work ideas.
References
Abdel-Massih, Ernest T., Zaki N.
Abdel-Malek, and El-Said M. Badawi.
1979. A Reference Grammar of Egyptian
Arabic. Georgetown University Press.
Al-Ani, Salman H. 1970. Arabic Phonology:
An Acoustical and Physiological Investigation.
Mouton.
Aoun, Joseph, Elabbas Benmamoun, and
Dominique Sportiche. 1994. Agreement,
word order, and conjunction in some
varieties of Arabic. Linguistic Inquiry,
25(2):195?220.
Badawi, El-Said and Martin Hinds. 1986.
A Dictionary of Egyptian Arabic. Librairie
du Liban.
Bassiouney, Reem. 2009. Arabic
Sociolinguistics. Edinburgh
University Press.
Biadsy, Fadi, Julia Hirschberg, and
Nizar Habash. 2009. Spoken Arabic dialect
identification using phonotactic modeling.
In Proceedings of the EACL Workshop on
Computational Approaches to Semitic
Languages, pages 53?61, Athens.
Buckwalter, Tim. 2002. Buckwalter Arabic
transliteration. http://www.qamus.org/
transliteration.htm.
Buckwalter, Tim. 2004. Buckwalter Arabic
morphological analyzer version 2.0.
Linguistic Data Consortium,
Philadelphia, PA.
Callison-Burch, Chris and Mark Dredze.
2010. Creating speech and language
data with Amazon?s Mechanical Turk.
In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk,
pages 1?12, Los Angeles, CA.
Cavnar, William B. and John M. Trenkle.
1994. N-gram-based text categorization.
In Proceedings of SDAIR-94, pages 161?175,
Vilnius.
Chen, Stanley F. and Joshua T. Goodman.
1998. An empirical study of smoothing
techniques for language modeling.
Technical Report TR-10-98, Computer
Science Group, Harvard University.
Chiang, David, Mona Diab, Nizar Habash,
Owen Rambow, and Safiullah Shareef.
2006. Parsing Arabic dialects.
In Proceedings of EACL, pages 369?376,
Trento.
Cowell, Mark W. 1964. A Reference
Grammar of Syrian Arabic. Georgetown
University Press.
Diab, Mona, Nizar Habash, Owen Rambow,
Mohamed Altantawy, and Yassine
Benajiba. 2010. COLABA: Arabic dialect
annotation and processing. In Proceedings
of the LREC Workshop on Semitic Language
Processing, pages 66?74.
Dunning, T. 1994. Statistical identification of
language. Technical Report MCCS 94-273,
New Mexico State University.
Erwin, Wallace. 1963. A Short Reference
Grammar of Iraqi Arabic. Georgetown
University Press.
Fleiss, Joseph L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Gairdner, William Henry Temple. 1925.
The Phonetics of Arabic. Oxford
University Press.
Garera, Nikesh and David Yarowsky. 2009.
Modeling latent biographic attributes in
200
Zaidan and Callison-Burch Arabic Dialect Identification
conversational genres. In Proceedings of
ACL, pages 710?718, Singapore.
Habash, Nizar. 2008. Four techniques for
online handling of out-of-vocabulary
words in Arabic-English statistical
machine translation. In Proceedings
of ACL, Short Papers, pages 57?60,
Columbus, OH.
Habash, Nizar, Mona Diab, and Owen
Rambow. 2012. Conventional orthography
for dialectal Arabic. In Proceedings of
the Language Resources and Evaluation
Conference (LREC), pages 711?718,
Istanbul.
Habash, Nizar, Ramy Eskander, and Abdelati
Hawwari. 2012. A morphological analyzer
for Egyptian Arabic. In Proceedings of the
Twelfth Meeting of the Special Interest Group
on Computational Morphology and Phonology,
pages 1?9, Montre?al.
Habash, Nizar and Owen Rambow. 2006.
MAGEAD: A morphological analyzer
and generator for the Arabic dialects.
In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 681?688,
Sydney.
Habash, Nizar, Owen Rambow, Mona Diab,
and Reem Kanjawi-Faraj. 2008. Guidelines
for annotation of Arabic dialectness.
In Proceedings of the LREC Workshop on
HLT & NLP within the Arabic World,
pages 49?53, Marrakech.
Habash, Nizar, Abdelhadi Soudi, and
Tim Buckwalter. 2007. On Arabic
transliteration. In Antal van den Bosch,
Abdelhadi Soudi, and Gu?nter Neumann,
editors, Arabic Computational Morphology:
Knowledge-based and Empirical Methods.
Kluwer/Springer Publications, chapter 2.
Habash, Nizar Y. 2010. Introduction to
Arabic Natural Language Processing.
Morgan & Claypool.
Haeri, Niloofar. 2003. Sacred Language,
Ordinary People: Dilemmas of Culture and
Politics in Egypt. Palgrave Macmillan.
Holes, Clive. 2004. Modern Arabic: Structures,
Functions, and Varieties. Georgetown
Classics in Arabic Language and
Linguistics. Georgetown University Press.
Ingham, Bruce. 1994. Najdi Arabic: Central
Arabian. John Benjamins.
Ka?stner, Hartmut. 1981. Phonetik und
Phonologie des modernen Hocharabisch.
Verlag Enzyklopa?die.
Landis, J. Richard and Gary G. Koch. 1977.
The measurement of observer agreement
for categorical data. Biometrics, 33:159?174.
Lei, Yun and John H. L. Hansen. 2011. Dialect
classification via text-independent training
and testing for Arabic, Spanish, and
Chinese. IEEE Transactions on Audio, Speech,
and Language Processing, 19(1):85?96.
Mitchell, Terence Frederick. 1990.
Pronouncing Arabic. Clarendon Press.
Mohand, Tilmatine. 1999. Substrat et
convergences: Le berbe?re et l?arabe
nord-africain. Estudios de Dialectologia?
Norteaafricana y andalus??, 4:99?119.
Newman, Daniel L. 2002. The phonetic status
of Arabic within the world?s languages.
Antwerp Papers in Linguistics, 100:63?75.
Novotney, Scott, Rich Schwartz, and Sanjeev
Khudanpur. 2011. Unsupervised Arabic
dialect adaptation with self-training.
In Interspeech, pages 541?544, Florence.
R?ehu?r?ek, Radim and Milan Kolkus. 2009.
Language Identification on the Web:
Extending the Dictionary Method,
volume 5449 of Lecture Notes in Computer
Science, pages 357?368. SpringerLink.
Salloum, Wael and Nizar Habash. 2011.
Dialectal to standard Arabic paraphrasing
to improve Arabic-English statistical
machine translation. In Proceedings
of the EMNLP Workshop on Algorithms
and Resources for Modelling of Dialects
and Language Varieties, pages 10?21,
Edinburgh.
Shlonsky, Ur. 1997. Clause Structure and
Word Order in Hebrew and Arabic:
An Essay in Comparative Semitic Syntax.
Oxford University Press.
Souter, Clive, Gavin Churcher, Judith Hayes,
John Hughes, and Stephen Johnson. 1994.
Natural language identification using
corpus-based models. Hermes Journal of
Linguistics, 13:183?203.
Suleiman, Yasir. 1994. Nationalism and
the Arabic language: A historical
overview. In Yasir Suleiman, editor,
Arabic Sociolinguistics. Curzon Press.
Thelwall, Robin and M. Akram Sa?Adeddin.
1990. Arabic. Journal of the International
Phonetic Association, 20(2):37?39.
Verma, Brijesh, Hong Lee, and John Zakos.
2009. An Automatic Intelligent Language
Classifier, volume 5507 of Lecture Notes
in Computer Science, pages 639?646.
SpringerLink.
Versteegh, Kees. 2001. The Arabic Language.
Edinburgh University Press.
Zaidan, Omar, Jason Eisner, and Christine
Piatko. 2007. Using ?annotator rationales?
to improve machine learning for text
categorization. In Human Language
Technologies 2007: The Conference of the
201
Computational Linguistics Volume 40, Number 1
North American Chapter of the Association
for Computational Linguistics; Proceedings
of the Main Conference, pages 260?267,
Rochester, NY.
Zaidan, Omar F. 2012. Crowdsourcing
Annotation for Machine Learning in Natural
Language Processing Tasks. Ph.D. thesis,
Johns Hopkins University, Baltimore, MD.
Zaidan, Omar F. and Chris Callison-Burch. 2011.
The Arabic Online Commentary Dataset:
An annotated dataset of informal Arabic
with high dialectal content. In Proceedings
of ACL, pages 37?41, Portland, OR.
Zbib, Rabih, Erika Malchiodi, Jacob Devlin,
David Stallard, Spyros Matsoukas, Richard
Schwartz, John Makhoul, Omar F. Zaidan,
and Chris Callison-Burch. 2012. Machine
translation of Arabic dialects. In the 2012
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 49?59, Montreal.
Zissman, Marc A. 1996. Comparison of
four approaches to automatic language
identification of telephone speech. IEEE
Transactions on Speech and Audio Processing,
4(1):31?44.
202
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 207?215,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Cheap, Fast and Good Enough:
Automatic Speech Recognition with Non-Expert Transcription
Scott Novotney and Chris Callison-Burch
Center for Language and Speech Processing
Johns Hopkins University
snovotne@bbn.com ccb@jhu.edu
Abstract
Deploying an automatic speech recogni-
tion system with reasonable performance
requires expensive and time-consuming
in-domain transcription. Previous work
demonstrated that non-professional anno-
tation through Amazon?s Mechanical Turk
can match professional quality. We use
Mechanical Turk to transcribe conversa-
tional speech for as little as one thir-
tieth the cost of professional transcrip-
tion. The higher disagreement of non-
professional transcribers does not have a
significant effect on system performance.
While previous work demonstrated that
redundant transcription can improve data
quality, we found that resources are bet-
ter spent collecting more data. Finally, we
describe a quality control method without
needing professional transcription.
1 Introduction
Successful speech recognition depends on huge
investments in data collection. Even after train-
ing on 2000+ hours of transcribed conversa-
tional speech, over a billion words of language
modeling text, and hand-crafted pronunciation
dictionaries, state of the art systems still have
an error rate of around 15% for English (Prasad
et al, 2005) Transcribing the large volumes of
data required for Large Vocabulary Continuous
Speech Recognition (LVCSR) of new languages
appears prohibitively expensive. Recent work
has shown that Amazon?s Mechanical Turk1 can
1http://www.mturk.com
be used to cheaply create data for other nat-
ural language processing applications (Snow et
al., 2008; Zaidan and Callison-Burch, 2009; Mc-
Graw et al, 2009). In this paper we focus
on reducing the cost of transcribing conversa-
tional telephone speech (CTS) data. Previous
measurements of Mechanical Turk stopped at
agreement/disagreement with professional an-
notation. We take the next logical step and
measure performance on systems trained with
non-professional transcription.
Mechanical Turk is an online labor mar-
ket where workers (or Turkers) perform simple
tasks called Human Intelligence Tasks (HITs)
for small amounts of money ? frequently as lit-
tle as $0.01 per HIT. Since HITs can be tasks
that are difficult for computers, but easy for hu-
mans, they are ideal for natural language pro-
cessing tasks (Snow et al, 2008). Mechanical
Turk has even spawned a business that special-
izes in manual speech transcription.2
Automatic speech recognition (ASR) of con-
versational speech is an extremely difficult prob-
lem. Characteristics like rapid speech, pho-
netic reductions and speaking style limit the
value of non-CTS data, necessitating in-domain
transcription. Even a few hours of transcrip-
tion is sufficient to bootstrap with unsupervised
methods like self-training (Lamel et al, 2002).
The speech community has built effective down-
stream solutions for the past twenty years de-
spite imperfect recognition. In topic classifi-
cation, 90% accuracy is possible on conversa-
tional data even with 80%+ word error rate
2http://castingwords.com/
207
(WER) (Gillick et al, 1993). Other successful
tasks include information retrieval from speech
(Miller et al, 2007) and spoken dialogue pro-
cessing (Young et al, 2007). Inexpensive tran-
scription would quickly open new languages or
domains (like meeting or lecture data) for auto-
matic speech recognition.
In this paper, we make the following points:
? Quality control isn?t necessary as a system
built with non-professional transcription is
only 6% worse for 130 the cost of professional
transcription.
? Resources are better spent collecting more
data than improving data quality.
? Transcriber skill can be accurately esti-
mated without gold standard data.
2 Related Work
Research into Mechanical Turk by the NLP com-
munity has largely focused on comparing the
quality of annotations produced by non-expert
Turkers against annotations created by experts.
Snow et al (2008) conducted a comprehensive
study across a variety of NLP tasks. They
showed that high agreement could be reached
with gold-standard expert annotation for these
tasks through a weighted combination of ten re-
dundant annotations produced by Turkers.
Callison-Burch (2009) showed similar results
for machine translation evaluation, and further
showed that Turkers could accomplish complex
tasks like translating Urdu or creating reading
comprehension tests.
McGraw et al (2009) used Mechanical Turk
to improve an English isolated word speech rec-
ognizer by having Turkers listen to a word and
select from a list of probable words at a cost of
$20 per hour of transcription.
Marge et al (2010) collected transcriptions of
verbal instructions to robots with clean speech.
By using five duplicate transcriptions, the aver-
age transcription disagreement with experts was
reduced from 4% to 2%.
Previous efforts at reducing the cost of tran-
scription include the EARS Fisher project (Cieri
et al, 2004), which collected 2000+ hours of En-
glish CTS data ? an order of magnitude more
than had previously been transcribed. To speed
transcription and lower costs, Kimball et al
(2004) created new transcription guidelines and
used automatic segmentation. These improved
the speed of transcription from fifty times real
time to six times real time, and made it cost
effective to transcribe 2000 hours at an aver-
age of $150 per hour. Models trained on the
faster transcripts exhibited almost no degra-
dation in performance, although discrimanitve
training was sensitive to transcription errrors.
3 Experiment Description
3.1 Corpora
We conducted most experiments on a twenty
hour subset of the English Switchboard corpus
(Godfrey et al, 1992) where two strangers con-
verse about an assigned topic. We used two sets
of transcription as our gold standard: high qual-
ity transcription from the LDC and those fol-
lowing the Fisher quick transcription guidelines
(Kimball et al, 2004) provided by a professional
transcription company. All English ASR models
were tested with the carefully transcribed three
hour Dev04 test set from the NIST HUB5 eval-
uation.3 A 75k word lexicon taken from the
EARS Fisher training corpus covers the LDC
training data and has a test OOV rate of 0.18%.
We also conducted experiments in Korean and
collected Hindi and Tamil data from the Call-
friend corpora 4. Participants were given a free
long distance phone call to talk with friends or
family in their native language, although En-
glish frequently appears. Since Callfriend was
originally intended for language identification,
only the 27 hour Korean portion has been tran-
scribed by the LDC.
3.2 LVCSR System
We used Byblos, a state-of-the-art multi-pass
LVCSR system with state-clustered Gaussian
tied-mixture acoustic models and modified
Kneser-Ney smoothed language models (Prasad
et al, 2005). While understanding the system
3http://www.itl.nist.gov/iad/mig/tests/ctr/
1998/current-plan.html
4http://www.ldc.upenn.edu/CallFriend2/
208
details is not essential for this work, we provide
a brief description for completeness.
Recognition begins with cepstral feature ex-
traction using concatenated frames with cepstral
mean subtraction and HLDA to reduce the fea-
ture dimension space. Vocal track length nor-
malization follows. Decoding then requires three
passes: a fast forward pass with coarse one-
gaussian-per-phone models and bigram LM fol-
lowed by a backward pass with triphone models
and a trigram LM to generate word confusion
lattices. The lattices are rescored with a more
powerful quinphone cross-word acoustic model
and trigram LM to extract the one best out-
put. These three steps are repeated after un-
supervised speaker adaptation with constrained
MLLR. Decoding is around ten times real time.
3.3 Transcription Task
Using language-independent speaker activity de-
tection models, we segmented each ten minute
conversation into five second utterances, greatly
simplifying the transcription task (Roy and Roy,
2009). Utterances were assigned in batches of
ten per HIT and played with a simple flash
player with a text box for entry. All non-empty
HITs were approved and we did not award
bonuses except as described in Section 5.1.
3.4 Measuring Annotation Quality
The usefullness of the transcribed data is ul-
timately measured by how much it benefits a
speech recognition system. Factors that inflate
disagreement (word error rate) between Turkers
and professionals do not necessarily impact sys-
tem performance. These include typographical
mistakes, transcription inconsistencies (like im-
properly marking hesitations or the many vari-
ations of um) and spelling variations (geez or
jeez are both valid spellings). Additionally, the
gold standard is itself imperfect, with typical
estimates of professional disagreement around
five percent. Therefore, we judge the quality of
Mechanical Turk data by comparing the perfor-
mance of one LVCSR system trained on Turker
annotation and another trained on professional
transcriptions of the same dataset.
Average Turker Transcription Productivity for English
Transcription time / Utterance length (xRT)
N
um
be
r o
f T
ur
ke
rs
0 10 20 30 40 50 60
0
20
40
60
80
10
0
Fi
sh
er
 Q
ui
ck
Tr
an
s 
S
pe
ed
Ty
pi
ca
l H
ig
h 
Q
ua
lit
y 
S
pe
ed
Mean Turker Productivity 12xRT
Figure 1: Histogram of per-turker transcription rate
for twenty hours of English CTS data. Historical
estimates for high quality transcription are 50xRT.
The 2004 Fisher transcription effort achieved 6xRT
and the average here is 11xRT.
4 Establishing Best Practices with
English Switchboard
As an initial test to see how cheaply conversa-
tional data could be transcribed, we uploaded
one hour of test data from Hub5 Dev04. We
first paid $0.20 per HIT ($0.02 per utterance).
This test finished quickly, and we measured the
average disagreement with professionals at 17%.
Next, we reduced payment to $0.10 per HIT
and disagreement was again 17%. Finally, we
pushed the price down to $0.05 per HIT or $5
per hour of transcription and again disagree-
ment was nearly identical at 18%, although a
few Turkers complained about the low pay.
Using this price, we then paid for the full
twenty hours to be redundantly transcribed
three times. 1089 Turkers participated in the
task at an incoming rate of 10 hours of tran-
scription per day. On average, each Turker tran-
scribed 30 utterance (earning 15 cents) at an
average professional disagreement of 23%. Tran-
scribing one minute of audio required an aver-
age eleven minutes of effort (denoted 11xRT).
63 workers transcribed more than one hundred
utterances and one prolific worker transcribed
1223 utterances.
209
4.1 Comparing Non-Professional to
Professional Transcription
Table 1 details the results of different selection
methods for redundant transcription. For each
method of selection, we build an acoustic and
language model and report WER on the heldout
test set (transcribed at very high accuracy).
We first randomly selected one of the three
transcriptions per utterance (as if the data were
only tanscribed once) and repeated this three
times with little variance. Selecting utterances
randomly by Turker performed similarly. Per-
formance of an LVCSR system trained on the
non-professional transcription degrades by only
2.5% absolute (6% relative) despite a disagree-
ment of 23%. This is without any quality
control besides throwing out empty utterances.
The degradation held constant as we swept the
amount of training data frome one to twenty
hours. Bot the acoustic and language models ex-
hibited the log-linear relationship between WER
and the amount of training data. Independent of
the amount of training data, the acoustic model
degraded by a nearly constant 1.7% and the lan-
guage model by 0.8%.
To evaluate the benefit of multiple transcrip-
tions, we built two oracle systems. The Turker
oracle ranks Turkers by the average error rate of
their transcribed utterances against the profes-
sionals and selects utterances by Turker until the
twenty hours is covered (Section 4.3 discusses a
fair way to rank Turkers). The utterance oracle
selects the best of the three different transcrip-
tions per utterance. The best of the three Turk-
ers per utterance wrote the best transcription
two thirds of the time.
The utterance oracle only recovered half of
the degradation for using non-professional tran-
scription. Cutting the disagreement in half
(from 23% to 13%) reduced the WER gap by
about half (from 2.5% to 1%). Using the stan-
dard system combination algorithm ROVER
(Fiscus, 1997) to combine the three transcrip-
tions per utterance only reduced disagreement
from 23% to 21%. While previous work bene-
fited from combining multiple annotations, this
task shows little benefit.
Transcription
Disagreement
ASR WER
with LDC
Random Utterance 23% 42.0%
Random Turker 20% 41.4%
Oracle Utterance 13% 40.9%
Oracle Turker 18% 41.1%
Contractor < 5% 39.6%
LDC - 39.5%
Table 1: Quality of Non-Professional Transcription
on 20 hours of English Switchboard. Even though
disagreement for random selection without quality
control has 23% disagreement with professional tran-
scription, an ASR system trained on the data is only
2.5% worse than using LDC transcriptions. The up-
per bound for quality control (row 3) recovers only
50% of the total loss.
4.2 Combining with External Sources
While in-domain speech transcription is typi-
cally the only effective way to improve the acous-
tic model, out-of-domain transcripts tend to
be useful for language models of conversational
speech. Broadcast News (BN) transcription is
particularly well suited for English Switchboard
data as the topics tend to cover news items
like terrorism or politics. We built a small
one million word language model (to simulate a
resource-poor language) and interpolated it with
varying amounts of LDC or Mechanical Turk
transcriptions. Figure 2 details the results.
4.3 The Value of Quality Control
With a fixed transcription budget, should one
even bother with redundant transcription to im-
prove an ASR system? To find out, we tran-
scribed 40 additional hours of Switchboard us-
ing Mechanical Turk. Disagreement to the LDC
transcriptions was 24%, similar to the initial
20 hours. The two percent degradation of test
WER when using Mechanical Turk compared to
LDC held up with 40 and 60 hours of training.
Given a fixed budget of 60 hours of transcrip-
tion, we compared the quality of 20 hours tran-
scribed three times to 60 hours transcribed once.
The best we could hope to recover from the three
redundant transcriptions is the utterance oracle.
Oracle and singly transcribed data had 13% and
24% disagreement with LDC respectively. Sys-
tem performance was 40.9% with 20 hours of
210
ll
l
l
l
Improving the Language Model
Words of Transcription for Training (log scale)
Te
st
 W
E
R
l l
l
l l
l
l
l
l
l
l l
l
l
l
10K 20K 40K 80K 160K
38
%
40
%
42
%
44
%
46
%
48
%
MTurk only
LDC Only
MTurk + 1M BN
LDC + 1M BN
1M word BN LM Initial WER
0.8% Average Degradation
0.6% Average Degradation
(All decodes with a fixed 16 hour LDC acoustic model)
Figure 2: WER with a varied amount of LM training
data and a fixed 16hr acoustic model. MTurk tran-
scription degrades WER by 0.8% absolute across LM
size. When interpolated with 1M words of broadcast
news, this degradation shrinks to 0.6%.
the former and 37.6% with 60 hours of the latter.
Even though perfect selection cuts disagreement
in half, three times as much data helps more.
The 2004 Fisher effort averaged a price of $150
per hour of English CTS transcription. The
company CastingWords produces high quality
(Passy, 2008) English transcription for $90 an
hour using Mechanical Turk by a multi-pass pro-
cess to collect and clean Turker-provided tran-
scripts. While we did not use their service, we
assume it is of comparable quality to the pri-
vate contractor used earlier. The price for LDC
transcription is not comparable here since it was
intended for more precise linguistic tasks. Ex-
trapolating from Figure 3, the entire 2000 Fisher
corpus could be transcribed using Mechanical
Turk at the same cost of collecting 60 hours of
professional transcription.
5 Collection in Other Languages
To test the feasability of improving low-resource
languages, we attempted to collect transcrip-
tions for Korean, Hindi, Tamil CTS data. We
built an LVCSR system in Korean since it is the
only one with reference LDC transcriptions to
use as a test set.
l
100 200 500 1000 2000 5000 10000
30
35
40
45
Comparing Cost of Reducing WER
Total Cost (Dollars) to Collect Data (log scale)
Te
st
 W
E
R l
l
l
MTurk ? $5/hr
Mturk w/Oracle QC ? $15/hr
Casting Words ? $90/hr
Private Contractor ? $150/hr
Test WER with 20?60 hours of Switchboard Transcription
Figure 3: Historical cost estimates are $150 per hour
of transcription (blue cirlces). The company Casting
Words uses Turkers to transcribe English at $90 per
hour which we estimated to be high quality (green
triangles). Transcription without quality control on
Mechanical Turk (red squares) is drastically cheaper
at $5 per hour. With a fixed budget, it is better
to transcribe more data at lower quality than to im-
prove quality. Contrast the oracle WER for 20 hours
transcribed three times (red diamond) with 60 hours
transcribed once (bottom red square).
5.1 Korean
Korean is spoken by roughly 78 million speak-
ers world wide and is written in Hangul, a pho-
netic orthography, although Chinese characters
frequently appear in written text. Since Korean
has essentially arbitrary spacing (Chong-Woo et
al., 2001), we report Phoneme Error Rate (PER)
instead of WER, which would be unfairly pe-
nalized. Both behave similarly as system per-
formance improves. For comparison, an English
WER of 39.5% has a PER of 34.8%.
We uploaded ten hours of audio to be tran-
scribed once, again segmented into short snip-
pets. Transcription was very slow at first and
we had to pay $0.20 per HIT to attract work-
ers. We posted a separate HIT to refer Korean
transcribers, paying a 25% bonus of the income
earned by referrals. This was quite successful
as two referred Turkers contributed over 80%
of the total transcription (at a cost of $25 per
211
hour instead of $20). We collected three hours
of transcriptions after five weeks, paying eight
Turkers $113 at a transcription rate of 10xRT.
Average Turker disagreement to the LDC
reference was 17% (computed at the charac-
ter level). Using these transcripts to train an
LVCSR system instead of those provided by
LDC degraded PER by 0.8% from 51.3% to
52.1%. For comparison, a system trained on the
entire 27 hours of LDC data had 41.2% PER.
Although performance seems poor, it is suf-
ficiently good to bootstrap with acoustic model
self-training (Lamel et al, 2002). The language
model can be improved by finding ?conversa-
tional? web text found with n-gram queries ex-
tracted from the three hours of transcripts (Bu-
lyko et al, 2003).
5.2 Hindi and Tamil
As a feasability experiment, we collected one
hour of transcription in Hindi and Tamil, pay-
ing $20 per hour of transcription. Hindi and
Tamil transcription finished in eight days, per-
haps due to the high prevalence of Turkers in
India (Ipeirotis, 2008). While we did not have
any professional reference, Hindi speaking col-
leagues viewed some of the data and pointed
out errors in English transliteration, but over-
all quality appeared fine. The true test will be
to build an LVCSR system and report WER.
6 Quality Control sans Quality Data
Although we have shown that redundantly tran-
scribing an entire corpus gives little gain, there
is value in some amount of quality control. We
could improve system performance by only re-
jecting Turkers with high disagreement, similar
to confidence selection for active learning or un-
supervised training (Ma and Schwartz, ). But if
we are transcribing a truly new domain, there is
no gold-standard data to use as reference, so we
must estimate disagreement against errorful ref-
erence. In this section we provide a practical use
for quality control without gold standard refer-
ence data.
Distribution of Turker Skill
Average Disagreement of Transcribed Utterances by Each Turker
0% 10% 30% 50% 70% 90%
N
or
m
al
iz
ed
 D
en
si
ty
Estimated Against Professionals
Estimated Against Other Turkers
23%
25%
Figure 4: Each Turker was judged against profes-
sional and non-professional reference and assigned
an overall disagreement. The distribution of Turker
disagreement follows a gamma distribution, with a
tight cluster of average Turkers and a long-tail of bad
Turkers. Estimating with non-professionals (even
though the reference is 23% wrong on average) is
surprisingly well matched to professional estimate.
Turker estimation over-estimated disagreement by
only 2%.
6.1 Estimating Turker Skill
Using the twenty hour English transcriptions
from Section 4, we computed disagreement for
each Turker against the professional transcrip-
tion for all utterances longer than four words.
Note that each utterance was transcribed by
three random turkers, so there is not one set of
utterances which were transcribed by all turk-
ers. Each Turker transcribed a different, par-
tially overlapping, subset of the data.
For a particular Turker, we estimated the dis-
agreement with other Turkers by using the two
other transcripts as reference and taking the
average. Figure 4 shows the density estimate
of Turker disagreement when calculated against
professional and non-professional transcription.
On average, the non-professional estimate was
3% off from the professional disagreement.
Given that non-professional disagreement is
a good estimate of professional disagreement
212
Quickly Estimating Disagreement
Number of Utterances to Estimate Non?Professional Disagreement
D
iff
er
en
ce
 fr
om
 P
ro
fe
ss
io
na
l E
st
im
at
io
n 
on
 A
ll 
U
tte
ra
nc
es
0%
5%
10
%
15
%
20
%
25
%
30
%
0 5 10 15 20 25 30
Minimum
First Quartile
Median
Third Quartile
Maximum
Figure 5: Boxplot of the difference of non-
professional disagreement with a fixed number of ut-
terances to professional disagreement over all utter-
ances. While error is expectedly high with one ut-
terance, 50% of the estimates are within 3% of the
truth after ten utterances and 75% of the estimates
are within 6% after fifteen utterances.
over all of a Turker?s utterances, we wondered
how few needed to be redundantly transcribed
by other non-professionals. For each Turker,
we started by randomly selecting one utter-
ance and computed the non-professional dis-
agreement. We compared the estimate to the
true professional disagreement over all of the ut-
terances and repeatedly sample 20 times. Then
we increased the number of utterances used to
estimate non-professional disagreement until all
utterances by that Turker are selected.
Figure 5 shows a boxplot of the differences of
non-professional to professional disagreement on
all utterances. As few as fifteen utterances need
to be redundantly transcribed to accurately es-
timate three out of four Turkers within 5% of
the professional disagreement.
6.2 Finding the Right Turkers
Since we can accurately predict a Turker?s skill
with as few as fifteen utterances on average, we
can rank Turkers by their professional and non-
professional disagremeents. By thresholding on
disagreement, we can either select good turk-
++ +
+
+
++ +
+
+
+
++++
+
+
+
+
++ +
+
+++ +
+ + +
+
+ +
++
+
+ +
+ +
+
+
++ +
++
+
+
+
+
+
+
+
+
+
++
+
+
++
+
++
++
++ ++
+
+
+
++ +
+
+
+
+
++ ++
+
+
+ +
+
+
+
+
++
+
+++
+
+
+
+
++
+
++ +
+
+ +
+
+
+
++
+
++
++
++
+
++
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+++
+ ++ ++
+
+
+
+
+++
+
++
+
+
+
+ ++
+
+ +
+
+
+
+
++
+
+
+
+
+
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Rating Turkers: Professional v. Non?Professional
Turker Disagreement against Professional
Tu
rk
er
 D
is
ag
re
em
en
t a
ga
in
st
 o
th
er
 T
ur
ke
rs
++ ++
+
+++
+++
+
+ +
+
+
+
+ ++
+
+++
+
+
+
+++
+
+
+
+
+
+
+
+
++
++
++
+
+++
+
+
+
+
+
+
++
++ ++
++
+
+
+
+
++
+
++
+++
+
++
+
+
+ +++++
+ ++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
+
+
+
+
+
+
+
+
++
+ +
+
+
+
+ ++
+ +
+ +
++
+
+
+
+
+
+ +
+
++
+
+
+
+
+
++ +
+
+
+
++
+
+
+
+
+
+
+
+
+
+++++
+
+
+
+
+
+
+
+
+ + +
+
+
+
+
+
+
+++++
++
+
+
+
+
+ ++
+
+
+
+
+ ++++
+
++ ++ +
+
+
+
+++
+
+
+
+
+++ +
++
++
+
+
+
+
+
+
+
++
++ ++ +
+
+
+
+++ +
++
+
Incorrect Reject
12.5%
Correct Accept
57.54%
Incorrect Accept
4.5%
Correct Reject
25.46%
Threshold at Mean Disagreement of 23.17%
Figure 6: Each Turker is a point with professional (X
axis) plotted against non-professional (Y axis) dis-
agreement. The non-professional disagreement cor-
relates surprisingly well with professional disagree-
ment even though the transcripts used as reference
are 23% wrong on average. By setting a selection
threshold, the space is divided into four quadrants.
The bottom left are correctly accepted: both non-
professional and professional disagreement are below
the threshold. The top left are incorrectly rejected:
using their transcripts would have helped, but they
don?t hurt system performance, just waste money.
The top right are correctly rejected for having high
disagreement. The bottom right are the troublesome
false positives that are included in training but actu-
ally may hurt performance. Luckily, the ratio of false
negatives to false positives is usually much larger.
ers or equivalently reject bad turkers. We can
view the ranking as a precision/recall problem to
select only the ?good? Turkers below the thresh-
old. Figure 6 plots each Turker where the X axis
is the professional disagreement and the Y axis
is the non-professional disagreement. Sweeping
the disagreement threshold from zero to one gen-
erates Figure 7, which reports F-score (the har-
monic mean of precision and recall). This sec-
tion suggests a concrete qualification test by first
transcribing 15-30 utterance multiple times to
create a gold standard. Using the transcription
from the best Turker as reference, approve new
Turkers with a WER less than the average WER
from the initial set.
213
0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Selecting Turkers by Estimated Skill
WER Selection Threshold
F?
sc
or
e
Figure 7: It is difficult to find only good Turkers since
the false positives outnumber the few good workers.
However, rejecting bad Turkers becomes very easy
once past the mean error rate of 23%. It is better to
use disagreement estimation to reject poor workers
instead of finding good workers.
7 Experience with Mechanical Turk
We initially expected to invest most of our ef-
fort in managing Turker transcription. But the
vast majority of Turkers completed the effort in
good faith with few complaints about pay. Many
left positive comments5 despite the very difficult
task. Indeed, the author?s own disagreement on
a few dozen English utterances were 17.7% and
26.8% despite an honest effort.
Instead, we spent most of our time normaliz-
ing the transcriptions for English acoustic model
training. Every single misspelling or new word
had to be mapped to a pronunciation in order
to be used in training. We initially discarded
any utterance with an out of vocabulary word,
but after losing half of the data, we used a set
of simple heuristics to produce pronunciations.
Even though there were a few thousand of these
errors, they were all singletons and had little
effect on performance. Turkers sometimes left
comments in the transcription box such as ?no
5One Turker left a comment ?You don?t grow pick-
les!!? in regards to the misinformed speakers she was
transcribing.
audio? or ?man1: man2:?. These errant tran-
scriptions could be detected by force aligning
the transcript with the audio and rejecting any
with low scores (Lamel et al, 2000). Extending
transcription to thousands of hours will require
robust methods to automatically deal with er-
rant transcripts and additionally run the risk of
exhausting the available pool of workers.
Finding Korean transcribers required the
most creativity. We found success in interact-
ing with the transcribers, providing feedback,
encouragement and paying bonuses for referring
other workers. Cultivating workers for a new
language is definitely a ?hands on? process.
For Hindi and Tamil, Turkers sometimes mis-
interpreted or ignored instructions and trans-
lated into English or transliterated into Roman
characters. Additionally, some linguistic knowl-
edge is required to classify phonemic categories
(like fricative or sonorant) required for acoustic
model training.
8 Conclusion
Unlike previous work which studied the quality
of Mechanical Turk annotations alone, we judge
its value in terms of the real task: improving
system performance. Despite relatively high dis-
agreement with professional transcription, data
collected with Mechanical Turk was nearly as
effective for training speech models. Since this
degradation is so small, redundant annotation to
improve quality is not worth the cost. Resources
are better spent collecting more transcription.
In addition to English, we demonstrated similar
trends in Korean and also collected transcripts
for Hindi and Tamil. Finally, we proposed an
effective procedure to reduce costs by maintain-
ing the quality of the annotator pool without
needing high quality annotation.
Acknowledgments
This research was supported by the EuroMa-
trixPlus project funded by the European Com-
mission by the DARPA GALE program under
Contract No. HR0011-06-2-0001, and NSF un-
der grant IIS-0713448 and by BBN Technologies.
The views and findings are the authors? alone.
214
References
Ivan Bulyko, Mari Ostendorf, and A. Stolcke. 2003.
Getting more mileage from web text sources for
conversational speech language modeling using
class-dependent mixtures. In HLT-NAACL.
Chris Callison-Burch. 2009. Fast, Cheap, and Cre-
ative: Evaluating Translation Quality Using Ama-
zons Mechanical Turk. EMNLP.
Seung-Shik Kang Chong-Woo, Chong woo Woo, and
Kookmin Univerity. 2001. Automatic segmen-
tation of words using syllable bigram statistics.
In 6th Natural Language Processing Pacific Rim
Symposium.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The fisher corpus: a resource for the next
generations of speech-to-text. In LREC.
Jonathan G. Fiscus. 1997. A post-processing sys-
tem to yield reduced word error rates: Recognizer
output voting error reduction (rover).
L. Gillick, J. Baker, J. Bridle, M. Hunt, Y. Ito,
S. Lowe, J. Orloff, B. Peskin, R. Roth, and F. Scat-
tone. 1993. Application of large vocabulary con-
tinuous speech recognition to topic and speaker
identification using telephone speech. In ICASSP.
Jack Godfrey, Edward Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech
corpus for research and development. In ICASSP.
Panos Ipeirotis. 2008. Mechanical turk: The de-
mographics. http://behind-the-enemy-lines.
blogspot.com/2010/03/
new-demographics-of-mechanical-turk.html.
Owen Kimball, Chai-Lin Kao, Teodoro Arvizo, John
Makhoul, and Rukmini Iyer. 2004. Quick tran-
scription and automatic segmentation of the fisher
conversational telephone speech corpus. In RT04
Workshop.
Lori Lamel, Jean luc Gauvain, and Gilles Adda.
2000. Lightly supervised acoustic model training.
In ISCA ITRW ASR2000.
Lori Lamel, Jean luc Gauvain, and Gilles Adda.
2002. Lightly supervised and unsupervised acous-
tic model training. Computer Speech and Lan-
guage, 16(1).
Jeff Ma and Rich Schwartz. Unsupervised versus su-
pervised training of acoustic models. In INTER-
SPEECH.
Matthew Marge, Satanjeev Banerjee, and Alexan-
der Rudnicky. 2010. Using the amazon me-
chanical turk for transcription of spoken language.
ICASSP, March.
Ian McGraw, Alexander Gruenstein, and Andrew
Sutherland. 2009. A self-labeling speech corpus:
Collecting spoken words with an online educa-
tional game. In INTERSPEECH.
D. Miller, M. Kleber, C. Kao, O. Kimball,
T. Colthurst, S.A. Lowe, R.M. Schwartz, and
H. Gish. 2007. Rapid and Accurate Spoken Term
Detection. In INTERSPEECH.
Charles Passy. 2008. Turning audio into words on
the screen. http://online.wsj.com/article/
SB122351860225518093.html.
R. Prasad, S. Matsoukas, CL Kao, J.Z. Ma, DX Xu,
T. Colthurst, O. Kimball, R. Schwartz, J.L. Gau-
vain, L. Lamel, et al 2005. The 2004 BBN/LIMSI
20xRT English conversational telephone speech
recognition system. In INTERSPEECH.
Brandon Roy and Deb Roy. 2009. Fast transcrip-
tion of unstructured audio recordings. In INTER-
SPEECH.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is
it good?: evaluating non-expert annotations for
natural language tasks. In EMNLP.
Steve. Young, Jost. Schatzmann, Karl. Weilhammer,
and Hui. Ye. 2007. The hidden information state
approach to dialog management. In ICASSP.
Omar F. Zaidan and Chris Callison-Burch. 2009.
Feasibility of human-in-the-loop minimum error
rate training. In EMNLP.
215
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 369?372,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Predicting Human-Targeted Translation Edit Rate 
via Untrained Human Annotators 
 
Omar F. Zaidan  and  Chris Callison-Burch 
Dept. of Computer Science, Johns Hopkins University 
Baltimore, MD 21218, USA 
{ozaidan,ccb}@cs.jhu.edu 
 
 
 
Abstract 
In the field of machine translation, automatic 
metrics have proven quite valuable in system 
development for tracking progress and meas-
uring the impact of incremental changes. 
However, human judgment still plays a large 
role in the context of evaluating MT systems. 
For example, the GALE project uses human-
targeted translation edit rate (HTER), wherein 
the MT output is scored against a post-edited 
version of itself (as opposed to being scored 
against an existing human reference). This 
poses a problem for MT researchers, since 
HTER is not an easy metric to calculate, and 
would require hiring and training human an-
notators to perform the editing task. In this 
work, we explore soliciting those edits from 
untrained human annotators, via the online 
service Amazon Mechanical Turk. We show 
that the collected data allows us to predict 
HTER-ranking of documents at a significantly 
higher level than the ranking obtained using 
automatic metrics. 
1 Introduction 
In the early days of machine translation (MT), it 
was typical to evaluate MT output by soliciting 
judgments from human subjects, such as evaluat-
ing the fluency and adequacy of MT output (LDC, 
2005). While this approach was appropriate (in-
deed desired) for evaluating a system, it was not a 
practical means of tracking the progress of a sys-
tem during its development, since collecting hu-
man judgments is both costly and time-consuming. 
The introduction of automatic metrics like BLEU 
contributed greatly to MT research, for instance 
allowing researchers to measure and evaluate the 
impact of small modifications to an MT system. 
However, manual evaluation remains a core 
component of system evaluation. Teams on the 
GALE project, a DARPA-sponsored MT research 
program, are evaluated using the HTER metric, 
which is a version of TER whereby the output is 
scored against a post-edited version of itself, in-
stead of a preexisting reference. Moreover, empha-
sis is placed on performing well across all 
documents and across all genres. Therefore, it is 
important for a research team to be able to evaluate 
their system using HTER, or at least determine the 
ranking of the documents according to HTER, for 
purposes of error analysis. Instead of hiring a 
human translator and training them, we propose 
moving the task to the virtual world of Amazon?s 
Mechanical Turk (AMT), hiring workers to edit the 
MT output and predict HTER from those edits. We 
show that edits collected this way are better at 
predicting document ranking than automatic 
metrics, and furthermore that it can be done at a 
low cost, both in terms of time and money. 
The paper is organized as follows. We first 
discuss options available to predict HTER, such as 
automatic metrics. We then discuss the possibility 
of relying on human annotators, and the inherent 
difficulty in training them, before discussing the 
concept of soliciting edits over AMT. We detail 
the task given to the workers and summarize the 
data that we collected, then show how we can 
combine their data to obtain significanly better 
rank predictions of documents. 
2 Human-Targeted TER 
Translation edit rate (TER) measures the number 
of edits required to transform a hypothesis into an 
appropriate sentence in terms of grammaticality 
and meaning (Snover et al, 2006). While TER 
usually scores a hypothesis against an existing ref-
erence sentence, human-targeted TER scores a 
hypothesis against a post-edited version of itself. 
369
While HTER has been shown to correlate quite 
well with human judgment of MT quality, it is 
quite challenging to obtain HTER scores for MT 
output, since this would require hiring and training 
human subjects to perform the editing task. There-
fore, other metrics such as BLEU or TER are used 
as proxies for HTER. 
2.1 Amazon?s Mechanical Turk 
The high cost associated with hiring and training a 
human editor makes it difficult to imagine an alter-
native to automatic metrics. However, we propose 
soliciting edits from workers on Amazon?s Me-
chanical Turk (AMT). AMT is a virtual market-
place where ?requesters? can post tasks to be 
completed by ?workers? (aka Turkers) around the 
world. Two main advantages of AMT are the pre-
existing infrastructure, and the low cost of com-
pleting tasks, both in terms of time and money. 
Data collected over AMT has already been used in 
several papers such as Snow et al (2008) and Cal-
lison-Burch (2009). 
When a requester creates a task to be completed 
over AMT, it is typical to have completed by more 
than one worker. The reason is that the use of 
AMT for data collection has an inherent problem 
with data quality. A requester has fewer tools at 
their disposal to ensure workers are doing the task 
properly (via training, feedback, etc) when com-
pared to hiring annotators in the ?real? world. 
Those redundant annotations are therefore col-
lected to increase the likelihood of at least one 
submission from a faithful (and competent) 
worker. 
2.2 AMT for HTER 
The main idea it to mimic the real-world HTER 
setup by supplying workers with the original MT 
output that needs to be edited. The worker is also 
given a human reference, produced independently 
from the MT output. The instructions ask the 
worker to modify the MT output, using as few ed-
its as possible, to match the human reference in 
meaning and grammaticality. 
The submitted edited hypothesis can then be 
used as the reference for calculating HTER. The 
idea is that, with this setup, a competent worker 
would be able to closely match the editing behav-
ior of the professionally trained editor. 
3 The Datasets 
We solicited edits of the output from one of 
GALE?s teams on the Arabic-to-English task. This 
MT output was submitted by this team and HTER-
scored by LDC-hired human translators. Therefore, 
we already had the edits produced by a 
professional translator. These edits were used as 
the ?gold-standard? to evaluate the edits solicited 
from AMT and to evaluate our methods of 
combining Turkers? submissions. 
The MT output is a translation of more than 
2,153 Arabic segments spread across 195 docu-
ments in 4 different genres: broadcast conversa-
tions (BC), broadcast news (BN), newswire (NW), 
and blogs (WB). Table 1 gives a summary of each 
genre?s dataset. 
 
Genre # docs Segs/doc Words/seg 
BC 40 15.8 28.3 
BN 48 9.6 36.1 
NW 54 8.7 39.5 
WB 53 11.1 31.6 
Table 1:  The 4 genres of the dataset. 
 
For each of the 2,153 MT output segments, we 
collected edits from 5 distinct workers on AMT, 
for a total of 10,765 post-edited segments by a total 
of about 500 distinct workers.1 The segments were 
presented in 1,210 groups of up to 15 segments 
each, with a reward of $0.25 per group. Hence the 
total rewards to workers was around $300, at a rate 
of 36 post-edited segments per dollar (or 2.8 pen-
nies per segment). 
4 What are we measuring? 
We are interested in predicting the ranking the 
documents according to HTER, not necessarily 
predicting the HTER itself (though of course at-
tempting to predict the latter accurately is the cor-
nerstone of our approach to predict the former). To 
measure the quality of a predicted ranking, we use 
Spearman?s rank correlation coefficient, ?, where 
we first convert the raw scores into ranks and then 
use the following formula to measure correlation: 
)1(
))()((6
1),( 2
1
2
?
?
?=
?
=
nn
yrankxrank
YX
n
i
ii
?  
                                                           
1 Data available at http://cs.jhu.edu/~ozaidan/hter. 
370
 
where n is the number of documents, and each of X 
and Y is a vector of n HTER scores. 
Notice that values for ? range from ?1 to 1, with 
+1 indicating perfect rank correlation, ?1 perfect 
inverse correlation, and 0 no correlation. That is, 
for a fixed X, the best-correlated Y is that for which 
),( YX?  is highest. 
5 Combining Tukers? Edits 
Once we have collected edits from the human 
workers, how should we attempt to predict HTER 
from them? If we could assume that all Turkers are 
doing the task faithfully (and doing it adequately), 
we should use the annotations of the worker per-
forming the least amount of editing, since that 
would mirror the real-life scenario. 
However, data collected from AMT should be 
treated with caution, since a non-trivial portion of 
the collected data is of poor quality. Note that this 
does not necessarily indicate a ?cheating? worker, 
for even if a worker is acting in good faith, they 
might not be able to perform the task adequately, 
due to misunderstanding the task, or neglecting to 
attempt to use a small number of edits. 
And so we need to combine the redundant edits in 
an intelligent manner. Recall that, given a segment, 
we collected edits from multiple workers. Some 
baseline methods include taking the minimum over 
the edits, taking the median, and taking the aver-
age. 
Once we start thinking of averages, we should 
consider taking a weighted average of the edits for 
a segment. The weight associated with a worker 
should reflect our confidence in the quality of that 
worker?s edits. But how can we evaluate a worker 
in the first place? 
5.1 Self Verification of Turkers 
We have available ?gold-standard? editing behav-
ior for the segments, and we treat a small portion 
of the segments edited by a Turker as a verification 
dataset. On that portion, we evaluate how closely 
the Turker matches the LDC editor, and weight 
them accordingly when predicting the number of 
edits of the rest of that group?s segments. Specifi-
cally, the Turker?s weight is the absolute difference 
between the Turker?s edit count and the profes-
sional editor?s edit count. 
Notice that we are not simply interested in a 
worker whose edited submission closely matches 
the edited submission of the professional transla-
tor. Rather, we are interested in mirroring the pro-
fessional translator?s edit rate. That is, the closer a 
Turker?s edit rate is to the LDC editor?s, the more 
we should prefer the worker. This is a subtle point, 
but it is indeed possible for a Turker to have simi-
lar edit rate as the LDC editor but still require a 
large number of edits to get the LDC editor?s sub-
mission itself. 
6 Experiments 
We examine the effectiveness of any of the above 
methods by comparing the resulting document 
ranking versus the desired ranking by HTER. In 
addition to the above methods, we use a baseline a 
ranking predicted by TER to a human reference. 
(For clarity, we omit discussion with other metrics 
such as BLEU and (TER?BLEU)/2, since those 
baselines are not as strong as the TER baseline. 
6.1 Experimental Setup 
We examine each genre individually, since genres 
vary quite a bit in difficulty, and, more impor-
tantly, we care about the internal ranking within 
each genre, to mirror the GALE evaluation proce-
dure. 
We examine the effect of varying the amount of 
data by which we judge a Turker?s data quality. 
The amount of this ?verification? data is varied as 
a percentage of the total available segments. Those 
segments are chosen at random, and we perform 
100 trials for each point. 
6.2 Experimental Results 
Figure 1 shows the rank correlations for various 
methods across different sizes of verification sub-
sets. Notice that some methods, such as the TER 
baseline, have horizontal lines, since these do not 
rate a Turker based on a verification subset. 
It is worth noting that the oracle performs very 
well. This is an indication that predicting HTER 
accurately is mostly a matter of identifying the best 
worker. While oracle scenarios usually represent 
unachievable upper bounds, keep in mind that 
there are only a very small number of editors per 
segment (five, as opposed to oracle scenarios deal-
ing with 100-best lists, etc). 
371
Other than that, in general, it is possible to 
achieve very high rank correlation using Turkers? 
data, significantly outperforming the TER ranking, 
even with a small verification subset. The genres 
do vary quite a bit in difficulty for Turkers, with 
BC and especially NW being quite difficult, 
though in the case of NW for instance, this is due 
to the human reference doing quite well to begin 
with, rather than Turkers performing poorly. 
7 Conclusions and Future Work 
We proposed soliciting edits of MT output via 
Amazon?s Mechanical Turk and showed we can 
predict ranking significantly better than an auto-
matic metric. The next step is to explicitly identify 
undesired worker behavior, such as not editing the 
MT output at all, or using the human reference as 
is instead of editing the MT output. This can be 
detected by not limiting our verification to compar-
ing behavior to the professional editor?s, but also 
by comparing submitted edits to the MT output 
itself and to the human reference. In other words, a 
worker?s submission could be characterized in 
terms of its distance to the MT output and to the 
human reference, thus building a complete ?pro-
file? of the worker, and adding another component 
to guard against poor data quality and to reward 
the desired behavior. 
Acknowledgments 
This work was supported by the EuroMatrixPlus 
Project (funded by the European Commission), and 
by the DARPA GALE program under Contract No. 
HR0011-06-2-0001. The views and findings are 
the authors' alone. 
References  
Chris Callison-Burch. 2009. Fast, Cheap, and Creative: 
Evaluating Translation Quality Using Amazon's Me-
chanical Turk. In Proceedings of EMNLP. 
LDC. 2005. Linguistic data annotation specification: 
Assessment of fluency and adequacy in translations. 
Revision 1.5. 
Matthew Snover, Bonnie J. Dorr, Richard Schwartz. 
2006. A Study of Translation Edit Rate with Targeted 
Human Annotation. Proceedings of AMTA. 
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and 
Andrew Y. Ng. 2008. Cheap and fast ? but is it 
good? Evaluating non-expert annotations for natural 
language tasks. In Proceedings of EMNLP. 
 
 
 
 
 
 
 
 
Figure 1: Rank correlation between predicted rank-
ing and HTER ranking for different prediction 
schemes, across the four genres, and across various 
sizes of the worker verification set. 
372
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 394?402,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Stream-based Translation Models for Statistical Machine Translation
Abby Levenberg
School of Informatics
University of Edinburgh
a.levenberg@ed.ac.uk
Chris Callison-Burch
Computer Science Department
Johns Hopkins University
ccb@cs.jhu.edu
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Abstract
Typical statistical machine translation sys-
tems are trained with static parallel corpora.
Here we account for scenarios with a continu-
ous incoming stream of parallel training data.
Such scenarios include daily governmental
proceedings, sustained output from transla-
tion agencies, or crowd-sourced translations.
We show incorporating recent sentence pairs
from the stream improves performance com-
pared with a static baseline. Since frequent
batch retraining is computationally demand-
ing we introduce a fast incremental alternative
using an online version of the EM algorithm.
To bound our memory requirements we use
a novel data-structure and associated training
regime. When compared to frequent batch re-
training, our online time and space-bounded
model achieves the same performance with
significantly less computational overhead.
1 Introduction
There is more parallel training data available to-
day than there has ever been and it keeps increas-
ing. For example, the European Parliament1 releases
new parallel data in 22 languages on a regular basis.
Project Syndicate2 translates editorials into seven
languages (including Arabic, Chinese and Russian)
every day. Existing translation systems often get
?crowd-sourced? improvements such as the option
to contribute a better translation to GoogleTrans-
late3. In these and many other instances, the data can
be viewed as an incoming unbounded stream since
1http://www.europarl.europa.eu
2http://www.project-syndicate.org
3http://www.translate.google.com
the corpus grows continually with time. Dealing
with such unbounded streams of parallel sentences
presents two challenges: making retraining efficient
and operating within a bounded amount of space.
Statistical Machine Translation (SMT) systems
are typically batch trained, often taking many CPU-
days of computation when using large volumes of
training material. Incorporating new data into these
models forces us to retrain from scratch. Clearly,
this makes rapidly adding newly translated sen-
tences into our models a daunting engineering chal-
lenge. We introduce an adaptive training regime us-
ing an online variant of EM that is capable of in-
crementally adding new parallel sentences without
incurring the burdens of full retraining.
For situations with large volumes of incoming
parallel sentences we are also forced to consider
placing space-bounds on our SMT system. We in-
troduce a dynamic suffix array which allows us to
add and delete parallel sentences, thereby maintain-
ing bounded space despite processing a potentially
high-rate input stream of unbounded length.
Taken as a whole we show that online translation
models operating within bounded space can perform
as well as systems which are batch-based and have
no space constraints thereby making our approach
suitable for stream-based translation.
2 Stepwise Online EM
The EM algorithm is a common way of inducing
latent structure from unlabeled data in an unsuper-
vised manner (Dempster et al, 1977). Given a set
of unlabeled examples and an initial, often uniform
guess at a probability distribution over the latent
variables, the EM algorithm maximizes the marginal
394
log-likelihood of the examples by repeatedly com-
puting the expectation of the conditional probability
of the latent data with respect to the current distri-
bution, and then maximizing the expectations over
the observations into a new distribution used in the
next iteration. EM (and related variants such as vari-
ational or sampling approaches) form the basis of
how SMT systems learn their translation models.
2.1 Batch vs. Online EM
Computing an expectation for the conditional prob-
abilities requires collecting the sufficient statistics S
over the set of n unlabeled examples. In the case
of a multinomial distribution, S is comprised of the
counts over each conditional observation occurring
in the n examples. In traditional batch EM, we col-
lect the counts over the entire dataset of n unlabeled
training examples via the current ?best-guess? proba-
bility model ??t at iteration t (E-step) before normal-
izing the counts into probabilities ??(S) (M-step)4.
After each iteration all the counts in the sufficient
statistics vector S are cleared and the count collec-
tion begins anew using the new distribution ??t+1.
When we move to processing an incoming data
stream, however, the batch EM algorithm?s require-
ment that all data be available for each iteration be-
comes impractical since we do not have access to all
n examples at once. Instead we receive examples
from the input stream incrementally. For this reason
online EM algorithms have been developed to up-
date the probability model ?? incrementally without
needing to store and iterate through all the unlabeled
training data repeatedly.
Various online EM algorithms have been investi-
gated (see Liang and Klein (2009) for an overview)
but our focus is on the stepwise online EM (sOEM)
algorithm (Cappe and Moulines, 2009). Instead
of iterating over the full set of training examples,
sOEM stochastically approximates the batch E-step
and incorporates the information from the newly
available streaming observations in steps. Each step
is called a mini-batch and is comprised of one or
more new examples encountered in the stream.
Unlike in batch EM, in sOEM the expected counts
are retained between EM iterations and not cleared.
4As the M-step can be computed in closed form we desig-
nate it in this work as ??(S).
Algorithm 1: Batch EM for Word Alignments
Input: {F (source),E (target)} sentence-pairs
Output: MLE ??T over alignments a
??0 ?MLE initialization;
for iteration k = 0, . . . , T do
S ? 0; // reset counts
foreach (f, e) ? {F,E} do // E-step
S ? S +
?
a??a
Pr(f, a?|e; ??t);
end
??t+1 ? ??t(S) ; // M-step
end
That is, for each new example we interpolate its ex-
pected count with the existing set of sufficient statis-
tics. For each step we use a stepsize parameter ?
which mixes the information from the current ex-
ample with information gathered from all previous
examples. Over time the sOEM model probabilities
begin to stabilize and are guaranteed to converge to
a local maximum (Cappe and Moulines, 2009).
Note that the stepsize ? has a dependence on the
current mini-batch. As we observe more incoming
data the model?s current probability distribution is
closer to the true distribution so the new observa-
tions receive less weight. From Liang and Klein
(2009), if we set the stepsize as ?t = (t + 2)??,
with 0.5 < ? ? 1, we can guarantee convergence in
the limit as n ? ?. If we set ? low, ? weighs the
newly observed statistics heavily whereas if ? is low
new observations are down-weighted.
2.2 Batch EM for Word Alignments
Batch EM is used in statistical machine translation
to estimate word alignment probabilities between
parallel sentences. From these alignments, bilingual
rules or phrase pairs can be extracted. Given a set
of parallel sentence examples, {F,E}, with F the
set of source sentences and E the corresponding tar-
get sentences, we want to find the latent alignments
a for a sentence pair (f , e) ? {F,E} that defines
the most probable correspondence between words fj
and ei such that aj = i. We can induce these align-
ments using an HMM-based alignment model where
the probability of alignment aj is dependent only on
the previous alignment at aj?1 (Vogel et al, 1996).
395
We can write
Pr(f ,a | e) =
?
a??a
|f |
?
j=1
p(aj | aj?1, |e|) ? p(fj | eaj )
where we assume a first-order dependence on previ-
ously aligned positions.
To find the most likely parameter weights for
the translation and alignment probabilities for the
HMM-based alignments, we employ the EM algo-
rithm via dynamic programming. Since HMMs have
multiple local minima, we seed the HMM-based
model probabilities with a better than random guess
using IBM Model 1 (Brown et al, 1993) as is stan-
dard. IBM Model 1 is of the same form as the
HMM-based model except it uses a uniform distri-
bution instead of a first-order dependency. Although
a series of more complex models are defined, IBM
Models 2 to Model 6 (Brown et al, 1993; Och and
Ney, 2003), researchers typically find that extract-
ing phrase pairs or translation grammar rules using
Model 1 and the HMM-based alignments results in
equivalently high translation quality. Nevertheless,
there is nothing in our approach which limits us to
using just Model 1 and the HMM model.
A high-level overview of the standard, batch EM
algorithm applied to HMM-based word alignment
model is shown in Algorithm 1.
2.3 Stepwise EM for Word Alignments
Application of sOEM to HMM and Model 1 based
word aligning is straightforward. The process of
collecting the counts over the expected conditional
probabilities inside each iteration loop remains the
same as in the batch case. However, instead of clear-
ing the sufficient statistics between the iterations we
retain them and interpolate them with the batch of
counts gathered in the next iteration.
Algorithm 2 shows high level pseudocode of our
sOEM framework as applied to HMM-based word
alignments. Here we have an unbounded input
stream of source and target sentences {F,E} which
we do not have access to in its entirety at once.
Instead we observe mini-batches {M} comprised
of chronologically ordered strict subsets of the full
stream. To word align the sentences for each mini-
batch m ? M, we use the probability assigned by
the current model parameters and then interpolate
Algorithm 2: sOEM Algorithm for Word Align-
ments
Input: mini-batches of sentence pairs
{M : M ? {F (source), E(target)}}
Input: stepsize weight ?
Output: MLE ??T over alignments a
??0 ?MLE initialization;
S ? 0; k = 0;
foreach mini-batch {m : m ?M} do
for iteration t = 0, . . . , T do
foreach (f, e) ? {m} do // E-step
s??
?
a??a
Pr(f, a?|e; ??t);
end
? = (k + 2)??; k = k + 1; // stepsize
S ? ?s? + (1? ?)S; // interpolate
??t+1 ? ??t(S) ; // M-step
end
end
the newest sufficient statistics s? with our full count
vector S using an interpolation parameter ?. The in-
terpolation parameter ? has a dependency on how
far along the input stream we are processing.
3 Dynamic Suffix Arrays
So far we have shown how to incrementally retrain
translation models. We now consider how we might
bound the space we use for them when processing
(potentially) unbounded streams of parallel data.
Suffix arrays are space-efficient data structures for
fast searching over large text strings (Manber and
Myers, 1990). Treating the entire corpus as a sin-
gle string, a suffix array holds in lexicographical or-
der (only) the starting index of each suffix of the
string. After construction, since the corpus is now
ordered, we can query the suffix array quickly us-
ing binary search to efficiently find all occurrences
of a particular token or sequence of tokens. Then we
can easily compute, on-the-fly, the statistics required
such as translation probabilities for a given source
phrase. Suffix arrays can also be compressed, which
make them highly attractive structures for represent-
ing massive translation models (Callison-Burch et
al., 2005; Lopez, 2008).
We need to delete items if we wish to maintain
396
                                                                                         epoch 2           
epoch 1 epoch 2 model coverage
model coverage
input stream
Test Points
input stream
Test Points
Static 
Unbounded
input stream
Test Points
Bounded
model coverage
sliding windows
Figure 1: Streaming coverage conditions. In traditional
batch based modeling the coverage of a trained model
never changes. Unbounded coverage operates without
any memory constraints so the model is able to contin-
ually add data from the input stream. Bounded coverage
uses just a fixed window.
constant space when processing unbounded streams.
Standard suffix arrays are static, store a fixed corpus
and do not support deletions. Nevertheless, a dy-
namic variant of the suffix array does support dele-
tions as well as insertions and therefore can be used
in our stream-based approach (Salson et al, 2009).
Using a dynamic suffix array, we can compactly
represent the set of parallel sentences from which
we eventually extract grammar rules. Furthermore,
when incorporating new parallel sentences, we sim-
ply insert them into the array and, to maintain con-
stant space usage, we delete an equivalent number.
4 Experiments
In this section we describe the experiments con-
ducted comparing various batch trained translation
models (TMs) versus online incrementally retrained
TMs in a full SMT setting with different conditions
set on model coverage. We used publicly available
resources for all our tests. We start by showing that
recency motivates incremental retraining.
4.1 Effects of Recency on SMT
For language modeling, it is known that perfor-
mance can be improved using the criterion of re-
cency where training data is drawn from times
chronologically closer to the test data (Rosenfeld,
 0
 0.5
 1
 1.5
 2
 2.5
 5  10  15  20  25  30  35
D
el
ta
 in
 B
LE
U
 sc
or
es
epochs
Figure 2: Recency effects to SMT performance. De-
picted are the differences in BLEU scores for multiple
test points decoded by a static baseline system and a sys-
tem batched retrained on a fixed sized window prior to
the test point in question. The results are accentuated at
the end of the timeline when more time has passed con-
firming that recent data impacts translation performance.
1995). Given an incoming stream of parallel text,
we gauged the extent to which incorporating recent
data into a TM affects translation quality.
We used the Europarl corpus5 with the Fr-En lan-
guage pair using French as source and English as tar-
get. Europarl is released in the format of a daily par-
liamentary session per time-stamped file. The actual
dates of the full corpus are interspersed unevenly
(they do not convene daily) over a continuous time-
line corresponding to the parliament sessions from
April,1996 through October, 2006, but for concep-
tual simplicity we treated the corpus as a continual
input stream over consecutive days.
As a baseline we aligned the first 500k sentence
pairs from the beginning of the corpus timeline. We
extracted a grammar for and translated 36 held out
test documents that were evenly spaced along the re-
mainder of the Europarl timeline. These test docu-
ments effectively divided the remaining training data
into epochs and we used a sliding window over the
timeline to build 36 distinct, overlapping training
sets of 500k sentences each.
We then translated all 36 test points again using
a new grammar for each document extracted from
only the sentences contained in the epoch that was
before it. To explicitly test the effect of recency
5Available at http://www.statmt.org/europarl
397
on the TM all other factors of the SMT pipeline re-
mained constant including the language model and
the feature weights. Hence, the only change from
the static baseline to the epochs performance was the
TM data which was based on recency. Note that at
this stage we did not use any incremental retraining.
Results are shown in Figure 2 as the differences
in BLEU score (Papineni et al, 2001) between the
baseline TM versus the translation models trained
on material chronologically closer to the given test
point. The consistently positive deltas in BLEU
scores between the model that is never retrained and
the models that are retrained show that we achieve a
higher translation performance when using more up-
to-date TMs that incorporate recent sentence pairs.
As the chronological distance between the initial,
static model and the retrained models increases, we
see ever-increasing differences in translation perfor-
mance. This underlines the need to retrain transla-
tion models with timely material.
4.2 Unbounded and Bounded Translation
Model Retraining
Here we consider how to process a stream along two
main axes: by bounding time (batch versus incre-
mental retraining) and by bounding space (either us-
ing all the stream seen so far, or only using a fixed
sized sample of it).
To ensure the recency results reported above were
not limited to French-English, this time our paral-
lel input stream was generated from the German-
English language pair of Europarl with German as
source and English again as target. For testing we
held out a total of 22k sentences from 10 evenly
spaced intervals in the input stream which divided
the input stream into 10 epochs. Stream statistics for
three example epochs are shown in Table 1. We held
out 4.5k sentence pairs as development data to opti-
mize the feature function weights using minimum
error rate training (Och, 2003) and these weights
were used by all models. We used Joshua (Li et
al., 2009), a syntax-based decoder with a suffix array
implementation, and rule induction via the standard
Hiero grammar extraction heuristics (Chiang, 2007)
for the TMs. Note that nothing hinges on whether
we used a syntax or a phrase-based system.
We used a 5-gram, Kneser-Ney smoothed lan-
guage model (LM) trained on the initial segment of
Ep From?To Sent Pairs Source/Target
00 04/1996?12/2000 600k 15.0M/16.0M
03 02/2002?09/2002 70k 1.9M/2.0M
06 10/2003?03/2004 60k 1.6M/1.7M
10 03/2006?09/2006 73k 1.9M/2.0M
Table 1: Date ranges, total sentence pairs, and source and
target word counts encountered in the input stream for
example epochs. Epoch 00 is baseline data that is also
used as a seed corpus for the online models.
the target side parallel data used in the first base-
line as described further in the next subsection. As
our initial experiments aim to isolate the effect of
changes to the TM on overall translation system per-
formance, our in-domain LM remains static for ev-
ery decoding run reported below until indicated.
We used the open-source toolkit GIZA++ (Och
and Ney, 2003) for all word alignments. For the
online adaptation experiments we modified Model
1 and the HMM model in GIZA++ to use the sOEM
algorithm. Batch baselines were aligned using the
standard version of GIZA++. We ran the batch and
incremental versions of Model 1 and HMM for the
same number of iterations each in both directions.
4.3 Time and Space Bounds
For both batch and sOEM we ran a number of ex-
periments listed below corresponding to the differ-
ent training scenarios diagrammed in Figure 1.
1. Static: We used the first half of the in-
put stream, approximately 600k sentences and
15/16 million source/target words, as parallel
training data. We then translated each of the 10
test sets using the static model. This is the tradi-
tional approach and the coverage of the model
never changes.
2. Unbounded Space: Batch or incremental re-
training with no memory constraint. For each
epoch in the stream, we retrained the TM us-
ing all the data from the beginning of the in-
put stream until just before the present with re-
spect to a given test point. As more time passes
our training data set grows so each batch run
of GIZA++ takes more time. Overall this is the
most computationally expensive approach.
398
Baseline Unbounded Bounded
Epoch Test Date Test Sent. Train Sent. Rules Train Sent. Rules Train Sent. Rules
03 09/23/2002 1.0k 580k 4.0M 800k 5.0M 580k 4.2M
06 03/29/2004 1.5k 580k 5.0M 1.0M 7.0M 580k 5.5M
10 09/26/2006 3.5k 580k 8.5M 1.3M 14.0M 580k 10.0M
Table 2: Translation model statistics for example epochs and the next test dates grouped by experimental condition.
Test and Train Sent. is the number of sentence pairs in test and training data respectively. Rules is the count of unique
Hiero grammar rules extracted for the corresponding test set.
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1  2  3  4  5  6  7  8  9  10
D
el
ta
 in
 B
LE
U
 sc
or
es
epochs
unbounded
bounded
Figure 3: Static vs. online TM performance. Gains in
translation performance measured by BLEU are achieved
when recent German-English sentence pairs are auto-
matically incorporated into the TM. Shown are relative
BLEU improvements for the online models against the
static baseline.
3. Bounded Space: Batch and incremental re-
training with an enforced memory constraint.
Here we batch or incrementally retrain using
a sliding window approach where the training
set size (the number of sentence pairs) remains
constant. In particular, we ensured that we
used the same number of sentences as the base-
line. Each batch run of GIZA++ takes approxi-
mately the same time.
The time for aligning in the sOEM model is unaf-
fected by the bounded/unbounded conditions since
we always only align the mini-batch of sentences
encountered in the last epoch. In contrast, for batch
EM we must realign all the sentences in our training
set from scratch to incorporate the new training data.
Similarly space usage for the batch training grows
with the training set size. For sOEM, in theory mem-
ory used is with respect to vocabulary size (which
grows slowly with the stream size) since we retain
count history for the entire stream. To make space
usage truly constant, we filter for just the needed
word pairs in the current epoch being aligned. This
effectively means that online EM is more mem-
ory efficient than the batch version. As our exper-
iments will show, the sufficient statistics kept be-
tween epochs by sOEM benefits performance com-
pared to the batch models which can only use infor-
mation present within the batch itself.
4.4 Incremental Retraining Procedure
Our incremental adaptation procedure was as fol-
lows: after the latest mini-batch of sentences had
been aligned using sOEM we added all newly
aligned sentence pairs to the dynamic suffix ar-
rays. For the experiments where our memory was
bounded, we also deleted an equal number of sen-
tences from the suffix arrays before extracting the
Hiero grammar for the next test point. For the un-
bounded coverage experiments we deleted nothing
prior to grammar extraction. Table 2 presents statis-
tics for the number of training sentence pairs and
grammar rules extracted for each coverage condition
for various test points.
4.5 Results
Figure 3 shows the results of the static baseline
against both the unbounded and bounded online EM
models. We can see that both the online models
outperform the static baseline. On average the un-
constrained model that contains more sentence pairs
for rule extraction slightly outperforms the bounded
condition which uses less data per epoch. However,
the static baseline and the bounded models both use
the same number of sentence-pairs for TM training.
We see there is a clear gain by incorporating recent
sentence-pairs made available by the stream.
399
Static Baseline Retrained (Unbounded) Retrained (Bounded)
Test Date Batch Batch Online Batch Online
09/23/2002 26.10 26.60 26.43 26.19 26.40
03/29/2004 27.40 28.33 28.42 28.06 28.38
09/26/2006 28.56 29.74 29.75 29.73 29.80
Table 3: Sample BLEU results for all baseline and online EM model conditions. The static baseline is a traditional
model that is never retrained. The batch unbounded and batch bounded models incorporate new data from the stream
but retraining is slow and computationally expensive (best results are bolded). In contrast both unbounded and bounded
online models incrementally retrain only the mini-batch of new sentences collected from the incoming stream so
quickly adopt the new data (best results are italicized).
Table 3 gives results of the online models com-
pared to the batch retrained models. For presentation
clarity we show only a sample of the full set of ten
test points though all results follow the pattern that
using more aligned sentences to derive our gram-
mar set resulted in slightly better performance ver-
sus a restricted training set. However, for the same
coverage constraints not only do we achieve com-
parable performance to batch retrained models us-
ing the sOEM method of incremental adaptation, we
are able to align and adopt new data from the input
stream orders of magnitude quicker since we only
align the mini-batch of sentences collected from the
last epoch. In the bounded condition, not only do
we benefit from quicker adaptation, we also see that
sOEM models slightly outperform the batch based
models due to the online algorithm employing a
longer history of count-based evidence to draw on
when aligning new sentence pairs.
Figure 4 shows two example test sentences that
benefited from the online TM adaptation. Trans-
lations from the online model produce more and
longer matching phrases for both sentences (e.g.,
?creation of such a?, ?of the occupying forces?)
leading to more fluent output as well as the improve-
ments achieved in BLEU scores.
We experimented with a variety of interpolation
parameters (see Algorithm 2) but found no signifi-
cant difference between them (the biggest improve-
ment gained over all test points for all parameter set-
tings was less than 0.1% BLEU).
4.6 Increasing LM Coverage
A natural and interesting extension to the experi-
ments above is to use the target side of the incoming
stream to extend the LM coverage alongside the TM.
Test Date Static Unbounded Bounded
09/23/2002 26.46 27.11 26.96
03/29/2004 28.11 29.53 29.20
09/26/2006 29.53 30.94 30.88
Table 4: Unbounded LM coverage improvements. Shown
are the BLEU scores for each experimental conditional
when we allow the LM coverage to increase.
It is well known that more LM coverage (via larger
training data sets) is beneficial to SMT performance
(Brants et al, 2007) so we investigated whether re-
cency gains for the TM were additive with recency
gains afforded by a LM.
To test this we added all the target side data from
the beginning of the stream to the most recent epoch
into the LM training set before each test point. We
then batch retrained6 and used the new LM with
greater coverage for the next decoding run. Experi-
ments were for the static baseline and online models.
Results are reported in Table 4. We can see that
increasing LM coverage is complimentary to adapt-
ing the TM with recent data. Comparing Tables
3 and 4, for the bounded condition, adapting only
the TM achieved an absolute improvement of +1.24
BLEU over the static baseline for the final test point.
We get another absolute gain of +1.08 BLEU by al-
lowing the LM coverage to adapt as well. Using an
online, adaptive model gives a total gain of +2.32
BLEU over a static baseline that does not adapt.
6Although we batch retrain the LMs we could use an online
LM that incorporates new vocabulary from the input stream as
in Levenberg and Osborne (2009).
400
Static: The commission is prepared, in the creation of a legal framework, taking account of four fundamental principles them.
Online: The commission is prepared to participate in the creation of such a legal framework, based on four fundamental principles.
Reference: The commission is willing to cooperate in the creation of such a legal framework on the basis of four essential principles.
Source: Die Kommission ist bereit, an der Schaffung eines solchen Rechtsrahmens unter Zugrundelegung von vier wesentlichen 
              Prinzipien mitzuwirken.
Static:  Our position is clear and we all know: we are against the war and the occupation of Iraq by the United States and the United    
             Kingdom, and we are calling for the immediate withdrawal of the besatzungsm?chte from this country.
Online: Our position is clear and well known: we are against the war and the occupation of Iraq by the United States and the United
             Kingdom, and we demand the immediate withdrawal of the occupying forces from this country .
Reference: Our position is clear and well known: we are against the war and the US-British occupation in Iraq and we demand the
                   immediate withdrawal of the occupying forces from that country.
Source: Unser Standpunkt ist klar und allseits bekannt: Wir sind gegen den Krieg und die Besetzung des Irak durch die USA und das   
              Vereinigte K?nigreich, und wir verlangen den unverz?glichen Abzug der Besatzungsm?chte aus diesem Land.
Figure 4: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent
sentences. In both examples we get longer matching phrases in the online translation compared to the static one.
5 Related Work
5.1 Translation Model Domain Adaptation
Our work is related to domain adaptation for transla-
tion models. See, for example, Koehn and Schroeder
(2007) or Bertoldi and Federico (2009). Most tech-
niques center around using mixtures of translation
models. Once trained, these models generally never
change. They therefore fall under the batch training
regime. The focus of this work instead is on incre-
mental retraining and also on supporting bounded
memory consumption. Our experiments examine
updating model parameters in a single domain over
different periods in time. Naturally, we could also
use domain adaptation techniques to further improve
how we incorporate new samples.
5.2 Online EM for SMT
For stepwise online EM for SMT models, the only
prior work we are aware of is Liang and Klein
(2009), where variations of online EM were exper-
imented with on various NLP tasks including word
alignments. They showed application of sOEM can
produce quicker convergence compared to the batch
EM algorithm. However, the model presented does
not incorporate any unseen data, instead iterating
over a static data set multiple times using sOEM.
For Liang and Klein (2009) incremental retraining
is simply an alternative way to use a fixed training
set.
5.3 Streaming Language Models
Recent work in Levenberg and Osborne (2009) pre-
sented a streaming LM that was capable of adapt-
ing to an unbounded monolingual input stream in
constant space and time. The LM has the ability to
add or delete n-grams (and their counts) based on
feedback from the decoder after translation points.
The model was tested in an SMT setting and results
showed recent data benefited performance. How-
ever, adaptation was only to the LM and no tests
were conducted on the TM.
6 Conclusion and Future Work
We have presented an online EM approach for word
alignments. We have shown that, for a SMT system,
incorporating recent parallel data into a TM from an
input stream is beneficial to translation performance
compared to a traditional, static baseline.
Our strategy for populating the suffix array was
simply to use a first-in, first-out stack. For future
work we will investigate whether information pro-
vided by the incoming stream coupled with the feed-
back from the decoder allows for more sophisti-
cated adaptation strategies that reinforce useful word
alignments and delete bad or unused ones.
In the near future we also hope to test the online
EM setup in an application setting such as a com-
puter aided translation or crowdsourced generated
streams via Amazon?s Mechanical Turk.
401
Acknowledgements
Research supported by EuroMatrixPlus funded by
the European Commission, by the DARPA GALE
program under Contract Nos. HR0011-06-2-0001
and HR0011-06-C-0022, and the NSF under grant
IIS-0713448.
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In WMT09: Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 182?189, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 858?867.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 255?262, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Olivier Cappe and Eric Moulines. 2009. Online EM al-
gorithm for latent data models. Journal Of The Royal
Statistical Society Series B, 71:593.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39:1?38.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based machine translation. In WMT09: Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation, pages 135?139, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Percy Liang and Dan Klein. 2009. Online EM for unsu-
pervised models. In North American Association for
Computational Linguistics (NAACL).
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Coling 2008 Organizing Committee.
Udi Manber and Gene Myers. 1990. Suffix arrays:
A new method for on-line string searches. In The
First Annual ACM-SIAM Symposium on Dicrete Algo-
rithms, pages 319?327.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Ronald Rosenfeld. 1995. Optimizing lexical and n-gram
coverage via judicious use of linguistic data. In In
Proc. European Conf. on Speech Technology, pages
1763?1766.
Mikae?l Salson, Thierry Lecroq, Martine Le?onard, and
Laurent Mouchard. 2009. Dynamic extended suffix
arrays. Journal of Discrete Algorithms, March.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics, pages 836?841, Morristown,
NJ, USA. Association for Computational Linguistics.
402
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49?59,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Machine Translation of Arabic Dialects
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas,
Richard Schwartz, John Makhoul, Omar F. Zaidan?, Chris Callison-Burch?
Raytheon BBN Technologies, Cambridge MA
?Microsoft Research, Redmond WA
?Johns Hopkins University, Baltimore MD
Abstract
Arabic Dialects present many challenges for
machine translation, not least of which is the
lack of data resources. We use crowdsourc-
ing to cheaply and quickly build Levantine-
English and Egyptian-English parallel cor-
pora, consisting of 1.1M words and 380k
words, respectively. The dialectal sentences
are selected from a large corpus of Arabic web
text, and translated using Amazon?s Mechan-
ical Turk. We use this data to build Dialec-
tal Arabic MT systems, and find that small
amounts of dialectal data have a dramatic im-
pact on translation quality. When translating
Egyptian and Levantine test sets, our Dialec-
tal Arabic MT system performs 6.3 and 7.0
BLEU points higher than a Modern Standard
Arabic MT system trained on a 150M-word
Arabic-English parallel corpus.
1 Introduction
The Arabic language is a well-known example of
diglossia (Ferguson, 1959), where the formal vari-
ety of the language, which is taught in schools and
used in written communication and formal speech
(religion, politics, etc.) differs significantly in its
grammatical properties from the informal varieties
that are acquired natively, which are used mostly for
verbal communication. The spoken varieties of the
Arabic language (which we refer to collectively as
Dialectal Arabic) differ widely among themselves,
depending on the geographic distribution and the
socio-economic conditions of the speakers, and they
diverge from the formal variety known as Mod-
ern Standard Arabic (MSA) (Embarki and Ennaji,
2011). Significant differences in the phonology,
morphology, lexicon and even syntax render some
of these varieties mutually incomprehensible.
The use of Dialectal Arabic has traditionally been
confined to informal personal speech, while writ-
ing has been done almost exclusively using MSA
(or its ancestor Classical Arabic). This situation is
quickly changing, however, with the rapid prolifer-
ation of social media in the Arabic-speaking part
of the world, where much of the communication
is composed in dialect. The focus of the Arabic
NLP research community, which has been mostly on
MSA, is turning towards dealing with informal com-
munication, with the introduction of the DARPA
BOLT program. This new focus presents new chal-
lenges, the most obvious of which is the lack of di-
alectal linguistic resources. Dialectal text, which is
usually user-generated, is also noisy, and the lack
of standardized orthography means that users often
improvise spelling. Dialectal data also includes a
wider range of topics than formal data genres, such
as newswire, due to its informal nature. These chal-
lenges require innovative solutions if NLP applica-
tions are to deal with Dialectal Arabic effectively.
In this paper:
? We describe a process for cheaply and quickly
developing parallel corpora for Levantine-
English and Egyptian-English using Amazon?s
Mechanical Turk crowdsourcing service (?3).
? We use the data to perform a variety of machine
translation experiments showing the impact of
morphological analysis, the limited value of
adding MSA parallel data, the usefulness of
cross-dialect training, and the effects of trans-
lating from dialect to MSA to English (?4).
We find that collecting dialect translations has a low
cost ($0.03/word) and that relatively small amounts
of data has a dramatic impact on translation quality.
When trained on 1.5M words of dialectal data, our
system performs 6.3 to 7.0 BLEU points higher than
when it is trained on 100 times more MSA data from
a mismatching domain.
49
2 Previous Work
Existing work on natural language processing of Di-
alectal Arabic text, including machine translation, is
somewhat limited. Previous research on Dialectal
Arabic MT has focused on normalizing dialectal in-
put words into MSA equivalents before translating
to English, and they deal with inputs that contain
a limited fraction of dialectal words. Sawaf (2010)
normalized the dialectal words in a hybrid (rule-
based and statistical) MT system, by performing a
combination of character- and morpheme-level map-
pings. They then translated the normalized source
to English using a hybrid MT or alternatively a
Statistical MT system. They tested their method
on proprietary test sets, observing about 1 BLEU
point (Papineni et al, 2002) increase on broadcast
news/conversation and about 2 points on web text.
Salloum and Habash (2011) reduced the proportion
of dialectal out-of-vocabulary (OOV) words also by
mapping their affixed morphemes to MSA equiva-
lents (but did not perform lexical mapping on the
word stems). They allowed for multiple morpho-
logical analyses, passing them on to the MT system
in the form of a lattice. They tested on a subset of
broadcast news and broadcast conversation data sets
consisting of sentences that contain at least one re-
gion marked as non-MSA, with an initial OOV rate
against an MSA training corpus of 1.51%. They
obtained a 0.62 BLEU point gain. Abo Bakr et
al. (2008) suggested another hybrid system to map
Egyptian Arabic to MSA, using morphological anal-
ysis on the input and an Egyptian-MSA lexicon.
Other work that has focused on tasks besides MT
includes that of Chiang et al (2006), who built a
parser for spoken Levantine Arabic (LA) transcripts
using an MSA treebank. They used an LA-MSA
lexicon in addition to morphological and syntac-
tic rules to map the LA sentences to MSA. Riesa
and Yarowsky (2006) built a statistical morphologi-
cal segmenter for Iraqi and Levantine speech tran-
scripts, and showed that they outperformed rule-
based segmentation with small amounts of training.
Some tools exist for preprocessing and tokenizing
Arabic text with a focus on Dialectal Arabic. For ex-
ample, MAGEAD (Habash and Rambow, 2006) is a
morphological analyzer and generator that can ana-
lyze the surface form of MSA and dialect words into
their root/pattern and affixed morphemes, or gener-
ate the surface form in the opposite direction.
Amazon?s Mechanical Turk (MTurk) is becom-
ing an essential tool for creating annotated resources
for computational linguistics. Callison-Burch and
Dredze (2010) provide an overview of various tasks
for which MTurk has been used, and offer a set of
best practices for ensuring high-quality data.
Zaidan and Callison-Burch (2011a) studied the
quality of crowdsourced translations, by quantifying
the quality of non-professional English translations
of 2,000 Urdu sentences that were originally trans-
lated by the LDC. They demonstrated a variety of
mechanisms that increase the translation quality of
crowdsourced translations to near professional lev-
els, with a total cost that is less than one tenth the
cost of professional translation.
Zaidan and Callison-Burch (2011b) created the
Arabic Online Commentary (AOC) dataset, a 52M-
word monolingual dataset rich in dialectal content.
Over 100k sentences from the AOC were annotated
by native Arabic speakers on MTurk to identify the
dialect level (and dialect itself) in each, and the col-
lected labels were used to train automatic dialect
identification systems. Although a large number
of dialectal sentences were identified (41% of sen-
tences), none were passed on to a translation phase.
3 Data Collection and Annotation
Following Zaidan and Callison-Burch (2011a,b), we
use MTurk to identify Dialectal Arabic data and to
create a parallel corpus by hiring non-professional
translators to translate the sentences that were la-
beled as being dialectal. We had Turkers perform
three steps for us: dialect classification, sentence
segmentation, and translation.
Since Dialectal Arabic is much less common in
written form than in spoken form, the first challenge
is to simply find instances of written Dialectal Ara-
bic. We draw from a large corpus of monolingual
Arabic text (approximately 350M words) that was
harvested from the web by the LDC, largely from
weblog and online user groups.1 Before present-
ing our data to annotators, we filter it to identify
1Corpora: LDC2006E32, LDC2006E77, LDC2006E90,
LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41,
LDC2008E54, LDC2009E14, LDC2009E93.
50
M
ag
hr
eb
i
E
gy
Ir
aq
i
G
ul
f
Ot
he
r
L
ev
Figure 1: One possible breakdown of spoken Arabic into
dialect groups: Maghrebi, Egyptian, Levantine, Gulf and
Iraqi. Habash (2010) gives a breakdown along mostly
the same lines. We used this map as an illustration for
annotators in our dialect classification task (Section 3.1),
with Arabic names for the dialects instead of English.
segments most likely to be dialectal (unlike Zaidan
and Callison-Burch (2011b), who did no such pre-
filtering). We eliminate documents with a large per-
centage of non-Arabic or MSA words. We then
retain documents that contain some number of di-
alectal words, using a set of manually selected di-
alectal words that was assembled by culling through
the transcripts of the Levantine Fisher and Egyp-
tian CallHome speech corpora. After filtering, the
dataset contained around 4M words, which we used
as a starting point for creating our Dialectal Arabic-
English parallel corpus.
3.1 Dialect Classification
To refine the document set beyond our keyword fil-
tering heuristic and to label which dialect each doc-
ument is written in, we hire Arabic annotators on
MTurk to perform classification similar to Zaidan
and Callison-Burch (2011b). Annotators were asked
to classify the filtered documents for being in MSA
or in one of four regional dialects: Egyptian, Lev-
antine, Gulf/Iraqi or Maghrebi, and were shown the
map in Figure 1 to explain what regions each of the
dialect labels corresponded to. We allowed an addi-
tional ?General? dialect option for ambiguous docu-
ments. Unlike Zaidan and Callison-Burch, our clas-
sification was applied to whole documents (corre-
sponding to a user online posting) instead of individ-
ual sentences. To perform quality control, we used
a set of documents for which correct labels were
known. We presented these 20% of the time, and
Dialect Classification HIT $10,064
Sentence Segmentation HIT $1,940
Translation HIT $32,061
Total cost $44,065
Num words translated 1,516,856
Cost per word 2.9 cents/word
Table 1: The total costs for the three MTurk subtasks in-
volved with the creation of our Dialectal Arabic-English
parallel corpus.
eliminated workers who did not correctly classify
them (2% of labels).
Identifying the dialect of a text snippet can be
challenging in the absence of phonetic cues. We
therefore required 3 classifications from different
workers for every document, and accepted a dialect
label if at least two of them agreed. The dialect dis-
tribution of the final output was: 43% Gulf/Iraqi,
28% Levantine, 11% Egyptian, and 16% could not
be classified. MSA and the other labels accounted
for 2%. We decided to translate only the Levantine
and Egyptian documents, since the pool of MTurk
workers contained virtually no workers from Iraq or
the Gulf region.
3.2 Sentence Segmentation
Since the data we annotated was mostly user-
generated informal web content, the existing punc-
tuation was often insufficient to determine sentence
boundaries. Since sentence boundaries are impor-
tant for correct translation, we segmented passages
into individual sentences using MTurk. We only re-
quired sentences longer than 15 words to be seg-
mented, and allowed Turkers to split and rejoin at
any point between the tokens. The instructions were
simply to ?divide the Arabic text into individual sen-
tences, where you believe it would be appropriate
to insert a period.? We also used a set of correctly
segmented passages for quality control, and scored
Turkers using a metric based on the precision and
recall of correct segmentation points. The rejection
rate was 1.2%.
3.3 Translation to English
Following Zaidan and Callison-Burch (2011a), we
hired non-professional translators on MTurk to
translate the Levantine and Egyptian sentences into
51
Sentence Arabic English
Data Set Pairs Tokens Tokens
MSA-150MW 8.0M 151.4M 204.4M
Dialect-1500KW 180k 1,545,053 2,257,041
MSA-1300KW 71k 1,292,384 1,752,724
MSA-Web-Tune 6,163 145,260 184,185
MSA-Web-Test 5,454 136,396 172,357
Lev-Web-Tune 2,600 20,940 27,399
Lev-Web-Test 2,600 21,092 27,793
Egy-Web-Test 2,600 23,671 33,565
E-Facebook-Tune 3,351 25,130 34,753
E-Facebook-Test 3,188 25,011 34,244
Table 2: Statistics about the training/tuning/test datasets
used in our experiments. The token counts are calculated
before MADA segmentation.
English. Among several quality control measures,
we rendered the Arabic sentences as images to pre-
vent Turkers from simply copying the Arabic text
into translation software. We still spot checked the
translations against the output of Google Translate
and Bing Translator. We also rejected gobbledygook
garbage translations that have a high percentage of
words not found in an English lexicon.
We quantified the quality of an individual Turker?s
translations in two ways: first by asking native Ara-
bic speaker judges to score a sample of the Turker?s
translations, and second by inserting control sen-
tences for which we have good reference translations
and measuring the Turker?s METEOR (Banerjee and
Lavie, 2005) and BLEU-1 scores (Papineni et al,
2002).2 The rejection rate of translation assignments
was 5%. We promoted good translators to a re-
stricted access ?preferred worker queue?. They were
paid at a higher rate, and were required to translate
control passages only 10% of the time as opposed
to 20% for general Turkers, thus providing us with a
higher translation yield for unseen data.
Worker turnout was initially slow, but increased
quickly as our reputation for being reliable payers
was established; workers started translating larger
volumes and referring their acquaintances. We had
121 workers who each completed 20 or more trans-
lation assignments. We eventually reached and sus-
tained a rate of 200k words of acceptable quality
2BLEU-1 provided a more reliable correlation with human
judgment in this case that the regular BLEU score (which uses
n-gram orders 1, . . . , 4), given the limited size of the sample
measured.
translated per week. Unlike Zaidan and Callison-
Burch (2011a), who only translated 2,000 Urdu sen-
tences, we translated sufficient volumes of Dialectal
Arabic to train machine translation systems. In total,
we had 1.1M words of Levantine and 380k words of
Egyptian translated into English, corresponding to
about 2.3M words on the English side.
Table 1 outlines the costs involved with creating
our parallel corpus. The total cost was $44k, or
$0.03/word ? an order of magnitude cheaper than
professional translation.
4 Experiments in Dialectal Arabic-English
Machine Translation
We performed a set of experiments to contrast sys-
tems trained using our dialectal parallel corpus with
systems trained on a (much larger) MSA-English
parallel corpus. All experiments use the same meth-
ods for training, decoding and parameter tuning, and
we only varied the corpora used for training, tun-
ing and testing. The MT system we used is based
on a phrase-based hierarchical model similar to that
of Shen et al (2008). We used GIZA++ (Och and
Ney, 2003) to align sentences and extract hierar-
chical rules. The decoder used a log-linear model
that combines the scores of multiple feature scores,
including translation probabilities, smoothed lexi-
cal probabilities, a dependency tree language model,
in addition to a trigram English language model.
Additionally, we used 50,000 sparse, binary-valued
source and target features based on Chiang et al
(2009). The English language model was trained on
7 billion words from the Gigaword and from a web
crawl. The feature weights were tuned to maximize
the BLEU score on a tuning set using the Expected-
BLEU optimization procedure (Devlin, 2009).
The Dialectal Arabic side of our corpus consisted
of 1.5M words (1.1M Levantine and 380k Egyp-
tian). Table 2 gives statistics about the various
train/tune/test splits we used in our experiments.
Since the Egyptian set was so small, we split it only
to training/test sets, opting not to have a tuning set.
The MSA training data we used consisted of Arabic-
English corpora totaling 150M tokens (Arabic side).
The MSA train/tune/test sets were constructed for
the DARPA GALE program.
We report translation quality in terms of BLEU
52
Simple Segment MADA Segment
Training Tuning BLEU OOV BLEU OOV ?BLEU ?OOV
MSA-Web-Test
MSA-150MW MSA-Web 26.21 1.69% 27.85 0.48% +1.64 -1.21%
MSA-1300KW 21.24 7.20% 25.23 1.95% +3.99 -5.25%
Egyptian-Web-Test
Dialect-1500KW Levantine-Web 18.55 6.31% 20.66 2.85% +2.11 -3.46%
Levantine-Web-Test
Dialect-1500KW Levantine-Web 17.00 6.22% 19.29 2.96% +2.29 -3.26%
Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal
Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are
more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora.
Training Tuning BLEU OOV BLEU OOV BLEU OOV
Egyptian-Web-Test Levantine-Web-Test MSA-Web-Test
MSA-150MW MSA-Web 14.76 4.42% 11.83 5.53% 27.85 0.48%
MSA-150MW Lev-Web 14.34 4.42% 12.29 5.53% 24.63 0.48%
MSA-150MW+Dial-1500KW 20.09 2.04% 19.11 2.27% 24.30 0.45%
Dialect-1500KW 20.66 2.85% 19.29 2.96% 15.53 3.70%
Egyptian-360KW 19.04 4.62% 11.21 9.00% - -
Levantine-360KW 14.05 7.11% 16.36 5.24% - -
Levantine-1100KW 17.79 4.83% 19.29 3.31% - -
Table 4: A comparison of translation quality of Egyptian, Levantine, andMSAweb text, using various training corpora.
The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian),
since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the
dialectal data results in marginally worse translations.
score.3 In addition, we also report the OOV rate of
the test set relative to the training corpus in each ex-
perimental setups.
4.1 Morphological Decomposition
Arabic has a complex morphology compared to En-
glish. Preprocessing the Arabic source by morpho-
logical segmentation has been shown to improve the
performance of Arabic MT (Lee, 2004; Habash and
Sadat, 2006) by decreasing the size of the source vo-
cabulary, and improving the quality of word align-
ments. The morphological analyzers that underlie
most segmenters were developed for MSA, but the
different dialects of Arabic share many of the mor-
phological affixes of MSA, and it is therefore not
unreasonable to expect MSA segmentation to also
improve Dialect Arabic to English MT. To test this,
3We also computed TER (Snover et al, 2006) andMETEOR
scores, but omit them because they demonstrated similar trends.
we ran experiments using the MADA morpholog-
ical analyzer (Habash and Rambow, 2005). Table
3 shows the effect of applying segmentation to the
text, for both MSA and Dialectal Arabic. The BLEU
score improves uniformly, although the improve-
ments are most dramatic for smaller datasets, which
is consistent with previous work (Habash and Sadat,
2006). Morphological segmentation gives a smaller
gain on dialectal input, which could be due to two
factors: the segmentation accuracy likely decreases
since we are using an unmodified MSA segmenter,
and there is higher variability in the written form of
dialect compared to MSA. Given the significant, al-
beit smaller gain on dialectal input, we use MADA
segmentation in all our experiments.
4.2 Effect of Dialectal Training Data Size
We next examine how the size of the dialectal train-
ing data affects MT performance, and whether it is
useful to combine it with MSA training data. We
53
oh
 ti
me
 (s
pa
ce
 om
itt
ed
). 
Ap
pe
are
d w
ith
in
 a 
po
em
.
11
yA
zm
n

?
lik
e y
ou
 (c
or
ru
pti
on
 of
 M
SA
 m
vl
k)
.
10
m
tlk
"#
$
by
 m
uc
h (
co
rru
pti
on
 of
 M
SA
 bk
vy
r).
11
bk
ty
r
&'$
()
I m
iss
 yo
u (
sp
ok
en
 to
 a 
fe
ma
le)
 ?
Eg
yp
tia
n.
14
w
H
$t
yn
y
/0
'$1
2?
Th
e l
as
t n
am
e (
Al
-N
a'o
om
) o
f a
 fo
ru
m 
ad
mi
n.
16
A
ln
E
w
m
?:;
0<?
a l
oo
ot 
(c
or
ru
pti
on
 of
 M
SA
 kv
yr
A
).
17
kt
yy
yr
&''
'$?
rea
lly
/fo
r r
ea
l ?
Le
va
nti
ne
.
31
E
nj
d
DE
0F
En
gli
sh
 E
qu
iva
len
t
Co
un
t
TL
Ar
ab
ic
Table 5: The most frequent OOV?s (with counts ? 10) of the dialectal test sets against the MSA training data.
Source (EGY):  ? ? ??	
?   ? ! !
Transliteration: Ant btEml lh AElAn wlA Ayh?!!
MSA-Sys. Output: You are working for a declarationand not?
Dial-Sys. Output: You are making the advertisementfor him or what?
Reference: Are you promoting it or what?!!
Source (EGY):  01?. ??78 6 35 34? ?
 9:;? <=>
Transliteration: nfsY Atm}n Elyh bEd mA $Af
AlSwrh dy
MSA-Sys. Output: Myself feel to see this image.
Dial-Sys. Output: I wish to check on him afterhe saw this picture.
Reference: I wish to be sure that he is fineafter he saw this images
Source (LEV):  ?0??? E7770 ?F? G7H
Transliteration: lhyk Aljw ktyyyr kwwwl
MSA-Sys. Output: God you the atmosphere.
Dial-Sys. Output: this is why the weather is so cool
Reference: This is why the weather is so cool
Source (LEV):  ?L M
 G3 0?;
Transliteration: Twl bAlk Em nmzH
MSA-Sys. Output: Do you think about a joke long.
Dial-Sys. Output: Calm down we are kidding
Reference: calm down, we are kidding
Figure 2: Examples of improvement in MT output when
training on our Dialectal Arabic-English parallel corpus
instead of an MSA-English parallel corpus.
Source (EGY):   	
 	  ? 
Transliteration: qAltlp Tb tEAlY nEd ,
MSA-Sys. Output: Medicine almighty promise.
Dial-Sys. Output: She said, OK, come and then
Reference: She told him, OK, lets count them ,
Source (LEV):  "#$%& 
#'01 ?-%. ! -,%+? ?? ?2 
Transliteration: fbqrA w>HyAnA bqDyhA Em
>tslY mE rfqAty
MSA-Sys. Output: I read and sometimes with gowith my uncle.
Dial-Sys. Output: So I read, and sometimes I spendtrying to make my self comfortwith my friends
Reference: So i study and sometimes I spendthe time having fun with my friends
Source (LEV):  ?@ ?< ??' => +? &#:9? B:C12D E?
?? %$?+G 
Transliteration: Allh ysAmHkn hlq kl wAHd TAlb
qrb bykwn bdw Erws
MSA-Sys. Output: God now each student near the
Bedouin bride.
Dial-Sys. Output: God forgive you, each one is aclose student would want the bride
Reference: God forgive you. Is every oneasking to be close, want a bride!
Figure 3: Examples of ambiguous words that are trans-
lated incorrectly by the MSA-English system, but cor-
rectly by the Dialectal Arabic-English system.
54
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
%
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Egyptian web test
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Levantine web test
Figure 4: Learning curves showing the effects of increas-
ing the size of dialectal training data, when combined
with the 150M-word MSA parallel corpus, and when
used alone. Adding the MSA training data is only use-
ful when the dialectal data is scarce (200k words).
started with a baseline system trained on the 150M-
word MSA parallel corpus, and added various sized
portions of the dialect parallel corpus to it. Figure 4
shows the resulting learning curve, and compares it
to the learning curve for a system trained solely on
the dialectal parallel corpus. When only 200k words
of dialectal data are available, combining it with the
150M-word MSA corpus results in improved BLEU
scores, adding 0.8?1.5 BLEU points. When 400k
words or more of dialectal data are available, the
MSA training data ceases to provide any gain, and
in fact starts to hurt the performance.
The performance of a system trained on the 1.5M-
word dialectal data is dramatically superior to a sys-
tem that uses only the 150M-word MSA data: +6.32
BLEU points on the Egyptian test set, or 44% rela-
tive gain, and +7.00 BLEU points on the Levantine
test set, or 57% relative gain (fourth line vs. second
line of Table 4). In Section 4.4, we show that those
gains are not an artifact of the similarity between test
and training datasets, or of using the same translator
pool to translate both sets.
Inspecting the difference in the outputs of the Di-
alectal vs. MSA systems, we see that the improve-
ment in score is a reflection of a significant improve-
ment in the quality of translations. Figure 2 shows
a few examples of sentences whose translations im-
prove significantly using the Dialectal system. Fig-
ure 3 shows a particularly interesting category of ex-
amples. Many words are homographs, with different
meanings (and usually different pronunciations) in
MSA vs. one or more dialects. The bolded tokens
in the sentences in Figure 3 are examples of such
words. They are translated incorrectly by the MSA
system, while the dialect system translates them cor-
rectly.4 If we examine the most frequent OOVwords
against the MSA training data (Table 5), we find a
number of corrupted MSA words and names, but
that a majority of OOVs are dialect words.
4.3 Cross-Dialect Training
Since MSA training data appeared to have little ef-
fect when translating dialectal input, we next inves-
tigated the effect of training data from one dialect on
translating the input of another dialect. We trained a
system with the 360k-word Egyptian training subset
of our dialectal parallel corpus, and another system
with a similar amount of Levantine training data. We
used each system to translate the test set of the other
dialect. As expected, a system performs better when
it translates a test set in the same dialect that it was
trained on (Table 4).
That said, since the Egyptian training set is so
small, adding the (full) Levantine training data im-
proves performance (on the Egyptian test set) by
1.62 BLEU points, compared to using only Egyp-
tian training data. In fact, using the Levantine
training data by itself outperforms the MSA-trained
system on the Egyptian test set by more than 3
BLEU points. (For the Levantine test set, adding
the Egyptian training data has no affect, possibly
due to the small amount of Egyptian data.) This
may suggest that the mismatch between dialects is
less severe than the mismatch between MSA and
dialects. Alternatively, the differences may be due
to the changes in genre from the MSA parallel cor-
pus (which is mainly formal newswire) to the news-
groups and weblogs that mainly comprise the dialec-
tal corpus.
4The word nfsY of Figure 2 (first word of second example)
is also a homograph, as it means myself in MSA and I wish in
Dialectal Arabic.
55
Training Tuning BLEU OOV
MSA-150MW Levantine-Web 13.80 4.16%
MSA-150MW+Dialect-1500KW 16.71 2.43%
Dialect-1500KW 15.75 3.79%
MSA-150MW Egyptian-Facebook 15.80 4.16%
MSA-150MW+Dialect-1500KW 18.50 2.43%
Dialect-1500KW 17.90 3.79%
Dialect-1000KW (random selection) Egyptian-Facebook 17.09 4.64%
Dialect-1000KW (no Turker overlap) 17.10 4.60%
Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are
entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:
+2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
4.4 Validation on Independent Test Data
To eliminate the possibility that the gains are solely
due to similarity between the test/training sets in the
dialectal data, we ran experiments using the same
dialectal training data, but using truly independent
test/tuning data sets selected at random from a larger
set of monolingual data that we collected from pub-
lic Egyptian Facebook pages. This data consists of
a set of original user postings and the subsequent
comments on each, giving the data a more conversa-
tional style than our other test sets. The postings
deal with current Egyptian political affairs, sports
and other topics. The test set we selected consisted
of 25,011 words (3,188 comments and 427 postings
from 86 pages), and the tuning set contained 25,130
words (3,351 comments and 415 conversations from
58 pages). We obtained reference translations for
those using MTurk as well.
Table 6 shows that using the 1.5M-word dialect
parallel corpus for training yields a 2 point BLEU
improvement over using the 150M-word MSA cor-
pus. Adding the MSA training data does yield an
improvement, though of less than a single BLEU
point. It remains true that training on 1.5M words
of dialectal data is better than training on 100 times
more MSA parallel data. The system performance
is sensitive to the tuning set choice, and improves
when it matches the test set in genre and origin.
To eliminate another potential source of artificial
bias, we also performed an experiment where we
removed any training translation contributed by a
Turker who translated any sentence in the Egyptian
Facebook set, to eliminate translator bias. For this,
we were left with 1M words of dialect training data.
This gave the same BLEU score as when training
with a randomly selected subset of the same size
(bottom part of Table 6).
4.5 Mapping from Dialectal Arabic to MSA
Before Translating to English
Given the large amount of linguistic resources that
have been developed for MSA over the past years,
and the extensive research that was conducted on
machine translation from MSA to English and other
languages, an obvious research question is whether
Dialectal Arabic is best translated to English by first
pivoting through MSA, rather than directly. The
proximity of Dialectal Arabic to MSA makes the
mapping in principle easier than general machine
translation, and a number of researchers have ex-
plored this direction (Salloum and Habash, 2011).
In this scenario, the dialectal source would first be
automatically transformed to MSA, using either a
rule-based or statistical mapping module.
The Dialectal Arabic-English parallel corpus we
created presents a unique opportunity to compare
the MSA-pivoting approach against direct transla-
tion. First, we collected equivalent MSA data for
the Levantine Web test and tuning sets, by asking
Turkers to transform dialectal passages to valid and
fluent MSA. Turkers were shown example transfor-
mations, and we encouraged fewer changes where
applicable (e.g. morphological rather than lexical
mapping), but allowed any editing operation in gen-
eral (deletion, substitution, reordering). Sample sub-
missions were independently shown to native Ara-
bic speaking judges, who confirmed they were valid
MSA. A lowOOV rate also indicated the correctness
of the mappings. By manually transforming the test
56
Training BLEU OOV BLEU OOV ?BLEU ?OOV
Direct dialect trans Map to MSA then trans
MSA-150MW 12.29 5.53% 14.59 1.53% +2.30 -4.00%
MSA-150MW+Dialect-200KW 15.37 3.59% 15.53 1.22% +0.16 -2.37%
MSA-150MW+Dialect-400KW 16.62 3.06% 16.25 1.13% -0.37 -1.93%
MSA-150MW+Dialect-800KW 17.83 2.63% 16.69 1.04% -1.14 -1.59%
MSA-150MW+Dialect-1500KW 19.11 2.27% 17.20 0.98% -1.91 -1.29%
Table 7: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English,
versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it
is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system,
the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.
dialectal sentence into MSA, we establish an opti-
mistic estimate of what could be done automatically.
Table 7 compares direct translation versus piv-
oting to MSA before translating, using the base-
line MSA-English MT system.5 The performance
of the system improves by 2.3 BLEU points with
dialect-to-MSA pivoting, compared to attempting to
translate the untransformed dialectal input directly.
As we add more dialectal training data, the BLEU
score when translating the untransformed dialect
test set improves rapidly (as seen previously in the
MSA+Dialect learning curve in Figure 4), while the
improvement is less rapid when the text is first trans-
formed to MSA. Direct translation becomes a better
option than mapping to MSA once 400k words of di-
alectal data are added, despite the significantly lower
OOV rate with MSA-mapping. This indicates that
simple vocabulary coverage is not sufficient, and
data domain mismatch, quantified by more complex
matching patterns, is more important.
5 Conclusion
We have described a process for building a Dialec-
tal Arabic-English parallel corpus, by selecting pas-
sages with a relatively high percentage of non-MSA
words from a monolingual Arabic web text corpus,
then using crowdsourcing to classify them by di-
alect, segment them into individual sentences and
translate them to English. The process was success-
fully scaled to the point of reaching and sustaining a
rate of 200k translated words per week, at 1/10 the
cost of professional translation. Our parallel corpus,
consisting of 1.5M words, was produced at a total
5The systems in each column of the table are tuned consis-
tently, using their corresponding tuning sets.
cost of $40k, or roughly $0.03/word.
We used the parallel corpus we constructed to
analyze the behavior of a Dialectal Arabic-English
MT system as a function of the size of the dialec-
tal training corpus. We showed that relatively small
amounts of training data render larger MSA corpora
from different data genres largely ineffective for this
test data. In practice, a system trained on the com-
bined Dialectal-MSA data is likely to give the best
performance, since informal Arabic data is usually
a mixture of Dialectal Arabic and MSA. An area of
future research is using the output of a dialect clas-
sifier, or other features to bias the translation model
towards the Dialectal or the MSA parts of the data.
We also validated the models built from the di-
alectal corpus by using them to translate an inde-
pendent data set collected from Egyptian Facebook
public pages. We finally investigated using MSA
as a ?pivot language? for Dialectal Arabic-English
translation, by simulating automatic dialect-to-MSA
mapping using MTurk. We obtained limited gains
from mapping the input to MSA, even when the
mapping is of good quality, and only at lower train-
ing set sizes. This suggests that the mismatch be-
tween training and test data is an important aspect of
the problem, beyond simple vocabulary coverage.
The aim of this paper is to contribute to setting
the direction of future research on Dialectal Arabic
MT. The gains we observed from using MSA mor-
phological segmentation can be further increased
with dialect-specific segmenters. Input preprocess-
ing can also be used to decrease the noise of the
user-generated data. Topic adaptation is another im-
portant problem to tackle if the large MSA linguistic
resources already developed are to be leveraged for
Dialectal Arabic-English MT.
57
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program, and in part by the EuroMatrixPlus project
funded by the European Commission (7th Frame-
work Programme). The views expressed are those
of the authors and do not reflect the official policy
or position of the Department of Defense or the U.S.
Government. Distribution Statement A (Approved
for Public Release, Distribution Unlimited).
References
Hitham M. Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for converting writ-
ten Egyptian colloquial dialect into diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008, Cairo, Egypt.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgments. In In Proc. of ACL
2005 Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, Ann Arbor,
Michigan.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Trento, Italy.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ?09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land, December.
Mohamed Embarki and Moha Ennaji, editors. 2011.
Modern Trends in Arabic Dialectology. The Red Sea
Press.
Charles A. Ferguson. 1959. Diglossia. Word, 15:325?
340.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Ann Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD: A
morphological analyzer and generator for the Arabic
dialects. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics (ACL),
Sydney, Australia.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the 2006 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics, New York,
New York.
Nizar Y. Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In HLT-NAACL ?04:
Proceedings of HLT-NAACL 2004, Boston, Mas-
sachusetts.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with applica-
tions to machine translation. In Proceedings of the 7th
Conf. of the Association for Machine Translation in the
Americas (AMTA 2006), Cambridge, MA.
Wael Salloum and Nizar Habash. 2011. Dialectal to stan-
dard Arabic paraphrasing to improve Arabic-English
statistical machine translation. In Proceedings of the
2011 Conference of Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Conf. of
the Association for Machine Translation in the Ameri-
cas (AMTA 2010), Denver, Colorado.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and Ralph Weischedel. 2006. A study of
translation error rate with targeted human annotation.
In Proceedings of the 7th Conf. of the Association for
Machine Translation in the Americas (AMTA 2006),
pages 223?231, Cambridge, MA.
58
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37?41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, June.
59
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621?625,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Expectations of Word Sense in Parallel Corpora
Xuchen Yao, Benjamin Van Durme and Chris Callison-Burch
Center for Language and Speech Processing, and HLTCOE
Johns Hopkins University
Abstract
Given a parallel corpus, if two distinct words
in language A, a1 and a2, are aligned to the
same word b1 in language B, then this might
signal that b1 is polysemous, or it might sig-
nal a1 and a2 are synonyms. Both assump-
tions with successful work have been put for-
ward in the literature. We investigate these
assumptions, along with other questions of
word sense, by looking at sampled parallel
sentences containing tokens of the same type
in English, asking how often they mean the
same thing when they are: 1. aligned to the
same foreign type; and 2. aligned to different
foreign types. Results for French-English and
Chinese-English parallel corpora show simi-
lar behavior: Synonymy is only very weakly
the more prevalent scenario, where both cases
regularly occur.
1 Introduction
Parallel corpora have been used for both paraphrase
induction and word sense disambiguation (WSD).
Usually one of the following two assumptions is
made for these tasks:
1. Polysemy If two different words in language
A are aligned to the same word in language B,
then the word in language B is polysemous.
2. Synonymy If two different words in language
A are aligned to the same word in language B,
then the two words in A are synonyms, and thus
is not evidence of polysemy in B.
Despite the alternate nature of these assumptions,
both have associated articles in which a researcher
claimed success. Under the polysemy assumption,
Gale et al (1992) used French translations as En-
glish sense indicators in the task of WSD. For in-
stance, for the English word duty, the French transla-
tion droit was taken to signal its tax sense and devoir
to signal its obligation sense. These French words
were used as labels for different English senses.
Similarly, in a cross-lingual WSD setting,1 Lefever
et al (2011) treated each English-foreign alignment
as a so-called ParaSense, using it as a proxy for hu-
man labeled training data.
Under the synonymy assumption, Diab and
Resnik (2002) did word sense tagging by grouping
together all English words that are translated into
the same French word and by further enforcing that
the majority sense for these English words was pro-
jected as the sense for the French word. Bannard and
Callison-Burch (2005) applied the idea that French
phrases aligned to the same English phrase are para-
phrases in a system that induces paraphrases by piv-
oting through aligned foreign phrases.
Based on this, and other successful prior work,
it seems neither of the assumptions must hold
universally. Therefore we investigate how often
we might expect one or the other to dominate:
we sample polysemous words from wide-domain
{French,Chinese}-English corpora, and use Ama-
zon?s Mechanical Turk (MTurk) to annotate word
sense on the English side. We calculate empirical
probabilities based on counting over the competing
polysemous and synonymous scenario labels.
A key factor deciding the validity of our conclu-
sion is the reliability of the annotations derived via
MTurk. Thus our first step is to evaluate the abil-
ity of Turkers to perform WSD. After verifying this
1E.g., given a sentence ?... more power, more duty ...?, the
task asks to give a French translation of duty, which should be
devior, after first recognizing the underlying obligation sense.
621
as a reasonable process for acquiring large amounts
of WSD labeled data, we go on to frame the experi-
mental design, giving final results in Sec. 4.
2 Turker Reliability
While Amazon?s Mechanical Turk (MTurk) has
been been considered in the past for constructing
lexical semantic resources (e.g., (Snow et al, 2008;
Akkaya et al, 2010; Parent and Eskenazi, 2010;
Rumshisky, 2011)), word sense annotation is sensi-
tive to subjectivity and usually achieves low agree-
ment rate even among experts. Thus we first asked
Turkers to re-annotate a sample of existing gold-
standard data. With an eye towards costs saving, we
also considered how many Turkers would be needed
per item to produce results of sufficient quality.
Turkers were presented sentences from the test
portion of the word sense induction task of
SemEval-2007 (Agirre and Soroa, 2007), covering
2,559 instances of 35 nouns, expert-annotated with
OntoNotes (Hovy et al, 2006) senses. Two versions
of the task were designed:
1. compare: given the same word in different
sentences, tell whether their meaning is THE
SAME, ALMOST THE SAME, UNLIKELY THE
SAME or DIFFERENT, where the results were
collapsed post-hoc into a binary same/different
categorization;
2. sense map: map the meaning of a given word
in a sentential context to its proper OntoNotes
definition.
For both tasks, 2, 599 examples were presented.
We measure inter-coder agreement using Krip-
pendorff?s Alpha (Krippendorff, 2004; Artstein and
Poesio, 2008), where ? ? 0.8 is considered to be
reliable and 0.667 ? ? < 0.8 allows for tenta-
tive conclusions. Two points emerge from Table 1:
there were greater agreement rates for sense map
than compare, and 3 Turkers were sufficient.
3 Experiment Design
Data Selection We used two parallel corpora: the
French-English 109 corpus (Callison-Burch et al,
2009) and the GALE Chinese-English corpus.
?-Turker ?-maj. maj.-agr.
compare5 0.47 0.66 0.87
compare3 0.44 0.52 0.83
sense map5 0.79 0.93 0.95
sense map3 0.75 0.87 0.91
Table 1: MTurk result on testing Turker reliability. Krip-
pendorff?s Alpha is used to measure agreement. ?-
Turker: how Turkers agree among themselves, ?-maj.:
how the majority agrees with true value, maj.-agr.: agree-
ment between the majority vote and true value. ?-maj.
indicates the confidence level about the maj.-agr. value.
Subscripts denote either 5 Turkers, or 3 randomly se-
lected of the 5.
For each corpus we selected 50 words, w, at ran-
dom from OntoNotes,2 constrained such that w: had
more than one sense; had a frequency ? 1, 000; and
was not a top 10% most frequent words.
Next we sampled 100 instances (aligned English-
foreign sentence pairs) for each word based on the
following constraints: the aligned foreign word, f ,
had a frequency ? 20 in the foreign corpus; f had a
non-trivial alignment probability.3 We sampled pro-
portionally to the distribution of the aligned foreign
words, ensuring that at least 5 instances from each
foreign translation are sampled.4
For each corpus, this results in 100 instances for
each of 50 words, totaling 5,000 instances. We used
3 Turkers per instance for sense annotation, under
the sense map task. We note that the set of 50
randomly selected English words from the Chinese-
English corpus were entirely distinct from the 50 se-
lected words from the French-English corpus.
Probability Estimation Suppose e1 and e2 are
two tokens of the same English word type e. s(e1)
is a function that returns the sense of e1, a(e1) is
a function that returns the aligned word of e1. Let
c() be our count function, where: c(e, f) returns the
2OntoNotes was used as the sense inventory over alterna-
tives, owing to its coarse-grained sense definitions.
3Defined as f having index i < k when foreign words are
ranked by most probable given e, where k is the minimum value
such that
?k
i p(fi | e) > 0.8. E.g., if we have decreasing
probabilities p(droit | duty) = 0.6, p(devoir | duty) =
0.25, p(le | duty) = 0.03, ... then only consider droit and
devoir. This ruled out many noisy alignments.
4Thus, the instances of droit compared to that of devoir
would be 0.6/0.25.
622
number of times English word e is aligned to foreign
word f ; c(es, f) returns the number of times En-
glish word e has sense s (tagged by Turkers), when
aligned to foreign word f ; c(e) is the total number
of tokens of English word e; and c(es) is the number
of tokens of e with sense s.
We estimate from labeled data the probability of
three scenarios, with scenario 1 as our primary con-
cern: when two English words of the same poly-
semous type are aligned to different foreign word
types, what is the chance that they have the same
sense? Given the tokens e1 and e2, we calculate P1
as follows:
P1e = P (s(e1) = s(e2) | a(e1) 6= a(e2))
?
?
s c
2(es)?
?
s,f c
2(es, f)
c2(e)?
?
f c2(e, f)
P1 says that given two words of the same type
(e1 and e2) that are not aligned to the same foreign
word type (a(e1) 6= a(e2)), what is the probabil-
ity that they have the same sense (s(e1) = s(e2)).
We approach this estimation combinatorially. For
instance, the number of ways to choose two words
of the same type is
( 2
c(e)
)
? 12c
2(e) when c(e) is
large.
A large value of P1 would be in support of Syn-
onymy, as the two foreign aligned words of distinct
type would have the same meaning.
Scenario 2 asks: given two English words of
the same polysemous type and aligned to the same
words (a(e1) = a(e2)), what is the probability that
they have the same sense (s(e1) = s(e2))?
P2e = P (s(e1) = s(e2) | a(e1) = a(e2))
?
?
s,f c
2(es, f)
?
f c2(e, f)
Finally, what is the probability of two tokens of
the same polysemous type agreeing when alignment
information is not known (e.g., without a parallel
corpus)?
P3e = P (s(e1) = s(e2)) ?
?
s c
2(es)
c2(e)
All the above equations are given per English word
type e. In later sections we report the average values
over multiple word types and their counts.
4 Results
Turker Experiments To minimize errors from
Turkers, for every HIT we inserted one control
sentence taken from the example sentences of
OntoNotes. Turker results with either extremely low
finishing time (<10s), or average accuracy on con-
trol sentences lower than accuracy by chance, were
rejected. On average Turkers took 185 seconds to
map 10 sentences in a HIT to their OntoNotes def-
inition, receiving $0.10 per HIT. The total time for
annotating 5000 sentences was 22 hours.
Turkers had no knowledge about alignments: we
hid the aligned French/Chinese sentences from them
and these sentences were later processed to compute
P1/2/3 values. Two foreign tokens aligned with the
same source type correspond to two senses of the
same type. To give an estimate of alignment errors,
we manually examined 1/10 of all 5000 sampled
Chinese-English alignments at random and found
only 3 of them were wrong: all due to that English
content words were aligned to common Chinese
function words. This error rate is much lower than
that typically reported by alignment tools. The main
reason is explained in footnote 3: foreign words with
trivial alignment probability were removed before
calculating P1/2/3 values. Thus we believe the align-
ment was reliable.
Probability Estimation Table 2 gives the dis-
tribution of senses and word types in the sam-
pled words. Take the second numeric column of
French-English as an example: out of 50 words ran-
domly sampled, 9 have 2 distinct sense definitions
in OntoNotes. However, 17 of 50 unique word types
had exactly 2 distinct senses annotated, out of the
100 examples of a given word type: 17 words had
2 distinct senses observed. Of the 9 words with 2
official senses, on average 1.9 of those senses were
observed.
Table 3 and Figures 1 and 2 shows the result for
P1, P2 and P3 using the {French,Chinese}-English
corpora, calculated based on the majority vote of
three Turkers. High P2 values suggests that for two
tokens of the same type, aligning to the same for-
eign type is a reasonable indicator of having the
same meaning. When working with open domain
corpora, without foreign alignments, the probabil-
ity of two English words of the same type having
623
French-English Chinese-English
#senses in OntoNotes 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 18
#types in OntoNotes 0 9 7 6 8 6 2 8 4 0 10 6 11 3 8 6 4 1 1
#types observed 2 17 9 4 7 7 4 0 0 3 19 9 12 5 2 0 0 0 0
avg #senses observed 0 1.9 2.1 3.2 3.8 4.7 6.5 4.9 5.8 0 1.9 2.2 2.9 2.7 4.4 3.8 3.8 3.0 5.0
Table 2: Statistics for words sampled from parallel corpora. Average #senses observed over all words: 2.6 (French-
English), and 2.4 (Chinese-English). The sampled word keep has 18 senses in OntoNotes, with 5 observed.
P1 P2 P3 Alpha
French-English 51.2% 66.7% 59.2% 0.70
Chinese-English 59.6% 78.7% 66.7% 0.68
Table 3: Expectations of word sense in parallel corpora.
Alpha measures how Turkers agreed with themselves.
identical meaning is estimated here to be roughly
59-67% (59.2% (French), 66.7% (Chinese)). This
accords with results from WSD evaluations, where
the first-sense heuristic is roughly 75-80% accu-
rate (e.g., 80.9% in SemEval?07 (Brody and Lap-
ata, 2009)). Minor algebra translates this into an ex-
pected P3 value in a range from 56%?62.5%, up to
64%? 68%, which captures our estimates.5
Finally for our motivating scenario: values for P1
are barely higher than 50%, suggesting that Syn-
onymy more regularly holds, but not conclusively.
We expect in narrower domains, where words have
less number of senses, this is more noticeable. As
suggested by Fig.s 1 and 2, less polysemous words
tend to have higher P values.
5 Conclusion
Curious as to the distinct threads of prior work based
on alternate assumptions of word sense and parallel
corpora, we derived empirical expectations on the
shared meaning of tokens of the same type appear-
ing in the same corpus. Our results suggest neither
the assumption of Polysemy nor Synonymy holds
significantly more often than the other, at least for
individual words (as opposed to phrases) and for the
open domain corpora used here. Further, we provide
an independent data point that supports earlier find-
ings as to the expected accuracy of the first sense
heuristic in word sense disambiguation.
5Assuming worst case: no two tokens that are not the first
sense ever match, and best case: any two tokens not the first
sense always match, then assuming first-sense accuracy of 0.8
gives a range on P3 of: (0.82, 0.82 + 0.22) = (0.64, 0.68).
Num.Senses.Observed
Prob
abilit
y
0.0
0.2
0.4
0.6
0.8
1.0
l
l
l
l
l
1 2 3 4 5 6 7
TypeP1 P2 P3
Figure 1: French-English values, by number of senses.
Num.Senses.Observed
Prob
abilit
y
0.2
0.4
0.6
0.8
1.0
l
l
1 2 3 4 5 6 7
TypeP1 P2 P3
Figure 2: Chinese-English values, by number of senses.
624
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
Task 02: Evaluating Word Sense Induction And Dis-
crimination Systems. In Proc. SemEval ?07.
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk
for subjectivity word sense disambiguation. In Proc.
NAACL Workshop on CSLDAMT.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4).
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. ACL.
Samuel Brody and Mirella Lapata. 2009. Bayesian Word
Sense Induction. In Proc. EACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings Of The 2009
Workshop On Statistical Machine Translation. In
Proc. StatMT.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proc. ACL.
W.A. Gale, K.W. Church, and D. Yarowsky. 1992. Using
bilingual materials to develop word sense disambigua-
tion methods. In Proc. TMI.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proc. NAACL-Short.
Klaus H. Krippendorff. 2004. Content Analysis: An In-
troduction to Its Methodology.
Els Lefever, Ve?ronique Hoste, and Martine De Cock.
2011. Parasense or how to use parallel corpora for
word sense disambiguation. In Proc. ACL.
Gabriel Parent and Maxine Eskenazi. 2010. Clustering
dictionary definitions using amazon mechanical turk.
In Proc. NAACL Workshop on CSLDAMT.
Anna Rumshisky. 2011. Crowdsourcing word sense def-
inition. In Proc. LAW V.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proc. EMNLP.
625
Proceedings of NAACL-HLT 2013, pages 518?523,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Supervised Bilingual Lexicon Induction with Multiple Monolingual SignalsAnn Irvine
Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch?
Computer and Information Science Dept.
University of PennsylvaniaAbstract
Prior research into learning translations from
source and target language monolingual texts
has treated the task as an unsupervised learn-
ing problem. Although many techniques take
advantage of a seed bilingual lexicon, this
work is the first to use that data for super-
vised learning to combine a diverse set of sig-
nals derived from a pair of monolingual cor-
pora into a single discriminative model. Even
in a low resource machine translation setting,
where induced translations have the potential
to improve performance substantially, it is rea-
sonable to assume access to some amount of
data to perform this kind of optimization. Our
work shows that only a few hundred transla-
tion pairs are needed to achieve strong per-
formance on the bilingual lexicon induction
task, and our approach yields an average rel-
ative gain in accuracy of nearly 50% over an
unsupervised baseline. Large gains in accu-
racy hold for all 22 languages (low and high
resource) that we investigate.1 Introduction
Bilingual lexicon induction is the task of identifying
word translation pairs using source and target mono-
lingual corpora, which are often comparable. Most
approaches to the task are based on the idea that
words that are translations of one another have sim-
ilar distributional properties across languages. Prior
research has shown that contextual similarity (Rapp,
1995), temporal similarity (Schafer and Yarowsky,
2002), and topical information (Mimno et al, 2009)
?Performed while faculty at Johns Hopkins University
are all good signals for learning translations from
monolingual texts.
Most prior work either makes use of only one or
two monolingual signals or uses unsupervised meth-
ods (like rank combination) to aggregate orthogonal
signals (Schafer and Yarowsky, 2002; Klementiev
and Roth, 2006). Surprisingly, no past research has
employed supervised approaches to combine diverse
monolingually-derived signals for bilingual lexicon
induction. The field of machine learning has shown
decisively that supervised models dramatically out-
perform unsupervised models, including for closely
related problems like statistical machine translation
(Och and Ney, 2002).
For the bilingual lexicon induction task, a super-
vised approach is natural, particularly because com-
puting contextual similarity typically requires a seed
bilingual dictionary (Rapp, 1995), and that same
dictionary may be used for estimating the param-
eters of a model to combine monolingual signals.
Alternatively, in a low resource machine transla-
tion (MT) setting, it is reasonable to assume a small
amount of parallel data from which a bilingual dic-
tionary can be extracted for supervision. In this set-
ting, bilingual lexicon induction is critical for trans-
lating source words which do not appear in the par-
allel data or dictionary.
We frame bilingual lexicon induction as a binary
classification problem; for a pair of source and tar-
get language words, we predict whether the two are
translations of one another or not. For a given source
language word, we score all target language can-
didates separately and then rerank them. We use
a variety of signals derived from source and target
518
monolingual corpora as features and use supervision
to estimate the strength of each. In this work we:
? Use the following similarity metrics derived
from monolingual corpora to score word pairs:
contextual, temporal, topical, orthographic, and
frequency.
? For the first time, explore using supervision to
combine monolingual signals and learn a dis-
criminative model for predicting translations.
? Present results for 22 low and high resource
languages paired with English and show large
accuracy gains over an unsupervised baseline.2 Previous Work
Prior work suggests that a wide variety of mono-
lingual signals, including distributional, temporal,
topic, and string similarity, may inform bilingual
lexicon induction (Rapp, 1995; Fung and Yee, 1998;
Rapp, 1999; Schafer and Yarowsky, 2002; Schafer,
2006; Klementiev and Roth, 2006; Koehn and
Knight, 2002; Haghighi et al, 2008; Mimno et
al., 2009; Mausam et al, 2010). Klementiev et al
(2012) use many of those signals to score an exist-
ing phrase table for end-to-end MT but do not learn
any new translations. Schafer and Yarowsky (2002)
use an unsupervised rank-combination method for
combining orthographic, contextual, temporal, and
frequency similarities into a single ranking.
Recently, Ravi and Knight (2011), Dou and
Knight (2012), and Nuhn et al (2012) have worked
toward learning a phrase-based translation model
from monolingual corpora, relying on decipherment
techniques. In contrast to that work, we use a
seed bilingual lexicon for supervision and multiple
monolingual signals proposed in prior work.
Haghighi et al (2008) and Daume? and Jagarla-
mudi (2011) use some supervision to learn how to
project contextual and orthographic features into a
low-dimensional space, with the goal of represent-
ing words which are translations of one another
as vectors which are close together in that space.
However, both of those approaches focus on only
two signals, high resource languages, and frequent
words (frequent nouns, in the case of Haghighi et
al. (2008)). In our classification framework, we can
incorporate any number of monolingual signals, in-
Language #Words Language #Words
Nepali 0.4 Somali 0.5
Uzbek 1.4 Azeri 2.6
Tamil 3.7 Albanian 6.5
Bengali 6.6 Welsh 7.5
Bosnian 12.9 Latvian 40.2
Indonesian 21.8 Romanian 24.1
Serbian 25.8 Turkish 31.2
Ukrainian 37.6 Hindi 47.4
Bulgarian 49.5 Polish 104.5
Slovak 124.3 Urdu 287.2
Farsi 710.3 Spanish 972
Table 1: Millions of monolingual web crawl and
Wikipedia word tokens
cluding contextual and string similarity, and directly
learn how to combine them.3 Monolingual Data and Signals3.1 Data
Throughout our experiments, we seek to learn how
to translate words in a given source language into
English. Table 1 lists our languages of interest,
along with the total amount of monolingual data
that we use for each. We use web crawled time-
stamped news articles to estimate temporal sim-
ilarity, Wikipedia pages which are inter-lingually
linked to English pages to estimate topic similarity,
and both datasets to estimate frequency and contex-
tual similarity. Following Irvine et al (2010), we
use pairs of Wikipedia page titles to train a simple
transliterator for languages written in a non-Roman
script, which allows us to compute orthographic
similarity for pairs of words in different scripts.3.2 Signals
Our definitions of orthographic, topic, temporal, and
contextual similarity are taken from Klementiev et
al. (2012), and the details of each may be found
there. Here, we give briefly describe them and give
our definition of a novel, frequency-based signal.Orthographic We measure orthographic similar-
ity between a pair of words as the normalized1 edit
distance between the two words. For non-Roman
script languages, we transliterate words into the Ro-
man script before measuring orthographic similarity.TopicWe use monolingual Wikipedia pages to es-
timate topical signatures for each source and target
1Normalized by the average of the lengths of the two words
519
language word. Signature vectors are the length of
the number of inter-lingually linked source and En-
glish Wikipedia pages and contain counts of how
many times the word appears on each page. We use
cosine similarity to compare pairs of signatures.Temporal We use time-stamped web crawl data
to estimate temporal signatures, which, for a given
word, are the length of the number of time-stamps
(dates) and contain counts of how many times the
word appears in news articles with the given date.
We use a sliding window of three days and use co-
sine similarity to compare signatures. We expect
that source and target language words which are
translations of one another will appear with similar
frequencies over time in monolingual data.Contextual We score monolingual contextual
similarity by first collecting context vectors for each
source and target language word. The context vector
for a given word contains counts of how many times
words appear in its context. We use bag of words
contexts in a window of size two. We gather both
source and target language contextual vectors from
our web crawl data and Wikipedia data (separately).Frequency Words that are translations of one an-
other are likely to have similar relative frequencies
in monolingual corpora. We measure the frequency
similarity of two words as the absolute value of the
difference between the logs of their relative mono-
lingual corpus frequencies.4 Supervised Bilingual Lexicon Induction4.1 Baseline
Our unsupervised baseline method is based on
ranked lists derived from each of the signals listed
above. For each source word, we generate ranked
lists of English candidates using the following six
signals: Crawls Context, Crawls Time, Wikipedia
Context, Wikipedia Topic, Edit distance, and Log
Frequency Difference. Then, for each English can-
didate we compute its mean reciprocal rank2 (MRR)
based on the six ranked lists. The baseline ranks En-
glish candidates according to the MRR scores. For
evaluation, we use the same test sets, accuracy met-
ric, and correct translations described below.
2The MRR of the jth English word, ej , is 1N
PN
i=1
1
rankij
,
where N is the number of signals and rankij is ej?s rank ac-
cording to signal i.
4.2 Supervised Approach
In addition to the monolingual resources described
in Section 3.1, we have a bilingual dictionary for
each language, which we use to project context vec-
tors and for supervision and evaluation. For each
language, we choose up to 8, 000 source language
words among those that occur in the monolingual
data at least three times and that have at least one
translation in our dictionary. We randomly divide
the source language words into three equally sized
sets for training, development, and testing. We use
the training data to train a classifier, the develop-
ment data to choose the best classification settings
and feature set, and the test set for evaluation.
For all experiments, we use a linear classifier
trained by stochastic gradient descent to minimize
squared error3 and perform 100 passes over the
training data.4 The binary classifiers predict whether
a pair of words are translations of one another or not.
The translations in our training data serve as posi-
tive supervision, and the source language words in
the training data paired with random English words5
serve as negative supervision. We used our develop-
ment data to tune the number of negative examples
to three for each positive example. At test time, af-
ter scoring all source language words in the test set
paired with all English words in our candidate set,6
we rank the English candidates by their classifica-
tion scores and evaluate accuracy in the top-k trans-
lations.4.3 Features
Our monolingual features are listed below and are
based on raw similarity scores as well as ranks:
? Crawls Context: Web crawl context similarity score
? Crawls Context RR: reciprocal rank of crawls con-
text
3We tried using logistic rather than linear regression, but
performance differences on our development set were very
small and not statistically significant.
4We use http://hunch.net/?vw/ version 6.1.4, and
run it with the following arguments that affect how updates are
made in learning: ?exact adaptive norm ?power t 0.5
5Among those that appear at least five times in our monolin-
gual data, consistent with our candidate set.
6All English words appearing at least five times in our
monolingual data. In practice, we further limit the set to those
that occur in the top-1000 ranked list according to at least one
of our signals.
520
??
?0.0
0.2
0.4
0.6
0.8
1.0
Acc
urac
y in 
Top?
10
CrawlContext EditDist CrawlTime WikiContext WikiTopic Is?Ident. DiffLg?Frq DiscrimAll
Figure 1: Each box-and-whisker plot summarizes per-
formance on the development set using the given fea-
ture(s) across all 22 languages. For each source word
in our development sets, we rank all English target words
according to the monolingual similarity metric(s) listed.
All but the last plot show the performance of individual
features. Discrim-All uses supervised data to train classi-
fiers for each language based on all of the features.
? Crawls Time: Web crawl temporal similarity score
? Crawls Time RR: reciprocal rank of crawls time
? Edit distance: normalized (by average length of
source and target word) edit distance
? Edit distance RR: reciprocal rank of edit distance
? Wiki Context: Wikipedia context similarity score
? Wiki Context RR: recip. rank of wiki context
? Wiki Topic: Wikipedia topic similarity score
? Wiki Topic RR: recip. rank of wiki topic
? Is-Identical: source and target words are the same
? Difference in log frequencies: Difference between
the logs of the source and target word monolingual
frequencies
? Log Freqs Diff RR: reciprocal rank of difference in
log frequencies
We train classifiers separately for each source lan-
guage, and the learned weights vary based on, for
example, corpora size and the relatedness of the
source language and English (e.g. edit distance is
informative if there are many cognates). In order to
use the trained classifiers to make top-k translation
predictions for a given source word, we rank candi-
dates by their classification scores.4.4 Feature Evaluation and Selection
After training initial classifiers, we use our develop-
ment data to choose the most informative subset of
features. Figure 1 shows the top-10 accuracy on the
development data when we use individual features
?
0.0
0.2
0.4
0.6
0.8
1.0
Acc
urac
y in
 Top
?10
WikiTopic WikiContext DiffLog?Freq EditDist. EditDist. RR CrawlContext AllFeatures
Figure 2: Performance on the development set goes up
as features are greedily added to the feature space. Mean
performance is slightly higher using this subset of six fea-
tures (second to last bar) than using all features (last bar).
to predict translations. Top-10 accuracy refers to the
percent of source language words for which a correct
English translation appears in the top-10 ranked En-
glish candidates. Each box-and-whisker plot sum-
marizes performance over the 22 languages. We
don?t display reciprocal rank features, as their per-
formance is very similar to that of the correspond-
ing raw similarity score. It?s easy to see that features
based on the Wikipedia topic signal are the most in-
formative. It is also clear that training a supervised
model to combine all of the features (the last plot)
yields performance that is dramatically higher than
using any individual feature alone.
Figure 2, from left to right, shows a greedy search
for the best subset of features among those listed
above. Again, the Wikipedia topic score is the most
informative stand-alone feature, and the Wikipedia
context score is the most informative second feature.
Adding features to the model beyond the six shown
in the figure does not yield additional performance
gains over our set of languages.4.5 Learning Curve Analysis
Figure 3 shows learning curves over the number of
positive training instances. In all cases, the number
of randomly generated negative training instances
is three times the number of positive. For all lan-
guages, performance is stable after about 300 cor-
rect translations are used for training. This shows
that our supervised method for combining signals
requires only a small training dictionary.
521
??
??
? ?
? ?
0 200 400 600 800 1000 1200
0.0
0.2
0.4
0.6
0.8
1.0
?
?
?
?
??
? ?
? ? ?
???
??
?
?
? ? ? ?
?
?
?
?
? ?
? ?
? ?
?
?
?
?
? ?
? ?
????
?
?
? ?
?
??
???
? ? ?
?
???
? ?
?
????
?
?
?
?
?
?
?
?
?
?
?
SpanishRomanianPolishBulgarianIndonesianWelshSlovakBosnianLatvianAlbanianUkrainianTurkishAzeriSerbianHindiBengaliUzbekFarsiSomaliTamilUrduNepali
Positive training data instances
Acc
urac
y in
 Top
?10
Figure 3: Learning curves over number of positive train-
ing instances, up to 1250. For some languages, 1250
positive training instances are not available. In all cases,
evaluation is on the development data and the number of
negative training instances is three times the number of
positive. For all languages, performance is fairly stable
after about 300 positive training instances.5 Results
We use a model based on the six features shown
in Figure 2 to score and rank English translation
candidates for the test set words in each language.
Table 2 gives the result for each language for the
MRR baseline and our supervised technique. Across
languages, the average top-10 accuracy using the
MRR baseline is 30.4, and the average using our
technique is 43.9, a relative improvement of about
44%. We did not attempt a comparison with more
sophisticated unsupervised rank aggregation meth-
ods. However, we believe the improvements we
observe drastically outweigh the expected perfor-
mance differences between different rank aggrega-
tion methods. Figure 4 plots the accuracies yielded
by our supervised technique versus the total amount
of monolingual data for each language. An increase
in monolingual data tends to improve accuracy. The
correlation isn?t perfect, however. For example, per-
formance on Urdu and Farsi is relatively poor, de-
spite the large amounts of monolingual data avail-
able for each. This may be due to the fact that we
have large web crawls for those languages, but their
Wikipedia datasets, which tend to provide a strong
topic signal, are relatively small.
AzeriBengali
BosnianWelsh
Hindi
Indonesian
Latvian
Nepali
Romanian
Slovak
Somali
Albanian
SerbianTamilUzbek
Farsi
Spanish
Urdu
TurkishBulgarianUkranian
Polish
Millions of Monolingual Word Tokens
Acc
urac
y
1e?01 1e+00 1e+01 1e+02 1e+03
0.0
0.2
0.4
0.6
0.8
1.0
Figure 4: Millions of monolingual word tokens vs. Lex-
icon Induction Top-10 Accuracy
Lang MRR Supv. Lang MRR Supv.
Nepali 11.2 13.6 Somali 16.7 18.1
Uzbek 23.2 29.6 Azeri 16.1 29.4
Tamil 28.4 33.3 Albanian 32.0 45.3
Bengali 19.3 32.8 Welsh 36.1 56.4
Bosnian 32.6 52.8 Latvian 29.6 47.7
Indonesian 41.5 63.5 Romanian 53.3 71.6
Serbian 29.0 33.3 Turkish 31.4 52.1
Ukrainian 29.7 46.0 Hindi 18.2 34.6
Bulgarian 40.2 57.9 Polish 47.4 67.1
Slovak 34.6 53.5 Urdu 13.2 21.2
Farsi 10.5 21.1 Spanish 74.8 85.0
Table 2: Top-10 Accuracy on test set. Performance
increases for all languages moving from the baseline
(MRR) to discriminative training (Supv).6 Conclusions
On average, we observe relative gains of more than
44% over an unsupervised rank-combination base-
line by using a seed bilingual dictionary and a di-
verse set of monolingual signals to train a supervised
classifier. Using supervision for bilingual lexicon in-
duction makes sense. In some cases a dictionary is
already assumed for computing contextual similar-
ity, and, in the remaining cases, one could be com-
piled easy, either automatically, e.g. Haghighi et al
(2008), or through crowdsourcing, e.g. Irvine and
Klementiev (2010) and Callison-Burch and Dredze
(2010). We have shown that only a few hundred
translation pairs are needed to achieve good perfor-
mance. Our framework has the additional advantage
that any new monolingually-derived similarity met-
rics can easily be added as new features.
522
7 Acknowledgements
This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and by
the Johns Hopkins University Human Language
Technology Center of Excellence. The views and
conclusions contained in this publication are those
of the authors and should not be interpreted as repre-
senting official policies or endorsements of DARPA
or the U.S. Government.References
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL Workshop on Cre-
ating Speech and Language Data with Amazon?s Me-
chanical Turk.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the Conference of the As-
sociation for Computational Linguistics (ACL).
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
frommonolingual corpora. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL).
Ann Irvine and Alexandre Klementiev. 2010. Using me-
chanical turk to annotate lexicons for less commonly
used languages. In Proceedings of the NAACL Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk.
Ann Irvine, Chris Callison-Burch, and Alexandre Kle-
mentiev. 2010. Transliterating from all languages. In
Proceedings of the Conference of the Association for
Machine Translation in the Americas (AMTA).
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Alex Klementiev, Ann Irvine, Chris Callison-Burch, and
David Yarowsky. 2012. Toward statistical machine
translation without parallel corpora. In Proceedings of
the Conference of the European Association for Com-
putational Linguistics (EACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In ACL
Workshop on Unsupervised Lexical Acquisition.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sammer,
and Jeff Bilmes. 2010. Panlingual lexical transla-
tion via probabilistic inference. Artificial Intelligence,
174:619?637, June.
DavidMimno, HannaWallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining language
models and context vectors. In Proceedings of the
Conference of the Association for Computational Lin-
guistics (ACL).
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL).
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the Conference of the Associ-
ation for Computational Linguistics (ACL).
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Charles Schafer. 2006. Translation Discovery Using Di-
verse Similarity Measures. Ph.D. thesis, Johns Hop-
kins University.
523
Proceedings of NAACL-HLT 2013, pages 758?764,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
PPDB: The Paraphrase Database
Juri Ganitkevitch1 Benjamin Van Durme1,2 Chris Callison-Burch2,3
1Center for Language and Speech Processing, Johns Hopkins University
2Human Language Technology Center of Excellence, Johns Hopkins University
3Computer and Information Science Department, University of Pennsylvania
Abstract
We present the 1.0 release of our para-
phrase database, PPDB. Its English portion,
PPDB:Eng, contains over 220 million para-
phrase pairs, consisting of 73 million phrasal
and 8 million lexical paraphrases, as well as
140 million paraphrase patterns, which cap-
ture many meaning-preserving syntactic trans-
formations. The paraphrases are extracted
from bilingual parallel corpora totaling over
100 million sentence pairs and over 2 billion
English words. We also release PPDB:Spa, a
collection of 196 million Spanish paraphrases.
Each paraphrase pair in PPDB contains a
set of associated scores, including paraphrase
probabilities derived from the bitext data and a
variety of monolingual distributional similar-
ity scores computed from the Google n-grams
and the Annotated Gigaword corpus. Our re-
lease includes pruning tools that allow users to
determine their own precision/recall tradeoff.
1 Introduction
Paraphrases, i.e. differing textual realizations of the
same meaning, have proven useful for a wide vari-
ety of natural language processing applications. Past
paraphrase collections include automatically derived
resources like DIRT (Lin and Pantel, 2001), the
MSR paraphrase corpus and phrase table (Dolan
et al, 2004; Quirk et al, 2004), among others.
Although several groups have independently ex-
tracted paraphrases using Bannard and Callison-
Burch (2005)?s bilingual pivoting technique (see
Zhou et al (2006), Riezler et al (2007), Snover et
al. (2010), among others), there has never been an
official release of this resource.
In this work, we release version 1.0 of the Para-
Phrase DataBase PPDB,1 a collection of ranked En-
glish and Spanish paraphrases derived by:
? Extracting lexical, phrasal, and syntactic para-
phrases from large bilingual parallel corpora
(with associated paraphrase probabilities).
? Computing distributional similarity scores for
each of the paraphrases using the Google n-
grams and the Annotated Gigaword corpus.
In addition to the paraphrase collection itself, we
provide tools to filter PPDB to only retain high pre-
cision paraphrases, scripts to limit the collection to
phrasal or lexical paraphrases (synonyms), and soft-
ware that enables users to extract paraphrases for
languages other than English.
2 Extracting Paraphrases from Bitexts
To extract paraphrases we follow Bannard and
Callison-Burch (2005)?s bilingual pivoting method.
The intuition is that two English strings e1 and e2
that translate to the same foreign string f can be as-
sumed to have the same meaning. We can thus pivot
over f and extract ?e1, e2? as a pair of paraphrases,
as illustrated in Figure 1. The method extracts a di-
verse set of paraphrases. For thrown into jail, it ex-
tracts arrested, detained, imprisoned, incarcerated,
jailed, locked up, taken into custody, and thrown
into prison, along with a set of incorrect/noisy para-
phrases that have different syntactic types or that are
due to misalignments.
For PPDB, we formulate our paraphrase collec-
tion as a weighted synchronous context-free gram-
mar (SCFG) (Aho and Ullman, 1972; Chiang, 2005)
1Freely available at http://paraphrase.org.
758
... f?nf Landwirte , weil
... 5 farmers were in Ireland ...
...
oder wurden , gefoltert
or have been , tortured
festgenommen 
thrown into jail
festgenommen
imprisoned
...
... ...
...
Figure 1: Phrasal paraphrases are extracted via bilingual
pivoting.
with syntactic nonterminal labels, similar to Cohn
and Lapata (2008) and Ganitkevitch et al (2011).
An SCFG rule has the form:
r
def= C ? ?f, e,?, ~??,
where the left-hand side of the rule,C, is a nontermi-
nal and the right-hand sides f and e are strings of ter-
minal and nonterminal symbols. There is a one-to-
one correspondence, ?, between the nonterminals
in f and e: each nonterminal symbol in f has to
also appear in e. Following Zhao et al (2008), each
rule r is annotated with a vector of feature functions
~? = {?1...?N} which are combined in a log-linear
model (with weights ~?) to compute the cost of ap-
plying r:
cost(r) = ?
N?
i=1
?i log?i. (1)
To create a syntactic paraphrase grammar we
first extract a foreign-to-English translation gram-
mar from a bilingual parallel corpus, using tech-
niques from syntactic machine translation (Koehn,
2010). Then, for each pair of translation rules where
the left-hand side C and foreign string f match:
r1
def= C ? ?f, e1,?1, ~?1?
r2
def= C ? ?f, e2,?2, ~?2?,
we pivot over f to create a paraphrase rule rp:
rp
def= C ? ?e1, e2,?p, ~?p?,
with a combined nonterminal correspondency func-
tion ?p. Note that the common source side f im-
plies that e1 and e2 share the same set of nonterminal
symbols.
The paraphrase rules obtained using this method
are capable of making well-formed generalizations
of meaning-preserving rewrites in English. For
instance, we extract the following example para-
phrase, capturing the English possessive rule:
NP ? the NP1 of NNS 2 | the NNS2 ?s NP1.
The paraphrase feature vector ~?p is computed
from the translation feature vectors ~?1 and ~?2 by
following the pivoting idea. For instance, we esti-
mate the conditional paraphrase probability p(e2|e1)
by marginalizing over all shared foreign-language
translations f :
p(e2|e1) ?
?
f
p(e2|f)p(f |e1). (2)
3 Scoring Paraphrases Using Monolingual
Distributional Similarity
The bilingual pivoting approach anchors para-
phrases that share an interpretation because of a
shared foreign phrase. Paraphrasing methods based
on monolingual text corpora, like DIRT (Lin and
Pantel, 2001), measure the similarity of phrases
based on distributional similarity. This results in a
range of different types of phrases, including para-
phrases, inference rules and antonyms. For instance,
for thrown into prison DIRT extracts good para-
phrases like arrested, detained, and jailed. How-
ever, it also extracts phrases that are temporarily
or causally related like began the trial of, cracked
down on, interrogated, prosecuted and ordered the
execution of, because they have similar distribu-
tional properties. Since bilingual pivoting rarely ex-
tracts these non-paraphrases, we can use monolin-
gual distributional similarity to re-rank paraphrases
extracted from bitexts (following Chan et al (2011))
or incorporate a set of distributional similarity scores
as features in our log-linear model.
Each similarity score relies on precomputed dis-
tributional signatures that describe the contexts that
a phrase occurs in. To describe a phrase e, we gather
counts for a set of contextual features for each oc-
currence of e in a corpus. Writing the context vector
for the i-th occurrence of e as ~se,i, we can aggre-
gate over all occurrences of e, resulting in a distri-
butional signature for e, ~se =
?
i ~se,i. Following the
intuition that phrases with similar meanings occur in
759
the long-term
achieve25
goals 23
plans 97
investment 10
confirmed64
revise43 the long-term
the long-term
the long-term
the long-term
the long-term
.
.
.
.
L-achieve = 25
L-confirmed
= 64
L-revise = 43
?
R-goals 
= 23
R-plans  = 97
R-investment 
= 10
?
the long-term
?
=~sig
?
(a) The n-gram corpus records the long-term as preceded
by revise (43 times), and followed by plans (97 times). We
add corresponding features to the phrase?s distributional
signature retaining the counts of the original n-grams.
long-term investment holding on to
det
amod
the
JJ NN VBG IN TO DT
NP
PP
VP
? ?
the long-term
?
=~sig
?
dep-det-R-investment
pos-L-TO 
pos-R-NN  
lex-R-investment 
lex-L-to 
dep-amod-R-investment
syn-gov-NP syn-miss-L-NN 
lex-L-on-to 
pos-L-IN-TO  
dep-det-R-NN dep-amod-R-NN
(b) Here, position-aware lexical and part-of-speech n-
gram features, labeled dependency links , and features
reflecting the phrase?s CCG-style label NP/NN are in-
cluded in the context vector.
Figure 2: Features extracted for the phrase the long term from the n-gram corpus (2a) and Annotated Gigaword (2b).
similar contexts, we can then quantify the goodness
of e? as a paraphrase of e by computing the cosine
similarity between their distributional signatures:
sim(e, e?) = ~se ? ~se?
|~se||~se? |
.
A wide variety of features have been used to de-
scribe the distributional context of a phrase. Rich,
linguistically informed feature-sets that rely on de-
pendency and constituency parses, part-of-speech
tags, or lemmatization have been proposed in work
such as by Church and Hanks (1991) and Lin and
Pantel (2001). For instance, a phrase is described by
the various syntactic relations such as: ?what verbs
have this phrase as the subject??, or ?what adjectives
modify this phrase??. Other work has used simpler
n-gram features, e.g. ?what words or bigrams have
we seen to the left of this phrase??. A substantial
body of work has focussed on using this type of
feature-set for a variety of purposes in NLP (Lapata
and Keller, 2005; Bhagat and Ravichandran, 2008;
Lin et al, 2010; Van Durme and Lall, 2010).
For PPDB, we compute n-gram-based context
signatures for the 200 million most frequent phrases
in the Google n-gram corpus (Brants and Franz,
2006; Lin et al, 2010), and richer linguistic signa-
tures for 175 million phrases in the Annotated Gi-
gaword corpus (Napoles et al, 2012). Our features
extend beyond those previously used in the work by
Ganitkevitch et al (2012). They are:
? n-gram based features for words seen to the left
and right of a phrase.
? Position-aware lexical, lemma-based, part-of-
speech, and named entity class unigram and bi-
gram features, drawn from a three-word win-
dow to the right and left of the phrase.
? Incoming and outgoing (wrt. the phrase) de-
pendency link features, labeled with the corre-
sponding lexical item, lemmata and POS.
? Syntactic features for any constituents govern-
ing the phrase, as well as for CCG-style slashed
constituent labels for the phrase.
Figure 2 illustrates the feature extraction for an ex-
ample phrase.
4 English Paraphrases ? PPDB:Eng
We combine several English-to-foreign bitext cor-
pora to extract PPDB:Eng: Europarl v7 (Koehn,
2005), consisting of bitexts for the 19 European lan-
guages, the 109 French-English corpus (Callison-
Burch et al, 2009), the Czech, German, Span-
ish and French portions of the News Commen-
tary data (Koehn and Schroeder, 2007), the United
Nations French- and Spanish-English parallel cor-
pora (Eisele and Chen, 2010), the JRC Acquis cor-
pus (Steinberger et al, 2006), Chinese and Arabic
760
Identity Paraphrases Total
Lexical 0.6M 7.6M 8.1M
Phrasal 4.9M 68.4M 73.2M
Syntactic 46.5M 93.6M 140.1M
All 52.0M 169.6M 221.4M
Table 1: A breakdown of PPDB:Eng size by paraphrase
type. We distinguish lexical (i.e. one-word) paraphrases,
phrasal paraphrases and syntactically labeled paraphrase
patterns.
newswire corpora used for the GALE machine trans-
lation campaign,2 parallel Urdu-English data from
the NIST translation task,3 the French portion of
the OpenSubtitles corpus (Tiedemann, 2009), and a
collection of Spanish-English translation memories
provided by TAUS.4
The resulting composite parallel corpus has more
than 106 million sentence pairs, over 2 billion En-
glish words, and spans 22 pivot languages. To ap-
ply the pivoting technique to this multilingual data,
we treat the various pivot languages as a joint Non-
English language. This simplifying assumption al-
lows us to share statistics across the different lan-
guages and apply Equation 2 unaltered.
Table 1 presents a breakdown of PPDB:Eng by
paraphrase type. We distinguish lexical (a single
word), phrasal (a continuous string of words), and
syntactic paraphrases (expressions that may con-
tain both words and nonterminals), and separate
out identity paraphrases. While we list lexical and
phrasal paraphrases separately, it is possible that a
single word paraphrases as a multi-word phrase and
vice versa ? so long they share the same syntactic
label.
5 Spanish Paraphrases ? PPDB:Spa
We also release a collection of Spanish paraphrases:
PPDB:Spa is extracted analogously to its English
counterpart and leverages the Spanish portions of the
bitext data available to us, totaling almost 355 mil-
lion Spanish words, in nearly 15 million sentence
pairs. The paraphrase pairs in PPDB:Spa are anno-
2http://projects.ldc.upenn.edu/gale/
data/Catalog.html
3LDC Catalog No. LDC2010T23
4http://www.translationautomation.com/
Identity Paraphrases Total
Lexical 1.0M 33.1M 34.1M
Phrasal 4.3M 73.2M 77.5M
Syntactic 29.4M 55.3M 84.7M
All 34.7M 161.6M 196.3M
Table 2: An overview of PPDB:Spa. Again, we parti-
tion the resource into lexical (i.e. one-word) paraphrases,
phrasal paraphrases and syntactically labeled paraphrase
patterns.
expect
NNS VBP
NP
VP
the data
NP VP
S
to show
JJ
economistsfew
......
S
...
RelArg0 Arg1
Figure 3: To inspect our coverage, we use the Penn
Treebank?s parses to map from Propbank annotations to
PPDB?s syntactic patterns. For the above annotation
predicate, we extract VBP ? expect, which is matched
by paraphrase rules like VBP ? expect | anticipate
and VBP ? expect | hypothesize. To search for
the entire relation, we replace the argument spans
with syntactic nonterminals. Here, we obtain S ?
NP expect S, for which PPDB has matching rules like
S ? NP expect S | NP would hope S, and S ?
NP expect S | NP trust S. This allows us to apply so-
phisticated paraphrases to the predicate while capturing
its arguments in a generalized fashion.
tated with distributional similarity scores based on
lexical features collected from the Spanish portion
of the multilingual release of the Google n-gram
corpus (Brants and Franz, 2009), and the Spanish
Gigaword corpus (Mendonca et al, 2009). Table 2
gives a breakdown of PPDB:Spa.
6 Analysis
To estimate the usefulness of PPDB as a resource
for tasks like semantic role labeling or parsing, we
analyze its coverage of Propbank predicates and
predicate-argument tuples (Kingsbury and Palmer,
2002). We use the Penn Treebank (Marcus et
al., 1993) to map Propbank annotations to patterns
which allow us to search PPDB:Eng for paraphrases
that match the annotated predicate. Figure 3 illus-
761
 1 3 5-30 -25 -20 -15 -10 -5  0Avg. Score Pruning Threshold 0 0.5 1-30 -25 -20 -15 -10 -5  0  0 50 100 150Coverage PP / Type
(a) PPDB:Eng coverage of Propbank predicates
(top), and average human judgment score (bottom)
for varying pruning thresholds.
 0 0.2 0.4 0.6 0.8 1-30 -25 -20 -15 -10 -5  0  0 20 40 60 80 100 120 140 160Coverage Paraphrases / TypePruning ThresholdRelation Tokens CoveredParaphrases / TypeRelation Types Covered
(b) PPDB:Eng?s coverage of Propbank predicates
with up to two arguments. Here we consider rules
that paraphrase the full predicate-argument expres-
sion.
Figure 4: An illustration of PPDB?s coverage of the manually annotated Propbank predicate phrases (4a) and binary
relations with argument non-terminals (4b). The curves indicate the coverage on tokens (solid) and types (dotted), as
well as the average number of paraphrases per covered type (dashed) at the given pruning level.
trates this mapping.
In order to quantify PPDB?s precision-recall
tradeoff in this context, we perform a sweep
over our collection, beginning with the full set of
paraphrase pairs and incrementally discarding the
lowest-scoring ones. We choose a simple estimate
for each paraphrase pair?s score by uniformly com-
bining its paraphrase probability features in Eq. 1.
The top graph in Figure 4a shows PPDB?s cover-
age of predicates (e.g. VBP ? expect) at the type
level (i.e. counting distinct predicates), as well as
the token level (i.e. counting predicate occurrences
in the corpus). We also keep track of average num-
ber of paraphrases per covered predicate type for
varying pruning levels. We find that PPDB has a
predicate type recall of up to 52% (accounting for
97.5% of tokens). Extending the experiment to full
predicate-argument relations with up to two argu-
ments (e.g. S ? NNS expect S), we obtain a 27%
type coverage rate that accounts for 40% of tokens
(Figure 4b). Both rates hold even as we prune the
database down to only contain high precision para-
phrases. Our pruning method here is based on a sim-
ple uniform combination of paraphrase probabilities
and similarity scores.
To gauge the quality of our paraphrases, the au-
thors judged 1900 randomly sampled predicate para-
phrases on a scale of 1 to 5, 5 being the best. The
bottom graph in Figure 4a plots the resulting human
score average against the sweep used in the cover-
age experiment. It is clear that even with a simple
weighing approach, the PPDB scores show a clear
correlation with human judgements. Therefore they
can be used to bias the collection towards greater re-
call or higher precision.
7 Conclusion and Future Work
We present the 1.0 release of PPDB:Eng and
PPDB:Spa, two large-scale collections of para-
phrases in English and Spanish. We illustrate the
resource?s utility with an analysis of its coverage of
Propbank predicates. Our results suggest that PPDB
will be useful in a variety of NLP applications.
Future releases of PPDB will focus on expand-
ing the paraphrase collection?s coverage with regard
to both data size and languages supported. Further-
more, we intend to improve paraphrase scoring by
incorporating additional sources of information, as
well as by better utilizing information present in the
data, like domain or topic. We will also address
points of refinement such as handling of phrase am-
biguity, and effects specific to individual pivot lan-
guages. Our aim is for PPDB to be a continuously
updated and improving resource.
Finally, we will explore extensions to PPDB to in-
clude aspects of related large-scale resources such as
lexical-semantic hierarchies (Snow et al, 2006), tex-
tual inference rules (Berant et al, 2011), relational
patterns (Nakashole et al, 2012), and (lexical) con-
ceptual networks (Navigli and Ponzetto, 2012).
762
Acknowledgements
We would like to thank Frank Ferraro for his Prop-
bank processing tools. This material is based
on research sponsored by the NSF under grant
IIS-1249516 and DARPA under agreement num-
ber FA8750-13-2-0017 (the DEFT program). The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes. The
views and conclusions contained in this publication
are those of the authors and should not be interpreted
as representing official policies or endorsements of
DARPA or the U.S. Government.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Jonathan Berant, Jacob Goldberger, and Ido Dagan.
2011. Global learning of typed entailment rules. In
Proceedings of ACL.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Thorsten Brants and Alex Franz. 2009. Web 1T 5-gram,
10 european languages version 1. Linguistic Data
Consortium, Philadelphia.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Pro-
ceedings of WMT, pages 1?28, Athens, Greece, March.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van
Durme. 2011. Reranking bilingually extracted para-
phrases using monolingual distributional similarity. In
EMNLP Workshop on GEMS.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
Kenneth Church and Patrick Hanks. 1991. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 6(1):22?29.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
COLING.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the COLING.
Andreas Eisele and Yu Chen. 2010. MultiUN: A multi-
lingual corpus from united nation documents. In Pro-
ceedings of LREC, Valletta, Malta.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2012. Monolingual distributional
similarity for text-to-text generation. In Proceedings
of *SEM. Association for Computational Linguistics.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of LREC.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of WMT, Prague, Czech Repub-
lic, June. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2(1).
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the Penn Treebank. Computational
Linguistics, 19(2).
Angelo Mendonca, David Andrew Graff, and Denise
DiPersio. 2009. Spanish Gigaword Second Edition.
Linguistic Data Consortium.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. PATTY: a taxonomy of rela-
tional patterns with semantic types. In Proceedings
of EMNLP.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP.
763
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the ACL.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of the ACL/Coling.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Da?niel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. In Pro-
ceedings of LREC, Genoa, Italy.
Jo?rg Tiedemann. 2009. News from OPUS: A collection
of multilingual parallel corpora with tools and inter-
faces. In Recent Advances in Natural Language Pro-
cessing, volume 5.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of HLT/NAACL.
764
Proceedings of NAACL-HLT 2013, pages 858?867,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Answer Extraction as Sequence Tagging with Tree Edit Distance
Xuchen Yao and Benjamin Van Durme
Johns Hopkins University
Baltimore, MD, USA
Chris Callison-Burch?
University of Pennsylvania
Philadelphia, PA, USA
Peter Clark
Vulcan Inc.
Seattle, WA, USA
Abstract
Our goal is to extract answers from pre-
retrieved sentences for Question Answering
(QA). We construct a linear-chain Conditional
Random Field based on pairs of questions
and their possible answer sentences, learning
the association between questions and answer
types. This casts answer extraction as an an-
swer sequence tagging problem for the first
time, where knowledge of shared structure be-
tween question and source sentence is incor-
porated through features based on Tree Edit
Distance (TED). Our model is free of man-
ually created question and answer templates,
fast to run (processing 200 QA pairs per sec-
ond excluding parsing time), and yields an F1
of 63.3% on a new public dataset based on
prior TREC QA evaluations. The developed
system is open-source, and includes an imple-
mentation of the TED model that is state of the
art in the task of ranking QA pairs.
1 Introduction
The success of IBM?s Watson system for Question
Answering (QA) (Ferrucci et al, 2010) has illus-
trated a continued public interest in this topic. Wat-
son is a sophisticated piece of software engineering
consisting of many components tied together in a
large parallel architecture. It took many researchers
working full time for years to construct. Such re-
sources are not available to individual academic re-
searchers. If they are interested in evaluating new
ideas on some aspect of QA, they must either con-
struct a full system, or create a focused subtask
?Performed while faculty at Johns Hopkins University.
paired with a representative dataset. We follow the
latter approach and focus on the task of answer ex-
traction, i.e., producing the exact answer strings for
a question.
We propose the use of a linear-chain Conditional
Random Field (CRF) (Lafferty et al, 2001) in or-
der to cast the problem as one of sequence tagging
by labeling each token in a candidate sentence as ei-
ther Beginning, Inside or Outside (BIO) of an an-
swer. This is to our knowledge the first time a
CRF has been used to extract answers.1 We uti-
lize not only traditional contextual features based on
POS tagging, dependency parsing and Named Entity
Recognition (NER), but most importantly, features
extracted from a Tree Edit Distance (TED) model
for aligning an answer sentence tree with the ques-
tion tree. The linear-chain CRF, when trained to
learn the associations between question and answer
types, is a robust approach against error propaga-
tion introduced in the NLP pipeline. For instance,
given an NER tool that always (i.e., in both train-
ing and test data) recognizes the pesticide DDT as
an ORG, our model realizes, when a question is
asked about the type of chemicals, the correct an-
swer might be incorrectly but consistently recog-
nized as ORG by NER. This helps reduce errors in-
troduced by wrong answer types, which were esti-
mated as the most significant contributer (36.4%)
of errors in the then state-of-the-art QA system of
Moldovan et al (2003).
The features based on TED allow us to draw the
1CRFs have been used in judging answer-bearing sentences
(Shima et al, 2008; Ding et al, 2008; Wang and Manning,
2010), but not extracting exact answers from these sentences.
858
connection between the question and answer sen-
tences before answer extraction, whereas tradition-
ally the exercise of answer validation (Magnini et
al., 2002; Penas et al, 2008; Rodrigo et al, 2009)
has been performed after as a remedy to ensure the
answer is really ?about? the question.
Motivated by a desire for a fast runtime,2 we
base our TED implementation on the dynamic-
programming approach of Zhang and Shasha
(1989), which helps our final system process 200
QA pairs per second on standard desktop hardware,
when input is syntactically pre-parsed.
In the following we first provide background on
the TED model, going on to evaluate our implemen-
tation against prior work in the context of question
answer sentence ranking (QASR), achieving state of
the art in that task. We then describe how we cou-
ple TED features to a linear-chain CRF for answer
extraction, providing the set of features used, and fi-
nally experimental results on an extraction dataset
we make public (together with the software) to the
community.3 Related prior work is interspersed
throughout the paper.
2 Tree Edit Distance Model
Tree Edit Distance (?2.1) models have been shown
effective in a variety of applications, including tex-
tual entailment, paraphrase identification, answer
ranking and information retrieval (Reis et al, 2004;
Kouylekov and Magnini, 2005; Heilman and Smith,
2010; Augsten et al, 2010). We chose the variant
proposed by Heilman and Smith (2010), inspired by
its simplicity, generality, and effectiveness. Our ap-
proach differs from those authors in their reliance
on a greedy search routine to make use of a complex
tree kernel. With speed a consideration, we opted
for the dynamic-programming solution of Zhang
and Shasha (1989) (?2.1). We added new lexical-
semantic features ?(2.2) to the model and then eval-
uated our implementation on the QASR task, show-
ing strong results ?(2.3).
Feature Description
distance tree edit distance from answer
sentence to question
renNoun
renVerb
renOther
# edits changing POS from or to
noun, verb, or other types
insN, insV,
insPunc,
insDet,
insOtherPos
# edits inserting a noun, verb,
punctuation mark, determiner
or other POS types
delN, delV, ... deletion mirror of above
ins{N,V,P}Mod
insSub, insObj
insOtherRel
# edits inserting a modifier for
{noun, verb, preposition}, sub-
ject, object or other relations
delNMod, ... deletion mirror of above
renNMod, ... rename mirror of above
XEdits # basic edits plus sum of in-
s/del/ren edits
alignNodes,
alignNum,
alignN, alignV,
alignProper
# aligned nodes, and those that
are numbers, nouns, verbs, or
proper nouns
Table 1: Features for ranking QA pairs.
2.1 Cost Design and Edit Search
Following Bille (2005), we define an edit script be-
tween trees T1, T2 as the edit sequence transforming
T1 to T2 according to a cost function, with the total
summed cost known as the tree edit distance. Basic
edit operations include: insert, delete and rename.
With T a dependency tree, we represent each node
by three fields: lemma, POS and the type of depen-
dency relation to the node?s parent (DEP). For in-
stance, Mary/nnp/sub is the proper noun Mary in
subject position.
Basic edits are refined into 9 types, where the
first six (INS LEAF, INS SUBTREE, INS, DEL LEAF,
DEL SUBTREE, DEL) insert or delete a leaf node, a
whole subtree, or a node that is neither a leaf nor
part of a whole inserted subtree. The last three
(REN POS, REN DEP, REN POS DEP) serve to re-
name a POS tag, dependency relation, or both.
2For instance, Watson was designed under the constraint of
a 3 second response time, arising from its intended live use in
the television gameshow, Jeopardy!.
3http://code.google.com/p/jacana/
859
prd
playernn jennifernn
capriatinnp 23cd
bevbzsubj
nmod nmod
tennisnn
nmod
Tennis player Jennifer Capriati is 23
TreeEdit Distance capriatinnp
jennifernnp
whatwp
sportnn
playvbz
dovbzvmod vmod
nmod
What sport does Jennifer Capriati play
insSubTree:
ins(play/vbz/vmod)ins(do/vbz/root)
WordNet
Figure 1: Edits transforming a source sentence (left) to a question (right). Each node consists of: lemma, POS tag
and dependency relation, with root nodes and punctuation not shown. Shown includes deletion (? and strikethrough
on the left), alignment (arrows) and insertion (shaded area). Order of operations is not displayed. The standard TED
model does not capture the alignment between tennis and sport (see Section 2.2).
We begin by uniformly assigning basic edits a
cost of 1.0,4 which brings the cost of a full node in-
sertion or deletion to 3 (all the three fields inserted or
deleted). We allow renaming of POS and/or relation
type iff the lemmas of source and target nodes are
identical.5 When two nodes are identical and thus
do not appear in the edit script, or when two nodes
are renamed due to the same lemma, we say they are
aligned by the tree edit model (see Figure 1).
We used Zhang and Shasha (1989)?s dynamic
programming algorithm to produce an optimal edit
script with the lowest tree edit distance. The ap-
proach explores both trees in a bottom-up, post-
order manner, running in time:
O(|T1| |T2|min(D1, L1)min(D2, L2))
where |Ti| is the number of nodes, Di is the depth,
and Li is the number of leaves, with respect to tree
Ti.
Additionally, we fix the cost of stopword renam-
ing to 2.5, even in the case of identity, regardless
of whether two stopwords have the same POS tags
or relations. Stopwords tend to have fixed POS tags
and dependency relations, which often leads to less
expensive alignments as compared to renaming con-
4This applies separately to each element of the tripartite
structure; e.g., deleting a POS entry, inserting a lemma, etc.
5This is aimed at minimizing node variations introduced by
morphology differences, tagging or parsing errors.
tent terms. In practice this gave stopwords ?too
much say? in guiding the overall edit sequence.
The resultant system is fast in practice, processing
10,000 pre-parsed tree pairs per second on a contem-
porary machine.6
2.2 TED for Sentence Ranking
The task of Question Answer Sentence Ranking
(QASR) takes a question and a set of source sen-
tences, returning a list sorted by the probability
likelihood that each sentence contains an appropri-
ate answer. Prior work in this includes that of:
Punyakanok et al (2004), based on mapping syn-
tactic dependency trees; Wang et al (2007) utiliz-
ing Quasi-Synchronous Grammar (Smith and Eis-
ner, 2006); Heilman and Smith (2010) using TED;
and Shima et al (2008), Ding et al (2008) and Wang
and Manning (2010), who each employed a CRF in
various ways. Wang et al (2007) made their dataset
public, which we use here for system validation. To
date, models based on TED have shown the best per-
formance for this task.
Our implementation follows Heilman and Smith
(2010), with the addition of 15 new features beyond
their original 33 (see Table 1). Based on results
6In later tasks, feature extraction and decoding will slow
down the system, but the final system was still able to process
200 pairs per second.
860
set source #ques. #pairs %pos. len.
TRAIN-ALL TREC8-12 1229 53417 12.0 any
TRAIN TREC8-12 94 4718 7.4 ? 40
DEV TREC13 82 1148 19.3 ? 40
TEST TREC13 89 1517 18.7 ? 40
Table 2: Distribution of data, with imbalance towards
negative examples (sentences without an answer).
in DEV, we extract edits in the direction from the
source sentence to the question.
In addition to syntactic features, we incorporated
the following lexical-semantic relations from Word-
Net: hypernym and synonym (nouns and verbs); en-
tailment and causing (verbs); and membersOf, sub-
stancesOf, partsOf, haveMember, haveSubstance,
havePart (nouns). Such relations have been used
in prior approaches to this task (Wang et al, 2007;
Wang and Manning, 2010), but not in conjunction
with the model of Heilman and Smith (2010).
These were made into features in two ways:
WNsearch loosens renaming and alignment within
the TED model from requiring strict lemma equal-
ity to allowing lemmas that shared any of the
above relations, leading to renaming operations such
as REN ...(country, china) and REN ...(sport,
tennis); WNfeature counts how many words be-
tween the sentence and answer sentence have each
of the above relations, separately as 10 independent
features, plus an aggregate count for a total of 11
new features beyond the earlier 48.
These features were then used to train a logistic
regression model using Weka (Hall et al, 2009).
2.3 QA Sentence Ranking Experiment
We trained and tested on the dataset from Wang et
al. (2007), which spans QA pairs from TREC QA
8-13 (see Table 2). Per question, sentences with
non-stopword overlap were first retrieved from the
task collection, which were then compared against
the TREC answer pattern (in the form of Perl regu-
lar expressions). If a sentence matched, then it was
deemed a (noisy) positive example. Finally, TRAIN,
DEV and TEST were manually corrected for errors.
Those authors decided to limit candidate source sen-
System MAP MRR
Wang et al (2007) 0.6029 0.6852
Heilman and Smith (2010) 0.6091 0.6917
Wang and Manning (2010) 0.5951 0.6951
this paper (48 features) 0.6319 0.7270
+WNsearch 0.6371 0.7301
+WNfeature (11 more feat.) 0.6307 0.7477
Table 3: Results on the QA Sentence Ranking task.
tences to be no longer than 40 words.7 Keeping
with prior work, those questions with only positive
or negative examples were removed, leaving 94 of
the original 100 questions for evaluation.
The data was processed by Wang et al (2007)
with the following tool chain: POS tags via MX-
POST (Ratnaparkhi, 1996); parse trees via MST-
Parser (McDonald et al, 2005) with 12 coarse-
grained dependency relation labels; and named enti-
ties via Identifinder (Bikel et al, 1999). Mean Av-
erage Precision (MAP) and Mean Reciprocal Rank
(MRR) are reported in Table 3. Our implementa-
tion gives state of the art performance, and is fur-
thered improved by our inclusion of semantic fea-
tures drawn from WordNet.8
3 CRF with TED for Answer Extraction
In this section we move from ranking source sen-
tences, to the next QA stage: answer extraction.
Given our competitive TED-based alignment model,
the most obvious solution to extraction would be to
report those spans aligned from a source sentence
to a question?s wh- terms. However, we show that
this approach is better formulated as a (strongly in-
dicative) feature of a larger set of answer extraction
signals.
3.1 Sequence Model
Figure 2 illustrates the task of tagging each token in
a candidate sentence with one of the following la-
7TRAIN-ALL is not used in QASR, but later for answer ex-
traction; TRAIN comes from the first 100 questions of TRAIN-
ALL.
8As the test set is of limited size (94 questions), then while
our MAP/MRR scores are 2.8% ? 5.6% higher than prior
work, this is not statistically significant according to the Paired
Randomization Test (Smucker et al, 2007), and thus should be
considered on par with the current state of the art.
861
prddla yenjri frddlcri tnyiln2l la 3b
vzsum o o o o o
Figure 2: An example of linear-chain CRF for an-
swer sequence tagging.
bels: B-ANSWER (beginning of answer), I-ANSWER
(inside of answer), O (outside of answer).
Besides local POS/NER/DEP features, at each to-
ken we need to inspect the entire input to connect the
answer sentence with the question sentence through
tree edits, drawing features from the question and
the edit script, motivating the use of a linear-chain
CRF model (Lafferty et al, 2001) over HMMs. To
the best of our knowledge this is the first time a
CRF has been used to label answer fragments, de-
spite success in other sequence tagging tasks.
3.2 Feature Design
In this subsection we describe the local and global
features used by the CRF.
Chunking We use the POS/NER/DEP tags directly
just as one would in a chunking task. Specifically,
suppose t represents the current token position and
pos[t] its POS tag, we extract unigram, bigram and
trigram features over the local context, e.g., pos[t?
2], pos[t ? 2] : pos[t ? 1], and pos[t ? 2] : pos[t ?
1] : pos[t]. Similar features are extracted for named
entity types (ner[t]), and dependency relation labels
(dep[t]).
Our intuition is these chunking features should al-
low for learning which types of words tend to be
answers. For instance, we expect adverbs to be as-
signed lower feature weights as they are rarely a
part of answer, while prepositions may have differ-
ent feature weights depending on their context. For
instance, of in kind of silly has an adjective on the
right, and is unlikely to be the Beginning of an an-
swer to a TREC-style question, as compared to in
when paired with a question on time, such as seen in
an answer in 90 days, where the preposition is fol-
lowed by a number then a noun.
Feature Description
edit=X type of edit feature. X: DEL,
DEL SUBTREE, DEL LEAF,
REN POS, REN DEP, REN POS DEP
or ALIGN.
X pos=?
X ner=?
X dep=?
Delete features. X is either DEL,
DEL SUBTREE or DEL LEAF. ?
represents the corresponding
POS/NER/DEP of the current token.
Xpos from=?f
Xpos to=?t
Xpos f t=?f ?t
Xner from=?f
Xner to=?t
Xner f t=?f ?t
Xdep from=?f
Xdep to=?t
Xdep f t=?f ?t
Rename features. X is either
REN POS, REN DEP or
REN POS DEP. Suppose word f in
answer is renamed to word t in
question, then ?f and ?t represent
corresponding POS/NER/DEP of f
and t.
align pos=?
align ner=?
align dep=?
Align features. ? represents the
corresponding POS/NER/DEP of the
current token.
Table 4: Features based on edit script for answer se-
quence tagging.
Question-type Chunking features do not capture
the connection between question word and an-
swer types. Thus they have to be combined
with question types. For instance, how many
questions are usually associated with numeric an-
swer types. We encode each major question-
type: who, whom, when, where, how many, how
much, how long, and then for each token, we
combine the question term with its chunking fea-
tures described in (most tokens have different fea-
tures because they have different POS/NER/DEP
types). One feature example of the QA pair
how much/100 dollars for the word 100 would be:
qword=how much|pos[t]=CD|pos[t+1]=NNS. We ex-
pect high weight for this feature since it is a good
pattern for matching question type and answer type.
Similar features also apply to what, which, why and
how questions, even though they do not indicate an
answer type as clearly as how much does.
Some extra features are designed for what/which
questions per required answer types. The question
862
dependency tree is analyzed and the Lexical Answer
Type (LAT) is extracted. The following are some
examples of LAT for what questions:
? color: what is Crips? gang color?
? animal: what kind of animal is an agouti?
The extra LAT=? feature is also used with chunking
features for what/which questions.
There is significant prior work in building spe-
cialized templates or classifiers for labeling question
types (Hermjakob, 2001; Li and Roth, 2002; Zhang
and Lee, 2003; Hacioglu and Ward, 2003; Metzler
and Croft, 2005; Blunsom et al, 2006; Moschitti
et al, 2007). We designed our shallow question
type features based on the intuitions of these prior
work, with the goal of having a relatively compact
approach that still extracts useful predictive signal.
One possible drawback, however, is that if an LAT is
not observed during training but shows up in testing,
the sequence tagger would not know which answer
type to associate with the question. In this case it
falls back to the more general qword=? feature and
will most likely pick the type of answers that are
mostly associated with what questions in training.
Edit script Our TED module produces an edit
trace for each word in a candidate sentence: the
word is either deleted, renamed (if there is a word
of the same lemma in the question tree) or strictly
aligned (if there is an identical node in the question
tree). A word in the deleted edit sequence is a cue
that it could be the answer. A word being aligned
suggests it is less likely to be an answer. Thus for
each word we extract features based on its edit type,
shown in Table 4.
These features are also appended with the token?s
POS/NER/DEP information. For instance, a deleted
noun usually carries higher edit feature weights than
an aligned adjective.
Alignment distance We observed that a candidate
answer often appears close to an aligned word (i.e.,
answer tokens tend to be located ?nearby? portions
of text that align across the pair), especially in com-
pound noun constructions, restrictive clauses, prepo-
sition phrases, etc. For instance, in the following
pair, the answer Limp Bizkit comes from the leading
compound noun:
? What is the name of Durst ?s group?
? Limp Bizkit lead singer Fred Durst did a lot ...
Past work has designed large numbers of specific
templates aimed at these constructions (Soubbotin,
2001; Ravichandran et al, 2003; Clark et al, 2003;
Sneiders, 2002). Here we use a single general fea-
ture that we expect to pick up much of this signal,
without the significant feature engineering.
Thus we incorporated a simple feature to roughly
model this phenomenon. It is defined as the distance
to the nearest aligned nonstop word in the original
word order. In the above example, the only aligned
nonstop word is Durst. Then this nearest alignment
distance feature for the word Limp is:
nearest dist to align(Limp):5
This is the only integer-valued feature. All other
features are binary-valued. Note this feature does
not specify answer types: an adverb close to an
aligned word can also be wrongly taken as a strong
candidate. Thus we also include a version of the
POS/NER/DEP based feature for each token:
? nearest dist pos(Limp)=NNP
? nearest dist dep(Limp)=NMOD
? nearest dist ner(Limp)=B-PERSON
3.3 Overproduce-and-vote
We make an assumption that each sentence produces
a candidate answer and then vote among all answer
candidates to select the most-voted as the answer to
the original question. Specifically, this overproduce-
and-vote strategy applies voting in two places:
1. If there are overlaps between two answer candi-
dates, a partial vote is performed. For instance,
for a when question, if one answer candidate is
April , 1994 and the other is 1994, then besides
the base vote of 1, both candidates have an ex-
tra partial vote of #overlap/#total words = 1/4. We
call this adjusted vote.
2. If the CRF fails to find an answer, we still try to
?force? an answer out of the tagged sequence,
O?s). thus forced vote. Due to its lower credi-
bility (the sequence tagger does not think it is
an answer), we manually downweight the pre-
diction score by a factor of 0.1 (divide by 10).
863
During what war d id Nimi tz serve ?
O O:0.921060 Conant
O O:0.991168 had
O O:0.997307 been
O O:0.998570 a
O O:0.998608 photographer
O O:0.999005 f o r
O O:0.877619 Adm
O O:0.988293 .
O O:0.874101 Chester
O O:0.924568 Nimi tz
O O:0.970045 dur ing
B?ANS O:0.464799 World
I?ANS O:0.493715 War
I?ANS O:0.449017 I I
O O:0.915448 .
Figure 3: A sample sequence tagging output that
fails to predict an answer. From line 2 on, the first
column is the reference output and the second col-
umn is the model output with the marginal probabil-
ity for predicated labels. Note that World War II has
much lower probabilities as an O than others.
The modified score for an answer candidate is thus:
total vote = adjusted vote + 0.1 ? forced vote. To
compute forced vote, we make the following obser-
vation. Sometimes the sequence tagger does not tag
an answer in a candidate sentence at all, if there
is not enough probability mass accumulated for B-
ANS. However, a possible answer can still be caught
if it has an ?outlier? marginal probability. Figure 3
shows an example. The answer candidate World War
II has a much lower marginal probability as an ?O?
but still not low enough to be part of B-ANS/I-ANS.
To catch such an outlier, we use Median Absolute
Deviation (MAD), which is the median of the abso-
lute deviation from the median of a data sequence.
Given a data sequence x, MAD is defined as:
MAD(x) = median(| x?median(x) |)
Compared to mean value and standard deviation,
MAD is more robust against the influence of out-
liers since it does not directly depend on them. We
select those words whose marginal probability is 50
times of MAD away from the median of the whole
sequence as answer candidates. They contribute to
the forced vote. Downweight ratio (0.1) and MAD
System Train Prec.% Rec.% F1%
CRF
TRAIN 55.7 43.8 49.1
TRAIN-ALL 67.2 50.6 57.7
CRF
+WNsearch
TRAIN 58.6 46.1 51.6
TRAIN-ALL 66.7 49.4 56.8
CRF forced
TRAIN 54.5 53.9 54.2
TRAIN-ALL 60.9 59.6 60.2
CRF forced
+WNsearch
TRAIN 55.2 53.9 54.5
TRAIN-ALL 63.6 62.9 63.3
Table 5: Performance on TEST. ?CRF? only takes
votes from candidates tagged by the sequence tag-
ger. ?CRF forced? (described in ?3.3) further col-
lects answer candidates from sentences that CRF
does not tag an answer by detecting outliers.
ratio (50) were hand-tuned on DEV.9
4 Experiments
4.1 QA Results
The dataset listed in Table 2 was not designed to
include an answer for each positive answer sen-
tence, but only a binary indicator on whether a sen-
tence contains an answer. We used the answer pat-
tern files (in Perl regular expressions) released along
with TREC8-13 to pinpoint the exact answer frag-
ments. Then we manually checked TRAIN, DEV, and
TEST for errors. TRAIN-ALL already came as a noisy
dataset so we did not manually clean it, also due to
its large size.
We trained on only the positive examples of
TRAIN and TRAIN-ALL separately with CRFsuite
(Okazaki, 2007). The reason for training solely with
positive examples is that they only constitute 10% of
all training data and if trained on all, the CRF tagger
was very biased on negative examples and reluctant
to give an answer for most of the questions. The
CRF tagger attempted an answer for about 2/3 of all
questions when training on just positive examples.
DEV was used to help design features. A practi-
cal benefit of our compact approach is that an entire
round of feature extraction, training on TRAIN and
testing on DEV took less than one minute. Table 5
9One might further improve this by leveraging the probabil-
ity of a sentence containing an answer from the QA pair ranker
described in Section 2 or via the conditional probability of the
sequence labels, p(y | x), under the CRF.
864
reports F1 scores on both the positive and negative
examples of TEST.
Our baseline model, which aligns the question
word with some content word in the answer sen-
tence,10 achieves 31.4% in F1. This model does not
require any training. ?CRF? only takes votes from
those sentences with an identified answer. It has the
best precision among all models. ?CRF forced? also
detects outliers from sentences not tagged with an
answer. Large amount of training data, even noisy,
is helpful. In general TRAIN-ALL is able to boost the
F1 value by 7 ? 8%. Also, the overgenerate-and-
vote strategy, used by the ?forced? approach, greatly
increased recall and achieved the best F1 value.
We also experimented with the two methods uti-
lizing WordNet in Section 2.2 , i.e., WNsearch and
WNfeature. In general, WNsearch helps F1 and
yields the best score (63.3%) for this task. For
WNfeature11 we observed that the CRF model con-
verged to a larger objective likelihood with these
features. However, it did not make a difference in
F1 after overgenerate-and-vote.
Finally, we found it difficult to do a head-to-head
comparison with other QA systems on this task.12
Thus we contribute this dataset to the community,
hoping to solicit direct comparisons in the future.
Also, we believe our chunking and question-type
features capture many intuitions most current QA
systems rely on, while our novel features are based
on TED. We further conduct an ablation test to com-
pare traditional and new QA features.
4.2 Ablation Test
We did an ablation test for each of the four types of
features. Note that the question type features are
used in combination with chunking features (e.g.,
qword=how much|pos[t]=CD|pos[t+1]=NN), while
the chunking feature is defined over POS/NER/DEP
10This only requires minimal modification to the original
TED algorithm: the question word is aligned with a certain
word in the answer tree instead of being inserted. Then the
whole subtree headed by the aligned word counts as the answer.
11These are binary features indicating whether an answer
candidate has a WordNet relation ( c.f. ?2.2) with the LAT.
For instance, tennis is a hyponym of the LAT word sport in the
what sport question in Figure 1.
12Reasons include: most available QA systems either retrieve
sentences from the web, have different preprocessing steps, or
even include templates learned from our test set.
CRF Forced CRF Forced
All 49.1 54.2 -above 3 19.4 25.3
-POS 44.7 48.9 -EDIT 44.3 47.5
-NER 44.0 50.8 -ALIGN 47.4 51.1
-DEP 49.4 54.5 -above 2 40.5 42.0
Table 6: F1 based on feature ablation tests.
NONE CHUNKING CHUNKING+TEDFeatures Used
0
10
20
30
40
50
60
F1(%
) 31.4
40.5
49.1
42.0
54.2
F1 with Different Features
BaselineCRFCRF forced
Figure 4: Impact of adding features based on chunk-
ing and question-type (CHUNKING) and tree edits
(TED), e.g., EDIT and ALIGN.
separately. We tested the CRF model with deletion
of one of the following features each time:
? POS, NER or DEP. These features are all com-
bined with question types.
? The three of the above. Deletion of these fea-
tures also deletes question type feature implic-
itly.
? EDIT. Features extracted from edit script.
? ALIGN. Alignment distance features.
? The two of the above, based on the TED model.
Table 6 shows the F1 scores of ablation test when
trained on TRAIN. NER and EDIT are the two single
most significant features. NER is important because
it closely relates question types with answer entity
types (e.g., qword=who|ner[t]=PERSON). EDIT is
also important because it captures the syntactic asso-
ciation between question tree and answer tree. Tak-
ing out all three POS/NER/DEP features means the
chunking and question type features do not fire any-
more. This has the biggest impact on F1. Note the
feature redundancy here: the question type features
are combined with all three POS/NER/DEP features
865
thus taking out a single one does not decrease per-
formance much. However, since TED related fea-
tures do not combine question type features, taking
out all three POS/NER/DEP features decreases F1 by
30%. Without TED related features (both EDIT and
ALIGN) F1 also drops more than 10%.
Figure 4 is a bar chart showing how much im-
provement each feature brings. While having a
baseline model with 31.4% in F1, traditional fea-
tures based on POS/DEP/NER and question types
brings a 10% increase with a simple sequence tag-
ging model (second bar labeled ?CHUNKING? in
the figure). Furthermore, adding TED based features
to the model boosted F1 by another 10%.
5 Conclusion
Answer extraction is an essential task for any text-
based question-answering system to perform. In this
paper, we have cast answer extraction as a sequence
tagging problem by deploying a fast and compact
CRF model with simple features that capture many
of the intuitions in prior ?deep pipeline? approaches.
We introduced novel features based on TED that
boosted F1 score by 10% compared with the use of
more standard features. Besides answer extraction,
our modified design of the TED model is the state
of the art in the task of ranking QA pairs. Finally,
to improve the community?s ability to evaluate QA
components without requiring increasingly imprac-
tical end-to-end implementations, we have proposed
answer extraction as a subtask worth evaluating in
its own right, and contributed a dataset that could
become a potential standard for this purpose. We
believe all these developments will contribute to the
continuing improvement of QA systems in the fu-
ture.
Acknowledgement We thank Vulcan Inc. for
funding this work. We also thank Michael Heil-
man and Mengqiu Wang for helpful discussion and
dataset, and the three anonymous reviewers for in-
sightful comments.
References
Nikolaus Augsten, Denilson Barbosa, Michael Bo?hlen,
and Themis Palpanas. 2010. TASM: Top-k Approx-
imate Subtree Matching. In Proceedings of the Inter-
national Conference on Data Engineering (ICDE-10),
pages 353?364, Long Beach, California, USA, March.
IEEE Computer Society.
D.M. Bikel, R. Schwartz, and R.M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
learning, 34(1):211?231.
P. Bille. 2005. A survey on tree edit distance and
related problems. Theoretical Computer Science,
337(1):217?239.
P. Blunsom, K. Kocik, and J.R. Curran. 2006. Question
classification with log-linear models. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, pages 615?616. ACM.
Peter Clark, Vinay Chaudhri, Sunil Mishra, Je?ro?me
Thome?re?, Ken Barker, and Bruce Porter. 2003. En-
abling domain experts to convey questions to a ma-
chine: a modified, template-based approach. In
Proceedings of the 2nd international conference on
Knowledge Capture, pages 13?19, New York, NY,
USA. ACM.
Shilin Ding, Gao Cong, Chin yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
texts and answers of questions from online forums. In
In Proceedings of ACL-08: HLT.
D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek,
A.A. Kalyanpur, A. Lally, J.W. Murdock, E. Nyberg,
J. Prager, et al 2010. Building Watson: An overview
of the DeepQA project. AI Magazine, 31(3):59?79.
K. Hacioglu and W. Ward. 2003. Question classifica-
tion with support vector machines and error correcting
codes. In Proceedings of NAACL 2003, short papers,
pages 28?30.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL 2010, pages 1011?1019, Los Angeles, Cali-
fornia.
U. Hermjakob. 2001. Parsing and question classification
for question answering. In Proceedings of the work-
shop on Open-domain question answering-Volume 12,
pages 1?6.
Milen Kouylekov and Bernardo Magnini. 2005. Recog-
nizing textual entailment with tree edit distance algo-
rithms. In PASCAL Challenges on RTE, pages 17?20.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
866
In Proceedings of the Eighteenth International Confer-
ence on Machine Learning, pages 282?289, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL 2002, pages 1?7.
B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2002.
Is it the right answer?: exploiting web redundancy for
answer validation. In Proceedings of ACL 2002, pages
425?432.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL 2005, pages 91?98.
D. Metzler and W.B. Croft. 2005. Analysis of statistical
question classification for fact-based questions. Infor-
mation Retrieval, 8(3):481?504.
D. Moldovan, M. Pas?ca, S. Harabagiu, and M. Surdeanu.
2003. Performance issues and error analysis in an
open-domain question answering system. ACM Trans-
actions on Information Systems (TOIS), 21(2):133?
154.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar.
2007. Exploiting syntactic and shallow semantic ker-
nels for question answer classification. In Proceedings
of ACL 2007, volume 45, page 776.
Naoaki Okazaki. 2007. CRFsuite: a fast implementation
of Conditional Random Fields (CRFs).
A. Penas, A. Rodrigo, V. Sama, and F. Verdejo. 2008.
Testing the reasoning for question answering valida-
tion. Journal of Logic and Computation, 18(3):459?
474.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2004.
Mapping Dependencies Trees: An Application to
Question Answering. In Proceedings of the 8th In-
ternational Symposium on Artificial Intelligence and
Mathematics, Fort Lauderdale, Florida.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP
1996, volume 1, pages 133?142.
Deepak Ravichandran, Abraham Ittycheriah, and Salim
Roukos. 2003. Automatic derivation of surface text
patterns for a maximum entropy based question an-
swering system. In Proceedings of NAACL 2003, short
papers, pages 85?87, Stroudsburg, PA, USA.
D. C. Reis, P. B. Golgher, A. S. Silva, and A. F. Laen-
der. 2004. Automatic web news extraction using tree
edit distance. In Proceedings of the 13th international
conference on World Wide Web, pages 502?511, New
York, NY, USA. ACM.
A?. Rodrigo, A. Pen?as, and F. Verdejo. 2009. Overview of
the answer validation exercise 2008. Evaluating Sys-
tems for Multilingual and Multimodal Information Ac-
cess, pages 296?313.
H. Shima, N. Lao, E. Nyberg, and T. Mitamura. 2008.
Complex cross-lingual question answering as sequen-
tial classification and multi-document summarization
task. In Proceedings of NTICIR-7 Workshop, Japan.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Translation,
pages 23?30, New York, June.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In CIKM ?07:
Proceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
pages 623?632, New York, NY, USA. ACM.
E. Sneiders. 2002. Automated question answering:
template-based approach. Ph.D. thesis, KTH.
Martin M. Soubbotin. 2001. Patterns of potential answer
expressions as clues to the right answers. In Proceed-
ings of the Tenth Text REtrieval Conference (TREC
2001).
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of ACL 2010, pages 1164?1172,
Stroudsburg, PA, USA.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 22?32,
Prague, Czech Republic, June.
D. Zhang and W.S. Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26?32. ACM.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related prob-
lems. SIAM J. Comput., 18(6):1245?1262, December.
867
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854?864,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bucking the Trend: Large-Scale Cost-Focused Active Learning for
Statistical Machine Translation
Michael Bloodgood
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
bloodgood@jhu.edu
Chris Callison-Burch
Center for Language and
Speech Processing
Johns Hopkins University
Baltimore, MD 21211
ccb@cs.jhu.edu
Abstract
We explore how to improve machine trans-
lation systems by adding more translation
data in situations where we already have
substantial resources. The main challenge
is how to buck the trend of diminishing re-
turns that is commonly encountered. We
present an active learning-style data solic-
itation algorithm to meet this challenge.
We test it, gathering annotations via Ama-
zon Mechanical Turk, and find that we get
an order of magnitude increase in perfor-
mance rates of improvement.
1 Introduction
Figure 1 shows the learning curves for two state of
the art statistical machine translation (SMT) sys-
tems for Urdu-English translation. Observe how
the learning curves rise rapidly at first but then a
trend of diminishing returns occurs: put simply,
the curves flatten.
This paper investigates whether we can buck the
trend of diminishing returns, and if so, how we can
do it effectively. Active learning (AL) has been ap-
plied to SMT recently (Haffari et al, 2009; Haffari
and Sarkar, 2009) but they were interested in start-
ing with a tiny seed set of data, and they stopped
their investigations after only adding a relatively
tiny amount of data as depicted in Figure 1.
In contrast, we are interested in applying AL
when a large amount of data already exists as is
the case for many important lanuage pairs. We de-
velop an AL algorithm that focuses on keeping an-
notation costs (measured by time in seconds) low.
It succeeds in doing this by only soliciting trans-
lations for parts of sentences. We show that this
gets a savings in human annotation time above and
beyond what the reduction in # words annotated
would have indicated by a factor of about three
and speculate as to why.
0 2 4 6 8 10x 1040
5
10
15
20
25
30
Number of Sentences in Training Data
BLEU 
Score
JSyntax and JHier Learning Curves on the LDC Urdu?English Language Pack (BLEU vs Sentences)
 
 
jHierjSyntax
as far as previous AL for SMT research studies were conducted
where we begin our main investigations into bucking the trend of diminishing returns
Figure 1: Syntax-based and Hierarchical Phrase-
Based MT systems? learning curves on the LDC
Urdu-English language pack. The x-axis measures
the number of sentence pairs in the training data.
The y-axis measures BLEU score. Note the di-
minishing returns as more data is added. Also
note how relatively early on in the process pre-
vious studies were terminated. In contrast, the
focus of our main experiments doesn?t even be-
gin until much higher performance has already
been achieved with a period of diminishing returns
firmly established.
We conduct experiments for Urdu-English
translation, gathering annotations via Amazon
Mechanical Turk (MTurk) and show that we can
indeed buck the trend of diminishing returns,
achieving an order of magnitude increase in the
rate of improvement in performance.
Section 2 discusses related work; Section 3
discusses preliminary experiments that show the
guiding principles behind the algorithm we use;
Section 4 explains our method for soliciting new
translation data; Section 5 presents our main re-
sults; and Section 6 concludes.
854
2 Related Work
Active learning has been shown to be effective
for improving NLP systems and reducing anno-
tation burdens for a number of NLP tasks (see,
e.g., (Hwa, 2000; Sassano, 2002; Bloodgood
and Vijay-Shanker, 2008; Bloodgood and Vijay-
Shanker, 2009b; Mairesse et al, 2010; Vickrey et
al., 2010)). The current paper is most highly re-
lated to previous work falling into three main ar-
eas: use of AL when large corpora already exist;
cost-focused AL; and AL for SMT.
In a sense, the work of Banko and Brill (2001)
is closely related to ours. Though their focus is
mainly on investigating the performance of learn-
ing methods on giant corpora many orders of mag-
nitude larger than previously used, they do lay out
how AL might be useful to apply to acquire data
to augment a large set cheaply because they rec-
ognize the problem of diminishing returns that we
discussed in Section 1.
The second area of work that is related to ours is
previous work on AL that is cost-conscious. The
vast majority of AL research has not focused on
accurate cost accounting and a typical assumption
is that each annotatable has equal annotation cost.
An early exception in the AL for NLP field was
the work of Hwa (2000), which makes a point of
using # of brackets to measure cost for a syntac-
tic analysis task instead of using # of sentences.
Another relatively early work in our field along
these lines was the work of Ngai and Yarowsky
(2000), which measured actual times of annota-
tion to compare the efficacy of rule writing ver-
sus annotation with AL for the task of BaseNP
chunking. Osborne and Baldridge (2004) argued
for the use of discriminant cost over unit cost for
the task of Head Phrase Structure Grammar parse
selection. King et al (2004) design a robot that
tests gene functions. The robot chooses which
experiments to conduct by using AL and takes
monetary costs (in pounds sterling) into account
during AL selection and evaluation. Unlike our
situation for SMT, their costs are all known be-
forehand because they are simply the cost of ma-
terials to conduct the experiments, which are al-
ready known to the robot. Hachey et al (2005)
showed that selectively sampled examples for an
NER task took longer to annotate and had lower
inter-annotator agreement. This work is related to
ours because it shows that how examples are se-
lected can impact the cost of annotation, an idea
we turn around to use for our advantage when de-
veloping our data selection algorithm. Haertel et
al. (2008) emphasize measuring costs carefully for
AL for POS tagging. They develop a model based
on a user study that can estimate the time required
for POS annotating. Kapoor et al (2007) assign
costs for AL based on message length for a voice-
mail classification task. In contrast, we show for
SMT that annotation times do not scale according
to length in words and we show our method can
achieve a speedup in annotation time above and
beyond what the reduction in words would indi-
cate. Tomanek and Hahn (2009) measure cost by #
of tokens for an NER task. Their AL method only
solicits labels for parts of sentences in the interest
of reducing annotation effort. Along these lines,
our method is similar in the respect that we also
will only solicit annotation for parts of sentences,
though we prefer to measure cost with time and
we show that time doesn?t track with token length
for SMT.
Haffari et al (2009), Haffari and Sarkar (2009),
and Ambati et al (2010) investigate AL for SMT.
There are two major differences between our work
and this previous work. One is that our intended
use cases are very different. They deal with the
more traditional AL setting of starting from an ex-
tremely small set of seed data. Also, by SMT stan-
dards, they only add a very tiny amount of data
during AL. All their simulations top out at 10,000
sentences of labeled data and the models learned
have relatively low translation quality compared to
the state of the art.
On the other hand, in the current paper, we
demonstrate how to apply AL in situations where
we already have large corpora. Our goal is to buck
the trend of diminishing returns and use AL to
add data to build some of the highest-performing
MT systems in the world while keeping annota-
tion costs low. See Figure 1 from Section 1, which
contrasts where (Haffari et al, 2009; Haffari and
Sarkar, 2009) stop their investigations with where
we begin our studies.
The other major difference is that (Haffari et al,
2009; Haffari and Sarkar, 2009) measure annota-
tion cost by # of sentences. In contrast, we bring
to light some potential drawbacks of this practice,
showing it can lead to different conclusions than
if other annotation cost metrics are used, such as
time and money, which are the metrics that we use.
855
3 Simulation Experiments
Here we report on results of simulation experi-
ments that help to illustrate and motivate the de-
sign decisions of the algorithm we present in Sec-
tion 4. We use the Urdu-English language pack1
from the Linguistic Data Consortium (LDC),
which contains ? 88000 Urdu-English sentence
translation pairs, amounting to? 1.7 million Urdu
words translated into English. All experiments in
this paper evaluate on a genre-balanced split of the
NIST2008 Urdu-English test set. In addition, the
language pack contains an Urdu-English dictio-
nary consisting of ? 114000 entries. In all the ex-
periments, we use the dictionary at every iteration
of training. This will make it harder for us to show
our methods providing substantial gains since the
dictionary will provide a higher base performance
to begin with. However, it would be artificial to
ignore dictionary resources when they exist.
We experiment with two translation models: hi-
erarchical phrase-based translation (Chiang, 2007)
and syntax augmented translation (Zollmann and
Venugopal, 2006), both of which are implemented
in the Joshua decoder (Li et al, 2009). We here-
after refer to these systems as jHier and jSyntax,
respectively.
We will now present results of experiments with
different methods for growing MT training data.
The results are organized into three areas of inves-
tigations:
1. annotation costs;
2. managing uncertainty; and
3. how to automatically detect when to stop so-
liciting annotations from a pool of data.
3.1 Annotation Costs
We begin our cost investigations with four sim-
ple methods for growing MT training data: ran-
dom, shortest, longest, and VocabGrowth sen-
tence selection. The first three methods are self-
explanatory. VocabGrowth (hereafter VG) selec-
tion is modeled after the best methods from previ-
ous work (Haffari et al, 2009; Haffari and Sarkar,
2009), which are based on preferring sentences
that contain phrases that occur frequently in un-
labeled data and infrequently in the so-far labeled
data. Our VG method selects sentences for transla-
tion that contain n-grams (for n in {1,2,3,4}) that
1LDC Catalog No.: LDC2006E110.
Init:
Go through all available training
data (labeled and unlabeled)
and obtain frequency counts for
every n-gram (n in {1, 2, 3, 4})
that occurs.
sortedNGrams? Sort n-grams by
frequency in descending order.
Loop
until stopping criterion (see Section 3.3) is met
1. trigger ? Go down sortedNGrams list
and find the first n-gram that isn?t covered in
the so far labeled training data.
2. selectedSentence? Find a sentence
that contains trigger.
3. Remove selectedSentence from unlabeled
data and add it to labeled training data.
End Loop
Figure 2: The VG sentence selection algorithm
do not occur at all in our so-far labeled data. We
call an n-gram ?covered? if it occurs at least once
in our so-far labeled data. VG has a preference
for covering frequent n-grams before covering in-
frequent n-grams. The VG method is depicted in
Figure 2.
Figure 3 shows the learning curves for both
jHier and jSyntax for VG selection and random
selection. The y-axis measures BLEU score (Pap-
ineni et al, 2002),which is a fast automatic way of
measuring translation quality that has been shown
to correlate with human judgments and is perhaps
the most widely used metric in the MT commu-
nity. The x-axis measures the number of sen-
tence translation pairs in the training data. The VG
curves are cut off at the point at which the stopping
criterion in Section 3.3 is met. From Figure 3 it
might appear that VG selection is better than ran-
dom selection, achieving higher-performing sys-
tems with fewer translations in the labeled data.
However, it is important to take care when mea-
suring annotation costs (especially for relatively
complicated tasks such as translation). Figure 4
shows the learning curves for the same systems
and selection methods as in Figure 3 but now the
x-axis measures the number of foreign words in
the training data. The difference between VG and
random selection now appears smaller.
For an extreme case, to illustrate the ramifica-
856
0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,0000
5
10
15
20
25
30jHier and jSyntax: VG vs Random selection (BLEU vs Sents)
Number of Sentence Pairs in the Training Data
BLEU
 Score
 
 
jHier: random selectionjHier: VG selectionjSyntax: random selectionjSyntax: VG selection
where we will start our main experiments
where previous AL for SMT research stopped their experiments
Figure 3: Random vs VG selection. The x-axis
measures the number of sentence pairs in the train-
ing data. The y-axis measures BLEU score.
tions of measuring translation annotation cost by #
of sentences versus # of words, consider Figures 5
and 6. They both show the same three selection
methods but Figure 5 measures the x-axis by # of
sentences and Figure 6 measures by # of words. In
Figure 5, one would conclude that shortest is a far
inferior selection method to longest but in Figure 6
one would conclude the opposite.
Measuring annotation time and cost in dol-
lars are probably the most important measures
of annotation cost. We can?t measure these for
the simulated experiments but we will use time
(in seconds) and money (in US dollars) as cost
measures in Section 5, which discusses our non-
simulated AL experiments. If # sentences or #
words track these other more relevant costs in pre-
dictable known relationships, then it would suffice
to measure # sentences or # words instead. But it?s
clear that different sentences can have very differ-
ent annotation time requirements according to how
long and complicated they are so we will not use
# sentences as an annotation cost any more. It is
not as clear how # words tracks with annotation
time. In Section 5 we will present evidence show-
ing that time per word can vary considerably and
also show a method for soliciting annotations that
reduces time per word by nearly a factor of three.
As it is prudent to evaluate using accurate cost
accounting, so it is also prudent to develop new
AL algorithms that take costs carefully into ac-
count. Hence, reducing annotation time burdens
0 0.5 1 1.5 2x 1060
5
10
15
20
25
30jHier and jSyntax: VG vs Random selection (BLEU vs FWords)
Number of Foreign Words in Training Data
BLEU
 Score
 
 
jHier: random selectionjHier: VG selectionjSyntax: random selectionjSyntax: VG selection
Figure 4: Random vs VG selection. The x-axis
measures the number of foreign words in the train-
ing data. The y-axis measures BLEU score.
instead of the # of sentences translated (which
might be quite a different thing) will be a corner-
stone of the algorithm we describe in Section 4.
3.2 Managing Uncertainty
One of the most successful of all AL methods de-
veloped to date is uncertainty sampling and it has
been applied successfully many times (e.g.,(Lewis
and Gale, 1994; Tong and Koller, 2002)). The
intuition is clear: much can be learned (poten-
tially) if there is great uncertainty. However, with
MT being a relatively complicated task (compared
with binary classification, for example), it might
be the case that the uncertainty approach has to
be re-considered. If words have never occurred
in the training data, then uncertainty can be ex-
pected to be high. But we are concerned that if a
sentence is translated for which (almost) no words
have been seen in training yet, though uncertainty
will be high (which is usually considered good for
AL), the word alignments may be incorrect and
then subsequent learning from that translation pair
will be severely hampered.
We tested this hypothesis and Figure 7 shows
empirical evidence that it is true. Along with VG,
two other selection methods? learning curves are
charted in Figure 7: mostNew, which prefers to
select those sentences which have the largest # of
unseen words in them; and moderateNew, which
aims to prefer sentences that have a moderate #
of unseen words, preferring sentences with ? ten
857
0 2 4 6 8 10x 1040
5
10
15
20
25 jHiero: Random, Shortest, and Longest selection
BLEU
 Score
Number of Sentences in Training Data
 
 
randomshortestlongest
Figure 5: Random vs Shortest vs Longest selec-
tion. The x-axis measures the number of sentence
pairs in the training data. The y-axis measures
BLEU score.
unknown words in them. One can see that most-
New underperforms VG. This could have been due
to VG?s frequency component, which mostNew
doesn?t have. But moderateNew also doesn?t have
a frequency preference so it is likely that mostNew
winds up overwhelming the MT training system,
word alignments are incorrect, and less is learned
as a result. In light of this, the algorithm we de-
velop in Section 4 will be designed to avoid this
word alignment danger.
3.3 Automatic Stopping
The problem of automatically detecting when to
stop AL is a substantial one, discussed at length
in the literature (e.g., (Bloodgood and Vijay-
Shanker, 2009a; Schohn and Cohn, 2000; Vla-
chos, 2008)). In our simulation, we stop VG once
all n-grams (n in {1,2,3,4}) have been covered.
Though simple, this stopping criterion seems to
work well as can be seen by where the curve for
VG is cut off in Figures 3 and 4. It stops af-
ter 1,293,093 words have been translated, with
jHier?s BLEU=21.92 and jSyntax?s BLEU=26.10
at the stopping point. The ending BLEU scores
(with the full corpus annotated) are 21.87 and
26.01 for jHier and jSyntax, respectively. So
our stopping criterion saves 22.3% of the anno-
tation (in terms of words) and actually achieves
slightly higher BLEU scores than if all the data
were used. Note: this ?less is more? phenomenon
0 0.5 1 1.5 2x 1060
5
10
15
20
25
Number of Foreign Words in Training Data
BLEU
 Score
jHiero: Longest, Shortest, and Random Selection
 
 
randomshortestlongest
Figure 6: Random vs Shortest vs Longest selec-
tion. The x-axis measures the number of foreign
words in the training data. The y-axis measures
BLEU score.
has been commonly observed in AL settings (e.g.,
(Bloodgood and Vijay-Shanker, 2009a; Schohn
and Cohn, 2000)).
4 Highlighted N-Gram Method
In this section we describe a method for solicit-
ing human translations that we have applied suc-
cessfully to improving translation quality in real
(not simulated) conditions. We call the method the
Highlighted N-Gram method, or HNG, for short.
HNG solicits translations only for trigger n-grams
and not for entire sentences. We provide senten-
tial context, highlight the trigger n-gram that we
want translated, and ask for a translation of just the
highlighted trigger n-gram. HNG asks for transla-
tions for triggers in the same order that the triggers
are encountered by the algorithm in Figure 2. A
screenshot of our interface is depicted in Figure 8.
The same stopping criterion is used as was used in
the last section. When the stopping criterion be-
comes true, it is time to tap a new unlabeled pool
of foreign text, if available.
Our motivations for soliciting translations for
only parts of sentences are twofold, corresponding
to two possible cases. Case one is that a translation
model learned from the so-far labeled data will be
able to translate most of the non-trigger words in
the sentence correctly. Thus, by asking a human
to translate only the trigger words, we avoid wast-
ing human translation effort. (We will show in
858
0 0.5 1 1.5 2x 1060
5
10
15
20
25
Number of Foreign Words in Training Data
BLEU
 Score
jHiero: VG vs mostNew vs moderateNew
 
 
VGmostNewmoderateNew
Figure 7: VG vs MostNew vs ModerateNew se-
lection. The x-axis measures the number of sen-
tence pairs in the training data. The y-axis mea-
sures BLEU score.
!"#$% "& '() '* +,-./0)1 234 5678 9:-! !"#$ %$&'$ &( )*+ ;<= '$ >/?@3 /A  
>. +B!C D)C EF GH?I '3") D)+0) +&  .
"J& "J& "$K$!1 2L)M8 ':?3N !O#)P& GQ6- '& R7@* /& ST& ST& !9,8 UV)WX  
'8 ,"*)- !.( /0." 234 !.C 234 !D#8 EY).<3 '8 MH3 G:Z !"-[$% '8 R3\  
5#)T= 5#)] '3E& >'#)P8 ><&  .
^ : S_ <* '(* C+& +:Z '* /`$>a UH$GX "& 5,-.b '8 "$c 9* S_ /& <* dH#$!<)  
+& ? e(@)f e3<g : #1.2 #1.2 "(:Z  .
<* e@* ':) K) C+) E#) '* +$ /H0) +& <* G:I ' 3.45 ' '& ')C',$% "& 5#:68 +$  .
!1 ') '(,) 67$ !8 ' )9 GQ)PI '& UI ') .hX +& !.C !1 '$ "i3 !"-!f "(:Z ':) K)  
/H0)  ' .
Figure 8: Screenshot of the interface we used for
soliciting translations for triggers.
the next section that we even get a much larger
speedup above and beyond what the reduction in
number of translated words would give us.) Case
two is that a translation model learned from the so-
far labeled data will (in addition to not being able
to translate the trigger words correctly) also not be
able to translate most of the non-trigger words cor-
rectly. One might think then that this would be a
great sentence to have translated because the ma-
chine can potentially learn a lot from the transla-
tion. Indeed, one of the overarching themes of AL
research is to query examples where uncertainty is
greatest. But, as we showed evidence for in the
last section, for the case of SMT, too much un-
certainty could in a sense overwhelm the machine
and it might be better to provide new training data
in a more gradual manner. A sentence with large
#s of unseen words is likely to get word-aligned
incorrectly and then learning from that translation
could be hampered. By asking for a translation
of only the trigger words, we expect to be able to
circumvent this problem in large part.
The next section presents the results of experi-
ments that show that the HNG algorithm is indeed
practically effective. Also, the next section ana-
lyzes results regarding various aspects of HNG?s
behavior in more depth.
5 Experiments and Discussion
5.1 General Setup
We set out to see whether we could use the HNG
method to achieve translation quality improve-
ments by gathering additional translations to add
to the training data of the entire LDC language
pack, including its dictionary. In particular, we
wanted to see if we could achieve translation im-
provements on top of already state-of-the-art per-
forming systems trained already on the entire LDC
corpus. Note that at the outset this is an ambitious
endeavor (recall the flattening of the curves in Fig-
ure 1 from Section 1).
Snow et al (2008) explored the use of the Ama-
zon Mechanical Turk (MTurk) web service for
gathering annotations for a variety of natural lan-
guage processing tasks and recently MTurk has
been shown to be a quick, cost-effective way to
gather Urdu-English translations (Bloodgood and
Callison-Burch, 2010). We used the MTurk web
service to gather our annotations. Specifically, we
first crawled a large set of BBC articles on the in-
ternet in Urdu and used this as our unlabeled pool
from which to gather annotations. We applied the
HNG method from Section 4 to determine what to
post on MTurk for workers to translate.2 We gath-
ered 20,580 n-gram translations for which we paid
$0.01 USD per translation, giving us a total cost
of $205.80 USD. We also gathered 1632 randomly
chosen Urdu sentence translations as a control set,
for which we paid $0.10 USD per sentence trans-
lation.3
2For practical reasons we restricted ourselves to not con-
sidering sentences that were longer than 60 Urdu words, how-
ever.
3The prices we paid were not market-driven. We just
chose prices we thought were reasonable. In hindsight, given
how much quicker the phrase translations are for people we
could have had a greater disparity in price.
859
5.2 Accounting for Translation Time
MTurk returns with each assignment the ?Work-
TimeInSeconds.? This is the amount of time be-
tween when a worker accepts an assignment and
when the worker submits the completed assign-
ment. We use this value to estimate annotation
times.4
Figure 9 shows HNG collection versus random
collection from MTurk. The x-axis measures the
number of seconds of annotation time. Note that
HNG is more effective. A result that may be par-
ticularly interesting is that HNG results in a time
speedup by more than just the reduction in trans-
lated words would indicate. The average time to
translate a word of Urdu with the sentence post-
ings to MTurk was 32.92 seconds. The average
time to translate a word with the HNG postings to
MTurk was 11.98 seconds. This is nearly three
times faster. Figure 10 shows the distribution of
speeds (in seconds per word) for HNG postings
versus complete sentence postings. Note that the
HNG postings consistently result in faster transla-
tion speeds than the sentence postings5.
We hypothesize that this speedup comes about
because when translating a full sentence, there?s
the time required to examine each word and trans-
late them in some sense (even if not one-to-one)
and then there is an extra significant overhead time
to put it all together and synthesize into a larger
sentence translation. The factor of three speedup
is evidence that this overhead is significant effort
compared to just quickly translating short n-grams
from a sentence. This speedup is an additional
benefit of the HNG approach.
5.3 Bucking the Trend
We gathered translations for? 54,500 Urdu words
via the use of HNG on MTurk. This is a rela-
tively small amount, ? 3% of the LDC corpus.
Figure 11 shows the performance when we add
this training data to the LDC corpus. The rect-
4It?s imperfect because of network delays and if a person
is multitasking or pausing between their accept and submit
times. Nonetheless, the times ought to be better estimates as
they are taken over larger samples.
5The average speed for the HNG postings seems to be
slower than the histogram indicates. This is because there
were a few extremely slow outlier speeds for a handful of
HNG postings. These are almost certainly not cases when the
turker is working continuously on the task and so the average
speed we computed for the HNG postings might be slower
than the actual speed and hence the true speedup may even
be faster than indicated by the difference between the aver-
age speeds we reported.
0 1 2 3 4 5 6x 10521.6
21.8
22
22.2
22.4
22.6
22.8
Number of Seconds of Annotation Time
BLEU
 Score
jHier: HNG Collection vs Random Collection of Annotations from MTurk
 
 
randomHNG
Figure 9: HNG vs Random collection of new data
via MTurk. y-axis measures BLEU. x-axis mea-
sures annotation time in seconds.
angle around the last 700,000 words of the LDC
data is wide and short (it has a height of 0.9 BLEU
points and a width of 700,000 words) but the rect-
angle around the newly added translations is nar-
row and tall (a height of 1 BLEU point and a
width of 54,500 words). Visually, it appears we
are succeeding in bucking the trend of diminish-
ing returns. We further confirmed this by running
a least-squares linear regression on the points of
the last 700,000 words annotated in the LDC data
and also for the points in the new data that we ac-
quired via MTurk for $205.80 USD. We find that
the slope fit to our new data is 6.6245E-06 BLEU
points per Urdu word, or 6.6245 BLEU points for
a million Urdu words. The slope fit to the LDC
data is only 7.4957E-07 BLEU points per word,
or only 0.74957 BLEU points for a million words.
This is already an order of magnitude difference
that would make the difference between it being
worth adding more data and not being worth it;
and this is leaving aside the added time speedup
that our method enjoys.
Still, we wondered why we could not have
raised BLEU scores even faster. The main hur-
dle seems to be one of coverage. Of the 20,580 n-
grams we collected, only 571 (i.e., 2.77%) of them
ever even occur in the test set.
5.4 Beyond BLEU Scores
BLEU is an imperfect metric (Callison-Burch et
al., 2006). One reason is that it rates all ngram
860
0 20 40 60 80 100 1200
0.05
0.1
0.15
0.2
0.25
Time (in seconds) per foreign word translated
Relati
ve Fre
quenc
y
Histogram showing the distribution of translation speeds (in seconds per foreign word) when translations are collected via n?grams versus via complete sentences
 
 
n?gramssentencesaverage time perword for sentencesaverage time perword for n?grams
Figure 10: Distribution of translation speeds (in
seconds per word) for HNG postings versus com-
plete sentence postings. The y-axis measures rel-
ative frequency. The x-axis measures translation
speed in seconds per word (so farther to the left is
faster).
mismatches equally although some are much more
important than others. Another reason is it?s not
intuitive what a gain of x BLEU points means in
practice. Here we show some concrete example
translations to show the types of improvements
we?re achieving and also some examples which
suggest improvements we can make to our AL se-
lection algorithm in the future. Figure 12 shows a
prototypical example of our system working.
Figure 13 shows an example where the strategy
is working partially but not as well as it might. The
Urdu phrase was translated by turkers as ?gowned
veil?. However, since the word aligner just aligns
the word to ?gowned?, we only see ?gowned? in
our output. This prompts a number of discussion
points. First, the ?after system? has better transla-
tions but they?re not rewarded by BLEU scores be-
cause the references use the words ?burqah? or just
?veil? without ?gowned?. Second, we hypothesize
that we may be able to see improvements by over-
riding the automatic alignment software when-
ever we obtain a many-to-one or one-to-many (in
terms of words) translation for one of our trigger
phrases. In such cases, we?d like to make sure that
every word on the ?many? side is aligned to the
1 1.2 1.4 1.6 1.8x 10621
21.5
22
22.5
23
23.5 Bucking the Trend: JHiero Translation Quality versus Number of Foreign Words Annotated
BLEU 
Score
Number of Foreign Words Annotated
the approx. 54,500 foreign wordswe selectively sampled for annotation cost = $205.80last approx. 700,000 foreign words annotated in LDC data 
Figure 11: Bucking the trend: performance of
HNG-selected additional data from BBC web
crawl data annotated via Amazon Mechanical
Turk. y-axis measures BLEU. x-axis measures
number of words annotated.
Figure 12: Example of strategy working.
single word on the ?one? side. For example, we
would force both ?gowned? and ?veil? to be aligned
to the single Urdu word instead of allowing the au-
tomatic aligner to only align ?gowned?.
Figure 14 shows an example where our ?before?
system already got the translation correct without
the need for the additional phrase translation. This
is because though the ?before? system had never
seen the Urdu expression for ?12May?, it had seen
the Urdu words for ?12? and ?May? in isolation
and was able to successfully compose them. An
area of future work is to use the ?before? system to
determine such cases automatically and avoid ask-
ing humans to provide translations in such cases.
861
Figure 13: Example showing where we can im-
prove our selection strategy.
Figure 14: Example showing where we can im-
prove our selection strategy.
6 Conclusions and Future Work
We succeeded in bucking the trend of diminishing
returns and improving translation quality while
keeping annotation costs low. In future work we
would like to apply these ideas to domain adap-
tation (say, general-purpose MT system to work
for scientific domain such as chemistry). Also, we
would like to test with more languages, increase
the amount of data we can gather, and investigate
stopping criteria further. Also, we would like to
investigate increasing the efficiency of the selec-
tion algorithm by addressing issues such as the one
raised by the 12 May example presented earlier.
Acknowledgements
This work was supported by the Johns Hopkins
University Human Language Technology Center
of Excellence. Any opinions, findings, conclu-
sions, or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the sponsor.
References
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Proceedings of the Seventh con-
ference on International Language Resources and
Evaluation (LREC?10), Valletta, Malta, may. Euro-
pean Language Resources Association (ELRA).
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics, pages
26?33, Toulouse, France, July. Association for Com-
putational Linguistics.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using mechanical turk to build machine translation
evaluation sets. In Proceedings of the Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk, Los Angeles, California, June.
Association for Computational Linguistics.
Michael Bloodgood and K Vijay-Shanker. 2008. An
approach to reducing annotation costs for bionlp.
In Proceedings of the Workshop on Current Trends
in Biomedical Natural Language Processing, pages
104?105, Columbus, Ohio, June. Association for
Computational Linguistics.
Michael Bloodgood and K Vijay-Shanker. 2009a. A
method for stopping active learning based on stabi-
lizing predictions and the need for user-adjustable
stopping. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 39?47, Boulder, Colorado,
June. Association for Computational Linguistics.
Michael Bloodgood and K Vijay-Shanker. 2009b. Tak-
ing into account the differences between actively
and passively acquired data: The case of active
learning with support vector machines for imbal-
anced datasets. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL), pages 137?
140, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006), Trento, Italy.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 144?151, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and Peter McClanahan. 2008. Assessing the
862
costs of sampling methods in active learning for an-
notation. In Proceedings of ACL-08: HLT, Short Pa-
pers, pages 65?68, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine trans-
lation. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 181?189, Suntec,
Singapore, August. Association for Computational
Linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 415?423,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In Hinrich Schu?tze and Keh-
Yih Su, editors, Proceedings of the 2000 Joint SIG-
DAT Conference on Empirical Methods in Natural
Language Processing, pages 45?53. Association for
Computational Linguistics, Somerset, New Jersey.
Ashish Kapoor, Eric Horvitz, and Sumit Basu. 2007.
Selective supervision: Guiding supervised learn-
ing with decision-theoretic active learning. In
Manuela M. Veloso, editor, IJCAI 2007, Proceed-
ings of the 20th International Joint Conference on
Artificial Intelligence, Hyderabad, India, January 6-
12, 2007, pages 877?882.
Ross D. King, Kenneth E. Whelan, Ffion M.
Jones, Philip G. K. Reiser, Christopher H. Bryant,
Stephen H. Muggleton, Douglas B. Kell, and
Stephen G. Oliver. 2004. Functional genomic hy-
pothesis generation and experimentation by a robot
scientist. Nature, 427:247?252, 15 January.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In SI-
GIR ?94: Proceedings of the 17th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 3?12, New
York, NY, USA. Springer-Verlag New York, Inc.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
135?139, Athens, Greece, March. Association for
Computational Linguistics.
Francois Mairesse, Milica Gasic, Filip Jurcicek, Simon
Keizer, Jorge Prombonas, Blaise Thomson, Kai Yu,
and Steve Young. 2010. Phrase-based statistical
language generation using graphical models and ac-
tive learning. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Uppsala, Sweden, July. Association
for Computational Linguistics.
Grace Ngai and David Yarowsky. 2000. Rule writ-
ing or annotation: cost-efficient resource usage for
base noun phrase chunking. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
89?96, Boston, Massachusetts, USA, May 2 - May
7. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for japanese
word segmentation. In ACL ?02: Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 505?512, Morristown, NJ,
USA. Association for Computational Linguistics.
Greg Schohn and David Cohn. 2000. Less is more:
Active learning with support vector machines. In
Proc. 17th International Conf. on Machine Learn-
ing, pages 839?846. Morgan Kaufmann, San Fran-
cisco, CA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it
good? evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 254?263, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Katrin Tomanek and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 1039?1047, Suntec, Singapore,
August. Association for Computational Linguistics.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Re-
search (JMLR), 2:45?66.
David Vickrey, Oscar Kipersztok, and Daphne Koller.
2010. An active learning approach to finding related
terms. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), Uppsala, Sweden, July. Association for
Computational Linguistics.
863
Andreas Vlachos. 2008. A stopping criterion for
active learning. Computer Speech and Language,
22(3):295?312.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT06), New
York, New York.
864
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620?631,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Incremental Syntactic Language Models for Phrase-based Translation
Lane Schwartz
Air Force Research Laboratory
Wright-Patterson AFB, OH USA
lane.schwartz@wpafb.af.mil
Chris Callison-Burch
Johns Hopkins University
Baltimore, MD USA
ccb@cs.jhu.edu
William Schuler
Ohio State University
Columbus, OH USA
schuler@ling.ohio-state.edu
Stephen Wu
Mayo Clinic
Rochester, MN USA
wu.stephen@mayo.edu
Abstract
This paper describes a novel technique for in-
corporating syntactic knowledge into phrase-
based machine translation through incremen-
tal syntactic parsing. Bottom-up and top-
down parsers typically require a completed
string as input. This requirement makes it dif-
ficult to incorporate them into phrase-based
translation, which generates partial hypothe-
sized translations from left-to-right. Incre-
mental syntactic language models score sen-
tences in a similar left-to-right fashion, and are
therefore a good mechanism for incorporat-
ing syntax into phrase-based translation. We
give a formal definition of one such linear-
time syntactic language model, detail its re-
lation to phrase-based decoding, and integrate
the model with the Moses phrase-based trans-
lation system. We present empirical results
on a constrained Urdu-English translation task
that demonstrate a significant BLEU score im-
provement and a large decrease in perplexity.
1 Introduction
Early work in statistical machine translation viewed
translation as a noisy channel process comprised of
a translation model, which functioned to posit ad-
equate translations of source language words, and
a target language model, which guided the fluency
of generated target language strings (Brown et al,
This research was supported by NSF CAREER/PECASE
award 0447685, NSF grant IIS-0713448, and the European
Commission through the EuroMatrixPlus project. Opinions, in-
terpretations, conclusions, and recommendations are those of
the authors and are not necessarily endorsed by the sponsors or
the United States Air Force. Cleared for public release (Case
Number 88ABW-2010-6489) on 10 Dec 2010.
1990). Drawing on earlier successes in speech
recognition, research in statistical machine trans-
lation has effectively used n-gram word sequence
models as language models.
Modern phrase-based translation using large scale
n-gram language models generally performs well
in terms of lexical choice, but still often produces
ungrammatical output. Syntactic parsing may help
produce more grammatical output by better model-
ing structural relationships and long-distance depen-
dencies. Bottom-up and top-down parsers typically
require a completed string as input; this requirement
makes it difficult to incorporate these parsers into
phrase-based translation, which generates hypothe-
sized translations incrementally, from left-to-right.1
As a workaround, parsers can rerank the translated
output of translation systems (Och et al, 2004).
On the other hand, incremental parsers (Roark,
2001; Henderson, 2004; Schuler et al, 2010; Huang
and Sagae, 2010) process input in a straightforward
left-to-right manner. We observe that incremental
parsers, used as structured language models, pro-
vide an appropriate algorithmic match to incremen-
tal phrase-based decoding. We directly integrate in-
cremental syntactic parsing into phrase-based trans-
lation. This approach re-exerts the role of the lan-
guage model as a mechanism for encouraging syn-
tactically fluent translations.
The contributions of this work are as follows:
? A novel method for integrating syntactic LMs
into phrase-based translation (?3)
? A formal definition of an incremental parser for
1While not all languages are written left-to-right, we will
refer to incremental processing which proceeds from the begin-
ning of a sentence as left-to-right.
620
statistical MT that can run in linear-time (?4)
? Integration with Moses (?5) along with empiri-
cal results for perplexity and significant transla-
tion score improvement on a constrained Urdu-
English task (?6)
2 Related Work
Neither phrase-based (Koehn et al, 2003) nor hierar-
chical phrase-based translation (Chiang, 2005) take
explicit advantage of the syntactic structure of either
source or target language. The translation models in
these techniques define phrases as contiguous word
sequences (with gaps allowed in the case of hierar-
chical phrases) which may or may not correspond
to any linguistic constituent. Early work in statisti-
cal phrase-based translation considered whether re-
stricting translation models to use only syntactically
well-formed constituents might improve translation
quality (Koehn et al, 2003) but found such restric-
tions failed to improve translation quality.
Significant research has examined the extent to
which syntax can be usefully incorporated into sta-
tistical tree-based translation models: string-to-tree
(Yamada and Knight, 2001; Gildea, 2003; Imamura
et al, 2004; Galley et al, 2004; Graehl and Knight,
2004; Melamed, 2004; Galley et al, 2006; Huang
et al, 2006; Shen et al, 2008), tree-to-string (Liu
et al, 2006; Liu et al, 2007; Mi et al, 2008; Mi
and Huang, 2008; Huang and Mi, 2010), tree-to-tree
(Abeille? et al, 1990; Shieber and Schabes, 1990;
Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan
et al, 2006; Nesson et al, 2006; Zhang et al, 2007;
DeNeefe et al, 2007; DeNeefe and Knight, 2009;
Liu et al, 2009; Chiang, 2010), and treelet (Ding
and Palmer, 2005; Quirk et al, 2005) techniques
use syntactic information to inform the translation
model. Recent work has shown that parsing-based
machine translation using syntax-augmented (Zoll-
mann and Venugopal, 2006) hierarchical translation
grammars with rich nonterminal sets can demon-
strate substantial gains over hierarchical grammars
for certain language pairs (Baker et al, 2009). In
contrast to the above tree-based translation models,
our approach maintains a standard (non-syntactic)
phrase-based translation model. Instead, we incor-
porate syntax into the language model.
Traditional approaches to language models in
speech recognition and statistical machine transla-
tion focus on the use of n-grams, which provide a
simple finite-state model approximation of the tar-
get language. Chelba and Jelinek (1998) proposed
that syntactic structure could be used as an alterna-
tive technique in language modeling. This insight
has been explored in the context of speech recogni-
tion (Chelba and Jelinek, 2000; Collins et al, 2005).
Hassan et al (2007) and Birch et al (2007) use
supertag n-gram LMs. Syntactic language models
have also been explored with tree-based translation
models. Charniak et al (2003) use syntactic lan-
guage models to rescore the output of a tree-based
translation system. Post and Gildea (2008) investi-
gate the integration of parsers as syntactic language
models during binary bracketing transduction trans-
lation (Wu, 1997); under these conditions, both syn-
tactic phrase-structure and dependency parsing lan-
guage models were found to improve oracle-best
translations, but did not improve actual translation
results. Post and Gildea (2009) use tree substitution
grammar parsing for language modeling, but do not
use this language model in a translation system. Our
work, in contrast to the above approaches, explores
the use of incremental syntactic language models in
conjunction with phrase-based translation models.
Our syntactic language model fits into the fam-
ily of linear-time dynamic programming parsers de-
scribed in (Huang and Sagae, 2010). Like (Galley
and Manning, 2009) our work implements an in-
cremental syntactic language model; our approach
differs by calculating syntactic LM scores over all
available phrase-structure parses at each hypothesis
instead of the 1-best dependency parse.
The syntax-driven reordering model of Ge (2010)
uses syntax-driven features to influence word order
within standard phrase-based translation. The syn-
tactic cohesion features of Cherry (2008) encour-
ages the use of syntactically well-formed translation
phrases. These approaches are fully orthogonal to
our proposed incremental syntactic language model,
and could be applied in concert with our work.
3 Parser as Syntactic Language Model in
Phrase-Based Translation
Parsing is the task of selecting the representation ??
(typically a tree) that best models the structure of
621
???????
?s?
??0
???????
?s? the
??11
???????
?s? that
??12
???????
?s? president
??13
. . .
???????
the president
??21
???????
that president
??22
???????
president Friday
??23
. . .
???????
president meets
??31
???????
Obama met
??32
. . .
Figure 1: Partial decoding lattice for standard phrase-based decoding stack algorithm translating the German
sentence Der Pra?sident trifft am Freitag den Vorstand. Each node h in decoding stack t represents the
application of a translation option, and includes the source sentence coverage vector, target language n-
gram state, and syntactic language model state ??th . Hypothesis combination is also shown, indicating
where lattice paths with identical n-gram histories converge. We use the English translation The president
meets the board on Friday as a running example throughout all Figures.
sentence e, out of all such possible representations
? . This set of representations may be all phrase
structure trees or all dependency trees allowed by
the parsing model. Typically, tree ?? is taken to be:
?? = argmax
?
P(? | e) (1)
We define a syntactic language model P(e) based
on the total probability mass over all possible trees
for string e. This is shown in Equation 2 and decom-
posed in Equation 3.
P(e) =
?
???
P(?, e) (2)
P(e) =
?
???
P(e | ?)P(?) (3)
3.1 Incremental syntactic language model
An incremental parser processes each token of in-
put sequentially from the beginning of a sentence to
the end, rather than processing input in a top-down
(Earley, 1968) or bottom-up (Cocke and Schwartz,
1970; Kasami, 1965; Younger, 1967) fashion. After
processing the tth token in string e, an incremen-
tal parser has some internal representation of possi-
ble hypothesized (incomplete) trees, ?t. The syntac-
tic language model probability of a partial sentence
e1...et is defined:
P(e1...et) =
?
???t
P(e1...et | ?)P(?) (4)
In practice, a parser may constrain the set of trees
under consideration to ??t, that subset of analyses or
partial analyses that remains after any pruning is per-
formed. An incremental syntactic language model
can then be defined by a probability mass function
(Equation 5) and a transition function ? (Equation
6). The role of ? is explained in ?3.3 below. Any
parser which implements these two functions can
serve as a syntactic language model.
P(e1...et) ? P(?? t) =
?
???? t
P(e1...et | ?)P(?) (5)
?(et, ?? t?1)? ?? t (6)
622
3.2 Decoding in phrase-based translation
Given a source language input sentence f , a trained
source-to-target translation model, and a target lan-
guage model, the task of translation is to find the
maximally probable translation e? using a linear
combination of j feature functions h weighted ac-
cording to tuned parameters ? (Och and Ney, 2002).
e? = argmax
e
exp(
?
j
?jhj(e,f)) (7)
Phrase-based translation constructs a set of trans-
lation options ? hypothesized translations for con-
tiguous portions of the source sentence ? from a
trained phrase table, then incrementally constructs a
lattice of partial target translations (Koehn, 2010).
To prune the search space, lattice nodes are orga-
nized into beam stacks (Jelinek, 1969) according to
the number of source words translated. An n-gram
language model history is also maintained at each
node in the translation lattice. The search space
is further trimmed with hypothesis recombination,
which collapses lattice nodes that share a common
coverage vector and n-gram state.
3.3 Incorporating a Syntactic Language Model
Phrase-based translation produces target language
words in an incremental left-to-right fashion, gen-
erating words at the beginning of a translation first
and words at the end of a translation last. Similarly,
incremental parsers process sentences in an incre-
mental fashion, analyzing words at the beginning of
a sentence first and words at the end of a sentence
last. As such, an incremental parser with transition
function ? can be incorporated into the phrase-based
decoding process in a straightforward manner. Each
node in the translation lattice is augmented with a
syntactic language model state ??t.
The hypothesis at the root of the translation lattice
is initialized with ?? 0, representing the internal state
of the incremental parser before any input words are
processed. The phrase-based translation decoding
process adds nodes to the lattice; each new node
contains one or more target language words. Each
node contains a backpointer to its parent node, in
which ?? t?1 is stored. Given a new target language
word et and ?? t?1, the incremental parser?s transi-
tion function ? calculates ?? t. Figure 1 illustrates
S
NP
DT
The
NN
president
VP
VP
VB
meets
NP
DT
the
NN
board
PP
IN
on
NP
Friday
Figure 2: Sample binarized phrase structure tree.
S
S/NP
S/PP
S/VP
NP
NP/NN
DT
The
NN
president
VP
VP/NN
VP/NP
VB
meets
DT
the
NN
board
IN
on
NP
Friday
Figure 3: Sample binarized phrase structure tree af-
ter application of right-corner transform.
a sample phrase-based decoding lattice where each
translation lattice node is augmented with syntactic
language model state ??t.
In phrase-based translation, many translation lat-
tice nodes represent multi-word target language
phrases. For such translation lattice nodes, ? will
be called once for each newly hypothesized target
language word in the node. Only the final syntac-
tic language model state in such sequences need be
stored in the translation lattice node.
4 Incremental Bounded-Memory Parsing
with a Time Series Model
Having defined the framework by which any in-
cremental parser may be incorporated into phrase-
based translation, we now formally define a specific
incremental parser for use in our experiments.
The parser must process target language words
incrementally as the phrase-based decoder adds hy-
potheses to the translation lattice. To facilitate this
incremental processing, ordinary phrase-structure
trees can be transformed into right-corner recur-
623
r1t?1
r2t?1
r3t?1
s1t?1
s2t?1
s3t?1
r1t
r2t
r3t
s1t
s2t
s3t
et?1 et
. . .
. . .
. . .
. . .
Figure 4: Graphical representation of the depen-
dency structure in a standard Hierarchic Hidden
Markov Model with D = 3 hidden levels that can
be used to parse syntax. Circles denote random vari-
ables, and edges denote conditional dependencies.
Shaded circles denote variables with observed val-
ues.
sive phrase structure trees using the tree transforms
in Schuler et al (2010). Constituent nontermi-
nals in right-corner transformed trees take the form
of incomplete constituents c?/c?? consisting of an
?active? constituent c? lacking an ?awaited? con-
stituent c?? yet to come, similar to non-constituent
categories in a Combinatory Categorial Grammar
(Ades and Steedman, 1982; Steedman, 2000). As
an example, the parser might consider VP/NN as a
possible category for input ?meets the?.
A sample phrase structure tree is shown before
and after the right-corner transform in Figures 2
and 3. Our parser operates over a right-corner trans-
formed probabilistic context-free grammar (PCFG).
Parsing runs in linear time on the length of the input.
This model of incremental parsing is implemented
as a Hierarchical Hidden Markov Model (HHMM)
(Murphy and Paskin, 2001), and is equivalent to a
probabilistic pushdown automaton with a bounded
pushdown store. The parser runs in O(n) time,
where n is the number of words in the input. This
model is shown graphically in Figure 4 and formally
defined in ?4.1 below.
The incremental parser assigns a probability
(Eq. 5) for a partial target language hypothesis, using
a bounded store of incomplete constituents c?/c??.
The phrase-based decoder uses this probability value
as the syntactic language model feature score.
4.1 Formal Parsing Model: Scoring Partial
Translation Hypotheses
This model is essentially an extension of an HHMM,
which obtains a most likely sequence of hidden store
states, s?1..D1..T , of some length T and some maxi-
mum depth D, given a sequence of observed tokens
(e.g. generated target language words), e1..T , using
HHMM state transition model ?A and observation
symbol model ?B (Rabiner, 1990):
s?1..D1..T
def
= argmax
s1..D1..T
T?
t=1
P?A(s
1..D
t | s
1..D
t?1 )?P?B(et | s
1..D
t )
(8)
The HHMM parser is equivalent to a probabilis-
tic pushdown automaton with a bounded push-
down store. The model generates each successive
store (using store model ?S) only after considering
whether each nested sequence of incomplete con-
stituents has completed and reduced (using reduc-
tion model ?R):
P?A(s
1..D
t | s
1..D
t?1 )
def
=
?
r1t ..r
D
t
D?
d=1
P?R(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
? P?S(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t ) (9)
Store elements are defined to contain only the
active (c?) and awaited (c??) constituent categories
necessary to compute an incomplete constituent
probability:
sdt
def
= ?c?, c??? (10)
Reduction states are defined to contain only the
complete constituent category crdt necessary to com-
pute an inside likelihood probability, as well as a
flag frdt indicating whether a reduction has taken
place (to end a sequence of incomplete constituents):
rdt
def
= ?crdt , frdt ? (11)
The model probabilities for these store elements
and reduction states can then be defined (from Mur-
phy and Paskin 2001) to expand a new incomplete
constituent after a reduction has taken place (frdt =
1; using depth-specific store state expansion model
?S-E,d), transition along a sequence of store elements
624
s11
s21
s31
e1
t=1
r12
r22
r32
s12
s22
s32
e2
t=2
r13
r23
r33
s13
s23
s33
e3
t=3
r14
r24
r34
s14
s24
s34
e4
t=4
r15
r25
r35
s15
s25
s35
e5
t=5
r16
r26
r36
s16
s26
s36
e6
t=6
r17
r27
r37
s17
s27
s37
e7
t=7
r18
r28
r38
=DT
=NP/NN
=NP
=NN
=S/VP
=VB
=S/VP
=VP/NP
=DT
=VP/NN
=S/VP
=NN
=VP
=S/PP
=IN
=S/NP
=S
=NP
=The =president =meets =the =board =on =Friday
Figure 5: Graphical representation of the Hierarchic Hidden Markov Model after parsing input sentence The
president meets the board on Friday. The shaded path through the parse lattice illustrates the recognized
right-corner tree structure of Figure 3.
if no reduction has taken place (frdt =0; using depth-
specific store state transition model ?S-T,d): 2
P?S(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
def
=
?
?
?
if frd+1t =1, frdt =1 : P?S-E,d(s
d
t | s
d?1
t )
if frd+1t =1, frdt =0 : P?S-T,d(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
if frd+1t =0, frdt =0 : Js
d
t = s
d
t?1K
(12)
and possibly reduce a store element (terminate
a sequence) if the store state below it has re-
duced (frd+1t = 1; using depth-specific reduction
model ?R,d):
P?R(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
def
=
{
if frd+1t =0 : Jr
d
t = r?K
if frd+1t =1 : P?R,d(r
d
t | r
d+1
t s
d
t?1 s
d?1
t?1 )
(13)
where r? is a null state resulting from the failure of
an incomplete constituent to complete, and constants
are defined for the edge conditions of s0t and r
D+1
t .
Figure 5 illustrates this model in action.
These pushdown automaton operations are then
refined for right-corner parsing (Schuler, 2009),
distinguishing active transitions (model ?S-T-A,d, in
which an incomplete constituent is completed, but
not reduced, and then immediately expanded to a
2An indicator function J?K is used to denote deterministic
probabilities: J?K = 1 if ? is true, 0 otherwise.
new incomplete constituent in the same store el-
ement) from awaited transitions (model ?S-T-W,d,
which involve no completion):
P?S-T,d(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
def
=
{
if rdt 6=r? : P?S-T-A,d(s
d
t | s
d?1
t r
d
t )
if rdt =r? : P?S-T-W,d(s
d
t | s
d
t?1r
d+1
t )
(14)
P?R,d(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
def
=
{
if crd+1t 6=xt : Jr
d
t = r?K
if crd+1t =xt : P?R-R,d(r
d
t | s
d
t?1s
d?1
t?1 )
(15)
These HHMM right-corner parsing operations are
then defined in terms of branch- and depth-specific
PCFG probabilities ?G-R,d and ?G-L,d: 3
3Model probabilities are also defined in terms of left-
progeny probability distribution E?G-RL?,d which is itself defined
in terms of PCFG probabilities:
E?G-RL?,d(c?
0
? c?0 ...)
def
=
?
c?1
P?G-R,d(c? ? c?0 c?1) (16)
E?G-RL?,d(c?
k
? c?0k0 ...)
def
=
?
c
?0k
E?G-RL?,d(c?
k?1
? c?0k ...)
?
?
c
?0k1
P?G-L,d(c?0k ? c?0k0 c?0k1) (17)
E?G-RL?,d(c?
?
? c?? ...)
def
=
??
k=0
E?G-RL?,d(c?
k
? c?? ...) (18)
E?G-RL?,d(c?
+
? c?? ...)
def
= E?G-RL?,d(c?
?
? c?? ...)
? E?G-RL?,d(c?
0
? c?? ...) (19)
625
???????
president meets
??31
. . .
???????
the board
??51
. . .
s13
s23
s33
e3
r14
r24
r34
s14
s24
s34
e4
r15
r25
r35
s15
s25
s35
e5=meets =the =board
Figure 6: A hypothesis in the phrase-based decoding lattice from Figure 1 is expanded using translation op-
tion the board of source phrase den Vorstand. Syntactic language model state ??31 contains random variables
s1..33 ; likewise ??51 contains s
1..3
5 . The intervening random variables r
1..3
4 , s
1..3
4 , and r
1..3
5 are calculated by
transition function ? (Eq. 6, as defined by ?4.1), but are not stored. Observed random variables (e3..e5) are
shown for clarity, but are not explicitly stored in any syntactic language model state.
? for expansions:
P?S-E,d(?c??, c
?
??? | ??, c??)
def
=
E?G-RL?,d(c?
?
? c?? ...) ? Jx?? = c??? = c??K (20)
? for awaited transitions:
P?S-T-W,d(?c?, c??1? | ?c
?
?, c??? c??0)
def
=
Jc? = c??K ?
P?G-R,d(c?? ? c??0 c??1)
E?G-RL?,d(c??
0
? c??0 ...)
(21)
? for active transitions:
P?S-T-A,d(?c??, c??1? | ??, c?? c??0)
def
=
E?G-RL?,d(c?
?
? c?? ...) ? P?G-L,d(c?? ? c??0 c??1)
E?G-RL?,d(c?
+
? c??0 ...)
(22)
? for cross-element reductions:
P?R-R,d(c??,1 | ??, c?? ?c
?
??,??)
def
=
Jc?? = c???K ?
E?G-RL?,d(c?
0
? c?? ...)
E?G-RL?,d(c?
?
? c?? ...)
(23)
? for in-element reductions:
P?R-R,d(c??,0 | ??, c?? ?c
?
??,??)
def
=
Jc?? = c???K ?
E?G-RL?,d(c?
+
? c?? ...)
E?G-RL?,d(c?
?
? c?? ...)
(24)
We use the parser implementation of (Schuler,
2009; Schuler et al, 2010).
5 Phrase Based Translation with an
Incremental Syntactic Language Model
The phrase-based decoder is augmented by adding
additional state data to each hypothesis in the de-
626
coder?s hypothesis stacks. Figure 1 illustrates an ex-
cerpt from a standard phrase-based translation lat-
tice. Within each decoder stack t, each hypothe-
sis h is augmented with a syntactic language model
state ??th . Each syntactic language model state is
a random variable store, containing a slice of ran-
dom variables from the HHMM. Specifically, ??th
contains those random variables s1..Dt that maintain
distributions over syntactic elements.
By maintaining these syntactic random variable
stores, each hypothesis has access to the current
language model probability for the partial transla-
tion ending at that hypothesis, as calculated by an
incremental syntactic language model defined by
the HHMM. Specifically, the random variable store
at hypothesis h provides P(??th) = P(e
h
1..t, s
1..D
1..t ),
where eh1..t is the sequence of words in a partial hy-
pothesis ending at h which contains t target words,
and where there are D syntactic random variables in
each random variable store (Eq. 5).
During stack decoding, the phrase-based decoder
progressively constructs new hypotheses by extend-
ing existing hypotheses. New hypotheses are placed
in appropriate hypothesis stacks. In the simplest
case, a new hypothesis extends an existing hypothe-
sis by exactly one target word. As the new hypothe-
sis is constructed by extending an existing stack ele-
ment, the store and reduction state random variables
are processed, along with the newly hypothesized
word. This results in a new store of syntactic ran-
dom variables (Eq. 6) that are associated with the
new stack element.
When a new hypothesis extends an existing hy-
pothesis by more than one word, this process is first
carried out for the first new word in the hypothe-
sis. It is then repeated for the remaining words in
the hypothesis extension. Once the final word in
the hypothesis has been processed, the resulting ran-
dom variable store is associated with that hypoth-
esis. The random variable stores created for the
non-final words in the extending hypothesis are dis-
carded, and need not be explicitly retained.
Figure 6 illustrates this process, showing how a
syntactic language model state ??51 in a phrase-based
decoding lattice is obtained from a previous syn-
tactic language model state ??31 (from Figure 1) by
parsing the target language words from a phrase-
based translation option.
In-domain Out-of-domain
LM WSJ 23 ppl ur-en dev ppl
WSJ 1-gram 1973.57 3581.72
WSJ 2-gram 349.18 1312.61
WSJ 3-gram 262.04 1264.47
WSJ 4-gram 244.12 1261.37
WSJ 5-gram 232.08 1261.90
WSJ HHMM 384.66 529.41
Interpolated WSJ
5-gram + HHMM 209.13 225.48
Giga 5-gram 258.35 312.28
Interp. Giga 5-gr
+ WSJ HHMM 222.39 123.10
Interp. Giga 5-gr
+ WSJ 5-gram 174.88 321.05
Figure 7: Average per-word perplexity values.
HHMM was run with beam size of 2000. Bold in-
dicates best single-model results for LMs trained on
WSJ sections 2-21. Best overall in italics.
Our syntactic language model is integrated into
the current version of Moses (Koehn et al, 2007).
6 Results
As an initial measure to compare language models,
average per-word perplexity, ppl, reports how sur-
prised a model is by test data. Equation 25 calculates
ppl using log base b for a test set of T tokens.
ppl = b
?logbP(e1...eT )
T (25)
We trained the syntactic language model from
?4 (HHMM) and an interpolated n-gram language
model with modified Kneser-Ney smoothing (Chen
and Goodman, 1998); models were trained on sec-
tions 2-21 of the Wall Street Journal (WSJ) tree-
bank (Marcus et al, 1993). The HHMM outper-
forms the n-gram model in terms of out-of-domain
test set perplexity when trained on the same WSJ
data; the best perplexity results for in-domain and
out-of-domain test sets4 are found by interpolating
4In-domain is WSJ Section 23. Out-of-domain are the En-
glish reference translations of the dev section , set aside in
(Baker et al, 2009) for parameter tuning, of the NIST Open
MT 2008 Urdu-English task.
627
Sentence Moses +HHMM +HHMM
length beam=50 beam=2000
10 0.21 533 1143
20 0.53 1193 2562
30 0.85 1746 3749
40 1.13 2095 4588
Figure 8: Mean per-sentence decoding time (in sec-
onds) for dev set using Moses with and without syn-
tactic language model. HHMM parser beam sizes
are indicated for the syntactic LM.
HHMM and n-gram LMs (Figure 7). To show the
effects of training an LM on more data, we also re-
port perplexity results on the 5-gram LM trained for
the GALE Arabic-English task using the English Gi-
gaword corpus. In all cases, including the HHMM
significantly reduces perplexity.
We trained a phrase-based translation model on
the full NIST Open MT08 Urdu-English translation
model using the full training data. We trained the
HHMM and n-gram LMs on the WSJ data in order
to make them as similar as possible. During tuning,
Moses was first configured to use just the n-gram
LM, then configured to use both the n-gram LM and
the syntactic HHMM LM. MERT consistently as-
signed positive weight to the syntactic LM feature,
typically slightly less than the n-gram LM weight.
In our integration with Moses, incorporating a
syntactic language model dramatically slows the de-
coding process. Figure 8 illustrates a slowdown
around three orders of magnitude. Although speed
remains roughly linear to the size of the source sen-
tence (ruling out exponential behavior), it is with an
extremely large constant time factor. Due to this
slowdown, we tuned the parameters using a con-
strained dev set (only sentences with 1-20 words),
and tested using a constrained devtest set (only sen-
tences with 1-20 words). Figure 9 shows a statis-
tically significant improvement to the BLEU score
when using the HHMM and the n-gram LMs to-
gether on this reduced test set.
7 Discussion
This paper argues that incremental syntactic lan-
guages models are a straightforward and appro-
Moses LM(s) BLEU
n-gram only 18.78
HHMM + n-gram 19.78
Figure 9: Results for Ur-En devtest (only sentences
with 1-20 words) with HHMM beam size of 2000
and Moses settings of distortion limit 10, stack size
200, and ttable limit 20.
priate algorithmic fit for incorporating syntax into
phrase-based statistical machine translation, since
both process sentences in an incremental left-to-
right fashion. This means incremental syntactic LM
scores can be calculated during the decoding pro-
cess, rather than waiting until a complete sentence is
posited, which is typically necessary in top-down or
bottom-up parsing.
We provided a rigorous formal definition of in-
cremental syntactic languages models, and detailed
what steps are necessary to incorporate such LMs
into phrase-based decoding. We integrated an incre-
mental syntactic language model into Moses. The
translation quality significantly improved on a con-
strained task, and the perplexity improvements sug-
gest that interpolating between n-gram and syntactic
LMs may hold promise on larger data sets.
The use of very large n-gram language models is
typically a key ingredient in the best-performing ma-
chine translation systems (Brants et al, 2007). Our
n-gram model trained only on WSJ is admittedly
small. Our future work seeks to incorporate large-
scale n-gram language models in conjunction with
incremental syntactic language models.
The added decoding time cost of our syntactic
language model is very high. By increasing the
beam size and distortion limit of the baseline sys-
tem, future work may examine whether a baseline
system with comparable runtimes can achieve com-
parable translation quality.
A more efficient implementation of the HHMM
parser would speed decoding and make more exten-
sive and conclusive translation experiments possi-
ble. Various additional improvements could include
caching the HHMM LM calculations, and exploiting
properties of the right-corner transform that limit the
number of decisions between successive time steps.
628
References
Anne Abeille?, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized tree adjoining grammars for
machine translation. In Proceedings of the 13th Inter-
national Conference on Computational Linguistics.
Anthony E. Ades and Mark Steedman. 1982. On the
order of words. Linguistics and Philosophy, 4:517?
558.
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation
(SIMT). SCALE summer workshop final report, Hu-
man Language Technology Center Of Excellence.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 9?16.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, John Lafferty,
Robert Mercer, and Paul Roossin. 1990. A statisti-
cal approach to machine translation. Computational
Linguistics, 16(2):79?85.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proceedings of the Ninth Ma-
chine Translation Summit of the International Associ-
ation for Machine Translation.
Ciprian Chelba and Frederick Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In Pro-
ceedings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics, pages 225?
231.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University.
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452.
John Cocke and Jacob Schwartz. 1970. Program-
ming languages and their compilers. Technical report,
Courant Institute of Mathematical Sciences, New York
University.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 507?514.
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 232?241.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 727?736.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755?763.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 541?548.
Jay Earley. 1968. An efficient context-free parsing algo-
rithm. Ph.D. thesis, Department of Computer Science,
Carnegie Mellon University.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
205?208.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 773?781.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
629
Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 273?
280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961?
968.
Niyu Ge. 2010. A direct syntax-driven reordering model
for phrase-based machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 849?857.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 80?87.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 105?112.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 288?295.
James Henderson. 2004. Lookahead in deterministic
left-corner parsing. In Proceedings of the Workshop
on Incremental Parsing: Bringing Engineering and
Cognition Together, pages 26?33.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 273?283.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077?1086.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
conference of the Association for Machine Translation
in the Americas.
Kenji Imamura, Hideo Okuma, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Example-based machine transla-
tion based on syntactic transfer with statistical models.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 99?105.
Frederick Jelinek. 1969. Fast sequential decoding al-
gorithm using a stack. IBM Journal of Research and
Development, pages 675?685.
T. Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Laboratory.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 177?180.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 609?616.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 704?711.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558?566.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics, pages
653?660.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 206?214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 192?199.
630
Kevin P. Murphy and Mark A. Paskin. 2001. Linear time
inference in hierarchical HMMs. In Proceedings of
Neural Information Processing Systems, pages 833?
840.
Rebecca Nesson, Stuart Shieber, and Alexander Rush.
2006. Induction of probabilistic synchronous tree-
insertion grammars for machine translation. In Pro-
ceedings of the 7th Biennial conference of the Associ-
ation for Machine Translation in the Americas, pages
128?137.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, pages
161?168.
Matt Post and Daniel Gildea. 2008. Parsers as language
models for statistical machine translation. In Proceed-
ings of the Eighth Conference of the Association for
Machine Translation in the Americas, pages 172?181.
Matt Post and Daniel Gildea. 2009. Language modeling
with tree substitution grammars. In NIPS workshop on
Grammar Induction, Representation of Language, and
Language Learning.
Arjen Poutsma. 1998. Data-oriented translation. In
Ninth Conference of Computational Linguistics in the
Netherlands.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 271?279.
Lawrence R. Rabiner. 1990. A tutorial on hid-
den Markov models and selected applications in
speech recognition. Readings in speech recognition,
53(3):267?296.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage incremental
parsing using human-like memory constraints. Com-
putational Linguistics, 36(1):1?30.
William Schuler. 2009. Positive results for parsing with a
bounded stack using a model-based right-corner trans-
form. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 344?352.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 577?585.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree adjoining grammars. In Proceedings of the 13th
International Conference on Computational Linguis-
tics.
Stuart M. Shieber. 2004. Synchronous grammars as tree
transducers. In Proceedings of the Seventh Interna-
tional Workshop on Tree Adjoining Grammar and Re-
lated Formalisms.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523?530.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n cubed. Information
and Control, 10(2):189?208.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Seng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of the 11th Machine Translation Summit
of the International Association for Machine Transla-
tion, pages 535?542.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141.
631
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1220?1229,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Crowdsourcing Translation: Professional Quality from Non-Professionals
Omar F. Zaidan and Chris Callison-Burch
Dept. of Computer Science, Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,ccb}@cs.jhu.edu
Abstract
Naively collecting translations by crowd-
sourcing the task to non-professional trans-
lators yields disfluent, low-quality results if
no quality control is exercised. We demon-
strate a variety of mechanisms that increase
the translation quality to near professional lev-
els. Specifically, we solicit redundant transla-
tions and edits to them, and automatically se-
lect the best output among them. We propose a
set of features that model both the translations
and the translators, such as country of resi-
dence, LM perplexity of the translation, edit
rate from the other translations, and (option-
ally) calibration against professional transla-
tors. Using these features to score the col-
lected translations, we are able to discriminate
between acceptable and unacceptable transla-
tions. We recreate the NIST 2009 Urdu-to-
English evaluation set with Mechanical Turk,
and quantitatively show that our models are
able to select translations within the range of
quality that we expect from professional trans-
lators. The total cost is more than an order of
magnitude lower than professional translation.
1 Introduction
In natural language processing research, translations
are most often used in statistical machine translation
(SMT), where systems are trained using bilingual
sentence-aligned parallel corpora. SMT owes its ex-
istence to data like the Canadian Hansards (which by
law must be published in both French and English).
SMT can be applied to any language pair for which
there is sufficient data, and it has been shown to pro-
duce state-of-the-art results for language pairs like
Arabic?English, where there is ample data. How-
ever, large bilingual parallel corpora exist for rela-
tively few languages pairs.
There are various options for creating new train-
ing resources for new language pairs. These include
harvesting the web for translations or comparable
corpora (Resnik and Smith, 2003; Munteanu and
Marcu, 2005; Smith et al, 2010; Uszkoreit et al,
2010), improving SMT models so that they are bet-
ter suited to the low resource setting (Al-Onaizan
et al, 2002; Probst et al, 2002; Oard et al, 2003;
Niessen and Ney, 2004), or designing models that
are capable of learning translations from monolin-
gual corpora (Rapp, 1995; Fung and Yee, 1998;
Schafer and Yarowsky, 2002; Haghighi et al, 2008).
Relatively little consideration is given to the idea of
simply hiring translators to create parallel data, be-
cause it would seem to be prohibitively expensive.
For example, Germann (2001) estimated the cost
of hiring professional translators to create a Tamil-
English corpus at $0.36/word. At that rate, translat-
ing enough data to build even a small parallel corpus
like the LDC?s 1.5 million word Urdu?English cor-
pus would exceed half a million dollars.
In this paper we examine the idea of creating low
cost translations via crowdscouring. We use Ama-
zon?s Mechanical Turk to hire a large group of non-
professional translators, and have them recreate an
Urdu?English evaluation set at a fraction of the cost
of professional translators. The original dataset al
ready has professionally-produced reference trans-
lations, which allows us to objectively and quantita-
tively compare the quality of professional and non-
professional translations. Although many of the in-
dividual non-expert translators produce low-quality,
disfluent translations, we show that it is possible to
1220
Signs of human livings have been found in many caves 
in   Attapure. In 1994, the remains of pre-historic man, 
which are believed to be 800,000 years old were 
discovered and they were named `Home Antecessor' 
meaning `The Founding Man'. Prior to that 6 lac years 
old humans, named as   Homogenisens in scientific 
terms,were believed to be the   oldest dwellers of this 
area. Archaeological experts say that evidence is found 
that proves that the inhabitants of this area used 
molded tools. The ground where these digs took place 
has been claimed to be the oldest known European 
discovery of civilization, as announced by the French 
News Agency.
 !"#$"% &' ()*"+*, &-,./%, 0#1 234 5, 0#1 1994
 67"89: ;2< &="> &*"1 &*,?@ A"B C'D 8 E"FG8?H= )>
 ?I"+*, &*"%? &JK8 ?+#B &LJ8, )1)< 0#MJ> 0#NO &'
P"#O "8: Q"* "'
 &+J-"B 0#MJ> I"+*, 2*,?@ C'D 6 RG$ 2B 5,
 5, ;2< "="> "M' S+J#>?GTU#< )1)< 0#1 VW3X,
P2Y= 2="> 2*"1 &Z-"<9 [8?= \8.$ 2' 234
 2+8, 0#M*, ]' 2< "JM' "' [8?<"1 2' ]^8.$ _9"`a
 2' 234 5, ]' 2< "/bc ]/@ 2B [> 0#< 2b1 .<,)d
P2Y= 2=?' A"^K/B, &Y% 9,ef, 2-)< 2#' &-Wgh i)T
Signs of human life of ancient people have been 
discovered in several caves of Atapuerca. In 1994, 
several homo antecessor fossils i.e. pioneer human 
were uncovered in this region, which are supposed to 
be 800,000 years old. Previously, 600,000 years old 
ancestors, called homo hudlabar [sic] in scientific 
term, were supposed to be the most ancient 
inhabitants of the region.Archeologists are of the view 
that they have gathered evidence that the people of 
this region had also been using fabricated tools.
On the basis of the level at which this excavation was 
carried out, the French news agency [AFP] has termed 
it the oldest European discovery.
Urdu source Professional LDC Translation Non-Professional Mechanical Turk Translation
Figure 1: A comparison of professional translations provided by the LDC to non-professional translations created on
Mechanical Turk.
get high quality translations in aggregate by solicit-
ing multiple translations, redundantly editing them,
and then selecting the best of the bunch.
To select the best translation, we use a machine-
learning-inspired approach that assigns a score to
each translation we collect. The scores discrimi-
nate acceptable translations from those that are not
(and competent translators from those who are not).
The scoring is based on a set of informative, intu-
itive, and easy-to-compute features. These include
country of residence, number of years speaking En-
glish, LM perplexity of the translation, edit rate from
the other translations, and (optionally) calibration
against professional translators, with the weights set
using a small set of gold standard data from profes-
sional translators.
2 Crowdsourcing Translation to
Non-Professionals
To collect crowdsourced translations, we use Ama-
zon?s Mechanical Turk (MTurk), an online market-
place designed to pay people small sums of money
to complete Human Intelligence Tasks (or HITs) ?
tasks that are difficult for computers but easy for
people. Example HITs range from labeling images
to moderating blog comments to providing feedback
on relevance of results for search queries. Anyone
with an Amazon account can either submit HITs or
work on HITs that were submitted by others. Work-
ers are referred to as ?Turkers?, and designers of
HITs as ?Requesters.? A Requester specifies the re-
ward to be paid for each completed item, sometimes
as low as $0.01. Turkers are free to select whichever
HITs interest them, and to bypass HITs they find un-
interesting or which they deem pay too little.
The advantages of Mechanical Turk include:
? zero overhead for hiring workers
? a large, low-cost labor force
? easy micropayment system
? short turnaround time, as tasks get completed
in parallel by many individuals
? access to foreign markets with native speakers
of many rare languages
One downside is that Amazon does not provide
any personal information about Turkers. (Each
Turker is identifiable only through an anonymous
ID like A23KO2TP7I4KK2.) In particular, no in-
formation is available about a worker?s educational
background, skills, or even native language(s). This
makes it difficult to determine if a Turker is qualified
to complete a translation task.
Therefore, soliciting translations from anony-
mous non-professionals carries a significant risk of
poor translation quality. Whereas hiring a profes-
sional translator ensures a degree of quality and
care, it is not very difficult to find bad translations
provided by Turkers. One Urdu headline, profes-
sionally translated as Barack Obama: America Will
Adopt a New Iran Strategy, was rendered disfluently
by a Turker as Barak Obam will do a new policy
with Iran. Another translated it with snarky sar-
casm: Barak Obama and America weave new evil
strategies against Iran. Figure 1 gives more typical
translation examples. The translations often reflect
non-native English, but are generally done conscien-
tiously (in spite of the relatively small payment).
To improve the accuracy of noisy labels from non-
experts, most existing quality control mechanisms
1221
employ some form of voting, assuming a discrete
set of possible labels. This is not the case for trans-
lations, where the ?labels? are full sentences. When
dealing with such a structured output, the space of
possible outputs is diverse and complex. We there-
fore need a different approach for quality control.
That is precisely the focus of this work: to propose,
and evaluate, such quality control mechanisms.
In the next section, we discuss reproducing the
Urdu-to-English 2009 NIST evaluation set. We then
describe a principled approach to discriminate good
translations from bad ones, given a set of redundant
translations for the same source sentence.
3 Datasets
3.1 The Urdu-to-English 2009 NIST
Evaluation Set
We translated the Urdu side of the Urdu?English test
set of the 2009 NIST MT Evaluation Workshop. The
set consists of 1,792 Urdu sentences from a vari-
ety of news and online sources. The set includes
four different reference translations for each source
sentence, produced by professional translation agen-
cies. NIST contracted the LDC to oversee the trans-
lation process and perform quality control.
This particular dataset, with its multiple reference
translations, is very useful because we can measure
the quality range for professional translators, which
gives us an idea of whether or not the crowdsourced
translations approach the quality of a professional
translator.
3.2 Translation HIT design
We solicited English translations for the Urdu sen-
tences in the NIST dataset. Amazon has enabled
payments in rupees, which has attracted a large de-
mographic of workers from India (Ipeirotis, 2010).
Although it does not yet have s direct payment in
Pakistan?s local currency, we found that a large con-
tingent of our workers are located in Pakistan.
Our HIT involved showing the worker a sequence
of Urdu sentences, and asking them to provide an
English translation for each one. The screen also
included a brief set of instructions, and a short ques-
tionnaire section. The reward was set at $0.10 per
translation, or roughly $0.005 per word.
In our first collection effort, we solicited only one
translation per Urdu sentence. After confirming that
the task is feasible due to the large pool of work-
ers willing and able to provide translations, we car-
ried out a second collection effort, this time solicit-
ing three translations per Urdu sentence (from three
distinct translators). The interface was also slightly
modified, in the following ways:
? Instead of asking Turkers to translate a full doc-
ument (as in our first pass), we instead split the
data set into groups of 10 sentences per HIT.
? We converted the Urdu sentences into images
so that Turkers could not cheat by copying-and-
pasting the Urdu text into an MT system.
? We collected information about each worker?s
geographic location, using a JavaScript plugin.
The translations from the first pass were of notice-
ably low quality, most likely due to Turkers using
automatic translation systems. That is why we used
images instead of text in our second pass, which
yielded significant improvements. That said, we do
not discard the translations from the first pass, and
we do include them in our experiments.
3.3 Post-editing and Ranking HITs
In addition to collecting four translations per source
sentence, we also collected post-edited versions
of the translations, as well as ranking judgments
about their quality.
Figure 2 gives examples of the unedited transla-
tions that we collected in the translation pass. These
typically contain many simple mistakes like mis-
spellings, typos, and awkward word choice. We
posted another MTurk task where we asked workers
to edit the translations into more fluent and gram-
matical sentences. We restrict the task to US-based
workers to increase the likelihood that they would be
native English speakers.
We also asked US-based Turkers to rank the trans-
lations. We presented the translations in groups of
four, and the annotator?s task was to rank the sen-
tences by fluency, from best to worst (allowing ties).
We collected redundant annotations in these two
tasks as well. Each translation is edited three times
(by three distinct editors). We solicited only one edit
per translation from our first pass translation effort.
So, in total, we had 10 post-edited translations for
1222
Avoiding dieting to prevent 
from flu
abstention from dieting in 
order to avoid Flu
Abstain from decrease eating in 
order to escape from flue
In order to be safer from flu 
quit dieting
This research of American 
scientists came in front after 
experimenting on mice.
This research from the 
American Scientists have 
come up after the 
experiments on rats.
This research of American 
scientists was shown after 
many experiments on mouses.
According to the American 
Scientist this research has come 
out after much 
experimentations on rats.
Experiments proved that mice 
on a lower calorie diet had 
comparatively less ability to 
fight the flu virus.
in has been proven from 
experiments that rats put on 
diet with less calories had less 
ability to resist the Flu virus.
It was proved by experiments 
the low calories eaters 
mouses had low defending 
power for flue in ratio.
Experimentaions have proved 
that those rats on less calories 
diet have developed a tendency 
of not overcoming the flu virus.
research has proven this old 
myth wrong that its better to 
fast during fever.
Research disproved the old 
axiom that " It is better to 
fast during fever"
The research proved this old 
talk that decrease eating is 
useful in fever.
This Research has proved the 
very old saying wrong that it is 
good to starve while in fever.
Figure 2: We redundantly translate each source sentence by soliciting multiple translations from different Turkers.
These translations are put through a subsequent editing set, where multiple edited versions are produced. We select
the best translation from the set using features that predict the quality of each translation and each translator.
each source sentence (plus the four original transla-
tions). In the ranking task, we collected judgments
from five distinct workers for each translation group.
3.4 Data Collection Cost
We paid a reward of $0.10 to translate a sentence,
$0.25 to edit a set of ten sentences, and $0.06 to rank
a set of four translation groups. Therefore, we had
the following costs:
? Translation cost: $716.80
? Editing cost: $447.50
? Ranking cost: $134.40
(If not done redundantly, those values would be
$179.20, $44.75, and $26.88, respectively.)
Adding Amazon?s 10% fee, this brings the grand
total to under $1,500, spent to collect 7,000+ transla-
tions, 17,000+ edited translations, and 35,000+ rank
labels.1 We also use about 10% of the existing pro-
fessional references in most of our experiments (see
4.2 and 4.3). If we estimate the cost at $0.30/word,
that would roughly be an additional $1,000.
3.5 MTurk Participation
52 different Turkers took part in the translation task,
each translating 138 sentences on average. In the
editing task, 320 Turkers participated, averaging 56
sentences each. In the ranking task, 245 Turkers par-
ticipated, averaging 9.1 HITs each, or 146 rank la-
bels (since each ranking HIT involved judging 16
translations, in groups of four).
1Data URL: www.cs.jhu.edu/?ozaidan/RCLMT.
4 Quality Control Model
Our approach to building a translation set from
the available data is to select, for each Urdu sen-
tence, the one translation that our model believes
to be the best out of the available translations. We
evaluate various selection techniques by compar-
ing the selected Turker translations against existing
professionally-produced translations. The more the
selected translations resemble the professional trans-
lations, the higher the quality.
4.1 Features Used to Select Best Translations
Our model selects one of the 14 English options gen-
erated by Turkers. For a source sentence si, our
model assigns a score to each sentence in the set
of available translations {ti,1, ...ti,14}. The chosen
translation is the highest scoring translation:
tr(si) = tri,j? s.t. j
? = argmax
j
score(ti,j) (1)
where score(.) is the dot product:
score(ti,j)
def
= ~w ? ~f(ti,j) (2)
Here, ~w is the model?s weight vector (tuned as
described below in 4.2), and ~f is a translation?s cor-
responding feature vector. Each feature is a function
computed from the English sentence string, the Urdu
sentence string, the workers (translators, editors, and
rankers), and/or the rank labels. We use 21 features,
categorized into the following three sets.
1223
Sentence-level (6 features). Most of the Turk-
ers performing our task were native Urdu speakers
whose second language was English, and they do not
always produce natural-sounding English sentences.
Therefore, the first set of features attempt to discrim-
inate good English sentences from bad ones.
? Language model features: each sentence is
assigned a log probability and per-word per-
plexity score, using a 5-gram language model
trained on the English Gigaword corpus.
? Sentence length features: a good translation
tends to be comparable in length to the source
sentence, whereas an overly short or long trans-
lation is probably bad. We add two features that
are the ratios of the two lengths (one penalizes
short sentences and one penalizes long ones).
? Web n-gram match percentage: we assign a
score to each sentence based on the percentage
of the n-grams (up to length 5) in the transla-
tion that exist in the Google N-Gram Database.
? Web n-gram geometric average: we calculate
the average over the different n-gram match
percentages (similar to the way BLEU is com-
puted). We add three features corresponding to
max n-gram lengths of 3, 4, and 5.
? Edit rate to other translations: a bad translation
is likely not to be very similar to other transla-
tions, since there are many more ways a trans-
lation can be bad than for it to be good. So, we
compute the average edit rate distance from the
other translations (using the TER metric).
Worker-level (12 features). We add worker-level
features that evaluate a translation based on who pro-
vided it.
? Aggregate features: for each sentence-level
feature above, we have a corresponding feature
computed over all of that worker?s translations.
? Language abilities: we ask workers to provide
information about their language abilities. We
have a binary feature indicating whether Urdu
is their native language, and a feature for how
long they have spoken it. We add a pair of
equivalent features for English.
? Worker location: two binary features reflect a
worker?s location, one to indicate if they are lo-
cated in Pakistan, and one to indicate if they are
located in India.
Ranking (3 features). The third set of features is
based on the ranking labels we collected (see 3.3).
? Average rank: the average of the five rank la-
bels provided for this translation.
? Is-Best percentage: how often the translation
was top-ranked among the four translations.
? Is-Better percentage: how often the translation
was judged as the better translation, over all
pairwise comparisons extracted from the ranks.
Other features (not investigated here) could in-
clude source-target information, such as translation
model scores or the number of source words trans-
lated correctly according to a bilingual dictionary.
4.2 Parameter Tuning
Once features are computed for the sentences, we
must set the model?s weight vector ~w. Naturally, the
weights should be chosen so that good translations
get high scores, and bad translations get low scores.
We optimize translation quality against a small sub-
set (10%) of reference (professional) translations.
To tune the weight vector, we use the linear search
method of Och (2003), which is the basis of Min-
imum Error Rate Training (MERT). MERT is an
iterative algorithm used to tune parameters of an
MT system, which operates by iteratively generating
new candidate translations and adjusting the weights
to give good translations a high score, then regener-
ating new candidates based on the updated weights,
etc. In our work, the set of candidate translations is
fixed (the 14 English sentences for each source sen-
tence), and therefore iterating the procedure is not
applicable. We use the Z-MERT software package
(Zaidan, 2009) to perform the search.
4.3 The Worker Calibration Feature
Since we use a small portion of the reference trans-
lations to perform weight tuning, we can also use
that data to compute another worker-specific fea-
ture. Namely, we can evaluate the competency of
each worker by scoring their translations against the
reference translations. We then use that feature for
every translation given by that worker. The intuition
1224
is that workers known to produce good translations
are likely to continue to produce good translations,
and the opposite is likely true as well.
4.4 Evaluation Strategy
To measure the quality of the translations, we make
use of the existing professional translations. Since
we have four professional translation sets, we can
calculate the BLEU score (Papineni et al, 2002) for
one professional translator P1 using the other three
P2,3,4 as a reference set. We repeat the process four
times, scoring each professional translator against
the others, to calculate the expected range of profes-
sional quality translation. We can see how a trans-
lation set T (chosen by our model) compares to this
range by calculating T ?s BLEU scores against the
same four sets of three reference translations. We
will evaluate different strategies for selecting such
a set T , and see how much each improves on the
BLEU score, compared to randomly picking from
among the Turker translations.
We also evaluate Turker translation quality by us-
ing them as reference sets to score various submis-
sions to the NIST MT evaluation. Specifically, we
measure the correlation (using Pearson?s r) between
BLEU scores of MT systems measured against non-
professional translations, and BLEU scores mea-
sured against professional translations. Since the
main purpose of the NIST dataset was to compare
MT systems against each other, this is a more di-
rect fitness-for-task measure. We chose the middle 6
systems (in terms of performance) submitted to the
NIST evaluation, out of 12, as those systems were
fairly close to each other, with less than 2 BLEU
points separating them.2
5 Experimental Results
We establish the performance of professional trans-
lators, calculate oracle upper bounds on Turker
translation quality, and carry out a set of experiments
that demonstrate the effectiveness of our model and
that determine which features are most helpful.
Each number reported in this section is an average
of four numbers, corresponding to the four possible
2Using all 12 systems artificially inflates correlation, due to
the vast differences between the systems. For instance, the top
system outperforms the bottom system by 15 BLEU points!
ways of choosing 3 of the 4 reference sets. Further-
more, each of those 4 numbers is itself based on a
five-fold cross validation, where 80% of the data is
used to compute feature values, and 20% used for
evaluation. The 80% portion is used to compute the
aggregate worker-level features. For the worker cal-
ibration feature, we utilize the references for 10% of
the data (which is within the 80% portion).
5.1 Translation Quality: BLEU Scores
Compared to Professionals
We first evaluated the reference sets against each
other, in order to quantify the concept of ?profes-
sional quality?. On average, evaluating one refer-
ence set against the other three gives a BLEU score
of 42.38 (Figure 3). A Turker set of translations
scores 28.13 on average, which highlights the loss in
quality when collecting translations from amateurs.
To make the gap clearer, the output of a state-of-
the-art machine translation system (the syntax-based
variant of Joshua; Li et al (2010)) achieves a score
of 26.91, a mere 1.22 worse than the Turkers.
We perform two oracle experiments to determine
if there exist high-quality Turker translations in the
first place. The first oracle operates on the segment
level: for each source segment, choose from the four
translations the one that scores highest against the
reference sentence. The second oracle operates on
the worker level: for each source segment, choose
from the four translations the one provided by the
worker whose translations (over all sentences) score
the highest. The two oracles achieve BLEU scores
of 43.75 and 40.64, respectively ? well within the
range of professional translators.
We examined two voting-inspired methods, since
taking a majority vote usually works well when deal-
ing with MTurk data. The first selects the translation
with the minimum average TER (Snover et al, 2006)
against the other three translations, since that would
be a ?consensus? translation. The second method se-
lects the translation that received the best average
rank, using the rank labels assigned by other Turkers
(see 3.3). These approaches achieve BLEU scores of
34.41 and 36.64, respectively.
The main set of experiments evaluated the fea-
tures from 4.1 and 4.3. We applied our approach
using each of the four feature types: sentence fea-
tures, Turker features, rank features, and the cali-
1225
26.91 28.13 43.75 40.64 34.41 36.6442.38 34.95 35.79 37.14 37.82 39.06
20
25
30
35
40
45
Reference
(ave.)
Joshua
(syntax)
Turker
(ave.)
Oracle
(segment)
Oracle
(Turker)
Lowest
TER
Best
rank
Sentence
features
Turker
features
Rank
features
Calibration
feature
All
features
B
L
E
U
Figure 3: BLEU scores for different selection methods, measured against the reference sets. Each score is an average
of four BLEU scores, each calculated against three LDC reference translations. The five right-most bars are colored
in orange to indicate selection over a set that includes both original translations as well as edited versions of them.
bration feature. That yielded BLEU scores ranging
from 34.95 to 37.82. With all features combined, we
achieve a higher score of 39.06, which is within the
range of scores for the professional translators.
5.2 Fitness for a Task: Correlation With
Professionals When Ranking MT Systems
We evaluated the selection methods by measuring
correlation with the references, in terms of BLEU
scores assigned to outputs of MT systems. The re-
sults, in Table 1, tell a fairly similar story as eval-
uating with BLEU: references and oracles naturally
perform very well, and the loss in quality when se-
lecting arbitrary Turker translations is largely elimi-
nated using our selection strategy.
Interestingly, when using the Joshua output as
a reference set, the performance is quite abysmal.
Even though its BLEU score is comparable to the
Turker translations, it cannot be used to distinguish
closely matched MT systems from each other.3
6 Analysis
The oracles indicate that there is usually an accept-
able translation from the Turkers for any given sen-
tence. Since the oracles select from a small group of
only 4 translations per source segment, they are not
overly optimistic, and rather reflect the true potential
of the collected translations.
The results indicate that, although some features
are more useful than others, much of the benefit
from combining all the features can be obtained
from any one set of features, with the benefit of
3It should be noted that the Joshua system was not one of
the six MT systems we scored in the correlation experiments.
34.71 35.45 37.14 37.22 37.96
20
25
30
35
40
45
Sentence
features
Turker
features
Rank
features
Calibration
feature
All
features
B
L
E
U
Figure 4: BLEU scores for the five right-most setups from
Figure 3, constrained over the original translations.
adding more features being somewhat orthogonal.
Finally, we performed a series of experiments ex-
ploring the calibration feature, varying the amount
of gold-standard references from 10% all the way up
to 80%. As expected, the performance improved as
more references were used to calibrate the transla-
tors (Figure 5). What?s particularly important about
this experiment is that it shows the added benefit
of the other features: We would have to use 30%?
40% of the references to get the same benefit ob-
tained from combining the non-calibration features
and only 10% for the calibration feature (dashed line
in the Figure; BLEU = 39.06).
6.1 Cost Reduction
While the combined cost of our data collection ef-
fort ($2,500; see 3.4) is quite low considering the
amount of collected data, it would be more attractive
if the cost could be reduced further without losing
much in translation quality. To that end, we inves-
tigated lowering cost along two dimensions: elimi-
nating the need for professional translations, and de-
creasing the amount of edited translations.
1226
Selection Method Pearson?s r2
Reference (ave.) 0.81 ? 0.07
Joshua (syntax) 0.08 ? 0.09
Turker (ave.) 0.60 ? 0.17
Oracle (segment) 0.81 ? 0.09
Oracle (Turker) 0.79 ? 0.10
Lowest TER 0.50 ? 0.26
Best rank 0.74 ? 0.17
Sentence features 0.56 ? 0.21
Turker features 0.59 ? 0.19
Rank features 0.75 ? 0.14
Calibration feature 0.76 ? 0.13
All features 0.77 ? 0.11
Table 1: Correlation (? std. dev.) for different selection
methods, compared against the reference sets.
The professional translations are used in our ap-
proach for computing the worker calibration feature
(subsection 4.3) and for tuning the weights of the
other features. We use a relatively small amount
for this purpose, but we investigate a different setup
whereby no professional translations are used at all.
This eliminates the worker calibration feature, but,
perhaps more critically, the feature weights must be
set in a different fashion, since we cannot optimize
BLEU on reference data anymore. Instead, we use
the rank labels (from 3.3) as a proxy for BLEU, and
set the weights so that better ranked translations re-
ceive higher scores.
Note that the rank features will also be excluded
in this setup, since they are perfect predictors of rank
labels. On the one hand, this means no rank labels
need to be collected, other than for a small set used
for weight tuning, further reducing the cost of data
collection. However, this leads to a significant drop
in performance, yielding a BLEU score of 34.86.
Another alternative for cost reduction would be to
reduce the number of collected edited translations.
To that end, we first investigate completely eliminat-
ing the editing phase, and considering only unedited
translations. In other words, the selection will be
over a group of four English sentences rather than
14 sentences. Completely eliminating the edited
translations has an adverse effect, as expected (Fig-
ure 4). Another option, rather than eliminating the
editing phase altogether, would be to consider the
edited translations of only the translation receiving
37.0
37.5
38.0
38.5
39.0
39.5
40.0
40.5
0 20 40 60 80 100
% References Used for Calibration
B
L
E
U
 10%+other features
(i.e. "All features"
from Figure 3)
Figure 5: The effect of varying the amount of calibra-
tion data (and using only the calibration feature). The
10% point (BLEU = 37.82) and the dashed line (BLEU =
39.06) correspond to the two right-most bars of Figure 3.
the best rank labels. This would reflect a data col-
lection process whereby the editing task is delayed
until after the rank labels are collected, with the rank
labels used to determine which translations are most
promising to post-edit (in addition to using the rank
labels for the ranking features). Using this approach
enables us to greatly reduce the number of edited
translations collected, while maintaining good per-
formance, obtaining a BLEU score of 38.67.
It is therefore our recommendation that crowd-
sourced translation efforts adhere to the follow-
ing pipeline: collect multiple translations for each
source sentence, collect rank labels for the transla-
tions, and finally collect edited versions of the top
ranked translations.
7 Related Work
Dawid and Skene (1979) investigated filtering
annotations using the EM algorithm, estimating
annotator-specific error rates in the context of patient
medical records. Snow et al (2008) were among the
first to use MTurk to obtain data for several NLP
tasks, such as textual entailment and word sense dis-
ambiguation. Their approach, based on majority
voting, had a component for annotator bias correc-
tion. They showed that for such tasks, a few non-
expert labels usually suffice.
Whitehill et al (2009) proposed a probabilistic
model to filter labels from non-experts, in the con-
text of an image labeling task. Their system genera-
tively models image difficulty, as well as noisy, even
1227
adversarial, annotators. They apply their method to
simulated labels rather than real-life labels.
Callison-Burch (2009) proposed several ways to
evaluate MT output on MTurk. One such method
was to collect reference translations to score MT
output. It was only a pilot study (50 sentences in
each of several languages), but it showed the pos-
sibility of obtaining high-quality translations from
non-professionals. As a followup, Bloodgood and
Callison-Burch (2010) solicited a single translation
of the NIST Urdu-to-English dataset we used. Their
evaluation was similar to our correlation experi-
ments, examining how well the collected transla-
tions agreed with the professional translations when
evaluating three MT systems.
That paper appeared in a NAACL 2010 workshop
organized by Callison-Burch and Dredze (2010), fo-
cusing on MTurk as a source of data for speech and
language tasks. Two relevant papers from that work-
shop were by Ambati and Vogel (2010), focusing on
the design of the translation HIT, and by Irvine and
Klementiev (2010), who created translation lexicons
between English and 42 rare languages.
Resnik et al (2010) explore a very interesting
way of creating translations on MTurk, relying only
on monolingual speakers. Speakers of the target
language iteratively identified problems in machine
translation output, and speakers of the source lan-
guage paraphrased the corresponding source por-
tion. The paraphrased source would then be re-
translated to produce a different translation, hope-
fully more coherent than the original.
8 Conclusion and Future Work
We have demonstrated that it is possible to ob-
tain high-quality translations from non-professional
translators, and that the cost is an order of magni-
tude cheaper than professional translation. We be-
lieve that crowdsourcing can play a pivotal role in
future efforts to create parallel translation datasets.
Beyond the cost and scalability, crowdsourcing pro-
vides access to languages that currently fall outside
the scope of statistical machine translation research.
We have begun an ongoing effort to collect transla-
tions for several low resource languages, including
Tamil, Yoruba, and dialectal Arabic. We plan to:
? Investigate improvements from system combi-
nation techniques to the redundant translations.
? Modify our editing step to collect an annotated
corpus of English as a second language errors.
? Calibrate against good Turkers, instead of pro-
fessionals, once they have been identified.
? Predict whether it is necessary to solicit another
translation instead of collecting a fixed number.
? Analyze how much quality matters if our goal
is to train a statistical translation system.
Acknowledgments
This research was supported by the Human Lan-
guage Technology Center of Excellence, by gifts
from Google and Microsoft, and by the DARPA
GALE program under Contract No. HR0011-06-2-
0001. The views and findings are the authors? alone.
We would like to thank Ben Bederson, Philip
Resnik, and Alain De?silets for organizing work-
shops focused on crowdsourcing translation (Bed-
erson and Resnik, 2010; De?silets, 2010). We are
grateful for the feedback of workshop participants,
which helped shape this research.
References
Yaser Al-Onaizan, Ulrich Germann, Ulf Hermjakob,
Kevin Knight, Philipp Koehn, Daniel Marcu, and
Kenji Yamada. 2002. Translation with scarce bilin-
gual resources. Machine Translation, 17(1), March.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In Proceedings of the NAACL HLT Workshop on Cre-
ating Speech and Language Data With Amazon?s Me-
chanical Turk, pages 62?65.
Ben Bederson and Philip Resnik. 2010. Workshop on
crowdsourcing and translation. http://www.cs.
umd.edu/hcil/monotrans/workshop/.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using Mechanical Turk to build machine translation
evaluation sets. In Proceedings of the NAACL HLT
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk, pages 208?211.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk, pages 1?12.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
1228
chanical Turk. In Proceedings of EMNLP, pages 286?
295.
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Applied Statistics, 28(1):20?28.
Alain De?silets. 2010. AMTA 2010 workshop on collabo-
rative translation: technology, crowdsourcing, and the
translator perspective. http://bit.ly/gPnqR2.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach for
translating new words from nonparallel, comparable
texts. In Proceedings of ACL/CoLing.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang for
the buck can we expect? In ACL 2001 Workshop on
Data-Driven Machine Translation, Toulouse, France.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
ACL/HLT.
Panos Ipeirotis. 2010. New demographics of Mechanical
Turk. http://behind-the-enemy-lines.
blogspot.com/2010/03/
new-demographics-of-mechanical-turk.
html.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Proceedings of the NAACL HLT
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk, pages 108?113.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with syn-
tax, semirings, discriminative training and other good-
ies. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, pages
133?137.
Dragos Munteanu and Daniel Marcu. 2005. Improving
machine translation performance by exploiting compa-
rable corpora. Computational Linguistics, 31(4):477?
504, December.
Sonja Niessen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntatic analysis. Computational Linguistics,
30(2):181?204.
Doug Oard, David Doermann, Bonnie Dorr, Daqing He,
Phillip Resnik, William Byrne, Sanjeeve Khudanpur,
David Yarowsky, Anton Leuski, Philipp Koehn, and
Kevin Knight. 2003. Desperately seeking Cebuano.
In Proceedings of HLT/NAACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Poukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311?318.
Katharina Probst, Lori Levin, Erik Peterson, Alon Lavie,
and Jamie Carbonell. 2002. MT for minority lan-
guages using elicitation-based learning of syntactic
transfer rules. Machine Translation, 17(4).
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of ACL.
Philip Resnik and Noah Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29(3):349?
380, September.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alex Quinn, and Benjamin Bederson. 2010. Improv-
ing translation via targeted paraphrasing. In Proceed-
ings of EMNLP, pages 127?137.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In Conference on Natural Lan-
guage Learning-2002, pages 146?152.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 403?411, Los An-
geles, California, June. Association for Computational
Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas (AMTA).
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? but is it
good? Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP, pages
254?263.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel document
mining for machine translation. In Proc. of the In-
ternational Conference on Computational Linguistics
(COLING).
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. In Proceedings of
NIPS, pages 2035?2043.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
1229
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 37?41,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
The Arabic Online Commentary Dataset:
an Annotated Dataset of Informal Arabic with High Dialectal Content
Omar F. Zaidan and Chris Callison-Burch
Dept. of Computer Science, Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,ccb}@cs.jhu.edu
Abstract
The written form of Arabic, Modern Standard
Arabic (MSA), differs quite a bit from the
spoken dialects of Arabic, which are the true
?native? languages of Arabic speakers used in
daily life. However, due to MSA?s prevalence
in written form, almost all Arabic datasets
have predominantly MSA content. We present
the Arabic Online Commentary Dataset, a
52M-word monolingual dataset rich in dialec-
tal content, and we describe our long-term an-
notation effort to identify the dialect level (and
dialect itself) in each sentence of the dataset.
So far, we have labeled 108K sentences, 41%
of which as having dialectal content. We also
present experimental results on the task of au-
tomatic dialect identification, using the col-
lected labels for training and evaluation.
1 Introduction
The Arabic language is characterized by an interest-
ing linguistic dichotomy, whereby the written form
of the language, Modern Standard Arabic (MSA),
differs in a non-trivial fashion from the various spo-
ken varieties of Arabic. As the variant of choice for
written and official communication, MSA content
significantly dominates dialectal content, and in turn
MSA dominates in datasets available for linguistic
research, especially in textual form.
The abundance of MSA data has greatly aided re-
search on computational methods applied to Arabic,
but only the MSA variant of it. A state-of-the-art
Arabic-to-English machine translation system per-
forms quite well when translating MSA source sen-
tences, but often produces incomprehensible output
when the input is dialectal. For example, most words
Src
 (M
SA
):   
     
     
     
    

 
?

 ? 
?  
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1374?1383,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Dirt Cheap Web-Scale Parallel Text from the Common Crawl
Jason R. Smith1,2
jsmith@cs.jhu.edu
Philipp Koehn3
pkoehn@inf.ed.ac.uk
Herve Saint-Amand3
herve@saintamh.org
Chris Callison-Burch1,2,5
ccb@cs.jhu.edu ?
Magdalena Plamada4
plamada@cl.uzh.ch
Adam Lopez1,2
alopez@cs.jhu.edu
1Department of Computer Science, Johns Hopkins University
2Human Language Technology Center of Excellence, Johns Hopkins University
3School of Informatics, University of Edinburgh
4Institute of Computational Linguistics, University of Zurich
5Computer and Information Science Department, University of Pennsylvania
Abstract
Parallel text is the fuel that drives modern
machine translation systems. The Web is a
comprehensive source of preexisting par-
allel text, but crawling the entire web is
impossible for all but the largest compa-
nies. We bring web-scale parallel text to
the masses by mining the Common Crawl,
a public Web crawl hosted on Amazon?s
Elastic Cloud. Starting from nothing more
than a set of common two-letter language
codes, our open-source extension of the
STRAND algorithm mined 32 terabytes of
the crawl in just under a day, at a cost of
about $500. Our large-scale experiment
uncovers large amounts of parallel text in
dozens of language pairs across a variety
of domains and genres, some previously
unavailable in curated datasets. Even with
minimal cleaning and filtering, the result-
ing data boosts translation performance
across the board for five different language
pairs in the news domain, and on open do-
main test sets we see improvements of up
to 5 BLEU. We make our code and data
available for other researchers seeking to
mine this rich new data resource.1
1 Introduction
A key bottleneck in porting statistical machine
translation (SMT) technology to new languages
and domains is the lack of readily available paral-
lel corpora beyond curated datasets. For a handful
of language pairs, large amounts of parallel data
?This research was conducted while Chris Callison-
Burch was at Johns Hopkins University.
1github.com/jrs026/CommonCrawlMiner
are readily available, ordering in the hundreds of
millions of words for Chinese-English and Arabic-
English, and in tens of millions of words for many
European languages (Koehn, 2005). In each case,
much of this data consists of government and news
text. However, for most language pairs and do-
mains there is little to no curated parallel data
available. Hence discovery of parallel data is an
important first step for translation between most
of the world?s languages.
The Web is an important source of parallel
text. Many websites are available in multiple
languages, and unlike other potential sources?
such as multilingual news feeds (Munteanu and
Marcu, 2005) or Wikipedia (Smith et al, 2010)?
it is common to find document pairs that are di-
rect translations of one another. This natural par-
allelism simplifies the mining task, since few re-
sources or existing corpora are needed at the outset
to bootstrap the extraction process.
Parallel text mining from the Web was origi-
nally explored by individuals or small groups of
academic researchers using search engines (Nie
et al, 1999; Chen and Nie, 2000; Resnik, 1999;
Resnik and Smith, 2003). However, anything
more sophisticated generally requires direct access
to web-crawled documents themselves along with
the computing power to process them. For most
researchers, this is prohibitively expensive. As a
consequence, web-mined parallel text has become
the exclusive purview of large companies with the
computational resources to crawl, store, and pro-
cess the entire Web.
To put web-mined parallel text back in the
hands of individual researchers, we mine parallel
text from the Common Crawl, a regularly updated
81-terabyte snapshot of the public internet hosted
1374
on Amazon?s Elastic Cloud (EC2) service.2 Us-
ing the Common Crawl completely removes the
bottleneck of web crawling, and makes it possi-
ble to run algorithms on a substantial portion of
the web at very low cost. Starting from nothing
other than a set of language codes, our extension
of the STRAND algorithm (Resnik and Smith,
2003) identifies potentially parallel documents us-
ing cues from URLs and document content (?2).
We conduct an extensive empirical exploration of
the web-mined data, demonstrating coverage in
a wide variety of languages and domains (?3).
Even without extensive pre-processing, the data
improves translation performance on strong base-
line news translation systems in five different lan-
guage pairs (?4). On general domain and speech
translation tasks where test conditions substan-
tially differ from standard government and news
training text, web-mined training data improves
performance substantially, resulting in improve-
ments of up to 1.5 BLEU on standard test sets, and
5 BLEU on test sets outside of the news domain.
2 Mining the Common Crawl
The Common Crawl corpus is hosted on Ama-
zon?s Simple Storage Service (S3). It can be
downloaded to a local cluster, but the transfer cost
is prohibitive at roughly 10 cents per gigabyte,
making the total over $8000 for the full dataset.3
However, it is unnecessary to obtain a copy of the
data since it can be accessed freely from Amazon?s
Elastic Compute Cloud (EC2) or Elastic MapRe-
duce (EMR) services. In our pipeline, we per-
form the first step of identifying candidate docu-
ment pairs using Amazon EMR, download the re-
sulting document pairs, and perform the remain-
ing steps on our local cluster. We chose EMR be-
cause our candidate matching strategy fit naturally
into the Map-Reduce framework (Dean and Ghe-
mawat, 2004).
Our system is based on the STRAND algorithm
(Resnik and Smith, 2003):
1. Candidate pair selection: Retrieve candidate
document pairs from the CommonCrawl cor-
pus.
2. Structural Filtering:
(a) Convert the HTML of each document
2commoncrawl.org
3http://aws.amazon.com/s3/pricing/
into a sequence of start tags, end tags,
and text chunks.
(b) Align the linearized HTML of candidate
document pairs.
(c) Decide whether to accept or reject each
pair based on features of the alignment.
3. Segmentation: For each text chunk, perform
sentence and word segmentation.
4. Sentence Alignment: For each aligned pair of
text chunks, perform the sentence alignment
method of Gale and Church (1993).
5. Sentence Filtering: Remove sentences that
appear to be boilerplate.
Candidate Pair Selection We adopt a strategy
similar to that of Resnik and Smith (2003) for find-
ing candidate parallel documents, adapted to the
parallel architecture of Map-Reduce.
The mapper operates on each website entry in
the CommonCrawl data. It scans the URL string
for some indicator of its language. Specifically,
we check for:
1. Two/three letter language codes (ISO-639).
2. Language names in English and in the lan-
guage of origin.
If either is present in a URL and surrounded by
non-alphanumeric characters, the URL is identi-
fied as a potential match and the mapper outputs
a key value pair in which the key is the original
URL with the matching string replaced by *, and
the value is the original URL, language name, and
full HTML of the page. For example, if we en-
counter the URL www.website.com/fr/, we
output the following.
? Key: www.website.com/*/
? Value: www.website.com/fr/, French,
(full website entry)
The reducer then receives all websites mapped
to the same ?language independent? URL. If two
or more websites are associated with the same key,
the reducer will output all associated values, as
long as they are not in the same language, as de-
termined by the language identifier in the URL.
This URL-based matching is a simple and in-
expensive solution to the problem of finding can-
didate document pairs. The mapper will discard
1375
most, and neither the mapper nor the reducer do
anything with the HTML of the documents aside
from reading and writing them. This approach is
very simple and likely misses many good potential
candidates, but has the advantage that it requires
no information other than a set of language codes,
and runs in time roughly linear in the size of the
dataset.
Structural Filtering A major component of the
STRAND system is the alignment of HTML docu-
ments. This alignment is used to determine which
document pairs are actually parallel, and if they
are, to align pairs of text blocks within the docu-
ments.
The first step of structural filtering is to lin-
earize the HTML. This means converting its DOM
tree into a sequence of start tags, end tags, and
chunks of text. Some tags (those usually found
within text, such as ?font? and ?a?) are ignored
during this step. Next, the tag/chunk sequences
are aligned using dynamic programming. The ob-
jective of the alignment is to maximize the number
of matching items.
Given this alignment, Resnik and Smith (2003)
define a small set of features which indicate the
alignment quality. They annotated a set of docu-
ment pairs as parallel or non-parallel, and trained
a classifier on this data. We also annotated 101
Spanish-English document pairs in this way and
trained a maximum entropy classifier. However,
even when using the best performing subset of fea-
tures, the classifier only performed as well as a
naive classifier which labeled every document pair
as parallel, in both accuracy and F1. For this rea-
son, we excluded the classifier from our pipeline.
The strong performance of the naive baseline was
likely due to the unbalanced nature of the anno-
tated data? 80% of the document pairs that we
annotated were parallel.
Segmentation The text chunks from the previ-
ous step may contain several sentences, so before
the sentence alignment step we must perform sen-
tence segmentation. We use the Punkt sentence
splitter from NLTK (Loper and Bird, 2002) to
perform both sentence and word segmentation on
each text chunk.
Sentence Alignment For each aligned text
chunk pair, we perform sentence alignment using
the algorithm of Gale and Church (1993).
Sentence Filtering Since we do not perform any
boilerplate removal in earlier steps, there are many
sentence pairs produced by the pipeline which
contain menu items or other bits of text which are
not useful to an SMT system. We avoid perform-
ing any complex boilerplate removal and only re-
move segment pairs where either the source and
target text are identical, or where the source or
target segments appear more than once in the ex-
tracted corpus.
3 Analysis of the Common Crawl Data
We ran our algorithm on the 2009-2010 version
of the crawl, consisting of 32.3 terabytes of data.
Since the full dataset is hosted on EC2, the only
cost to us is CPU time charged by Amazon, which
came to a total of about $400, and data stor-
age/transfer costs for our output, which came to
roughly $100. For practical reasons we split the
run into seven subsets, on which the full algo-
rithm was run independently. This is different
from running a single Map-Reduce job over the
entire dataset, since websites in different subsets
of the data cannot be matched. However, since
the data is stored as it is crawled, it is likely that
matching websites will be found in the same split
of the data. Table 1 shows the amount of raw par-
allel data obtained for a large selection of language
pairs.
As far as we know, ours is the first system built
to mine parallel text from the Common Crawl.
Since the resource is new, we wanted to under-
stand the quantity, quality, and type of data that
we are likely to obtain from it. To this end, we
conducted a number of experiments to measure
these features. Since our mining heuristics are
very simple, these results can be construed as a
lower bound on what is actually possible.
3.1 Recall Estimates
Our first question is about recall: of all the pos-
sible parallel text that is actually available on the
Web, how much does our algorithm actually find
in the Common Crawl? Although this question
is difficult to answer precisely, we can estimate
an answer by comparing our mined URLs against
a large collection of previously mined URLs that
were found using targeted techniques: those in the
French-English Gigaword corpus (Callison-Burch
et al, 2011).
We found that 45% of the URL pairs would
1376
French German Spanish Russian Japanese Chinese
Segments 10.2M 7.50M 5.67M 3.58M 1.70M 1.42M
Source Tokens 128M 79.9M 71.5M 34.7M 9.91M 8.14M
Target Tokens 118M 87.5M 67.6M 36.7M 19.1M 14.8M
Arabic Bulgarian Czech Korean Tamil Urdu
Segments 1.21M 909K 848K 756K 116K 52.1K
Source Tokens 13.1M 8.48M 7.42M 6.56M 1.01M 734K
Target Tokens 13.5M 8.61M 8.20M 7.58M 996K 685K
Bengali Farsi Telugu Somali Kannada Pashto
Segments 59.9K 44.2K 50.6K 52.6K 34.5K 28.0K
Source Tokens 573K 477K 336K 318K 305K 208K
Target Tokens 537K 459K 358K 325K 297K 218K
Table 1: The amount of parallel data mined from CommonCrawl for each language paired with English.
Source tokens are counts of the foreign language tokens, and target tokens are counts of the English
language tokens.
have been discovered by our heuristics, though we
actually only find 3.6% of these URLs in our out-
put.4 If we had included ?f? and ?e? as identi-
fiers for French and English respectively, coverage
of the URL pairs would increase to 74%. How-
ever, we chose not to include single letter identi-
fiers in our experiments due to the high number of
false positives they generated in preliminary ex-
periments.
3.2 Precision Estimates
Since our algorithms rely on cues that are mostly
external to the contents of the extracted data
and have no knowledge of actual languages, we
wanted to evaluate the precision of our algorithm:
how much of the mined data actually consists of
parallel sentences?
To measure this, we conducted a manual anal-
ysis of 200 randomly selected sentence pairs for
each of three language pairs. The texts are het-
erogeneous, covering several topical domains like
tourism, advertising, technical specifications, fi-
nances, e-commerce and medicine. For German-
English, 78% of the extracted data represent per-
fect translations, 4% are paraphrases of each other
(convey a similar meaning, but cannot be used
for SMT training) and 18% represent misalign-
ments. Furthermore, 22% of the true positives
are potentially machine translations (judging by
the quality), whereas in 13% of the cases one of
the sentences contains additional content not ex-
4The difference is likely due to the coverage of the Com-
monCrawl corpus.
pressed in the other. As for the false positives,
13.5% of them have either the source or target
sentence in the wrong language, and the remain-
ing ones representing failures in the alignment
process. Across three languages, our inspection
revealed that around 80% of randomly sampled
data appeared to contain good translations (Table
2). Although this analysis suggests that language
identification and SMT output detection (Venu-
gopal et al, 2011) may be useful additions to the
pipeline, we regard this as reasonably high preci-
sion for our simple algorithm.
Language Precision
Spanish 82%
French 81%
German 78%
Table 2: Manual evaluation of precision (by sen-
tence pair) on the extracted parallel data for Span-
ish, French, and German (paired with English).
In addition to the manual evaluation of preci-
sion, we applied language identification to our
extracted parallel data for several additional lan-
guages. We used the ?langid.py? tool (Lui and
Baldwin, 2012) at the segment level, and report the
percentage of sentence pairs where both sentences
were recognized as the correct language. Table 3
shows our results. Comparing against our man-
ual evaluation from Table 2, it appears that many
sentence pairs are being incorrectly judged as non-
parallel. This is likely because language identifi-
cation tends to perform poorly on short segments.
1377
French German Spanish Arabic
63% 61% 58% 51%
Chinese Japanese Korean Czech
50% 48% 48% 47%
Russian Urdu Bengali Tamil
44% 31% 14% 12%
Kannada Telugu Kurdish
12% 6.3% 2.9%
Table 3: Automatic evaluation of precision
through language identification for several lan-
guages paired with English.
3.3 Domain Name and Topic Analysis
Although the above measures tell us something
about how well our algorithms perform in aggre-
gate for specific language pairs, we also wondered
about the actual contents of the data. A major
difficulty in applying SMT even on languages for
which we have significant quantities of parallel
text is that most of that parallel text is in the news
and government domains. When applied to other
genres, such systems are notoriously brittle. What
kind of genres are represented in the Common
Crawl data?
We first looked at the domain names which con-
tributed the most data. Table 4 gives the top five
domains by the number of tokens. The top two do-
main names are related to travel, and they account
for about 10% of the total data.
We also applied Latent Dirichlet Allocation
(LDA; Blei et al, 2003) to learn a distribution over
latent topics in the extracted data, as this is a pop-
ular exploratory data analysis method. In LDA
a topic is a unigram distribution over words, and
each document is modeled as a distribution over
topics. To create a set of documents from the ex-
tracted CommonCrawl data, we took the English
side of the extracted parallel segments for each
URL in the Spanish-English portion of the data.
This gave us a total of 444, 022 documents. In
our first experiment, we used the MALLET toolkit
(McCallum, 2002) to generate 20 topics, which
are shown in Table 5.
Some of the topics that LDA finds cor-
respond closely with specific domains,
such as topics 1 (blingee.com) and 2
(opensubtitles.org). Several of the topics
correspond to the travel domain. Foreign stop
words appear in a few of the topics. Since our sys-
tem does not include any language identification,
this is not surprising.5 However it does suggest an
avenue for possible improvement.
In our second LDA experiment, we compared
our extracted CommonCrawl data with Europarl.
We created a set of documents from both Com-
monCrawl and Europarl, and again used MAL-
LET to generate 100 topics for this data.6 We then
labeled each document by its most likely topic (as
determined by that topic?s mixture weights), and
counted the number of documents from Europarl
and CommonCrawl for which each topic was most
prominent. While this is very rough, it gives some
idea of where each topic is coming from. Table 6
shows a sample of these topics.
In addition to exploring topics in the datasets,
we also performed additional intrinsic evaluation
at the domain level, choosing top domains for
three language pairs. We specifically classified
sentence pairs as useful or boilerplate (Table 7).
Among our observations, we find that commer-
cial websites tend to contain less boilerplate ma-
terial than encyclopedic websites, and that the ra-
tios tend to be similar across languages in the same
domain.
FR ES DE
www.booking.com 52% 71% 52%
www.hotel.info 34% 44% -
memory-alpha.org 34% 25% 55%
Table 7: Percentage of useful (non-boilerplate)
sentences found by domain and language pair.
hotel.info was not found in our German-
English data.
4 Machine Translation Experiments
For our SMT experiments, we use the Moses
toolkit (Koehn et al, 2007). In these experiments,
a baseline system is trained on an existing parallel
corpus, and the experimental system is trained on
the baseline corpus plus the mined parallel data.
In all experiments we include the target side of the
mined parallel data in the language model, in order
to distinguish whether results are due to influences
from parallel or monolingual data.
5We used MALLET?s stop word removal, but that is only
for English.
6Documents were created from Europarl by taking
?SPEAKER? tags as document boundaries, giving us
208,431 documents total.
1378
Genre Domain Pages Segments Source Tokens Target Tokens
Total 444K 5.67M 71.5M 67.5M
travel www.booking.com 13.4K 424K 5.23M 5.14M
travel www.hotel.info 9.05K 156K 1.93M 2.13M
government www.fao.org 2.47K 60.4K 1.07M 896K
religious scriptures.lds.org 7.04K 47.2K 889K 960K
political www.amnesty.org 4.83K 38.1K 641K 548K
Table 4: The top five domains from the Spanish-English portion of the data. The domains are ranked by
the combined number of source and target tokens.
Index Most Likely Tokens
1 glitter graphics profile comments share love size girl friends happy blingee cute anime twilight sexy emo
2 subtitles online web users files rar movies prg akas dwls xvid dvdrip avi results download eng cd movie
3 miles hotels city search hotel home page list overview select tokyo discount destinations china japan
4 english language students details skype american university school languages words england british college
5 translation japanese english chinese dictionary french german spanish korean russian italian dutch
6 products services ni system power high software design technology control national applications industry
7 en de el instructions amd hyper riv saab kfreebsd poland user fr pln org wikimedia pl commons fran norway
8 information service travel services contact number time account card site credit company business terms
9 people time life day good years work make god give lot long world book today great year end things
10 show km map hotels de hotel beach spain san italy resort del mexico rome portugal home santa berlin la
11 rotary international world club korea foundation district business year global hong kong president ri
12 hotel reviews stay guest rooms service facilities room smoking submitted customers desk score united hour
13 free site blog views video download page google web nero internet http search news links category tv
14 casino game games play domaine ago days music online poker free video film sports golf live world tags bet
15 water food attribution health mango japan massage medical body baby natural yen commons traditional
16 file system windows server linux installation user files set debian version support program install type
17 united kingdom states america house london street park road city inn paris york st france home canada
18 km show map hotels hotel featured search station museum amsterdam airport centre home city rue germany
19 hotel room location staff good breakfast rooms friendly nice clean great excellent comfortable helpful
20 de la en le el hotel es het del und die il est der les des das du para
Table 5: A list of 20 topics generated using the MALLET toolkit (McCallum, 2002) and their most likely
tokens.
4.1 News Domain Translation
Our first set of experiments are based on systems
built for the 2012 Workshop on Statistical Ma-
chine Translation (WMT) (Callison-Burch et al,
2012) using all available parallel and monolingual
data for that task, aside from the French-English
Gigaword. In these experiments, we use 5-gram
language models when the target language is En-
glish or German, and 4-gram language models for
French and Spanish. We tune model weights using
minimum error rate training (MERT; Och, 2003)
on the WMT 2008 test data. The results are given
in Table 8. For all language pairs and both test
sets (WMT 2011 and WMT 2012), we show an
improvement of around 0.5 BLEU.
We also included the French-English Gigaword
in separate experiments given in Table 9, and Table
10 compares the sizes of the datasets used. These
results show that even on top of a different, larger
parallel corpus mined from the web, adding Com-
monCrawl data still yields an improvement.
4.2 Open Domain Translation
A substantial appeal of web-mined parallel data
is that it might be suitable to translation of do-
mains other than news, and our topic modeling
analysis (?3.3) suggested that this might indeed be
the case. We therefore performed an additional
set of experiments for Spanish-English, but we
include test sets from outside the news domain.
1379
Europarl CommonCrawl Most Likely Tokens
9 2975 hair body skin products water massage treatment natural oil weight acid plant
2 4383 river mountain tour park tours de day chile valley ski argentina national peru la
8 10377 ford mercury dealer lincoln amsterdam site call responsible affiliates displayed
7048 675 market services european competition small public companies sector internal
9159 1359 time president people fact make case problem clear good put made years situation
13053 849 commission council european parliament member president states mr agreement
1660 5611 international rights human amnesty government death police court number torture
1617 4577 education training people cultural school students culture young information
Table 6: A sample of topics along with the number of Europarl and CommonCrawl documents where
they are the most likely topic in the mixture. We include topics that are mostly found in Europarl or
CommonCrawl, and some that are somewhat prominent in both.
WMT 11 FR-EN EN-FR ES-EN EN-ES EN-DE
Baseline 30.46 29.96 30.79 32.41 16.12
+Web Data 30.92 30.51 31.05 32.89 16.74
WMT 12 FR-EN EN-FR ES-EN EN-ES EN-DE
Baseline 29.25 27.92 32.80 32.83 16.61
+Web Data 29.82 28.22 33.39 33.41 17.30
Table 8: BLEU scores for several language pairs before and after adding the mined parallel data to
systems trained on data from WMT data.
WMT 11 FR-EN EN-FR
Baseline 30.96 30.69
+Web Data 31.24 31.17
WMT 12 FR-EN EN-FR
Baseline 29.88 28.50
+Web Data 30.08 28.76
Table 9: BLEU scores for French-English and
English-French before and after adding the mined
parallel data to systems trained on data from
WMT data including the French-English Giga-
word (Callison-Burch et al, 2011).
For these experiments, we also include training
data mined from Wikipedia using a simplified ver-
sion of the sentence aligner described by Smith
et al (2010), in order to determine how the ef-
fect of such data compares with the effect of web-
mined data. The baseline system was trained using
only the Europarl corpus (Koehn, 2005) as par-
allel data, and all experiments use the same lan-
guage model trained on the target sides of Eu-
roparl, the English side of all linked Spanish-
English Wikipedia articles, and the English side
of the mined CommonCrawl data. We use a 5-
gram language model and tune using MERT (Och,
Corpus EN-FR EN-ES EN-DE
News Commentary 2.99M 3.43M 3.39M
Europarl 50.3M 49.2M 47.9M
United Nations 316M 281M -
FR-EN Gigaword 668M - -
CommonCrawl 121M 68.8M 88.4M
Table 10: The size (in English tokens) of the train-
ing corpora used in the SMT experiments from Ta-
bles 8 and 9 for each language pair.
2003) on the WMT 2009 test set.
Unfortunately, it is difficult to obtain meaning-
ful results on some open domain test sets such as
the Wikipedia dataset used by Smith et al (2010).
Wikipedia copied across the public internet, and
we did not have a simple way to filter such data
from our mined datasets.
We therefore considered two tests that were
less likely to be problematic. The Tatoeba cor-
pus (Tiedemann, 2009) is a collection of example
sentences translated into many languages by vol-
unteers. The front page of tatoeba.org was
discovered by our URL matching heuristics, but
we excluded any sentence pairs that were found in
the CommonCrawl data from this test set.
1380
The second dataset is a set of crowdsourced
translation of Spanish speech transcriptions from
the Spanish Fisher corpus.7 As part of a re-
search effort on cross-lingual speech applications,
we obtained English translations of the data using
Amazon Mechanical Turk, following a protocol
similar to one described by Zaidan and Callison-
Burch (2011): we provided clear instructions,
employed several quality control measures, and
obtained redundant translations of the complete
dataset (Lopez et al, 2013). The advantage of
this data for our open domain translation test is
twofold. First, the Fisher dataset consists of con-
versations in various Spanish dialects on a wide
variety of prompted topics. Second, because we
obtained the translations ourselves, we could be
absolutely assured that they did not appear in some
form anywhere on the Web, making it an ideal
blind test.
WMT10 Tatoeba Fisher
Europarl 89/72/46/20 94/75/45/18 87/69/39/13
+Wiki 92/78/52/24 96/80/50/21 91/75/44/15
+Web 96/82/56/27 99/88/58/26 96/83/51/19
+Both 96/84/58/29 99/89/60/27 96/83/52/20
Table 11: n-gram coverage percentages (up to 4-
grams) of the source side of our test sets given our
different parallel training corpora computed at the
type level.
WMT10 Tatoeba Fisher
Europarl 27.21 36.13 46.32
+Wiki 28.03 37.82 49.34
+Web 28.50 41.07 51.13
+Both 28.74 41.12 52.23
Table 12: BLEU scores for Spanish-English be-
fore and after adding the mined parallel data to a
baseline Europarl system.
We used 1000 sentences from each of the
Tatoeba and Fisher datasets as test. For com-
parison, we also test on the WMT 2010 test
set (Callison-Burch et al, 2010). Following
Munteanu and Marcu (2005), we show the n-gram
coverage of each corpus (percentage of n-grams
from the test corpus which are also found in the
training corpora) in Table 11. Table 12 gives
end-to-end results, which show a strong improve-
ment on the WMT test set (1.5 BLEU), and larger
7Linguistic Data Consortium LDC2010T04.
improvements on Tatoeba and Fisher (almost 5
BLEU).
5 Discussion
Web-mined parallel texts have been an exclusive
resource of large companies for several years.
However, when web-mined parallel text is avail-
able to everyone at little or no cost, there will
be much greater potential for groundbreaking re-
search to come from all corners. With the advent
of public services such as Amazon Web Services
and the Common Crawl, this may soon be a re-
ality. As we have shown, it is possible to obtain
parallel text for many language pairs in a variety
of domains very cheaply and quickly, and in suf-
ficient quantity and quality to improve statistical
machine translation systems. However, our effort
has merely scratched the surface of what is pos-
sible with this resource. We will make our code
and data available so that others can build on these
results.
Because our system is so simple, we believe that
our results represent lower bounds on the gains
that should be expected in performance of systems
previously trained only on curated datasets. There
are many possible means through which the sys-
tem could be improved, including more sophisti-
cated techniques for identifying matching URLs,
better alignment, better language identification,
better filtering of data, and better exploitation of
resulting cross-domain datasets. Many of the com-
ponents of our pipeline were basic, leaving consid-
erable room for improvement. For example, the
URL matching strategy could easily be improved
for a given language pair by spending a little time
crafting regular expressions tailored to some ma-
jor websites. Callison-Burch et al (2011) gathered
almost 1 trillion tokens of French-English parallel
data this way. Another strategy for mining parallel
webpage pairs is to scan the HTML for links to the
same page in another language (Nie et al, 1999).
Other, more sophisticated techniques may also
be possible. Uszkoreit et al (2010), for ex-
ample, translated all non-English webpages into
English using an existing translation system and
used near-duplicate detection methods to find can-
didate parallel document pairs. Ture and Lin
(2012) had a similar approach for finding paral-
lel Wikipedia documents by using near-duplicate
detection, though they did not need to apply a full
translation system to all non-English documents.
1381
Instead, they represented documents in bag-of-
words vector space, and projected non-English
document vectors into the English vector space us-
ing the translation probabilities of a word align-
ment model. By comparison, one appeal of our
simple approach is that it requires only a table
of language codes. However, with this system
in place, we could obtain enough parallel data to
bootstrap these more sophisticated approaches.
It is also compelling to consider ways in which
web-mined data obtained from scratch could be
used to bootstrap other mining approaches. For
example, Smith et al (2010) mine parallel sen-
tences from comparable documents in Wikipedia,
demonstrating substantial gains on open domain
translation. However, their approach required seed
parallel data to learn models used in a classifier.
We imagine a two-step process, first obtaining par-
allel data from the web, followed by comparable
data from sources such as Wikipedia using mod-
els bootstrapped from the web-mined data. Such a
process could be used to build translation systems
for new language pairs in a very short period of
time, hence fulfilling one of the original promises
of SMT.
Acknowledgements
Thanks to Ann Irvine, Jonathan Weese, and our
anonymous reviewers from NAACL and ACL for
comments on previous drafts. The research lead-
ing to these results has received funding from the
European Union Seventh Framework Programme
(FP7/2007-2013) under grant agreement 288487
(MosesCore). This research was partially funded
by the Johns Hopkins University Human Lan-
guage Technology Center of Excellence, and by
gifts from Google and Microsoft.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, WMT ?10, pages 17?53. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 22?64. Associ-
ation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Jiang Chen and Jian-Yun Nie. 2000. Parallel web text
mining for cross-language ir. In IN IN PROC. OF
RIAO, pages 62?77.
J. Dean and S. Ghemawat. 2004. Mapreduce: simpli-
fied data processing on large clusters. In Proceed-
ings of the 6th conference on Symposium on Opeart-
ing Systems Design & Implementation-Volume 6,
pages 10?10. USENIX Association.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Comput. Linguist., 19:75?102, March.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180. Association for Computa-
tional Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Edward Loper and Steven Bird. 2002. Nltk: the natu-
ral language toolkit. In Proceedings of the ACL-02
Workshop on Effective tools and methodologies for
teaching natural language processing and computa-
tional linguistics - Volume 1, ETMTNLP ?02, pages
63?70. Association for Computational Linguistics.
Adam Lopez, Matt Post, and Chris Callison-Burch.
2013. Parallel speech, transcription, and translation:
The Fisher and Callhome Spanish-English speech
translation corpora. Technical Report 11, Johns
Hopkins University Human Language Technology
Center of Excellence.
Marco Lui and Timothy Baldwin. 2012. langid.py:
an off-the-shelf language identification tool. In Pro-
ceedings of the ACL 2012 System Demonstrations,
ACL ?12, pages 25?30. Association for Computa-
tional Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
1382
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Non-Parallel Corpora. Comput. Linguist.,
31:477?504, December.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the web. In Proceedings of
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?99, pages 74?81, New York, NY,
USA. ACM.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In acl, pages 160?
167, Sapporo, Japan.
P. Resnik and N. A Smith. 2003. The web as a parallel
corpus. Computational Linguistics, 29(3):349?380.
Philip Resnik. 1999. Mining the web for bilingual text.
In Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics, ACL ?99, pages 527?534. As-
sociation for Computational Linguistics.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting Parallel Sentences from Compara-
ble Corpora using Document Level Alignment. In
NAACL 2010.
Jo?rg Tiedemann. 2009. News from OPUS - A col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237?248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.
Ferhan Ture and Jimmy Lin. 2012. Why not grab a
free lunch? mining large corpora for parallel sen-
tences to improve translation modeling. In Proceed-
ings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
626?630, Montre?al, Canada, June. Association for
Computational Linguistics.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 1101?
1109. Association for Computational Linguistics.
Ashish Venugopal, Jakob Uszkoreit, David Talbot,
Franz J. Och, and Juri Ganitkevitch. 2011. Water-
marking the outputs of structured prediction with an
application in statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ?11, pages
1363?1372. Association for Computational Linguis-
tics.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proc. of ACL.
1383
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 63?68,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PARMA: A Predicate Argument Aligner
Travis Wolfe, Benjamin Van Durme, Mark Dredze, Nicholas Andrews,
Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder,
Jonathan Weese, Tan Xu?, and Xuchen Yao
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, Maryland USA
?University of Maryland, College Park, Maryland USA
Abstract
We introduce PARMA, a system for cross-
document, semantic predicate and argu-
ment alignment. Our system combines a
number of linguistic resources familiar to
researchers in areas such as recognizing
textual entailment and question answering,
integrating them into a simple discrimina-
tive model. PARMA achieves state of the
art results on an existing and a new dataset.
We suggest that previous efforts have fo-
cussed on data that is biased and too easy,
and we provide a more difficult dataset
based on translation data with a low base-
line which we beat by 17% F1.
1 Introduction
A key step of the information extraction pipeline
is entity disambiguation, in which discovered en-
tities across many sentences and documents must
be organized to represent real world entities. The
NLP community has a long history of entity dis-
ambiguation both within and across documents.
While most information extraction work focuses
on entities and noun phrases, there have been a
few attempts at predicate, or event, disambigua-
tion. Commonly a situational predicate is taken to
correspond to either an event or a state, lexically
realized in verbs such as ?elect? or nominaliza-
tions such as ?election?. Similar to entity coref-
erence resolution, almost all of this work assumes
unanchored mentions: predicate argument tuples
are grouped together based on coreferent events.
The first work on event coreference dates back to
Bagga and Baldwin (1999). More recently, this
task has been considered by Bejan and Harabagiu
(2010) and Lee et al (2012). As with unanchored
entity disambiguation, these methods rely on clus-
tering methods and evaluation metrics.
Another view of predicate disambiguation seeks
to link or align predicate argument tuples to an ex-
isting anchored resource containing references to
events or actions, similar to anchored entity dis-
ambiguation (entity linking) (Dredze et al, 2010;
Han and Sun, 2011). The most relevant, and per-
haps only, work in this area is that of Roth and
Frank (2012) who linked predicates across docu-
ment pairs, measuring the F1 of aligned pairs.
Here we present PARMA, a new system for pred-
icate argument alignment. As opposed to Roth and
Frank, PARMA is designed as a a trainable plat-
form for the incorporation of the sort of lexical se-
mantic resources used in the related areas of Rec-
ognizing Textual Entailment (RTE) and Question
Answering (QA). We demonstrate the effective-
ness of this approach by achieving state of the art
performance on the data of Roth and Frank despite
having little relevant training data. We then show
that while the ?lemma match? heuristic provides a
strong baseline on this data, this appears to be an
artifact of their data creation process (which was
heavily reliant on word overlap). In response, we
evaluate on a new and more challenging dataset for
predicate argument alignment derived from multi-
ple translation data. We release PARMA as a new
framework for the incorporation and evaluation of
new resources for predicate argument alignment.1
2 PARMA
PARMA (Predicate ARguMent Aligner) is a
pipelined system with a wide variety of features
used to align predicates and arguments in two doc-
uments. Predicates are represented as mention
spans and arguments are represented as corefer-
ence chains (sets of mention spans) provided by
in-document coreference resolution systems such
as included in the Stanford NLP toolkit. Results
indicated that the chains are of sufficient quality
so as not to limit performance, though future work
1https://github.com/hltcoe/parma
63
RF
? Australian [police]1 have [arrested]2 a man in the western city of Perth over an alleged [plot]3 to [bomb]4 Israeli diplomatic
[buildings]5 in the country , police and the suspect s [lawyer]6 [said]7
? Federal [police]1 have [arrested]2 a man over an [alleged]5 [plan]3 to [bomb]4 Israeli diplomatic [posts]8 in Australia , the
suspect s [attorney]6 [said]7 Tuesday
LDC MTC
? As I [walked]1 to the [veranda]2 side , I [saw]2 that a [tent]3 is being decorated for [Mahfil-e-Naat]4 -LRB- A [get-together]5
in which the poetic lines in praise of Prophet Mohammad are recited -RRB-
? I [came]1 towards the [balcony]2 , and while walking over there I [saw]2 that a [camp]3 was set up outside for the [Naatia]4
[meeting]5 .
Figure 1: Example of gold-standard alignment pairs from Roth and Frank?s data set and our data set
created from the LDC?s Multiple Translation Corpora. The RF data set exhibits high lexical overlap,
where most of the alignments are between identical words like police-police and said-said. The LDC
MTC was constructed to increase lexical diversity, leading to more challenging alignments like veranda-
balcony and tent-camp
may relax this assumption.
We refer to a predicate or an argument as an
?item? with type predicate or argument. An align-
ment between two documents is a subset of all
pairs of items in either documents with the same
type.2 We call the two documents being aligned
the source document S and the target document
T . Items are referred to by their index, and ai,j is a
binary variable representing an alignment between
item i in S and item j in T . A full alignment is an
assignment ~a = {aij : i ? NS , j ? NT }, where
NS and NT are the set of item indices for S and T
respectively.
We train a logistic regression model on exam-
ple alignmentsand maximize the likelihood of a
document alignment under the assumption that the
item alignments are independent. Our objective
is to maximize the log-likelihood of all p(S, T )
with an L1 regularizer (with parameter ?). After
learning model parameters w by regularized max-
imum likelihood on training data, we introducing
a threshold ? on alignment probabilities to get a
classifier. We perform line search on ? and choose
the value that maximizes F1 on dev data. Train-
ing was done using the Mallet toolkit (McCallum,
2002).
2.1 Features
The focus of PARMA is the integration of a diverse
range of features based on existing lexical seman-
tic resources. We built PARMA on a supervised
framework to take advantage of this wide variety
of features since they can describe many different
correlated aspects of generation. The following
features cover the spectrum from high-precision
2Note that type is not the same thing as part of speech: we
allow nominal predicates like ?death?.
to high-recall. Each feature has access to the pro-
posed argument or predicate spans to be linked and
the containing sentences as context. While we use
supervised learning, some of the existing datasets
for this task are very small. For extra training data,
we pool material from different datasets and use
the multi-domain split feature space approach to
learn dataset specific behaviors (Daume?, 2007).
Features in general are defined over mention
spans or head tokens, but we split these features
to create separate feature-spaces for predicates and
arguments.3
For argument coref chains we heuristically
choose a canonical mention to represent each
chain, and some features only look at this canon-
ical mention. The canonical mention is cho-
sen based on length,4 information about the head
word,5 and position in the document.6 In most
cases, coref chains that are longer than one are
proper nouns and the canonical mention is the first
and longest mention (outranking pronominal ref-
erences and other name shortenings).
PPDB We use lexical features from the Para-
phrase Database (PPDB) (Ganitkevitch et al,
2013). PPDB is a large set of paraphrases ex-
tracted from bilingual corpora using pivoting tech-
niques. We make use of the English lexical portion
which contains over 7 million rules for rewriting
terms like ?planet? and ?earth?. PPDB offers a
variety of conditional probabilities for each (syn-
chronous context free grammar) rule, which we
3While conceptually cleaner, In practice we found this
splitting to have no impact on performance.
4in tokens, not counting some words like determiners and
auxiliary verbs
5like its part of speech tag and whether the it was tagged
as a named entity
6mentions that appear earlier in the document and earlier
in a given sentence are given preference
64
treat as independent experts. For each of these rule
probabilities (experts), we find all rules that match
the head tokens of a given alignment and have a
feature for the max and harmonic mean of the log
probabilities of the resulting rule set.
FrameNet FrameNet is a lexical database based
on Charles Fillmore?s Frame Semantics (Fill-
more, 1976; Baker et al, 1998). The database
(and the theory) is organized around seman-
tic frames that can be thought of as descrip-
tions of events. Frames crucially include spec-
ification of the participants, or Frame Elements,
in the event. The Destroying frame, for in-
stance, includes frame elements Destroyer or
Cause Undergoer. Frames are related to other
frames through inheritance and perspectivization.
For instance the frames Commerce buy and
Commerce sell (with respective lexical real-
izations ?buy? and ?sell?) are both perspectives of
Commerce goods-transfer (no lexical re-
alizations) which inherits from Transfer (with
lexical realization ?transfer?).
We compute a shortest path between headwords
given edges (hypernym, hyponym, perspectivized
parent and child) in FrameNet and bucket by dis-
tance to get features. We also have a binary feature
for whether two tokens evoke the same frame.
TED Alignments Given two predicates or argu-
ments in two sentences, we attempt to align the
two sentences they appear in using a Tree Edit
Distance (TED) model that aligns two dependency
trees, based on the work described by (Yao et al,
2013). We represent a node in a dependency tree
with three fields: lemma, POS tag and the type
of dependency relation to the node?s parent. The
TED model aligns one tree with the other using
the dynamic programming algorithm of Zhang and
Shasha (1989) with three predefined edits: dele-
tion, insertion and substitution, seeking a solution
yielding the minimum edit cost. Once we have
built a tree alignment, we extract features for 1)
whether the heads of the two phrases are aligned
and 2) the count of how many tokens are aligned
in both trees.
WordNet WordNet (Miller, 1995) is a database
of information (synonyms, hypernyms, etc.) per-
taining to words and short phrases. For each entry,
WordNet provides a set of synonyms, hypernyms,
etc. Given two spans, we use WordNet to deter-
mine semantic similarity by measuring how many
synonym (or other) edges are needed to link two
terms. Similar words will have a short distance.
For features, we find the shortest path linking the
head words of two mentions using synonym, hy-
pernym, hyponym, meronym, and holonym edges
and bucket the length.
String Transducer To represent similarity be-
tween arguments that are names, we use a stochas-
tic edit distance model. This stochastic string-to-
string transducer has latent ?edit? and ?no edit?
regions where the latent regions allow the model
to assign high probability to contiguous regions of
edits (or no edits), which are typical between vari-
ations of person names. In an edit region, param-
eters govern the relative probability of insertion,
deletion, substitution, and copy operations. We
use the transducer model of Andrews et al (2012).
Since in-domain name pairs were not available, we
picked 10,000 entities at random from Wikipedia
to estimate the transducer parameters. The entity
labels were used as weak supervision during EM,
as in Andrews et al (2012).
For a pair of mention spans, we compute the
conditional log-likelihood of the two mentions go-
ing both ways, take the max, and then bucket to get
binary features. We duplicate these features with
copies that only fire if both mentions are tagged as
PER, ORG or LOC.
3 Evaluation
We consider three datasets for evaluating PARMA.
For richer annotations that include lemmatiza-
tions, part of speech, NER, and in-doc corefer-
ence, we pre-processed each of the datasets using
tools7 similar to those used to create the Annotated
Gigaword corpus (Napoles et al, 2012).
Extended Event Coreference Bank Based on
the dataset of Bejan and Harabagiu (2010), Lee et
al. (2012) introduced the Extended Event Coref-
erence Bank (EECB) to evaluate cross-document
event coreference. EECB provides document clus-
ters, within which entities and events may corefer.
Our task is different from Lee et al but we can
modify the corpus setup to support our task. To
produce source and target document pairs, we se-
lect the first document within every cluster as the
source and each of the remaining documents as
target documents (i.e. N ? 1 pairs for a cluster
of size N ). This yielded 437 document pairs.
Roth and Frank The only existing dataset for
our task is from Roth and Frank (2012) (RF), who
7https://github.com/cnap/anno-pipeline
65
annotated documents from the English Gigaword
Fifth Edition corpus (Parker et al, 2011). The data
was generated by clustering similar news stories
from Gigaword using TF-IDF cosine similarity of
their headlines. This corpus is small, containing
only 10 document pairs in the development set and
60 in the test set. To increase the training size,
we train PARMA with 150 randomly selected doc-
ument pairs from both EECB and MTC, and the
entire dev set from Roth and Frank using multi-
domain feature splitting. We tuned the threshold
? on the Roth and Frank dev set, but choose the
regularizer ? based on a grid search on a 5-fold
version of the EECB dataset.
Multiple Translation Corpora We constructed
a new predicate argument alignment dataset
based on the LDC Multiple Translation Corpora
(MTC),8 which consist of multiple English trans-
lations for foreign news articles. Since these mul-
tiple translations are semantically equivalent, they
provide a good resource for aligned predicate ar-
gument pairs. However, finding good pairs is a
challenge: we want pairs with significant overlap
so that they have predicates and arguments that
align, but not documents that are trivial rewrites
of each other. Roth and Frank selected document
pairs based on clustering, meaning that the pairs
had high lexical overlap, often resulting in mini-
mal rewrites of each other. As a result, despite ig-
noring all context, their baseline method (lemma-
alignment) worked quite well.
To create a more challenging dataset, we se-
lected document pairs from the multiple transla-
tions that minimize the lexical overlap (in En-
glish). Because these are translations, we know
that there are equivalent predicates and arguments
in each pair, and that any lexical variation pre-
serves meaning. Therefore, we can select pairs
with minimal lexical overlap in order to create
a system that truly stresses lexically-based align-
ment systems.
Each document pair has a correspondence be-
tween sentences, and we run GIZA++ on these
sentences to produce token-level alignments. We
take all aligned nouns as arguments and all aligned
verbs (excluding be-verbs, light verbs, and report-
ing verbs) as predicates. We then add negative ex-
amples by randomly substituting half of the sen-
tences in one document with sentences from an-
8LDC2010T10, LDC2010T11, LDC2010T12,
LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01,
LDC2003T18, and LDC2005T05
0.3 0.4 0.5 0.6 0.7 0.8
0.0
0.2
0.4
0.6
0.8
1.0
Performance vs Lexical Overlap
Doc-pair Cosine Similarity
F1
Figure 2: We plotted the PARMA?s performance on
each of the document pairs. Red squares show the
F1 for individual document pairs drawn from Roth
and Frank?s data set, and black circles show F1 for
our Multiple Translation Corpora test set. The x-
axis represents the cosine similarity between the
document pairs. On the RF data set, performance
is correlated with lexical similarity. On our more
lexically diverse set, this is not the case. This
could be due to the fact that some of the docu-
ments in the RF sets are minor re-writes of the
same newswire story, making them easy to align.
other corpus, guaranteed to be unrelated. The
amount of substitutions we perform can vary the
?relatedness? of the two documents in terms of
the predicates and arguments that they talk about.
This reflects our expectation of real world data,
where we do not expect perfect overlap in predi-
cates and arguments between a source and target
document, as you would in translation data.
Lastly, we prune any document pairs that have
more than 80 predicates or arguments or have a
Jaccard index on bags of lemmas greater than 0.5,
to give us a dataset of 328 document pairs.
Metric We use precision, recall, and F1. For the
RF dataset, we follow Roth and Frank (2012) and
Cohn et al (2008) and evaluate on a version of F1
that considers SURE and POSSIBLE links, which
are available in the RF data. Given an alignment
to be scored A and a reference alignment B which
contains SURE and POSSIBLE links, Bs andBp re-
spectively, precision and recall are:
P = |A ?Bp||A| R =
|A ?Bs|
|Bs|
(1)
66
F1 P R
EECB lemma 63.5 84.8 50.8
PARMA 74.3 80.5 69.0
RF lemma 48.3 40.3 60.3
Roth and Frank 54.8 59.7 50.7
PARMA 57.6 52.4 64.0
MTC lemma 42.1 51.3 35.7
PARMA 59.2 73.4 49.6
Table 1: PARMA outperforms the baseline lemma
matching system on the three test sets, drawn from
the Extended Event Coreference Bank, Roth and
Frank?s data, and our set created from the Multiple
Translation Corpora. PARMA achieves a higher F1
and recall score than Roth and Frank?s reported
result.
and F1 as the harmonic mean of the two. Results
for EECB and MTC reflect 5-fold cross validation,
and RF uses the given dev/test split.
Lemma baseline Following Roth and Frank we
include a lemma baseline, in which two predicates
or arguments align if they have the same lemma.9
4 Results
On every dataset PARMA significantly improves
over the lemma baselines (Table 1). On RF,
compared to Roth and Frank, the best published
method for this task, we also improve, making
PARMA the state of the art system for this task.
Furthermore, we expect that the smallest improve-
ments over Roth and Frank would be on RF, since
there is little training data. We also note that com-
pared to Roth and Frank we obtain much higher
recall but lower precision.
We also observe that MTC was more challeng-
ing than the other datasets, with a lower lemma
baseline. Figure 2 shows the correlation between
document similarity and document F1 score for
RF and MTC. While for RF these two measures
are correlated, they are uncorrelated for MTC. Ad-
ditionally, there is more data in the MTC dataset
which has low cosine similarity than in RF.
5 Conclusion
PARMA achieves state of the art performance on
three datasets for predicate argument alignment.
It builds on the development of lexical semantic
resources and provides a platform for learning to
utilize these resources. Additionally, we show that
9We could not reproduce lemma from Roth and Frank
(shown in Table 1) due to a difference in lemmatizers. We ob-
tained 55.4; better than their system but worse than PARMA.
task difficulty can be strongly tied to lexical simi-
larity if the evaluation dataset is not chosen care-
fully, and this provides an artificially high baseline
in previous work. PARMA is robust to drops in lex-
ical similarity and shows large improvements in
those cases. PARMA will serve as a useful bench-
mark in determining the value of more sophis-
ticated models of predicate-argument alignment,
which we aim to address in future work.
While our system is fully supervised, and thus
dependent on manually annotated examples, we
observed here that this requirement may be rela-
tively modest, especially for in-domain data.
Acknowledgements
We thank JHU HLTCOE for hosting the winter
MiniSCALE workshop that led to this collabora-
tive work. This material is based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program). The U.S. Government
is authorized to reproduce and distribute reprints
for Governmental purposes. The views and con-
clusions contained in this publication are those of
the authors and should not be interpreted as repre-
senting official policies or endorsements of NSF,
DARPA, or the U.S. Government.
References
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Empirical Methods in Natural
Language Processing (EMNLP).
Amit Bagga and Breck Baldwin. 1999. Cross-
document event coreference: Annotations, exper-
iments, and observations. In Proceedings of the
Workshop on Coreference and its Applications,
pages 1?8. Association for Computational Linguis-
tics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
67
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
put. Linguist., 34(4):597?614, December.
Hal Daume?. 2007. Frustratingly easy domain adap-
tation. In Annual meeting-association for computa-
tional linguistics, volume 45, page 256.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Conference on
Computational Linguistics (Coling).
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, 280(1):20?
32.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 945?
954. Association for Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012, June.
Robert Parker, David Graff, Jumbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword fifth
edition.
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 218?
227, Montre?al, Canada, 7-8 June. Association for
Computational Linguistics.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer extraction as
sequence tagging with tree edit distance. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262, De-
cember.
68
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 702?707,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Lightweight and High Performance Monolingual Word Aligner
Xuchen Yao and Benjamin Van Durme
Johns Hopkins University
Baltimore, MD, USA
Chris Callison-Burch?
University of Pennsylvania
Philadelphia, PA, USA
Peter Clark
Vulcan Inc.
Seattle, WA, USA
Abstract
Fast alignment is essential for many nat-
ural language tasks. But in the setting of
monolingual alignment, previous work has
not been able to align more than one sen-
tence pair per second. We describe a dis-
criminatively trained monolingual word
aligner that uses a Conditional Random
Field to globally decode the best align-
ment with features drawn from source and
target sentences. Using just part-of-speech
tags and WordNet as external resources,
our aligner gives state-of-the-art result,
while being an order-of-magnitude faster
than the previous best performing system.
1 Introduction
In statistical machine translation, alignment is typ-
ically done as a one-off task during training. How-
ever for monolingual tasks, like recognizing tex-
tual entailment or question answering, alignment
happens repeatedly: once or multiple times per
test item. Therefore, the efficiency of the aligner is
of utmost importance for monolingual alignment
tasks. Monolingual word alignment also has a va-
riety of distinctions than the bilingual case, for ex-
ample: there is often less training data but more
lexical resources available; semantic relatedness
may be cued by distributional word similarities;
and, both the source and target sentences share the
same grammar.
These distinctions suggest a model design that
utilizes arbitrary features (to make use of word
similarity measure and lexical resources) and ex-
ploits deeper sentence structures (especially in the
case of major languages where robust parsers are
available). In this setting the balance between
precision and speed becomes an issue: while we
might leverage an extensive NLP pipeline for a
?Performed while faculty at Johns Hopkins University.
language like English, such pipelines can be com-
putationally expensive. One earlier attempt, the
MANLI system (MacCartney et al, 2008), used
roughly 5GB of lexical resources and took 2 sec-
onds per alignment, making it hard to be deployed
and run in large scale. On the other extreme, a sim-
ple non-probabilistic Tree Edit Distance (TED)
model (c.f. ?4.2) is able to align 10, 000 pairs
per second when the sentences are pre-parsed, but
with significantly reduced performance. Trying to
embrace the merits of both worlds, we introduce a
discriminative aligner that is able to align tens to
hundreds of sentence pairs per second, and needs
access only to a POS tagger and WordNet.
This aligner gives state-of-the-art performance
on the MSR RTE2 alignment dataset (Brockett,
2007), is faster than previous work, and we re-
lease it publicly as the first open-source monolin-
gual word aligner: Jacana.Align.1
2 Related Work
The MANLI aligner (MacCartney et al, 2008)
was first proposed to align premise and hypothe-
sis sentences for the task of natural language in-
ference. It applies perceptron learning and han-
dles phrase-based alignment of arbitrary phrase
lengths. Thadani and McKeown (2011) opti-
mized this model by decoding via Integer Linear
Programming (ILP). Benefiting from modern ILP
solvers, this led to an order-of-magnitude speedup.
With extra syntactic constraints added, the exact
alignment match rate for whole sentence pairs was
also significantly improved.
Besides the above supervised methods, indirect
supervision has also been explored. Among them,
Wang and Manning (2010) extended the work of
McCallum et al (2005) and modeled alignment
as latent variables. Heilman and Smith (2010)
used tree kernels to search for the alignment that
1http://code.google.com/p/jacana/
702
yields the lowest tree edit distance. Other tree
or graph matching work for alignment includes
that of (Punyakanok et al, 2004; Kouylekov and
Magnini, 2005; Chambers et al, 2007; Mehdad,
2009; Roth and Frank, 2012).
Finally, feature and model design in monolin-
gual alignment is often inspired by bilingual work,
including distortion modeling, phrasal alignment,
syntactic constraints, etc (Och and Ney, 2003;
DeNero and Klein, 2007; Bansal et al, 2011).
3 The Alignment Model
3.1 Model Design
Our work is heavily influenced by the bilingual
alignment literature, especially the discriminative
model proposed by Blunsom and Cohn (2006).
Given a source sentence s of length M , and a tar-
get sentence t of length N , the alignment from s
to t is a sequence of target word indices a, where
am?[1,M ] ? [0, N ]. We specify that when am = 0,
source word st is aligned to a NULL state, i.e.,
deleted. This models a many-to-one alignment
from source to target. Multiple source words can
be aligned to the same target word, but not vice
versa. One-to-many alignment can be obtained
by running the aligner in the other direction. The
probability of alignment sequence a conditioned
on both s and t is then:
p(a | s, t) =
exp(
?
m,k ?kfk(am?1, am, s, t))
Z(s, t)
This assumes a first-order Conditional Random
Field (Lafferty et al, 2001). The word alignment
task is evaluated over F1. Instead of directly op-
timizing F1, we employ softmax-margin training
(Gimpel and Smith, 2010) and add a cost function
to the normalizing function Z(s, t) in the denom-
inator, which becomes:
?
a?
exp(
?
m,k
?kfk(a?m?1, a?m, s, t) + cost(at, a?))
where at is the true alignments. cost(at, a?)
can be viewed as special ?features? with uniform
weights that encourage consistent with true align-
ments. It is only computed during training in the
denominator because cost(at,at) = 0 in the nu-
merator. Hamming cost is used in practice.
One distinction of this alignment model com-
pared to other commonly defined CRFs is that
the input is two dimensional: at each position m,
the model inspects both the entire sequence of
source words (as the observation) and target words
(whose offset indices are states). The other dis-
tinction is that the size of its state space is not
fixed (e.g., unlike POS tagging, where states are
for instance 45 Penn Treebank tags), but depends
on N , the length of target sentence. Thus we can
not ?memorize? what features are mostly associ-
ated with what states. For instance, in the task of
tagging mail addresses, a feature of ?5 consecu-
tive digits? is highly indicative of a POSTCODE.
However, in the alignment model, it does not make
sense to design features based on a hard-coded
state, say, a feature of ?source word lemma match-
ing target word lemma? fires for state index 6.
To avoid this data sparsity problem, all features
are defined implicitly with respect to the state. For
instance:
fk(am?1, am, s, t) =
{
1 lemmas match: sm, tam
0 otherwise
Thus this feature fires for, e.g.:
(s3 = sport, t5 = sports, a3 = 5), and:
(s2 = like, t10 = liked, a2 = 10).
3.2 Feature Design
String Similarity Features include the following
similarity measures: Jaro Winkler, Dice Sorensen,
Hamming, Jaccard, Levenshtein, NGram overlap-
ping and common prefix matching.2 Also, two
binary features are added for identical match and
identical match ignoring case.
POS Tags Features are binary indicators of
whether the POS tags of two words match. Also,
a ?possrc2postgt? feature fires for each word pair,
with respect to their POS tags. This would capture,
e.g., ?vbz2nn?, when a verb such as arrests aligns
with a noun such as custody.
Positional Feature is a real-valued feature for the
positional difference of the source and target word
(abs(mM ? amN )).WordNet Features indicate whether two words
are of the following relations of each other: hyper-
nym, hyponym, synonym, derived form, entailing,
causing, members of, have member, substances of,
have substances, parts of, have part; or whether
2Of these features the trained aligner preferred Dice
Sorensen and NGram overlapping.
703
their lemmas match.3
Distortion Features measure how far apart the
aligned target words of two consecutive source
words are: abs(am + 1 ? am?1). This learns a
general pattern of whether these two target words
aligned with two consecutive source words are
usually far away from each other, or very close.
We also added special features for corner cases
where the current word starts or ends the source
sentence, or both the previous and current words
are deleted (a transition from NULL to NULL).
Contextual Features indicate whether the left or
the right neighbor of the source word and aligned
target word are identical or similar. This helps
especially when aligning functional words, which
usually have multiple candidate target functional
words to align to and string similarity features can-
not help. We also added features for neighboring
POS tags matching.
3.3 Symmetrization
To expand from many-to-one alignment to many-
to-many, we ran the model in both directions and
applied the following symmetrization heuristics
(Koehn, 2010): INTERSECTION, UNION, GROW-
DIAG-FINAL.
4 Experiments
4.1 Setup
Since no generic off-the-shelf CRF software is de-
signed to handle the special case of dynamic state
indices and feature functions (Blunsom and Cohn,
2006), we implemented this aligner model in the
Scala programming language, which is fully in-
teroperable with Java. We used the L2 regular-
izer and LBFGS for optimization. OpenNLP4 pro-
vided the POS tagger and JWNL5 interfaced with
WordNet (Fellbaum, 1998).
To make results directly comparable, we closely
followed the setup of MacCartney et al (2008) and
Thadani and McKeown (2011). Training and test
data (Brockett, 2007) each contains 800 manually
aligned premise and hypothesis pairs from RTE2.
Note that the premises contain 29 words on av-
erage, and the hypotheses only 11 words. We take
the premise as the source and hypothesis as the tar-
get, and use S2T to indicate the model aligns from
3We found that each word has to be POS tagged to get an
accurate relation, otherwise this feature will not help.
4http://opennlp.apache.org/
5http://jwordnet.sf.net/
source to target and T2S from target to source.
4.2 Simple Baselines
We additionally used two baseline systems for
comparison. One was GIZA++, with the IN-
TERSECTION tricks post-applied, which worked
the best among all other symmetrization heuris-
tics. The other was a Tree Edit Distance (TED)
model, popularly used in a series of NLP appli-
cations (Punyakanok et al, 2004; Kouylekov and
Magnini, 2005; Heilman and Smith, 2010). We
used uniform cost for deletion, insertion and sub-
stitutions, and applied a dynamic program algo-
rithm (Zhang and Shasha, 1989) to decode the
tree edit sequence with the minimal cost, based
on the Stanford dependency tree (De Marneffe
and Manning, 2008). This non-probabilistic ap-
proach turned out to be extremely fast, processing
about 10,000 sentence pairs per second with pre-
parsed trees, performing quantitatively better than
the Stanford RTE aligner (Chambers et al, 2007).
4.3 MANLI Baselines
MANLI was first developed by MacCartney et al
(2008), and then improved by Thadani and McKe-
own (2011) with faster and exact decoding via ILP.
There are four versions to be compared here:
MANLI the original version.
MANLI-approx. re-implemented version by
Thadani and McKeown (2011).
MANLI-exact decoding via ILP solvers.
MANLI-constraint MANLI-exact with hard
syntactic constraints, mainly on common ?light?
words (determiners, prepositions, etc.) attachment
to boost exact match rate.
4.4 Results
Following Thadani and McKeown (2011), perfor-
mance is evaluated by macro-averaged precision,
recall, F1 of aligned token pairs, and exact (per-
fect) match rate for a whole pair, shown in Ta-
ble 1. As our baselines, GIZA++ (with align-
ment intersection of two directions) and TED are
on par with previously reported results using the
Stanford RTE aligner. The MANLI-family of sys-
tems provide stronger baselines, notably MANLI-
constraint, which has the best F1 and exact match
rate among themselves.
We ran our aligner in two directions: S2T and
T2S, then merged the results with INTERSECTION,
UNION and GROW-DIAG-FINAL. Our system beats
704
System P % R % F1 % E %
GIZA++, ? 82.5 74.4 78.3 14.0
TED 80.6 79.0 79.8 13.5
Stanford RTE? 82.7 75.8 79.1 -
MANLI? 85.4 85.3 85.3 21.3
MANLI-approx./ 87.2 86.3 86.7 24.5
MANLI-exact/ 87.2 86.1 86.8 24.8
MANLI-constraint/ 89.5 86.2 87.8 33.0
this work, S2T 91.8 83.4 87.4 25.9
this work, T2S 93.7 84.0 88.6 35.3
S2T ? T2S 95.4 80.8 87.5 31.3
S2T ? T2S 90.3 86.6 88.4 29.6
GROW-DIAG-FINAL 94.4 81.8 87.6 30.8
Table 1: Results on the 800 pairs of test data. E% stands
for exact (perfect) match rate. Systems marked with ? are
reported by MacCartney et al (2008), with / by Thadani and
McKeown (2011).
the weak and strong baselines6 in all measures ex-
cept recall. Some patterns are very clearly shown:
Higher precision, lower recall is due to the
higher-quality and lower-coverage of WordNet,
where the MANLI-family systems used addi-
tional, automatically derived lexical resources.
Imbalance of exact match rate between S2T and
T2S with a difference of 9.4% is due to the many-
to-one nature of the aligner. When aligning from
source (longer) to target (shorter), multiple source
words can align to the same target word. This
is not desirable since multiple duplicate ?light?
words are aligned to the same ?light? word in the
target, which breaks perfect match. When align-
ing T2S, this problem goes away: the shorter tar-
get sentence contains less duplicate words, and in
most cases there is an one-to-one mapping.
MT heuristics help, with INTERSECTION and
UNION respectively improving precision and re-
call.
4.5 Runtime Test
Table 2 shows the runtime comparison. Since the
RTE2 corpus is imbalanced, with premise length
(words) of 29 and hypothesis length of 11, we
also compare on the corpus of FUSION (McKeown
et al, 2010), with both sentences in a pair aver-
aging 27. MANLI-approx. is the slowest, with
quadratic growth in the number of edits with sen-
tence length. MANLI-exact is in second place, re-
lying on the ILP solver. This work has a precise
O(MN2) decoding time, with M the source sen-
tence length and N the target sentence length.
6Unfortunately both MacCartney and Thadani no longer
have their original output files (personal communication), so
we cannot run a significance test against their result.
corpus sent. pair
length
MANLI-
approx.
MANLI-
exact
this
work
RTE2 29/11 1.67 0.08 0.025
FUSION 27/27 61.96 2.45 0.096
Table 2: Alignment runtime in seconds per sentence pair on
two corpora: RTE2 (Cohn et al, 2008) and FUSION (McKe-
own et al, 2010). The MANLI-* results are from Thadani
and McKeown (2011), on a Xeon 2.0GHz with 6MB Cache.
The runtime for this work takes the longest timing from S2T
and T2S, on a Xeon 2.2GHz with 4MB cache (the closest
we can find to match their hardware). Horizontally in a real-
world application where sentences have similar length, this
work is roughly 20x faster (0.096 vs. 2.45). Vertically, the
decoding time for our work increases less dramatically when
sentence length increases (0.025?0.096 vs. 0.08?2.45).
features P % R % F1 % E %
full (T2S) 93.7 84.0 88.6 35.3
- POS 93.2 83.5 88.1 31.4
- WordNet 93.2 83.7 88.2 33.5
- both 93.1 83.2 87.8 30.1
Table 3: Performance without POS and/or Word-
Net features.
While MANLI-exact is about twenty-fold faster
than MANLI-approx., our aligner is at least an-
other twenty-fold faster than MANLI-exact when
the sentences are longer and balanced. We also
benefit from shallower pre-processing (no parsing)
and can store all resources in main memory.7
4.6 Ablation Test
Since WordNet and the POS tagger is the only used
external resource, we removed them8 from the fea-
ture sets and reported performance in Table 3. This
somehow reflects how the model would perform
for a language without a suitable POS tagger, or
more commonly, WordNet in that language. At
this time, the model falls back to relying on string
similarities, distortion, positional and contextual
features, which are almost language-independent.
A loss of less than 1% in F1 suggests that the
aligner can still run reasonably well without a POS
tagger and WordNet.
7WordNet (?30MB) is a smaller footprint than the 5GB of
external resources used by MANLI.
8per request of reviewers. Note that WordNet is less pre-
cise without a POS tagger. When we removed the POS tag-
ger, we enumerated all POS tags for a word to find its hyper-
nym/synonym/... synsets.
705
4.7 Error Analysis
There were three primary categories of error:9
1. Token-based paraphrases that are not covered
by WordNet, such as program and software,
business and venture. This calls for broader-
coverage paraphrase resources.
2. Words that are semantically related but not
exactly paraphrases, such as married and
wife, beat and victory. This calls for re-
sources of close distributional similarity.
3. Phrases of the above kinds, such as elected
and won a seat, politician and presidential
candidate. This calls for further work on
phrase-based alignment.10
There is a trade-off using WordNet vs. larger,
noisier resources in exchange of higher preci-
sion vs. recall and memory/disk allocation. We
think this is an application-specific decision; other
resources could be easily incorporated into our
model, which we may explore in the future to ex-
plore the trade-off in addressing items 1 and 2.
5 Conclusion
We presented a model for monolingual sentence
alignment that gives state-of-the-art performance,
and is significantly faster than prior work. We re-
lease our implementation as the first open-source
monolingual aligner, which we hope to be of ben-
efit to other researchers in the rapidly expanding
area of natural language inference.
Acknowledgement
We thank Vulcan Inc. for funding this work. We
also thank Jason Smith, Travis Wolfe, Frank Fer-
raro for various discussion, suggestion, comments
and the three anonymous reviewers.
References
Mohit Bansal, Chris Quirk, and Robert Moore. 2011.
Gappy phrasal alignment by agreement. In Proceed-
ings of ACL, Portland, Oregon, June.
9We submitted a browser in JavaScript
(AlignmentBrowser.html) in the supporting material
that compares the gold alignment and test output; readers are
encouraged to try it out.
10Note that MacCartney et al (2008) showed that in the
MANLI system setting phrase size to larger than one there
was only a 0.2% gain in F1, while the complexity became
much larger.
P. Blunsom and T. Cohn. 2006. Discriminative word
alignment with conditional random fields. In Pro-
ceedings of ACL2006, pages 65?72.
Chris Brockett. 2007. Aligning the RTE 2006 corpus.
Technical report, Microsoft Research.
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-
don, B. MacCartney, M.C. de Marneffe, D. Ramage,
E. Yeh, and C.D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing, pages 165?170.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
put. Linguist., 34(4):597?614, December.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL2007.
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
margin crfs: training log-linear models with cost
functions. In NAACL 2010, pages 733?736.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL 2010, pages 1011?1019, Los Angeles, Cali-
fornia, June.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance
algorithms. In PASCAL Challenges on RTE, pages
17?20.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA.
B. MacCartney, M. Galley, and C.D. Manning. 2008.
A phrase-based alignment model for natural lan-
guage inference. In Proceedings of EMNLP2008,
pages 802?811.
Andrew McCallum, Kedar Bellare, and Fernando
Pereira. 2005. A Conditional Random Field
for Discriminatively-trained Finite-state String Edit
Distance. In Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence (UAI 2005),
July.
706
Kathleen McKeown, Sara Rosenthal, Kapil Thadani,
and Coleman Moore. 2010. Time-efficient creation
of an accurate sentence fusion corpus. In ACL2010
short, pages 317?320.
Y. Mehdad. 2009. Automatic cost estimation for tree
edit distance using particle swarm optimization. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 289?292.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2004.
Mapping Dependencies Trees: An Application to
Question Answerin. In Proceedings of the 8th In-
ternational Symposium on Artificial Intelligence and
Mathematics, Fort Lauderdale, Florida.
Michael Roth and Anette Frank. 2012. Aligning pred-
icates across monolingual comparable texts using
graph-based clustering. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 171?182, Jeju Island,
Korea, July.
Kapil Thadani and Kathleen McKeown. 2011. Opti-
mal and syntactically-informed decoding for mono-
lingual phrase-based alignment. In Proceedings of
ACL short.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question an-
swering. In Proceedings of the 23rd International
Conference on Computational Linguistics, COLING
?10, pages 1164?1172, Stroudsburg, PA, USA.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262, De-
cember.
707
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1134?1144,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Are Two Heads Better than One? Crowdsourced Translation via a
Two-Step Collaboration of Non-Professional Translators and Editors
Rui Yan, Mingkun Gao, Ellie Pavlick, and Chris Callison-Burch
Computer and Information Science Department,
University of Pennsylvania, Philadelphia, PA 19104, U.S.A.
{ruiyan,gmingkun,epavlick}@seas.upenn.edu, ccb@cis.upenn.edu
Abstract
Crowdsourcing is a viable mechanism for
creating training data for machine trans-
lation. It provides a low cost, fast turn-
around way of processing large volumes
of data. However, when compared to pro-
fessional translation, naive collection of
translations from non-professionals yields
low-quality results. Careful quality con-
trol is necessary for crowdsourcing to
work well. In this paper, we examine
the challenges of a two-step collaboration
process with translation and post-editing
by non-professionals. We develop graph-
based ranking models that automatically
select the best output from multiple redun-
dant versions of translations and edits, and
improves translation quality closer to pro-
fessionals.
1 Introduction
Statistical machine translation (SMT) systems are
trained using bilingual sentence-aligned parallel
corpora. Theoretically, SMT can be applied to
any language pair, but in practice it produces the
state-of-art results only for language pairs with
ample training data, like English-Arabic, English-
Chinese, French-English, etc. SMT gets stuck
in a severe bottleneck for many minority or ?low
resource? languages with insufficient data. This
drastically limits which languages SMT can be
successfully applied to. Because of this, collect-
ing parallel corpora for minor languages has be-
come an interesting research challenge. There are
various options for creating training data for new
language pairs. Past approaches have examined
harvesting translated documents from the web
(Resnik and Smith, 2003; Uszkoreit et al, 2010;
Smith et al, 2013), or discovering parallel frag-
ments from comparable corpora (Munteanu and
Marcu, 2005; Abdul-Rauf and Schwenk, 2009;
Smith et al, 2010). Until relatively recently, lit-
tle consideration has been given to creating par-
allel data from scratch. This is because the cost
of hiring professional translators is prohibitively
high. For instance, Germann (2001) hoped to hire
professional translators to create a modest sized
100,000 word Tamil-English parallel corpus, but
were stymied by the costs and the difficulty of
finding good translators for a short-term commit-
ment.
Recently, crowdsourcing has opened the possi-
bility of translating large amounts of text at low
cost using non-professional translators. Facebook
localized its web site into different languages us-
ing volunteers (TechCrunch, 2008). DuoLingo
turns translation into an educational game, and
translates web content using its language learners
(von Ahn, 2013).
Rather than relying on volunteers or gamifica-
tion, NLP research into crowdsourcing transla-
tion has focused on hiring workers on the Ama-
zon Mechanical Turk (MTurk) platform (Callison-
Burch, 2009). This setup presents unique chal-
lenges, since it typically involves non-professional
translators whose language skills are varied, and
since it sometimes involves participants who try
to cheat to get the small financial reward (Zaidan
and Callison-Burch, 2011). A natural approach
for trying to shore up the skills of weak bilinguals
is to pair them with a native speaker of the tar-
get language to edit their translations. We review
relevant research from NLP and human-computer
interaction (HCI) on collaborative translation pro-
cesses in Section 2. To sort good translations from
bad, researchers often solicit multiple, redundant
translations and then build models to try to predict
which translations are the best, or which transla-
tors tend to produce the highest quality transla-
tions.
The contributions of this paper are:
1134
? An analysis of the difficulties posed by a two-
step collaboration between editors and trans-
lators in Mechanical Turk-style crowdsourc-
ing environments. Editors vary in quality,
and poor editing can be difficult to detect.
? A new graph-based algorithm for selecting
the best translation among multiple transla-
tions of the same input. This method takes
into account the collaborative relationship
between the translators and the editors.
2 Related work
In the HCI community, several researchers have
proposed protocols for collaborative translation
efforts (Morita and Ishida, 2009b; Morita and
Ishida, 2009a; Hu, 2009; Hu et al, 2010). These
have focused on an iterative collaboration between
monolingual speakers of the two languages, facil-
itated with a machine translation system. These
studies are similar to ours in that they rely on na-
tive speakers? understanding of the target language
to correct the disfluencies in poor translations. In
our setup the poor translations are produced by
bilingual individuals who are weak in the target
language, and in their experiments the translations
are the output of a machine translation system.
1
Another significant difference is that the HCI
studies assume cooperative participants. For in-
stance, Hu et al (2010) recruited volunteers from
the International Children?s Digital Library (Hour-
cade et al, 2003) who were all well intentioned
and participated out a sense of altruism and to
build a good reputation among the other volunteer
translators at childrenslibrary.org. Our
setup uses anonymous crowd workers hired on
Mechanical Turk, whose motivation to participate
is financial. Bernstein et al (2010) characterized
the problems with hiring editors via MTurk for a
word processing application. Workers were either
lazy (meaning they made only minimal edits) or
overly zealous (meaning they made many unnec-
essary edits). Bernstein et al (2010) addressed
this problem with a three step find-fix-verify pro-
cess. In the first step, workers click on one word
or phrase that needed to be corrected. In the next
step, a separate group of workers proposed correc-
1
A variety of HCI and NLP studies have confirmed the
efficacy of monolingual or bilingual individuals post-editing
of machine translation output (Callison-Burch, 2005; Koehn,
2010; Green et al, 2013). Past NLP work has also examined
automatic post-editing(Knight and Chander, 1994).
tions to problematic regions that had been identi-
fied by multiple workers in the first pass. In the
final step, other workers would validate whether
the proposed corrections were good.
Most NLP research into crowdsourcing has fo-
cused on Mechanical Turk, following pioneering
work by Snow et al (2008) who showed that the
platform was a viable way of collecting data for a
wide variety of NLP tasks at low cost and in large
volumes. They further showed that non-expert an-
notations are similar to expert annotations when
many non-expert labelings for the same input
are aggregated, through simple voting or through
weighting votes based on how closely non-experts
matched experts on a small amount of calibra-
tion data. MTurk has subsequently been widely
adopted by the NLP community and used for an
extensive range of speech and language applica-
tions (Callison-Burch and Dredze, 2010).
Although hiring professional translators to cre-
ate bilingual training data for machine translation
systems has been deemed infeasible, Mechanical
Turk has provided a low cost way of creating large
volumes of translations (Callison-Burch, 2009;
Ambati and Vogel, 2010). For instance, Zbib et
al. (2012; Zbib et al (2013) translated 1.5 mil-
lion words of Levine Arabic and Egyptian Arabic,
and showed that a statistical translation system
trained on the dialect data outperformed a system
trained on 100 times more MSA data. Post et al
(2012) used MTurk to create parallel corpora for
six Indian languages for less than $0.01 per word.
MTurk workers translated more than half a million
words worth of Malayalam in less than a week.
Several researchers have examined the use of ac-
tive learning to further reduce the cost of transla-
tion (Ambati et al, 2010; Ambati, 2012; Blood-
good and Callison-Burch, 2010). Crowdsourcing
allowed real studies to be conducted whereas most
past active learning were simulated. Pavlick et al
(2014) conducted a large-scale demographic study
of the languages spoken by workers on MTurk by
translating 10,000 words in each of 100 languages.
Chen and Dolan (2012) examined the steps neces-
sary to build a persistent multilingual workforce
on MTurk.
This paper is most closely related to previous
work by Zaidan and Callison-Burch (2011), who
showed that non-professional translators could ap-
proach the level of professional translators. They
solicited multiple redundant translations from dif-
1135
Urdu translator:
According to the territory?s people the pamphlets from
the Taaliban had been read in the announcements in all
the mosques of the Northern Wazeerastan.
English post-editor:
According to locals, the pamphlet released by the Taliban
was read out on the loudspeakers of all the mosques in
North Waziristan.
LDC professional:
According to the local people, the Taliban?s pamphlet
was read over the loudspeakers of all mosques in North
Waziristan.
Table 1: Different versions of translations.
ferent Turkers for a collection of Urdu sentences
that had been previously professionally translated
by the Linguistics Data Consortium. They built a
model to try to predict on a sentence-by-sentence
and Turker-by-Turker which was the best transla-
tion or translator. They also hired US-based Turk-
ers to edit the translations, since the translators
were largely based in Pakistan and exhibited er-
rors that are characteristic of speakers of English
as a language. Zaidan and Callison-Burch (2011)
observed only modest improvements when incor-
porating these edited translation into their model.
We attempt to analyze why this is, and we pro-
posed a new model to try to better leverage their
data.
3 Crowdsourcing Translation
Setup We conduct our experiments using the
data collected by Zaidan and Callison-Burch
(2011). This data set consists 1,792 Urdu sen-
tences from a variety of news and online sources,
each paired with English translations provided by
non-professional translators on Mechanical Turk.
Each Urdu sentence was translated redundantly
by 3 distinct translators, and each translation was
edited by 3 separate (native English-speaking) ed-
itors to correct for grammatical and stylistic er-
rors. In total, this gives us 12 non-professional
English candidate sentences (3 unedited, 9 edited)
per original Urdu sentence. 52 different Turkers
took part in the translation task, each translating
138 sentences on average. In the editing task, 320
Turkers participated, averaging 56 sentences each.
For comparison, the data also includes 4 differ-
ent reference translations for each source sentence,
produced by professional translators.
Table 1 gives an example of an unedited trans-
lation, an edited translation, and a professional
translation for the same sentence. The transla-
tions provided by translators on MTurk are gen-
erally done conscientiously, preserving the mean-
ing of the source sentence, but typically con-
tain simple mistakes like misspellings, typos, and
awkward word choice. English-speaking editors,
despite having no knowledge of the source lan-
guage, are able to fix these errors. In this work,
we show that the collaboration design of two
heads? non-professional Urdu translators and non-
professional English editors? yields better trans-
lated output than would either one working in iso-
lation, and can better approximate the quality of
professional translators.
Analysis We know from inspection that trans-
lations seem to improve with editing (Table 1).
Given the data from MTurk, we explore whether
this is the case in general: Do all translations im-
prove with editing? To what extent does the in-
dividual translator and the individual editor effect
the quality of the final sentence?
Figure 1: Relationship between editor aggressive-
ness and effectiveness. Each point represents an
editor/translation pair. Aggressiveness (x-axis) is
measured as the TER between the pre-edit and
post-edit version of the translation, and effective-
ness (y-axis) is measured as the average amount
by which the editing reduces the translation?s
TER
gold
. While many editors make only a few
changes, those who make many changes can bring
the translation substantially closer to professional
quality.
We use translation edit rate (TER) as a mea-
sure of translation similarity. TER represents the
amount of change necessary to transform one sen-
tence into another, so a low TER means the two
1136
0.020.05
0.07
? T
ER g
old
Editor ? TERgold 0.03  0.50
0.00
0.01
0.02
? T
ER g
old
Editor ? TERgold 0.01  0.03
-0.03
-0.01
0.01
? T
ER g
old
Editor ? TERgold -0.01  0.01
-0.08
-0.04
-0.00
? T
ER g
old
Editor ? TERgold -0.03  -0.01
0.3  0.9 0.2  0.3 0.2  0.2 0.1  0.2 0.0  0.1Translation TERgold
-0.30
-0.15
-0.01
? T
ER g
old
Editor ? TERgold -0.64  -0.03
Figure 2: Effect of editing on translations of vary-
ing quality. Rows reflect bins of editors, with the
worse editors (those whose changes result in in-
creased TER
gold
) on the top and the most effective
editors (those whose changes result in the largest
reduction in TER
gold
) on the bottom. Bars re-
flect bins of translations, with the highest TER
gold
translations on the left, and the lowest on the
right. We can see from the consistently negative
? TER
gold
in the bottom row that good editors are
able to improve both good and bad translations.
sentences are very similar. To capture the quality
(?professionalness?) of a translation, we take the
average TER of the translation against each of our
gold translations. That is, we define TER
gold
of
translation t as
TER
gold
=
1
4
4
?
i=1
TER(gold
i
, t) (1)
where a lower TER
gold
is indicative of a higher
quality (more professional-sounding) translation.
We first look at editors along two dimensions:
their aggressiveness and their effectiveness. Some
editors may be very aggressive (they make many
changes to the original translation) but still be in-
effective (they fail to bring the quality of the trans-
lation closer to that of a professional). We measure
aggressiveness by looking at the TER between
the pre- and post-edited versions of each editor?s
translations; higher TER implies more aggressive
editing. To measure effectiveness, we look at the
change in TER
gold
that results from the editing;
negative ?TER
gold
means the editor effectively
improved the quality of the translation, while pos-
itive ?TER
gold
means the editing actually brought
the translation further from our gold standard.
Figure 1 shows the relationship between these
two qualities for individual editor/translation
pairs. We see that while most translations re-
quire only a few edits, there are a large number
of translations which improve substantially after
heavy editing. This trend conforms to our intu-
ition that editing is most useful when the transla-
tion has much room for improvement, and opens
the question of whether good editors can offer im-
provements to translations of all qualities.
To address this question, we split our transla-
tions into 5 bins, based on their TER
gold
. We also
split our editors into 5 bins, based on their effec-
tiveness (i.e. the average amount by which their
editing reduces TER
gold
). Figure 2 shows the de-
gree to which editors at each level are able to im-
prove the translations from each bin. We see that
good editors are able to make improvements to
translations of all qualities, but that good editing
has the greatest impact on lower quality transla-
tions. This result suggests that finding good ed-
itor/translator pairs, rather than good editors and
good translators in isolation, should produce the
best translations overall. Figure 3 gives an exam-
ple of how an initially medium-quality translation,
when combined with good editing, produces a bet-
ter result than the higher-quality translation paired
with mediocre editing.
4 Problem Formulation
The problem definition of the crowdsourcing
translation task is straightforward: given a set of
candidate translations for a source sentence, we
want to choose the best output translation.
This output translation is the result of the com-
bined translation and editing stages. Therefore,
our method operates over a heterogeneous net-
work that includes translators and post-editors as
well as the translated sentences that they pro-
duce. We frame the problem as follows. We form
two graphs: the first graph (G
T
) represents Turk-
ers (translator/editor pairs) as nodes; the second
graph (G
C
) represents candidate translated and
1137
Figure 3: Three alternative translations (left) and the edited versions of each (right). Each edit on the
right was produced by a different editor. Order reflects the TER
gold
of each translation, with the lowest
TER
gold
on the top. Some translators receive low TER
gold
scores due to superficial errors, which can be
easily improved through editing. In the above example, the middle-ranked translation (green) becomes
the best translation after being revised by a good editor.
post-edited sentences (henceforth ?candidates?) as
nodes. These two graphs, G
T
and G
C
are com-
bined as subgraphs of a third graph (G
TC
). Edges
in G
TC
connect author pairs (nodes in G
T
) to the
candidate that they produced (nodes in G
C
). To-
gether, G
T
, G
C
, and G
TC
define a co-ranking
problem (Yan et al, 2012a; Yan et al, 2011b; Yan
et al, 2012b) with linkage establishment (Yan et
al., 2011a; Yan et al, 2012c), which we define for-
mally as follows.
Let G denote the heterogeneous graph with
nodes V and edges E. Let G = (V ,E) =
(V
T
, V
C
, E
T
, E
C
, E
TC
). G is divided into three
subgraphs, G
T
, G
C
, and G
TC
. G
C
= (V
C
, E
C
) is
a weighted undirected graph representing the can-
didates and their lexical relationships to one an-
other. Let V
C
denote a collection of translated
and edited candidates, and E
C
the lexical simi-
larity between the candidates (see Section 4.3 for
details). G
T
= (V
T
, E
T
) is a weighted undirected
graph representing collaborations between Turk-
ers. V
T
is the set of translator/editor pairs. Edges
E
T
connect translator/editor pairs in V
T
which
share a translator and/or editor. Each collabora-
tion (i.e. each node in V
T
) produces a candidate
(i.e. a node in V
C
). G
TC
= (V
TC
, E
TC
) is an
unweighted bipartite graph that ties G
T
and G
C
together and represents ?authorship?. The graph
G consists of nodes V
TC
= V
T
? V
C
and edges
E
TC
connecting each candidate with its authoring
translator/post-editor pair. The three sub-networks
(G
T
, G
C
, and G
TC
) are illustrated in Figure 4.
4.1 Inter-Graph Ranking
The framework includes three random walks, one
on G
T
, one on G
C
and one on G
TC
. A random
walk on a graph is a Markov chain, its states be-
ing the vertices of the graph. It can be described
by a stochastic square matrix, where the dimen-
sion is the number of vertices in the graph, and the
entries describe the transition probabilities from
one vertex to the next. The mutual reinforcement
framework couples the two random walks on G
T
and G
C
that rank candidates and Turkers in iso-
lation. The ranking method allows us to obtain
a global ranking by taking into account the intra-
/inter-component dependencies. In the following
sections, we describe how we obtain the rankings
on G
T
and G
C
, and then move on to discuss how
the two are coupled.
Our algorithm aims to capture the following in-
tuitions. A candidate is important if 1) it is similar
to many of the other proposed candidates and 2)
it is authored by better qualified translators and/or
post-editors. Analogously, a translator/editor pair
is believed to be better qualified if 1) the editor
is collaborating with a good translator and vice
versa and 2) the pair has authored important candi-
dates. This ranking schema is actually a reinforced
process across the heterogeneous graphs. We use
two vectors c = [pi(c)]
|c|?1
and t = [pi(t)]
|t|?1
to
denote the saliency scores pi(.) of candidates and
Turker pairs. The above-mentioned intuitions can
be formulated as follows:
? Homogeneity. We use adjacency matrix
1138
Figure 4: 2-step collaborative crowdsourcing translation model based on graph ranking framework in-
cluding three sub-networks. The undirected links between users denotes translation-editing collabora-
tion. The undirected links between candidate translations indicate lexical similarity between candidates.
A bipartite graph ties candidate and Turker networks together by authorship (to make the figure clearer,
some linkage is omitted). A dashed circle indicates the group of candidate translations for a single source
sentence to translate.
[M ]
|c|?|c|
to describe the homogeneous affinity
between candidates and [N ]
|t|?|t|
to describe the
affinity between Turkers.
c ?M
T
c, t ? N
T
t (2)
where c = |V
C
| is the number of vertices in the
candidate graph and t = |V
T
| is the number of ver-
tices in the Turker graph. The adjacency matrix
[M ] denotes the transition probabilities between
candidates, and analogously matrix [N ] denotes
the affinity between Turker collaboration pairs.
? Heterogeneity. We use an adjacency matrix
[
?
W ]
|c|?|t|
and [
?
W ]
|t|?|c|
to describe the authorship
between the output candidate and the producer
Turker pair from both of the candidate-to-Turker
and Turker-to-candidate perspectives.
c ?
?
W
T
t, t ?
?
W
T
c (3)
All affinity matrices will be defined in the next
section. By fusing the above equations, we can
have the following iterative calculation in matrix
forms. For numerical computation of the saliency
scores, the initial scores of all sentences and Turk-
ers are set to 1 and the following two steps are
alternated until convergence to select the best can-
didate.
Step 1: compute the saliency scores of candi-
dates, and then normalize using `-1 norm.
c
(n)
= (1? ?)M
T
c
(n?1)
+ ?
?
W t
(n?1)
c
(n)
= c
(n)
/||c
(n)
||
1
(4)
Step 2: compute the saliency scores of Turker
pairs, and then normalize using `-1 norm.
t
(n)
= (1? ?)N
T
t
(n?1)
+ ?
?
W c
(n?1)
t
(n)
= t
(n)
/||t
(n)
||
1
(5)
where ? specifies the relative contributions to the
saliency score trade-off between the homogeneous
affinity and the heterogeneous affinity. In order
to guarantee the convergence of the iterative form,
we must force the transition matrix to be stochastic
and irreducible. To this end, we must make the
c and t column stochastic (Langville and Meyer,
2004). c and t are therefore normalized after each
iteration of Equation (4) and (5).
4.2 Intra-Graph Ranking
The standard PageRank algorithm starts from an
arbitrary node and randomly selects to either fol-
low a random out-going edge (considering the
weighted transition matrix) or to jump to a random
node (treating all nodes with equal probability).
1139
In a simple random walk, it is assumed that all
nodes in the transitional matrix are equi-probable
before the walk starts. Then c and t are calculated
as:
c = ?M
T
c + (1? ?)
1
|V
C
|
(6)
and
t = ?N
T
t + (1? ?)
1
|V
T
|
(7)
where 1 is a vector with all elements equaling to 1
and the size is correspondent to the size of V
C
or
V
T
. ? is the damping factor usually set to 0.85, as
in the PageRank algorithm.
4.3 Affinity Matrix Establishment
We introduce the affinity matrix calculation, in-
cluding homogeneous affinity (i.e., M,N ) and
heterogeneous affinity (i.e.,
?
W,
?
W ).
As discussed, we model the collection of can-
didates as a weighted undirected graph, G
C
, in
which nodes in the graph represent candidate sen-
tences and edges represent lexical relatedness. We
define an edge?s weight to be the cosine similarity
between the candidates represented by the nodes
that it connects. The adjacency matrix M describes
such a graph, with each entry corresponding to the
weight of an edge.
F(c
i
, c
j
) =
c
i
? c
j
||c
i
||||c
j
||
M
ij
=
F(c
i
, c
j
)
?
k
F(c
i
, c
k
)
(8)
where F(.) is the cosine similarity and c is a term
vector corresponding to a candidate. We treat a
candidate as a short document and weight each
term with tf.idf (Manning et al, 2008), where tf
is the term frequency and idf is the inverse docu-
ment frequency.
The Turker graph, G
T
, is an undirected graph
whose edges represent ?collaboration.? Formally,
let t
i
and t
j
be two translator/editor pairs; we say
that pair t
i
?collaborates with? pair t
j
(and there-
fore, there is an edge between t
i
and t
j
) if t
i
and
t
j
share either a translator or an editor (or share
both a translator and an editor). Let the function
I(t
i
, t
j
) denote the number of ?collaborations?
(#col) between t
i
and t
j
.
I(t
i
, t
j
) =
{
#col (e
ij
? E
T
)
0 otherwise
, (9)
Then the adjacency matrix N is then defined as
N
ij
=
I(t
i
, t
j
)
?
k
I(t
i
, t
k
)
(10)
In the bipartite candidate-Turker graph G
TC
,
the entry E
TC
(i, j) is an indicator function denot-
ing whether the candidate c
i
is generated by t
j
:
A(c
i
, t
j
) =
{
1 (e
ij
? E
TC
)
0 otherwise
(11)
Through E
TC
we define the weight matrices
?
W
ij
and
?
W
ij
, containing the conditional probabil-
ities of transitions from c
i
to t
j
and vice versa:
?
W
ij
=
A(c
i
, t
j
)
?
k
A(c
i
, t
k
)
,
?
W
ij
=
A(c
i
, t
j
)
?
k
A(c
k
, t
j
)
(12)
5 Evaluation
We are interested in testing our random walk
method, which incorporates information from
both the candidate translations and from the Turk-
ers. We want to test two versions of our pro-
posed collaborative co-ranking method: 1) based
on the unedited translations only and 2) based on
the edited sentences after translator/editor collab-
orations.
Metric Since we have four professional transla-
tion sets, we can calculate the Bilingual Evalu-
ation Understudy (BLEU) score (Papineni et al,
2002) for one professional translator (P1) using
the other three (P2,3,4) as a reference set. We
repeat the process four times, scoring each pro-
fessional translator against the others, to calculate
the expected range of professional quality transla-
tion. In the following sections, we evaluate each of
our methods by calculating BLEU scores against
the same four sets of three reference translations.
Therefore, each number reported in our experi-
mental results is an average of four numbers, cor-
responding to the four possible ways of choosing 3
of the 4 reference sets. This allows us to compare
the BLEU score achieved by our methods against
the BLEU scores achievable by professional trans-
lators.
Baselines As a naive baseline, we choose one
candidate translation at random for each input
Urdu sentence. To establish an upper bound for
our methods, and to determine if there exist high-
quality Turker translations at all, we compute four
1140
Reference (Avg.) 42.51
Oracle (Seg-Trans) 44.93
Oracle (Seg-Trans+Edit) 48.44
Oracle (Turker-Trans) 38.66
Oracle (Turker-Trans+Edit) 39.16
Random 30.52
Lowest TER 35.78
Graph Ranking (Trans) 38.88
Graph Ranking (Trans+Edit) 41.43
Table 2: Overall BLEU performance for all
methods (with and without post-editing). The
highlighted result indicates the best performance,
which is based on both candidate sentences and
Turker information.
oracle scores. The first oracle operates at the seg-
ment level on the sentences produced by transla-
tors only: for each source segment, we choose
from the translations the one that scores highest
(in terms of BLEU) against the reference sen-
tences. The second oracle is applied similarly,
but chooses from the candidates produced by the
collaboration of translator/post-editor pairs. The
third oracle operates at the worker level: for each
source segment, we choose from the translations
the one provided by the worker whose transla-
tions (over all sentences) score the highest on
average. The fourth oracle also operates at the
worker level, but selects from sentences produced
by translator/post-editor collaborations. These or-
acle methods represent ideal solutions under our
scenario. We also examine two voting-inspired
methods. The first method selects the translation
with the minimum average TER (Snover et al,
2006) against the other translations; intuitively,
this would represent the ?consensus? translation.
The second method selects the translation gen-
erated by the Turker who, on average, provides
translations with the minimum average TER.
Results A summary of our results in given in Ta-
ble 2. As expected, random selection yields bad
performance, with a BLEU score of 30.52. The
oracles indicate that there is usually an acceptable
translation from the Turkers for any given sen-
tence. Since the oracles select from a small group
of only 4 translations per source segment, they are
not overly optimistic, and rather reflect the true po-
tential of the collected translations. On average,
the reference translations give a score of 42.38. To
put this in perspective, the output of a state-of-the-
Figure 5: Effect of candidate-Turker coupling (?)
on BLEU score.
art machine translation system (the syntax-based
variant of Joshua) achieves a score of 26.91, which
is reported in (Zaidan and Callison-Burch, 2011).
The approach which selects the translations with
the minimum average TER (Snover et al, 2006)
against the other three translations (the ?consen-
sus? translation) achieves BLEU scores of 35.78.
Using the raw translations without post-editing,
our graph-based ranking method achieves a BLEU
score of 38.89, compared to Zaidan and Callison-
Burch (2011)? s reported score of 28.13, which
they achieved using a linear feature-based classi-
fication. Their linear classifier achieved a reported
score of 39.06
2
when combining information from
both translators and editors. In contrast, our pro-
posed graph-based ranking framework achieves a
score of 41.43 when using the same information.
This boost in BLEU score confirms our intuition
that the hidden collaboration networks between
candidate translations and transltor/editor pairs are
indeed useful.
Parameter Tuning There are two parameters in
our experimental setups: ? controls the probability
of starting a new random walk and ? controls the
coupling between the candidate and Turker sub-
graphs. We set the damping factor ? to 0.85, fol-
lowing the standard PageRank paradigm. In order
to determine a value for ?, we used the average
BLEU, computed against the professional refer-
2
Note that the data we used in our experiments are slightly
different, by discarding nearly 100 NULL sentences in the
raw data. We do not re-implement this baseline but report the
results from the paper directly. According to our experiments,
most of the results generated by baselines and oracles are very
close to the previously reported values.
1141
Plain ranking 38.89
w/o collaboration 38.88
Shared translator 41.38
Shared post-editor 41.29
Shared Turker 41.43
Table 3: Variations of all component settings.
ence translations, as a tuning metric. We experi-
mented with values of ? ranging from 0 to 1, with
a step size of 0.05 (Figure 5). Small ? values place
little emphasis on the candidate/Turker coupling,
whereas larger values rely more heavily on the co-
ranking. Overall, we observed better performance
with values within the range of 0.05-0.15. This
suggests that both sources of information? the can-
didate itself and its authors? are important for the
crowdsourcing translation task. In all of our re-
ported results, we used the ? = 0.1.
Analysis We examine the relative contribution
of each component of our approach on the overall
performance. We first examine the centroid-based
ranking on the candidate sub-graph (G
C
) alone
to see the effect of voting among translated sen-
tences; we denote this strategy as plain ranking.
Then we incorporate the standard random walk on
the Turker graph (G
T
) to include the structural in-
formation but without yet including any collabo-
ration information; that is, we incorporate infor-
mation from G
T
and G
C
without including edges
linking the two together. The co-ranking paradigm
is exactly the same as the framework described in
Section 3.2, but with simplified structures.
Finally, we examine the two-step collaboration
based candidate-Turker graph using several varia-
tions on edge establishment. As before, the nodes
are the translator/post-editor working pairs. We
investigate three settings in which 1) edges con-
nect two nodes when they share only a transla-
tor, 2) edges connect two nodes when they share
only a post-editor, and 3) edges connect two nodes
when they share either a translator or a post-editor.
These results are summarized in Table 3.
Interestingly, we observe that when modeling
the linkage between the collaboration pairs, con-
necting Turker pairs which share either a transla-
tor or the post-editor achieves better performance
than connecting pairs that share only translators or
connecting pairs which share only editors. This
result supports the intuition that a denser collabo-
ration matrix will help propagate saliency to good
translators/post-editors and hence provides better
predictions for candidate quality.
6 Conclusion
We have proposed an algorithm for using a two-
step collaboration between non-professional trans-
lators and post-editors to obtain professional-
quality translations. Our method, based on a
co-ranking model, selects the best crowdsourced
translation from a set of candidates, and is capable
of selecting translations which near professional
quality.
Crowdsourcing can play a pivotal role in fu-
ture efforts to create parallel translation datasets.
In addition to its benefits of cost and scalabil-
ity, crowdsourcing provides access to languages
that currently fall outside the scope of statistical
machine translation research. In future work on
crowdsourced translation, further benefits in qual-
ity improvement and cost reduction could stem
from 1) building ground truth data sets based on
high-quality Turkers? translations and 2) identify-
ing when sufficient data has been collected for a
given input, to avoid soliciting unnecessary redun-
dant translations.
Acknowledgements
This material is based on research sponsored by
a DARPA Computer Science Study Panel phase 3
award entitled ?Crowdsourcing Translation? (con-
tract D12PC00368). The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as represent-
ing official policies or endorsements by DARPA
or the U.S. Government. This research was sup-
ported by the Johns Hopkins University Human
Language Technology Center of Excellence and
through gifts from Microsoft, Google and Face-
book.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages
16?23, March.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation sys-
tems? In Workshop on Creating Speech and Lan-
guage Data with MTurk.
1142
Vamshi Ambati, Stephan Vogel, and Jaime G Car-
bonell. 2010. Active learning and crowd-sourcing
for machine translation. In LREC, volume 11, pages
2169?2174. Citeseer.
Vamshi Ambati. 2012. Active Learning and Crowd-
sourcing for Machine Translation in Low Resource
Scenarios. Ph.D. thesis, Language Technologies In-
stitute, School of Computer Science, Carnegie Mel-
lon University, Pittsburgh, PA.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjrn Hartmann, Mark S. Ackerman, David R.
Karger, David Crowell, and Katrina Panovich.
2010. Soylent: a word processor with a crowd in-
side. In Proceedings of the ACM Symposium on
User Interface Software and Technology (UIST).
Michael Bloodgood and Chris Callison-Burch. 2010.
Large-scale cost-focused active learning for statisti-
cal machine translation. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with Amazon?s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 1?12,
June.
Chris Callison-Burch. 2005. Linear B system de-
scription for the 2005 NIST MT evaluation exercise.
In Proceedings of Machine Translation Evaluation
Workshop.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazon?s me-
chanical turk. In Proceedings of EMNLP.
David L. Chen and William B. Dolan. 2012. Build-
ing a persistent workforce on mechanical turk for
multilingual data collection. In Proceedings of the
Human Computer Interaction International Confer-
ence.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang
for the buck can we expect? In Proceedings of
the Workshop on Data-driven Methods in Machine
Translation - Volume 14, DMMT ?01, pages 1?8.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, CHI ?13, pages 439?448.
Juan Pablo Hourcade, Benjamin B Bederson, Allison
Druin, Anne Rose, Allison Farber, and Yoshifumi
Takayama. 2003. The international children?s digi-
tal library: viewing digital books online. Interacting
with Computers, 15(2):151?167.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration be-
tween monolingual users. In Proceedings of
ACM SIGKDD Workshop on Human Computation
(HCOMP).
Chang Hu, Benjamin B. Bederson, Philip Resnik, and
Yakov Kronrod. 2011. Monotrans2: A new human
computation system to support monolingual trans-
lation. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, CHI ?11,
pages 1133?1136.
Chang Hu. 2009. Collaborative translation by mono-
lingual users. In CHI ?09 Extended Abstracts on Hu-
man Factors in Computing Systems, CHI EA ?09,
pages 3105?3108.
Martin Kay. 1998. The proper place of men and ma-
chines in language translation. Machine Transla-
tion, 12(1/2):3?23, January.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In In Proceedings of
AAAI.
Philipp Koehn. 2010. Enabling monolingual transla-
tors: Post-editing vs. options. In HLT-NAACL?10,
pages 537?545, June.
Amy N Langville and Carl D Meyer. 2004. Deeper
inside pagerank. Internet Mathematics, 1(3):335?
380.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 133?137, July.
Annie Louis and Ani Nenkova. 2013. What makes
writing great? first experiments on article quality
prediction in the science journalism domain. Trans-
actions of Association for Computational Linguis-
tics.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Daisuke Morita and Toru Ishida. 2009a. Collaborative
translation by monolinguals with machine transla-
tors. In Proceedings of the 14th International Con-
ference on Intelligent User Interfaces, IUI ?09, pages
361?366.
Daisuke Morita and Toru Ishida. 2009b. Designing
protocols for collaborative translation. In Interna-
tional Conference on Principles of Practice in Multi-
Agent Systems (PRIMA-09), pages 17?32. Springer.
1143
Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Comput. Linguist.,
31(4):477?504, December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318.
Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev,
and Chris Callison-Burch. 2014. The language de-
mographics of Amazon Mechanical Turk. Transac-
tions of the Association for Computational Linguis-
tics (TACL), 2(Feb):79?92.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six Indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380, September.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
HLT-NAACL?10, pages 403?411.
Jason Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt cheap web-scale paral-
lel text from the Common Crawl. In Proceedings of
the 2013 Conference of the Association for Compu-
tational Linguistics (ACL 2013), July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223?231.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for nat-
ural language tasks. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 254?263.
TechCrunch. 2008. Facebook taps users to create
translated versions of site, January.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 1101?
1109.
Luis von Ahn. 2013. Duolingo: Learn a language for
free while helping to translate the web. In Proceed-
ings of the 2013 International Conference on Intel-
ligent User Interfaces, IUI ?13, pages 1?2.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 433?443.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolutionary
timeline summarization: A balanced optimization
framework via iterative substitution. In Proceed-
ings of the 34th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?11, pages 745?754.
Rui Yan, Mirella Lapata, and Xiaoming Li. 2012a.
Tweet recommendation with graph co-ranking. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 516?525.
Rui Yan, Xiaojun Wan, Mirella Lapata, Wayne Xin
Zhao, Pu-Jen Cheng, and Xiaoming Li. 2012b.
Visualizing timelines: Evolutionary summarization
via iterative reinforcement between text and image
streams. In Proceedings of the 21st ACM Interna-
tional Conference on Information and Knowledge
Management, CIKM ?12, pages 275?284.
Rui Yan, Zi Yuan, Xiaojun Wan, Yan Zhang, and Xi-
aoming Li. 2012c. Hierarchical graph summariza-
tion: leveraging hybrid information through visible
and invisible linkage. In PAKDD?12, pages 97?108.
Springer.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1220?1229.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine translation of Arabic di-
alects. In The 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, and John Makhoul. 2013.
Systematic comparison of professional and crowd-
sourced reference translations for machine transla-
tion. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia.
Xin Wayne Zhao, Yanwei Guo, Rui Yan, Yulan He,
and Xiaoming Li. 2013. Timeline generation with
social attention. In Proceedings of the 36th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?13,
pages 1061?1064.
1144
Transactions of the Association for Computational Linguistics, 1 (2013) 165?178. Action Editor: David Chiang.
Submitted 11/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Learning to translate with products of novices: a suite of open-ended
challenge problems for teaching MT
Adam Lopez1, Matt Post1, Chris Callison-Burch1,2, Jonathan Weese, Juri Ganitkevitch,
Narges Ahmidi, Olivia Buzek, Leah Hanson, Beenish Jamil, Matthias Lee, Ya-Ting Lin,
Henry Pao, Fatima Rivera, Leili Shahriyari, Debu Sinha, Adam Teichert,
Stephen Wampler, Michael Weinberger, Daguang Xu, Lin Yang, and Shang Zhao?
Department of Computer Science, Johns Hopkins University
1Human Language Technology Center of Excellence, Johns Hopkins University
2Computer and Information Science Department, University of Pennsylvania
Abstract
Machine translation (MT) draws from several
different disciplines, making it a complex sub-
ject to teach. There are excellent pedagogical
texts, but problems in MT and current algo-
rithms for solving them are best learned by
doing. As a centerpiece of our MT course,
we devised a series of open-ended challenges
for students in which the goal was to im-
prove performance on carefully constrained
instances of four key MT tasks: alignment,
decoding, evaluation, and reranking. Students
brought a diverse set of techniques to the prob-
lems, including some novel solutions which
performed remarkably well. A surprising and
exciting outcome was that student solutions
or their combinations fared competitively on
some tasks, demonstrating that even newcom-
ers to the field can help improve the state-of-
the-art on hard NLP problems while simulta-
neously learning a great deal. The problems,
baseline code, and results are freely available.
1 Introduction
A decade ago, students interested in natural lan-
guage processing arrived at universities having been
exposed to the idea of machine translation (MT)
primarily through science fiction. Today, incoming
students have been exposed to services like Google
Translate since they were in secondary school or ear-
lier. For them, MT is science fact. So it makes sense
to teach statistical MT, either on its own or as a unit
? The first five authors were instructors and the remaining au-
thors were students in the worked described here. This research
was conducted while Chris Callison-Burch was at Johns Hop-
kins University.
in a class on natural language processing (NLP), ma-
chine learning (ML), or artificial intelligence (AI). A
course that promises to show students how Google
Translate works and teach them how to build some-
thing like it is especially appealing, and several uni-
versities and summer schools now offer such classes.
There are excellent introductory texts?depending
on the level of detail required, instructors can choose
from a comprehensive MT textbook (Koehn, 2010),
a chapter of a popular NLP textbook (Jurafsky and
Martin, 2009), a tutorial survey (Lopez, 2008), or
an intuitive tutorial on the IBM Models (Knight,
1999b), among many others.
But MT is not just an object of academic study.
It?s a real application that isn?t fully perfected, and
the best way to learn about it is to build an MT sys-
tem. This can be done with open-source toolkits
such as Moses (Koehn et al, 2007), cdec (Dyer et
al., 2010), or Joshua (Ganitkevitch et al, 2012), but
these systems are not designed for pedagogy. They
are mature codebases featuring tens of thousands of
source code lines, making it difficult to focus on
their core algorithms. Most tutorials present them
as black boxes. But our goal is for students to learn
the key techniques in MT, and ideally to learn by
doing. Black boxes are incompatible with this goal.
We solve this dilemma by presenting students
with concise, fully-functioning, self-contained com-
ponents of a statistical MT system: word alignment,
decoding, evaluation, and reranking. Each imple-
mentation consists of a na??ve baseline algorithm in
less than 150 lines of Python code. We assign them
to students as open-ended challenges in which the
goal is to improve performance on objective eval-
uation metrics as much as possible. This setting
mirrors evaluations conducted by the NLP research
165
community and by the engineering teams behind
high-profile NLP projects such as Google Translate
and IBM?s Watson. While we designate specific al-
gorithms as benchmarks for each task, we encour-
age creativity by awarding more points for the best
systems. As additional incentive, we provide a web-
based leaderboard to display standings in real time.
In our graduate class on MT, students took a va-
riety of different approaches to the tasks, in some
cases devising novel algorithms. A more exciting re-
sult is that some student systems or combinations of
systems rivaled the state of the art on some datasets.
2 Designing MT Challenge Problems
Our goal was for students to freely experiment with
different ways of solving MT problems on real data,
and our approach consisted of two separable com-
ponents. First, we provided a framework that strips
key MT problems down to their essence so students
could focus on understanding classic algorithms or
invent new ones. Second, we designed incentives
that motivated them to improve their solutions as
much as possible, encouraging experimentation with
approaches beyond what we taught in class.
2.1 Decoding, Reranking, Evaluation, and
Alignment for MT (DREAMT)
We designed four assignments, each corresponding
to a real subproblem in MT: alignment, decoding,
evaluation, and reranking.1 From the more general
perspective of AI, they emphasize the key problems
of unsupervised learning, search, evaluation design,
and supervised learning, respectively. In real MT
systems, these problems are highly interdependent,
a point we emphasized in class and at the end of each
assignment?for example, that alignment is an exer-
cise in parameter estimation for translation models,
that model choice is a tradeoff between expressivity
and efficient inference, and that optimal search does
not guarantee optimal accuracy. However, present-
ing each problem independently and holding all else
constant enables more focused exploration.
For each problem we provided data, a na??ve solu-
tion, and an evaluation program. Following Bird et
al. (2008) and Madnani and Dorr (2008), we imple-
mented the challenges in Python, a high-level pro-
1http://alopez.github.io/dreamt
gramming language that can be used to write very
concise programs resembling pseudocode.2,3 By de-
fault, each baseline system reads the test data and
generates output in the evaluation format, so setup
required zero configuration, and students could be-
gin experimenting immediately. For example, on re-
ceipt of the alignment code, aligning data and eval-
uating results required only typing:
> align | grade
Students could then run experiments within minutes
of beginning the assignment.
Three of the four challenges also included unla-
beled test data (except the decoding assignment, as
explained in ?4). We evaluated test results against a
hidden key when assignments were submitted.
2.2 Incentive Design
We wanted to balance several pedagogical goals: un-
derstanding of classic algorithms, free exploration
of alternatives, experience with typical experimental
design, and unhindered collaboration.
Machine translation is far from solved, so we ex-
pected more than reimplementation of prescribed al-
gorithms; we wanted students to really explore the
problems. To motivate exploration, we made the as-
signments competitive. Competition is a powerful
force, but must be applied with care in an educa-
tional setting.4 We did not want the consequences
of ambitious but failed experiments to be too dire,
and we did not want to discourage collaboration.
For each assignment, we guaranteed a passing
grade for matching the performance of a specific tar-
get alorithm. Typically, the target was important
but not state-of-the-art: we left substantial room for
improvement, and thus competition. We told stu-
dents the exact algorithm that produced the target ac-
curacy (though we expected them to derive it them-
selves based on lectures, notes, or literature). We
did not specifically require them to implement it, but
the guarantee of a passing grade provided a power-
ful incentive for this to be the first step of each as-
signment. Submissions that beat this target received
additional credit. The top five submissions received
full credit, while the top three received extra credit.
2http://python.org
3Some well-known MT systems have been implemented in
Python (Chiang, 2007; Huang and Chiang, 2007).
4Thanks to an anonymous reviewer for this turn of phrase.
166
This scheme provided strong incentive to continue
experimentation beyond the target alorithm.5
For each assignment, students could form teams
of any size, under three rules: each team had to pub-
licize its formation to the class, all team members
agreed to receive the same grade, and teams could
not drop members. Our hope was that these require-
ments would balance the perceived competitive ad-
vantage of collaboration against a reluctance to take
(and thus support) teammates who did not contribute
to the competitive effort.6 This strategy worked: out
of sixteen students, ten opted to work collaboratively
on at least one assignment, always in pairs.
We provided a web-based leaderboard that dis-
played standings on the test data in real time, iden-
tifying each submission by a pseudonymous han-
dle known only to the team and instructors. Teams
could upload solutions as often as they liked before
the assignment deadline. The leaderboard displayed
scores of the default and target alorithms. This in-
centivized an early start, since teams could verify
for themselves when they met the threshold for a
passing grade. Though effective, it also detracted
from realism in one important way: it enabled hill-
climbing on the evaluation metric. In early assign-
ments, we observed a few cases of this behavior,
so for the remaining assignments, we modified the
leaderboard so that changes in score would only be
reflected once every twelve hours. This strategy
trades some amount of scientific realism for some
measure of incentive, a strategy that has proven
effective in other pedagogical tools with real-time
feedback (Spacco et al, 2006).
To obtain a grade, teams were required to sub-
mit their results, share their code privately with the
instructors, and publicly describe their experimen-
tal process to the class so that everyone could learn
from their collective effort. Teams were free (but not
required) to share their code publicly at any time.
5Grades depend on institutional norms. In our case, high grades
in the rest of class combined with matching all assignment tar-
get alorithms would earn a B+; beating two target alorithms
would earn an A-; top five placement on any assignment would
earn an A; and top three placement compensated for weaker
grades in other course criteria. Everyone who completed all
four assignments placed in the top five at least once.
6The equilibrium point is a single team, though this team would
still need to decide on a division of labor. One student contem-
plated organizing this team, but decided against it.
Some did so after the assignment deadline.
3 The Alignment Challenge
The first challenge was word alignment: given a par-
allel text, students were challenged to produce word-
to-word alignments with low alignment error rate
(AER; Och and Ney, 2000). This is a variant of a
classic assignment not just in MT, but in NLP gen-
erally. Klein (2005) describes a version of it, and we
know several other instructors who use it.7 In most
of these, the object is to implement IBM Model 1
or 2, or a hidden Markov model. Our version makes
it open-ended by asking students to match or beat an
IBM Model 1 baseline.
3.1 Data
We provided 100,000 sentences of parallel data from
the Canadian Hansards, totaling around two million
words.8 This dataset is small enough to align in
a few minutes with our implementation?enabling
rapid experimentation?yet large enough to obtain
reasonable results. In fact, Liang et al (2006) report
alignment accuracy on data of this size that is within
a fraction of a point of their accuracy on the com-
plete Hansards data. To evaluate, we used manual
alignments of a small fraction of sentences, devel-
oped by Och and Ney (2000), which we obtained
from the shared task resources organized by Mihal-
cea and Pedersen (2003). The first 37 sentences
of the corpus were development data, with manual
alignments provided in a separate file. Test data con-
sisted of an additional 447 sentences, for which we
did not provide alignments.9
3.2 Implementation
We distributed three Python programs with the
data. The first, align, computes Dice?s coefficient
(1945) for every pair of French and English words,
then aligns every pair for which its value is above an
adjustable threshold. Our implementation (most of
7Among them, Jordan Boyd-Graber, John DeNero, Philipp
Koehn, and Slav Petrov (personal communication).
8http://www.isi.edu/natural-language/download/hansard/
9This invited the possibility of cheating, since alignments of the
test data are publicly available on the web. We did not adver-
tise this, but as an added safeguard we obfuscated the data by
distributing the test sentences randomly throughout the file.
167
Listing 1 The default aligner in DREAMT: thresh-
olding Dice?s coefficient.
for (f, e) in bitext:
for f_i in set(f):
f_count[f_i] += 1
for e_j in set(e):
fe_count[(f_i,e_j)] += 1
for e_j in set(e):
e_count[e_j] += 1
for (f_i, e_j) in fe_count.keys():
dice[(f_i,e_j)] = \
2.0 * fe_count[(f_i, e_j)] / \
(f_count[f_i] + e_count[e_j])
for (f, e) in bitext:
for (i, f_i) in enumerate(f):
for (j, e_j) in enumerate(e):
if dice[(f_i,e_j)] >= cutoff:
print "%i-%i " % (i,j)
which is shown in Listing 1) is quite close to pseu-
docode, making it easy to focus on the algorithm,
one of our pedagogical goals. The grade program
computes AER and optionally prints an alignment
grid for sentences in the development data, showing
both human and automatic alignments. Finally the
check program verifies that the results represent
a valid solution, reporting an error if not?enabling
students to diagnose bugs in their submissions.
The default implementation enabled immediate
experimentation. On receipt of the code, students
were instructed to align the first 1,000 sentences and
compute AER using a simple command.
> align -n 1000 | grade By varying the
number of input sentences and the threshold for an
alignment, students could immediately see the effect
of various parameters on alignment quality.
We privately implemented IBM Model 1 (Brown
et al, 1993) as the target alorithm for a passing
grade. We ran it for five iterations with English
as the target language and French as the source.
Our implementation did not use null alignment
or symmetrization?leaving out these common im-
provements offered students the possibility of dis-
covering them independently, and thereby rewarded.
A
E
R
?
10
0
20
30
40
50
60
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 1: Submission history for the alignment challenge.
Dashed lines represent the default and baseline system
performance. Each colored line represents a student, and
each dot represents a submission. For clarity, we show
only submissions that improved the student?s AER.
3.3 Challenge Results
We received 209 submissions from 11 teams over a
period of two weeks (Figure 1). Everyone eventually
matched or exceeded IBM Model 1 AER of 31.26.
Most students implemented IBM Model 1, but we
saw many other solutions, indicating that many truly
experimented with the problem:
? Implementing heuristic constraints to require
alignment of proper names and punctuation.
? Running the algorithm on stems rather than sur-
face words.
? Initializing the first iteration of Model 1 with
parameters estimated on the observed align-
ments in the development data.
? Running Model 1 for many iterations. Most re-
searchers typically run Model 1 for five itera-
tions or fewer, and there are few experiments
in the literature on its behavior over many iter-
ations, as there are for hidden Markov model
taggers (Johnson, 2007). Our students carried
out these experiments, reporting runs of 5, 20,
100, and even 2000 iterations. No improve-
ment was observed after 20 iterations.
168
? Implementing various alternative approaches
from the literature, including IBM Model 2
(Brown et al, 1993), competitive linking
(Melamed, 2000), and smoothing (Moore,
2004).
One of the best solutions was competitive linking
with Dice?s coefficient, modified to incorporate the
observation that alignments tend to be monotonic by
restricting possible alignment points to a window of
eight words around the diagonal. Although simple,
it acheived an AER of 18.41, an error reduction over
Model 1 of more than 40%.
The best score compares unfavorably against a
state-of-the-art AER of 3.6 (Liu et al, 2010). But
under a different view, it still represents a significant
amount of progress for an effort taking just over two
weeks: on the original challenge from which we ob-
tained the data (Mihalcea and Pedersen, 2003) the
best student system would have placed fifth out of
fifteen systems. Consider also the combined effort of
all the students: when we trained a perceptron clas-
sifier on the development data, taking each student?s
prediction as a feature, we obtained an AER of 15.4,
which would have placed fourth on the original chal-
lenge. This is notable since none of the systems
incorporated first-order dependencies on the align-
ments of adjacent words, long noted as an impor-
tant feature of the best alignment models (Och and
Ney, 2003). Yet a simple system combination of stu-
dent assignments is as effective as a hidden Markov
Model trained on a comparable amount of data (Och
and Ney, 2003).
It is important to note that AER does not neces-
sarily correlate with downstream performance, par-
ticularly on the Hansards dataset (Fraser and Marcu,
2007). We used the conclusion of the assignment as
an opportunity to emphasize this point.
4 The Decoding Challenge
The second challenge was decoding: given a fixed
translation model and a set of input sentences, stu-
dents were challenged to produce translations with
the highest model score. This challenge introduced
the difficulties of combinatorial optimization under
a deceptively simple setup: the model we provided
was a simple phrase-based translation model (Koehn
et al, 2003) consisting only of a phrase table and tri-
gram language model. Under this simple model, for
a French sentence f of length I , English sentence
e of length J , and alignment a where each element
consists of a span in both e and f such that every
word in both e and f is aligned exactly once, the
conditional probability of e and a given f is as fol-
lows.10
p(e, a|f) =
?
?i,i?,j,j???a
p(f i?i |ej
?
j )
J+1?
j=1
p(ej |ej?1, ej?2)
(1)
To evaluate output, we compute the conditional
probability of e as follows.
p(e|f) =
?
a
p(e, a|f) (2)
Note that this formulation is different from the typ-
ical Viterbi objective of standard beam search de-
coders, which do not sum over all alignments, but
approximate p(e|f) by maxa p(e, a|f). Though the
computation in Equation 2 is intractable (DeNero
and Klein, 2008), it can be computed in a few min-
utes via dynamic programming on reasonably short
sentences. We ensured that our data met this crite-
rion. The corpus-level probability is then the prod-
uct of all sentence-level probabilities in the data.
The model includes no distortion limit or distor-
tion model, for two reasons. First, leaving out the
distortion model slightly simplifies the implementa-
tion, since it is not necessary to keep track of the last
word translated in a beam decoder; we felt that this
detail was secondary to understanding the difficulty
of search over phrase permutations. Second, it actu-
ally makes the problem more difficult, since a simple
distance-based distortion model prefers translations
with fewer permutations; without it, the model may
easily prefer any permutation of the target phrases,
making even the Viterbi search problem exhibit its
true NP-hardness (Knight, 1999a; Zaslavskiy et al,
2009).
Since the goal was to find the translation with the
highest probability, we did not provide a held-out
test set; with access to both the input sentences and
10For simplicity, this formula assumes that e is padded with two
sentence-initial symbols and one sentence-final symbol, and
ignores the probability of sentence segmentation, which we
take to be uniform.
169
the model, students had enough information to com-
pute the evaluation score on any dataset themselves.
The difficulty of the challenge lies simply in finding
the translation that maximizes the evaluation. In-
deed, since the problem is intractable, even the in-
structors did not know the true solution.11
4.1 Data
We chose 48 French sentences totaling 716 words
from the Canadian Hansards to serve as test data.
To create a simple translation model, we used the
Berkeley aligner to align the parallel text from the
first assignment, and extracted a phrase table using
the method of Lopez (2007), as implemented in cdec
(Dyer et al, 2010). To create a simple language
model, we used SRILM (Stolcke, 2002).
4.2 Implementation
We distributed two Python programs. The first,
decode, decodes the test data monotonically?
using both the language model and translation
model, but without permuting phrases. The imple-
mentation is completely self-contained with no ex-
ternal dependencies: it implements both models and
a simple stack decoding algorithm for monotonic
translation. It contains only 122 lines of Python?
orders of magnitude fewer than most full-featured
decoders. To see its similarity to pseudocode, com-
pare the decoding algorithm (Listing 2) with the
pseudocode in Koehn?s (2010) popular textbook (re-
produced here as Algorithm 1). The second pro-
gram, grade, computes the log-probability of a set
of translations, as outline above.
We privately implemented a simple stack decoder
that searched over permutations of phrases, similar
to Koehn (2004). Our implementation increased the
codebase by 44 lines of code and included param-
eters for beam size, distortion limit, and the maxi-
mum number of translations considered for each in-
put phrase. We posted a baseline to the leaderboard
using values of 50, 3, and 20 for these, respectively.
11We implemented a version of the Lagrangian relaxation algo-
rithm of Chang and Collins (2011), but found it difficult to
obtain tight (optimal) solutions without iteratively reintroduc-
ing all of the original constraints. We suspect this is due to
the lack of a distortion penalty, which enforces a strong pref-
erence towards translations with little reordering. However,
the solution found by this algorithm is only approximates the
objective implied by Equation 2, which sums over alignments.
We also posted an oracle containing the most prob-
able output for each sentence, selected from among
all submissions received so far. The intent of this
oracle was to provide a lower bound on the best pos-
sible output, giving students additional incentive to
continue improving their systems.
4.3 Challenge Results
We received 71 submissions from 10 teams (Fig-
ure 2), again exhibiting variety of solutions.
? Implementation of greedy decoder which at
each step chooses the most probable translation
from among those reachable by a single swap
or retranslation (Germann et al, 2001; Langlais
et al, 2007).
? Inclusion of heuristic estimates of future cost.
? Implementation of a private oracle. Some stu-
dents observed that the ideal beam setting was
not uniform across the corpus. They ran their
decoder under different settings, and then se-
lected the most probable translation of each
sentence.
Many teams who implemented the standard stack
decoding algorithm experimented heavily with its
pruning parameters. The best submission used ex-
tremely wide beam settings in conjunction with a
reimplementation of the future cost estimate used in
Moses (Koehn et al, 2007). Five of the submissions
beat Moses using its standard beam settings after it
had been configured to decode with our model.
We used this assignment to emphasize the im-
portance of good models: the model score of the
submissions was generally inversely correlated with
BLEU, possibly because our simple model had no
distortion limits. We used this to illustrate the differ-
ence between model error and search error, includ-
ing fortuitous search error (Germann et al, 2001)
made by decoders with less accurate search.
5 The Evaluation Challenge
The third challenge was evaluation: given a test cor-
pus with reference translations and the output of sev-
eral MT systems, students were challenged to pro-
duce a ranking of the systems that closely correlated
with a human ranking.
170
Listing 2 The default decoder in DREAMT: a stack decoder for monotonic translation.
stacks = [{} for _ in f] + [{}]
stacks[0][lm.begin()] = initial_hypothesis
for i, stack in enumerate(stacks[:-1]):
for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:alpha]:
for j in xrange(i+1,len(f)+1):
if f[i:j] in tm:
for phrase in tm[f[i:j]]:
logprob = h.logprob + phrase.logprob
lm_state = h.lm_state
for word in phrase.english.split():
(lm_state, word_logprob) = lm.score(lm_state, word)
logprob += word_logprob
logprob += lm.end(lm_state) if j == len(f) else 0.0
new_hypothesis = hypothesis(logprob, lm_state, h, phrase)
if lm_state not in stacks[j] or \
stacks[j][lm_state].logprob < logprob:
stacks[j][lm_state] = new_hypothesis
winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
def extract_english(h):
return "" if h.predecessor is None else "%s%s " %
(extract_english(h.predecessor), h.phrase.english)
print extract_english(winner)
Algorithm 1 Basic stack decoding algorithm,
adapted from Koehn (2010), p. 165.
place empty hypothesis into stack 0
for all stacks 0...n? 1 do
for all hypotheses in stack do
for all translation options do
if applicable then
create new hypothesis
place in stack
recombine with existing hypothesis
prune stack if too big
5.1 Data
We chose the English-to-German translation sys-
tems from the 2009 and 2011 shared task at the an-
nual Workshop for Machine Translation (Callison-
Burch et al, 2009; Callison-Burch et al, 2011), pro-
viding the first as development data and the second
as test data. We chose these sets because BLEU
(Papineni et al, 2002), our baseline metric, per-
formed particularly poorly on them; this left room
for improvement in addition to highlighting some
lo
g 1
0
p(
e|f
)?
C
-1200
-1250
-1300
-1350
-1400
-20
days
-18
days
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 2: Submission history for the decoding challenge.
The dotted green line represents the oracle over submis-
sions.
deficiencies of BLEU. For each dataset we pro-
vided the source and reference sentences along with
anonymized system outputs. For the development
data we also provided the human ranking of the sys-
171
tems, computed from pairwise human judgements
according to a formula recommended by Bojar et al
(2011).12
5.2 Implementation
We provided three simple Python programs:
evaluate implements a simple ranking of the sys-
tems based on position-independent word error rate
(PER; Tillmann et al, 1997), which computes a bag-
of-words overlap between the system translations
and the reference. The grade program computes
Spearman?s ? between the human ranking and an
output ranking. The check program simply ensures
that a submission contains a valid ranking.
We were concerned about hill-climbing on the test
data, so we modified the leaderboard to report new
results only twice a day. This encouraged students to
experiment on the development data before posting
new submissions, while still providing intermittent
feedback.
We privately implemented a version of BLEU,
which obtained a correlation of 38.6 with the human
rankings, a modest improvement over the baseline
of 34.0. Our implementation underperforms the one
reported in Callison-Burch et al (2011) since it per-
forms no tokenization or normalization of the data.
This also left room for improvement.
5.3 Evaluation Challenge Results
We received 212 submissions from 12 teams (Fig-
ure 3), again demonstrating a wide range of tech-
niques.
? Experimentation with the maximum n-gram
length and weights in BLEU.
? Implementation of smoothed versions of BLEU
(Lin and Och, 2004).
? Implementation of weighted F-measure to bal-
ance both precision and recall.
? Careful normalization of the reference and ma-
chine translations, including lowercasing and
punctuation-stripping.
12This ranking has been disputed over a series of papers (Lopez,
2012; Callison-Burch et al, 2012; Koehn, 2012). The paper
which initiated the dispute, written by the first author, was di-
rectly inspired by the experience of designing this assignment.
Sp
ea
rm
an
?s
?
0.8
0.6
0.4
-7
days
-6
days
-5
days
-4
days
-3
days
-2
days
-1
days
due
Figure 3: Submission history for the evaluation chal-
lenge.
? Implementation of several techniques used in
AMBER (Chen and Kuhn, 2005).
The best submission, obtaining a correlation of
83.5, relied on the idea that the reference and ma-
chine translation should be good paraphrases of each
other (Owczarzak et al, 2006; Kauchak and Barzi-
lay, 2006). It employed a simple paraphrase sys-
tem trained on the alignment challenge data, us-
ing the pivot technique of Bannard and Callison-
Burch (2005), and computing the optimal alignment
between machine translation and reference under a
simple model in which words could align if they
were paraphrases. When compared with the 20
systems submitted to the original task from which
the data was obtained (Callison-Burch et al, 2011),
this system would have ranked fifth, quite near the
top-scoring competitors, whose correlations ranged
from 88 to 94.
6 The Reranking Challenge
The fourth challenge was reranking: given a test cor-
pus and a large N -best list of candidate translations
for each sentence, students were challenged to select
a candidate translation for each sentence to produce
a high corpus-level BLEU score. Due to an error
our data preparation, this assignment had a simple
solution that was very difficult to improve on. Nev-
ertheless, it featured several elements that may be
useful for future courses.
172
6.1 Data
We obtained 300-best lists from a Spanish-English
translation system built with the Joshua toolkit
(Ganitkevitch et al, 2012) using data and resources
from the 2011 Workshop on Machine Translation
(Callison-Burch et al, 2011). We provided 1989
training sentences, consisting of source and refer-
ence sentences along with the candidate translations.
We also included a test set of 250 sentences, for
which we provided only the source and candidate
translations. Each candidate translation included six
features from the underlying translation system, out
of an original 21; our hope was that students might
rediscover some features through experimentation.
6.2 Implementation
We conceived of the assignment as one in which stu-
dents could apply machine learning or feature engi-
neering to the task of reranking the systems, so we
provided several tools. The first of these, learn,
was a simple program that produced a vector of
feature weights using pairwise ranking optimization
(PRO; Hopkins and May, 2011), with a perceptron
as the underlying learning algorithm. A second,
rerank, takes a weight vector as input and reranks
the sentences; both programs were designed to work
with arbitrary numbers of features. The grade pro-
gram computed the BLEU score on development
data, while check ensured that a test submission
is valid. Finally, we provided an oracle program,
which computed a lower bound on the achievable
BLEU score on the development data using a greedy
approximation (Och et al, 2004). The leaderboard
likewise displayed an oracle on test data. We did
not assign a target alorithm, but left the assignment
fully open-ended.
6.3 Reranking Challenge Outcome
For each assignment, we made an effort to create
room for competition above the target alorithm.
However, we did not accomplish this in the rerank-
ing challenge: we had removed most of the features
from the candidate translations, in hopes that stu-
dents might reinvent some of them, but we left one
highly predictive implicit feature in the data: the
rank order of the underlying translation system. Stu-
dents discovered that simply returning the first can-
didate earned a very high score, and most of them
quickly converged to this solution. Unfortunately,
the high accuracy of this baseline left little room for
additional competition. Nevertheless, we were en-
couraged that most students discovered this by acci-
dent while attempting other strategies to rerank the
translations.
? Experimentation with parameters of the PRO
algorithm.
? Substitution of alternative learning algorithms.
? Implementation of a simplified minimum
Bayes risk reranker (Kumar and Byrne, 2004).
Over a baseline of 24.02, the latter approach ob-
tained a BLEU of 27.08, nearly matching the score
of 27.39 from the underlying system despite an im-
poverished feature set.
7 Pedagogical Outcomes
Could our students have obtained similar results by
running standard toolkits? Undoubtedly. However,
our goal was for students to learn by doing: they
obtained these results by implementing key MT al-
gorithms, observing their behavior on real data, and
improving them. This left them with much more in-
sight into how MT systems actually work, and in
this sense, DREAMT was a success. At the end of
class, we requested written feedback on the design
of the assignments. Many commented positively on
the motivation provided by the challenge problems:
? The immediate feedback of the automatic grad-
ing was really nice.
? Fast feedback on my submissions and my rela-
tive position on the leaderboard kept me both
motivated to start the assignments early and to
constantly improve them. Also knowing how
well others were doing was a good way to
gauge whether I was completely off track or not
when I got bad results.
? The homework assignments were very engag-
ing thanks to the clear yet open-ended setup
and their competitive aspects.
Students also commented that they learned a lot
about MT and even research in general:
173
Question 1 2 3 4 5 N/A
Feedback on my work for this course is useful - - - 4 9 3
This course enhanced my ability to work effectively in a team 1 - 5 8 2 -
Compared to other courses at this level, the workload for this course is high - 1 7 6 1 1
Table 1: Response to student survey questions on a Likert scale from 1 (strongly disagree) to 5 (strongly agree).
? I learned the most from the assignments.
? The assignments always pushed me one step
more towards thinking out loud how the par-
ticular task can be completed.
? I appreciated the setup of the homework prob-
lems. I think it has helped me learn how to
set up and attack research questions in an or-
ganized way. I have a much better sense for
what goes into an MT system and what prob-
lems aren?t solved.
We also received feedback through an anonymous
survey conducted at the end of the course before
posting final grades. Each student rated aspects
of the course on a five point Likert scale, from 1
(strongly disagree) to 5 (strongly agree). Several
questions pertained to assignments (Table 1), and al-
lay two possible concerns about competition: most
students felt that the assignments enhanced their col-
laborative skills, and that their open-endedness did
not result in an overload of work. For all survey
questions, student satisfaction was higher than av-
erage for courses in our department.
8 Discussion
DREAMT is inspired by several different ap-
proaches to teaching NLP, AI, and computer sci-
ence. Eisner and Smith (2008) teach NLP using
a competitive game in which students aim to write
fragments of English grammar. Charniak et al
(2000) improve the state-of-the-art in a reading com-
prehension task as part of a group project. Christo-
pher et al (1993) use NACHOS, a classic tool for
teaching operating systems by providing a rudimen-
tary system that students then augment. DeNero and
Klein (2010) devise a series of assignments based
on Pac-Man, for which students implement several
classic AI techniques. A crucial element in such ap-
proaches is a highly functional but simple scaffold-
ing. The DREAMT codebase, including grading and
validation scripts, consists of only 656 lines of code
(LOC) over four assignments: 141 LOC for align-
ment, 237 LOC for decoding, 86 LOC for evalua-
tion, and 192 LOC for reranking. To simplify imple-
mentation further, the optional leaderboard could be
delegated to Kaggle.com, a company that organizes
machine learning competitions using a model sim-
ilar to the Netflix Challenge (Bennet and Lanning,
2007), and offers pro bono use of its services for
educational challenge problems. A recent machine
learning class at Oxford hosted its assignments on
Kaggle (Phil Blunsom, personal communication).
We imagine other uses of DREAMT. It could be
used in an inverted classroom, where students view
lecture material outside of class and work on prac-
tical problems in class. It might also be useful in
massive open online courses (MOOCs). In this for-
mat, course material (primarily lectures and quizzes)
is distributed over the internet to an arbitrarily large
number of interested students through sites such as
coursera.org, udacity.com, and khanacademy.org. In
many cases, material and problem sets focus on spe-
cific techniques. Although this is important, there is
also a place for open-ended problems on which stu-
dents apply a full range of problem-solving skills.
Automatic grading enables them to scale easily to
large numbers of students.
On the scientific side, the scale of MOOCs might
make it possible to empirically measure the effec-
tiveness of hands-on or competitive assignments,
by comparing course performance of students who
work on them against that of those who do not.
Though there is some empirical work on competi-
tive assignments in the computer science education
literature (Lawrence, 2004; Garlick and Akl, 2006;
Regueras et al, 2008; Ribeiro et al, 2009), they
generally measure student satisfaction and retention
rather than the more difficult question of whether
such assignments actually improve student learning.
However, it might be feasible to answer such ques-
174
tions in large, data-rich virtual classrooms offered
by MOOCs. This is an interesting potential avenue
for future work.
Because our class came within reach of state-of-
the-art on each problem within a matter of weeks,
we wonder what might happen with a very large
body of competitors. Could real innovation oc-
cur? Could we solve large-scale problems? It may
be interesting to adopt a different incentive struc-
ture, such as one posed by Abernethy and Frongillo
(2011) for crowdsourcing machine learning prob-
lems: rather than competing, everyone works to-
gether to solve a shared task, with credit awarded
proportional to the contribution that each individual
makes. In this setting, everyone stands to gain: stu-
dents learn to solve problems as they are found in
the real world, instructors learn new insights into the
problems they pose, and, in the long run, users of
AI technology benefit from overall improvements.
Hence it is possible that posing open-ended, real-
world problems to students might be a small piece
of the puzzle of providing high-quality NLP tech-
nologies.
Acknowledgments
We are grateful to Colin Cherry and Chris Dyer
for testing the assignments in different settings and
providing valuable feedback, and to Jessie Young
for implementing a dual decomposition solution to
the decoding assignment. We thank Jason Eis-
ner, Frank Ferraro, Yoav Goldberg, Matt Gormley,
Ann Irvine, Rebecca Knowles, Ben Mitchell, Court-
ney Napoles, Michael Rushanan, Joanne Selinski,
Svitlana Volkova, and the anonymous reviewers for
lively discussion and helpful comments on previous
drafts of this paper. Any errors are our own.
References
J. Abernethy and R. M. Frongillo. 2011. A collaborative
mechanism for crowdsourcing prediction problems. In
Proc. of NIPS.
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of ACL.
J. Bennet and S. Lanning. 2007. The netflix prize. In
Proc. of the KDD Cup and Workshop.
S. Bird, E. Klein, E. Loper, and J. Baldridge. 2008.
Multidisciplinary instruction with the natural language
toolkit. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
O. Bojar, M. Ercegovc?evic?, M. Popel, and O. Zaidan.
2011. A grain of salt for the WMT manual evaluation.
In Proc. of WMT.
P. E. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2).
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-
cut, and L. Specia. 2012. Findings of the 2012 work-
shop on statistical machine translation. In Proc. of
WMT.
Y.-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through Lagrangian
relaxation. In Proc. of EMNLP.
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett,
M. Kosmala, T. Moscovich, L. Pang, C. Pyo, Y. Sun,
W. Wy, Z. Yang, S. Zeiler, and L. Zorn. 2000. Read-
ing comprehension programs in a statistical-language-
processing class. In Proc. of Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
B. Chen and R. Kuhn. 2005. AMBER: A modified
BLEU, enhanced ranking metric. In Proc. of WMT.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
W. A. Christopher, S. J. Procter, and T. E. Anderson.
1993. The nachos instructional operating system. In
Proc. of USENIX.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Proc. of ACL.
J. DeNero and D. Klein. 2010. Teaching introductory
articial intelligence with Pac-Man. In Proc. of Sym-
posium on Educational Advances in Artificial Intelli-
gence.
L. R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297?302.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL.
J. Eisner and N. A. Smith. 2008. Competitive grammar
writing. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
175
A. Fraser and D. Marcu. 2007. Measuring word align-
ment quality for statistical machine translation. Com-
putational Linguistics, 33(3).
J. Ganitkevitch, Y. Cao, J. Weese, M. Post, and
C. Callison-Burch. 2012. Joshua 4.0: Packing, PRO,
and paraphrases. In Proc. of WMT.
R. Garlick and R. Akl. 2006. Intra-class competitive
assignments in CS2: A one-year study. In Proc. of
International Conference on Engineering Education.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast decoding and optimal decoding for
machine translation. In Proc. of ACL.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. of
ACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP.
D. Jurafsky and J. H. Martin. 2009. Speech and Lan-
guage Processing. Prentice Hall, 2nd edition.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. of HLT-NAACL.
D. Klein. 2005. A core-tools statistical NLP course. In
Proc. of Workshop on Effective Tools and Methodolo-
gies for Teaching NLP and CL.
K. Knight. 1999a. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4).
K. Knight. 1999b. A statistical MT tutorial workbook.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proc. of AMTA.
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
P. Koehn. 2012. Simulating human judgment in machine
translation evaluation campaigns. In Proc. of IWSLT.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Proc.
of HLT-NAACL.
P. Langlais, A. Patry, and F. Gotti. 2007. A greedy de-
coder for phrase-based statistical machine translation.
In Proc. of TMI.
R. Lawrence. 2004. Teaching data structures using
competitive games. IEEE Transactions on Education,
47(4).
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In Proc. of COLING.
Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word
alignment by linear modeling. Computational Lin-
guistics, 36(3).
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proc. of EMNLP.
A. Lopez. 2008. Statistical machine translation. ACM
Computing Surveys, 40(3).
A. Lopez. 2012. Putting human assessments of machine
translation systems in order. In Proc. of WMT.
N. Madnani and B. Dorr. 2008. Combining open-source
with research to re-engineer a hands-on introductory
NLP course. In Proc. of Workshop on Issues in Teach-
ing Computational Linguistics.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2).
R. Mihalcea and T. Pedersen. 2003. An evaluation ex-
ercise for word alignment. In Proc. on Workshop on
Building and Using Parallel Texts.
R. C. Moore. 2004. Improving IBM word alignment
model 1. In Proc. of ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of
features for statistical machine translation. In Proc. of
NAACL.
K. Owczarzak, D. Groves, J. V. Genabith, and A. Way.
2006. Contextual bitext-derived paraphrases in auto-
matic MT evaluation. In Proc. of WMT.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL.
L. Regueras, E. Verdu?, M. Verdu?, M. Pe?rez, J. de Castro,
and M. Mun?oz. 2008. Motivating students through
on-line competition: An analysis of satisfaction and
learning styles.
P. Ribeiro, M. Ferreira, and H. Simo?es. 2009. Teach-
ing artificial intelligence and logic programming in a
competitive environment. Informatics in Education,
(Vol 8 1):85.
J. Spacco, D. Hovemeyer, W. Pugh, J. Hollingsworth,
N. Padua-Perez, and F. Emad. 2006. Experiences with
marmoset: Designing and using an advanced submis-
sion and testing system for programming courses. In
Proc. of Innovation and technology in computer sci-
ence education.
176
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based search for statistical
translation. In Proc. of European Conf. on Speech
Communication and Technology.
M. Zaslavskiy, M. Dymetman, and N. Cancedda. 2009.
Phrase-based statistical machine translation as a trav-
eling salesman problem. In Proc. of ACL.
177
178
The Language Demographics of Amazon Mechanical Turk
Ellie Pavlick1 Matt Post2 Ann Irvine2 Dmitry Kachaev2 Chris Callison-Burch1,2
1Computer and Information Science Department, University of Pennsylvania
2Human Language Technology Center of Excellence, Johns Hopkins University
Abstract
We present a large scale study of the languages
spoken by bilingual workers on Mechanical
Turk (MTurk). We establish a methodology
for determining the language skills of anony-
mous crowd workers that is more robust than
simple surveying. We validate workers? self-
reported language skill claims by measuring
their ability to correctly translate words, and
by geolocating workers to see if they reside in
countries where the languages are likely to be
spoken. Rather than posting a one-off survey,
we posted paid tasks consisting of 1,000 as-
signments to translate a total of 10,000 words
in each of 100 languages. Our study ran
for several months, and was highly visible on
the MTurk crowdsourcing platform, increas-
ing the chances that bilingual workers would
complete it. Our study was useful both to cre-
ate bilingual dictionaries and to act as cen-
sus of the bilingual speakers on MTurk. We
use this data to recommend languages with the
largest speaker populations as good candidates
for other researchers who want to develop
crowdsourced, multilingual technologies. To
further demonstrate the value of creating data
via crowdsourcing, we hire workers to create
bilingual parallel corpora in six Indian lan-
guages, and use them to train statistical ma-
chine translation systems.
1 Overview
Crowdsourcing is a promising new mechanism for
collecting data for natural language processing re-
search. Access to a fast, cheap, and flexible work-
force allows us to collect new types of data, poten-
tially enabling new language technologies. Because
crowdsourcing platforms like Amazon Mechanical
Turk (MTurk) give researchers access to a world-
wide workforce, one obvious application of crowd-
sourcing is the creation of multilingual technologies.
With an increasing number of active crowd workers
located outside of the United States, there is even the
potential to reach fluent speakers of lower resource
languages. In this paper, we investigate the feasi-
bility of hiring language informants on MTurk by
conducting the first large-scale demographic study
of the languages spoken by workers on the platform.
There are several complicating factors when try-
ing to take a census of workers on MTurk. The
workers? identities are anonymized, and Amazon
provides no information about their countries of ori-
gin or their language abilities. Posting a simple sur-
vey to have workers report this information may be
inadequate, since (a) many workers may never see
the survey, (b) many opt not to do one-off surveys
since potential payment is low, and (c) validating the
answers of respondents is not straightforward.
Our study establishes a methodology for deter-
mining the language demographics of anonymous
crowd workers that is more robust than simple sur-
veying. We ask workers what languages they speak
and what country they live in, and validate their
claims by measuring their ability to correctly trans-
late words and by recording their geolocation. To
increase the visibility and the desirability of our
tasks, we post 1,000 assignments in each of 100 lan-
guages. These tasks each consist of translating 10
foreign words into English. Two of the 10 words
have known translations, allowing us to validate that
the workers? translations are accurate. We construct
bilingual dictionaries with up to 10,000 entries, with
the majority of entries being new.
Surveying thousands of workers allows us to ana-
lyze current speaker populations for 100 languages.
79
Transactions of the Association for Computational Linguistics, 2 (2014) 79?92. Action Editor: Mirella Lapata.
Submitted 12/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
11/26/13 turkermap.html
file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html 1/1
1 1,998
Figure 1: The number of workers per country. This map was generated based on geolocating the IP address
of 4,983 workers in our study. Omitted are 60 workers who were located in more than one country during
the study, and 238 workers who could not be geolocated. The size of the circles represents the number
of workers from each country. The two largest are India (1,998 workers) and the United States (866). To
calibrate the sizes: the Philippines has 142 workers, Egypt has 25, Russia has 10, and Sri Lanka has 4.
The data also allows us to answer questions like:
How quickly is work completed in a given language?
Are crowdsourced translations reliably good? How
often do workers misrepresent their language abili-
ties to obtain financial rewards?
2 Background and Related Work
Amazon?s Mechanical Turk (MTurk) is an on-
line marketplace for work that gives employers
and researchers access to a large, low-cost work-
force. MTurk allows employers to provide micro-
payments in return for workers completing micro-
tasks. The basic units of work on MTurk are called
?Human Intelligence Tasks? (HITs). MTurk was de-
signed to accommodate tasks that are difficult for
computers, but simple for people. This facilitates
research into human computation, where people can
be treated as a function call (von Ahn, 2005; Little et
al., 2009; Quinn and Bederson, 2011). It has appli-
cation to research areas like human-computer inter-
action (Bigham et al., 2010; Bernstein et al., 2010),
computer vision (Sorokin and Forsyth, 2008; Deng
et al., 2010; Rashtchian et al., 2010), speech pro-
cessing (Marge et al., 2010; Lane et al., 2010; Parent
and Eskenazi, 2011; Eskenazi et al., 2013), and natu-
ral language processing (Snow et al., 2008; Callison-
Burch and Dredze, 2010; Laws et al., 2011).
On MTurk, researchers who need work completed
are called ?Requesters?, and workers are often re-
ferred to as ?Turkers?. MTurk is a true market, mean-
ing that Turkers are free to choose to complete the
HITs which interest them, and Requesters can price
their tasks competitively to try to attract workers and
have their tasks done quickly (Faridani et al., 2011;
Singer and Mittal, 2011). Turkers remain anony-
mous to Requesters, and all payment occurs through
Amazon. Requesters are able to accept submitted
work or reject work that does not meet their stan-
dards. Turkers are only paid if a Requester accepts
their work.
Several reports examine Mechanical Turk as an
economic market (Ipeirotis, 2010a; Lehdonvirta and
Ernkvist, 2011). When Amazon introduced MTurk,
it first offered payment only in Amazon credits, and
later offered direct payment in US dollars. More re-
cently, it has expanded to include one foreign cur-
rency, the Indian rupee. Despite its payments be-
ing limited to two currencies or Amazon credits,
MTurk claims over half a million workers from 190
countries (Amazon, 2013). This suggests that its
worker population should represent a diverse set of
languages.
80
A demographic study by Ipeirotis (2010b) fo-
cused on age, gender, martial status, income lev-
els, motivation for working on MTurk, and whether
workers used it as a primary or supplemental form
of income. The study contrasted Indian and US
workers. Ross et al. (2010) completed a longitudi-
nal follow-on study. A number of other studies have
informally investigated Turkers? language abilities.
Munro and Tily (2011) compiled survey responses
of 2,000 Turkers, revealing that four of the six most
represented languages come from India (the top six
being Hindi, Malayalam, Tamil, Spanish, French,
and Telugu). Irvine and Klementiev (2010) had
Turkers evaluate the accuracy of translations that
had been automatically inducted from monolingual
texts. They examined translations of 100 words in
42 low-resource languages, and reported geolocated
countries for their workers (India, the US, Romania,
Pakistan, Macedonia, Latvia, Bangladesh and the
Philippines). Irvine and Klementiev discussed the
difficulty of quality control and assessing the plausi-
bility of workers? language skills for rare languages,
which we address in this paper.
Several researchers have investigated using
MTurk to build bilingual parallel corpora for ma-
chine translation, a task which stands to benefit
low cost, high volume translation on demand (Ger-
mann, 2001). Ambati et al. (2010) conducted a pilot
study by posting 25 sentences to MTurk for Span-
ish, Chinese, Hindi, Telugu, Urdu, and Haitian Cre-
ole. In a study of 2000 Urdu sentences, Zaidan
and Callison-Burch (2011) presented methods for
achieving professional-level translation quality from
Turkers by soliciting multiple English translations
of each foreign sentence. Zbib et al. (2012) used
crowdsourcing to construct a 1.5 million word par-
allel corpus of dialect Arabic and English, train-
ing a statistical machine translation system that pro-
duced higher quality translations of dialect Arabic
than a system a trained on 100 times more Mod-
ern Standard Arabic-English parallel data. Zbib et
al. (2013) conducted a systematic study that showed
that training an MT system on crowdsourced trans-
lations resulted in the same performance as training
on professional translations, at 15 the cost. Hu etal. (2010; Hu et al. (2011) performed crowdsourced
translation by having monolingual speakers collab-
orate and iteratively improve MT output.
English 689 Tamil 253 Malayalam 219
Hindi 149 Spanish 131 Telugu 87
Chinese 86 Romanian 85 Portuguese 82
Arabic 74 Kannada 72 German 66
French 63 Polish 61 Urdu 56
Tagalog 54 Marathi 48 Russian 44
Italian 43 Bengali 41 Gujarati 39
Hebrew 38 Dutch 37 Turkish 35
Vietnamese 34 Macedonian 31 Cebuano 29
Swedish 26 Bulgarian 25 Swahili 23
Hungarian 23 Catalan 22 Thai 22
Lithuanian 21 Punjabi 21 Others ? 20
Table 1: Self-reported native language of 3,216
bilingual Turkers. Not shown are 49 languages with
?20 speakers. We omit 1,801 Turkers who did not
report their native language, 243 who reported 2 na-
tive languages, and 83 with ?3 native languages.
Several researchers have examined cost optimiza-
tion using active learning techniques to select the
most useful sentences or fragments to translate (Am-
bati and Vogel, 2010; Bloodgood and Callison-
Burch, 2010; Ambati, 2012).
To contrast our research with previous work, the
main contributions of this paper are: (1) a robust
methodology for assessing the bilingual skills of
anonymous workers, (2) the largest-scale census to
date of language skills of workers on MTurk, and (3)
a detailed analysis of the data gathered in our study.
3 Experimental Design
The central task in this study was to investigate Me-
chanical Turk?s bilingual population. We accom-
plished this through self-reported surveys combined
with a HIT to translate individual words for 100
languages. We evaluate the accuracy of the work-
ers? translations against known translations. In cases
where these were not exact matches, we used a sec-
ond pass monolingual HIT, which asked English
speakers to evaluate if a worker-provided translation
was a synonym of the known translation.
Demographic questionnaire At the start of each
HIT, Turkers were asked to complete a brief survey
about their language abilities. The survey asked the
following questions:
? Is [language] your native language?
? How many years have you spoken [language]?
81
? Is English your native language?
? How many years have you spoken English?
? What country do you live in?
We automatically collected each worker?s current lo-
cation by geolocating their IP address. A total of
5,281 unique workers completed our HITs. Of these,
3,625 provided answers to our survey questions, and
we were able to geolocate 5,043. Figure 1 plots
the location of workers across 106 countries. Table
1 gives the most common self-reported native lan-
guages.
Selection of languages We drew our data from the
different language versions of Wikipedia. We se-
lected the 100 languages with the largest number of
articles 1 (Table 2). For each language, we chose
the 1,000 most viewed articles over a 1 year period,2
and extracted the 10,000 most frequent words from
them. The resulting vocabularies served as the input
to our translation HIT.
Translation HIT For the translation task, we
asked Turkers to translate individual words. We
showed each word in the context of three sentences
that were drawn from Wikipedia. Turkers were al-
lowed to mark that they were unable to translate a
word. Each task contained 10 words, 8 of which
were words with unknown translations, and 2 of
which were quality control words with known trans-
lations. We gave special instruction for translat-
ing names of people and places, giving examples
of how to handle ?Barack Obama? and ?Australia?
using their interlanguage links. For languages with
non-Latin alphabets, names were transliterated.
The task paid $0.15 for the translation of 10
words. Each set of 10 words was independently
translated by three separate workers. 5,281 workers
completed 256,604 translation assignments, totaling
more than 3 million words, over a period of three
and a half months.
Gold standard translations A set of gold stan-
dard translations were automatically harvested from
1http://meta.wikimedia.org/wiki/List_of_
Wikipedias
2http://dumps.wikimedia.org/other/
pagecounts-raw/
500K+ ARTICLES: German (de), English (en), Spanish (es), French
(fr), Italian (it), Japanese (ja), Dutch (nl), Polish (pl), Portuguese
(pt), Russian (ru)
100K-500K ARTICLES: Arabic (ar), Bulgarian (bg), Catalan (ca),
Czech (cs), Danish (da), Esperanto (eo), Basque (eu), Persian (fa),
Finnish (fi), Hebrew (he), Hindi (hi), Croatian (hr), Hungarian (hu),
Indonesian (id), Korean (ko), Lithuanian (lt), Malay (ms), Norwe-
gian (Bokmal) (no), Romanian (ro), Slovak (sk), Slovenian (sl), Ser-
bian (sr), Swedish (sv), Turkish (tr), UKrainian (UK), Vietnamese
(vi), Waray-Waray (war), Chinese (zh)
10K-100K ARTICLES: Afrikaans (af) Amharic (am) Asturian (ast)
Azerbaijani (az) Belarusian (be) Bengali (bn) Bishnupriya Manipuri
(bpy) Breton (br) Bosnian (bs) Cebuano (ceb) Welsh (cy) Zazaki
(diq) Greek (el) West Frisian (fy) Irish (ga) Galician (gl) Gujarati
(gu) Haitian (ht) Armenian (hy) Icelandic (is) Javanese (jv) Geor-
gian (ka) Kannada (kn) Kurdish (ku) Luxembourgish (lb) Latvian
(lv) Malagasy (mg) Macedonian (mk) Malayalam (ml) Marathi
(mr) Neapolitan (nap) Low Saxon (nds) Nepali (ne) Newar / Nepal
Bhasa (new) Norwegian (Nynorsk) (nn) Piedmontese (pms) Sicil-
ian (scn) Serbo-Croatian (sh) Albanian (sq) Sundanese (su) Swahili
(sw) Tamil (ta) Telugu (te) Thai (th) Tagalog (tl) Urdu (ur) Yoruba
(yo)
<10K ARTICLES: Central Bicolano (bcl) Tibetan (bo) Ilokano (ilo)
Punjabi (pa) Kapampangan (pam) Pashto (ps) Sindhi (sd) Somali
(so) Uzbek (uz) Wolof (wo)
Table 2: A list of the languages that were used in our
study, grouped by the number of Wikipedia articles
in the language. Each language?s code is given in
parentheses. These language codes are used in other
figures throughout this paper.
Wikipedia for every language to use as embedded
controls. We used Wikipedia?s inter-language links
to pair titles of English articles with their corre-
sponding foreign article?s title. To get a more trans-
latable set of pairs, we excluded any pairs where: (1)
the English word was not present in the WordNet
ontology (Miller, 1995), (2) either article title was
longer than a single word, (3) the English Wikipedia
page was a subcategory of person or place, or (4)
the English and the foreign titles were identical or a
substring of the other.
Manual evaluation of non-identical translations
We counted all translations that exactly matched
the gold standard translation as correct. For non-
exact matches we created a second-pass quality as-
surance HIT. Turkers were shown a pair of En-
glish words, one of which was a Turker?s transla-
tion of the foreign word used for quality control,
and the other of which was the gold-standard trans-
lation of the foreign word. Evaluators were asked
whether the two words had the same meaning, and
chose between three answers: ?Yes?, ?No?, or ?Re-
82
Figure 2: Days to complete the translation HITs for
40 of the languages. Tick marks represent the com-
pletion of individual assignments.
lated but not synonymous.? Examples of mean-
ing equivalent pairs include: <petroglyphs, rock
paintings>, <demo, show> and <loam, loam: soil
rich in decaying matter>. Non-meaning equiva-
lents included: <assorted, minutes>, and <major,
URL of image>. Related items were things like
<sky, clouds>. Misspellings like <lactation, lac-
tiation > were judged to have same meaning, and
were marked as misspelled. Three separate Turkers
judged each pair, allowing majority votes for diffi-
cult cases.
We checked Turkers who were working on this
task by embedding pairs of words which were ei-
?? ?$ %??? ( ?? + ?$ ??? %?.? ?? ?? ???? 5 ?? ?????9 ???:? ?? ??<? 
In retribution pakistan also did six nuclear tests on 28 may 1998.
On 28 May Pakistan also conducted six nuclear tests as an act
of redressal.
Retaliating on this ?Pakistan? conducted Six(6) Nuclear Tests
on 28 May, 1998.
pakistan also did 6 nuclear test in retribution on 28 may, 1998
Figure 3: An example of the Turkers? translations of
a Hindi sentence. The translations are unedited and
contain fixable spelling, capitalization and grammat-
ical errors.
ther known to be synonyms (drawn from Word-
Net) or unrelated (randomly chosen from a corpus).
Automating approval/rejections for the second-pass
evaluation allowed the whole pipeline to be run au-
tomatically. Caching judgments meant that we ulti-
mately needed only 20,952 synonym tasks to judge
all of the submitted translations (a total of 74,572
non-matching word pairs). These were completed
by an additional 1,005 workers. Each of these as-
signments included 10 word pairs and paid $0.10.
Full sentence translations To demonstrate the
feasibility of using crowdsourcing to create multi-
lingual technologies, we hire Turkers to construct
bilingual parallel corpora from scratch for six In-
dian languages. Germann (2001) attempted to build
a Tamil-English translation system from scratch by
hiring professional translators, but found the cost
prohibitive. We created parallel corpora by trans-
lating the 100 most viewed Wikipedia pages in Ben-
gali, Malyalam, Hindi, Tamil, Telugu, and Urdu into
English. We collected four translations from differ-
ent Turkers for each source sentence.
Workers were paid $0.70 per HIT to translate
10 sentences. We accepted or rejected translations
based on a manual review of each worker?s submis-
sions, which included a comparison of the transla-
tions to a monotonic gloss (produced with a dic-
tionary), and metadata such as the amount of time
the worker took to complete the HIT and their geo-
graphic location.
Figure 3 shows an example of the translations we
obtained. The lack of a professionally translated
reference sentences prevented us from doing a sys-
tematic comparison between the quality of profes-
83
pt bs sh tl it sr ro es ms de af te hr id da nl tr gu sk fi he ml fr ja pa bg mk no gl ht ga sv cy lv hu kn az be lt ko ne eo ar pl mr ca cs sw ta hi bn nn ka so zh jv el ceb vi bcl is su uz lb bpy scn new ur sd br ps ru am wo bo
0.0
0.2
0.4
0.6
0.8
1.0
Figure 4: Translation quality for languages with at least 50 Turkers. The dark blue bars indicate the pro-
portion of translations which exactly matched gold standard translations, and light blue indicate translations
which were judged to be correct synonyms. Error bars show the 95% confidence intervals for each language.
sion and non-professional translations as Zaidan and
Callison-Burch (2011) did. Instead we evaluate the
quality of the data by using it to train SMT systems.
We present results in section 5.
4 Measuring Translation Quality
For single word translations, we calculate the qual-
ity of translations on the level of individual assign-
ments and aggregated over workers and languages.
We define an assignment?s quality as the proportion
of controls that are correct in a given assignment,
where correct means exactly correct or judged to be
synonymous.
Quality(ai) = 1ki
ki?
j=1
?(trij ? syns[gj]) (1)
where ai is the ith assignment, ki is the number of
controls in ai, trij is the Turker?s provided transla-
tion of control word j in assignment i, gj is the gold
standard translation of control word j, syns[gj] is
the set of words judged to be synonymous with gj
and includes gj , and ?(x) is Kronecker?s delta and
takes value 1 when x is true. Most assignments had
two known words embedded, so most assignments
had scores of either 0, 0.5, or 1.
Since computing overall quality for a language as
the average assignment quality score is biased to-
wards a small number of highly active Turkers, we
instead report language quality scores as the aver-
age per-Turker quality, where a Turker?s quality is
the average quality of all the assignments that she
completed:
Quality(ti) =
?
aj?assigns[i] Quality(aj)
| assigns[i] | (2)
where assigns[i] is the assignments completed
by Turker i, and Quality(a) is as above.
Quality for a language is then given by
Quality(li) =
?
tj?turkers[i] Quality(tj)
| turkers[i] | (3)
When a Turker completed assignments in more than
one language, their quality was computed separately
for each language. Figure 4 shows the transla-
tion quality for languages with contributions from
at least 50 workers.
Cheating using machine translation One obvi-
ous way for workers to cheat is to use available
online translation tools. Although we followed
best practices to deter copying-and-pasting into on-
line MT systems by rendering words and sentences
84
as images (Zaidan and Callison-Burch, 2011), this
strategy does not prevent workers from typing the
words into an MT system if they are able to type in
the language?s script.
To identify and remove workers who appeared to
be cheating by using Google Translate, we calcu-
lated each worker?s overlap with the Google transla-
tions. We used Google to translate all 10,000 words
for the 51 foreign languages that Google Trans-
late covered at the time of the study. We mea-
sured the percent of workers? translations that ex-
actly matched the translation returned from Google.
Figure 5a shows overlap between Turkers?s trans-
lations and Google Translate. When overlap is high,
it seems likely that those Turkers are cheating. It is
also reasonable to assume that honest workers will
overlap with Google some amount of the time as
Google?s translations are usually accurate. We di-
vide the workers into three groups: those with very
high overlap with Google (likely cheating by using
Google to translate words), those with reasonable
overlap, and those with no overlap (likely cheating
by other means, for instance, by submitting random
text).
Our gold-standard controls are designed to iden-
tify workers that fall into the third group (those who
are spamming or providing useless translations), but
they will not effectively flag workers who are cheat-
ing with Google Translate. We therefore remove the
500 Turkers with the highest overlap with Google.
This equates to removing all workers with greater
than 70% overlap. Figure 5b shows that removing
workers at or above the 70% threshold retains 90%
of the collected translations and over 90% of the
workers.
Quality scores reported throughout the paper re-
flect only translations from Turkers whose overlap
with Google falls below this 70% threshold.
5 Data Analysis
We performed an analysis of our data to address the
following questions:
? Do workers accurately represent their language
abilities? Should we constrain tasks by region?
? How quickly can we expect work to be com-
pleted in a particular language?
(a) Individual workers? overlap with Google Translate.
We removed the 500 workers with the highest overlap
(shaded region on the left) from our analyses, as it is rea-
sonable to assume these workers are cheating by submit-
ting translations from Google. Workers with no overlap
(shaded region on the right) are also likely to be cheating,
e.g. by submitting random text.
(b) Cumulative distribution of overlap with Google trans-
late for workers and translations. We see that eliminating
all workers with >70% overlap with google translate still
preserves 90% of translations and >90% of workers.
Figure 5
? Can Turkers? translations be used to train MT
systems?
? Do our dictionaries improve MT quality?
Language skills and location We measured the
average quality of workers who were in countries
that plausibly speak a language, versus workers from
countries that did not have large speaker populations
of that language. We used the Ethnologue (Lewis
85
Avg. Turker quality (# Ts) Primary locations Primary locations
In region Out of region of Turkers in region of Turkers out of region
Hindi 0.63 (296) 0.69 (7) India (284) UAE (5) UK (3) Saudi Arabia (2) Russia (1) Oman (1)
Tamil 0.65 (273) ** 0.25 (2) India (266) US (3) Canada (2) Tunisia (1) Egypt (1)
Malayalam 0.76 (234) 0.83 (2) India (223) UAE (6) US (3) Saudi Arabia (1) Maldives (1)
Spanish 0.81 (191) 0.84 (18) US (122) Mexico (16) Spain (14) India (15) New Zealand (1) Brazil (1)
French 0.75 (170) 0.82 (11) India (62) US (45) France (23) Greece (2) Netherlands (1) Japan (1)
Chinese 0.60 (116) 0.55 (21) US (75) Singapore (13) China (9) Hong Kong (6) Australia (3) Germany (2)
German 0.82 (91) 0.77 (41) Germany (48) US (25) Austria (7) India (34) Netherlands (1) Greece (1)
Italian 0.86 (90) * 0.80 (42) Italy (42) US (29) Romania (7) India (33) Ireland (2) Spain (2)
Amharic 0.14 (16) ** 0.01 (99) US (14) Ethiopia (2) India (70) Georgia (9) Macedonia (5)
Kannada 0.70 (105) NA (0) India (105)
Arabic 0.74 (60) ** 0.60 (45) Egypt (19) Jordan (16) Morocco (9) US (19) India (11) Canada (3)
Sindhi 0.19 (96) 0.06 (9) India (58) Pakistan (37) US (1) Macedonia (4) Georgia (2) Indonesia (2)
Portuguese 0.87 (101) 0.96 (3) Brazil (44) Portugal (31) US (15) Romania (1) Japan (1) Israel (1)
Turkish 0.76 (76) 0.80 (27) Turkey (38) US (18) Macedonia (8) India (19) Pakistan (4) Taiwan (1)
Telugu 0.80 (102) 0.50 (1) India (98) US (3) UAE (1) Saudi Arabia (1)
Irish 0.74 (54) 0.71 (47) US (39) Ireland (13) UK (2) India (36) Romania (5) Macedonia (2)
Swedish 0.73 (54) 0.71 (45) US (25) Sweden (22) Finland (3) India (23) Macedonia (6) Croatia (2)
Czech 0.71 (45) * 0.61 (50) US (17) Czech Republic (14) Serbia (5) Macedonia (22) India (10) UK (5)
Russian 0.15 (67) * 0.12 (27) US (36) Moldova (7) Russia (6) India (14) Macedonia (4) UK (3)
Breton 0.17 (3) 0.18 (89) US (3) India (83) Macedonia (2) China (1)
Table 3: Translation quality when partitioning the translations into two groups, one containing translations
submitted by Turkers whose location is within regions that plausibly speak the foreign language, and the
other containing translations from Turkers outside those regions. In general, in-region Turkers provide
higher quality translations. (**) indicates differences significant at p=0.05, (*) at p=0.10.
et al., 2013) to compile the list of countries where
each language is spoken. Table 3 compares the av-
erage translation quality of assignments completed
within the region of each language, and compares it
to the quality of assignments completed outside that
region.
Our workers reported speaking 95 languages na-
tively. US workers alone reported 61 native lan-
guages. Overall, 4,297 workers were located in a
region likely to speak the language from which they
were translating, and 2,778 workers were located
in countries considered out of region (meaning that
about a third of our 5,281 Turkers completed HITs
in multiple languages).
Table 3 shows the differences in translation qual-
ity when computed using in-region versus out-of-
region Turkers, for the languages with the greatest
number of workers. Within region workers typi-
cally produced higher quality translations. Given the
number of Indian workers on Mechanical Turk, it
is unsurprising that they represent majority of out-
of-region workers. For the languages that had more
than 75 out of region workers (Malay, Amharic, Ice-
landic, Sicilian, Wolof, and Breton), Indian workers
represented at least 70% of the out of region workers
in each language.
A few languages stand out for having suspiciously
strong performance by out of region workers, no-
tably Irish and Swedish, for which out of region
workers account for a near equivalent volume and
quality of translations to the in region workers. This
is admittedly implausible, considering the relatively
small number of Irish speakers worldwide, and the
very low number living in the countries in which our
Turkers were based (primarily India). Such results
highlight the fact that cheating using online transla-
tion resources is a real problem, and despite our best
efforts to remove workers using Google Translate,
some cheating is still evident. Restricting to within
region workers is an effective way to reduce the
prevalence of cheating. We discuss the languages
which are best supported by true native speakers in
section 6.
Speed of translation Figure 2 gives the comple-
tion times for 40 languages. The 10 languages to
finish in the shortest amount of time were: Tamil,
Malayalam, Telugu, Hindi, Macedonian, Spanish,
Serbian, Romanian, Gujarati, and Marathi. Seven of
the ten fastest languages are from India, which is un-
86
320 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
800,000
0
100,000
200,000
300,000
400,000
500,000
600,000
700,000
Malay
alam
Tamil
Telugu
Hindi
Urdu
Bengali
Figure 6: The total volume of translations (measured
in English words) as a function of elapsed days.
sentence English + dictionary
language pairs foreign words entries
Bengali 22k 732k 22k
Hindi 40k 1,488k 22k
Malayalam 32k 863k 23k
Tamil 38k 916k 25k
Telugu 46k 1,097k 21k
Urdu 35k 1,356k 20k
Table 4: Size of parallel corpora and bilingual dic-
tionaries collected for each language.
surprising given the geographic distribution of work-
ers. Some languages follow the pattern of having a
smattering of assignments completed early, with the
rate picking up later.
Figure 6 gives the throughput of the full-sentence
translation task for the six Indian languages. The
fastest language was Malayalam, for which we col-
lected half a million words of translations in just un-
der a week. Table 4 gives the size of the data set that
we created for each of these languages.
Training SMT systems We trained statistical
translation models from the parallel corpora that we
created for the six Indian languages using the Joshua
machine translation system (Post et al., 2012). Table
5 shows the translation performance when trained
on the bitexts alone, and when incorporating the
bilingual dictionaries created in our earlier HIT. The
scores reflect the performance when tested on held
out sentences from the training data. Adding the dic-
trained on bitext + BLEU
language bitexts alone dictionaries ?
Bengali 12.03 17.29 5.26
Hindi 16.19 18.10 1.91
Malayalam 6.65 9.72 3.07
Tamil 8.08 9.66 1.58
Telugu 11.94 13.70 1.76
Urdu 19.22 21.98 2.76
Table 5: BLEU scores for translating into English
using bilingual parallel corpora by themselves, and
with the addition of single-word dictionaries. Scores
are calculated using four reference translations and
represent the mean of three MERT runs.
tionaries to the training set produces consistent per-
formance gains, ranging from 1 to 5 BLEU points.
This represents a substantial improvement. It is
worth noting, however, that while the source doc-
uments for the full sentences used for testing were
kept disjoint from those used for training, there is
overlap between the source materials for the dictio-
naries and those from the test set, since both the dic-
tionaries and the bitext source sentences were drawn
from Wikipedia.
6 Discussion
Crowdsourcing platforms like Mechanical Turk give
researchers instant access to a diverse set of bilin-
gual workers. This opens up exciting new avenues
for researchers to develop new multilingual systems.
The demographics reported in this study are likely to
shift over time. Amazon may expand its payments to
new currencies. Posting long-running HITs in other
languages may recruit more speakers of those lan-
guages. New crowdsourcing platforms may emerge.
The data presented here provides a valuable snap-
shot of the current state of MTurk, and the methods
used can be applied generally in future research.
Based on our study, we can confidently recom-
mend 13 languages as good candidates for research
now: Dutch, French, German, Gujarati, Italian, Kan-
nada, Malayalam, Portuguese, Romanian, Serbian,
Spanish, Tagalog, and Telugu. These languages
have large Turker populations who complete tasks
quickly and accurately. Table 6 summarizes the
strengths and weaknesses of all 100 languages cov-
ered in our study. Several other languages are viable
87
workers quality speed
many high fast Dutch, French, German, Gu-
jarati, Italian, Kannada, Malay-
alam, Portuguese, Romanian,
Serbian, Spanish, Tagalog, Tel-
ugu
slow Arabic, Hebrew, Irish, Punjabi,
Swedish, Turkish
low fast Hindi, Marathi, Tamil, Urdu
or
medium
slow Bengali, Bishnupriya Ma-
nipuri, Cebuano, Chinese,
Nepali, Newar, Polish, Russian,
Sindhi, Tibetan
few high fast Bosnia, Croatian, Macedonian,
Malay, Serbo-Croatian
slow Afrikaans, Albanian,
Aragonese, Asturian, Basque,
Belarusian, Bulgarian, Central
Bicolano, Czech, Danish,
Finnish, Galacian, Greek,
Haitian, Hungarian, Icelandic,
Ilokano, Indonesian, Japanese,
Javanese, Kapampangan,
Kazakh, Korean, Lithuanian,
Low Saxon, Malagasy, Nor-
wegian (Bokmal), Sicilian,
Slovak, Slovenian, Thai, UKra-
nian, Uzbek, Waray-Waray,
West Frisian, Yoruba
low fast ?
or
medium
slow Amharic, Armenian, Azer-
baijani, Breton, Catalan,
Georgian, Latvian, Luxembour-
gish, Neapolitian, Norwegian
(Nynorsk), Pashto, Pied-
montese, Somali, Sudanese,
Swahili, Tatar, Vietnamese,
Walloon, Welsh
none low or
medium
slow Esperanto, Ido, Kurdish, Per-
sian, Quechua, Wolof, Zazaki
Table 6: The green box shows the best languages to
target on MTurk. These languages have many work-
ers who generate high quality results quickly. We
defined many workers as 50 or more active in-region
workers, high quality as?70% accuracy on the gold
standard controls, and fast if all of the 10,000 words
were completed within two weeks.
candidates provided adequate quality control mech-
anisms are used to select good workers.
Since Mechanical Turk provides financial incen-
tives for participation, many workers attempt to
complete tasks even if they do not have the lan-
guage skills necessary to do so. Since MTurk does
not provide any information about workers demo-
graphics, including their language competencies, it
can be hard to exclude such workers. As a result
naive data collection on MTurk may result in noisy
data. A variety of techniques should be incorporated
into crowdsourcing pipelines to ensure high quality
data. As a best practice, we suggest: (1) restricting
workers to countries that plausibly speak the foreign
language of interest, (2) embedding gold standard
controls or administering language pretests, rather
than relying solely on self-reported language skills,
and (3) excluding workers whose translations have
high overlap with online machine translation sys-
tems like Google translate. If cheating using exter-
nal resources is likely, then also consider (4) record-
ing information like time spent on a HIT (cumulative
and on individual items), patterns in keystroke logs,
tab/window focus, etc.
Although our study targeted bilingual workers on
Mechanical Turk, and neglected monolingual work-
ers, we believe our results reliably represent the cur-
rent speaker populations, since the vast majority of
the work available on the crowdsourced platform
is currently English-only. We therefore assume the
number of non-English speakers is small. In the fu-
ture, it may be desirable to recruit monolingual for-
eign workers. In such cases, we recommend other
tests to validate their language abilities in place of
our translation test. These could include perform-
ing narrative cloze, or listening to audio files con-
taining speech in different language and identifying
their language.
7 Data release
With the publication of this paper, we are releasing
all data and code used in this study. Our data release
includes the raw data, along with bilingual dictionar-
ies that are filtered to be high quality. It will include
256,604 translation assignments from 5,281 Turkers
and 20,952 synonym assignments from 1,005 Turk-
ers, along with meta information like geolocation
88
and time submitted, plus external dictionaries used
for validation. The dictionaries will contain 1.5M
total translated words in 100 languages, along with
code to filter the dictionaries based on different cri-
teria. The data also includes parallel corpora for six
Indian languages, ranging in size between 700,000
to 1.5 million words.
8 Acknowledgements
This material is based on research sponsored by
a DARPA Computer Science Study Panel phase 3
award entitled ?Crowdsourcing Translation? (con-
tract D12PC00368). The views and conclusions
contained in this publication are those of the authors
and should not be interpreted as representing offi-
cial policies or endorsements by DARPA or the U.S.
Government. This research was supported by the
Johns Hopkins University Human Language Tech-
nology Center of Excellence and through gifts from
Microsoft and Google.
The authors would like to thank the anonymous
reviewers for their thoughtful comments, which sub-
stantially improved this paper.
References
Amazon. 2013. Service summary tour for re-
questers on Amazon Mechanical Turk. https://
requester.mturk.com/tour.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk. Association for Computational Lin-
guistics.
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC).
Vamshi Ambati. 2012. Active Learning and Crowd-
sourcing for Machine Translation in Low Resource
Scenarios. Ph.D. thesis, Language Technologies In-
stitute, School of Computer Science, Carnegie Mellon
University, Pittsburgh, PA.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjrn Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent:
a word processor with a crowd inside. In Proceed-
ings of the ACM Symposium on User Interface Soft-
ware and Technology (UIST).
Jeffrey P. Bigham, Chandrika Jayant, Hanjie Ji, Greg Lit-
tle, Andrew Miller, Robert C. Miller, Robin Miller,
Aubrey Tatarowicz, Brandyn White, Samual White,
and Tom Yeh. 2010. VizWiz: nearly real-time an-
swers to visual questions. In Proceedings of the ACM
Symposium on User Interface Software and Technol-
ogy (UIST).
Michael Bloodgood and Chris Callison-Burch. 2010.
Large-scale cost-focused active learning for statisti-
cal machine translation. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June. Association for Computational Linguistics.
Jia Deng, Alexander Berg, Kai Li, and Li Fei-Fei. 2010.
What does classifying more than 10,000 image cate-
gories tell us? In Proceedings of the 12th European
Conference of Computer Vision (ECCV, pages 71?84.
Maxine Eskenazi, Gina-Anne Levow, Helen Meng,
Gabriel Parent, and David Suendermann. 2013.
Crowdsourcing for Speech Processing, Applications to
Data Collection, Transcription and Assessment. Wi-
ley.
Siamak Faridani, Bjo?rn Hartmann, and Panagiotis G.
Ipeirotis. 2011. What?s the right price? pricing tasks
for finishing on time. In Third AAAI Human Compu-
tation Workshop (HCOMP?11).
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang for
the buck can we expect? In ACL 2001 Workshop on
Data-Driven Machine Translation, Toulouse, France.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Proceedings of ACM SIGKDD
Workshop on Human Computation (HCOMP).
Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-
delman, Olivia Buzek, and Benjamin B. Bederson.
2011. The value of monolingual crowdsourcing in
a real-world translation scenario: Simulation using
haitian creole emergency sms messages. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 399?404, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Panagiotis G. Ipeirotis. 2010a. Analyzing the mechani-
cal turk marketplace. In ACM XRDS, December.
Panagiotis G. Ipeirotis. 2010b. Demographics of
Mechanical Turk. Technical Report Working paper
89
CeDER-10-01, New York University, Stern School of
Business.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Workshop on Creating Speech and
Language Data with MTurk.
Ian Lane, Matthias Eck, Kay Rottmann, and Alex
Waibel. 2010. Tools for collecting speech corpora
via mechanical-turk. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk, Los An-
geles.
Florian Laws, Christian Scheible, and Hinrich Schu?tze.
2011. Active learning with amazon mechanical turk.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, Edinburgh,
Scotland.
Matthew Lease, Jessica Hullman, Jeffrey P. Bigham,
Juho Kim Michael S. Bernstein and, Walter Lasecki,
Saeideh Bakhshi, Tanushree Mitra, and Robert C.
Miller. 2013. Mechanical Turk is not anony-
mous. http://dx.doi.org/10.2139/ssrn.
2228728.
Vili Lehdonvirta and Mirko Ernkvist. 2011. Knowl-
edge map of the virtual economy: Converting
the virtual economy into development potential.
http://www.infodev.org/en/Document.
1056.pdf, April. An InfoDev Publication.
M. Paul Lewis, Gary F. Simons, and Charles D. Fennig
(eds.). 2013. Ethnologue: Languages of the world,
seventeenth edition. http://www.ethnologue.
com.
Greg Little, Lydia B. Chilton, Rob Miller, and Max Gold-
man. 2009. Turkit: Tools for iterative tasks on me-
chanical turk. In Proceedings of the Workshop on
Human Computation at the International Conference
on Knowledge Discovery and Data Mining (KDD-
HCOMP ?09), Paris.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010. Using the Amazon Mechanical Turk
to transcribe and annotate meeting speech for extrac-
tive summarization. In Workshop on Creating Speech
and Language Data with MTurk.
George A. Miller. 1995. WordNet: a lexical database for
english. Communications of the ACM, 38(11):39?41.
Robert Munro and Hal Tily. 2011. The start of the
art: Introduction to the workshop on crowdsourcing
technologies for language and cognition studies. In
Crowdsourcing Technologies for Language and Cog-
nition Studies, Boulder.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
fast and good enough: Automatic speech recognition
with non-expert transcription. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 207?215. Association for
Computational Linguistics.
Gabriel Parent and Maxine Eskenazi. 2011. Speaking
to the crowd: looking at past achievements in using
crowdsourcing for speech and predicting future chal-
lenges. In Proceedings Interspeech 2011, Special Ses-
sion on Crowdsourcing.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 401?409, Montre?al, Canada, June. Association
for Computational Linguistics.
Alexander J. Quinn and Benjamin B. Bederson. 2011.
Human computation: A survey and taxonomy of a
growing field. In Computer Human Interaction (CHI).
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using Amazon?s Mechanical Turk. In Workshop on
Creating Speech and Language Data with MTurk.
Joel Ross, Lilly Irani, M. Six Silberman, Andrew Zal-
divar, and Bill Tomlinson. 2010. Who are the crowd-
workers?: Shifting demographics in Amazon Mechan-
ical Turk. In alt.CHI session of CHI 2010 extended
abstracts on human factors in computing systems, At-
lanta, Georgia.
Yaron Singer and Manas Mittal. 2011. Pricing mecha-
nisms for online labor markets. In Third AAAI Human
Computation Workshop (HCOMP?11).
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of EMNLP.
Alexander Sorokin and David Forsyth. 2008. Utility
data annotation with amazon mechanical turk. In First
IEEE Workshop on Internet Vision at CVPR.
Luis von Ahn. 2005. Human Computation. Ph.D. thesis,
School of Computer Science, Carnegie Mellon Uni-
versity, Pittsburgh, PA.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229. Association for Computational Linguistics.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of Arabic dialects. In The
2012 Conference of the North American Chapter of
the Association for Computational Linguistics. Asso-
ciation for Computational Linguistics.
90
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, and John Makhoul. 2013. Sys-
tematic comparison of professional and crowdsourced
reference translations for machine translation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Atlanta,
Georgia.
91
92
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 256?264,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Monolingual Distributional Similarity for Text-to-Text Generation
Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch
Center for Language and Speech Processing
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
Abstract
Previous work on paraphrase extraction and
application has relied on either parallel
datasets, or on distributional similarity met-
rics over large text corpora. Our approach
combines these two orthogonal sources of in-
formation and directly integrates them into
our paraphrasing system?s log-linear model.
We compare different distributional similar-
ity feature-sets and show significant improve-
ments in grammaticality and meaning reten-
tion on the example text-to-text generation
task of sentence compression, achieving state-
of-the-art quality.
1 Introduction
A wide variety of applications in natural language
processing can be cast in terms of text-to-text gen-
eration. Given input in the form of natural lan-
guage, a text-to-text generation system produces
natural language output that is subject to a set of
constraints. Compression systems, for instance, pro-
duce shorter sentences. Paraphrases, i.e. differ-
ing textual realizations of the same meaning, are a
crucial components of text-to-text generation sys-
tems, and have been successfully applied to tasks
such as multi-document summarization (Barzilay et
al., 1999; Barzilay, 2003), query expansion (An-
ick and Tipirneni, 1999; Riezler et al, 2007), ques-
tion answering (McKeown, 1979; Ravichandran and
Hovy, 2002), sentence compression (Cohn and La-
pata, 2008; Zhao et al, 2009), and simplification
(Wubben et al, 2012).
Paraphrase collections for text-to-text generation
have been extracted from a variety of different cor-
pora. Several approaches rely on bilingual paral-
lel data (Bannard and Callison-Burch, 2005; Zhao
et al, 2008; Callison-Burch, 2008; Ganitkevitch et
al., 2011), while others leverage distributional meth-
ods on monolingual text corpora (Lin and Pantel,
2001; Bhagat and Ravichandran, 2008). So far, how-
ever, only preliminary studies have been undertaken
to combine the information from these two sources
(Chan et al, 2011).
In this paper, we describe an extension of Gan-
itkevitch et al (2011)?s bilingual data-based ap-
proach. We augment the bilingually-sourced para-
phrases using features based on monolingual distri-
butional similarity. More specifically:
? We show that using monolingual distributional
similarity features improves paraphrase quality
beyond what we can achieve with features esti-
mated from bilingual data.
? We define distributional similarity for para-
phrase patterns that contain constituent-level
gaps, e.g.
sim(one JJ instance of NP , a JJ case of NP).
This generalizes over distributional similarity
for contiguous phrases.
? We compare different types of monolingual
distributional information and show that they
can be used to achieve significant improve-
ments in grammaticality.
? Finally, we compare our method to several
strong baselines on the text-to-text generation
task of sentence compression. Our method
shows state-of-the-art results, beating a purely
bilingually sourced paraphrasing system.
256
... ihre Pl?ne w?rden
their plans would
...
ohne aufzugeben
without
langfristigen
in the long term
langfristigen
long-term
...
... ...
...Pl?ne
plans
seine
giving up his
......
Figure 1: Pivot-based paraphrase extraction for con-
tiguous phrases. Two phrases translating to the same
phrase in the foreign language are assumed to be
paraphrases of one another.
2 Background
Approaches to paraphrase extraction differ based on
their underlying data source. In Section 2.1 we out-
line pivot-based paraphrase extraction from bilin-
gual data, while the contextual features used to de-
termine closeness in meaning in monolingual ap-
proaches is described in Section 2.2.
2.1 Paraphrase Extraction via Pivoting
Following Ganitkevitch et al (2011), we formulate
our paraphrases as a syntactically annotated syn-
chronous context-free grammar (SCFG) (Aho and
Ullman, 1972; Chiang, 2005). An SCFG rule has
the form:
r = C ? ?f, e,?, ~??,
where the left-hand side of the rule, C, is a nonter-
minal and the right-hand sides f and e are strings
of terminal and nonterminal symbols. There is a
one-to-one correspondency between the nontermi-
nals in f and e: each nonterminal symbol in f has
to also appear in e. The function ? captures this bi-
jective mapping between the nonterminals. Drawing
on machine translation terminology, we refer to f as
the source and e as the target side of the rule.
Each rule is annotated with a feature vector of fea-
ture functions ~? = {?1...?N} that, using a corre-
sponding weight vector ~?, are combined in a log-
linear model to compute the cost of applying r:
cost(r) = ?
N?
i=1
?i log?i. (1)
A wide variety of feature functions can be formu-
lated. We detail the feature-set used in our experi-
ments in Section 4.
NP NN
NP
EU
NN NP
NP
intentions
's
EUder 
......?
h i
's
in the long term
in the long term 
langfristigen Pl?ne
the long-term of
Europeofthe long-term plans
IBM goals
IBM 's
's in the long term 
l?ngerfristige Ziele
IBMofthe long-term ambitions
..
Figure 2: Extraction of syntactic paraphrases via the
pivoting approach: We aggregate over different sur-
face realizations, matching the lexicalized portions
of the rule and generalizing over the nonterminals.
To extract paraphrases we follow the intuition that
two English strings e1 and e2 that translate to the
same foreign string f can be assumed to have the
same meaning, as illustrated in Figure 1.1
First, we use standard machine translation meth-
ods to extract a foreign-to-English translation gram-
mar from a bilingual parallel corpus (Koehn, 2010).
Then, for each pair of translation rules where the
left-hand side C and foreign string f match:
r1 = C ? ?f, e1,?1, ~?1?
r2 = C ? ?f, e2,?2, ~?2?,
we pivot over f to create a paraphrase rule rp:
rp = C ? ?e1, e2,?p, ~?p?,
with a combined nonterminal correspondency func-
tion ?p. Note that the common source side f im-
plies that e1 and e2 share the same set of nonterminal
symbols.
The paraphrase feature vector ~?p is computed
from the translation feature vectors ~?1 and ~?2 by
following the pivoting idea. For instance, we esti-
mate the conditional paraphrase probability p(e2|e1)
by marginalizing over all shared foreign-language
translations f :
p(e2|e1) =
?
f
p(e2, f |e1) (2)
=
?
f
p(e2|f, e1)p(f |e1) (3)
?
?
f
p(e2|f)p(f |e1). (4)
1See Yao et al (2012) for an analysis of this assumption.
257
twelve cartoons insulting the prophet mohammad
CD NNS JJ DT
NNP
NP
NP
VP
NP
DT+NNP
12 the prophet mohammad
CD NNS JJ
DT
NNP
NP
NP
VP
NP
DT+NNP
cartoons offensiveof the that are to
Figure 3: An example of a synchronous paraphras-
tic derivation, here a sentence compression. Shaded
words are deleted in the indicated rule applications.
Figure 2 illustrates syntax-constrained pivoting and
feature aggregation over multiple foreign language
translations for a paraphrase pattern.
After the SCFG has been extracted, it can be used
within standard machine translation machinery, such
as the Joshua decoder (Ganitkevitch et al, 2012).
Figure 3 shows an example for a synchronous para-
phrastic derivation produced as a result of applying
our paraphrase grammar in the decoding process.
The approach outlined relies on aligned bilingual
texts to identify phrases and patterns that are equiva-
lent in meaning. When extracting paraphrases from
monolingual text, we have to rely on an entirely dif-
ferent set of semantic cues and features.
2.2 Monolingual Distributional Similarity
Methods based on monolingual text corpora mea-
sure the similarity of phrases based on contextual
features. To describe a phrase e, we define a set of
features that capture the context of an occurrence of
e in our corpus. Writing the context vector for the
i-th occurrence of e as ~se,i, we can aggregate over
all occurrences of e, resulting in a distributional sig-
nature for e, ~se =
?
i ~se,i. Following the intuition
that phrases with similar meanings occur in similar
contexts, we can then quantify the goodness of e? as
a paraphrase of e by computing the cosine similarity
between their distributional signatures:
sim(e, e?) = ~se ? ~se?
|~se||~se? |
.
A wide variety of features have been used to de-
scribe the distributional context of a phrase. Rich,
linguistically informed feature-sets that rely on de-
pendency and constituency parses, part-of-speech
tags, or lemmatization have been proposed in widely
known work such as by Church and Hanks (1991)
and Lin and Pantel (2001). For instance, a phrase
is described by the various syntactic relations it has
with lexical items in its context, such as: ?for what
verbs do we see with the phrase as the subject??, or
?what adjectives modify the phrase??.
However, when moving to vast text collections or
collapsed representations of large text corpora, lin-
guistic annotations can become impractically expen-
sive to produce. A straightforward and widely used
solution is to fall back onto lexical n-gram features,
e.g. ?what words or bigrams have we seen to the left
of this phrase?? A substantial body of work has fo-
cussed on using this type of feature-set for a variety
of purposes in NLP (Lapata and Keller, 2005; Bha-
gat and Ravichandran, 2008; Lin et al, 2010; Van
Durme and Lall, 2010).
2.3 Other Related Work
Recently, Chan et al (2011) presented an initial in-
vestigation into combining phrasal paraphrases ob-
tained through bilingual pivoting with monolingual
distributional information. Their work investigated
a reranking approach and evaluated their method via
a substitution task, showing that the two sources of
information are complementary and can yield im-
provements in paraphrase quality when combined.
3 Incorporating Distributional Similarity
In order to incorporate distributional similarity in-
formation into the paraphrasing system, we need
to calculate similarity scores for the paraphrastic
SCFG rules in our grammar. For rules with purely
lexical right-hand sides e1 and e2 this is a simple
task, and the similarity score sim(e1, e2) can be di-
rectly included in the rule?s feature vector ~?. How-
ever, if e1 and e2 are long, their occurrences become
sparse and their similarity can no longer be reliably
estimated. In our case, the right-hand sides of our
rules often contain gaps and computing a similarity
score is less straightforward.
Figure 4 shows an example of such a discontin-
uous rule and illustrates our solution: we decom-
pose the discontinuous patterns that make up the
258
NP
the's NP
ofNP
long
long-term
term
the
inNN
NN
the long-term
in the long term
's
of
?
+ sim
? ?!
sim(r) = 12
 
sim
?
Figure 4: Scoring a rule by extracting and scoring
contiguous phrases consistent with the alignment.
The overall score of the rule is determined by av-
eraging across all pairs of contiguous subphrases.
right-hand sides of a rule r into pairs of contiguous
phrases P(r) = {?e, e??}, for which we can look
up distributional signatures and compute similarity
scores. This decomposition into phrases is non-
trivial, since our sentential paraphrase rules often
involve significant reordering or structural changes.
To avoid comparing unrelated phrase pairs, we re-
quire P(r) to be consistent with a token alignment
a. The alignment is defined analogously to word
alignments in machine translation, and computed by
treating the source and target sides of our paraphrase
rules as a parallel corpus.
We define the overall similarity score of the rule
to be the average of the similarity scores of all ex-
tracted phrase pairs:
sim(r,a) = 1
|P(a)|
?
(e,e?)?P(a)
sim(e, e?).
Since the distributional signatures for long, rare
phrases may be computed from only a handful of
occurrences, we additionally query for the shorter
sub-phrases that are more likely to have been ob-
served often enough to have reliable signatures and
thus similarity estimates.
Our definition of the similarity of two discon-
tinuous phrases substantially differs from others in
the literature. This difference is due to a differ-
ence in motivation. Lin and Pantel (2001), for in-
stance, seek to find new paraphrase pairs by compar-
ing their arguments. In this work, however, we try
to add orthogonal information to existing paraphrase
pairs. Both our definition of pattern similarity and
our feature-set (see Section 4.3) are therefore geared
towards comparing the substitutability and context
similarity of a pair of paraphrases.
Our two similarity scores are incorporated into
the paraphraser as additional rule features in ~?,
simngram and simsyn , respectively. We estimate the
corresponding weights along with the other ?i as de-
tailed in Section 4.
4 Experimental Setup
4.1 Task: Sentence Compression
To evaluate our method on a real text-to-text appli-
cation, we use the sentence compression task. To
tune the parameters of our paraphrase system for
sentence compression, we need an appropriate cor-
pus of reference compressions. Since our model is
designed to compress by paraphrasing rather than
deletion, the commonly used deletion-based com-
pression data sets like the Ziff-Davis corpus are not
suitable. We thus use the dataset introduced in our
previous work (Ganitkevitch et al, 2011).
Beginning with 9570 tuples of parallel English?
English sentences obtained from multiple reference
translations for machine translation evaluation, we
construct a parallel compression corpus by select-
ing the longest reference in each tuple as the source
sentence and the shortest reference as the target sen-
tence. We further retain only those sentence pairs
where the compression ratio cr falls in the range
0.5 < cr ? 0.8. From these, we select 936 sen-
tences for the development set, as well as 560 sen-
tences for a test set that we use to gauge the perfor-
mance of our system.
We contrast our distributional similarity-informed
paraphrase system with a pivoting-only baseline, as
well as an implementation of Clarke and Lapata
(2008)?s state-of-the-art compression model which
uses a series of constraints in an integer linear pro-
gramming (ILP) solver.
4.2 Baseline Paraphrase Grammar
We extract our paraphrase grammar from the
French?English portion of the Europarl corpus (ver-
sion 5) (Koehn, 2005). The Berkeley aligner (Liang
et al, 2006) and the Berkeley parser (Petrov and
Klein, 2007) are used to align the bitext and parse
the English side, respectively. The paraphrase gram-
mar is produced using the Hadoop-based Thrax
259
the long-term
achieve25
goals 23
plans 97
investment 10
confirmed64
revise43
Left Right
the long-term
the long-term
the long-term
the long-term
the long-term
.
.
.
.
L-achieve = 25
L-confirmed
= 64
L-revise = 43
?
R-goals 
= 23
R-plans  = 97
R-investment 
= 10
?
the long-term
?
=~signgram
?
Figure 5: An example of the n-gram feature extrac-
tion on an n-gram corpus. Here, ?the long-term? is
seen preceded by ?revise? (43 times) and followed
by ?plans? (97 times). The corresponding left- and
right-side features are added to the phrase signature
with the counts of the n-grams that gave rise to them.
grammar extractor?s paraphrase mode (Ganitkevitch
et al, 2012). The syntactic nonterminal labels we
allowed in the grammar were limited to constituent
labels and CCG-style slashed categories. Paraphrase
grammars extracted via pivoting tend to grow very
large. To keep the grammar size manageable, we
pruned away all paraphrase rules whose phrasal
paraphrase probabilities p(e1|e2) or p(e2|e1) were
smaller than 0.001.
We extend the feature-set used in Ganitkevitch et
al. (2011) with a number of features that aim to bet-
ter describe a rule?s compressive power: on top of
the word count features wcountsrc and wcount tgt
and the word count difference feature wcountdiff ,
we add character based count and difference features
ccountsrc , ccount tgt , and ccountdiff , as well as log-
compression ratio features wordcr = log wcount tgtwcountsrc
and the analogously defined charcr = log ccount tgtccountsrc .
For model tuning and decoding we used the
Joshua machine translation system (Ganitkevitch et
al., 2012). The model weights are estimated using an
implementation of the PRO tuning algorithm (Hop-
kins and May, 2011), with PRE?CIS as our objective
function (Ganitkevitch et al, 2011). The language
model used in our paraphraser and the Clarke and
Lapata (2008) baseline system is a Kneser-Ney dis-
counted 5-gram model estimated on the Gigaword
corpus using the SRILM toolkit (Stolcke, 2002).
long-term investment holding on to
det
amod
the
JJ NN VBG IN TO DT
NP
PP
VP
? ?
the long-term
?
=~sigsyntax
?
dep-det-R-investment
pos-L-TO 
pos-R-NN  
lex-R-investment 
lex-L-to 
dep-amod-R-investment
syn-gov-NP syn-miss-L-NN 
lex-L-on-to 
pos-L-IN-TO  
dep-det-R-NN dep-amod-R-NN
Figure 6: An example of the syntactic feature-
set. The phrase ?the long-term? is annotated with
position-aware lexical and part-of-speech n-gram
features (e.g. ?on to? on the left, and ?investment?
and ?NN? to its right), labeled dependency links
(e.g. amod ? investment) and features derived
from the phrase?s CCG label NP/NN .
4.3 Distributional Similarity Model
To investigate the impact of the feature-set used to
construct distributional signatures, we contrast two
approaches: a high-coverage collection of distribu-
tional signatures with a relatively simple feature-set,
and a much smaller set of signatures with a rich, syn-
tactically informed feature-set.
4.3.1 n-gram Model
The high-coverage model (from here on: n-gram
model) is drawn from a web-scale n-gram corpus
(Brants and Franz, 2006; Lin et al, 2010). We ex-
tract signatures for phrases up to a length of 4. For
each phrase p we look at n-grams of the form wp
and pv, where w and v are single words. We then
extract the corresponding features wleft and vright .
The feature count is set to the count of the n-gram,
reflecting the frequency with which p was preceded
or followed, respectively, by w and v in the data the
n-gram corpus is based on. Figure 5 illustrates this
feature extraction approach. The resulting collection
comprises distributional signatures for the 200 mil-
lion most frequent 1-to-4-grams in the n-gram cor-
pus.
260
4.3.2 Syntactic Model
For the syntactically informed signature model
(from here on: syntax model), we use the
constituency and dependency parses provided in
the Annotated Gigaword corpus (Napoles et al,
2012). We limit ourselves to the Los Angeles
Times/Washington Post portion of the corpus and
extract phrases up to a length of 4. The following
feature set is used to compute distributional signa-
tures for the extracted phrases:
? Position-aware lexical and part-of-speech uni-
gram and bigram features, drawn from a three-
word window to the right and left of the phrase.
? Features based on dependencies for both links
into and out of the phrase, labeled with the cor-
responding lexical item and POS. If the phrase
corresponds to a complete subtree in the con-
stituency parse we additionally include lexical
and POS features for its head word.
? Syntactic features for any constituents govern-
ing the phrase, as well as for CCG-style slashed
constituent labels for the phrase. The latter are
split in governing constituent and missing con-
stituent (with directionality).
Figure 6 illustrates the syntax model?s feature ex-
traction for an example phrase occurrence. Using
this method we extract distributional signatures for
over 12 million 1-to-4-gram phrases.
4.3.3 Locality Sensitive Hashing
Collecting distributional signatures for a large
number of phrases quickly leads to unmanageably
large datasets. Storing the syntax model?s 12 mil-
lion signatures in a compressed readable format,
for instance, requires over 20GB of disk space.
Like Ravichandran et al (2005) and Bhagat and
Ravichandran (2008), we rely on locality sensitive
hashing (LSH) to make the use of these large collec-
tions practical.
In order to avoid explicitly computing the fea-
ture vectors, which can be memory intensive for fre-
quent phrases, we chose the online LSH variant de-
scribed by Van Durme and Lall (2010), as imple-
mented in the Jerboa toolkit (Van Durme, 2012).
This method, based on the earlier work of Indyk and
Motwani (1998) and Charikar (2002), approximates
the cosine similarity between two feature vectors
based on the Hamming distance in a dimensionality-
reduced bitwise representation. Two feature vec-
tors u, v each of dimension d are first projected
through a d?b random matrix populated with draws
from N (0, 1). We then convert the resulting b-
dimensional vectors into bit-vectors by setting each
bit of the signature conditioned on whether the cor-
responding projected value is less than 0. Now,
given the bit signatures h(~u) and h(~v), we can ap-
proximate the cosine similarity of u and v as:
sim ?(u, v) = cos
(D(h(~u), h(~v))
b pi
)
,
where d(?, ?) is the Hamming distance. In our ex-
periments we use 256-bit signatures. This reduces
the memory requirements for the syntax model to
around 600MB.
5 Evaluation Results
To rate the quality of our output, we solicit human
judgments of the compressions along two five-point
scales: grammaticality and meaning preservation.
Judges are instructed to decide how much the mean-
ing from a reference translation is retained in the
compressed sentence, with a score of 5 indicating
that all of the important information is present, and
1 being that the compression does not retain any of
the original meaning. Similarly, a grammar score
of 5 indicates perfect grammaticality, while a score
of 1 is assigned to sentences that are entirely un-
grammatical. We ran our evaluation on Mechani-
cal Turk, where a total of 126 judges provided 3 re-
dundant judgments for each system output. To pro-
vide additional quality control, our HITs were aug-
mented with both positive and negative control com-
pressions. For the positive control we used the refer-
ence compressions from our test set. Negative con-
trol was provided by adding a compression model
based on random word deletions to the mix.
In Table 1 we compare our distributional
similarity-augmented systems to the plain pivoting-
based baseline and the ILP approach. The compres-
sion ratios of the paraphrasing systems are tuned to
match the average compression ratio seen on the de-
velopment and test set. The ILP system is config-
261
ured to loosely match this ratio, as to not overly con-
strain its search space. Our results indicate that the
paraphrase approach significantly outperforms ILP
on meaning retention. However, the baseline sys-
tem shows notable weaknesses in grammaticality.
Adding the n-gram distributional similarity model
to the paraphraser recovers some of the difference in
grammaticality while simultaneously yielding some
gain in the compressions? meaning retention. Mov-
ing to distributional similarity estimated on the syn-
tactic feature-set yields additional improvement, de-
spite the model?s lower coverage.
It is known that human evaluation scores correlate
linearly with the compression ratio produced by a
sentence compression system (Napoles et al, 2011).
Thus, to ensure fairness in our comparisons, we pro-
duce a pairwise comparison breakdown that only
takes into account compressions of almost identical
length.2 Figure 7 shows the results of this analysis,
detailing the number of wins and ties in the human
judgements.
We note that the gains in meaning retention over
both the baseline and the ILP system are still present
in the pairwise breakdown. The gains over the
paraphrasing baseline, as well as the improvement
in meaning over ILP are statistically significant at
p < 0.05 (using the sign test).
We can observe that there is substantial overlap
between the baseline paraphraser and the n-gram
model, while the syntax model appears to yield no-
ticeably different output far more often.
Table 2 shows two example sentences drawn from
our test set and the compressions produced by the
different systems. It can be seen that both the
paraphrase-based and ILP systems produce good
quality results, with the paraphrase system retaining
the meaning of the source sentence more accurately.
6 Conclusion
We presented a method to incorporate monolingual
distributional similarity into linguistically informed
paraphrases extracted from bilingual parallel data.
Having extended the notion of similarity to dis-
contiguous pattern with multi-word gaps, we inves-
tigated the effect of using feature-sets of varying
2We require the compressions to be within ?10% length of
one another.
Score 0
50100
150200
250300
050
100150
200250
300
Syntax :: ILP Syntax :: n?gram n?gram :: PP
Grammar
Meaning
Figure 7: A pairwise breakdown of the human judg-
ments comparing the systems. Dark grey regions
show the number of times the two systems were tied,
and light grey shows how many times one system
was judged to be better than the other.
CR Meaning Grammar
Reference 0.80 4.80 4.54
ILP 0.74 3.44 3.41
PP 0.78 3.53 2.98
PP + n-gram 0.80 3.65 3.16
PP + syntax 0.79 3.70 3.26
Random Deletions 0.78 2.91 2.53
Table 1: Results of the human evaluation on longer
compressions: pairwise compression rates (CR),
meaning and grammaticality scores. Bold indicates
a statistically significance difference at p < 0.05.
complexity to compute distributional similarity for
our paraphrase collection. We conclude that, com-
pared to a simple large-scale model, a rich, syntax-
based feature-set, even with significantly lower cov-
erage, noticeably improves output quality in a text-
to-text generation task. Our syntactic method sig-
nificantly improves grammaticality and meaning re-
tention over a strong paraphrastic baseline, and of-
fers substantial gains in meaning retention over a
deletion-based state-of-the-art system.
Acknowledgements This research was supported
in part by the NSF under grant IIS-0713448 and
in part by the EuroMatrixPlus project funded by
the European Commission (7th Framework Pro-
gramme). Opinions, interpretations, and conclu-
sions are the authors? alone.
262
Source should these political developments have an impact on sports ?
Reference should these political events affect sports ?
Syntax should these events have an impact on sports ?
n-gram these political developments impact on sports ?
PP should these events impact on sports ?
ILP political developments have an impact
Source now we have to think and make a decision about our direction and choose only one way .
thanks .
Reference we should ponder it and decide our path and follow it , thanks .
Syntax now we think and decide on our way and choose one way . thanks .
n-gram now we have and decide on our way and choose one way . thanks .
PP now we have and decide on our way and choose one way . thanks .
ILP we have to think and make a decision and choose way thanks
Table 2: Example compressions produced by our systems and the baselines Table 1 for three input sentences
from our test data.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Peter G. Anick and Suresh Tipirneni. 1999. The para-
phrase search assistant: terminological feedback for
iterative information seeking. In Proceedings of SI-
GIR.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings of
ACL.
Regina Barzilay. 2003. Information Fusion for Mutli-
document Summarization: Paraphrasing and Genera-
tion. Ph.D. thesis, Columbia University, New York.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van
Durme. 2011. Reranking bilingually extracted para-
phrases using monolingual distributional similarity. In
EMNLP Workshop on GEMS.
Moses Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of STOC.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
Kenneth Church and Patrick Hanks. 1991. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 6(1):22?29.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
COLING.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
263
and Chris Callison-Burch. 2012. Joshua 4.0: Packing,
PRO, and paraphrases. In Proceedings of WMT12.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2(1).
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT/NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Kathleen R. McKeown. 1979. Paraphrasing using given
and new information in a question-answer system. In
Proceedings of ACL.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphrastic
sentence compression with a character-based metric:
Tightening without deletion. Workshop on Monolin-
gual Text-To-Text Generation.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
sufrace text patterns for a question answering system.
In Proceedings of ACL.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized Algorithms and NLP: Using Lo-
cality Sensitive Hash Functions for High Speed Noun
Clustering. In Proceedings of ACL.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of ACL.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceeding of the International
Conference on Spoken Language Processing.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.
Benjamin Van Durme. 2012. Jerboa: A toolkit for
randomized and streaming algorithms. Technical Re-
port 7, Human Language Technology Center of Excel-
lence, Johns Hopkins University.
Sander Wubben, Antal van den Bosch, and Emiel Krah-
mer. 2012. Sentence simplification by monolingual
machine translation. In Proceedings of ACL.
Xuchen Yao, Benjamin Van Durme, and Chris Callison-
Burch. 2012. Expectations of word sense in parallel
corpora. In Proceedings of HLT/NAACL.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In Proceedings of
ACL/HLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of ACL.
264
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 41?48
Manchester, August 2008
Affinity Measures based on the Graph Laplacian
Delip Rao
Dept. of Computer Science
Johns Hopkins University
delip@cs.jhu.edu
David Yarowsky
Dept. of Computer Science
Johns Hopkins University
yarowsky@cs.jhu.edu
Chris Callison-Burch
Dept. of Computer Science
Johns Hopkins University
ccb@cs.jhu.edu
Abstract
Several language processing tasks can be
inherently represented by a weighted graph
where the weights are interpreted as a
measure of relatedness between two ver-
tices. Measuring similarity between ar-
bitary pairs of vertices is essential in solv-
ing several language processing problems
on these datasets. Random walk based
measures perform better than other path
based measures like shortest-path. We
evaluate several random walk measures
and propose a new measure based on com-
mute time. We use the psuedo inverse
of the Laplacian to derive estimates for
commute times in graphs. Further, we
show that this pseudo inverse based mea-
sure could be improved by discarding the
least significant eigenvectors, correspond-
ing to the noise in the graph construction
process, using singular value decomposi-
tion.
1 Introduction
Natural language data lend themselves to a graph
based representation. Words could be linked by
explicit relations as in WordNet (Fellbaum, 1989)
or documents could be linked to one another via
hyperlinks. Even in the absence of such a straight-
forward representation it is possible to derive
meaningful graphs such as the nearest neighbor
graphs as done in certain manifold learning meth-
ods (Roweis and Saul, 2000; Belkin and Niyogi,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
2001). All of these graphs share the following
properties:
? They are edge-weighted.
? The edge weight encodes some notion of re-
latedness between the vertices.
? The relation represented by edges is at least
weakly transitive. Examples of such rela-
tions include, ?is similar to?, ?is more general
than?, and so on. It is important that the re-
lations selected are transitive for the random
walk to make sense.
Such graphs present several possibilities in solv-
ing language problems on the data. One such task
is, given two vertices in the graph we would like
to know how related the two vertices are. There
is an abundance of literature on this topic, some
of which will be reviewed here. Finding similarity
between vertices in a graph could be an end in it-
self, as in the lexical similarity task, or could be a
stage before solving other problems like clustering
and classification.
2 Contributions of this paper
The major contributions of this paper are
? A comprehensive evaluation of various ran-
dom walk based measures
? Propose a new similarity measure based on
commute time.
? An improvement to the above measure by
eliminating noisy features via singular value
decomposition.
41
3 Problem setting
Consider an undirected graph G(V,E,W) with
vertices V , edges E, and W = [w
ij
] be the sym-
metric adjacency weight matrix with w
ij
as the
weight of the edge connecting vertices i and j. The
weight, w
ij
= 0 for vertices i and j that are not
neighbors and when w
ij
> 0 it is interpreted as an
indication of relatedness between i and j. In our
case, we consider uniformly weighted graphs, i.e,
w
ij
= 1 for neighbors but this need not be the case.
Let n = |V | be the order of the graph. We define
a relation sim : V ? V ? R
+
such that sim(i, j)
is the relatedness between vertices i and j. There
are several ways to define sim; the ones explored
in this paper are:
? sim
G
(i, j) is the reciprocal of the shortest
path length between vertices i and j. Note
that this is not a random walk based mea-
sure but a useful baseline for comparison pur-
poses.
? sim
B
(i, j) is the probability of a random walk
from vertex i to vertex j using all paths of
length less than m.
? sim
P
(i, j) is the probability of a random walk
from vertex i to vertex j defined via a pager-
ank model.
? sim
C
(i, j) is a function of the commute time
between vertex i and vertex j.
4 Data and Evaluation
We evaluate each of the similarity measure we
consider by using a linguistically motivated task
of finding lexical similarity. Deriving lexical
relatedness between terms has been a topic of
interest with applications in word sense disam-
biguation (Patwardhan et al, 2005), paraphras-
ing (Kauchak and Barzilay, 2006), question an-
swering (Prager et al, 2001), and machine trans-
lation (Blatz et al, 2004) to name a few. Lex-
ical relatedness between terms could be derived
either from a thesaurus like WordNet or from
raw monolingual corpora via distributional simi-
larity (Pereira et al, 1993). WordNet is an inter-
esting graph-structured thesaurus where the ver-
tices are the words and the edges represent rela-
tions between the words. For the purpose of this
work, we only consider relations like hypernymy,
hyponymy, and synonymy. The importance of this
problem has generated copious literature in the
past ? see (Pedersen et al, 2004) or (Budanitsky
and Hirst, 2006) for a detailed review of various
lexical relatedness measures on WordNet. Our fo-
cus in this paper is not to derive the best similar-
ity measure for WordNet but to use WordNet and
the lexical relatedness task as a method to evalu-
ate the various random walk based similarity mea-
sures. Following the tradition in previous litera-
ture we evaluate on the Miller and Charles (1991)
dataset. This data consists of 30 word-pairs along
with human judgements which is a real value be-
tween 1 and 4. For every measure we consider,
we derive similarity scores and compare with the
human judgements using the Spearman rank cor-
relation coefficient.
5 Graph construction
For the purpose of evaluation of the random walk
measures, we construct a graph for every pair of
words for which similarity has to be computed.
This graph is derived from WordNet as follows:
? For each word w in the pair (w
1
, w
2
):
? Add an edge between w and all of its
parts of speech. For example, if the word
is coast, add edges between coast and
coast#noun and coast#verb.
? For each word#pos combination,
add edges to all of its senses (For
example, coast#noun#1 through
coast#noun#4.
? For each word sense, add edges to all of
its hyponyms
? For each word sense, add edges to all of
its hypernyms recursively.
In this paper we consider uniform weights on all
edges as our main aim is to illustrate the differ-
ent random walk measures rather than fine tune the
graph construction process.
6 Shortest path based measure
The most obvious measure of distance in a graph is
the shortest path between the vertices which is de-
fined as the minimum number of intervening edges
between two vertices. This is also known as the
geodesic distance. To convert this distance mea-
sure to a similarity measure, we take the recipro-
cal of the shortest-path length. We refer to this as
the geodesic similarity. This is not a random walk
42
Figure 1: Shortest path distances on graphs
measure but will serve as an important baseline for
our work. As can be observed from Table 1, the
Method Spearman correlation
Geodesic 0.275
Table 1: Similarity using shortest-path measure.
correlation is rather poor for the shortest path mea-
sure.
7 Why are shortest path distances bad?
While shortest-path distances are useful in many
applications, it fails to capture the following obser-
vation. Consider the subgraph of WordNet shown
in Figure 1. The term moon is connected to the
terms religious leader and satellite
1
.
Observe that both religious leader and
satellite are at the same shortest path dis-
tance from moon. However, the connectivity
structure of the graph would suggest satellite
to be ?more? similar than religious leader
as there are multiple senses, and hence multiple
paths, connecting satellite and moon.
Thus it is desirable to have a measure that cap-
tures not only path lengths but also the connectiv-
ity structure of the graph. This notion is elegantly
captured using random walks on graphs.
7.1 Similarity via Random walks
A random walk is a stochastic process that consists
of a sequence of discrete steps taken at random de-
fined by a distribution. Random walks have inter-
esting connections to Brownian motion, heat diffu-
sion and have been used in semi-supervised learn-
ing ? for example, see (Zhu et al, 2003). Certain
properties of random walks are defined for ergodic
processes only
2
. In our work, we assume these
1
The religious leader sense of moon is due to Sun
Myung Moon, a US religious leader.
2
A stochastic process is ergodic if the underlying Markov
chain is irreducible and aperiodic. A Markov chain is irre-
hold true as the graphs we deal with are connected,
undirected, and non-bipartite.
7.1.1 Bounded length walks
As our first random walk measure, we consider
the bounded length walk ? i.e., all random walks of
length less than or equal to a bound m. We derive
a probability transition matrix P from the weight
matrix W as follows:
P = D
?1
W
where, D is a diagonal matrix with d
ii
=
?
n
j = 1
w
ij
. Observe that:
? p
ij
= P[i, j] ? 0, and
?
?
n
j = 1
p
ij
= 1
Hence p
ij
can be interpreted as the probability
of transition from vertex i to vertex j in one step. It
is easy to observe that P
k
gives the transition prob-
ability from vertex i to vertex j in k steps. This
leads to the following similarity measure:
S = P + P
2
+ P
3
+ ...+ P
m
Observe that S[i, j] derives the total probability of
transition from vertex i to vertex j in at most m
steps
3
. Given S, we can derive several measures of
similarity:
1. Bounded Walk: S[i, j]
2. Bounded Walk Cosine: dot product of
rowvectors S
i
and S
j
.
When we evaluate these measures on the Miller-
Charles data the results shown in Table 2. are ob-
served. For this experiment, we consider all walks
that are at most 20 steps long, i.e., m = 20. Ob-
serve that these results are significantly better than
the Geodesic similarity based on shortest-paths.
ducible if there exists a path between any two states and it is
aperiodic if the GCD of all cycle lengths is one.
3
The matrix S is row normalized to ensure that the entries
can be interpreted as probabilities.
43
Method Spearman correlation
Bounded Walk 0.346
Bounded Walk Cosine 0.365
Table 2: Similarity using bounded random walks
(m = 20).
7.1.2 How many paths are sufficient?
In the previous experiment, we arbitrarily fixed
m = 20. However, as observed in Figure 2. , be-
yond a certain value the choice of m does not affect
the result as the random walk converges to its sta-
tionary distribution. The choice of m depends on
Figure 2: Effect of m in Bounded walk
the amount of computation available. A reason-
ably large value of m (m > 10) should be suffi-
cient for most purposes and one could use lower
values of m to derive an approximation for this
measure. One could derive an upper bound on the
value of m using the mixing time of the underlying
Markov chain (Aldous and Fill, 2001).
7.1.3 Similarity via pagerank
Pagerank (Page et al, 1998) is the celebrated ci-
tation ranking algorithm that has been applied to
several natural language problems from summa-
rization (Erkan and Radev, 2004) to opinion min-
ing (Esuli and Sebastiani, 2007) to our task of
lexical relatedness (Hughes and Ramage, 2007).
Pagerank is yet another random walk model with a
difference that it allows the random walk to ?jump?
to its initial state with a nonzero probability (?).
Given the probability transition matrix P as defined
above, a stationary distribution vector for any ver-
tex (say i) could be derived as follows:
1. Let e
i
be a vector of all zeros with e
i
(i) = 1
2. Let v
0
= e
i
3. Repeat until ?v
t
? v
t?1
?
F
< ?
? v
t+1
= ?v
t
P + (1? ?)v
0
? t = t+ 1
4. Assign v
t+1
as the stationary distribution for
vertex i.
Armed with the stationary distribution vectors for
vertices i and j, we define pagerank similarity ei-
ther as the cosine of the stationary distribution vec-
tors or the reciprocal Jensen-Shannon (JS) diver-
gence
4
between them. Table 3. shows results on
the Miller-Charles data. We use ? = 0.1, the best
value on this data. Observe that these results are
Method Spearman correlation
Pagerank JS-Divergence 0.379
Pagerank Cosine 0.393
Table 3: Similarity via pagerank (? = 0.1).
better than the best bounded walk result. We fur-
ther note that our results are different from that
of (Hughes and Ramage, 2007) as they use exten-
sive feature engineering and weight tuning during
the graph generation process that we have not been
able to reproduce. Hence for simplicity we stuck to
a simpler graph generation process. Nevertheless,
the result in Table 3. is still useful as we are in-
terested in the performance of the various spectral
similarity measures rather than achieving the best
performance on the lexical relatedness task. The
graphs we use in all methods are identical making
comparisons across methods possible.
7.2 Similarity via Hitting Time
Given a graph with the transition probability ma-
trix P as defined above, the hitting time between
vertices i and j, denoted as h(i, j), is defined as
the expected number of steps taken by a random
walker to first encounter vertex j starting from ver-
tex i. This can be recursively defined as follows:
h(i, j) =
?
?
?
1 +
?
k : w
ik
> 0
p
ik
h(k, j) if i 6= j
0 if i = j
(1)
4
The Jensen-Shannon divergence between two distribu-
tions p and q is defined as D(p ? a)+D(q ? a), where D(. ?
.) is the Kullback-Liebler divergence and a = (p + q)/2.
Note that unlike KL-divergence this measure is symmetric.
See (Lin, 1991) for additional details.
44
The lower the hitting times of two vertices, the
more similar they are. It can be easily verified
that hitting time is not a symmetric relation hence
graph theory literature suggests another symmet-
ric measure ? the commute time.
5
The commute
time, c(i, j), is the expected number of steps taken
to leave vertex i, reach vertex j, and return back to
i. Thus,
c(i, j) = h(i, j) + h(j, i) (2)
Observe that, the commute time is a metric in that
it is positive definite, symmetric, and satisifies tri-
angle inequality. Hence, commute time could be
used as a distance measure as well. We derive a
similarity measure from this distance measure us-
ing the following lemma.
Lemma 1. For every edge (i, j), c(i, j) ? 2l
where l = |E|, the number of edges.
Proof. This can be easily observed by defining a
Markov chain on the edges with probability tran-
sition matrix Q with 2l states, such that Q
e
1
e
2
=
1/degree(e
1
? e
2
). Since this matrix is doubly
stochastic, the stationary distribution on this chain
will be uniform with a probability 1/2l. Now
c(i, j) = h(i, j)+h(j, i), is the expected time for a
walk to start at i, visit j, and return back to i. When
the stationary probability at each edge is 1/2l, this
expected time evaluates to 2l. Hence the commute
time can be at most 2l.
This lemma allows us to define a similarity mea-
sure as follows:
sim
C
(i, j) = 1?
c(i, j)
2l
(3)
Observe that the measure defined in Equation 3 is
a metric and further its range is defined in [0, 1].
We now only need a way to compute the commute
times to use Equation 3. One could compute the
hitting times and hence the commute times from
the Equations 1 and 2 using dynamic program-
ming, akin to shortest paths in graphs. In this pa-
per, we instead choose to derive commute times
via the graph Laplacian. This also allows us to
handle ?noise? in the graph construction process
which cannot be taken care by naive dynamic pro-
gramming.
5
Note that distance measures, in general, need not be sym-
metric but we interpret distance as proximity which mandates
symmetry.
Chandra et. al. (1989) show that the commute
time between two vertices is equal to the resis-
tance distance between them. Resistance distance,
as proposed by Klein and Randic (1993), is the
effective resistance between two vertices in the
electrical network represented by the graph, where
the edges have resistance 1/w
ij
. Xiao and Gut-
man (2003), show the relation between resistance
distances in graphs to the Laplacian spectrum, thus
enabling a way to derive commute times from the
graph Laplacian in closed form.
We now introduce graph Laplacians, which are
interesting in their own right besides being related
to commute time. The Laplacian of a graph could
be viewed as a discrete version of the Laplace-
Beltrami operator on Riemannian manifolds. It is
defined as
L = D ? W
The graph Laplacian has interesting properties and
a wide range of applications, in semi-supervised
learning (Zhu et al, 2003), non-linear dimension-
ality reduction (Roweis and Saul, 2000; Belkin and
Niyogi, 2001), and so on. See (Chung, 1997) for
a thorough introduction on Laplacians and their
properties. We depend on the fact that L is:
1. symmetric (since D and W are for undirected
graphs)
2. positive-semidefinite : since it is symmet-
ric, all of the eigenvalues are real and by
the Greshgorin circle theorem, the eigenval-
ues must also be non-negative and hence L is
positive-semidefinite.
Throughout this paper we use normalized Lapla-
cians as defined below:
L = D
?1/2
LD
?1/2
= I ? D
?1/2
WD
?1/2
The normalized Laplacians preserve all properties
of the Laplacian by construction.
As noted in Xiao and Gutman (2003), the re-
sistance distances can be derived from the gener-
alized Moore-Penrose pseudo-inverse of the graph
Laplacian(L
?
) ? also called the inverse Laplacian.
Like Laplacians, their pseudo inverse counterparts
are also symmetric, and positive semi-definite.
Lemma 2. L
?
is symmetric
Proof. The Moore-Penrose pseudo-inverse is de-
fined as L
?
= (L
T
L)
?1
L
T
. From this definition,
it is clear that (L
?
)
T
= (L
T
)
?
. By the symmetry
45
property of graph Laplacians, L
T
= L. Hence,
(L
?
)
T
= L
?
.
Lemma 3. L
?
is positive semi-definite
Proof. We make use of the following properties
from (Chung, 1997):
? The Laplacian, L, is positive semi-definite
(also shown above).
? If the Eigen-decomposition of L is Q?Q
T
,
then the Eigen-decomposition of the pseudo-
inverse L
?
is Q?
?1
Q
T
. If any of the eigenval-
ues of L is zero then the corresponding eigen-
value for L
?
is also zero.
Since L is positive semi-definite, and the eigen-
values of L
?
have the same sign as L, the pseudo
inverse L
?
has to be positive semi-definite.
Lemma 4. The inverse Laplacian is a gram matrix
Proof. To prove this, we use the fact that the
Laplacian Matrix is symmetric and positive semi-
definite. Hence by Cholesky decomposition we
can write L = UU
T
.
Therefore L
?
= (U
T
)
?
U
?
= (U
?
)
T
(U
?
).
Hence L
?
is a matrix of dot-products or a gram-
matrix.
Thus, from Lemmas 2, 3 and 4, the inverse
Laplacian L
?
is a valid Kernel.
7.2.1 Similarity measures from the Laplacian
The pseudo inverse of the Laplacian allows us
to compute the following similarity measures.
1. Since L
?
is a kernel, L
?
ij
can be interpreted a
similarity value of vertices i and j.
2. Commute time: This is due to (Aldous and
Fill, 2001). The commute time, c(i, j) ?
(L
?
ii
+ L
?
jj
? 2L
?
ij
). This allows us to derive
similarities using Equation 3.
Evaluating the above measures with the Miller-
Charles data yields results shown in Table 4.
Again, these results are better than the other ran-
dom walk methods compared in the paper.
Method Spearman correlation
L
?
ij
0.469
Commute Time (sim
C
) 0.520
Table 4: Similarity via inverse Laplacian.
7.2.2 Noise in the graph construction process
The graph construction process outlined in Sec-
tion 5 is not necessarily the best one. In fact, any
method that constructs graphs from existing data
incorporates ?noise? or extraneous features. These
could be spurious edges between vertices, miss-
ing edges, or even improper edge weights. It is
however impossible to know any of this a priori
and some noise is inevitable. The derivation of
commute times via the pseudo inverse of a noisy
Laplacian matrix makes it even worse because the
pseudo inverse amplifies the noise in the original
matrix. This is because the largest singular value
of the pseudo inverse of a matrix is equal to the in-
verse of the smallest singular value of the original
matrix. A standard technique in signal processing
and information retrieval to eliminate noise or han-
dle missing values is to use singular value decom-
position (Deerwester et al, 1990). We apply SVD
to handle noise in the graph construction process.
For a given matrix A, SVD decomposes A into
three matrices U, S, and V such that A = USV
T
,
where S is a diagonal matrix of eigenvalues of A,
and U and V are orthonormal matrices containing
the left and the right eigenvectors respectively. The
top-k eigenvectors and eigenvalues are computed
using the iterative method by Lanczos-Arnoldi (us-
ing LAPACK) and the product of these matrices
represents a ?smoothed? version of the original
Laplacian. The pseudo inverse is then computed
on this smooth Laplacian. Table 5., shows the im-
provements obtained by discarding bottom 20% of
the eigenvalues.
Method Original After SVD
L
?
ij
0.469 0.472
Commute Time (sim
C
) 0.520 0.542
Table 5: Denoising graph Laplacian via SVD
Figure 3. shows the dependence on the num-
ber of eigenvalues selected. As can be observed in
both curves there is a reduction in performance by
adding the last few eigenvectors and hence may be
safely discarded. This observation is true in other
text processing tasks like document clustering or
classification using Latent Semantic Indexing.
8 Related Work
Apart from the related work cited throughout this
paper, we would also like to note the paper by Yen
46
Figure 3: Noise reduction via SVD.
et al(2007) on using sigmoid commute time kernel
on a graph for document clustering but our work
differs in that our goal was to study various ran-
dom walk measures rather than a specific task and
we provide a new similarity measure (ref. Eqn
3) based on an upper bound on the commute time
(Lemma 1). Our work also suggests a way to han-
dle noise in the graph construction process.
9 Conclusions and Future Work
This paper presented an evaluation of random
walk based similarity measures on weighted undi-
rected graphs. We provided an intuitive explana-
tion of why random walk based measures perform
better than shortest-path or geodesic measures,
and backed it with empirical evidence. The ran-
dom walk measures we consider include bounded
length walks, pagerank based measures, and a new
measure based on the commute times in graphs.
We derived the commute times via pseudo inverse
of the graph Laplacian. This enables a new method
of graph similarity using SVD that is robust to the
noise in the graph construction process. Further,
the inverse Laplacian is also interesting in that it is
a kernel by itself and could be used for other tasks
like word clustering, for example.
Acknowledgements
The authors would like to thank David Smith and
Petros Drineas for useful discussions and to Fan
Chung for the wonderful book on Spectral Graph
theory.
References
Aldous and Fill. 2001. Reversible Markov Chains and
Random Walks on Graphs. In preparation.
Belkin, Mikhail and Partha Niyogi. 2001. Laplacian
eigenmaps and spectral techniques for embedding
and clustering. In Proceedings of the NIPS.
Blatz, John, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto San-
chis, and Nicola Ueffing. 2004. Confidence estima-
tion for machine translation. In Proceeding of the
COLING.
Budanitsky, Alexander and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Chandra, Ashok, Prabhakar Raghavan, Walter Ruzzo,
Roman Smolensky, and Prasoon Tiwari. 1989. The
electrical resistance of a graph captures its commute
and cover times. In Proceedings of the STOC.
Chung, Fan. 1997. Spectral graph theory. In CBMS:
Conference Board of the Mathematical Sciences, Re-
gional Conference Series.
Deerwester, Scott, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41.
Erkan, G?unes and Dragomir Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence Re-
search (JAIR), 22:457?479.
Esuli, Andrea and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of the ACL, pages 424?431.
Fellbaum, Christaine, editor. 1989. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Hughes, Thad and Daniel Ramage. 2007. Lexical
semantic relatedness with random graph walks. In
Proceedings of the EMNLP.
Kauchak, David and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
HLT-NAACL.
Klein, D. and M. Randic. 1993. Resistance distance.
Journal of Mathematical Chemistry, 12:81?95.
Lin, Jianhua. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1).
Miller, G. and W. Charles. 1991. Contextual correlates
of semantic similarity. In Language and Cognitive
Process.
Page, Larry, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1998. The pagerank citation ranking:
Bringing order to the web. Technical report, Stan-
ford University, Stanford, CA.
47
Patwardhan, Siddharth, Satanjeev Banerjee, and Ted
Pedersen. 2005. Senserelate:: Targetword-A gen-
eralized framework for word sense disambiguation.
In Proceedings of the ACL.
Pedersen, Ted, Siddharth Patwardhan, and Jason
Michelizzi. 2004. Wordnet::similarity - measuring
the relatedness of concepts. In Proceedings of the
AAAI.
Pereira, Fernando, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the ACL.
Prager, John M., Jennifer Chu-Carroll, and Krzysztof
Czuba. 2001. Use of wordnet hypernyms for an-
swering what-is questions. In Proceedings of the
Text REtrieval Conference.
Roweis, Sam and Lawrence Saul. 2000. Nonlinear di-
mensionality reduction by locally linear embedding.
Science, 290:2323?2326.
Xiao, W. and I. Gutman. 2003. Resistance distance and
laplacian spectrum. Theoretical Chemistry Associa-
tion, 110:284?289.
Yen, Luh, Francois Fouss, Christine Decaestecker, Pas-
cal Francq, and Marco Saerens. 2007. Graph nodes
clustering based on the commute-time kernel. In
Proceedings of the PAKDD.
Zhu, Xiaojin, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the
ICML.
48
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 1?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Creating Speech and Language Data With Amazon?s Mechanical Turk
Chris Callison-Burch and Mark Dredze
Human Language Technology Center of Excellence
& Center for Language and Speech Processing
Johns Hopkins University
ccb,mdredze@cs.jhu.edu
Abstract
In this paper we give an introduction to us-
ing Amazon?s Mechanical Turk crowdsourc-
ing platform for the purpose of collecting
data for human language technologies. We
survey the papers published in the NAACL-
2010 Workshop. 24 researchers participated
in the workshop?s shared task to create data for
speech and language applications with $100.
1 Introduction
This paper gives an overview of the NAACL-2010
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk. A number of re-
cent papers have evaluated the effectiveness of us-
ing Mechanical Turk to create annotated data for
natural language processing applications. The low
cost, scalable workforce available through Mechan-
ical Turk (MTurk) and other crowdsourcing sites
opens new possibilities for annotating speech and
text, and has the potential to dramatically change
how we create data for human language technolo-
gies. Open questions include: What kind of research
is possible when the cost of creating annotated train-
ing data is dramatically reduced? What new tasks
should we try to solve if we do not limit ourselves to
reusing existing training and test sets? Can complex
annotation be done by untrained annotators? How
can we ensure high quality annotations from crowd-
sourced contributors?
To begin addressing these questions, we orga-
nized an open-ended $100 shared task. Researchers
were given $100 of credit on Amazon Mechanical
Turk to spend on an annotation task of their choos-
ing. They were required to write a short paper de-
scribing their experience, and to distribute the data
that they created. They were encouraged to ad-
dress the following questions: How did you convey
the task in terms that were simple enough for non-
experts to understand? Were non-experts as good as
experts? What did you do to ensure quality? How
quickly did the data get annotated? What is the cost
per label? Researchers submitted a 1 page proposal
to the workshop organizers that described their in-
tended experiments and expected outcomes. The
organizers selected proposals based on merit, and
awarded $100 credits that were generously provided
by Amazon Mechanical Turk. In total, 35 credits
were awarded to researchers.
Shared task participants were given 10 days to run
experiments between the distribution of the credit
and the initial submission deadline. 30 papers were
submitted to the shared task track, of which 24 were
accepted. 14 papers were submitted to the general
track of which 10 were accepted, giving a 77% ac-
ceptance rate and a total of 34 papers. Shared task
participants were required to provide the data col-
lected as part of their experiments. All of the shared
task data is available on the workshop website.
2 Mechanical Turk
Amazon?s Mechanical Turk1 is an online market-
place for work. Amazon?s tag line for Mechani-
cal Turk is artificial artificial intelligence, and the
name refers to a historical hoax from the 18th cen-
1http://www.mturk.com/
1
< 1
1-2
2-4
4-8
8-20
20-40
40+
< 1 HIT
1-5
5-10
10-20
20-50
50-100
100-200
200-500
500-1k
1k-5k
5k+ HITs
< $1
$1-5
$5-10
10-20
20-50
50-100
100-200
$200+
5%
17%
22%
26%
19%
10%
3%
100%
1%
6%
9%
13%
19%
17%
12%
13%
5%
4%
1%
100%
11%
36%
22%
15%
11%
4%
2%
0%
100%
0%
5%
10%
15%
20%
25%
30%
< 1 1-2 2-4 4-8 8-20 40+
Hours spent on Mechanical Turk per week
0%
5%
10%
15%
20%
< 1 HIT 5-10 20-50 100-200 500-1k 5k+ HITs
Number of HITs completed per week
0%
10%
20%
30%
40%
< $1 $5-10 20-50 100-200
Weekly income from Mechanical Turk
Figure 1: Time spent, HITs completed, and amount earned from a survey of 1,000 Turkers by Ipeirotis (2010).
tury where a chess-playing automaton appeared to
be able to beat human opponents using a mecha-
nism, but was, in fact, controlled by a person hiding
inside the machine. These hint at the the primary fo-
cus of the web service, which is to get people to per-
form tasks that are simple for humans but difficult
for computers. The basic unit of work on MTurk is
even called a Human Intelligence Task (HIT).
Amazon?s web service provides an easy way to
pay people small amounts of money to perform
HITs. Anyone with an Amazon account can either
submit HITs or work on HITs that were submitted
by others. Workers are referred to as ?Turkers? and
people designing the HITs are called ?Requesters.?
Requesters set the amount that they will pay for each
item that is completed. Payments are frequently as
low as $0.01. Turkers are free to select whichever
HITs interest them.], and to disregard HITs that they
find uninteresting or which they deem pay too little.
Because of its focus on tasks requiring human in-
telligence, Mechanical Turk is obviously applicable
to the field of natural language processing. Snow
et al (2008) used Mechanical Turk to inexpensively
collect labels for several NLP tasks including word
sense disambiguation, word similarity, textual en-
tailment, and temporal ordering of events. Snow et
al. had two exciting findings. First, they showed that
a strong correlation between non-expert and expert
annotators can be achieved by combining the judg-
ments of multiple non-experts, for instance by vot-
ing on each label using 10 different Turkers. Cor-
relation and accuracy of labeling could be further
improved by weighting each Turker?s vote by cal-
ibrating them on a small amount of gold standard
data created by expert annotators. Second, they col-
lected a staggering number of labels for a very small
amount of money. They collected 21,000 labels for
just over $25. Turkers put in over 140+ hours worth
Why do you complete tasks in MTurk? US India
To spend free time fruitfully and get
cash (e.g., instead of watching TV)
70% 60%
For ?primary? income purposes (e.g.,
gas, bills, groceries, credit cards)
15% 27%
For ?secondary? income purposes,
pocket change (for hobbies, gadgets)
60% 37%
To kill time 33% 5%
The tasks are fun 40% 20%
Currently unemployed or part time work 30% 27%
Table 1: Motivations for participating on Mechanical
Turk from a survey of 1,000 Turkers by Ipeirotis (2010).
of human effort to generate the labels. The amount
of participation is surprisingly high, given the small
payment.
Turker demographics
Given the amount of work that can get done for so
little, it is natural to ask: who would contribute so
much work for so little pay, and why? The answers
to these questions are often mysterious because
Amazon does not provide any personal informa-
tion about Turkers (each Turker is identifiable only
through a serial number like A23KO2TP7I4KK2).
Ipeirotis (2010) elucidates some of the reasons by
presenting a demographic analysis of Turkers. He
built a profile of 1000 Turkers by posting a survey to
MTurk and paying $0.10 for people to answer ques-
tions about their reasons for participating on Me-
chanical Turk, the amount that they earn each week,
and how much time they spend, as well as demo-
graphic information like country of origin, gender,
age, education level, and household income.
One suspicion that people often have when they
first hear about MTurk is that it is some sort of dig-
ital sweatshop that exploits workers in third world
countries. However, Ipeirotis reports that nearly half
2
(47%) of the Turkers who answered his survey were
from the United States, with the next largest group
(34%) coming from India, and the remaining 19%
spread between 66 other countries.
Table 1 gives the survey results for questions
relating to why people participate on Mechanical
Turk. It shows that most US-based workers use Me-
chanical Turk for secondary income purposes (to
have spending money for hobbies or going out),
but that the overwhelming majority of them use
it to spend their time more fruitfully (i.e., instead
of watching TV). The economic downturn may
have increased participation, with 30% of the US-
based Turkers reporting that they are unemployed
or underemployed. The public radio show Mar-
ketplace recently interviewed unemployed Turkers
(Rose, 2010). It reports that they earn a little in-
come, but that they do not earn enough to make a
living. Figure 1 confirms this, giving a break down
of how much time people spend on Mechanical Turk
each week, how many HITs they complete, and how
much money they earn. Most Turkers spend less
than 8 hours per week on Mechanical Turk, and earn
less than $10 per week through the site.
3 Quality Control
Ipeirotis (2010) reports that just over half of Turkers
have a college education. Despite being reasonably
well educated, it is important to keep in mind that
Turkers do not have training in specialized subjects
like NLP. Because the Turkers are non-experts, and
because the payments are generally so low, quality
control is an important consideration when creating
data with MTurk.
Amazon provides three mechanisms to help en-
sure quality:
? Requesters have the option of rejecting the
work of individual Turkers, in which case they
are not paid.2 Turkers can also be blocked from
doing future work for a requester.
2Since the results are downloadable even if they are rejected,
this could allow unscrupulous Requesters to abuse Turkers by
rejecting all of their work, even if it was done well. Turkers have
message boards at http://www.turkernation.com/,
where they discuss Requesters. They even have a Firefox plu-
gin called Turkopticon that lets them see ratings of how good
the Requesters are in terms of communicating with Turkers, be-
ing generous and fair, and paying promptly.
? Requesters can specify that each HIT should
be redundantly completed by several different
Turkers. This allows higher quality labels to
be selected, for instance, by taking the majority
label.
? Requesters can require that all workers meet
a particular set of qualifications, such as suffi-
cient accuracy on a small test set or a minimum
percentage of previously accepted submissions.
Amazon provides two qualifications that a Re-
quester can use by default. These are past HIT Ap-
proval Rate and Location. The location qualifica-
tion allows the Requester to have HITs done only by
residents of a certain country (or to exclude Turk-
ers from certain regions). Additionally, Requesters
can design custom Qualification Tests that Turkers
must complete before working on a particular HIT.
These can be created through the MTurk API, and
can either be graded manually or automatically. An
important qualification that isn?t among Amazon?s
default qualifications is language skills. One might
design a qualification test to determine a Turker?s
ability to speak Arabic or Farsi before allowing them
to do part of speech tagging in those languages, for
instance.
There are several reasons that poor quality data
might be generated. The task may be too complex or
the instructions might not be clear enough for Turk-
ers to follow. The financial incentives may be too
low for Turkers to act conscientiously, and certain
HIT designs may allow them to simply randomly
click instead of thinking about the task. Mason and
Watts (2009) present a study of financial incentives
on Mechanical Turk and find, counterintuitively, that
increasing the amount of compensation for a partic-
ular task does not tend to improve the quality of the
results. Anecdotally, we have observed that some-
times there is an inverse relationship between the
amount of payment and the quality of work, because
it is more tempting to cheat on high-paying HITs if
you don?t have the skills to complete them. For ex-
ample, a number of Turkers tried to cheat on an Urdu
to English translation HIT by cutting-and-pasting
the Urdu text into an online machine translation sys-
tem (expressly forbidden in the instructions) because
we were paying the comparatively high amount of
$1.
3
3.1 Designing HITs for quality control
We suggest designing your HITs in a way that will
deter cheating or that will make cheating obvious.
HIT design is part of the art of using MTurk. It
can?t be easily quantified, but it has a large impact on
the outcome. For instance, we reduced cheating on
our translation HIT by changing the design so that
we displayed images of the Urdu sentences instead
of text, which made it impossible to copy-and-paste
into an MT system for anyone who could not type in
Arabic script.
Another suggestion is to include information
within the data that you upload to MTurk that will
not be displayed to the Turkers, but will be useful
to you when reviewing the HITs. For example, we
include machine translation output along with the
source sentences. Although this is not displayed to
Turkers, when we review the Turkers? translations
we compare them to the MT output. This allows us
to reject translations that are identical to the MT, or
which are just random sentences that are unrelated to
the original Urdu. We also use a javascript3 to gather
the IP addresses of the Turkers and do geolocation
to look up their location. Turkers in Pakistan require
less careful scrutiny since they are more likely to be
bilingual Urdu speakers than those in Romania, for
instance.
CrowdFlower4 provides an interface for design-
ing HITs that includes a phase for the Requester to
input gold standard data with known labels. Insert-
ing items with known labels alongside items which
need labels allows a Requester to see which Turkers
are correctly replicating the gold standard labels and
which are not. This is an excellent idea. If it is possi-
ble to include positive and negative controls in your
HITs, then do so. Turkers who fail the controls can
be blocked and their labels can be excluded from the
final data set. CrowdFlower-generated HITs even
display a score to the Turkers to give them feedback
on how well they are doing. This provides training
for Turkers, and discourages cheating.
3http://wiki.github.com/callison-burch/
mechanical_turk_workshop/geolocation
4http://crowdflower.com/
3.2 Iterative improvements on MTurk
Another class of quality control on Mechanical Turk
is through iterative HITs that build on the output of
previous HITs. This could be used to have Turkers
judge whether the results from a previous HIT con-
formed to the instructions, and whether it is of high
quality. Alternately, the second set of Turkers could
be used to improve the quality of what the first Turk-
ers created. For instance, in a translation task, a sec-
ond set of US-based Turkers could edit the English
produced by non-native speakers.
CastingWords,5 a transcription company that uses
Turker labor, employs this strategy by having a first-
pass transcription graded and iteratively improved
in subsequent passes. Little et al (2009) even de-
signed an API specifically for running iterative tasks
on MTurk.6
4 Recommended Practices
Although it is hard to define a set of ?best practices?
that applies to all HITs, or even to all NLP HITs, we
recommend the following guidelines to Requesters.
First and foremost, it is critical to convey instruc-
tions appropriately for non-experts. The instructions
should be clear and concise. To calibrate whether
the HIT is doable, you should first try the task your-
self, and then have a friend from outside the field try
it. This will help to ensure that the instructions are
clear, and to calibrate how long each HIT will take
(which ought to allow you to price the HITs fairly).
If possible, you should insert positive and nega-
tive controls so that you can quickly screen out bad
Turkers. This is especially important for HITs that
only require clicking buttons to complete. If pos-
sible, you should include a small amount of gold
standard data in each HIT. This will allow you to
determine which Turkers are good, but will also al-
low you weight the Turkers if you are combining
the judgments of multiple Turkers. If you are hav-
ing Turkers evaluate the output of systems, then ran-
domize the order that the systems are shown in.
When publishing papers that use Mechanical Turk
as a source of training data or to evaluate the output
of an NLP system, report how you ensured the qual-
ity of your data. You can do this by measuring the
5http://castingwords.com/
6http://groups.csail.mit.edu/uid/turkit/
4
inter-annotator agreement of the Turkers against ex-
perts on small amounts of gold standard data, or by
stating what controls you used and what criteria you
used to block bad Turkers. Finally, whenever possi-
ble you should publish the data that you generate on
Mechanical Turk (and your analysis scripts and HIT
templates) alongside your paper so that other people
can verify it.
5 Related work
In the past two years, several papers have published
about applying Mechanical Turk to a diverse set of
natural language processing tasks, including: cre-
ating question-answer sentence pairs (Kaisser and
Lowe, 2008), evaluating machine translation qual-
ity and crowdsouring translations (Callison-Burch,
2009), paraphrasing noun-noun compouds for Se-
mEval (Butnariu et al, 2009), human evaluation of
topic models (Chang et al, 2009), and speech tran-
scription (McGraw et al, 2010; Marge et al, 2010a;
Novotney and Callison-Burch, 2010a). Others have
used MTurk for novel research directions like non-
simulated active learning for NLP tasks such as sen-
timent classification (Hsueh et al, 2009) or doing
quixotic things like doing human-in-the-loop min-
imum error rate training for machine translation
(Zaidan and Callison-Burch, 2009).
Some projects have demonstrated the super-
scalability of crowdsourced efforts. Deng et al
(2009) used MTurk to construct ImageNet, an anno-
tated image database containing 3.2 million that are
hierarchically categorized using the WordNet ontol-
ogy (Fellbaum, 1998). Because Mechanical Turk
allows researchers to experiment with crowdsourc-
ing by providing small incentives to Turkers, other
successful crowdsourcing efforts like Wikipedia or
Games with a Purpose (von Ahn and Dabbish, 2008)
also share something in common with MTurk.
6 Shared Task
The workshop included a shared task in which par-
ticipants were provided with $100 to spend on Me-
chanical Turk experiments. Participants submitted a
1 page proposal in advance describing their intended
use of the funds. Selected proposals were provided
$100 seed money, to which many participants added
their own funds. As part of their participation, each
team submitted a workshop paper describing their
experiments as well as the data collected and de-
scribed in the paper. Data for the shared papers is
available at the workshop website.7
This section describes the variety of data types ex-
plored and collected in the shared task. Of the 24
participating teams, most did not exceed the $100
that they were awarded by a significant amount.
Therefore, the variety and extent of data described in
this section is the result of a minimal $2,400 invest-
ment. This achievement demonstrates the potential
for MTurk?s impact on the creation and curation of
speech and language corpora.
6.1 Traditional NLP Tasks
An established core set of computational linguistic
tasks have received considerable attention in the nat-
ural language processing community. These include
knowledge extraction, textual entailment and word
sense disambiguation. Each of these tasks requires a
large and carefully curated annotated corpus to train
and evaluate statistical models. Many of the shared
task teams attempted to create new corpora for these
tasks at substantially reduced costs using MTurk.
Parent and Eskenazi (2010) produce new corpora
for the task of word sense disambiguation. The
study used MTurk to create unique word definitions
for 50 words, which Turkers then also mapped onto
existing definitions. Sentences containing these 50
words were then assigned to unique definitions ac-
cording to word sense.
Madnani and Boyd-Graber (2010) measured the
concept of transitivity of verbs in the style of Hop-
per and Thompson (1980), a theory that goes beyond
simple grammatical transitivity ? whether verbs take
objects (transitive) or not ? to capture the amount of
action indicated by a sentence. Videos that portrayed
verbs were shown to Turkers who described the ac-
tions shown in the video. Additionally, sentences
containing the verbs were rated for aspect, affirma-
tion, benefit, harm, kinesis, punctuality, and volition.
The authors investigated several approaches for elic-
iting descriptions of transitivity from Turkers.
Two teams explored textual entailment tasks.
Wang and Callison-Burch (2010) created data for
7http://sites.google.com/site/
amtworkshop2010/
5
recognizing textual entailment (RTE). They submit-
ted 600 text segments and asked Turkers to identify
facts and counter-facts (unsupported facts and con-
tradictions) given the provided text. The resulting
collection includes 790 facts and 203 counter-facts.
Negri and Mehdad (2010) created a bi-lingual en-
tailment corpus using English and Spanish entail-
ment pairs, where the hypothesis and text come from
different languages. The authors took a publicly
available English RTE data set (the PASCAL-RTE3
dataset1) and created an English-Spanish equivalent
by having Turkers translating the hypotheses into
Spanish. The authors include a timeline of their
progress, complete with total cost over the 10 days
that they ran the experiments.
In the area of natural language generation, Heil-
man and Smith (2010) explored the potential of
MTurk for ranking of computer generated questions
about provided texts. These questions can be used to
test reading comprehension and understanding. 60
Wikipedia articles were selected, for each of which
20 questions were generated. Turkers provided 5 rat-
ings for each of the 1,200 questions, creating a sig-
nificant corpus of scored questions.
Finally, Gordon et al (2010) relied on MTurk to
evaluate the quality and accuracy of automatically
extracted common sense knowledge (factoids) from
news and Wikipedia articles. Factoids were pro-
vided by the KNEXT knowledge extraction system.
6.2 Speech and Vision
While MTurk naturally lends itself to text tasks,
several teams explored annotation and collection of
speech and image data. We note that one of the pa-
pers in the main track described tools for collecting
such data (Lane et al, 2010).
Two teams used MTurk to collect text annotations
on speech data. Marge et al (2010b) identified easy
and hard sections of meeting speech to transcribe
and focused data collection on difficult segments.
Transcripts were collected on 48 audio clips from
4 different speakers, as well as other types of an-
notations. Kunath and Weinberger (2010) collected
ratings of accented English speech, in which non-
native speakers were rated as either Arabic, Man-
darin or Russian native speakers. The authors ob-
tained multiple annotations for each speech sample,
and tracked the native language of each annotator,
allowing for an analysis of rating accuracy between
native English and non-native English annotators.
Novotney and Callison-Burch (2010b) used
MTurk to elicit new speech samples. As part of an
effort to increase the accessibility of public knowl-
edge, such as Wikipedia, the team prompted Turkers
to narrate Wikipedia articles. This required Turkers
to record audio files and upload them. An additional
HIT was used to evaluate the quality of the narra-
tions.
A particularly creative data collection approach
asked Turkers to create handwriting samples and
then to submit images of their writing (Tong et al,
2010). Turkers were asked to submit handwritten
shopping lists (large vocabulary) or weather descrip-
tions (small vocabulary) in either Arabic or Spanish.
Subsequent Turkers provided a transcription and a
translation. The team collected 18 images per lan-
guage, 2 transcripts per image and 1 translation per
transcript.
6.3 Sentiment, Polarity and Bias
Two papers investigated the topics of sentiment, po-
larity and bias. Mellebeek et al (2010) used several
methods to obtain polarity scores for Spanish sen-
tences expressing opinions about automative topics.
They evaluated three HITs for collecting such data
and compared results for quality and expressiveness.
Yano et al (2010) evaluated the political bias of blog
posts. Annotators labeled 1000 sentences to deter-
mine biased phrases in political blogs from the 2008
election season. Knowledge of the annotators own
biases allowed the authors to study how bias differs
on the different ends of the political spectrum.
6.4 Information Retrieval
Large scale evaluations requiring significant human
labor for evaluation have a long history in the in-
formation retrieval community (TREC). Grady and
Lease (2010) study four factors that influence Turker
performance on a document relevance search task.
The authors present some negative results on how
these factors influence data collection. For further
work on MTurk and information retrieval, readers
are encouraged to see the SIGIR 2010 Workshop on
Crowdsourcing for Search Evaluation.8
8http://www.ischool.utexas.edu/?cse2010/
call.htm
6
6.5 Information Extraction
Information extraction (IE) seeks to identify specific
types of information in natural languages. The IE
papers in the shared tasks focused on new domains
and genres as well as new relation types.
The goal of relation extraction is to identify rela-
tions between entities or terms in a sentence, such as
born in or religion. Gormley et al (2010) automat-
ically generate potential relation pairs in sentences
by finding relation pairs appearing in news articles
as given by a knowledge base. They ask Turkers if
a sentence supports a relation, does not support a re-
lation, or whether the relation makes sense. They
collected close to 2500 annotations for 17 different
person relation types.
The other IE papers explored new genres and do-
mains. Finin et al (2010) obtained named entity an-
notations (person, organization, geopolitical entity)
for several hundred Twitter messages. They con-
ducted experiments using both MTurk and Crowd-
Flower. Yetisgen-Yildiz et al (2010) explored
medical named entity recognition. They selected
100 clinical trial announcements from ClinicalTri-
als.gov. 4 annotators for each of the 100 announce-
ments identified 3 types of medical entities: medical
conditions, medications, and laboratory test.
6.6 Machine Translation
The most popular shared task topic was Machine
Translation (MT). MT is a data hungry task that re-
lies on huge corpora of parallel texts between two
languages. Performance of MT systems depends
on the size of training corpora, so there is a con-
stant search for new and larger data sets. Such data
sets are traditionally expensive to produce, requiring
skilled translators. One of the advantages to MTurk
is the diversity of the Turker population, making it
an especially attractive source of MT data. Shared
task papers in MT explored the full range of MT
tasks, including alignments, parallel corpus creation,
paraphrases and bilingual lexicons.
Gao and Vogel (2010) create alignments in a 300
sentence Chinese-English corpus (Chinese aligned
to English). Both Ambati and Vogel (2010) and
Bloodgood and Callison-Burch (2010) explore the
potential of MTurk in the creation of MT paral-
lel corpora for evaluation and training. Bloodgood
and Callison-Burch replicate the NIST 2009 Urdu-
English test set of 1792 sentences, paying only $0.10
a sentence, a substantially reduced price than the
typical annotator cost. The result is a data set that is
still effective for comparing MT systems in an eval-
uation. Ambati and Vogel create corpora with 100
sentences and 3 translations per sentence for all the
language pairs between English, Spanish, Urdu and
Telugu. This demonstrates the feasibility of creating
cheap corpora for high and low resource languages.
Two papers focused on the creation and evalua-
tion of paraphrases. Denkowski et al (2010) gen-
erated and evaluated 728 paraphrases for Arabic-
English translation. MTurk was used to identify
correct and fix incorrect paraphrases. Over 1200
high quality paraphrases were created. Buzek et
al. (2010) evaluated error driven paraphrases for
MT. In this setting, paraphrases are used to sim-
plify potentially difficult to translate segments of
text. Turkers identified 1780 error regions in 1006
English/Chinese sentences. Turkers provided 4821
paraphrases for these regions.
External resources can be an important part of an
MT system. Irvine and Klementiev (2010) created
lexicons for low resource languages. They evaluated
translation candidates for 100 English words in 32
languages and solicited translations for 10 additional
languages. Higgins et al (2010) expanded name
lists in Arabic by soliciting common Arabic nick-
names. The 332 collected nicknames were primar-
ily provided by Turkers in Arab speaking countries
(35%), India (46%), and the United States (13%).
Finally, Zaidan and Ganitkevitch (2010) explored
how MTurk could be used to directly improve an MT
grammar. Each rule in an Urdu to English transla-
tion system was characterized by 12 features. Turk-
ers were provided examples for which their feed-
back was used to rescore grammar productions di-
rectly. This approach shows the potential of fine
tuning an MT system with targeted feedback from
annotators.
7 Future Directions
Looking ahead, we can?t help but wonder what im-
pact MTurk and crowdsourcing will have on the
speech and language research community. Keep-
ing in mind Niels Bohr?s famous exhortation ?Pre-
7
diction is very difficult, especially if it?s about the
future,? we attempt to draw some conclusions and
predict future directions and impact on the field.
Some have predicted that access to low cost,
highly scalable methods for creating language and
speech annotations means the end of work on un-
supervised learning. Many a researcher has advo-
cated his or her unsupervised learning approach be-
cause of annotation costs. However, if 100 exam-
ples for any task are obtainable for less than $100,
why spend the time and effort developing often infe-
rior unsupervised methods? Such a radical change is
highly debatable, in fact, one of this paper?s authors
is a strong advocate of such a position while the
other disagrees, perhaps because he himself works
on unsupervised methods. Certainly, we can agree
that the potential exists for a change in focus in a
number of ways.
In natural language processing, data drives re-
search. The introduction of new large and widely
accessible data sets creates whole new areas of re-
search. There are many examples of such impact,
the most famous of which is the Penn Treebank
(Marcus. et al, 1994), which has 2910 citations in
Google scholar and is the single most cited paper
on the ACL anthology network (Radev et al, 2009).
Other examples include the CoNLL named entity
corpus (Sang and Meulder (2003) with 348 citations
on Google Scholar), the IMDB movie reviews senti-
ment data (Pang et al (2002) with 894 citations) and
the Amazon sentiment multi-domain data (Blitzer et
al. (2007) with 109 citations) . MTurk means that
creating similar data sets is now much cheaper and
easier than ever before. It is highly likely that new
MTurk produced data sets will achieve prominence
and have significant impact. Additionally, the cre-
ation of shared data means more comparison and
evaluation against previous work. Progress is made
when it can be demonstrated against previous ap-
proaches on the same data. The reduction of data
cost and the rise of independent corpus producers
likely means more accessible data.
More than a new source for cheap data, MTurk is
a source for new types of data. Several of the pa-
pers in this workshop collected information about
the annotators in addition to their annotations. This
creates potential for studying how different user de-
mographics understand language and allow for tar-
geting specific demographics in data creation. Be-
yond efficiencies in cost, MTurk provides access to
a global user population far more diverse than those
provided by more professional annotation settings.
This will have a significant impact on low resource
languages as corpora can be cheaply built for a much
wider array of languages. As one example, Irvine
and Klementiev (2010) collected data for 42 lan-
guages without worrying about how to find speak-
ers of such a wide variety of languages. Addition-
ally, the collection of Arabic nicknames requires a
diverse and numerous Arabic speaking population
(Higgins et al, 2010). In addition to extending into
new languages, MTurk also allows for the creation
of evaluation sets in new genres and domains, which
was the focus of two papers in this workshop (Finin
et al, 2010; Yetisgen-Yildiz et al, 2010). We ex-
pect to see new research emphasis on low resource
languages and new domains and genres.
Another factor is the change of data type and its
impact on machine learning algorithms. With pro-
fessional annotators, great time and care are paid to
annotation guidelines and annotator training. These
are difficult tasks with MTurk, which favors simple
intuitive annotations and little training. Many papers
applied creative methods of using simpler annota-
tion tasks to create more complex data sets. This
process can impact machine learning in a number
of ways. Rather than a single gold standard, anno-
tations are now available for many users. Learn-
ing across multiple annotations may improve sys-
tems (Dredze et al, 2009). Additionally, even with
efforts to clean up MTurk annotations, we can ex-
pect an increase in noisy examples in data. This will
push for new more robust learning algorithms that
are less sensitive to noise. If we increase the size
of the data ten-fold but also increase the noise, can
learning still be successful? Another learning area
of great interest is active learning, which has long
relied on simulated user experiments. New work
evaluated active learning methods with real users us-
ing MTurk (Baker et al, 2009; Ambati et al, 2010;
Hsueh et al, 2009; ?). Finally, the composition of
complex data set annotations from simple user in-
puts can transform the method by which we learn
complex outputs. Current approaches expect exam-
ples of labels that exactly match the expectation of
the system. Can we instead provide lower level sim-
8
pler user annotations and teach systems how to learn
from these to construct complex output? This would
open more complex annotation tasks to MTurk.
A general trend in research is that good ideas
come from unexpected places. Major transforma-
tions in the field have come from creative new ap-
proaches. Consider the Penn Treebank, an ambitious
and difficult project of unknown potential. Such
large changes can be uncommon since they are often
associated with high cost, as was the Penn Treebank.
However, MTurk greatly reduces these costs, en-
couraging researchers to try creative new tasks. For
example, in this workshop Tong et al (2010) col-
lected handwriting samples in multiple languages.
Their creative data collection may or may not have
a significant impact, but it is unlikely that it would
have been tried had the cost been very high.
Finally, while obtaining new data annotations
from MTurk is cheap, it is not trivial. Workshop par-
ticipants struggled with how to attract Turkers, how
to price HITs, HIT design, instructions, cheating de-
tection, etc. No doubt that as work progresses, so
will a communal knowledge and experience of how
to use MTurk. There can be great benefit in new
toolkits for collecting language data using MTurk,
and indeed some of these have already started to
emerge (Lane et al, 2010)9.
Acknowledgements
Thanks to Sharon Chiarella of Amazon?s Mechan-
ical Turk for providing $100 credits for the shared
task, and to CrowdFlower for allowing free use of
their tool to workshop participants.
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are the
authors? alone.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon Mechanical Turk for
subjectivity word sense disambiguation. In NAACL
9http://wiki.github.com/callison-burch/
mechanical_turk_workshop/
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In NAACL Workshop on Creating Speech and Lan-
guage Data With Amazon?s Mechanical Turk.
Vamshi Ambati, Stephan Vogel, and Jamie Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. Language Resources and Evalua-
tion (LREC).
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Ann Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically-informed machine translation.
Technical Report 002, Johns Hopkins Human Lan-
guage Technology Center of Excellence, Summer
Camp for Applied Language Exploration, Johns Hop-
kins University, Baltimore, MD.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In As-
sociation for Computational Linguistics (ACL).
Michael Bloodgood and Chris Callison-Burch. 2010a.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In 48th
Annual Meeting of the Association for Computational
Linguistics, Uppsala, Sweden.
Michael Bloodgood and Chris Callison-Burch. 2010b.
Using Mechanical Turk to build machine translation
evaluation sets. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechan-
ical Turk.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2009. Semeval-2010 Task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Workshop on Semantic Evaluations.
Olivia Buzek, Philip Resnik, and Ben Bederson. 2010.
Error driven paraphrase annotation using Mechanical
Turk. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazon?s mechan-
ical turk. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2009), Singapore.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Neural
Information Processing Systems.
9
Jonathan Chang. 2010. Not-so-Latent Dirichlet Allo-
cation: Collapsed Gibbs sampling using human judg-
ments. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hier-
archical image database. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR, Miami Beach, Floriday.
Michael Denkowski and Alon Lavie. 2010. Explor-
ing normalization techniques for human judgments of
machine translation adequacy collected using Amazon
Mechanical Turk. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechani-
cal Turk.
Michael Denkowski, Hassan Al-Haj, and Alon Lavie.
2010. Turker-assisted paraphrasing for English-
Arabic machine translation. In NAACL Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk.
Mark Dredze, Partha Pratim Talukdar, and Koby Cram-
mer. 2009. Sequence learning from data with multiple
labels. In ECML/PKDD Workshop on Learning from
Multi-Label Data (MLD).
Keelan Evanini, Derrick Higgins, and Klaus Zechner.
2010. Using Amazon Mechanical Turk for transcrip-
tion of non-native speech. In NAACL Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Touring PER, ORG and LOC on $100 a day. In
NAACL Workshop on Creating Speech and Language
Data With Amazon?s Mechanical Turk.
Qin Gao and Stephan Vogel. 2010. Semi-supervised
word alignment with Mechanical Turk. In NAACL
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Dan Gillick and Yang Liu. 2010. Non-expert evaluation
of summarization systems is risky. In NAACL Work-
shop on Creating Speech and Language Data With
Amazon?s Mechanical Turk.
Jonathan Gordon, Benjamin Van Durme, and Lenhart
Schubert. 2010. Evaluation of commonsense knowl-
edge with Mechanical Turk. In NAACL Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk.
Matthew R. Gormley, Adam Gerber, Mary Harper, and
Mark Dredze. 2010. Non-expert correction of auto-
matically generated relation annotations. In NAACL
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Catherine Grady and Matthew Lease. 2010. Crowd-
sourcing document relevance assessment with Me-
chanical Turk. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechan-
ical Turk.
Michael Heilman and Noah A. Smith. 2010. Rating
computer-generated questions with Mechanical Turk.
In NAACL Workshop on Creating Speech and Lan-
guage Data With Amazon?s Mechanical Turk.
Chiara Higgins, Elizabeth McGrath, and Laila Moretto.
2010. AMT crowdsourcing: A viable method for rapid
discovery of Arabic nicknames? In NAACL Workshop
on Creating Speech and Language Data With Ama-
zon?s Mechanical Turk.
Paul J. Hopper and Sandra A. Thompson. 1980. Transi-
tivity in grammar and discourse. Language, 56:251?
299.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: A study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 27?35, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
Panos Ipeirotis. 2010. New demographics of Mechanical
Turk. http://behind-the-enemy-lines.
blogspot.com/2010/03/
new-demographics-of-mechanical-turk.
html.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechan-
ical Turk.
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosen-
thal, and Kathleen McKeown. 2010. Corpus creation
for new genres: A crowdsourced approach to PP at-
tachment. In NAACL Workshop on Creating Speech
and Language Data With Amazon?s Mechanical Turk.
Michael Kaisser and John Lowe. 2008. Creating a re-
search collection of question answer sentence pairs
with Amazons Mechanical Turk. In Proceedings of
the Sixth International Language Resources and Eval-
uation (LREC?08), Marrakech, Morocco.
Stephen Kunath and Steven Weinberger. 2010. The wis-
dom of the crowd?s ear: Speech accent rating and an-
notation with Amazon Mechanical Turk. In NAACL
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Ian Lane, Matthias Eck, Kay Rottmann, and Alex
Waibel. 2010. Tools for collecting speech corpora via
Mechanical-Turk. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechani-
cal Turk.
10
Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen Yildiz. 2010. Annotating large email
datasets for named entity recognition with Mechanical
Turk. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Greg Little, Lydia B. Chilton, Rob Miller, and Max Gold-
man. 2009. Turkit: Tools for iterative tasks on me-
chanical turk. In Proceedings of the Workshop on
Human Computation at the International Conference
on Knowledge Discovery and Data Mining (KDD-
HCOMP ?09), Paris.
Nitin Madnani and Jordan Boyd-Graber. 2010. Measur-
ing transitivity using untrained annotators. In NAACL
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Mitch Marcus., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational lin-
guistics, 19(2):313?330.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010a. Using the Amazon Mechanical
Turk for transcription of spoken language. ICASSP,
March.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010b. Using the Amazon Mechanical
Turk to transcribe and annotate meeting speech for ex-
tractive summarization. In NAACL Workshop on Cre-
ating Speech and Language Data With Amazon?s Me-
chanical Turk.
Winter Mason and Duncan J. Watts. 2009. Financial
incentives and the ?performance of crowds?. In Pro-
ceedings of the Workshop on Human Computation at
the International Conference on Knowledge Discovery
and Data Mining (KDD-HCOMP ?09), Paris.
Ian McGraw, Chia ying Lee, Lee Hetherington, and Jim
Glass. 2010. Collecting voices from the crowd.
LREC, May.
Bart Mellebeek, Francesc Benavent, Jens Grivolla, Joan
Codina, Marta R. Costa-Jussa`, and Rafael Banchs.
2010. Opinion mining of spanish customer comments
with non-expert annotations on Mechanical Turk. In
NAACL Workshop on Creating Speech and Language
Data With Amazon?s Mechanical Turk.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher Potts,
Tyler Schnoebelen, and Harry Tily. 2010. Crowd-
sourcing and language studies: the new generation
of linguistic data. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechani-
cal Turk.
Matteo Negri and Yashar Mehdad. 2010. Creating a
bi-lingual entailment corpus through translations with
Mechanical Turk: $100 for a 10 days rush. In NAACL
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Scott Novotney and Chris Callison-Burch. 2010a.
Cheap, fast and good enough: Automatic speech
recognition with non-expert transcription. NAACL,
June.
Scott Novotney and Chris Callison-Burch. 2010b.
Crowdsourced accessibility: Elicitation of Wikipedia
articles. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Empirical Methods in
Natural Language Processing (EMNLP).
Gabriel Parent and Maxine Eskenazi. 2010. Cluster-
ing dictionary definitions using Amazon Mechanical
Turk. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network. In Pro-
ceedings of the 2009 Workshop on Text and Citation
Analysis for Scholarly Digital Libraries, pages 54?61,
Suntec City, Singapore, August. Association for Com-
putational Linguistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia
Hockenmaier. 2010. Collecting image annotations us-
ing Amazon?s Mechanical Turk. In NAACL Workshop
on Creating Speech and Language Data With Ama-
zon?s Mechanical Turk.
Joel Rose. 2010. Some turn to ?Mechanical? job search.
http://marketplace.publicradio.org/
display/web/2009/06/30/pm_turking/.
Marketplace public radio. Air date: June 30, 2009.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In CoNLL-
2003.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2008), Honolulu, Hawaii.
Audrey Tong, Jerome Ajot, Mark Przybocki, and
Stephanie Strassel. 2010. Document image collection
using Amazon?s Mechanical Turk. In NAACL Work-
shop on Creating Speech and Language Data With
Amazon?s Mechanical Turk.
Luis von Ahn and Laura Dabbish. 2008. General tech-
niques for designing games with a purpose. Commu-
nications of the ACM.
Rui Wang and Chris Callison-Burch. 2010. Cheap facts
and counter-facts. In NAACL Workshop on Creating
11
Speech and Language Data With Amazon?s Mechani-
cal Turk.
Tae Yano, Philip Resnik, and Noah A Smith. 2010.
Shedding (a thousand points of) light on biased lan-
guage. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Meliha Yetisgen-Yildiz, Imre Solti, Scott Halgrim, and
Fei Xia. 2010. Preliminary experiments with Ama-
zon?s Mechanical Turk for annotating medical named
entities. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In Proceedings of EMNLP 2009, pages 52?61,
August.
Omar Zaidan and Juri Ganitkevitch. 2010. An enriched
MT grammar for under $100. In NAACL Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk.
12
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 41?44,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Shared Task: Crowdsourced Accessibility
Elicitation of Wikipedia Articles
Scott Novotney and Chris Callison-Burch
Center for Language and Speech Processing
Johns Hopkins University
3400 North Charles Street
Baltimore, MD, USA
snovotne@bbn.com ccb@jhu.edu
Abstract
Mechanical Turk is useful for generating
complex speech resources like conversational
speech transcription. In this work, we ex-
plore the next step of eliciting narrations of
Wikipedia articles to improve accessibility for
low-literacy users. This task proves a use-
ful test-bed to implement qualitative vetting
of workers based on difficult to define metrics
like narrative quality. Working with the Me-
chanical Turk API, we collected sample nar-
rations, had other Turkers rate these samples
and then granted access to full narration HITs
depending on aggregate quality. While narrat-
ing full articles proved too onerous a task to
be viable, using other Turkers to perform vet-
ting was very successful. Elicitation is possi-
ble on Mechanical Turk, but it should conform
to suggested best practices of simple tasks that
can be completed in a streamlined workflow.
1 Introduction
The rise of Mechanical Turk publications in the NLP
community leaves no doubt that non-experts can
provide useful annotations for low cost. Emerging
best practices suggest designing short, simple tasks
that require little amount of upfront effort to most ef-
fectively use Mechanical Turk?s labor pool. Suitable
tasks are best limited to those easily accomplished
in ?short bites? requiring little context switching. For
instance, most annotation tasks in prior work (Snow
et al, 2008) required selection from an enumerated
list, allowing for easy automated quality control and
data collection.
More recent work to collect speech transcrip-
tion (Novotney and Callison-Burch, 2010) or paral-
lel text translations (Callison-Burch, 2009) demon-
strated that Turkers can provide useful free-form an-
notation.
In this paper, we extend open ended collec-
tion even further by eliciting narrations of English
Wikipedia articles. To vet prospective narrators,
we use qualitative qualifications by aggregating the
opinions of other Turkers on narrative style, thus
avoiding quantification of qualitative tasks.
The Spoken Wikipedia Project1 aims to increase
the accessibility of Wikipedia by recording articles
for use by blind or illiterate users. Since 2008, over
1600 English articles covering topics from art to
technology have been narrated by volunteers. The
charitable nature of this work should provide addi-
tional incentive for Turkers to complete this task.
We use Wikipedia narrations as an initial proof-of-
concept for other more challenging elicitation tasks
such as spontaneous or conversational speech.
While previous work used other Turkers in
second-pass filtering for quality control, we flip this
process and instead require that narrators be judged
favorably before working on full narration tasks. Re-
lying on human opinion sidesteps the difficult task
of automatically judging narrative quality. This re-
quires a multi-pass workflow to manage potential
narrators and grant them access to the full narration
HITs through Mechanical Turk?s Qualifications.
In this paper, we make the following points:
? Vetting based on qualitative criteria like nar-
ration quality can be effectively implemented
through Turker-provided ratings.
1http://en.wikipedia.org/wiki/Wikipedia:
WikiProject_Spoken_Wikipedia
41
? Narrating full articles is too complex and time-
consuming for timely task throughput - best
practices are worth following.
? HITs should be streamlined as much as possi-
ble. Requiring Turkers to perform work outside
of the web interface seemingly hurt task com-
pletion rate.
2 Prior Work
The research community has demonstrated that
complex annotations (like speech transcription and
elicitation) can be provided through Mechanical
Turk.
Callison-Burch (2009) showed that Turkers could
accomplish complex tasks like translating Urdu or
creating reading comprehension tests.
McGraw et al (2009) used Mechanical Turk to
improve an English isolated word speech recognizer
by having Turkers listen to a word and select from
a list of probable words at a cost of $20 per hour of
transcription.
Marge et al (2010) collected transcriptions of
clean speech and demonstrated that duplicate tran-
scription of non-experts can match expert transcrip-
tion.
Novotney and Callison-Burch (2010) collected
transcriptions of conversational speech for as little
as $5 / hour of transcription and demonstrated that
resources are better spent annotating more data than
improving data quality.
McGraw et al (2010) elicited short snippets of
English street addresses through a web interface.
103 hours were elicted in just over three days.
3 Narration Task
Using a python library for parsing Wikipedia2, we
extracted all text under the <p> tag as a heuristic
for readable content. We ignored all other content
like lists, info boxes or headings. Since we wanted
to preserve narrative flow, each article was posted
as one HIT, paying $0.05 per paragraph. Articles
averaged 40 paragraphs, so each HIT averaged $2 in
payment - some as little as $0.25.
We provided instructions for using recording
software and asked Turkers to record one para-
graph at a time. Using Mechanical Turk?s API,
2http://github.com/j2labs/wikipydia
we generated an XML template for each para-
graph and let the Turker upload a file through the
FileUploadAnswer form. The API supports
constraints on file extensions, so we were able to re-
quire that all files be in mp3 format before the Turker
could submit the work.
Mechanical Turk?s API supports file requests
through the GetFileUploadURL call. A URL is
dynamically generated on Amazon?s servers which
stays active for one minute. We then fetched each
audio file and stored them locally on our own servers
for later processing.
Since these narrations are meant for public con-
sumption and are difficult to quality control, we re-
quired prospective Turkers first qualify.
4 Granting Qualitative Qualifications
Qualifications are prerequisites that limit which
Turkers can work on a HIT. A common qualifica-
tion provided by Mechanical Turk is a minimum ap-
proval rating for a Turker, indicating what percent-
age of submitted work was approved. We created a
qualification for our narration tasks since we wanted
to ensure only those turkers with a good speaking
voice would complete our tasks.
However, the definition of a ?good speaking
voice? is not easy to quantify. Luckily, this task is
well suited to Mechanical Turk?s concept of artifi-
cial artificial intelligence. Humans can easily decide
a narrator?s quality while automatic methods would
be impractical. Additionally, we never define what
a ?good? narration voice is, relying instead on public
opinion.
4.1 Workflow
We implemented the qualification ratings using the
API with three different steps. Turkers who wish
to complete the full narration HITs are first directed
to a ?qualification? HIT with one sample paragraph
paying $0.05. We then use other Turkers to rate the
quality of the narrator, asking them to judge based
on speaking style, audio clarity and pronunciation.
Post Qualification The narration qualification and
full narration HITs are posted.
Sample HIT A prospective narrator uploads a
recording of a sample paragraph earning $0.05.
42
The audio is downloaded and hosted on our
web host.
Rating HIT A HIT is created to be completed ten
times. Turkers make a binary decision as to
whether they would listen to a full article by
the narrator and optionally suggest feedback.
Grant Qualification The ten ratings are collected
and if five or more are positive we grant the
qualification. The narrator is then automati-
cally contacted with the decision and provided
with any feedback from the rating Turkers.
Although not straightforward, the API made it
possible to dynamically create HITs, approve as-
signments, sync audio files and ratings,notify work-
ers and grant qualifications. It does not, however,
manage state across HITs, requiring us to implement
our own control logic for associating workers with
narration and rating HITs. Once implemented, man-
aging the process was as simple as invoking three
perl scripts a few times a day. These could easily
be rolled into one background process automatically
controlling the entire workflow.
4.2 Effectiveness of Turker Ratings
Thirteen Turkers submitted sample audio files over
the course of a week. Collecting the ten ratings took
a few hours per Turker. The average rating for the
narrators was 7.5, with three of the thirteen being
rejected for having a score less than 5. The authors
agreed with the sentiment of the raters and feel that
the qualification process correctly filtered out the
poor narrators.
Below is a sample of the comments for an ap-
proved narrator and a rejected narrator.
This Turker was approved with 9/10 votes.
? The narration was very easy to understand. The
speaker?s tone was even, well-paced, and clear.
Great narration.
? Very good voice, good pace and modulation.
? Very nice voice and pleasant to listen to. I would
have guessed that this was a professional voice ac-
tor.
This Turker was rejected with 3/10 votes.
? Monotone voice, uninterested and barely literate. I
would never listen to this voice for any length of
time.
? muddy audio quality; narrator has a tired and a very
low tone quality.
? Very solemn voice - didn?t like listening to it.
5 Data Analysis
Of the thirteen qualified Turkers, only two went on
to complete full narrations. This happened only af-
ter we shortened the articles to the initial five para-
graphs and raised payment to $0.25 per paragraph.
While the audio was clear, both authors exhibited
mispronunciations of domain-specific terms. For in-
stance, one author narrating Isaac Newton mispro-
nounced Principia with a soft c (/prInsIpi9/) instead
of a hard c (/prInkIpi9/) and indices as /Ind>aIsEz/.
Since the text is known ahead of time, one could in-
clude a pronunciation guide for rare words to assist
the narrator.
The more disapointing result, however, is the very
slow return of the narration task. Contrasting with
the successful elicitation of (McGraw et al, 2010),
two reasons clearly stand out.
First, these tasks were much too long in length.
This was due to constraints we placed on collection
to improve data quality. We assumed that multiple
narrators for a single article would ruin the narrative
flow. Since few workers were willing to complete
five recordings, future work could chop each article
into smaller chunks to be completed by multiple nar-
rators. In contrast, eliciting spoken addresses has no
need for continuity across samples, thus the individ-
ual HITs in (McGraw et al, 2010) could be much
smaller.
Second, and more importantly, our HITs required
much more effort on the part of the Turker. We chose
to fully use Mechanical Turk?s API to manage data
and did not implement audio recording or data trans-
mission through the browser. Turkers were required
to record audio in a separate program and then up-
load the files. We thought the added ability to re-
record and review audio would be a plus compared
to in-browser recording. In contrast, (McGraw et al,
2010) used a javascript package to record narrations
directly in the browser window. While it was sim-
ple to use the API, it raised too much of a barrier for
Turkers to complete the task.
43
5.1 Feasability for Full Narration
Regardless of the task effectiveness, it is not clear
that Mechanical Turk is cost effective for large scale
narration. A reasonable first task would be to nar-
rate the 2500 featured articles on Wikipedia?s home
page. They average 44 paragraphs in length with
around 4311 words per article. Narrating this corpus
would cost $5500 at the rate of $0.05 per paragraph -
if workers would be willing to complete at that rate.
6 Conclusion
Our experiments with Mechanical Turk attempted
to find the limits of data collection and nebulous
task definitions. Long-form narration was unsuc-
cessful due to the length of the tasks and the lack
of a streamlined workflow for the Turkers. How-
ever, assigning qualifications based upon aggregat-
ing qualitative opinions was very successful. This
task exploited the strenghts of Mechanical Turk by
quickly gathering judgements that are easy for hu-
mans to make but near impossible to reliably auto-
mate.
The contrast between the failure of this narration
task and the success of previous elicitation is due
to the nature of the underlying task. Our desire to
have one narrator per article prevented elicitation in
short bites of a few seconds long. Additionally, our
efforts to solely use Mechanical Turk?s API limited
the simplicity of the workflow. While our backend
work was greatly simplified since we relied on ex-
isting data management code, the lack of in-browser
recording placed too much burden on the Turkers.
We would make the following changes if we were
to reimplement this task:
1. Integrate the workflow into the browser.
2. Perform post-process quality control to block
bad narrators from completing more HITs.
3. Drop the requirement of one narrator per ar-
ticle. A successful compromise might be one
section, averaging around five paragraphs.
4. Only narrate the lead in to an article (first par-
gagraph) first. If a user requests a full narration,
then seek out the rest of the article.
5. Place qualification as a much larger set of as-
signments. Turkers often sort HITs by avail-
able assignments, so the qualification HIT was
rarely seen.
References
Chris Callison-Burch. 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazons Me-
chanical Turk. EMNLP.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010. Using the amazon mechanical turk
for transcription of spoken language. ICASSP, March.
Ian McGraw, Alexander Gruenstein, and Andrew Suther-
land. 2009. A self-labeling speech corpus: Collecting
spoken words with an online educational game. In IN-
TERSPEECH.
Ian McGraw, Chia ying Lee, Lee Hetherington, and Jim
Glass. 2010. Collecting Voices from the Crowd.
LREC, May.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
Fast and Good Enough: Automatic Speech Recogni-
tion with Non-Expert Transcription . NAACL, June.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP.
44
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 163?167,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Cheap Facts and Counter-Facts
Rui Wang
Computational Linguistics Department
Saarland University
Room 2.04, Building C 7.4
Saarbruecken, 66123 Germany
rwang@coli.uni-sb.de
Chris Callison-Burch
Computer Science Department
Johns Hopkins University
3400 N. Charles Street (CSEB 226-B)
Baltimore, MD 21218, USA
ccb@cs.jhu.com.edu
Abstract
This paper describes our experiments of us-
ing Amazon?s Mechanical Turk to generate
(counter-)facts from texts for certain named-
entities. We give the human annotators a para-
graph of text and a highlighted named-entity.
They will write down several (counter-)facts
about this named-entity in that context. The
analysis of the results is performed by com-
paring the acquired data with the recognizing
textual entailment (RTE) challenge dataset.
1 Motivation
The task of RTE (Dagan et al, 2005) is to say
whether a person would reasonably infer some short
passage of text, the Hypothesis (H), given a longer
passage, the Text (T). However, collections of such
T-H pairs are rare to find and these resources are the
key to solving the problem.
The datasets used in the RTE task were collected
by extracting paragraphs of news text and manu-
ally constructing hypotheses. For the data collected
from information extraction task, the H is usually
a statement about a relation between two named-
entities (NEs), which is written by expertise. Simi-
larly, the H in question answering data is constructed
using both the question and the (in)correct answers.
Therefore, the research questions we could ask are,
1. Are these hypotheses really those ones people
interested in?
2. Are hypotheses different if we construct them
in other ways?
3. What would be a good negative hypotheses
compared with the positive ones?
In this paper, we address these issues by using
Amazon?s Mechanical Turk (MTurk), online non-
expert annotators (Snow et al, 2008). Instead of
constructing the hypotheses targeted to IE or QA, we
just ask the human annotators to come up with some
facts they consider as relevant to the given text. For
negative hypotheses, we change the instruction and
ask them to write counter-factual but still relevant
statements. In order to narrow down the content of
the generated hypotheses, we give a focused named-
entity (NE) for each text to guide the annotators.
2 Related Work
The early related research was done by Cooper et al
(1996), where they manually construct a textbook-
style corpus aiming at different semantic phenom-
ena involved in inference. However, the dataset is
not large enough to train a robust machine-learning-
based RTE system. The recent research from the
RTE community focused on acquiring large quan-
tities of textual entailment pairs from news head-
lines (Burger and Ferro, 2005) and negative exam-
ples from sequential sentences with transitional dis-
course connectives (Hickl et al, 2006). Although
the quality of the data collected were quite good,
most of the positive examples are similar to summa-
rization and the negative examples are more like a
comparison/contrast between two sentences instead
of a contradiction. Those data are the real sen-
tences used in news articles, but the way of obtain-
ing them is not necessarily the (only) best way to
163
find entailment pairs. In this paper, we investigate
an alternative inexpensive way of collecting entail-
ment/contradiction text pairs by crowdsourcing.
In addition to the information given by the text,
common knowledge is also allowed to be involved
in the inference procedure. The Boeing-Princeton-
ISI (BPI) textual entailment test suite1 is specifically
designed to look at entailment problems requiring
world knowledge. We will also allow this in the de-
sign of our task.
3 Design of the Task
The basic idea of the task is to give the human an-
notators a paragraph of text with one highlighted
named-entity and ask them to write some (counter-
)facts about it. In particular, we first preprocess
an existing RTE corpus using a named-entity rec-
ognizer to mark all the named-entities appearing in
both T and H. When we show the texts to Turkers,
we highlight one named-entity and give them one of
these two sets of instructions:
Facts: Please write several facts about the high-
lighted words according to the paragraph. You
may add additional common knowledge (e.g.
Paris is in France), but please mainly use the
information contained in the text. But please
do not copy and paste!
Counter-Facts: Please write several statements that
are contradictory to the text. Make your state-
ments about the highlighted words. Please use
the information mainly in the text. Avoid using
words like not or never.
Then there are three blank lines given for the annota-
tors to fill in facts or counter-factual statements. For
each HIT, we gather facts or counter-facts for five
texts, and for each text, we ask three annotators to
perform the task. We give Turkers one example as a
guide along with the instructions.
4 Experiments and Results
The texts we use in our experiments are the develop-
ment set of the RTE-5 challenge (Bentivogli et al,
1http://www.cs.utexas.edu/?pclark/
bpi-test-suite/
Total Average (per Text)
Extracted NEs
Facts 244 1.19
Counter-Facts 121 1.11
Generated Hypotheses
Facts 790 3.85
Counter-Facts 203 1.86
Table 1: The statistics of the (valid) data we collect. The
Total column presents the number of extracted NEs and
generated hypotheses and the Average column shows the
average numbers per text respectively.
2009), and we preprocess the data using the Stan-
ford named-entity recognizer (Finkel et al, 2005).
In all, it contains 600 T-H pairs, and we use the texts
to generate facts and counter-facts and hypotheses as
references. We put our task online through Crowd-
Flower2, and on average, we pay one cent for each
(counter-)fact to the Turkers. CrowdFlower can help
with finding trustful Turkers and the data were col-
lected within a few hours.
To get a sense of the quality of the data we collect,
we mainly focus on analyzing the following three
aspects: 1) the statistics of the datasets themselves;
2) the comparison between the data we collect and
the original RTE dataset; and 3) the comparison be-
tween the facts and the counter-facts.
Table 1 show some basic statistics of the data we
collect. After excluding invalid and trivial ones3, we
acquire 790 facts and 203 counter-facts. In general,
the counter-facts seem to be more difficult to obtain
than the facts, since both the total number and the
average number of the counter-facts are less than
those of the facts. Notice that the NEs are not many
since they have to appear in both T and H.
The comparison between our data and the original
RTE data is shown in Table 2. The average length of
the generated hypotheses is longer than the original
hypotheses, for both the facts and the counter-facts.
Counter-facts seem to be more verbose, since addi-
tional (contradictory) information is added. For in-
stance, example ID 425 in Table 4, Counter Fact 1
can be viewed as the more informative but contra-
dictory version of Fact 1 (and the original hypoth-
2http://crowdflower.com/
3Invalid data include empty string or single words; and the
trivial ones are those sentences directly copied from the texts.
164
esis). The average bag-of-words similarity scores
are calculated by dividing the number of overlap-
ping words of T and H by the total number of words
in H. In the original RTE dataset, the entailed hy-
potheses have a higher BoW score than the contra-
dictory ones; while in our data, facts have a lower
score than the counter-facts. This might be caused
by the greater variety of the facts than the counter-
facts. Fact 1 of example ID 425 in Table 4 is almost
the same as the original hypothesis, and Fact 2 of
example ID 374 as well, though the latter has some
slight differences which make the answer different
from the original one. The NE position in the sen-
tence is another aspect to look at. We find that peo-
ple tend to put the NEs at the beginning of the sen-
tences more than other positions, while in the RTE
datasets, NEs appear in the middle more frequently.
In order to get a feeling of the quality of the
data, we randomly sampled 50 generated facts and
counter-facts and manually compared them with the
original hypotheses. Table 3 shows that generated
facts are easier for the systems to recognize, and the
counter-facts have the same difficulty on average.
Although it is subjective to evaluate the difficulty
of the data by human reading, in general, we follow
the criteria that
1. Abstraction is more difficult than extraction;
2. Inference is more difficult than the direct en-
tailment;
3. The more sentences in T are involved, the more
difficult that T-H pair is.
Therefore, we view the Counter Fact 1 in example
ID 16 in Table 4 is more difficult than the original
hypothesis, since it requires more inference than the
direct fact validation. However, in example ID 374,
Fact 1 is easier to be verified than the original hy-
pothesis, and same as those facts in example ID 506.
Similar hypotheses (e.g. Fact 1 in example ID 425
and the original hypothesis) are treated as being at
the same level of difficulty.
After the quantitive analysis, let?s take a closer
look at the examples in Table 4. The facts are usually
constructed by rephrasing some parts of the text (e.g.
in ID 425, ?after a brief inspection? is paraphrased
by ?investigated by? in Fact 2) or making a short
Valid Harder Easier Same
Facts 76% 16% 24% 36%
Counter-Facts 84% 36% 36% 12%
Table 3: The comparison of the generated (counter-)facts
with the original hypotheses. The Valid column shows the
percentage of the valid (counter-)facts; and other columns
present the distribution of harder, easier cases than the
original hypotheses or with the same difficulty.
RTE-5 Our Data
Counter-/Facts 300/300 178/178
All ?YES? 50% 50%
BoW Baseline 57.5% 58.4%
Table 5: The results of baseline RTE systems on the data
we collected, compared with the original RTE-5 dataset.
The Counter-/Facts row shows the number of the T-H
pairs contained in the dataset; and the other scores in per-
centage are accuracy of the systems.
summary (e.g. Fact 1 in ID 374, ?George Stranahan
spoke of Thompson?s death.?). For counter-facts, re-
moving the negation words or changing into another
adjective is one common choice, e.g. in ID 374,
Counter Fact 1 removed ?n?t? and Counter Fact 3
changed ?never? into ?fully?. The antonyms can
also make the contradiction, as ?rotten? to ?great?
in Counter Fact 2 in ID 374.
Example ID 506 in Table 4 is another interest-
ing case. There are many facts about Yemen, but
no valid counter-facts are generated. Furthermore,
if we compare the generated facts with the original
hypothesis, we find that people tend to give straight-
forward facts instead of abstracts4.
At last, we show some preliminary results on test-
ing a baseline RTE system on this dataset. For
the sake of comparison, we extract a subset of the
dataset, which is balanced on entailment and con-
tradiction text pairs, and compare the results with
the same system on the original RTE-5 dataset. The
baseline system uses a simple BoW-based similar-
ity measurement between T and H (Bentivogli et al,
2009) and the results are shown in Table 5.
The results indicate that our data are slightly ?eas-
ier? than the original RTE-5 dataset, which is consis-
tent with our human evaluation on the sampled data
4But this might also be caused by the design of our task.
165
Ave. Length Ave. BoW
NE Position
Head Middle Tail
Original Entailment Hypotheses 7.6 0.76 46% 53% 1%
Facts 9.8 0.68 68% 29% 3%
Original Contradiction Hypotheses 7.5 0.72 44% 56% 0%
Counter-Facts 12.3 0.75 59% 38% 3%
Table 2: The comparison between the generated (counter-)facts and the original hypotheses from the RTE dataset. The
Ave. Length column represents the average number of words in each hypothesis; The Ave. BoW shows the average
bag-of-words similarity compared with the text. The three columns on the right are all about the position of the NE
appearing in the sentence, how likely it is at the head, middle, or tail of the sentence.
(Table 3). However, it is still too early to draw con-
clusions based on the simple baseline results.
5 Conclusion and Future Work
In this paper, we report our experience of using
MTurk to collect facts and counter-facts about the
given NEs and texts. We find that the generated hy-
potheses are not entirely the same as the original
hypotheses in the RTE data. One direct extension
would be to use more than one NE at one time, but it
may also cause problems, if those NEs do not have
any relations in-between. Another line of research
would be to test this generated resources using some
real existing RTE systems and compare the results
with the original RTE datasets, and also further ex-
plore the potential application of this resource.
Acknowledgments
The first author is supported by the PIRE scholar-
ship program. The second author is supported by
the EuroMatrixPlusProject (funded by the European
Commission), by the DARPA GALE program under
Contract No. HR0011-06-2-0001, and by the NSF
under grant IIS-0713448. The views and findings
are the authors? alone.
References
L. Bentivogli, B. Magnini, I. Dagan, H.T. Dang, and
D. Giampiccolo. 2009. The fifth pascal recogniz-
ing textual entailment challenge. In Proceedings of
the Text Analysis Conference (TAC 2009) Workshop,
Gaithersburg, Maryland, USA, November. National
Institute of Standards and Technology.
John Burger and Lisa Ferro. 2005. Generating an entail-
ment corpus from news headlines. In Proceedings of
the ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, pages 49?54, Ann Arbor,
Michigan, USA. Association for Computational Lin-
guistics.
Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox,
Johan Van Genabith, Jan Jaspars, Hans Kamp, David
Milward, Manfred Pinkal, Massimo Poesio, and Steve
Pulman. 1996. Using the framework. Technical Re-
port LRE 62-051 D-16, The FraCaS Consortium.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
pascal recognising textual entailment challenge. In
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2005),
pages 363?370. Association for Computational Lin-
guistics.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recogniz-
ing textual entailment with lcc?s groundhog system. In
Proceedings of the Second PASCAL Challenges Work-
shop.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast - but is it good?
Evaluating non- expert annotations for natural lan-
guage tasks. In Proceedings of EMNLP.
166
ID: 16 Answer: Contradiction
Original Text The father of an Oxnard teenager accused of gunning down a gay classmate who was romanti-
cally attracted to him has been found dead, Ventura County authorities said today. Bill McIner-
ney, 45, was found shortly before 8 a.m. in the living room of his Silver Strand home by a friend,
said James Baroni, Ventura County?s chief deputy medical examiner. The friend was supposed
to drive him to a court hearing in his son?s murder trial, Baroni said. McInerney?s 15-year-old
son, Brandon, is accused of murder and a hate crime in the Feb. 12, 2008, shooting death of
classmate Lawrence ?Larry? King, 15. The two boys had been sparring in the days before the
killing, allegedly because Larry had expressed a romantic interest in Brandon.
Original Hypothesis Bill McInerney is accused of killing a gay teenager.
NE 1: Bill McInerney
Counter Fact 1 Bill McInerney is still alive.
ID: 374 Answer: Contradiction
Original Text Other friends were not surprised at his death. ?I wasn?t surprised,? said George Stranahan, a
former owner of the Woody Creek Tavern, a favourite haunt of Thompson. ?I never expected
Hunter to die in a hospital bed with tubes coming out of him.? Neighbours have said how his
broken leg had prevented him from leaving his house as often as he had liked to. One neighbour
and long-standing friend, Mike Cleverly, said Thompson was clearly hobbled by the broken leg.
?Medically speaking, he?s had a rotten year.?
Original Hypothesis The Woody Creek Tavern is owned by George Stranahan.
NE 1: George Stranahan
Fact 1 George Stranahan spoke of Thompson?s death.
Fact 2 George Stranahan once owned the Woody Creek Tavern.
Counter Fact 1 George Stranahan was surprised by his friend?s death.
Counter Fact 2 Medically, George Stranahan?s friend, Humter Thompson, had a great year.
Counter Fact 3 George Stranahan fully expected Thompson to die in a hospital with tubes coming out of him.
NE 2: Woody Creek Tavern
Fact 1 Woody Creek Tavern was previously owned by George Stranahan.
ID: 425 Answer: Entailment
Original Text Merseyside Police concluded after a brief inspection that the controversial blog Liverpool Evil
Cabal does not break criminal law. However the council officers continue to search for the
editor. The blog has been blocked on computers controlled by Liverpool Direct Ltd, a company
jointly owned by Liverpool City Council and British Telecom. The council?s elected officials
have denied ordering the block and are currently investigating its origin.
Original Hypothesis Liverpool Evil Cabal is the name of an online blog.
NE 1: Liverpool Evil Cabal
Fact 1 Liverpool Evil Cabal is a web blog.
Fact 2 Liverpool Evil Cabal was a blog investigated by the Merseyside Police.
Counter Fact 1 Liverpool Evil Cabal is a blog of Liverpool Direct Ltd.
Counter Fact 2 Liverpool Evil Cabal is freed from the charges of law breaking.
ID: 506 Answer: Entailment
Original Text At least 58 people are now dead as a result of the recent flooding in Yemen, and at least 20,000
in the country have no access to shelter. Five people are also reported missing. The Yemeni
government has pledged to send tents to help the homeless. The flooding is caused by the recent
heavy rain in Yemen, which came as a shock due to the fact that the country only receives several
centimeters of rain per year.
Original Hypothesis Heavy rain caused flooding in Yemen.
NE 1: Yemen
Fact 1 58 people are dead in Yemen because of flooding.
Fact 2 5 people in Yemen are missing.
Fact 3 At least 58 people are dead in Yemen because of flooding.
Table 4: Examples of facts and counter-facts, compared with the original texts and hypotheses. We ask the Turkers to
write several (counter-)facts about the highlighted NEs, and only part of the results are shown here.
167
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 208?211,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Mechanical Turk to Build Machine Translation Evaluation Sets
Michael Bloodgood
Human Language Technology
Center of Excellence
Johns Hopkins University
bloodgood@jhu.edu
Chris Callison-Burch
Center for Language and
Speech Processing
Johns Hopkins University
ccb@cs.jhu.edu
Abstract
Building machine translation (MT) test sets is
a relatively expensive task. As MT becomes
increasingly desired for more and more lan-
guage pairs and more and more domains, it
becomes necessary to build test sets for each
case. In this paper, we investigate using Ama-
zon?s Mechanical Turk (MTurk) to make MT
test sets cheaply. We find that MTurk can
be used to make test sets much cheaper than
professionally-produced test sets. More im-
portantly, in experiments with multiple MT
systems, we find that the MTurk-produced
test sets yield essentially the same conclu-
sions regarding system performance as the
professionally-produced test sets yield.
1 Introduction
Machine translation (MT) research is empirically
evaluated by comparing system output against refer-
ence human translations, typically using automatic
evaluation metrics. One method for establishing a
translation test set is to hold out part of the training
set to be used for testing. However, this practice typ-
ically overestimates system quality when compared
to evaluating on a test set drawn from a different do-
main. Therefore, it?s necessary to make new test sets
not only for new language pairs but also for new do-
mains.
Creating reasonable sized test sets for new do-
mains can be expensive. For example, the Workshop
on Statistical Machine Translation (WMT) uses a
mix of non-professional and professional translators
to create the test sets for its annual shared translation
tasks (Callison-Burch et al, 2008; Callison-Burch
et al, 2009). For WMT09, the total cost of creat-
ing the test sets consisting of roughly 80,000 words
across 3027 sentences in seven European languages
was approximately $39,800 USD, or slightly more
than $0.08 USD/word. For WMT08, creating test
sets consisting of 2,051 sentences in six languages
was approximately $26,500 USD or slightly more
than $0.10 USD/word.
In this paper we examine the use of Amazon?s
Mechanical Turk (MTurk) to create translation test
sets for statistical machine translation research.
Snow et al (2008) showed that MTurk can be useful
for creating data for a variety of NLP tasks, and that
a combination of judgments from non-experts can
attain expert-level quality in many cases. Callison-
Burch (2009) showed that MTurk could be used for
low-cost manual evaluation of machine translation
quality, and suggested that it might be possible to
use MTurk to create MT test sets after an initial pi-
lot study where turkers (the people who complete
the work assignments posted on MTurk) produced
translations of 50 sentences in five languages.
This paper explores this in more detail by ask-
ing turkers to translate the Urdu sentences of the
Urdu-English test set used in the 2009 NIST Ma-
chine Translation Evaluation Workshop. We evalu-
ate multiple MT systems on both the professionally-
produced NIST2009 test set and our MTurk-
produced test set and find that the MTurk-produced
test set yields essentially the same conclusions about
system performance as the NIST2009 set yields.
208
2 Gathering the Translations via
Mechanical Turk
The NIST2009 Urdu-English test set1 is a pro-
fessionally produced machine translation evalua-
tion set, containing four human-produced reference
translations for each of 1792 Urdu sentences. We
posted the 1792 Urdu sentences onMTurk and asked
for translations into English. We charged $0.10 USD
per translation, giving us a total translation cost of
$179.20 USD. A challenge we encountered during
this data collection was that many turkers would
cheat, giving us fake translations. We noticed that
many turkers were pasting the Urdu into an online
machine translation system and giving us the output
as their response even though our instructions said
not to do this. We manually monitored for this and
rejected these responses and blocked these workers
from computing any of our future work assignments.
In the future, we plan to combat this in a more prin-
cipled manner by converting our Urdu sentences into
an image and posting the images. This way, the
cheating turkers will not be able to cut and paste into
a machine translation system.
We also noticed that many of the translations had
simple mistakes such as misspellings and typos. We
wanted to investigate whether these would decrease
the value of our test set so we did a second phase
of data collection where we posted the translations
we gathered and asked turkers (likely to be com-
pletely different people than the ones who provided
the initial translations) to correct simple grammar
mistakes, misspellings, and typos. For this post-
editing phase, we paid $0.25 USD per ten sentences,
giving a total post-editing cost of $44.80 USD.
In summary, we built two sets of reference trans-
lations, one with no editing, and one with post-
editing. In the next section, we present the results
of experiments that test how effective these test sets
are for evaluating MT systems.
3 Experimental Results
A main purpose of an MT test set is to evaluate vari-
ousMT systems? performances relative to each other
and assist in drawing conclusions about the relative
1http://www.itl.nist.gov/iad/894.01/tests/mt/2009/
ResultsRelease/currentUrdu.html
quality of the translations produced by the systems.2
Therefore, if a given system, say System A, out-
performs another given system, say System B, on
a high-quality professionally-produced test set, then
we would want to see that System A also outper-
forms System B on our MTurk-produced test set. It
is also desirable that the magnitudes of the differ-
ences in performance between systems also be main-
tained.
In order to measure the differences in perfor-
mance, using the differences in the absolute mag-
nitudes of the BLEU scores will not work well be-
cause the magnitudes of the BLEU scores are af-
fected by many factors of the test set being used,
such as the number of reference translations per for-
eign sentence. For determining performance differ-
ences between systems and especially for compar-
ing them across different test sets, we use percentage
of baseline performance. To compute percentage of
baseline performance, we designate one system as
the baseline system and use percentage of that base-
line system?s performance. For example, Table 1
shows both absolute BLEU scores and percentage
performance for three MT systems when tested on
five different test sets. The first test set in the table
is the NIST-2009 set with all four reference trans-
lations per Urdu sentence. The next four test sets
use only a single reference translation per Urdu sen-
tence (ref 1 uses the first reference translation only,
ref 2 the second only, etc.). Note that the BLEU
scores for the single-reference translation test sets
are much lower than for the test set with all four ref-
erence translations and the difference in the absolute
magnitudes of the BLEU scores between the three
different systems are different for the different test
sets. However, the percentage performance of the
MT systems is maintained (both the ordering of the
systems and the amount of the difference between
them) across the different test sets.
We evaluated three different MT systems on the
NIST2009 test set and on our two MTurk-produced
test sets (MTurk-NoEditing and MTurk-Edited).
Two of the MT systems (ISI Syntax (Galley et al,
2Another useful purpose would be to get some absolute
sense of the quality of the translations but that seems out of
reach currently as the values of BLEU scores (the defacto stan-
dard evaluation metric) are difficult to map to precise levels of
translation quality.
209
Eval ISI JHU Joshua
Set (Syntax) (Syntax) (Hier.)
NIST-2009 33.10 32.77 26.65
(4 refs) 100% 99.00% 80.51%
NIST-2009 17.22 16.98 14.25
(ref 1) 100% 98.61% 82.75%
NIST-2009 17.76 17.14 14.69
(ref 2) 100% 96.51% 82.71%
NIST-2009 16.94 16.54 13.80
(ref 3) 100% 97.64% 81.46%
NIST-2009 13.63 13.67 11.05
(ref 4) 100% 100.29% 81.07%
Table 1: This table shows three MT systems evaluated
on five different test sets. For each system-test set pair,
two numbers are displayed. The top number is the BLEU
score for that system when using that test set. For ex-
ample, ISI-Syntax tested on the NIST-2009 test set has
a BLEU score of 33.10. The bottom number is the per-
centage of baseline system performance that is achieved.
ISI-Syntax (the highest-performing system on NIST2009
to our knowledge) is used as the baseline. Thus, it will
always have 100% as the percentage performance for all
of the test sets. To illustrate computing the percentage
performance for the other systems, consider for JHU-
Syntax tested on NIST2009, that its BLEU score of 32.77
divided by the BLEU score of the baseline system is
32.77/33.10 ? 99.00%
2004; Galley et al, 2006) and JHU Syntax (Li et al,
2009) augmented with (Zollmann and Venugopal,
2006)) were chosen because they represent state-
of-the-art performance, having achieved the highest
scores on NIST2009 to our knowledge. They also
have very similar performance on NIST2009 so we
want to see if that similar performance is maintained
as we evaluate on our MTurk-produced test sets.
The third MT system (Joshua-Hierarchical) (Li et
al., 2009), an open source implementation of (Chi-
ang, 2007), was chosen because though it is a com-
petitive system, it had clear, markedly lower perfor-
mance on NIST2009 than the other two systems and
we want to see if that difference in performance is
also maintained if we were to shift evaluation to our
MTurk-produced test sets.
Table 2 shows the results. There are a number
of observations to make. One is that the absolute
magnitude of the BLEU scores is much lower for
all systems on the MTurk-produced test sets than on
Eval ISI JHU Joshua
Set (Syntax) (Syntax) (Hier.)
NIST- 33.10 32.77 26.65
2009 100% 99.00% 80.51%
MTurk- 13.81 13.93 11.10
NoEditing 100% 100.87% 80.38%
MTurk- 14.16 14.23 11.68
Edited 100% 100.49% 82.49%
Table 2: This table shows three MT systems evaluated us-
ing the official NIST2009 test set and the two test sets we
constructed (MTurk-NoEditing and MTurk-Edited). For
each system-test set pair, two numbers are displayed. The
top number is the BLEU score for that system when using
that test set. For example, ISI-Syntax tested on the NIST-
2009 test set has a BLEU score of 33.10. The bottom
number is the percentage of baseline system performance
that is achieved. ISI-Syntax (the highest-performing sys-
tem on NIST2009 to our knowledge) is used as the base-
line.
the NIST2009 test set. This is primarily because the
NIST2009 set had four translations per foreign sen-
tence whereas the MTurk-produced sets only have
one translation per foreign sentence. Due to this
different scale of BLEU scores, we compare perfor-
mances using percentage of baseline performance.
We use the ISI Syntax system as the baseline since
it achieved the highest results on NIST2009. The
main observation of the results in Table 2 is that
both the relative performance of the various MT sys-
tems and the amount of the differences in perfor-
mance (in terms of percentage performance of the
baseline) are maintained when we use the MTurk-
produced test sets as when we use the NIST2009 test
set. In particular, we can see that whether using the
NIST2009 test set or the MTurk-produced test sets,
one would conclude that ISI Syntax and JHU Syn-
tax perform about the same and Joshua-Hierarchical
delivers about 80% of the performance of the two
syntax systems. The post-edited test set did not
yield different conclusions than the non-edited test
set yielded so the value of post-editing for test set
creation remains an open question.
4 Conclusions and Future Work
In conclusion, we have shown that it is feasible to
use MTurk to build MT evaluation sets at a sig-
210
nificantly reduced cost. But the large cost sav-
ings does not hamper the utility of the test set for
evaluating systems? translation quality. In exper-
iments, MTurk-produced test sets lead to essen-
tially the same conclusions about multiple MT sys-
tems? translation quality as much more expensive
professionally-produced MT test sets.
It?s important to be able to build MT test sets
quickly and cheaply because we need new ones for
new domains (as discussed in Section 1). Now that
we have shown the feasibility of using MTurk to
build MT test sets, in the future we plan to build
new MT test sets for specific domains (e.g., enter-
tainment, science, etc.) and release them to the com-
munity to spur work on domain-adaptation for MT.
We also envision using MTurk to collect addi-
tional training data to tune an MT system for a new
domain. It?s been shown that active learning can be
used to reduce training data annotation burdens for
a variety of NLP tasks (see, e.g., (Bloodgood and
Vijay-Shanker, 2009)). Therefore, in future work,
we plan to use MTurk combined with an active
learning approach to gather new data in the new do-
main to investigate improving MT performance for
specialized domains. But we?ll need new test sets in
the specialized domains to be able to evaluate the ef-
fectiveness of this line of research and therefore, we
will need to be able to build new test sets. In light of
the findings we presented in this paper, it seems we
can build those test sets using MTurk for relatively
low costs without sacrificing much in their utility for
evaluating MT systems.
Acknowledgements
This research was supported by the EuroMatrix-
Plus project funded by the European Commission,
by the DARPA GALE program under Contract No.
HR0011-06-2-0001, and the NSF under grant IIS-
0713448. Thanks to Amazon Mechanical Turk for
providing a $100 credit.
References
Michael Bloodgood and K Vijay-Shanker. 2009. Taking
into account the differences between actively and pas-
sively acquired data: The case of active learning with
support vector machines for imbalanced datasets. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 137?140, Boulder, Colorado, June. Association
for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation (WMT09), March.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, pages 286?295, Singapore, August. Association
for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the Human Language Technology Con-
ference of the North American chapter of the Asso-
ciation for Computational Linguistics (HLT/NAACL-
2004), Boston, Massachusetts.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics (ACL-
CoLing-2006), Sydney, Australia.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, pages 135?
139, Athens, Greece, March. Association for Compu-
tational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP-2008,
Honolulu, Hawaii.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the NAACL-2006 Workshop on Statis-
tical Machine Translation (WMT-06), New York, New
York.
211
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 17?53,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Findings of the 2010 Joint Workshop on
Statistical Machine Translation and Metrics for Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb@cs.jhu.edu
Philipp Koehn
University of Edinburgh
pkoehn@inf.ed.ac.uk
Christof Monz
University of Amsterdam
c.monz@uva.nl
Kay Peterson and Mark Przybocki
National Institute of Standards and Technology
kay.peterson,mark.przybocki@nist.gov
Omar F. Zaidan
Johns Hopkins University
ozaidan@cs.jhu.edu
Abstract
This paper presents the results of the
WMT10 and MetricsMATR10 shared
tasks,1 which included a translation task,
a system combination task, and an eval-
uation task. We conducted a large-scale
manual evaluation of 104 machine trans-
lation systems and 41 system combina-
tion entries. We used the ranking of these
systems to measure how strongly auto-
matic metrics correlate with human judg-
ments of translation quality for 26 metrics.
This year we also investigated increasing
the number of human judgments by hiring
non-expert annotators through Amazon?s
Mechanical Turk.
1 Introduction
This paper presents the results of the shared
tasks of the joint Workshop on statistical Ma-
chine Translation (WMT) and Metrics for MA-
chine TRanslation (MetricsMATR), which was
held at ACL 2010. This builds on four previ-
ous WMT workshops (Koehn and Monz, 2006;
Callison-Burch et al, 2007; Callison-Burch et al,
2008; Callison-Burch et al, 2009), and one pre-
vious MetricsMATR meeting (Przybocki et al,
2008). There were three shared tasks this year:
a translation task between English and four other
European languages, a task to combine the out-
put of multiple machine translation systems, and
a task to predict human judgments of translation
quality using automatic evaluation metrics. The
1The MetricsMATR analysis was not complete in time for
the publication deadline. An updated version of paper will be
made available on http://statmt.org/wmt10/ prior
to July 15, 2010.
performance on each of these shared task was de-
termined after a comprehensive human evaluation.
There were a number of differences between
this year?s workshop and last year?s workshop:
? Non-expert judgments ? In addition to hav-
ing shared task participants judge translation
quality, we also collected judgments from
non-expert annotators hired through Ama-
zon?s Mechanical Turk. By collecting a large
number of judgments we hope to reduce the
burden on shared task participants, and to in-
crease the statistical significance of our find-
ings. We discuss the feasibility of using non-
experts evaluators, by analyzing the cost, vol-
ume and quality of non-expert annotations.
? Clearer results for system combination ?
This year we excluded Google translations
from the systems used in system combina-
tion. In last year?s evaluation, the large mar-
gin between Google and many of the other
systems meant that it was hard to improve on
when combining systems. This year, the sys-
tem combinations perform better than their
component systems more often than last year.
? Fewer rule-based systems ? This year there
were fewer rule-based systems submitted. In
past years, University of Saarland compiled a
large set of outputs from rule-based machine
translation (RBMT) systems. The RBMT
systems were not submitted this year. This
is unfortunate, because they tended to outper-
form the statistical systems for German, and
they were often difficult to rank properly us-
ing automatic evaluation metrics.
The primary objectives of this workshop are to
evaluate the state of the art in machine transla-
17
tion, to disseminate common test sets and pub-
lic training data with published performance num-
bers, and to refine evaluation methodologies for
machine translation. As with past years, all of the
data, translations, and human judgments produced
for our workshop are publicly available.2 We hope
they form a valuable resource for research into sta-
tistical machine translation, system combination,
and automatic evaluation of translation quality.
2 Overview of the shared translation and
system combination tasks
The workshop examined translation between En-
glish and four other languages: German, Span-
ish, French, and Czech. We created a test set for
each language pair by translating newspaper arti-
cles. We additionally provided training data and
two baseline systems.
2.1 Test data
The test data for this year?s task was created
by hiring people to translate news articles that
were drawn from a variety of sources from mid-
December 2009. A total of 119 articles were se-
lected, in roughly equal amounts from a variety
of Czech, English, French, German and Spanish
news sites:3
Czech: iDNES.cz (5), iHNed.cz (1), Lidov-
ky (16)
French: Les Echos (25)
Spanish: El Mundo (20), ABC.es (4), Cinco
Dias (11)
English: BBC (5), Economist (2), Washington
Post (12), Times of London (3)
German: Frankfurter Rundschau (11), Spie-
gel (4)
The translations were created by the profes-
sional translation agency CEET4. All of the trans-
lations were done directly, and not via an interme-
diate language.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
2http://statmt.org/wmt10/results.html
3For more details see the XML test files. The docid
tag gives the source and the date for each document in the
test set, and the origlang tag indicates the original source
language.
4http://www.ceet.eu/
train language models, and development sets to
tune parameters. Some statistics about the train-
ing materials are given in Figure 1.
2.3 Baseline systems
To lower the barrier of entry for newcomers to
the field, we provided two open source toolkits
for phrase-based and parsing-based statistical ma-
chine translation (Koehn et al, 2007; Li et al,
2009).
2.4 Submitted systems
We received submissions from 33 groups from 29
institutions, as listed in Table 1, a 50% increase
over last year?s shared task.
We also evaluated 2 commercial off the shelf
MT systems, and two online statistical machine
translation systems. We note that these companies
did not submit entries themselves. The entries for
the online systems were done by translating the
test data via their web interfaces. The data used
to train the online systems is unconstrained. It is
possible that part of the reference translations that
were taken from online news sites could have been
included in the online systems? language models.
2.5 System combination
In total, we received 153 primary system submis-
sions along with 28 secondary submissions. These
were made available to participants in the sys-
tem combination shared task. Based on feedback
that we received on last year?s system combina-
tion task, we provided two additional resources to
participants:
? Development set: We reserved 25 articles
to use as a dev set for system combination.
These were translated by all participating
sites, and distributed to system combination
participants along with reference translations.
? n-best translations: We requested n-best
lists from sites whose systems could produce
them. We received 20 n-best lists accompa-
nying the system submissions.
Table 2 lists the 9 participants in the system
combination task.
3 Human evaluation
As with past workshops, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention
18
Europarl Training Corpus
Spanish? English French? English German? English
Sentences 1,650,152 1,683,156 1,540,549
Words 47,694,560 46,078,122 50,964,362 47,145,288 40,756,801 43,037,967
Distinct words 173,033 95,305 123,639 95,846 316,365 92,464
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 98,598 84,624 100,269 94,742
Words 2,724,141 2,432,064 2,405,082 2,101,921 2,505,583 2,443,183 2,050,545 2,290,066
Distinct words 69,410 46,918 53,763 43,906 101,529 47,034 125,678 45,306
United Nations Training Corpus
Spanish? English French? English
Sentences 6,222,450 7,230,217
Words 213,877,170 190,978,737 243,465,100 216,052,412
Distinct words 441,517 361,734 402,491 412,815
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 7,227,409
Words 72,993,427 84,856,749
Distinct words 1,088,642 522,770
Europarl Language Model Data
English Spanish French German
Sentence 1,843,035 1,822,021 1,855,589 1,772,039
Words 50,132,615 51,223,902 54,273,514 43,781,217
Distinct words 99,206 178,934 127,689 328,628
News Language Model Data
English Spanish French German Czech
Sentence 48,653,884 3,857,414 15,670,745 17,474,133 13,042,040
Words 1,148,480,525 106,716,219 382,563,246 321,165,206 205,614,201
Distinct words 1,451,719 548,169 998,595 1,855,993 1,715,376
News Test Set
English Spanish French German Czech
Sentences 2489
Words 62,988 65,654 68,107 62,390 53,171
Distinct words 9,457 11,409 10,775 12,718 15,825
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and
the number of distinct words is based on the provided tokenizer.
19
ID Participant
AALTO Aalto University, Finland (Virpioja et al, 2010)
CAMBRIDGE Cambridge University (Pino et al, 2010)
CMU Carnegie Mellon University?s Cunei system (Phillips, 2010)
CMU-STATXFER Carnegie Mellon University?s statistical transfer system (Hanneman et al, 2010)
COLUMBIA Columbia University
CU-BOJAR Charles University Bojar (Bojar and Kos, 2010)
CU-TECTO Charles University Tectogramatical MT (Z?abokrtsky? et al, 2010)
CU-ZEMAN Charles University Zeman (Zeman, 2010)
DCU Dublin City University (Penkale et al, 2010)
DFKI Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (Federmann et al, 2010)
EU European Parliament, Luxembourg (Jellinghaus et al, 2010)
EUROTRANS commercial MT provider from the Czech Republic
FBK Fondazione Bruno Kessler (Hardmeier et al, 2010)
GENEVA University of Geneva
HUICONG Shanghai Jiao Tong University (Cong et al, 2010)
JHU Johns Hopkins University (Schwartz, 2010)
KIT Karlsruhe Institute for Technology (Niehues et al, 2010)
KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)
LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al, 2010)
LIMSI LIMSI (Allauzen et al, 2010)
LIU Linko?ping University (Stymne et al, 2010)
LIUM University of Le Mans (Lambert et al, 2010)
NRC National Research Council Canada (Larkin et al, 2010)
ONLINEA an online machine translation system
ONLINEB an online machine translation system
PC-TRANS commercial MT provider from the Czech Republic
POTSDAM Potsdam University
RALI RALI - Universite? de Montre?al (Huet et al, 2010)
RWTH RWTH Aachen (Heger et al, 2010)
SFU Simon Fraser University (Sankaran et al, 2010)
UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010)
UEDIN University of Edinburgh (Koehn et al, 2010)
UMD University of Maryland (Eidelman et al, 2010)
UPC Universitat Polite`cnica de Catalunya (Henr??quez Q. et al, 2010)
UPPSALA Uppsala University (Tiedemann, 2010)
UPV Universidad Polite?cnica de Valencia (Sanchis-Trilles et al, 2010)
UU-MS Uppsala University - Saers (Saers et al, 2010)
Table 1: Participants in the shared translation task. Not all groups participated in all language pairs.
20
ID Participant
BBN-COMBO BBN system combination (Rosti et al, 2010)
CMU-COMBO-HEAFIELD CMU system combination (Heafield and Lavie, 2010)
CMU-COMBO-HYPOSEL CMU system combo with hyp. selection (Hildebrand and Vogel, 2010)
DCU-COMBO Dublin City University system combination (Du et al, 2010)
JHU-COMBO Johns Hopkins University system combination (Narsale, 2010)
KOC-COMBO Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)
LIUM-COMBO University of Le Mans system combination (Barrault, 2010)
RWTH-COMBO RWTH Aachen system combination (Leusch and Ney, 2010)
UPV-COMBO Universidad Polite?cnica de Valencia (Gonza?lez-Rubio et al, 2010)
Table 2: Participants in the system combination task.
Language Pair Sentence Ranking Edited Translations Yes/No Judgments
German-English 5,212 830 824
English-German 6,847 755 751
Spanish-English 5,653 845 845
English-Spanish 2,587 920 690
French-English 4,147 925 921
English-French 3,981 1,325 1,223
Czech-English 2,688 490 488
English-Czech 6,769 1,165 1,163
Totals 37,884 7,255 6,905
Table 3: The number of items that were collected for each task during the manual evaluation. An item
is defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/no
judgment in the judgment task.
21
that automatic measures are an imperfect substi-
tute for human assessment of translation quality.
Therefore, we define the manual evaluation to be
primary, and use the human judgments to validate
automatic metrics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct it on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partic-
ipants, interested volunteers, and a small number
of paid annotators. More than 120 people partic-
ipated in the manual evaluation5, with 89 people
putting in more than an hour?s worth of effort, and
29 putting in more than four hours. A collective
total of 337 hours of labor was invested.6
We asked people to evaluate the systems? output
in two different ways:
? Ranking translated sentences relative to each
other. This was our official determinant of
translation quality.
? Editing the output of systems without dis-
playing the source or a reference translation,
and then later judging whether edited transla-
tions were correct.
The total number of judgments collected for the
different modes of annotation is given in Table 3.
In all cases, the output of the various translation
systems were judged on equal footing; the output
of system combinations was judged alongside that
of the individual system, and the constrained and
unconstrained systems were judged together.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the in-
structions simple:
Rank translations from Best to Worst rel-
ative to the other choices (ties are al-
lowed).
5We excluded data from three errant annotators, identified
as follows. We considered annotators completing at least 3
screens, whose P (A) with others (see 3.2) is less than 0.33.
Out of seven such annotators, four were affiliated with shared
task teams. The other three had no apparent affiliation, and
so we discarded their data, less than 5% of the total data.
6Whenever an annotator appears to have spent more than
ten minutes on a single screen, we assume they left their sta-
tion and left the window open, rather than actually needing
more than ten minutes. In those cases, we assume the time
spent to be ten minutes.
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions. For each of the lan-
guage pairs, there were more than 5 submissions.
We did not attempt to get a complete ordering over
the systems, and instead relied on random selec-
tion and a reasonably large sample size to make
the comparisons fair.
Relative ranking is our official evaluation met-
ric. Individual systems and system combinations
are ranked based on how frequently they were
judged to be better than or equal to any other sys-
tem. The results of this are reported in Section 4.
Appendix A provides detailed tables that contain
pairwise comparisons between systems.
3.2 Inter- and Intra-annotator agreement in
the ranking task
We were interested in determining the inter- and
intra-annotator agreement for the ranking task,
since a reasonable degree of agreement must ex-
ist to support our process as a valid evaluation
setup. To ensure we had enough data to measure
agreement, we purposely designed the sampling of
source segments shown to annotators so that items
were likely to be repeated, both within an annota-
tor?s assigned tasks and across annotators. We did
so by assigning an annotator a batch of 20 screens
(each with three ranking sets; see 3.1) that were to
be completed in full before generating new screens
for that annotator.
Within each batch, the source segments for nine
of the 20 screens (45%) were chosen from a small
pool of 60 source segments, instead of being sam-
pled from the larger pool of 1,000 source segments
designated for the ranking task.7 The larger pool
was used to choose source segments for nine other
screens (also 45%). As for the remaining two
screens (10%), they were chosen randomly from
the set of eighteen screens already chosen. Fur-
thermore, in the two ?local repeat? screens, the
system choices were also preserved.
Heavily sampling from a small pool of source
segments ensured we had enough data to measure
inter-annotator agreement, while purposely mak-
ing 10% of each annotator?s screens repeats of pre-
viously seen sets in the same batch ensured we
7Each language pair had its own 60-sentence pool, dis-
joint from other language pairs? pools, but ach of the 60-
sentence pools was a subset of the 1,000-sentence pool.
22
INTER-ANNOTATOR AGREEMENT
P (A) K
With references 0.658 0.487
Without references 0.626 0.439
WMT ?09 0.549 0.323
INTRA-ANNOTATOR AGREEMENT
P (A) K
With references 0.755 0.633
Without references 0.734 0.601
WMT ?09 0.707 0.561
Table 4: Inter- and intra-annotator agreement for
the sentence ranking task. In this task, P (E) is
0.333.
had enough data to measure intra-annotator agree-
ment.
We measured pairwise agreement among anno-
tators using the kappa coefficient (K), which is de-
fined as
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance.
For inter-annotator agreement for the ranking
tasks we calculated P (A) by examining all pairs
of systems which had been judged by two or more
judges, and calculated the proportion of time that
they agreed thatA > B, A = B, orA < B. Intra-
annotator agreement was computed similarly, but
we gathered items that were annotated on multiple
occasions by a single annotator.
Table 4 gives K values for inter-annotator and
intra-annotator agreement. These give an indi-
cation of how often different judges agree, and
how often single judges are consistent for repeated
judgments, respectively. The exact interpretation
of the kappa coefficient is difficult, but according
to Landis and Koch (1977), 0? .2 is slight, .2? .4
is fair, .4 ? .6 is moderate, .6 ? .8 is substantial
and the rest is almost perfect.
Based on these interpretations the agreement
for sentence-level ranking is moderate for inter-
annotator agreement and substantial for intra-
annotator agreement. These levels of agreement
are higher than in previous years, partially due to
the fact that that year we randomly included the
references along the system outputs. In general,
judges tend to rank the reference as the best trans-
lation, so people have stronger levels of agreement
when it is included. That said, even when compar-
isons involving reference are excluded, we still see
an improvement in agreement levels over last year.
3.3 Editing machine translation output
In addition to simply ranking the output of sys-
tems, we also had people edit the output of MT
systems. We did not show them the reference
translation, which makes our edit-based evalu-
ation different from the Human-targeted Trans-
lation Edit Rate (HTER) measure used in the
DARPA GALE program (NIST, 2008). Rather
than asking people to make the minimum number
of changes to the MT output in order capture the
same meaning as the reference, we asked them to
edit the translation to be as fluent as possible with-
out seeing the reference. Our hope was that this
would reflect people?s understanding of the out-
put.
The instructions given to our judges were as fol-
lows:
Correct the translation displayed, mak-
ing it as fluent as possible. If no correc-
tions are needed, select ?No corrections
needed.? If you cannot understand the
sentence well enough to correct it, select
?Unable to correct.?
A screenshot is shown in Figure 2. This year,
judges were shown the translations of 5 consec-
utive source sentences, all produced by the same
machine translation system. In last year?s WMT
evaluation they were shown only one sentence at a
time, which made the task more difficult because
the surrounding context could not be used as an
aid to understanding.
Since we wanted to prevent judges from see-
ing the reference before editing the translations,
we split the test set between the sentences used
in the ranking task and the editing task (because
they were being conducted concurrently). More-
over, annotators edited only a single system?s out-
put for one source sentence to ensure that their un-
derstanding of it would not be influenced by an-
other system?s output.
3.4 Judging the acceptability of edited output
Halfway through the manual evaluation period, we
stopped collecting edited translations, and instead
asked annotators to do the following:
23
Edit Machine Translation Outputs
Instructions:
You are shown several machine translation outputs.
Your task is to edit each translation to make it as fluent as possible.
It is possible that the translation is already fluent. In that case, select No corrections needed.
If you cannot understand the sentence well enough to correct it, select Unable to correct.
The sentences are all from the same article. You can use the earlier and later sentences
to help understand a confusing sentence.
Your edited translations           The machine translations
   
The shortage of snow in mountain worries the hoteliers
Edited     No corrections needed     Unable to
correct         Reset
 
The shortage of snow in mountain
worries the hoteliers
   
The deserted tracks are not putting down problem only at the exploitants 
of skilift.
Edited     No corrections needed     Unable to
correct         Reset
 
The deserted tracks are not
putting down problem only at the
exploitants of skilift.
   
The lack of snow deters the people to reserving their stays at the ski in 
the hotels and pension.
Edited     No corrections needed     Unable to
correct         Reset
 
The lack of snow deters the people
to reserving their stays at the ski
in the hotels and pension.
   
Thereby, is always possible to track free bedrooms for all the dates in 
winter, including Christmas and Nouvel An.
Edited     No corrections needed     Unable to
correct         Reset
 
Thereby, is always possible to
track free bedrooms for all the
dates in winter, including
Christmas and Nouvel An.
   
We have many of visit on our site
Figure 2: This screenshot shows what an annotator sees when beginning to edit the output of a machine
translation system.
24
Indicate whether the edited transla-
tions represent fully fluent and meaning-
equivalent alternatives to the reference
sentence. The reference is shown with
context, the actual sentence is bold.
In addition to edited translations, unedited items
that were either marked as acceptable or as incom-
prehensible were also shown. Judges gave a sim-
ple yes/no indication to each item.
4 Translation task results
We used the results of the manual evaluation to
analyze the translation quality of the different sys-
tems that were submitted to the workshop. In our
analysis, we aimed to address the following ques-
tions:
? Which systems produced the best translation
quality for each language pair?
? Did the system combinations produce better
translations than individual systems?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 5 shows the best individual systems. We
define the best systems as those which had no
other system that was statistically significantly
better than them under the Sign Test at p ? 0.1.
Multiple systems are listed as the winners for
many language pairs because it was not possible to
draw a statistically significant difference between
the systems. There is no individual system clearly
outperforming all other systems across the differ-
ent language pairs. With the exception of French-
English and English-French one can observe that
top-performing constrained systems did as well as
the unconstrained system ONLINEB.
Table 6 shows the best combination systems.
For all language directions, except Spanish-
English, one can see that the system combina-
tion runs outperform the individual systems and
that in most cases the differences are statistically
significant. While this is to be expected, system
combination is not guaranteed to improve perfor-
mance as some of the lower ranked combination
runs show, which are outperformed by individual
systems. Also note that except for Czech-English
translation the online systems ONLINEA and ON-
LINEB where not included for the system combi-
nation runs
Understandability
Our hope is that judging the acceptability of edited
output as discussed in Section 3 gives some indi-
cation of how often a system?s output was under-
standable. Figure 3 gives the percentage of times
that each system?s edited output was judged to
be acceptable (the percentage also factors in in-
stances when judges were unable to improve the
output because it was incomprehensible).
This style of manual evaluation is experimental
and should not be taken to be authoritative. Some
caveats about this measure:
? There are several sources of variance that are
difficult to control for: some people are better
at editing, and some sentences are more dif-
ficult to edit. Therefore, variance in the un-
derstandability of systems is difficult to pin
down.
? The acceptability measure does not strongly
correlate with the more established method of
ranking translations relative to each other for
all the language pairs.
5 Shared evaluation task overview
In addition to allowing the analysis of subjective
translation quality measures for different systems,
the judgments gathered during the manual evalu-
ation may be used to evaluate how well the au-
tomatic evaluation metrics serve as a surrogate to
the manual evaluation processes. NIST began run-
ning a ?Metrics for MAchine TRanslation? chal-
lenge (MetricsMATR), and presented their find-
ings at a workshop at AMTA (Przybocki et al,
2008). This year we conducted a joint Metrics-
MATR and WMT workshop, with NIST running
the shared evaluation task and analyzing the re-
sults.
In this year?s shared evaluation task 14 different
research groups submitted a total of 26 different
automatic metrics for evaluation:
Aalto University of Science and Technology
(Dobrinkat et al, 2010)
? MT-NCD ? A machine translation metric
based on normalized compression distance
(NCD), a general information-theoretic mea-
sure of string similarity. MT-NCD mea-
sures the surface level similarity between two
strings with a general compression algorithm.
More similar strings can be represented with
25
French-English
551?755 judgments per system
System C? ?others
LIUM ?? Y 0.71
ONLINEB ? N 0.71
NRC ?? Y 0.66
CAMBRIDGE ?? Y +GW 0.66
LIMSI ? Y +GW 0.65
UEDIN Y 0.65
RALI ?? Y +GW 0.65
JHU Y 0.59
RWTH ?? Y +GW 0.55
LIG Y 0.53
ONLINEA N 0.52
CMU-STATXFER Y 0.51
HUICONG Y 0.51
DFKI N 0.42
GENEVA Y 0.27
CU-ZEMAN Y 0.21
English-French
664?879 judgments per system
System C? ?others
UEDIN ?? Y 0.70
ONLINEB ? N 0.68
RALI ?? Y +GW 0.66
LIMSI ?? Y +GW 0.66
RWTH ?? Y +GW 0.63
CAMBRIDGE ? Y +GW 0.63
LIUM Y 0.63
NRC Y 0.62
ONLINEA N 0.55
JHU Y 0.53
DFKI N 0.40
GENEVA Y 0.35
EU N 0.32
CU-ZEMAN Y 0.26
KOC Y 0.26
Czech-English
788?868 judgments per system
System C? ?others
ONLINEB ? N 0.7
UEDIN ? Y 0.61
CMU Y 0.55
CU-BOJAR N 0.55
AALTO Y 0.43
ONLINEA N 0.37
CU-ZEMAN Y 0.22
German-English
723?879 judgments per system
System C? ?others
ONLINEB ? N 0.73
KIT ?? Y +GW 0.72
UMD ?? Y 0.68
UEDIN ? Y 0.66
FBK ? Y +GW 0.66
ONLINEA ? N 0.63
RWTH Y +GW 0.62
LIU Y 0.59
UU-MS Y 0.55
JHU Y 0.53
LIMSI Y +GW 0.52
UPPSALA Y 0.51
DFKI N 0.50
HUICONG Y 0.47
CMU Y 0.46
AALTO Y 0.42
CU-ZEMAN Y 0.36
KOC Y 0.23
English-German
1284?1542 judgments per system
System C? ?others
ONLINEB ? N 0.70
DFKI ? N 0.62
UEDIN ?? Y 0.62
KIT ? Y 0.60
ONLINEA N 0.59
FBK ? Y 0.56
LIU Y 0.55
RWTH Y 0.51
LIMSI Y 0.51
UPPSALA Y 0.47
JHU Y 0.46
SFU Y 0.34
KOC Y 0.30
CU-ZEMAN Y 0.28
English-Czech
1375?1627 judgments per system
System C? ?others
ONLINEB ? N 0.70
CU-BOJAR ? N 0.66
PC-TRANS ? N 0.62
UEDIN ?? Y 0.62
CU-TECTO Y 0.60
EUROTRANS N 0.54
CU-ZEMAN Y 0.50
SFU Y 0.45
ONLINEA N 0.44
POTSDAM Y 0.44
DCU N 0.38
KOC Y 0.33
Spanish-English
1448?1577 judgments per system
System C? ?others
ONLINEB ? N 0.70
UEDIN ?? Y 0.69
CAMBRIDGE Y +GW 0.61
JHU Y 0.61
ONLINEA N 0.54
UPC ? Y 0.51
HUICONG Y 0.50
DFKI N 0.45
COLUMBIA Y 0.45
CU-ZEMAN Y 0.27
English-Spanish
540?722 judgments per system
System C? ?others
ONLINEB ? N 0.71
ONLINEA ? N 0.69
UEDIN ? Y 0.61
DCU N 0.61
DFKI ? N 0.55
JHU ? Y 0.55
UPV ? Y 0.55
CAMBRIDGE ? Y +GW 0.54
UHC-UPV ? Y 0.54
SFU Y 0.40
CU-ZEMAN Y 0.23
KOC Y 0.19
Systems are listed in the order of how often their translations were ranked higher than or equal to any other system. Ties are
broken by direct comparison.
C? indicates constrained condition, meaning only using the supplied training data, standard monolingual linguistic tools, and
optionally the LDC?s GigaWord, which was allowed this year (entries that used the GigaWord are marked +GW).
? indicates a win in the category, meaning that no other system is statistically significantly better at p-level?0.1 in pairwise
comparison.
? indicates a constrained win, no other constrained system is statistically better.
For all pairwise comparisons between systems, please check the appendix.
Table 5: Official results for the WMT10 translation task, based on the human evaluation (ranking trans-
lations relative to each other)
26
French-English
589?716 judgments per combo
System ?others
RWTH-COMBO ? 0.77
CMU-HYP-COMBO ? 0.77
DCU-COMBO ? 0.72
LIUM ? 0.71
CMU-HEA-COMBO ? 0.70
UPV-COMBO ? 0.68
NRC 0.66
CAMBRIDGE 0.66
UEDIN ? 0.65
LIMSI ? 0.65
JHU-COMBO 0.65
RALI 0.65
LIUM-COMBO 0.64
BBN-COMBO 0.64
RWTH 0.55
English-French
740?829 judgments per combo
System ?others
RWTH-COMBO ? 0.75
CMU-HEA-COMBO ? 0.74
UEDIN 0.70
KOC-COMBO ? 0.68
UPV-COMBO 0.66
RALI ? 0.66
LIMSI 0.66
RWTH 0.63
CAMBRIDGE 0.63
Czech-English
766?843 judgments per combo
System ?others
CMU-HEA-COMBO ? 0.71
ONLINEB ? 0.7
BBN-COMBO ? 0.70
RWTH-COMBO ? 0.65
UPV-COMBO ? 0.63
JHU-COMBO 0.62
UEDIN 0.61
German-English
743?835 judgments per combo
System ?others
BBN-COMBO ? 0.77
RWTH-COMBO ? 0.75
CMU-HEA-COMBO 0.73
KIT ? 0.72
UMD ? 0.68
JHU-COMBO 0.67
UEDIN ? 0.66
FBK 0.66
CMU-HYP-COMBO 0.65
UPV-COMBO 0.64
RWTH 0.62
KOC-COMBO 0.59
English-German
1340?1469 judgments per combo
System ?others
RWTH-COMBO ? 0.65
DFKI ? 0.62
UEDIN ? 0.62
KIT ? 0.60
CMU-HEA-COMBO ? 0.59
KOC-COMBO 0.59
FBK ? 0.56
UPV-COMBO 0.55
English-Czech
1405?1496 judgments per combo
System ?others
DCU-COMBO ? 0.75
ONLINEB ? 0.70
RWTH-COMBO 0.70
CMU-HEA-COMBO 0.69
UPV-COMBO 0.68
CU-BOJAR 0.66
KOC-COMBO 0.66
PC-TRANS 0.62
UEDIN 0.62
Spanish-English
1385?1535 judgments per combo
System ?others
UEDIN ? 0.69
CMU-HEA-COMBO ? 0.66
UPV-COMBO ? 0.66
BBN-COMBO 0.62
JHU-COMBO 0.55
UPC 0.51
English-Spanish
516?673 judgments per combo
System ?others
CMU-HEA-COMBO ? 0.68
KOC-COMBO 0.62
UEDIN ? 0.61
UPV-COMBO 0.60
RWTH-COMBO 0.59
DFKI ? 0.55
JHU 0.55
UPV 0.55
CAMBRIDGE ? 0.54
UPV-NNLM ? 0.54
System combinations are listed in the order of how often their translations were ranked higher than or equal to any other system.
Ties are broken by direct comparison. We show the best individual systems alongside the system combinations, since the goal
of combination is to produce better quality translation than the component systems.
? indicates a win for the system combination meaning that no other system or system combination is statistically signifi-
cantly better at p-level?0.1 in pairwise comparison.
? indicates an individual system that none of the system combinations beat by a statistically significant margin at p-
level?0.1.
For all pairwise comparisons between systems, please check the appendix.
Note: ONLINEA and ONLINEB were not included among the systems being combined in the system combination shared tasks,
except in the Czech-English and English-Czech conditions, where ONLINEB was included.
Table 6: Official results for the WMT10 system combination task, based on the human evaluation (rank-
ing translations relative to each other)
27
System % Yes Yes count No count N/A count Total count *** en-cz ***
ref 0.97 63 2 0 65 en-cz
dcu-c 0.58 29 21 0 50 en-cz
onlineB 0.55 22 18 0 40 en-cz
rwth-c 0.49 56 59 0 115 en-cz
koc-c 0.45 29 36 0 65 en-cz
pc-trans 0.43 26 34 0 60 en-cz
upv-c 0.42 23 32 0 55 en-cz
cu-bojar 0.4 20 30 0 50 en-cz
eurotrans 0.4 18 27 0 45 en-cz
uedin 0.34 24 46 0 70 en-cz
cu-tecto 0.34 29 55 1 85 en-cz
cmu-hea-c 0.29 13 32 0 45 en-cz
sfu 0.24 14 44 0 58 en-cz
potsdam 0.24 13 42 0 55 en-cz
cu-zeman 0.21 15 55 0 70 en-cz
koc 0.21 21 79 0 100 en-cz
onlineA 0.2 13 52 0 65 en-cz
dcu 0.19 13 57 0 70 en-cz
0.1260077028
*** en-de ***
ref 0.94 47 3 0 50 en-de
onlineA 0.8 20 5 0 25 en-de
koc-c 0.68 17 8 0 25 en-de
uppsala 0.65 26 14 0 40 en-de
uedin 0.62 50 30 0 80 en-de
kit 0.62 37 23 0 60 en-de
upv-c 0.57 30 23 0 53 en-de
onlineB 0.52 21 19 0 40 en-de
dfki 0.52 13 12 0 25 en-de
koc 0.51 18 17 0 35 en-de
limsi 0.51 18 16 1 35 en-de
liu 0.51 28 27 0 55 en-de
rwth 0.5 15 15 0 30 en-de
rwth-c 0.49 22 23 0 45 en-de
jhu 0.48 12 13 0 25 en-de
cmu-hea-c 0.47 14 16 0 30 en-de
fbk 0.4 4 6 0 10 en-de
sfu 0.31 11 24 0 35 en-de
cu-zeman 0.19 10 40 3 53 en-de
0.1364453014
System % Yes Yes count No count N/A count Total count *** en-es ***
ref 0.83 48 10 0 58 en-es
onlineB 0.58 25 18 0 43 en-es
upv 0.5 20 20 0 40 en-es
rwth-c 0.46 13 15 0 28 en-es
dcu 0.42 16 22 0 38 en-es
koc 0.4 17 24 1 42 en-es
upv-nnlm 0.39 15 23 0 38 en-es
onlineA 0.38 11 18 0 29 en-es
jhu 0.38 17 27 1 45 en-es
koc-c 0.38 20 33 0 53 en-es
uedin 0.36 12 21 0 33 en-es
upb-c 0.32 13 27 0 40 en-es
cmu-hea-c 0.32 16 34 0 50 en-es
camb 0.3 12 27 1 40 en-es
dfki 0.29 7 17 0 24 en-es
cu-zeman 0.29 16 39 0 55 en-es
sfu 0.26 9 25 0 34 en-es
0.0845946216
System % Yes Yes count No count N/A count Total count *** en-fr ***
ref 0.91 64 4 2 70 en-fr
rwth-c 0.54 27 23 0 50 en-fr
onlineB 0.52 47 42 1 90 en-fr
upv-c 0.51 34 33 0 67 en-fr
koc-c 0.48 32 34 0 66 en-fr
uedin 0.48 30 32 1 63 en-fr
rali 0.47 21 24 0 45 en-fr
rwth 0.45 25 30 0 55 en-fr
lium 0.43 20 27 0 47 en-fr
camb 0.42 26 36 0 62 en-fr
onlineA 0.41 15 22 0 37 en-fr
limsi 0.37 26 44 0 70 en-fr
jhu 0.37 27 46 0 73 en-fr
nrc 0.36 13 23 0 36 en-fr
cmu-hea-c 0.32 22 47 0 69 en-fr
geneva 0.31 32 70 0 102 en-fr
eu 0.3 13 30 0 43 en-fr
dfki 0.28 16 42 0 58 en-fr
koc 0.21 12 44 1 57 en-fr
cu-zeman 0.17 11 52 0 63 en-fr
0.1045877454
System % Yes Yes count No count N/A count Total count *** cz-en ***
ref 1.00 33 0 0 33 cz-en
cu-bojar 0.6 3 2 0 5 cz-en
upv-c 0.43 15 20 0 35 cz-en
cmu-hea-c 0.35 14 26 0 40 cz-en
rwth-c 0.32 16 34 0 50 cz-en
onlineB 0.3 12 28 0 40 cz-en
bbn-c 0.28 17 43 0 60 cz-en
uedin 0.28 11 28 1 40 cz-en
aalto 0.27 8 22 0 30 cz-en
jhu-c 0.26 13 37 0 50 cz-en
onlineA 0.2 6 24 0 30 cz-en
cmu 0.17 5 25 0 30 cz-en
cu-zeman 0.09 4 40 1 45 cz-en
0.1292958787
System % Yes Yes count No count N/A count Total count *** de-en ***
ref 0.98 44 1 0 45 de-en
umd 0.8 8 2 0 10 de-en
bbn-c 0.67 10 5 0 15 de-en
onlineB 0.65 13 7 0 20 de-en
cmu-hea-c 0.52 12 11 0 23 de-en
jhu-c 0.51 18 17 0 35 de-en
upv-c 0.51 18 16 1 35 de-en
fbk 0.5 20 20 0 40 de-en
uppsala 0.5 20 19 1 40 de-en
limsi 0.46 30 34 1 65 de-en
kit 0.45 18 22 0 40 de-en
liu 0.44 19 24 0 43 de-en
uedin 0.44 11 14 0 25 de-en
dfki 0.4 12 18 0 30 de-en
onlineA 0.4 6 9 0 15 de-en
rwth 0.4 14 21 0 35 de-en
cmu-hyp-c 0.37 11 19 0 30 de-en
huicong 0.36 9 16 0 25 de-en
koc-c 0.36 9 14 2 25 de-en
rwth-c 0.36 10 18 0 28 de-en
koc 0.31 11 23 1 35 de-en
cu-zeman 0.3 12 28 0 40 de-en
uu-ms 0.26 13 37 0 50 de-en
jhu 0.26 9 26 0 35 de-en
cmu 0.24 6 19 0 25 de-en
aalto 0.07 1 14 0 15 de-en
0.1512635669
System % Yes Yes count No count N/A count Total count *** es-en ***
ref 0.98 39 0 1 40 es-en
onlineB 0.71 39 15 1 55 es-en
onlineA 0.64 32 18 0 50 es-en
upv-c 0.6 36 24 0 60 es-en
huicong 0.54 27 23 0 50 es-en
jhu 0.54 35 30 0 65 es-en
cmu-hea-c 0.52 26 23 1 50 es-en
bbn-c 0.51 36 33 1 70 es-en
uedin 0.51 33 30 2 65 es-en
jhu-c 0.47 28 31 1 60 es-en
dfki 0.46 16 18 1 35 es-en
upc 0.43 28 36 1 65 es-en
cu-zeman 0.4 18 26 1 45 es-en
camb 0.36 25 45 0 70 es-en
columbia 0.29 19 46 0 65 es-en
0.1104436607
System % Yes Yes count No count N/A count Total count *** fr-en ***
ref 0.91 32 3 0 35 fr-en
cmu-hyp-c 0.7 21 9 0 30 fr-en
uedin 0.58 23 17 0 40 fr-en
bbn-c 0.56 14 10 1 25 fr-en
rwth-c 0.53 16 14 0 30 fr-en
onlineB 0.51 28 27 0 55 fr-en
camb 0.5 20 19 1 40 fr-en
rali 0.48 31 34 0 65 fr-en
lium 0.46 23 27 0 50 fr-en
dcu-c 0.45 15 16 2 33 fr-en
lig 0.45 9 11 0 20 fr-en
cmu-statxfer 0.44 11 14 0 25 fr-en
nrc 0.43 15 20 0 35 fr-en
dfki 0.4 8 12 0 20 fr-en
jhu 0.4 10 14 1 25 fr-en
jhu-c 0.4 22 30 3 55 fr-en
upv-c 0.4 14 20 1 35 fr-en
lium-c 0.4 27 41 0 68 fr-en
cmu-hea-c 0.35 14 26 0 40 fr-en
limsi 0.35 14 26 0 40 fr-en
onlineA 0.33 20 40 0 60 fr-en
huicong 0.32 13 25 2 40 fr-en
cu-zeman 0.24 6 19 0 25 fr-en
geneva 0.24 6 19 0 25 fr-en
rwth 0.2 1 4 0 5 fr-en
0.1143475506
0
0.25
0.5
0.75
1
r
e
f
d
c
u
-
c
o
n
l
i
n
e
B
r
w
t
h
-
c
k
o
c
-
c
p
c
-
t
r
a
n
s
u
p
v
-
c
c
u
-
b
o
j
a
r
e
u
r
o
t
r
a
n
s
u
e
d
i
n
c
u
-
t
e
c
t
o
c
m
u
-
h
e
a
-
c
s
f
u
p
o
t
s
d
a
m
c
u
-
z
e
m
a
n
k
o
c
o
n
l
i
n
e
A
d
c
u
.19.2.21.21.24.24.29.34.34.4.4.42.43.45.49.55.58.97
English-Czech
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
A
k
o
c
-
c
u
p
p
s
a
l
a
u
e
d
i
n
k
i
t
u
p
v
-
c
o
n
l
i
n
e
B
d
f
k
i
k
o
c
l
i
m
s
i
l
i
u
r
w
t
h
r
w
t
h
-
c
j
h
u
c
m
u
-
h
e
a
-
c
f
b
k
s
f
u
c
u
-
z
e
m
a
n
.19.31.4.47.48.49.5.51.51.51.52.52.57.62.62.65.68.8.94
English-German
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
B
u
p
v
r
w
t
h
-
c
d
c
u
k
o
c
u
p
v
-
n
n
l
m
o
n
l
i
n
e
A
j
h
u
k
o
c
-
c
u
e
d
i
n
u
p
b
-
c
c
m
u
-
h
e
a
-
c
c
a
m
b
d
f
k
i
c
u
-
z
s
f
u
.26.29.29.3.32.32.36.38.38.38.39.4.42.46.5.58.83
English-Spanish
0
0.25
0.5
0.75
1
r
e
f
r
w
t
h
-
c
o
n
l
i
n
e
B
u
p
v
-
c
k
o
c
-
c
u
e
d
i
n
r
a
l
i
r
w
t
h
l
i
u
m
c
a
m
b
o
n
l
i
n
e
A
l
i
m
s
i
j
h
u
n
r
c
c
m
u
-
h
e
a
-
c
g
e
n
e
v
a
e
u
d
f
k
i
k
o
c
c
u
-
z
e
m
a
n
.17.21.28.3.31.32.36.37.37.41.42.43.45.47.48.48.51.52.54.91
English-French
0
0.25
0.5
0.75
1
r
e
f
u
m
d
b
b
n
-
c
o
n
l
i
n
e
B
c
m
u
-
h
e
a
-
c
j
h
u
-
c
u
p
v
-
c
f
b
k
u
p
p
s
a
l
a
l
i
m
s
i
k
i
t
l
i
u
u
e
d
i
n
d
f
k
i
o
n
l
i
n
e
A
r
w
t
h
c
m
u
-
h
y
p
-
c
h
u
i
c
o
n
g
k
o
c
-
c
r
w
t
h
-
c
k
o
c
c
u
-
z
e
m
a
n
u
u
-
m
s
j
h
u
c
m
u
a
a
l
t
o
.07.24.26.26.3.31.36.36.36.37.4.4.4.44.44.45.46.5.5.51.51.52.65.67.8.98
German-English
0
0.25
0.5
0.75
1
r
e
f
c
u
-
b
o
j
a
r
u
p
v
-
c
c
m
u
-
h
e
a
-
c
r
w
t
h
-
c
o
n
l
i
n
e
B
b
b
n
-
c
u
e
d
i
n
a
a
l
t
o
j
h
u
-
c
o
n
l
i
n
e
A
c
m
u
c
u
-
z
e
m
a
n
.09.17.2.26.27.28.28.3.32.35.43.61.0
Czech-English
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
B
o
n
l
i
n
e
A
u
p
v
-
c
h
u
i
c
o
n
g
j
h
u
c
m
u
-
h
e
a
-
c
b
b
n
-
c
4
5
j
h
u
-
c
d
f
k
i
u
p
c
c
u
-
z
e
m
a
n
c
a
m
b
c
o
l
u
m
b
i
a
.29.36.4.43.46.47.51.51.52.54.54.6.64.71.98
Spanish-English
0
0.25
0.5
0.75
1
r
e
f
c
m
u
-
h
y
p
-
c
u
e
d
i
n
b
b
n
-
c
r
w
t
h
-
c
o
n
l
i
n
e
B
c
a
m
b
r
a
l
i
l
i
u
m
d
c
u
-
c
l
i
g
c
m
u
-
s
t
a
t
x
f
e
r
n
r
c
d
f
k
i
j
h
u
j
h
u
-
c
u
p
v
-
c
l
i
u
m
-
c
c
m
u
-
h
e
a
-
c
l
i
m
s
i
o
n
l
i
n
e
A
h
u
i
c
o
n
g
c
u
-
z
e
m
a
n
g
e
n
e
v
a
r
w
t
h
.2.24.24.32.33.35.35.4.4.4.4.4.43.44.45.45.46.48.5.51.53.56.58.7.91
French-English
Figure 3: The percent of time that each system?s edited output was judged to be an acceptable translation.
These numbers also include judgments of the system?s output when it was marked either incomprehen-
sible or acceptable and left unedited. Note that the reference translation was edited alongside the system
outputs. Error bars show one positive and one negative standard deviation for the systems in that lan-
guage pair.
28
a shorter description when concatenated be-
fore compression than when concatenated af-
ter compression. MT-NCD does not require
any language specific resources.
? MT-mNCD ? Enhances MT-NCD with flex-
ible word matching provided by stemming
and synonyms. It works analogously to
M-BLEU and M-TER and uses METEOR?s
aligner module to find relaxed word-to-word
alignments. MT-mNCD exploits English
WordNet data and increases correlation to hu-
man judgments for English over MT-NCD.
Due to a processing issue inherent to the metric,
the scores reported were generated excluding the
first segment of each document. Also, a separate
issue was found for the MT-mNCD metric, and ac-
cording to the developer the scores reported here
would like change with a correction of the issue.
BabbleQuest International8
? Badger 2.0 full ? Uses the Smith-Waterman
alignment algorithm with Gotoh improve-
ments to measure segment similarity. The
full version uses a multilingual knowledge
base to assign a substitution cost which sup-
ports normalization of word infection and
similarity.
? Badger 2.0 lite ? The lite version uses default
gap, gap extension and substitution costs.
City University of Hong Kong (Wong and Kit,
2010)
? ATEC 2.1 ? This version of ATEC extends
the measurement of word choice and word or-
der by various means. The former is assessed
by matching word forms at linguistic levels,
including surface form, stem, sense and se-
mantic similarity, and further by weighting
the informativeness of both matched and un-
matched words. The latter is quantified in
term of the discordance of word position and
word sequence between an MT output and its
reference.
Due to a version discrepancy of the metric, final
scores for ATECD-2.1 differ from those reported
here, but only minimally.
8http://www.babblequest.com/badger2
Carnegie Mellon University (Denkowski and
Lavie, 2010)
? METEOR-NEXT-adq ? Evaluates a machine
translation hypothesis against one or more
reference translations by calculating a simi-
larity score based on an alignment between
the hypothesis and reference strings. Align-
ments are based on exact, stem, synonym,
and paraphrase matches between words and
phrases in the strings. Metric parameters are
tuned to maximize correlation with human
judgments of translation quality (adequacy
judgments).
? METEOR-NEXT-hter ? METEOR-NEXT
tuned to HTER.
? METEOR-NEXT-rank ? METEOR-NEXT
tuned to human judgments of rank.
Columbia University9
? SEPIA ? A syntactically-aware machine
translation evaluation metric designed with
the goal of assigning bigger weight to gram-
matical structural bigrams with long surface
spans that cannot be captured with surface n-
gram metrics. SEPIA uses a dependency rep-
resentation produced for both hypothesis and
reference(s). SEPIA is configurable to allow
using different combinations of structural n-
grams, surface n-grams, POS tags, depen-
dency relations and lemmatization. SEPIA is
a precision-based metric and as such employs
clipping and length penalty to minimize met-
ric gaming.
Charles University Prague (Bojar and Kos,
2010)
? SemPOS ? Computes overlapping of autose-
mantic (content-bearing) word lemmas in the
candidate and reference translations given a
fine-grained semantic part of speech (sem-
pos) and outputs average overlapping score
over all sempos types. The overlapping is de-
fined as the number of matched lemmas di-
vided by the total number of lemmas in the
candidate and reference translations having
the same sempos type.
9http://www1.ccls.columbia.edu/?SEPIA/
29
? SemPOS-BLEU ? A linear combination of
SemPOS and BLEU with equal weights.
BLEU is computed on surface forms of au-
tosemantic words that are used by SemPOS,
i.e. auxiliary verbs or prepositions are not
taken into account.
Dublin City University (He et al, 2010)
? DCU-LFG ? A combination of syntactic and
lexical information. It measures the similar-
ity of the hypothesis and reference in terms
of matches of Lexical Functional Grammar
(LFG) dependency triples. The matching
module can also access the WordNet syn-
onym dictionary and Snover?s paraphrase
database10.
University of Edinburgh (Birch and Osborne,
2010)
? LRKB4 ? A novel metric which directly mea-
sures reordering success using Kendall?s tau
permutation distance metrics. The reordering
component is combined with a lexical metric,
capturing the two most important elements
of translation quality. This simple combined
metric only has one parameter, which makes
its scores easy to interpret. It is also fast
to run and language-independent. It uses
Kendall?s tau permutation.
? LRHB4 ? LRKB4, replacing Kendall?s tau
permutation distance metric with the Ham-
ming distance permutation distance metric.
Due to installation issues, the reported submitted
scores for these two metrics have not been verified
to produce identical scores at NIST.
Harbin Institute of Technology, China
? I-letter-BLEU ? Normal BLEU based on let-
ters. Moreover, the maximum length of N-
gram is decided by the average length for
each sentence, respectively.
? I-letter-recall ? A geometric mean of N-gram
recall based on letters. Moreover, the maxi-
mum length of N-gram is decided by the av-
erage length for each sentence, respectively.
10Available at http://www.umiacs.umd.edu/
?snover/terp/.
? SVM-RANK ? Uses support vector ma-
chines rank models to predict an ordering
over a set of system translations with lin-
ear kernel. Features include Meteor-exact,
BLEU-cum-1, BLEU-cum-2, BLEU-cum-5,
BLEU-ind-1, BLEU-ind-2, ROUGE-L re-
call, letter-based TER, letter-based BLEU-
cum-5, letter-based ROUGE-L recall, and
letter-based ROUGE-S recall.
National University of Singapore (Liu et al,
2010)
? TESLA-M ? Based on matching of bags of
unigrams, bigrams, and trigrams, with con-
sideration of WordNet synonyms. The match
is done in the framework of real-valued lin-
ear programming to enable the discounting of
function words.
? TESLA ? Built on TESLA-M, this metric
also considers bilingual phrase tables to dis-
cover phrase-level synonyms. The feature
weights are tuned on the development data
using SVMrank.
Stanford University
? Stanford ? A discriminatively trained
string-edit distance metric with various
similarity-matching, synonym-matching, and
dependency-parse-tree-matching features.
The model resembles a Conditional Random
Field, but performs regression instead of
classification. It is trained on Arabic, Chi-
nese, and Urdu data from the MT-Eval 2008
dataset.
Due to installation issues, the reported scores for
this metric have not been verified to produce iden-
tical scores at NIST.
University of Maryland11
? TER-plus (TERp) ? An extension of the
Translation Edit Rate (TER) metric that mea-
sures the number of edits between a hypoth-
esized translation and a reference translation.
TERp extends TER by using stemming, syn-
onymy, and paraphrases as well as tunable
edit costs to better measure the distance be-
tween the two translations. This version
of TERp improves upon prior versions by
adding brevity and length penalties.
11http://www.umiacs.umd.edu/?snover/
terp
30
Scores were not submitted along with this metric,
and due to installation issues were not produced at
NIST in time to be included in this report.
University Polite`cnica de Catalunya/University
de Barcelona (Comelles et al, 2010)
? DR ? An arithmetic mean over a set of
three metrics based on discourse representa-
tions, respectively computing lexical overlap,
morphosyntactic overlap, and semantic tree
matching.
? DRdoc ? Is analogous to DR but, instead of
operating at the segment level, it analyzes
similarities over whole document discourse
representations.
? ULCh ? An arithmetic mean over a
heuristically-defined set of metrics operat-
ing at different linguistic levels (ROUGE,
METEOR, and measures of overlap between
constituent parses, dependency parses, se-
mantic roles, and discourse representations).
University of Southern California, ISI
? BEwT-E ? Basic Elements with Transfor-
mations for Evaluation, is a recall-oriented
metric that compares basic elements, small
portions of contents, between the two trans-
lations. The basic elements (BEs) consist
of content words and various combinations
of syntactically-related words. A variety of
transformations are performed to allow flexi-
ble matching so that words and syntactic con-
structions conveying similar content in dif-
ferent manners may be matched. The trans-
formations cover synonymy, preposition vs.
noun compounding, differences in tenses,
etc. BEwT-E was originally created for sum-
marization evaluation and is English-specific.
? Bkars ? Measures overlap between character
trigrams in the system and reference trans-
lations. It is heavily weighted toward recall
and contains a fragmentation penalty. Bkars
produces a score both with and without stem-
ming (using the Snowball package of stem-
mers) and averages the results together. It is
not English-specific.
Scores were not submitted for BEwT-E; the run-
time required for this metric to process the WMT-
10 data set prohibited the production of scores in
time for publication.
6 Evaluation task results
The results reported here are preliminary; a final
release of results will be published on the WMT10
website before July 15, 2010. Metric developers
submitted metrics for installation at NIST; they
were also asked to submit metric scores on the
WMT10 test set alng with their metrics. Not
all developers submitted scores, and not all met-
rics were verified to produce the same scores as
submitted at NIST in time for publication. Any
such caveats are reported with the description of
the metrics above.
The results reported here are limited to a com-
parison of metric scores on the full WMT10
test set with human assessments on the human-
assessed subset. An analysis comparing the hu-
man assessments with the automatic metrics run
only on the human-assessed subset will follow at
a later date.
The WMT10 system output used to generate
the reported metric scores was found to have im-
properly escaped characters for a small number of
segments. While we plan to regenerate the met-
ric scores with this issue resolved, we do not ex-
pect this to significantly alter the results, given the
small number of segments affected.
6.1 System Level Metric Scores
The tables in Appendix B list the metric scores
for the language pairs processed by each metric.
These first four tables present scores for transla-
tions out of English into Czech, French, German
and Spanish. In addition to the metric scores of
the submitted metrics identified above, we also
present (1) the ranking of the system as deter-
mined by the human assessments; and (2) the
metrics scores for two popular baseline metrics,
BLEU as calculated by NIST?s mteval software12
and the NIST score. For each method of system
measurement the absolute highest score is identi-
fied by being outlined in a box.
Similarly, the remaining tables in Appendix B
list the metric scores for the submitted metrics and
the two baseline metrics, and the ranking based
on the human assessments for translations into En-
glish from Czech, French, German and Spanish.
As some metrics employ language-specific re-
sources, not all metrics produced scores for all lan-
guage pairs.
12ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a-20091001.tar.gz
31
cz-
en
fr-
en
de-
en
es-
en
avg
SemPOS .78 .77 .60 .95 .77
IQmt-DRdoc .61 .79 .65 .98 .76
SemPOS-BLEU .75 .70 .61 .96 .75
i-letter-BLEU .71 .70 .60 .98 .75
NIST .85 .72 .55 .86 .74
TESLA .70 .70 .60 .97 .74
MT-NCD .71 .72 .58 .95 .74
Bkars .71 .67 .58 .98 .74
ATEC-2.1 .71 .67 .59 .96 .73
meteor-next-rank .69 .68 .60 .96 .73
IQmt-ULCh .70 .64 .60 .99 .73
IQmt-DR .68 .67 .60 .97 .73
meteor-next-hter .71 .66 .59 .95 .73
meteor-next-adq .69 .67 .60 .96 .73
badger-2.0-lite .70 .70 .56 .94 .73
DCU-LFG .69 .69 .58 .96 .73
badger-2.0-full .69 .70 .57 .94 .73
SEPIA .71 .70 .57 .92 .73
SVM-rank .66 .65 .61 .98 .73
i-letter-recall .65 .64 .61 .98 .72
TESLA-M .67 .67 .57 .95 .72
BLEU-4-v13a .69 .68 .52 .90 .70
LRKB4 .63 .62 .53 .89 .67
LRHB4 .62 .65 .50 .87 .66
MT-mNCD .69 .64 .52 .70 .64
Stanford .58 .19 .60 .46 .46
Table 7: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation into English.
It is noticeable that system combinations are of-
ten among those achieving the highest scores.
6.2 System-Level Correlations
To assess the performance of the automatic met-
rics, we correlated the metrics? scores with the hu-
man rankings at the system level. We assigned a
consolidated human-assessment rank to each sys-
tem based on the number of times that the given
system?s translations were ranked higher than or
equal to the translations of any other system in
the manual evaluation of the given language pair.
We then compared the ranking of systems by the
human assessments to that provided by the au-
tomatic metric system level scores on the com-
plete WMT10 test set for each language pair, us-
ing Spearman?s ? rank correlation coefficient. The
correlations are shown in Table 7 for translations
to English, and Table 8 out of English, with base-
line metrics listed at the bottom. The highest cor-
relation for each language pair and the highest
overall average are bolded.
Overall, correlations are higher for translations
to English than compared to translations from En-
glish. For all language pairs, there are a number
of new metrics that yield noticeably higher corre-
en-
cz
en-
fr
en-
de
en-
es
avg
SVM-rank .29 .54 .68 .67 .55
TESLA-M .27 .49 .74 .66 .54
LRKB4 .39 .58 .47 .71 .54
i-letter-recall .28 .51 .61 .66 .52
LRHB4 .39 .59 .41 .63 .51
i-letter-BLEU .26 .49 .56 .65 .49
ATEC-2.1 .38 .52 .44 .62 .49
badger-2.0-full .37 .58 .41 .59 .49
Bkars .22 .54 .52 .66 .48
BLEU-4-v13a .35 .58 .39 .57 .47
badger-2.0-lite .32 .57 .41 .59 .47
TESLA .09 .62 .66 .50 .47
meteor-next-rank .34 .59 .39 .51 .46
Stanford .34 .48 .70 .32 .46
MT-NCD .17 .54 .51 .61 .46
NIST .30 .52 .41 .50 .43
MT-mNCD .26 .49 .17 .43 .34
SemPOS .31 n/a n/a n/a .31
SemPOS-BLEU .29 n/a n/a n/a .29
Table 8: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation out of English.
lations with human assessments than either of the
two included baseline metrics. In particular, Bleu
performed in the bottom half of the into-English
and out-of-English directions.
6.3 Segment-Level Metric Analysis
The method employed to collect human judgments
of rank preferences at the segment level produces
a sparse matrix of decision points. It is unclear
whether attempts to normalize the segment level
rankings to 0.0?1.0 values, representing the rela-
tive rank of a system per segment given the num-
ber of comparisons it is involved with, is proper.
An intuitive display of how well metrics mirror the
human judgments may be shown via a confusion
matrix. We compare the human ranks to the ranks
as determined by a metric. Below, we show an ex-
ample of the confusion matrix for the SVM-rank
metric which had the highest summed diagonal
(occurrences when a particular rank by the met-
ric?s score exactly matches the human judgments)
for all segments translated into English. The num-
bers provided are percentages of the total count.
The summed diagonal constitutes 39.01% of all
counts in this example matrix. The largest cell is
the 1/1 ranking cell (top left). We included the
reference translation as a system in this analysis,
which is likely to lead to a lot of agreement on the
highest rank between humans and automatic met-
rics.
32
Metric Human Rank
Rank 1 2 3 4 5
1 12.79 4.48 2.75 1.82 0.92
2 2.77 7.94 5.55 3.79 2.2
3 1.57 4.29 6.74 5.4 4.46
4 0.97 2.42 3.76 4.99 6.5
5 0.59 1.54 1.84 3.38 6.55
No allowances for ties were made in this analy-
sis. That is, if a human ranked two system transla-
tions the same, this analysis expects the metrics to
provide the same score in order to get them both
correct. Future analysis could relax this constraint.
As not all human rankings start with the highest
possible rank of ?1? (due to ties and withholding
judgment on a particular system output being al-
lowed), we set the highest automatic metric rank
to the highest human rank and shifted the lower
metric ranks down accordingly.
Table 9 shows the summed diagonal percent-
ages of the total count of all datapoints for all met-
rics that WMT10 scores were available for, both
combined for all languages to English (X-English)
and separately for each language into English.
The results are ordered by the highest percent-
age for the summed diagonal on all languages
to English combined. There are quite noticeable
changes in ranking of the metrics for the separate
language pairs; further analysis into the reasons
for this will be necessary.
We plan to also analyze metric performance for
translation into English.
7 Feasibility of Using Non-Expert
Annotators in Future WMTs
In this section we analyze the data that we col-
lected data by posting the ranking task on Ama-
zon?s Mechanical Turk (MTurk). Although we did
not use this data when creating the official results,
our hope was that it may be useful in future work-
shops in two ways. First, if we find that it is pos-
sible to obtain a sufficient amount of data of good
quality, then we might be able to reduce the time
commitment expected from the system develop-
ers in future evaluations. Second, the additional
collected labels might enable us to detect signifi-
cant differences between systems that would oth-
erwise be insignificantly different using only the
data from the volunteers (which we will now refer
to as the ?expert? data).
7.1 Data collection
To that end, we prepared 600 ranking sets for each
of the eight language pairs, with each set con-
taining five MT outputs to be ranked, using the
same interface used by the volunteers. We posted
the data to MTurk and requested, for each one,
five redundant assignments, from different work-
ers. Had all the 5? 8? 600 = 24,000 assignments
been completed, we would have obtained 24,000
? 5 = 120,000 additional rank labels, compared
to the 37,884 labels we collected from the volun-
teers (Table 3). In actuality, we collected closer to
55,000 rank labels, as we discuss shortly.
To minimize the amount of data that is of poor
quality, we placed two requirements that must be
satisfied by any worker before completing any of
our tasks. First, we required that a worker have an
existing approval rating of at least 85%. Second,
we required a worker to reside in a country where
the target language of the task can be assumed to
be the spoken language. Finally, anticipating a
large pool of workers located in the United States,
we felt it possible for us to add a third restriction
for the *-to-English language pairs, which is that a
worker must have had at least five tasks previously
approved on MTurk.13 We organized the ranking
sets in groups of 3 per screen, with a monetary re-
ward of $0.05 per screen.
When we created our tasks, we had no expecta-
tion that all the assignments would be completed
over the tasks? lifetime of 30 days. This was in-
deed the case (Table 10), especially for language
pairs with a non-English target language, due to
workers being in short supply outside the US.
Overall, we see that the amount of data collected
from non-US workers is relatively small (left half
of Table 10), whereas the pool of US-based work-
ers is much larger, leading to much higher com-
pletion rates for language pairs with English as the
target language (right half of Table 10). This is in
spite of the additional restriction we placed on US
workers.
13We suspect that newly registered workers on MTurk al-
ready start with an ?approval rating? of 100%, and so requir-
ing a high approval rating alone might not guard against new
workers. It is not entirely clear if our suspicion is true, but our
past experiences with MTurk usually involved a noticeably
faster completion rate than what we experienced this time
around, indicating our suspicion might very well be correct.
33
Metric *-English Czech-English French-English German-English Spanish-English
SVM-rank 39.01 41.21 36.07 38.81 40.3
i-letter-recall 38.85 41.71 36.19 38.8 39.5
MT-NCD 38.77 42.55 35.31 38.7 39.48
i-letter-BLEU 38.69 40.54 36.05 38.82 39.64
meteor-next-rank 38.5 40.1 34.41 39.25 40.05
meteor-next-adq 38.27 39.58 34.41 39.5 39.35
meteor-next-hter 38.21 38.61 34.1 39.13 40.18
Bkars 37.98 40.1 35.08 38.6 38.52
Stanford 37.97 39.87 36.19 38.27 38.09
ATEC-2.1 37.95 40.06 34.96 38.6 38.53
TESLA 37.57 38.68 34.38 38.67 38.36
NIST 37.47 39.54 35.54 37.13 38.2
SemPOS 37.21 38.8 37.39 35.73 37.69
SemPOS-BLEU 37.16 38.05 36.57 37.11 37.21
badger-2.0-full 37.12 37.5 36 36.21 38.62
badger-2.0-lite 37.08 37.2 35.88 36.23 38.69
SEPIA 37.06 38.98 34.6 36.46 38.52
BLEU-4-v13a 36.71 37.83 34.84 36.44 37.81
LRHB4 36.14 38.35 34.65 34.24 37.93
TESLA-M 36.13 37.01 34 35.79 37.6
LRKB4 36.12 38.72 33.47 35.25 37.63
IQmt-ULCh 35.86 37.64 33.95 35.81 36.45
IQmt-DR 35.77 36.27 34.43 34.43 37.74
DCU-LFG 34.72 36.38 32.29 33.87 36.49
MT-mNCD 34.51 34.93 31.78 35.73 35.13
IQmt-DRdoc 31.9 33.85 28.99 32.9 32.18
Table 9: The segment-level performance for metrics for the into-English direction.
en-de en-es en-fr en-cz de-en es-en fr-en cz-en
Location DE ES/MX FR CZ US US US US
Completed 1 time 37% 38% 29% 19% 3.5% 1.5% 14% 2.0%
Completed 2 times 18% 14% 12% 1.5% 6.0% 5.5% 19% 4.5%
Completed 3 times 2.5% 4.5% 0.5% 0.0% 8.5% 11% 20% 10%
Completed 4 times 1.5% 0.5% 0.5% 0.0% 22% 19% 23% 17%
Completed 5 times 0.0% 0.5% 0.0% 0.0% 60% 63% 22% 67%
Completed ? once 59% 57% 42% 21% 100% 99% 96% 100%
Label count 2,583 2,488 1,578 627 12,570 12,870 9,197 13,169
(% of expert data) (38%) (96%) (40%) (9%) (241%) (228%) (222%) (490%)
Table 10: Statistics for data collected on MTurk for the ranking task. In total, 55,082 rank labels were
collected across the eight language pairs (145% of expert data). Each language pair had 600 sets, and
we requested each set completed by 5 different workers. Since each set provides 5 labels, we could have
potentially obtained 600 ? 5 ? 5 = 15,000 labels for each language pair. The Label count row indicates
to what extent that potential was met (over the 30-day lifetime of our tasks), and the ?Completed...? rows
give a breakdown of redundancy. For instance, the right-most column indicates that, in the cz-en group,
2.0% of the 600 sets were completed by only one worker, while 67% of the sets were completed by 5
workers, with 100% of the sets completed at least once. The total cost of this data collection effort was
roughly $200.
34
INTER-ANNOTATOR AGREEMENT
P (A) K K?
With references 0.466 0.198 0.487
Without references 0.441 0.161 0.439
INTRA-ANNOTATOR AGREEMENT
P (A) K K?
With references 0.539 0.309 0.633
Without references 0.538 0.307 0.601
Table 11: Inter- and intra-annotator agreement for
the MTurk workers on the sentence ranking task.
(As before, P (E) is 0.333.) For comparison, we
repeat here the kappa coefficients of the experts
(K?), taken from Table 4.
7.2 Quality of MTurk data
It is encouraging to see that we can collect a large
amount of rank labels from MTurk. That said, we
still need to guard against data from bad work-
ers, who are either not being faithful and click-
ing randomly, or who might simply not be compe-
tent enough. Case in point, if we examine inter-
and intra-annotator agreement on the MTurk data
(Table 11), we see that the agreement rates are
markedly lower than their expert counterparts.
Another indication of the presence of bad work-
ers is a low reference preference rate (RPR),
which we define as the proportion of time a ref-
erence translation wins (or ties in) a comparison
when it appears in one. Intuitively, the RPR
should be quite high, since it is quite rare that an
MT output ought to be judged better than the refer-
ence. This rate is 96.5% over the expert data, but
only 83.7% over the MTurk data. Compare this
to a randomly-clicking RPR of 66.67% (because
the two acceptable answers are that the reference
is either better than a system?s output or tied with
it).
Also telling would be the rate at which MTurk
workers agree with experts. To ensure that we ob-
tain enough overlapping data to calculate such a
rate, we purposely select one-sixth14 of our rank-
ing sets so that the five-system group is exactly one
that has been judged by an expert. This way, at
least one-sixth of the comparisons obtained from
an MTurk worker?s labels are comparisons for
14This means that on average Turkers ranked a set of sys-
tem outputs that had been ranked by experts on every other
screen, since each screen?s worth of work had three sets.
which we already have an expert judgment. When
we calculate the rate of agreement on this data,
we find that MTurk workers agree with the ex-
pert workers 53.2% of the time, or K = 0.297, and
when references are excluded, the agreement rate
is 50.0%, or K = 0.249. Ideally, we would want
those values to be in the 0.4?0.5 range, since that
is where the inter-annotator kappa coefficient lies
for the expert annotators.
7.3 Filtering MTurk data by agreement with
experts
We can use the agreement rate with experts to
identify MTurk workers who are not performing
the task as required. For each worker w of the
669 workers for whom we have such data, we
compute the worker?s agreement rate with the ex-
perts, and from it a kappa coefficient Kexp(w) for
that worker. (Given that P (E) is 0.333, Kexp(w)
ranges between?0.5 and +1.0.) We sort the work-
ers based on Kexp(w) in ascending order, and ex-
amine properties of the MTurk data as we remove
the lowest-ranked workers one by one (Figure 4).
We first note that the amount of data we ob-
tained from MTurk is so large, that we could af-
ford to eliminate close to 30% of the labels, and
we would still have twice as much data than us-
ing the expert data alone. We also note that two
workers in particular (the 103rd and 130th to be
removed) are likely responsible for the majority
of the bad data, since removing their data leads to
noticeable jumps in the reference preference rate
and the inter-annotator agreement rate (right two
curves of Figure 4). Indeed, examining the data for
those two workers, we find that their RPR values
are 55.7% and 51.9%, which is a clear indication
of random clicking.15
Looking again at those two curves shows de-
grading values as we continue to remove workers
in large droves, indicating a form of ?overfitting?
to agreement with experts (which, naturally, con-
tinues to increase until reaching 1.0; bottom left
curve). It is therefore important, if one were to fil-
ter out the MTurk data by removing workers this
way, to choose a cutoff carefully so that no crite-
rion is degraded dramatically.
In Appendix A, after reporting head-to-head
comparisons using only the expert data, we also
report head-to-head comparisons using the expert
15In retrospect, we should have performed this type of
analysis as the data was being collected, since such workers
could have been identified early on and blocked.
35
-20
40
60
80
100
120
140
160
0 100 200 300 400 500 600 700
# Workers Removed
M
T
u
r
k
 
D
a
t
a
 
R
e
m
a
i
n
i
n
g
(
%
 
o
f
 
E
x
p
e
r
t
 
D
a
t
a
)
-
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0 100 200 300 400 500 600 700
# Workers Removed
A
g
r
e
e
m
e
n
t
 
w
i
t
h
 
E
x
p
e
r
t
 
D
a
t
a
 
(
k
a
p
p
a
)
82%
84%
86%
88%
90%
92%
94%
96%
98%
100%
0 100 200 300 400 500 600 700
# Workers Removed
R
e
f
e
r
e
n
c
e
 
P
r
e
f
e
r
e
n
c
e
 
R
a
t
e
0.10
0.15
0.20
0.25
0.30
0 100 200 300 400 500 600 700
# Workers Removed
I
n
t
e
r
-
A
n
n
o
t
a
t
o
r
 
A
g
r
e
e
m
e
n
t
 
(
k
a
p
p
a
)
Figure 4: The effect of removing an increasing number of MTurk workers. The order in which workers
are removed is by Kexp(w), the kappa agreement coefficient with expert data (excluding references).
data combined with the MTurk data, in order to
be able to detect more significant differences be-
tween the systems. We choose the 300-worker
point as a reasonable cutoff point before combin-
ing the MTurk data with the expert data, based
on the characteristics of the MTurk data at that
point: a high reference preference rate, high inter-
annotator agreement, and, critically, a kappa co-
efficient vs. expert data of 0.449, which is close
to the expert inter-annotator kappa coefficient of
0.439.
7.4 Feasibility of using only MTurk data
In the previous subsection, we outlined an ap-
proach by which MTurk data can be filtered out
using expert data. Since we were to combine the
filtered MTurk data with the expert data to ob-
tain more significant differences, it was reason-
able to use agreement with experts to quantify the
MTurk workers? competency. However, we also
would like to know whether it is feasible to use the
MTurk data alone. Our aim here is not to boost the
differences we see by examining expert data, but
to eliminate our reliance on obtaining expert data
in the first place.
We briefly examined some simple ways of fil-
tering/combining the MTurk data, and measured
the Spearman rank correlations obtained from the
MTurk data (alone), as compared to the rankings
obtained using the expert data (alone), and report
them in Table 12. (These correlations do not in-
clude the references.)
We first see that even when using the MTurk
data untouched, we already obtain relatively high
correlation with expert ranking (?Unfiltered?).
This is especially true for the *-to-English lan-
guage pairs, where we collected much more data
than English-to-*. In fact, the relationship be-
tween the amount of data and the correlation val-
ues is very strong, and it is reasonable to expect
the correlation numbers for English-to-* to catch
up had more data been collected.
We also measure rank correlations when apply-
ing some simple methods of cleaning/weighting
MTurk data. The first method (?Voting?) is per-
forming a simple vote whenever redundant com-
parisons (i.e. from different workers) are avail-
able. The second method (?Kexp-filtered?) first re-
moves labels from the 300 worst workers accord-
ing to agreement with experts. The third method
36
(?RPR-filtered?) first removes labels from the 62
worst workers according to their RPR. The num-
bers 300 and 62 were chosen since those are the
points at which the MTurk data reaches the level
of expert data in the inter-annotator agreement and
RPR of the experts.
The fourth and fifth methods (?Weighted by
Kexp? and ?Weighted by K(RPR)?) do not re-
move any data, instead assigning weights to work-
ers based on their agreement with experts and their
RPR, respectively. Namely, for each worker, the
weight assigned by the fourth method is Kexp for
that worker, and the weight assigned by the fifth
method is K(RPR) for that worker.
Examining the correlation coefficients obtained
from those methods (Table 12), we see mixed re-
sults, and there is no clear winner among those
methods. It is also difficult to draw any conclusion
as to which method performs best when. However,
it is encouraging to see that the two RPR-based
methods perform well. This is noteworthy, since
there is no need to use expert data to weight work-
ers, which means that it is possible to evaluate a
worker using inherent, ?built-in? properties of that
worker?s own data, without resorting to making
comparisons with other workers or with experts.
8 Summary
As in previous editions of this workshop we car-
ried out an extensive manual and automatic eval-
uation of machine translation performance for
translating from European languages into English,
and vice versa.
The number of participants grew substantially
compared to previous editions of the WMT work-
shop, with 33 groups from 29 institutions partic-
ipating in WMT10. Most groups participated in
the translation task only, while the system combi-
nation task attracted a somewhat smaller number
of participants
Unfortunately, fewer rule-based systems partic-
ipated in this year?s edition of WMT, compared
to previous editions. We hope to attract more
rule-based systems in future editions as they in-
crease the variation of translation output and for
some language pairs, such as German-English,
tend to outperform statistical machine translation
systems.
This was the first time that the WMT workshop
was held as a joint workshop with NIST?s Metric-
sMATR evaluation initiative. This joint effort was
very productive as it allowed us to focus more on
the two evaluation dimensions: manual evaluation
of MT performance and the correlation between
manual metrics and automated metrics.
This year was also the first time we have in-
troduced quality assessments by non-experts. In
previous years all assessments were carried out
through peer evaluation exclusively consisting of
developers of machine translation systems, and
thereby people who are used to machine transla-
tion output. This year we have facilitated Ama-
zon?s Mechanical Turk to investigate two as-
pects of manual evaluation: How stable are man-
ual assessments across different assessor profiles
(experts vs. non-experts) and how reliable are
quality judgments of non-expert users? While
the intra- and inter-annotator agreements between
non-expert assessors are considerably lower than
for their expert counterparts, the overall rankings
of translation systems exhibit a high degree of cor-
relation between experts and non-experts. This
correlation can be further increased by applying
various filtering strategies reducing the impact of
unreliable non-expert annotators.
As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.16
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-
0022, and the US National Science Foundation un-
der grant IIS-0713448.
References
Alexandre Allauzen, Josep M. Crego, lknur Durgar El-
Kahlout, and Francois Yvon. 2010. Limsi?s statisti-
cal translation systems for wmt?10. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 29?34, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Lo??c Barrault. 2010. Many: Open source mt system
combination at wmt?10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
16http://www.statmt.org/wmt09/results.
html
37
Label Unfiltered Voting Kexp-filtered RPR-filtered Weighted by Weighted by
count Kexp K(RPR)
en-de 2,583 0.862 0.779 0.818 0.862 0.868 0.862
en-es 2,488 0.759 0.785 0.797 0.797 0.768 0.806
en-fr 1,578 0.826 0.840 0.791 0.814 0.802 0.814
en-cz 627 0.833 0.818 0.354 0.833 0.851 0.828
de-en 12,570 0.914 0.925 0.920 0.931 0.933 0.926
es-en 12,870 0.934 0.969 0.965 0.987 0.978 0.987
fr-en 9,197 0.880 0.865 0.920 0.919 0.907 0.917
cz-en 13,169 0.951 0.909 0.965 0.944 0.930 0.944
Table 12: Spearman rank coefficients for the MTurk data across the various language pairs, using differ-
ent methods to clean the data or weight workers. (These correlations were computed after excluding the
references.) Kexp is the kappa coefficient of the worker?s agreement rate with experts, with P (A) = 0.33.
K(RPR) is the kappa coefficient of the worker?s RPR (see 7.2), with P (A) = 0.66. In Kexp-filtering,
42% of labels remain, after removing 300 workers. In K(RPR)-filtering, 69% of labels remain, after
removing 62 workers.
and MetricsMATR, pages 252?256, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Ergun Bicici and S. Serdar Kozat. 2010. Adaptive
model weighting and transductive regression for pre-
dicting best system combinations. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 257?262, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Ergun Bicici and Deniz Yuret. 2010. L1 regularized
regression for reranking and system combination in
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 263?270, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
302?307, Uppsala, Sweden, July. Association for
Computational Linguistics.
Ondrej Bojar and Kamil Kos. 2010. 2010 failures
in english-czech phrase-based mt. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 35?41, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with amazons mechan-
ical turk. In Proceedings NAACL-2010 Workshop on
Creating Speech and Language Data With Amazons
Mechanical Turk, Los Angeles.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation (WMT07), Prague, Czech Repub-
lic.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, , Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the findings
of the 2009 workshop on statistical machine trans-
lation. In Proceedings of the Fourth Workshop on
Statistical Machine Translation (WMT09), Athens,
Greece.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: Evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2009), Singapore.
Elisabet Comelles, Jesus Gimenez, Lluis Marquez,
Irene Castellon, and Victoria Arranz. 2010.
Document-level automatic mt evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 308?313, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Hui Cong, Zhao Hai, Lu Bao-Liang, and Song Yan.
2010. An empirical study on development set se-
lection strategy for machine translation learning.
In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 42?46, Uppsala, Sweden, July. Association
for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2010. Meteor-
next and the meteor paraphrase tables: Improved
38
evaluation support for five target languages. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 314?
317, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Marcus Dobrinkat, Tero Tapiovaara, Jaakko Va?yrynen,
and Kimmo Kettunen. 2010. Normalized compres-
sion distance based measures for metricsmatr 2010.
In Proceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR, pages
318?323, Uppsala, Sweden, July. Association for
Computational Linguistics.
Jinhua Du, Pavel Pecina, and Andy Way. 2010. An
augmented three-pass system combination frame-
work: Dcu combination system for wmt 2010. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
271?276, Uppsala, Sweden, July. Association for
Computational Linguistics.
Vladimir Eidelman, Chris Dyer, and Philip Resnik.
2010. The university of maryland statistical ma-
chine translation system for the fifth workshop on
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 47?51, Uppsala, Sweden,
July. Association for Computational Linguistics.
Christian Federmann, Andreas Eisele, Yu Chen, Sabine
Hunsicker, Jia Xu, and Hans Uszkoreit. 2010.
Further experiments with shallow hybrid mt sys-
tems. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 52?56, Uppsala, Sweden, July. Association
for Computational Linguistics.
Jesu?s Gonza?lez-Rubio, Germa?n Sanchis-Trilles, Joan-
Andreu Sa?nchez, Jesu?s Andre?s-Ferrer, Guillem
Gasco?, Pascual Mart??nez-Go?mez, Martha-Alicia
Rocha, and Francisco Casacuberta. 2010. The upv-
prhlt combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 277?
281, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Greg Hanneman, Jonathan Clark, and Alon Lavie.
2010. Improved features and grammar selection for
syntax-based mt. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 57?62, Uppsala, Sweden, July.
Association for Computational Linguistics.
Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. Fbk at wmt 2010: Word lattices for
morphological reduction and chunk-based reorder-
ing. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 63?67, Uppsala, Sweden, July. Association
for Computational Linguistics.
Yifan He, Jinhua Du, Andy Way, and Josef van Gen-
abith. 2010. The dcu dependency-based metric in
wmt-metricsmatr 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 324?328, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Kenneth Heafield and Alon Lavie. 2010. Cmu multi-
engine machine translation for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 68?
73, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Carmen Heger, Joern Wuebker, Matthias Huck, Gregor
Leusch, Saab Mansour, Daniel Stein, and Hermann
Ney. 2010. The rwth aachen machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 74?78, Uppsala, Sweden,
July. Association for Computational Linguistics.
Carlos A. Henr??quez Q., Marta Ruiz Costa-jussa`, Vi-
das Daudaravicius, Rafael E. Banchs, and Jose? B.
Marin?o. 2010. Using collocation segmentation to
augment the phrase table. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 79?83, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Almut Silja Hildebrand and Stephan Vogel. 2010.
Cmu system combination via hypothesis selection
for wmt?10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 282?285, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry,
and Philippe Langlais. 2010. The rali machine
translation system for wmt 2010. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 84?90, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Michael Jellinghaus, Alexandros Poulis, and David
Kolovratn??k. 2010. Exodus - exploring smt for eu
institutions. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 91?95, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL
2006 Workshop on Statistical Machine Translation,
New York, New York.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions, Prague, Czech Republic.
39
Philipp Koehn, Barry Haddow, Philip Williams, and
Hieu Hoang. 2010. More linguistic annotation for
statistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 96?101, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger
Schwenk. 2010. Lium smt machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 102?107, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Samuel Larkin, Boxing Chen, George Foster, Ulrich
Germann, Eric Joanis, Howard Johnson, and Roland
Kuhn. 2010. Lessons from nrcs portage system at
wmt 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 108?113, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Gregor Leusch and Hermann Ney. 2010. The rwth
system combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 290?
295, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 114?118, Uppsala, Sweden, July.
Association for Computational Linguistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. Tesla: Translation evaluation of sentences
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 329?
334, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Sushant Narsale. 2010. Jhu system combination
scheme for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 286?289, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Jan Niehues, Teresa Herrmann, Mohammed Mediani,
and Alex Waibel. 2010. The karlsruhe institute
for technology translation system for the acl-wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 119?123, Uppsala, Sweden, July. Association
for Computational Linguistics.
NIST. 2008. Evaluation plan for gale go/no-go phase
3 / phase 3.5 translation evaluations. June 18, 2008.
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat,
Pratyush Banerjee, Ankit K. Srivastava, Jinhua Du,
Pavel Pecina, Sudip Kumar Naskar, Mikel L. For-
cada, and Andy Way. 2010. Matrex: The dcu mt
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 124?129, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Aaron Phillips. 2010. The cunei machine translation
platform for wmt ?10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 130?135, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Juan Pino, Gonzalo Iglesias, Adria` de Gispert, Graeme
Blackwood, Jamie Brunning, and William Byrne.
2010. The cued hifst system for the wmt10 trans-
lation shared task. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 136?141, Uppsala, Sweden,
July. Association for Computational Linguistics.
Marion Potet, Laurent Besacier, and Herve? Blanchon.
2010. The lig machine translation system for wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 142?147, Uppsala, Sweden, July. Association
for Computational Linguistics.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 ?Met-
rics for MAchine TRanslation? challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation, Honolulu, Hawaii.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. Bbn system descrip-
tion for wmt10 system combination task. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 296?
301, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010.
Linear inversion transduction grammar alignments
as a second translation path. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 148?152, Uppsala,
40
Sweden, July. Association for Computational Lin-
guistics.
Germa?n Sanchis-Trilles, Jesu?s Andre?s-Ferrer, Guillem
Gasco?, Jesu?s Gonza?lez-Rubio, Pascual Mart??nez-
Go?mez, Martha-Alicia Rocha, Joan-Andreu
Sa?nchez, and Francisco Casacuberta. 2010.
Upv-prhlt english?spanish system for wmt10. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
153?157, Uppsala, Sweden, July. Association for
Computational Linguistics.
Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.
2010. Incremental decoding for phrase-based sta-
tistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 197?204, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Lane Schwartz. 2010. Reproducible results in parsing-
based machine translation: The jhu shared task sub-
mission. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 158?163, Uppsala, Sweden, July.
Association for Computational Linguistics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and oovs: Two problems for translation
between german and english. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 164?169, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Jo?rg Tiedemann. 2010. To cache or not to cache?
experiments with adaptive models in statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 170?175, Uppsala, Sweden,
July. Association for Computational Linguistics.
Sami Virpioja, Jaakko Va?yrynen, Andre Man-
sikkaniemi, and Mikko Kurimo. 2010. Apply-
ing morphological decompositions to statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 176?181, Uppsala, Sweden,
July. Association for Computational Linguistics.
Zdene?k Z?abokrtsky?, Martin Popel, and David Marec?ek.
2010. Maximum entropy translation model in
dependency-based mt framework. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 182?187, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Billy Wong and Chunyu Kit. 2010. The parameter-
optimized atec metric for mt evaluation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 335?
339, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Francisco Zamora-Martinez and Germa?n Sanchis-
Trilles. 2010. Uch-upv english?spanish system for
wmt10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 188?192, Uppsala, Sweden, July.
Association for Computational Linguistics.
Daniel Zeman. 2010. Hierarchical phrase-based mt
at the charles university for the wmt 2010 shared
task. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 193?196, Uppsala, Sweden, July. Association
for Computational Linguistics.
41
A Pairwise system comparisons by human judges
Tables 13?20 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row. Bolding indicates the winner of the two systems. The difference between
100 and the sum of the complimentary cells is the percent of time that the two systems were judged to
be equal.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
B Automatic scores
The tables on pages 33?32 give the automatic scores for each of the systems.
C Pairwise system comparisons for combined expert and non-expert data
Tables 21?20 show pairwise comparisons between systems for the into English direction when non-
expert judgments have been added.
The number of pairwise comparisons at the ? level of significance increases from 48 to 50, and the
number at the ? level of significants increases from 79 to 80 (basically same number). However, the
? level of significance went up considerably, from 280 to 369. That?s a 31% increase. 75 of ? are
comparisons involving the reference, then the non-reference ? count went up from 205 to 294, a 43%
increase.
R
E
F
C
A
M
B
R
ID
G
E
C
M
U
-S
T
A
T
X
F
E
R
C
U
-Z
E
M
A
N
D
F
K
I
G
E
N
E
V
A
H
U
IC
O
N
G
JH
U
L
IG
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
C
M
U
-H
Y
P
O
S
E
L
-C
O
M
B
O
D
C
U
-C
O
M
B
O
JH
U
-C
O
M
B
O
L
IU
M
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .00? .00? .00? .00? .04? .03? .00? .00? .00? .00? .04? .00? .04? .00? .00? .00? .00? .05? .06? .03? .09? .04? .04?
CAMBRIDGE .79? ? .36 .16? .12? .23? .27 .43 .26? .38 .24 .3 .28 .51 .34 .23 .37 .24 .32 .46 .24 .29 .45 .59? .44
CMU-STATXFER .84? .58 ? .16? .48 .14? .19 .39 .33 .54 .54? .50? .36 .50 .70? .55? .50 .46 .58? .67? .50 .56? .48 .58? .52?
CU-ZEMAN 1.00? .77? .72? ? .76? .37 .73? .74? .79? .77? .77? .81? .75? .94? .86? .77? .89? .67 .77? .79? .81? .81? .77? .96? .86?
DFKI 1.00? .72? .45 .12? ? .32 .48 .50 .52 .53 .56 .65 .53 .62 .55 .43 .61? .50 .68? .73? .70? .60 .59? .72? .71?
GENEVA 1.00? .69? .76? .48 .56 ? .47 .71? .79? .72? .79? .71? .68? .76? .83? .57 .86? .72? .71? .69? .76? .65? .88? .96? .70
HUICONG .86? .54 .29 .12? .26 .37 ? .48 .31 .43 .63? .62? .53 .55 .53? .44 .50 .55 .52 .68? .52? .51 .52? .57 .53
JHU .83? .39 .42 .13? .33 .19? .3 ? .3 .36 .56? .56? .47 .52 .46 .29 .36 .42 .42 .59? .50 .31 .43 .29 .37
LIG .97? .63? .36 .15? .37 .18? .40 .60 ? .62? .57? .39 .35 .54? .46 .33 .34 .38 .54? .48? .42 .44 .50 .61? .56
LIMSI .96? .41 .23 .19? .31 .17? .32 .50 .28? ? .35 .42 .21 .62? .25 .21 .33 .22 .42 .35 .43 .32 .26 .35 .41
LIUM .83? .33 .21? .13? .41 .05? .13? .15? .09? .3 ? .39 .19 .36 .43 .26 .23? .28 .29 .45 .28 .26 .28 .33 .28
NRC .96? .3 .10? .10? .32 .24? .15? .22? .22 .33 .43 ? .26 .58 .26 .24 .3 .50 .36 .45 .47? .23 .38 .36? .35
ONLINEA .96? .55 .57 .14? .42 .16? .42 .4 .39 .53 .52 .47 ? .52? .46 .36 .64 .57 .59 .50 .59 .42 .46 .43 .48
ONLINEB .87? .37 .33 .03? .29 .12? .31 .26 .16? .12? .39 .35 .20? ? .33 .38 .17? .36 .29 .21 .33 .3 .3 .32 .21?
RALI .89? .45 .15? .06? .35 .04? .12? .42 .35 .46 .32 .42 .39 .52 ? .32 .31 .26 .43 .41 .27 .43 .40 .63? .26
RWTH .91? .46 .21? .05? .51 .36 .44 .46 .53 .39 .48 .48 .39 .48 .48 ? .39 .38 .39 .52 .46 .53? .52 .50? .25
UEDIN .96? .40 .33 .03? .28? .03? .28 .29 .49 .38 .61? .3 .32 .50? .34 .24 ? .42 .33 .43 .48 .18? .13 .27 .38
BBN-C .90? .48 .46 .29 .39 .22? .27 .27 .46 .43 .28 .35 .33 .39 .29 .34 .26 ? .28 .44? .33 .26 .62? .36 .28
CMU-HEA-C .89? .50 .23? .14? .30? .21? .26 .25 .17? .33 .43 .16 .36 .43 .26 .29 .24 .24 ? .48 .27 .13 .25 .30 .15
CMU-HYP-C .81? .17 .19? .11? .19? .19? .14? .14? .19? .40 .23 .18 .29 .46 .35 .29 .21 .15? .17 ? .26 .18 .07? .32 .21
DCU-C .88? .27 .25 .11? .22? .24? .20? .28 .21 .35 .50 .10? .31 .44 .27 .29 .22 .21 .2 .30 ? .12? .26 .26 .08
JHU-C .86? .48 .16? .16? .33 .21? .35 .41 .32 .44 .39 .35 .39 .37 .26 .19? .50? .23 .32 .43 .40? ? .36 .27 .39
LIUM-C .87? .41 .36 .13? .31? .08? .21? .48 .31 .47 .44 .24 .39 .52 .28 .28 .33 .27? .25 .67? .26 .44 ? .54? .48
RWTH-C .88? .18? .13? .04? .22? .04? .14 .24 .25? .3 .33 .05? .43 .50 .30? .13? .23 .14 .18 .21 .19 .23 .11? ? .24
UPV-C .92? .25 .12? .10? .16? .3 .25 .34 .29 .31 .34 .29 .39 .65? .39 .36 .3 .45 .27 .36 .23 .16 .24 .28 ?
> others .90 .44 .31 .13 .33 .18 .29 .37 .34 .42 .44 .38 .37 .51 .41 .31 .38 .35 .38 .48 .39 .36 .40 .46 .37
>= others .98 .66 .51 .21 .42 .27 .51 .59 .53 .65 .71 .66 .52 .71 .65 .55 .65 .64 .70 .77 .72 .65 .64 .77 .68
Table 13: Sentence-level ranking for the WMT10 French-English News Task
42
R
E
F
C
A
M
B
R
ID
G
E
C
U
-Z
E
M
A
N
D
F
K
I
E
U
G
E
N
E
V
A
JH
U
K
O
C
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .08? .02? .00? .04? .08? .13? .06? .09? .09? .07? .16? .11? .12? .12? .12? .05? .07? .08? .09?
CAMBRIDGE .82? ? .16? .24? .15? .07? .35 .10? .42 .36 .43 .27 .67? .46 .39 .44 .40 .46 .48? .40
CU-ZEMAN .98? .82? ? .47 .54? .62? .71? .41 .79? .82? .70? .67? .85? .90? .75? .72? .92? .82? .88? .82?
DFKI .95? .66? .31 ? .46 .25? .78? .36 .59 .62? .75? .65? .45 .56? .75? .69? .71? .63? .57 .65?
EU .96? .78? .30? .41 ? .55 .68? .16? .76? .72? .82? .67? .63? .86? .78? .78? .76? .76? .75? .71?
GENEVA .86? .81? .23? .55? .34 ? .65? .25? .65? .70? .69? .66? .77? .71? .70? .89? .75? .63? .84? .75?
JHU .77? .42 .15? .22? .22? .22? ? .06? .58? .47 .52? .49 .70? .61? .53 .64? .53? .65? .68? .50
KOC .85? .67? .4 .58 .55? .69? .82? ? .76? .85? .81? .72? .86? .82? .86? .85? .77? .77? .74? .79?
LIMSI .84? .23 .08? .29 .09? .30? .21? .08? ? .33 .37 .17? .51 .40 .29 .45 .49 .40 .61? .28
LIUM .85? .39 .07? .32? .11? .21? .44 .07? .46 ? .44 .4 .32 .44 .37 .64? .35 .40 .35 .42
NRC .91? .43 .15? .20? .11? .25? .21? .09? .31 .45 ? .32 .48 .44 .49 .61? .52? .30 .58? .40
ONLINEA .80? .51 .21? .33? .23? .15? .41 .14? .60? .42 .54 ? .52? .56? .36 .67? .61? .45 .50 .44
ONLINEB .87? .23? .08? .43 .23? .11? .12? .08? .27 .36 .43 .25? ? .38 .31 .33 .52 .33? .46 .29
RALI .83? .38 .05? .27? .11? .15? .22? .10? .36 .44 .49 .31? .50 ? .38 .44 .42 .37 .38 .34
RWTH .76? .33 .11? .12? .15? .17? .34 .05? .34 .44 .29 .42 .49 .40 ? .56 .48 .44 .53? .50
UEDIN .84? .29 .20? .17? .12? .09? .19? .07? .33 .23? .24? .24? .56 .31 .3 ? .36? .27 .51 .18?
CMU-HEAFIELD-COMBO .90? .23 .04? .23? .18? .12? .22? .11? .32 .41 .20? .23? .28 .31 .31 .11? ? .29 .24 .3
KOC-COMBO .91? .26 .08? .31? .17? .28? .20? .07? .23 .26 .19 .36 .57? .37 .32 .32 .42 ? .38 .34
RWTH-COMBO .85? .21? .02? .36 .16? .07? .12? .07? .16? .3 .30? .4 .34 .32 .06? .26 .35 .16 ? .21?
UPV-COMBO .87? .38 .08? .30? .19? .19? .37 .11? .39 .24 .33 .37 .44 .27 .34 .46? .35 .28 .50? ?
> others .87 .43 .15 .30 .22 .25 .38 .13 .44 .45 .46 .41 .53 .49 .44 .52 .53 .45 .53 .45
>= others .92 .63 .26 .40 .32 .35 .53 .26 .66 .63 .62 .55 .68 .66 .63 .70 .74 .68 .75 .66
Table 14: Sentence-level ranking for the WMT10 English-French News Task
43
R
E
F
A
A
L
T
O
C
M
U
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
H
U
IC
O
N
G
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
U
E
D
IN
U
M
D
U
P
P
S
A
L
A
U
U
-M
S
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
C
M
U
-H
Y
P
O
S
E
L
-C
O
M
B
O
JH
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .03? .00? .06? .03? .00? .00? .05? .00? .00? .03? .06? .09? .06? .00? .09? .03? .03? .14? .03? .06? .03? .03? .06? .00?
AALTO 1.00? ? .50 .31 .60 .69? .39 .41 .71? .31 .45 .60? .59? .65? .66? .64? .81? .45 .41 .69? .72? .75? .55 .55? .76? .57?
CMU .93? .31 ? .29 .49 .57? .38 .50 .74? .13? .44 .59? .57? .59? .60? .67? .59? .41 .50 .68? .67? .46 .64? .55? .67? .54?
CU-ZEMAN 1.00? .44 .56 ? .58 .64? .17 .44 .75? .38 .50 .54? .76? .79? .73? .72? .72? .50? .73? .78? .80? .68? .72? .62? .68? .73?
DFKI .92? .25 .32 .27 ? .53 .36 .46 .65? .07? .50 .47 .47 .69? .56 .35 .55 .58 .47 .67? .61? .52 .47 .38 .67? .51
FBK .97? .20? .16? .14? .38 ? .11? .31 .45 .10? .22? .36 .50 .57? .37 .43 .40 .12? .17? .48? .43 .35 .38 .22 .38 .39
HUICONG .93? .35 .28 .46 .43 .75? ? .52 .69? .16? .39 .42 .64? .79? .31 .51? .78? .27 .41 .49 .74? .68? .60? .37 .68? .56?
JHU .86? .34 .29 .16 .43 .31 .26 ? .61? .15? .35 .36 .45 .69? .52? .56? .64? .27 .36 .70? .53 .47 .66? .52 .68? .44
KIT .89? .21? .10? .14? .29? .33 .19? .14? ? .03? .27 .21? .36 .46 .17? .29 .24 .25? .25? .48 .23? .31 .38 .2 .36 .12?
KOC .96? .58 .77? .48 .70? .77? .58? .71? .97? ? .77? .90? .72? .82? .76? .84? .81? .84? .66? .83? .87? .79? .77? .75? .93? .71?
LIMSI 1.00? .23 .28 .35 .35 .53? .33 .45 .41 .19? ? .49 .48 .63? .49 .63? .52 .36 .29 .73? .53? .45 .59? .29 .56? .59?
LIU .88? .12? .15? .16? .39 .21 .46 .36 .61? .00? .27 ? .44 .63? .49 .45 .53 .27? .33 .67? .55? .46 .44 .32 .37 .55
ONLINEA .92? .15? .23? .24? .42 .34 .21? .35 .50 .10? .32 .36 ? .41 .4 .44 .37 .32 .34 .36 .4 .47 .3 .26 .48 .41
ONLINEB .68? .18? .29? .17? .26? .24? .18? .23? .33 .18? .23? .27? .34 ? .3 .15? .29 .24? .15? .44 .28 .33? .20? .21? .38 .3
RWTH .88? .17? .20? .20? .37 .49 .41 .23? .61? .16? .4 .3 .43 .56 ? .39 .50 .26 .49 .37 .29 .34 .41 .26 .44 .2
UEDIN .89? .14? .22? .13? .62 .34 .18? .22? .39 .03? .17? .3 .44 .67? .42 ? .39 .15? .14? .52? .40 .36 .43 .26 .41 .38
UMD .91? .07? .14? .08? .36 .34 .11? .25? .48 .16? .24 .34 .52 .56 .41 .45 ? .16? .21? .41 .28 .29 .43 .29 .25 .23
UPPSALA .97? .32 .34 .17? .36 .54? .23 .37 .70? .00? .41 .62? .56 .68? .57 .64? .59? ? .2 .63? .69? .51? .60? .33 .69? .63?
UU-MS .82? .22 .43 .14? .45 .51? .19 .21 .68? .14? .39 .52 .60 .64? .44 .53? .61? .28 ? .36 .58? .52? .53? .30 .64? .44
BBN-C .86? .25? .10? .07? .27? .17? .23 .18? .35 .07? .15? .12? .32 .41 .3 .19? .22 .15? .27 ? .39 .06? .23? .11? .21 .18?
CMU-HEA-C .87? .14? .15? .08? .29? .33 .04? .26 .53? .00? .20? .24? .44 .31 .46 .23 .53 .15? .13? .27 ? .40 .2 .14? .22 .28
CMU-HYP-C .94? .25? .24 .14? .44 .3 .15? .26 .47 .08? .45 .31 .42 .67? .24 .36 .46 .14? .21? .50? .32 ? .43 .28 .51? .42
JHU-C .97? .34 .11? .20? .29 .34 .29? .03? .38 .12? .07? .29 .55 .67? .34 .32 .23 .24? .24? .48? .40 .32 ? .27 .37 .31
KOC-C .88? .00? .23? .21? .53 .44 .29 .22 .43 .08? .36 .50 .53 .63? .39 .37 .39 .28 .19 .64? .61? .38 .55 ? .48? .46
RWTH-C .82? .09? .06? .29? .25? .25 .18? .18? .24 .03? .19? .26 .36 .54 .25 .26 .33 .06? .14? .29 .22 .23? .3 .17? ? .13?
UPV-C .97? .17? .21? .17? .36 .36 .23? .19 .67? .20? .18? .29 .41 .40 .40 .38 .48 .17? .31 .50? .43 .27 .27 .27 .65? ?
> others .91 .23 .25 .20 .39 .42 .24 .30 .53 .11 .31 .38 .47 .59 .42 .43 .48 .27 .30 .53 .49 .42 .44 .31 .51 .41
>= others .96 .42 .46 .36 .50 .66 .47 .53 .72 .23 .52 .59 .63 .73 .62 .66 .68 .51 .55 .77 .73 .65 .67 .59 .75 .64
Table 15: Sentence-level ranking for the WMT10 German-English News Task
R
E
F
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
S
F
U
U
E
D
IN
U
P
P
S
A
L
A
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .03? .06? .01? .02? .05? .00? .00? .01? .04? .03? .01? .01? .01? .02? .01? .01? .05? .06?
CU-ZEMAN .97? ? .85? .67? .62? .78? .58? .70? .64? .80? .85? .64? .52 .80? .61? .79? .69? .76? .73?
DFKI .89? .14? ? .36? .24? .38 .30? .27? .36? .36? .55 .35? .21? .41 .39 .46 .38? .47 .37?
FBK .97? .30? .59? ? .35? .42 .12? .36 .48 .48 .64? .39 .29? .46 .30? .44 .46 .48 .38
JHU .98? .27? .72? .57? ? .59? .30? .51 .53 .56? .65? .43 .39 .66? .45 .56 .61? .52 .47
KIT .92? .18? .55 .42 .29? ? .23? .32 .32? .43 .53? .41 .27? .43 .23? .41 .41 .42 .37
KOC 1.00? .37? .64? .82? .62? .70? ? .74? .74? .74? .82? .63? .48 .62? .65? .73? .67? .81? .71?
LIMSI .95? .27? .68? .39 .45 .49 .17? ? .49 .74? .70? .51 .28? .58? .32 .51 .53? .52? .31
LIU .95? .32? .59? .4 .36 .58? .21? .37 ? .39 .74? .33? .23? .55? .36? .49 .42 .46 .38
ONLINEA .95? .16? .55? .4 .36? .45 .21? .23? .50 ? .56? .38 .23? .41 .23? .48 .4 .50 .33?
ONLINEB .92? .12? .42 .26? .27? .33? .14? .23? .21? .32? ? .24? .14? .39 .19? .29? .27? .36 .32?
RWTH .98? .33? .61? .51 .47 .46 .30? .33 .52? .55 .71? ? .33? .57? .45 .40 .51? .47 .46
SFU .98? .42 .77? .66? .51 .69? .48 .68? .69? .72? .77? .56? ? .82? .53 .65? .69? .73? .62?
UEDIN .94? .17? .51 .4 .31? .49 .34? .25? .30? .52 .52 .36? .10? ? .33? .31 .42 .38 .22?
UPPSALA .97? .36? .55 .51? .47 .70? .25? .46 .57? .67? .71? .41 .38 .54? ? .53? .42 .58? .40
CMU-HEAFIELD-COMBO .96? .17? .49 .36 .36 .37 .21? .35 .49 .42 .64? .38 .28? .48 .28? ? .35 .46 .35
KOC-COMBO .99? .27? .56? .32 .27? .32 .23? .32? .41 .55 .64? .30? .21? .37 .36 .41 ? .34 .36
RWTH-COMBO .92? .17? .50 .34 .35 .41 .09? .25? .38 .4 .54 .38 .20? .42 .19? .28 .35 ? .16?
UPV-COMBO .93? .23? .58? .38 .36 .51 .23? .50 .49 .57? .60? .42 .28? .51? .3 .38 .46 .48? ?
> others .95 .24 .57 .44 .37 .48 .24 .39 .45 .51 .63 .40 .27 .51 .34 .45 .44 .49 .39
>= others .98 .28 .62 .56 .46 .60 .30 .51 .55 .59 .70 .51 .34 .62 .47 .59 .59 .65 .55
Table 16: Sentence-level ranking for the WMT10 English-German News Task
44
R
E
F
C
A
M
B
R
ID
G
E
C
O
L
U
M
B
IA
C
U
-Z
E
M
A
N
D
F
K
I
H
U
IC
O
N
G
JH
U
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
U
P
C
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .01? .01? .01? .00? .00? .00? .00? .00? .01? .02? .05? .01? .04?
CAMBRIDGE .95? ? .23? .14? .34? .31? .41 .34 .62? .45? .35 .40? .42 .22? .44
COLUMBIA .97? .58? ? .25? .52 .45 .59? .53? .65? .60? .47 .56? .55? .45 .58?
CU-ZEMAN .96? .71? .59? ? .60? .68? .79? .66? .75? .80? .66? .79? .78? .69? .75?
DFKI .97? .51? .37 .23? ? .43 .59? .52? .66? .62? .48 .53? .55? .55? .64?
HUICONG .95? .50? .34 .21? .41 ? .45 .50 .66? .61? .39 .50? .59? .40 .52?
JHU .98? .39 .22? .12? .30? .33 ? .37 .56? .51? .34 .39 .34? .22? .34
ONLINEA .96? .46 .37? .23? .32? .38 .44 ? .59? .53? .4 .50 .36 .30? .54?
ONLINEB .88? .25? .21? .16? .23? .21? .27? .23? ? .35 .24? .28? .34? .22? .36
UEDIN .96? .31? .28? .10? .25? .19? .25? .31? .48 ? .23? .27? .31 .23? .2
UPC .94? .47 .4 .20? .41 .33 .43 .46 .66? .56? ? .50? .52? .48? .49?
BBN-COMBO .95? .26? .31? .09? .32? .34? .33 .37 .54? .44? .33? ? .35 .24? .34
CMU-HEAFIELD-COMBO .91? .39 .21? .08? .34? .22? .16? .42 .57? .45 .31? .31 ? .14? .27
JHU-COMBO .95? .40? .32 .15? .36? .31 .44? .50? .66? .50? .32? .47? .43? ? .43?
UPV-COMBO .92? .35 .28? .16? .27? .23? .38 .28? .47 .30 .28? .26 .35 .25? ?
> others .95 .41 .30 .15 .33 .32 .39 .39 .56 .48 .34 .41 .43 .32 .43
>= others .99 .61 .45 .27 .45 .50 .61 .54 .70 .69 .51 .62 .66 .55 .66
Table 17: Sentence-level ranking for the WMT10 Spanish-English News Task
R
E
F
C
A
M
B
R
ID
G
E
C
U
-Z
E
M
A
N
D
C
U
D
F
K
I
JH
U
K
O
C
O
N
L
IN
E
A
O
N
L
IN
E
B
S
F
U
U
E
D
IN
U
P
V
U
C
H
-U
P
V
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .02? .07? .15? .07? .02? .11? .14? .07? .07? .03? .06? .09? .06? .03? .07?
CAMBRIDGE .91? ? .28? .45 .38 .45 .11? .52 .61? .21? .52 .47 .35 .54 .51 .39 .49
CU-ZEMAN .95? .70? ? .79? .75? .85? .49 .83? .82? .74? .87? .67? .85? .81? .80? .70? .74?
DCU .93? .32 .21? ? .45 .32 .09? .70? .59 .24? .48 .38 .29 .32 .36 .24 .14?
DFKI .80? .41 .15? .45 ? .38 .12? .64? .57 .4 .57 .31 .41 .59 .50 .48 .47
JHU .90? .37 .10? .52 .56 ? .17? .67? .67? .26? .34 .3 .49 .54 .53? .47 .35
KOC .98? .87? .47 .88? .73? .76? ? .76? .87? .67? .83? .86? .90? .87? .90? .86? .86?
ONLINEA .82? .42 .08? .30? .18? .24? .20? ? .49 .36 .25? .17? .25? .45 .30? .29 .18?
ONLINEB .76? .26? .10? .32 .37 .22? .10? .34 ? .21? .28 .24? .32 .33 .22? .19? .27?
SFU .91? .54? .19? .67? .51 .63? .27? .64 .72? ? .74? .57? .68? .77? .71? .64? .46
UEDIN .91? .3 .08? .4 .38 .34 .14? .71? .49 .09? ? .34 .4 .58 .33 .3 .31
UPV .94? .34 .07? .41 .53 .54 .07? .73? .61? .27? .45 ? .37 .51 .44 .38 .48?
UCH-UPV .90? .55 .07? .58 .51 .41 .08? .69? .52 .24? .51 .46 ? .47 .41 .49 .49
CMU-HEAFIELD-COMBO .83? .29 .13? .37 .38 .35 .07? .48 .54 .08? .29 .26 .28 ? .17? .21? .21
KOC-COMBO .88? .27 .15? .40 .42 .24? .03? .62? .60? .15? .41 .27 .34 .53? ? .3 .40
RWTH-COMBO .92? .36 .21? .52 .33 .31 .10? .55 .65? .14? .37 .22 .41 .52? .48 ? .31
UPV-COMBO .91? .32 .13? .69? .4 .32 .09? .76? .52? .36 .38 .19? .31 .45 .35 .28 ?
> others .89 .39 .15 .48 .44 .41 .14 .61 .58 .29 .46 .36 .42 .51 .44 .39 .40
>= others .93 .54 .23 .61 .55 .55 .19 .69 .71 .40 .61 .55 .54 .68 .62 .59 .60
Table 18: Sentence-level ranking for the WMT10 English-Spanish News Task
45
R
E
F
A
A
L
T
O
C
M
U
C
U
-B
O
JA
R
C
U
-Z
E
M
A
N
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .04? .02? .03? .00? .02? .00? .03? .03? .04? .01? .04? .02?
AALTO .88? ? .49 .51 .22? .38 .64? .55? .57? .71? .64? .65? .59?
CMU .97? .35 ? .4 .14? .18? .59? .49? .45? .57? .50? .34 .43
CU-BOJAR .90? .33 .43 ? .12? .20? .64? .45 .45 .54? .42 .42 .41
CU-ZEMAN .99? .60? .77? .75? ? .56? .81? .78? .88? .79? .84? .84? .76?
ONLINEA .92? .46 .68? .59? .28? ? .65? .54? .72? .75? .58? .57? .66?
ONLINEB .97? .27? .28? .21? .10? .17? ? .25? .32 .22 .21? .32 .28
UEDIN .95? .28? .26? .38 .07? .22? .49? ? .60? .52? .33 .31 .32
BBN-COMBO .92? .31? .20? .39 .08? .15? .41 .16? ? .27 .25 .3 .26
CMU-HEAFIELD-COMBO .90? .13? .23? .25? .07? .15? .31 .23? .34 ? .18? .35 .28
JHU-COMBO .93? .20? .19? .33 .08? .25? .48? .39 .38 .52? ? .37 .42
RWTH-COMBO .92? .18? .37 .38 .13? .25? .34 .28 .43 .40 .26 ? .25
UPV-COMBO .96? .25? .36 .41 .11? .27? .45 .35 .37 .44 .31 .34 ?
> others .93 .28 .36 .38 .11 .23 .49 .38 .47 .48 .38 .40 .40
>= others .98 .43 .55 .55 .22 .37 .70 .61 .70 .71 .62 .65 .63
Table 19: Sentence-level ranking for the WMT10 Czech-English News Task
R
E
F
C
U
-B
O
JA
R
C
U
-T
E
C
T
O
C
U
-Z
E
M
A
N
D
C
U
E
U
R
O
T
R
A
N
S
K
O
C
O
N
L
IN
E
A
O
N
L
IN
E
B
P
C
-T
R
A
N
S
P
O
T
S
D
A
M
S
F
U
U
E
D
IN
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
D
C
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .04? .04? .03? .01? .05? .03? .08? .04? .04? .03? .02? .02? .04? .08? .04? .07? .04?
CU-BOJAR .87? ? .46 .27? .12? .28? .16? .17? .44 .4 .11? .27? .41 .28 .52? .28 .42 .43
CU-TECTO .88? .36 ? .30? .23? .38 .17? .28? .56? .44 .29? .27? .36 .45 .51? .4 .58? .35
CU-ZEMAN .91? .58? .51? ? .38 .49 .19? .39 .62? .63? .36 .41 .48 .51? .58? .48? .54? .55?
DCU .98? .73? .52? .43 ? .59? .22? .47 .74? .63? .47? .53? .56? .77? .77? .62? .76? .71?
EUROTRANS .88? .61? .47 .33 .30? ? .10? .33 .51 .54? .25? .27? .49 .57? .59? .49 .57? .60?
KOC .93? .69? .67? .54? .49? .77? ? .54? .71? .70? .51? .55? .64? .72? .78? .65? .76? .78?
ONLINEA .91? .62? .57? .51 .39 .44 .24? ? .66? .62? .39 .43 .55? .60? .61? .59? .73? .61?
ONLINEB .91? .31 .29? .27? .13? .33 .14? .19? ? .44 .22? .09? .39 .19 .34 .24? .22? .39
PC-TRANS .88? .45 .43 .24? .26? .29? .21? .24? .49 ? .22? .27? .37 .43 .55? .33? .49 .41
POTSDAM .88? .60? .51? .40 .27? .59? .25? .47 .63? .64? ? .45 .52? .56? .69? .61? .70? .68?
SFU .95? .52? .56? .4 .30? .61? .27? .39 .65? .64? .29 ? .55? .54? .76? .53? .70? .60?
UEDIN .94? .39 .44 .33 .23? .32 .20? .26? .32 .49 .25? .26? ? .43 .57? .18 .46? .42
CMU-HEAFIELD-COMBO .91? .42 .39 .23? .10? .27? .14? .19? .23 .35 .24? .19? .28 ? .48? .28 .34 .29
DCU-COMBO .84? .23? .27? .23? .03? .31? .10? .21? .42 .31? .15? .10? .16? .20? ? .18? .27? .22?
KOC-COMBO .91? .37 .49 .25? .10? .39 .17? .32? .42? .55? .17? .27? .26 .33 .41? ? .32 .22
RWTH-COMBO .88? .29 .34? .28? .05? .26? .10? .17? .48? .43 .16? .15? .24? .33 .46? .36 ? .29
UPV-COMBO .92? .37 .52 .22? .09? .25? .10? .19? .28 .47 .15? .25? .33 .24 .49? .34 .39 ?
> others .91 .45 .44 .32 .20 .39 .16 .29 .49 .49 .25 .28 .40 .43 .54 .39 .50 .45
>= others .96 .66 .60 .50 .38 .54 .33 .44 .70 .62 .44 .45 .62 .69 .75 .66 .70 .68
Table 20: Sentence-level ranking for the WMT10 English-Czech News Task
46
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-. +,/- +,/. +,-0 +,/+ +,-1 +,-0 +,-. +,-1 +,/+ +,-/ +,2- +,3/ +,24 +,./ +,/3 +,--''567*8'* +,4+ +,/1 !"#$ !"#% !"#% !"#% !"#% !"#! !"## !"#& !"#' !"() !"'& !"*& !"%% !"#$ !"%)78&69:67*8'* !"&' +,/1 +,// +,/- +,/- +,/. +,/. +,/+ +,/- !"#& +,/+ !"() !"'& +,.; !"%% !"#$ +,-178& +,// +,// +,/. +,/2 +,/2 +,/3 +,/2 +,-; +,/2 +,/- +,-1 +,24 !"'& +,.. +,-3 +,/- +,-;7&6'*<"= +,// +,/; +,/. +,/+ +,/3 +,-0 +,/+ +,-/ +,/3 +,/. +,-4 +,2/ +,3/ +,.+ +,.4 +,/. +,-;7&6>?8"5 +,22 +,-0 +,/2 +,-/ +,-; +,-/ +,-/ +,.0 +,-/ +,-; +,-2 +,23 +,3. +,2+ +,.+ +,/+ +,-3<@&67*8'* +,;2 +,/4 !"#$ !"#% !"#% +,/. !"#% +,-1 +,/. +,// +,-0 +,21 !"'& +,./ +,-2 +,/. +,-4*5#A5?B +,.4 +,/- +,/2 +,-0 +,/+ +,-1 +,/+ +,-2 +,-4 +,-0 +,-- +,2- +,3- +,24 +,.- +,-0 +,-2*5#A5?C +,4+ !"#) !"#$ !"#% +,/. +,/. +,/. +,-0 +,/- +,/; +,/+ !"() !"'& +,.; +,-. +,/. +,-;=D)@67*8'* +,;/ +,/1 +,// !"#% !"#% +,/. +,/. +,-0 +,/. +,/; +,-0 !"() !"'& +,./ +,-2 +,// +,-1&?EA5 +,;3 +,/4 +,/- +,/2 +,/. +,/3 +,/2 +,-1 +,/. +,// +,-0 +,21 +,3; +,.- +,-2 +,// +,-1&FG67*8'* +,;. +,/1 +,// !"#% !"#% +,/. +,/. +,-0 +,/- +,/; +,/+ !"() !"'& +,./ +,-. +,// +,-1!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-0 +,/2 -,04 !"%' +,+2 +,+4 +,24 +,-2 +,2/ +,/2 +,/2 +,31 ;,2+''567*8'* +,/- !"#& /,-; !"%+ !"!* +,33 !"** !"%+ !"*! !"#+ !"#+ !"(# &"'!78&69:67*8'* +,/- +,/4 /,-. !"%+ !"!* +,+0 !"** +,-1 !"*! +,/4 +,/4 !"(# ;,0178& +,/+ +,/. /,3+ +,-/ +,+2 +,+0 +,.3 +,-; +,21 +,/- +,/- +,22 ;,;;7&6'*<"= +,/2 +,// /,2. +,-/ +,+2 +,+/ +,.+ +,-/ +,24 +,// +,// +,30 ;,2-7&6>?8"5 +,-4 +,-0 -,44 +,.; +,+2 +,+. +,2. +,.4 +,22 +,-0 +,-0 +,3- /,-/<@&67*8'* +,/. +,// /,2/ +,-; +,+2 +,+. +,.2 +,-; +,20 +,/; +,/; +,2. 4,++*5#A5?B +,-0 +,/3 -,14 +,-2 +,+2 +,+4 +,24 +,-. +,2/ +,/2 +,/2 +,31 ;,3.*5#A5?C !"## !"#& /,-3 !"%+ !"!* !"'$ +,.2 +,-4 !"*! !"#+ !"#+ +,2- ;,3.=D)@67*8'* +,/- +,/; /,-+ !"%+ !"!* +,+4 +,.2 +,-4 +,20 +,/4 +,/4 +,2- 4,+;&?EA5 +,/. !"#& /,-+ !"%+ !"!* +,3- +,.2 +,-4 +,20 +,/4 +,/4 +,2. ;,;-&FG67*8'* +,/- !"#& #"%& !"%+ !"!* +,+4 !"** +,-4 +,20 !"#+ !"#+ +,2- ;,00
="5H
IJK)"5L*=E MNJFO(#?P?=(=?7"## KQR(J"5H MNKSB CNDM6N CH"=%
RNMNTJ("EU RNMNTJ(="5HRNMNTJ(@)?= KNVOB K?8VTK,-./012345670888((66((((((R?)=A7%(%&'8A)?E()*(WOKM(R?)=A7%RBMJ(2+3+X(F#&%()D*('"%?#A5?(8?)=A7%(!CSNY("5E(WOKM$,((K7*=?%(L*=(Z"##Z(!?5[=?(\RM3+()?%)%?)$("5E(Z%&'Z(!%&'%?)(*L()@?(@&8"5#]("%%?%%?E(E")"$("=?(%@*D5, SJ^C- SJ9C-
WOKMCSNY(G3."
RM6W_I RM68W_I C"E`?=(L&## C"E`?=(#A)? BMN_(2,3
I=E*7 YS_@O(#?P?=(CSNY
K?8VTK(CSNY I_Y6S:a
MNKSB(R
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./01 ./0. %&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 %&'% %&'* ./2.+"-'56789 ./00 ./0. %&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* ./1. ./1; %&'* ./2.+-&*<=*+,-', ./>. ./0. %&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 ./1; %&'* ./2.+-&*?@A*+,-', %&(( ./0. %&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* %&-/ %&-) ./2> ./2.+-&*%B"BCD95 ./24 ./2: ./22 ./22 ./21 ./1; ./20 ./2: ./24 ./E; %&/( %&.( %&-. ./20 ./1:+&*F9-") ./E4 ./2E ./22 ./24 ./2. ./10 ./2E ./21 ./1: ./E1 %&/' %&,. %&.( ./23 ./117+&*+,-', ./>E %&+/ %&'( %&') %&'* %&'- %&') ./0E %&'- %&.. %&/* %&-. %&'% %&'* %&'/7G6 ./1E ./20 ./23 ./2E ./24 ./10 ./23 ./21 ./1; ./E2 ./4> ./3E ./1. ./22 ./1089)9H" ./E> ./22 ./2E ./1; ./1; ./11 ./2. ./24 ./10 ./E3 ./41 ./E> ./3> ./23 ./11?&6+,)8 ./24 ./20 ./22 ./22 ./21 ./2. ./20 ./2: ./24 ./E: ./4> ./3E ./11 ./22 ./1>I?&*+,-', ./02 ./0. %&'( ./2: %&'* ./23 ./2: ./04 ./23 ./3E %&/* ./1. ./1: %&'* ./2.I?& ./2; ./0. %&'( ./2> ./20 ./2E ./2: ./0. ./23 ./34 %&/* ./3: ./1> ./2> ./1;#68 ./23 ./2; ./20 ./22 ./22 ./24 ./20 ./2; ./2E ./3. ./4> ./30 ./12 ./20 ./1:#6-%6 ./02 ./2; ./20 ./2> ./20 ./2E ./2> ./0. ./23 ./34 ./4> ./3: ./1> ./2> ./1:#6&-*+,-', ./01 ./0. %&'( ./2: ./2> ./23 ./2: ./04 ./23 %&.. %&/* ./14 ./1; ./2> ./1;#6&- ./>4 ./0. %&'( ./2: ./2> ./23 %&') ./04 %&'- ./3E %&/* ./14 ./1; %&'* ./2.)5+ ./00 ./0. %&'( ./2> ./20 ./2E ./2: ./04 ./23 ./3E %&/* ./3; ./1: ./2> ./1;,)#6)9J ./2E ./2; ./20 ./22 ./22 ./2. ./20 ./2: ./24 ./3. ./4> ./30 ./11 ./21 ./10,)#6)9K ./>4 ./0. %&'( ./2: %&'* ./23 ./2: ./04 %&'- ./3E %&/* ./1E ./1; ./2> ./1;5"#6 ./02 ./0. ./20 ./2> ./20 ./2E ./2: ./04 %&'- ./3E %&/* ./1. ./1: ./2> ./1;5LB?*+,-', %&(( ./0. %&'( ./2; %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* ./2.5LB? ./22 ./2; ./20 ./2> ./20 ./2E ./2: ./0. ./23 ./34 ./4> ./3: ./10 %&'( ./1;&976) ./02 ./0. ./20 ./2> ./2> ./23 ./2: ./04 %&'- ./3E %&/* ./1. ./1; %&'* ./1;&AH*+,-', ./0: ./0. %&'( ./2: %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* %&'/!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', %&'* ./0. 2/:. ./24 %&%. ./.E %&.( ./2. %&.- %&+/ ./3. (&*/+"-'56789 ./2> ./0. 2/:. ./24 %&%. ./.2 %&.( ./2. ./33 %&+/ ./3. >/02+-&*<=*+,-', %&'* ./0. 2/:4 %&', %&%. ./.3 %&.( %&'/ ./31 %&+/ ./3. >/0;+-&*?@A*+,-', %&'* ./0. 2/:1 %&', %&%. ./.3 %&.( %&'/ %&.- %&+/ ./3. >/0;+-&*%B"BCD95 ./22 ./2> 2/20 ./1: %&%. ./.3 ./31 ./1; ./34 ./2: ./E2 >/40+&*F9-") ./24 ./21 2/3. ./1. ./.E ./.E ./E> ./1. ./E> ./22 ./4; 0/4E7+&*+,-', %&'* %&+/ 2/:> ./2E %&%. ./.3 %&.( %&'/ %&.- %&+/ %&./ >/>:7G6 ./24 ./20 2/10 ./10 ./.E ./.1 ./34 ./12 ./E; ./22 ./4; 0/.489)9H" ./1; ./22 2/E; ./12 ./.E ./.2 ./E; ./13 ./E> ./21 ./4> 2/0>?&6+,)8 ./22 ./2: 2/0. ./1: %&%. ./.3 ./33 ./1> ./34 ./2> ./E1 0/;;I?&*+,-', ./2> ./0. 2/>> ./24 %&%. ./.4 ./30 ./2. ./33 ./0. ./E; >/04I?& ./2> ./2; 2/>1 ./2. %&%. ./.: ./32 ./2. ./3E ./0. ./E: >/11#68 ./22 ./2: 2/0E ./1: %&%. ./.2 ./31 ./1; ./34 ./2: ./E0 >/43#6-%6 ./20 ./2; 2/>E ./24 %&%. ./.2 ./30 ./2. ./33 ./2; ./E> >/34#6&-*+,-', ./2> ./0. 2/>> ./24 %&%. ./.3 %&.( %&'/ ./33 ./0. ./E; >/>.#6&- %&'* %&+/ '&** %&', %&%. ./.0 %&.( %&'/ %&.- %&+/ ./E; >/2;)5+ ./2> ./0. 2/>; ./2. %&%. ./.1 ./30 ./2. ./33 ./0. ./E; >/23,)#6)9J ./22 ./2: 2/21 ./2. %&%. ./.: ./33 ./1: ./34 ./2: ./E2 >/E;,)#6)9K %&'* ./0. 2/:. %&', %&%. %&// %&.( %&'/ %&.- %&+/ ./E; >/>35"#6 ./2> ./0. 2/:. %&', %&%. ./.0 ./30 ./2. ./33 ./0. ./E: >/115LB?*+,-', %&'* %&+/ 2/:> %&', %&%. ./.4 %&.( %&'/ %&.- %&+/ ./3. >/>35LB? ./20 ./2; 2/>3 ./2. %&%. ./.1 ./30 ./2. ./33 ./0. ./E: >/3;&976) ./2> ./0. 2/:E ./24 %&%. ./.1 ./30 ./2. ./33 %&+/ ./E: >/11&AH*+,-', %&'* %&+/ '&** %&', %&%. ./.3 %&.( ./2. %&.- %&+/ ./3. >/01
0123456738#9:5;(((**(((((M9B56+%(%&'-6N97(B,(OPQR(M9B56+%MJRS(E.4.T(A#&%(BL,('"%9#6)9(-9B56+%(!KUVW(")7(OPQR$/((Q+,59%(D,5(X"##X(!9)Y59(ZMR4.(B9%B%9B$(")7(X%&'X(!%&'%9B(,D(B?9(?&-")#@("%%9%%97(7"B"$("59(%?,L)/5")[ MR*O\] MR*-O\] K"7895(D&## K"7895(#6B9 ]\W*U=^ US_K1 US<K1Q9-`aQ Q9-`aQ(KUVW
P(#9N95(KUVW P(#9N95(59+"## QbM(S")[ RVQUJ(M RVQUJ QB")D,57
MVRVaS("7c MVRVaS(?B95 MVRVaS(5")[ QV`PJJRV\(E/4
KUVW(43" OPQRRVSA ]S ]57,+ WU\? KVLR*V K["5%
47
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-. +,/- +,/0 +,/+ +,-1 +,-0 +,/+ +,/. +,-2 +,./ +,3/ +,./ +,0/ +,-4 +,02''567*8'* !"## !"$% +,// !"$$ !"$& +,/+ +,/2 +,/1 +,/. !"'( +,39 +,02 +,-2 !"$& !"&)78&6:;67*8'* +,90 +,/4 +,// !"$$ !"$& +,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !"$& !"&)78&6<=>67*8'* +,2/ +,/4 +,// +,/- +,/0 +,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-378& +,-2 +,/- +,/3 +,/+ +,/+ +,-- +,/3 +,/0 +,-9 +,.2 +,32 +,0+ +,04 +,/3 +,047&6?@8"5 +,02 +,/3 +,/0 +,-9 +,-9 +,-. +,-4 +,/+ +,-/ +,.0 +,3- +,.+ +,0. +,/+ +,02ABC +,/+ +,/- +,/. +,-1 +,-4 +,-- +,/3 +,/. +,-9 +,.- +,32 +,.1 +,0/ +,/+ +,02DE +,22 +,/4 +,/- +,/3 +,/3 +,-1 +,// +,/9 +,/+ +,.9 +,32 +,03 +,-. +,/. +,01<&C7*5F +,-9 +,/0 +,/3 +,-1 +,-4 +,-0 +,/+ +,/. +,-9 +,./ +,3/ +,./ +,02 +,/+ +,02G<&67*8'* +,29 +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3G<& +,/0 +,// +,/. +,/. +,/3 +,-/ +,/3 +,/- +,-9 +,.9 +,32 +,.1 +,09 +,-1 +,09EC) +,9. +,/4 +,/- +,/0 +,/. +,-1 +,// +,/9 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-+E*767*8'* +,/1 +,/2 +,/- +,/0 +,/. +,-4 +,/0 +,/2 +,-1 +,.4 +,32 +,03 +,-3 +,/. +,-+E*7 +,.0 +,-1 +,-9 +,-/ +,-/ +,09 +,-0 +,-/ +,-3 +,.3 +,30 +,.3 +,.4 +,-1 +,0/#C8%C +,/. +,/9 +,/- +,/. +,/3 +,-4 +,/- +,/2 +,/+ +,.4 +,32 +,0. +,-3 +,/. +,01#C& +,/1 +,/9 +,/- +,/3 +,/+ +,-9 +,/0 +,// +,-1 +,.9 +,32 +,0. +,-3 +,/. +,01*5#C5@H +,20 +,/9 +,/- +,/. +,/3 +,-4 +,/0 +,// +,-1 +,.9 +,32 +,03 +,-+ +,-1 +,02*5#C5@I +,90 !"$% !"$* !"$$ !"$& !"$) +,/9 !"*! !"$' !"'( !"(+ !"'# !"&# !"$& +,-3JK)<67*8'* +,9/ +,/4 +,// +,/- !"$& +,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !"$& +,-3JK)< +,2. +,/4 +,// +,/0 +,/. +,-1 +,// +,/9 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-3&@AC5 +,22 +,/4 +,/- +,/0 +,/. +,/+ +,// +,/9 +,/3 +,.1 +,32 +,00 +,-0 +,/0 +,01&8A +,24 +,/4 +,// +,/0 +,/. +,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,00 +,-- +,/0 +,-+&>>%"#" +,/3 +,// +,/0 +,/+ +,/+ +,-/ +,/3 +,/- +,-4 +,.2 +,3/ +,.4 +,04 +,/3 +,04&>L67*8'* +,2- +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3&&68% +,// +,// +,/. +,/+ +,-1 +,-/ +,/3 +,/0 +,-9 +,.2 +,3/ +,.1 +,09 +,/3 +,04!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,/+ +,/. /,+3 +,-3 +,+. +,+3 +,.1 +,-. +,./ +,/0 +,31 2,04''567*8'* +,// +,/9 /,-2 +,-4 !"!' +,+- +,0/ +,-9 +,0+ +,/4 !")* #")+78&6:;67*8'* +,// +,/9 /,-9 +,-4 !"!' +,+0 +,0- +,-2 +,0+ +,/4 !")* 9,3178&6<=>67*8'* +,/- +,/9 /,-2 +,-9 !"!' +,+- +,00 +,-2 +,.1 +,/4 +,./ 9,+178& +,/+ +,/0 /,+4 +,-. +,+. +,+0 +,0+ +,-- +,.2 +,/. +,.3 2,-97&6?@8"5 +,-4 +,/3 -,14 +,09 +,+. +,+. +,.2 +,01 +,.0 +,/3 +,32 /,93ABC +,-1 +,/0 /,3. +,-/ +,+. +,+2 +,03 +,-/ +,.4 +,/0 +,39 /,1/DE +,/- +,/9 /,-0 +,-9 !"!' +,+. +,00 +,-2 +,.4 +,/4 +,.. 2,29<&C7*5F +,/+ +,/0 /,33 +,-. +,+. +,+. +,0+ +,-0 +,.2 +,/. +,31 2,30G<&67*8'* +,/- +,/9 /,-0 +,-9 !"!' +,+. +,0- +,-2 +,.1 +,/4 +,./ 9,30G<& +,/3 +,/0 /,3. +,-. +,+. +,+/ +,03 +,-- +,.9 +,/- +,.3 2,90EC) +,/- +,/4 /,/3 +,-4 !"!' +,+/ +,0- +,-9 +,0+ +,/4 +,.- 2,13E*767*8'* +,/. +,// /,03 +,-/ +,+. +,+. +,00 +,-/ +,.4 +,/2 +,.- 2,44E*7 +,-0 +,-4 -,9. +,02 +,+. +,+3 +,.9 +,-+ +,.- +,-9 +,32 /,01#C8%C +,/0 +,/2 /,0/ +,-9 +,+. +,+- +,0. +,-2 +,.4 +,/9 +,.0 2,90#C& +,/0 +,/2 /,0- +,-2 +,+. +,+0 +,0. +,-2 +,.4 +,/2 +,.. 2,2+*5#C5@H +,/0 +,// /,.- +,-2 +,+. +,3+ +,00 +,-2 +,.1 +,/2 +,.3 2,44*5#C5@I !"$* !"*! $"#( !"$) !"!' !"(# !"'* !"&% !"') !"*( !")* 9,34JK)<67*8'* +,// +,/4 /,/+ +,-4 !"!' +,+. +,0/ +,-9 +,0+ +,/1 !")* 9,.+JK)< +,/- +,/9 /,-- +,-9 !"!' +,+0 +,00 +,-2 +,.1 +,/4 +,.- 2,12&@AC5 +,/- +,/4 /,/0 +,-1 !"!' +,+2 +,0- +,-4 +,0+ +,/4 +,.0 2,4+&8A +,// +,/4 /,/+ +,-4 !"!' +,+9 +,00 +,-9 +,.1 +,/4 +,.0 2,99&>>%"#" +,/3 +,/- /,.3 +,-0 +,+. +,+. +,03 +,-- +,.9 +,/- +,.3 2,/3&>L67*8'* +,/- +,/9 /,-/ +,-4 !"!' +,+0 +,0- +,-/ +,.1 +,/4 +,./ 9,3/&&68% +,/+ +,/- /,34 +,-- +,+. +,+. +,03 +,-/ +,.9 +,/- +,.+ 2,0.
,-./0123145678(((((66(((((M@)JC7%(%&'8CN@A()*(OPQR(M@)JC7%MHRS(.+3+T(>#&%()K*('"%@#C5@(8@)JC7%(!IUVW("5A(OPQR$,((Q7*J@%(X*J(Y"##Y(!@5ZJ@([MR3+()@%)%@)$("5A(Y%&'Y(!%&'%@)(*X()<@(<&8"5#=("%%@%%@A(A")"$("J@(%<*K5,J"5E MR6O\] MR68O\] I"AF@J(X&## I"AF@J(#C)@ ]\W6U;^ US_I- US:I-Q@8`aQ Q@8`aQ(IUVW
P(#@N@J(IUVW P(#@N@J(J@7"## QbM(S"5E RVQUH(M RVQUH Q)"5X*JA
MVRVaS("Ac MVRVaS(<)@J MVRVaS(J"5E QV`PHHRV\(.,3
IUVW(30" OPQRRVS> ]S ]JA*7 WU\< IVKR6V IE"J%
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./01 ./02 ./34 ./35 ./35 ./36 ./0. ./07 ./33 ./76 ./24 ./67 ./37 ./35 !"#$+"-'89:;< ./02 ./0. ./34 ./34 ./3= ./37 ./35 ./01 ./33 ./76 ./24 ./62 ./31 ./35 !"#$+-&*>?*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0. ./07 ./33 ./76 ./24 ./62 ./31 ./35 !"#$+,#&-'9" ./63 ./34 ./30 ./33 ./36 ./3. ./30 ./35 ./31 ./7. ./2= ./73 ./6= ./3= ./64+&*@<-") ./1= ./33 ./34 ./31 ./31 ./64 ./36 ./30 ./3. ./1= ./23 ./13 ./62 ./33 ./63:A9 ./63 ./34 ./33 ./36 ./37 ./65 ./30 ./34 ./32 ./14 ./24 ./73 ./60 ./30 ./6=B&9+,); ./3. ./3= ./30 ./30 ./33 ./32 ./34 ./02 ./37 ./72 ./2= ./73 ./6= ./3= ./64CB&*+,-', ./33 ./0. ./34 ./34 ./34 ./37 ./35 ./01 ./36 ./77 ./24 ./6. ./32 ./34 ./3.CB& ./02 ./0. ./3= ./3= ./3= ./37 ./35 ./01 ./36 ./77 ./24 ./75 ./3. ./34 ./65,)#9)"D ./36 ./0. ./3= ./3= ./3= ./37 ./34 ./02 ./36 ./71 ./2= ./75 ./64 ./33 ./60,)#9)<E !"%! !"&' !"#( !"&) !"&$ !"#& !"&) !"&# !"#% !"'& !")! !"*% !"## ./35 ./3.&<:9) ./05 ./02 ./3= ./34 ./34 ./36 ./0. ./07 ./33 ./77 ./24 ./61 ./31 ./35 !"#$&F+ ./32 ./35 ./30 ./33 ./33 ./32 ./34 ./0. ./37 ./72 ./2= ./7= ./64 ./34 ./65&FG*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0. ./07 ./33 ./76 ./24 ./61 ./37 !"&! !"#$!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./35 ./02 3/51 ./31 !"!' ./.6 ./74 ./3. ./76 ./01 ./71 =/56+"-'89:;< ./34 ./0. 3/43 ./32 !"!' ./.= ./7= ./3. ./77 ./02 ./72 =/==+-&*>?*+,-', ./35 ./02 3/5. ./31 !"!' ./.6 ./7= ./3. ./76 ./01 ./72 =/4=+,#&-'9" ./33 ./3= 3/37 ./6= ./.1 ./.6 ./76 ./60 ./7. ./3= ./1= =/25+&*@<-") ./36 ./30 3/60 ./61 ./.1 ./.7 ./14 ./62 ./1= ./30 ./11 0/03:A9 ./36 ./34 3/07 ./3. ./.1 ./.4 ./76 ./6= ./72 ./34 ./17 0/0.B&9+,); ./3= ./35 3/=7 ./65 !"!' ./.3 ./76 ./64 ./72 ./35 ./10 =/7=CB&*+,-', ./34 ./0. 3/47 ./32 !"!' ./.2 ./7= ./3. ./77 ./02 ./7. =/42CB& ./34 ./0. 3/47 ./32 !"!' ./.4 ./70 ./65 ./77 ./02 ./15 =/=1,)#9)"D ./3= ./35 3/=. ./31 !"!' ./22 ./70 ./3. ./71 ./02 ./14 =/==,)#9)<E !"&$ !"&' &"!' !"#& !"!' !")# !"*! !"#' !"'& !"&* !"'' +"'(&<:9) ./35 ./01 3/5= ./36 !"!' ./21 ./74 ./32 ./76 ./01 ./7. =/04&F+ ./30 ./0. 3/=3 ./3. !"!' ./.7 ./73 ./64 ./71 ./0. ./10 =/13&FG*+,-', ./35 ./01 3/5= ./37 !"!' ./.6 ./74 ./3. ./76 ./01 ./72 =/41
,-./01234/560127((((**(((((H<I89+%(%&'-9J<:(I,(KLMN(H<I89+%HDNO(1.2.P(F#&%(IQ,('"%<#9)<(-<I89+%(!ERST("):(KLMN$/((M+,8<%(U,8(V"##V(!<)W8<(XHN2.(I<%I%<I$("):(V%&'V(!%&'%<I(,U(IB<(B&-")#Y("%%<%%<:(:"I"$("8<(%B,Q)/8")Z HN*K[\ HN*-K[\ E":;<8(U&## E":;<8(#9I< \[T*R?] RO^E6 RO>E6M<-_`M M<-_`M(ERST
L(#<J<8(ERST L(#<J<8(8<+"## MaH(O")Z NSMRD(H NSMRD MI")U,8:
HSNS`O(":b HSNS`O(BI<8 HSNS`O(8")Z MS_LDDNS[(1/2
ERST(27" KLMNNSOF \O \8:,+ TR[B ESQN*S EZ"8%
48
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. /012 /034 /035 /03/ /056 /076 !"#$ /076 /052 /038 /055)&+'.9": /011 /036 /035 /052 /051 /071 /057 /071 /056 /03/ /057)&+;<);. /01/ /036 /037 /051 /057 /075 /05/ /077 /05= /054 /058)&+><*"? /03/ /033 /03= /051 /057 /078 /072 /075 /05= /051 /072@)&+).*'. !"%$ !"$& !"$$ !"$' !"#( !")& !"#$ !")( !"$' !"$* !"#+@)& /074 /035 /038 /051 /057 /07= /072 /077 /058 /052 /057<&:.;:"?% /035 /035 /038 /057 /058 /0=2 /076 /078 /074 /054 /05=A.)+).*'. /011 /036 /035 /052 /051 /076 /057 /076 /056 /038 /053A.) /077 /03= /052 /055 /058 /0=2 /071 /077 /072 /052 /05=.?#B?<C /055 /033 /038 /053 /058 /078 /074 /077 /058 /054 /05=.?#B?<D /06/ /034 /035 /052 /051 /076 /055 /076 /054 /038 /053E)+;:"?% /01= /033 /038 /055 /058 /078 /076 /0=2 /071 /052 /05=E.;%@"* /055 /033 /038 /053 /05= /078 /074 /075 /05= /052 /05=:F;G+).*'. /06/ /036 /037 /03/ /056 /074 /055 /076 /052 /038 /053%H& /053 /033 /03= /055 /058 /078 /074 /077 /058 /052 /057&<@B? /01= /036 /037 /054 /053 /071 /05= /076 /056 /03/ /055&EI+).*'. /014 /034 /033 /03/ /056 /074 !"#$ /076 /052 /038 /053!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. !"#+ /056 504/ !"$$ +/0/= /0/3 /037 /086 301=)&+'.9": /053 /051 506/ /037 ,!"!' /0/1 /038 /081 3056)&+;<);. /057 /053 501= /037 ,!"!' /0/2 /03/ /087 30=3)&+><*"? /058 /05= 5071 /03/ ,!"!' /0/3 /056 /08= 5021@)&+).*'. /056 !"#( #"&! !"$$ ,!"!' !"'# !"$# !"'& $"&+@)& /05/ /058 50=3 /056 ,!"!' /0/3 /051 /087 5023<&:.;:"?% /05/ /05= 505= /038 ,!"!' /0/5 /051 /08/ 5055A.)+).*'. /053 /051 506/ /037 ,!"!' /0/= /038 /086 3034A.) /074 /05/ 50=5 /057 ,!"!' /0/7 /055 /08= 5037.?#B?<C /058 /057 5054 /038 ,!"!' /0/1 /056 /08= 5013.?#B?<D !"#+ !"#( 5046 /035 ,!"!' /08/ /037 /086 3031E)+;:"?% /058 /057 503= /03/ ,!"!' /0/3 /056 /08/ 5031E.;%@"* /05/ /05= 5074 /052 ,!"!' /0/7 /051 /08= 5065:F;G+).*'. !"#+ /056 5048 !"$$ ,!"!' /0/5 /038 /084 304=%H& /058 /057 5055 /03/ ,!"!' /0/5 /056 /088 5014&<@B? /053 /051 5067 /035 ,!"!' /088 /03= /081 3057&EI+).*'. !"#+ !"#( 5045 /035 ,!"!' /0/1 /037 /084 3044
:"?A
JKL;"?H.:@ MNKEO(#<P<:(:<)"## LQR(K"?A MNLSC DNFM+N DA":%
RNMNTK("@U RNMNTK(:"?ARNMNTK(G;<: LNVOC L<*VTL-./0123,45673888((++((((((R<;:B)%(%&'*B;<@(;.(WOLM(R<;:B)%RCMK(=/8/X(E#&%(;F.('"%<#B?<(*<;:B)%(!DSNY("?@(WOLM$0((L).:<%(H.:(Z"##Z(!<?[:<(\RM8/(;<%;%<;$("?@(Z%&'Z(!%&'%<;(.H(;G<(G&*"?#]("%%<%%<@(@";"$(":<(%G.F?0 SK^D5 SK,D5
WOLMDSNY(I87"
RM+W_J RM+*W_J D"@`<:(H&## D"@`<:(#B;< CMN_(=08
J:@.) YS_GO(#<P<:(DSNY
L<*VTL(DSNY J_Y+S-a
MNLSC(R
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0123 0145 0146 !"#$ 0144 0164 0167 0148 0140)*&9:;9)<*'< 0156 0148 !"## !"#$ !"#% !"&$ !"#' !"%! !"#()&9=/*"> 01?2 014@ 0143 0165 0162 0138 0163 0146 0163-A, 0160 0146 014@ 0140 0167 0137 0166 0144 0162/& 013? 0143 014@ 0168 0165 0138 0166 0142 0162./>/B" 0134 0144 014@ 0167 0168 0135 016? 0146 0166CD& 0143 0145 0143 0146 0143 0166 0167 0148 0168E<)9)<*'< 0128 !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(E<) 01?2 0143 014@ 0167 0168 0135 0163 0144 0162#,*%, 0122 0148 0146 0144 0146 0164 0167 0147 0140#,&* 0123 0148 0146 0142 0144 0162 !"#' !"%! 014@>+) 012? 0145 0146 0146 0143 0164 0167 0148 0167<>#,>/F 0144 0145 0143 0146 0143 0166 0168 0148 0167<>#,>/G 0128 0148 0146 !"#$ 0144 0162 !"#' !"%! 014@+"#, 0122 0148 0146 0142 0144 0162 0140 !"%! 014@+HID9)<*'< !"$# !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(+HID 0123 0148 0146 0144 0146 0164 0140 0147 014@&/-,> 0150 0148 0146 0142 0144 0164 0140 !"%! 014@&JB9)<*'< 0122 !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0144 0145 4143 014? 9010? 0102 0148 01?8 5165)*&9:;9)<*'< !"#$ 0147 4156 0146 *!"!' 0103 !"%! !"+! 514?)&9=/*"> 0140 014? 4103 0160 90103 0103 014? 01@2 4163-A, 014@ 0146 41?7 0162 9010? 0102 0146 01?0 21@2/& 0140 0146 41?@ 0162 9010? 0103 0143 01@7 4176./>/B" 0140 014? 41@0 0168 90103 0104 014? 01@8 2106CD& 0144 0145 414@ 0140 9010? 0104 0145 01?6 2124E<)9)<*'< !"#$ 0147 4152 0146 *!"!' 0100 !"%! 01?7 5143E<) 0167 014@ 4102 0164 9010? 010? 014? 01?0 21@6#,*%, 0142 0148 4125 0146 9010? 0102 0148 01?5 51?0#,&* !"#$ 0147 4152 0146 *!"!' 0105 0147 01?7 5138>+) 0142 0148 412@ 014? *!"!' 0102 0148 01?5 51??<>#,>/F 0144 0145 4148 014? 9010? 0105 0148 01?4 2177<>#,>/G !"#$ !"%! #",' 0146 9010? !"'& !"%! 01?8 513?+"#, !"#$ 0147 4153 0146 *!"!' 0102 0147 01?8 513?+HID9)<*'< !"#$ 0147 4157 !"## *!"!' 0106 !"%! !"+! 512?+HID 0142 0148 4127 0143 *!"!' 0102 0147 01?8 51?8&/-,> 0142 0148 4150 0146 *!"!' 0107 0147 01?8 51?6&JB9)<*'< !"#$ 0147 4158 !"## *!"!' 0104 !"%! !"+! $"%#
-./0123*456.738(((99(((((K/I+,)%(%&'*,L/-(I<(MNOP(K/I+,)%KFPQ(?0@0R(J#&%(IH<('"%/#,>/(*/I+,)%(!GSTU(">-(MNOP$1((O)<+/%(V<+(W"##W(!/>X+/(YKP@0(I/%I%/I$(">-(W%&'W(!%&'%/I(<V(ID/(D&*">#Z("%%/%%/-(-"I"$("+/(%D<H>1+">E KP9M[\ KP9*M[\ G"-./+(V&## G"-./+(#,I/ \[U9S;] SQ^G6 SQ:G6O/*_`O O/*_`O(GSTU
N(#/L/+(GSTU N(#/L/+(+/)"## OaK(Q">E PTOSF(K PTOSF OI">V<+-
KTPT`Q("-b KTPT`Q(DI/+ KTPT`Q(+">E OT_NFFPT[(?1@
GSTU(@3" MNOPPTQJ \Q \+-<) US[D GTHP9T GE"+%
49
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. /012 /011 /034 /034 /035 /064 /074 !"#$ !"%&)&+89*": /07; /035 !"&' /037 /03< /06/ /077 /036 /06<=>? /057 /016 /035 /033 /033 /063 /073 /031 /066@A /015 /011 /034 /031 /031 /065 /075 /031 /063BC& /035 /016 /034 /035 /031 /063 /071 /036 /06<A?D /05/ /011 /034 /034 /035 /065 /075 /031 /066A.)+).*'. /012 /011 /034 /034 /035 /064 /075 /035 !"%&A.) /06/ /01/ /031 /037 /037 /06< /077 /033 /067#?*%? /01< /013 /035 /031 /033 /061 /071 /031 /066#?& /011 /013 /034 /035 /031 /061 /071 /033 /066.:#?:9E /012 /013 /035 /035 /031 /061 /071 /033 /067.:#?:9F !"$! !"&( /03; !"#) !"#$ !"%* !"') !"#$ !"%&GHDC+).*'. /051 /011 /034 /034 /035 /064 /074 /035 !"%&GHDC /01< /013 /034 /035 /031 /061 /075 /031 /063%I& /063 /01< /031 /033 /036 /072 /07< /062 /07;&9=?: /057 /011 /034 /035 /031 /065 /075 /035 /063&JJ%"#" /034 /013 /034 /031 /033 /061 /071 /031 /063&JK+).*'. /011 /011 /034 /034 !"#$ /064 /074 /031 /063!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+C9"L9#=+).*'. /034 /03; 30;5 /031 /0/6 /0/1 /016 /0<; 102;)&+89*": /03< /037 3067 /066 /0/6 /0/6 /034 /0<7 10/6=>? /031 /034 3042 /035 !"!# !"+' /01/ /0<3 107;@A /035 /034 3045 /031 /0/6 /0/6 /017 /0<5 10;1BC& /033 /031 3017 /037 /0/6 /0/4 /01/ /0<1 1042A?D /034 /03; 30;6 /031 /0/6 /0/5 /016 /0<4 102;A.)+).*'. /035 /034 3044 /031 /0/6 /0// /016 /0<4 102;A.) /03< /037 303/ /062 /0/6 /0/7 /035 /0<3 10<2#?*%? /031 /035 3055 /036 /0/6 /0/1 /01< /0<5 104/#?& /035 /03; 3045 /031 /0/6 /0/3 /017 /0<5 10;1.:#?:9E /035 /03; 30;7 /035 !"!# /0/2 /017 /0<1 1051.:#?:9F !"#* !"&! &"!& !"#) !"!# /07/ !"&& !"+* ("+)GHDC+).*'. /035 /03; 3044 /031 /0/6 /0/6 /016 /0<4 50/5GHDC /031 /034 304< /033 /0/6 /0/1 /017 /0<5 10;5%I& /06; /062 6023 /06; /0/6 /0/6 /031 /0<< 30;7&9=?: /034 /03; 30;1 /035 /0/6 /0/; /016 /0<4 1023&JJ%"#" /031 /035 3051 /036 /0/6 /0/6 /01< /0<5 1043&JK+).*'. /035 /034 3041 /033 /0/6 /0/6 /016 /0<4 50</
,-./012345678-(((((++(((((M9DG?)%(%&'*?N9=(D.(OPQR(M9DG?)%MERS(7/</T(J#&%(DH.('"%9#?:9(*9DG?)%(!FUVW(":=(OPQR$0((Q).G9%(I.G(X"##X(!9:YG9(ZMR</(D9%D%9D$(":=(X%&'X(!%&'%9D(.I(DC9(C&*":#[("%%9%%9=(="D"$("G9(%C.H:0G":A MR+O\] MR+*O\] F"=^9G(I&## F"=^9G(#?D9 ]\W+U-_ US`F3 US,F3Q9*abQ Q9*abQ(FUVW
P(#9N9G(FUVW P(#9N9G(G9)"## QcM(S":A RVQUE(M RVQUE QD":I.G=
MVRVbS("=d MVRVbS(CD9G MVRVbS(G":A QVaPEERV\(70<
FUVW(<6" OPQRRVSJ ]S ]G=.) WU\C FVHR+V FA"G%
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0123 0124 0125 0126 0122 0132 0172 0126 0120)*&89:8);*'; 015< 0150 !"#$ 0124 012< 013< !"%& 012< 0127)&8=/*"> 017? 0122 !"#$ 012@ 0120 013@ 0177 0127 013?-)& 015@ 0150 !"#$ 012< 0126 0136 0172 012< 012@-A, 0122 0126 0123 0123 0127 0137 017? 0123 0136BC& 0122 0124 0125 0126 0122 0132 0172 0125 013<D;)8);*'; 0157 0150 !"#$ 012< 0126 0136 !"%& 012< 012@D;) 01@4 0123 012? 012@ 0134 01?< 017@ 012? 0135;>#,>/E 0154 0150 !"#$ 012< 0126 013< 0172 012< 012@;>#,>/F !"$' !"&' !"#$ !"&! !"#( !")( !"%& !"&! !"#*+GHC8);*'; 0124 0124 0125 0124 012< 013< !"%& 012< 012@%I& 0130 012< 0122 0122 0123 0137 0173 012? 0132&/-,> 015@ 0150 0125 012< 0126 0136 0172 012< 012@&J'8);*'; 0150 0150 !"#$ 0124 012< 013< !"%& 012< 012@&JK8>>#* 0123 0124 0125 0125 0122 0132 0173 0126 0134&JK 0122 0124 0125 0126 0125 0135 0172 0126 0120!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0126 0124 2162 0132 !"!' 0100 0150 0174 6120)*&89:8);*'; 0124 015@ 2140 013< !"!' 0100 0157 !"*% 6145)&8=/*"> 0123 0125 213@ 0130 0100 0100 0125 017@ 5123-)& 0124 015@ 2140 0136 !"!' 0100 015@ 01?0 6167-A, 0123 0126 2122 0137 0100 0100 0126 0173 51<?BC& 0126 0124 2167 0132 !"!' 0100 0150 017< 612<D;)8);*'; 012< 0150 21<3 0135 !"!' 0100 015@ 01?0 6164D;) 012@ 012? 21@4 01?5 0100 0100 012? 017@ 51?6;>#,>/E 0124 0157 2146 013< !"!' 0100 0157 01?0 6152;>#,>/F !"&! !"&* &"'% !")( !"!' 0100 0153 !"*% 61<6+GHC8);*'; 0124 015@ 21<4 0136 !"!' 0100 0150 01?@ $"($%I& 0123 0122 21?2 0137 0100 0100 0125 0173 61@<&/-,> 012< 015@ 21<5 0136 !"!' !"!' 015@ 01?0 6155&J'8);*'; 012< 0150 21<< 0136 !"!' 0100 015@ 01?@ 614?&JK8>>#* 0126 0124 216@ 0133 !"!' 0100 0124 017< 61?4&JK 012< 0150 21<0 0132 !"!' 0100 0150 0174 613<
+,-./012345,/016((((88(((((L/H+,)%(%&'*,M/-(H;(NOPQ(L/H+,)%LEQR(70@0S(J#&%(HG;('"%/#,>/(*/H+,)%(!FTUV(">-(NOPQ$1((P);+/%(I;+(W"##W(!/>X+/(YLQ@0(H/%H%/H$(">-(W%&'W(!%&'%/H(;I(HC/(C&*">#Z("%%/%%/-(-"H"$("+/(%C;G>1+">D LQ8N[\ LQ8*N[\ F"-./+(I&## F"-./+(#,H/ \[V8T:] TR^F3 TR9F3P/*_`P P/*_`P(FTUV
O(#/M/+(FTUV O(#/M/+(+/)"## PaL(R">D QUPTE(L QUPTE PH">I;+-
LUQU`R("-b LUQU`R(CH/+ LUQU`R(+">D PU_OEEQU[(71@
FTUV(@?" NOPQQURJ \R \+-;) VT[C FUGQ8U FD"+%
50
R
E
F
A
A
L
T
O
C
M
U
C
U
-B
O
JA
R
C
U
-Z
E
M
A
N
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
B
B
N
-C
C
M
U
-H
E
A
-C
JH
U
-C
R
W
T
H
-C
U
P
V
-C
REF ? .03? .02? .03? .01? .03? .02? .05? .02? .06? .03? .05? .03?
AALTO .93? ? .54? .54? .23? .36 .58? .56? .65? .69? .64? .67? .62?
CMU .94? .30? ? .47 .14? .22? .52? .41 .50? .57? .45? .44 .38
CU-BOJAR .94? .26? .38 ? .10? .22? .61? .47? .46 .55? .42 .49? .44
CU-ZEMAN .98? .58? .73? .77? ? .55? .79? .71? .84? .80? .77? .79? .75?
ONLINEA .94? .41 .61? .57? .23? ? .68? .63? .71? .71? .63? .54? .61?
ONLINEB .93? .30? .31? .26? .10? .17? ? .32? .35 .31 .22? .29? .38
UEDIN .91? .27? .35 .34? .11? .18? .47? ? .54? .50? .35 .29 .35
BBN-C .95? .21? .22? .36 .06? .17? .38 .26? ? .32 .24? .31? .26?
CMU-HEA-C .90? .17? .19? .23? .09? .18? .32 .27? .34 ? .31? .31? .30?
JHU-C .93? .19? .30? .35 .09? .24? .50? .34 .47? .45? ? .41? .36
RWTH-C .91? .16? .35 .29? .12? .27? .41? .37 .42? .42? .23? ? .24?
UPV-C .94? .24? .40 .36 .09? .28? .39 .32 .46? .47? .33 .36? ?
> others .93 .26 .37 .38 .11 .24 .47 .40 .49 .49 .38 .41 .40
>= others .97 .42 .56 .55 .25 .39 .67 .62 .70 .70 .61 .65 .62
Table 21: Sentence-level ranking for the WMT10 Czech-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
51
R
E
F
A
A
L
T
O
C
M
U
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
H
U
IC
O
N
G
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
U
E
D
IN
U
M
D
U
P
P
S
A
L
A
U
U
-M
S
B
B
N
-C
C
M
U
-H
E
A
-C
C
M
U
-H
Y
P
O
-C
JH
U
-C
K
O
C
-C
R
W
T
H
-C
U
P
V
-C
REF ? .00? .02? .00? .07? .04? .03? .00? .06? .04? .00? .02? .07? .07? .07? .02? .09? .03? .03? .10? .04? .04? .03? .02? .07? .06?
AALTO 1.00? ? .43 .39 .48 .60? .38 .41 .74? .18? .42 .57? .50? .63? .55? .68? .79? .42 .33 .71? .61? .66? .54 .51? .66? .56?
CMU .95? .34 ? .19? .45 .52? .38 .50 .63? .17? .51? .55? .56? .66? .55? .60? .56? .30 .40 .62? .64? .49? .58? .46 .64? .46?
CU-ZEMAN 1.00? .44 .64? ? .43 .72? .31 .45? .69? .36 .55 .62? .75? .75? .78? .75? .75? .48? .56? .79? .82? .72? .68? .63? .67? .84?
DFKI .92? .29 .33 .35 ? .37 .40 .34 .59 .08? .42 .50 .49 .64? .35 .44 .44 .50 .41 .70? .61? .57 .46 .47 .62? .44
FBK .93? .26? .23? .17? .49 ? .12? .30 .52? .08? .20? .45? .41 .62? .44 .44 .48? .18? .25? .53? .47 .38 .38 .22? .41 .51?
HUICONG .92? .34 .39 .37 .38 .71? ? .53? .67? .18? .51? .47 .60? .65? .49? .55? .78? .35 .41 .56? .77? .74? .58? .41 .65? .57?
JHU .92? .35 .30 .17? .52 .45 .25? ? .58? .16? .43 .38 .57? .60? .54? .60? .70? .29 .25 .65? .75? .56? .62? .49? .66? .48?
KIT .90? .14? .16? .14? .35 .28? .19? .16? ? .03? .29? .20? .35 .53? .21? .24? .30 .20? .22? .44 .29 .38 .35 .24 .40 .24?
KOC .95? .66? .71? .51 .75? .80? .58? .68? .93? ? .75? .87? .72? .74? .74? .81? .81? .78? .66? .89? .85? .80? .80? .72? .91? .73?
LIMSI .99? .26 .24? .32 .45 .61? .25? .38 .50? .10? ? .50? .55? .69? .52? .57? .57? .29? .22? .60? .52? .42 .47? .37 .60? .56?
LIU .87? .17? .20? .14? .34 .22? .31 .38 .66? .04? .27? ? .51? .53? .52? .53? .51 .20? .33 .64? .59? .48? .48 .51 .37 .53?
ONLINEA .90? .25? .29? .18? .34 .43 .23? .28? .49 .08? .32? .30? ? .44 .38 .40 .42 .32? .35? .39 .47 .51 .27? .35 .43 .40
ONLINEB .76? .22? .24? .14? .27? .27? .25? .25? .32? .22? .21? .28? .32 ? .27? .21? .30? .23? .15? .41 .31 .40 .23? .16? .42 .29
RWTH .89? .22? .23? .13? .49 .35 .29? .21? .62? .15? .32? .29? .46 .57? ? .39 .49 .25 .38 .41 .27 .34 .36 .27 .48? .22?
UEDIN .91? .15? .20? .12? .49 .35 .24? .22? .49? .04? .22? .30? .46 .62? .43 ? .39 .11? .15? .45 .33 .40 .45 .33 .34 .33
UMD .91? .12? .23? .06? .35 .29? .11? .16? .47 .14? .23? .35 .40 .55? .36 .47 ? .16? .17? .44 .29? .27 .37 .26 .27 .24?
UPPSALA .94? .30 .41 .23? .35 .53? .26 .37 .66? .03? .54? .71? .57? .65? .45 .72? .67? ? .25 .59? .69? .49? .63? .33 .60? .64?
UU-MS .83? .28 .42 .24? .41 .49? .28 .42 .68? .10? .55? .48 .55? .63? .49 .56? .60? .32 ? .52? .58? .61? .64? .46? .64? .50?
BBN-C .90? .15? .16? .10? .22? .17? .22? .18? .41 .06? .16? .21? .35 .45 .30 .26 .34 .13? .20? ? .42? .14? .27 .11? .25 .21?
CMU-HEA-C .83? .20? .18? .07? .29? .32 .06? .10? .49 .05? .26? .21? .41 .33 .37 .43 .58? .10? .14? .18? ? .33 .32 .11? .34 .24?
CMU-HYPO-C .96? .24? .20? .07? .37 .33 .12? .21? .40 .10? .41 .26? .40 .54 .25 .37 .44 .13? .17? .49? .31 ? .34 .23? .51? .45
JHU-C .97? .33 .22? .18? .31 .30 .27? .18? .33 .12? .19? .33 .59? .60? .39 .32 .30 .19? .20? .44 .29 .34 ? .21? .36 .23
KOC-C .93? .11? .31 .17? .41 .50? .25 .27? .44 .11? .42 .36 .47 .68? .43 .41 .40 .33 .18? .59? .57? .46? .47? ? .52? .43
RWTH-C .87? .20? .10? .21? .25? .27 .15? .23? .24 .02? .20? .30 .34 .47 .27? .34 .36 .14? .20? .33 .26 .21? .24 .20? ? .17?
UPV-C .93? .14? .20? .10? .42 .29? .25? .25? .57? .20? .22? .33? .39 .45 .47? .40 .50? .24? .28? .44? .42? .27 .34 .28 .56? ?
> others .92 .25 .28 .18 .39 .41 .25 .30 .52 .12 .34 .39 .47 .57 .42 .46 .51 .27 .28 .52 .49 .45 .44 .34 .50 .42
>= others .96 .46 .49 .35 .53 .62 .45 .51 .71 .24 .54 .58 .63 .72 .63 .66 .70 .50 .51 .75 .73 .68 .67 .59 .74 .64
Table 22: Sentence-level ranking for the WMT10 German-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
R
E
F
C
A
M
B
R
ID
G
E
C
O
L
U
M
B
IA
C
U
-Z
E
M
A
N
D
F
K
I
H
U
IC
O
N
G
JH
U
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
U
P
C
B
B
N
-C
C
M
U
-H
E
A
-C
JH
U
-C
U
P
V
-C
REF ? .05? .01? .02? .03? .03? .01? .02? .04? .03? .04? .03? .07? .05? .04?
CAMBRIDGE .90? ? .24? .11? .35? .26? .43 .35 .50? .45? .33? .40 .46 .28? .41
COLUMBIA .97? .61? ? .25? .47 .44 .61? .53? .62? .59? .48? .59? .57? .45? .57?
CU-ZEMAN .92? .73? .59? ? .62? .66? .71? .65? .75? .79? .58? .75? .78? .71? .72?
DFKI .95? .50? .41 .21? ? .46 .56? .52? .65? .62? .47 .52? .56? .52? .60?
HUICONG .93? .57? .34 .21? .36 ? .47? .43 .67? .58? .40 .51? .62? .46? .52?
JHU .94? .39 .22? .16? .30? .32? ? .41 .52? .47? .37 .41 .33? .28 .35
ONLINEA .92? .45 .35? .24? .34? .41 .41 ? .60? .58? .38 .55? .46 .36 .57?
ONLINEB .87? .34? .24? .15? .21? .19? .33? .25? ? .34? .26? .34? .37? .24? .40
UEDIN .94? .33? .26? .12? .24? .22? .25? .25? .50? ? .25? .28? .32? .25? .26
UPC .89? .45? .36? .23? .39 .37 .42 .48 .62? .57? ? .54? .51? .50? .53?
BBN-C .91? .33 .25? .11? .32? .30? .34 .31? .51? .41? .30? ? .36 .26? .31
CMU-HEA-C .89? .37 .20? .10? .29? .23? .23? .35 .50? .44? .31? .34 ? .23? .31
JHU-C .89? .39? .31? .17? .37? .33? .38 .42 .63? .47? .31? .42? .42? ? .37?
UPV-C .91? .35 .30? .16? .29? .26? .32 .28? .44 .35 .27? .27 .30 .24? ?
> others .92 .42 .29 .16 .33 .32 .39 .37 .54 .48 .34 .42 .44 .35 .43
>= others .97 .62 .45 .29 .46 .50 .61 .52 .68 .68 .51 .64 .65 .58 .66
Table 23: Sentence-level ranking for the WMT10 Spanish-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
52
R
E
F
C
A
M
B
R
ID
G
E
C
M
U
-S
T
A
T
X
F
E
R
C
U
-Z
E
M
A
N
D
F
K
I
G
E
N
E
V
A
H
U
IC
O
N
G
JH
U
L
IG
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
B
B
N
-C
C
M
U
-H
E
A
-C
C
M
U
-H
Y
P
O
-C
D
C
U
-C
JH
U
-C
L
IU
M
-C
R
W
T
H
-C
U
P
V
-C
REF ? .02? .00? .00? .00? .00? .05? .02? .00? .00? .00? .02? .06? .02? .04? .02? .04? .03? .02? .05? .05? .04? .05? .06? .02?
CAMBRIDGE .82? ? .42 .16? .12? .35 .31 .45 .21? .47 .29 .38 .28? .54 .43 .33 .38 .28 .39 .45? .24 .25 .34 .54? .37
CMU-STATXFER .91? .50 ? .17? .41 .17? .28 .44 .36 .48? .56? .57? .47 .56? .70? .49 .50 .47 .61? .68? .55? .50 .42 .52? .51?
CU-ZEMAN 1.00? .74? .71? ? .74? .46 .67? .73? .73? .74? .75? .76? .75? .89? .78? .66? .83? .74? .87? .73? .80? .83? .77? .95? .82?
DFKI 1.00? .77? .48 .17? ? .27? .49 .52 .48 .64? .69? .67? .47 .62? .53 .47 .64? .60? .73? .72? .79? .58? .66? .73? .74?
GENEVA .98? .58 .70? .44 .59? ? .55? .67? .70? .70? .77? .73? .63? .81? .81? .69? .77? .73? .62? .66? .75? .60? .73? .88? .67?
HUICONG .89? .53 .34 .13? .34 .30? ? .41 .36 .43 .70? .56? .57 .59? .56? .43 .55? .45 .51? .64? .48 .49 .49 .53? .57?
JHU .88? .36 .38 .11? .34 .25? .35 ? .33? .46 .49? .48 .40 .50 .40 .34 .36 .39 .33 .59? .54? .41 .42 .40 .41
LIG .98? .65? .34 .18? .44 .26? .39 .56? ? .60? .55? .51? .45 .54? .53 .39 .38 .52? .54? .53? .51? .53? .55 .51 .58?
LIMSI .98? .40 .24? .23? .23? .15? .29 .38 .25? ? .28 .38 .27? .64? .35 .30 .41 .27 .33 .49 .45 .37 .28 .45 .39
LIUM .90? .40 .19? .12? .30? .11? .11? .26? .15? .36 ? .36 .25? .37 .39 .26 .29 .24 .34 .49? .34 .33 .34 .31 .38
NRC .93? .31 .06? .15? .29? .23? .20? .32 .16? .38 .36 ? .23? .53 .36 .24? .31 .44 .37 .47? .45? .29 .39 .38 .42
ONLINEA .92? .60? .47 .15? .44 .22? .32 .46 .34 .57? .52? .60? ? .52? .34 .44 .57? .56 .51 .51 .64? .46 .51 .41 .60
ONLINEB .85? .35 .32? .09? .33? .10? .29? .31 .25? .17? .40 .34 .24? ? .38 .32? .28 .39 .30 .42 .37 .41 .35 .32 .22?
RALI .90? .31 .19? .10? .38 .10? .17? .47 .35 .38 .33 .38 .48 .48 ? .29? .31 .29 .38 .40 .38 .34 .31 .57? .21?
RWTH .93? .43 .33 .12? .47 .26? .39 .40 .47 .35 .45 .49? .44 .53? .54? ? .44? .42 .48 .51? .54? .48? .49 .50? .26
UEDIN .92? .42 .32 .10? .22? .10? .28? .30 .42 .30 .55 .36 .23? .43 .33 .20? ? .41 .24 .52? .46 .25 .22 .27 .37
BBN-C .92? .49 .33 .24? .28? .18? .40 .39 .28? .45 .27 .27 .36 .39 .35 .35 .31 ? .26 .45? .43 .26 .58? .36 .28
CMU-HEA-C .90? .41 .21? .06? .23? .29? .28? .27 .22? .39 .40 .22 .39 .43 .29 .30 .40 .28 ? .43 .28 .15? .25 .26 .16
CMU-HYPO-C .84? .18? .20? .14? .20? .22? .21? .19? .16? .31 .22? .21? .36 .38 .34 .27? .22? .16? .24 ? .36 .23 .10? .33 .24
DCU-C .92? .27 .24? .12? .17? .23? .30 .29? .24? .32 .43 .22? .28? .41 .23 .27? .28 .22 .23 .25 ? .23 .23 .24 .17
JHU-C .88? .47 .26 .10? .33? .24? .36 .34 .24? .41 .39 .40 .42 .39 .34 .25? .42 .28 .37? .38 .39 ? .37 .32 .38?
LIUM-C .90? .48 .42 .13? .25? .20? .33 .50 .30 .44 .37 .34 .37 .52 .43 .34 .33 .22? .34 .56? .33 .43 ? .49? .44
RWTH-C .89? .22? .19? .03? .23? .12? .19? .23 .27 .30 .36 .19 .47 .54 .26? .16? .27 .19 .26 .28 .16 .22 .16? ? .22
UPV-C .89? .27 .15? .10? .16? .29? .30? .31 .25? .36 .42 .24 .32 .64? .46? .34 .27 .44 .33 .44 .23 .17? .31 .24 ?
> others .91 .43 .32 .14 .31 .21 .31 .39 .31 .42 .44 .40 .38 .52 .43 .33 .40 .37 .40 .49 .43 .38 .4 .44 .39
>= others .97 .64 .51 .24 .40 .31 .50 .59 .50 .63 .68 .65 .51 .68 .65 .55 .66 .63 .69 .75 .71 .64 .62 .74 .67
Table 24: Sentence-level ranking for the WMT10 French-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
53
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133?137,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joshua 2.0: A Toolkit for Parsing-Based Machine Translation
with Syntax, Semirings, Discriminative Training and Other Goodies
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,
Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,? Wren N.G. Thornton,
Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe the progress we have made in
the past year on Joshua (Li et al, 2009a),
an open source toolkit for parsing based
machine translation. The new functional-
ity includes: support for translation gram-
mars with a rich set of syntactic nonter-
minals, the ability for external modules to
posit constraints on how spans in the in-
put sentence should be translated, lattice
parsing for dealing with input uncertainty,
a semiring framework that provides a uni-
fied way of doing various dynamic pro-
gramming calculations, variational decod-
ing for approximating the intractable MAP
decoding, hypergraph-based discrimina-
tive training for better feature engineering,
a parallelized MERT module, document-
level and tail-based MERT, visualization
of the derivation trees, and a cleaner
pipeline for MT experiments.
1 Introduction
Joshua is an open-source toolkit for parsing-based
machine translation that is written in Java. The
initial release of Joshua (Li et al, 2009a) was a
re-implementation of the Hiero system (Chiang,
2007) and all its associated algorithms, includ-
ing: chart parsing, n-gram language model inte-
gration, beam and cube pruning, and k-best ex-
traction. The Joshua 1.0 release also included
re-implementations of suffix array grammar ex-
traction (Lopez, 2007; Schwartz and Callison-
Burch, 2010) and minimum error rate training
(Och, 2003; Zaidan, 2009). Additionally, it in-
cluded parallel and distributed computing tech-
niques for scalability (Li and Khudanpur, 2008).
This paper describes the additions to the toolkit
over the past year, which together form the 2.0 re-
lease. The software has been heavily used by the
authors and several other groups in their daily re-
search, and has been substantially refined since the
first release. The most important new functions in
the toolkit are:
? Support for any style of synchronous context
free grammar (SCFG) including syntax aug-
ment machine translation (SAMT) grammars
(Zollmann and Venugopal, 2006)
? Support for external modules to posit transla-
tions for spans in the input sentence that con-
strain decoding (Irvine et al, 2010)
? Lattice parsing for dealing with input un-
certainty, including ambiguous output from
speech recognizers or Chinese word seg-
menters (Dyer et al, 2008)
? A semiring architecture over hypergraphs
that allows many inference operations to be
implemented easily and elegantly (Li and
Eisner, 2009)
? Improvements to decoding through varia-
tional decoding and other approximate meth-
ods that overcome intractable MAP decoding
(Li et al, 2009b)
? Hypergraph-based discriminative training for
better feature engineering (Li and Khudan-
pur, 2009b)
? A parallelization of MERT?s computations,
and supporting document-level and tail-based
optimization (Zaidan, 2010)
? Visualization of the derivation trees and hy-
pergraphs (Weese and Callison-Burch, 2010)
? A convenient framework for designing and
running reproducible machine translation ex-
periments (Schwartz, under review)
The sections below give short descriptions for
each of these new functions.
133
2 Support for Syntax-based Translation
The initial release of Joshua supported only
Hiero-style SCFGs, which use a single nontermi-
nal symbol X. This release includes support for ar-
bitrary SCFGs, including ones that use a rich set
of linguistic nonterminal symbols. In particular
we have added support for Zollmann and Venu-
gopal (2006)?s syntax-augmented machine trans-
lation. SAMT grammar extraction is identical to
Hiero grammar extraction, except that one side of
the parallel corpus is parsed, and syntactic labels
replace the X nonterminals in Hiero-style rules.
Instead of extracting this Hiero rule from the bi-
text
[X]? [X,1] sans [X,2] | [X,1] without [X,2]
the nonterminals can be labeled according to
which constituents cover the nonterminal span on
the parsed side of the bitext. This constrains what
types of phrases the decoder can use when produc-
ing a translation.
[VP]? [VBN] sans [NP] | [VBN] without [NP]
[NP]? [NP] sans [NP] | [NP] without [NP]
Unlike GHKM (Galley et al, 2004), SAMT has
the same coverage as Hiero, because it allows
non-constituent phrases to get syntactic labels us-
ing CCG-style slash notation. Experimentally, we
have found that the derivations created using syn-
tactically motivated grammars exhibit more coher-
ent syntactic structure than Hiero and typically re-
sult in better reordering, especially for languages
with word orders that diverge from English, like
Urdu (Baker et al, 2009).
3 Specifying Constraints on Translation
Integrating output from specialized modules
(like transliterators, morphological analyzers, and
modality translators) into the MT pipeline can
improve translation performance, particularly for
low-resource languages. We have implemented
an XML interface that allows external modules
to propose alternate translation rules (constraints)
for a particular word span to the decoder (Irvine
et al, 2010). Processing that is separate from
the MT engine can suggest translations for some
set of source side words and phrases. The XML
format allows for both hard constraints, which
must be used, and soft constraints, which compete
with standard extracted translation rules, as well
as specifying associated feature weights. In ad-
dition to specifying translations, the XML format
allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a par-
ticular span to be translated as an NP. We modi-
fied Joshua?s chart-based decoder to support these
constraints.
4 Semiring Parsing
In Joshua, we use a hypergraph (or packed forest)
to compactly represent the exponentially many
derivation trees generated by the decoder for an
input sentence. Given a hypergraph, we may per-
form many atomic inference operations, such as
finding one-best or k-best translations, or com-
puting expectations over the hypergraph. For
each such operation, we could implement a ded-
icated dynamic programming algorithm. How-
ever, a more general framework to specify these
algorithms is semiring-weighted parsing (Good-
man, 1999). We have implemented the in-
side algorithm, the outside algorithm, and the
inside-outside speedup described by Li and Eis-
ner (2009), plut the first-order expectation semir-
ing (Eisner, 2002) and its second-order version (Li
and Eisner, 2009). All of these use our newly im-
plemented semiring framework.
The first- and second-order expectation semi-
rings can also be used to compute many interesting
quantities over hypergraphs. These quantities in-
clude expected translation length, feature expec-
tation, entropy, cross-entropy, Kullback-Leibler
divergence, Bayes risk, variance of hypothesis
length, gradient of entropy and Bayes risk, covari-
ance and Hessian matrix, and so on.
5 Word Lattice Input
We generalized the bottom-up parsing algorithm
that generates the translation hypergraph so that
it supports translation of word lattices instead of
just sentences. Our implementation?s runtime and
memory overhead is proportional to the size of the
lattice, rather than the number of paths in the lat-
tice (Dyer et al, 2008). Accepting lattice-based
input allows the decoder to explore a distribution
over input sentences, allowing it to select the best
translation from among all of them. This is es-
pecially useful when Joshua is used to translate
the output of statistical preprocessing components,
such as speech recognizers or Chinese word seg-
menters, which can encode their alternative analy-
ses as confusion networks or lattices.
134
6 Variational Decoding
Statistical models in machine translation exhibit
spurious ambiguity. That is, the probability of an
output string is split among many distinct deriva-
tions (e.g., trees or segmentations) that have the
same yield. In principle, the goodness of a string
is measured by the total probability of its many
derivations. However, finding the best string dur-
ing decoding is then NP-hard. The first version of
Joshua implemented the Viterbi approximation,
which measures the goodness of a translation us-
ing only its most probable derivation.
The Viterbi approximation is efficient, but it ig-
nores most of the derivations in the hypergraph.
We implemented variational decoding (Li et al,
2009b), which works as follows. First, given a for-
eign string (or lattice), the MT system produces a
hypergraph, which encodes a probability distribu-
tion p over possible output strings and their deriva-
tions. Second, a distribution q is selected that ap-
proximates p as well as possible but comes from
a family of distributions Q in which inference is
tractable. Third, the best string according to q
(instead of p) is found. In our implementation,
the q distribution is parameterized by an n-gram
model, under which the second and third steps can
be performed efficiently and exactly via dynamic
programming. In this way, variational decoding
considers all derivations in the hypergraph but still
allows tractable decoding.
7 Hypergraph-based Discriminative
Training
Discriminative training with a large number of
features has potential to improve the MT perfor-
mance. We have implemented the hypergraph-
based minimum risk training (Li and Eisner,
2009), which minimizes the expected loss of the
reference translations. The minimum-risk objec-
tive can be optimized by a gradient-based method,
where the risk and its gradient can be computed
using a second-order expectation semiring. For
optimization, we use both L-BFGS (Liu et al,
1989) and Rprop (Riedmiller and Braun, 1993).
We have also implemented the average Percep-
tron algorithm and forest-reranking (Li and Khu-
danpur, 2009b). Since the reference translation
may not be in the hypergraph due to pruning or in-
herent defficiency of the translation grammar, we
need to use an oracle translation (i.e., the transla-
tion in the hypergraph that is most simmilar to the
reference translation) as a surrogate for training.
We implemented the oracle extraction algorithm
described by Li and Khudanpur (2009a) for this
purpose.
Given the current infrastructure, other training
methods (e.g., maximum conditional likelihood or
MIRA as used by Chiang et al (2009)) can also be
easily supported with minimum coding. We plan
to implement a large number of feature functions
in Joshua so that exhaustive feature engineering is
possible for MT.
8 Minimum Error Rate Training
Joshua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu (Och, 2003).
We have parallelized our MERT module in
two ways: parallelizing the computation of met-
ric scores, and parallelizing the search over pa-
rameters. The computation of metric scores is
a computational concern when tuning to a met-
ric that is slow to compute, such as translation
edit rate (Snover et al, 2006). Since scoring a
candidate is independent from scoring any other
candidate, we parallelize this computation using a
multi-threaded solution1. Similarly, we parallelize
the optimization of the intermediate initial weight
vectors, also using a multi-threaded solution.
Another feature is the module?s awareness of
document information, and the capability to per-
form optimization of document-based variants of
the automatic metric (Zaidan, 2010). For example,
in document-based Bleu, a Bleu score is calculated
for each document, and the tuned score is the aver-
age of those document scores. The MERT module
can furthermore be instructed to target a specific
subset of those documents, namely the tail subset,
where only the subset of documents with the low-
est document Bleu scores are considered.2
More details on the MERT method and the im-
plementation can be found in Zaidan (2009).3
1Based on sample code by Kenneth Heafield.
2This feature is of interest to GALE teams, for instance,
since GALE?s evaluation criteria place a lot of focus on trans-
lation quality of tail documents.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
135
9 Visualization
We created tools for visualizing two of the
main data structures used in Joshua (Weese and
Callison-Burch, 2010). The first visualizer dis-
plays hypergraphs. The user can choose from a
set of input sentences, then call the decoder to
build the hypergraph. The second visualizer dis-
plays derivation trees. Setting a flag in the con-
figuration file causes the decoder to output parse
trees instead of strings, where each nonterminal is
annotated with its source-side span. The visual-
izer can read in multiple n-best lists in this format,
then display the resulting derivation trees side-by-
side. We have found that visually inspecting these
derivation trees is useful for debugging grammars.
We would like to add visualization tools for
more parts of the pipeline. For example, a chart
visualizer would make it easier for researchers to
tell where search errors were happening during
decoding, and why. An alignment visualizer for
aligned parallel corpora might help to determine
how grammar extraction could be improved.
10 Pipeline for Running MT
Experiments
Reproducing other researchers? machine transla-
tion experiments is difficult because the pipeline is
too complex to fully detail in short conference pa-
pers. We have put together a workflow framework
for designing and running reproducible machine
translation experiments using Joshua (Schwartz,
under review). Each step in the machine transla-
tion workflow (data preprocessing, grammar train-
ing, MERT, decoding, etc) is modeled by a Make
script that defines how to run the tools used in that
step, and an auxiliary configuration file that de-
fines the exact parameters to be used in that step
for a particular experimental setup. Workflows
configured using this framework allow a complete
experiment to be run ? from downloading data and
software through scoring the final translated re-
sults ? by executing a single Makefile.
This framework encourages researchers to sup-
plement research publications with links to the
complete set of scripts and configurations that
were actually used to run the experiment. The
Johns Hopkins University submission for the
WMT10 shared translation task was implemented
in this framework, so it can be easily and exactly
reproduced.
Acknowledgements
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors? alone.
References
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,
and Chris Callison-Burch. 2010. Integrating out-
put from specialized modules in machine transla-
tion: Transliteration in joshua. The Prague Bulletin
of Mathematical Linguistics, 93:107?116.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP, Singapore.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
136
Zhifei Li and Sanjeev Khudanpur. 2009b. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge
Nocedal. 1989. On the limited memory bfgs
method for large scale optimization. Mathematical
Programming, 45:503?528.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTER-
NATIONAL CONFERENCE ON NEURAL NET-
WORKS, pages 586?591.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua.
The Prague Bulletin of Mathematical Linguistics,
93:157?166.
Lane Schwartz. under review. Reproducible results in
parsing-based machine translation: The JHU shared
task submission. In WMT10.
Matthew Snover, Bonnie J. Dorr, and Richard
Schwartz. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Jonathan Weese and Chris Callison-Burch. 2010. Vi-
sualizing data structures in parsing-based machine
translation. The Prague Bulletin of Mathematical
Linguistics, 93:127?136.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Omar F. Zaidan. 2010. Document- and tail-based min-
imum error rate training of machine translation sys-
tems. In preparation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
137
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 33?40,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
WikiTopics: What is Popular on Wikipedia and Why
Byung Gyu Ahn1 and Benjamin Van Durme1,2 and Chris Callison-Burch1
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We establish a novel task in the spirit of news sum-
marization and topic detection and tracking (TDT):
daily determination of the topics newly popular with
Wikipedia readers. Central to this effort is a new
public dataset consisting of the hourly page view
statistics of all Wikipedia articles over the last three
years. We give baseline results for the tasks of:
discovering individual pages of interest, clustering
these pages into coherent topics, and extracting the
most relevant summarizing sentence for the reader.
When compared to human judgements, our system
shows the viability of this task, and opens the door
to a range of exciting future work.
1 Introduction
In this paper we analyze a novel dataset: we have
collected the hourly page view statistics1 for every
Wikipedia page in every language for the last three years.
We show how these page view statistics, along with other
features like article text and inter-page hyperlinks, can
be used to identify and explain popular trends, including
popular films and music, sports championships, elections,
natural disasters, etc.
Our approach is to select a set of articles whose daily
pageviews for the last fifteen days dramatically increase
above those of the preceding fifteen day period. Rather
than simply selecting the most popular articles for a given
day, this selects articles whose popularity is rapidly in-
creasing. These popularity spikes tend to be due to sig-
nificant current events in the real world. We examine 100
such articles for each of 5 randomly selected days in 2009
and attempt to group the articles into clusters such that
the clusters coherently correspond to current events and
extract a summarizing sentence that best explains the rel-
evant event. Quantitative and qualitative analyses are pro-
vided along with the evaluation dataset.
1The data does not contain any identifying information about who
viewed the pages. See http://dammit.lt/wikistats
Barack Obama
Joe Biden
White House
Inauguration
. . .
US Airways Flight 1549
Chesley Sullenberger
Hudson River
. . .
Super Bowl
Arizona Cardinals
Figure 1: Automatically selected articles for Jan 27, 2009.
We compare our automatically collected articles to
those in the daily current events portal of Wikipedia
where Wikipedia editors manually chronicle current
events, which comprise armed conflicts, international re-
lations, law and crime, natural disasters, social, political,
sports events, etc. Each event is summarized with a sim-
ple phrase or sentence that links to related articles. We
view our work as an automatic mechanism that could po-
tentially supplant this hand-curated method of selecting
current events by editors.
Figure 1 shows examples of automatically selected ar-
ticles for January 27, 2009. We would group the arti-
cles into 3 clusters, {Barack Obama, Joe Biden, White
House, Inauguration} which corresponds to the inaugu-
ration of Barack Obama, {US Airways Flight 1549, Ches-
ley Sullenburger, Hudson River} which corresponds to
the successful ditching of an airplane into the Hudson
river without loss of life, and {Superbowl, Arizona Car-
dinals} which corresponds to the then upcoming Super-
bowl XLIII.
We further try to explain the clusters by selecting sen-
tences from the articles. For the first cluster, a good se-
lection would be ?the inauguration of Barack Obama as
the 44th president . . . took place on January 20, 2009?.
For the second cluster, ?Chesley Burnett ?Sully? Sullen-
berger III (born January 23, 1951) is an American com-
33
mercial airline pilot, . . . , who successfully carried out the
emergency water landing of US Airways Flight 1549 on
the Hudson River, offshore from Manhattan, New York
City, on January 15, 2009, . . . ? would be a nice sum-
mary, which also provides links to the other articles in
the same cluster. For the third cluster, ?Superbowl XLIII
will feature the American Football Conference champion
Pittsburgh Steelers (14-4) and the National Football Con-
ference champion Arizona Cardinals (12-7) .? would be
a good choice which delineates the association with Ari-
zona Cardinals.
Different clustering methods and sentence selection
features are evaluated and results are compared. Topic
models, such as K-means (Manning et al, 2008) vector
space clustering and latent Dirichlet alocation (Blei et
al., 2003), are compared to clustering using Wikipedia?s
link structure. To select sentences we make use of NLP
technologies such as coreference resolution, and named
entity and date taggers. Note that the latest revision of
each article on the day on which the article is selected is
used in clustering and textualization to simulate the situa-
tion where article selection, clustering, and textualization
are performed once every day.
Figure 2 illustrates the pipeline of our WikiTopics sys-
tem: article selection, clustering, and textualization.
2 Article selection
We would like to identify an uptrend in popularity of ar-
ticles. In an online encyclopedia such as Wikipedia, the
pageviews for an article reflect its popularity. Following
the Trending Topics software2, WikiTopics?s articles se-
lection algorithm determines each articles? monthly trend
value as increase in pageviews within last 30 days. The
monthly trend value tk of an article k is defined as be-
low:
tk =
15?
i=1
dki ?
30?
i=16
dki
where
dki = daily pageviews i? 1 days ago for an article k
We selected 100 articles of the highest trend value for
each day in 2009. We call the articles WikiTopics articles.
We leave as future work other possibilities to determine
the trend value and choose articles3, and only briefly dis-
cuss some alternatives in this section.
Wikipedia has a portal page called ?current events?,
in which significant current events are listed manu-
ally by Wikipedia editors. Figure 3 illustrates spikes in
2http://www.trendingtopics.org
3For example, one might leverage additional signals of real world
events, such as Twitter feeds, etc.
 100
 1000
 10000
 100000
 1e+06
 1e+07
 1e+08
Dec 06 Dec 20 Jan 03 Jan 17 Jan 31
P
ag
e 
vi
ew
s
Barack Obama
United States
List of Presidents of the United States
President of the United States
African American
List of African-American firsts
Figure 3: Pageviews for all the hand-curated articles related
to the inauguration of Barack Obama. Pageviews spike on the
same day as the event took place?January 20, 2009.
pageviews of the hand-curated articles related to the in-
auguration of Barack Obama, which shows clear correla-
tion between the spikes and the day on which the relevant
event took place. It is natural to contrast WikiTopics ar-
ticles to this set of hand-curated articles. We evaluated
WikiTopics articles against hand-curated articles as gold
standard and had negative results with precision of 0.13
and recall of 0.28.
There are a few reasons for this. First, there are
much fewer hand-curated articles than WikiTopics arti-
cles: 17,253 hand-selected articles vs 36,4004 WikiTopics
articles; so precision cannot be higher than 47%. Second,
many of the hand-selected articles turned out to have very
low pageviews: 6,294 articles (36.5%) have maximum
daily pageviews less than 1,000 whereas WikiTopics arti-
cles have increase in pageviews of at least 10,000. It is ex-
tremely hard to predict the hand-curated articles based on
pageviews. Figure 4 further illustrates hand-curated arti-
cles? lack of increase in pageviews as opposed to Wiki-
Topics articles. On the contrary, nearly half of the hand-
curated articles have decrease in pageviews. For the hand-
curated articles, it seems that spikes in pageviews are
an exception rather than a commonality. We therefore
concluded that it is futile to predict hand-curated arti-
cles based on pageviews. The hand-curated articles suffer
from low popularity and do not spike in pageviews often.
Figure 5 contrasts the WikiTopics articles and the hand-
curated articles. The WikiTopics articles shown here do
not appear in the hand-curated articles within fifteen days
before or after, and vice versa. WikiTopics selected arti-
cles about people who played a minor role in the relevant
event, recently released films, their protagonists, popular
TV series, etc. Wikipedia editors selected articles about
4One day is missing from our 2009 pageviews statistics.
34
Daily Page Views Topic Selection Clustering Textualization
Figure 2: Process diagram: (a) Topic selection: select interesting articles based on increase in pageviews. (b) Clustering: cluster the
articles according to relevant events using topic models or Wikipedia?s hyperlink structure. (c) Textualization: select the sentence
that best summarizes the relevant event.
-2
 0
 2
 4
 6
 8
 0  0.2  0.4  0.6  0.8  1
lo
g 
ra
tio
quantile
WikiTopics articles
hand-curated articles
Figure 4: Log ratio of the increase in pageviews:
log
?
i = 115dik/
?
i = 1630. Zero means no change
in pageviews. WikiTopics articles show pageviews increase in
a few orders of magnitude as opposed to hand-curated articles.
actions, things, geopolitical or organizational names in
the relevant event and their event description mentions
all of them.
For this paper we introduce the problem of topic se-
lection along with a baseline solution. There are vari-
ous viable alternatives to the monthly trend value. As
one of them, we did some preliminary experiments with
the daily trend value, which is defined by dk1 ? d
k
2 , i.e.
the difference of the pageviews between the day and the
previous day: we found that articles selected using the
daily trend value have little overlap?less than half the ar-
ticles overlapped with the monthly trend value. Future
work will consider the addition of sources other than
pageviews, such as edit histories and Wikipedia category
information, along with more intelligent techniques to
combine these different sources.
3 Clustering
Clustering plays a central role to identify current events;
a group of coherently related articles corresponds to a
WikiTopics articles
Joe Biden
Notorious (2009 film)
The Notorious B.I.G.
Lost (TV series)
. . .
hand-curated articles
Fraud
Florida
Hedge fund
Arthur Nadel
Federal Bureau of Investigation
Figure 5: Illustrative articles for January 27, 2009. WikiTopics
articles here do not appear in hand-curated articles within fifteen
days before or after, and vice versa. The hand-curated articles
shown here are all linked from a single event ?Florida hedge
fund manager Arthur Nadel is arrested by the United States Fed-
eral Bureau of Investigation and charged with fraud.?
current event. Clusters, in general, may have hierarchies
and an element may be a member of multiple clusters.
Whereas Wikipedia?s current events are hierarchically
compiled into different levels of events, we focus on flat
clustering, leaving hierarchical clustering as future work,
but allow multiple memberships.
In addition to clustering using Wikipedia?s inter-page
hyperlink structure, we experimented with two families
of clustering algorithms pertaining to topic models: the
K-means clustering vector space model and the latent
Dirichlet alocation (LDA) probabilistic topic model. We
used the Mallet software (McCallum, 2002) to run these
topic models. We retrieve the latest revision of each arti-
cle on the day that WikiTopics selected it. We strip unnec-
essary HTML tags and Wiki templates with mwlib5 and
split sentences with NLTK (Loper and Bird, 2002). Nor-
malization, tokenization, and stop words removal were
performed, but no stemming was performed. The uni-
gram (bag-of-words) model was used and the number
5http://code.pediapress.com/wiki/wiki/mwlib
35
Test set # Clusters B3 F-score
Human-1 48.6 0.70 ? 0.08
Human-2 50.0 0.71 ? 0.11
Human-3 53.8 0.74 ? 0.10
ConComp 31.8 0.42 ? 0.18
OneHop 45.2 0.58 ? 0.17
K-means tf 50 0.52 ? 0.04
K-means tf-idf 50 0.58 ? 0.09
LDA 44.8 0.43 ? 0.08
Table 1: Clustering evaluation: F-scores are averaged across
gold standard datasets. ConComp and OneHop are using the
link structure. K-means clustering with tf-idf performs best.
Manual clusters were evaluated against those of the other two
annotators to determine inter-annotator agreement.
of clusters/topics K was set to 50, which is the average
number of clusters in the human clusters6. For K-means,
the common settings were used: tf and tf-idf weighting
and cosine similarity (Allan et al, 2000). For LDA, we
chose the most probable topic for each article as the clus-
ter ID. Two different clustering schemes make use of the
inter-page hyperlink structure: ConComp and OneHop.
In these schemes, the link structure is treated as a graph,
in which each page corresponds to a vertex and each link
to an undirected edge. ConComp groups a set of arti-
cles that are connected together. OneHop chooses an ar-
ticle and groups a set of articles that are directly linked.
The number of resulting clusters depends on the order
in which you choose an article. To find the minimum or
maximum number of such clusters would be computa-
tionally expensive. Instead of attempting to find the op-
timal number of clusters, we take a greedy approach and
iteratively create clusters that maximize the central node
connectivity, stopping when all nodes are in at least one
cluster. This allows for singleton clusters.
Three annotators manually clustered WikiTopics arti-
cles for five randomly selected days. The three manual
clusters were evaluated against each other to measure
inter-annotator agreement, using the multiplicity B3 met-
ric (Amigo? et al, 2009). Table 1 shows the results. The
B3 metric is an extrinsic clustering evaluation metric and
needs a gold standard set of clusters to evaluate against.
The multiplicity B3 works nicely for overlapping clus-
ters: the metric does not need to match cluster IDs and
only considers the number of the clusters that a pair of
data points shares. For a pair of data points e and e?, let
C(e) be the set of the test clusters that e belongs to, and
L(e) be the set of e?s gold standard clusters. The multi-
6K=50 worked reasonably well for the most cases. We are planning
to explore a more principled way to set the number.
Airbus A320 family
Air Force One
Chesley Sullenberger
US Airways Flight 1549
Super Bowl XLIII
Arizona Cardinals
Super Bowl
Kurt Warner
2009 flu pandemic by country
Severe acute respiratory syndrome
2009 flu pandemic in the United States
Figure 6: Examples of clusters: K-means clustering on the arti-
cles of January 27, 2009 and May 12, 2009. The centroid article
for each cluster, defined as the closest article to the center of the
cluster in vector space, is in bold.
plicity B3 scores are evaluated as follows:
Prec(e, e?) =
min (|C(e) ? C(e?)|, |L(e) ? L(e?)|)
|C(e) ? C(e?)|
Recall(e, e?) =
min (|C(e) ? C(e?)|, |L(e) ? L(e?)|)
|L(e) ? L(e?)|
The overall B3 scores are evaluated as follows:
Prec = AvgeAvge?.C(e)?C(e?)6=0Prec(e, e
?)
Recall = AvgeAvge?.L(e)?L(e?)6=0Recall(e, e
?)
The inter-annotator agreement in the B3 scores are in the
range of 67%?74%. K-means clustering performs best,
achieving 79% precision compared to manual cluster-
ing. OneHop clustering using the link structure achieved
comparable performance. LDA performed significantly
worse, comparable to ConComp clustering.
Clustering the articles according to the relevance to re-
cent popularity is not trivial even for humans. In Wiki-
Topics articles for February 10, 2009, Journey (band) and
Bruce Springsteen may seem to be relevant to Grammy
Awards, but in fact they are relevant on this day because
they performed the halftime show at the Super Bowl. K-
means fails to recognize this and put them into the cluster
of Grammy Awards, while ConComp merged Grammy
Awards and Super Bowl into the same cluster. OneHop
kept the two clusters intact and benefited from putting
Bruce Springsteen into both the clusters. LDA cluster-
ing does not have such a benefit; its performance might
have suffered from our allowing only a single member-
ship for an article. Clustering using the link structure per-
forms comparably with other clustering algorithms with-
out using topic models. It is worth noting that there are
a few ?octopus? articles that have links to many articles.
The United States on January 27, 2009 was disastrous,
with its links to 58 articles, causing ConComp clustering
to group 89 articles into a single cluster. OneHop clus-
tering?s condition that groups only articles that are one
hop away alleviates the issue and it also benefited from
putting an article into multiple clusters.
36
To see if external source help better clustering, we ex-
plored the use of news articles. We included the news ar-
ticles that we crawled from various news websites into
the same vector space as the Wikipedia articles, and ran
K-means clustering with the same settings as before. For
each day, we experimented with news articles within dif-
ferent numbers of past days. The results did not show
significant improvement over clustering without external
news articles. This needs further investigation7.
4 Textualization
We would like to generate textual descriptions for the
clustered articles to explain why they are popular and
what current event they are relevant to. We started with
a two-step approach similar to multi-document extrac-
tive summarization approaches (Mckeown et al, 2005).
The first step is sentence selection; we extract the best
sentence that describes the relevant event for each arti-
cle. The second step is combining the selected sentences
of a cluster into a coherent summary. Here, we focus on
the first step of selecting a sentence and evaluate the se-
lected sentences. The selected sentences for each clus-
ter are then put together without modification, where the
quality of generated summary mainly depends on the ex-
tracted sentences at the first step. We consider each article
separately, using as features only information such as date
expressions and references to the topic of the article. Fu-
ture work will consider sentence extraction, aware of the
related articles in the same cluster, and better summariza-
tion techniques, such as sentence fusion or paraphrasing.
We preprocess the Wikipedia articles using the Serif
system (Boschee et al, 2005) for date tagging and coref-
erence resolution. The identified temporal expressions
are in various formats such as exact date (?February 12,
1809?), a season (?spring?), a month (?December 1808?),
a date without a specific year (?November 19?), and even
relative time (?now?, ?later that year?, ?The following
year?). Some examples are shown in Figure 7. The en-
tities mentioned in a given article are compiled into a list
and the mentions of each entity, including pronouns, are
linked to the entity as a coreference chain. Some exam-
ples are shown in Figure 9.
In our initial scheme, we picked the first sentence
of each article because the first sentence is usually an
overview of the topic of the article and often relevant to
the current event. For example, a person?s article often
has the first line with one?s recent achievement or death.
An article about an album or a film often begins with the
release date. We call this First.
7News articles tend to group with other news articles. We are cur-
rently experimenting with different filtering and parameters. Also note
that we only experimented with all news articles on a given day. Clus-
tering with selective news articles might help.
February 12, 1809
1860
now
the 17th century
some time
December 1808
34 years old
spring
September
Later that year
November 19
that same month
The following winter
The following year
April 1865
late 1863
Figure 7: Selected examples of temporal expressions identified
by Serif from 247 such date and time expressions extracted from
the article Abraham Lincoln.
We also picked the sentence with the most recent date
to the day on which the article was selected. Dates in the
near future are considered in the same way as the recent
dates. Dates may appear in various formats, so we make a
more specific format take precedence, i.e. ?February 20,
2009? is selected over vaguer dates such as ?February
2009? or ?2009?. We call this scheme Recent.
As the third scheme, we picked the sentence with the
most recent date among those with a reference to the ar-
ticle?s title. The reasoning behind this is if the sentence
refers to the title of the article, it is more likely to be rel-
evant to the current event. We call this scheme Self.
After selecting a sentence for each cluster, we substi-
tute personal pronouns in the sentence with their proper
names. This step enhances readability of the sentence,
which often refers to people by a pronoun such as ?he?,
?his?, ?she?, or ?her?. The examples of substituted proper
names appear in Figure 9 in bold. The Serif system classi-
fies which entity mentions are proper names for the same
person, but choosing the best name among the names is
not a trivial task: proper names may vary from John to
John Kennedy to John Fitzgerald ?Jack? Kennedy. We
choose the most frequent proper name.
For fifty randomly chosen articles over the five se-
lected days, two annotators selected the sentences that
best describes why an article gained popularity recently,
among 289 sentences per each article on average from
the article text. For each article, annotators picked a sin-
gle best sentence, and possibly multiple alternative sen-
tences. If there is no such single sentence that best de-
scribes a relevant event, annotators marked none as the
best sentence and listed alternative sentences that par-
tially explain the relevant event. The evaluation results
for all the selection schemes are shown in Table 2. To
see inter-annotator agreement, two annotators? selections
were evaluated against each other. The other selection
schemes are evaluated against both the two annotators?
selection and their scores in the table are averaged across
the two. The precision and recall score for best sentences
are determined by evaluating a scheme?s selection of the
37
2009-01-27: Inauguration of Barack Obama
Gold: The inauguration of Barack Obama as the forty-fourth President
of the United States took place on January 20, 2009.
Alternatives: 1. The inauguration, with a record attendance for any
event held in Washington, D.C., marked the commencement of the
four-year term of Barack Obama as President and Joseph Biden as
Vice President. 2. With his inauguration as President of the United
States, Obama became the first African American to hold the office
and the first President born in Hawaii. 3. Official events were held in
Washington, D.C. from January 18 to 21, 2009, including the We Are
One: The Obama Inaugural Celebration at the Lincoln Memorial, a day
of service on the federal observance of the Martin Luther King, Jr. Day,
a ?Kids? Inaugural: We Are the Future? concert event at the Verizon
Center, the inaugural ceremony at the U.S. Capitol, an inaugural
luncheon at National Statuary Hall, a parade along Pennsylvania
Avenue, a series of inaugural balls at the Washington Convention
Center and other locations, a private White House gala and an inaugural
prayer service at the Washington National Cathedral.
First: The inauguration of Barack Obama as the forty-fourth President
of the United States took place on January 20, 2009.
Recent: On January 22, 2009, a spokesperson for the Joint Committee
on Inaugural Ceremonies also announced that holders of blue, purple
and silver tickets who were unable to enter the Capitol grounds to view
the inaugural ceremony would receive commemorative items.
Self: On January 21, 2009, President Obama, First Lady Michelle
Obama, Vice President Biden and Dr. Jill Biden attended an inaugural
prayer service at the Washington National Cathedral.
2009-02-10: February 2009 Great Britain and Ireland snowfall
Gold: The snowfall across Great Britain and Ireland in February 2009
is a prolonged period of snowfall that began on 1 February 2009.
Alternative: Many areas experienced their largest snowfall levels in 18
years.
First: The snowfall across Great Britain and Ireland in February 2009
is a prolonged period of snowfall that began on 1 February 2009.
Recent: BBC regional summary - 4 February 2009
Self: The snowfall across Great Britain and Ireland in February 2009 is
a prolonged period of snowfall that began on 1 February 2009.
2009-04-19: Wilkins Sound
Gold: On 5 April 2009 the thin bridge of ice to the Wilkins Ice Shelf
off the coast of Antarctica splintered, and scientists expect it could
cause the collapse of the Shelf.
Alternatives: 1. There are reports the shelf has exploded into hundreds
of small ice bergs. 2. On 5 April 2009, the ice bridge connecting part
of the ice shelf to Charcot Island collapsed.
First: Wilkins Sound is a seaway in Antarctica that is largely occupied
by the Wilkins Ice Shelf.
Recent: On 5 April 2009 the thin bridge of ice to the Wilkins Ice Shelf
off the coast of Antarctica splintered, and scientists expect it could
cause the collapse of the Shelf.
Self: On 25 March 2008 a chunk of the Wilkins ice shelf disintegrated,
putting an even larger portion of the glacial ice shelf at risk.
Figure 8: Sentence selection: First selects the first sentence, and
often fails to relate the current event. Recent tend to pinpoint the
exact sentence that describes the relevant current event, but fails
when there are several sentences with a recent temporal expres-
sion. Self helps avoid sentences that does not refer to the topic
of the article, but suffers from errors propagated from corefer-
ence resolution.
2009-01-27: Barack Obama
Before: He was inaugurated as President on January 20, 2009.
After: Obama was inaugurated as President on January 20,
2009.
Coref: {Barack Hussein Obama II (brk hsen obm; born August
4,, Barack Obama, Barack Obama as the forty-fourth President,
Barack Obama, Sr. , Crain?s Chicago Business naming Obama,
Michelle Obama, Obama, Obama in Indonesian, Senator
Obama,}
2009-02-10: Rebirth (Lil Wayne album)
Before: He also stated the album will be released on April 7,
2009.
After: Lil Wayne also stated the album will be released on
April 7, 2009.
Coref: {American rapper Lil Wayne, Lil Wayne, Wayne}
2009-04-19: Phil Spector
Before: His second trial resulted in a conviction of second
degree murder on April 13, 2009.
After: Spector?s second trial resulted in a conviction of second
degree murder on April 13, 2009.
Coref: {Mr. Spector, Phil Spector, Phil Spector? The character
of Ronnie ?Z, Spector, Spector-, Spector (as a producer),
Spector himself, Spector of second-degree murder, Spector,
who was conducting the band for all the acts,, Spektor, wife
Ronnie Spector}
2009-05-12: Eminem
Before: He is planning on releasing his first album since 2004,
Relapse, on May 15, 2009.
After: Eminem is planning on releasing his first album since
2004, Relapse, on May 15, 2009.
Coref: {Eminem, Marshall Bruce Mathers, Marshall Bruce
Mathers III, Marshall Bruce Mathers III (born October 17,,
Mathers}
2009-10-12: Brett Favre
Before: He came out of retirement for the second time and
signed with the Minnesota Vikings on August 18, 2009.
After: Favre came out of retirement for the second time and
signed with the Minnesota Vikings on August 18, 2009.
Coref: {Bonita Favre, Brett Favre, Brett Lorenzo Favre, Brett?s
father Irvin Favre, Deanna Favre, Favre, Favre,, Favre (ISBN
978-1590710364) which discusses their personal family and
Green Bay Packers family, Irvin Favre, Southern Miss. Favre,
the Brett Favre, The following season Favre, the jersey Favre}
Figure 9: Pronoun replacement: Personal pronouns are substi-
tuted with their proper names, which are italicized. The coref-
erence chain for the entity is also shown; our method correctly
avoids names wrongly placed in the chain. Note that unlike the
other sentences, the last one is not related to the current event,
Brett Favre?s victory against Green Bay Packers.
38
Single best Alternatives
Scheme Precision Recall Precision Recall
Human 0.50 0.55 0.85 0.75
First 0.14 0.20 0.33 0.40
Recent 0.31 0.44 0.51 0.60
Self 0.31 0.36 0.49 0.48
Self fallback 0.33 0.46 0.52 0.62
Table 2: Textualization: evaluation results of sentence selection
schemes. Self fallback scheme first tries to select the best sen-
tence as the Self scheme, and if it fails to select one it falls back
to the Recent scheme.
best sentences against a gold standard?s selection. To
evaluate alternative sentences, precision is measured as
the fraction of articles where the test and gold standard
selections overlap (share at least one sentence), compared
to the total number of articles that have at least one sen-
tence selected according to the test set. Recall is defined
by instead dividing by the number of articles that have at
least one sentence selected in the gold standard.
The low inter-annotator agreement for selecting the
best sentence shows the difficulty of the task. However,
when their sentence selection is evaluated by allowing
multiple alternative gold standard sentences, the agree-
ment is higher. It seems that there are a set of articles for
which it is easy to pick the best sentence that two anno-
tators and automatic selection schemes easily agree on,
and another set of articles for which it is difficult to find
such a sentence. In the easier articles, the best sentence
often includes a recent date expression, which is easily
picked up by the Recent scheme. Figure 8 illustrates such
cases. In the more difficult articles, there are no such sen-
tences with recent dates. X2 (film) is such an example; it
was released in 2003. The release of the prequel X-Men
Origins: Wolverine in 2009 renewed its popularity and
the X2 (film) article still does not have any recent dates.
There is a more subtle case: the article Farrah Fawcett
includes many sentences with recent dates in a section,
which describes the development of a recent event. It is
hard to pinpoint the best one among them.
Sentence selection heavily depends on other NLP com-
ponents, so errors in them could result in the error in sen-
tence selection. Serena Williams is an example where an
error in sentence splitting propagates to sentence selec-
tion. The best sentence manually selected was the first
sentence in the article ?Serena Jameka Williams . . . , as of
February 2, 2009, is ranked World No. 1 by the Women?s
Tennis Association . . . .? The sentence was disastrously
divided into two sentences right after ?No.? by NLTK
during preprocessing. In other words, the gold standard
sentence could not be selected no matter how well se-
lection performs. Another source of error propagation is
coreference resolution. The Self scheme limits sentence
selection to the sentences with a reference to the articles?
title, and it failed to improve over Recent. In qualitative
analysis, 3 out of 4 cases that made a worse choice re-
sulted from failing to recognize a reference to the topic
of the article. By having it fall back to Recent?s selection
when it failed to find any best sentence, its performance
marginally improved. Improvements of the components
would result in better performance of sentence selection.
WikiTopics?s current sentence extraction succeeded in
generating the best or alternative sentences that summa-
rizes the relevant current event for more than half of the
articles, in enhanced readability through coreference res-
olution. For the other difficult cases, it needs to take dif-
ferent strategies rather than looking for the most recent
date expressions. Alternatives may consider references to
other related articles. In future work, selected sentences
will be combined to create summary of a current event,
and will use sentence compression, fusion and paraphras-
ing to create more succinct summaries.
5 Related work
WikiTopics?s pipeline architecture resembles that of news
summarization systems such as Columbia Newsblaster
(McKeown et al, 2002). Newsblaster?s pipeline is com-
prised of components for performing web crawls, article
text extraction, clustering, classification, summarization,
and web page generation. The system processes a con-
stant stream of newswire documents. In contrast, Wiki-
Topics analyzes a static set of articles. Hierarchical clus-
tering like three-level clustering of Newsblaster (Hatzi-
vassiloglou et al, 2000) could be applied to WikiTopics
to organize current events hierarchically. Summarizing
multiple sentences that are extracted from the articles in
the same cluster would provide a comprehensive descrip-
tion about the current event. Integer linear programming-
based models (Woodsend and Lapata, 2010) may prove to
be useful to generate summaries while global constraints
like length, grammar, and coverage are met.
The problem of Topic Detection and Tracking (TDT)
is to identify and follow new events in newswire, and
to detect the first story about a new event (Allan et al,
1998). Allan et al (2000) evaluated a variety of vector
space clustering schemes, where the best settings from
those experiments were then used in our work. This was
followed recently by Petrovic? et al (2010), who took an
approximate approach to first story detection, as applied
to Twitter in an on-line streaming setting. Such a system
might provide additional information to WikiTopics by
helping to identify and describe current events that have
yet to be explicitly described in a Wikipedia article. Svore
et al (2007) explored enhancing single-document sum-
mariation using news query logs, which may also be ap-
plicable to WikiTopics.
Wikipedia?s inter-article links have been utilized to
39
construct a topic ontology (Syed et al, 2008), word seg-
mentation corpora (Gabay et al, 2008), or to compute
semantic relatedness (Milne and Witten, 2008). In our
work, we found the link structure to be as useful to cluster
topically related articles as well as the article text. In fu-
ture work, the text and the link structure will be combined
as Chaudhuri et al (2009) explored multi-view hierarchi-
cal clustering for Wikipedia articles.
6 Conclusions
We have described a pipeline for article selection, clus-
tering, and textualization in order to identify and describe
significant current events as according to Wikipedia con-
tent, and metadata. Similarly to Wikipedia editors main-
taining that site?s ?current events? pages, we are con-
cerned with neatly collecting articles of daily relevance,
only automatically, and more in line with expressed user
interest (through the use of regularly updated page view
logs). We have suggested that Wikipedia?s hand-curated
articles cannot be predicted solely based on pageviews.
Clustering methods based on topic models and inter-
article link structure are shown to be useful to group
a set of articles that are coherently related to a current
event. Clustering based on only link structure achieved
comparable performance with clustering based on topic
models. In a third of cases, the sentence that best de-
scribed a current event could be extracted from the ar-
ticle text based on temporal expressions within an article.
We employed a coreference resolution system assist in
text generation, for improved readability. As future work,
sentence compression, fusion, and paraphrasing could be
applied to selected sentences with various strategies to
more succinctly summarize the current events. Our ap-
proach is language independent, and may be applied to
multi-lingual current event detection, exploiting further
the online encyclopedia?s cross-language references. Fi-
nally, we plan to leverage social media such as Twit-
ter as an additional signal, especially in cases where es-
sential descriptive information has yet to be added to a
Wikipedia article of interest.
Acknowledgments
We appreciate Domas Mituzas and Fre?de?ric Schu?tz for
the pageviews statistics and Peter Skomoroch for the
Trending Topics software. We also thank three anony-
mous reviewers for their thoughtful advice. This re-
search was supported in part by the NSF under grant IIS-
0713448 and the EC through the EuroMatrixPlus project.
The first author was funded by Samsung Scholarship.
Opinions, interpretations, and conclusions are those of
the authors and not necessarily endorsed by the sponsors.
References
James Allan, Jaime Carbonell, George Doddington, Jonathan
Yamron, and Yiming Yang. 1998. Topic Detection and
Tracking Pilot Study Final Report. In Proceedings of the
DARPA Broadcast News Transcription and Understanding
Workshop.
James Allan, Victor Lavrenko, Daniella Malin, and Russell
Swan. 2000. Detections, bounds, and timelines: UMass
and TDT-3. In Proceedings of Topic Detection and Track-
ing Workshop.
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Inf. Retr.,
12(4):461?486.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent Dirichlet
allocation. Journal of Machine Learning Research.
Elizabeth Boschee, Ralph Weischedel, and Alex Zamanian.
2005. Automatic information extraction. In Proceedings of
IA.
Kamalika Chaudhuri, Sham M. Kakade, Karen Livescu, and
Karthik Sridharan. 2009. Multi-view clustering via canoni-
cal correlation analysis. In Proceedings of ICML.
David Gabay, Ziv Ben-Eliahu, and Michael Elhadad. 2008. Us-
ing wikipedia links to construct word segmentation corpora.
In Proceedings of AAAI Workshops.
Vasileios Hatzivassiloglou, Luis Gravano, and Ankineedu Ma-
ganti. 2000. An investigation of linguistic features and clus-
tering algorithms for topical document clustering. In Pro-
ceedings of SIGIR.
Edward Loper and Steven Bird. 2002. NLTK: the Natural Lan-
guage Toolkit. In Proceedings of ACL.
C. Manning, P. Raghavan, and H. Schu?tze. 2008. Introduction
to information retrieval. Cambridge University Press.
Andrew Kachites McCallum. 2002. MALLET: A Machine
Learning for Language Toolkit. http://mallet.cs.umass.edu.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova,
Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002.
Tracking and summarizing news on a daily basis with
Columbia?s Newsblaster. In Proceedings of HLT.
Kathleen Mckeown, Rebecca J. Passonneau, David K. Elson,
Ani Nenkova, and Julia Hirschberg. 2005. Do summaries
help? a task-based evaluation of multi-document summariza-
tion. In Proceedings of SIGIR.
David Milne and Ian H. Witten. 2008. An effective, low-cost
measure of semantic relatedness obtained from Wikipedia
links. In Proceedings of AAAI Workshops.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko. 2010.
Streaming first story dectection with application to Twitter.
In Proceedings of NAACL.
Krysta M. Svore, Lucy Vanderwende, and Christopher J.C.
Burges. 2007. Enhancing single-document summarization
by combining ranknet and third-party sources. In Proceed-
ings of EMNLP-CoLing.
Zareen Saba Syed, Tim Finin, and Anupam Joshi. 2008.
Wikipedia as an ontology for describing documents. In Pro-
ceedings of ICWSM.
Kristian Woodsend and Mirella Lapata. 2010. Automatic gen-
eration of story highlights. In Proceedings of ACL.
40
Paraphrase Fragment Extraction from Monolingual Comparable Corpora
Rui Wang
Language Technology Lab
DFKI GmbH
Stuhlsatzenhausweg 3 / Building D3 2
Saarbruecken, 66123 Germany
rwang@coli.uni-sb.de
Chris Callison-Burch
Computer Science Department
Johns Hopkins University
3400 N. Charles Street (CSEB 226-B)
Baltimore, MD 21218, USA
ccb@cs.jhu.edu
Abstract
We present a novel paraphrase fragment pair
extraction method that uses a monolingual
comparable corpus containing different arti-
cles about the same topics or events. The pro-
cedure consists of document pair extraction,
sentence pair extraction, and fragment pair ex-
traction. At each stage, we evaluate the in-
termediate results manually, and tune the later
stages accordingly. With this minimally su-
pervised approach, we achieve 62% of accu-
racy on the paraphrase fragment pairs we col-
lected and 67% extracted from the MSR cor-
pus. The results look promising, given the
minimal supervision of the approach, which
can be further scaled up.
1 Introduction
Paraphrase is an important linguistic phenomenon
which occurs widely in human languages. Since
paraphrases capture the variations of linguistic ex-
pressions while preserving the meaning, they are
very useful in many applications, such as machine
translation (Marton et al, 2009), document summa-
rization (Barzilay et al, 1999), and recognizing tex-
tual entailment (RTE) (Dagan et al, 2005).
However, such resources are not trivial to ob-
tain. If we make a comparison between para-
phrase and MT, the latter has large parallel bilin-
gual/multilingual corpora to acquire translation pairs
in different granularity; while it is difficult to find a
?naturally? occurred paraphrase ?parallel? corpora.
Furthermore, in MT, certain words can be translated
into a (rather) small set of candidate words in the
target language; while in principle, each paraphrase
can have infinite number of ?target? expressions,
which reflects the variety of each human language.
A variety of paraphrase extraction approaches
have been proposed recently, and they require dif-
ferent types of training data. Some require bilingual
parallel corpora (Callison-Burch, 2008; Zhao et al,
2008), others require monolingual parallel corpora
(Barzilay and McKeown, 2001; Ibrahim et al, 2003)
or monolingual comparable corpora (Dolan et al,
2004).
In this paper, we focus on extracting paraphrase
fragments from monolingual corpora, because this is
the most abundant source of data. Additionally, this
would potentially allow us to extract paraphrases for
a variety of languages that have monolingual cor-
pora, but which do not have easily accessible paral-
lel corpora.
This paper makes the following contributions:
1. We adapt a translation fragment pair extrac-
tion method to paraphrase extraction, i.e., from
bilingual corpora to monolingual corpora.
2. We construct a large collection of para-
phrase fragments from monolingual compara-
ble corpora and achieve similar quality from a
manually-checked paraphrase corpus.
3. We evaluate both intermediate and final results
of the paraphrase collection, using the crowd-
sourcing technique, which is effective, fast, and
cheap.
52
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 52?60,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Corpora Sentence level Sub-sentential level
Paraphrase acquisition
Monolingual Parallel e.g., Barzilay and McKeown (2001) This paperComparable e.g., Quirk et al (2004) e.g., Shinyama et al (2002) & This paper
Bilingual Parallel N/A e.g., Bannard and Callison-Burch (2005)
Statistical machine translation
Bilingual Parallel Most SMT systems SMT phrase tablesComparable e.g., Fung and Lo (1998) e.g., Munteanu and Marcu (2006)
Table 1: Previous work in paraphrase acquisition and machine translation.
2 Related Work
Roughly speaking, there are three dimensions to
characterize the previous work in paraphrase ac-
quisition and machine translation, whether the
data comes from monolingual or bilingual corpora,
whether the corpora are parallel or comparable, and
whether the output is at the sentence level or at the
sub-sentential level. Table 1 gives one example in
each category.
Paraphrase acquisition is mostly done at the
sentence-level, e.g., (Barzilay and McKeown, 2001;
Barzilay and Lee, 2003; Dolan et al, 2004), which is
not straightforward to be used as a resource for other
NLP applications. Quirk et al (2004) adopted the
MT approach to ?translate? one sentence into a para-
phrased one. As for the corpora, Barzilay and McK-
eown (2001) took different English translations of
the same novels (i.e., monolingual parallel corpora),
while the others experimented on multiple sources
of the same news/events, i.e., monolingual compa-
rable corpora.
At the sub-sentential level, interchangeable pat-
terns (Shinyama et al, 2002; Shinyama and Sekine,
2003) or inference rules (Lin and Pantel, 2001)
are extracted, which are quite successful in named-
entity-centered tasks, like information extraction,
while they are not generalized enough to be applied
to other tasks or they have a rather small coverage,
e.g. RTE (Dinu and Wang, 2009). To our best
knowledge, there is few focused study on general
paraphrase fragments extraction at the sub-sentential
level, from comparable corpora. A recent study
by Belz and Kow (2010) mainly aimed at natural
language generation, which they performed a small
scale experiment on a specific topic, i.e., British
hills.
Given the available parallel corpora from the MT
community, there are studies focusing on extracting
paraphrases from bilingual corpora (Bannard and
Callison-Burch, 2005; Callison-Burch, 2008; Zhao
et al, 2008). The way they do is to treat one lan-
guage as an pivot and equate two phrases in the other
languages as paraphrases if they share a common
pivot phrase. Paraphrase extraction draws on phrase
pair extraction from the translation literature. Since
parallel corpora have many alternative ways of ex-
pressing the same foreign language concept, large
quantities of paraphrase pairs can be extracted.
As for the MT research, the standard statistical
MT systems require large size of parallel corpora for
training and then extract sub-sentential translation
phrases. Apart from the limited parallel corpora,
comparable corpora are non-parallel bilingual cor-
pora whose documents convey the similar informa-
tion are also widely considered by many researchers,
e.g., (Fung and Lo, 1998; Koehn and Knight, 2000;
Vogel, 2003; Fung and Cheung, 2004a; Fung and
Cheung, 2004b; Munteanu and Marcu, 2005; Wu
and Fung, 2005). A recent study by Smith et al
(2010) extracted parallel sentences from comparable
corpora to extend the existing resources.
At the sub-sentential level, Munteanu and
Marcu (2006) extracted sub-sentential translation
pairs from comparable corpora based on the log-
likelihood-ratio of word translation probability.
They exploit the possibility of making use of reports
within a limited time window, which are about the
same event or having overlapping contents, but in
different languages. Quirk et al (2007) extracted
fragments using a generative model of noisy transla-
tions. They show that even in non-parallel corpora,
useful parallel words or phrases can still be found
and the size of such data is much larger than that of
53
Document Pair 
Extraction
Sentence Pair 
Extraction
Fragment Pair 
Extraction
Corpora
(Gigaword)
<doc> .
.. in 
1995 ... 
</doc>
<doc> .
.. Jan., 
1995 ... 
</doc>
Comparability
<sent> 
NATO ... 
in 1995 ... 
</sent>
<sent> In 
1995, 
NATO ... 
</sent>
N-Gram 
Overlapping
<frag> the 
finance chief 
</frag>
<frag> the 
chief financial 
officer </frag>
Interchangeability
Paraphrased 
Fragments
Paraphrase 
Collection
(MSR)
Paraphrase 
Collecton 
(CCB)
Figure 1: A three stage pipeline is used to extract paraphrases from monolingual texts
parallel corpora. In this paper, we adapt ideas from
the MT research on extracting sub-sentential trans-
lation fragments from bilingual comparable corpora
(Munteanu and Marcu, 2006), and use the tech-
niques to extract paraphrases from monolingual par-
allel and comparable corpora.
Evaluation is another challenge for resource col-
lection, which usually requires tremendous labor re-
sources. Both Munteanu and Marcu (2006) and
Quirk et al (2007) evaluated their resources indi-
rectly in MT systems, while in this paper, we make
use of the crowd-sourcing technique to manually
evaluate the quality of the paraphrase collection.
In parcitular, Amazon?s Mechanical Turk1 (MTurk)
provides a way to pay people small amounts of
money to perform tasks that are simple for humans
but difficult for computers. Examples of these Hu-
man Intelligence Tasks (or HITs) range from label-
ing images to moderating blog comments to pro-
viding feedback on relevance of results for a search
query. Using MTurk for NLP task evaluation has
been shown to be significantly cheaper and faster,
and there is a high agreement between aggregate
non-expert annotations and gold-standard annota-
tions provided by the experts (Snow et al, 2008).
1http://www.mturk.com/
3 Fragment Pair Acquisition
Figure 1 shows the pipeline of our paraphrase ac-
quisition method. We evaluate quality at each stage
using Amazon?s Mechanical Turk. In order to en-
sure that the non-expert annotators complete the task
accurately, we used both positive and negative con-
trols. If annotators answered either control incor-
rectly, we excluded their answers. For all the ex-
periments we describe in this paper, we obtain the
answers within a couple of hours or an overnight.
Our focus in this paper is on fragment extraction,
but we briefly describe document and sentence pair
extraction first.
3.1 Document Pair Extraction
Monolingual comparable corpora contain texts
about the same events or subjects, written in one lan-
guage by different authors (Barzilay and Elhadad,
2003). We extract pairs of newswire articles written
by different news agencies from the GIGAWORD cor-
pus, which contains articles from six different agen-
cies. Although the comparable documents are not in
parallel, at the sentential or sub-sentential level, the
paraphrased fragments may still exist.
To quantify the comparability between two doc-
uments, we calculate the number of overlapping
words and give them different weights based on
TF-IDF (Salton and McGill, 1983) using the More-
54
LikeThis2 function provided by Lucene.
After collecting the document pairs, we asked an-
notators, ?Are these two documents about the same
topic??, and allowing them to answer ?Yes?, ?No?,
and ?Not sure?. Each set of six document pairs con-
tained, four to be evaluated, one positive control (a
pair of identical documents) and one negative con-
trol (a pair of random documents). We sampled
400 document pairs with the comparability score be-
tween 0.8 and 0.9, and another 400 pairs greater than
0.9. We presented them in a random order and each
was labeled by three annotations. After excluding
the annotations containing incorrect answers for ei-
ther control, we took a majority vote for every docu-
ment pair, and if three annotations are different from
each other.
We found document pairs with >0.9 were classi-
fied by annotators to be related more than half the
time, and a higher threshold would greatly decrease
the number of document pairs extracted. We per-
formed subsequent steps on the 3896 document pairs
that belonged to this category.
3.2 Sentence Pair Extraction
After extracting pairs of related documents, we
next selected pairs of related sentences from within
paired documents. The motivation behind is that
the standard word alignment algorithms can be eas-
ily applied to the paired sentences instead of docu-
ments. To do so we selected sentences with overlap-
ping n-grams up to length n=4. Obviously for para-
phrasing, we want some of the n-grams to differ, so
we varied the amount of overlap and evaluated sen-
tence pairs with a variety of threshold bands3.
We evaluated 10 pairs of sentences at a time, in-
cluding one positive control and two negative con-
trols. A random pair of sentential paraphrases from
the RTE task acted as the positive control. The
negative controls included one random pair of non-
paraphrased, but highly relevant sentences, and a
random pair of sentences. Annotators classified the
sentence pairs as: paraphrases, related sentences,
2http://lucene.apache.org/java/2_9_1/
api/contrib-queries/org/apache/lucene/
search/similar/MoreLikeThis.html
3In the experiment setting, the thresholds (maximum com-
parability and minimum comparability) for the 4 groups are,
{0.78,0.206}, {0.206,0.138}, {0.138,0.115}, {0.115,0.1}.
0%	 ?
10%	 ?
20%	 ?
30%	 ?
40%	 ?
50%	 ?
60%	 ?
70%	 ?
80%	 ?
90%	 ?
100%	 ?
0.1-??
0.10
8	 ?
0.10
8-??0.
115	 ?
0.11
5-??0.
125	 ?
0.12
5-??0.
138	 ?
0.13
8-??0.
16	 ?
0.16
-??0.2
06	 ?
0.20
6-??0.
78	 ?
invalid	 ?
not_related	 ?
related	 ?
paraphrases	 ?
Figure 2: Results of the sentence pair extraction. The x-
axis is the threshold for the comparability scores; and the
y-axis is the distribution of the annotations.
and non-related sentences.
We uniformly sampled 200 sentence pairs from
each band. They are randomly shuffled into more
than 100 HITs and each HIT got three annotations.
Figure 2 shows the distribution of annotations across
different groups, after excluding answers that failed
the controls.
Our best scoring threshold band was 0.2-0.8. Sen-
tence pairs with this overlap were judged to be para-
phrases 45% of the time, to be related 30% of the
time, and to be unrelated 25% of the time. Although
the F2 heuristic proposed by Dolan et al (2004),
which takes the first two sentences of each document
pair, obtains higher relatedness score (we evalu-
ated F2 sentences as 50% paraphrases, 37% related,
and 13% unrelated), our n-gram overlap method ex-
tracted much more sentence pairs per document pair.
One interesting observation other than the general
increasing tendency is that the portion of the related
sentence pairs is not monotonic, which exactly re-
flects our intuition about a good comparability value
(neither too high nor too low). However, some er-
rors are difficult to exclude. For instance, one sen-
tence says ?The airstrikes were halted for 72 hours
last Thursday...? and the other says ?NATO and UN
officials extended the suspension of airstrikes for a
further 72 hours from late Sunday...?. Without fine-
grained analysis of the temporal expressions, it is
difficult to know whether they are talking about the
same event. The F2 method does provide us a fairly
good way to exclude some unrelated sentence pairs,
but note that the pairs collected by this method are
55
Figure 3: An example of fragment pair extraction. Stop words are all set to 1 initially. Zero is the threshold, and the
underscored phrases are the outputs.
only about 0.5% of using the comparability scores.
We show in Figure 1 that we also use an addi-
tional sentence-level paraphrase corpus as the input
of this module. We take all the positive instances
(i.e. the two sentences in a pair are paraphrase to
each other) and pass them to the later stage as well,
as for comparison with our paraphrase collection ex-
tracted from the comparable sentence pairs. In all,
we used 276,120 sentence pairs to feed our fragment
extraction method.
3.3 Fragment Pair Extraction
The basic procedure is to 1) establish alignments be-
tween words or n-grams and 2) extract target para-
phrase fragments. For the first step, we use two ap-
proaches. One is to change the common substring
alignment problem from string to word sequence
and we extend the longest common substring (LCS)
extraction algorithm to multiple common n-grams.
An alternative way is to use a normal word aligner
(widely used as the first step in MT systems) to ac-
complish the job. For our experiments, we use the
BerkeleyAligner4 (Liang et al, 2006) by feeding it a
dictionary of pairs of identical words along with the
paired sentences. We can also combine these two
methods by performing the LCS alignment first and
adding additional word alignments from the aligner.
These form the three configurations of our system
(Table 2).
Following Munteanu and Marcu (2006), we use
both positive and negative lexical associations for
the alignment. The positive association measures
4http://code.google.com/p/
berkeleyaligner/
how likely one word will be aligned to another
(value from 0 to 1); and the negative associations
indicates how unlikely an alignment exists between
a word pair (from -1 to 0). The basic idea to have
both is that when a word cannot be aligned with any
other words, it will choose the least unlikely one. If
the positive association of w1 being aligned with w2
is defined as the conditional probability p(w1|w2),
the negative associations will simply be p(w1|?w2).
Since we obtain a distribution of all the possible
words aligned with w1 from the word aligner, both
p(w1|w2) and p(w1|?w2) can be calculated; for the
LCS alignment, we simply set p(w1|w2) as 1 and
p(w1|?w2) as -1, if w1 and w2 are aligned; and vice
versa, if not.
After the initialization of all the word alignments
using the two associations, each word takes the av-
erage of the neighboring four words and itself. The
intuition of this smoothing is to tolerate a few un-
aligned parts (if they are surrounded by aligned
parts). Finally, all the word alignments having a pos-
itive score will be selected as the candidate fragment
elements. Figure 3 shows an example of this pro-
cess.
The second step, fragment extraction, is a bit
tricky, since a fragment is not clearly defined like
a document or a sentence. One option is to fol-
low the MT definition of a phrase, which means a
sub-sentential n-gram string (usually n is less than
10). Munteanu and Marcu (2006) adopted this, and
considered all the possible sub-sentential translation
fragments as their targets, i.e. the adjacent n-grams.
For instance, in Figure 3, all the adjacent words
above the threshold (i.e. zero) will form the target
56
Configurations
Aligner+ LCS+ Word+ LCS+Word+
Phrase Extraction Chunk N-Gram Chunk
Our Corpus
PARAPHRASE 15% 36% 32%
RELATED 21% 26% 21%
SUM 36% 62% 53%
The MSR Corpus
PARAPHRASE 38% 44% 49%
RELATED 20% 19% 18%
SUM 58% 63% 67%
Table 2: Distribution of the Extracted Fragment Pairs of
our Corpus and MSR Corpus. We manually evaluated
1051 sentence pairs in all. We use LCS or word aligner as
the initialization and apply n-gram-based or chunk-based
phrase extraction. The first column serves as the baseline.
paraphrase, ?the Bosnian Serbs to pull their heavy
weapons back from? and those aligned words in the
other sentence ?the Bosnian Serbs withdraw their
heavy weapons from? will be the source paraphrase.
The disadvantage of this definition is that the ex-
tracted fragment pairs might not be easy for human
beings to interpret or they are even ungrammatical
(cf. the fourth example in Table 5). An alternative
way is to follow the linguistic definition of a phrase,
e.g. noun phrase (NP), verb phrase (VP), etc. In this
case, we need to use (at least) a chunker to prepro-
cess the text and obtain the proper boundary of each
fragment and we used the OpenNLP chunker.
We finalize our paraphrase collection by filter-
ing out identical fragment pairs, subsumed fragment
pairs (one fragment is fully contained in the other),
and fragment having only one word. Apart from sen-
tence pairs collected from the comparable corpora,
we also did experiments on the existing MSR para-
phrase corpus (Dolan and Brockett, 2005), which is
a collection of manually annotated sentential para-
phrases.
The evaluation on both collections is done by the
MTurk. Each task contains 8 pairs of fragments to
be evaluated, plus one positive control using iden-
tical fragment pairs, and one negative control using
a pair of random fragments. All the fragments are
shown with the corresponding sentences from where
they are extracted5. The question being asked is
5We thought about evaluating pairs of isolated fragments,
?How are the two highlighted phrases related??, and
the possible answers are, ?These phrases refer to the
same thing as each other? (PARAPHRASE), ?These
phrases are overlap but contain different informa-
tion? (RELATED), and ?The phrases are unrelated or
invalid? (INVALID). Table 2 shows the results (ex-
cluding invalid sentence pairs) and Table 5 shows
some examples.
In general, the results on MSR is better than those
on our corpus6. Comparing the different settings,
for our corpus, word alignment with n-gram frag-
ment extraction works better; and for corpora with
higher comparability (e.g. the MSR corpus), the
configuration of using both LCS and word align-
ments and the chunk-based fragment extraction out-
performs the others. In fact, PARAPHRASE and RE-
LATED are not quite comparable7, since the bound-
ary mismatch of the fragments may not be obvious
to the Turkers. Nevertheless, we would assume a
cleaner output from the chunk-based method, and
both approaches achieve similar levels of quality.
Zhao et al (2008) extracted paraphrase frag-
ment pairs from bilingual parallel corpora, and
their log-liner model outperforms Bannard and
Callison-Burch (2005)?s maximum likelihood esti-
mation method with 67% to 60%. Notice that, our
starting corpora are (noisy) comparable corpora in-
stead of parallel ones (for our corpus), and the ap-
proach is almost unsupervised8, so that it can be
easily scaled up to other larger corpora, e.g. the
news websites. Furthermore, we compared our frag-
ment pair collection with Callison-Burch (2008)?s
approach on the same MSR corpus, only about 21%
of the extracted paraphrases appear on both sides,
which shows the potential to combine different re-
sources.
4 Analysis of the Collections
In this section, we present some analysis on the frag-
ment pair collection. We show the basic statistics of
the corpora and then some examples of the output.
but later found out it was difficult to make the judgement.
6A sample of the corpus can be downloaded here:
http://www.coli.uni-saarland.de/?rwang/
resources/paraphrases.
7Thanks to the anonymous reviewer who pointed this out.
8The MTurk annotations can be roughly viewed as a guide
for parameter tuning instead of training the system
57
As for comparison, we choose two other paraphrase
collections, one is acquired from parallel bilingual
corpora (Callison-Burch, 2008) and the other is us-
ing the same fragment extraction algorithm on the
MSR corpus.
4.1 Statistics of the Corpora
Stage Collection Size %
GIGAWORD (1995) 600,000 10%
Documents Retrieved 150,000 2.5%
Document Pairs Selected 10,000 0.25%
Sentence Pairs Extracted 270,000 0.1%
Fragment Pairs Extracted 90,000 0.01%
Table 3: The size of our corpus. We only used ca. 10%
of the GIGAWORD corpus in the experiments and the size
of the collection at each stage are shown in the table.
Table 3 roughly shows the percentage of the ex-
tracted data compared with the original GIGAWORD
corpus at each stage9. In the experiments reported
here, we only use a subset of the news articles in
1995. If we scale to the full GIGAWORD corpus (19
Gigabytes, news from 1994 to 2006), we expect an
order of magnitude more fragment pairs to be col-
lected.
Apart from the size of the corpus, we are also in-
terested in the composition of the corpus. Table 4
shows the proportions of some n-grams contained in
the corpus. Here CCB denotes the paraphrase col-
lection acquired from parallel bilingual corpora re-
ported in (Callison-Burch, 2008), and MSR? denotes
the collection using the same algorithm on the MSR
corpus.
In Table 4, the four columns from the left are
about the fragments (one part of each fragment pair),
and the six columns from the right are about para-
phrases. For example, 1 & 2 indicates the paraphrase
contains one single word on one side and a 2-gram
on the other side. Since we deliberately exclude sin-
gle words, the n-gram distributions of OUR and MSR
are ?flatter? than the other two corpora, but still, 2-
grams fragments occupy more than 40% in all cases.
The n-gram distributions of the paraphrases are even
more diverse for the OUR and MSR corpora. The sum
9All the numbers in the table are roughly estimated, due to
the variations of different settings. This just gives us an impres-
sion of the space for improvement.
of the listed proportions are only around 45%, while
for CCB and MSR?, the sums are about 95%.
4.2 Examples
Table 5 shows some examples from the best two set-
tings. From our corpus, both simple paraphrases
(?Governor ... said? and ?Gov. ... announced?)
and more varied ones (?rose to fame as? and ?the
highlight of his career?) can be extracted. It?s clear
that the smoothing and extraction algorithms do help
with finding non-trivial paraphrases (shown in Fig-
ure 3). The extracted phrase ?campaign was? shows
the disadvantage of n-gram-based phrase extraction
method, since the boundary of the fragment could be
improper. Using a chunker can effectively exclude
such problems, as shown in the lower part of the ta-
ble, where all the extracted paraphrases are gram-
matical phrases. Even from a parallel paraphrase
corpus at the sentence level, the acquired fragment
pairs (w/o context) could be non-paraphrases. For
instance, the second pair from the MSR corpus shows
that one news agency gives more detailed informa-
tion about the launching site than the other, and the
last example is also debatable, whether it?s ?under
$200? or ?around $200? depending on the reliability
of the information source.
5 Summary and Future Work
In this paper, we present our work on paraphrase
fragment pair extraction from monolingual compa-
rable corpora, inspired by Munteanu and Marcu
(2006)?s bilingual method. We evaluate our inter-
mediate results at each of the stages using MTurk.
Both the quality and the quantity of the collected
paraphrase fragment pairs are promising given the
minimal supervision. As for the ongoing work, we
are currently expanding our extraction process to the
whole GIGAWORD corpus, and we plan to apply it
to other comparable corpora as well. For the fu-
ture work, we consider incorporating more linguistic
constraints, e.g. using a syntactic parser (Callison-
Burch, 2008), to further improve the quality of the
collection. More importantly, applying the collected
paraphrase fragment pairs to other NLP applications
(e.g. MT, RTE, etc.) will give us a better view of the
utility of this resource.
58
N-grams Phrases Para-phrases1 2 3 4 1 & 1 1 & 2 2 & 2 1 & 3 2 & 3 3 & 3
OUR N/A 43.4% 30.5% 16.4% N/A N/A 20.0% N/A 16.7% 8.8%
MSR N/A 41.7% 30.5% 16.0% N/A N/A 20.1% N/A 16.6% 9.4%
CCB 10.7% 42.7% 32.0% 10.9% 34.7% 16.3% 24.0% 2.5% 9.4% 6.9%
MSR? 8.1% 41.4% 37.2% 10.0% 29.0% 16.6% 26.8% 2.8% 10.7% 9.6%
Table 4: The (partial) distribution of N-grams (N=1-4) in different paraphrase collections.
From Our Corpus: using word aligner and n-gram-based phrase extraction
... unveiled a detailed peace plan calling for the Bosnian Serbs to pull their heavy weapons back from Sarajevo. ParaphraseIf the Bosnian Serbs withdraw their heavy weapons from Sarajevo?s outskirts, ...
In San Juan, Puerto Rico, Governor Pedro Rosello said the the storm could hit the US territory by Friday, ... ParaphraseIn Puerto Rico, Gov. Pedro Rossello announced that banks will be open only until 11 a.m. Friday and ...
Kunstler rose to fame as the lead attorney for the ?Chicago Seven,? ... ParaphraseThe highlight of his career came when he defended the Chicago Seven ...
... initiated the air attacks in response to Serb shelling of Sarajevo that killed 38 people Monday. InvalidThe campaign was to respond to a shelling of Sarajevo Monday that killed 38 people.
From MSR Corpus: using both LCS and word aligner and chunk-based phrase extraction
O?Brien?s attorney, Jordan Green, declined to comment. ParaphraseJordan Green, the prelate?s private lawyer, said he had no comment.
Iraq?s nuclear program had been dismantled, and there ?was no convincing evidence of its reconstitution.? ParaphraseIraq?s nuclear program had been dismantled and there was no convincing evidence it was being revived, ...
... to blast off between next Wednesday and Friday from a launching site in the Gobi Desert. Related... to blast off as early as tomorrow or as late as Friday from the Jiuquan launching site in the Gobi Desert.
... Super Wireless Media Router, which will be available in the first quarter of 2004, at under $200. RelatedThe router will be available in the first quarter of 2004 and will cost around $200, the company said.
Table 5: Some examples of the extracted paraphrase fragment pairs.
Acknowledgments
The first author would like to thank the EuroMatrix-
Plus project (IST-231720) which is funded by the
European Commission under the Seventh Frame-
work Programme. The second author is supported
by the EuroMatrixPlusProject, by the DARPA
GALE program under Contract No. HR0011-06-
2-0001, and by the NSF under grant IIS-0713448.
The authors would like to thank Mirella Lapata and
Delip Rao for the useful discussions as well as the
anonymous Turkers who helped us to accomplish
the tasks.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
R. Barzilay and N. Elhadad. 2003. Sentence alignment
for monolingual comparable corpora. In Proceedings
of EMNLP.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of
ACL, College Park, MD.
Anja Belz and Eric Kow. 2010. Extracting parallel frag-
ments from comparable corpora for data-to-text gen-
eration. In Proceedings of the 6th International Nat-
ural Language Generation Conference, Stroudsburg,
PA, USA.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
I. Dagan, O. Glickman, and B. Magnini. 2005. The pas-
cal recognising textual entailment challenge. In Pro-
ceedings of the RTE Workshop.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entailment.
59
In Proceedings of EACL, Athens, Greece. Association
of Computational Linguistics.
William B. Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of the IWP2005.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of COLING.
Pascale Fung and Percy Cheung. 2004a. Mining very
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Proceedings of
EMNLP.
Pascale Fung and Percy Cheung. 2004b. Multi-level
bootstrapping for extracting parallel sentences from a
quasi-comparable corpus. In Proceedings of COLING.
P. Fung and Y. Y. Lo. 1998. An IR approach for translat-
ing new words from nonparallel, comparable texts. In
Proceedings of ACL.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora. In Proceedings of ACL.
Philipp Koehn and Kevin Knight. 2000. Estimating word
translation probabilities from unrelated monolingual
corpora using the EM algorithm. In Proceedings of
the National Conference on Artificial Intelligence.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of
inference rules from text. In Proceedings of the ACM
SIGKDD.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved statistical machine translation using
monolingually-derived paraphrases. In Proceedings of
EMNLP, Singapore.
Dragos Munteanu and Daniel Marcu. 2005. Improving
machine translation performance by exploiting compa-
rable corpora. Computational Linguistics, 31(4), De-
cember.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of ACL.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for paraphrase
generation. In Proceedings of the 2004 Conference on
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, Barcelona, Spain.
Chris Quirk, Raghavendra Udupa, and Arul Menezes.
2007. Generative models of noisy translations with
applications to parallel fragment extraction. In Pro-
ceedings of MT Summit XI, Copenhagen, Denmark.
G. Salton and M. J. McGill. 1983. Introduction to
modern information retrieval. ISBN 0-07-054484-0.
McGraw-Hill.
Y. Shinyama and S. Sekine. 2003. Paraphrase acquisition
for information extraction. In Proceedings of Interna-
tional Workshop on Paraphrasing.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles yusuke shinyama satoshi sekine automatic para-
phrase acquisition from news articles. In Proceed-
ings of Human Language Technology Conference, San
Diego, USA.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Stroudsburg, PA,
USA. Association of Computational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast - but is it good?
Evaluating non- expert annotations for natural lan-
guage tasks. In Proceedings of EMNLP.
Stephan Vogel. 2003. Using noisy bilingual data for sta-
tistical machine translation. In Proceedings of EACL.
Dekai Wu and Pascale Fung. 2005. Inversion transduc-
tion grammar constraints for mining parallel sentences
from quasi-comparable corpora. In Proceedings of
IJCNLP, Jeju Island, Korea.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In Proceedings of ACL.
60
Workshop on Monolingual Text-To-Text Generation, pages 84?90,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 84?90,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Paraphrastic Sentence Compression with a Character-based Metric:
Tightening without Deletion
Courtney Napoles1 and Chris Callison-Burch1 and Juri Ganitkevitch1 and
Benjamin Van Durme1,2
1Department of Computer Science
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present a substitution-only approach to
sentence compression which ?tightens? a sen-
tence by reducing its character length. Replac-
ing phrases with shorter paraphrases yields
paraphrastic compressions as short as 60% of
the original length. In support of this task,
we introduce a novel technique for re-ranking
paraphrases extracted from bilingual corpora.
At high compression rates1 paraphrastic com-
pressions outperform a state-of-the-art dele-
tion model in an oracle experiment. For fur-
ther compression, deleting from oracle para-
phrastic compressions preserves more mean-
ing than deletion alone. In either setting, para-
phrastic compression shows promise for sur-
passing deletion-only methods.
1 Introduction
Sentence compression is the process of shortening a
sentence while preserving the most important infor-
mation. Because it was developed in support of ex-
tractive summarization (Knight and Marcu, 2000),
much of the previous work considers deletion-based
models, which extract a subset of words from a long
sentence to create a shorter sentence such that mean-
ing and grammar are maximally preserved. This
framework imposes strict constraints on the task and
does not accurately model human-written compres-
sions, which tend to be abstractive rather than ex-
tractive (Marsi et al, 2010). This is one sense in
which paraphrastic compression can improve exist-
ing compression methodologies.
1Compression rate is defined as the compression length over
original length, so lower values indicate shorter sentences.
We distinguish two non-identical notions of sen-
tence compression: making a sentence substantially
shorter versus ?tightening? a sentence by remov-
ing unnecessary verbiage. We propose a method to
tighten sentences with just substitution and no dele-
tion operations. Using paraphrases extracted from
bilingual text and re-ranked on monolingual data,
our system selects the set of paraphrases that min-
imizes the character length of a sentence.
While not currently the standard, character-based
lengths have been considered before in compres-
sion, and we believe that it is relevant for current
and future applications. Character lengths have been
used for document summarization (DUC 2004, Over
and Yen (2004)), summarizing for mobile devices
(Corston-Oliver, 2001), and subtitling (Glickman et
al., 2006). Although in the past strict word limits
have been imposed for various documents, informa-
tion transmitted electronically is often limited by the
number of bytes, which directly relates to the num-
ber of characters. Mobile devices, SMS messages,
and microblogging sites such as Twitter are increas-
ingly important for quickly spreading information.
In this context, it is important to consider character-
based constraints.
We examine whether paraphrastic compression
allows more information to be conveyed in the same
number of characters as deletion-only compressions.
For example, the length constraint of Twitter posts or
tweets is 140 characters, and many article lead sen-
tences exceed this limit. A paraphrase substitution
oracle compresses the sentence in the table below to
76% of its original length (162 to 123 characters; the
first is the original). The compressed tweet is 140
84
characters, including spaces 17-character shortened
link to the original article.2
Congressional leaders reached a last-gasp agreement
Friday to avert a shutdown of the federal government,
after days of haggling and tense hours of brinksman-
ship.
Congress made a final agreement Fri. to avoid govern-
ment shutdown, after days of haggling and tense hours
of brinkmanship. on.wsj.com/h8N7n1
In contrast, using deletion to compress to the same
length may not be as expressive:
Congressional leaders reached agreement Friday to
avert a shutdown of federal government, after haggling
and tense hours. on.wsj.com/h8N7n1
This work presents a model that makes paraphrase
choices to minimize the character length of a sen-
tence. An oracle paraphrase-substitution experiment
shows that human judges rate paraphrastic compres-
sions higher than deletion-based compressions. To
achieve further compression, we shortened the or-
acle compressions using a deletion model to yield
compressions 80% of the original sentence length
and compared these to compressions generated us-
ing just deletions. Manual evaluation found that
the oracle-then-deletion compressions to preserve
more meaning than deletion-only compressions at
uniform compression rates.
2 Related work
Most of the previous research on sentence compres-
sion focuses on deletion using syntactic informa-
tion, (e.g., Galley and McKeown (2007), Knight
and Marcu (2002), Nomoto (2009), Galanis and An-
droutsopoulos (2010), Filippova and Strube (2008),
McDonald (2006), Yamangil and Shieber (2010),
Cohn and Lapata (2008), Cohn and Lapata (2009),
Turner and Charniak (2005)). Woodsend et al
(2010) incorporate paraphrase rules into a deletion
model. Previous work in subtitling has made one-
word substitutions to decrease character length at
high compression rates (Glickman et al, 2006).
More recent approaches in steganography have used
paraphrase substitution to encode information in text
but focus on grammaticality, not meaning preserva-
tion (Chang and Clark, 2010). Zhao et al (2009) ap-
plied an adaptable paraphrasing pipeline to sentence
2Taken from the main page of http://wsj.com, April 9, 2011.
compression, optimizing for F-measure over a man-
ually annotated set of gold standard paraphrases.
Sentence compression has been considered be-
fore in contexts outside of summarization, such as
headline, title, and subtitle generation (Dorr et al,
2003; Vandeghinste and Pan, 2004; Marsi et al,
2009). Corston-Oliver (2001) deleted characters
from words to shorten the character length of sen-
tences. To our knowledge character-based compres-
sion has not been examined before with the surging
popularity and utility of Twitter.
3 Sentence Tightening
The distinction between tightening and compression
can be illustrated by considering how much space
needs to be preserved. In the case of microblogging,
often a sentence has just a few too many characters
and needs to be ?tightened?. On the other hand, if a
sentence is much longer than a desired length, more
drastic compression is necessary. The first subtask
is relevant in any context with strict word or charac-
ter limits. Some sentences may not be compressible
beyond a certain limit. For example, we found that
near 10% of the compressions generated by Clarke
and Lapata (2008) were identical to the original sen-
tence. In situations where the sentence must meet
a minimum length, tightening can be used to meet
these requirements.
Multi-reference translations provide an instance
of the natural length variation of human-generated
sentences. These translations represent different
ways to express the foreign same sentence, so there
should be no meaning lost between the different ref-
erence translations. The character-based length of
different translations of a given sentence varies on
average by 80% when compared to the shortest sen-
tence in a set.3 This provides evidence that sen-
tences can be tightened to some extent without los-
ing any meaning.
Through the lens of sentence tightening, we con-
sider whether paraphrase substitutions alone can
yield compressions competitive with a deletion at
the same length. A character-based compression
rate is crucial in this framework, as two compres-
3This value will vary by collection and with the number of
references: for example, the NIST05 Arabic reference set has a
mean compression rate of 0.92 with 4 references per set.
85
sions having the same character-based compres-
sion rate may have different word-based compres-
sion rates. The advantage of a character-based sub-
stitution model is in choosing shorter words when
possible, freeing space for more content words. Go-
ing by word length alone would exclude the many
paraphrases with fewer characters than the original
phrase and the same number of words (or more).
3.1 Paraphrase Acquisition
To generate paraphrases for use in our experiments,
we took the approach described by Bannard and
Callison-Burch (2005), which extracts paraphrases
from bilingual parallel corpora. Figure 1 illustrates
the process. A phrase to be paraphrased, like thrown
into jail, is found in a German-English parallel cor-
pus. The corresponding foreign phrase (festgenom-
men) is identified using word alignment and phrase
extraction techniques from phrase-based statistical
machine translation (Koehn et al, 2003). Other oc-
currences of the foreign phrase in the parallel corpus
may align to another English phrase like jailed. Fol-
lowing Bannard and Callison-Burch, we treated any
English phrases that share a common foreign phrase
as potential paraphrases of each other.
As the original phrase occurs several times and
aligns with many different foreign phrases, each of
these may align to a variety of other English para-
phrases. Thus, thrown into jail not only paraphrases
as jailed, but also as arrested, detained, impris-
oned, incarcerated, locked up, taken into custody,
and thrown into prison . Moreover, because the
method relies on noisy and potentially inaccurate
word alignments, it is prone to generate many bad
paraphrases, such as maltreated, thrown, cases, cus-
tody, arrest, owners, and protection.
To rank candidates, Bannard and Callison-Burch
defined the paraphrase probability p(e2|e1) based
on the translation model probabilities p(e|f) and
p(f |e) from statistical machine translation. Follow-
ing Callison-Burch (2008), we refine selection by re-
quiring both the original phrase and paraphrase to
be of the same syntactic type, which leads to more
grammatical paraphrases.
Although many excellent paraphrases are ex-
tracted from parallel corpora, many others are un-
suitable and the translation score does not always
accurately distinguish the two. Therefore, we re-
Paraphrase Monlingual Bilingual
study in detail 1.00 0.70
scrutinise 0.94 0.08
consider 0.90 0.20
keep 0.83 0.03
learn 0.57 0.10
study 0.42 0.07
studied 0.28 0.01
studying it in detail 0.16 0.05
undertook 0.06 0.06
Table 1: Candidate paraphrases for study in detail with
corresponding approximate cosine similarity (Monolin-
gual) and translation model (Bilingual) scores.
ranked our candidates based on monolingual distri-
butional similarity, employing the method described
by Van Durme and Lall (2010) to derive approxi-
mate cosine similarity scores over feature counts us-
ing single token, independent left and right contexts.
Features were computed from the web-scale n-gram
collection of Lin et al (2010). As 5-grams are the
highest order of n-gram in this collection, the al-
lowable set of paraphrases have at most four words
(which allows at least one word of context).
To our knowledge this is the first time such tech-
niques have been used in combination in order to
derive higher quality paraphrase candidates. See Ta-
ble 1 for an example.
The monolingual-filtering technique we describe
is by no means limited to paraphrases extracted from
bilingual corpora. It could be applied to other data-
driven paraphrasing techniques (see Madnani and
Dorr (2010) for a survey). Although it is particularly
well suited to the bilingual extracted corpora, since
the information that it adds is orthogonal to that
model, it would presumably add less to paraphras-
ing techniques that already take advantage of mono-
lingual distributional similarity (Pereira et al, 1993;
Lin and Pantel, 2001; Barzilay and Lee, 2003).
In order to evaluate the paraphrase candidates
and scoring techniques, we randomly selected 1,000
paraphrase sets where the source phrase was present
in the corpus described in Clarke and Lapata (2008).
For each phrase and set of candidate paraphrases, we
extracted all of the contexts from the corpus in which
the source phrase appeared. Human judges were
presented each sentence with the original phrase and
the same sentences with each paraphrase candidate
86
... letzteWoche wurden in Irland f?nf Landwirte festgenommen , weil sie verhindern wollten
... last week five farmers were thrown into jail in Ireland because they resisted ...
...
Zahlreiche Journalisten sind verschwunden oder wurden festgenommen , gefoltert und get?tet .
Quite a few journalists have disappeared or have been imprisoned , tortured and killed .
Figure 1: Using a bilingual parallel corpus to extract paraphrases.
substituted in. Each paraphrase substitution was
graded based on the extent to which it preserved
the meaning and affected the grammaticality of the
sentence. While both the bilingual translation score
and monolingual cosine similarity positively corre-
lated with human judgments, the monolingual score
proved a stronger predictor of quality in both dimen-
sions. Using Kendall?s tau correlation coefficient,
the agreement between the ranking imposed by the
monolingual score and human ratings surpassed that
of the original ranking as derived during the bilin-
gual extraction, for both meaning and grammar.4 In
our substitution framework, we ignore the transla-
tion probabilities and use only the approximate co-
sine similarity in the paraphrase decision task.
4 Framework for Sentence Tightening
Our sentence tightening approach uses a dynamic
programming strategy to find the combination of
non-overlapping paraphrases that minimizes a sen-
tence?s character length. The threshold of the mono-
lingual score for paraphrases can be varied to widen
or narrow the search space, which may be further in-
creased by considering any lexical paraphrases not
subject to syntactic constraints. Sentences with a
compression rate as low as 0.6 can be generated
without thresholding the paraphrase scores. Because
the system can generate multiple paraphrased sen-
tences of equal length, we apply two layers of filter-
ing to generate a single output. First we calculate a
word-overlap score between the original and candi-
date sentences to favor compressions similar to the
original sentence; then, from among the sentences
4For meaning and grammar respectively, ? = 0.28 and 0.31
for monolingual scores and 0.19 and 0.15 for bilingual scores.
with the highest word overlap, we select the com-
pression with the best language model score.
Higher paraphrase thresholds guarantee more ap-
propriate paraphrases but yield longer compressions.
Using a cosine-similarity threshold of 0.95, the av-
erage compression rate is 0.968, which is consider-
ably longer than the compressions using no thresh-
old (0.60). In these experiments we did not syntac-
tically constrain paraphrases. However, we believe
that our monolingual refining of paraphrase sets im-
proves paraphrase selection and is a reasonable al-
ternative to using syntactic constraints.
In case judges favor compressions that have high
word overlap with the original sentence, we com-
pressed the longest sentence from each set of ref-
erence translations (Huang et al, 2002) and ran-
domly chose a sentence from the set of reference
translations to use as the standard for comparison.
Paraphrastic compressions were generated at cosine-
similarity thresholds ranging from 0.60 to 0.95.
We implemented a state-of-the-art deletion model
(Clarke and Lapata, 2008) to generate deletion-only
compressions. We fixed the compression length
to ?5 characters of the length of each paraphras-
tic compression, in order to isolate the compression
quality from the effect of compression rate (Napoles
et al, 2011). Manual evaluation used Amazon?s
Mechanical Turk with three-way redundancy and
positive and negative controls to filter bad workers.
Meaning and grammar judgments were collected us-
ing two 5-point scales (5 being the highest score).
5 Evaluation
The initial results of our substitution system show
room for improvement in future work (Table 2). We
believe this is due to erroneous paraphrase substi-
87
System Grammar Meaning CompR Cos.
Substitution 3.8 3.7 0.97 0.95
Deletion 4.1 4.0 0.97 -
Substitution 3.4 3.2 0.89 0.85
Deletion 4.0 3.8 0.89 -
Substitution 3.1 3.0 0.85 0.75
Deletion 3.9 3.7 0.85 -
Substitution 2.9 2.9 0.82 0.65
Deletion 3.8 3.5 0.82 -
Table 2: Mean ratings of compressions using just deletion
or substitution at different paraphrase thresholds (Cos.).
Deletion performed better in all settings.
tutions, since phrases with the same syntactic cate-
gory and distributional similarity are not necessarily
semantically identical. Illustrative examples include
WTO for United Nations and east or west for south.
Because the quality of the multi-reference transla-
tions is not uniformly high, for the following exper-
iment we used a dataset of English newspaper arti-
cles.
To control against these errors and test the viabil-
ity of a substitution-only approach, we generated all
possible paraphrase substitutions above a threshold
of 0.80 within a set of 20 randomly chosen sentences
from the written corpus of Clarke and Lapata (2008).
We solicited humans to make a ternary decision of
whether a paraphrase was acceptable in the context
(good, bad, or not sure). We applied our model to
generate compressions using only paraphrase substi-
tutions on which all three annotators agreed that the
paraphrase was good. The oracle generated com-
pressions with an average compression rate of 0.90.
On the same set of original sentences, we used
the deletion model to generate compressions con-
strained to ?5 characters of the length of the ora-
cle compression. Next, we examined whether apply-
ing the deletion model to paraphrastic compressions
would improve compression quality. In manual eval-
uation along the dimensions of grammar and mean-
ing, both the oracle compressions and oracle-plus-
deletion compressions outperformed the deletion-
only compressions at uniform lengths (Table 3)5.
These results suggest that improvements in para-
phrase acquisition will make our system competitive
with deletion-only models.
5Paraphrastic compressions were rated significantly higher
for meaning, p < 0.05
Model Grammar Meaning CompR
Oracle 4.1 4.3 0.90
Deletion 4.0 4.1 0.90
Gold 4.3 3.8 0.75
Oracle+deletion 3.4 3.7 0.80
Deletion 3.2 3.4 0.80
Table 3: Mean ratings of compressions generated by a
substitution oracle, deletion only, deletion on the oracle
compression, and the gold standard. Being able to choose
the best paraphrases would enable our substitution model
to outperform the deletion model.
6 Conclusion
This work shows promise for the use of only sub-
stitution in the task of sentence tightening. There
are myriad possible extensions and improvements
to this method, most notably richer features be-
yond paraphrase length. We do not currently use
syntactic information in our paraphrastic compres-
sion model because it places limits on the number
of paraphrases available for a sentence and thereby
limits the possible compression rate. The current
method for paraphrase extraction does not include
certain types of rewriting, such as passivization, and
should be extended to incorporate even more short-
ening paraphrases. Future work can directly apply
these methods to Twitter and extract additional para-
phrases and abbreviations from Twitter and/or SMS
data. Our substitution approach can be improved by
applying more sophisticated techniques to choosing
the best candidate compression, or by framing it as
an optimization problem over more than just mini-
mal length. Overall, we find these results to be en-
couraging for the possibility of sentence compres-
sion without deletion.
Acknowledgments
We are grateful to John Carroll for helping us obtain
the RASP parser. This research was partially funded
by the JHU Human Language Technology Center of
Excellence. This research was funded in part by the
NSF under grant IIS-0713448. The views and find-
ings are the authors? alone.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
88
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Ching-Yun Chang and Stephen Clark. 2010. Linguis-
tic steganography using automatically generated para-
phrases. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
591?599. Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399?429.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Simon Corston-Oliver. 2001. Text compaction for dis-
play on very small screens. In Proceedings of the
NAACL Workshop on Automatic Summarization.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generation. In Proceedings of the HLT-NAACL
Workshop on Text summarization Workshop.
Katja Filippova and Michael Strube. 2008. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference. Association for Computational Linguis-
tics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen R. McKeown. 2007. Lex-
icalized Markov grammars for sentence compression.
the Proceedings of NAACL/HLT.
Oren Glickman, Ido Dagan, Mikaela Keller, Samy Ben-
gio, and Walter Daelemans. 2006. Investigating lexi-
cal substitution scoring for subtitle generation. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning, pages 45?52. Associa-
tion for Computational Linguistics.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization ? Step one: Sentence compression. In
Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343?360.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New Tools for Web-Scale
N-grams. In Proceedings of LREC.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Walter
Daelemans. 2009. Is sentence compression an NLG
task? In Proceedings of the 12th European Workshop
on Natural Language Generation.
Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Walter
Daelemans. 2010. On the limits of sentence com-
pression by deletion. Empirical Methods in Natural
Language Generation, pages 45?66.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In In Proceed-
ings of EACL.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence compres-
sion: Pitfalls and suggested remedies. In Proceedings
of ACL, Workshop on Monolingual Text-To-Text Gen-
eration.
Tadashi Nomoto. 2009. A comparison of model free ver-
sus model intensive approaches to sentence compres-
sion. In Proceedings of EMNLP.
Paul Over and James Yen. 2004. An introduction to
DUC 2004: Intrinsic evaluation of generic news text
summarization systems. In Proceedings of DUC 2004
Document Understanding Workshop, Boston.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In ACL-93.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.
Vincent Vandeghinste and Yi Pan. 2004. Sentence com-
pression for automated subtitling: A hybrid approach.
In Proceedings of the ACL workshop on Text Summa-
rization.
89
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Generation with quasi-synchronous grammar.
In Proceedings of EMNLP.
Elif Yamangil and Stuart M. Shieber. 2010. Bayesian
synchronous tree-substitution grammar induction and
its application to sentence compression. In Proceed-
ings of ACL.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
90
Workshop on Monolingual Text-To-Text Generation, pages 91?97,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 91?97,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Evaluating sentence compression: Pitfalls and suggested remedies
Courtney Napoles1 and Benjamin Van Durme1,2 and Chris Callison-Burch1
1Department of Computer Science
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
This work surveys existing evaluation
methodologies for the task of sentence
compression, identifies their shortcomings,
and proposes alternatives. In particular,
we examine the problems of evaluating
paraphrastic compression and comparing the
output of different models. We demonstrate
that compression rate is a strong predictor
of compression quality and that perceived
improvement over other models is often a side
effect of producing longer output.
1 Introduction
Sentence compression is the natural language gen-
eration (NLG) task of automatically shortening sen-
tences. Because good compressions should be gram-
matical and retain important meaning, they must be
evaluated along these two dimensions. Evaluation is
a difficult problem for NLG, and many of the prob-
lems identified in this work are relevant for other
generation tasks. Shared tasks are popular in many
areas as a way to compare system performance in an
unbiased manner. Unlike other tasks, such as ma-
chine translation, there is no shared-task evaluation
for compression, even though some compression
systems are indirectly evaluated as a part of DUC.
The benefits of shared-task evaluation have been dis-
cussed before (e.g., Belz and Kilgarriff (2006) and
Reiter and Belz (2006)), and they include compar-
ing systems fairly under the same conditions.
One difficulty in evaluating compression systems
fairly is that an unbiased automatic metric is hard
to define. Automatic evaluation relies on a com-
parison to a single gold standard at a predetermined
length, which greatly limits the types of compres-
sions that can be fairly judged. As we will discuss
in Section 2.1.1, automatic evaluation assumes that
deletions are independent, considers only a single
gold standard, and cannot handle compressions with
paraphrasing. Like for most areas in NLG, human
evaluation is preferable. However, as we discuss in
Section 2.2, there are some subtleties to appropri-
ate experiment design, which can give misleading
results if not handled properly.
This work identifies the shortcomings of widely
practiced evaluation methodologies and proposes al-
ternatives. We report on the effect of compression
rate on perceived quality and suggest ways to control
for this dependency when evaluating across different
systems. In this work we:
? highlight the importance of comparing systems
with similar compression rates,
? argue that comparisons in many previous pub-
lications are invalid,
? provide suggestions for unbiased evaluation.
While many may find this discussion intuitive, these
points are not addressed in much of the existing re-
search, and therefore it is crucial to enumerate them
in order to improve the scientific validity of the task.
2 Current Practices
Because it was developed in support of extractive
summarization (Knight and Marcu, 2000), com-
pression has mostly been framed as a deletion task
(e.g., McDonald (2006), Galanis and Androutsopou-
los (2010), Clarke and Lapata (2008), and Galley
91
Words Sentence
31 Kaczynski faces charges contained in a 10-count federal indictment naming him as the person responsible for trans-
porting bombs and bomb parts from Montana to California and mailing them to victims .
17 Kaczynski faces charges naming him responsible for transporting bombs to California and mailing them to victims .
18 Kaczynski faces charges naming him responsible for transporting bombs and bomb parts and mailing them to victims .
18 Kaczynski faces a 10-count federal indictment for transporting bombs and bomb parts and mailing them to victims .
Table 1: Three acceptable compressions of a sentence created by different annotators (the first is the original).
and McKeown (2007)). In this context, a compres-
sion is an extracted subset of words from a long
sentence. There are limited compression corpora
because, even when an aligned corpus exists, the
number of extractive sentence pairs will be few and
therefore gold-standard compressions must be man-
ually annotated. The most popular corpora are the
Ziff-Davis corpus (Knight and Marcu, 2000), which
contains a small set of 1067 extracted sentences
from article/abstract pairs, and the manually anno-
tated Clarke and Lapata (2008) corpus, consisting of
nearly 3000 sentences from news articles and broad-
cast news transcripts. These corpora contain one
gold standard for each sentence.
2.1 Automatic Techniques
One of the most widely used automatic metrics is the
F1 measure over grammatical relations of the gold-
standard compressions (Riezler et al, 2003). This
metric correlates significantly with human judg-
ments and is better than Simple String Accuracy
(Bangalore et al, 2000) for judging compression
quality (Clarke and Lapata, 2006). F1 has also been
used over unigrams (Martins and Smith, 2009) and
bigrams (Unno et al, 2006). Unno et al (2006)
compared the F1 measures to BLEU scores (using
the gold standard as a single reference) over vary-
ing compression rates, and found that BLEU be-
haves similarly to both F1 measures. A syntactic
approach considers the alignment over parse trees
(Jing, 2000), and a similar technique has been used
with dependency trees to evaluate the quality of sen-
tence fusions (Marsi and Krahmer, 2005).
The only metric that has been shown to correlate
with human judgments is F1 (Clarke and Lapata,
2006), but even this is not entirely reliable. F1 over
grammatical relations also depends on parser accu-
racy and the type of dependency relations used.1
1For example, the RASP parser uses 16 grammatical depen-
2.1.1 Pitfalls of Automatic Evaluation
Automatic evaluation operates under three often
incorrect assumptions:
Deletions are independent. The dependency
structure of a sentence may be unaltered when de-
pendent words are not deleted as a unit. Examples
of words that should be treated as a single unit in-
clude negations and negative polarity items or cer-
tain multi-word phrases (such as deleting Latin and
leaving America). F1 treats all deletions equally,
when in fact errors of this type may dramatically al-
ter the meaning or the grammaticality of a sentence
and should be penalized more than less serious er-
rors, such as deleting an article.
The gold standard is the single best compres-
sion. Automatic evaluation considers a single
gold-standard compression. This ignores the pos-
sibility of different length compressions and equally
good compressions of the same length, where mul-
tiple non-overlapping deletions are acceptable. For
an example, see Table 1.
Having multiple gold standards would provide
references at different compression lengths and re-
flect different deletion choices (see Section 3). Since
no large corpus with multiple gold standards exists
to our knowledge, systems could instead report the
quality of compressions at several different com-
pression rates, as Nomoto (2008) did. Alternatively,
systems could evaluate compressions that are of a
similar length as the gold standard compression, to
fix a length for the purpose of evaluation. Output
length is controlled for evaluation in some other ar-
eas, notably DUC.
Systems compress by deletion and not substitu-
tion. More recent approaches to compression in-
troduce reordering and paraphrase operations (e.g.,
dencies (Briscoe, 2006) while there are over 50 Stanford De-
pendencies (de Marneffe and Manning, 2008).
92
Cohn and Lapata (2008), Woodsend et al (2010),
and Napoles et al (2011)). For paraphrastic com-
pressions, manual evaluation alone reliably deter-
mines the compression quality. Because automatic
evaluation metrics compare shortened sentences to
extractive gold standards, they cannot be applied to
paraphrastic compression.
To apply automatic techniques to substitution-
based compression, one would need a gold-standard
set of paraphrastic compressions. These are rare.
Cohn and Lapata (2008) created an abstractive cor-
pus, which contains word reordering and paraphras-
ing in addition to deletion. Unfortunately, this cor-
pus is small (575 sentences) and only includes one
possible compression for each sentence.
Other alternatives include deriving such corpora
from existing corpora of multi-reference transla-
tions. The longest reference translation can be
paired with the shortest reference to represent a
long sentence and corresponding paraphrased gold-
standard compression.
Similar to machine translation or summarization,
automatic translation of paraphrastic compressions
would require multiple references to capture allow-
able variation, since there are often many equally
valid ways of compressing an input. ROUGE
or BLEU could be applied to a set of multiple-
reference compressions, although BLEU is not with-
out its own shortcomings (Callison-Burch et al,
2006). One benefit of both ROUGE and BLEU is
that they are based on n-gram recall and precision
(respectively) instead of word-error rate, so reorder-
ing and word substitutions can be evaluated. Dorr et
al. (2003) used BLEU for evaluation in the context
of headline generation, which uses rewording and
is related to sentence compression. Alternatively,
manual evalation can be adapted from other NLG
domains, such as the techniques described in the fol-
lowing section.
2.2 Manual Evaluation
In order to determine semantic and syntactic suit-
ability, manual evaluation is preferable over au-
tomatic techniques whenever possible. The most
widely practiced manual evaluation methodology
was first used by Knight and Marcu (2002). Judges
grade each compressed sentence against the original
and make two separate decisions: how grammatical
is the compression and how much of the meaning
from the original sentence is preserved. Decisions
are rated along a 5-point scale (LDC, 2005).
Most compression systems consider sentences out
of context (a few exceptions exist, e.g., Daume? III
and Marcu (2002), Martins and Smith (2009), and
Lin (2003)). Contextual cues and discourse struc-
ture may not be a factor to consider if the sentences
are generated for use out of context. An example
of a context-aware approach considered the sum-
maries formed by shortened sentences and evalu-
ated the compression systems based on how well
people could answer questions about the original
document from the summaries (Clarke and Lapata,
2007). This technique has been used before for
evaluating summarization and text comprehension
(Mani et al, 2002; Morris et al, 1992).
2.2.1 Pitfalls of Manual Evaluation
Grammar judgments decrease when the compres-
sion is presented alongside the original sentence.
Figure 1 shows that the mean grammar rating for the
same compressions is on average about 0.3 points
higher when the compression is judged in isolation.
Researchers should be careful to state when gram-
mar is judged on compressions lacking reference
sentences.
Another factor is the group of judges. Obvi-
ously different studies will rely on different judges,
so whenever possible the sentences from an exist-
ing model should be re-evaluated alongside the new
model. The ?McD? entries in Table 2 represent a set
of sentences generated from the exact same model
evaluated by two different sets of judges. The mean
grammar and meaning ratings in each evaluation
setup differ by 0.5?0.7 points.
3 Compression Rate Predicts Performance
The dominant assumption in compression research
is that the system makes the determination about the
optimal compression length. For this reason, com-
pression rates can vary drastically across systems. In
order to get unbiased evaluations, systems should be
compared only when they are compressing at similar
rates.
Compression rate is defined as:
# of tokens in compressed sentence
# of tokens in original sentence
? 100 (1)
93
CR
Mean
ing
1
2
3
4
5
l l
l l
l l
l l
0 20 40 60 80 100
Modell DeletionGold
CR
Gram
mar
1
2
3
4
5
l l
l l
l
l l
l
0 20 40 60 80 100
Modell DeletionGold.1Gold.2
Figure 1: Compression rate strongly correlates with human judgments of meaning and grammaticality. Gold represents
gold-standard compression and Deletion the results of a leading deletion model. Gold.1 grammar judgments were
made alongside the original sentence and Gold.2 were made in isolation.
It seems intuitive that sentence quality diminishes
in relation to the compression rate. Each word
deleted increases the probability that errors are intro-
duced. To verify this notion, we generated compres-
sions at decreasing compression rates of 250 sen-
tences randomly chosen from the written corpus of
Clarke and Lapata (2008), generated by our imple-
mentation of a leading extractive compression sys-
tem (Clarke and Lapata, 2008). We collected hu-
man judgments using the 5-point scales of meaning
and grammar described above. Both quality judg-
ments decreased linearly with the compression rate
(see ?Deletion? in Figure 1).
As this behavior could have been an artifact of
the particular model employed, we next developed
a unique gold-standard corpus for 50 sentences se-
lected at random from the same corpus described
above. The authors manually compressed each sen-
tence at compression rates ranging from less than
10 to 100. Using the same setup as before, we
collected human judgments of these gold standards
to determine an upper bound of perceived quality
at a wide range of compression rates. Figure 1
demonstrates that meaning and grammar ratings de-
cay more drastically at compression rates below 40
(see ?Gold?). Analysis suggests that humans are of-
ten able to practice ?creative deletion? to tighten a
sentence up to a certain point, before hitting a com-
pression barrier, shortening beyond which leads to
significant meaning and grammatically loss.
4 Mismatched Comparisons
We have observed that a difference in compression
rates as small as 5 percentage points can influence
the quality ratings by as much as 0.1 points and
conclude: systems must be compared using simi-
lar levels of compression. In particular, if system
A?s output is higher quality, but longer than system
B?s, then it is not necessarily the case that A is better
than B. Conversely, if B has results at least as good
as system A, one can claim that B is better, since B?s
output is shorter.
Here are some examples in the literature of mis-
matched comparisons:
? Nomoto (2009) concluded their system signif-
icantly outperformed that of Cohn and Lapata
(2008). However, the compression rate of their
system ranged from 45 to 74, while the com-
pression rate of Cohn and Lapata (2008) was
35. This claim is unverifiable without further
comparison.
? Clarke and Lapata (2007), when comparing
against McDonald (2006), reported signifi-
cantly better results at a 5-point higher com-
pression rate. At first glance, this does not
seem like a remarkable difference. However,
94
Model Meaning Grammar CompR
C&L 3.83 3.66 64.1
McD 3.94 3.87 64.2
C&L 3.76? 3.53? 78.4?
McD 3.50? 3.17? 68.5?
Table 2: Mean quality ratings of two competing mod-
els once the compression rates have been standardized,
and as reported in the original work (denoted ?). There
is no significant improvement, but the numerically better
model changes.
the study evaluated the quality of summaries
containing automatically shortened sentences.
The average document length in the test set was
20 sentences, and with approximately 24 words
per sentence, a typical 65.4% compressed doc-
ument would have 80 more words than a typical
60.1% McDonald compression. The aggregate
loss from 80 words can be considerable, which
suggests that this comparison is inconclusive.
We re-evaluated the model described in Clarke
and Lapata (2008) (henceforth C&L) against the
McDonald (2006) model with global constraints, but
fixed the compression rates to be equal. We ran-
domly selected 100 sentences from that same cor-
pus and generated compressions with the same com-
pression rate as the sentences generated by the Mc-
Donald model (McD), using our implementation of
C&L. Although not statistically significant, this new
evaluation reversed the polarity of the results re-
ported by Clarke and Lapata (Table 2). This again
stresses the importance of using similar compression
rates to draw accurate conclusions about different
models.
An example of unbiased evaluation is found in
Cohn and Lapata (2009). In this work, their model
achieved results significantly better than a compet-
ing system (McDonald, 2006). Recognizing that
their compression rate was about 15 percentage
points higher than the competing system, they fixed
the target compression rate to one similar to McDon-
ald?s output, and still found significantly better per-
formance using automatic measures. This work is
one of the few that controls their output length in
order to make an objective comparison (another ex-
ample is found in McDonald (2006)), and this type
of analysis should be emulated in the future.
5 Suggestions
Models should be tested on the same corpus, be-
cause different corpora will likely have different fea-
tures that make them easier or harder to compress. In
order to make non-vacuous comparisons of different
models, a system also needs to be constrained to pro-
duce the same length output as another system, or
report results at least as good for shorter compres-
sions. Using the multi-reference gold-standard col-
lection described in Section 3, relative performance
could be estimated through comparison to the gold-
standard curve. The reference set we have annotated
is yet small, but this is an area for future work based
on feedback from the community.2
Other methods for limiting quality disparities in-
troduced by the compression rate include fixing the
target length to that of the gold standard (e.g., Unno
et al (2006)). Alternately, results for a system at
varying compression levels can be reported,3 allow-
ing for comparisons at similar lengths. This is a
practice to be emulated, if possible, because systems
that cannot control output length can make compar-
isons against the appropriate compression rate.
In conclusion, we have provided justification for
the following practices in evaluating compressions:
? Compare systems at similar compression rates.
? Provide results across multiple compression
rates when possible.
? Report that system A surpasses B iff: A and
B have the same compression rate and A does
better than B, or A produces shorter output than
B and A does at least as well B.
? New corpora for compression should have mul-
tiple gold standards for each sentence.
Acknowledgments
We are very grateful to James Clarke for helping us
obtain the results of existing systems and to the re-
viewers for their helpful comments and recommen-
dations. The first author was supported by the JHU
Human Language Technology Center of Excellence.
This research was funded in part by the NSF under
grant IIS-0713448. The views and findings are the
authors? alone.
2This data is available on request.
3For example, Nomoto (2008) reported results ranging over
compression rates: 0.50?0.70.
95
References
Srinivas Bangalore, Owen Rambow, and Steve Whittaker.
2000. Evaluation metrics for generation. In Proceed-
ings of the first international conference on Natural
language generation-Volume 14, pages 1?8. Associa-
tion for Computational Linguistics.
A. Belz and A. Kilgarriff. 2006. Shared-task eval-
uations in HLT: Lessons for NLG. In Proceedings
of the Fourth International Natural Language Gen-
eration Conference, pages 133?135. Association for
Computational Linguistics.
Ted Briscoe. 2006. An introduction to tag sequence
grammars and the RASP system parser. Computer
Laboratory Technical Report, 662.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In Proceedings of EACL,
Trento, Italy.
James Clarke and Mirella Lapata. 2006. Models for
sentence compression: A comparison across domains,
training requirements and evaluation measures. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
pages 377?384. Association for Computational Lin-
guistics.
James Clarke and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1?11.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399?429.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 449?456. Association for
Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generation. In Proceedings of the HLT-NAACL
Workshop on Text summarization Workshop.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen R. McKeown. 2007. Lex-
icalized Markov grammars for sentence compression.
the Proceedings of NAACL/HLT.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the sixth con-
ference on Applied natural language processing, pages
310?315. Association for Computational Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization ? Step one: Sentence compression. In
Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression: a pilot study. In
Proceedings of the sixth international workshop on In-
formation retrieval with Asian languages-Volume 11,
pages 1?8. Association for Computational Linguistics.
Inderjeet Mani, Gary Klein, David House, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
2002. SUMMAC: a text summarization evaluation.
Natural Language Engineering, 8(01):43?68.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proceedings of the European
Workshop on Natural Language Generation, pages 8?
10.
Andre? F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Pro-
cessing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In In Proceed-
ings of EACL.
Andrew H. Morris, George M. Kasper, and Dennis A.
Adams. 1992. The effects and limitations of auto-
mated text condensing on reading comprehension per-
formance. INFORMATION SYSTEMS RESEARCH,
3(1):17?35.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphrastic
sentence compression with a character-based metric:
Tightening without deletion. In Proceedings of ACL,
Workshop on Monolingual Text-To-Text Generation.
96
Tadashi Nomoto. 2008. A generic sentence trimmer with
CRFs. Proceedings of ACL-08: HLT, pages 299?307.
Tadashi Nomoto. 2009. A comparison of model free ver-
sus model intensive approaches to sentence compres-
sion. In Proceedings of EMNLP.
E. Reiter and A. Belz. 2006. GENEVAL: A proposal
for shared-task evaluation in NLG. In Proceedings
of the Fourth International Natural Language Gen-
eration Conference, pages 136?138. Association for
Computational Linguistics.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 118?125. Association for Computational Lin-
guistics.
Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and
Jun?ichi Tsujii. 2006. Trimming CFG parse trees
for sentence compression using machine learning ap-
proaches. In Proceedings of the COLING/ACL on
Main conference poster sessions, pages 850?857. As-
sociation for Computational Linguistics.
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Generation with quasi-synchronous grammar.
In Proceedings of EMNLP.
97
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 22?64,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Findings of the 2011 Workshop on Statistical Machine Translation
Chris Callison-Burch
Center for Language and Speech Processing
Johns Hopkins University
Philipp Koehn
School of Informatics
University of Edinburgh
Christof Monz
Informatics Institute
University of Amsterdam
Omar F. Zaidan
Center for Language and Speech Processing
Johns Hopkins University
Abstract
This paper presents the results of the WMT11
shared tasks, which included a translation
task, a system combination task, and a task for
machine translation evaluation metrics. We
conducted a large-scale manual evaluation of
148 machine translation systems and 41 sys-
tem combination entries. We used the rank-
ing of these systems to measure how strongly
automatic metrics correlate with human judg-
ments of translation quality for 21 evaluation
metrics. This year featured a Haitian Creole
to English task translating SMS messages sent
to an emergency response service in the af-
termath of the Haitian earthquake. We also
conducted a pilot ?tunable metrics? task to test
whether optimizing a fixed system to differ-
ent metrics would result in perceptibly differ-
ent translation quality.
1 Introduction
This paper presents the results of the shared tasks
of the Workshop on statistical Machine Translation
(WMT), which was held at EMNLP 2011. This
workshop builds on five previous WMT workshops
(Koehn and Monz, 2006; Callison-Burch et al,
2007; Callison-Burch et al, 2008; Callison-Burch
et al, 2009; Callison-Burch et al, 2010). The work-
shops feature three shared tasks: a translation task
between English and other languages, a task to com-
bine the output of multiple machine translation sys-
tems, and a task to predict human judgments of
translation quality using automatic evaluation met-
rics. The performance for each of these shared tasks
is determined through a comprehensive human eval-
uation. There were a two additions to this year?s
workshop that were not part of previous workshops:
? Haitian Creole featured task ? In addition to
translation between European language pairs,
we featured a new translation task: translating
Haitian Creole SMS messages that were sent
to an emergency response hotline in the im-
mediate aftermath of the 2010 Haitian earth-
quake. The goal of this task is to encourage re-
searchers to focus on challenges that may arise
in future humanitarian crises. We invited Will
Lewis, Rob Munro and Stephan Vogel to pub-
lish a paper about their experience developing
translation technology in response to the crisis
(Lewis et al, 2011). They provided the data
used in the Haitian Creole featured translation
task. We hope that the introduction of this new
dataset will provide a testbed for dealing with
low resource languages and the informal lan-
guage usage found in SMS messages.
? Tunable metric shared task ? We conducted
a pilot of a new shared task to use evaluation
metrics to tune the parameters of a machine
translation system. Although previous work-
shops have shown evaluation metrics other than
BLEU are more strongly correlated with human
judgments when ranking outputs from multiple
systems, BLEU remains widely used by system
developers to optimize their system parameters.
We challenged metric developers to tune the
parameters of a fixed system, to see if their met-
rics would lead to perceptibly better translation
quality for the system?s resulting output.
22
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dissem-
inate common test sets and public training data with
published performance numbers, and to refine eval-
uation methodologies for machine translation. As
with previous workshops, all of the data, transla-
tions, and collected human judgments are publicly
available.1 We hope these datasets form a valuable
resource for research into statistical machine transla-
tion, system combination, and automatic evaluation
of translation quality.
2 Overview of the Shared Translation and
System Combination Tasks
The recurring task of the workshop examines trans-
lation between English and four other languages:
German, Spanish, French, and Czech. We created a
test set for each language pair by translating newspa-
per articles. We additionally provided training data
and two baseline systems.
2.1 Test data
The test data for this year?s task was created by
hiring people to translate news articles that were
drawn from a variety of sources from early Decem-
ber 2010. A total of 110 articles were selected, in
roughly equal amounts from a variety of Czech, En-
glish, French, German, and Spanish news sites:2
Czech: aktualne.cz (4), Novinky.cz (7), iH-
Ned.cz (4), iDNES.cz (4)
French: Canoe (5), Le Devoir (5), Le Monde (5),
Les Echos (5), Liberation (5)
Spanish: ABC.es (6), Cinco Dias (6), El Period-
ico (6), Milenio (6), Noroeste (7)
English: Economist (4), Los Angeles Times (6),
New York Times (4), Washington Post (4)
German: FAZ (3), Frankfurter Rundschau (2), Fi-
nancial Times Deutschland (3), Der Spie-
gel (5), Su?ddeutsche Zeitung (3)
The translations were created by the professional
translation agency CEET.3 All of the translations
1http://statmt.org/wmt11/results.html
2For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
3http://www.ceet.eu/
were done directly, and not via an intermediate lan-
guage.
Although the translations were done profession-
ally, in some cases errors still cropped up. For in-
stance, in parts of the English-French translations,
some of the English source remains in the French
reference as if the translator forgot to delete it.
2.2 Training data
As in past years we provided parallel corpora to train
translation models, monolingual corpora to train lan-
guage models, and development sets to tune system
parameters. Some statistics about the training mate-
rials are given in Figure 1.
2.3 Baseline systems
To lower the barrier of entry for newcomers to
the field, we provided two open source toolkits for
phrase-based and parsing-based statistical machine
translation (Koehn et al, 2007; Li et al, 2010).
2.4 Submitted systems
We received submissions from 56 groups across 37
institutions, as listed in Tables 1, 2 and 3. We also
included two commercial off-the-shelf MT systems,
two online statistical MT systems, and five online
rule-based MT systems. (Not all systems supported
all language pairs.) We note that these nine compa-
nies did not submit entries themselves, and are there-
fore anonymized in this paper. Rather, their entries
were created by translating the test data via their web
interfaces.4 The data used to construct these systems
is not subject to the same constraints as the shared
task participants. It is possible that part of the refer-
ence translations that were taken from online news
sites could have been included in the online systems?
models, for instance. We therefore categorize all
commercial systems as unconstrained when evalu-
ating the results.
2.5 System combination
In total, we had 148 primary system entries (includ-
ing the 46 entries crawled from online sources), and
60 contrastive entries. These were made available to
4We would like to thank Ondr?ej Bojar for harvesting the
commercial entries (2), Christian Federmann for the statistical
MT entries (14), and Herve? Saint-Amand for the rule-based MT
entries (30)!
23
Europarl Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,786,594 1,825,077 1,739,154 462,351
Words 51,551,370 49,411,045 54,568,499 50,551,047 45,607,269 47,978,832 10,573,983 12,296,772
Distinct words 171,174 113,655 137,034 114,487 362,563 111,934 152,788 56,095
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 132,571 115,562 136,227 122,754
Words 3,739,293 3,285,305 3,290,280 2,866,929 3,401,766 3,309,619 2,658,688 2,951,357
Distinct words 73,906 53,699 59,911 50,323 120,397 53,921 130,685 50,457
United Nations Training Corpus
Spanish? English French? English
Sentences 10,662,993 12,317,600
Words 348,587,865 304,724,768 393,499,429 344,026,111
Distinct words 578,599 564,489 621,721 729,233
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 7,227,409
Words 72,993,427 84,856,749
Distinct words 1,088,642 522,770
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,032,006 1,942,761 2,002,266 1,985,560 479,636
Words 54,720,731 55,105,358 57,860,307 48,648,697 10,770,230
Distinct words 119,315 176,896 141,742 376,128 154,129
News Language Model Data
English Spanish French German Czech
Sentence 30,888,595 3,416,184 11,767,048 17,474,133 12,333,268
Words 777,425,517 107,088,554 302,161,808 289,171,939 216,692,489
Distinct words 2,020,549 595,681 1,250,259 3,091,700 2,068,056
News Test Set
English Spanish French German Czech
Sentences 3003
Words 75,762 79,710 85,999 73,729 65,427
Distinct words 10,088 11,989 11,584 14,345 16,922
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of
distinct words (case-insensitive) is based on the provided tokenizer.
24
ID Participant
ALACANT University of Alicante (Sa?nchez-Cartagena et al, 2011)
CEU-UPV CEU University Cardenal Herrera
& Polytechnic University of Valencia (Zamora-Martinez and Castro-Bleda, 2011)
CMU-DENKOWSKI Carnegie Mellon University - Denkowski (Denkowski and Lavie, 2011b)
CMU-DYER Carnegie Mellon University - Dyer (Dyer et al, 2011)
CMU-HANNEMAN Carnegie Mellon University - Hanneman (Hanneman and Lavie, 2011)
COPENHAGEN Copenhagen Business School
CST Centre for Language Technology @ Copenhagen University (Rish?j and S?gaard, 2011)
CU-BOJAR Charles University - Bojar (Marec?ek et al, 2011)
CU-MARECEK Charles University - Marec?ek (Marec?ek et al, 2011)
CU-POPEL Charles University - Popel (Popel et al, 2011)
CU-TAMCHYNA Charles University - Tamchyna (Bojar and Tamchyna, 2011)
CU-ZEMAN Charles University - Zeman (Zeman, 2011)
DFKI-FEDERMANN Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz - Federmann
(Federmann and Hunsicker, 2011)
DFKI-XU Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz - Xu (Xu et al, 2011b)
HYDERABAD IIIT-Hyderabad
ILLC-UVA Institute for Logic, Language and Computation @ University of Amsterdam
(Khalilov and Sima?an, 2011)
JHU Johns Hopkins University (Weese et al, 2011)
KIT Karlsruhe Institute of Technology (Herrmann et al, 2011)
KOC Koc University (Bicici and Yuret, 2011)
LATL-GENEVA Language Technology Laboratory @ University of Geneva (Wehrli et al, 2009)
LIA-LIG Laboratoire Informatique d?Avignon @ The University of Avignon
& Laboratoire d?Informatique de Grenoble @ University of Grenoble (Potet et al, 2011)
LIMSI LIMSI (Allauzen et al, 2011)
LINGUATEC Linguatec Language Technologies (Aleksic and Thurmair, 2011)
LIU Linko?ping University (Holmqvist et al, 2011)
LIUM University of Le Mans (Schwenk et al, 2011)
PROMT ProMT
RWTH-FREITAG RWTH Aachen - Freitag (Huck et al, 2011)
RWTH-HUCK RWTH Aachen - Huck (Huck et al, 2011)
RWTH-WUEBKER RWTH Aachen - Wu?bker (Huck et al, 2011)
SYSTRAN SYSTRAN
UEDIN University of Edinburgh (Koehn et al, 2007)
UFAL-UM Charles University and University of Malta (Corb??-Bellot et al, 2005)
UOW University of Wolverhampton (Aziz et al, 2011)
UPM Technical University of Madrid (Lo?pez-Luden?a and San-Segundo, 2011)
UPPSALA Uppsala University (Koehn et al, 2007)
UPPSALA-FBK Uppsala University
& Fondazione Bruno Kessler (Hardmeier et al, 2011)
ONLINE-[A,B] two online statistical machine translation systems
RBMT-[1?5] five online rule-based machine translation systems
COMMERCIAL-[1,2] two commercial machine translation systems
Table 1: Participants in the shared translation task (European language pairs; individual system track). Not all teams
participated in all language pairs. The translations from commercial and online systems were crawled by us, not
submitted by the respective companies, and are therefore anonymized.
25
ID Participant
BBN-COMBO Raytheon BBN Technologies (Rosti et al, 2011)
CMU-HEAFIELD-COMBO Carnegie Mellon University (Heafield and Lavie, 2011)
JHU-COMBO Johns Hopkins University (Xu et al, 2011a)
KOC-COMBO Koc University (Bicici and Yuret, 2011)
LIUM-COMBO University of Le Mans (Barrault, 2011)
QUAERO-COMBO Quaero Project? (Freitag et al, 2011)
RWTH-LEUSCH-COMBO RWTH Aachen (Leusch et al, 2011)
UOW-COMBO University of Wolverhampton (Specia et al, 2010)
UPV-PRHLT-COMBO Polytechnic University of Valencia (Gonza?lez-Rubio and Casacuberta, 2011)
UZH-COMBO University of Zurich (Sennrich, 2011)
Table 2: Participants in the shared system combination task. Not all teams participated in all language pairs.
? The Quaero Project entry combined outputs they received directly from LIMSI, KIT, SYSTRAN, and RWTH.
participants in the system combination shared task.
Continuing our practice from last year?s workshop,
we separated the test set into a tuning set and a final
held-out test set for system combinations. The tun-
ing portion was distributed to system combination
participants along with reference translations, to aid
them set any system parameters.
In the European language pairs, the tuning set
consisted of 1,003 segments taken from 37 docu-
ments, whereas the test set consisted of 2,000 seg-
ments taken from 73 documents. In the Haitian Cre-
ole task, the split was 674 segments for tuning and
600 for testing.
Table 2 lists the 10 participants in the system com-
bination task.
3 Featured Translation Task
The featured translation task of WMT11 was to
translate Haitian Creole SMS messages into En-
glish. These text messages were sent by people in
Haiti in the aftermath of the January 2010 earth-
quake. In the wake of the earthquake, much of the
country?s conventional emergency response services
failed. Since cell phone towers remained stand-
ing after the earthquake, text messages were a vi-
able mode of communication. Munro (2010) de-
scribes how a text-message-based emergency report-
ing system was set up by a consortium of volunteer
organizations named ?Mission 4636? after a free
SMS short code telephone number that they estab-
lished. The SMS messages were routed to a system
for reporting trapped people and other emergencies.
Search and rescue teams within Haiti, including the
US Military, recognized the quantity and reliabil-
ity of actionable information in these messages and
used them to provide aid.
The majority of the SMS messages were writ-
ten in Haitian Creole, which was not spoken by
most of first responders deployed from overseas.
A distributed, online translation effort was estab-
lished, drawing volunteers from Haitian Creole- and
French-speaking communities around the world.
The volunteers not only translated messages, but
also categorized them and pinpointed them on a
map.5 Collaborating online, they employed their lo-
cal knowledge of locations, regional slang, abbre-
viations and spelling variants to process more than
40,000 messages in the first six weeks alone. First
responders indicated that this volunteer effort helped
to save hundreds of lives and helped direct the first
food and aid to tens of thousands. Secretary of State
Clinton described one success of the Mission 4636
program:?The technology community has set up in-
teractive maps to help us identify needs and target
resources. And on Monday, a seven-year-old girl
and two women were pulled from the rubble of a
collapsed supermarket by an American search-and-
rescue team after they sent a text message calling
for help.? Ushahidi@Tufts described another:?The
World Food Program delivered food to an informal
camp of 2500 people, having yet to receive food or
water, in Diquini to a location that 4636 had identi-
5A detailed map of Haiti was created by a crowdsourcing
effort in the aftermath of the earthquake (Lacey-Hall, 2011).
26
ID Participant
BM-I2R Barcelona Media
& Institute for Infocomm Research (Costa-jussa` and Banchs, 2011)
CMU-DENKOWSKI Carnegie Mellon University - Denkowski (Denkowski and Lavie, 2011b)
CMU-HEWAVITHARANA Carnegie Mellon University - Hewavitharana (Hewavitharana et al, 2011)
HYDERABAD IIIT-Hyderabad
JHU Johns Hopkins University (Weese et al, 2011)
KOC Koc University (Bicici and Yuret, 2011)
LIU Linko?ping University (Stymne, 2011)
UMD-EIDELMAN University of Maryland - Eidelman (Eidelman et al, 2011)
UMD-HU University of Maryland - Hu (Hu et al, 2011)
UPPSALA Uppsala University (Hardmeier et al, 2011)
Table 3: Participants in the featured translation task (Haitian Creole SMS into English; individual system track). Not
all teams participated in both the ?Clean? and ?Raw? tracks.
fied for them.?
In parallel with Rob Munro?s crowdsourcing
translation efforts, the Microsoft Translator team de-
veloped a Haitian Creole statistical machine transla-
tion engine from scratch in a compressed timeframe
(Lewis, 2010). Despite the impressive number
of translations completed by volunteers, machine
translation was viewed as a potentially useful tool
for higher volume applications or to provide trans-
lations of English medical documents into Haitian
Creole. The Microsoft Translator team quickly as-
sembled parallel data from a number of sources,
including Mission 4636 and from the archives of
Carnegie Mellon?s DIPLOMAT project (Frederking
et al, 1997). Through a series of rapid prototyp-
ing efforts, the team improved their system to deal
with non-standard orthography, reduced pronouns,
and SMS shorthand. They deployed a functional
translation system to relief workers in the field in
less than 5 days ? impressive even when measured
against previous rapid MT development efforts like
DARPA?s surprise language exercise (Oard, 2003;
Oard and Och, 2003).
We were inspired by the efforts of Rob Munro and
Will Lewis on translating Haitian Creole in the af-
termath of the disaster, so we worked with them to
create a featured task at WMT11. We thank them for
generously sharing the data they assembled in their
own efforts. We invited Rob Munro, Will Lewis,
and Stephan Vogel to speak at the workshop on the
topic of developing translation technology for future
crises, and they recorded their thoughts in an invited
publication (Lewis et al, 2011).
3.1 Haitian Creole Data
For the WMT11 featured translation task, we
anonymized the SMS Haitian Creole messages
along with the translations that the Mission 4636
volunteers created. Examples of these messages are
given in Table 4. The goal of anonymizing the SMS
data was so that it may be shared with researchers
who are developing translation and mapping tech-
nologies to support future emergency relief efforts
and social development. We ask that any researcher
working with these messages to be aware that they
are actual communications sent by people in need in
a time of crisis. Researchers who use this data are
asked to be cognizant of the following:
? Some messages may be distressing in content.
? The people who sent the messages (and who
are discussed in them) were victims of a natural
disaster and a humanitarian crisis. Please treat
the messages with the appropriate respect for
these individuals.
? The primary motivation for using this data
should be to understand how we can better re-
spond to future crises.
Participants who received the Haitian Creole data
for WMT11 were given anonymization guidelines
27
mwen se [FIRSTNAME] mwen gen twaset ki mouri mwen
mande nou ed pou nou edem map tan repons
I am [FIRSTNAME], I have three sisters who have died. I
ask help for us, I await your response.
Ki kote yap bay manje Where are they giving out food?
Eske lekol kolej marie anne kraze?mesi Was the College Marie Anne school destroyed? Thank you.
Nou pa ka anpeche moustik yo mo`de nou paske yo anpil. We can?t prevent the mosquitoes from biting because there
are so many.
tanpri ke`m ap kase mwen pa ka pran nouvel manmanm. Please heart is breaking because I have no news of my
mother.
4636:Opital Medesen san Fwontie` delmas 19 la fe`men.
Opital sen lwi gonzag nan delma 33 pran an chaj gratwit-
man tout moun ki malad ou blese
4636: The Doctors without Borders Hospital in Delmas 19
is closed. The Saint Louis Gonzaga hospital in Delmas 33
is taking in sick and wounded people for free
Mwen re?se?voua mesaj nou yo 5 sou 5 men mwen ta vle di
yon bagay kile` e koman nap kapab fe`m jwin e`d sa yo pou
moune b la kay mwen ki sinistwe? adre`s la se?
I received your message 5/5 but I would like to ask one
thing when and how will you be able to get the aid to me for
the people around my house who are victims of the earth-
quake? The address is
Sil vous plait map chehe [LASTNAME][FIRSTNAME].di
yo relem nan [PHONENUMBER].mwen se [LAST-
NAME] [FIRSTNAME]
I?m looking for [LASTNAME][FIRSTNAME]. Tell him
to call me at [PHONENUMBER] I am [LASTNAME]
[FIRSTNAME]
Bonswa mwen rele [FIRSTNAME] [LASTNAME] kay
mwen krase mwen pagin anyin poum mange ak fanmi-m
tampri di yon mo pou mwen fem jwen yon tante tou ak
mange. .mrete n
Hello my name is [FIRSTNAME] [LASTNAME]my house
fell down, I?ve had nothing to eat and I?m hungry. Please
help me find food. I live
Mwen viktim kay mwen kraze e`skem ka ale sendomeng
mwen gen paspo`
I?m a victim. My home has been destroyed. Am I allowed
to go to the Dominican Republic? I have a Passport.
KISAM DWE FE LEGEN REPLIK,ESKE MOUN SAINT
MARC AP JWENN REPLIK.
What should I do when there is an aftershock? Will the
people of Saint Marc have aftershocks?
MWEN SE YON JEN ETIDYAN AN ASYANS ENFO-
MATIK KI PASE ANPIL MIZE NAN TRANBLEMAN
DE TE 12 JANVYE A TOUT FANMIM FIN MOURI
MWEN SANTIM SEL MWEN TE VLE ALE VIV
I?m a young student in computer science, who has suffered
a lot during and after the earthquake of January 12th. All
my family has died and I feel alone. I wanted to go live.
Mw rele [FIRSTNAME], mw fe` mason epi mw abite
laple`n. Yo dim minustah ap bay djob mason ki kote pou
mw ta pase si mw ta vle jwenn nan djob sa yo.
My name is [FIRSTNAME], I?m a construction worker and
I live in La Plaine. I heard that the MINUSTAH was giving
jobs to construction workers. What do I have to go to find
one of these jobs?
Souple mande lapolis pou fe on ti pase nan magloire am-
broise prolonge zone muler ak cadet jeremie ginyin jen ga-
son ki ap pase nan zone sa yo e ki agresi
please ask the police to go to magloire ambroise going to-
wards the ?muler? area and cadet jeremie because there are
very aggressive young men in these areas
KIBO MOUN KA JWENN MANJE POU YO MANJE
ANDEYO KAPITAL PASKE DEPI 12 JANVYE YO
VOYE MANJE POU PEP LA MEN NOU PA JANM
JWENN ANYEN. NAP MOURI AK GRANGOU
Where can people get food to eat outside of the capital be-
cause since January 12th, they?ve sent food for the people
but we never received anything. We are dying of hunger
Mwen se [FIRSTNAME][LASTNAME] mwen nan aken
mwen se yon je`n ki ansent mwen te genyen yon paran ki tap
ede li mouri po`toprens, mwen pral akouye nan ko`mansman
feviye
I am [FIRSTNAME][LASTNAME] I am in Aquin I am a
pregnant young person I had a parent who was helping me,
she died in Port-au-Prince, I?m going to give birth at the
start of February
Table 4: Examples of some of the Haitian Creole SMS messages that were sent to the 4636 short code along with
their translations into English. Translations were done by volunteers who wanted to help with the relief effort. Prior
to being distributed, the messages were anonymized to remove names, phone numbers, email addresses, etc. The
anonymization guidelines specified that addresses be retained to facilitate work on mapping technologies.
28
Training set Parallel Words
sentences per lang
In-domain SMS data 17,192 35k
Medical domain 1,619 10k
Newswire domain 13,517 30k
Glossary 35,728 85k
Wikipedia parallel sentence 8,476 90k
Wikipedia named entities 10,499 25k
The bible 30,715 850k
Haitisurf dictionary 3,763 4k
Krengle dictionary 1,687 3k
Krengle sentences 658 3k
Table 5: Training data for the Haitian Creole-English fea-
tured translation task. The in-domain SMS data consists
primarily of raw (noisy) SMS data. The in-domain data
was provided by Mission 4636. The other data is out-of-
domain. It comes courtesy of Carnegie Mellon Univer-
sity, Microsoft Research, Haitisurf.com, and Krengle.net.
alongside the SMS data. The WMT organizers re-
quested that if they discovered messages with incor-
rect or incomplete anonymization, that they notify
us and correct the anonymization using the version
control repository.
To define the shared translation task, we divided
the SMS messages into an in-domain training set,
along with designated dev, devtest, and test sets. We
coordinated with Microsoft and CMU to make avail-
able additional out-of-domain parallel corpora. De-
tails of the data are given in Table 5. In addition
to this data, participants in the featured task were
allowed to use any of the data provided in the stan-
dard translation task, as well as linguistic tools such
as taggers, parsers, or morphological analyzers.
3.2 Clean and Raw Test Data
We provided two sets of testing and development
data. Participants used their systems to translate two
test sets consisting of 1,274 unseen Haitian Creole
SMS messages. One of the test sets contains the
?raw? SMS messages as they were sent, and the
other contains messages that were cleaned up by hu-
man post-editors. The English side is the same in
both cases, and the only difference is the Haitian
Creole input sentences.
The post-editors were Haitian Creole language
informants hired by Microsoft Research. They pro-
vided a number of corrections to the SMS messages,
including expanding SMS shorthands, correcting
spelling/grammar/capitalization, restoring diacritics
that were left out of the original message, and
cleaning up accented characters that were lost when
the message was transmitted in the wrong encoding.
Original Haitian Creole messages:
Sil vou ple? e?de mwen avek moun ki vik-
tim yo nan tranbleman de te? a,ki kite? poto-
prins ki vini nan provins- mwen ede ak ti
kob mwen te ginyin kounie? a
4636: Manje vin pi che nan PaP apre tran-
bleman te-a. mamit diri ap van?n 250gd
kounye, sete 200gd avan. Mayi-a 125gd,
avan sete 100gd
Edited Haitian Creole messages:
Silvouple ede mwen ave`k moun ki viktim
yo nan tranblemannte` a, ki kite Po`toprens
ki vini nan pwovens, mwen ede ak ti ko`b
mwen te genyen kounye a
4636: Manje vin pi che` nan PaP apre tran-
blemannte` a. Mamit diri ap vann 250gd
kounye a, sete 200gd avan. Mayi-a 125gd,
avan sete 100gd.
For the test and development sets the informants
also edited the English translations. For instance,
there were cases where the original crowdsourced
translation summarized the content of the message
instead of translating it, instances where parts of
the source were omitted, and where explanatory
notes were added. The editors improved the trans-
lations so that they were more suitable for machine
translation, making them more literal, correcting
disfluencies on the English side, and retranslating
them when they were summaries.
Crowdsourced English translation:
We are in the area of Petit Goave, we
would like .... we need tents and medi-
cation for flu/colds...
Post-edited translation:
We are in the area of Petit Goave, we
would like to receive assistance, however,
29
it should not be the way I see the Minus-
tah guys are handling the people. We need
lots of tents and medication for flu/colds,
and fever
The edited English is provided as the reference for
both the ?clean? and the ?raw? sets, since we intend
that distinction to refer to the form that the source
language comes in, rather than the target language.
Tables 47 and 48 in the Appendix show a signifi-
cant difference in the translation quality between the
clean and the raw test sets. In most cases, systems?
output for the raw condition was 4 BLEU points
lower than for the clean condition. We believe that
the difference in performance on the raw vs. cleaned
test sets highlight the importance of handling noisy
input data.
All of the in-domain training data is in the raw for-
mat. The original SMS messages are unaltered, and
the translations are just as the volunteered provided
them. In some cases, the original SMS messages are
written in French or English instead of Haitian Cre-
ole, or contain a mixture of languages. It may be
possible to further improve the quality of machine
translation systems trained from this data by improv-
ing the quality of the data itself.
3.3 Goals and Challenges
The goals of the Haitian Creole to English transla-
tion task were:
? To focus researchers on the problems presented
by low resource languages
? To provide a real-world data set consisting of
SMS messages, which contain abbreviations,
non-standard spelling, omitted diacritics, and
other noisy character encodings
? To develop techniques for building translation
systems that will be useful in future crises
There are many challenges in translating noisy
data in a low resource language, and there are a vari-
ety of strategies that might be considered to attempt
to tackle them. For instance:
? Automated cleaning of the raw (noisy) SMS
data in the training set.
? Leveraging a larger French-English model to
translate out of vocabulary Haitian words, by
creating a mapping from Haitian words onto
French.
? Incorporation of morphological and/or syntac-
tic models to better cope with the low resource
language pair.
It is our hope that by introducing this data as a
shared challenge at WMT11 that we will establish a
useful community resource so that researchers may
explore these challenges and publish about them in
the future.
4 Human Evaluation
As with past workshops, we placed greater empha-
sis on the human evaluation than on the automatic
evaluation metric scores. It is our contention that
automatic measures are an imperfect substitute for
human assessment of translation quality. Therefore,
we define the manual evaluation to be primary, and
use the human judgments to validate automatic met-
rics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partici-
pants, interested volunteers, and a small number of
paid annotators (recruited by the participating sites).
More than 130 people participated in the manual
evaluation, with 91 people putting in more than an
hour?s worth of effort, and 29 putting in more than
four hours. There was a collective total of 361 hours
of labor.
We asked annotators to evaluate system outputs
by ranking translated sentences relative to each
other. This was our official determinant of trans-
lation quality. The total number of judgments col-
lected for the different ranking tasks is given in Ta-
ble 6.
We performed the manual evaluation of the indi-
vidual systems separately from the manual evalua-
tion of the system combination entries, rather than
comparing them directly against each other. Last
year?s results made it clear that there is a large (ex-
pected) gap in performance between the two groups.
This year, we opted to reduce the number of pairwise
30
comparisons with the hope that we would be more
likely to find statistically significant differences be-
tween the systems in the same groups. To that same
end, we also eliminated the editing/acceptability
task that was featured in last year?s evaluation, in-
stead we had annotators focus solely on the system
ranking task.
4.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the instruc-
tions simple:
You are shown a source sentence followed
by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions, and asked to rank them.
With the exception of a few tasks in the system
combination track, there were many more than 5
systems participating in any given task?up to 23
for the English-German individual systems track.
Rather than attempting to get a complete ordering
over the systems, we instead relied on random se-
lection and a reasonably large sample size to make
the comparisons fair.
We use the collected rank labels to assign each
system a score that reflects how highly that system
was usually ranked by the annotators. The score for
some systemA reflects how frequently it was judged
to be better than or equal to other systems. Specif-
ically, each block in which A appears includes four
implicit pairwise comparisons (against the other pre-
sented systems). A is rewarded once for each of the
four comparisons in which A wins or ties. A?s score
is the number of such winning (or tying) pairwise
comparisons, divided by the total number of pair-
wise comparisons involving A.
The system scores are reported in Section 5. Ap-
pendix A provides detailed tables that contain pair-
wise head-to-head comparisons between pairs of
systems.
4.2 Inter- and Intra-annotator agreement in
the ranking task
We were interested in determining the inter- and
intra-annotator agreement for the ranking task, since
a reasonable degree of agreement must exist to sup-
port our process as a valid evaluation setup. To en-
sure we had enough data to measure agreement, we
purposely designed the sampling of source segments
and translations shown to annotators in a way that
ensured some items would be repeated, both within
the screens completed by an individual annotator,
and across screens completed by different annota-
tors.
We did so by ensuring that 10% of the generated
screens are exact repetitions of previously gener-
ated screen within the same batch of screens. Fur-
thermore, even within the other 90%, we ensured
that a source segment appearing in one screen ap-
pears again in two more screens (though with differ-
ent system outputs). Those two details, intentional
repetition of source sentences and intentional repeti-
tion of system outputs, ensured we had enough data
to compute meaningful inter- and intra-annotator
agreement rates.
We measured pairwise agreement among anno-
tators using Cohen?s kappa coefficient (?) (Cohen,
1960), which is defined as
? =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the anno-
tators agree, and P (E) is the proportion of time that
they would agree by chance. Note that ? is basically
a normalized version of P (A), one which takes into
account how meaningful it is for annotators to agree
with each other, by incorporating P (E). Note also
that ? has a value of at most 1 (and could possibly
be negative), with higher rates of agreement result-
ing in higher ?.
The above definition of ? is actually used by sev-
eral definitions of agreement measures, which differ
in how P (A) and P (E) are computed.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate at
31
Inividual System Track System Combination Track
Language Pair # Systems Label Labels # Systems Label Labels
Count per System Count per System
Czech-English 8 2,490 276.7 4 1,305 261.0
English-Czech 10 8,985 816.8 2 2,700 900.0
German-English 20 4,620 220.0 8 1,950 216.7
English-German 22 6,540 284.4 4 2,205 441.0
Spanish-English 15 2,850 178.1 6 2,115 302.1
English-Spanish 15 5,595 349.7 4 3,000 600.0
French-English 18 3,540 186.3 6 1,500 214.3
English-French 17 4,590 255.0 2 900 300.0
Haitian (Clean)-English 9 3,360 336.0 3 1,200 300.0
Haitian (Raw)-English 6 1,875 267.9 2 900 300.0
Urdu-English 8 3,165 351.7 N/A N/A N/A
(tunable metrics task)
Overall 148 47,610 299.4 41 17,775 348.5
Table 6: A summary of the WMT11 ranking task, showing the number of systems and number of labels collected in
each of the individual and system combination tracks. The system count does not include the reference translation,
which was included in the evaluation, and so a value under ?Labels per System? can be obtained only after adding 1
to the system count, before dividing the label count (e.g. in German-English, 4, 620/21 = 220.0).
which annotators agree, in the context of pairwise
comparisons. P (A) is computed similarly for intra-
annotator agreement (i.e. self-consistency), but over
pairwise comparisons that were annotated more than
once by a single annotator.
As for P (E), it should capture the probability that
two annotators would agree randomly. Therefore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is com-
puted empirically, by observing how often annota-
tors actually rank two systems as being tied. We
note here that this empirical computation is a depar-
ture from previous years? analyses, where we had
assumed that the three categories are equally likely
(yielding P (E) = 19 +
1
9 +
1
9 =
1
3 ). We believe that
this is a more principled approach, which faithfully
reflects the motivation of accounting for P (E) in the
first place.6
6Even if we wanted to assume a ?random clicker? model,
setting P (E) = 13 is still not entirely correct. Given that
Table 7 gives ? values for inter-annotator and
intra-annotator agreement across the various evalu-
ation tasks. These give an indication of how often
different judges agree, and how often single judges
are consistent for repeated judgments, respectively.
There are some general and expected trends that
can be seen in this table. First of all, intra-annotator
agreement is higher than inter-annotator agreement.
Second, reference translations are noticeably better
than other system outputs, which means that anno-
tators have an artificially high level of agreement on
pairwise comparisons that include a reference trans-
lation. For this reason, we also report the agreement
levels when such comparisons are excluded.
The exact interpretation of the kappa coefficient is
difficult, but according to Landis and Koch (1977),
0?0.2 is slight, 0.2?0.4 is fair, 0.4?0.6 is moder-
ate, 0.6? 0.8 is substantial, and 0.8? 1.0 is almost
perfect. Based on these interpretations, the agree-
ment for sentence-level ranking is moderate to sub-
stantial for most tasks.
annotators rank five outputs at once, P (A = B) = 15 , not
1
3 , since there are only five (out of 25) label pairs that satisfy
A = B. Working this back into P (E)?s definition, we have
P (A > B) = P (A < B) = 25 , and therefore P (E) = 0.36
rather than 0.333.
32
INTER-ANNOTATOR AGREEMENT (I.E. ACROSS ANNOTATORS)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
European languages, individual systems 0.601 0.362 0.375 0.561 0.355 0.320
European languages, system combinations 0.671 0.335 0.505 0.598 0.342 0.389
Haitian-English, individual systems 0.691 0.362 0.516 0.639 0.350 0.446
Haitian-English, system combinations 0.761 0.358 0.628 0.674 0.335 0.509
Tunable metrics task (Urdu-English) 0.692 0.337 0.535 0.641 0.363 0.437
WMT10 (European languages, all systems) 0.658 0.374 0.454 0.626 0.367 0.409
INTRA-ANNOTATOR AGREEMENT (I.E. SELF-CONSISTENCY)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
European languages, individual systems 0.722 0.362 0.564 0.685 0.355 0.512
European languages, system combinations 0.787 0.335 0.680 0.717 0.342 0.571
Haitian-English, individual systems 0.763 0.362 0.628 0.700 0.350 0.539
Haitian-English, system combinations 0.882 0.358 0.816 0.784 0.335 0.675
Tunable metrics task (Urdu-English) 0.857 0.337 0.784 0.856 0.363 0.774
WMT10 (European languages, all systems) 0.755 0.374 0.609 0.734 0.367 0.580
Table 7: Inter- and intra-annotator agreement rates, for the various manual evaluation tracks of WMT11. See Tables 49
and 50 below for a detailed breakdown by language pair.
However, one result that is of concern is that
agreement rates are noticeably lower for European
language pairs, in particular for the individual sys-
tems track. When excluding reference comparisons,
the inter- and intra-annotator agreement levels are
0.320 and 0.512, respectively. Not only are those
numbers lower than for the other tasks, but they
are also lower than last year?s numbers, which were
0.409 and 0.580.
We investigated this result a bit deeper. Tables 49
and 50 in the Appendix break down the results fur-
ther, by reporting agreement levels for each lan-
guage pair. One observation is that the agreement
level for some language pairs deviates in a non-
trivial amount from the overall agreement rate.
Let us focus on inter-annotator agreement rates
in the individual track (excluding reference compar-
isons), in the top right portion of Table 49. The over-
all ? is 0.320, but it ranges from 0.264 for German-
English, to 0.477 for Spanish-English.
What distinguishes those two language pairs from
each other? If we examine the results in Table 8,
we see that Spanish-English had two very weak sys-
tems, which were likely easy for annotators to agree
on comparisons involving them. (This is the con-
verse of annotators agreeing more often on com-
parisons involving the reference.) English-French is
similar in that regard, and it too has a relatively high
agreement rate.
On the other hand, the participants in German-
English formed a large pool of more closely-
matched systems, where the gap separating the bot-
tom system is not as pronounced. So it seems that
the low agreement rates are indicative of a more
competitive evaluation and more closely-matched
systems.
5 Results of the Translation Tasks
We used the results of the manual evaluation to an-
alyze the translation quality of the different systems
that were submitted to the workshop. In our analy-
sis, we aimed to address the following questions:
? Which systems produced the best translation
quality for each language pair?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
33
Czech-English
1023?1166 comparisons/system
System C? ?others
UEDIN ?? Y 0.69
ONLINE-B ? N 0.68
CU-BOJAR N 0.60
JHU N 0.57
UPPSALA Y 0.57
SYSTRAN N 0.51
CST Y 0.47
CU-ZEMAN Y 0.44
Spanish-English
583?833 comparisons/system
System C? ?others
ONLINE-B ? N 0.72
ONLINE-A ? N 0.72
KOC ? Y 0.67
SYSTRAN ? N 0.66
ALACANT ? N 0.66
RBMT-1 N 0.63
RBMT-3 N 0.61
RBMT-2 N 0.60
RBMT-4 N 0.60
RBMT-5 N 0.51
UEDIN Y 0.51
UPM Y 0.50
UFAL-UM Y 0.47
HYDERABAD Y 0.17
CU-ZEMAN Y 0.16
French-English
608?883 comparisons/system
System C? ?others
ONLINE-A ? N 0.66
LIMSI ?? Y+G 0.66
ONLINE-B ? N 0.66
LIA-LIG Y 0.64
KIT ?? Y+G 0.64
LIUM Y+G 0.63
CMU-DENKOWSKI ? Y 0.62
JHU Y+G 0.61
RWTH-HUCK Y+G 0.58
RBMT-1 ? N 0.58
CMU-HANNEMAN Y+G 0.58
RBMT-3 N 0.55
SYSTRAN N 0.54
RBMT-4 N 0.53
RBMT-2 N 0.52
UEDIN Y 0.50
RBMT-5 N 0.45
CU-ZEMAN Y 0.37
English-Czech
3126?3397 comparisons/system
System C? ?others
ONLINE-B ? N 0.65
CU-BOJAR N 0.64
CU-MARECEK ? N 0.63
CU-TAMCHYNA N 0.62
UEDIN ? Y 0.59
CU-POPEL ? Y 0.58
COMMERCIAL2 N 0.51
COMMERCIAL1 N 0.51
JHU N 0.49
CU-ZEMAN Y 0.43
English-Spanish
1300?1480 comparisons/system
System C? ?others
ONLINE-B ? N 0.74
ONLINE-A ? N 0.72
RBMT-3 ? N 0.71
PROMT ? N 0.70
CEU-UPV ? Y 0.65
UEDIN ? Y 0.64
UPPSALA ? Y 0.61
RBMT-4 N 0.61
RBMT-1 N 0.60
UOW Y 0.59
RBMT-2 N 0.57
KOC Y 0.56
RBMT-5 N 0.54
CU-ZEMAN Y 0.49
UPM Y 0.34
English-French
868?1121 comparisons/system
System C? ?others
LIMSI ?? Y+G 0.73
ONLINE-B ? N 0.70
KIT ?? Y+G 0.69
RWTH-HUCK Y+G 0.65
LIUM Y+G 0.64
RBMT-1 N 0.61
ONLINE-A N 0.60
UEDIN Y 0.58
RBMT-3 N 0.58
RBMT-5 N 0.55
UPPSALA Y 0.55
JHU Y 0.55
UPPSALA-FBK Y 0.54
RBMT-4 N 0.49
RBMT-2 N 0.46
LATL-GENEVA N 0.39
CU-ZEMAN Y 0.20
German-English
741?998 comparisons/system
System C? ?others
ONLINE-B ? N 0.72
CMU-DYER ?? Y+G 0.66
ONLINE-A ? N 0.66
RBMT-3 N 0.64
LINGUATEC N 0.63
RBMT-4 N 0.61
RBMT-1 N 0.60
DFKI-XU N 0.60
RWTH-WUEBKER ? Y+G 0.59
KIT Y+G 0.57
LIU Y 0.57
LIMSI Y+G 0.56
RBMT-5 N 0.56
UEDIN Y 0.55
RBMT-2 N 0.54
CU-ZEMAN Y 0.47
UPPSALA Y 0.47
KOC Y 0.45
JHU Y+G 0.43
CST Y 0.37
English-German
1051?1230 comparisons/system
System C? ?others
RBMT-3 ? N 0.73
ONLINE-B ? N 0.73
RBMT-1 ? N 0.70
DFKI-FEDERMANN ? N 0.68
DFKI-XU N 0.67
RBMT-4 ? N 0.66
RBMT-2 ? N 0.66
ONLINE-A ? N 0.65
LIMSI ? Y+G 0.65
KIT ? Y 0.64
UEDIN Y 0.60
LIU Y 0.59
RBMT-5 N 0.58
RWTH-FREITAG Y 0.56
COPENHAGEN ? Y 0.56
JHU Y 0.54
KOC Y 0.53
UOW Y 0.53
CU-TAMCHYNA Y 0.50
UPPSALA Y 0.49
ILLC-UVA Y 0.48
CU-ZEMAN Y 0.38
C? indicates whether system is constrained: trained only using supplied training data, standard monolingual linguis-
tic tools, and, optionally, LDC?s English Gigaword. Eentries that used the Gigaword are marked with +G.
? indicates a win: no other system is statistically significantly better at p-level?0.10 in pairwise comparison.
? indicates a constrained win: no other constrained system is statistically better.
Table 8: Official results for the WMT11 translation task. Systems are ordered by their ?others score, reflecting how
often their translations won or tied pairwise comparisons. For detailed head-to-head comparisons, see Appendix A.
34
Czech-English
1036?1042 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.64
BBN-COMBO ? 0.62
JHU-COMBO 0.58
UPV-PRHLT-COMBO 0.47
English-Czech
1788?1792 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.48
UPV-PRHLT-COMBO 0.41
German-English
811?927 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.70
RWTH-LEUSCH-COMBO 0.65
BBN-COMBO 0.61
UZH-COMBO ? 0.60
JHU-COMBO 0.56
UPV-PRHLT-COMBO 0.52
QUAERO-COMBO 0.46
KOC-COMBO 0.45
English-German
1746?1752 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.61
UZH-COMBO ? 0.58
UPV-PRHLT-COMBO 0.56
KOC-COMBO 0.46
Spanish-English
1132?1249 comparisons/combo
System ?others
RWTH-LEUSCH-COMBO ? 0.71
CMU-HEAFIELD-COMBO ? 0.67
BBN-COMBO ? 0.64
UPV-PRHLT-COMBO 0.64
JHU-COMBO 0.62
KOC-COMBO 0.56
English-Spanish
2360?2378 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.69
UOW-COMBO 0.63
UPV-PRHLT-COMBO 0.59
KOC-COMBO 0.58
French-English
820?916 comparisons/combo
System ?others
BBN-COMBO ? 0.67
RWTH-LEUSCH-COMBO ? 0.63
CMU-HEAFIELD-COMBO 0.62
JHU-COMBO ? 0.59
LIUM-COMBO 0.53
UPV-PRHLT-COMBO 0.53
English-French
586?587 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.51
UPV-PRHLT-COMBO 0.43
? indicates a win: no other system combination is statistically significantly better at p-level?0.10 in pairwise
comparison.
Table 9: Official results for the WMT11 system combination task. Systems are ordered by their ?others score,
reflecting how often their translations won or tied pairwise comparisons. For detailed head-to-head comparisons, see
Appendix A.
35
Haitian Creole (Clean)-English
(individual systems)
1256?1435 comparisons/system
System ?others
BM-I2R ? 0.71
CMU-DENKOWSKI 0.66
CMU-HEWAVITHARANA 0.64
UMD-EIDELMAN 0.63
UPPSALA 0.57
LIU 0.55
UMD-HU 0.52
HYDERABAD 0.43
KOC 0.31
Haitian Creole (Raw)-English
(individual systems)
1065?1136 comparisons/system
System ?others
BM-I2R ? 0.65
CMU-HEWAVITHARANA 0.60
CMU-DENKOWSKI 0.59
LIU 0.55
UMD-EIDELMAN 0.52
JHU 0.41
Haitian Creole (Clean)-English
(system combinations)
896?898 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.52
UPV-PRHLT-COMBO 0.48
KOC-COMBO 0.38
Haitian Creole (Raw)-English
(system combinations)
600?600 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO 0.47
UPV-PRHLT-COMBO 0.43
? indicates a win: no other system is statistically significantly better at p-level?0.10 in pairwise comparison.
Table 10: Official results for the WMT11 featured translation task (Haitian Creole SMS into English). Systems are
ordered by their ?others score, reflecting how often their translations won or tied pairwise comparisons. For detailed
head-to-head comparisons, see Appendix A.
36
Tables 8?10 show the system ranking for each
of the translation tasks. For each language pair,
we define a system as ?winning? if no other system
was found statistically significantly better (using the
Sign Test, at p ? 0.10). In some cases, multiple sys-
tems are listed as winners, either due to a large num-
ber of participants or a low number of judgments per
system pair, both of which are factors that make it
difficult to achieve statistical significance.
We start by examining the results for the individ-
ual system track for the European languages (Ta-
ble 8). In Spanish?English and German?English,
unconstrained systems are observed to perform bet-
ter than constrained systems. In other language
pairs, particularly French?English, constrained
systems are found to be able to be on the same level
or outperform unconstrained systems. It also seems
that making use of the Gigaword corpora is likely
to yield better systems, even when translating out of
English, as in English-French and English-German.
For English-German the rule-based MT systems per-
formed well.
Of the participating teams, there is no individ-
ual system clearly outperforming all other systems
across the different language pairs. However, one
of the crawled systems, ONLINE-B, performs con-
sistently well, being one of the winners in all eight
language pairs.
As for the system combination track (Table 9),
the CMU-HEAFIELD-COMBO entry performed quite
well, being a winner in seven out of eight language
pairs. This performance is carried over to the Haitian
Creole task, where it again comes out on top (Ta-
ble 10). In the individual track of the Haitian Creole
task, BM-I2R is the sole winner in both the ?clean?
and ?raw? tracks.
6 Evaluation Task
In addition to allowing us to analyze the translation
quality of different systems, the data gathered during
the manual evaluation is useful for validating auto-
matic evaluation metrics. Our evaluation shared task
is similar to the MetricsMATR workshop (Metrics
for MAchine TRanslation) that NIST runs (Przy-
bocki et al, 2008; Callison-Burch et al, 2010). Ta-
ble 11 lists the participants in this task, along with
their metrics.
A total of 21 metrics and their variants were sub-
mitted to the evaluation task by 9 research groups.
We asked metrics developers to score the outputs of
the machine translation systems and system com-
binations at the system-level and at the segment-
level. The system-level metrics scores are given in
the Appendix in Tables 39?48. The main goal of the
evaluation shared task is not to score the systems,
but instead to validate the use of automatic metrics
by measuring how strongly they correlate with hu-
man judgments. We used the human judgments col-
lected during the manual evaluation for the transla-
tion task and the system combination task to calcu-
late how well metrics correlate at system-level and
at the segment-level.
This year the strongest metric was a new metric
developed by Columbia and ETS called MTeRater-
Plus. MTeRater-Plus is a machine-learning-based
metric that use features from ETS?s e-rater, an auto-
mated essay scoring engine designed to assess writ-
ing proficiency (Attali and Burstein, 2006). The fea-
tures include sentence-level and document-level in-
formation. Some examples of the e-rater features
include:
? Preposition features that calculate the proba-
bility of prepositions appearing in the given
context of a sentence (Tetreault and Chodorow,
2008)
? Collocation features that indicate whether the
collocations in the document are typical of na-
tive use (Futagi et al, 2008).
? A sentence fragment feature that counts the
number of ill-formed sentences in a document.
? A feature that counts the number of words with
inflection errors
? A feature that counts the the number of article
errors in the sentence citeHan2006.
MTeRater uses only the e-rater features, and mea-
sures fluency without any need for reference transla-
tions. MTeRater-Plus is a meta-metric that incorpo-
rates adequacy by combining MTeRater with other
MT evaluation metrics and heuristics that take the
reference translations into account.
Please refer to the proceedings for papers provid-
ing detailed descriptions of all of the metrics.
37
Metric IDs Participant
AMBER, AMBER-NL, AMBER-IT National Research Council Canada (Chen and Kuhn, 2011)
F15, F15G3 Koc? University (Bicici and Yuret, 2011)
METEOR-1.3-ADQ, METEOR-1.3-RANK Carnegie Mellon University (Denkowski and Lavie, 2011a)
MTERATER, MTERATER-PLUS Columbia / ETS (Parton et al, 2011)
MP4IBM1, MPF, WMPF DFKI (Popovic?, 2011; Popovic? et al, 2011)
PARSECONF DFKI (Avramidis et al, 2011)
ROSE, ROSE-POS The University of Sheffield (Song and Cohn, 2011)
TESLA-B, TESLA-F, TESLA-M National University of Singapore (Dahlmeier et al, 2011)
TINE University of Wolverhampton (Rios et al, 2011)
BLEU provided baseline (Papineni et al, 2002)
TER provided baseline (Snover et al, 2006)
Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics
as baselines.
E
N
-C
Z
-
10
S
Y
S
T
E
M
S
E
N
-D
E
-
22
S
Y
S
T
E
M
S
E
N
-E
S
-
15
S
Y
S
T
E
M
S
E
N
-F
R
-
17
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
A
V
E
R
A
G
E
W
/O
C
Z
System-level correlation for translation out of English
TESLA-M .90 .95 .96 .94
TESLA-B .81 .90 .91 .87
MPF .72 .63 .87 .89 .78 .80
WMPF .72 .61 .87 .89 .77 .79
MP4IBM1 -.76 -.91 -.71 -.61 .75 .74
ROSE .65 .41 .90 .86 .71 .73
BLEU .65 .44 .87 .86 .70 .72
AMBER-TI .56 .54 .88 .84 .70 .75
AMBER .56 .53 .87 .84 .70 .74
AMBER-NL .56 .45 .88 .83 .68 .72
F15G3 .50 .30 .89 .84 .63 .68
METEORrank .65 .30 .74 .85 .63 .63
F15 .52 .19 .86 .85 .60 .63
TER -.50 -.12 -.81 -.84 .57 .59
TESLA-F .86 .80 -.83 .28
Table 12: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average abso-
lute value. We did not calculate correlations with the hu-
man judgments for the system combinations for the out of
English direction, because none of them had more than 4
items.
6.1 System-Level Metric Analysis
We measured the correlation of the automatic met-
rics with the human judgments of translation quality
at the system-level using Spearman?s rank correla-
tion coefficient ?. We converted the raw scores as-
signed to each system into ranks. We assigned a hu-
man ranking to the systems based on the percent of
time that their translations were judged to be better
than or equal to the translations of any other system
in the manual evaluation. The reference was not in-
cluded as an extra translation.
When there are no ties, ? can be calculated using
the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher absolute value
for ? is making predictions that are more similar to
the human judgments than an automatic evaluation
metric with a lower absolute ?.
The system-level correlations are shown in Ta-
ble 13 for translations into English, and Table 12
out of English, sorted by average correlation across
the language pairs. The highest correlation for
each language pair and the highest overall average
are bolded. This year, nearly all of the metrics
38
C
Z
-E
N
-
8
S
Y
S
T
E
M
S
D
E
-E
N
-
20
S
Y
S
T
E
M
S
D
E
-E
N
-
8
C
O
M
B
O
S
E
S
-E
N
-
15
S
Y
S
T
E
M
S
E
S
-E
N
-
6
C
O
M
B
O
S
F
R
-E
N
-
18
S
Y
S
T
E
M
S
F
R
-E
N
-
6
C
O
M
B
O
S
A
V
E
R
A
G
E
(E
U
R
O
P
E
A
N
L
A
N
G
S
)
H
T
-E
N
(C
L
E
A
N
)
-
9
S
Y
S
T
E
M
S
H
T
-E
N
(R
A
W
)
-
6
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
(A
L
L
L
A
N
G
S
)
System-level correlation for metrics scoring translations into English
MTERATER-PLUS -.95 -.90 -.93 -.91 -.94 -.93 -.77 .90 -.82 -.54 .85
TINE-SRL-MATCH .95 .69 .95 .95 1.00 .87 .66 .87
TESLA-F .95 .70 .98 .96 .94 .90 .60 .86 .93 .83 .87
TESLA-B .98 .88 .98 .91 .94 .91 .31 .84 .93 .83 .85
MTERATER -.91 -.88 -.91 -.88 -.89 -.79 -.60 .83 .13 .77 .55
METEOR-1.3-ADQ .93 .68 .91 .91 .83 .93 .66 .83 .95 .77 .84
TESLA-M .95 .94 .95 .82 .94 .87 .31 .83 .95 .83 .84
METEOR-1.3-RANK .91 .71 .91 .88 .77 .93 .66 .82 .95 .83 .84
AMBER-NL .88 .58 .91 .88 .94 .94 .60 .82
AMBER-TI .88 .63 .93 .85 .83 .94 .60 .81
AMBER .88 .59 .91 .86 .83 .95 .60 .80
MPF .95 .69 .91 .83 .60 .87 .54 .77 .95 .77 .79
WMPF .95 .66 .86 .83 .60 .87 .54 .76 .93 .77 .78
F15 .93 .45 .88 .96 .49 .87 .60 .74
F15G3 .93 .48 .83 .94 .49 .88 .60 .74
ROSE .88 .59 .83 .92 .60 .86 .26 .70 .93 .77 .74
BLEU .88 .48 .83 .90 .49 .85 .43 .69 .90 .83 .73
TER -.83 -.33 -.64 -.89 -.37 -.77 -.89 .67 -.93 -.83 .72
MP4IBM1 -.91 -.56 -.50 -.12 -.43 -.08 .14 .35
DFKI-PARSECONF .31 .52
Table 13: System-level Spearman?s rho correlation of the automatic evaluation metrics with the human judgments
for translation into English, ordered by average absolute value for the European languages. We did not calculate
correlations with the human judgments for the system combinations for Czech to English and for Haitian Creole to
English, because they had too few items (? 4) for reliable statistics.
39
F
R
-E
N
(6
33
7
PA
IR
S
)
D
E
-E
N
(8
95
0
PA
IR
S
)
E
S
-E
N
(5
97
4
PA
IR
S
)
C
Z
-E
N
(3
69
5
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations into English
MTERATER-PLUS .30 .36 .45 .36 .37
TESLA-F .28 .24 .39 .32 .31
TESLA-B .28 .26 .36 .29 .30
METEOR-1.3-RANK .23 .25 .38 .28 .29
METEOR-1.3-ADQ .24 .25 .37 .27 .28
MPF .25 .23 .34 .28 .28
AMBER-TI .24 .26 .33 .27 .28
AMBER .24 .25 .33 .27 .27
WMPF .24 .23 .34 .26 .27
AMBER-NL .24 .24 .30 .27 .26
MTERATER .19 .26 .33 .24 .26
TESLA-M .21 .23 .29 .23 .24
TINE-SRL-MATCH .20 .19 .30 .24 .23
F15G3 .17 .15 .29 .21 .21
F15 .16 .14 .27 .22 .20
MP4IBM1 .15 .16 .18 .12 .15
DFKI-PARSECONF n/a .24 n/a n/a
Table 14: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average correla-
tion.
had stronger correlation with human judgments than
BLEU. The metrics that had the strongest correlation
this year included two metrics, MTeRater and TINE,
as well as metrics that have demonstrated strong cor-
relation in previous years like TESLA and Meteor.
6.2 Segment-Level Metric Analysis
We measured the metrics? segment-level scores with
the human rankings using Kendall?s tau rank corre-
lation coefficient. The reference was not included as
an extra translation.
We calculated Kendall?s tau as:
? =
num concordant pairs - num discordant pairs
total pairs
where a concordant pair is a pair of two translations
of the same segment in which the ranks calculated
from the same human ranking task and from the cor-
responding metric scores agree; in a discordant pair,
they disagree. In order to account for accuracy- vs.
E
N
-F
R
(6
93
4
PA
IR
S
)
E
N
-D
E
(1
07
32
PA
IR
S
)
E
N
-E
S
(8
83
7
PA
IR
S
)
E
N
-C
Z
(1
16
51
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations out of English
AMBER-TI .32 .22 .31 .21 .27
AMBER .31 .21 .31 .22 .26
MPF .31 .22 .30 .20 .26
WMPF .31 .22 .29 .19 .25
AMBER-NL .30 .19 .29 .20 .25
METEOR-1.3-RANK .31 .14 .26 .19 .23
F15G3 .26 .08 .22 .13 .17
F15 .26 .07 .22 .12 .17
MP4IBM1 .21 .13 .13 .06 .13
TESLA-B .29 .20 .28 n/a
TESLA-M .25 .18 .27 n/a
TESLA-F .30 .19 .26 n/a
Table 15: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average corre-
lation.
error-based metrics correctly, counts of concordant
vs. discordant pairs were calculated specific to these
two metric types. The possible values of ? range
between 1 (where all pairs are concordant) and ?1
(where all pairs are discordant). Thus an automatic
evaluation metric with a higher value for ? is mak-
ing predictions that are more similar to the human
judgments than an automatic evaluation metric with
a lower ? .
We did not include cases where the human rank-
ing was tied for two systems. As the metrics produce
absolute scores, compared to five relative ranks in
the human assessment, it would be potentially un-
fair to the metric to count a slightly different met-
ric score as discordant with a tie in the relative hu-
man rankings. A tie in automatic metric rank for
two translations was counted as discordant with two
corresponding non-tied human judgments.
The correlations are shown in Table 14 for trans-
lations into English, and Table 15 out of English,
sorted by average correlation across the four lan-
guage pairs. The highest correlation for each lan-
guage pair and the highest overall average are
40
ID Participant Metric Name
CMU-METEOR Carnegie Mellon University METEOR (Denkowski and Lavie, 2011a)
CU-SEMPOS-BLEU Charles University SemPOS/BLEU (Macha?c?ek and Bojar, 2011)
NUS-TESLA-F National University of Singapore TESLA-F (Dahlmeier et al, 2011)
RWTH-CDER RWTH Aachen CDER (Leusch and Ney, 2009)
SHEFFIELD-ROSE The University of Sheffield ROSE (single reference) (Song and Cohn, 2011)
STANFORD-DCP Stanford DCP (based on Liu and Gildea (2005))
BLEU provided baseline BLEU
BLEU-SINGLE provided baseline BLEU (single reference)
Table 16: Participants in the tunable-metric shared task. For comparison purposes, we included two BLEU-optimized
systems in the evaluation as baselines.
bolded. There is a clear winner for the metrics that
score translations into English: the MTeRater-Plus
metric (Parton et al, 2011) has the highest segment
level correlation across the board. For metrics that
score translation into other languages, there is not
such a clear-cut winner. The AMBER metric variants
do well, as do MPF and WMPF.
7 Tunable Metrics Task
This year we introduced a new shared task that fo-
cuses on using evaluation metrics to tune the param-
eters of a statistical machine translation system. The
intent of this task was to get researchers who de-
velop automatic evaluation metrics for MT to work
on the problem of using their metric to optimize
the parameters of MT systems. Previous workshops
have demonstrated that a number of metrics perform
better than BLEU in terms of having stronger cor-
relation with human judgments about the rankings
of multiple machine translation systems. However,
most MT system developers still optimize the pa-
rameters of their systems to BLEU. Here we aim
to investigate the question of whether better metrics
will result in better quality output when a system is
optimized to them.
Because this was the first year that we ran the
tunable metrics task, participation was limited to a
few groups on an invitation-only basis. Table 16
lists the participants in this task. Metrics developers
were invited to integrate their evaluation metric into
a MERT optimization routine, which was then used
to tune the parameters of a fixed statistical machine
translation system. We evaluated whether the sys-
tem tuned on their metrics produced higher-quality
output than the baseline system that was tuned to
BLEU, as is typically done. In order to evaluate
whether the quality was better, we conducted a man-
ual evaluation, in the same fashion that we evalu-
ate the different MT systems submitted to the shared
translation task.
We provide the participants with a fixed MT sys-
tem for Urdu-English, along with a small parallel
set to be used for tuning. Specifically, we provide
developers with the following components:
? Decoder - the Joshua decoder was used in this
pilot.
? Decoder configuration file - a Joshua configu-
ration file that ensures all systems use the same
search parameters.
? Translation model - an Urdu-to-English trans-
lation model, with syntax-based SCFG rules
(Baker et al, 2010).
? Language model - a large 5-gram language
model trained on the English Gigaword corpus
? Development set - a development set, with 4
English reference sets, to be used to optimize
the system parameters.
? Test set - a test set consisting of 883 Urdu sen-
tences, to be translated by the tuned system (no
references provided).
? Optimization routine - we provide an imple-
mentation of minimum error rate training that
allows new metrics to be easily integrated as
the objective function.
41
Tunable Metrics Task
1324?1484 comparisons/system
System ?others >others
BLEU ? 0.79 0.28
BLEU-SINGLE ? 0.77 0.27
CMU-METEOR ? 0.76 0.27
RWTH-CDER 0.76 0.26
CU-SEMPOS-BLEU ? 0.74 0.29
STANFORD-DCP ? 0.73 0.27
NUS-TESLA-F 0.68 0.28
SHEFFIELD-ROSE 0.05 0.00
? indicates a win: no other system combination is sta-
tistically significantly better at p-level?0.10 in pair-
wise comparison.
Table 17: Official results for the WMT11 tunable-metric
task. Systems are ordered by their ?others score, re-
flecting how often their translations won or tied pairwise
comparisons. The > column reflects how often a system
strictly won a pairwise comparison.
We provided the metrics developers with Omar
Zaidan?s Z-MERT software (Zaidan, 2009), which
implements Och (2003)?s minimum error rate train-
ing procedure. Z-MERT is designed to be modular
with respect to the objective function, and allows
BLEU to be easily replaced with other automatic
evaluation metrics. Metric developers incorporated
their metrics into Z-MERT by subclassing the Eval-
uationMetric.java abstract class. They ran Z-MERT
on the dev set with the provided decoder/models,
and created a weight vector for the system param-
eters.
Each team produced a distinct final weight vec-
tor, which was used to produce English translations
of sentences in the test set. The different transla-
tions produced by tuning the system to different met-
rics were then evaluated using the manual evaluation
pipeline.7
7.1 Results of the Tunable Metrics Task
The results of the evaluation are in Table 18. The
scores show that the entries were quite close to each
other, with the notable exception of the SHEFFIELD-
ROSE-tuned system, which produced overly-long
7We also recased and detokenized each system?s output, to
ensure the outputs are more readable and easier to evaluate.
R
E
F
B
L
E
U
B
L
E
U
-S
IN
G
L
E
C
M
U
-M
E
T
E
O
R
C
U
-S
E
M
P
O
S
-B
L
E
U
N
U
S
-T
E
S
L
A
-F
R
W
T
H
-C
D
E
R
S
H
E
F
F
IE
L
D
-R
O
S
E
S
T
A
N
F
O
R
D
-D
C
P
REF ? .15? .11? .13? .09? .09? .10? .00? .11?
BLEU .78? ? .15 .11 .20 .19? .13? .01? .14
BLEU-SINGLE .82? .20 ? .11 .16 .21 .11 .00? .20
CMU-METEOR .84? .09 .15 ? .21 .20 .19 .00? .19
CU-SEMPOS-BLEU .82? .23 .21 .21 ? .12? .18 .00? .21
NUS-TESLA-F .80? .32? .31 .28 .28? ? .31 .00? .28
RWTH-CDER .79? .22? .16 .16 .22 .23 ? .00? .15
SHEFFIELD-ROSE .98? .93? .93? .96? .95? .95? .93? ? .94?
STANFORD-DCP .82? .17 .18 .26 .27 .28 .15 .00? ?
> others .83 .28 .27 .27 .29 .28 .26 .00 .27
>= others .90 .79 .77 .76 .74 .68 .76 .05 .73
Table 18: Head to head comparisons for the tunable met-
rics task. The numbers indicate how often the system in
the column was judged to be better than the system in
the row. The difference between 100 and the sum of the
corresponding cells is the percent of time that the two
systems were judged to be equal.
and erroneous output (possibly due to an implemen-
tation issue). This is also evident from the fact that
38% of pairwise comparisons indicated a tie be-
tween the two systems, with the tie rate increasing
to a full 47% when excluding comparisons involving
the reference. This is a very high tie rate ? the cor-
responding figure in, say, European language pairs
(individual systems) is only 21%.
What makes the different entries appear even
more closely-matched is that the ranking changes
significantly when ordering systems by their
>others score rather than the ?others score (i.e.
when rewarding only wins, and not rewarding ties).
NUS-TESLA-F goes from being a bottom entry to be-
ing a top entry, with CU-SEMPOS-BLEU also bene-
fiting, changing from the middle to the top rank.
Either way, we see that a BLEU -tuned system
is performing just as well as systems tuned to the
other metrics. This might be an indication that some
work remains to be done before a move away from
BLEU-tuning is fully justified. On the other hand,
the close results might be an artifact of the language
pair choice. Urdu-English translation is still a rel-
atively difficult problem, and MT outputs are still
of a relatively low quality. It might be the case that
human annotators are simply not very good at distin-
42
guishing one bad translation from another bad trans-
lation, especially at such a fine-grained level.
It is worth noting that the designers of the TESLA
family replicated the setup of this tunable metric task
for three European language pairs, and found that
human judges did perceive a difference in quality
between a TESLA-tuned system and a BLEU -tuned
system (Liu et al, 2011).
7.2 Anticipated Changes Next Year
This year?s effort was a pilot of the task, so we in-
tentionally limited the task to some degree, to make
it easier to iron out the details. Possible changes for
next year include:
? More language pairs / translations into lan-
guages other than English. This year we fo-
cus on Urdu-English because the language pair
requires a lot of reordering, and our syntactic
model has more parameters to optimize than
the standard Hiero and phrase-based models.
? Provide some human judgments about the
model?s output, so that people can experiment
with regression models.
? Include a single reference track along with the
multiple reference track. Some metrics may be
better at dealing with the (more common) case
of there being only a single reference transla-
tion available for every source sentence.
? Allow for experimentation with the MIRA op-
timization routine instead of MERT. MIRA can
scale to a greater number of features, but re-
quires that metrics be decomposable.
8 Summary
As in previous editions of this workshop we car-
ried out an extensive manual and automatic evalua-
tion of machine translation performance for translat-
ing from European languages into English, and vice
versa.
The number of participants grew slightly com-
pared to previous editions of the WMT workshop,
with 36 groups from 27 institutions participating in
the translation task of WMT11, 10 groups from 10
institutions participating in the system combination
task, and 10 groups from 8 institutions participating
in the featured translation task (Haitian Creole SMS
into English).
This year was also the first time that we included a
language pair (Haitian-English) with non-European
source language and with very limited resources for
the source language side. Also the genre of the
Haitian-English task differed from previous WMT
tasks as the Haitian-English translations are SMS
messages.
WMT11 also introduced a new shared task focus-
ing on evaluation metrics to tune the parameters of
a statistical machine translation system in which 6
groups have participated.
As in previous years, all data sets generated by
this workshop, including the human judgments, sys-
tem translations and automatic scores, are publicly
available for other researchers to analyze.8
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-0022,
the US National Science Foundation under grant
IIS-0713448, and the CoSyne project FP7-ICT-4-
248531 funded by the European Commission. The
views and findings are the authors? alone. A big
thank you to Ondr?ej Bojar, Simon Carter, Chris-
tian Federmann, Will Lewis, Rob Munro and Herve?
Saint-Amand, and to the shared task participants.
References
Vera Aleksic and Gregor Thurmair. 2011. Personal
Translator at WMT2011. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Alexandre Allauzen, He?le`ne Bonneau-Maynard, Hai-Son
Le, Aure?lien Max, Guillaume Wisniewski, Franc?ois
Yvon, Gilles Adda, Josep Maria Crego, Adrien
Lardilleux, Thomas Lavergne, and Artem Sokolov.
2011. LIMSI @ WMT11. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3):159?174.
Eleftherios Avramidis, Maja Popovic?, David Vilar, and
Aljoscha Burchardt. 2011. Evaluate with confidence
8http://statmt.org/wmt11/results.html
43
estimation: Machine ranking of translation outputs us-
ing grammatical features. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Wilker Aziz, Miguel Rios, and Lucia Specia. 2011. Shal-
low semantic trees for SMT. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Kathryn Baker, Michael Bloodgood, Chris Callison-
Burch, Bonnie Dorr, Scott Miller, Christine Pi-
atko, Nathaniel W. Filardo, and Lori Levin. 2010.
Semantically-informed syntactic machine translation:
A tree-grafting approach. In Proceedings of AMTA.
Lo??c Barrault. 2011. MANY improvements for
WMT?11. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Ergun Bicici and Deniz Yuret. 2011. RegMT system for
machine translation, system combination, and evalua-
tion. In Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Improving
translation model by monolingual data. In Proceed-
ings of the Sixth Workshop on Statistical Machine
Translation.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion (WMT07), Prague, Czech Republic.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation (WMT09), Athens, Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT10), Uppsala, Swe-
den.
Boxing Chen and Roland Kuhn. 2011. Amber: A mod-
ified bleu, enhanced ranking metric. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measur-
ment, 20(1):37?46.
Antonio M. Corb??-Bellot, Mikel L. Forcada, Sergio Ortiz-
Rojas, Juan Antonio Pe?rez-Ortiz, Gema Ram??rez-
Sa?nchez, Felipe Sa?nchez-Mart??nez, In?aki Alegria,
Aingeru Mayor, and Kepa Sarasola. 2005. An open-
source shallow-transfer machine translation engine for
the romance languages of Spain. In Proceedings of the
European Association for Machine Translation, pages
79?86.
Marta R. Costa-jussa` and Rafael E. Banchs. 2011. The
BM-I2R Haitian-Cre?ole-to-English translation system
description for the WMT 2011 evaluation campaign.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011.
TESLA at WMT 2011: Translation evaluation and tun-
able metric. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Michael Denkowski and Alon Lavie. 2011a. Meteor 1.3:
Automatic metric for reliable optimization and evalu-
ation of machine translation systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Michael Denkowski and Alon Lavie. 2011b. METEOR-
Tuned Phrase-Based SMT: CMU French-English and
Haitian-English Systems for WMT 2011. Technical
Report CMU-LTI-11-011, Language Technologies In-
stitute, Carnegie Mellon University.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Vladimir Eidelman, Kristy Hollingshead, and Philip
Resnik. 2011. Noisy SMS machine translation in low-
density languages. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Christian Federmann and Sabine Hunsicker. 2011.
Stochastic parse tree selection for an existing RBMT
system. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
Robert Frederking, Alexander Rudnicky, and Christopher
Hogan. 1997. Interactive speech translation in the
DIPLOMAT project. In Proceedings of the ACL-1997
Workshop on Spoken Language Translation.
Markus Freitag, Gregor Leusch, Joern Wuebker, Stephan
Peitz, Hermann Ney, Teresa Herrmann, Jan Niehues,
Alex Waibel, Alexandre Allauzen, Gilles Adda,
Josep Maria Crego, Bianka Buschbeck, Tonio Wand-
macher, and Jean Senellart. 2011. Joint WMT sub-
mission of the QUAERO project. In Proceedings of
the Sixth Workshop on Statistical Machine Translation.
Yoko Futagi, Paul Deane, Martin Chodorow, and Joel
Tetreault. 2008. A computational approach to de-
tecting collocation errors in the writing of non-native
speakers of English. Computer Assisted Language
Learning Journal.
Jesu?s Gonza?lez-Rubio and Francisco Casacuberta. 2011.
The UPV-PRHLT combination system for WMT 2011.
44
In Proceedings of the Sixth Workshop on Statistical
Machine Translation.
Greg Hanneman and Alon Lavie. 2011. CMU syntax-
based machine translation at WMT 2011. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Christian Hardmeier, Jo?rg Tiedemann, Markus Saers,
Marcello Federico, and Mathur Prashant. 2011. The
Uppsala-FBK systems at WMT 2011. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Kenneth Heafield and Alon Lavie. 2011. CMU system
combination in WMT 2011. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Teresa Herrmann, Mohammed Mediani, Jan Niehues,
and Alex Waibel. 2011. The Karlsruhe Institute of
Technology translation systems for the WMT 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation.
Sanjika Hewavitharana, Nguyen Bach, Qin Gao, Vamshi
Ambati, and Stephan Vogel. 2011. CMU Haitian
Creole-English translation system for WMT 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation.
Maria Holmqvist, Sara Stymne, and Lars Ahrenberg.
2011. Experiments with word alignment, normaliza-
tion and clause reordering for SMT between English
and German. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-
delman, Olivia Buzek, and Benjamin B. Bederson.
2011. The value of monolingual crowdsourcing in
a real-world translation scenario: Simulation using
Haitian Creole emergency SMS messages. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Matthias Huck, Joern Wuebker, Christoph Schmidt,
Markus Freitag, Stephan Peitz, Daniel Stein, Arnaud
Dagnelies, Saab Mansour, Gregor Leusch, and Her-
mann Ney. 2011. The RWTH Aachen machine trans-
lation system for WMT 2011. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Maxim Khalilov and Khalil Sima?an. 2011. ILLC-UvA
translation system for EMNLP-WMT 2011. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation, New
York, New York.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the ACL-2007 Demo and Poster Sessions,
Prague, Czech Republic.
Oliver Lacey-Hall. 2011. The guardian?s poverty matters
blog: How remote teams can help the rapid response
to disasters, March.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Gregor Leusch and Hermann Ney. 2009. Edit distances
with block movements and error rate confidence esti-
mates. Machine Translation, 23:129?140.
Gregor Leusch, Markus Freitag, and Hermann Ney.
2011. The RWTH system combination system for
WMT 2011. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
William Lewis, Robert Munro, and Stephan Vogel. 2011.
Crisis MT: Developing a cookbook for MT in crisis
situations. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
William D. Lewis. 2010. Haitian Creole: How to
build and ship an MT engine from scratch in 4 days,
17hours, & 30 minutes. In Proceedings of EAMT
2010.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with syn-
tax, semirings, discriminative training and other good-
ies. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, Upp-
sala, Sweden, July.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceedings
of the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 25?32.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of EMNLP.
Vero?nica Lo?pez-Luden?a and Rube?n San-Segundo. 2011.
UPM system for the translation task. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Matous? Macha?c?ek and Ondr?ej Bojar. 2011. Approxi-
mating a deep-syntactic metric for MT evaluation and
tuning. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
45
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, and
Ondr?ej Bojar. 2011. Two-step translation with gram-
matical post-processing. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Robert Munro. 2010. Crowdsourced translation for
emergency response in Haiti: the global collabora-
tion of local knowledge. In Proceedings of the AMTA
Workshop on Collaborative Crowdsourcing for Trans-
lation.
Douglas W. Oard and Franz Josef Och. 2003. Rapid-
response machine translation for unexpected lan-
guages. In Proceedings of MT Summit IX.
Douglas W. Oard. 2003. The surprise language exer-
cises. ACM Transactions on Asian Language Infor-
mation Processing, 2(2):79?84.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL-2002), Philadelphia, Pennsylvania.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation.
Martin Popel, David Marec?ek, Nathan Green, and
Zdene?k Z?abokrtsky?. 2011. Influence of parser choice
on dependency-based MT. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Maja Popovic?, David Vilar, Eleftherios Avramidis, and
Aljoscha Burchardt. 2011. Evaluation without ref-
erences: IBM1 scores as evaluation metrics. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Maja Popovic?. 2011. Morphemes and POS tags for n-
gram based evaluation metrics. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Marion Potet, Raphae?l Rubino, Benjamin Lecouteux,
Ste?phane Huet, Laurent Besacier, Herve? Blanchon,
and Fabrice Lefe`vre. 2011. The LIGA (LIG/LIA)
machine translation system for WMT 2011. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 ?Met-
rics for MAchine TRanslation? challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation, Honolulu, Hawaii.
Miguel Rios, Wilker Aziz, and Lucia Specia. 2011.
TINE: A metric to assess MT adequacy. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Christian Rish?j and Anders S?gaard. 2011. Factored
translation with unsupervised word clusters. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected BLEU training for
graphs: BBN system description for WMT11 system
combination task. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
V??ctor M. Sa?nchez-Cartagena, Felipe Sa?nchez-Mart??nez,
and Juan Antonio Pe?rez-Ortiz. 2011. The Univer-
sitat d?Alacant hybrid machine translation system for
WMT 2011. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Sadaf Abdul-Rauf, Haithem Afli,
and Kashif Shah. 2011. LIUM?s SMT machine trans-
lation systems for WMT 2011. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Rico Sennrich. 2011. The UZH system combination sys-
tem for WMT 2011. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Americas
(AMTA-2006), Cambridge, Massachusetts.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence level MT eval-
uation. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Ma-
chine translation evaluation versus quality estimation.
Machine Translation, 24(1):39?50.
Sara Stymne. 2011. Spell checking techniques for re-
placement of unknown words and data cleaning for
Haitian Creole SMS translation. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of preposition error detection. In Proceedings
of COLING, Manchester, UK.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the Thrax
grammar extractor. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Eric Wehrli, Luka Nerima, and Yves Scherrer. 2009.
Deep linguistic multilingual translation and bilingual
dictionaries. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 90?94.
46
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011a.
Description of the JHU system combination scheme
for WMT 2011. In Proceedings of the Sixth Workshop
on Statistical Machine Translation.
Jia Xu, Hans Uszkoreit, Casey Kennington, David Vilar,
and Xiaojun Zhang. 2011b. DFKI hybrid machine
translation system for WMT 2011 - on the integration
of SMT and RBMT. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Francisco Zamora-Martinez and Maria Jose Castro-
Bleda. 2011. CEU-UPV English-Spanish system for
WMT11. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Daniel Zeman. 2011. Hierarchical phrase-based MT at
the Charles University for the WMT 2011 shared task.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation.
47
A Pairwise System Comparisons by Human Judges
Tables 19?38 show pairwise comparisons between systems for each language pair. The numbers in each of
the tables? cells indicate the percentage of times that the system in that column was judged to be better than
the system in that row. Bolding indicates the winner of the two systems. The difference between 100 and
the sum of the complementary cells is the percent of time that the two systems were judged to be equal.
Because there were so many systems and data conditions the significance of each pairwise comparison
needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differences
(rather than differences that are attributable to chance). In the following tables ? indicates statistical signif-
icance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical significance at
p ? 0.01, according to the Sign Test.
B Automatic Scores
Tables 39?48 give the automatic scores for each of the systems.
C Meta-evaluation
Tables 49 and 50 give a detailed breakdown of intra- and inter-annotator agreement rates for all of manual
evaluation tracks of WMT11, broken down by language pair.
48
R
E
F
C
S
T
C
U
-B
O
JA
R
C
U
-Z
E
M
A
N
JH
U
O
N
L
IN
E
-B
S
Y
S
T
R
A
N
U
E
D
IN
U
P
P
S
A
L
A
REF ? .02? .04? .01? .04? .04? .04? .05? .04?
CST .88? ? .49? .36 .49? .59? .41 .58? .44?
CU-BOJAR .91? .27? ? .27? .30 .48? .28? .41? .41
CU-ZEMAN .94? .31 .49? ? .47? .67? .47? .64? .49?
JHU .89? .29? .39 .28? ? .47? .36 .41? .36
ONLINE-B .84? .20? .27? .19? .28? ? .24? .30 .27?
SYSTRAN .91? .31 .49? .30? .39 .59? ? .56? .37
UEDIN .89? .16? .25? .16? .27? .36 .23? ? .25?
UPPSALA .84? .28? .40 .24? .37 .49? .38 .45? ?
> others .89 .23 .36 .23 .33 .46 .31 .43 .33
>= others .96 .47 .60 .44 .57 .68 .51 .69 .57
Table 19: Ranking scores for entries in the Czech-English task (individual system track).
R
E
F
C
O
M
M
E
R
C
IA
L
-1
C
O
M
M
E
R
C
IA
L
-2
C
U
-B
O
JA
R
C
U
-M
A
R
E
C
E
K
C
U
-P
O
P
E
L
C
U
-T
A
M
C
H
Y
N
A
C
U
-Z
E
M
A
N
JH
U
O
N
L
IN
E
-B
U
E
D
IN
REF ? .05? .04? .04? .04? .05? .05? .04? .03? .04? .04?
COMMERCIAL-1 .91? ? .36 .53? .50? .47? .44? .33? .33? .55? .45?
COMMERCIAL-2 .87? .42 ? .52? .47? .47? .50? .30? .40 .50? .43
CU-BOJAR .89? .31? .31? ? .29 .41 .21? .19? .27? .42? .31?
CU-MARECEK .88? .31? .37? .27 ? .35? .28 .21? .30? .39 .28?
CU-POPEL .85? .33? .29? .43 .45? ? .41 .27? .31? .50? .39
CU-TAMCHYNA .87? .34? .35? .30? .32 .40 ? .22? .25? .45? .32
CU-ZEMAN .91? .47? .52? .56? .56? .55? .55? ? .44? .64? .54?
JHU .91? .43? .41 .50? .47? .51? .51? .31? ? .52? .48?
ONLINE-B .86? .27? .32? .33? .39 .33? .29? .18? .23? ? .31?
UEDIN .85? .34? .40 .40? .37? .42 .36 .24? .25? .44? ?
> others .88 .33 .34 .39 .39 .40 .36 .23 .28 .44 .35
>= others .96 .51 .51 .64 .63 .58 .62 .43 .49 .65 .59
Table 20: Ranking scores for entries in the English-Czech task (individual system track).
49
R
E
F
C
M
U
-D
Y
E
R
C
S
T
C
U
-Z
E
M
A
N
D
F
K
I-
X
U
JH
U
K
IT
K
O
C
L
IM
S
I
L
IN
G
U
A
T
E
C
L
IU
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-W
U
E
B
K
E
R
U
E
D
IN
U
P
P
S
A
L
A
REF ? .05? .02? .03? .04? .00? .08? .04? .00? .07? .05? .07? .14? .02? .08? .00? .06? .08? .02? .10? .08?
CMU-DYER .95? ? .18? .17? .33 .26? .22? .12? .29? .43 .23? .43 .54 .32 .20? .40 .43 .48 .31 .19? .18?
CST .96? .74? ? .42 .62? .35 .68? .44? .47? .78? .62? .77? .73? .81? .70? .74? .67? .53? .65? .47 .51
CU-ZEMAN .97? .67? .22 ? .56? .26? .41 .22? .48 .66? .46 .60? .62? .73? .57? .60? .62? .53? .40 .44 .48
DFKI-XU .94? .44 .06? .24? ? .10? .26 .17? .49? .47 .21? .42 .45 .52 .42 .45 .51 .39 .40 .48 .29
JHU 1.00?.61? .33 .55? .64? ? .59? .45 .51? .59 .52? .68? .63? .62? .64? .65? .58? .46 .61? .44 .38
KIT .87? .65? .12? .21 .44 .23? ? .34 .40 .54 .30 .43 .57? .44 .43 .47 .50 .53 .40 .28 .17?
KOC .96? .64? .09? .49? .66? .36 .43 ? .43 .69? .57? .69? .63? .62? .41 .63? .59 .52? .51 .59? .40
LIMSI .96? .54? .24? .30 .22? .25? .38 .27 ? .63? .52 .43 .55? .43 .43 .59? .47 .40 .41 .32 .44
LINGUATEC .91? .45 .13? .24? .38 .32 .34 .18? .27? ? .26? .45 .62? .46 .20? .49 .53 .36 .41 .32? .29?
LIU .89? .49? .14? .29 .54? .25? .48 .24? .31 .64? ? .47 .61? .52 .46 .48 .50 .23? .48 .37 .36
ONLINE-A .88? .47 .12? .25? .42 .18? .41 .19? .39 .39 .30 ? .32 .26? .28 .46 .36 .35 .42 .19? .27?
ONLINE-B .78? .38 .16? .23? .33 .28? .26? .16? .26? .29? .22? .38 ? .23? .23? .29? .29? .22? .27 .22? .18?
RBMT-1 .96? .42 .09? .18? .35 .21? .51 .23? .43 .41 .38 .56? .62? ? .31 .46 .39 .13 .48 .50 .30?
RBMT-2 .86? .54? .15? .28? .48 .29? .43 .41 .39 .55? .44 .51 .64? .43 ? .55? .47 .54? .44 .41 .29?
RBMT-3 .92? .42 .11? .27? .32 .23? .47 .18? .19? .34 .38 .49 .55? .38 .26? ? .36 .29? .34 .33 .28?
RBMT-4 .88? .36 .19? .24? .38 .29? .43 .38 .45 .32 .37 .44 .56? .33 .34 .45 ? .35 .29? .51 .24?
RBMT-5 .92? .45 .27? .27? .45 .32 .37 .27? .47 .47 .61? .55 .67? .26 .24? .53? .46 ? .45 .47 .39
RWTH-WUEBKER .93? .50 .23? .26 .33 .20? .24 .36 .41 .44 .39 .47 .55 .44 .38 .53 .56? .45 ? .21 .39
UEDIN .88? .59? .24 .28 .28 .33 .50 .24? .45 .65? .40 .67? .62? .34 .39 .52 .41 .36 .43 ? .48
UPPSALA .92? .64? .27 .29 .39 .44 .58? .32 .41 .66? .53 .68? .69? .59? .59? .58? .61? .54 .36 .31 ?
> others .92 .50 .17 .28 .40 .26 .40 .26 .38 .51 .40 .51 .57 .43 .38 .49 .47 .39 .41 .36 .32
>= others .95 .66 .37 .47 .60 .43 .57 .45 .56 .63 .57 .66 .72 .60 .54 .64 .61 .56 .59 .55 .47
Table 21: Ranking scores for entries in the German-English task (individual system track).
50
R
E
F
C
O
P
E
N
H
A
G
E
N
C
U
-T
A
M
C
H
Y
N
A
C
U
-Z
E
M
A
N
D
F
K
I-
F
E
D
E
R
M
A
N
N
D
F
K
I-
X
U
IL
L
C
-U
V
A
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-F
R
E
IT
A
G
U
E
D
IN
U
O
W
U
P
P
S
A
L
A
REF ? .08? .06? .00? .13? .02? .05? .05? .02? .02? .16? .06? .11? .07? .14? .14? .19? .11? .11? .16? .07? .07? .08?
COPENHAGEN .85? ? .31 .09? .60? .39 .25 .32 .41 .27 .36 .34 .49? .61? .56? .61? .64? .64? .60 .26 .49 .30 .16
CU-TAMCHYNA .92? .37 ? .13? .61? .48? .30 .38 .58? .33 .39 .41? .55? .57? .72? .69? .81? .49 .59? .47 .39 .40 .43
CU-ZEMAN 1.00?.60? .41? ? .76? .78? .51? .47? .64? .53? .66? .49? .77? .68? .69? .64? .70? .64? .72? .55? .47 .44 .50
DFKI-FEDERMANN .72? .19? .17? .16? ? .39 .25? .38 .38 .24? .32 .29 .35 .40 .43 .33 .39 .19 .33? .22? .31 .11? .30
DFKI-XU .84? .31 .21? .08? .37 ? .25? .32 .34 .12? .37 .30 .35 .47 .54? .30 .51? .43 .37 .20? .22? .25? .14?
ILLC-UVA .90? .39 .37 .25? .63? .50? ? .41? .58? .35 .56? .38 .55? .63? .61? .63? .71? .75? .62? .33 .56? .38 .41
JHU .91? .45 .27 .27? .41 .40 .20? ? .37 .27 .43 .50? .58? .59? .43 .55? .72? .50 .50 .50? .47 .46 .22?
KIT .87? .24 .23? .17? .41 .43 .26? .37 ? .16? .51 .27? .37 .45? .47 .39 .58? .53 .47 .23? .24 .21? .17?
KOC .95? .35 .35 .13? .61? .65? .38 .42 .57? ? .47? .33 .47? .62? .61? .53? .64? .63? .45 .20 .38 .37 .18?
LIMSI .77? .31 .26 .11? .48 .35 .18? .30 .33 .23? ? .36 .39 .50? .52 .47 .48 .39 .42 .18? .22? .28 .14?
LIU .84? .32 .20? .25? .51 .38 .26 .21? .51? .35 .39 ? .51 .49? .63? .52? .56 .48? .56 .29 .38 .25 .25
ONLINE-A .75? .21? .24? .09? .48 .41 .22? .30? .37 .25? .37 .37 ? .46 .37 .41 .47 .33 .44 .27? .28 .22? .16?
ONLINE-B .91? .17? .15? .13? .44 .22 .17? .16? .20? .15? .24? .25? .27 ? .43 .35 .48 .33 .17? .17? .26 .12? .20?
RBMT-1 .80? .23? .11? .20? .37 .28? .18? .29 .38 .25? .36 .30? .41 .38 ? .34 .45 .36 .02? .17? .17? .28? .24?
RBMT-2 .80? .20? .10? .16? .43 .38 .20? .27? .45 .22? .36 .30? .38 .51 .43 ? .48 .40 .42 .31? .28? .16? .25?
RBMT-3 .65? .18? .14? .15? .37 .29? .17? .22? .25? .20? .27 .33 .33 .29 .30 .31 ? .34 .16? .24? .35 .20? .11?
RBMT-4 .80? .21? .28 .22? .19 .26 .09? .32 .29 .27? .39 .27? .43 .44 .38 .38 .45 ? .42 .29? .36 .27? .31?
RBMT-5 .88? .35 .31? .15? .54? .51 .26? .34 .36 .36 .44 .35 .44 .59? .37? .33 .62? .38 ? .29 .45 .38 .30
RWTH-FREITAG .80? .31 .27 .17? .62? .55? .19 .25? .56? .30 .49? .41 .53? .59? .56? .53? .62? .57? .45 ? .36 .38 .24
UEDIN .82? .27 .27 .27 .46 .47? .17? .28 .36 .33 .48? .27 .47 .43 .75? .55? .52 .50 .43 .21 ? .35 .27
UOW .86? .39 .21 .23 .74? .53? .36 .38 .64? .20 .38 .41 .74? .61? .56? .64? .57? .65? .38 .26 .41 ? .31
UPPSALA .79? .32 .35 .29 .54 .57? .34 .51? .51? .45? .53? .43 .73? .70? .55? .64? .77? .57? .55 .43 .33 .41 ?
> others .84 .29 .24 .17 .48 .42 .24 .31 .42 .27 .40 .34 .46 .51 .51 .47 .56 .46 .41 .29 .34 .29 .25
>= others .91 .56 .50 .38 .68 .67 .48 .54 .64 .53 .65 .59 .65 .730 .70 .66 .732 .66 .58 .56 .60 .53 .49
Table 22: Ranking scores for entries in the English-German task (individual system track).
R
E
F
A
L
A
C
A
N
T
C
U
-Z
E
M
A
N
H
Y
D
E
R
A
B
A
D
K
O
C
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
S
Y
S
T
R
A
N
U
E
D
IN
U
FA
L
-U
M
U
P
M
REF ? .03? .02? .00? .02? .03? .12? .15? .04? .07? .05? .02? .03? .03? .03? .07?
ALACANT .86? ? .07? .08? .30 .52 .31 .27? .29? .54 .49 .32? .51 .27? .26? .26?
CU-ZEMAN .98? .89? ? .48 .84? .85? .94? .90? .83? .87? .85? .78? .97? .79? .79? .91?
HYDERABAD .98? .86? .27 ? .88? .95? .92? .85? .96? .74? .82? .80? .88? .91? .80? .86?
KOC .93? .48 .06? .06? ? .28 .39 .40 .34 .44 .38 .26? .59? .22? .20? .18?
ONLINE-A .90? .28 .02? .02? .48 ? .32 .34 .34 .26? .34 .19? .35 .20? .11? .20?
ONLINE-B .79? .33 .04? .00? .47 .30 ? .24? .31? .31? .27? .25? .33 .27? .21? .07?
RBMT-1 .81? .52? .05? .11? .50 .57 .62? ? .50 .36 .34 .17 .40 .39 .34 .30?
RBMT-2 .96? .61? .09? .04? .52 .47 .59? .37 ? .39 .46 .27 .58? .29? .24? .45
RBMT-3 .88? .31 .09? .13? .44 .56? .60? .53 .37 ? .47 .14? .52 .40 .23? .31
RBMT-4 .90? .38 .08? .16? .50 .53 .60? .41 .43 .38 ? .43 .52 .33? .18? .22?
RBMT-5 .94? .61? .06? .10? .54? .70? .63? .37 .45 .59? .41 ? .66? .42 .50 .43
SYSTRAN .92? .33 .02? .10? .25? .53 .53 .42 .30? .36 .38 .27? ? .21? .41 .24?
UEDIN .95? .63? .13? .02? .63? .67? .59? .47 .61? .53 .59? .42 .53? ? .32? .45
UFAL-UM .94? .63? .10? .11? .56? .70? .74? .51 .61? .59? .74? .36 .47 .61? ? .44
UPM .85? .54? .02? .03? .62? .61? .81? .59? .45 .55 .68? .40 .60? .42 .38 ?
> others .91 .51 .07 .10 .52 .56 .59 .48 .48 .47 .48 .35 .54 .39 .34 .36
>= others .96 .66 .16 .17 .67 .723 .723 .63 .60 .61 .60 .51 .66 .51 .47 .50
Table 23: Ranking scores for entries in the Spanish-English task (individual system track).
51
R
E
F
C
E
U
-U
P
V
C
U
-Z
E
M
A
N
K
O
C
O
N
L
IN
E
-A
O
N
L
IN
E
-B
P
R
O
M
T
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
U
E
D
IN
U
O
W
U
P
M
U
P
P
S
A
L
A
REF ? .06? .03? .09? .09? .09? .05? .03? .06? .04? .08? .02? .08? .02? .03? .04?
CEU-UPV .84? ? .21? .20? .43 .36 .42 .37 .34? .50? .31 .34 .32 .21? .13? .22
CU-ZEMAN .87? .56? ? .38? .56? .56? .58? .46? .40 .70? .46? .49? .51? .45? .19? .49?
KOC .84? .41? .22? ? .56? .51? .48? .54? .39 .55? .42 .35 .51? .44 .11? .34
ONLINE-A .72? .31 .24? .15? ? .36 .37 .28? .23? .35 .25? .20? .29? .25? .08? .09?
ONLINE-B .72? .30 .17? .18? .26 ? .29 .23? .20? .37 .20? .19? .19? .22? .02? .23?
PROMT .76? .29 .21? .25? .42 .43 ? .24? .24 .19 .27? .26? .32 .25? .18? .21?
RBMT-1 .85? .37 .29? .23? .51? .54? .48? ? .35 .45? .40? .05? .47 .39 .25? .39
RBMT-2 .86? .50? .35 .38 .51? .48? .35 .39 ? .41? .34 .36 .45 .36 .23? .41
RBMT-3 .86? .26? .18? .22? .40 .35 .19 .20? .22? ? .25? .23? .24? .33 .10? .22?
RBMT-4 .80? .45 .29? .34 .53? .51? .43? .21? .38 .43? ? .24? .34 .30 .20? .45?
RBMT-5 .96? .43 .29? .42 .57? .61? .46? .22? .38 .49? .47? ? .50 .46 .27? .47
UEDIN .74? .28 .20? .21? .46? .48? .43 .37 .31 .49? .45 .35 ? .20? .14? .23
UOW .90? .44? .18? .32 .46? .52? .56? .39 .39 .44 .45 .36 .38? ? .10? .32
UPM .93? .65? .53? .67? .74? .71? .69? .59? .51? .74? .60? .51? .64? .68? ? .62?
UPPSALA .84? .36 .21? .32 .49? .42? .45? .39 .35 .45? .29? .41 .35 .30 .15? ?
> others .83 .38 .24 .30 .47 .46 .41 .33 .32 .43 .35 .29 .38 .33 .14 .31
>= others .94 .65 .49 .56 .72 .74 .70 .60 .57 .71 .61 .54 .64 .59 .34 .61
Table 24: Ranking scores for entries in the English-Spanish task (individual system track).
R
E
F
C
M
U
-D
E
N
K
O
W
S
K
I
C
M
U
-H
A
N
N
E
M
A
N
C
U
-Z
E
M
A
N
JH
U
K
IT
L
IA
-L
IG
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-H
U
C
K
S
Y
S
T
R
A
N
U
E
D
IN
REF ? .10? .18? .06? .03? .14? .15? .14? .14? .12? .05? .12? .09? .05? .06? .05? .05? .07? .02?
CMU-DENKOWSKI .79? ? .35 .12? .34 .32 .41 .35 .21? .47? .46 .49 .32 .33 .36 .35 .25 .45 .29
CMU-HANNEMAN .79? .35 ? .17? .29 .44? .43 .52? .45 .45 .49 .51 .39 .44 .38 .35 .35 .43 .37
CU-ZEMAN .94? .61? .67? ? .54? .66? .66? .58? .60? .59? .88? .62? .59? .63? .60? .56 .68? .64? .40
JHU .82? .34 .29 .22? ? .26 .54? .40 .36 .43 .40 .49 .42 .40 .34 .35 .36 .47 .20?
KIT .79? .39 .20? .16? .40 ? .26? .46 .34 .38 .52 .38 .35 .39 .28 .38 .15? .32 .30
LIA-LIG .75? .24 .31 .28? .24? .59? ? .49 .27 .40 .46 .35 .26 .31? .29 .32 .32 .33? .35
LIMSI .86? .30 .25? .21? .31 .26 .26 ? .38 .40 .42 .35 .18? .43 .34 .16? .34 .34 .33
LIUM .78? .45? .33 .16? .38 .34 .44 .40 ? .38 .30 .44 .26? .33? .38 .28 .29 .33 .28
ONLINE-A .80? .23? .21 .22? .37 .35 .36 .33 .46 ? .43 .35 .16? .33 .24? .20? .26 .34 .27?
ONLINE-B .86? .37 .31 .04? .46 .22 .36 .33 .43 .26 ? .40 .20? .16? .44 .20? .41 .38 .22?
RBMT-1 .87? .44 .35 .23? .46 .44 .54 .48 .44 .53 .54 ? .39 .37 .33 .11? .39 .17? .35
RBMT-2 .84? .47 .37 .26? .40 .50 .45 .52? .54? .58? .67? .45 ? .51 .35 .22? .51 .57 .41
RBMT-3 .89? .44 .42 .19? .40 .43 .54? .46 .61? .50 .71? .37 .32 ? .42 .35 .42 .47 .40
RBMT-4 .85? .53 .36 .26? .51 .47 .55 .52 .46 .59? .40 .43 .50 .42 ? .34 .46 .44 .41
RBMT-5 .93? .58 .55 .33 .54 .54 .59 .70? .56 .66? .65? .36? .54? .46 .37 ? .50 .54? .54
RWTH-HUCK .92? .43 .38 .14? .36 .59? .41 .44 .29 .53 .48 .46 .30 .46 .32 .38 ? .37 .17?
SYSTRAN .93? .39 .38 .24? .44 .48 .60? .50 .40 .55 .57 .45? .36 .29 .44 .21? .49 ? .36
UEDIN .93? .48 .41 .40 .51? .48 .54 .49 .46 .60? .57? .52 .37 .47 .39 .39 .51? .52 ?
> others .85 .39 .36 .21 .39 .41 .46 .46 .41 .46 .50 .41 .33 .39 .35 .28 .37 .39 .32
>= others .91 .62 .58 .37 .61 .64 .64 .661 .63 .661 .66 .58 .52 .55 .53 .45 .58 .54 .50
Table 25: Ranking scores for entries in the French-English task (individual system track).
52
R
E
F
C
U
-Z
E
M
A
N
JH
U
K
IT
L
A
T
L
-G
E
N
E
V
A
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-H
U
C
K
U
E
D
IN
U
P
P
S
A
L
A
U
P
P
S
A
L
A
-F
B
K
REF ? .07? .06? .25? .07? .13? .20? .15? .20? .10? .09? .18? .11? .12? .14? .18? .16? .16?
CU-ZEMAN .92? ? .83? .86? .63? .85? .90? .86? .81? .89? .70? .75? .75? .61? .78? .79? .81? .81?
JHU .91? .07? ? .55? .30? .60? .50? .55? .59? .45 .41 .34? .30? .50 .40 .42 .42 .44
KIT .63? .04? .29? ? .18? .47 .37 .30? .37 .38 .30? .37 .24? .34 .28 .34 .24? .13?
LATL-GENEVA .86? .29? .54? .73? ? .77? .67? .71? .79? .55? .39 .66? .52 .58? .58? .51 .52 .58?
LIMSI .75? .04? .21? .29 .13? ? .23? .28? .37 .27? .27? .24? .24? .21? .27? .28? .25? .31
LIUM .76? .04? .26? .44 .24? .46? ? .33 .52 .48 .25? .36 .25? .28? .43 .40 .35 .32
ONLINE-A .78? .10? .31? .51? .22? .51? .46 ? .44 .39 .36 .41 .30? .41 .41 .32? .46 .33
ONLINE-B .70? .06? .27? .41 .13? .39 .32 .30 ? .47 .22? .26? .13? .28? .32 .26? .33 .27?
RBMT-1 .83? .07? .38 .46 .23? .56? .39 .41 .42 ? .17? .34 .36 .13 .52 .33? .40 .40
RBMT-2 .88? .25? .47 .59? .37 .65? .63? .51 .57? .54? ? .58? .39 .54? .63? .61? .47 .42
RBMT-3 .80? .19? .54? .42 .20? .60? .47 .44 .52? .42 .18? ? .21? .43 .51 .55 .41 .39
RBMT-4 .82? .22? .54? .63? .33 .63? .64? .54? .59? .41 .44 .46? ? .47 .68? .53 .42 .39
RBMT-5 .86? .18? .46 .53 .20? .62? .56? .46 .61? .22 .33? .40 .34 ? .43 .52 .40 .53?
RWTH-HUCK .76? .08? .33 .38 .21? .60? .40 .38 .43 .36 .18? .37 .21? .38 ? .39 .22? .29
UEDIN .78? .15? .37 .46 .34 .49? .38 .53? .58? .56? .33? .35 .36 .37 .47 ? .38 .31
UPPSALA .77? .07? .36 .53? .36 .49? .46 .46 .56 .46 .38 .42 .39 .55 .57? .39 ? .47
UPPSALA-FBK .80? .10? .40 .71? .27? .50 .47 .51 .53? .42 .48 .41 .52 .29? .50 .47 .40 ?
> others .80 .12 .39 .51 .25 .55 .48 .45 .52 .43 .32 .41 .33 .39 .46 .43 .39 .38
>= others .86 .20 .55 .69 .39 .73 .64 .60 .70 .61 .46 .58 .49 .55 .65 .58 .55 .54
Table 26: Ranking scores for entries in the English-French task (individual system track).
R
E
F
B
M
-I
2R
C
M
U
-D
E
N
K
O
W
S
K
I
C
M
U
-H
E
W
A
V
IT
H
A
R
A
N
A
H
Y
D
E
R
A
B
A
D
K
O
C
L
IU
U
M
D
-E
ID
E
L
M
A
N
U
M
D
-H
U
U
P
P
S
A
L
A
REF ? .03? .01? .03? .02? .01? .00? .01? .01? .02?
BM-I2R .91? ? .28? .27? .13? .08? .19? .30? .30? .24?
CMU-DENKOWSKI .93? .44? ? .25 .22? .15? .28? .33 .29? .31?
CMU-HEWAVITHARANA .91? .40? .31 ? .21? .16? .29? .35 .39 .30
HYDERABAD .96? .71? .59? .58? ? .27? .56? .57? .42 .52?
KOC .94? .78? .75? .64? .55? ? .65? .69? .62? .64?
LIU .92? .56? .42? .44? .27? .24? ? .43 .41 .39
UMD-EIDELMAN .94? .44? .35 .35 .17? .17? .34 ? .37 .31?
UMD-HU .90? .50? .57? .45 .35 .21? .46 .45 ? .42
UPPSALA .93? .48? .47? .39 .31? .20? .40 .43? .37 ?
> others .93 .49 .42 .39 .25 .17 .35 .40 .36 .35
>= others .98 .71 .66 .64 .43 .31 .55 .63 .52 .57
Table 27: Ranking scores for entries in the Haitian Creole (Clean)-English task (individual system track).
53
R
E
F
B
M
-I
2R
C
M
U
-D
E
N
K
O
W
S
K
I
C
M
U
-H
E
W
A
V
IT
H
A
R
A
N
A
JH
U
L
IU
U
M
D
-E
ID
E
L
M
A
N
REF ? .05? .03? .04? .02? .02? .03?
BM-I2R .83? ? .29? .25? .22? .30? .30?
CMU-DENKOWSKI .89? .44? ? .37? .23? .37 .30?
CMU-HEWAVITHARANA .86? .43? .26? ? .27? .37 .32
JHU .96? .62? .53? .49? ? .52? .47?
LIU .92? .48? .38 .34 .31? ? .36
UMD-EIDELMAN .92? .48? .44? .42 .29? .41 ?
> others .90 .43 .34 .33 .23 .34 .30
>= others .97 .65 .59 .60 .41 .55 .52
Table 28: Ranking scores for entries in the Haitian Creole (Raw)-English task (individual system track).
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .01? .02? .01? .01?
BBN-COMBO .91? ? .25 .18? .16?
CMU-HEAFIELD-COMBO .90? .24 ? .17? .12?
JHU-COMBO .92? .27? .29? ? .20?
UPV-PRHLT-COMBO .94? .41? .42? .36? ?
> others .92 .23 .24 .18 .12
>= others .99 .62 .64 .58 .47
Table 29: Ranking scores for entries in the Czech-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .04? .04?
CMU-HEAFIELD-COMBO .86? ? .17?
UPV-PRHLT-COMBO .88? .30? ?
> others .87 .17 .11
>= others .96 .48 .41
Table 30: Ranking scores for entries in the English-Czech task (system combination track).
54
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
Q
U
A
E
R
O
-C
O
M
B
O
R
W
T
H
-L
E
U
S
C
H
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
U
Z
H
-C
O
M
B
O
REF ? .11? .09? .04? .09? .10? .14? .05? .09?
BBN-COMBO .79? ? .45? .32 .21? .28? .39 .31? .36
CMU-HEAFIELD-COMBO .84? .23? ? .21? .17? .19? .25? .19? .31
JHU-COMBO .85? .42 .55? ? .25? .28? .40? .28? .47?
KOC-COMBO .83? .56? .62? .45? ? .41 .54? .40? .51?
QUAERO-COMBO .86? .52? .64? .45? .36 ? .54? .49? .48
RWTH-LEUSCH-COMBO .83? .28 .41? .22? .20? .22? ? .22? .38
UPV-PRHLT-COMBO .85? .47? .57? .42? .25? .26? .48? ? .49?
UZH-COMBO .86? .34 .38 .31? .29? .32 .41 .30? ?
> others .84 .36 .46 .30 .22 .26 .39 .27 .39
>= others .91 .61 .70 .56 .45 .46 .65 .52 .60
Table 31: Ranking scores for entries in the German-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
U
Z
H
-C
O
M
B
O
REF ? .11? .09? .10? .11?
CMU-HEAFIELD-COMBO .81? ? .19? .23? .32
KOC-COMBO .84? .48? ? .38? .47?
UPV-PRHLT-COMBO .81? .36? .23? ? .37?
UZH-COMBO .80? .34 .24? .31? ?
> others .81 .320 .19 .25 .318
>= others .90 .61 .46 .56 .58
Table 32: Ranking scores for entries in the English-German task (system combination track).
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-L
E
U
S
C
H
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .05? .09? .05? .07? .06? .08?
BBN-COMBO .81? ? .34 .27 .21? .27 .26
CMU-HEAFIELD-COMBO .84? .31 ? .18? .15? .29 .20
JHU-COMBO .83? .25 .32? ? .27 .35? .25
KOC-COMBO .84? .39? .39? .32 ? .39? .31?
RWTH-LEUSCH-COMBO .81? .24 .23 .16? .17? ? .14?
UPV-PRHLT-COMBO .77? .30 .26 .27 .22? .35? ?
> others .82 .25 .27 .21 .18 .28 .21
>= others .93 .64 .67 .62 .56 .71 .64
Table 33: Ranking scores for entries in the Spanish-English task (system combination track).
55
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
U
O
W
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .10? .07? .09? .08?
CMU-HEAFIELD-COMBO .70? ? .15? .21? .17?
KOC-COMBO .76? .35? ? .36? .19
UOW-COMBO .72? .29? .22? ? .25?
UPV-PRHLT-COMBO .76? .35? .16 .35? ?
> others .73 .27 .15 .25 .17
>= others .91 .69 .58 .63 .59
Table 34: Ranking scores for entries in the English-Spanish task (system combination track).
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
L
IU
M
-C
O
M
B
O
R
W
T
H
-L
E
U
S
C
H
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .04? .04? .06? .06? .06? .02?
BBN-COMBO .82? ? .35 .25 .18? .21? .21?
CMU-HEAFIELD-COMBO .90? .29 ? .30 .20? .29 .25?
JHU-COMBO .83? .35 .40 ? .31? .36 .21?
LIUM-COMBO .83? .42? .40? .44? ? .38? .35
RWTH-LEUSCH-COMBO .83? .34? .29 .30 .22? ? .21?
UPV-PRHLT-COMBO .91? .49? .40? .34? .30 .40? ?
> others .85 .32 .31 .28 .21 .28 .21
>= others .95 .67 .62 .59 .53 .63 .53
Table 35: Ranking scores for entries in the French-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .11? .11?
CMU-HEAFIELD-COMBO .74? ? .23?
UPV-PRHLT-COMBO .77? .38? ?
> others .76 .24 .17
>= others .89 .51 .43
Table 36: Ranking scores for entries in the English-French task (system combination track).
56
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .01? .01? .01?
CMU-HEAFIELD-COMBO .94? ? .29? .21?
KOC-COMBO .96? .48? ? .41?
UPV-PRHLT-COMBO .94? .34? .29? ?
> others .95 .28 .20 .21
>= others .99 .52 .38 .48
Table 37: Ranking scores for entries in the Haitian Creole (Clean)-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .02? .02?
CMU-HEAFIELD-COMBO .83? ? .24
UPV-PRHLT-COMBO .86? .29 ?
> others .84 .16 .13
>= others .98 .47 .43
Table 38: Ranking scores for entries in the Haitian Creole (Raw)-English task (system combination track).
57
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Czech-English News Task
BBN-COMBO 0.24 0.24 0.25 0.29 0.31 0.19 ?9627 ?10667 1.97 0.53 0.49 0.61 0.34 ?65 44 0.48 0.03 0.51 43
CMU-HEAFIELD-COMBO 0.24 0.24 0.24 0.28 0.3 0.18 ?9604 ?10933 1.97 0.54 0.5 0.60 0.33 ?65 43 0.48 0.03 0.52 42
CST 0.19 0.19 0.2 0.16 0.21 0.10 ?27410 ?27880 1.94 0.64 0.40 0.5 0.28 ?65 34 0.38 0.02 0.42 33
CU-BOJAR 0.21 0.21 0.22 0.19 0.24 0.13 ?23441 ?22289 1.95 0.64 0.44 0.55 0.30 ?65 37 0.42 0.02 0.46 36
CU-ZEMAN 0.20 0.2 0.21 0.14 0.21 0.11 ?33520 ?30938 1.93 0.66 0.38 0.52 0.29 ?66 31 0.37 0.02 0.40 30
JHU 0.22 0.21 0.22 0.2 0.25 0.13 ?21278 ?20480 1.95 0.60 0.43 0.55 0.30 ?65 37 0.42 0.02 0.46 36
JHU-COMBO 0.24 0.23 0.24 0.29 0.31 0.19 ?12563 ?12688 1.97 0.53 0.5 0.60 0.33 ?65 44 0.48 0.03 0.52 43
ONLINE-B 0.24 0.23 0.24 0.29 0.31 0.19 ?10673 ?11506 1.97 0.52 0.50 0.60 0.33 ?65 44 0.49 0.03 0.52 43
SYSTRAN 0.20 0.2 0.21 0.18 0.22 0.11 ?23996 ?24570 1.94 0.63 0.42 0.52 0.29 ?65 36 0.4 0.02 0.45 34
UEDIN 0.22 0.22 0.23 0.22 0.26 0.14 ?14958 ?15342 1.96 0.59 0.45 0.57 0.31 ?65 40 0.44 0.03 0.48 39
UPPSALA 0.21 0.20 0.21 0.20 0.23 0.12 ?22233 ?22509 1.95 0.62 0.43 0.53 0.29 ?65 37 0.41 0.02 0.46 36
UPV-PRHLT-COMBO 0.24 0.23 0.24 0.29 0.31 0.19 ?13904 ?15260 1.97 0.54 0.49 0.60 0.33 ?65 44 0.48 0.03 0.52 43
Table 39: Automatic evaluation metric scores for systems in the WMT11 Czech-English News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
D
F
K
I-
PA
R
S
E
C
O
N
F
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
German-English News Task
BBN-COMBO 0.23 0.22 0.23 0.25 0.28 0.16 ?17103 ?17837 1.97 0.56 0.46 0.06 0.59 0.32 ?43 42 0.46 0.03 0.49 41
CMU-DYER 0.21 0.21 0.22 0.22 0.25 0.13 ?26089 ?29214 1.95 0.59 0.44 0.04 0.56 0.31 ?45 39 0.43 0.03 0.47 38
CMU-HEAFIELD-COMBO 0.23 0.22 0.23 0.24 0.27 0.15 ?12868 ?16156 1.96 0.57 0.47 0.07 0.58 0.32 ?44 41 0.46 0.03 0.51 40
CST 0.19 0.18 0.19 0.17 0.22 0.11 ?61131 ?60157 1.94 0.63 0.39 0.03 0.5 0.27 ?46 34 0.37 0.02 0.41 33
CU-ZEMAN 0.2 0.19 0.20 0.14 0.22 0.11 ?64860 ?61329 1.93 0.65 0.37 0.06 0.51 0.28 ?47 31 0.37 0.02 0.4 30
DFKI-XU 0.21 0.20 0.21 0.21 0.25 0.14 ?40171 ?39455 1.95 0.58 0.44 0.03 0.54 0.3 ?45 38 0.42 0.02 0.46 37
JHU 0.19 0.19 0.2 0.17 0.22 0.11 ?62997 ?58673 1.94 0.64 0.39 0.03 0.51 0.28 ?45 34 0.38 0.02 0.41 33
JHU-COMBO 0.22 0.22 0.23 0.24 0.27 0.15 ?30492 ?27016 1.96 0.57 0.46 0.04 0.57 0.31 ?44 41 0.45 0.03 0.48 39
KIT 0.21 0.21 0.22 0.22 0.25 0.13 ?31064 ?31930 1.95 0.6 0.44 0.05 0.55 0.31 ?44 39 0.43 0.02 0.47 37
KOC 0.2 0.2 0.20 0.18 0.23 0.12 ?52337 ?50231 1.94 0.63 0.41 0.05 0.52 0.29 ?45 35 0.39 0.02 0.43 34
KOC-COMBO 0.21 0.21 0.21 0.22 0.26 0.14 ?40002 ?38374 1.96 0.59 0.44 0.03 0.54 0.3 ?44 38 0.42 0.02 0.46 37
LIMSI 0.21 0.20 0.21 0.20 0.24 0.13 ?39419 ?38297 1.95 0.61 0.43 0.04 0.54 0.3 ?44 38 0.42 0.02 0.46 36
LINGUATEC 0.19 0.19 0.2 0.16 0.22 0.11 ?26064 ?31116 1.94 0.68 0.42 0.15 0.53 0.29 ?46 35 0.42 0.02 0.47 34
LIU 0.21 0.20 0.21 0.2 0.24 0.13 ?40281 ?40496 1.95 0.62 0.43 0.04 0.53 0.29 ?44 37 0.41 0.02 0.45 36
ONLINE-A 0.22 0.21 0.22 0.21 0.26 0.14 ?25411 ?25675 1.95 0.6 0.45 0.06 0.57 0.31 ?44 39 0.45 0.03 0.48 38
ONLINE-B 0.22 0.22 0.23 0.23 0.27 0.15 ?15149 ?19578 1.96 0.58 0.46 0.06 0.57 0.32 ?44 41 0.46 0.03 0.5 39
QUAERO-COMBO 0.21 0.21 0.22 0.22 0.26 0.14 ?34486 ?33449 1.96 0.58 0.45 0.03 0.55 0.30 ?44 39 0.43 0.03 0.47 38
RBMT-1 0.20 0.2 0.21 0.16 0.21 0.11 ?32960 ?34972 1.94 0.67 0.42 0.08 0.52 0.29 ?45 36 0.42 0.02 0.46 34
RBMT-2 0.19 0.19 0.2 0.15 0.2 0.1 ?40842 ?43413 1.94 0.69 0.4 0.11 0.50 0.28 ?45 34 0.4 0.02 0.44 33
RBMT-3 0.20 0.2 0.21 0.17 0.22 0.11 ?32476 ?33417 1.94 0.65 0.42 0.09 0.53 0.29 ?44 36 0.42 0.02 0.47 35
RBMT-4 0.20 0.2 0.21 0.17 0.22 0.11 ?34287 ?34604 1.94 0.66 0.42 0.08 0.52 0.29 ?45 36 0.42 0.02 0.47 35
RBMT-5 0.19 0.19 0.20 0.15 0.20 0.10 ?49097 ?46635 1.94 0.68 0.40 0.07 0.50 0.28 ?46 34 0.4 0.02 0.44 33
RWTH-LEUSCH-COMBO 0.22 0.22 0.23 0.24 0.28 0.16 ?22878 ?22089 1.96 0.56 0.46 0.03 0.58 0.32 ?44 41 0.45 0.03 0.49 40
RWTH-WUEBKER 0.21 0.20 0.21 0.21 0.24 0.13 ?35973 ?37140 1.95 0.60 0.44 0.04 0.54 0.3 ?45 38 0.42 0.02 0.45 37
UEDIN 0.21 0.20 0.21 0.19 0.23 0.12 ?32791 ?34633 1.95 0.63 0.43 0.07 0.54 0.3 ?45 37 0.42 0.02 0.46 36
UPPSALA 0.20 0.2 0.21 0.2 0.23 0.12 ?40448 ?41548 1.95 0.63 0.42 0.06 0.53 0.29 ?45 37 0.41 0.02 0.44 36
UPV-PRHLT-COMBO 0.22 0.21 0.22 0.23 0.27 0.15 ?33413 ?31778 1.96 0.58 0.45 0.03 0.57 0.31 ?44 40 0.44 0.03 0.48 39
UZH-COMBO 0.22 0.21 0.22 0.23 0.27 0.15 ?16326 ?20831 1.96 0.58 0.45 0.07 0.57 0.31 ?44 40 0.45 0.03 0.48 39
Table 40: Automatic evaluation metric scores for systems in the WMT11 German-English News Task
(newssyscombtest2011)
58
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
French-English News Task
BBN-COMBO 0.25 0.25 0.26 0.31 0.32 0.21 ?19552 ?22107 1.98 0.48 0.51 0.64 0.36 ?43 47 0.49 0.03 0.54 46
CMU-DENKOWSKI 0.24 0.24 0.24 0.26 0.29 0.17 ?34357 ?37807 1.97 0.53 0.48 0.61 0.34 ?45 43 0.46 0.03 0.50 42
CMU-HANNEMAN 0.24 0.23 0.24 0.27 0.29 0.17 ?33662 ?37698 1.97 0.52 0.49 0.60 0.33 ?45 44 0.46 0.03 0.51 42
CMU-HEAFIELD-COMBO 0.25 0.25 0.25 0.30 0.31 0.2 ?18365 ?22937 1.98 0.5 0.51 0.63 0.35 ?44 46 0.49 0.03 0.54 45
CU-ZEMAN 0.22 0.22 0.23 0.17 0.24 0.13 ?67586 ?64688 1.94 0.6 0.41 0.56 0.31 ?47 34 0.39 0.02 0.42 33
JHU 0.24 0.24 0.24 0.25 0.29 0.17 ?41567 ?39578 1.96 0.53 0.47 0.61 0.34 ?45 42 0.46 0.03 0.5 41
JHU-COMBO 0.25 0.25 0.25 0.31 0.32 0.20 ?32785 ?31712 1.98 0.49 0.50 0.63 0.35 ?43 47 0.48 0.03 0.53 45
KIT 0.25 0.24 0.25 0.29 0.31 0.19 ?22678 ?28283 1.98 0.51 0.50 0.63 0.35 ?44 46 0.49 0.03 0.53 44
LIA-LIG 0.25 0.24 0.25 0.29 0.3 0.18 ?34063 ?34716 1.97 0.52 0.49 0.62 0.34 ?44 45 0.48 0.03 0.52 44
LIMSI 0.25 0.24 0.25 0.28 0.29 0.18 ?26269 ?29363 1.97 0.52 0.5 0.62 0.34 ?44 45 0.48 0.03 0.52 44
LIUM 0.25 0.24 0.25 0.29 0.30 0.19 ?29288 ?36137 1.98 0.52 0.49 0.62 0.34 ?44 45 0.48 0.03 0.53 44
LIUM-COMBO 0.25 0.24 0.25 0.31 0.31 0.2 ?30678 ?35365 1.98 0.50 0.5 0.62 0.34 ?44 46 0.48 0.03 0.53 45
ONLINE-A 0.25 0.24 0.25 0.27 0.3 0.18 ?38761 ?34096 1.97 0.52 0.49 0.62 0.34 ?44 44 0.48 0.03 0.52 43
ONLINE-B 0.25 0.24 0.25 0.29 0.31 0.19 ?19157 ?25284 1.98 0.50 0.51 0.62 0.35 ?45 46 0.49 0.03 0.54 44
RBMT-1 0.24 0.23 0.24 0.23 0.26 0.15 ?49115 ?39153 1.96 0.59 0.46 0.60 0.33 ?43 42 0.46 0.03 0.51 41
RBMT-2 0.23 0.22 0.23 0.21 0.24 0.13 ?59549 ?50466 1.95 0.63 0.44 0.57 0.32 ?43 40 0.43 0.02 0.48 39
RBMT-3 0.23 0.23 0.23 0.22 0.25 0.14 ?52047 ?45073 1.96 0.59 0.46 0.58 0.32 ?44 41 0.45 0.02 0.50 40
RBMT-4 0.23 0.22 0.24 0.22 0.25 0.14 ?54507 ?42933 1.96 0.63 0.45 0.59 0.33 ?43 40 0.44 0.02 0.49 39
RBMT-5 0.23 0.22 0.23 0.21 0.24 0.13 ?55545 ?48332 1.95 0.62 0.45 0.57 0.32 ?44 40 0.44 0.02 0.49 38
RWTH-HUCK 0.24 0.24 0.25 0.28 0.3 0.18 ?44018 ?42549 1.97 0.52 0.49 0.61 0.34 ?44 44 0.47 0.03 0.51 43
RWTH-LEUSCH-COMBO 0.26 0.25 0.26 0.31 0.32 0.20 ?21914 ?21746 1.98 0.49 0.51 0.64 0.35 ?43 47 0.50 0.03 0.54 46
SYSTRAN 0.24 0.23 0.24 0.25 0.27 0.16 ?34321 ?40119 1.96 0.54 0.48 0.59 0.33 ?44 43 0.46 0.03 0.51 41
UEDIN 0.23 0.23 0.24 0.25 0.27 0.16 ?47202 ?47955 1.96 0.56 0.47 0.59 0.33 ?45 42 0.45 0.03 0.49 40
UPV-PRHLT-COMBO 0.25 0.25 0.26 0.31 0.32 0.20 ?26947 ?28689 1.98 0.5 0.51 0.63 0.35 ?43 47 0.49 0.03 0.54 46
Table 41: Automatic evaluation metric scores for systems in the WMT11 French-English News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Spanish-English News Task
ALACANT 0.24 0.23 0.24 0.27 0.28 0.17 ?30135 ?29622 1.97 0.53 0.46 0.61 0.34 ?45 43 0.46 0.03 0.50 42
BBN-COMBO 0.25 0.25 0.25 0.32 0.33 0.21 ?15284 ?16192 1.98 0.48 0.5 0.64 0.35 ?44 47 0.49 0.03 0.53 46
CMU-HEAFIELD-COMBO 0.25 0.25 0.25 0.32 0.31 0.20 ?13456 ?16113 1.98 0.5 0.5 0.64 0.35 ?44 47 0.5 0.03 0.54 46
CU-ZEMAN 0.20 0.20 0.21 0.16 0.22 0.12 ?49428 ?48440 1.93 0.61 0.36 0.51 0.28 ?49 32 0.35 0.02 0.38 31
HYDERABAD 0.20 0.20 0.21 0.17 0.21 0.11 ?47754 ?47059 1.94 0.61 0.39 0.50 0.28 ?47 34 0.36 0.02 0.41 33
JHU-COMBO 0.25 0.25 0.25 0.32 0.32 0.20 ?23939 ?22685 1.98 0.49 0.49 0.63 0.35 ?44 47 0.48 0.03 0.52 46
KOC 0.24 0.24 0.24 0.26 0.29 0.17 ?22724 ?25857 1.96 0.53 0.46 0.61 0.34 ?45 42 0.46 0.03 0.49 41
KOC-COMBO 0.25 0.24 0.25 0.28 0.30 0.19 ?22678 ?22267 1.97 0.52 0.48 0.62 0.34 ?44 44 0.48 0.03 0.52 43
ONLINE-A 0.25 0.24 0.25 0.28 0.3 0.18 ?19017 ?20120 1.97 0.52 0.48 0.63 0.35 ?44 45 0.48 0.03 0.52 43
ONLINE-B 0.24 0.24 0.24 0.29 0.30 0.19 ?11980 ?18589 1.97 0.50 0.49 0.62 0.34 ?45 45 0.49 0.03 0.53 44
RBMT-1 0.24 0.24 0.25 0.28 0.28 0.17 ?31202 ?26151 1.97 0.57 0.46 0.61 0.34 ?44 45 0.47 0.03 0.51 43
RBMT-2 0.23 0.23 0.24 0.24 0.25 0.15 ?35157 ?31405 1.96 0.6 0.44 0.59 0.33 ?44 42 0.44 0.02 0.49 41
RBMT-3 0.23 0.23 0.24 0.25 0.26 0.15 ?28289 ?26082 1.97 0.59 0.45 0.6 0.33 ?43 43 0.46 0.03 0.51 42
RBMT-4 0.24 0.23 0.24 0.25 0.26 0.16 ?27892 ?25546 1.97 0.59 0.46 0.60 0.33 ?43 43 0.46 0.03 0.52 42
RBMT-5 0.24 0.23 0.24 0.27 0.26 0.16 ?36770 ?31613 1.96 0.58 0.45 0.6 0.33 ?45 43 0.45 0.03 0.50 42
RWTH-LEUSCH-COMBO 0.25 0.25 0.26 0.32 0.32 0.21 ?15172 ?15261 1.98 0.49 0.5 0.64 0.35 ?43 48 0.50 0.03 0.54 47
SYSTRAN 0.24 0.23 0.24 0.27 0.28 0.17 ?20129 ?26051 1.97 0.53 0.47 0.60 0.33 ?46 44 0.46 0.03 0.51 42
UEDIN 0.22 0.22 0.23 0.22 0.25 0.14 ?25462 ?31678 1.96 0.58 0.45 0.57 0.32 ?47 40 0.44 0.03 0.48 39
UFAL-UM 0.23 0.22 0.23 0.23 0.24 0.14 ?42123 ?37765 1.96 0.60 0.43 0.58 0.32 ?43 41 0.43 0.02 0.48 40
UPM 0.22 0.22 0.23 0.22 0.24 0.14 ?39748 ?38433 1.95 0.58 0.43 0.57 0.32 ?45 40 0.42 0.02 0.46 38
UPV-PRHLT-COMBO 0.25 0.25 0.26 0.32 0.32 0.20 ?16094 ?17723 1.98 0.50 0.49 0.64 0.35 ?43 47 0.5 0.03 0.54 46
Table 42: Automatic evaluation metric scores for systems in the WMT11 Spanish-English News Task
(newssyscombtest2011)
59
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
W
M
P
F
English-Czech News Task
CMU-HEAFIELD-COMBO 0.2 0.19 0.20 0.19 0.22 0.12 2.03 0.62 0.24 ?62 29 27
COMMERCIAL1 0.16 0.15 0.16 0.11 0.16 0.08 2.01 0.70 0.19 ?65 22 21
COMMERCIAL2 0.12 0.10 0.13 0.09 0.15 0.06 2.00 0.73 0.18 ?65 21 19
CU-BOJAR 0.18 0.17 0.18 0.16 0.2 0.1 2.02 0.65 0.23 ?63 26 24
CU-MARECEK 0.18 0.17 0.18 0.16 0.2 0.1 2.02 0.65 0.22 ?63 26 24
CU-POPEL 0.17 0.16 0.18 0.14 0.19 0.1 2.02 0.66 0.21 ?64 25 23
CU-TAMCHYNA 0.18 0.17 0.18 0.15 0.2 0.1 2.02 0.65 0.22 ?63 26 24
CU-ZEMAN 0.17 0.16 0.17 0.13 0.18 0.09 2.02 0.66 0.21 ?63 23 22
JHU 0.18 0.18 0.18 0.16 0.21 0.11 2.02 0.63 0.22 ?63 26 24
ONLINE-B 0.2 0.19 0.20 0.2 0.22 0.12 2.03 0.62 0.24 ?63 29 27
UEDIN 0.19 0.18 0.19 0.17 0.21 0.11 2.03 0.63 0.23 ?63 27 26
UPV-PRHLT-COMBO 0.2 0.19 0.20 0.20 0.23 0.13 2.03 0.61 0.24 ?63 29 28
Table 43: Automatic evaluation metric scores for systems in the WMT11 English-Czech News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
English-German News Task
CMU-HEAFIELD-COMBO 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.66 0.39 ?46 36 0.41 0.03 0.45 35
COPENHAGEN 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?47 33 0.38 0.02 0.42 32
CU-TAMCHYNA 0.17 0.17 0.18 0.11 0.18 0.09 1.94 0.70 0.36 ?48 31 0.36 0.02 0.4 30
CU-ZEMAN 0.16 0.15 0.16 0.05 0.17 0.08 1.92 0.71 0.34 ?51 25 0.31 0.02 0.34 25
DFKI-FEDERMANN 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.71 0.34 ?47 33 0.38 0.03 0.44 32
DFKI-XU 0.18 0.17 0.18 0.15 0.19 0.1 1.96 0.68 0.37 ?47 35 0.39 0.03 0.43 34
ILLC-UVA 0.15 0.14 0.15 0.12 0.18 0.08 1.95 0.68 0.33 ?49 32 0.36 0.02 0.4 31
JHU 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.68 0.35 ?47 33 0.37 0.02 0.42 32
KIT 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.68 0.37 ?47 35 0.39 0.03 0.43 34
KOC 0.17 0.16 0.17 0.12 0.17 0.08 1.95 0.69 0.35 ?47 32 0.36 0.02 0.40 31
KOC-COMBO 0.18 0.17 0.18 0.15 0.2 0.1 1.95 0.67 0.37 ?47 34 0.38 0.02 0.42 33
LIMSI 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.67 0.36 ?47 35 0.39 0.03 0.44 33
LIU 0.17 0.17 0.18 0.15 0.19 0.09 1.95 0.68 0.36 ?47 34 0.38 0.02 0.43 33
ONLINE-A 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.67 0.37 ?47 35 0.40 0.03 0.45 33
ONLINE-B 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.65 0.38 ?46 36 0.42 0.03 0.46 35
RBMT-1 0.17 0.17 0.18 0.13 0.18 0.08 1.95 0.7 0.35 ?46 34 0.39 0.03 0.45 33
RBMT-2 0.16 0.16 0.17 0.12 0.16 0.08 1.94 0.73 0.33 ?47 32 0.37 0.03 0.43 31
RBMT-3 0.18 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?46 35 0.39 0.03 0.46 34
RBMT-4 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.70 0.34 ?47 33 0.38 0.03 0.45 32
RBMT-5 0.17 0.16 0.17 0.12 0.17 0.08 1.95 0.71 0.34 ?47 33 0.38 0.03 0.44 32
RWTH-FREITAG 0.17 0.17 0.17 0.15 0.19 0.09 1.95 0.68 0.36 ?47 34 0.37 0.02 0.41 33
UEDIN 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?47 34 0.38 0.02 0.42 33
UOW 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.7 0.35 ?47 33 0.37 0.02 0.42 32
UPPSALA 0.17 0.16 0.17 0.14 0.18 0.09 1.95 0.68 0.35 ?47 33 0.37 0.02 0.42 32
UPV-PRHLT-COMBO 0.18 0.18 0.19 0.17 0.20 0.10 1.96 0.66 0.38 ?46 36 0.4 0.03 0.44 35
UZH-COMBO 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.66 0.38 ?46 36 0.40 0.03 0.44 35
Table 44: Automatic evaluation metric scores for systems in the WMT11 English-German News Task
(newssyscombtest2011)
60
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
English-French News Task
CMU-HEAFIELD-COMBO 0.25 0.25 0.26 0.34 0.35 0.23 2.02 0.5 0.57 ?41 52 0.54 ?0.01 0.60 50
CU-ZEMAN 0.18 0.17 0.18 0.13 0.19 0.09 1.96 0.68 0.39 ?46 35 0.34 ?0.03 0.40 33
JHU 0.23 0.23 0.24 0.27 0.31 0.19 2.01 0.53 0.52 ?43 47 0.49 ?0.01 0.55 45
KIT 0.24 0.23 0.24 0.29 0.31 0.19 2.01 0.52 0.53 ?42 49 0.51 ?0.01 0.57 47
LATL-GENEVA 0.20 0.2 0.21 0.19 0.23 0.12 1.99 0.62 0.44 ?43 41 0.44 ?0.02 0.51 39
LIMSI 0.24 0.24 0.24 0.3 0.31 0.19 2.01 0.53 0.53 ?41 49 0.51 ?0.01 0.58 48
LIUM 0.24 0.23 0.24 0.29 0.31 0.19 2.01 0.53 0.53 ?42 49 0.51 ?0.01 0.57 47
ONLINE-A 0.24 0.23 0.24 0.27 0.3 0.18 2.01 0.53 0.52 ?42 47 0.5 ?0.01 0.56 46
ONLINE-B 0.25 0.25 0.25 0.33 0.35 0.23 2.02 0.5 0.56 ?42 51 0.53 ?0.01 0.59 50
RBMT-1 0.23 0.22 0.23 0.24 0.27 0.16 2.00 0.56 0.5 ?41 45 0.48 ?0.02 0.56 44
RBMT-2 0.22 0.21 0.22 0.22 0.25 0.14 1.99 0.58 0.47 ?42 44 0.46 ?0.02 0.53 42
RBMT-3 0.23 0.22 0.23 0.25 0.28 0.16 2.00 0.56 0.5 ?41 46 0.48 ?0.02 0.56 44
RBMT-4 0.22 0.21 0.22 0.23 0.26 0.15 1.99 0.58 0.47 ?42 43 0.45 ?0.02 0.51 42
RBMT-5 0.22 0.22 0.23 0.23 0.27 0.15 2 0.57 0.49 ?41 45 0.47 ?0.02 0.55 43
RWTH-HUCK 0.23 0.23 0.24 0.29 0.30 0.18 2.01 0.54 0.52 ?42 48 0.5 ?0.01 0.56 47
UEDIN 0.23 0.22 0.23 0.27 0.3 0.18 2.01 0.54 0.51 ?42 47 0.49 ?0.01 0.55 46
UPPSALA 0.23 0.22 0.23 0.27 0.29 0.17 2.00 0.55 0.51 ?42 46 0.48 ?0.01 0.55 45
UPPSALA-FBK 0.23 0.23 0.23 0.28 0.29 0.18 2.01 0.55 0.51 ?42 47 0.49 ?0.01 0.55 46
UPV-PRHLT-COMBO 0.25 0.24 0.25 0.32 0.34 0.22 2.02 0.50 0.55 ?41 51 0.53 ?0.01 0.59 49
Table 45: Automatic evaluation metric scores for systems in the WMT11 English-French News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
English-Spanish News Task
CEU-UPV 0.24 0.24 0.24 0.29 0.3 0.18 2.01 0.51 0.55 ?45 46 0.45 0.01 0.45 45
CMU-HEAFIELD-COMBO 0.26 0.25 0.26 0.35 0.34 0.22 2.02 0.47 0.58 ?44 50 0.49 0.01 0.49 49
CU-ZEMAN 0.23 0.22 0.23 0.22 0.27 0.15 1.99 0.55 0.52 ?48 39 0.41 0.00 0.41 38
KOC 0.23 0.23 0.23 0.25 0.27 0.16 2 0.54 0.52 ?46 43 0.42 0.00 0.43 42
KOC-COMBO 0.25 0.24 0.25 0.31 0.32 0.2 2.01 0.5 0.56 ?44 47 0.46 0.01 0.47 46
ONLINE-A 0.25 0.24 0.25 0.31 0.32 0.2 2.01 0.49 0.56 ?44 48 0.46 0.01 0.47 46
ONLINE-B 0.25 0.25 0.25 0.33 0.32 0.2 2.02 0.50 0.57 ?44 49 0.47 0.01 0.47 48
PROMT 0.24 0.23 0.24 0.28 0.28 0.17 2.00 0.53 0.52 ?45 45 0.44 0.01 0.46 43
RBMT-1 0.23 0.23 0.23 0.25 0.27 0.16 2 0.55 0.51 ?45 43 0.42 0.00 0.44 42
RBMT-2 0.23 0.22 0.23 0.25 0.26 0.15 1.99 0.55 0.5 ?44 43 0.41 0.00 0.42 41
RBMT-3 0.24 0.23 0.24 0.28 0.28 0.17 2.00 0.53 0.52 ?44 45 0.43 0.00 0.45 43
RBMT-4 0.23 0.22 0.23 0.26 0.26 0.16 1.99 0.54 0.51 ?44 44 0.42 0.00 0.43 42
RBMT-5 0.23 0.22 0.23 0.24 0.26 0.15 1.99 0.57 0.49 ?45 42 0.41 0.00 0.43 41
UEDIN 0.24 0.24 0.24 0.31 0.3 0.18 2.01 0.51 0.55 ?45 47 0.45 0.01 0.45 46
UOW 0.23 0.23 0.24 0.28 0.28 0.16 2.00 0.53 0.53 ?45 45 0.42 0.01 0.43 44
UOW-COMBO 0.25 0.25 0.25 0.33 0.32 0.2 2.01 0.50 0.56 ?44 49 0.47 0.01 0.47 47
UPM 0.21 0.21 0.21 0.21 0.22 0.12 1.98 0.61 0.47 ?47 39 0.37 0.00 0.37 38
UPPSALA 0.24 0.24 0.24 0.3 0.29 0.18 2.01 0.51 0.54 ?45 46 0.44 0.01 0.44 45
UPV-PRHLT-COMBO 0.25 0.25 0.25 0.33 0.32 0.21 2.02 0.49 0.57 ?44 49 0.47 0.01 0.48 48
Table 46: Automatic evaluation metric scores for systems in the WMT11 English-Spanish News Task
(newssyscombtest2011)
61
B
L
E
U
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Haitian Creole (clean)-English Haitian Creole SMS Emergency Response Featured Translation Task
BM-I2R 0.33 ?6798 ?4575 1.96 0.51 0.62 0.34 43 0.44 0.03 0.46 43
CMU-DENKOWSKI 0.29 ?6849 ?6172 1.95 0.53 0.58 0.32 40 0.39 0.02 0.40 39
CMU-HEAFIELD-COMBO 0.32 ?6188 ?4347 1.96 0.51 0.61 0.34 42 0.43 0.03 0.45 42
CMU-HEWAVITHARANA 0.28 ?6523 ?6341 1.95 0.57 0.57 0.32 39 0.38 0.02 0.40 38
HYDERABAD 0.14 ?7548 ?8502 1.92 0.66 0.50 0.28 26 0.3 0.02 0.30 26
KOC 0.23 ?6490 ?9020 1.94 0.67 0.49 0.27 36 0.32 0.02 0.34 35
KOC-COMBO 0.29 ?4901 ?5349 1.95 0.57 0.56 0.31 39 0.38 0.02 0.4 39
LIU 0.27 ?6526 ?6078 1.95 0.59 0.56 0.31 38 0.38 0.02 0.39 37
UMD-EIDELMAN 0.26 ?4407 ?6215 1.95 0.57 0.55 0.31 38 0.37 0.02 0.4 37
UMD-HU 0.22 ?6379 ?7460 1.94 0.59 0.51 0.28 35 0.36 0.02 0.39 34
UPPSALA 0.27 ?5497 ?6754 1.95 0.59 0.54 0.3 38 0.36 0.02 0.39 37
UPV-PRHLT-COMBO 0.32 ?6896 ?5968 1.96 0.53 0.6 0.33 42 0.41 0.02 0.43 41
Table 47: Automatic evaluation metric scores for systems in the WMT11 Haitian Creole (clean)-English Haitian
Creole SMS Emergency Response Featured Translation Task (newssyscombtest2011)
B
L
E
U
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Haitian Creole (raw)-English Haitian Creole SMS Emergency Response Featured Translation Task
BM-I2R 0.29 ?3885 ?3017 1.96 0.57 0.57 0.32 39 0.42 0.02 0.44 38
CMU-DENKOWSKI 0.25 ?3965 ?3905 1.95 0.60 0.53 0.3 35 0.38 0.02 0.4 35
CMU-HEAFIELD-COMBO 0.28 ?3057 ?2588 1.96 0.57 0.57 0.32 39 0.42 0.02 0.44 38
CMU-HEWAVITHARANA 0.25 ?3701 ?3824 1.95 0.61 0.53 0.3 35 0.37 0.02 0.39 35
JHU 0.14 ?3207 ?4279 1.92 0.74 0.43 0.24 26 0.30 0.02 0.32 26
LIU 0.25 ?3447 ?3445 1.95 0.60 0.54 0.30 36 0.38 0.02 0.4 35
UMD-EIDELMAN 0.24 ?2826 ?3754 1.94 0.64 0.52 0.29 34 0.36 0.02 0.39 34
UPV-PRHLT-COMBO 0.28 ?3591 ?3370 1.95 0.58 0.56 0.32 38 0.4 0.02 0.42 38
Table 48: Automatic evaluation metric scores for systems in the WMT11 Haitian Creole (raw)-English Haitian Creole
SMS Emergency Response Featured Translation Task (newssyscombtest2011)
62
INTER-ANNOTATOR AGREEMENT (I.E. ACROSS ANNOTATORS)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
Czech-English, individual systems 0.591 0.354 0.367 0.535 0.343 0.293
English-Czech, individual systems 0.608 0.359 0.388 0.552 0.350 0.312
German-English, individual systems 0.562 0.377 0.298 0.536 0.370 0.264
English-German, individual systems 0.564 0.352 0.327 0.528 0.348 0.276
Spanish-English, individual systems 0.695 0.398 0.493 0.683 0.393 0.477
English-Spanish, individual systems 0.574 0.343 0.352 0.548 0.339 0.317
French-English, individual systems 0.616 0.367 0.393 0.584 0.361 0.349
English-French, individual systems 0.631 0.382 0.403 0.603 0.376 0.363
European languages, individual systems 0.601 0.362 0.375 0.561 0.355 0.320
Czech-English, system combinations 0.700 0.334 0.549 0.577 0.369 0.329
English-Czech, system combinations 0.812 0.348 0.711 0.696 0.392 0.500
German-English, system combinations 0.675 0.353 0.498 0.629 0.341 0.437
English-German, system combinations 0.608 0.346 0.401 0.547 0.334 0.320
Spanish-English, system combinations 0.638 0.335 0.456 0.604 0.359 0.382
English-Spanish, system combinations 0.657 0.335 0.485 0.603 0.371 0.369
French-English, system combinations 0.654 0.336 0.479 0.608 0.336 0.410
English-French, system combinations 0.678 0.352 0.503 0.595 0.339 0.388
European languages, system combinations 0.671 0.335 0.505 0.598 0.342 0.389
Haitian (Clean)-English, individual systems 0.693 0.364 0.517 0.640 0.353 0.443
Haitian (Raw)-English, individual systems 0.689 0.357 0.517 0.639 0.344 0.450
Haitian-English, individual systems 0.691 0.362 0.516 0.639 0.350 0.446
Haitian (Clean)-English, system combinations 0.770 0.367 0.636 0.645 0.333 0.468
Haitian (Raw)-English, system combinations 0.745 0.345 0.611 0.753 0.361 0.613
Haitian-English, system combinations 0.761 0.358 0.628 0.674 0.335 0.509
Tunable metrics task (Urdu-English) 0.692 0.337 0.535 0.641 0.363 0.437
WMT10 (European languages, individual vs. individual) 0.663 0.394 0.445 0.620 0.385 0.382
WMT10 (European languages, combo vs. combo) 0.728 0.344 0.586 0.629 0.334 0.443
WMT10 (European languages, individual vs. combo) N/A N/A N/A 0.634 0.360 0.428
WMT10 (European languages, all systems) 0.658 0.374 0.454 0.626 0.367 0.409
Table 49: Inter-annotator agreement rates, for the various manual evaluation tracks of WMT11, broken down by
language pair. The highlighted rows correspond to rows in the top half of Table 7. See Table 50 below for detailed
intra-annotator agreement rates.
63
INTRA-ANNOTATOR AGREEMENT (I.E. SELF-CONSISTENCY)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
Czech-English, individual systems 0.762 0.354 0.632 0.713 0.343 0.564
English-Czech, individual systems 0.743 0.359 0.598 0.700 0.350 0.539
German-English, individual systems 0.675 0.377 0.478 0.670 0.370 0.475
English-German, individual systems 0.704 0.352 0.543 0.700 0.348 0.541
Spanish-English, individual systems 0.750 0.398 0.585 0.719 0.393 0.537
English-Spanish, individual systems 0.644 0.343 0.458 0.601 0.339 0.396
French-English, individual systems 0.829 0.367 0.730 0.816 0.361 0.712
English-French, individual systems 0.716 0.382 0.541 0.681 0.376 0.488
European languages, individual systems 0.722 0.362 0.564 0.685 0.355 0.512
Czech-English, system combinations 0.756 0.334 0.633 0.657 0.369 0.457
English-Czech, system combinations 0.923 0.348 0.882 0.842 0.392 0.740
German-English, system combinations 0.732 0.353 0.586 0.716 0.341 0.569
English-German, system combinations 0.722 0.346 0.575 0.676 0.334 0.513
Spanish-English, system combinations 0.783 0.335 0.673 0.720 0.359 0.562
English-Spanish, system combinations 0.741 0.335 0.610 0.711 0.371 0.540
French-English, system combinations 0.772 0.336 0.657 0.659 0.336 0.487
English-French, system combinations 0.841 0.352 0.755 0.714 0.339 0.568
European languages, system combinations 0.787 0.335 0.680 0.717 0.342 0.571
Haitian (Clean)-English, individual systems 0.758 0.364 0.619 0.686 0.353 0.515
Haitian (Raw)-English, individual systems 0.783 0.357 0.663 0.756 0.344 0.628
Haitian-English, individual systems 0.763 0.362 0.628 0.700 0.350 0.539
Haitian (Clean)-English, system combinations 0.882 0.367 0.813 0.778 0.333 0.667
Haitian (Raw)-English, system combinations 0.882 0.345 0.820 0.802 0.361 0.690
Haitian-English, system combinations 0.882 0.358 0.816 0.784 0.335 0.675
Tunable metrics task (Urdu-English) 0.857 0.337 0.784 0.856 0.363 0.774
WMT10 (European languages, individual vs. individual) 0.757 0.394 0.599 0.728 0.385 0.557
WMT10 (European languages, combo vs. combo) 0.783 0.344 0.670 0.719 0.334 0.578
WMT10 (European languages, individual vs. combo) N/A N/A N/A 0.746 0.360 0.603
WMT10 (European languages, all systems) 0.755 0.374 0.609 0.734 0.367 0.580
Table 50: Intra-annotator agreement rates, for the various manual evaluation tracks of WMT11, broken down by
language pair. The highlighted rows correspond to rows in the bottom half of Table 7. See Table 49 above for detailed
inter-annotator agreement rates.
64
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 478?484,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joshua 3.0: Syntax-based Machine Translation
with the Thrax Grammar Extractor
Jonathan Weese1, Juri Ganitkevitch1, Chris Callison-Burch1, Matt Post2 and Adam Lopez1,2
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present progress on Joshua, an open-
source decoder for hierarchical and syntax-
based machine translation. The main fo-
cus is describing Thrax, a flexible, open
source synchronous context-free grammar ex-
tractor. Thrax extracts both hierarchical (Chi-
ang, 2007) and syntax-augmented machine
translation (Zollmann and Venugopal, 2006)
grammars. It is built on Apache Hadoop for
efficient distributed performance, and can eas-
ily be extended with support for new gram-
mars, feature functions, and output formats.
1 Introduction
Joshua is an open-source1 toolkit for hierarchical
machine translation of human languages. The origi-
nal version of Joshua (Li et al, 2009) was a reim-
plementation of the Python-based Hiero machine-
translation system (Chiang, 2007); it was later ex-
tended (Li et al, 2010) to support richer formalisms,
such as SAMT (Zollmann and Venugopal, 2006).
The main focus of this paper is to describe this
past year?s work in developing Thrax (Weese, 2011),
an open-source grammar extractor for Hiero and
SAMT grammars. Grammar extraction has shown
itself to be something of a black art, with decod-
ing performance depending crucially on a variety
of features and options that are not always clearly
described in papers. This hindered direct com-
parison both between and within grammatical for-
malisms. Thrax standardizes Joshua?s grammar ex-
1http://github.com/joshua-decoder/joshua
traction procedures by providing a flexible and con-
figurable means of specifying these settings. Sec-
tion 3 presents a systematic comparison of the two
grammars using identical feature sets.
In addition, Joshua now includes a single pa-
rameterized script that implements the entire MT
pipeline, from data preparation to evaluation. This
script is built on top of a module called CachePipe.
CachePipe is a simple wrapper around shell com-
mands that uses SHA-1 hashes and explicitly-
provided lists of dependencies to determine whether
a command needs to be run, saving time both in run-
ning and debugging machine translation pipelines.
2 Thrax: grammar extraction
In modern machine translation systems such as
Joshua (Li et al, 2009) and cdec (Dyer et al, 2010),
a translation model is represented as a synchronous
context-free grammar (SCFG). Formally, an SCFG
may be considered as a tuple
(N,S, T?, T? , G)
where N is a set of nonterminal symbols of the
grammar, S ? N is the goal symbol, T? and T?
are the source- and target-side terminal symbol vo-
cabularies, respectively, and G is a set of production
rules of the grammar.
Each rule in G is of the form
X ? ??, ?,??
where X ? N is a nonterminal symbol, ? is a se-
quence of symbols from N ? T?, ? is a sequence of
478
symbols from N ? T? , and ? is a one-to-one cor-
respondence between the nonterminal symbols of ?
and ?.
The language of an SCFG is a set of ordered pairs
of strings. During decoding, the set of candidate
translations of an input sentence f is the set of all
e such that the pair (f, e) is licensed by the transla-
tion model SCFG. Each candidate e is generated by
applying a sequence of production rules (r1 . . . rn).
The cost of applying each rule is:
w(X ? ??, ??) =
?
i
?i(X ? ??, ??)
?i (1)
where each ?i is a feature function and ?i is the
weight for ?i. The total translation model score of
a candidate e is the product of the rules used in its
derivation. This translation model score is then com-
bined with other features (such as a language model
score) to produce an overall score for each candidate
translation.
2.1 Hiero and SAMT
Throughout this work, we will reference two par-
ticular SCFG types known as Hiero and Syntax-
Augmented Machine Translation (SAMT).
A Hiero grammar (Chiang, 2007) is an SCFG
with only one type of nonterminal symbol, tradi-
tionally labeled X . A Hiero grammar can be ex-
tracted from a parallel corpus of word-aligned sen-
tence pairs as follows: If (f ji , e
l
k) is a sub-phrase
of the sentence pair, we say it is consistent with
the pair?s alignment if none of the words in f ji are
aligned to words outside of elk, and vice-versa. The
consistent sub-phrase may be extracted as an SCFG
rule. Furthermore, if a consistent phrase is contained
within another one, a hierarchical rule may be ex-
tracted by replacing the smaller piece with a nonter-
minal.
An SAMT grammar (Zollmann and Venugopal,
2006) is similar to a Hiero grammar, except that the
nonterminal symbol set is much larger, and its la-
bels are derived from a parse tree over either the
source or target side in the following manner. For
each rule, if the target side is spanned by one con-
stituent of the parse tree, we assign that constituent?s
label as the nonterminal symbol for the rule. Other-
wise, we assign an extended category of the form
C1 + C2, C1/C2, or C2 \C1 ? indicating that the
das begr??e ich sehr .
i
very
much
welcome this
.
PRP
NP
S
RB RB
ADVP
VP
VBP DT
NP
.
Figure 1: An aligned sentence pair.
target side spans two adjacent constituents, is a C1
missing a C2 to the right, or is a C1 missing a C2
on the left, respectively. Table 1 contains a list of
Hiero and SAMT rules extracted from the training
sentence pair in Figure 1.
2.2 System overview
The following were goals in the design of Thrax:
? the ability to extract different SCFGs (such as
Hiero and SAMT), and to adjust various extrac-
tion parameters for the grammars;
? the ability to easily change and extend the fea-
ture sets for each rule
? scalability to arbitrarily large training corpora.
Thrax treats the grammar extraction and scoring
as a series of dependent Hadoop jobs. Hadoop
(Venugopal and Zollmann, 2009) is an implementa-
tion of Google?s MapReduce (Dean and Ghemawat,
2004), a framework for distributed processing of
large data sets. Hadoop jobs have two parts. In the
map step, a set of key/value pairs is mapped to a set
of intermediate key/value pairs. In the reduce step,
all intermediate values associated with an interme-
diate key are merged.
The first step in the Thrax pipeline is to extract all
the grammar rules. The map step in this job takes as
input word-aligned sentence pairs and produces a set
of ordered pairs (r, c) where r is a rule and c is the
number of times it was extracted. During the reduce
step, these rule counts are summed, so the result is
a set of rules, along with the total number of times
each rule was extracted from the entire corpus.
479
Span Hiero SAMT
[1, 3] X ? ?sehr, very much? ADV P ? ?sehr, very much?
[0, 3] X ? ?X sehr, X very much? PRP +ADV P ? ?PRP sehr, PRP very much?
[3, 4] X ? ?begru??e,welcome? V BP ? ?begru??e,welcome?
[0, 6] X ? ?X ich sehr ., i very much X .? S ? ?V P ich sehr ., i very much V P .?
[0, 6] X ? ?X ., X .? S ? ?S/. ., S/. .?
Table 1: A subset of the Hiero and SAMT rules extracted from the sentence pair of Figure 1.
Given the rules and their counts, a separate
Hadoop job is run for each feature. These jobs can
all be submitted at once and run in parallel, avoid-
ing the linear sort-and-score workflow. The output
from each feature job is the same set of pairs (r, c)
as the input, except each rule r has been annotated
with some feature score f .
After the feature jobs have been completed, we
have several copies of the grammar, each of which
has been scored with one feature. A final Hadoop
job combines all these scores to produce the final
grammar.
Some users may not have access to a Hadoop
cluster. Thrax can be run in standalone or pseudo-
distributed mode on a single machine. It can also
be used with Amazon Elastic MapReduce,2 a web
service that provides computation time on a Hadoop
cluster on-demand.
2.3 Extraction
The first step in the Thrax workflow is the extraction
of grammar rules from an input corpus. As men-
tioned above, Hiero and SAMT grammars both re-
quire a parallel corpus with word-level alignments.
SAMT additionally requires that the target side of
the corpus be parsed.
There are several parameters that can make a sig-
nificant difference in a grammar?s overall translation
performance. Each of these parameters is easily ad-
justable in Thrax by changing its value in a configu-
ration file.
? maximum rule span
? maximum span of consistent phrase pairs
? maximum number of nonterminals
? minimum number of aligned terminals in rule
2http://aws.amazon.com/elasticmapreduce/
? whether to allow adjacent nonterminals on
source side
? whether to allow unaligned words at the edges
of consistent phrase pairs
Chiang (2007) gives reasonable heuristic choices
for these parameters when extracting a Hiero gram-
mar, and Lopez (2008) confirms some of them (max-
imum rule span of 10, maximum number of source-
side symbols at 5, and maximum number of non-
terminals at 2 per rule). ?) provided comparisons
among phrase-based, hierarchical, and syntax-based
models, but did not report extensive experimentation
with the model parameterizations.
When extracting Hiero- or SAMT-style gram-
mars, the first Hadoop job in the Thrax workflow
takes in a parallel corpus and produces a set of rules.
But in fact Thrax?s extraction mechanism is more
general than that; all it requires is a function that
maps a string to a set of rules. This makes it easy
to implement new grammars and extract them using
Thrax.
2.4 Feature functions
Thrax considers feature functions of two types: first,
there are features that can be calculated by looking
at each rule in isolation. Such features do not re-
quire a Hadoop job to calculate their scores, since
we may inspect the rules in any order. (In practice,
we calculate the scores at the very last moment be-
fore outputting the final grammar.) We call these
features simple features. Thrax implements the fol-
lowing simple features:
? a binary indicator functions denoting:
? whether the rule is purely abstract (i.e.,
has no terminal symbols)
480
? the rule is purely lexical (i.e., has no non-
terminals)
? the rule is monotonic or has reordering
? the rule has adjacent nonterminals on the
source side
? counters for
? the number of unaligned words in the rule
? the number of terminals on the target side
of the rule
? a constant phrase penalty
In addition to simple features, Thrax also imple-
ments map-reduce features. These are features that
require comparing rules in a certain order. Thrax
uses Hadoop to sort the rules efficiently and calcu-
late these feature functions. Thrax implements the
following map-reduce features:
? Phrasal translation probabilities p(?|?) and
p(?|?), calculated with relative frequency:
p(?|?) =
C(?, ?)
C(?)
(2)
(and vice versa), where C(?) is the number of
times a given event was extracted.
? Lexical weighting plex(?|?,A) and
plex(?|?,A). We calculate these weights
as given in (Koehn et al, 2003): let A be the
alignment between ? and ?, so (i, j) ? A if
and only if the ith word of ? is aligned to the
jth word of ?. Then we can define plex(?|?) as
n?
i=1
1
|{j : (i, j) ? A}|
?
(i,j)?A
w(?j |?i) (3)
where ?i is the ith word of ?, ?j is the jth word
of ?, and w(y|x) is the relative frequency of
seeing word y given x.
? Rarity penalty, given by
exp(1? C(X ? ??, ??)) (4)
where again C(?) is a count of the number of
times the rule was extracted.
The above features are all implemented and can
be turned on or off with a keyword in the Thrax con-
figuration file.
It is easy to extend Thrax with new feature func-
tions. For simple features, all that is needed is to im-
plement Thrax?s SIMPLEFEATURE interface defin-
ing a method that takes in a rule and calculates a
feature score. Map-reduce features are slightly more
complex: to subclass MAPREDUCEFEATURE, one
must define a mapper and reducer, but also a sort
comparator to determine in what order the rules are
compared during the reduce step.
2.5 Related work
Joshua includes a simple Hiero extractor (Schwartz
and Callison-Burch, 2010). The extractor runs as a
single Java process, which makes it difficult to ex-
tract larger grammars, since the host machine must
have enough memory to hold all of the rules at once.
Joshua?s extractor scores each rule with three feature
functions ? lexical probabilities in two directions,
and one phrasal probability score p(?|?).
The SAMT implementation of Zollmann and
Venugopal (2006) includes a several-thousand-line
Perl script to extract their rules. In addition to
phrasal and lexical probabilities, this extractor im-
plements several other features that are also de-
scribed in section 2.4.
Finally, the cdec decoder (Dyer et al, 2010) in-
cludes a grammar extractor that performs well only
when all rules can be held in memory.
Memory usage is a limitation of both the Joshua
and cdec extractors. Translation models can be very
large, and many feature scores require accumulation
of statistical data from the entire set of extracted
rules. Since it is impractical to keep the entire gram-
mar in memory, rules are usually sorted on disk and
then read sequentially. Different feature calcula-
tions may require different sort orders, leading to a
linear workflow that alternates between sorting the
grammar and calculating a feature score. To cal-
culate more feature scores, more sorts have to be
performed. This discourages the implementation of
new features. For example, Joshua?s built-in rule ex-
tractor calculates the phrasal probability p(?|?) for
each rule but, to save time, does not calculate its ob-
vious counterpart p(?|?), which would require an-
other sort.
481
Language pair sentences (K) words (M)
cs?en 332 4.7
de?en 279 5.5
en?cs 487 6.9
en?de 359 7.2
en?fr 682 12.5
fr?en 792 14.4
Table 2: Training data size after subsampling.
The SAMT extractor does not have a problem
with large data sets; SAMT can run on Hadoop, as
Thrax does.
The Joshua and cdec extractors only extract Hiero
grammars, and Zollmann and Venugopal?s extractor
can only extract SAMT-style grammars. They are
not designed to score arbitrary feature sets, either.
Since variation in translation models and feature sets
can have a significant effect on translation perfor-
mance, we have developed Thrax in order to make it
easy to build and test new models.
3 Experiments
We built systems for six language pairs for the WMT
2011 shared task: cz-en, en-cz, de-en, en-de, fr-en,
and en-fr.3 For each language pair, we built both
SAMT and hiero grammars.4 Table 3 contains the
results on the complete WMT 2011 test set.
To train the translation models, we used the pro-
vided Europarl and news commentary data. For cz-
en and en-cz, we also used sections of the CzEng
parallel corpus (Bojar and Z?abokrtsky?, 2009). The
parallel data was subsampled using Joshua?s built-
in subsampler to select sentences with n-grams rel-
evant to the tuning and test set. We used SRILM
to train a 5-gram language model with Kneser-Ney
smoothing using the appropriate side of the paral-
lel data. For the English LM, we also used English
Gigaword Fourth Edition.5
Before extracting an SCFG with Thrax, we used
the provided Perl scripts to tokenize and normalize
3fr=French, cz=Czech, de=German, en=English.
4Except for fr-en and en-fr. We were unable to decode with
SAMT grammars for these language pairs due to their large size.
We have since resolved this issue and will have scores for the
final version of the paper.
5LDC2009T13
pair hiero SAMT improvement
cz-en 21.1 21.7 +0.6
en-cz 16.8 16.9 +0.1
de-en 18.9 19.5 +0.6
en-de 14.3 14.9 +0.6
fr-en 28.0 - -
en-fr 30.4 - -
Table 3: Single-reference BLEU-4 scores.
the data. We also removed any sentences longer than
50 tokens (after tokenization). For SAMT grammar
extraction, we parsed the English training data us-
ing the Berkeley Parser (Petrov et al, 2006) with the
provided Treebank-trained grammar.
We tuned the model weights against the
WMT08 test set (news-test2008) using Z-
MERT (Zaidan, 2009), an implementation of mini-
mum error-rate training included with Joshua. We
decoded the test set to produce a 300-best list of
unique translations, then chose the best candidate for
each sentence using Minimum Bayes Risk reranking
(Kumar and Byrne, 2004). Figure 2 shows an exam-
ple derivation with an SAMT grammar. To re-case
the 1-best test set output, we trained a true-case 5-
gram language model using the same LM training
data as before, and used an SCFG translation model
to translate from the lowercased to true-case output.
The translation model used rules limited to five to-
kens in length, and contained no hierarchical rules.
4 CachePipe: Cached pipeline runs
Machine translation pipelines involve the specifica-
tion and execution of many different datasets, train-
ing procedures, and pre- and post-processing tech-
niques that can have large effects on translation out-
come, and which make direct comparisons between
systems difficult. The complexity of managing these
pipelines and experimental environments has led to a
number of different experimental management sys-
tems, such as Experiment.perl,6 Joshua 2.0?s Make-
file system (Li et al, 2010), and LoonyBin (Clark
and Lavie, 2010). In addition to managing the
pipeline, these scripts employ different techniques
to avoid expensive recomputation by caching steps.
6http://www.statmt.org/moses/?n=
FactoredTraining.EMS
482
the
reactor type will be operated with uranium
VBN
DT+NP
GLUE
VP
PP
der reaktortyp , das nicht
angereichert
wird zwar mit uran betrieben
, which is
not
enriched
ist .
NP
GLUE
NN
COMMA+SBAR+.
ADJP
JJ
.
S
VBN
DT+NP
GLUE
VP
PP
NP
GLUE
NN
COMMA+SBAR+.
ADJP
JJ
S
Figure 2: An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals
and non-terminals. The unshaded terminals are directly dominated by a nonterminal symbol.
However, these approaches are based on simple but
unreliable heuristics (such as timestamps or file ex-
istence) to make the caching determination.
Our solution to the caching dependency problem
is CachePipe. CachePipe is designed with the fol-
lowing goals: (1) robust content-based dependency
checking and (2) ease of use, including minimal
editing of existing scripts. CachePipe is essentially
a wrapper around command invocations. Presented
with a command to run and a list of file dependen-
cies, it computes SHA-1 hashes of the dependencies
and of the command invocation and stores them; the
command is executed only if any of those hashes are
different from previous runs. A basic invocation in-
volves specifying (1) a name or identifier associated
with the command or step, (2) the command to run,
and (3) a list of file dependencies. For example, to
copy file a to b from a shell prompt, the following
command could be used:
cachecmd copy "cp a b" a b
The first time the command is run, the file would be
copied; afterwards, the command would be skipped
after CachePipe verified that the contents of the de-
pendencies a and b had not changed.
CachePipe is open-source software, distributed
with Joshua or available separately.7 It currently
provides both a shell script interface and a program-
matic API for Perl. It accepts a number of other
arguments and dependency types. It also serves as
the foundation of a new script in Joshua 3.0 that im-
plements the complete Joshua pipeline, from data
preparation to evaluation.
5 Future work
Thrax is currently limited to SCFG-based translation
models. A natural development would be to extract
GHKM grammars (Galley et al, 2004) or more re-
cent tree-to-tree models (Zhang et al, 2008; Liu et
al., 2009; Chiang, 2010). We also hope that Thrax
will continue to be extended with more feature func-
tions as researchers develop and contribute them.
Acknowledgements
This research was supported by in part by the Eu-
roMatrixPlus project funded by the European Com-
mission (7th Framework Programme), and by the
NSF under grant IIS-0713448. Opinions, interpre-
tations, and conclusions are the authors? alone.
7https://github.com/joshua-decoder/
cachepipe
483
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92. in
print.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, Uppsala, Sweden,
July.
Jonathan H. Clark and Alon Lavie. 2010. Loony-
bin: Keeping language technologists sane through au-
tomated management of experimental (hyper) work-
flows. In Proc. LREC.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. In OSDI.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc.
ACL 2010 System Demonstrations, pages 7?12.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
NAACL, Boston, Massachusetts, USA, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL, Morristown, NJ, USA.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. NAACL, Boston, Massachusetts, USA, May.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proc. WMT, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proc. WMT.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
ACL, Suntec, Singapore, August.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. COLING, Manchester, UK,
August.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. ACL, Sydney, Aus-
tralia, July.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua:
Suffix arrays and prefix trees. The Prague Bulletin of
Mathematical Linguistics, 93:157?166, January.
Mark Steedman. 1999. Alternating quantifier scope in
ccg. In Proc. ACL, Stroudsburg, PA, USA.
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. The Prague
Bulletin of Mathematical Linguistics, 91:67?78.
Jonathan Weese. 2011. A systematic comparison of syn-
chronous context-free grammars for machine transla-
tion. Master?s thesis, Johns Hopkins University, May.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL, Columbus, Ohio, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. NAACL Workshop on Statistcal Machine Trans-
lation, New York, New York.
484
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 33?42,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Reranking Bilingually Extracted Paraphrases
Using Monolingual Distributional Similarity
Tsz Ping Chan, Chris Callison-Burch and Benjamin Van Durme
Center for Language and Speech Processing, and HLTCOE
Johns Hopkins University
Abstract
This paper improves an existing bilingual
paraphrase extraction technique using mono-
lingual distributional similarity to rerank can-
didate paraphrases. Raw monolingual data
provides a complementary and orthogonal
source of information that lessens the com-
monly observed errors in bilingual pivot-
based methods. Our experiments reveal that
monolingual scoring of bilingually extracted
paraphrases has a significantly stronger cor-
relation with human judgment for grammat-
icality than the probabilities assigned by the
bilingual pivoting method does. The results
also show that monolingual distribution simi-
larity can serve as a threshold for high preci-
sion paraphrase selection.
1 Introduction
Paraphrasing is the rewording of a phrase such
that meaning is preserved. Data-driven paraphrase
acquisition techniques can be categorized by the
type of data that they use (Madnani and Dorr,
2010). Monolingual paraphrasing techniques clus-
ter phrases through statistical characteristics such
as dependency path similarities or distributional co-
occurrence information (Lin and Pantel, 2001; Pasca
and Dienes, 2005). Bilingual paraphrasing tech-
niques use parallel corpora to extract potential para-
phrases by grouping English phrases that share the
same foreign translations (Bannard and Callison-
Burch, 2005). Other efforts blur the lines between
the two, applying techniques from statistical ma-
chine translation to monolingual data or extract-
ing paraphrases from multiple English translations
of the same foreign text (Barzilay and McKeown,
2001; Pang et al, 2003; Quirk et al, 2004).
We exploit both methodologies, applying a
monolingually-derived similarity metric to the out-
put of a pivot-based bilingual paraphrase model. In
this paper we investigate the strengths and weak-
nesses of scoring paraphrases using monolingual
distributional similarity versus the bilingually calcu-
lated paraphrase probability. We show that monolin-
gual cosine similarity calculated on large volumes
of text ranks bilingually-extracted paraphrases bet-
ter than the paraphrase probability originally defined
by Bannard and Callison-Burch (2005). While our
current implementation shows improvement mainly
in grammaticality, other contextual features are ex-
pected to enhance the meaning preservation of para-
phrases. We also show that monolingual scores can
provide a reasonable threshold for picking out high
precision paraphrases.
2 Related Work
2.1 Paraphrase Extraction from Bitexts
Bannard and Callison-Burch (2005) proposed iden-
tifying paraphrases by pivoting through phrases in a
bilingual parallel corpora. Figure 1 illustrates their
paraphrase extraction process. The target phrase,
e.g. thrown into jail, is found in a German-English
parallel corpus. The corresponding foreign phrase
(festgenommen) is identified using word alignment
and phrase extraction techniques from phrase-based
statistical machine translation (Koehn et al, 2003).
Other occurrences of the foreign phrase in the par-
allel corpus may align to a distinct English phrase,
such as jailed. As the original phrase occurs sev-
eral times and aligns with many different foreign
phrases, each of these may align to a variety of other
English paraphrases. Thus, thrown into jail not only
paraphrases as jailed, but also as arrested, detained,
imprisoned, incarcerated, locked up, and so on. Bad
paraphrases, such as maltreated, thrown, cases, cus-
tody, arrest, and protection, may also arise due to
poor word alignment quality and other factors.
33
... letzteWoche wurden in Irland f?nf Landwirte festgenommen , weil sie verhindern wollten
... last week five farmers were thrown into jail in Ireland because they resisted ...
...
Zahlreiche Journalisten sind verschwunden oder wurden festgenommen , gefoltert und get?tet .
Quite a few journalists have disappeared or have been imprisoned , tortured and killed .
Figure 1: Using a bilingual parallel corpus to extract
paraphrases.
Bannard and Callison-Burch (2005) defined a
paraphrase probability to rank these paraphrase can-
didates, as follows:
e?2 = arg max
e2 6=e1
p(e2|e1) (1)
p(e2|e1) =
?
f
p(e2, f |e1) (2)
=
?
f
p(e2|f, e1)p(f |e1) (3)
?
?
f
p(e2|f)p(f |e1) (4)
where p(e2|e1) is the paraphrase probability, and
p(e|f) and p(f |e) are translation probabilities from
a statistical translation model.
Anecdotally, this paraphrase probability some-
times seems unable to discriminate between good
and bad paraphrases, so some researchers disregard
it and treat the extracted paraphrases as an unsorted
set (Snover et al, 2010). Callison-Burch (2008)
attempts to improve the ranking by limiting para-
phrases to be the same syntactic type.
We attempt to rerank the paraphrases using other
information. This is similar to the efforts of Zhao
et al (2008), who made use of multiple resources to
derive feature functions and extract paraphrase ta-
bles. The paraphrase that maximizes a log-linear
combination of various feature functions is then se-
lected as the optimal paraphrase. Feature weights
in the model are optimized by minimizing a phrase
substitution error rate, a measure proposed by the
authors, on a development set.
2.2 Monolingual Distributional Similarity
Prior work has explored the acquisition of para-
phrases using distributional similarity computed
from monolingual resources, such as in the DIRT
results of Lin and Pantel (2001). In these models,
phrases are judged to be similar based on the cosine
distance of their associated context vectors. In some
cases, such as by Lin and Pantel, or the seminal work
of Church and Hanks (1991), distributional context
is defined using frequencies of words appearing in
various syntactic relations with other lexical items.
For example, the nouns apple and orange are con-
textually similar partly because they both often ap-
pear as the object of the verb eat. While syntac-
tic contexts provide strong evidence of distributional
preferences, it is computationally expensive to parse
very large corpora, so it is also common to represent
context vectors with simpler representations like ad-
jacent words and n-grams (Lapata and Keller, 2005;
Bhagat and Ravichandran, 2008; Lin et al, 2010;
Van Durme and Lall, 2010). In these models, ap-
ple and orange might be judged similar because both
tend to be one word to the right of some, and one to
the left of juice.
Here we calculate distributional similarity using a
web-scale n-gram corpus (Brants and Franz, 2006;
Lin et al, 2010). Given both the size of the collec-
tion, and that the n-grams are sub-sentential (the n-
grams are no longer than 5 tokens by design), it was
not feasible to parse, which led to the use of n-gram
contexts. Here we use adjacent unigrams. For each
phrase x we wished to paraphrase, we extracted the
context vector of x from the n-gram collection as
such: every (n-gram, frequency) pair of the form:
(ax, f ), or (xb, f ), gave rise to the (feature, value)
pair: (wi?1=a, f ), or (wi+1=b, f ), respectively. In
order to scale to this size of a collection, we relied
on Locality Sensitive Hashing (LSH), as was done
previously by Ravichandran et al (2005) and Bha-
gat and Ravichandran (2008). To avoid computing
feature vectors explicitly, which can be a memory
intensive bottleneck, we employed the online LSH
variant described by Van Durme and Lall (2010).
This variant, based on the earlier work of Indyk
and Motwani (1998) and Charikar (2002), approxi-
mates the cosine similarity between two feature vec-
tors based on the Hamming distance in a reduced bit-
wise representation. In brief, for the feature vectors
~u, ~v, each of dimension d, then the cosine similarity
is defined as: ~u?~v|~u||~v| . If we project ~u and ~v through
a d by b random matrix populated with draws from
34
huge amount of
BiP SyntBiP BiP-MonoDS
large number of, .33 large number of, .38 huge amount of, 1.0
in large numbers, .11 great number of, .09 large quantity of, .98
great number of, .08 huge amount of, .06 large number of, .98
large numbers of, .06 vast number of, .06 great number of, .97
vast number of, .06 vast number of, .94
huge amount of, .06 in large numbers, .10
large quantity of, .03 large numbers of, .08
Table 1: Paraphrases for huge amount of according to the
bilingual pivoting (BiP), syntactic-constrainted bilingual
pivoting (SyntBiP) translation score and the monolingual
similarity score via LSH (MonoDS), ranked by corre-
sponding scores listed next to each paraphrase. Syntactic
type of the phrase is [JJ+NN+IN].
N(0, 1), then we convert our feature vectors to bit
signatures of length b, by setting each bit of the sig-
nature conditioned on whether or not the respective
projected value is greater than or equal to 0. Given
the bit signatures h(~u) and h(~v), we approximate
cosine with the formula: cos(D(h(~u),h(~v))b pi), where
D() is Hamming distance.
3 Ranking Paraphrases
We use several different methods to rank candidate
sets of paraphrases that are extracted from bilingual
parallel corpora. Our three scoring methods are:
? MonoDS ? monolingual distributional similar-
ity calculated over the Google n-gram corpus
via LSH, as described in Section 2.2.
? BiP ? bilingual pivoting is calculated as in
Equation 4 following Bannard and Callison-
Burch (2005). The translation model probabili-
ties are estimated from a French-English paral-
lel corpus.
? SyntBiP ? syntactically-constrained bilingual
pivoting. This refinement to BiP, proposed in
Callison-Burch (2008), constrains paraphrases
to be the same syntactic type as the original
phrase in the pivoting step of the paraphrase ta-
ble construction.
When we use MonoDS to re-score a candidate set,
we indicate which bilingual paraphrase extraction
method was used to extract the candidates as prefix,
as in BiP-MonoDS or SyntBiP-MonoDS.
reluctant
MonoDShand?selected BiP
*willing, .99 not, .56
loath, .98 unwilling, .04
*eager, .98 reluctance, .03
somewhat reluctant, .98 reticent, .03
unable, .98 hesitant, .02
denied access, .98 reticent about, .01
disinclined, .98 reservations, .01
very unwilling, .97 reticence, .01
conducive, .97 hesitate, .01
linked, .97 are reluctant, .01
Table 2: Ordered reranked paraphrase candidates for the
phrase reluctant according to monolingual distributional
similarity (MonoDShand?selected) and bilingual pivoting
paraphrase (BiP) method. Two hand-selected phrases are
labeled with asterisks.
3.1 Example Paraphrase Scores
Table 1 shows the paraphrase candidates for the
phrase huge amount of along with the values for each
of our three scoring methods. Although MonoDS
does not explicitly impose syntactic restrictions, the
syntactic structure of the paraphrase in large num-
bers contributes to the large difference in the left
and right context of the paraphrase and of the orig-
inal phrase. Hence, the paraphrase was assigned a
low score of 0.098 as compared to other paraphrase
candidates with the correct syntactic type. Note that
the SyntBiP produced significantly fewer paraphrase
candidates, since its paraphrase candidates must be
the same syntactic type as the original phrase. Iden-
tity paraphrases are excluded for the rest of the dis-
cussion in this paper.
3.2 Susceptibility to Antonyms
Monolingual distributional similarity is widely
known to conflate words with opposite meaning and
has motivated a large body of prior work on antonym
detection (Lin and Zhao, 2003; Lin and Pantel,
2001; Mohammad et al, 2008a; Mohammad et al,
2008b; Marneffe et al, 2008; Voorhees, 2008). In
contrast, the antonyms of a phrase are rarely pro-
duced during pivoting of the BiP methods because
they tend not to share the same foreign translations.
Since the reranking framework proposed here be-
gins with paraphrases acquired by the BiP methodol-
35
ogy, MonoDS can considerably enhance the quality
of ranking while sidestepping the antonym problem
that arises from using MonoDS alone.
To support this intuition, an example of a para-
phrase list with inserted hand-selected phrases
ranked by each reranking methods is shown in Ta-
ble 21. Hand-selected antonyms of reluctant are in-
serted into the paraphrase candidates extracted by
BiP before they are reranked by MonoDS. This is
analogous to the case without pre-filtering of para-
phrases by BiP and all phrases are treated equally
by MonoDS alone. BiP cannot rank these hand-
selected paraphrases since, by construction, they do
not share any foreign translation and hence their
paraphrase scores are not defined. As expected from
the drawbacks of monolingual-based statistics, will-
ing and eager are assigned top scores by MonoDS,
although good paraphrases such as somewhat reluc-
tant and disinclined are also ranked highly. This
illustrates how BiP complements the monolingual
reranking technique by providing orthogonal infor-
mation to address the issue of antonyms for Mon-
oDS.
3.3 Implementation Details
For BiP and SyntBiP, the French-English parallel
text from the Europarl corpus (Koehn, 2005) was
used to train the paraphrase model. The parallel
corpus was extracted from proceedings of the Eu-
ropean parliament with a total of about 1.3 million
sentences and close to 97 million words in the En-
glish text. Word alignments were generated with
the Berkeley aligner. For SyntBiP, the English side
of the parallel corpus was parsed using the Stan-
ford parser (Klein and Manning, 2003). The transla-
tion models were trained with Thrax, a grammar ex-
tractor for machine translation (Weese et al, 2011).
Thrax extracts phrase pairs that are labeled with
complex syntactic labels following Zollmann and
Venugopal (2006).
For MonoDS, the web-scale n-gram collection of
Lin et al (2010) was used to compute the mono-
lingual distributional similarity features, using 512
bits per signature in the resultant LSH projection.
Following Van Durme and Lall (2010), we implic-
1Generating a paraphrase list by MonoDS alone requires
building features for all phrases in the corpus, which is com-
putationally impractical and hence, was not considered here.
itly represented the projection matrix with a pool of
size 10,000. In order to expand the coverage of the
candidates scored by the monolingual method, the
LSH signatures are obtained only for the phrases in
the union set of the phrase-level outputs from the
original and from the syntactically constrained para-
phrase models. Since the n-gram corpus consists
of at most 5-gram and each distributional similar-
ity feature requires a single neighboring token, the
LSH signatures are generated only for phrases that
are 4-gram or less. Phrases that didn?t appear in the
n-grams with at least one feature were discarded.
4 Human Evaluation
The different paraphrase scoring methods were com-
pared through a manual evaluation conducted on
Amazon Mechanical Turk. A set of 100 test phrases
were selected and for each test phrase, five distinct
sentences were randomly sampled to capture the fact
that paraphrases are valid in some contexts but not
others (Szpektor et al, 2007). Judges evaluated the
paraphrase quality through a substitution test: For
each sampled sentence, the test phrase is substituted
with automatically-generated paraphrases. The sen-
tences and the phrases are drawn from the English
side of the Europarl corpus. Judges indicated the
amount of the original meaning preserved by the
paraphrases and the grammaticality of the resulting
sentences. They assigned two values to each sen-
tence using the 5-point scales defined in Callison-
Burch (2008).
The 100 test phrases consisted of 25 unigrams,
25 bigrams, 25 trigrams and 25 4-grams. These 25
phrases were randomly sampled from the paraphrase
table generated by the bilingual pivoting method,
with the following restrictions:
? The phrase must have occurred at least 5 times
in the parallel corpus and must have appeared
in the web-scale n-grams.
? The size of the union of paraphrase candidates
from BiP and SyntBiP must be 10 or more.
4.1 Calculating Correlation
In addition to their average scores on the 5-point
scales, the different paraphrase ranking methods
were quantitatively evaluated by calculating their
correlation with human judgments. Their correla-
tion is calculated using Kendall?s tau coefficient, a
36
Reranking Method Meaning Grammar
BiP 0.14 0.04
BiP-MonoDS 0.14 0.24?
SyntBiP 0.19 0.08
SyntBip-MonoDS 0.15 0.22?
SyntBiPmatched 0.20 0.15
SyntBiPmatched-MonoDS 0.17 0.16
SyntBiP* 0.21 0.09
SyntBiP-MonoDS* 0.16 0.22?
Table 3: Kendall?s Tau rank correlation coefficients be-
tween human judgment of meaning and grammaticality
for the different paraphrase scoring methods. Bottom
panel: SyntBiPmatched is the same as SyntBiP except
paraphrases must match with the original phrase in syn-
tactic type. SyntBiP* and MonoDS* are the same as
before except they share the same phrase support with
SyntBiPmatched. (?: MonoDS outperforms the corre-
sponding BiP reranking at p-value?0.01, and ? at?0.05)
common measure of correlation between two ranked
lists. Kendall?s tau coefficient ranges between -1 and
1, where 1 indicates a perfect agreement between a
pair of ranked lists.
Since tied rankings occur in the human judgments
and reranking methods, Kendall?s tau b, which ig-
nores pairs with ties, is used in our analysis. An
overall Kendall?s tau coefficient presented in the re-
sults section is calculated by averaging all Kendall?s
tau coefficients of a particular reranking method
over all phrase-sentence combinations.
5 Experimental Results
5.1 Correlation
The Kendall?s tau coefficients for the three para-
phrase ranking methods are reported Table 3. A
total of 100 phrases and 5 sentence per phrase are
selected for the experiment, resulting in a max-
imum support size of 500 for Kendall?s tau co-
efficient calculation. The overall sizes of sup-
port are 500, 335, and 304 for BiP, SyntBiP and
SyntBiPmatched, respectively. The positive values of
Kendall?s tau confirm both monolingual and bilin-
gual approaches for paraphrase reranking are posi-
tively correlated with human judgments overall. For
grammaticality, monolingual distributional simi-
larity reranking correlates stronger with human
judgments than bilingual pivoting methods. For
example, in the top panel, given a paraphrase ta-
ble generated through bilingual pivoting, Kendall?s
tau for monolingual distributional similarity (BiP-
MonoDS) achieves 0.24 while that of the bilin-
gual pivoting ranking (BiP) is only 0.04. Simi-
larly, reranking of the paraphrases extracted with
syntactically-constrained bilingual pivoting shows a
stronger correlation between SyntBiP-MonoDS and
grammar judgments (0.22) than the SyntBiP (0.08).
This result further supports the intuition of distri-
butional similarity being suitable for paraphrase
reranking in terms of grammaticality.
In terms of meaning preservation, the Kendall?s
tau coefficient for MonoDS is often lower than the
bilingual approaches, suggesting that paraphrase
probability from the bilingual approach correlates
better with phrasal meaning than the monolingual
metric. For instance, SyntBiP reaches a Kendall?s
tau of 0.19, which is a slightly stronger correlation
than that of SyntBiP-MonoDS. Although paraphrase
candidates were generated by bilingual pivoting,
distributional similarity depends only on contextual
similarity and does not guarantee paraphrases that
match with the original meaning; whereas Bilingual
pivoting methods are derived based on shared for-
eign translations which associate meaning.
In the bottom panel of Table 3, only paraphrases
of the same syntactic type as the source phrase are
included in the ranked list for Kendall?s tau calcula-
tion. The phrases associated with these paraphrases
are used for calculating Kendall?s tau for the orig-
inal reranking methods (labeled as SyntBiP* and
SyntBiP-MonoDS*). Comparing only the bilingual
methods across panels, syntactic matching increases
the correlation of bilingual pivoting metrics with
human judgments in grammaticality (e.g., 0.15 for
SyntBiPmatched and 0.08 for SyntBiP) but with only
minimal effects on meaning. The maximum values
in the bottom panel for both categories are roughly
the same as that in the corresponding category in
the upper panel ({0.21,0.19} in meaning and {0.22,
0.24} in grammar for lower and upper panels, re-
spectively.) This suggests that syntactic type match-
ing offers similar improvement in grammaticality
as MonoDS, although syntactically-constrained ap-
proaches have more confined paraphrase coverage.
We performed a one-tailed sign test on the
Kendall?s Tau values across phrases to examine
37
Figure 2: Averaged scores in the top K paraphrase can-
didates as a function of K for different reranking metrics.
All methods performs similarly in meaning preservation,
but SyntBiP-MonoDS outperforms other scoring meth-
ods in grammaticality, as shown in the bottom graph.
the statistical significance of the performance gain
due to MonoDS. For grammaticality, except for the
case of syntactic type matching (SyntBiPmatched), p-
values are less than 0.05, confirming the hypothesis
that MonoDS outperforms BiP. The p-value for com-
paring MonoDS and SyntBiPmatched exceeds 0.05,
agreeing with our conclusion from Table 3 that the
two methods perform similarly.
5.2 Thresholding Using MonoDS Scores
One possible use for the paraphrase scores would be
as a cutoff threshold where any paraphrases exceed-
ing that value would be selected. Ideally, this would
retain only high precision paraphrases.
To verify whether scores from each method corre-
spond to human judgments for paraphrases extracted
by BiP, human evaluation scores are averaged for
meaning and grammar within each range of para-
phrase score for BiP and approximate cosine dis-
tance for MonoDS, as shown in Table 4. The BiP
paraphrase score bin sizes are linear in log scale.
BiP Paraphrase Score MonoDS LSH Score
Region M G Region M G
1.00 ? x > 0.37 3.6 3.7 1 ? x > 0.95 4.0 4.4
0.37 ? x > 0.14 3.6 3.7 0.95 ? x > 0.9 3.2 4.0
0.14 ? x > 0.05 3.4 3.6 0.9 ? x > 0.85 3.3 4.0
0.05 ? x > 1.8e-2 3.4 3.6 0.85 ? x > 0.8 3.3 4.0
1.8e-2 ? x > 6.7e-3 3.4 3.6 0.8 ? x > 0.7 3.2 3.9
6.7e-3 ? x > 2.5e-3 3.2 3.7 0.7 ? x > 0.6 3.3 3.8
2.5e-3 ? x > 9.1e-4 3.0 3.6 0.6 ? x > 0.5 3.1 3.7
9.1e-4 ? x > 3.4e-4 3.0 3.8 0.5 ? x > 0.4 3.1 3.6
3.4e-4 ? x > 1.2e-4 2.6 3.6 0.4 ? x > 0.3 3.1 3.5
1.2e-4 ? x > 4.5e-5 2.7 3.6 0.3 ? x > 0.2 2.9 3.4
x ? 4.5e-5 2.5 3.7 0.2 ? x > 0.1 3.0 3.3
0.1 ? x > 0 2.9 3.2
Table 4: Averaged human judgment scores as a func-
tion of binned paraphrase scores and binned LSH scores.
MonoDS serves as much better thresholding score for ex-
tracting high precision paraphrases.
MonoDS LSH BiP Paraphrase Threshold
Threshold ? 0.05 ? 0.01 ? 6.7e-3
? 0.9 4.2 / 4.4 4.1 / 4.4 4.0 / 4.4
? 0.8 4.0 / 4.3 3.9 / 4.3 3.9 / 4.2
? 0.7 3.9 / 4.1 3.8 / 4.2 3.8 / 4.1
Table 5: Thresholding using both the MonoDS and BiP
scores further improves the average human judgment of
Meaning / Grammar.
Observe that for the BiP paraphrase scores on the
left panel, no trend on the averaged grammar scores
across all score bins is present. While a mild cor-
relation exists between the averaged meaning scores
and the paraphrase scores, the top score region (1> x
? 0) corresponds to merely an averaged value of 3.6
on a 5-point scale. Therefore, thresholding on BiP
scores among a set of candidates would not guaran-
tee accurate paraphrases in grammar or meaning.
On the right panel, MonoDS LSH scores on para-
phrase candidates produced by BiP are uniformly
higher in grammar than meaning across all score
bins, similar to the correlation results in Table 3.
The averaged grammar scores decreases monoton-
ically and proportionally to the change in LSH val-
ues. With regard to meaning scores, the averaged
values roughly correspond to the decrease of LSH
values, implying distributional similarity correlates
weakly with human judgment in the meaning preser-
38
vation of paraphrase. Note that the drop in averaged
scores is the largest from the top bin (1? x > 0.95)
to the second bin (0.95 ? x > 0.9) is the largest
within both meaning and grammar. This suggests
that thresholding on top tiered MonoDS scores
can be a good filter for extracting high precision
paraphrases. BiP scores, by comparison, are not as
useful for thresholding grammaticality.
Additional performance gain attained by combin-
ing the two thresholding are illustrated in Table 5,
where averaged meaning and grammar scores are
listed for each combination of thresholding. At a
threshold of 0.9 for MonoDS LSH score and 0.05
for BiP paraphrase score, the averaged meaning ex-
ceeds the highest value reported in Table 4, whereas
the grammar scores reaches the value in the top bin
in Table 4. General trends of improvement from uti-
lizing the two reranking methods are observed by
comparing Tables 4 and 5.
5.3 Top K Analysis
Figure 2 shows the mean human assigned score
within the top K candidates averaged across all
phrases. Compared across the two categories,
meaning scores have lower range of score and
a more uniform trend of decreasing values as K
grows. In grammaticality, BiP clearly underper-
forms whereas the SyntBiP-MonoDS maintains the
best score among all methods over all values of K.
In addition, a slow drop-off up until K = 4 in the
curve for SyntBiP-MonoDS implies that the quality
of paraphrases remains relatively high going from
top 1 to top 4 candidates.
In applications such as question answering or
search, the order of answers presented is important
because the lower an answer is ranked, the less likely
it would be looked at by a user. Based on this intu-
ition, the paraphrase ranking methods are evaluated
using the maximum human judgment score among
the top K candidates obtained by each method. As
shown in Table 6, when only the top candidate
is considered, the averaged score corresponding to
the monolingual reranking methods are roughly the
same as that to the bilingual methods in meaning, but
as K grows, the bilingual methods outperforms the
monolingual methods. In terms of grammaticality,
scores associated with monolingual reranking meth-
ods are consistently higher than the bilingual meth-
Reranking Method
K BiP BiP-MonoDS SyntBiP SyntBiP-MonoDS
M
1 3.62 3.67 3.58 3.58
3 4.13 4.07 4.13 4.01
5 4.26 4.19 4.20 4.09
10 4.39 4.30 4.25 4.23
G
1 3.83 4.11 4.04 4.23
3 4.22 4.45 4.47 4.54
5 4.38 4.54 4.55 4.62
10 4.52 4.62 4.63 4.67
Table 6: Average of the maximum human evaluation
score from top K candidates for each reranking method.
Support sizes for BiP- and SyntBiP-based metrics are 500
and 335, respectively. (M = Meaning, G = Grammar)
ods but the difference tapers off as K increases. This
suggests that when only limited top paraphrase can-
didates can be evaluated, MonoDS is likely to pro-
vide better quality of results.
6 Detailed Examples
6.1 MonoDS Filters Bad BiP Paraphrases
The examples in the top panel of Table 7 illustrates a
few disadvantages of the bilingual paraphrase scores
and how monolingual reranking complements the
bilingual methods. Translation models based on
bilingual corpora are known to suffer from misalign-
ment of the parallel text (Bannard and Callison-
Burch, 2005), producing incorrect translations that
propagate through in the paraphrase model. This is-
sue is exemplified in the phrase pairs {considerable
changes, caused quite}, {always declared, always
been}, and {significantly affected, known} listed Ta-
ble 7. The paraphrases are clearly unrelated to the
corresponding phrases as evident from the low rank-
ings from human judges. Nonetheless, they were in-
cluded as candidates likely due to misalignment and
were ranked relatively high by BiP metric. For ex-
ample, considerable changes was aligned to modifie
conside?rablement correctly. However, due to a com-
bination of loose translations and difficulty in align-
ing multiple words that are spread out in a sentence,
the French phrase was inaccurately matched with
caused quite by the aligner, inducing a bad para-
phrase. Note that in these cases LSH produces the
results that agrees with the human rankings.
39
Ranking
Phrase Paraphrase Sizepool Meaning Grammar BiP BiP-MonoDS
significantly affected known 20 19 18.5 1 17
considerable changes caused quite 23 23 23 2.5 23
always declared always been 20 20 20 2 13
hauled delivered 23 7 5.5 21.5 5.0
fiscal burden? taxes 18 13.5 18 6 16
fiscal burden? taxes 18 2 8 6 16
legalise legalize 23 1 1 10 1
to deal properly with address 35 4.5 5.5 4 29.5
you have just stated you have just suggested 31 13.5 8.5 4 30
Table 7: Examples of phrase pair rankings by different reranking methods and human judgments in terms of meaning
and grammar. Higher rank (smaller numbers) corresponds to more favorable paraphrases by the associated metric.
(?: Phrases are listed twice to show the ranking variation when substitutions are evaluated in different sentences.)
6.2 Context Matters
Occasionally, paraphrases are context-dependent,
meaning the relevance of the paraphrase depends on
the context in a sentence. Bilingual methods can
capture limited context through syntactic constraints
if the POS tags of the paraphrases and the sentence
are available, while the distributional similarity met-
ric, in its current implementation, is purely based on
the pattern of co-occurrence with neighboring con-
text n-grams. As a result, LSH scores should be
slightly better at gauging the paraphrases defined by
context, as suggested by some examples in Table 7.
The phrase pair {hauled, delivered} differ slightly
in how they describe the manner that an object is
moved. However, in the context of the following
sentence, they roughly correspond to the same idea:
countries which do not comply with community
legislation should be hauled before the court of
justice and i think mrs palacio will do so .
As a result, out of 23 candidates, human judges
ranked delivered 7 and 5.5 for meaning and gram-
mar, respectively. The monolingual-based metric
also assigns a higher rank to the paraphrase while
BiP puts it near the lowest rank.
Another example of context-dependency is the
phrase pair {fiscal burden, taxes}, which could have
some foreign translations in common. The original
phrase appears in the following sentence:
... the member states can reduce the fiscal burden
consisting of taxes and social contributions .
The paraphrase candidate taxes is no longer ap-
propriate with the consideration of the context sur-
rounding the original phrase. As such, taxes re-
ceived rankings of 13.5, 18 and 16 out of 18
for meaning, grammar, and MonoDS, respectively,
whereas BiP assigns a 6 to the paraphrase. The same
phrase pair but a different sentence, the context in-
duces opposite effects on the paraphrase judgments,
where the paraphrase received 2 and 8 in the two
categories as shown in Table 7:
the economic data for our eu as regards employ-
ment and economic growth are not particularly
good , and , in addition , the fiscal burden in eu-
rope , which is to be borne by the citizen , has
reached an all-time high of 46 % .
Hence, distributional similarity offers additional
advantages over BiP only when the paraphrase ap-
pears in a context that also defines most of the non-
zero dimensions of the LSH signature vector.
The phrase pair {legalise, legalize} exemplifies
the effect of using different corpora to train 2 para-
phrase reranking models as shown in Table 7. Mean-
ing, grammar and MonoDS all received top rank out
of all paraphrases, whereas BiP ranks the paraphrase
10 out of 23. Since the BiP method was trained
with Europarl data, which is dominated by British
English, BiP fails to acknowledge the American
spelling of the same word. On the other hand, dis-
tributional similarity feature vectors were extracted
from the n-gram corpus with different variations of
English, which was informative for paraphrase rank-
ing. This property can be exploited for adaptation of
specific domain of paraphrases selection.
40
6.3 Limitations of MonoDS Implementation
While the monolingual distributional similarity
shows promise as a paraphrase ranking method,
there are a number of additional drawbacks associ-
ated with the implementation.
The method is currently limited to phrases with
up to 4 contiguous words that are present in the
n-gram corpus for LSH feature vector extraction.
Since cosine similarity is a function of the angle
between 2 vectors irrespective of the vector mag-
nitudes, thresholding on low occurrences of higher
n-grams in the corpus construction causes larger n-
grams to suffer from feature sparsity and be sus-
ceptible to noise. A few examples from the exper-
iment demonstrate such scenario. For a phrase to
deal properly with, a paraphrase candidate address
receives rankings of 4.5, 5.5 and 4 out of 35 for
meaning, grammar and BiP, respectively, it is ranked
29.5 by BiP-MonoDS. The two phrases are expected
to have similar neighboring context in regular En-
glish usage, but it might be misrepresented by the
LSH feature vector due to the lack of occurrences of
the 4-gram in the corpus.
Another example of how sparsity affects LSH fea-
ture vectors is the phrase you have just stated. An
acceptable paraphrase you have just suggested was
ranked 13.5, 8.5 and 6.5 out of a total of 31 can-
didates by meaning, grammar and BiP, respectively,
but MonoDS only ranks it at 30. The cosine sim-
ilarity between the phrases are 0.05, which is very
low. However, the only tokens that differentiate the
4-gram phrases, i.e. {stated,suggested}, have a sim-
ilarity score of 0.91. This suggests that even though
the additional words in the phrase don?t alter the
meaning significantly, the feature vectors are mis-
represented due to the sparsity of the 4-gram. This
highlights a weakness of the current implementa-
tion of distributional similarity, namely that context
within a phrase is not considered for larger n-grams.
7 Conclusions and Future Work
We have presented a novel paraphrase ranking met-
ric that assigns a score to paraphrase candidates ac-
cording to their monolingual distributional similar-
ity to the original phrase. While bilingual pivoting-
based paraphrase models provide wide coverage
of paraphrase candidates and syntactic constraints
on the model confines the structural match, addi-
tional contextual similarity information provided by
monolingual semantic statistics increases the accu-
racy of paraphrase ranking within the target lan-
guage. Through a manual evaluation, it was shown
that monolingual distributional scores strongly cor-
relate with human assessment of paraphrase quality
in terms of grammaticality, yet have minimal effects
on meaning preservation of paraphrases.
While we speculated that MonoDS would im-
prove both meaning and grammar scoring for para-
phrases, we found in the results that only gram-
maticality was improved from the monolingual ap-
proach. This is likely due to the choice of how con-
text is represented, which in this case is only single
neighboring words. A consideration for future work
to enhance paraphrasal meaning preservation would
be to explore other contextual representations, such
as syntactic dependency parsing (Lin, 1997), mu-
tual information between co-occurences of phrases
Church and Hanks (1991), or increasing the number
of neighboring words used in n-gram based repre-
sentations.
In future work we will make use of other com-
plementary bilingual and monolingual knowledge
sources by combining other features such as n-gram
length, language model scores, etc. One approach
would be to perform minimum error rate training
similar to Zhao et al (2008) in which linear weights
of a feature function for a set of paraphrases candi-
date are trained iteratively to minimize the phrasal-
substitution-based error rate. Instead of phrasal sub-
stitution in Zhao?s method, quantitative measure of
correlation with human judgment can be used as
the objective function to be optimized during train-
ing. Other techniques such as SVM-rank (Joachims,
2002) may also be investigated for aggregating re-
sults from multiple ranked lists.
8 Acknowledgements
Thanks to Courtney Napoles for advice regarding
a pilot version of this work. Thanks to Jonathan
Weese, Matt Post and Juri Ganitkevitch for their as-
sistance with Thrax. This research was supported by
the EuroMatrixPlus project funded by the European
Commission (7th Framework Programme), and by
the NSF under grant IIS-0713448. Opinions, inter-
pretations, and conclusions are the authors? alone.
41
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Moses Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of STOC.
Kenneth Church and Patrick Hanks. 1991. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 6(1):22?29.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Min-
ing.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. Advances in NIPS, 15:3?10.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2(1).
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7:343?360.
Dekang Lin and Shaojun Zhao. 2003. Identifying syn-
onyms among distributionally similar words. In Pro-
ceedings of IJCAI-03, pages 1492?1493.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Dekang Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Proceed-
ings of ACL.
Nitin Madnani and Bonnie Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Marie-Catherine De Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL 2008.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008a. Computing word-pair antonymy. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 982?991. Association
for Computational Linguistics.
Saif Mohammad, Bonnie J. Dorr, Melissa Egan, Nitin
Madnani, David Zajic, and Jimmy Lin. 2008b. Mul-
tiple alternative sentence compressions and word-pair
antonymy for automatic text summarization and rec-
ognizing textual entailment.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Marius Pasca and Peter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the web.
In Proceedings of IJCNLP, pages 119?130.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP, pages 142?149.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized Algorithms and NLP: Using Lo-
cality Sensitive Hash Functions for High Speed Noun
Clustering. In Proceedings of ACL.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. TER-plus: paraphrase,
semantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of ACL.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.
Ellen M. Voorhees. 2008. Contradictions and justifica-
tions: Extensions to the textual entailment task.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the thrax
grammar extractor. EMNLP 2011 - Workshop on sta-
tistical machine translation.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of WMT06.
42
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 75?78,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Processing Informal, Romanized Pakistani Text Messages
Ann Irvine and Jonathan Weese and Chris Callison-Burch
Center for Language and Speech Processing
Johns Hopkins University
Abstract
Regardless of language, the standard character
set for text messages (SMS) and many other
social media platforms is the Roman alphabet.
There are romanization conventions for some
character sets, but they are used inconsistently
in informal text, such as SMS. In this work, we
convert informal, romanized Urdu messages
into the native Arabic script and normalize
non-standard SMS language. Doing so pre-
pares the messages for existing downstream
processing tools, such as machine translation,
which are typically trained on well-formed,
native script text. Our model combines infor-
mation at the word and character levels, al-
lowing it to handle out-of-vocabulary items.
Compared with a baseline deterministic ap-
proach, our system reduces both word and
character error rate by over 50%.
1 Introduction
There are many reasons why systematically process-
ing informal text, such as Twitter posts or text mes-
sages, could be useful. For example, during the Jan-
uary 2010 earthquake in Haiti, volunteers translated
Creole text messages that survivors sent to English
speaking relief workers. Machine translation (MT)
could supplement or replace such crowdsourcing ef-
forts in the future. However, working with SMS data
presents several challenges. First, messages may
have non-standard spellings and abbreviations (?text
speak?), which we need to normalize into standard
language. Second, many languages that are typically
written in a non-Roman script use a romanized ver-
sion for SMS, which we need to deromanize. Nor-
malizing and deromanizing SMS messages would
allow us to use existing MT engines, which are typ-
ically trained on well-formed sentences written in
their native-script, in order to translate the messages.
With this work, we use and release a corpus of
1 million (4, 195 annotated) anonymized text mes-
sages sent in Pakistan1. We deromanize and normal-
ize messages written in Urdu, although the general
approach is language-independent. Using Mechan-
ical Turk (MTurk), we collect normalized Arabic
script annotations of romanized messages in order to
both train and evaluate a Hidden Markov Model that
automates the conversion. Our model drastically
outperforms our baseline deterministic approach and
its performance is comparable to the agreement be-
tween annotators.
2 Related Work
There is a strong thread of research dedicated to nor-
malizing Twitter and SMS informal English (Sproat
et al, 2001). Choudhury et al (2007) use a super-
vised English SMS dataset and build a character-
level HMM to normalize individual tokens. Aw et
al. (2006) model the same task using a statistical MT
system, making the output context-sensitive at the
cost of including a character-level analysis. More
recently, Han and Baldwin (2011) use unsupervised
methods to build a pipeline that identifies ill-formed
English SMS word tokens and builds a dictionary
of their most likely normalized forms. Beaufort et
al. (2010) use a large amount of training data to su-
pervise an FST-based French SMS normalizer. Li
and Yarowsky (2008) present methods that take ad-
vantage of monolingual distributional similarities to
identify the full form of abbreviated Chinese words.
One challenge in working with SMS data is that pub-
lic data is sparse (Chen and Kan, 2011). Translit-
eration is well-studied (Knight and Graehl, 1997;
Haizhou et al, 2004; Li et al, 2010) and is usually
viewed as a subproblem of MT.
With this work, we release a corpus of SMS mes-
sages and attempt to normalize Urdu SMS texts. Do-
ing so involves the same challenges as normalizing
English SMS texts and has the added complexity
that we must also deromanize, a process similar to
the transliteration task.
1See http://www.cs.jhu.edu/?anni/papers/
urduSMS/ for details about obtaining the corpus.
75
!"#$#%&'()*++&$*! ,#-./(0&1&%($&#2(13(4&5&5('3$6(7$4&(1*(8&"1&#(13("1#(1*9(:1&'3(+1&2&+1(8&"1('39();<=>?@=!
"#$%&#%'! ()*&!
+',-./#$01#2.$! !"#$!!"#$!%&!'#(!)%*!+!#,-*!%&!'(")*+!%&!'&,!%&!+!%./! 0#1#2!-*+!%*!
3$%4056!7)#$54#2.$! 86')'!#)'!9.&!:'.:4';!5''/5!'<')9.$'!05!5=&*90$%>!.?!5=&*9!0=5!%..*!
Figure 1: Example of SMS with MTurk annotations
3 Data and Annotation
Our Pakistani SMS dataset was provided by the
Transnational Crisis Project, and it includes 1 mil-
lion (724,999 unique) text messages that were sent
in Pakistan just prior to the devastating July 2010
floods. The messages have been stripped of all
metadata including sender, receiver, and timestamp.
Messages are written in several languages, though
most are in Urdu, English, or a combination of the
two. Regardless of language, all messages are com-
posed in the Roman alphabet. The dataset contains
348,701 word types, 49.5% of which are singletons.
We posted subsets of the SMS data to MTurk to
perform language identification, followed by dero-
manization and normalization on Urdu messages.
In the deromanization and normalization task, we
asked MTurk workers to convert all romanized
words into script Urdu and use full, non-abbreviated
word forms. We applied standard techniques for
eliminating noise in the annotation set (Callison-
Burch and Dredze, 2010) and limited annotators to
those in Pakistan. We also asked annotators to in-
dicate if a message contained private, sensitive, or
offensive material, and we removed such messages
from our dataset.
We gathered deromanization and normalization
MTurk annotations for 4,195 messages. In all ex-
periments, we use 3,695 of our annotated SMS texts
for training and 500 for testing. We found that 18%
of word tokens and 44% of word types in the test
data do not appear in the training data. An example
of a fully annotated SMS is shown in Figure 1.
Figure 2 shows that, in general, productive MTurk
annotators also tend to produce high quality annota-
tions, as measured by an additional round of MTurk
annotations which asked workers to choose the best
annotation among the three we gathered. The raw
average annotator agreements as measured by char-
acter and word level edit distance are 40.5 and 66.9,
respectively. However, the average edit distances
0 100 200 300 400
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of MTurk HITs completed
P
e
r
c
e
n
t
 
o
f
 
a
n
n
o
t
a
t
i
o
n
s
 
v
o
t
e
d
 
b
e
s
t
Good Performance
Mediocre Performance
Poor Performance
Figure 2: Productivity vs. percent of annotations voted
best among three deromanizations gathered on MTurk.
!"#$%&' ()*+,$-+.),/'01#2342,"56' 7,89$/:'
'!"#' ;#+*'0<6';+#+*'0<6';+#*'0=6';+#2*'0>6' 8#+"2'
'$%&'#'' ;),/+'0?6';,'/+'0@6';,/+'0<6';),'/+'0=6' A:$":'
'()"*)+'' B4/#+'0>6'!"#$0>6'B)/+#)'0>6'B4/#),'0>6' )&:2#'%2)%92'
','-'' ;:4/:'0<=6';$'0>6';:4/'0>6' :+%%5'
''./0'' C48,)'0>6'8+,C)'0>6' D#2E5'
'123$4'' F+&2$,'0G6'F+&2,'0G6'F++&2$,'0@6'F+&+$,'0@6'F&2$,'0<6' ":$&H":+&'
Figure 3: Urdu words romanized in multiple ways. The
Urdu word for ?2? is pronounced approximately ?du.?
between ?good? MTurk workers (at least 50% of a
worker?s messages are voted best) and the deroman-
ization which was voted best (when the two are dif-
ferent) are 25.1 (character) and 53.7 (word).
We used an automatic aligner to align the words
in each Arabic script annotation to words in the orig-
inal romanized message. The alignments show an
average fertility of 1.04 script words per romanized
word. Almost all alignments are one-to-one and
monotonic. Since there is no reordering, the align-
ment is a simplified case of word alignment in MT.
Using the aligned dataset, we examine how Urdu
words are romanized. The average entropy for non-
singleton script word tokens is 1.49 bits. This means
it is common for script words to be romanized in
multiple ways (4.2 romanizations per script word on
average). Figure 3 shows some examples.
4 Deromanization and Normalization
In order to deromanize and normalize Urdu SMS
texts in a single step, we use a Hidden Markov
Model (HMM), shown in Figure 4. To estimate the
probability that one native-script word follows an-
76
???? ??? ???????
walo kia soratehal
Figure 4: Illustration of HMM with an example from
SMS data. English translation: ?What?s the situation??
other, we use a bigram language model (LM) with
add-1 smoothing (Lidstone, 1920) and compare two
sources of LM training data.
We use two sources of data to estimate the prob-
ability of a romanized word given a script word:
(1) a dictionary of candidates generated from auto-
matically aligned training data, (2) a character-based
transliteration model (Irvine et al, 2010).
If r is a romanized word and u is a script Urdu
word, the dictionary-based distribution, pDICT(r|u),
is given by relative frequency estimations over the
aligned training data, and the transliteration-based
distribution, pTRANS(r|u), is defined by the transliter-
ation model scores. We define the model?s emission
probability distribution as the linear interpolation of
these two distributions:
pe(r|u) = (1? ?)pDICT(r|u) + ?pTRANS(r|u)
When ? = 0, the model uses only the dictionary,
and when ? = 1 only the transliterations.
Intuitively, we want the dictionary-based model to
memorize patterns like abbreviations in the training
data and then let the transliterator take over when a
romanized word is out-of-vocabulary (OOV).
5 Results and discussion
In the eight experiments summarized in Table 1, we
vary the following: (1) whether we estimate HMM
emissions from the dictionary, the transliterator, or
both (i.e., we vary ?), (2) language model training
data, and (3) transliteration model training data.
Our baseline uses an Urdu-extension of the Buck-
walter Arabic deterministic transliteration map.
Even our worst-performing configuration outper-
forms this baseline by a large margin, and the best
configuration has a performance comparable to the
agreement among good MTurk workers.
LM Translit ? CER WER
1 News ? Dict 41.5 63.3
2 SMS ? Dict 38.2 57.1
3 SMS Eng Translit 33.4 76.2
4 SMS SMS Translit 33.3 74.1
5 News SMS Both 29.0 58.1
6 News Eng Both 28.4 57.2
7 SMS SMS Both 25.0 50.1
8 SMS Eng Both 24.4 49.5
Baseline: Buckwalter Determ. 64.6 99.9
Good MTurk Annotator Agreement 25.1 53.7
Table 1: Deromanization and normalization results on
500 SMS test set. Evaluation is by character (CER) and
word error rate (WER); lower scores are better. ?LM?
indicates the data used to estimate the language model
probabilities: News refers to Urdu news corpus and SMS
to deromanized side of our SMS training data. ?Translit?
column refers to the training data that was used to train
the transliterator: SMS; SMS training data; Eng; English-
Urdu transliterations. ? refers to the data used to estimate
emissions: transliterations, dictionary entries, or both.
Unsurprisingly, using the dictionary only (Exper-
iments 1-2) performs better than using translitera-
tions only (Experiments 3-4) in terms of word error
rate, and the opposite is true in terms of character
error rate. Using both the dictionary derived from
the SMS training data and the transliterator (Experi-
ments 5?8) outperforms using only one or the other
(1?4). This confirms our intuition that using translit-
eration to account for OOVs in combination with
word-level learning from the training data is a good
strategy2.
We compare results using two language model
training corpora: (1) the Urdu script side of our
SMS MTurk data, and (2) the Urdu side of an Urdu-
English parallel corpus,3 which contains news-
domain text. We see that using the SMS MTurk data
(7?8) outperforms the news text (5?6). This is due to
the fact that the news text is out of domain with re-
spect to the content of SMS texts. In future work, we
plan to mine Urdu script blog and chat data, which
may be closer in domain to the SMS texts, providing
better language modeling probabilities.
2We experimented with different ? values on held out data
and found its value did not impact system performance signifi-
cantly unless it was set to 0 or 1, ignoring the transliterations or
dictionary. We set ? = 0.5 for the rest of the experiments.
3LDC2006E110
77
Training Freq. bins Length Diff. bins
Bin CER WER Bin CER WER
100+ 9.8 14.8 0 23.5 43.3
10?99 15.2 22.1 1, 2 29.1 48.7
1?9 27.5 37.2 -1, -2 42.3 70.1
0 73.5 96.6 ?3 100.3 100.0
?-3 66.4 87.3
Table 2: Results on tokens in the test set, binned by train-
ing frequency or difference in character length with their
reference. Length differences are number of characters
in romanized token minus the number of characters in its
deromanization. ? = 0.5 for all.
We compare using a transliterator trained on ro-
manized/deromanized word pairs extracted from the
SMS text training data with a transliterator trained
on English words paired with their Urdu translitera-
tions and find that performance is nearly equivalent.
The former dataset is noisy, small, and in-domain
while the latter is clean, large, and out-of-domain.
We expect that the SMS word pairs based translit-
erator would outperform the English-Urdu trained
transliterator given more, cleaner data.
To understand in more detail when our system
does well and when it does not, we performed ad-
ditional experiments on the token level. That is, in-
stead of deromanizing and normalizing entire SMS
messages, we take a close look at the kinds of ro-
manized word tokens that the system gets right and
wrong. We bin test set word tokens by their frequen-
cies in the training data and by the difference be-
tween their length (in characters) and the length of
their reference deromanization. Results are given in
Table 2. Not surprisingly, the system performs better
on tokens that it has seen many times in the training
data than on tokens it has never seen. It does not
perform perfectly on high frequency items because
the entropy of many romanized word types is high.
The system also performs best on romanized word
types that have a similar length to their deromanized
forms. This suggests that the system is more suc-
cessful at the deromanization task than the normal-
ization task, where lengths are more likely to vary
substantially due to SMS abbreviations.
6 Summary
We have defined a new task: deromanizing and nor-
malizing SMS messages written in non-native Ro-
man script. We have introduced a unique new anno-
tated dataset that allows exploration of informal text
for a low resource language.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of COLING/ACL.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing SMS messages. In Proceedings of ACL.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In NAACL-NLT Workshop on Creating Speech
and Language Data With Mechanical Turk.
Tao Chen and Min-Yen Kan. 2011. Creating a live, pub-
lic short message service corpus: The NUS SMS cor-
pus. Computation and Language, abs/1112.2468.
Monojit Choudhury, Vijit Jain Rahul Saraf, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. In International Journal on Docu-
ment Analysis and Recognition.
Li Haizhou, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration. In
Proceedings of ACL.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
Proceedings of ACL.
Ann Irvine, Chris Callison-Burch, and Alexandre Kle-
mentiev. 2010. Transliterating from all languages. In
Proceedings of the Association for Machine Transla-
tion in the America, AMTA ?10.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of ACL.
Zhifei Li and David Yarowsky. 2008. Unsupervised
translation induction for Chinese abbreviations using
monolingual corpora. In Proceedings of ACL/HLT.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-
vouchine. 2010. Report of NEWS 2010 translitera-
tion generation shared task. In Proceedings of the ACL
Named Entities WorkShop.
George James Lidstone. 1920. Note on the general case
of the Bayes-Laplace formula for inductive or a poste-
riori probabilities. Transactions of the Faculty of Ac-
tuaries, 8:182?192.
Richard Sproat, Alan W. Black, Stanley F. Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech & Language, pages 287?333.
78
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 10?51,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Findings of the 2012 Workshop on Statistical Machine Translation
Chris Callison-Burch
Johns Hopkins University
Philipp Koehn
University of Edinburgh
Christof Monz
University of Amsterdam
Matt Post
Johns Hopkins University
Radu Soricut
SDL Language Weaver
Lucia Specia
University of Sheffield
Abstract
This paper presents the results of the WMT12
shared tasks, which included a translation
task, a task for machine translation evaluation
metrics, and a task for run-time estimation of
machine translation quality. We conducted a
large-scale manual evaluation of 103 machine
translation systems submitted by 34 teams.
We used the ranking of these systems to mea-
sure how strongly automatic metrics correlate
with human judgments of translation quality
for 12 evaluation metrics. We introduced a
new quality estimation task this year, and eval-
uated submissions from 11 teams.
1 Introduction
This paper presents the results of the shared tasks
of the Workshop on statistical Machine Translation
(WMT), which was held at NAACL 2012. This
workshop builds on six previous WMT workshops
(Koehn and Monz, 2006; Callison-Burch et al,
2007; Callison-Burch et al, 2008; Callison-Burch
et al, 2009; Callison-Burch et al, 2010; Callison-
Burch et al, 2011). In the past, the workshops have
featured a number of shared tasks: a translation task
between English and other languages, a task for au-
tomatic evaluation metrics to predict human judg-
ments of translation quality, and a system combina-
tion task to get better translation quality by combin-
ing the outputs of multiple translation systems. This
year we discontinued the system combination task,
and introduced a new task in its place:
? Quality estimation task ? Structured predic-
tion tasks like MT are difficult, but the dif-
ficulty is not uniform across all input types.
It would thus be useful to have some mea-
sure of confidence in the quality of the output,
which has potential usefulness in a range of set-
tings, such as deciding whether output needs
human post-editing or selecting the best trans-
lation from outputs from a number of systems.
This shared task focused on sentence-level es-
timation, and challenged participants to rate
the quality of sentences produced by a stan-
dard Moses translation system on an English-
Spanish news corpus in one of two tasks:
ranking and scoring. Predictions were scored
against a blind test set manually annotated with
relevant quality judgments.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dissem-
inate common test sets and public training data with
published performance numbers, and to refine eval-
uation methodologies for machine translation. As
with previous workshops, all of the data, transla-
tions, and collected human judgments are publicly
available.1 We hope these datasets form a valuable
resource for research into statistical machine transla-
tion, system combination, and automatic evaluation
or automatic prediction of translation quality.
2 Overview of the Shared Translation Task
The recurring task of the workshop examines trans-
lation between English and four other languages:
German, Spanish, French, and Czech. We created a
1http://statmt.org/wmt12/results.html
10
test set for each language pair by translating newspa-
per articles. We additionally provided training data
and two baseline systems.
2.1 Test data
The test data for this year?s task was created by hir-
ing people to translate news articles that were drawn
from a variety of sources from November 15, 2011.
A total of 99 articles were selected, in roughly equal
amounts from a variety of Czech, English, French,
German, and Spanish news sites:2
Czech: Blesk (1), CTK (1), E15 (1), den??k (4),
iDNES.cz (3), iHNed.cz (3), Ukacko (2),
Zheny (1)
French: Canoe (3), Croix (3), Le Devoir (3), Les
Echos (3), Equipe (2), Le Figaro (3), Libera-
tion (3)
Spanish: ABC.es (4), Milenio (4), Noroeste (4),
Nacion (3), El Pais (3), El Periodico (3), Prensa
Libre (3), El Universal (4)
English: CNN (3), Fox News (2), Los Angeles
Times (3), New York Times (3), Newsweek (1),
Time (3), Washington Post (3)
German: Berliner Kurier (1), FAZ (3), Giessener
Allgemeine (2), Morgenpost (3), Spiegel (3),
Welt (3)
The translations were created by the professional
translation agency CEET.3 All of the translations
were done directly, and not via an intermediate lan-
guage.
Although the translations were done profession-
ally, we observed a number of errors. These errors
ranged from minor typographical mistakes (I was
terrible. . . instead of It was terrible. . . ) to more
serious errors of incorrect verb choices and nonsen-
sical constructions. An example of the latter is the
French sentence (translated from German):
Il a gratte? une planche de be?ton, perdit des
pie`ces du ve?hicule.
(He scraped against a concrete crash bar-
rier and lost parts of the car.)
2For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
3http://www.ceet.eu/
Here, the French verb gratter is incorrect, and the
phrase planche de be?ton does not make any sense.
We did not quantify errors, but collected a number
of examples during the course of the manual evalua-
tion. These errors were present in the data available
to all the systems and therefore did not bias the re-
sults, but we suggest that next year a manual review
of the professionally-collected translations be taken
prior to releasing the data in order to correct mis-
takes and provide feedback to the translation agency.
2.2 Training data
As in past years we provided parallel corpora to train
translation models, monolingual corpora to train lan-
guage models, and development sets to tune system
parameters. Some statistics about the training mate-
rials are given in Figure 1.
2.3 Submitted systems
We received submissions from 34 groups across 18
institutions. The participants are listed in Table 1.
We also included two commercial off-the-shelf MT
systems, three online statistical MT systems, and
three online rule-based MT systems. Not all systems
supported all language pairs. We note that the eight
companies that developed these systems did not sub-
mit entries themselves, but were instead gathered by
translating the test data via their interfaces (web or
PC).4 They are therefore anonymized in this paper.
The data used to construct these systems is not sub-
ject to the same constraints as the shared task partic-
ipants. It is possible that part of the reference trans-
lations that were taken from online news sites could
have been included in the systems? models, for in-
stance. We therefore categorize all commercial sys-
tems as unconstrained when evaluating the results.
3 Human Evaluation
As with past workshops, we placed greater empha-
sis on the human evaluation than on the automatic
evaluation metric scores. It is our contention that
automatic measures are an imperfect substitute for
human assessment of translation quality. Therefore,
we define the manual evaluation to be primary, and
4We would like to thank Ondr?ej Bojar for harvesting the
commercial entries, Christian Federmann for the statistical MT
entries, and Herve? Saint-Amand for the rule-based MT entries.
11
Europarl Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,965,734 2,007,723 1,920,209 646,605
Words 56,895,229 54,420,026 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 176,258 117,481 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 157,302 137,097 158,840 136,151
Words 4,449,786 3,903,339 3,915,218 3,403,043 3,950,394 3,856,795 2,938,308 3,264,812
Distinct words 78,383 57,711 63,805 53,978 130,026 57,464 136,392 52,488
United Nations Training Corpus
Spanish? English French? English
Sentences 11,196,913 12,886,831
Words 318,788,686 365,127,098 411,916,781 360,341,450
Distinct words 593,567 581,339 565,553 666,077
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,218,201 2,123,835 2,190,579 2,176,537 668,595
Words 59,848,044 60,476,282 63,439,791 53,534,167 14,946,399
Distinct words 123,059 181,837 145,496 394,781 172,461
News Language Model Data
English Spanish French German Czech
Sentence 51,827,706 8,627,438 16,708,622 30,663,107 18,931,106
Words 1,249,883,955 247,722,726 410,581,568 576,833,910 315,167,472
Distinct words 2,265,254 926,999 1,267,582 3,336,078 2,304,933
News Test Set
English Spanish French German Czech
Sentences 3003
Words 73,785 78,965 81,478 73,433 65,501
Distinct words 9,881 12,137 11,441 14,252 17,149
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of
distinct words (case-insensitive) is based on the provided tokenizer.
12
ID Participant
CMU Carnegie Mellon University (Denkowski et al, 2012)
CU-BOJAR Charles University - Bojar (Bojar et al, 2012)
CU-DEPFIX Charles University - DEPFIX (Rosa et al, 2012)
CU-POOR-COMB Charles University - Bojar (Bojar et al, 2012)
CU-TAMCH Charles University - Tamchyna (Tamchyna et al, 2012)
CU-TECTOMT Charles University - TectoMT (Dus?ek et al, 2012)
DFKI-BERLIN German Research Center for Artificial Intelligence (Vilar, 2012)
DFKI-HUNSICKER German Research Center for Artificial Intelligence - Hunsicker (Hunsicker et al, 2012)
GTH-UPM Technical University of Madrid (Lo?pez-Luden?a et al, 2012)
ITS-LATL Language Technology Laboratory @ University of Geneva (Wehrli et al, 2009)
JHU Johns Hopkins University (Ganitkevitch et al, 2012)
KIT Karlsruhe Institute of Technology (Niehues et al, 2012)
LIMSI LIMSI (Le et al, 2012)
LIUM University of Le Mans (Servan et al, 2012)
PROMT ProMT (Molchanov, 2012)
QCRI Qatar Computing Research Institute (Guzman et al, 2012)
QUAERO The QUAERO Project (Markus et al, 2012)
RWTH RWTH Aachen (Huck et al, 2012)
SFU Simon Fraser University (Razmara et al, 2012)
UEDIN-WILLIAMS University of Edinburgh - Williams (Williams and Koehn, 2012)
UEDIN University of Edinburgh (Koehn and Haddow, 2012)
UG University of Toronto (Germann, 2012)
UK Charles University - Zeman (Zeman, 2012)
UPC Technical University of Catalonia (Formiga et al, 2012)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C] Three online statistical machine translation systems
RBMT-[1,3,4] Three rule-based statistical machine translation systems
Table 1: Participants in the shared translation task. Not all teams participated in all language pairs. The translations
from the commercial, online, and rule-based systems were crawled by us, not submitted by the respective companies,
and are therefore anonymized. Anonymized identifiers were chosen so as to correspond with the WMT11 systems.
13
Language Pair Num Label Labels per
Systems Count System
Czech-English 6 6,470 1,078.3
English-Czech 13 11,540 887.6
German-English 16 7,135 445.9
English-German 15 8,760 584.0
Spanish-English 12 5,705 475.4
English-Spanish 11 7,375 670.4
French-English 15 6,975 465.0
English-French 15 7,735 515.6
Overall 103 61,695 598
Table 2: A summary of the WMT12 ranking task, show-
ing the number of systems and number of labels (rank-
ings) collected for each of the language translation tasks.
use the human judgments to validate automatic met-
rics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct on the scale of our
workshop. We distributed the workload across a
number of people, beginning with shared-task par-
ticipants and interested volunteers. This year, we
also opened up the evaluation to non-expert anno-
tators hired on Amazon Mechanical Turk (Callison-
Burch, 2009). To ensure that the Turkers provided
high quality annotations, we used controls con-
structed from the machine translation ranking tasks
from prior years. Control items were selected such
that there was high agreement across the system de-
velopers who completed that item. In all, there were
229 people who participated in the manual evalua-
tion, with 91 workers putting in more than an hour?s
worth of effort, and 21 putting in more than four
hours. After filtering Turker rankings against the
controls to discard Turkers who fell below a thresh-
old level of agreement on the control questions,
there was a collective total of 336 hours of usable
labor. This is similar to the total of 361 hours of
labor collected for WMT11.
We asked annotators to evaluate system outputs
by ranking translated sentences relative to each
other. This was our official determinant of trans-
lation quality. The total number of judgments col-
lected for each of the language pairs is given in Ta-
ble 2.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the instruc-
tions simple:
You are shown a source sentence followed
by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions, and asked to rank them.
We refer to each of these as ranking tasks or some-
times blocks.
Every language task had more than five partici-
pating systems ? up to a maximum of 16 for the
German-English task. Rather than attempting to get
a complete ordering over the systems in each rank-
ing task, we instead relied on random selection and
a reasonably large sample size to make the compar-
isons fair.
We use the collected rank labels to assign each
system a score that reflects how highly that system
was usually ranked by the annotators. The score for
some systemA reflects how frequently it was judged
to be better than other systems. Specifically, each
block in whichA appears includes four implicit pair-
wise comparisons (against the other presented sys-
tems). A is rewarded once for each of the four com-
parisons in which A wins, and its score is the num-
ber of such winning pairwise comparisons, divided
by the total number of non-tying pairwise compar-
isons involving A.
This scoring metric is different from that used in
prior years in two ways. First, the score previously
included ties between system rankings. In that case,
the score for A reflected how often A was rated as
better than or equal to other systems, and was nor-
malized by all comparisons involving A. However,
this approach unfairly rewards systems that are sim-
ilar (and likely to be ranked as tied). This is prob-
lematic since many of the systems use variations of
the same underlying decoder (Bojar et al, 2011).
A second difference is that this year we no longer
include comparisons against reference translations.
In the past, reference translations were included
14
among the systems to be ranked as controls, and
the pairwise comparisons were used in determin-
ing the best system. However, workers have a very
clear preference for reference translations, so includ-
ing them unduly penalized systems that, through
(un)luck of the draw, were pitted against the ref-
erences more often. These changes are part of a
broader discussion of the best way to produce the
system ranking, which we discuss at length in Sec-
tion 4.
The system scores are reported in Section 3.3.
Appendix A provides detailed tables that contain
pairwise head-to-head comparisons between pairs of
systems.
3.2 Inter- and Intra-annotator agreement in
the ranking task
Each year we calculate the inter- and intra-annotator
agreement for the human evaluation, since a reason-
able degree of agreement must exist to support our
process as a valid evaluation setup. To ensure we
had enough data to measure agreement, we occa-
sionally showed annotators items that were repeated
from previously completed items. These repeated
items were drawn from ones completed by the same
annotator and from different annotators.
We measured pairwise agreement among anno-
tators using Cohen?s kappa coefficient (?) (Cohen,
1960), which is defined as
? =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the anno-
tators agree, and P (E) is the proportion of time that
they would agree by chance. Note that ? is basically
a normalized version of P (A), one which takes into
account how meaningful it is for annotators to agree
with each other, by incorporating P (E). Note also
that ? has a value of at most 1 (and could possibly
be negative), with higher rates of agreement result-
ing in higher ?.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate at
which annotators agree, in the context of pairwise
comparisons. P (A) is computed similarly for intra-
annotator agreement (i.e. self-consistency), but over
pairwise comparisons that were annotated more than
once by a single annotator.
As for P (E), it should capture the probability that
two annotators would agree randomly. Therefore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is com-
puted empirically, by observing how often annota-
tors actually rank two systems as being tied. We
note here that this empirical computation is a depar-
ture from previous years? analyses, where we had
assumed that the three categories are equally likely
(yielding P (E) = 19 +
1
9 +
1
9 =
1
3 ). We believe that
this is a more principled approach, which faithfully
reflects the motivation of accounting for P (E) in the
first place.
Table 3 gives ? values for inter-annotator and
intra-annotator agreement. These give an indica-
tion of how often different judges agree, and how
often single judges are consistent for repeated judg-
ments, respectively. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0 ? 0.2 is slight, 0.2 ? 0.4
is fair, 0.4 ? 0.6 is moderate, 0.6 ? 0.8 is sub-
stantial, and 0.8 ? 1.0 is almost perfect. Based on
these interpretations, the agreement for sentence-
level ranking is fair for inter-annotator and moder-
ate for intra-annotator agreement. Consistent with
previous years, intra-annotator agreement is higher
than inter-annotator agreement, except for English?
Czech.
An important difference from last year is that the
evaluations were not constrained only to workshop
participants, but were made available to all Turk-
ers. The workshop participants were trusted to com-
plete the tasks in good faith, and we have multiple
years of data establishing general levels of inter- and
intra-annotator agreement. Their HITs were unpaid,
and access was limited with the use of a qualifica-
tion. The Turkers completed paid tasks, and we used
controls to filter out fraudulent and unconscientious
workers.
15
INTER-ANNOTATOR AGREEMENT INTRA-ANNOTATOR AGREEMENT
LANGUAGE PAIRS P (A) P (E) ? P (A) P (E) ?
Czech-English 0.567 0.405 0.272 0.660 0.405 0.428
English-Czech 0.576 0.383 0.312 0.566 0.383 0.296
German-English 0.595 0.401 0.323 0.733 0.401 0.554
English-German 0.598 0.394 0.336 0.732 0.394 0.557
Spanish-English 0.540 0.408 0.222 0.792 0.408 0.648
English-Spanish 0.504 0.398 0.176 0.566 0.398 0.279
French-English 0.568 0.406 0.272 0.719 0.406 0.526
English-French 0.519 0.388 0.214 0.634 0.388 0.401
WMT12 0.568 0.396 0.284 0.671 0.396 0.455
WMT11 0.601 0.362 0.375 0.722 0.362 0.564
Table 3: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11
rows contain the results from the European languages individual systems task (Callison-Burch et al (2011), Table 7).
Agreement rates vary widely across languages.
For inter-annotator agreements, the range is 0.176 to
0.336, while intra-annotator agreement ranges from
0.279 to 0.648. We note in particular the low agree-
ment rates among judgments in the English-Spanish
task, which is reflected in the relative lack of statis-
tical significance Table 4. The agreement rates for
this year were somewhat lower than last year.
3.3 Results of the Translation Task
We used the results of the manual evaluation to an-
alyze the translation quality of the different systems
that were submitted to the workshop. In our analy-
sis, we aimed to address the following questions:
? Which systems produced the best translation
quality for each language pair?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 4 shows the system ranking for each of the
translation tasks. For each language pair, we define
a system as ?winning? if no other system was found
statistically significantly better (using the Sign Test,
at p ? 0.10). In some cases, multiple systems are
listed as winners, either due to a large number of par-
ticipants or a low number of judgments per system
pair, both of which are factors that make it difficult
to achieve statistical significance.
As in prior years, unconstrained online systems
A and B are among the best for many tasks, with
a few notable exceptions. CU-DEPFIX, which post-
processes the output of ONLINE-B, was judged as
the best system for English-Czech. For the French-
English and English-French tasks, constrained sys-
tems came out on top, with LIMSI appearing both
times. Consistent with prior years, the rule-based
systems performed very well on the English-German
task. A rule-based system also had a good showing
for English-Spanish, but not really anywhere else.
Among the systems competing in all tasks, no sin-
gle system consistently appeared among the top en-
trants. Participants that competed in all tasks tended
to fair worse, with the exception of UEDIN. Addi-
tionally, KIT appeared in four tasks and was a con-
strained winner each time.
4 Methods for Overall Ranking
Last year one of the long papers published at WMT
criticized our method for compiling the overall rank-
ing for systems in the translation task (Bojar et
al., 2011). This year another paper shows some
additional potential inconsistencies in the rankings
(Lopez, 2012). In this section we delve into a de-
tailed analysis of a variety of methods that use the
human evaluation to create an overall ranking of sys-
tems.
In the human evaluation, we collect ranking judg-
ments for output from five systems at a time. We in-
terpret them as 10 ?
(
5?4
2
)
pairwise judgments over
systems and use these to analyze how each system
faired compared against each of the others. Not all
16
Czech-English
3,603?3,718 comparisons/system
System C? >others
ONLINE-B ? N 0.65
UEDIN ? Y 0.60
CU-BOJAR Y 0.53
ONLINE-A N 0.53
UK Y 0.37
JHU Y 0.32
Spanish-English
1,527?1,775 comparisons/system
System C? >others
ONLINE-A ? N 0.62
ONLINE-B ? N 0.61
QCRI ? Y 0.60
UEDIN ?? Y 0.58
UPC Y 0.57
GTH-UPM Y 0.52
RBMT-3 N 0.51
JHU Y 0.48
RBMT-4 N 0.46
RBMT-1 N 0.42
ONLINE-C N 0.42
UK Y 0.19
French-English
1,437?1,701 comparisons/system
System C? >others
LIMSI ?? Y 0.63
KIT ?? Y 0.61
ONLINE-A ? N 0.59
CMU ?? Y 0.57
ONLINE-B ? N 0.57
UEDIN Y 0.55
LIUM Y 0.52
RWTH Y 0.52
RBMT-1 N 0.46
RBMT-3 N 0.46
UK Y 0.44
SFU Y 0.44
RBMT-4 N 0.43
JHU Y 0.41
ONLINE-C N 0.32
English-Czech
2,652?3,146 comparisons/system
System C? >others
CU-DEPFIX ? N 0.66
ONLINE-B N 0.63
UEDIN ? Y 0.56
CU-TAMCH N 0.56
CU-BOJAR ? Y 0.54
CU-TECTOMT ? Y 0.53
ONLINE-A N 0.53
COMMERCIAL-1 N 0.48
COMMERCIAL-2 N 0.46
CU-POOR-COMB Y 0.44
UK Y 0.44
SFU Y 0.36
JHU Y 0.32
English-Spanish
2,013?2,294 comparisons/system
System C? >others
ONLINE-B ? N 0.65
RBMT-3 N 0.58
ONLINE-A ? N 0.56
PROMT N 0.55
UPC ? Y 0.52
UEDIN ? Y 0.52
RBMT-4 N 0.46
RBMT-1 N 0.45
ONLINE-C N 0.43
UK Y 0.41
JHU Y 0.36
English-French
1,410?1,697 comparisons/system
System C? >others
LIMSI ?? Y 0.66
RWTH Y 0.62
ONLINE-B N 0.60
KIT ?? Y 0.59
LIUM Y 0.55
UEDIN Y 0.53
RBMT-3 N 0.52
ONLINE-A N 0.51
PROMT N 0.51
RBMT-1 N 0.48
JHU Y 0.44
UK Y 0.40
RBMT-4 N 0.39
ONLINE-C N 0.39
ITS-LATL N 0.36
German-English
1,386?1,567 comparisons/system
System C? >others
ONLINE-A ? N 0.65
ONLINE-B ? N 0.65
QUAERO Y 0.61
RBMT-3 N 0.60
UEDIN ? Y 0.60
RWTH ? Y 0.56
KIT ? Y 0.55
LIMSI Y 0.54
QCRI Y 0.52
RBMT-1 N 0.51
RBMT-4 N 0.50
ONLINE-C N 0.43
DFKI-BERLIN Y 0.40
UK Y 0.37
JHU Y 0.34
UG Y 0.17
English-German
1,777?2,160 comparisons/system
System C? >others
ONLINE-B ? N 0.64
RBMT-3 N 0.63
RBMT-4 ? N 0.58
RBMT-1 N 0.56
LIMSI ? Y 0.55
ONLINE-A N 0.54
UEDIN-WILLIAMS ? Y 0.51
KIT ? Y 0.50
DFKI-HUNSICKER N 0.48
UEDIN ? Y 0.47
RWTH ? Y 0.47
ONLINE-C N 0.47
UK Y 0.45
JHU Y 0.43
DFKI-BERLIN Y 0.25
C? indicates whether system is constrained (unhighlighted rows): trained only using supplied training data, standard
monolingual linguistic tools, and, optionally, LDC?s English Gigaword.
? indicates a win: no other system is statistically significantly better at p-level ? 0.10 in pairwise comparison.
? indicates a constrained win: no other constrained system is statistically better.
Table 4: Official results for the WMT12 translation task. Systems are ordered by their > others score, reflecting how
often their translations won in pairwise comparisons. For detailed head-to-head comparisons, see Appendix A.
17
pairwise comparisons detect statistical significantly
superior quality of either system, and we note this
accordingly.
It is desirable to additionally produce an overall
ranking. In the past evaluation campaigns, we used
two different methods to obtain such a ranking, and
this year we use yet another one. In this section, we
discuss each of these overall ranking methods and a
few more.
4.1 Rank Ranges
In the first human evaluation, we use fluency and
adequacy judgments on a scale from 1 to 5 (Koehn
and Monz, 2006). We normalized the scores on a
per-sentence basis, thus converting them to a rela-
tive ranking in a 5-system comparison. We listed
systems by the average of these scores over all sen-
tences, in which they were judged.
We did not report ranks, but rank ranges. To
give an example: if a system scored neither sta-
tistically significantly better nor statistically signif-
icantly worse than 3 other systems, we assign it the
rank range 1?4. The given evidence is not sufficient
to rank it exactly, but it does rank somewhere in the
top 4.
In subsequent years, we did not continue the re-
porting of rank ranges (although they can be ob-
tained by examining the pairwise comparison ta-
bles), but we continued to report systems as win-
ners whenever there was not statistically signifi-
cantly outperformed by any other system.
4.2 Ratio of Wins and Ties
In the following years (Callison-Burch et al, 2007;
Callison-Burch et al, 2008; Callison-Burch et al,
2009; Callison-Burch et al, 2010; Callison-Burch et
al., 2011), we abandoned the idea of using fluency
and adequacy judgments, since they showed to be
less reliable than simple ranking of system transla-
tions. We also started to interpret the 5-system com-
parison as a set of pairwise comparisons.
Systems were then ranked by the ratio of how of-
ten they were ranked better or equal to any of the
other systems.
Given a set J of sentence-level judgments
(s1, s2, c) where s1 ? S and s2 ? S are two sys-
tems and
c =
?
??
??
win if s1 better than s2
tie if s1 equal to s2
loss if s1 worse than s2
(1)
then we can count the total number of wins and ties
of a system s as
win(s) = |{(s1, s2, c) ? J : s = s1, c = win}|+
|{(s1, s2, c) ? J : s = s2, c = loss}|
loss(s) = |{(s1, s2, c) ? J : s = s1, c = loss}|+
|{(s1, s2, c) ? J : s = s2, c = win}|
tie(s) = |{(s1, s2, c) ? J : s = s1, c = tie}|+
|{(s1, s2, c) ? J : s = s2, c = tie}|
(2)
and rank systems by the ratio
score(s) =
win(s) + tie(s)
win(s) + loss(s) + tie(s)
(3)
This ratio was used for the official rankings over
the last five years.
4.3 Ratio of Wins (Ignoring Ties)
Bojar et al (2011) present a persuasive argument
that our ranking scheme is biased towards systems
that are similar to many other systems. Given that
most of the systems are based on phrase-based mod-
els trained on the same training data, this is indeed a
valid concern.
They suggest ignoring ties, and using as ranking
score instead the following ratio:
score(s) =
win(s)
win(s) + loss(s)
(4)
This ratio is used for the official ranking this year.
4.4 Minimizing Pairwise Ranking Violations
Lopez (2012, in this volume) argues against using
aggregate statistics over a set of very diverse judg-
ments. Instead, a ranking that has the least number
of pairwise ranking violations is said to be preferred.
If we define the number of pairwise wins as
win(s1, s2) = |{(s1, s2, c) ? J : c = win}|+
|{(s2, s1, c) ? J : c = loss}|
(5)
then we define a count function for pairwise order
violations as
18
score(s1, s2) = max(0,win(s2, s1)? win(s1, s2))
(6)
Given a bijective ranking function R(s)? i with
the codomain of consecutive integers starting at 1,
the total number of pairwise ranking violations is de-
fined as
score(R) =
?
R(si)<R(sj)
score(si, sj) (7)
Finding the optimal rankingR that minimizes this
score is not trivial, but given the number of systems
involved in this evaluation campaign, it is quite man-
ageable.
4.5 Most Probable Ranking
We now introduce a variant to Lopez?s ranking
method. We motivate it first.
Consider the following scenario:
win(A,B) = 20 win(B,A) = 0
win(B,C) = 40 win(C,B) = 20
win(C,A) = 60 win(A,C) = 40
Since this constitutes a circle, there are three
rankings with the minimum number of 20 violation
(ABC, BCA, CAB).
However, we may want to take the ratio of wins
and losses for each pairwise ranking into account.
Using maximum likelihood estimation, we can de-
fine the probability that system s1 is better than sys-
tem s2 on a randomly drawn sentence as
p(s1 > s2) =
win(s1, s2)
win(s1, s2) + win(s2, s1)
(8)
We can then go on to define5 the probability of a
5Sketch of derivation:
p(s1 > s2 > s3) = p(s1 first)p(s2 second|s1 first)
(chain rule)
p(s1 first) = p(s1 > s2 and s1 > s3)
= p(s1 > s2)p(s1 > s3)
(independence assumption)
p(s2 sec.|s1 first) = p(s2 second)
(independence assumption)
= p(s2 > s3)
ranking of three systems as:
p(s1 > s2 > s3) = p(s1 > s2)p(s1 > s3)p(s2 > s3)
(9)
This function scores the three rankings in the ex-
ample above as follows:
p(A > B > C) = 2020
40
100
40
60 = 0.27
p(B > C > A) = 4060
0
20
60
100 = 0
p(C > A > B) = 60100
20
60
20
20 = 0.20
One disadvantage of this and the previous rank-
ing method is that they do not take advantage of all
available evidence. Consider the example:
win(A,B) = 100 win(B,A) = 0
win(A,C) = 60 win(C,A) = 40
win(B,C) = 50 win(C,B) = 50
Here, system A is clearly ahead, but how about B
and C? They are tied in their pairwise comparison.
So, both ABC and ACB have no pairwise ranking
violations and their most probable ranking score, as
defined above, is the same.
B is clearly worse than A, but C has a fighting
chance, and this should be reflected in the ranking.
The following two overall ranking methods over-
come this problem.
4.6 Monte Carlo Playoffs
The sports world is accustomed to the problem of
finding a ranking of sports teams, but being only able
to have pairwise competitions (think basketball or
football). One strategy is to stage playoffs.
Let?s say there are 4 systems: A,B, C, andD. As
in well-known play-off fashion, they are first seeded.
In our case, this happens randomly, say, 1:A, 2:B,
3:C, 4:D (for simplicity?s sake).
First round: A plays against D, B plays against
C. How do they play? We randomly select a sen-
tence on which they were compared (no ties). If A
is better according to human judgment than D, then
A wins.
Let?s say, A wins against D, and B loses against
C. This leads us to the final A against C and the
3rd place game D against B, in which, say, A and D
win. The resulting final ranking is ACDB.
We repeat this a million times with a different ran-
dom seeding every time, and compute the average
rank, which is then used for overall ranking.
19
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.641: ONLINE-B RBMT-4 RBMT-4 6.16: ONLINE-B 0.640 (1-2): ONLINE-B
2 0.627: RBMT-3 ONLINE-B ONLINE-B 6.39: RBMT-3 0.622 (1-2): RBMT-3
3 0.577: RBMT-4 RBMT-3 RBMT-3 6.98: RBMT-4 0.578 (3-5): RBMT-4
4 0.557: RBMT-1 RBMT-1 RBMT-1 7.32: RBMT-1 0.553 (3-6): RBMT-1
5 0.547: LIMSI ONLINE-A ONLINE-A 7.46: LIMSI 0.543 (3-7): LIMSI
6 0.537: ONLINE-A UEDIN-WILLIAMS LIMSI 7.57: ONLINE-A 0.534 (4-8): ONLINE-A
7 0.509: UEDIN-WILLIAMS LIMSI UEDIN-WILLIAMS 7.87: UEDIN-WILLIAMS 0.511 (5-9): UEDIN-WILLIAMS
8 0.503: KIT KIT KIT 7.98: KIT 0.503 (6-11): KIT
9 0.476: DFKI-HUNSICKER DFKI-HUNSICKER DFKI-HUNSICKER 8.32: UEDIN 0.477 (7-13): UEDIN
10 0.475: UEDIN ONLINE-C ONLINE-C 8.38: DFKI-HUNSICKER 0.472 (8-13): DFKI-HUNSICKER
11 0.470: RWTH UEDIN UEDIN 8.41: ONLINE-C 0.470 (8-13): ONLINE-C
12 0.470: ONLINE-C UK UK 8.44: RWTH 0.468 (8-13): RWTH
13 0.448: UK RWTH RWTH 8.72: UK 0.447 (10-14): UK
14 0.435: JHU JHU JHU 8.87: JHU 0.434 (12-14): JHU
15 0.249: DFKI-BERLIN DFKI-BERLIN DFKI-BERLIN 11.15: DFKI-BERLIN 0.249 (15): DFKI-BERLIN
Table 5: Overall ranking with different methods (English?German)
4.7 Expected Wins
In European national football competitions, each
team plays against each other team, and at the end
the number of wins decides the rankings.6 We can
simulate this type of tournament as well with Monte
Carlo methods. However, in the limit, each team will
be on average ranked based on its expected number
of wins in the competition. We can compute the ex-
pected number of wins straightforward as
score(si) =
1
|S| ? 1
?
j,j 6=i
p(si > sj) (10)
Note that this is very similar to Bojar?s method of
ranking systems, with one additional and important
twist. We can rewrite Equation 4, the variant that
ignores ties, as:
score(si) =
win(si)
win(si)+loss(si)
(11)
=
?
j,j 6=i win(si,sj)?
j,j 6=i win(si,sj)+loss(si,sj)
(12)
This section?s Equation 10 can be rewritten as:
score(si) =
1
|S|
?
j,j 6=i
win(si, sj)
win(si, sj) + loss(si, sj)
(13)
The difference is that the new overall ranking
method normalizes the win ratios per pairwise rank-
ing. And this makes sense, since it overcomes one
6They actually play twice against each other, to balance out
home field advantage, which is not a concern here.
problem with our traditional and Bojar?s ranking
method.
Previously, some systems were put at an dis-
advantage, if they are compared more frequently
against good systems than against bad systems. This
could happen, if participants were not allowed to
rank their own systems (a constraint we enforced
in the past, but no longer). This was noticed by
judges a few years ago, when we had instant re-
porting of rankings during the evaluation period. If
you have one of the best systems and carry out a lot
of human judgments, then competitors? systems will
creep up higher, since they are not compared against
your own (very good) system anymore, but more fre-
quently against bad systems.
4.8 Comparison
Table 5 shows the different rankings for English?
German, a rather typical example. The table dis-
plays the ranking of the systems according to five
different methods, alongside with system scores ac-
cording to the ranking method: the win ratio (Bo-
jar), the average rank (MC Playoffs), and the ex-
pected win ratio (Expected Wins). For the latter, we
performed bootstrap resampling and computed rank
ranges that lie in a 95% confidence interval. You
can find the tables for the other language pairs in the
annex.
The win-based methods (Bojar, MC Playoffs, Ex-
pected Wins) give very similar rankings ? exhibit-
ing mostly just the occasional pairwise flip or for
20
many language pairs the ranking is identical. The
same is true for the two methods based on pairwise
rankings (Lopez, Most Probable). However, the two
types of ranking lead to significantly different out-
comes.
For instance, the win-based methods are pretty
sure that ONLINE-B and RBMT-3 are the two top
performers. Bootstrap resampling of rankings ac-
cording to Expected Wins ranking draws a clear
line between them and the rest. However, Lopez?s
method ranks RBMT-4 first. Why? In direct com-
parison of the three systems, RBMT-4 beats statis-
tically insignificantly ONLINE-B 45% wins against
42% wins and essentially ties with RBMT-3 41%
wins against 41% wins (ONLINE-B beats RBMT-3
49%?35%, p ? 0.01).
We use Bojar?s method as our official method for
ranking in Table 4 and as the human judgments that
we used when calculating how well automatic eval-
uation metrics correlate with human judgments.
4.9 Number of Judgments Needed
In general, there are not enough judgments to rank
systems unambiguously. How many judgments do
we need?
We may extrapolate this number from the num-
ber of judgments we have. Figure 2 provides some
hints. The outlier is Czech?English, for which only
6 systems were submitted and we can separate them
almost completely even at p-level 0.01. For all the
other language pairs, we can only draw for around
40% of the pairwise comparisons conclusions with
that level of statistical significance.
Since the plots also contains the ratio of signifi-
cant conclusions when sub-sampling the number of
judgments, we obtain curves with a clear upward
slope. For English?Czech, for which we were able
to collect much more judgments, we can draw over
60% significant conclusions. The curve for this lan-
guage pair does not look much different than the
other languages, suggesting that doubling the num-
ber of judgments should allow similar levels for
them as well.
5 Metrics Task
In addition to allowing us to analyze the translation
quality of different systems, the data gathered during
p-level 0.01
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
p-level 0.05
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
p-level 0.10
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
Figure 2: Ratio of statistically significant pairwise com-
parisons at different p-levels, based on number of pair-
wise judgments collected.
21
Metric IDs Participant
AMBER National Research Council Canada (Chen et al, 2012)
METEOR CMU (Denkowski and Lavie, 2011)
SAGAN-STS FaMAF, UNC, Argentina (Castillo and Estrella, 2012)
SEMPOS Charles University (Macha?c?ek and Bojar, 2011)
SIMBLEU University of Sheffield (Song and Cohn, 2011)
SPEDE Stanford University (Wang and Manning, 2012)
TERRORCAT University of Zurich, DFKI, Charles U (Fishel et al, 2012)
BLOCKERRCATS, ENXERRCATS, WORD-
BLOCKERRCATS, XENERRCATS, POSF
DFKI (Popovic, 2012)
Table 6: Participants in the metrics task.
the manual evaluation is useful for validating auto-
matic evaluation metrics. Table 6 lists the partici-
pants in this task, along with their metrics.
A total of 12 metrics and their variants were sub-
mitted to the metrics task by 8 research groups. We
provided BLEU and TER scores as baselines. We
asked metrics developers to score the outputs of
the machine translation systems and system com-
binations at the system-level and at the segment-
level. The system-level metrics scores are given in
the Appendix in Tables 29?36. The main goal of
the metrics shared task is not to score the systems,
but instead to validate the use of automatic metrics
by measuring how strongly they correlate with hu-
man judgments. We used the human judgments col-
lected during the manual evaluation for the transla-
tion task and the system combination task to calcu-
late how well metrics correlate at system-level and
at the segment-level.
5.1 System-Level Metric Analysis
We measured the correlation of the automatic met-
rics with the human judgments of translation qual-
ity at the system-level using Spearman?s rank cor-
relation coefficient ?. We converted the raw scores
assigned to each system into ranks. We assigned a
human ranking to the systems based on the percent
of time that their translations were judged to be bet-
ter than the translations of any other system in the
manual evaluation (Equation 4).
When there are no ties, ? can be calculated using
the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
C
S
-E
N
-
6
S
Y
S
T
E
M
S
D
E
-E
N
-
16
S
Y
S
T
E
M
S
E
S
-E
N
-
12
S
Y
S
T
E
M
S
F
R
-E
N
-
15
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
System-level correlation for translations into English
SEMPOS .94 .92 .94 .80 .90
AMBER .83 .79 .97 .85 .86
METEOR .66 .89 .95 .84 .83
TERRORCAT .71 .76 .97 .88 .83
SIMPBLEU .89 .70 .89 .82 .82
TER -.89 -.62 -.92 -.82 .81
BLEU .89 .67 .87 .81 .81
POSF .66 .66 .87 .83 .75
BLOCKERRCATS -.64 -.75 -.88 -.74 .75
WORDBLOCKEC -.66 -.67 -.85 -.77 .74
XENERRCATS -.66 -.64 -.87 -.77 .74
SAGAN-STS .66 n/a .91 n/a n/a
Table 7: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average absolute
value.
22
E
N
-C
Z
-
10
S
Y
S
T
E
M
S
E
N
-D
E
-
22
S
Y
S
T
E
M
S
E
N
-E
S
-
15
S
Y
S
T
E
M
S
E
N
-F
R
-
17
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
System-level correlation for translations out of English
SIMPBLEU .83 .46 .42 .94 .66
BLOCKERRCATS -.65 -.53 -.47 -.93 .64
ENXERRCATS -.74 -.38 -.47 -.93 .63
POSF .80 .54 .37 .69 .60
WORDBLOCKEC -.71 -.37 -.47 -.81 .59
TERRORCAT .65 .48 .58 .53 .56
AMBER .71 .25 .50 .75 .55
TER -.69 -.41 -.45 -.66 .55
METEOR .73 .18 .45 .82 .54
BLEU .80 .22 .40 .71 .53
SEMPOS .52 n/a n/a n/a n/a
Table 8: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average abso-
lute value.
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher absolute value
for ? is making predictions that are more similar to
the human judgments than an automatic evaluation
metric with a lower absolute ?.
The system-level correlations are shown in Ta-
ble 7 for translations into English, and Table 8 out
of English, sorted by average correlation across the
language pairs. The highest correlation for each
language pair and the highest overall average are
bolded. Once again this year, many of the metrics
had stronger correlation with human judgments than
BLEU. The metrics that had the strongest correlation
this year were SEMPOS for the into English direc-
tion and SIMPBLEU for the out of English direc-
tion.
5.2 Segment-Level Metric Analysis
We measured the metrics? segment-level scores with
the human rankings using Kendall?s tau rank corre-
F
R
-E
N
(1
15
94
PA
IR
S
)
D
E
-E
N
(1
19
34
PA
IR
S
)
E
S
-E
N
(9
79
6
PA
IR
S
)
C
S
-E
N
(1
10
21
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations into English
SPEDE07-PP .26 .28 .26 .21 .25
METEOR .25 .27 .25 .21 .25
AMBER .24 .25 .23 .19 .23
SIMPBLEU .19 .17 .19 .13 .17
TERRORCAT .18 .19 .18 .19 .19
XENERRCATS .17 .18 .18 .13 .17
POSF .16 .18 .15 .12 .15
WORDBLOCKEC .15 .16 .17 .13 .15
BLOCKERRCATS .07 .08 .08 .06 .07
SAGAN-STS n/a n/a .21 .20 n/a
Table 9: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average correla-
tion.
E
N
-F
R
(1
15
62
PA
IR
S
)
E
N
-D
E
(1
45
53
PA
IR
S
)
E
N
-E
S
(1
18
34
PA
IR
S
)
E
N
-C
S
(1
88
05
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations out of English
METEOR .26 .18 .21 .16 .20
AMBER .23 .17 .22 .15 .19
TERRORCAT .18 .19 .18 .18 .18
SIMPBLEU .2 .13 .18 .10 .15
ENXERRCATS .20 .11 .17 .09 .14
POSF .15 .13 .15 .13 .14
WORDBLOCKEC .19 .1 .17 .1 .14
BLOCKERRCATS .13 .04 .12 .01 .08
Table 10: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average corre-
lation.
23
lation coefficient. We calculated Kendall?s tau as:
? =
num concordant pairs - num discordant pairs
total pairs
where a concordant pair is a pair of two translations
of the same segment in which the ranks calculated
from the same human ranking task and from the cor-
responding metric scores agree; in a discordant pair,
they disagree. In order to account for accuracy- vs.
error-based metrics correctly, counts of concordant
vs. discordant pairs were calculated specific to these
two metric types. The possible values of ? range
between 1 (where all pairs are concordant) and ?1
(where all pairs are discordant). Thus an automatic
evaluation metric with a higher value for ? is mak-
ing predictions that are more similar to the human
judgments than an automatic evaluation metric with
a lower ? .
We did not include cases where the human rank-
ing was tied for two systems. As the metrics produce
absolute scores, compared to five relative ranks in
the human assessment, it would be potentially un-
fair to the metric to count a slightly different met-
ric score as discordant with a tie in the relative hu-
man rankings. A tie in automatic metric rank for
two translations was counted as discordant with two
corresponding non-tied human judgments.
The correlations are shown in Table 9 for trans-
lations into English, and Table 10 out of English,
sorted by average correlation across the four lan-
guage pairs. The highest correlation for each lan-
guage pair and the highest overall average are
bolded. For the into English direction SPEDE and
METEOR tied for the highest segment-level correla-
tion. METEOR performed the best for the out of En-
glish direction, with AMBER doing admirably well
in both the into- and the out-of-English directions.
6 Quality Estimation task
Quality estimation aims to provide a quality indica-
tor for machine translated sentences at various gran-
ularity levels. It differs from MT evaluation, because
quality estimation techniques do not rely on refer-
ence translations. Instead, quality estimation is gen-
erally addressed using machine learning techniques
to predict quality scores. Potential applications of
quality estimation include:
? Deciding whether a given translation is good
enough for publishing as is
? Informing readers of the target language only
whether or not they can rely on a translation
? Filtering out sentences that are not good
enough even for post-editing by professional
translators
? Selecting the best translation among options
from multiple systems.
This shared-task provides a first common ground
for development and comparison of quality estima-
tion systems, focusing on sentence-level estimation.
It provides training and test datasets, along with
evaluation metrics and a baseline system. The goals
of this shared task are:
? To identify new and effective quality indicators
(features)
? To identify alternative machine learning tech-
niques for the problem
? To test the suitability of the proposed evalua-
tion metrics for quality estimation systems
? To establish the state of the art performance in
the field
? To contrast the performance of regression and
ranking techniques.
The task provides datasets for a single language
pair, text domain and MT system: English-Spanish
news texts produced by a phrase-based SMT sys-
tem (Moses) trained on Europarl and News Com-
mentaries corpora provided in the WMT10 transla-
tion task. As training data, translations were man-
ually annotated for quality in terms of post-editing
effort (1-5 scores) and were provided together with
their source sentences, reference translations, and
post-edited translations (Section 6.1). The shared-
task consisted on automatically producing quality-
estimations for a blind test-set, where English source
sentences and their MT-translations were used as in-
puts. Hidden (and subsequently publicly-released)
manual effort-annotations of those translations (ob-
tained in the same fashion as for the training data)
24
were used as reference labels to evaluate the per-
formance of the participating systems (Section 6.1).
Participants also had full access to the translation
engine-related resources (Section 6.1) and could use
any additional external resources. We have also pro-
vided a software package to extract baseline quality
estimation features (Section 6.3).
Participants could submit up to two systems for
two variations of the task: ranking, where par-
ticipants submit a ranking of translations (no ties
allowed), without necessarily giving any explicit
scores for translations, and scoring, where partici-
pants submit a score for each sentence (in the [1,5]
range). Each of these subtasks is evaluated using
specific metrics (Section 6.2).
6.1 Datasets and resources
Training data
The training data used was selected from data
available from previous WMT shared-tasks for
machine-translation: a subset of the WMT10
English-Spanish test set, and a subset of the WMT09
English-Spanish test set, for a total of 1832 sen-
tences.
The training data consists of the following re-
sources:
? English source sentences
? Spanish machine-translation outputs, created
using the SMT Moses engine
? Effort scores, created by using three profes-
sional post-editors using guidelines describ-
ing Post-Editing (PE) effort from highest effort
(score 1) to lowest effort (score 5)
? Post-Editing output, created by a pool of pro-
fessional post-editors starting from the source
sentences and the Moses translations; these PE
outputs were created before the effort scores
were elicited, and were shown to the PE-effort
judges to facilitate their effort estimates
? Spanish translation outputs, created as part of
the WMT machine-translation shared-task as
reference translations for the English source
sentences (independent of any MT output).
The guidelines used by the PE-effort judges to as-
sign scores 1-5 for each of the ?source, MT-output,
PE-output? triplets are the following:
[1] The MT output is incomprehensible, with lit-
tle or no information transferred accurately. It
cannot be edited, needs to be translated from
scratch.
[2] About 50-70% of the MT output needs to be
edited. It requires a significant editing effort in
order to reach publishable level.
[3] About 25-50% of the MT output needs to be
edited. It contains different errors and mis-
translations that need to be corrected.
[4] About 10-25% of the MT output needs to be
edited. It is generally clear and intelligible.
[5] The MT output is perfectly clear and intelligi-
ble. It is not necessarily a perfect translation,
but requires little or no editing.
Providing reliable effort estimates turned out to
be a difficult task for the PE-effort judges, even in
the current set-up (with post edited outputs available
for consultation). To eliminate some of the noise
from these judgments, we performed an intermedi-
ate cleaning step, in which we eliminated the sen-
tences for which the difference between the max-
imum score and the minimum score assigned be-
tween the three judges was > 1. We started the
data-creation process from a total of 2000 sentences
for the training set, and the final 1832 sentences we
selected as training data were the ones that passed
through this intermediate cleaning step.
Besides score disagreement, we noticed another
trend on the human judgements of PE-effort. Some
judges tend to give more moderate scores (in the
middle of available range), while others like to com-
mit also to scores that are more in the extremes of
the available range. Since the quality estimation task
would be negatively influenced by having most of
the scores in the middle of the range, we have chosen
to compute the final effort scores as an weighted av-
erage between the three PE-effort scores, with more
weight given to the judges with higher standard de-
viation from their own mean score. We have used
25
weights 3, 2, and 1 for the three PE-effort judges ac-
cording to this criterion. There is an additional ad-
vantage resulting from this weighted average score:
instead of obtaining average numbers only at val-
ues x.0, x.33, and x.66 (for unweighted average)7,
the weighted averages are spread more evenly in the
range [1, 5].
A few variations of the training data were pro-
vided, including version with cases restored and a
version detokenized. In addition, engine-internal
information from Moses such as phrase and word
alignments, detailed model scores, etc. (parameter
-trace), n-best lists and stack information from the
search graph as a word graph (parameter -output-
word-graph) as produced by the Moses engine were
provided.
The rationale behind releasing this engine-
internal data was to make it possible for this shared-
task to address quality estimation using a glass-box
approach, that is, making use of information from
the internal workings of the MT engine.
Test data
The test data was a subset of the WMT12 English-
Spanish test set, consisting of 442 sentences. The
test data consists of the following files:
? English source sentences
? Spanish machine-translation outputs, created
using the same SMT Moses engine used to cre-
ate the training data
? Effort scores, created by using three profes-
sional post-editors8 using guidelines describing
PE effort from highest effort (score 1) to lowest
effort (score 5)
The first two files were the input for the quality-
estimation shared-task participating systems. Since
the Moses engine used to create the MT outputs was
the same as the one used for generating the train-
ing data, the engine-internal resources are the same
7These three values are the only ones possible given the
cleaning step we perform prior to averaging the scores, which
ensures that the difference between the maximum score and the
minimum score is at most 1.
8The same post-editors that were used to create the training
data were used to create the test data.
as the ones we released as part of the training data
package.
The effort scores were released after the partic-
ipants submitted their shared-task submission, and
were solely used to evaluate the submissions accord-
ing to the established metrics. The guidelines used
by the PE-effort judges to assign 1-5 scores were the
same as the ones used for creating the training data.
We have used the same criteria to ensure the con-
sistency of the human judgments. The initial set of
candidates consisted of 604 sentences, of which only
442 met this criteria. The final scores used as gold-
values have been obtained using the same weighted-
average scheme as for the training data.
Resources
In addition to the training and test materials, we
made several additional resources that were used for
the baseline QE system and/or the SMT system that
produced the training and test datasets:
? The SMT training corpus: source and target
sides of the corpus used to train the Moses en-
gine. These are a concatenation of the Eu-
roparl and the news-commentary data sets from
WMT10 that were tokenized, cleaned (remov-
ing sentences longer than 80 tokens) and true-
cased.
? Two Language models: 5-gram LM generated
from the interpolation of the two target cor-
pora after tokenization and truecasing (used
by Moses) and a trigram LM generated from
the two source corpora and filtered to remove
singletons (used by the baseline QE system).
We also provided unigram, bigram and trigram
counts (used in the baseline QE system).
? An IBM Model 1 table that generated by
Giza++ using the SMT training corpora.
? A word-alignment file as produced by the
grow-diag-final heuristic in Moses for the SMT
training set.
? A phrase table with word alignment informa-
tion generated from the parallel corpora.
? The Moses configuration file used for decod-
ing.
26
6.2 Evaluation metrics
Ranking metrics
For the ranking task, we defined a novel met-
ric that provides some advantages over a more tra-
ditional ranking metrics like Spearman correlation.
Our metric, called DeltaAvg, assumes that the refer-
ence test set has a number associated with each en-
try that represents its extrinsic value. For instance,
using the effort scale we described in Section 6.1,
we associate a value between 1 and 5 with each
sentence, representing the quality of that sentence.
Given these values, our metric does not need an ex-
plicit reference ranking, the way the Spearman rank-
ing correlation does.9 The goal of the DeltaAvg met-
ric is to measure how valuable a proposed ranking
(which we call a hypothesis ranking) is according to
the extrinsic values associated with the test entries.
We first define a parameterized version of this
metric, called DeltaAvg[n]. The following notations
are used: for a given entry sentence s, V (s) repre-
sents the function that associates an extrinsic value
to that entry; we extend this notation to a set S, with
V (S) representing the average of all V (s), s ? S.
Intuitively, V (S) is a quantitative measure of the
?quality? of the set S, as induced by the extrinsic
values associated with the entries in S. For a set
of ranked entries S and a parameter n, we denote
by S1 the first quantile of set S (the highest-ranked
entries), S2 the second quantile, and so on, for n
quantiles of equal sizes.10 We also use the notation
Si,j =
?j
k=i Sk. Using these notations, we define:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1
? V (S) (14)
When the valuation function V is clear from the con-
text, we write DeltaAvg[n] for DeltaAvgV [n]. The
parameter n represents the number of quantiles we
want to split the set S into. For instance, n = 2
gives DeltaAvg[2] = V (S1) ? V (S), hence it mea-
sures the difference between the quality of the top
9A reference ranking can be implicitly induced according to
these values; if, as in our case, higher values mean better sen-
tences, then the reference ranking is defined such that higher-
scored sentences rank higher than lower-scored sentences.
10If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
quantile (top half) S1 and the overall quality (rep-
resented by V (S)). For n = 3, DeltaAvg[3] =
(V (S1)+V (S1,2)/2?V (S) = ((V (S1)?V (S))+
(V (S1,2 ? V (S)))/2, hence it measures an aver-
age difference across two cases: between the quality
of the top quantile (top third) and the overall qual-
ity, and between the quality of the top two quan-
tiles (S1?S2, top two-thirds) and the overall quality.
In general, DeltaAvg[n] measures an average differ-
ence in quality across n ? 1 cases, with each case
measuring the impact in quality of adding an addi-
tional quantile, from top to bottom. Finally, we de-
fine:
DeltaAvgV =
?N
n=2 DeltaAvgV [n]
N ? 1
(15)
whereN = |S|/2. As before, we write DeltaAvg for
DeltaAvgV when the valuation function V is clear
from the context. The DeltaAvg metric is an aver-
age across all DeltaAvg[n] values, for those n values
for which the resulting quantiles have at least 2 en-
tries (no singleton quantiles). The DeltaAvg metric
has some important properties that are desired for a
ranking metric (see Section 6.4 for the results of the
shared-task that substantiate these claims):
? it is non-parametric (i.e., it does not depend on
setting particular parameters)
? it is automatic and deterministic (and therefore
consistent)
? it measures the quality of a hypothesis rank-
ing from an extrinsic perspective (as offered by
function V )
? its values are interpretable: for a given set of
ranked entries, a value DeltaAvg of 0.5 means
that, on average, the difference in quality be-
tween the top-ranked quantiles and the overall
quality is 0.5
? it has a high correlation with the Spearman rank
correlation coefficient, which makes it as use-
ful as the Spearman correlation, with the added
advantage of its values being extrinsically in-
terpretable.
27
In the rest of this paper, we present results for
DeltaAvg using as valuation function V the Post-
Editing effort scores, as defined in Section 6.1.
We also report the results of the ranking task using
the more-traditional Spearman correlation.
Scoring metrics
For the scoring task, we use two metrics that have
been traditionally used for measuring performance
for regression tasks: Mean Absolute Error (MAE) as
a primary metric, and Root of Mean Squared Error
(RMSE) as a secondary metric. For a given test set
S with entries si, 1 ? i ? |S|, we denote by H(si)
the proposed score for entry si (hypothesis), and by
V (si) the reference value for entry si (gold-standard
value). We formally define our metrics as follows:
MAE =
?N
i=1 |H(si)? V (si)|
N
(16)
RMSE =
?
?N
i=1(H(si)? V (si))
2
N
(17)
where N = |S|. Both these metrics are non-
parametric, automatic and deterministic (and there-
fore consistent), and extrinsically interpretable. For
instance, a MAE value of 0.5 means that, on aver-
age, the absolute difference between the hypothe-
sized score and the reference score value is 0.5. The
interpretation of RMSE is similar, with the differ-
ence that RMSE penalizes larger errors more (via
the square function).
6.3 Participants
Eleven teams (listed in Table 11) submitted one or
more systems to the shared task, with most teams
submitting for both ranking and scoring subtasks.
Each team was allowed up to two submissions (for
each subtask). In the descriptions below participa-
tion in the ranking is denoted (R) and scoring is de-
noted (S).
Baseline system (R, S): the baseline system used
the feature extraction software (also provided
to all participants). It analyzed the source and
translation files and the SMT training corpus
to extract the following 17 system-independent
features that were found to be relevant in previ-
ous work (Specia et al, 2009):
? number of tokens in the source and target
sentences
? average source token length
? average number of occurrences of the tar-
get word within the target sentence
? number of punctuation marks in source
and target sentences
? LM probability of source and target sen-
tences using language models described in
Section 6.1
? average number of translations per source
word in the sentence: as given by IBM 1
model thresholded so that P (t|s) > 0.2,
and so that P (t|s) > 0.01 weighted by
the inverse frequency of each word in the
source side of the SMT training corpus
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower fre-
quency words) and 4 (higher frequency
words) in the source side of the SMT train-
ing corpus
? percentage of unigrams in the source sen-
tence seen in the source side of the SMT
training corpus
These features are used to train a Support Vec-
tor Machine (SVM) regression algorithm using
a radial basis function kernel with the LIBSVM
package (Chang and Lin, 2011). The ?,  and C
parameters were optimized using a grid-search
and 5-fold cross validation on the training set.
We note that although the system is referred to
as a ?baseline?, it is in fact a strong system.
Although it is simple it has proved to be ro-
bust across a range of language pairs, MT sys-
tems, and text domains. It is a simpler variant
of the system used in (Specia, 2011). The ratio-
nale behind having such a strong baseline was
to push systems to exploit alternative sources
of information and combination / learning ap-
proaches.
SDLLW (R, S): Both systems use 3 sets of fea-
tures: the 17 baseline features, 8 system-
dependent features from the decoder logs of
Moses, and 20 features developed internally.
Some of these features made use of additional
data and/or resources, such as a secondary
28
ID Participating team
PRHLT-UPV Universitat Politecnica de Valencia, Spain (Gonza?lez-Rubio et al, 2012)
UU Uppsala University, Sweden (Hardmeier et al, 2012)
SDLLW SDL Language Weaver, USA (Soricut et al, 2012)
Loria LORIA Institute, France (Langlois et al, 2012)
UPC Universitat Politecnica de Catalunya, Spain (Pighin et al, 2012)
DFKI DFKI, Germany (Avramidis, 2012)
WLV-SHEF University of Wolverhampton & University of Sheffield, UK (Felice and Specia, 2012)
SJTU Shanghai Jiao Tong University, China (Wu and Zhao, 2012)
DCU-SYMC Dublin City University, Ireland & Symantec, Ireland (Rubino et al, 2012)
UEdin University of Edinburgh, UK (Buck, 2012)
TCD Trinity College Dublin, Ireland (Moreau and Vogel, 2012)
Table 11: Participants in the WMT12 Quality Evaluation shared task.
MT system that was used as pseudo-reference
for the hypothesis, and POS taggers for both
languages. Feature-selection algorithms were
used to select subsets of features that directly
optimize the metrics used in the task. System
?SDLLW M5PbestAvgDelta? uses a resulting
15-feature set optimized towards the AvgDelta
metric. It employs an M5P model to learn a
decision-tree with only two linear equations.
System ?SDLLW SVM? uses a 20-feature set
and an SVM epsilon regression model with ra-
dial basis function kernel with parameters C,
gamma, and epsilon tuned on a development
set (305 training instances). The model was
trained with 10-fold cross validation and the
tuning process was restarted several times us-
ing different starting points and step sizes to
avoid overfitting. The final model was selected
based on its performance on the development
set and the number of support vectors.
UU (R, S): System ?UU best? uses the 17 base-
line features, plus 82 features from Hardmeier
(2011) (with some redundancy and some over-
lap with baseline features), and constituency
trees over input sentences generated by the
Stanford parser and dependency trees over both
input and output sentences generated by the
MaltParser. System ?UU bltk? uses only the
17 baseline features plus constituency and de-
pendency trees as above. The machine learn-
ing component in both cases is SVM regres-
sion (SVMlight software). For the ranking task,
the ranking induced by the regression output
is used. The system uses polynomial kernels
of degree 2 (UU best) and 3 (UU bltk) as well
as two different types of tree kernels for con-
stituency and dependency trees, respectively.
The SVM margin/error trade-off, the mixture
proportion between tree kernels and polyno-
mial kernels and the degree of the polynomial
kernels were optimised using grid search with
5-fold cross-validation over the training set.
TCD (R, S): ?TCD M5P-resources-only? uses
only the baseline features, while ?TCD M5P-
all? uses the baseline and additional features.
A number of metrics (used as features in
TCD M5P-all) were proposed which work in
the following way: given a sentence to eval-
uate (source sentence for complexity or target
sentence for fluency), it is compared against
some reference data using similarity mea-
sures (various metrics which compare distri-
butions of n-grams). The training data was
used as reference, along with the Google n-
grams dataset. Several learning methods were
tested using Weka on the training data (10-
fold cross-validation). The system submission
uses the M5P (regression with decision trees)
algorithm which performed best. Contrary to
what had been observed on the training data
using cross-validation, ?TCD M5P-resources-
only? performs better than ?TCD M5P-all? on
the test data.
29
PRHLT-UPV (R, S): The system addresses the
task using a regression algorithm with 475 fea-
tures, including the 17 the baseline features.
Most of the features are defined as word scores.
Among them, the features obtained form a
smoothed naive Bayes classifier have shown to
be particularly interesting. Different methods
to combine word-level scores into sentence-
level features were investigated. For model
building, SVM regression was used. Given
the large number of features, the training data
provided as part of the task was insufficient
yielding unstable systems with not so good per-
formance. Different feature selection methods
were implemented to determine a subset of rel-
evant features. The final submission used these
relevant features to train an SVM system whose
parameters were optimized with respect to the
final evaluation metrics.
UEDIN (R, S): The system uses the baseline fea-
tures along with some additional features: bi-
nary features for named entities in source using
Stanford NER Tagger; binary indicators for oc-
currence of quotes or parenthetical segments,
words in upper case and numbers; geometric
mean of target word probabilities and proba-
bility of worst scoring word under a Discrim-
inative Word Lexicon Model; Sparse Neural
Network directly mapping from source to tar-
get (using the vector space model) with source
and target side either filtered to relevant words
or hashed to reduce dimensionality; number of
times at least a 3-gram is seen normalized by
sentence length; and Levenshtein distance of
either source or translation to closest entry of
the SMT training corpus on word or character
level. An ensemble of neural networks opti-
mized for RMSE was used for prediction (scor-
ing) and ranking. The contribution of new fea-
tures was tested by adding them to the baseline
features using 5-fold cross-validation. Most
features did not result in any improvement over
the baseline. The final submission was a com-
bination of all feature sets that showed im-
provement.
SJTU (R, S): The task is treated as a regression
problem using the epsilon-SVM method. All
features are extracted from the official data, in-
volving no external NLP tools/resources. Most
of them come from the phrase table, decod-
ing data and SMT training data. The focus
is on special word relations and special phrase
patterns, thus several feature templates on this
topic are extracted. Since the training data is
not large enough to assign weights to all fea-
tures, methods for estimating common strings
or sequences of words are used. The training
data is divided in 3/4 for training and 1/4 for
development to filter ineffective features. Be-
sides the baseline features, the final submission
contains 18 feature templates and about 4 mil-
lion features in total.
WLV-SHEF (R, S): The systems integrates novel
linguistic features from the source and target
texts in an attempt to overcome the limitations
of existing shallow features for quality estima-
tion. These linguistically-informed features in-
clude part-of-speech information, phrase con-
stituency, subject-verb agreement and target
lexicon analysis, which are extracted using
parsers, corpora and auxiliary resources. Sys-
tems are built using epsilon-SVM regression
with parameters optimised using 5-fold cross-
validation on the training set and two differ-
ent feature sets: ?WLV-SHEF BL? uses the 17
baseline features plus 70 linguistically inspired
features, while ?WLV-SHEF FS? uses a larger
set of 70 linguistic plus 77 shallow features (in-
cluding the baseline). Although results indicate
that the models fall slightly below the baseline,
further analysis shows that linguistic informa-
tion is indeed informative and complementary
to shallow indicators.
DFKI (R, S): ?DFKI morphPOSibm1LM? (R) is
a simple linear interpolation of POS 6-gram
language model scores, morpheme 6-gram lan-
guage model scores, IBM 1 scores (both ?di-
rect? and ?inverse?) for POS 4-grams and for
morphemes. The parallel News corpora from
WMT10 is used as extra data to train the lan-
guage model and the IBM 1 model. ?DFKI cfs-
30
plsreg? and ?DFKI grcfs-mars? (S) use a col-
lection of 264 features generated containing
the baseline features and additional resources.
Numerous methods of feature selection were
tested using 10-fold cross validation on the
training data, reducing these to 23 feature sets.
Several regression and (discretized) classifica-
tion algorithms were employed to train predic-
tion models. The best-performing models in-
cluded features derived from PCFG parsing,
language quality checking and LM scoring, of
both source and target, besides features from
the SMT search graph and a few baseline fea-
tures. ?DFKI cfs-plsreg? uses a Best First
correlation-based feature selection technique,
trained with Partial Least Squares Regression,
while ?DFKI grcfs-mars? uses a Greedy Step-
wise correlation-based feature selection tech-
nique, trained with multivariate adaptive re-
gression splines.
DCU-SYMC (R, S): Systems are based on a clas-
sification approach using a set of features that
includes the baseline features. The manually
assigned quality scores provided for each MT
output in the training set were rounded in or-
der to apply classification algorithms on a lim-
ited set of classes (integer values from 1 to 5).
Three classifiers were combined by averaging
the predicted classes: SVM using sequential
minimal optimization and RBF kernel (parame-
ters optimized by grid search), Naive Bayes and
Random Forest. ?DCU-SYMC constrained? is
based on a set of 70 features derived only from
the data provided for the task. These include
a set of features which attempt to model trans-
lation adequacy using a bilingual topic model
built using Latent Dirichlet Allocation. ?DCU-
SYMC unconstrained? is based on 308 fea-
tures including the constrained ones and oth-
ers extracted using external tools: grammatical-
ity features extracted from the source segments
using the TreeTagger part-of-speech tagger, an
English precision grammar, the XLE parser and
the Brown re-ranking parser and features based
on part-of-speech tag counts extracted from the
MT output using a Spanish TreeTagger model.
Loria (S): Several numerical or boolean features
are computed from the source and target sen-
tences and used to train an SVM regression al-
gorithm with linear (?Loria SVMlinear?) and
radial basis function (?Loria SVMrbf?) as ker-
nel. For the radial basis function, a grid search
is performed to optimise the parameter ?. The
official submission use the baseline features
and a number of features proposed in previous
work (Raybaud et al, 2011), amounting to 66
features. A feature selection algorithm is used
in order to remove non-informative features.
No additional data other than that provided for
the shared task is considered. The training data
is split into a training part (1000 sentences) and
a development part (832 sentences) to learn the
regression model and optimise the parameters
of the regression and for feature selection.
UPC (R, S): The systems use several features on
top of the baseline features. These are mostly
based on different language models estimated
on reference and automatic Spanish transla-
tions of the news-v7 corpus. The automatic
translations are generated by the system used
for the shared task. N-gram LMs are esti-
mated on word forms, POS tags, stop words
interleaved by POS tags, stop-word patterns,
plus variants in which the POS tags are re-
placed with the stem or root of each target
word. The POS tags on the target side are ob-
tained by projecting source side annotations via
automatic alignments. The resulting features
are: the perplexity of each additional language
model, according to the two translations, and
the ratio between the two perplexities. Addi-
tionally, features that estimate the likelihood
of the projection of dependency parses on the
two translations are encoded. For learning, lin-
ear SVM regression is used. Optimization was
done via 5-fold cross-validation on a develop-
ment data. Features are encoded by means of
their z-scores, i.e. how many standard devia-
tions the observed value is above or below the
mean. A variant of the system, ?UPC-2? uses
an option of SVMLight that removes inconsis-
tent points from the training set and retrains the
model until convergence.
31
6.4 Results
Here we give the official results for the ranking and
scoring subtasks followed by a discussion that high-
lights the main findings of the task.
Ranking subtask
Table 12 gives the results for the ranking sub-
task. The table is sorted from best to worse using
the DeltaAvg metric scores (Equation 15) as pri-
mary key and the Spearman correlation scores as
secondary key.
The winning submissions for the ranking subtask
are SDLLW?s M5PbestDeltaAvg and SVM entries,
which have DeltaAvg scores of 0.63 and 0.61, re-
spectively. The difference with respect to all the
other submissions is statistically significant at p =
0.05, using pairwise bootstrap resampling (Koehn,
2004). The state-of-the-art baseline system has a
DeltaAvg score of 0.55 (Spearman rank correla-
tion of 0.58). Five other submissions have perfor-
mances that are not different from the baseline at a
statistically-significant level (p = 0.05), as shown
by the gray area in the middle of Table 12. Three
submissions scored higher than the baseline system
at p = 0.05 (systems above the middle gray area),
which indicates that this shared-task succeeded in
pushing the state-of-the-art performance to new lev-
els. The range of performance for the submissions
in the ranking task varies from a DeltaAvg of 0.65
down to a DeltaAvg of 0.15 (with Spearman values
varying from 0.64 down to 0.19).
In addition to the performance of the official sub-
mission, we report here results obtained by var-
ious oracle methods. The oracle methods make
use of various metrics that are associated in a or-
acle manner to the test input: the gold-label Ef-
fort metric for ?Oracle Effort?, the HTER metric
computed against the post-edited translations as ref-
erence for ?Oracle HTER?, and the BLEU metric
computed against the same post-edited translations
as reference for ?Oracle (H)BLEU?.11 The ?Oracle
Effort? DeltaAvg score of 0.95 gives an upperbound
in terms of DeltaAvg for the test set used in this
evaluation. It basically indicates that, for this set,
11We use the (H)BLEU notation to underscore the use of
Post-Edited translations as reference, as opposed to using ref-
erences that are not the product of a Post-Editing process, as for
the traditional BLEU metric.
the difference in PE effort between the top-quality
quantiles and the overall quality is 0.95 on average.
We would like to emphasize here that the DeltaAvg
metric does not have any a-priori range for its values.
The upperbound, for instance, is test-dependent, and
therefore an ?Oracle Effort? score is useful for un-
derstanding the performance level of real system-
submissions. The ?Oracle HTER? DeltaAvg score
of 0.77 is a more realistic upperbound for the cur-
rent set. Since the HTER metric is considered a
good approximation for the effort required in post-
editing, ranking the test set based on the HTER
scores (from lowest HTER to highest HTER) pro-
vides a good oracle comparison point. The oracle
based on (H)BLEU gives a lower DeltaAvg score,
which can be interpreted to mean that the BLEU
metric provides a lower correlation to post-editing
effort compared to HTER. We also note here that
there is room for improvement between the highest-
scoring submission (at DeltaAvg 0.63) and the ?Ora-
cle HTER? DeltaAvg score of 0.77. We are not sure
if this difference can be bridged completely, but hav-
ing measured a quantitative difference between the
current best-performance and a realistic upperbound
is an important achievement of this shared-task.
Scoring subtask
The results for the scoring task are presented in
Table 13, sorted from best to worse by using the
MAE metric scores (Equation 16) as primary key
and the RMSE metric scores (Equation 17) as sec-
ondary key.
The winning submission is SDLLW?s
M5PbestDeltaAvg, with an MAE of 0.61 and
an RMSE of 0.75 (the difference with respect to
all the other submissions is statistically significant
at p = 0.05, using pairwise bootstrap resam-
pling (Koehn, 2004)). The strong, state-of-the-art
quality-estimation baseline system is measured to
have an MAE of 0.69 and RMSE of 0.82, with six
other submissions having performances that are
not different from the baseline at a statistically-
significant level (p = 0.05), as shown by the gray
area in the middle of Table 13). Five submissions
scored higher than the baseline system at p = 0.05
(systems above the middle gray area), which
indicates that this shared-task also succeeded in
pushing the state-of-the-art performance to new
32
System ID DeltaAvg Spearman Corr
? SDLLW M5PbestDeltaAvg 0.63 0.64
? SDLLW SVM 0.61 0.60
UU bltk 0.58 0.61
UU best 0.56 0.62
TCD M5P-resources-only* 0.56 0.56
Baseline (17FFs SVM) 0.55 0.58
PRHLT-UPV 0.55 0.55
UEdin 0.54 0.58
SJTU 0.53 0.53
WLV-SHEF FS 0.51 0.52
WLV-SHEF BL 0.50 0.49
DFKI morphPOSibm1LM 0.46 0.46
DCU-SYMC unconstrained 0.44 0.41
DCU-SYMC constrained 0.43 0.41
TCD M5P-all* 0.42 0.41
UPC 1 0.22 0.26
UPC 2 0.15 0.19
Oracle Effort 0.95 1.00
Oracle HTER 0.77 0.70
Oracle (H)BLEU 0.71 0.62
Table 12: Official results for the ranking subtask of the WMT12 Quality Evaluation shared-task. The winning submis-
sions are indicated by a ? (the difference with respect to other systems is statistically significant with p = 0.05). The
systems in the gray area are not significantly different from the baseline system. Entries with * represent submissions
for which a bug-fix was applied after the submission deadline.
33
System ID MAE RMSE
? SDLLW M5PbestDeltaAvg 0.61 0.75
UU best 0.64 0.79
SDLLW SVM 0.64 0.78
UU bltk 0.64 0.79
Loria SVMlinear 0.68 0.82
UEdin 0.68 0.82
TCD M5P-resources-only* 0.68 0.82
Baseline (17FFs SVM) 0.69 0.82
Loria SVMrbf 0.69 0.83
SJTU 0.69 0.83
WLV-SHEF FS 0.69 0.85
PRHLT-UPV 0.70 0.85
WLV-SHEF BL 0.72 0.86
DCU-SYMC unconstrained 0.75 0.97
DFKI grcfs-mars 0.82 0.98
DFKI cfs-plsreg 0.82 0.99
UPC 1 0.84 1.01
DCU-SYMC constrained 0.86 1.12
UPC 2 0.87 1.04
TCD M5P-all 2.09 2.32
Oracle Effort 0.00 0.00
Oracle HTER (linear mapping into [1.5-5.0]) 0.56 0.73
Oracle (H)BLEU (linear mapping into [1.5-5.0]) 0.61 0.84
Table 13: Official results for the scoring subtask of the WMT12 Quality Evaluation shared-task. The winning submis-
sion is indicated by a ? (the difference with respect to the other submissions is statistically significant at p = 0.05).
The systems in the gray area are not different from the baseline system at a statistically significant level (p = 0.05).
Entries with * represent submissions for which a bug-fix was applied after the submission deadline.
34
levels in terms of absolute scoring. The range of
performance for the submissions in the scoring task
varies from an MAE of 0.61 up to an MAE of 0.87
(the outlier MAE of 2.09 is reportedly due to bugs).
We also calculate scoring Oracles using the meth-
ods used for the ranking Oracles. The difference is
that the HTER and (H)BLEU oracles need a way
of mapping their scores (which are usually in the
[0, 100] range) into the [1, 5] range. For the compar-
ison here, we did the mapping by excluding the 5%
top and bottom outlier scores, and then linearly map-
ping the remaining range into the [1.5, 5] range. The
?Oracle Effort? scores are not very indicative in this
case. However, the ?Oracle HTER? MAE score of
0.56 is a somewhat realistic lowerbound for the cur-
rent set (although the score could be decreased by a
smarter mapping from the HTER range to the Effort
range). We argue that since the HTER metric is con-
sidered a good approximation for the effort required
in post-editing, effort-like scores derived from the
HTER score provide a good way to compute oracle
scores in a deterministic manner. Note that again
the oracle based on (H)BLEU gives a worse MAE
score at 0.61, which support the interpretation that
the (H)BLEU metric provides a lower correlation
to post-editing effort compared to (H)TER. Over-
all, we consider the MAE values for these HTER
and (H)BLEU-based oracles to indicate high error
margins. Most notably the performance of the best
system gets the same MAE score as the (H)BLEU
oracle, at 0.61 MAE. We take this to mean that the
scoring task is more difficult compared to the rank-
ing task, since even oracle-based solutions get high
error scores.
6.5 Discussion
When looking back at the goals that we identified for
this shared-task, most of them have been success-
fully accomplished. In addition, we have achieved
additional ones that were not explicitly stated from
the beginning. In this section, we discuss the accom-
plishments of this shared-task in more detail, start-
ing from the defined goals and beyond.
Identify new and effective quality indicators
The vast majority of the participating systems use
external resources in addition to those provided for
the task, such as parsers, part-of-speech taggers,
named entity recognizers, etc. This has resulted in
a wide variety of features being used. Many of the
novel features have tried to exploit linguistically-
oriented features. While some systems did not
achieve improvements over the baseline while ex-
ploiting such features, others have (the ?UU? sub-
missions, for instance, exploiting both constituency
and dependency trees).
Another significant set of features that has been
previously overlooked is the feature set of the MT
decoder. Considering statistical engines, these fea-
tures are immediately available for quality predic-
tion from the internal trace of the MT decoder (in
a glass-box prediction scenario), and its contribu-
tion is significant. These features, which reflect the
?confidence? of the SMT system on the translations
it produces, have been shown to be complemen-
tary to other, system-independent (black-box) fea-
tures. For example, the ?SDLLW? submissions in-
corporate these features, and their feature selection
strategy consistently favored this feature set. The
power of this set of features alone is enough to yield
(when used with an M5P model) outputs that would
have been placed 4th in the ranking task and 5th
in the scoring task, a remarkable achievement. An-
other interesting feature used by the ?SDLLW? sub-
missions rely on pseudo-references, i.e., translations
produced by other MT systems for the same input
sentence.
Identify alternative machine learning techniques
Although SVM regression was used to compute the
baseline performance, the baseline ?system? pro-
vided for the task consisted solely of a software to
extract features, as opposed to a model built us-
ing the regression algorithm. The rationale behind
this decision was to encourage participants to exper-
iment with alternative methods for combining differ-
ent quality indicators. This was achieved to a large
extent.
The best-performing machine learning techniques
were found to be the M5P Regression Trees and the
SVM Regression (SVR) models. The merit of the
M5P Regression Trees is that it provides compact
models that are less prone to overfitting. In contrast,
the SVR models can easily overfit given the small
amount of training data available and the large num-
bers of features commonly used. Indeed, many of
35
the submissions that fell below the baseline perfor-
mance can blame overfitting for (part of) their sub-
optimal performance. However, SVR models can
achieve high performance through the use of tun-
ing and feature selection techniques to avoid overfit-
ting. Structured learning techniques were success-
fully used by the ?UU? submissions ? the second
best performing team ? to represent parse trees. This
seems an interesting direction to encode other sorts
of linguistic information about source and trans-
lation texts. Other interesting learning techniques
have been tried, such as Neural Networks, Par-
tial Least Squares Regression, or multivariate adap-
tive regression splines, but their performance does
not suggest they are strong candidates for learning
highly-performing quality-estimation models.
Test the suitability of evaluation metrics for qual-
ity estimation DeltaAvg, our proposed metric for
measuring ranking performance, proved suitable for
scoring the ranking subtask. Its high correlation with
the Spearman ranking metric, coupled with its ex-
trinsic interpretability, makes it a preferred choice
for future measurements. It is also versatile, in the
sense that the its valuation function V can change to
reflect different extrinsic measures of quality.
Establish the state of the art performance The
results on both the ranking and the scoring subtasks
established new state of the art levels on the test set
used in this shared task. In addition to these lev-
els, the oracle performance numbers also help under-
stand the current performance level, and how much
of a gap in performance there still exists. Addi-
tional data points regarding quality estimation per-
formance are needed to establish how stable this
measure of the performance gap is.
Contrast the performance of regression and
ranking techniques Most of the submissions in
the ranking task used the results provided by a re-
gression solution (submitted for the scoring task) to
infer the rankings. Also, optimizing for ranking per-
formance via a regression solution seems to result in
regression models that perform very well, as in the
case of the top-ranked submission.
6.6 Quality Estimation Conclusions
There appear to be significant differences between
considering the quality estimation task as a ranking
problem versus a scoring problem. The ranking-
based approach appears to be somewhat simpler
and more easily amenable to automatic solutions,
and at the same time provides immediate benefits
when integrated into larger applications (see, for in-
stance, the post-editing application described in Spe-
cia (2011)). The scoring-based approach is more dif-
ficult, as the high error rate even of oracle-based so-
lutions indicates. It is also well-known from human
evaluations of MT outputs that human judges also
have a difficult time agreeing on absolute-number
judgements to translations.
Our experience in creating the current datasets
confirms that, even with highly-trained profession-
als, it is difficult to arrive at consistent judge-
ments. We plan to have future investigations on
how to achieve more consistent ways of generating
absolute-number scores that reflect the quality of au-
tomated translations.
7 Summary
As in previous incarnations of this workshop we car-
ried out an extensive manual and automatic evalu-
ation of machine translation performance, and we
used the human judgements that we collected to val-
idate automatic metrics of translation quality. This
year was also the debut of a new quality estimation
task, which tries to predict the effort involved in hav-
ing post editors correct MT output. The quality es-
timation task differs from the metrics task in that it
does not involve reference translations.
As in previous years, all data sets generated by
this workshop, including the human judgments, sys-
tem translations and automatic scores, are publicly
available for other researchers to analyze.12
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-0022,
12http://statmt.org/wmt12/results.html
36
the US National Science Foundation under grant
IIS-0713448, and the CoSyne project FP7-ICT-4-
248531 funded by the European Commission. The
views and findings are the authors? alone. Thanks
for Adam Lopez for discussions about alternative
ways of ranking the overall system scores. The
Quality Estimation shared task organizers thank
Wilker Aziz for his help with the SMT models and
resources, and Mariano Felice for his help with the
system for the extraction of baseline features.
References
Eleftherios Avramidis. 2012. Quality estimation for
machine translation output using linguistic analysis
and decoding features. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A grain of salt for the wmt man-
ual evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 1?11, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ondrej Bojar, Bushra Jawaid, and Amir Kamran. 2012.
Probes in a taxonomy of factored phrase-based mod-
els. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Christian Buck. 2012. Black box features for the WMT
2012 quality estimation shared task. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion (WMT07), Prague, Czech Republic.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation (WMT09), Athens, Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT10), Uppsala, Swe-
den.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2009), Singapore.
Julio Castillo and Paula Estrella. 2012. Semantic tex-
tual similarity for MT evaluation. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27. Software available at http://www.
csie.ntu.edu.tw/?cjlin/libsvm.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving amber, an MT evaluation metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measur-
ment, 20(1):37?46.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic metric for reliable optimization and evalu-
ation of machine translation systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The CMU-avenue French-English translation
system. In Proceedings of the Seventh Workshop on
Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Ondr?ej Dus?ek, Zdene?k Z?abokrtsky?, Martin Popel, Mar-
tin Majlis?, Michal Nova?k, and David Marec?ek. 2012.
Formemes in English-Czech deep syntactic mt. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Mariano Felice and Lucia Specia. 2012. Linguistic fea-
tures for quality estimation. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
37
Mark Fishel, Rico Sennrich, Maja Popovic?, and Ondr?ej
Bojar. 2012. TerrorCat: a translation error
categorization-based MT quality metric. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Lluis Formiga, Carlos A. Henr??quez Q., Adolfo
Herna?ndez, Jose? B. Marin?o, Enric Monte, and Jose?
A. R. Fonollosa. 2012. The TALP-UPC phrase-based
translation systems for WMT12: Morphology simpli-
fication and domain adaptation. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
and Chris Callison-Burch. 2012. Joshua 4.0: Packing,
PRO, and paraphrases. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Ulrich Germann. 2012. Syntax-aware phrase-based sta-
tistical machine translation: System description. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Jesu?s Gonza?lez-Rubio, Alberto Sanch??s, and Francisco
Casacuberta. 2012. PRHLT submission to the
WMT12 quality estimation task. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
Francisco Guzman, Preslav Nakov, Ahmed Thabet, and
Stephan Vogel. 2012. QCRI at WMT12: Exper-
iments in Spanish-English and German-English ma-
chine translation of news text. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiedemann.
2012. Tree kernels for machine translation quality
estimation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Christian Hardmeier. 2011. Improving machine transla-
tion quality prediction with syntactic tree kernels. In
Proceedings of the 15th conference of the European
Association for Machine Translation, pages 233?240,
Leuven, Belgium.
Matthias Huck, Stephan Peitz, Markus Freitag, Malte
Nuhn, and Hermann Ney. 2012. The RWTH aachen
machine translation system for WMT 2012. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Sabine Hunsicker, Chen Yu, and Christian Federmann.
2012. Machine learning for hybrid machine transla-
tion. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards effec-
tive use of training data in statistical machine transla-
tion. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation, New
York, New York.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the Empirical Methods in Natural Language Process-
ing Conference.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
David Langlois, Sylvain Raybaud, and Kamel Sma??li.
2012. LORIA system for the WMT12 quality esti-
mation shared task. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,
Marianna Apidianaki, Li Gong, Aure?lien Max, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois Yvon.
2012. LIMSI @ WMT12. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Vero?nica Lo?pez-Luden?a, Rube?n San-Segundo, and
Juan M. Montero. 2012. UPM system for WMT 2012.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Adam Lopez. 2012. Putting human assessments of ma-
chine translation systems in order. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Matous? Macha?c?ek and Ondej Bojar. 2011. Approxi-
mating a deep-syntactic metric for mt evaluation and
tuning. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 373?379, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Freitag Markus, Peitz Stephan, Huck Matthias, Ney Her-
mann, Niehues Jan, Herrmann Teresa, Waibel Alex,
38
Hai-son Le, Lavergne Thomas, Allauzen Alexandre,
Buschbeck Bianka, Crego Joseph Maria, and Senel-
lart Jean. 2012. Joint WMT 2012 submission of
the QUAERO project. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Alexander Molchanov. 2012. PROMT deephybrid sys-
tem for WMT12 shared translation task. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Erwan Moreau and Carl Vogel. 2012. Quality estima-
tion: an experimental study using unsupervised simi-
larity measures. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, Montreal,
Canada, June. Association for Computational Linguis-
tics.
Jan Niehues, Yuqi Zhang, Mohammed Mediani, Teresa
Herrmann, Eunah Cho, and Alex Waibel. 2012. The
karlsruhe institute of technology translation systems
for the WMT 2012. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Daniele Pighin, Meritxell Gonza?lez, and Llu??s Ma`rquez.
2012. The upc submission to the WMT 2012 shared
task on quality estimation. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Sylvain Raybaud, David Langlois, and Kamel Sma??li.
2011. ?This sentence is wrong.? Detecting errors in
machine-translated sentences. Machine Translation,
25(1):1?34.
Majid Razmara, Baskaran Sankaran, Ann Clifton, and
Anoop Sarkar. 2012. Kriya - the SFU system for
translation task at WMT-12. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek. 2012.
DEPFIX: A system for automatic correction of czech
MT outputs. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. DCU-Symantec submission for the
WMT 2012 quality estimation task. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Christophe Servan, Patrik Lambert, Anthony Rousseau,
Holger Schwenk, and Lo??c Barrault. 2012. LIUM?s
smt machine translation systems for WMT 2012. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence level MT eval-
uation. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver systems in the WMT12
quality estimation shared task. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the Sentence-Level Quality of Machine Transla-
tion Systems. In Proceedings of the 13th Conference
of the European Association for Machine Translation,
pages 28?37, Barcelona.
Lucia Specia. 2011. Exploiting Objective Annotations
for Measuring Translation Post-editing Effort. In Pro-
ceedings of the 15th Conference of the European As-
sociation for Machine Translation, pages 73?80, Leu-
ven.
Ales? Tamchyna, Petra Galus?c?a?kova?, Amir Kamran, Milos?
Stanojevic?, and Ondr?ej Bojar. 2012. Selecting data for
English-to-Czech machine translation. In Proceedings
of the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
David Vilar. 2012. DFKI?s smt system for WMT 2012.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Mengqiu Wang and Christopher Manning. 2012.
SPEDE: Probabilistic edit distance metrics for MT
evaluation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Eric Wehrli, Luka Nerima, and Yves Scherrer. 2009.
Deep linguistic multilingual translation and bilingual
dictionaries. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 90?94.
Philip Williams and Philipp Koehn. 2012. GHKM rule
extraction and scope-3 parsing in Moses. In Proceed-
ings of the Seventh Workshop on Statistical Machine
39
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Chunyang Wu and Hai Zhao. 2012. Regression with
phrase indicators for estimating MT quality. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Daniel Zeman. 2012. Data issues of the multilingual
translation matrix. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
40
C
U
-B
O
JA
R
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
U
E
D
IN
U
K
CU-BOJAR ? .29? .43 .53? .47? .31?
JHU .59? ? .59? .67? .65? .44?
ONLINE-A .44 .28? ? .52? .46? .32?
ONLINE-B .36? .23? .34? ? .38? .25?
UEDIN .36? .23? .36? .48? ? .27?
UK .56? .33? .56? .63? .60? ?
> others 0.53 0.32 0.53 0.65 0.60 0.37
Table 14: Head to head comparison for Czech-English systems
A Pairwise System Comparisons by Human Judges
Tables 14?21 show pairwise comparisons between systems for each language pair. The numbers in each of
the tables? cells indicate the percentage of times that the system in that column was judged to be better than
the system in that row. Bolding indicates the winner of the two systems. The difference between 100 and
the sum of the complementary cells is the percent of time that the two systems were judged to be equal.
Because there were so many systems and data conditions the significance of each pairwise comparison
needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differences
(rather than differences that are attributable to chance). In the following tables ? indicates statistical signif-
icance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical significance at
p ? 0.01, according to the Sign Test.
Each table contains a final row showing how often a system was ranked to be > than the others. As
suggested by Bojar et al (2011) present, this is calculated ignoring ties as:
score(s) =
win(s)
win(s) + loss(s)
(18)
B Automatic Scores
Tables 29?36 give the automatic scores for each of the systems.
41
C
O
M
M
E
R
C
IA
L
2
C
U
-B
O
JA
R
C
U
-D
E
P
F
IX
C
U
-P
O
O
R
-C
O
M
B
C
U
-T
A
M
C
H
C
U
-T
E
C
T
O
M
T
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
C
O
M
M
E
R
C
IA
L
1
S
F
U
U
E
D
IN
U
K
COMMERCIAL2 ? .48? .56? .43 .49? .50? .32? .49? .54? .36 .38? .50? .42
CU-BOJAR .33? ? .49? .29? .26 .39 .26? .40 .51? .37? .27? .43 .33?
CU-DEPFIX .28? .36? ? .26? .30? .32? .18? .31? .13? .33? .21? .31? .25?
CU-POOR-COMB .42 .40? .59? ? .41? .51? .34? .49? .57? .45 .33? .47? .42
CU-TAMCH .38? .24 .51? .27? ? .39 .22? .42 .47? .38? .28? .39 .28?
CU-TECTOMT .32? .42 .49? .33? .47 ? .24? .42 .46? .36? .33? .46 .40
JHU .54? .59? .69? .50? .62? .60? ? .59? .61? .52? .44 .62? .48?
ONLINE-A .36? .41 .51? .36? .43 .43 .24? ? .51? .40 .26? .45 .32?
ONLINE-B .32? .34? .24? .28? .35? .35? .22? .33? ? .31? .23? .33? .22?
COMMERCIAL1 .41 .48? .55? .41 .50? .49? .36? .46 .54? ? .30? .48 .41
SFU .47? .56? .64? .47? .55? .52? .36 .53? .64? .56? ? .58? .48?
UEDIN .36? .36 .50? .29? .38 .43 .24? .37 .48? .40 .25? ? .30?
UK .43 .47? .59? .43 .52? .44 .26? .50? .59? .47 .35? .52? ?
> others 0.46 0.54 0.66 0.44 0.56 0.53 0.32 0.53 0.63 0.48 0.36 0.56 0.44
Table 15: Head to head comparison for English-Czech systems
IT
S
-L
A
T
L
JH
U
K
IT
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
P
R
O
M
T
R
W
T
H
U
E
D
IN
U
K
ITS-LATL ? .49? .54? .55? .53? .59? .58? .38 .47? .32 .45? .47? .62? .53? .48
JHU .35? ? .47? .55? .42 .45 .55? .36 .49? .37 .46 .46? .47? .46? .29
KIT .25? .25? ? .37 .29? .28? .39 .27? .35 .30? .32? .33 .36 .24? .22?
LIMSI .23? .23? .34 ? .26 .21? .29? .25? .29? .19? .19? .32? .22? .29 .16?
LIUM .25? .36 .42? .34 ? .27? .46? .21? .40 .25? .37 .34 .35 .29 .30?
ONLINE-A .22? .33 .40? .45? .42? ? .44? .26? .43 .33? .38 .33 .47? .35 .30?
ONLINE-B .20? .22? .33 .43? .32? .29? ? .27? .36 .26? .33 .34 .39 .29? .24?
RBMT-4 .37 .47 .56? .60? .60? .55? .52? ? .41 .36 .39 .40 .58? .51? .42
RBMT-3 .30? .35? .43 .45? .40 .39 .37 .34 ? .27? .29 .23 .55? .42 .34?
ONLINE-C .36 .46 .46? .55? .49? .50? .58? .38 .48? ? .45? .43 .62? .45 .39
RBMT-1 .28? .36 .49? .58? .40 .42 .44 .35 .38 .31? ? .41 .45 .37 .30?
PROMT .20? .34? .41 .50? .46 .40 .40 .34 .22 .33 .32 ? .48? .41 .27?
RWTH .22? .28? .34 .37? .31 .28? .32 .27? .26? .22? .34 .31? ? .29 .17?
UEDIN .28? .29? .40? .39 .34 .35 .42? .31? .39 .34 .36 .34 .34 ? .27?
UK .37 .36 .53? .53? .44? .43? .48? .38 .52? .39 .44? .46? .52? .46? ?
> others 0.36 0.44 0.59 0.66 0.55 0.51 0.6 0.39 0.52 0.39 0.48 0.51 0.62 0.53 0.4
Table 16: Head to head comparison for English-French systems
42
D
F
K
I-
B
E
R
L
IN
D
F
K
I-
H
U
N
S
IC
K
E
R
JH
U
K
IT
L
IM
S
I
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
R
W
T
H
U
E
D
IN
-W
IL
L
IA
M
S
U
E
D
IN
U
K
DFKI-BERLIN ? .62? .58? .64? .71? .68? .80? .68? .71? .58? .65? .62? .64? .61? .60?
DFKI-HUNSICKER .28? ? .42 .48 .51? .47 .52? .49? .57? .38 .53? .39 .39 .41 .39
JHU .24? .45 ? .43? .43 .47? .62? .56? .60? .46 .47? .46? .47? .39 .42
KIT .22? .41 .27? ? .39 .45 .60? .54? .58? .37 .47 .33 .43 .39 .26?
LIMSI .15? .37? .34 .36 ? .47 .49? .43 .43 .35 .48 .36 .37 .32 .31?
ONLINE-A .20? .37 .35? .41 .39 ? .45? .42 .51? .38 .49 .42 .40 .37 .36?
ONLINE-B .15? .35? .26? .27? .35? .30? ? .45 .35? .29? .41 .30? .34? .30? .18?
RBMT-4 .25? .22? .31? .31? .45 .45 .42 ? .41 .38 .40 .44 .35? .36? .36?
RBMT-3 .18? .27? .24? .28? .38 .36? .49? .41 ? .33? .26? .29? .28? .31? .34?
ONLINE-C .27? .47 .35 .49 .46 .44 .63? .48 .55? ? .49? .40 .43 .43 .46
RBMT-1 .19? .30? .33? .41 .41 .39 .45 .45 .50? .32? ? .34? .40 .39 .39
RWTH .20? .43 .30? .45 .45 .44 .58? .50 .58? .43 .53? ? .41 .40 .41
UEDIN-WILLIAMS .20? .46 .30? .36 .36 .45 .54? .52? .54? .41 .46 .38 ? .32 .30?
UEDIN .20? .45 .40 .38 .43 .48 .56? .56? .53? .47 .48 .29 .39 ? .35
UK .25? .49 .40 .45? .51? .49? .64? .51? .52? .44 .47 .34 .48? .40 ?
> others 0.25 0.48 0.43 0.50 0.55 0.54 0.64 0.58 0.63 0.47 0.56 0.47 0.51 0.47 0.45
Table 17: Head to head comparison for English-German systems
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
P
R
O
M
T
U
E
D
IN
U
K
U
P
C
JHU ? .52? .59? .50? .58? .48? .49? .56? .48? .44? .52?
ONLINE-A .27? ? .45 .34? .44 .31? .31? .44 .37 .28? .37
ONLINE-B .21? .37 ? .28? .35? .25? .28? .31? .30? .23? .31?
RBMT-4 .35? .52? .56? ? .49? .39 .40 .46? .45 .38? .45
RBMT-3 .26? .39 .46? .34? ? .32? .28? .24 .34? .32? .37
ONLINE-C .33? .54? .61? .40 .47? ? .43 .50? .50? .42 .48
RBMT-1 .39? .51? .61? .39 .49? .34 ? .47? .50? .39 .46
PROMT .28? .41 .51? .33? .29 .33? .34? ? .42 .32? .40
UEDIN .25? .41 .48? .38 .47? .30? .35? .43 ? .28? .39
UK .31? .52? .57? .48? .53? .42 .44 .52? .42? ? .50?
UPC .24? .40 .53? .40 .43 .39 .39 .46 .36 .28? ?
> others 0.36 0.56 0.65 0.46 0.58 0.43 0.45 0.55 0.52 0.41 0.52
Table 18: Head to head comparison for English-Spanish systems
43
C
M
U
JH
U
K
IT
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
R
W
T
H
S
F
U
U
E
D
IN
U
K
CMU ? .34? .32 .46 .35 .41 .39 .30? .36 .29? .35? .32 .28? .45 .33?
JHU .50? ? .63? .55? .53? .63? .57? .43 .42 .31? .46 .52? .43 .53? .43
KIT .40 .21? ? .36 .30 .35 .44 .33? .33? .23? .31? .25? .28? .23? .30?
LIMSI .35 .26? .37 ? .31? .35 .40 .29? .32? .23? .33? .29? .28? .29 .23?
LIUM .47 .25? .43 .53? ? .44 .42 .36 .43 .28? .38 .38 .32? .40 .42
ONLINE-A .45 .22? .41 .47 .40 ? .41 .30? .25? .28? .23? .40 .27? .40 .25?
ONLINE-B .45 .32? .38 .42 .41 .39 ? .34? .39 .30? .33? .30? .34? .44 .32?
RBMT-4 .56? .40 .54? .61? .48 .54? .54? ? .43 .31? .48? .45 .42 .52? .46
RBMT-3 .50 .46 .53? .53? .46 .54? .47 .33 ? .28? .40 .53? .52 .50 .48
ONLINE-C .59? .57? .72? .66? .59? .60? .61? .45? .54? ? .58? .65? .53? .66? .58?
RBMT-1 .54? .43 .58? .54? .48 .62? .55? .31? .44 .20? ? .47 .41 .56? .38
RWTH .39 .35? .50? .52? .43 .50 .55? .42 .37? .23? .40 ? .34? .36 .29?
SFU .57? .38 .55? .54? .48? .55? .51? .42 .38 .35? .45 .50? ? .41 .46
UEDIN .37 .32? .42? .42 .40 .43 .40 .34? .40 .24? .36? .39 .41 ? .29?
UK .50? .40 .48? .59? .44 .58? .50? .42 .41 .35? .49 .53? .35 .51? ?
> others 0.57 0.41 0.61 0.63 0.52 0.59 0.57 0.43 0.46 0.32 0.46 0.52 0.44 0.55 0.44
Table 19: Head to head comparison for French-English systems
D
F
K
I-
B
E
R
L
IN
JH
U
K
IT
L
IM
S
I
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
Q
C
R
I
Q
U
A
E
R
O
R
W
T
H
U
E
D
IN
U
G
U
K
DFKI-BERLIN ? .38 .49 .52? .57? .65? .55? .62? .50 .49 .51? .66? .53? .61? .17? .37
JHU .45 ? .60? .66? .66? .69? .57? .60? .52 .62? .58? .67? .59? .62? .21? .37
KIT .36 .16? ? .47 .60? .50 .41 .50 .31? .39 .32 .36 .32 .39 .15? .26?
LIMSI .30? .14? .35 ? .49? .57? .49 .54 .34? .33? .43 .31 .44 .49? .14? .30?
ONLINE-A .32? .20? .22? .32? ? .39 .30? .44 .20? .30? .37 .35? .32? .31? .16? .29?
ONLINE-B .25? .21? .38 .29? .38 ? .27? .39 .31? .37 .30? .43 .34 .33? .12? .24?
RBMT-4 .33? .33? .49 .44 .57? .63? ? .46 .26? .40 .53? .51? .56? .48 .21? .32?
RBMT-3 .26? .30? .39 .40 .45 .45 .32 ? .35 .36 .34? .48 .33? .41 .13? .23?
ONLINE-C .36 .37 .58? .54? .70? .62? .57? .50 ? .53? .48 .57? .55? .58? .14? .45
RBMT-1 .41 .32? .48 .55? .64? .52 .42 .47 .34? ? .51 .49 .48 .45 .15? .25?
QCRI .31? .26? .43 .37 .48 .51? .36? .52? .43 .38 ? .48? .48? .45? .14? .23?
QUAERO .18? .19? .29 .33 .51? .43 .33? .42 .31? .37 .23? ? .34 .48? .09? .16?
RWTH .29? .25? .38 .34 .51? .48 .37? .58? .38? .40 .29? .39 ? .44 .20? .24?
UEDIN .24? .20? .38 .30? .55? .52? .42 .44 .35? .37 .29? .32? .38 ? .08? .22?
UG .68? .61? .72? .76? .76? .82? .72? .80? .70? .76? .73? .76? .73? .84? ? .57?
UK .43 .37 .48? .48? .54? .62? .57? .64? .44 .59? .49? .58? .51? .56? .20? ?
> others 0.40 0.34 0.55 0.54 0.65 0.65 0.50 0.60 0.43 0.51 0.52 0.61 0.56 0.6 0.17 0.37
Table 20: Head to head comparison for German-English systems
44
G
T
H
-U
P
M
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
Q
C
R
I
U
E
D
IN
U
K
U
P
C
GTH-UPM ? .41 .50? .52? .38 .46 .32? .35? .44? .46 .17? .41
JHU .37 ? .54? .56? .44 .48 .39 .39 .47? .50? .15? .47?
ONLINE-A .34? .31? ? .43 .28? .38? .29? .29? .40 .39 .16? .41
ONLINE-B .36? .30? .44 ? .34? .38 .30? .32? .37? .38 .18? .41
RBMT-4 .50 .45 .61? .57? ? .46 .41 .40 .53? .57? .21? .56?
RBMT-3 .42 .40 .53? .51 .36 ? .36? .31? .60? .54? .14? .54?
ONLINE-C .54? .48 .58? .62? .49 .50? ? .40 .58? .59? .23? .55?
RBMT-1 .56? .50 .59? .57? .40 .53? .41 ? .57? .59? .23? .58?
QCRI .28? .31? .45 .50? .38? .32? .29? .34? ? .31 .12? .33?
UEDIN .39 .27? .49 .49 .33? .38? .31? .31? .34 ? .15? .38
UK .74? .71? .81? .76? .73? .76? .69? .66? .76? .75? ? .77?
UPC .42 .32? .49 .49 .38? .36? .33? .35? .44? .36 .14? ?
> others 0.52 0.48 0.62 0.61 0.46 0.51 0.42 0.42 0.60 0.58 0.19 0.57
Table 21: Head to head comparison for Spanish-English systems
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.643: ONLINE-B ONLINE-B ONLINE-B 2.88: ONLINE-B 0.642 (1): ONLINE-B
2 0.606: UEDIN UEDIN UEDIN 3.07: UEDIN 0.603 (2): UEDIN
3 0.530: ONLINE-A CU-BOJAR CU-BOJAR 3.40: CU-BOJAR 0.528 (3-4): ONLINE-A
4 0.530: CU-BOJAR ONLINE-A ONLINE-A 3.40: ONLINE-A 0.528 (3-4): CU-BOJAR
5 0.375: UK UK UK 4.01: UK 0.379 (5): UK
6 0.318: JHU JHU JHU 4.24: JHU 0.320 (6): JHU
Table 22: Overall ranking with different methods (Czech?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.646: ONLINE-A ONLINE-B ONLINE-B 6.35: ONLINE-A 0.647 (1-3): ONLINE-A
2 0.645: ONLINE-B ONLINE-A ONLINE-A 6.44: ONLINE-B 0.642 (1-3): ONLINE-B
3 0.612: QUAERO UEDIN UEDIN 6.94: QUAERO 0.609 (2-5): QUAERO
4 0.599: RBMT-3 QUAERO QUAERO 7.04: RBMT-3 0.600 (2-6): RBMT-3
5 0.597: UEDIN RBMT-3 RBMT-3 7.16: UEDIN 0.593 (3-6): UEDIN
6 0.558: RWTH KIT KIT 7.76: RWTH 0.551 (5-9): RWTH
7 0.545: LIMSI RWTH RWTH 7.83: KIT 0.547 (5-10): KIT
8 0.544: KIT QCRI QCRI 7.85: LIMSI 0.545 (6-10): LIMSI
9 0.524: QCRI RBMT-4 RBMT-4 8.20: QCRI 0.521 (7-11): QCRI
10 0.505: RBMT-1 LIMSI LIMSI 8.40: RBMT-4 0.506 (8-11): RBMT-1
11 0.502: RBMT-4 RBMT-1 RBMT-1 8.42: RBMT-1 0.506 (8-11): RBMT-4
12 0.434: ONLINE-C ONLINE-C ONLINE-C 9.43: ONLINE-C 0.434 (12-13): ONLINE-C
13 0.402: DFKI-BERLIN DFKI-BERLIN DFKI-BERLIN 9.86: DFKI-BERLIN 0.405 (12-14): DFKI-BERLIN
14 0.374: UK UK UK 10.25: UK 0.377 (13-15): UK
15 0.337: JHU JHU JHU 10.81: JHU 0.338 (14-15): JHU
16 0.179: UG UG UG 13.26: UG 0.180 (16): UG
Table 23: Overall ranking with different methods (German?English)
45
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.630: LIMSI LIMSI LIMSI 6.33: LIMSI 0.626 (1-3): LIMSI
2 0.613: KIT CMU CMU 6.55: KIT 0.610 (1-4): KIT
3 0.593: ONLINE-A ONLINE-B ONLINE-B 6.80: ONLINE-A 0.592 (1-5): ONLINE-A
4 0.573: CMU KIT KIT 7.06: CMU 0.571 (2-6): CMU
5 0.569: ONLINE-B ONLINE-A ONLINE-A 7.12: ONLINE-B 0.567 (3-7): ONLINE-B
6 0.546: UEDIN LIUM LIUM 7.51: UEDIN 0.538 (5-8): UEDIN
7 0.523: LIUM RWTH RWTH 7.73: LIUM 0.522 (5-8): LIUM
8 0.515: RWTH UEDIN UEDIN 7.88: RWTH 0.510 (6-9): RWTH
9 0.459: RBMT-1 RBMT-1 RBMT-1 8.51: RBMT-1 0.463 (8-12): RBMT-1
10 0.457: RBMT-3 UK UK 8.56: RBMT-3 0.458 (9-13): RBMT-3
11 0.444: UK SFU SFU 8.75: SFU 0.444 (9-14): SFU
12 0.444: SFU RBMT-3 RBMT-3 8.78: UK 0.441 (9-14): UK
13 0.429: RBMT-4 RBMT-4 RBMT-4 8.92: RBMT-4 0.430 (10-14): RBMT-4
14 0.412: JHU JHU JHU 9.19: JHU 0.409 (12-14): JHU
15 0.321: ONLINE-C ONLINE-C ONLINE-C 10.31: ONLINE-C 0.319 (15): ONLINE-C
Table 24: Overall ranking with different methods (French?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.617: ONLINE-A ONLINE-A ONLINE-A 5.38: ONLINE-A 0.617 (1-4): ONLINE-A
2 0.612: ONLINE-B ONLINE-B ONLINE-B 5.43: ONLINE-B 0.611 (1-4): ONLINE-B
3 0.603: QCRI QCRI QCRI 5.56: QCRI 0.600 (1-4): QCRI
4 0.585: UEDIN UPC UPC 5.75: UEDIN 0.581 (2-5): UEDIN
5 0.565: UPC UEDIN UEDIN 5.89: UPC 0.567 (3-6): UPC
6 0.528: GTH-UPM RBMT-3 RBMT-3 6.29: GTH-UPM 0.526 (5-7): GTH-UPM
7 0.512: RBMT-3 JHU JHU 6.37: RBMT-3 0.518 (6-8): RBMT-3
8 0.477: JHU GTH-UPM GTH-UPM 6.73: JHU 0.480 (7-9): JHU
9 0.461: RBMT-4 RBMT-4 RBMT-4 6.92: RBMT-4 0.460 (8-10): RBMT-4
10 0.423: RBMT-1 ONLINE-C ONLINE-C 7.19: RBMT-1 0.429 (9-11): RBMT-1
11 0.420: ONLINE-C RBMT-1 RBMT-1 7.24: ONLINE-C 0.423 (9-11): ONLINE-C
12 0.189: UK UK UK 9.25: UK 0.188 (12): UK
Table 25: Overall ranking with different methods (Spanish?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.662: CU-DEPFIX CU-DEPFIX CU-DEPFIX 5.25: CU-DEPFIX 0.660 (1): CU-DEPFIX
2 0.628: ONLINE-B ONLINE-B ONLINE-B 5.78: ONLINE-B 0.616 (2): ONLINE-B
3 0.557: UEDIN UEDIN UEDIN 6.42: UEDIN 0.557 (3-6): UEDIN
4 0.555: CU-TAMCH CU-TAMCH CU-TAMCH 6.45: CU-TAMCH 0.555 (3-6): CU-TAMCH
5 0.543: CU-BOJAR CU-BOJAR CU-BOJAR 6.58: CU-BOJAR 0.541 (3-7): CU-BOJAR
6 0.531: CU-TECTOMT CU-TECTOMT CU-TECTOMT 6.69: CU-TECTOMT 0.532 (4-7): CU-TECTOMT
7 0.528: ONLINE-A ONLINE-A ONLINE-A 6.72: ONLINE-A 0.529 (4-7): ONLINE-A
8 0.478: COMMERCIAL1 COMMERCIAL2 COMMERCIAL2 7.27: COMMERCIAL1 0.477 (8-10): COMMERCIAL1
9 0.459: COMMERCIAL2 COMMERCIAL1 COMMERCIAL1 7.46: COMMERCIAL2 0.459 (8-11): COMMERCIAL2
10 0.442: CU-POOR-COMB CU-POOR-COMB CU-POOR-COMB 7.61: CU-POOR-COMB 0.443 (9-11): CU-POOR-COMB
11 0.437: UK UK UK 7.65: UK 0.440 (9-11): UK
12 0.360: SFU SFU SFU 8.40: SFU 0.362 (12): SFU
13 0.326: JHU JHU JHU 8.72: JHU 0.328 (13): JHU
Table 26: Overall ranking with different methods (English?Czech)
46
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.655: LIMSI LIMSI LIMSI 5.98: LIMSI 0.651 (1-2): LIMSI
2 0.615: RWTH RWTH RWTH 6.57: RWTH 0.609 (2-4): RWTH
3 0.595: ONLINE-B ONLINE-B ONLINE-B 6.84: ONLINE-B 0.589 (2-5): ONLINE-B
4 0.590: KIT KIT KIT 6.86: KIT 0.587 (2-5): KIT
5 0.554: LIUM LIUM LIUM 7.36: LIUM 0.550 (4-8): LIUM
6 0.534: UEDIN UEDIN UEDIN 7.67: UEDIN 0.526 (5-9): UEDIN
7 0.516: RBMT-3 RBMT-3 RBMT-3 7.85: RBMT-3 0.514 (5-10): RBMT-3
8 0.513: ONLINE-A ONLINE-A ONLINE-A 7.92: PROMT 0.507 (6-10): ONLINE-A
9 0.506: PROMT PROMT PROMT 7.92: ONLINE-A 0.507 (6-10): PROMT
10 0.483: RBMT-1 RBMT-1 RBMT-1 8.23: RBMT-1 0.483 (8-11): RBMT-1
11 0.436: JHU JHU JHU 8.85: JHU 0.436 (10-12): JHU
12 0.396: UK UK RBMT-4 9.34: RBMT-4 0.397 (11-15): RBMT-4
13 0.394: ONLINE-C RBMT-4 ITS-LATL 9.38: ONLINE-C 0.393 (12-15): ONLINE-C
14 0.394: RBMT-4 ITS-LATL ONLINE-C 9.41: UK 0.391 (12-15): UK
15 0.360: ITS-LATL ONLINE-C UK 9.81: ITS-LATL 0.360 (13-15): ITS-LATL
Table 27: Overall ranking with different methods (English?French)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.648: ONLINE-B ONLINE-B ONLINE-B 4.70: ONLINE-B 0.646 (1): ONLINE-B
2 0.579: RBMT-3 RBMT-3 RBMT-3 5.35: RBMT-3 0.577 (2-4): RBMT-3
3 0.561: ONLINE-A PROMT PROMT 5.49: ONLINE-A 0.561 (2-5): ONLINE-A
4 0.545: PROMT ONLINE-A ONLINE-A 5.66: PROMT 0.542 (3-6): PROMT
5 0.526: UEDIN UPC UPC 5.78: UEDIN 0.528 (4-6): UEDIN
6 0.524: UPC UEDIN UEDIN 5.81: UPC 0.525 (4-6): UPC
7 0.463: RBMT-4 RBMT-1 RBMT-1 6.33: RBMT-4 0.464 (7-9): RBMT-4
8 0.452: RBMT-1 RBMT-4 RBMT-4 6.42: RBMT-1 0.452 (7-9): RBMT-1
9 0.430: ONLINE-C UK ONLINE-C 6.57: ONLINE-C 0.434 (8-10): ONLINE-C
10 0.412: UK ONLINE-C UK 6.73: UK 0.415 (9-10): UK
11 0.357: JHU JHU JHU 7.17: JHU 0.357 (11): JHU
Table 28: Overall ranking with different methods (English?Spanish)
47
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
A
G
A
N
-S
T
S
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
Czech-English News Task
CU-BOJAR 0.17 0.2 39 0.31 44 0.66 0.50 0.21 0.65 0.2 50 639
JHU 0.16 0.18 41 0.28 41 0.63 0.47 0.19 0.65 0.10 53 692
ONLINE-A 0.18 0.21 40 0.31 43 0.68 0.51 0.21 0.62 0.22 50 648
ONLINE-B 0.18 0.23 40 0.30 42 0.67 0.53 0.23 0.59 0.20 52 660
UEDIN 0.18 0.22 39 0.32 45 0.69 0.53 0.23 0.60 0.25 49 627
UK 0.16 0.18 41 0.29 41 0.63 0.49 0.19 0.67 0.17 53 682
Table 29: Automatic evaluation metric scores for systems in the WMT12 Czech-English News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
German-English News Task
DFKI-BERLIN 0.17 0.21 40 0.3 43 0.46 0.18 0.61 0.25 50 653
JHU 0.17 0.2 41 0.29 42 0.42 0.21 0.61 0.20 52 672
KIT 0.18 0.23 39 0.31 45 0.46 0.23 0.58 0.28 49 630
LIMSI 0.18 0.23 39 0.31 45 0.48 0.23 0.6 0.30 49 628
ONLINE-A 0.18 0.21 40 0.32 44 0.50 0.22 0.6 0.27 50 645
ONLINE-B 0.19 0.24 39 0.31 44 0.53 0.24 0.59 0.29 50 636
RBMT-4 0.16 0.16 41 0.29 42 0.44 0.18 0.68 0.24 53 690
RBMT-3 0.16 0.17 40 0.3 42 0.47 0.19 0.66 0.29 52 677
ONLINE-C 0.15 0.14 42 0.28 40 0.43 0.17 0.70 0.26 54 711
RBMT-1 0.15 0.15 43 0.29 40 0.45 0.17 0.69 0.24 54 711
QCRI 0.18 0.23 40 0.31 44 0.46 0.23 0.59 0.26 50 639
QUAERO 0.19 0.24 38 0.32 46 0.49 0.24 0.57 0.3 48 613
RWTH 0.18 0.23 39 0.31 45 0.48 0.24 0.58 0.27 49 626
UEDIN 0.18 0.23 39 0.31 46 0.51 0.23 0.59 0.32 49 630
UG 0.11 0.11 45 0.24 35 0.38 0.14 0.77 0.10 59 768
UK 0.16 0.18 42 0.29 40 0.42 0.2 0.65 0.27 53 683
Table 30: Automatic evaluation metric scores for systems in the WMT12 German-English News Task
48
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
French-English News Task
CMU 0.20 0.29 36 0.34 51 0.54 0.29 0.52 0.25 44 561
JHU 0.19 0.26 37 0.33 47 0.50 0.26 0.54 0.20 46 596
KIT 0.21 0.30 35 0.34 51 0.54 0.3 0.51 0.25 43 551
LIMSI 0.21 0.30 35 0.34 52 0.55 0.3 0.51 0.25 43 546
LIUM 0.20 0.29 36 0.34 50 0.54 0.29 0.52 0.24 44 558
ONLINE-A 0.2 0.27 37 0.34 48 0.52 0.27 0.53 0.24 45 584
ONLINE-B 0.20 0.30 36 0.33 48 0.55 0.29 0.51 0.22 46 582
RBMT-4 0.18 0.20 38 0.32 45 0.49 0.21 0.64 0.15 48 622
RBMT-3 0.18 0.21 39 0.31 46 0.49 0.22 0.61 0.15 48 637
ONLINE-C 0.18 0.19 38 0.31 45 0.45 0.21 0.64 0.10 48 633
RBMT-1 0.18 0.21 39 0.32 47 0.5 0.22 0.62 0.15 48 626
RWTH 0.20 0.29 36 0.34 50 0.53 0.28 0.53 0.20 44 563
SFU 0.2 0.25 37 0.33 48 0.51 0.26 0.54 0.17 46 596
UEDIN 0.20 0.30 35 0.34 51 0.54 0.3 0.51 0.25 43 549
UK 0.19 0.25 38 0.33 47 0.52 0.25 0.57 0.17 47 602
Table 31: Automatic evaluation metric scores for systems in the WMT12 French-English News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
A
G
A
N
-S
T
S
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
Spanish-English News Task
GTH-UPM 0.21 0.29 35 0.35 51 0.7 0.55 0.29 0.51 0.31 43 565
JHU 0.21 0.29 35 0.35 51 0.7 0.56 0.29 0.51 0.31 43 560
ONLINE-A 0.22 0.31 34 0.36 52 0.72 0.58 0.31 0.49 0.36 42 535
ONLINE-B 0.22 0.38 33 0.36 53 0.70 0.60 0.35 0.45 0.35 41 523
RBMT-4 0.19 0.23 36 0.33 49 0.69 0.54 0.24 0.60 0.29 45 591
RBMT-3 0.19 0.23 36 0.33 49 0.69 0.54 0.23 0.60 0.29 45 590
ONLINE-C 0.19 0.22 37 0.33 47 0.68 0.5 0.23 0.61 0.24 46 598
RBMT-1 0.18 0.22 38 0.33 48 0.67 0.52 0.23 0.62 0.23 47 607
QCRI 0.22 0.33 33 0.36 54 0.71 0.6 0.32 0.49 0.32 40 523
UEDIN 0.22 0.33 33 0.36 54 0.71 0.59 0.32 0.48 0.32 40 519
UK 0.18 0.22 37 0.30 44 0.6 0.48 0.23 0.60 0.10 48 634
UPC 0.22 0.32 34 0.36 54 0.71 0.57 0.31 0.49 0.33 41 531
Table 32: Automatic evaluation metric scores for systems in the WMT12 Spanish-English News Task
49
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-Czech News Task
COMMERCIAL-2 0.01 0.08 47 693 0.17 23 0.38 0.1 0.76 0.17 61
CU-BOJAR 0.17 0.13 45 644 0.21 28 0.4 0.13 0.69 0.26 57
CU-DEPFIX 0.19 0.16 44 623 0.22 28 0.45 0.15 0.66 0.30 55
CU-POOR-COMB 0.14 0.12 48 710 0.19 27 0.35 0.12 0.67 0.23 60
CU-TAMCH 0.17 0.13 45 647 0.21 28 0.38 0.13 0.69 0.29 57
CU-TECTOMT 0.16 0.12 48 690 0.19 26 0.36 0.12 0.68 0.22 60
JHU 0.16 0.1 47 691 0.2 23 0.39 0.11 0.69 0.10 60
ONLINE-A 0.17 0.13 n/a n/a 0.21 n/a 0.42 0.13 0.67 0.25 n/a
ONLINE-B 0.19 0.16 44 623 0.21 28 0.45 0.15 0.66 0.30 55
COMMERCIAL-1 0.11 0.09 48 692 0.18 22 0.38 0.10 0.74 0.21 61
SFU 0.15 0.11 47 674 0.19 23 0.39 0.11 0.71 0.21 60
UEDIN 0.18 0.15 45 639 0.21 27 0.41 0.14 0.66 0.40 56
UK 0.15 0.11 47 669 0.19 25 0.39 0.12 0.71 0.35 59
Table 33: Automatic evaluation metric scores for systems in the WMT12 English-Czech News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-German News Task
DFKI-BERLIN 0.18 0.14 46 628 0.35 41 0.13 0.69 0.10 57
DFKI-HUNSICKER 0.18 0.14 45 621 0.35 42 0.15 0.69 0.17 57
JHU 0.2 0.15 45 618 0.37 42 0.16 0.68 0.17 56
KIT 0.20 0.17 45 606 0.38 43 0.17 0.66 0.14 55
LIMSI 0.2 0.17 45 615 0.37 43 0.17 0.65 0.15 56
ONLINE-A 0.20 0.16 45 617 0.38 43 0.17 0.65 0.36 55
ONLINE-B 0.22 0.18 43 589 0.38 42 0.18 0.64 0.35 55
RBMT-4 0.18 0.14 45 623 0.35 42 0.15 0.69 0.35 57
RBMT-3 0.19 0.15 44 608 0.36 44 0.16 0.68 0.37 56
ONLINE-C 0.16 0.11 47 655 0.32 39 0.13 0.74 0.37 60
RBMT-1 0.17 0.13 47 643 0.34 42 0.15 0.70 0.36 58
RWTH 0.2 0.16 44 609 0.37 43 0.16 0.67 0.25 56
UEDIN-WILLIAMS 0.19 0.16 45 628 0.37 43 0.17 0.66 0.33 57
UEDIN 0.20 0.16 45 611 0.37 43 0.17 0.66 0.29 55
UK 0.18 0.14 46 632 0.36 40 0.15 0.71 0.27 58
Table 34: Automatic evaluation metric scores for systems in the WMT12 English-German News Task
50
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-French News Task
ITS-LATL 0.24 0.21 41 548 0.45 48 0.21 0.61 0.15 50
JHU 0.26 0.25 38 511 0.49 51 0.25 0.57 0.15 47
KIT 0.28 0.28 36 480 0.52 55 0.28 0.54 0.22 44
LIMSI 0.28 0.29 36 472 0.52 55 0.28 0.54 0.22 44
LIUM 0.28 0.28 37 480 0.51 54 0.28 0.55 0.20 45
ONLINE-A 0.26 0.25 39 512 0.5 52 0.26 0.57 0.17 47
ONLINE-B 0.24 0.21 36 473 0.48 45 0.26 0.77 0.10 49
RBMT-4 0.24 0.21 40 539 0.46 48 0.22 0.60 0.10 49
RBMT-3 0.26 0.24 39 511 0.48 52 0.24 0.58 0.14 47
ONLINE-C 0.23 0.2 41 550 0.45 50 0.21 0.62 0.10 50
RBMT-1 0.25 0.22 40 531 0.47 51 0.23 0.6 0.13 49
PROMT 0.26 0.24 38 502 0.49 52 0.25 0.58 0.18 46
RWTH 0.28 0.29 36 478 0.52 54 0.28 0.54 0.22 44
UEDIN 0.28 0.28 36 479 0.52 54 0.28 0.55 0.27 45
UK 0.25 0.23 39 523 0.48 51 0.24 0.6 0.17 48
Table 35: Automatic evaluation metric scores for systems in the WMT12 English-French News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-Spanish News Task
JHU 0.29 0.29 37 494 0.54 52 0.29 0.51 0.14 45
ONLINE-A 0.31 0.31 36 475 0.56 54 0.31 0.48 0.2 43
ONLINE-B 0.33 0.36 34 431 0.57 54 0.34 0.48 0.25 42
RBMT-4 0.27 0.24 39 528 0.5 50 0.25 0.55 0.14 48
RBMT-3 0.28 0.26 39 510 0.51 51 0.26 0.54 0.13 46
ONLINE-C 0.26 0.24 40 532 0.5 49 0.25 0.55 0.10 48
RBMT-1 0.26 0.23 40 534 0.50 49 0.25 0.57 0.13 49
PROMT 0.29 0.27 38 497 0.52 52 0.28 0.53 0.18 45
UEDIN 0.31 0.32 35 466 0.56 55 0.32 0.49 0.19 42
UK 0.29 0.28 38 510 0.54 51 0.28 0.52 0.17 46
UPC 0.31 0.32 36 476 0.56 54 0.31 0.49 0.19 43
Table 36: Automatic evaluation metric scores for systems in the WMT12 English-Spanish News Task
51
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 222?231,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Using Categorial Grammar to Label Translation Rules
Jonathan Weese and Chris Callison-Burch and Adam Lopez
Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
Adding syntactic labels to synchronous
context-free translation rules can improve
performance, but labeling with phrase struc-
ture constituents, as in GHKM (Galley et al,
2004), excludes potentially useful translation
rules. SAMT (Zollmann and Venugopal,
2006) introduces heuristics to create new
non-constituent labels, but these heuristics
introduce many complex labels and tend to
add rarely-applicable rules to the translation
grammar. We introduce a labeling scheme
based on categorial grammar, which allows
syntactic labeling of many rules with a mini-
mal, well-motivated label set. We show that
our labeling scheme performs comparably to
SAMT on an Urdu?English translation task,
yet the label set is an order of magnitude
smaller, and translation is twice as fast.
1 Introduction
The Hiero model of Chiang (2007) popularized
the usage of synchronous context-free grammars
(SCFGs) for machine translation. SCFGs model
translation as a process of isomorphic syntactic
derivation in the source and target language. But the
Hiero model is formally, not linguistically syntactic.
Its derivation trees use only a single non-terminal la-
bel X , carrying no linguistic information. Consider
Rule 1.
X ? ? maison ; house ? (1)
We can add syntactic information to the SCFG
rules by parsing the parallel training data and pro-
jecting parse tree labels onto the spans they yield and
their translations. For example, if house was parsed
as a noun, we could rewrite Rule 1 as
N ? ? maison ; house ?
But we quickly run into trouble: how should we
label a rule that translates pour l?e?tablissement de
into for the establishment of? There is no phrase
structure constituent that corresponds to this English
fragment. This raises a model design question: what
label do we assign to spans that are natural trans-
lations of each other, but have no natural labeling
under a syntactic parse? One possibility would be
to discard such translations from our model as im-
plausible. However, such non-compositional trans-
lations are important in translation (Fox, 2002), and
they have been repeatedly shown to improve trans-
lation performance (Koehn et al, 2003; DeNeefe et
al., 2007).
Syntax-Augmented Machine Translation (SAMT;
Zollmann and Venugopal, 2006) solves this prob-
lem with heuristics that create new labels from the
phrase structure parse: it labels for the establish-
ment of as IN+NP+IN to show that it is the con-
catenation of a noun phrase with a preposition on
either side. While descriptive, this label is unsatis-
fying as a concise description of linguistic function,
fitting uneasily alongside more natural labels in the
phrase structure formalism. SAMT introduces many
thousands of such labels, most of which are seen
very few times. While these heuristics are effective
(Zollmann et al, 2008), they inflate grammar size,
hamper effective parameter estimation due to feature
sparsity, and slow translation speed.
Our objective is to find a syntactic formalism that
222
enables us to label most translation rules without re-
lying on heuristics. Ideally, the label should be small
in order to improve feature estimation and reduce
translation time. Furthering an insight that informs
SAMT, we show that combinatory categorial gram-
mar (CCG) satisfies these requirements.
Under CCG, for the establishment of is labeled
with ((S\NP)\(S\NP))/NP. This seems complex, but
it describes exactly how the fragment should com-
bine with other English words to create a complete
sentence in a linguistically meaningful way. We
show that CCG is a viable formalism to add syntax
to SCFG-based translation.
? We introduce two models for labeling SCFG
rules. One uses labels from a 1-best CCG parse
tree of training data; the second uses the top la-
bels in each cell of a CCG parse chart.
? We show that using 1-best parses performs as
well as a syntactic model using phrase structure
derivations.
? We show that using chart cell labels per-
forms almost as well than SAMT, but the non-
terminal label set is an order of magnitude
smaller and translation is twice as fast.
2 Categorial grammar
Categorial grammar (CG) (Adjukiewicz, 1935; Bar-
Hillel et al, 1964) is a grammar formalism in
which words are assigned grammatical types, or cat-
egories. Once categories are assigned to each word
of a sentence, a small set of universal combinatory
rules uses them to derive a sentence-spanning syn-
tactic structure.
Categories may be either atomic, like N, VP, S,
and other familiar types, or they may be complex
function types. A function type looks like A/B and
takes an argument of type B and returns a type A.
The categories A and B may themselves be either
primitives or functions. A lexical item is assigned a
function category when it takes an argument ? for
example, a verb may be function that needs to be
combined with its subject and object, or an a adjec-
tive may be a function that takes the noun it modifies
as an argument.
Lexical item Category
and conj
cities NP
in (NP\NP)/NP
own (S\NP)/NP
properties NP
they NP
various NP/NP
villages NP
Table 1: An example lexicon, mapping words to cat-
egories.
We can combine two categories with function ap-
plication. Formally, we write
X/Y Y ? X (2)
to show that a function type may be combined with
its argument type to produce the result type. Back-
ward function application also exists, where the ar-
gument occurs to the left of the function.
Combinatory categorial grammar (CCG) is an ex-
tension of CG that includes more combinators (op-
erations that can combine categories). Steedman
and Baldridge (2011) give an excellent overview of
CCG.
As an example, suppose we want to analyze the
sentence ?They own properties in various cities and
villages? using the lexicon shown in Table 1. We as-
sign categories according to the lexicon, then com-
bine the categories using function application and
other combinators to get an analysis of S for the
complete sentence. Figure 1 shows the derivation.
As a practical matter, very efficient CCG parsers
are available (Clark and Curran, 2007). As shown
by Fowler and Penn (2010), in many cases CCG is
context-free, making it an ideal fit for our problem.
2.1 Labels for phrases
Consider the German?English phrase pair der gro?e
Mann ? the tall man. It is easily labeled as an NP
and included in the translation table. By contrast,
der gro?e? the tall, doesn?t typically correspond to
a complete subtree in a phrase structure parse. Yet
translating the tall is likely to be more useful than
translating the tall man, since it is more general?it
can be combined with any other noun translation.
223
They own properties in various cities and villages
NP (S\NP )/NP NP (NP\NP )/NP NP/NP NP conj NP
> <?>
NP NP\NP
<
NP
>
NP\NP
<
NP
>
S\NP
<
S
Figure 1: An example CCG derivation for the sentence ?They own properties in various cities and villages?
using the lexicon from Table 1. ? indicates a conjunction operation; > and < are forward and backward
function application, respectively.
Using CG-style labels with function types, we can
assign the type (for example) NP/N to the tall to
show that it can be combined with a noun on its right
to create a complete noun phrase.1 In general, CG
can produce linguistically meaningful labels of most
spans in a sentence simply as a matter of course.
2.2 Minimal, well-motivated label set
By allowing slashed categories with CG, we in-
crease the number of labels allowed. Despite the in-
crease in the number of labels, CG is advantageous
for two reasons:
1. Our labels are derived from CCG derivations,
so phrases with slashed labels represent well-
motivated, linguistically-informed derivations,
and the categories can be naturally combined.
2. The set of labels is small, relative to SAMT ?
it?s restricted to the labels seen in CCG parses
of the training data.
In short, using CG labels allows us to keep more
linguistically-informed syntactic rules without mak-
ing the set of syntactic labels too big.
3 Translation models
3.1 Extraction from parallel text
To extract SCFG rules, we start with a heuristic to
extract phrases from a word-aligned sentence pair
1We could assign NP/N to the determiner the and N/N to the
adjective tall, then combine those two categories using function
composition to get a category NP/N for the two words together.
For
most
people
,
P
o
u
r
l
a
m
a
j
o
r
i
t
?
d
e
s
g
e
n
s
,
Figure 2: A word-aligned sentence pair fragment,
with a box indicating a consistent phrase pair.
(Tillmann, 2003). Figure 2 shows a such a pair, with
a consistent phrase pair inside the box. A phrase
pair (f, e) is said to be consistent with the alignment
if none of the words of f are aligned outside the
phrase e, and vice versa ? that is, there are no align-
ment points directly above, below, or to the sides of
the box defined by f and e.
Given a consistent phrase pair, we can immedi-
ately extract the rule
X ? ?f, e? (3)
as we would in a phrase-based MT system. How-
ever, whenever we find a consistent phrase pair that
is a sub-phrase of another, we may extract a hierar-
chical rule by treating the inner phrase as a gap in
the larger phrase. For example, we may extract the
rule
X ? ? Pour X ; For X ? (4)
from Figure 3.
224
For
most
people
,
P
o
u
r
l
a
m
a
j
o
r
i
t
?
d
e
s
g
e
n
s
,
Figure 3: A consistent phrase pair with a sub-phrase
that is also consistent. We may extract a hierarchical
SCFG rule from this training example.
The focus of this paper is how to assign labels
to the left-hand non-terminal X and to the non-
terminal gaps on the right-hand side. We discuss five
models below, of which two are novel CG-based la-
beling schemes.
3.2 Baseline: Hiero
Hiero (Chiang, 2007) uses the simplest labeling pos-
sible: there is only one non-terminal symbol, X , for
all rules. Its advantage over phrase-based translation
in its ability to model phrases with gaps in them,
enabling phrases to reorder subphrases. However,
since there?s only one label, there?s no way to in-
clude syntactic information in its translation rules.
3.3 Phrase structure parse tree labeling
One first step for adding syntactic information is to
get syntactic labels from a phrase structure parse
tree. For each word-aligned sentence pair in our
training data, we also include a parse tree of the tar-
get side.
Then we can assign syntactic labels like this: for
each consistent phrase pair (representing either the
left-hand non-terminal or a gap in the right hand
side) we see if the target-language phrase is the exact
span of some subtree of the parse tree.
If a subtree exactly spans the phrase pair, we can
use the root label of that subtree to label the non-
terminal symbol. If there is no such subtree, we
throw away any rules derived from the phrase pair.
As an example, suppose the English side of the
phrase pair in Figure 3 is analyzed as
PP
IN
For
NP
JJ
most
NN
people
Then we can assign syntactic labels to Rule 4 to pro-
duce
PP ? ? Pour NP ; For NP ? (5)
The rules extracted by this scheme are very sim-
ilar to those produced by GHKM (Galley et al,
2004), in particular resulting in the ?composed
rules? of Galley et al (2006), though we use sim-
pler heuristics for handling of unaligned words and
scoring in order to bring the model in line with both
Hiero and SAMT baselines. Under this scheme we
throw away a lot of useful translation rules that don?t
translate exact syntactic constituents. For example,
we can?t label
X ? ? Pour la majorite? des ; For most ? (6)
because no single node exactly spans For most: the
PP node includes people, and the NP node doesn?t
include For.
We can alleviate this problem by changing the
way we get syntactic labels from parse trees.
3.4 SAMT
The Syntax-Augmented Machine Translation
(SAMT) model (Zollmann and Venugopal, 2006)
extracts more rules than the other syntactic model
by allowing different labels for the rules. In SAMT,
we try several different ways to get a label for a
span, stopping the first time we can assign a label:
? As in simple phrase structure labeling, if a sub-
tree of the parse tree exactly spans a phrase, we
assign that phrase the subtree?s root label.
? If a phrase can be covered by two adjacent sub-
trees with labels A and B, we assign their con-
catenation A+B.
? If a phrase spans part of a subtree labeled A that
could be completed with a subtree B to its right,
we assign A/B.
225
? If a phrase spans part of a subtree A but is miss-
ing a B to its left, we assign A\B.
? Finally, if a phrase spans three adjacent sub-
trees with labels A, B, and C, we assign
A+B+C.
Only if all of these assignments fail do we throw
away the potential translation rule.
Under SAMT, we can now label Rule 6. For is
spanned by an IN node, and most is spanned by a JJ
node, so we concatenate the two and label the rule
as
IN+JJ? ? Pour la majorite? des ; For most ? (7)
3.5 CCG 1-best derivation labeling
Our first CG model is similar to the first phrase struc-
ture parse tree model. We start with a word-aligned
sentence pair, but we parse the target sentence using
a CCG parser instead of a phrase structure parser.
When we extract a rule, we see if the consistent
phrase pair is exactly spanned by a category gener-
ated in the 1-best CCG derivation of the target sen-
tence. If there is such a category, we assign that cat-
egory label to the non-terminal. If not, we throw
away the rule.
To continue our extended example, suppose the
English side of Figure 3 was analyzed by a CCG
parser to produce
For most people
(S/S)/N N/N N
>
N
>
S/S
Then just as in the phrase structure model, we
project the syntactic labels down onto the extractable
rule yielding
S/S ? ? Pour N ; For N ? (8)
This does not take advantage of CCG?s ability to
label almost any fragment of language: the frag-
ments with labels in any particular sentence depend
on the order that categories were combined in the
sentence?s 1-best derivation. We can?t label Rule 6,
because no single category spanned For most in the
derivation. In the next model, we increase the num-
ber of spans we can label.
S/S
S/S N
(S/S)/N N/N N
For peoplemost
Figure 4: A portion of the parse chart for a sentence
starting with ?For most people . . . .? Note that the
gray chart cell is not included in the 1-best derivation
of this fragment in Section 3.5.
3.6 CCG parse chart labeling
For this model, we do not use the 1-best CCG deriva-
tion. Instead, when parsing the target sentence, for
each cell in the parse chart, we read the most likely
label according to the parsing model. This lets us as-
sign a label for almost any span of the sentence just
by reading the label from the parse chart.
For example, Figure 4 represents part of a CCG
parse chart for our example fragment of ?For most
people.? Each cell in the chart shows the most prob-
able label for its span. The white cells of the chart
are in fact present in the 1-best derivation, which
means we could extract Rule 8 just as in the previous
model.
But the 1-best derivation model cannot label Rule
6, and this model can. The shaded chart cell in Fig-
ure 4 holds the most likely category for the span For
most. So we assign that label to the X:
S/S ? ? Pour la majorite? des ; For most ? (9)
By including labels from cells that weren?t used
in the 1-best derivation, we can greatly increase the
number of rules we can label.
4 Comparison of resulting grammars
4.1 Effect of grammar size and label set on
parsing efficiency
There are sound theoretical reasons for reducing the
number of non-terminal labels in a grammar. Trans-
lation with a synchronous context-free grammar re-
quires first parsing with the source-language projec-
tion of the grammar, followed by intersection of the
226
target-language projection of the resulting grammar
with a language model. While there are many possi-
ble algorithms for these operations, they all depend
on the size of the grammar.
Consider for example the popular cube pruning
algorithm of Chiang (2007), which is a simple ex-
tension of CKY. It works by first constructing a set
of items of the form ?A, i, j?, where each item corre-
sponds to (possibly many) partial analyses by which
nonterminal A generates the sequence of words from
positions i through j of the source sentence. It then
produces an augmented set of items ?A, i, j, u, v?, in
which items of the first type are augmented with left
and right language model states u and v. In each
pass, the number of items is linear in the number of
nonterminal symbols of the grammar. This observa-
tion has motivated work in grammar transformations
that reduce the size of the nonterminal set, often re-
sulting in substantial gains in parsing or translation
speed (Song et al, 2008; DeNero et al, 2009; Xiao
et al, 2009).
More formally, the upper bound on parsing com-
plexity is always at least linear in the size of the
grammar constant G, where G is often loosely de-
fined as a grammar constant; Iglesias et al (2011)
give a nice analysis of the most common translation
algorithms and their dependence on G. Dunlop et
al. (2010) provide a more fine-grained analysis of G,
showing that for a variety of implementation choices
that it depends on either or both the number of rules
in the grammar and the number of nonterminals in
the grammar. Though these are worst-case analyses,
it should be clear that grammars with fewer rules or
nonterminals can generally be processed more effi-
ciently.
4.2 Number of rules and non-terminals
Table 2 shows the number of rules we can extract
under various labeling schemes. The rules were ex-
tracted from an Urdu?English parallel corpus with
202,019 translations, or almost 2 million words in
each language.
As we described before, moving from the phrase-
structure syntactic model to the extended SAMT
model vastly increases the number of translation
rules ? from about 7 million to 40 million rules.
But the increased rule coverage comes at a cost: the
non-terminal set has increased in size from 70 (the
Model Rules NTs
Hiero 4,171,473 1
Syntax 7,034,393 70
SAMT 40,744,439 18,368
CG derivations 8,042,683 505
CG parse chart 28,961,161 517
Table 2: Number of translation rules and non-
terminal labels in an Urdu?English grammar under
various models.
size of the set of Penn Treebank tags) to over 18,000.
Comparing the phrase structure syntax model to
the 1-best CCG derivation model, we see that the
number of extracted rules increases slightly, and the
grammar uses a set of about 500 non-terminal labels.
This does not seem like a good trade-off; since we
are extracting from the 1-best CCG derivation there
really aren?t many more rules we can label than with
a 1-best phrase structure derivation.
But when we move to the full CCG parse chart
model, we see a significant difference: when read-
ing labels off of the entire parse chart, instead of
the 1-best derivation, we don?t see a significant in-
crease in the non-terminal label set. That is, most
of the labels we see in parse charts of the train-
ing data already show up in the top derivations: the
complete chart doesn?t contain many new labels that
have never been seen before.
But by using the chart cells, we are able to as-
sign syntactic information to many more translation
rules: over 28 million rules, for a grammar about 34
the size of SAMT?s. The parse chart lets us extract
many more rules without significantly increasing the
size of the syntactic label set.
4.3 Sparseness of nonterminals
Examining the histograms in Figure 5 gives us a
different view of the non-terminal label sets in our
models. In each histogram, the horizontal axis mea-
sures label frequency in the corpus. The height of
each bar shows the number of non-terminals with
that frequency.
For the phrase structure syntax model, we see
there are maybe 20 labels out of 70 that show up
on rules less than 1000 times. All the other labels
show up on very many rules.
227
More sparse Less sparse
1 10 102 103 104 105 106
Label Frequency (logscale)
1
10
1
10
1
10
1
10
102
103
N
um
be
r
of
L
ab
el
s
(l
og
sc
al
e)
Phrase structure
CCG 1-best
CCG chart
SAMT
Figure 5: Histograms of label frequency for each model, illustrating the sparsity of each model.
Moving to SAMT, with its heuristically-defined
labels, shows a very different story. Not only does
the model have over 18,000 non-terminal labels, but
thousands of them show up on fewer than 10 rules
apiece. If we look at the rare label types, we see that
a lot of them are improbable three way concatena-
tions A+B+C.
The two CCG models have similar sparseness
profiles. We do see some rare labels occurring only
a few times in the grammars, but the number of
singleton labels is an order of magnitude smaller
than SAMT. Most of the CCG labels show up in
the long tail of very common occurrences. Interest-
ingly, when we move to extracting labels from parse
charts rather than derivations, the number of labels
increases only slightly. However, we also obtain a
great deal more evidence for each observed label,
making estimates more reliable.
5 Experiments
5.1 Data
We tested our models on an Urdu?English transla-
tion task, in which syntax-based systems have been
quite effective (Baker et al, 2009; Zollmann et al,
2008). The training corpus was the National Insti-
tute of Standards and Technology Open Machine
Translation 2009 Evaluation (NIST Open MT09).
According to the MT09 Constrained Training Con-
ditions Resources list2 this data includes NIST Open
MT08 Urdu Resources3 and the NIST Open MT08
Current Test Set Urdu?English4. This gives us
202,019 parallel translations, for approximately 2
million words of training data.
5.2 Experimental design
We used the scripts included with the Moses MT
toolkit (Koehn et al, 2007) to tokenize and nor-
malize the English data. We used a tokenizer and
normalizer developed at the SCALE 2009 workshop
(Baker et al, 2009) to preprocess the Urdu data. We
used GIZA++ (Och and Ney, 2000) to perform word
alignments.
For phrase structure parses of the English data, we
used the Berkeley parser (Petrov and Klein, 2007).
For CCG parses, and for reading labels out of a parse
chart, we used the C&C parser (Clark and Curran,
2007).
After aligning and parsing the training data, we
used the Thrax grammar extractor (Weese et al,
2011) to extract all of the translation grammars.
We used the same feature set in all the transla-
tion grammars. This includes, for each rule C ?
?f ; e?, relative-frequency estimates of the probabil-
2http://www.itl.nist.gov/iad/mig/tests/
mt/2009/MT09_ConstrainedResources.pdf
3LDC2009E12
4LDC2009E11
228
Model BLEU sec./sent.
Hiero 25.67 (0.9781) 0.05
Syntax 27.06 (0.9703) 3.04
SAMT 28.06 (0.9714) 63.48
CCG derivations 27.3 (0.9770) 5.24
CCG parse chart 27.64 (0.9673) 33.6
Table 3: Results of translation experiments on
Urdu?English. Higher BLEU scores are better.
BLEU?s brevity penalty is reported in parentheses.
ities p(f |A), p(f |e), p(f |e,A), p(e|A), p(e|f), and
p(e|f,A).
The feature set alo includes lexical weighting for
rules as defined by Koehn et al (2003) and various
binary features as well as counters for the number of
unaligned words in each rule.
To train the feature weights we used the Z-MERT
implementation (Zaidan, 2009) of the Minimum
Error-Rate Training algorithm (Och, 2003).
To decode the test sets, we used the Joshua ma-
chine translation decoder (Weese et al, 2011). The
language model is a 5-gram LM trained on English
GigaWord Fourth Edition.5
5.3 Evaluation criteria
We measure machine translation performance using
the BLEU metric (Papineni et al, 2002). We also
report the translation time for the test set in seconds
per sentence. These results are shown in Table 3.
All of the syntactic labeling schemes show an im-
provement over the Hiero model. Indeed, they all
fall in the range of approximately 27?28 BLEU. We
can see that the 1-best derivation CCG model per-
forms slightly better than the phrase structure model,
and the CCG parse chart model performs a little bet-
ter than that. SAMT has the highest BLEU score.
The models with a larger number of rules perform
better; this supports our assertion that we shouldn?t
throw away too many rules.
When it comes to translation time, the three
smaller models (Hiero, phrase structure syntax, and
CCG 1-best derivations) are significantly faster than
the two larger ones. However, even though the CCG
parse chart model is almost 34 the size of SAMT in
terms of number of rules, it doesn?t take 34 of the
5LDC2009T13
time. In fact, it takes only half the time of the SAMT
model, thanks to the smaller rule label set.
6 Discussion and Future Work
Finding an appropriate mechanism to inform phrase-
based translation models and their hierarchical vari-
ants with linguistic syntax is a difficult problem
that has attracted intense interest, with a variety
of promising approaches including unsupervised
clustering (Zollmann and Vogel, 2011), merging
(Hanneman et al, 2011), and selection (Mylonakis
and Sima?an, 2011) of labels derived from phrase-
structure parse trees very much like those used by
our baseline systems. What we find particularly
attractive about CCG is that it naturally assigns
linguistically-motivated labels to most spans of a
sentence using a reasonably concise label set, possi-
bility obviating the need for further refinement. In-
deed, the analytical flexibility of CCG has motivated
its increasing use in MT, from applications in lan-
guage modeling (Birch et al, 2007; Hassan et al,
2007) to more recent proposals to incorporate it into
phrase-based (Mehay, 2010) and hierarchical trans-
lation systems (Auli, 2009).
Our new model builds on these past efforts, rep-
resenting a more fully instantiated model of CCG-
based translation. We have shown that the label
scheme allows us to keep many more translation
rules than labels based on phrase structure syntax,
extracting almost as many rules as the SAMT model,
but keeping the label set an order of magnitude
smaller, which leads to more efficient translation.
This simply scratches the surface of possible uses of
CCG in translation. In future work, we plan to move
from a formally context-free to a formally CCG-
based model of translation, implementing combina-
torial rules such as application, composition, and
type-raising.
Acknowledgements
Thank you to Michael Auli for providing code to
inspect the full chart from the C&C parser. This
research was supported in part by the NSF under
grant IIS-0713448 and in part by the EuroMatrix-
Plus project funded by the European Commission
(7th Framework Programme). Opinions, interpreta-
tions, and conclusions are the authors? alone.
229
References
Kazimierz Adjukiewicz. 1935. Die syntaktische kon-
nexita?t. In Storrs McCall, editor, Polish Logic 1920?
1939, pages 207?231. Oxford University Press.
Michael Auli. 2009. CCG-based models for statisti-
cal machine translation. Ph.D. Proposal, University
of Edinburgh.
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Ann Irvine,
Mike Kayser, Lori Levin, Justin Marinteau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation: Fi-
nal report of the 2009 summer camp for advanced lan-
guage exploration (scale). Technical report, Human
Language Technology Center of Excellence.
Yehoshua Bar-Hillel, Chaim Gaifman, and Eliyahu
Shamir. 1964. On categorial and phrase-structure
grammars. In Yehoshua Bar-Hillel, editor, Language
and Information, pages 99?115. Addison-Wesley.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proc. of WMT.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics, 33(4).
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proc. EMNLP.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In
Proc. NAACL, pages 227?235.
Aaron Dunlop, Nathan Bodenstab, and Brian Roark.
2010. Reducing the grammar constant: an analysis
of CYK parsing efficiency. Technical report CSLU-
2010-02, OHSU.
Timothy A. D. Fowler and Gerald Penn. 2010. Accu-
rate context-free parsing with combinatory categorial
grammar. In Proc. ACL, pages 335?344.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. of EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve Deneefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich
syntactic translation models. In In ACL, pages 961?
968.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proc. of WMT.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proc. of ACL.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adria`
de Gispert, and Michael Riley. 2011. Hierarchi-
cal phrase-based translation representations. In Proc.
EMNLP, pages 1373?1383.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Frederico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL Demonstration Session.
Dennis Mehay. 2010. Linguistically motivated syntax
for machine translation. Ph.D. Proposal, Ohio State
University.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proc. of ACL-HLT.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of ACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. HLT-NAACL.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008.
Better binarization for the CKY parsing. In Proc.
EMNLP, pages 167?176.
Mark Steedman and Jason Baldridge. 2011. Combina-
tory categorial grammar. In Robert Borsley and Kersti
Bo?rjars, editors, Non-Transformational Syntax. Wiley-
Blackwell.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proc. of
EMNLP.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the thrax
grammar extractor. In Proc. of WMT.
Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu, and
Ming Zhou. 2009. Better synchronous binarization
for machine translation. In Proc. EMNLP, pages 362?
370.
230
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91(1):79?88.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proc. of ACL-HLT.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proc. of COLING.
231
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 283?291,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Joshua 4.0: Packing, PRO, and Paraphrases
Juri Ganitkevitch1, Yuan Cao1, Jonathan Weese1, Matt Post2, and Chris Callison-Burch1
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present Joshua 4.0, the newest version
of our open-source decoder for parsing-based
statistical machine translation. The main con-
tributions in this release are the introduction
of a compact grammar representation based
on packed tries, and the integration of our
implementation of pairwise ranking optimiza-
tion, J-PRO. We further present the exten-
sion of the Thrax SCFG grammar extractor
to pivot-based extraction of syntactically in-
formed sentential paraphrases.
1 Introduction
Joshua is an open-source toolkit1 for parsing-based
statistical machine translation of human languages.
The original version of Joshua (Li et al, 2009) was
a reimplementation of the Python-based Hiero ma-
chine translation system (Chiang, 2007). It was later
extended to support grammars with rich syntactic
labels (Li et al, 2010a). More recent efforts in-
troduced the Thrax module, an extensible Hadoop-
based extraction toolkit for synchronous context-
free grammars (Weese et al, 2011).
In this paper we describe a set of recent exten-
sions to the Joshua system. We present a new com-
pact grammar representation format that leverages
sparse features, quantization, and data redundancies
to store grammars in a dense binary format. This al-
lows for both near-instantaneous start-up times and
decoding with extremely large grammars. In Sec-
tion 2 we outline our packed grammar format and
1joshua-decoder.org
present experimental results regarding its impact on
decoding speed, memory use and translation quality.
Additionally, we present Joshua?s implementation
of the pairwise ranking optimization (Hopkins and
May, 2011) approach to translation model tuning.
J-PRO, like Z-MERT, makes it easy to implement
new metrics and comes with both a built-in percep-
tron classifier and out-of-the-box support for widely
used binary classifiers such as MegaM and Max-
Ent (Daume? III and Marcu, 2006; Manning and
Klein, 2003). We describe our implementation in
Section 3, presenting experimental results on perfor-
mance, classifier convergence, and tuning speed.
Finally, we introduce the inclusion of bilingual
pivoting-based paraphrase extraction into Thrax,
Joshua?s grammar extractor. Thrax?s paraphrase ex-
traction mode is simple to use, and yields state-of-
the-art syntactically informed sentential paraphrases
(Ganitkevitch et al, 2011). The full feature set of
Thrax (Weese et al, 2011) is supported for para-
phrase grammars. An easily configured feature-level
pruning mechanism allows to keep the paraphrase
grammar size manageable. Section 4 presents de-
tails on our paraphrase extraction module.
2 Compact Grammar Representation
Statistical machine translation systems tend to per-
form better when trained on larger amounts of bilin-
gual parallel data. Using tools such as Thrax, trans-
lation models and their parameters are extracted
and estimated from the data. In Joshua, translation
models are represented as synchronous context-free
grammars (SCFGs). An SCFG is a collection of
283
rules {ri} that take the form:
ri = Ci ? ??i, ?i,?i, ~?i?, (1)
where left-hand side Ci is a nonterminal symbol, the
source side ?i and the target side ?i are sequences
of both nonterminal and terminal symbols. Further,
?i is a one-to-one correspondence between the non-
terminal symbols of ?i and ?i, and ~?i is a vector of
features quantifying the probability of ?i translat-
ing to ?i, as well as other characteristics of the rule
(Weese et al, 2011). At decoding time, Joshua loads
the grammar rules into memory in their entirety, and
stores them in a trie data structure indexed by the
rules? source side. This allows the decoder to effi-
ciently look up rules that are applicable to a particu-
lar span of the (partially translated) input.
As the size of the training corpus grows, so does
the resulting translation grammar. Using more di-
verse sets of nonterminal labels ? which can signifi-
cantly improve translation performance ? further ag-
gravates this problem. As a consequence, the space
requirements for storing the grammar in memory
during decoding quickly grow impractical. In some
cases grammars may become too large to fit into the
memory on a single machine.
As an alternative to the commonly used trie struc-
tures based on hash maps, we propose a packed trie
representation for SCFGs. The approach we take is
similar to work on efficiently storing large phrase
tables by Zens and Ney (2007) and language mod-
els by Heafield (2011) and Pauls and Klein (2011) ?
both language model implementations are now inte-
grated with Joshua.
2.1 Packed Synchronous Tries
For our grammar representation, we break the SCFG
up into three distinct structures. As Figure 1 in-
dicates, we store the grammar rules? source sides
{?i}, target sides {?i}, and feature data {~?i} in sep-
arate formats of their own. Each of the structures
is packed into a flat array, and can thus be quickly
read into memory. All terminal and nonterminal
symbols in the grammar are mapped to integer sym-
bol id?s using a globally accessible vocabulary map.
We will now describe the implementation details for
each representation and their interactions in turn.
2.1.1 Source-Side Trie
The source-side trie (or source trie) is designed
to facilitate efficient lookup of grammar rules by
source side, and to allow us to completely specify a
matching set of rule with a single integer index into
the trie. We store the source sides {?i} of a grammar
in a downward-linking trie, i.e. each trie node main-
tains a record of its children. The trie is packed into
an array of 32-bit integers. Figure 1 illustrates the
composition of a node in the source-side trie. All
information regarding the node is stored in a con-
tiguous block of integers, and decomposes into two
parts: a linking block and a rule block.
The linking block stores the links to the child trie
nodes. It consists of an integer n, the number of chil-
dren, and n blocks of two integers each, containing
the symbol id aj leading to the child and the child
node?s address sj (as an index into the source-side
array). The children in the link block are sorted by
symbol id, allowing for a lookup via binary or inter-
polation search.
The rule block stores all information necessary to
reconstruct the rules that share the source side that
led to the current source trie node. It stores the num-
ber of rules, m, and then a tuple of three integers
for each of the m rules: we store the symbol id of
the left-hand side, an index into the target-side trie
and a data block id. The rules in the data block are
initially in an arbitrary order, but are sorted by ap-
plication cost upon loading.
2.1.2 Target-Side Trie
The target-side trie (or target trie) is designed to
enable us to uniquely identify a target side ?i with a
single pointer into the trie, as well as to exploit re-
dundancies in the target side string. Like the source
trie, it is stored as an array of integers. However,
the target trie is a reversed, or upward-linking trie:
a trie node retains a link to its parent, as well as the
symbol id labeling said link.
As illustrated in Figure 1, the target trie is ac-
cessed by reading an array index from the source
trie, pointing to a trie node at depth d. We then fol-
low the parent links to the trie root, accumulating
target side symbols gj into a target side string gd1 as
we go along. In order to match this traversal, the tar-
get strings are entered into the trie in reverse order,
i.e. last word first. In order to determine d from a
284
# children
# rules
child symbol
child address
rule left-hand side
target address
data block id
n ?
m ?
a
j
s
j
+
1
C
j
t
j
b
j
.
.
.
.
.
.
n
m
.
.
.
.
.
.
parent symbol
parent address
g
j
t
j-1
.
.
.
.
.
.
# features
feature id
feature value
n ?
f
j
v
j
.
.
.
n
.
.
.
.
.
Feature block 
index
Feature byte
buffer
Target trie
array
Source trie
array
f
j
.
.
.
.
.
.
Quantization
b
j
.
.
.
.
.
.
f
j
q
j
Figure 1: An illustration of our packed grammar data structures. The source sides of the grammar rules are
stored in a packed trie. Each node may contain n children and the symbols linking to them, and m entries
for rules that share the same source side. Each rule entry links to a node in the target-side trie, where the full
target string can be retrieved by walking up the trie until the root is reached. The rule entries also contain
a data block id, which identifies feature data attached to the rule. The features are encoded according to a
type/quantization specification and stored as variable-length blocks of data in a byte buffer.
pointer into the target trie, we maintain an offset ta-
ble in which we keep track of where each new trie
level begins in the array. By first searching the offset
table, we can determine d, and thus know how much
space to allocate for the complete target side string.
To further benefit from the overlap there may be
among the target sides in the grammar, we drop the
nonterminal labels from the target string prior to in-
serting them into the trie. For richly labeled gram-
mars, this collapses all lexically identical target sides
that share the same nonterminal reordering behavior,
but vary in nonterminal labels into a single path in
the trie. Since the nonterminal labels are retained in
the rules? source sides, we do not lose any informa-
tion by doing this.
2.1.3 Features and Other Data
We designed the data format for the grammar
rules? feature values to be easily extended to include
other information that we may want to attach to a
rule, such as word alignments, or locations of occur-
rences in the training data. In order to that, each rule
ri has a unique block id bi associated with it. This
block id identifies the information associated with
the rule in every attached data store. All data stores
are implemented as memory-mapped byte buffers
that are only loaded into memory when actually re-
quested by the decoder. The format for the feature
data is detailed in the following.
The rules? feature values are stored as sparse fea-
tures in contiguous blocks of variable length in a
byte buffer. As shown in Figure 1, a lookup table
is used to map the bi to the index of the block in the
buffer. Each block is structured as follows: a sin-
gle integer, n, for the number of features, followed
by n feature entries. Each feature entry is led by an
integer for the feature id fj , and followed by a field
of variable length for the feature value vj . The size
of the value is determined by the type of the feature.
Joshua maintains a quantization configuration which
maps each feature id to a type handler or quantizer.
After reading a feature id from the byte buffer, we
retrieve the responsible quantizer and use it to read
the value from the byte buffer.
Joshua?s packed grammar format supports Java?s
standard primitive types, as well as an 8-bit quan-
tizer. We chose 8 bit as a compromise between
compression, value decoding speed and transla-
285
Grammar Format Memory
Hiero (43M rules)
Baseline 13.6G
Packed 1.8G
Syntax (200M rules)
Baseline 99.5G
Packed 9.8G
Packed 8-bit 5.8G
Table 1: Decoding-time memory use for the packed
grammar versus the standard grammar format. Even
without lossy quantization the packed grammar rep-
resentation yields significant savings in memory
consumption. Adding 8-bit quantization for the real-
valued features in the grammar reduces even large
syntactic grammars to a manageable size.
tion performance (Federico and Bertoldi, 2006).
Our quantization approach follows Federico and
Bertoldi (2006) and Heafield (2011) in partitioning
the value histogram into 256 equal-sized buckets.
We quantize by mapping each feature value onto the
weighted average of its bucket. Joshua allows for an
easily per-feature specification of type. Quantizers
can be share statistics across multiple features with
similar value distributions.
2.2 Experiments
We assess the packed grammar representation?s
memory efficiency and impact on the decoding
speed on the WMT12 French-English task. Ta-
ble 1 shows a comparison of the memory needed
to store our WMT12 French-English grammars at
runtime. We can observe a substantial decrease in
memory consumption for both Hiero-style gram-
mars and the much larger syntactically annotated
grammars. Even without any feature value quantiza-
tion, the packed format achieves an 80% reduction
in space requirements. Adding 8-bit quantization
for the log-probability features yields even smaller
grammar sizes, in this case a reduction of over 94%.
In order to avoid costly repeated retrievals of indi-
vidual feature values of rules, we compute and cache
the stateless application cost for each grammar rule
at grammar loading time. This, alongside with a lazy
approach to rule lookup allows us to largely avoid
losses in decoding speed.
Figure shows a translation progress graph for the
WMT12 French-English development set. Both sys-
 0 500 1000 1500 2000 2500
 0  500  1000  1500  2000  2500Sentences Translated Seconds PassedStandardPacked
Figure 2: A visualization of the loading and decod-
ing speed on the WMT12 French-English develop-
ment set contrasting the packed grammar represen-
tation with the standard format. Grammar loading
for the packed grammar representation is substan-
tially faster than that for the baseline setup. Even
with a slightly slower decoding speed (note the dif-
ference in the slopes) the packed grammar finishes
in less than half the time, compared to the standard
format.
tems load a Hiero-style grammar with 43 million
rules, and use 16 threads for parallel decoding. The
initial loading time for the packed grammar repre-
sentation is dramatically shorter than that for the
baseline setup (a total of 176 seconds for loading and
sorting the grammar, versus 1897 for the standard
format). Even though decoding speed is slightly
slower with the packed grammars (an average of 5.3
seconds per sentence versus 4.2 for the baseline), the
effective translation speed is more than twice that of
the baseline (1004 seconds to complete decoding the
2489 sentences, versus 2551 seconds with the stan-
dard setup).
3 J-PRO: Pairwise Ranking Optimization
in Joshua
Pairwise ranking optimization (PRO) proposed by
(Hopkins and May, 2011) is a new method for dis-
criminative parameter tuning in statistical machine
translation. It is reported to be more stable than the
popular MERT algorithm (Och, 2003) and is more
scalable with regard to the number of features. PRO
treats parameter tuning as an n-best list reranking
problem, and the idea is similar to other pairwise
ranking techniques like ranking SVM and IR SVMs
286
(Li, 2011). The algorithm can be described thusly:
Let h(c) = ?w,?(c)? be the linear model score
of a candidate translation c, in which ?(c) is the
feature vector of c and w is the parameter vector.
Also let g(c) be the metric score of c (without loss
of generality, we assume a higher score indicates a
better translation). We aim to find a parameter vector
w such that for a pair of candidates {ci, cj} in an n-
best list,
(h(ci)? h(cj))(g(ci)? g(cj)) =
?w,?(ci)??(cj)?(g(ci)? g(cj)) > 0,
namely the order of the model score is consistent
with that of the metric score. This can be turned into
a binary classification problem, by adding instance
??ij = ?(ci)??(cj)
with class label sign(g(ci) ? g(cj)) to the training
data (and symmetrically add instance
??ji = ?(cj)??(ci)
with class label sign(g(cj) ? g(ci)) at the same
time), then using any binary classifier to find the w
which determines a hyperplane separating the two
classes (therefore the performance of PRO depends
on the choice of classifier to a large extent). Given
a training set with T sentences, there are O(Tn2)
pairs of candidates that can be added to the training
set, this number is usually much too large for effi-
cient training. To make the task more tractable, PRO
samples a subset of the candidate pairs so that only
those pairs whose metric score difference is large
enough are qualified as training instances. This fol-
lows the intuition that high score differential makes
it easier to separate good translations from bad ones.
3.1 Implementation
PRO is implemented in Joshua 4.0 named J-PRO.
In order to ensure compatibility with the decoder
and the parameter tuning module Z-MERT (Zaidan,
2009) included in all versions of Joshua, J-PRO is
built upon the architecture of Z-MERT with sim-
ilar usage and configuration files(with a few extra
lines specifying PRO-related parameters). J-PRO in-
herits Z-MERT?s ability to easily plug in new met-
rics. Since PRO allows using any off-the-shelf bi-
nary classifiers, J-PRO provides a Java interface that
enables easy plug-in of any classifier. Currently, J-
PRO supports three classifiers:
? Perceptron (Rosenblatt, 1958): the percep-
tron is self-contained in J-PRO, no external re-
sources required.
? MegaM (Daume? III and Marcu, 2006): the clas-
sifier used by Hopkins and May (2011).2
? Maximum entropy classifier (Manning and
Klein, 2003): the Stanford toolkit for maxi-
mum entropy classification.3
The user may specify which classifier he wants to
use and the classifier-specific parameters in the J-
PRO configuration file.
The PRO approach is capable of handling a large
number of features, allowing the use of sparse dis-
criminative features for machine translation. How-
ever, Hollingshead and Roark (2008) demonstrated
that naively tuning weights for a heterogeneous fea-
ture set composed of both dense and sparse features
can yield subpar results. Thus, to better handle the
relation between dense and sparse features and pro-
vide a flexible selection of training schemes, J-PRO
supports the following four training modes. We as-
sume M dense features and N sparse features are
used:
1. Tune the dense feature parameters only, just
like Z-MERT (M parameters to tune).
2. Tune the dense + sparse feature parameters to-
gether (M +N parameters to tune).
3. Tune the sparse feature parameters only with
the dense feature parameters fixed, and sparse
feature parameters scaled by a manually speci-
fied constant (N parameters to tune).
4. Tune the dense feature parameters and the scal-
ing factor for sparse features, with the sparse
feature parameters fixed (M+1 parameters to
tune).
J-PRO supports n-best list input with a sparse fea-
ture format which enumerates only the firing fea-
tures together with their values. This enables a more
compact feature representation when numerous fea-
tures are involved in training.
2hal3.name/megam
3nlp.stanford.edu/software
287
0 10 20 300
10
20
30
40
Iteration
BLE
U
Dev set MT03 (10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLE
U
Test set MT04(10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLE
U
Test set MT05(10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Dev set MT03 (1026 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Test set MT04(1026 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Test set MT05(1026 features)
 
 
PercepMegaMMax?Ent
Figure 3: Experimental results on the development and test sets. The x-axis is the number of iterations (up to
30) and the y-axis is the BLEU score. The three curves in each figure correspond to three classifiers. Upper
row: results trained using only dense features (10 features); Lower row: results trained using dense+sparse
features (1026 features). Left column: development set (MT03); Middle column: test set (MT04); Right
column: test set (MT05).
Datasets Z-MERT
J-PRO
Percep MegaM Max-Ent
Dev (MT03) 32.2 31.9 32.0 32.0
Test (MT04) 32.6 32.7 32.7 32.6
Test (MT05) 30.7 30.9 31.0 30.9
Table 2: Comparison between the results given by Z-MERT and J-PRO (trained with 10 features).
3.2 Experiments
We did our experiments using J-PRO on the NIST
Chinese-English data, and BLEU score was used as
the quality metric for experiments reported in this
section.4 The experimental settings are as the fol-
lowing:
Datasets: MT03 dataset (998 sentences) as devel-
opment set for parameter tuning, MT04 (1788 sen-
tences) and MT05 (1082 sentences) as test sets.
Features: Dense feature set include the 10 regular
features used in the Hiero system; Sparse feature set
4We also experimented with other metrics including TER,
METEOR and TER-BLEU. Similar trends as reported in this
section were observed. These results are omitted here due to
limited space.
includes 1016 target-side rule POS bi-gram features
as used in (Li et al, 2010b).
Classifiers: Perceptron, MegaM and Maximum
entropy.
PRO parameters: ? = 8000 (number of candidate
pairs sampled uniformly from the n-best list), ? = 1
(sample acceptance probability), ? = 50 (number of
top candidates to be added to the training set).
Figure 3 shows the BLEU score curves on the
development and test sets as a function of itera-
tions. The upper and lower rows correspond to
the results trained with 10 dense features and 1026
dense+sparse features respectively. We intentionally
selected very bad initial parameter vectors to verify
the robustness of the algorithm. It can be seen that
288
with each iteration, the BLEU score increases mono-
tonically on both development and test sets, and be-
gins to converge after a few iterations. When only 10
features are involved, all classifiers give almost the
same performance. However, when scaled to over a
thousand features, the maximum entropy classifier
becomes unstable and the curve fluctuates signifi-
cantly. In this situation MegaM behaves well, but
the J-PRO built-in perceptron gives the most robust
performance.
Table 2 compares the results of running Z-MERT
and J-PRO. Since MERT is not able to handle nu-
merous sparse features, we only report results for
the 10-feature setup. The scores for both setups
are quite close to each other, with Z-MERT doing
slightly better on the development set but J-PRO
yielding slightly better performance on the test set.
4 Thrax: Grammar Extraction at Scale
4.1 Translation Grammars
In previous years, our grammar extraction methods
were limited by either memory-bounded extractors.
Moving towards a parallelized grammar extraction
process, we switched from Joshua?s formerly built-
in extraction module to Thrax for WMT11. How-
ever, we were limited to a simple pseudo-distributed
Hadoop setup. In a pseudo-distributed cluster, all
tasks run on separate cores on the same machine
and access the local file system simultaneously, in-
stead of being distributed over different physical ma-
chines and harddrives. This setup proved unreliable
for larger extractions, and we were forced to reduce
the amount of data that we used to train our transla-
tion models.
For this year, however, we had a permanent clus-
ter at our disposal, which made it easy to extract
grammars from all of the available WMT12 data.
We found that on a properly distributed Hadoop
setup Thrax was able to extract both Hiero gram-
mars and the much larger SAMT grammars on the
complete WMT12 training data for all tested lan-
guage pairs. The runtimes and resulting (unfiltered)
grammar sizes for each language pair are shown in
Table 3 (for Hiero) and Table 4 (for SAMT).
Language Pair Time Rules
Cs ? En 4h41m 133M
De ? En 5h20m 219M
Fr ? En 16h47m 374M
Es ? En 16h22m 413M
Table 3: Extraction times and grammar sizes for Hi-
ero grammars using the Europarl and News Com-
mentary training data for each listed language pair.
Language Pair Time Rules
Cs ? En 7h59m 223M
De ? En 9h18m 328M
Fr ? En 25h46m 654M
Es ? En 28h10m 716M
Table 4: Extraction times and grammar sizes for
the SAMT grammars using the Europarl and News
Commentary training data for each listed language
pair.
4.2 Paraphrase Extraction
Recently English-to-English text generation tasks
have seen renewed interest in the NLP commu-
nity. Paraphrases are a key component in large-
scale state-of-the-art text-to-text generation systems.
We present an extended version of Thrax that im-
plements distributed, Hadoop-based paraphrase ex-
traction via the pivoting approach (Bannard and
Callison-Burch, 2005). Our toolkit is capable of
extracting syntactically informed paraphrase gram-
mars at scale. The paraphrase grammars obtained
with Thrax have been shown to achieve state-of-the-
art results on text-to-text generation tasks (Ganitke-
vitch et al, 2011).
For every supported translation feature, Thrax im-
plements a corresponding pivoted feature for para-
phrases. The pivoted features are set up to be aware
of the prerequisite translation features they are de-
rived from. This allows Thrax to automatically de-
tect the needed translation features and spawn the
corresponding map-reduce passes before the pivot-
ing stage takes place. In addition to features use-
ful for translation, Thrax also offers a number of
features geared towards text-to-text generation tasks
such as sentence compression or text simplification.
Due to the long tail of translations in unpruned
289
Source Bitext Sentences Words Pruning Rules
Fr ? En 1.6M 45M p(e1|e2), p(e2|e1) > 0.001 49M
{Da + Sv + Cs + De + Es + Fr} ? En 9.5M 100M
p(e1|e2), p(e2|e1) > 0.02 31M
p(e1|e2), p(e2|e1) > 0.001 91M
Table 5: Large paraphrase grammars extracted from EuroParl data using Thrax. The sentence and word
counts refer to the English side of the bitexts used.
translation grammars and the combinatorial effect
of pivoting, paraphrase grammars can easily grow
very large. We implement a simple feature-level
pruning approach that allows the user to specify up-
per or lower bounds for any pivoted feature. If a
paraphrase rule is not within these bounds, it is dis-
carded. Additionally, pivoted features are aware of
the bounding relationship between their value and
the value of their prerequisite translation features
(i.e. whether the pivoted feature?s value can be guar-
anteed to never be larger than the value of the trans-
lation feature). Thrax uses this knowledge to dis-
card overly weak translation rules before the pivot-
ing stage, leading to a substantial speedup in the ex-
traction process.
Table 5 gives a few examples of large paraphrase
grammars extracted from WMT training data. With
appropriate pruning settings, we are able to obtain
paraphrase grammars estimated over bitexts with
more than 100 million words.
5 Additional New Features
? With the help of the respective original au-
thors, the language model implementations by
Heafield (2011) and Pauls and Klein (2011)
have been integrated with Joshua, dropping
support for the slower and more difficult to
compile SRILM toolkit (Stolcke, 2002).
? We modified Joshua so that it can be used as
a parser to analyze pairs of sentences using a
synchronous context-free grammar. We imple-
mented the two-pass parsing algorithm of Dyer
(2010).
6 Conclusion
We present a new iteration of the Joshua machine
translation toolkit. Our system has been extended to-
wards efficiently supporting large-scale experiments
in parsing-based machine translation and text-to-text
generation: Joshua 4.0 supports compactly repre-
sented large grammars with its packed grammars,
as well as large language models via KenLM and
BerkeleyLM.We include an implementation of PRO,
allowing for stable and fast tuning of large feature
sets, and extend our toolkit beyond pure translation
applications by extending Thrax with a large-scale
paraphrase extraction module.
Acknowledgements This research was supported
by in part by the EuroMatrixPlus project funded
by the European Commission (7th Framework Pro-
gramme), and by the NSF under grant IIS-0713448.
Opinions, interpretations, and conclusions are the
authors? alone.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126.
Chris Dyer. 2010. Two monolingual parses are bet-
ter than one (synchronous parse). In Proceedings of
HLT/NAACL, pages 263?266. Association for Compu-
tational Linguistics.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings of WMT06, pages
94?101. Association for Computational Linguistics.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197. Association for Computational Linguistics.
290
Kristy Hollingshead and Brian Roark. 2008. Rerank-
ing with baseline system scores and ranks as features.
Technical report, Center for Spoken Language Under-
standing, Oregon Health & Science University.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proc. WMT, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010a. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proc. WMT.
Zhifei Li, Ziyuan Wang, and Sanjeev Khudanpur. 2010b.
Unsupervised discriminative language model training
for machine translation using simulated confusion sets.
In Proceedings of COLING, Beijing, China, August.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Morgan &
Claypool Publishers.
Chris Manning and Dan Klein. 2003. Optimization,
maxent models, and conditional estimation without
magic. In Proceedings of HLT/NAACL, pages 8?8. As-
sociation for Computational Linguistics.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41rd
Annual Meeting of the Association for Computational
Linguistics (ACL-2003), Sapporo, Japan.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-
gram language models. In Proceedings of ACL, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Frank Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the Thrax
grammar extractor. In Proceedings of WMT11.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Richard Zens and Hermann Ney. 2007. Efficient phrase-
table representation for machine translation with appli-
cations to online MT and speech translation. In Pro-
ceedings of HLT/NAACL, pages 492?499, Rochester,
New York, April. Association for Computational Lin-
guistics.
291
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 401?409,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing
Matt Post? and Chris Callison-Burch?? and Miles Osborne?
?Human Langage Technology Center of Excellence, Johns Hopkins University
?Center for Language and Speech Processing, Johns Hopkins University
?School of Informatics, University of Edinburgh
Abstract
Recent work has established the efficacy of
Amazon?s Mechanical Turk for constructing
parallel corpora for machine translation re-
search. We apply this to building a collec-
tion of parallel corpora between English and
six languages from the Indian subcontinent:
Bengali, Hindi, Malayalam, Tamil, Telugu,
and Urdu. These languages are low-resource,
under-studied, and exhibit linguistic phenom-
ena that are difficult for machine translation.
We conduct a variety of baseline experiments
and analysis, and release the data to the com-
munity.
1 Introduction
The quality of statistical machine translation (MT)
systems is strongly related to the amount of paral-
lel text available for the language pairs. However,
most language pairs have little or no readily available
bilingual training data available. As a result, most
contemporary MT research tends to opportunisti-
cally focus on language pairs with large amounts of
parallel data.
A consequence of this bias is that language ex-
hibiting certain linguistic phenomena are underrep-
resented, including languages with complex mor-
phology and languages with divergent word order-
ings. In this paper, we describe our work gather-
ing and refining document-level parallel corpora be-
tween English and each of six verb-final languages
spoken on the Indian subcontinent: Bengali, Hindi,
Malayalam, Tamil, Telugu, and Urdu. This paper?s
contributions are as follows:
? We apply an established protocol for using
Amazon?s Mechanical Turk (MTurk) to collect
parallel data to train and evaluate translation
systems for six Indian languages.
? We investigate the relative performance of syn-
tactic translation models over hierarchical ones,
showing that syntax results in higher BLEU
scores in most cases.
? We explore the impact of training data quality
on the quality of the resulting model.
? We release the corpora to the research commu-
nity under the Creative Commons Attribution-
Sharealike 3.0 Unported License (CC BY-SA
3.0).1
2 Why Indian languages?
Indian languages are important objects of study for
a number of reasons. These languages are low-
resource languages in terms of the availability of
MT systems2 (and NLP tools in general) yet together
they represent nearly half a billion native speakers
(Table 1). Their speakers are well-educated, with
many of them speaking English either natively or as a
second language. Together with the degree of Inter-
net penetration in India, it is reasonably straightfor-
ward to find and hire non-expert translators through
crowdsourcing services like Amazon?s Mechanical
Turk.
1joshua-decoder.org/indian-parallel-corpora
2See sampark.iiit.ac.in/sampark/web/index.php/
content for a notable growing effort.
401
??????? ???? ???????????? ?????
senator her remarks prepared
Figure 1: An example of SOV word ordering in Tamil.
Translation: The senator prepared her remarks.
??? ?? ? ??
walk CONT PAST 1p
Figure 2: An example of the morphology of the Bengali
word ?????????, meaning [I] was walking. CONT denotes
the continuous aspect, while PAST denotes past tense.
In addition to a general desire to collect suitable
training corpora for low-resource languages, Indian
languages demonstrate a variety of linguistic phe-
nomena that are divergent from English and under-
studied. One example is head-finalness, exhibited
most obviously in a subject-object-verb (SOV) pat-
tern of sentence structure, in contrast to the gen-
eral SVO ordering of English sentences. One of
the motivations underlying linguistically-motivated
syntactic translation systems like GHKM (Galley et
al., 2004; Galley et al, 2006) or SAMT (Zollmann
and Venugopal, 2006) is to describe such transfor-
mations. This difference in word order has the po-
tential to serve as a better test bed for syntax-based
MT3 compared to translating between English and
European languages, most of which largely share its
word order. Figure 1 contains an example of SOV
reordering in Tamil.
A second important phenomenon present in these
languages is a high degree of morphological com-
plexity relative to English (Figure 2). Indian lan-
guages can be highly agglutinative, which means
that words are formed by concatenating morpholog-
ical affixes that convey information such as tense,
person, number, gender, mood, and voice. Mor-
phological complexity is a considerable hindrance at
all stages of the MT pipeline, but particularly align-
ment, where inflectional variations mask patterns
from alignment tools that treat words as atoms.
3Weuse hierarchical to denote translation grammars that use
only a single nonterminal (Chiang, 2007), in contrast to syntac-
tic systems, which make use of linguistic annotations (Zollmann
and Venugopal, 2006; Galley et al, 2006).
language script family L1
Bengali ????? Indo-Aryan 181M
Hindi ???? ?????? Indo-Aryan 180M
Malayalam ?????? Dravidian 35M
Tamil ????? Dravidian 65M
Telugu ?????? Dravidian 69M
Urdu ???? Indo-Aryan 60M
Table 1: Languages. L1 is the worldwide number of na-
tive speakers according to Lewis (2009).
3 Data collection
The source of the documents for our translation task
for each of the languages in Table 1 was the set of
the top-100 most-viewed documents from each lan-
guage?s Wikipedia. These lists were obtained us-
ing page view statistics compiled from dammit.lt/
wikistats over a one year period. We did not apply
any filtering for topic or content. Table 2 contains
a manually categorized list of documents for Hindi,
with some minimal annotations indicating how the
documents relate to those in the other languages.
These documents constitute a diverse set of topics,
including culture, the internet, and sex.
We collected the parallel corpora using a three-
step process designed to ensure the integrity of the
non-professional translations. The first step was to
build a bilingual dictionary (?3.1). These dictionar-
ies were used to bootstrap the experimental controls
in the collection of four translations of each source
sentence (?3.2). Finally, as a measure of data qual-
ity, we independently collect votes on the which of
the four redundant translations is the best (?3.3).
3.1 Dictionaries
A key component of managing MTurk workers is to
ensure that they are competently and conscientiously
undertaking the tasks. As non-speakers of all of the
Indian languages, we had no simple and scalable way
to judge the quality of the workers? translations. Our
solutionwas to bootstrap the process by first building
bilingual dictionaries for each of the datasets. The
dictionaries were then used to produce glosses of the
complete source sentences, which we compared to
the translations produced by the workers as a rough
means of manually gauging trust (?3.2).
The dictionaries were built in a separate MTurk
402
PLACES PEOPLE PEOPLE TECHNOLOGY LANGUAGE AND RELIGION
Agra A. P. J. Abdul Kalam Premchand Blog CULTURE Bhagavad Gita
Bihar Aishwarya Rai Rabindranath Tagore Google Ayurveda Diwali
China Akbar Rani Lakshmibai Hindi Web Resources Constitution of India Hanuman
Delhi Amitabh Bachchan Sachin Tendulkar Internet Cricket Hinduism
Himalayas Barack Obama Sarojini Naidu Mobile phone English language Hinduism
India Bhagat Singh Subhas Chandra Bose News aggregator Hindi Cable News Holi
Mumbai Dainik Jagran Surdas RSS Hindi literature Islam
Nepal Gautama Buddha Swami Vivekananda Wikipedia Hindi-Urdu grammar Mahabharata
Pakistan Harivansh Rai Bachchan Tulsidas YouTube Horoscope Puranas
Rajasthan Indira Gandhi Indian cuisine Quran
Red Fort Jaishankar Prasad THINGS SEX Sanskrit Ramayana
Taj Mahal Jawaharlal Nehru Air pollution Anal sex Standard Hindi Shiva
United States Kabir Earth Kama Sutra Shiva
Uttar Pradesh Kalpana Chawla Essay Masturbation EVENTS Taj Majal: Shiva Temple?
Mahadevi Varma Ganges Penis History of India Vedas
Meera General knowledge Sex positions World War II Vishnu
Mohammed Rafi Global warming Sexual intercourse
Mohandas Karamchand Gandhi Pollution Vagina
Mother Teresa Solar energy
Navbharat Times Terrorism
Table 2: The 100 most viewed Hindi Wikipedia articles (titles translated to English using inter-language links and
Google translate and manually categorized). Entries in bold were present in the top 100 lists of at least four of the
Indian top 100 lists. Earth, India,World War II, and Wikipedia were in the top 100 lists of all six languages.
language entries translations
Bengali 4,075 6,011
Hindi - -
Malayalam 41,502 144,505
Tamil 11,592 69,128
Telugu 12,193 38,532
Urdu 26,363 113,911
Table 3: Dictionary statistics. Entries is the number of
source-language types, while translations lists the num-
ber of words or phrases they translated to (i.e., the num-
ber of pairs in the dictionary). Controls for Hindi were
obtained using Google translate, the only one of these lan-
guages that were available at the outset of this project.
task, in which workers were asked to translate sin-
gle words and short phrases from the complete set of
Wikipedia documents. For each word, MTurk work-
ers were presented with three sentences containing
that word, which provided context. The control for
this task was obtained from the Wikipedia article ti-
tles which are linked across languages, and can thus
be assumed to be translations of each other. Workers
who performed too poorly on these known transla-
tions had their work rejected.
Table 3 lists the size of the dictionaries we con-
structed.
3.2 Translations
With the dictionaries in hand, we moved on to trans-
late the entireWikipedia documents. Each human in-
telligence task (HIT) posted onMTurk contained ten
sequential source-language sentences from a doc-
ument, and asked the worker to enter a free-form
translation for each. We collected four translations
from different translators for each source sentence.
To discourage cheating through cutting-and-pasting
into automatic translation systems, sentences were
presented as images. Workers were paid $0.70 per
HIT. We then manually determined whether to ac-
cept or reject a worker?s HITs based on a review of
each worker?s submissions, which included a com-
parison of the translations to a monotonic gloss (pro-
duced with the dictionary), the percentage of empty
translations, the amount of time the worker took to
complete the HIT, geographic location (self-reported
and geolocated by way of the worker?s IP address),
and by comparing different translations of the same
source segments against one another.
We obtained translations of the source-language
documents in a relatively short amount of time. Fig-
ure 3 depicts the number of translations collected as
a function of the amount of time from the posting of
the task. Malayalam provided the highest through-
put, generating half a million words in just under a
403
320 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
800,000
0
100,000
200,000
300,000
400,000
500,000
600,000
700,000
Malay
alam
Tamil
Telugu
Hindi
Urdu
Bengali
Figure 3: The total volume of translations (measured in
English words) as a function of elapsed days. For Malay-
alam, we collected half a million words of translations in
just under a week.
week. For comparison, the Europarl corpus (Koehn,
2005) has about 50million words of English for each
of the Spanish and French parallel corpora.
As has been previously reported (Zbib et al,
2012), cost is another advantage of building train-
ing data on Mechanical Turk. Germann (2001) puts
the cost of professionally translated English at about
$0.30 perword for translation fromTamil. Our trans-
lations were obtained for less than $0.01 per word.
The rate of collection could likely be increased by
raising these payments, but it is unclear whether
quality would be affected by raising the base pay
(although it could be improved by paying for sub-
sequent quality control HITs, like editing).
The tradeoff for low-cost translations is increased
variance in translation quality when compared to the
more consistently-good professional translations.
Figure 4 contains some hand-picked examples of the
sorts of translations we obtained. Later, in the Exper-
iments section (?4), we will investigate the effects
this variance in translation quality has on the qual-
ity of the models that can be constructed. For now,
the variancemotivated the collection of an additional
dataset, described in the next section.
3.3 Votes
A prevailing issue with translations collected on
MTurk is the prevalence of low-quality translations.
Quality suffers for a variety of reasons: Turkers
lack formal training, often translate into a nonna-
tive tongue, may give insufficient attention to the
task, and likely desire to maximize their throughput
(and thus their wage). Unlike Zaidan and Callison-
Burch (2011), who embed controls containing source
language sentences with known professional trans-
lations, we had no professionally translated data.
Therefore, we could not measure the BLEU score of
the Turkers.
Motivated by desire to have some measure of the
relative quality and variance of the translations, we
designed another task in which we presented an in-
dependent set of Turkers with an original sentence
and its four translations, and asked them to vote on
which was best.4 Five independent workers voted
on the translations of each source sentence. Tallying
the resulting votes, we found that roughly 65% of
the sentences had five votes cast on just one or two
of the translations, and about 95% of the sentences
had all the votes cast on one, two, or three sentences.
This suggests both (1) that there was a difference in
the quality of the translations, and (2) the voters were
able to discern these differences, and took their task
seriously enough to report them.
3.4 Data sets
For each parallel corpus, we created a standardized
test set in the following manner. We first manu-
ally assigned each of the Wikipedia documents for
each language into one of the following nine cate-
gories: EVENTS, LANGUAGE AND CULTURE,
PEOPLE, PLACES, RELIGION, SEX, TECHNOL-
OGY, THINGS, or MISC. We then assigned doc-
uments to training, development, development test,
and test sets in round-robin fashion using a ratio of
roughly 7:1:1:1. For training data, each source sen-
tence was repeated four times in order to allow it
to be paired with each of its translations. For the
development and test sets, the multiple translations
served as alternate references. Table 4 lists sentence-
and word-level statistics for the datasets for each lan-
guage pair (these counts are prior to any tokeniza-
tion).
4We did not collect votes for Malayalam.
404
?????? 15,2007??? ???????????? ?????? ?????? ???? ?????? ???????????.
In March 15,2007 Wiki got a place in Oxford English dictionary.
On March 15, 2007 wiki was included in the Oxford English dictionary. (5)
ON MARCH 15, 2007, WIKI FOUND A PLACE IN THE OXFORD ENGLISH DICTIONARY
March 15, 2007 oxford english index of wiki?s place.
Figure 4: An example of the variance in translation quality for the human translations of a Tamil sentence; the format-
ting of the translations has been preserved exactly. The parenthesized number indicates the number of votes received
in the voting task (?3.3).
language dict train dev devtest test
Bengali 16k 539k 63k 61k 69k
6k 20k 914 907 1k
Hindi 0 1,249k 67k 98k 74k
0 37k 1k 993 1k
Malayalam 410k 664k 61k 68k 70k
144k 29k 1k 1k 1k
Tamil 189k 747k 62k 53k 54k
69k 35k 1k 1k 1k
Telugu 106k 951k 52k 45k 49k
38k 43k 1k 916 1k
Urdu 253k 1,198k 67k 49k 42k
113k 33k 736 777 605
Table 4: Data set sizes for each language pair: words in
the first row, parallel sentences in the second. (The dictio-
naries contains short phrases in addition to words, which
accounts for the difference in dictionary word and line
counts.)
4 Experiments
In this section, we present experiments on the col-
lected data sets in order to quantify their perfor-
mance. The experiments aim to address the follow-
ing questions:
1. How well can we translate the test sets?
2. Do linguistically motivated translation models
improve translation results?
3. What is the effect of data quality onmodel qual-
ity?
4.1 Setup
A principal point of comparison in this paper is be-
tween Hiero grammars (Chiang, 2007) and SAMT
grammars (Zollmann and Venugopal, 2006), the lat-
ter of which make use of linguistic annotations to
improve nonterminal reordering. These grammars
were trained with the Thrax grammar extractor us-
ing its default settings, and translated using Joshua
(Weese et al, 2011). We tuned with minimum error-
rate training (Och, 2003) using Z-MERT (Zaidan,
2009) and present the mean BLEU score on test
data over three separate runs (Clark et al, 2011).
MBR reranking (Kumar and Byrne, 2004) was ap-
plied to Joshua?s 300-best (unique) output, and eval-
uation was conducted with case-insensitive BLEU
with four references.
The training data was produced by pairing a
source sentence with each of its four translations.
We also added the dictionaries to the training data.
We built five-gram language models from the target
side of the training data using interpolated Kneser-
Ney smoothing. We also experimented with a larger-
scale language model built from English Gigaword,
but, notably, found a drop of over a point in BLEU
score. This points forward to some of the difficul-
ties encountered with the lack of text normalization,
discussed in ?5.
4.2 Baseline translations
We begin by presenting BLEU scores for Hiero and
SAMT translations of each of the six Indian language
test sets (Table 5). For comparison purposes, we
also present BLEU scores from Google translations
of these languages (where available).
We observe that systems built with SAMT gram-
mars improve measurably above the Hiero models,
with the exception of Tamil and Telugu. As an ex-
ternal reference point, the Google baseline transla-
tion scores far surpass the results of any of our sys-
tems, but were likely constructed from much larger
datasets.
Table 6 lists some manually-selected examples of
405
language Hiero SAMT diff Google
Bengali 12.72 13.53 +0.81 20.01
Hindi 15.53 17.29 +1.76 25.21
Malayalam 13.72 14.28 +0.56 -
Tamil 9.81 9.85 +0.04 13.51
Telugu 12.46 12.61 +0.15 16.03
Urdu 19.53 20.99 +1.46 23.09
Table 5: BLEU scores translating into English (four ref-
erences). BLEU scores are the mean of three MERT runs.
the sorts of translations we obtained from our sys-
tems. While anecdotal and not characteristic of over-
all quality, together with the generally good BLEU
scores, these examples provide a measure of the abil-
ity to obtain good translations from this dataset.
4.3 Voted training data
We noted above the high variance in the quality of
the translations obtained on MTurk. For data col-
lection efforts, there is a question of how much time
and effort to invest in quality control, since it comes
at the expense of simply collecting more data. We
can either collect additional redundant translations
(to increase quality) or translate more foreign sen-
tences (to increase coverage).
To test this, we constructed two smaller datasets,
each making use of only one of the four translations
of each source sentence:
? Selected randomly
? Selected by choosing the translation that re-
ceived a plurality of the votes (?3.3), breaking
ties randomly (best)
We again included the dictionaries in the training
data (where available). Table 7 contains results on
the same test sets as before. These results do not
clearly indicate that quality control through redun-
dant translations are worth the extra expense. Novot-
ney and Callison-Burch (2010) had a similar finding
for crowdsourced transcriptions.
5 Further Analysis
The previous section has shown that reasonable
BLEU scores can be obtained from baseline transla-
tion systems built from these corpora. While trans-
lation quality is an issue (for example, very lit-
?????????? ????? ?????
in srilanka solar government
chola rule in sri lanka
in srilanka chozhas ruled
chola reign in sri lanka
Figure 5: An example of inconsistent orthography. Words
in bold are translations of the second Tamil word.
eral translations, etc), the previous section?s voted
dataset experiments suggest this is not one of the
most important issues to address.
In this section, we undertake a manual analysis of
the collected datasets to inform future work. There
are a number of issues that arise due to non-Roman
scripts, high-variance translation quality, and the rel-
atively small amount of training data.
5.1 Orthographic issues
Manual analysis demonstrates that inconsistencies
with orthography are a serious problem. An exam-
ple of this can be found in Figure 5, which contains
a set of translations of a Tamil sentence. In particu-
lar, the spelling of the Tamil word ????? has three
different realizations among the sentence?s transla-
tions. The discrepancy between zha and la is due
to phonetic variants (phonetic similarity may also
account for the word solar). This discrepancy is
present throughout the training and test data, where
the -la variant is preferred to -zha by about 6:1 (the
counts are 848 and 142, respectively).
In addition to mistakes potentially caused by for-
eign scripts, there are many mistakes that are sim-
ply spelling errors. Table 8 contains examples of
misspellings (along with their counts) in the train-
ing portion of the Urdu-English dataset. As a point
of comparison, there are no misspellings of the word
in Europarl.
Such errors are present in many collections, of
course, but they are particularly harmful in small
datasets, and they appear to be especially prevalent
in datasets like these, translated as they were by non-
native speakers. Whether caused by Turker care-
lessness or difficulty in translation from non-Roman
scripts, these are common issues, solutions for which
could yield significant improvement in translation
performance.
406
Bengali ?? ????? ???? ???? ???? ?????????????? ??????? ??? ?
Hiero in this time dhaka university was established on the year 1921 .
SAMT in this time dhaka university was established in 1921 .
Malayalam ????????? ???????????? ?????????? ? ?????? 5 , 700 ?k ?????? ??????????????? .
Hiero the surface temperature of sun 5 , 700 degree k to down to .
SAMT temperature in the surface of the sun 5 , 700 degree k to down to .
Table 6: Some example translations.
Hiero SAMT
language random best random best
Bengali 9.43 9.29 9.65 9.50
Hindi 11.74 12.18 12.61 12.69
Tamil 7.73 7.48 7.88 7.76
Telugu 10.49 10.61 10.75 10.72
Urdu 13.51 14.26 14.63 16.03
Table 7: BLEU scores translating into English on a quar-
ter of the training data (plus dictionary), selected in two
ways: best (result of vote), and random. There is little
difference, suggesting quality control may not be terribly
important. We did not collect votes for Malayalam.
misspelling count
japenese 91
japans 40
japenes 9
japenies 3
japeneses 3
japeneese 1
japense 1
Table 8: Misspellings of japanese (947) in the training
portion of the Urdu-English data, along with their counts.
5.2 Alignments
Inconsistent orthography fragments the training
data, exacerbating problems already present due to
morpohological richness. One place this is mani-
fested is during alignment, where different spellings
mask patterns from the standard alignment tech-
niques. We observe a large number of poor align-
ments, due to interactions among these problems,
as well as the small size of the training data, well-
documented alignment mistakes (such as garbage
collecting), and the divergent sentence structures. In
particular, it seems that the defacto alignment heuris-
tics may be particularly ill-suited to these language
pairs and data conditions. Figure 6 (top) contains an
example of a particularly poor alignment produced
by the default alignment heuristic, the grow-diag-
and method described in Koehn et al (2003).
As a means of testing this, we varied the align-
ment combination heuristics using five alternatives
described in Koehn et al (2003) and available in the
symal program distributed with Moses (Koehn et
al., 2007). Experiments on Tamil produce a range
of BLEU scores between 7.45 and 10.19 (each result
is the average of three MERT runs). If we plot gram-
mar size versus BLEU score, we observe a general
trend that larger grammars seem to positively cor-
relate with BLEU score. We tested this more gen-
erally across languages using the Berkeley aligner5
(Liang et al, 2006) instead of GIZA alignments, and
found a consistent increase in BLEU score for the
Hiero grammars, often putting them on par with the
original SAMT results (Table 9). Manual analysis
suggests that the Berkeley aligner produces fewer,
more reasonable-looking alignments than the Moses
heuristics (Figure 6). This suggest a fruitful ap-
proaches in revisiting assumptions underlying align-
ment heuristics.
6 Related Work
Crowdsourcing datasets has been found to be helpful
for many tasks in natural language processing. Ger-
mann (2001) showed that humans could perform sur-
prisingly well with very poor translations obtained
from non-expert translators, in part likely because
coarse-level translational adequacy is sufficient for
the tasks they evaluated. That work was also pitched
as a rapid resource acquisition task, meant to test our
ability to quickly build systems in emergency set-
tings. This work further demonstrates the ability to
quickly acquire training data for MT systems with
5code.google.com/p/berkeleyaligner/
407
X?
X X
?
X X
?
X
?
X X
X
?
?
X
?
?"#
$??'(
)?+
??./0
??3
???
.
a
a
s
a
i
w
a
s
t
h
e
f
i
r
s
t
s
u
c
c
e
s
s
f
u
l
l
m
o
v
i
e
f
o
r
a
j
i
t
h
k
u
m
a
r
.
?
X
?
?
?
?
?
X X
?
a
a
s
a
i
w
a
s
t
h
e
f
i
r
s
t
s
u
c
c
e
s
s
f
u
l
l
m
o
v
i
e
f
o
r
a
j
i
t
h
k
u
m
a
r
.
?"#
$??'(
)?+
??./0
??3
???
.
Figure 6: A bad Tamil alignment produced with the
grow-diag-and alignment combination heuristic (top); the
Berkeley aligner is better (bottom). A ? is a correct
guess, an X marks a false positive, and a ? denotes a false
negative. Hiero?s extraction heuristics yield 4 rules for
the top alignment and 16 for the bottom.
reasonable translation accuracy.
Closely related to our work here is that of Novot-
ney and Callison-Burch (2010), who showed that
transcriptions for training speech recognition sys-
tems could be obtained from Mechanical Turk with
near baseline recognition performance and at a sig-
nificantly lower cost. They also showed that redun-
dant annotation was not worthwhile, and suggested
that money was better spent obtaining more data.
Separately, Ambati and Vogel (2010) probed the
MTurk worker pool for workers capable of translat-
ing a number of low-resource languages, including
Hindi, Telugu, and Urdu, demonstrating that such
workers could be found and quantifying acceptable
grammar size
pair GIZA++ Berkeley BLEU gain
Bengali 15m 27m 13.54 +0.82
Hindi 34m 60m 16.47 +0.94
Malayalam 12m 27m 12.70 -1.02
Tamil 19m 30m 10.10 +0.29
Telugu 28m 46m 13.36 +0.90
Urdu 38m 58m 20.41 +0.88
Table 9: Hiero translation results using Berkeley align-
ments instead of GIZA++ heuristics. The gain columns
denotes improvements relative to the Hiero systems in Ta-
ble 5. In many cases (bold gains), the BLEU scores are
at or above even the SAMT models from that table.
wages and collection rates.
The techniques described here are similar to those
described in Zaidan and Callison-Burch (2011), who
showed that crowdsourcing with appropriate quality
controls could be used to produce professional-level
translations for Urdu-English translation. This pa-
per extends that work by applying their techniques
to a larger set of Indian languages and scaling it to
training-data-set sizes.
7 Summary
We have described the collection of six parallel cor-
pora containing four-way redundant translations of
the source-language text. The Indian languages of
these corpora are low-resource and understudied,
and exhibit markedly different linguistic properties
compared to English. We performed baseline exper-
iments quantifying the translation performance of a
number of systems, investigated the effect of data
quality on model quality, and suggested a number of
approaches that could improve the quality of models
constructed from the datasets. The parallel corpora
provide a suite of SOV languages for translation re-
search and experiments.
Acknowledgments We thank Lexi Birch for dis-
cussions about strategies for selecting and assem-
bling the data sets. This research was supported in
part by gifts from Google and Microsoft, the Euro-
MatrixPlus project funded by the EuropeanCommis-
sion (7th Framework Programme), and a DARPA
grant entitled ?Crowdsourcing Translation?. The
views in this paper are the authors? alone.
408
References
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk, Los Angeles, California.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In ACL, pages 176?181. Association for Com-
putational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
NAACL, Boston, Massachusetts, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL, Sydney, Australia, July.
Ulrich Germann. 2001. Building a statistical ma-
chine translation system from scratch: how much
bang for the buck can we expect? In ACL work-
shop on Data-driven methods in machine translation,
Toulouse, France, July. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL, Edmonton, Alberta, Canada, May?June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ond?ej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, Prague, Czech Republic,
June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. InMachine translation
summit.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. NAACL, Boston, Massachusetts, USA, May.
M. Paul Lewis, editor. 2009. Ethnologue: Languages of
the World. SIL International, Dallas, TX, USA, six-
teenth edition.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL, pages 104?111.
Association for Computational Linguistics.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
fast and good enough: Automatic speech recognition
with non-expert transcription. In Proc. NAACL, Los
Angeles, California, June.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, Sapporo,
Japan, July.
JonathanWeese, Juri Ganitkevitch, Chris Callison-Burch,
Matt Post, and Adam Lopez. 2011. Joshua 3.0:
Syntax-based machine translation with the thrax gram-
mar extractor. InProceedings of the SixthWorkshop on
Statistical Machine Translation.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: professional quality from non-
professionals. In Proc. ACL, Portland, Oregon, USA,
June.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of arabic dialects. In Proc.
NAACL, Montreal, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, New York, New York, USA, June.
409
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Findings of the 2013 Workshop on Statistical Machine Translation
Ondr?ej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Chris Callison-Burch
University of Pennsylvania
Christian Federmann
Saarland University
Barry Haddow
University of Edinburgh
Philipp Koehn
University of Edinburgh
Christof Monz
University of Amsterdam
Matt Post
Johns Hopkins University
Radu Soricut
Google
Lucia Specia
University of Sheffield
Abstract
We present the results of the WMT13
shared tasks, which included a translation
task, a task for run-time estimation of ma-
chine translation quality, and an unoffi-
cial metrics task. This year, 143 machine
translation systems were submitted to the
ten translation tasks from 23 institutions.
An additional 6 anonymized systems were
included, and were then evaluated both au-
tomatically and manually, in our largest
manual evaluation to date. The quality es-
timation task had four subtasks, with a to-
tal of 14 teams, submitting 55 entries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2013. This workshop builds
on seven previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al, 2007, 2008,
2009, 2010, 2011, 2012).
This year we conducted three official tasks: a
translation task, a human evaluation of transla-
tion results, and a quality estimation task.1 In
the translation task (?2), participants were asked
to translate a shared test set, optionally restrict-
ing themselves to the provided training data. We
held ten translation tasks this year, between En-
glish and each of Czech, French, German, Span-
ish, and Russian. The Russian translation tasks
were new this year, and were also the most popu-
lar. The system outputs for each task were evalu-
ated both automatically and manually.
The human evaluation task (?3) involves ask-
ing human judges to rank sentences output by
anonymized systems. We obtained large numbers
of rankings from two groups: researchers (who
1The traditional metrics task is evaluated in a separate pa-
per (Macha?c?ek and Bojar, 2013).
contributed evaluations proportional to the number
of tasks they entered) and workers on Amazon?s
Mechanical Turk (who were paid). This year?s ef-
fort was our largest yet by a wide margin; we man-
aged to collect an order of magnitude more judg-
ments than in the past, allowing us to achieve sta-
tistical significance on the majority of the pairwise
system rankings. This year, we are also clustering
the systems according to these significance results,
instead of presenting a total ordering over systems.
The focus of the quality estimation task (?6)
is to produce real-time estimates of sentence- or
word-level machine translation quality. This task
has potential usefulness in a range of settings, such
as prioritizing output for human post-editing, or
selecting the best translations from a number of
systems. This year the following subtasks were
proposed: prediction of percentage of word edits
necessary to fix a sentence, ranking of up to five al-
ternative translations for a given source sentence,
prediction of post-editing time for a sentence, and
prediction of word-level scores for a given trans-
lation (correct/incorrect and types of edits). The
datasets included English-Spanish and German-
English news translations produced by a number
of machine translation systems. This marks the
second year we have conducted this task.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation methodologies for machine trans-
lation. As before, all of the data, translations,
and collected human judgments are publicly avail-
able.2 We hope these datasets serve as a valu-
able resource for research into statistical machine
translation, system combination, and automatic
evaluation or prediction of translation quality.
2http://statmt.org/wmt13/results.html
1
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and five other lan-
guages: German, Spanish, French, Czech, and ?
new this year ? Russian. We created a test set for
each language pair by translating newspaper arti-
cles and provided training data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources. A total of 52
articles were selected, in roughly equal amounts
from a variety of Czech, English, French, German,
Spanish, and Russian news sites:3
Czech: aktua?lne?.cz (1), CTK (1), den??k (1),
iDNES.cz (3), lidovky.cz (1), Novinky.cz (2)
French: Cyber Presse (3), Le Devoir (1), Le
Monde (3), Liberation (2)
Spanish: ABC.es (2), BBC Spanish (1), El Peri-
odico (1), Milenio (3), Noroeste (1), Primera
Hora (3)
English: BBC (2), CNN (2), Economist (1),
Guardian (1), New York Times (2), The Tele-
graph (1)
German: Der Standard (1), Deutsche Welle (1),
FAZ (1), Frankfurter Rundschau (2), Welt (2)
Russian: AIF (2), BBC Russian (2), Izvestiya (1),
Rosbalt (1), Vesti (1)
The stories were translated by the professional
translation agency Capita, funded by the EU
Framework Programme 7 project MosesCore, and
by Yandex, a Russian search engine.4 All of the
translations were done directly, and not via an in-
termediate language.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
train language models, and development sets to
tune system parameters. Some training corpora
were identical from last year (Europarl5, United
Nations, French-English 109 corpus, CzEng),
some were updated (News Commentary, mono-
lingual data), and new corpora were added (Com-
mon Crawl (Smith et al, 2013), Russian-English
3For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
4http://www.yandex.com/
5As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
parallel data provided by Yandex, Russian-English
Wikipedia Headlines provided by CMU).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their en-
try names are listed in Table 1; each system did
not necessarily appear in all translation tasks. We
also included three commercial off-the-shelf MT
systems and three online statistical MT systems,6
which we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
We run the evaluation campaign using an up-
dated version of Appraise (Federmann, 2012); the
tool has been extended to support collecting judg-
ments using Amazon?s Mechanical Turk, replac-
ing the annotation system used in previous WMTs.
The software, including all changes made for this
year?s workshop, is available from GitHub.7
This year differs from prior years in a few im-
portant ways:
? We collected about ten times more judgments
that we have in the past, using judgments
from both participants in the shared task and
non-experts hired on Amazon?s Mechanical
Turk.
? Instead of presenting a total ordering of sys-
tems for each pair, we cluster them and report
a ranking over the clusters.
6Thanks to Herve? Saint-Amand and Martin Popel for har-
vesting these entries.
7https://github.com/cfedermann/Appraise
2
Europarl Parallel Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,965,734 2,007,723 1,920,209 646,605
Words 56,895,229 54,420,026 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 176,258 117,481 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 174,441 157,168 178,221 140,324 150,217
Words 5,116,388 4,520,796 4,928,135 4,066,721 4,597,904 4,541,058 3,206,423 3,507,249 3,841,950 4,008,949
Distinct words 84,273 61,693 69,028 58,295 142,461 61,761 138,991 54,270 145,997 57,991
Common Crawl Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 1,845,286 3,244,152 2,399,123 161,838 878,386
Words 49,561,060 46,861,758 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 710,755 640,778 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
Spanish? English French? English
Sentences 11,196,913 12,886,831
Words 318,788,686 365,127,098 411,916,781 360,341,450
Distinct words 593,567 581,339 565,553 666,077
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English
Sentences 514,859
Words 1,191,474 1,230,644
Distinct words 282,989 251,328
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,218,201 2,123,835 2,190,579 2,176,537 668,595
Words 59,848,044 60,476,282 63,439,791 53,534,167 14,946,399
Distinct words 123,059 181,837 145,496 394,781 172,461
News Language Model Data
English Spanish French German Czech Russian
Sentence 68,521,621 13,384,314 21,195,476 54,619,789 27,540,749 19,912,911
Words 1,613,778,461 386,014,234 524,541,570 983,818,841 456,271,247 351,595,790
Distinct words 3,392,137 1,163,825 1,590,187 6,814,953 2,655,813 2,195,112
News Test Set
English Spanish French German Czech Russian
Sentences 3000
Words 64,810 73,659 73,659 63,412 57,050 58,327
Distinct words 8,935 10,601 11,441 12,189 15,324 15,736
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
3
ID Institution
BALAGUR Yandex School of Data Analysis (Borisov et al, 2013)
CMU
CMU-TREE-TO-TREE
Carnegie Mellon University (Ammar et al, 2013)
CU-BOJAR,
CU-DEPFIX,
CU-TAMCHYNA
Charles University in Prague (Bojar et al, 2013)
CU-KAREL, CU-ZEMAN Charles University in Prague (B??lek and Zeman, 2013)
CU-PHRASEFIX,
CU-TECTOMT
Charles University in Prague (Galus?c?a?kova? et al, 2013)
DCU Dublin City University (Rubino et al, 2013a)
DCU-FDA Dublin City University (Bicici, 2013a)
DCU-OKITA Dublin City University (Okita et al, 2013)
DESRT Universita` di Pisa (Miceli Barone and Attardi, 2013)
ITS-LATL University of Geneva
JHU Johns Hopkins University (Post et al, 2013)
KIT Karlsruhe Institute of Technology (Cho et al, 2013)
LIA Universite? d?Avignon (Huet et al, 2013)
LIMSI LIMSI (Allauzen et al, 2013)
MES-* Munich / Edinburgh / Stuttgart (Durrani et al, 2013a; Weller et al, 2013)
OMNIFLUENT SAIC (Matusov and Leusch, 2013)
PROMT PROMT Automated Translations Solutions
QCRI-MES Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al, 2013)
QUAERO QUAERO (Peitz et al, 2013a)
RWTH RWTH Aachen (Peitz et al, 2013b)
SHEF University of Sheffield
STANFORD Stanford University (Green et al, 2013)
TALP-UPC TALP Research Centre (Formiga et al, 2013a)
TUBITAK TU?BI?TAK-BI?LGEM (Durgar El-Kahlout and Mermer, 2013)
UCAM University of Cambridge (Pino et al, 2013)
UEDIN,
UEDIN-HEAFIELD
University of Edinburgh (Durrani et al, 2013b)
UEDIN-SYNTAX University of Edinburgh (Nadejde et al, 2013)
UMD University of Maryland (Eidelman et al, 2013)
UU Uppsala University (Stymne et al, 2013)
COMMERCIAL-1,2,3 Anonymized commercial systems
ONLINE-A,B,G Anonymized online systems
Table 1: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
4
3.1 Ranking translations of sentences
The ranking among systems is produced by col-
lecting a large number of rankings between the
systems? translations. Every language task had
many participating systems (the largest was 19,
for the Russian-English task). Rather than asking
judges to provide a complete ordering over all the
translations of a source segment, we instead ran-
domly select five systems and ask the judge to rank
just those. We call each of these a ranking task.
A screenshot of the ranking interface is shown in
Figure 2.
For each ranking task, the judge is presented
with a source segment, a reference translation,
and the outputs of five systems (anonymized and
randomly-ordered). The following simple instruc-
tions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
The rankings of the systems are numbered from 1
to 5, with 1 being the best translation and 5 be-
ing the worst. Each ranking task has the potential
to provide 10 pairwise rankings, and fewer if the
judge chooses any ties. For example, the ranking
{A:1, B:2, C:4, D:3, E:5}
provides 10 pairwise rankings, while the ranking
{A:3, B:3, C:4, D:3, E:1}
provides just 7. The absolute value of the ranking
or the degree of difference is not considered.
We use the collected pairwise rankings to assign
each system a score that reflects how highly that
system was usually ranked by the annotators. The
score for some system A reflects how frequently it
was judged to be better than other systems when
compared on the same segment; its score is the
number of pairwise rankings where it was judged
to be better, divided by the total number of non-
tying pairwise comparisons. These scores were
used to compute clusters of systems and rankings
between them (?3.4).
3.2 Collecting the data
A goal this year was to collect enough data to
achieve statistical significance in the rankings. We
distributed the workload among two groups of
judges: researchers and Turkers. The researcher
group comprised partipants in the shared task, who
were asked to contribute judgments on 300 sen-
tences for each system they contributed. The re-
searcher evaluation was held over three weeks
from May 17?June 7, and yielded about 280k pair-
wise rankings.
The Turker group was composed of non-expert
annotators hired on Amazon?s Mechanical Turk
(MTurk). A basic unit of work on MTurk is called
a Human Intelligence Task (HIT) and included
three ranking tasks, for which we paid $0.25. To
ensure that the Turkers provided high quality an-
notations, this portion of the evaluation was be-
gun after the researcher portion had completed,
enabling us to embed controls in the form of high-
consensus pairwise rankings in the Turker HITs.
To build these controls, we collected ranking tasks
containing pairwise rankings with a high degree of
researcher consensus. An example task is here:
SENTENCE 504
SOURCE Vor den heiligen Sta?tten verbeugen
REFERENCE Let?s worship the holy places
SYSTEM A Before the holy sites curtain
SYSTEM B Before we bow to the Holy Places
SYSTEM C To the holy sites bow
SYSTEM D Bow down to the holy sites
SYSTEM E Before the holy sites pay
MATRIX
A B C D E
A - 0 0 0 3
B 5 - 0 1 5
C 6 6 - 0 6
D 6 8 5 - 6
E 0 0 0 0 -
Matrix entry Mi,j records the number of re-
searchers who judged System i to be better than
System j. We use as controls pairwise judgments
for which |Mi,j?Mj,i| > 5, i.e., judgments where
the researcher consensus ran strongly in one direc-
tion. We rejected HITs from Turkers who encoun-
tered at least 10 of these controls and failed more
than 50% of them.
There were 463 people who participated in the
Turker portion of the manual evaluation, contribut-
ing 664k pairwise rankings from Turkers who
passed the controls. Together with the researcher
judgments, we collected close to a million pair-
wise rankings, compared to 101k collected last
year: a ten-fold increase. Table 2 contains more
detail.
5
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly-ordered) and has to rank
these according to their translation quality, ties are allowed. For technical reasons, annotators on Amazon?s Mechanical Turk
received all three ranking tasks for a single HIT on a single page, one upon the other.
3.3 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960), which is de-
fined as
? = P (A)? P (E)1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. Note that ? is ba-
sically a normalized version of P (A), one which
takes into account how meaningful it is for anno-
tators to agree with each other, by incorporating
P (E). The values for ? range from 0 to 1, with
zero indicating no agreement and 1 perfect agree-
ment.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it should capture the probability
that two annotators would agree randomly. There-
fore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 3 gives ? values for inter-annotator agree-
ment for WMT11?WMT13 while Table 4 de-
tails intra-annotator agreement scores. Due to the
change of annotation software, we used a slightly
different way of computing annotator agreement
scores. Therefore, we chose to re-compute values
for previous WMTs to allow for a fair comparison.
The exact interpretation of the kappa coefficient is
difficult, but according to Landis and Koch (1977),
0?0.2 is slight, 0.2?0.4 is fair, 0.4?0.6 is moderate,
6
LANGUAGE PAIR Systems Rankings Average
Czech-English 11 85,469 7,769.91
English-Czech 12 102,842 8,570.17
German-English 17 128,668 7,568.71
English-German 15 77,286 5,152.40
Spanish-English 12 67,832 5,652.67
English-Spanish 13 60,464 4,651.08
French-English 13 80,741 6,210.85
English-French 17 100,783 5,928.41
Russian-English 19 151,422 7,969.58
English-Russian 14 87,323 6,237.36
Total 148 942,840 6,370.54
WMT12 103 101,969 999.69
WMT11 133 63,045 474.02
Table 2: Amount of data collected in the WMT13 manual evaluation. The final two rows report summary information from the
previous two workshops.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.400 0.311 0.244 0.342 0.279
English-Czech 0.460 0.359 0.168 0.408 0.075
German-English 0.324 0.385 0.299 0.443 0.324
English-German 0.378 0.356 0.267 0.457 0.239
Spanish-English 0.494 0.298 0.277 0.415 0.295
English-Spanish 0.367 0.254 0.206 0.333 0.249
French-English 0.402 0.272 0.275 0.405 0.321
English-French 0.406 0.296 0.231 0.434 0.237
Russian-English ? ? 0.278 0.315 0.324
English-Russian ? ? 0.243 0.416 0.207
Table 3: ? scores measuring inter-annotator agreement. The WMT13r and WMT13m columns provide breakdowns for re-
searcher annotations and MTurk annotations, respectively. See Table 4 for corresponding intra-annotator agreement scores.
0.6?0.8 is substantial, and 0.8?1.0 is almost per-
fect. We find that the agreement rates are more or
less the same as in prior years.
The WMT13 column contains both researcher
and Turker annotations at a roughly 1:2 ratio. The
final two columns break out agreement numbers
between these two groups. The researcher agree-
ment rates are similar to agreement rates from past
years, while the Turker agreement are well below
researcher agreement rates, varying widely, but of-
ten comparable to WMT11 and WMT12. Clearly,
researchers are providing us with more consistent
opinions, but whether these differences are ex-
plained by Turkers racing through jobs, the partic-
ularities that inform researchers judging systems
they know well, or something else, is hard to tell.
Intra-annotator agreement scores are also on par
from last year?s level, and are often much better.
We observe better intra-annotator agreement for
researchers compared to Turkers.
As a small test, we varied the threshold of ac-
ceptance against the controls for the Turker data
alone and computed inter-annotator agreement
scores on the datasets for the Russian?English task
(the only language pair where we had enough data
at high thresholds). Table 5 shows that higher
thresholds do indeed give us better agreements,
but not monotonically. The increasing ?s sug-
gests that we can find a segment of Turkers who
do a better job and that perhaps a slightly higher
threshold of 0.6 would serve us better, while the
remaining difference against the researchers sug-
gests there may be different mindsets informing
the decisions. In any case, getting the best perfor-
mance out of the Turkers remains difficult.
3.4 System Score
Given the multitude of pairwise comparisons, we
would like to rank the systems according to a
single score computed for each system. In re-
7
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.597 0.454 0.479 0.483 0.478
English-Czech 0.601 0.390 0.290 0.547 0.242
German-English 0.576 0.392 0.535 0.643 0.515
English-German 0.528 0.433 0.498 0.649 0.452
Spanish-English 0.574 1.000 0.575 0.605 0.537
English-Spanish 0.426 0.329 0.492 0.468 0.492
French-English 0.673 0.360 0.578 0.585 0.565
English-French 0.524 0.414 0.495 0.630 0.486
Russian-English ? ? 0.450 0.363 0.477
English-Russian ? ? 0.513 0.582 0.500
Table 4: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation. The WMT13r and WMT13m columns provide breakdowns for researcher annotations and MTurk annota-
tions, respectively. The perfect inter-annotator agreement for Spanish-English is a result of there being very little data for that
language pair.
thresh. rankings ?
0.5 16,605 0.234
0.6 9,999 0.337
0.7 3,219 0.360
0.8 1,851 0.395
0.9 849 0.336
Table 5: Agreement as a function of threshold for Turkers on
the Russian?English task. The threshold is the percentage of
controls a Turker must pass for her rankings to be accepted.
cent evaluation campaigns, we tweaked the metric
and now arrived at a intuitive score that has been
demonstrated to be accurate in ranking systems ac-
cording to their true quality (Koehn, 2012).
The score, which we call EXPECTED WINS, has
an intuitive explanation. If the system is compared
against a randomly picked opposing system, on a
randomly picked sentence, by a randomly picked
judge, what is the probability that its translation is
ranked higher?
Formally, the score for a system Si among a set
of systems {Sj} given a pool of pairwise rankings
summarized as win(A,B) ? the number of times
system A is ranked higher than system B ? is
defined as follows:
score(Si) = 1|{Sj}|
?
j,j 6=i
win(Si, Sj)
win(Si, Sj) + win(Sj , Si)
Note that this score ignores ties.
3.5 Rank Ranges and Clusters
Given the scores, we would like to rank the sys-
tems, which is straightforward. But we would also
like to know, if the obtained system ranking is
statistically significant. Typically, given the large
number of systems that participate, and the simi-
larity of the systems given a common training data
condition and often common toolsets, there will be
some systems that will be very close in quality.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute the expected wins score for each system
based on this sample, and rank each system. By
repeating this procedure a 1,000 times, we can de-
termine a range of ranks, into which system falls
at least 95% of the time (i.e., at least 950 times) ?
corresponding to a p-level of p ? 0.05.
Furthermore, given the rank ranges for each sys-
tem, we can cluster systems with overlapping rank
ranges.8
For all language pairs and all systems, Table 6
reports all system scores, rank ranges, and clus-
ters. The official interpretation of these results
is that systems in the same cluster are considered
tied. Given the large number of judgements that
we collected, it was possible to group on average
about two systems in a cluster, even though the
systems in the middle are typically in larger clus-
ters.
8Formally, given ranges defined by start(Si) and end(Si),
we seek the largest set of clusters {Cc} that satisfies:
?S ?C : S ? C
S ? Ca, S ? Cb ? Ca = Cb
Ca 6= Cb ? ?Si ? Ca, Sj ? Cb :
start(Si) > end(Sj) or start(Sj) > end(Si)
8
Czech-English
# score range system
1 0.607 1 UEDIN-HEAFIELD
2 0.582 2-3 ONLINE-B
0.573 2-4 MES
0.562 3-5 UEDIN
0.547 4-7 ONLINE-A
0.542 5-7 UEDIN-SYNTAX
0.534 6-7 CU-ZEMAN
8 0.482 8 CU-TAMCHYNA
9 0.458 9 DCU-FDA
10 0.321 10 JHU
11 0.297 11 SHEF-WPROA
English-Czech
# score range system
1 0.580 1-2 CU-BOJAR
0.578 1-2 CU-DEPFIX
3 0.562 3 ONLINE-B
4 0.525 4 UEDIN
5 0.505 5-7 CU-ZEMAN
0.502 5-7 MES
0.499 5-8 ONLINE-A
0.484 7-9 CU-PHRASEFIX
0.476 8-9 CU-TECTOMT
10 0.457 10-11 COMMERCIAL-1
0.450 10-11 COMMERCIAL-2
12 0.389 12 SHEF-WPROA
Spanish-English
# score range system
1 0.624 1 UEDIN-HEAFIELD
2 0.595 2 ONLINE-B
3 0.570 3-5 UEDIN
0.570 3-5 ONLINE-A
0.567 3-5 MES
6 0.537 6 LIMSI-SOUL
7 0.514 7 DCU
8 0.488 8-9 DCU-OKITA
0.484 8-9 DCU-FDA
10 0.462 10 CU-ZEMAN
11 0.425 11 JHU
12 0.169 12 SHEF-WPROA
English-Spanish
# rank range system
1 0.637 1 ONLINE-B
2 0.582 2-4 ONLINE-A
0.578 2-4 UEDIN
0.567 3-4 PROMT
5 0.535 5-6 MES
0.528 5-6 TALP-UPC
7 0.491 7-8 LIMSI
0.474 7-9 DCU
0.472 8-10 DCU-FDA
0.455 9-11 DCU-OKITA
0.446 10-11 CU-ZEMAN
12 0.417 12 JHU
13 0.324 13 SHEF-WPROA
German-English
# rank range system
1 0.660 1 ONLINE-B
2 0.620 2-3 ONLINE-A
0.608 2-3 UEDIN-SYNTAX
4 0.586 4-5 UEDIN
0.584 4-5 QUAERO
0.571 5-7 KIT
0.562 6-7 MES
8 0.543 8-9 RWTH-JANE
0.533 8-10 MES-REORDER
0.526 9-10 LIMSI-SOUL
11 0.480 11 TUBITAK
12 0.462 12-13 UMD
0.462 12-13 DCU
14 0.396 14 CU-ZEMAN
15 0.367 15 JHU
16 0.311 16 SHEF-WPROA
17 0.238 17 DESRT
English-German
# rank range system
1 0.637 1-2 ONLINE-B
0.636 1-2 PROMT
3 0.614 3 UEDIN-SYNTAX
0.587 3-5 ONLINE-A
0.571 4-6 UEDIN
0.554 5-6 KIT
7 0.523 7 STANFORD
8 0.507 8 LIMSI-SOUL
9 0.477 9-11 MES-REORDER
0.476 9-11 JHU
0.460 10-12 CU-ZEMAN
0.453 11-12 TUBITAK
13 0.361 13 UU
14 0.329 14-15 SHEF-WPROA
0.323 14-15 RWTH-JANE
English-Russian
# rank range system
1 0.641 1 PROMT
2 0.623 2 ONLINE-B
3 0.556 3-4 CMU
0.542 3-6 ONLINE-G
0.538 3-7 ONLINE-A
0.531 4-7 UEDIN
0.520 5-7 QCRI-MES
8 0.498 8 CU-KAREL
9 0.478 9-10 MES-QCRI
0.469 9-10 JHU
11 0.434 11-12 COMMERCIAL-3
0.426 11-13 LIA
0.419 12-13 BALAGUR
14 0.331 14 CU-ZEMAN
French-English
# rank range system
1 0.638 1 UEDIN-HEAFIELD
2 0.604 2-3 UEDIN
0.591 2-3 ONLINE-B
4 0.573 4-5 LIMSI-SOUL
0.562 4-5 KIT
0.541 5-6 ONLINE-A
7 0.512 7 MES-SIMPLIFIED
8 0.486 8 DCU
9 0.439 9-10 RWTH
0.429 9-11 CMU-T2T
0.420 10-11 CU-ZEMAN
12 0.389 12 JHU
13 0.322 13 SHEF-WPROA
English-French
# rank range system
1 0.607 1-2 UEDIN
0.600 1-3 ONLINE-B
0.588 2-4 LIMSI-SOUL
0.584 3-4 KIT
5 0.553 5-7 PROMT
0.551 5-8 STANFORD
0.547 5-8 MES
0.537 6-9 MES-INFLECTION
0.533 7-10 RWTH-PB
0.516 9-11 ONLINE-A
0.499 10-11 DCU
12 0.427 12 CU-ZEMAN
13 0.408 13 JHU
14 0.382 14 OMNIFLUENT
15 0.350 15 ITS-LATL
16 0.326 16 ITS-LATL-PE
Russian-English
# rank range system
1 0.657 1 ONLINE-B
2 0.604 2-3 CMU
0.588 2-3 ONLINE-A
4 0.562 4-6 ONLINE-G
0.561 4-6 PROMT
0.550 5-7 QCRI-MES
0.546 5-7 UCAM
8 0.527 8-9 BALAGUR
0.519 8-10 MES-QCRI
0.507 9-11 UEDIN
0.497 10-12 OMNIFLUENT
0.492 11-14 LIA
0.483 12-15 OMNIFLUENT-C
0.481 12-15 UMD
0.476 13-15 CU-KAREL
16 0.432 16 COMMERCIAL-3
17 0.417 17 UEDIN-SYNTAX
18 0.396 18 JHU
19 0.215 19 CU-ZEMAN
Table 6: Official results for the WMT13 translation task. Systems are ordered by the expected win score. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05. This method is also used to determine the
range of ranks into which system falls. Systems with grey background indicate use of resources that fall outside the constraints
provided for the shared task.
9
4 Understandability of English?Czech
For the English-to-Czech translation, we con-
ducted a variation of the ?understandability? test
as introduced in WMT09 (Callison-Burch et al,
2009) and used in WMT10. In order to obtain
additional reference translations, we conflated this
test with post-editing. The procedure was as fol-
lows:
1. Monolingual editing (also called blind edit-
ing). The first annotator is given just the MT
output and requested to correct it. Given er-
rors in MT outputs, some guessing of the
original meaning is often inevitable and the
annotators are welcome to try. If unable, they
can mark the sentences as incomprehensible.
2. Review. A second annotator is asked to
validate the monolingual edit given both the
source and reference translations. Our in-
structions specify three options:
(a) If the monolingual edit is an adequate
translation and acceptably fluent Czech,
confirm it without changes.
(b) If the monolingual edit is adequate but
needs polishing, modify the sentence
and prefix it with the label ?OK:?.
(c) If the monolingual edit is wrong, cor-
rect it. You may start from the origi-
nal unedited MT output, if that is eas-
ier. Avoid using the reference directly,
prefer words from MT output whenever
possible.
The motivation behind this procedure is that we
want to save the time necessary for reading the
sentence. If the reviewer has already considered
whether the sentence is an acceptable translation,
they do not need to read the MT output again in
order to post-edit it. Our approach is thus some-
what the converse of Aziz et al (2013) who ana-
lyze post-editing effort to obtain rankings of MT
systems. We want to measure the understandabil-
ity of MT outputs and obtain post-edits at the same
time.
Both annotation steps were carried out in
the CASMACAT/Matecat post-editing user inter-
face.9, modified to provide the relevant variants of
the sentence next to the main edit box. Screen-
shots of the two annotation phases are given in
Figure 3 and Figure 4.
9http://www.casmacat.eu/index.php?n=Workbench
Occurrence GOOD ALMOST BAD EMPTY Total
First 34.7 0.1 42.3 11.0 4082
Repeated 41.1 0.1 41.0 6.1 805
Overall 35.8 0.1 42.1 10.2 4887
Table 7: Distribution of review statuses.
Similarly to the traditional ranking task, we pro-
vided three consecutive sentences from the origi-
nal text, each translated with a different MT sys-
tem. The annotators are free to use this contex-
tual information when guessing the meaning or re-
viewing the monolingual edits. Each ?annotation
HIT? consists of 24 sentences, i.e. 8 snippets of 3
consecutive sentences.
4.1 Basic Statistics on Editing
In total, 21 annotators took part in the exercise, 20
of them contributed to monolingual editing and 19
contributed to the reviews.
Connecting each review with the monolingual
edit (some edits received multiple reviews), we ob-
tain one data row. We collected 4887 data rows
(i.e. sentence revisions) for 3538 monolingual ed-
its, covering 1468 source sentences as translated
by 12 MT systems (including the reference).
Not all MT systems were considered for each
sentence, we preferred to obtain judgments for
more source sentences.
Based on the annotation instructions, each data
row has one of the four possible statuses: GOOD,
ALMOST, BAD, and EMPTY. GOOD rows are
those where the reviewer accepted the monolin-
gual edit without changes, ALMOST edits were
modified by the reviewer but they were marked as
?OK?. BAD edits were changed by the reviewer
and no ?OK? mark was given. Finally, the sta-
tus EMPTY is assigned to rows where the mono-
lingual editor refused to edit the sentence. The
EMPTY rows nevertheless contain the (?regular?)
post-edit of the reviewer, so they still provide a
new reference translation for the sentence.
Table 7 summarizes the distribution of row sta-
tuses depending on one more significant distinc-
tion: whether the monolingual editor has seen the
sentence before or not. We see that EMPTY and
BAD monolingual edits together drop by about
6% absolute when the sentence is not new to the
monolingual editor. The occurrence is counted as
?repeated? regardless whether the annotator has
previously seen the sentence in an editing or re-
viewing task. Unless stated otherwise, we exclude
repeated edits from our calculations.
10
Figure 3: In this screen, the annotator is expected to correct the MT output given only the context of at most two neighbouring
machine-translated sentences.
ALMOST Pairwise
treated Comparisons Agreement ?
inter
separate 2690 56.0 0.270
as BAD 2690 67.9 0.351
as GOOD 2690 65.2 0.289
intra
separate 170 65.3 0.410
as BAD 170 69.4 0.386
as GOOD 170 71.8 0.422
Table 8: Annotator agreement when reviewing monolingual
edits.
4.2 Agreement on Understandability
Before looking at individual system results, we
consider annotator agreement in the review step.
Details are given in Table 8. Given a (non-
EMPTY) string from a monolingual edit, we
would like to know how often two acceptability
judgments by two different reviewers (inter-) or
the same reviewer (intra-) agree. The repeated ed-
its remain in this analysis because we are not in-
terested in the origin of the string.
Our annotation setup leads to three possible la-
bels: GOOD, ALMOST, and BAD. The agree-
ment on one of three classes is bound to be lower
than the agreement on two classes, so we also re-
interpret ALMOST as either GOOD or BAD. Gen-
erally speaking, ALMOST is a positive judgment,
so it would be natural to treat it as GOOD. How-
ever, in our particular setup, when the reviewer
modified the sentence and forgot to add the label
?OK:?, the item ended up in the BAD class. We
conclude that this is indeed the case: the inter-
annotator agreement appears higher if ALMOST
is treated as BAD. Future versions of the review-
ing interface should perhaps first ask for the yes/no
judgment and only then allow to post-edit.
The ? values in Table 8 are the Fleiss?
kappa (Fleiss, 1971), accounting for agreement by
chance given the observed label distributions.
In WMT09, the agreements for this task were
higher: 77.4 for inter-AA and 86.6 for intra-AA.
(In 2010, the agreements for this task were not re-
ported.) It is difficult to say whether the differ-
ence lies in the particular language pair, the dif-
ferent set of annotators, or the different user in-
terface for our reviewing task. In 2009 and 2010,
the reviewers were shown 5 monolingual edits at
once and they were asked to judge each as accept-
able or not acceptable. We show just one segment
and they have probably set their minds on the post-
editing rather than acceptability judgment. We be-
lieve that higher agreements can be reached if the
reviewers first validate one or more of the edits and
only then are allowed to post-edit it.
4.3 Understandability of English?Czech
Table 9 brings about the first main result of our
post-editing effort. For each system (including
the reference translation), we check how often a
monolingual edit was marked OK or ALMOST
by the subsequent reviewer. The average under-
standability across all MT systems into Czech is
44.2?1.6%. This is a considerable improvement
compared to 2009 where the best systems pro-
duced about 32% understandable sentences. In
11
Figure 4: In this screen, the annotator is expected to validate the monolingual edit, correcting it if necessary. The annotator is
expected to add the prefix ?OK:? if the correction was more or less cosmetic.
Rank System Total Observations % Understandable
Overall incl. ref. 4082 46.7?1.6
Overall without ref. 3808 44.2?1.6
1 Reference 274?31 80.3?4.8
2-6 CU-ZEMAN 348?34 51.7?5.1
2-6 UEDIN 332?33 51.5?5.4
2-6 ONLINE-B 337?34 50.7?5.3
2-6 CU-BOJAR 341?35 50.7?5.2
2-7 CU-DEPFIX 350?34 48.0?5.3
6-10 COMMERCIAL-2 358?36 43.6?5.2
6-11 COMMERCIAL-1 316?34 41.5?5.5
7-12 CU-TECTOMT 338?34 39.4?5.2
8-12 MES 346?36 38.4?5.2
8-12 CU-PHRASEFIX 394?40 38.1?4.8
10-12 SHEF-WPROA 348?32 34.2?5.1
2009 Reference 91
2009 Best System 32
2010 Reference 97
2010 Best System 58
Table 9: Understandability of English?Czech systems. The
? values indicate empirical confidence bounds at 95%. Rank
ranges were also obtained in the same resampling: in 95% of
observations, the system was ranked in the given range.
2010, the best systems or system combinations
reached 55%?58%. The test set across years and
the quality of references and judgments also play a
role. In our annotation setup, the references appear
to be correctly understandable only to 80.3?4.8%.
To estimate the variance of these results due
to the particular sentences chosen, we draw 1000
random samples from the dataset, preserving the
dataset size and repeating some. The exact num-
ber of judgments per system can thus vary. We
report the 95% empirical confidence interval after
the ??? signs in Table 9 (the systems range from
?4.8 to?5.5). When we drop individual blind ed-
itors or reviewers, the understandability judgments
differ by about ?2 to ?4. In other words, the de-
pendence on the test set appears higher than the
dependence on the annotators.
The limited size of our dataset alows us only
to separate two main groups of systems: those
ranking 2?6 and those ranking worse. This rough
grouping vaguely matches with WMT13 ranking
results as given in Table 6. A somewhat surpris-
ing observation is that two automatic corrections
ranked better in WMT13 ranking but score worse
in understandability: CU-DEPFIX fixes some lost
negation and some agreement errors of CU-BOJAR
and CU-PHRASEFIX is a standard statistical post-
editing of a transfer-based system CU-TECTOMT.
A detailed inspection of the data is necessary to
explain this.
5 More Reference Translations for Czech
Our annotation procedure described in Section 4
allowed us to obtain a considerable number of ad-
ditional reference translations on top of official
single reference.
12
Refs 1 2 3 4 5 6 7 8 9 10-16
Sents 233 709 174 123 60 48 40 27 25 29
Table 10: Number of source sentences with the given number
of distinct reference translations.
In total, our edits cover 1468 source sentences,
i.e. about a half of the official test set size, and pro-
vide 4311 unique references. On average, one sen-
tence in our set has 2.94?2.17 unique reference
translations. Table 10 provides a histogram.
It is well known that automatic MT evalua-
tion methods perform better with more references,
because a single one may not confirm a correct
part of MT output. This issue is more severe
for morphologically rich languages like Czech
where about 1/3 of MT output was correct but not
confirmed by the reference (Bojar et al, 2010).
Advanced evaluation methods apply paraphras-
ing to smooth out some of the lexical divergence
(Kauchak and Barzilay, 2006; Snover et al, 2009;
Denkowski and Lavie, 2010). Simpler techniques
such as lemmatizing are effective for morphologi-
cally rich languages (Tantug et al, 2008; Kos and
Bojar, 2009) but they will lose resolution once the
systems start performing generally well.
WMTs have taken the stance that a big enough
test set with just a single reference should compen-
sate for the lack of other references. We use our
post-edited reference translations to check this as-
sumption for BLEU and NIST as implemented in
mteval-13a (international tokenization switched
on, which is not the default setting).
We run many probes, randomly picking the test
set size (number of distinct sentences) and the
number of distinct references per sentence. Note
that such test sets are somewhat artificially more
diverse; in narrow domains, source sentences can
repeat and even appear verbatim in the training
data, and in natural test sets with multiple refer-
ences, short sentences can receive several identical
translations.
For each probe, we measure the Spearman?s
rank correlation coefficient ? of the ranks pro-
posed by BLEU or NIST and the manual ranks.
We use the same implementation as applied in the
WMT13 Shared Metrics Task (Macha?c?ek and Bo-
jar, 2013). Note that the WMT13 metrics task still
uses the WMT12 evaluation method ignoring ties,
not the expected wins. As Koehn (2012) shows,
the two methods do not differ much.
Overall, the correlation is strongly impacted by
Figure 5: Correlation of BLEU and WMT13 manual ranks
for English?Czech translation
Figure 6: Correlation of NIST and WMT13 manual ranks
for English?Czech translation
the particular choice of test sentences and refer-
ence translations. By picking sentences randomly,
similarly or equally sized test sets can reach dif-
ferent correlations. Indeed, e.g. for a test set of
about 1500 distinct sentences selected from the
3000-sentence official test set (1 reference trans-
lation), we obtain correlations for BLEU between
0.86 and 0.94.
Figure 5 plots the correlations of BLEU and the
system rankings, Figure 6 provides the same pic-
ture for NIST. The upper triangular part of the plot
contains samples from our post-edited reference
translations, the lower rectangular part contains
probes from the official test set of 3000 sentences
with 1 reference translation.
To interpret the observations, we also calculate
the average and standard deviation of correlations
for each cell in Figures 5 and 6. Figures 7 and
8 plot the values for 1, 6, 7 and 8 references for
13
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
B
L
E
U
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 7: Projections from Figure 5 of BLEU and WMT13
manual ranks for English?Czech translation
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
N
I
S
T
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 8: Projections from Figure 6 of NIST and WMT13
manual ranks for English?Czech translation
BLEU and NIST, resp. The projections confirm
that the average correlations grow with test set
size, the growth is however sub-logarithmic.
Starting from as few as a dozen of sentences, we
see that using more references is better than using
a larger test set. For BLEU, we however already
seem to reach false positives at 7 references for
one or two hundred sentences: larger sets with just
one reference may correlate slightly better.
Using one reference obtained by post-editing
seems better than using the official (independent)
reference translations. BLEU is more affected
than NIST by this difference even at relatively
large test set size. Note that our post-edits are in-
spired by all MT systems, the good as well as the
bad ones. This probably provides our set with a
certain balance.
Overall, the best balance between the test set
size and the number of references seems to lie
somewhere around 7 references and 100 or 200
sentences. Creating such a test set could be even
cheaper than the standard 3000 sentences with just
one reference. However, the wide error bars re-
mind us that even this setting can lead to correla-
tions anywhere between 0.86 and 0.96. For other
languages, data sets types or other MT evaluation
methods, the best setting can be quite different and
has to be sought for.
6 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
second edition of the WMT shared task on qual-
ity estimation builds on the previous edition of the
task (Callison-Burch et al, 2012), with variants to
this previous task, including both sentence-level
and word-level estimation, with new training and
test datasets, along with evaluation metrics and
baseline systems.
The motivation to include both sentence- and
word-level estimation come from the different po-
tential applications of these variants. Some inter-
esting uses of sentence-level quality estimation are
the following:
? Decide whether a given translation is good
enough for publishing as is.
? Inform readers of the target language only
whether or not they can rely on a translation.
? Filter out sentences that are not good enough
for post-editing by professional translators.
? Select the best translation among options
from multiple MT and/or translation memory
systems.
Some interesting uses of word-level quality es-
timation are the following:
? Highlight words that need editing in post-
editing tasks.
? Inform readers of portions of the sentence
which are not reliable.
? Select the best segments among options from
multiple translation systems for MT system
combination.
The goals of this year?s shared task were:
14
? To explore various granularity levels for the
task (sentence-level and word-level).
? To explore the prediction of more objective
scores such as edit distance and post-editing
time.
? To explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics in the task of ranking alternative
translations generated by different MT sys-
tems.
? To identify new and effective quality indica-
tors (features) for all variants of the quality
estimation task.
? To identify effective machine learning tech-
niques for all variants of the quality estima-
tion task.
? To establish the state of the art performance
in the field.
Four subtasks were proposed, as we discuss in
Sections 6.1 and 6.2. Each subtask provides spe-
cific datasets, annotated for quality according to
the subtask (Section 6.3), and evaluates the system
submissions using specific metrics (Section 6.6).
When available, external resources (e.g. SMT
training corpus) and translation engine-related re-
sources were given to participants (Section 6.4),
who could also use any additional external re-
sources (no distinction between open and close
tracks is made). Participants were also provided
with a software package to extract quality esti-
mation features and perform model learning (Sec-
tion 6.5), with a suggested list of baseline features
and learning method (Section 6.7). Participants
could submit up to two systems for each subtask.
6.1 Sentence-level Quality Estimation
Task 1.1 Predicting Post-editing Distance This
task is similar to the quality estimation task in
WMT12, but with one important difference in the
scoring variant: instead of using the post-editing
effort scores in the [1-5] range, we use HTER
(Snover et al, 2006) as quality score. This score
is to be interpreted as the minimum edit distance
between the machine translation and its manually
post-edited version, and its range is [0, 1] (0 when
no edit needs to be made, and 1 when all words
need to be edited). Two variants of the results
could be submitted in the shared task:
? Scoring: A quality score for each sentence
translation in [0,1], to be interpreted as an
HTER score; lower scores mean better trans-
lations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning). The reference ranking is de-
fined based on the true HTER scores.
Task 1.2 Selecting Best Translation This task
consists in ranking up to five alternative transla-
tions for the same source sentence produced by
multiple MT systems. We use essentially the same
data provided to participants of previous years
WMT?s evaluation metrics task ? where MT eval-
uation metrics are assessed according to how well
they correlate with human rankings. However, ref-
erence translations produced by humans are not be
used in this task.
Task 1.3 Predicting Post-editing Time For this
task systems are required to produce, for each
translation, the expected time (in seconds) it
would take a translator to post-edit such an MT
output. The main application for predictions of
this type is in computer-aided translation where
the predicted time can be used to select among dif-
ferent hypotheses or even to omit any MT output
in cases where no good suggestion is available.
6.2 Word-level Quality Estimation
Based on the data of Task 1.3, we define Task 2, a
word-level annotation task for which participants
are asked to produce a label for each token that
indicates whether the word should be changed by
a post-editor or kept in the final translation. We
consider the following two sets of labels for pre-
diction:
? Binary classification: a keep/change label,
the latter meaning that the token should be
corrected in the post-editing process.
? Multi-class classification: a label specifying
the edit action that should be performed on
the token (keep as is, delete, or substitute).
6.3 Datasets
Task 1.1 Predicting post-editing distance For
the training of models, we provided the WMT12
15
quality estimation dataset: 2,254 English-
Spanish news sentences extracted from previous
WMT translation task English-Spanish test sets
(WMT09, WMT10, and WMT12). These were
translated by a phrase-based SMT Moses system
trained on Europarl and News Commentaries cor-
pora as provided by WMT, along with their source
sentences, reference translations, post-edited
translations, and HTER scores. We used TERp
(default settings: tokenised, case insensitive,
etc., but capped to 1)10 to compute the HTER
scores. Likert scores in [1,5] were also provided,
as participants may choose to use them for the
ranking variant.
As test data, we use a subset of the WMT13
English-Spanish news test set with 500 sentences,
whose translations were produced by the same
SMT system used for the training set. To com-
pute the true HTER labels, the translations were
post-edited under the same conditions as those on
the training set. As in any blind shared task, the
HTER scores were solely used to evaluate the sub-
missions, and were only released to participants
after they submitted their systems.
A few variations of the training and test data
were provided, including a version with cases re-
stored and a version detokenized. In addition,
we provided a number of engine-internal informa-
tion from Moses for glass-box feature extraction,
such as phrase and word alignments, model scores,
word graph, n-best lists and information from the
decoder?s search graph.
Task 1.2 Selecting best translation As training
data, we provided a large set of up to five alter-
native machine translations produced by different
MT systems for each source sentence and ranked
for quality by humans. This was the outcome of
the manual evaluation of the translation task from
WMT09-WMT12. It includes two language pairs:
German-English and English-Spanish, with 7,098
and 4,592 source sentences and up to five ranked
translations, totalling 32,922 and 22,447 transla-
tions, respectively.
As test data, a set of up to five alternative ma-
chine translations per source sentence from the
WMT08 test sets was provided, with 365 (1,810)
and 264 (1,315) source sentences (translations)
for German-English and English-Spanish, respec-
tively. We note that there was some overlap be-
tween the MT systems used in the training data
10http://www.umiacs.umd.edu/?snover/terp/
and test datasets, but not all systems were the
same, as different systems participate in WMT
over the years.
Task 1.3 and Task 2 Predicting post-editing
time and word-level edits For Tasks 1.3 and 2
we provides a new dataset consisting of 22 English
news articles which were translated into Span-
ish using Moses and post-edited during a CAS-
MACAT11 field trial. Of these, 15 documents have
been processed repeatedly by at least 2 out of 5
translators, resulting in a total of 1,087 segments.
For each segment we provided:
? English source and Spanish translation.
? Spanish MT output which was used as basis
for post-editing.
? Document and translator ID.
? Position of the segment within the document.
The metadata about translator and document was
made available as we expect that translator perfor-
mance and normalisation over document complex-
ity can be helpful when predicting the time spend
on a given segment.
For the training portion of the data we also pro-
vided:
? Time to post-edit in seconds (Task 1.3).
? Binary (Keep, Change) and multiclass (Keep,
Substitute, Delete) labels on word level along
with explicit tokenization (Task 2).
The labels in Task 2 are derived by comput-
ing WER between the original machine translation
and its post-edited version.
6.4 Resources
For all tasks, we provided resources to extract
quality estimation features when these were avail-
able:
? The SMT training corpus (WMT News and
Europarl): source and target sides of the cor-
pus used to train the SMT engines for Tasks
1.1, 1.3, and 2, and truecase models gener-
ated from these. These corpora can also be
used for Task 1.2, but we note that some of
the MT systems used in the datasets of this
task were not statistical or did not use (only)
the training corpus provided by WMT.
11http://casmacat.eu/
16
? Language models: n-gram language models
of source and target languages generated us-
ing the SMT training corpora and standard
toolkits such as SRILM Stolcke (2002), and
a language model of POS tags for the target
language. We also provided unigram, bigram
and trigram counts.
? IBM Model 1 lexical tables generated by
GIZA++ using the SMT training corpora.
? Phrase tables with word alignment informa-
tion generated by scripts provided by Moses
from the parallel corpora.
? For Tasks 1.1, 1.3 and 2, the Moses config-
uration file used for decoding or the code to
re-run the entire Moses system.
? For Task 1.1, both English and Spanish re-
sources for a number of advanced features
such as pre-generated PCFG parsing models,
topic models, global lexicon models and mu-
tual information trigger models.
We refer the reader to the QUEST website12 for
a detailed list of resources provided for each task.
6.5 QUEST Framework
QUEST (Specia et al, 2013) is an open source
framework for quality estimation which provides a
wide variety of feature extractors from source and
translation texts and external resources and tools.
These range from simple, language-independent
features, to advanced, linguistically motivated fea-
tures. They include features that rely on informa-
tion from the MT system that generated the trans-
lations (glass-box features), and features that are
oblivious to the way translations were produced
(black-box features).
QUEST also integrates a well-known machine
learning toolkit, scikit-learn,13 and other algo-
rithms that are known to perform well on this task
(e.g. Gaussian Processes), providing a simple and
effective way of experimenting with techniques
for feature selection and model building, as well
as parameter optimisation through grid search.
From QUEST, a subset of 17 features and an
SVM regression implementation were used as
baseline for Tasks 1.1, 1.2 and 1.3. The software
was made available to all participants.
12http://www.quest.dcs.shef.ac.uk/
13http://scikit-learn.org/
6.6 Evaluation Metrics
Task 1.1 Predicting post-editing distance
Evaluation is performed against the HTER and/or
ranking of translations using the same metrics as
in WMT12. For the scoring variant of the task,
we use two standard metrics for regression tasks:
Mean Absolute Error (MAE) as a primary metric,
and Root of Mean Squared Error (RMSE) as a
secondary metric. To improve readability, we
report these error numbers by first mapping the
HTER values to the [0, 100] interval, to be read
as percentage-points of the HTER metric. For a
given test set S with entries si, 1 ? i ? |S|, we
denote by H(si) the proposed score for entry si
(hypothesis), and by V (si) the reference value for
entry si (gold-standard value):
MAE =
?N
i=1 |H(si)? V (si)|
|S|
RMSE =
??N
i=1(H(si)? V (si))2
|S|
Both these metrics are non-parametric, auto-
matic and deterministic (and therefore consistent),
and extrinsically interpretable. For instance, a
MAE value of 10 means that, on average, the ab-
solute difference between the hypothesized score
and the reference score value is 10 percentage
points (i.e., 0.10 difference in HTER scores). The
interpretation of RMSE is similar, with the differ-
ence that RMSE penalises larger errors more (via
the square function).
For the ranking variant of the task, we use the
DeltaAvg metric proposed in the 2012 edition of
the task (Callison-Burch et al, 2012) as our main
metric. This metric assumes that each reference
test instance has an extrinsic number associated
with it that represents its ranking with respect to
the other test instances. For completeness, we
present here again the definition of DeltaAvg.
The goal of the DeltaAvg metric is to measure
how valuable a proposed ranking (which we call a
hypothesis ranking) is, according to the true rank-
ing values associated with the test instances. We
first define a parametrised version of this metric,
called DeltaAvg[n]. The following notations are
used: for a given entry sentence s, V (s) represents
the function that associates an extrinsic value to
that entry; we extend this notation to a set S, with
V (S) representing the average of all V (s), s ? S.
17
Intuitively, V (S) is a quantitative measure of the
?quality? of the set S, as induced by the extrinsic
values associated with the entries in S. For a set
of ranked entries S and a parameter n, we denote
by S1 the first quantile of set S (the highest-ranked
entries), S2 the second quantile, and so on, for n
quantiles of equal sizes.14 We also use the nota-
tion Si,j = ?jk=i Sk. Using these notations, wedefine:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1 ? V (S)
When the valuation function V is clear from the
context, we write DeltaAvg[n] for DeltaAvgV [n].
The parameter n represents the number of quan-
tiles we want to split the set S into. For instance,
n = 2 gives DeltaAvg[2] = V (S1)?V (S), hence it
measures the difference between the quality of the
top quantile (top half) S1 and the overall quality
(represented by V (S)). For n = 3, DeltaAvg[3] =
(V (S1)+V (S1,2)/2?V (S) = ((V (S1)?V (S))+
(V (S1,2?V (S)))/2, hence it measures an average
difference across two cases: between the quality of
the top quantile (top third) and the overall quality,
and between the quality of the top two quantiles
(S1 ? S2, top two-thirds) and the overall quality.
In general, DeltaAvg[n] measures an average dif-
ference in quality across n ? 1 cases, with each
case measuring the impact in quality of adding an
additional quantile, from top to bottom. Finally,
we define:
DeltaAvgV =
?N
n=2 DeltaAvgV [n]
N ? 1
where N = |S|/2. As before, we write DeltaAvg
for DeltaAvgV when the valuation function V is
clear from the context. The DeltaAvg metric is an
average across all DeltaAvg[n] values, for those
n values for which the resulting quantiles have at
least 2 entries (no singleton quantiles).
We present results for DeltaAvg using as valu-
ation function V the HTER scores, as defined in
Section 6.3. We also use Spearman?s rank correla-
tion coefficient ? as a secondary metric.
Task 1.2 Selecting best translation The perfor-
mance on the task of selecting the best transla-
tion from a pool of translation candidates is mea-
14If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
sured by comparing proposed (hypothesis) rank-
ings against human-produced rankings. The met-
ric used is Kendall?s ? rank correlation coefficient,
computed as follows:
? = |concordant pairs| ? |discordant pairs||total pairs|
where a concordant pair is a pair of two transla-
tions for the same source segment in which the
ranking order proposed by a human annotator and
the ranking order of the hypothesis agree; in a dis-
cordant pair, they disagree. The possible values of
? range between 1 (where all pairs are concordant)
and ?1 (where all pairs are discordant). Thus a
system with ranking predictions having a higher
? value makes predictions that are more similar
to human judgements than a system with ranking
predictions having a lower ? . Note that, in general,
being able to predict rankings with an accuracy
of ? = ?1 is as difficult as predicting rankings
with an accuracy of ? = 1, whereas a completely
random ranking would have an expected value of
? = 0. The range is therefore said to be symmet-
ric.
However, there are two distinct ways of mea-
suring rank correlation using Kendall?s ? , related
to the way ties are treated. They greatly affect how
Kendall?s ? numbers are to be interpreted, and es-
pecially the symmetry property. We explain the
difference in detail in what follows.
Kendall?s ? with ties penalised If the goal is
to measure to what extent the difference in qual-
ity visible to a human annotator has been captured
by an automatically produced hypothesis (recall-
oriented view), then proposing a tie between t1
and t2 (t1-equal-to-t2) when the pair was judged
(in the reference) as t1-better-than-t2 is treated as
a failure-to-recall. In other words, it is as bad as
proposing t1-worse-than-t2. Henceforth, we call
this recall-oriented measure ?Kendall?s ? with ties
penalised?. This metric has the following proper-
ties:
? it is completely fair when comparing differ-
ent methods to produce ranking hypotheses,
because the denominator (number of total
pairs) is the same (it is the number of non-
tied pairs under the human judgements).
? it is non-symmetric, in the sense that a value
of ? = ?1 is not as difficult to obtain as ? =
18
1 (simply proposing only ties gets a ? = ?1);
hence, the sign of the ? value matters.
? the expected value of a completely random
ranking is not necessarily ? = 0, but rather
depends on the number of ties in the refer-
ence rankings (i.e., it is test set dependent).
Kendall?s ? with ties ignored If the goal
is to measure to what extent the difference in
quality signalled by an automatically produced
hypothesis is reflected in the human annota-
tion (precision-oriented view), then proposing t1-
equal-to-t2 when the pair was judged differently
in the reference does no harm the metric.
Henceforth, we call this precision-oriented
measure ?Kendall?s ? with ties ignored?. This
metric has the following properties:
? it is not completely fair when comparing dif-
ferent methods to produce ranking hypothe-
ses, because the denominator (number of to-
tal pairs) may not be the same (it is the num-
ber of non-tied pairs under each system?s pro-
posal).
? it is symmetric, in the sense that a value of
? = ?1 is as difficult to obtain as ? = 1;
hence, the sign of the ? value may not mat-
ter. 15
? the expected value of a completely random
ranking is ? = 0 (test-set independent).
The first property is the most worrisome from
the perspective of reporting the results of a shared
task, because a system may fare very well on this
metric simply because it choses not to commit
(proposes ties) most of the time. Therefore, to
give a better understanding of the systems? perfor-
mance, for Kendall?s ? with ties ignored we also
provide the number of non-ties proposed by each
system.
Task 1.3 Predicting post-editing time Submis-
sions are evaluated in terms of Mean Average Er-
ror (MAE) against the actual time spent by post-
editors (in seconds). By using a linear error mea-
sure we limit the influence of outliers: sentences
that took very long to edit or where the measure-
ment taken is questionable.
15In real life applications this distinction matters. Even
if, from a computational perspective, it is as hard to get ?
close to?1 as it is to get it close to 1, knowing the sign is the
difference between selecting the best or the worse translation.
To further analyse the influence of extreme val-
ues, we also compute Spearman?s rank correlation
? coefficient which does not depend on the abso-
lute values of the predictions.
We also give RMSE and Pearson?s correlation
coefficient r for reference.
Task 2 Predicting word-level scores The word-
level task is primarily evaluated by macro-
averaged F-measure. Because the class distribu-
tion is skewed ? in the test data about one third
of the tokens are marked as correct ? we compute
precision and recall and F1 for each class individ-
ually. Consider the following confusion matrix for
the two classes Keep and Change:
predicted
(K)eep (C)hange
expected (K)eep 10 20(C)hange 30 40
For the given example we derive true-positive
(tp), true-negative (tn), false-positive (fp), and
false-negative (fn) counts:
tpK = 10 fpK = 30 fnK = 20
tpC = 40 fpC = 20 fnC = 30
precisionK =
tpK
tpK + fpK
= 10/40
recallK =
tpK
tpK + fnK
= 10/30
F1,K =
2 ? precisionK ? recallK
precisionK +recallK
A single cumulative statistic can be computed
by averaging the resulting F-measures (macro av-
eraging) or by micro averaging in which case pre-
cision and recall are first computed by accumulat-
ing the relevant values for all classes (O?zgu?r et al,
2005), e.g.
precision = tpK + tpC(tpK + fpK) + (tpC + fpC)
The latter gives equal weight to each exam-
ple and is therefore dominated by performance on
the largest class while macro-averaged F-measure
gives equal weight to each class.
The same setup is used to evaluate the perfor-
mance in the multiclass setting. Please note that
here the test data only contains 4% examples for
class (D)elete.
19
ID Participating team
CMU Carnegie Mellon University, USA (Hildebrand and Vogel, 2013)
CNGL Centre for Next Generation Localization, Ireland (Bicici, 2013b)
DCU Dublin City University, Ireland (Almaghout and Specia, 2013)
DCU-SYMC Dublin City University & Symantec, Ireland (Rubino et al, 2013b)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis and
Popovic, 2013)
FBK-UEdin Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de
Souza et al, 2013)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al, 2013)
LIMSI Laboratoire d?Informatique pour la Me?canique et les Sciences de l?Inge?nieur,
France (Singh et al, 2013)
LORIA Lorraine Laboratory of Research in Computer Science and its Applications,
France (Langlois and Smaili, 2013)
SHEF University of Sheffield, UK (Beck et al, 2013)
TCD-CNGL Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013)
TCD-DCU-CNGL Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and
Rubino, 2013)
UMAC University of Macau, China (Han et al, 2013)
UPC Universitat Politecnica de Catalunya, Spain (Formiga et al, 2013b)
Table 11: Participants in the WMT13 Quality Estimation shared task.
6.7 Participants
Table 11 lists all participating teams submitting
systems to any subtask in this shared task. Each
team was allowed up to two submissions for each
subtask. In the descriptions below participation in
specific tasks is denoted by a task identifier: T1.1,
T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.3):
QUEST was used to extract 17 system-
independent features from the source and
translation files and the SMT training cor-
pus that were found to be relevant in previous
work (same features as in the WMT12 shared
task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? Language model probability of source
and target sentences using language
models provided by the task.
? average number of translations per
source word in the sentence: as given
by IBM 1 model thresholded so that
P (t|s) > 0.2, and so that P (t|s) > 0.01
weighted by the inverse frequency of
each word in the source side of the SMT
training corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source side of the
SMT training corpus
? percentage of unigrams in the source
sentence seen in the source side of the
SMT training corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within the
SCIKIT-LEARN toolkit. The ?,  and C pa-
rameters were optimized using a grid-search
and 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as a ?baseline?, it is in fact a strong
system. For tasks of the same type as 1.1
and 1.3, it has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting post-editing effort, as it
has also been shown in the previous edition
of the task (Callison-Burch et al, 2012).
The same features could be useful for a base-
line system for Task 1.2. In our official re-
20
sults, however, the baseline for Task 1.2 is
simpler than that: it proposes random ranks
for each pair of alternative translations for a
given source sentence, as we will discuss in
Section 6.8.
CMU (T1.1, T1.2, T1.3): The CMU quality
estimation system was trained on features
based on language models, the MT sys-
tem?s distortion model and phrase table fea-
tures, statistical word lexica, several sentence
length statistics, source language word and
bi-gram frequency statistics, n-best list agree-
ment and diversity, source language parse,
source-target word alignment and a depen-
dency parse based cohesion penalty. These
features were extracted using GIZA++, a
forced alignment algorithm and the Stanford
parser (de Marneffe et al, 2006). The pre-
diction models were trained using four clas-
sifiers in the Weka toolkit (Hall et al, 2009):
linear regression, M5P trees, multi layer per-
ceptron and SVM regression. In addition to
main system submission, a classic n-best list
re-ranking approach was used for Task 1.2.
CNGL (T1.1, T1.2, T1.3, T2): CNGL systems
are based on referential translation machines
(RTM) (Bic?ici and van Genabith, 2013), par-
allel feature decay algorithms (FDA) (Bicici,
2013a), and machine translation performance
predictor (MTPP) (Bic?ici et al, 2013), all
of which allow to obtain language and MT
system-independent predictions. For each
task, RTM models were developed using the
parallel corpora and the language model cor-
pora distributed by the WMT13 translation
task and the language model corpora pro-
vided by LDC for English and Spanish.
The sentence-level features are described in
MTPP (Bic?ici et al, 2013); they include
monolingual or bilingual features using n-
grams defined over text or common cover
link (CCL) (Seginer, 2007) structures as the
basic units of information over which sim-
ilarity calculations are made. RTMs use
308 features about coverage and diversity,
IBM1, and sentence translation performance,
retrieval closeness and minimum Bayes re-
trieval risk, distributional similarity and en-
tropy, IBM2 alignment, character n-grams,
and sentence readability. The learning mod-
els are Support Vector Machines (SVR) and
SVR with partial least squares (SVRPLS).
The word-level features include CCL links,
word length, location, prefix, suffix, form,
context, and alignment, totalling 511K fea-
tures for binary classification, and 637K for
multiclass classification. Generalised lin-
ear models (GLM) (Collins, 2002) and GLM
with dynamic learning (GLMd) were used.
DCU (T1.2): The main German-English submis-
sion uses six Combinatory Categorial Gram-
mar (CCG) features: CCG supertag lan-
guage model perplexity and log probability,
the number of maximal CCG constituents in
the translation output which are the highest-
probability minimum number of CCG con-
stituents that span the translation output, the
percentage of CCG argument mismatches be-
tween each subsequent CCG supertags, the
percentage of CCG argument mismatches be-
tween each subsequent CCG maximal cate-
gories and the minimum number of phrases
detected in the translation output. A second
submission uses the aforementioned CCG
features combined with 80 features from
QUEST as described in (Specia, 2011). For
the CCG features, the C&C parser was used
to parse the translation output. Moses was
used to build the phrase table from the SMT
training corpus with maximum phrase length
set to 7. The language model of supertags
was built using the SRILM toolkit. As learn-
ing algorithm, Logistic Regression as pro-
vided by the SCIKIT-LEARN toolkit was used.
The training data was prepared by converting
each ranking of translation outputs to a set
of pairwise comparisons according to the ap-
proach proposed by Avramidis et al (2011).
The rankings were generated back from pair-
wise comparisons predicted by the model.
DCU-SYMC (T1.1): The DCU-Symantec team
employed a wide set of features which in-
cluded language model, n-gram counts and
word-alignment features as well as syntac-
tic features, topic model features and pseudo-
reference features. The main learning algo-
rithm was SVR, but regression tree learning
was used to perform feature selection, re-
ducing the initial set of 442 features to 96
features (DCU-Symantec alltypes) and 134
21
(DCU-Symantec combine). Two methods
for feature selection were used: a best-first
search in the feature space using regression
trees to evaluate the subsets, and reading bi-
narised features directly from the nodes of
pruned regression trees.
The following NLP tools were used in feature
extraction: the Brown English Wall-Street-
Journal-trained statistical parser (Charniak
and Johnson, 2005), a Lexical Functional
Grammar parser (XLE), together with a
hand-crafted Lexical Functional Grammar,
the English ParGram grammar (Kaplan et al,
2004), and the TreeTagger part-of-speech
tagger (Schmidt, 1994) with off-the-shelf
publicly available pre-trained tagging mod-
els for English and Spanish. For pseudo-
reference features, the Bing, Moses and Sys-
tran translation systems were used. The Mal-
let toolkit (McCallum, 2002) was used to
build the topic models and features based on
a grammar checker were extracted with Lan-
guageTool.16
DFKI (T1.2, T1.3): DFKI?s submission for Task
1.2 was based on decomposing rankings into
pairs (Avramidis, 2012), where the best sys-
tem for each pair was predicted with Lo-
gistic Regression (LogReg). For German-
English, LogReg was trained with Stepwise
Feature Selection (Hosmer, 1989) on two
feature sets: Feature Set 24 includes ba-
sic counts augmented with PCFG parsing
features (number of VPs, alternative parses,
parse probability) on both source and tar-
get sentences (Avramidis et al, 2011), and
pseudo-reference METEOR score; the most
successful set, Feature Set 33 combines those
24 features with the 17 baseline features. For
English-Spanish, LogReg was used with L2
Regularisation (Lin et al, 2007) and two fea-
ture sets were devised after scoring features
with ReliefF (Kononenko, 1994) and Infor-
mation Gain (Hunt et al, 1966). Feature Set
431 combines 30 features with highest abso-
lute Relief-F and Information Gain (15 from
each). features with the highest
Task 1.3 was modelled using feature sets
selected after Relief-F scoring of external
black-box and glass-box features extracted
16http://www.languagetool.org/
from the SMT decoding process. The most
successful submission (linear6) was trained
with Linear Regression including the 17 fea-
tures with highest positive Relief-F. Most
prominent features include the alternative
possible parses of the source and target sen-
tence, the positions of the phrases with the
lowest and highest probability and future
cost estimate in the translation, the counts of
phrases in the decoding graph whose prob-
ability or whether the future cost estimate
is higher/lower than their standard deviation,
counts of verbs and determiners, etc. The
second submission (pls8) was trained with
Partial Least Squares regression (Stone and
Brooks, 1990) including more glass-box fea-
tures.
FBK-Uedin (T1.1, T1.3):
The submissions explored features built on
MT engine resources including automatic
word alignment, n-best candidate translation
lists, back-translations and word posterior
probabilities. Information about word align-
ments is used to extract quantitative (amount
and distribution of the alignments) and qual-
itative (importance of the aligned terms) fea-
tures under the assumption that alignment
information can help tasks where sentence-
level semantic relations need to be identified
(Souza et al, 2013). Three similar English-
Spanish systems are built and used to provide
pseudo-references (Soricut et al, 2012) and
back-translations, from which automatic MT
evaluation metrics could be computed and
used as features.
All features were computed over a concatena-
tion of several publicly available parallel cor-
pora for the English-Spanish language pair
such as Europarl, News Commentary, and
MultiUN. The models were developed using
supervised learning algorithms: SVMs (with
feature selection step prior to model learning)
and extremely randomized trees.
LIG (T2): The LIG systems are designed to
deal with both binary and multiclass variants
of the word level task. They integrate sev-
eral features including: system-based (graph
topology, language model, alignment con-
text, etc.), lexical (Part-of-Speech tags), syn-
tactic (constituent label, distance to the con-
22
stituent tree root) and semantic (target and
source polysemy count). Besides the exist-
ing components of the SMT system, feature
extraction requires further external tools and
resources, such as: TreeTagger (for POS tag-
ging), Bekerley Parser trained with AnCora
treebank (for generating constituent trees in
Spanish), WordNet and BabelNet (for pol-
ysemy count), Google Translate. The fea-
ture set is then combined and trained using
a Conditional Random Fields (CRF) learn-
ing method. During the labelling phase, the
optimal threshold is tuned using a small de-
velopment set split from the original training
set. In order to retain the most informative
features and eliminate the redundant ones, a
Sequential Backward Selection algorithm is
employed over the all-feature systems. With
the binary classifier, the Boosting technique
is applied to allow a number of sub feature
sets to complement each other, resulting in
the ?stronger? combined system.
LIMSI (T1.1, T1.3): The two tasks were treated
as regression problems using a simple elas-
tic regression, a linear model trained with L1
and L2 regularisers. Regarding features, the
submissions mainly aimed at evaluating the
usefulness for quality estimation of n-gram
posterior probabilities (Gispert et al, 2013)
that quantify the probability for a given n-
gram to be part of the system output. Their
computation relies on all the hypotheses con-
sidered by a SMT system during decoding:
intuitively, the more hypotheses a n-gram ap-
pears in, the more confident the system is
that this n-gram is part of the correct trans-
lation, and the higher its posterior probabil-
ity is. The feature set contains 395 other fea-
tures that differs, in two ways, from the tra-
ditional features used in quality estimation.
First, it includes several features based on
large span continuous space language mod-
els (Le et al, 2011) that have already proved
their efficiency both for the translation task
and the quality estimation task. Second, each
feature was expanded into two ?normalized
forms? in which their value was divided ei-
ther by the source length or the target length
and, when relevant, into a ?ratio form? in
which the feature value computed on the tar-
get sentence is divided by its value computed
in the source sentence.
LORIA (T1.1): The system uses the 17 baseline
features, plus several numerical and boolean
features computed from the source and target
sentences (Langlois et al, 2012). These are
based on language model information (per-
plexity, level of back-off, intra-lingual trig-
gers), translation table (IBM1 table, inter-
lingual triggers). For language models, for-
ward and backward models are built. Each
feature gives a score to each word in the sen-
tence, and the score of the sentence is the av-
erage of word scores. For several features,
the score of a word depends on the score of its
neighbours. This leads to 66 features. Sup-
port Vector Machines are used to learn a re-
gression model. In training is done in a multi-
stage procedure aimed at increasing the size
of the training corpus. Initially, the train-
ing corpus with machine translated sentences
provided by the task is used to train an SVM
model. Then this model is applied to the post-
edited and reference sentences (also provided
as part of the task). These are added to the
quality estimation training corpus using as la-
bels the SVM predictions. An algorithm to
tune the predicted scores on a development
corpus is used.
SHEF (T1.1, T1.3): These submissions use
Gaussian Processes, a non-parametric prob-
abilistic learning framework for regression,
along with two techniques to improve predic-
tion performance and minimise the amount
of resources needed for the problem: feature
selection based on optimised hyperparame-
ters and active learning to reduce the training
set size (and therefore the annotation effort).
The initial set features contains all black box
and glass box features available within the
QUEST framework (Specia et al, 2013) for
the dataset at hand (160 in total for Task 1.1,
and 80 for Task 1.3). The query selection
strategy for active learning is based on the
informativeness of the instances using Infor-
mation Density, a measure that leverages be-
tween the variance among instances and how
dense the region (in the feature space) where
the instance is located is. To perform fea-
ture selection, following (Shah et al, 2013)
features are ranked by the Gaussian Process
23
algorithm according to their learned length
scales, which can be interpreted as the rel-
evance of such feature for the model. This
information was used for feature selection
by discarding the lowest ranked (least use-
ful) ones. based on empirical results found
in (Shah et al, 2013), the top 25 features for
both models were selected and used to retrain
the same regression algorithm.
UPC (T1.2): The methodology used a broad set
of features, mainly available through the last
version of the Asiya toolkit for MT evalua-
tion (Gonza`lez et al, 2012)17. Concretely,
86 features were derived for the German-to-
English and 97 features for the English-to-
Spanish tasks. These features cover differ-
ent approaches and include standard qual-
ity estimation features, as provided by the
above mentioned Asiya and QUEST toolk-
its, but also a variety of features based on
pseudo-references, explicit semantic analy-
sis and specialised language models trained
on the parallel and monolingual corpora pro-
vided by the WMT Translation Task.
The system selection task is approached by
means of pairwise ranking decisions. It uses
Random Forest classifiers with ties, expand-
ing the work of 402013cFormiga et al), from
which a full ranking can be derived and the
best system per sentence is identified. Once
the classes are given by the Random Forest,
one can build a graph by means of the adja-
cency matrix of the pairwise decision. The fi-
nal ranking is assigned through a dominance
scheme similar to Pighin et al (2012).
An important remark of the methodology is
the feature selection process, since it was no-
ticed that the learner was sensitive to the fea-
tures used. Selecting the appropriate set of
features was crucial to achieve a good per-
formance. The best feature combination was
composed of: i) a baseline quality estimation
feature set (Asiya or Quest) but not both of
them, ii) Length Model, iii) Pseudo-reference
aligned based features, and iv) adapted lan-
guage models. However, within the de-en
task, substituting Length Model and Aligned
Pseudo-references by the features based on
17http://asiya.lsi.upc.edu/
Semantic Roles could bring marginally bet-
ter accuracy.
TCD-CNGL (T1.1) and TCD-DCU-CNGL
(T1.3): The system is based on features
which are commonly used for style classifi-
cation (e.g. author identification). The as-
sumption is that low/high quality translations
can be characterised by some patterns which
are frequent and/or differ significantly from
the opposite category. Such features are in-
tended to focus on striking patterns rather
than to capture the global quality in a sen-
tence, but they are used in conjunction with
classical features for quality estimation (lan-
guage modelling, etc.). This requires two
steps in the training process: first the refer-
ence categories against which sentences will
be compared are built, then the standard qual-
ity estimation model training stage is per-
formed. Both datasets (Tasks 1.1 and 1.3)
were used for both tasks. Since the number
of features can be very high (up to 65,000),
a combination of various heuristics for se-
lecting features was used before the training
stage (the submitted systems were trained us-
ing SVM with RBF kernels).
UMAC (T1.1, T1.2, T2): For Task 1.1, the fea-
ture set consists in POS sequences of the
source and target languages, using 12 uni-
versal tags that are common in both lan-
guages. The algorithm is an enhanced ver-
sion of the BLEU metric (EBLEU) designed
with a modified length penalty and added re-
call factor, and having the precision and re-
call components grouped using the harmonic
mean. For Task 1.2, in addition to the uni-
versal POS sequences of the source and tar-
get languages, features include the scores of
length penalty, precision, recall and rank.
Variants of EBLEU with different strategies
for alignment are used, as well as a Na??ve
Bayes classification algorithm. For Task 2,
the features used are unigrams (from previous
4th to following 3rd tokens), bigrams (from
previous 2nd to following 2nd tokens), skip
bigrams (previous and next token), trigrams
(from previous 2nd to following 2nd tokens).
The learning algorithms are Conditional Ran-
dom Fields and Na??ve Bayes.
24
6.8 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing distance
Table 12 summarises the results for the ranking
variant of the task. They are sorted from best to
worse using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are CNGL SVRPLS, with a
DeltaAvg score of 11.09, and DCU-SYMC all-
types, with a DeltaAvg score of 10.13. While the
former holds the higher score, the difference is not
significant at the p ? 0.05 level as estimated by a
bootstrap resampling test.
Both submissions are better than the baseline
system by a very wide margin, a larger relative im-
provement than that obtained in the corresponding
WMT12 task. In addition, five submissions (out
of 12 systems) scored significantly higher than the
baseline system (systems above the middle gray
area), which is a larger proportion than that in last
year?s task (only 3 out of 16 systems), indicat-
ing that this shared task succeeded in pushing the
state-of-the-art performance to new levels.
In addition to the performance of the official
submission, we report results obtained by two or-
acle methods: the gold-label HTER metric com-
puted against the post-edited translations as ref-
erence (Oracle HTER), and the BLEU metric (1-
BLEU to obtain the same range as HTER) com-
puted against the same post-edited translations as
reference (Oracle HBLEU). The ?Oracle HTER?
DeltaAvg score of 16.38 gives an upperbound in
terms of DeltaAvg for the test set used in this eval-
uation. It indicates that, for this set, the differ-
ence in post-editing effort between the top quality
quantiles and the overall quality is 16.38 on aver-
age. The oracle based on HBLEU gives a lower
DeltaAvg score, which is expected since HTER
was our actual gold label. However, it is still
significantly higher than the score of the winning
submission, which shows that there is significant
room for improvement even by the highest scor-
ing submissions.
The results for the scoring variant of the task
are presented in Table 13, sorted from best to
worse by using the MAE metric scores as primary
key and the RMSE metric scores as secondary key.
According to MAE scores, the winning submis-
sion is SHEF FS (MAE = 12.42), which uses fea-
ture selection and a novel learning algorithm for
the task, Gaussian Processes. The baseline sys-
tem is measured to have an MAE of 14.81, with
six other submissions having performances that
are not different from the baseline at a statisti-
cally significant level, as shown by the gray area
in the middle of Table 13). Nine submissions (out
of 16) scored significantly higher than the base-
line system (systems above the middle gray area),
a considerably higher proportion of submissions
as compared to last year (5 out of 19), which indi-
cates that this shared task also succeeded in push-
ing the state-of-the-art performance to new levels
in terms of absolute scoring. Only one (6%) sys-
tem scored significantly lower than the baseline,
as opposed to 8 (42%) in last year?s task.
For the sake of completeness, we also show or-
acles figures using the same methods as for the
ranking variant of the task. Here the lowerbound
in error (Oracle HTER) will clearly be zero, as
both MAE and RMSE are measured against the
same gold label used for the oracle computation.
?Oracle HBLEU? is also not indicative in this
case, as the although the values for the two metrics
(HTER and HBLEU) are within the same ranges,
they are not directly comparable. This explains the
larger MAE/RMSE figures for ?Oracle HBLEU?
than those for most submissions.
Task 1.2 Selecting the best translation
Below we present the results for this task for each
of the two Kendall?s ? flavours presented in Sec-
tion 6.6, for the German-English test set (Tables 14
and 16) and the English-Spanish test set (Tables 15
and 17). The results are sorted from best to worse
using each of the Kendall?s ? metric flavours.
For German-English, the winning submission is
DFKI?s logRegFss33 entry, for both Kendall?s ?
with ties penalised and ties ignored, with ? = 0.31
(since this submission has no ties, the two met-
rics give the same ? value). A trivial baseline that
proposes random ranks (with ties allowed) has a
Kendall?s ? with ties penalised of -0.12 (as this
metric penalises the system?s ties that were non-
ties in the reference), and a Kendall?s ? with ties
ignored of 0.08. Most of the submissions per-
formed better than this simple baseline. More in-
terestingly perhaps is the comparison between the
best submission and the performance by an ora-
25
System ID DeltaAvg Spearman ?
? CNGL SVRPLS 11.09 0.55
? DCU-SYMC alltypes 10.13 0.59
SHEF FS 9.76 0.57
CNGL SVR 9.88 0.51
DCU-SYMC combine 9.84 0.59
CMU noB 8.98 0.57
SHEF FS-AL 8.85 0.50
Baseline bb17 SVR 8.52 0.46
CMU full 8.23 0.54
LIMSI 8.15 0.44
TCD-CNGL open 6.03 0.33
TCD-CNGL restricted 5.85 0.31
UMAC 2.74 0.11
Oracle HTER 16.38 1.00
Oracle HBLEU 15.74 0.93
Table 12: Official results for the ranking variant of the WMT13 Quality Estimation Task 1.1. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test. Oracle results that use human-references are also shown for comparison purposes.
System ID MAE RMSE
? SHEF FS 12.42 15.74
SHEF FS-AL 13.02 17.03
CNGL SVRPLS 13.26 16.82
LIMSI 13.32 17.22
DCU-SYMC combine 13.45 16.64
DCU-SYMC alltypes 13.51 17.14
CMU noB 13.84 17.46
CNGL SVR 13.85 17.28
FBK-UEdin extra 14.38 17.68
FBK-UEdin rand-svr 14.50 17.73
LORIA inctrain 14.79 18.34
Baseline bb17 SVR 14.81 18.22
TCD-CNGL open 14.81 19.00
LORIA inctraincont 14.83 18.17
TCD-CNGL restricted 15.20 19.59
CMU full 15.25 18.97
UMAC 16.97 21.94
Oracle HTER 0.00 0.00
Oracle HBLEU (1-HBLEU) 16.85 19.72
Table 13: Official results for the scoring variant of the WMT13 Quality Estimation Task 1.1. The winning submission is
indicated by a ? (it is significantly better than the other submissions according to bootstrap resampling (10k times) with 95%
confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant level
according to the same test. Oracle results that use human-references are also shown for comparison purposes.
26
German-English System ID Kendall?s ? with ties penalised
? DFKI logRegFss33 0.31
DFKI logRegFss24 0.28
CNGL SVRPLSF1 0.17
CNGL SVRF1 0.17
DCU CCG 0.15
UPC AQE+SEM+LM 0.11
UPC AQE+LeM+ALGPR+LM 0.10
DCU baseline+CCG 0.00
Baseline Random-ranks-with-ties -0.12
UMAC EBLEU-I -0.39
UMAC NB-LPR -0.49
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.19
Oracle BLEU (margin 0.01) 0.05
Oracle METEOR-ex (margin 0.00) 0.23
Oracle METEOR-ex (margin 0.01) 0.06
Table 14: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties penalised
? CNGL SVRPLSF1 0.15
CNGL SVRF1 0.13
DFKI logRegL2-411 0.09
DFKI logRegL2-431 0.04
UPC QQE+LeM+ALGPR+LM -0.03
UPC AQE+LeM+ALGPR+LM -0.06
CMU BLEUopt -0.11
Baseline Random-ranks-with-ties -0.23
UMAC EBLEU-A -0.27
UMAC EBLEU-I -0.35
CMU cls -0.63
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.17
Oracle BLEU (margin 0.02) -0.06
Oracle METEOR-ex (margin 0.00) 0.19
Oracle METEOR-ex (margin 0.02) 0.05
Table 15: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
27
German-English System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? DFKI logRegFss33 0.31 882/882
DFKI logRegFss24 0.28 882/882
UPC AQE+SEM+LM 0.27 768/882
UPC AQE+LeM+ALGPR+LM 0.24 788/882
DCU CCG 0.18 862/882
CNGL SVRPLSF1 0.17 882/882
CNGL SVRF1 0.17 881/882
Baseline Random-ranks-with-ties 0.08 718/882
DCU baseline+CCG 0.01 874/882
UMAC NB-LPR 0.01 447/882
UMAC EBLEU-I -0.03 558/882
Oracle Human 1.00 882/882
Oracle BLEU (margin 0.00) 0.22 859/882
Oracle BLEU (margin 0.01) 0.27 728/882
Oracle METEOR-ex (margin 0.00) 0.20 869/882
Oracle METEOR-ex (margin 0.01) 0.24 757/882
Table 16: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? CMU cls 0.23 192/633
CNGL SVRPLSF1 0.16 632/633
CNGL SVRF1 0.13 631/633
DFKI logRegL2-411 0.13 610/633
UPC QQE+LeM+ALGPR+LM 0.11 554/633
UPC AQE+LeM+ALGPR+LM 0.08 554/633
UMAC EBLEU-A 0.07 430/633
DFKI logRegL2-431 0.04 633/633
Baseline Random-ranks-with-ties 0.03 507/633
UMAC EBLEU-I 0.02 407/633
CMU BLEUopt -0.11 633/633
Oracle Human 1.00 633/633
Oracle BLEU (margin 0.00) 0.19 621/633
Oracle BLEU (margin 0.02) 0.26 474/633
Oracle METEOR-ex (margin 0.00) 0.25 623/633
Oracle METEOR-ex (margin 0.02) 0.28 517/633
Table 17: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
28
cle method that has access to human-created refer-
ences. This oracle uses human references to com-
pute BLEU and METEOR scores for each trans-
lation segment, and consequently computes rank-
ings for the competing translations based on these
scores. To reflect the impact of ties on the two
versions of Kendall?s ? metric we use, we allow
these ranks to be tied if the difference between the
oracle BLEU or METEOR scores is smaller than
a margin (see lower section of Tables 14 and 16,
with margins of 0 and 0.01 for the scores). For ex-
ample, under a regime of BLEU with margin 0.01,
a translation with BLEU score of 0.172 would get
the same rank as a translation with BLEU score of
0.164 (difference of 0.008), but a higher rank than
a translation with BLEU score of 0.158 (difference
of 0.014). Not surprisingly, under the Kendall?s
? with ties penalised the best Oracle BLEU or
METEOR performance happens for a 0.0 mar-
gin (which makes ties possible only for exactly-
matching scores), for a value of ? = 0.19 and
? = 0.23, respectively. Under the Kendall?s ? with
ties ignored, the Oracle BLEU performance for a
0.01 margin (i.e, translations under 1 BLEU point
should be considered as having the same rank)
achieves ? = 0.27, while Oracle METEOR for a
0.01 margin achieves ? = 0.24. These values are
lower than the ? = 0.31 of the winning submis-
sion without access to reference translations, sug-
gesting that quality estimation models are capable
of better modelling translation differences com-
pared to traditional, human reference-based MT
evaluation metrics.
For English-Spanish, under Kendall?s ? with
ties penalised the winning submission is CNGL?s
SVRPLSF1, with ? = 0.15. Under Kendall?s ?
with ties ignored, the best scoring submission is
CMU?s cls with ? = 0.23, but this is achieved
by offering non-tie judgements only for 192 of the
633 total judgements (30% of them). As we dis-
cussed in Section 6.6, the ?Kendall?s ? with ties
ignored? metric is weak with respect to compar-
ing different submissions, since it favours systems
that are do not commit to a given rank and rather
produce a large number of ties. This becomes even
clearer when we look at the performance of the or-
acle methods (Tables 15 and 17). Under Kendall?s
? with ties penalised, ?Oracle BLEU? (margin
0.00) achieves ? = 0.17, while under Kendall?s
? with ties ignored, ?Oracle BLEU? (margin 0.02)
has a ? = 0.26. This results in 474 non-tie deci-
sions (75% of them), and a better ? value com-
pared to ?Oracle BLEU? (margin 0.00), with a
? = 0.19 under the same metric. The oracle values
for both BLEU and METEOR are close to the ?
values of the winning submissions, supporting the
conclusion that quality estimation techniques can
successfully replace traditional, human reference-
based MT evaluation metrics.
Task 1.3 Predicting post-editing time
Results for this task are presented in Table 18.
A third of the submissions was able to beat the
baseline. Among these FBK-UEDIN?s submission
ranked best in terms of MAE, our main metric for
this task, and also achieved the lowest RMSE.
Only three systems were able to beat our base-
line in terms of MAE. Please note that while all
features were available to the participants, our
baseline is actually a competitive system.
The second-best entry, CNGL SVR, reached
the highest Spearman?s rank correlation, our sec-
ondary metric. Furthermore, in terms of this met-
ric all four top-ranking entries, two by CNGL and
FBK-UEDIN respectively, are significantly better
than the baseline (10k bootstrap resampling test
with 95% confidence intervals). As high ranking
submissions also yield strong rank correlation to
the observed post-editing time, we can be confi-
dent that improvements in MAE are not only due
to better handling of extreme cases.
Many participants submitted two variants of
their systems with different numbers of features
and/or machine learning approaches. In Table 18
we can see these are grouped closely together giv-
ing rise to the assumption that the general pool of
available features and thereby the used resources
and strongest features are most relevant for a sys-
tem?s performance. Another hint in that direction
is the observation the top-ranked systems rely on
additional data and resources to generate their fea-
tures.
Task 2 Predicting word-level scores
Results for this task are presented in Table 19 and
20, sorted by macro average F1. Since this is a
new task, we have yet to establish a strong base-
line. For reference we provide a trivial baseline
that predicts the dominant class ? (K)eep ? for ev-
ery token.
The first observation in Table 19 is that this triv-
ial baseline is difficult to beat in terms of accuracy.
However, considering our main metric ? macro-
29
System ID MAE RMSE Pearson?s r Spearman?s ?
? FBK-UEDIN Extra 47.5 82.6 0.65 0.75
? FBK-UEDIN Rand-SVR 47.9 86.7 0.66 0.74
CNGL SVR 49.2 90.4 0.67 0.76
CNGL SVRPLS 49.6 86.6 0.68 0.74
CMU slim 51.6 84.7 0.63 0.68
Baseline bb17 SVR 51.9 93.4 0.61 0.70
DFKI linear6 52.4 84.3 0.64 0.68
CMU full 53.6 92.2 0.58 0.60
DFKI pls8 53.6 88.3 0.59 0.67
TCD-DCU-CNGL SVM2 55.8 98.9 0.47 0.60
TCD-DCU-CNGL SVM1 55.9 99.4 0.48 0.60
SHEF FS 55.9 103.1 0.42 0.61
SHEF FS-AL 64.6 99.1 0.57 0.60
LIMSI elastic 70.6 114.4 0.58 0.64
Table 18: Official results for the Task 1.3 of the WMT13 Quality Estimation shared-task. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test.
Keep Change
System ID Accuracy Prec. Recall F1 Prec. Recall F1 Macro F1
? LIG FS BIN 0.74 0.79 0.86 0.82 0.56 0.43 0.48 0.65
? LIG BOOST BIN 0.74 0.78 0.88 0.83 0.57 0.37 0.45 0.64
CNGL GLM 0.70 0.76 0.86 0.80 0.47 0.31 0.38 0.59
UMAC NB 0.56 0.82 0.49 0.62 0.37 0.73 0.49 0.55
CNGL GLMd 0.71 0.74 0.93 0.82 0.51 0.19 0.28 0.55
UMAC CRF 0.71 0.72 0.98 0.83 0.49 0.04 0.07 0.45
Baseline (one class) 0.71 0.71 1.00 0.83 0.00 0.00 0.00 0.42
Table 19: Official results for Task 2: binary classification on word level of the WMT13 Quality Estimation shared-task. The
winning submissions are indicated by a ?.
System ID F1 Keep F1 Substitute F1 Delete Micro-F1 Macro-F1
? LIG FS MULT 0.83 0.44 0.072 0.72 0.45
? LIG ALL MULT 0.83 0.45 0.064 0.72 0.45
UMAC NB 0.62 0.43 0.042 0.52 0.36
CNGL GLM 0.83 0.18 0.028 0.71 0.35
CNGL GLMd 0.83 0.14 0.034 0.72 0.34
UMAC CRF 0.83 0.04 0.012 0.71 0.29
Baseline (one class) 0.83 0.00 0.000 0.71 0.28
Table 20: Official results for Task 2: multiclass classification on word level of the WMT13 Quality Estimation shared-task.
The winning submissions are indicated by a ?.
30
average F1 ? it is clear that all systems outperform
the baseline. The winning systems by LIG for the
binary task are also the top ranking systems on the
multiclass task.
While promising results are found for the bi-
nary variant of the task where systems are able to
achieve an F1 of almost 0.5 for the relevant class
? Change, the multiclass prediction variant of the
task seem to suffer from its severe class imbalance.
In fact, none of the systems shows good perfor-
mance when predicting deletions.
6.9 Discussion
In what follows, we discuss the main accomplish-
ments of this shared task starting from the goals
we had previously identified for it.
Explore various granularity levels for the
quality-prediction task The decision on which
level of granularity quality estimation is applied
depends strongly on the intended application. In
Task 2 we tested binary word-level classification
in a post-editing setting. If such annotation is pre-
sented through a user interface we imagine that
words marked as incorrect would be hidden from
the editor, highlighted as possibly wrong or that a
list of alternatives would we generated.
With respect to the poor improvements over
trivial baselines, we consider that the results for
word-level prediction could be mostly connected
to limitations of the datasets provided, which are
very small for word-level prediction, as compared
to successful previous work such as (Bach et al,
2011). Despite the limited amount of training
data, several systems were able to predict dubious
words (binary variant of the task), showing that
this can be a promising task. Extending the granu-
larity even further by predicting the actual editing
action necessary for a word yielded less positive
results than the binary setting.
We cannot directly compare sentence- and
word-level results. However, since sentence-level
predictions can benefit from more information
available and therefore more signal on which the
prediction is based, the natural conclusion is that,
if there is a choice in the prediction granularity,
to opt for the coarser one possible (i.e., sentence-
level over word-level). But certain applications
may require finer granularity levels, and therefore
word-level predictions can still be very valuable.
Explore the prediction of more objective scores
Given the multitude of possible applications for
quality estimation we must decide which predicted
values are both useful and accurate. In this year?s
task we have attempted to address the useful-
ness criterion by moving from the subjective, hu-
man judgement-based scores, to the prediction of
scores that can be more easily interpreted for prac-
tical applications: post-editing distance or types of
edits (word-level), post-editing time, and ranking
of alternative translations.
The general promise of using objective scores is
that predicting a value that is related to the use case
will make quality estimation more applicable and
yield lower deviance compared to the use of proxy
metrics. The magnitude of this benefit should be
sufficient to account for the possible additional ef-
fort related to collecting such scores.
While a direct comparison between the differ-
ent types of scores used for this year?s tasks is not
possible as they are based on different datasets, if
we compare last year?s task on predicting 1-5 lik-
ert scores (and generating an overall ranking of all
translations in the test set) with this year?s Task
1.1, which is virtually the same, but using post-
editing distance as gold-label, we see that the num-
ber of systems that outperform the baseline 18 is
proportionally larger this year. We can also notice
a higher relative improvement of these submis-
sions over the baseline system. While this could
simply be a consequence of progress in the field, it
may also provide an indication that objective met-
rics are more suitable for the problem.
Particularly with respect to post-editing time,
given that this label has a long tailed distribution
and is not trivial to measure even in a controlled
environment, the results of Task 1.3 are encour-
aging. Comparison with the better results seen
on Tasks 1.1 and 1.2, however, suggests that, for
Task 1.3, additional data processing, filtering, and
modelling (including modelling translator-specific
traits such as their variance in time) is required, as
evidenced in (Cohn and Specia, 2013).
Explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics When it comes to the task of au-
tomatically ranking alternative translations gener-
ated by different MT systems, the traditional use
of reference-based MT evaluation metrics is chal-
lenged by the findings of this task.
The top ranking quality estimation submissions
18The two baselines are exactly the same, and therefore the
comparison is meaningful.
31
to Task 1.2 have performances that outperform or
are at least at the same level with the ones that
involve the use of human references. The most in-
teresting property of these techniques is that, be-
ing reference-free, they can be used for any source
sentences, and therefore are ready to be deployed
for arbitrary texts.
An immediate application for this capability is
a procedure by which MT system-selection is per-
formed, based on the output of such quality esti-
mators. Additional measurements are needed to
determine the level of improvement in translation
quality that the current performance of these tech-
niques can achieve in a system-selection scenario.
Identify new and effective quality indicators
Quality indicators, or features, are core to the
problem of quality estimation. One significant dif-
ference this year with respect to previous year was
the availability of QUEST, a framework for the ex-
traction of a large number of features. A few sub-
missions used these larger sets ? as opposed to the
17 baseline features used in the 2012 edition ? as
their starting point, to which they added other fea-
tures. Most features available in this framework,
however, had already been used in previous work.
Novel families of features used this year which
seems to have played an important role are those
proposed by CNGL. They include a number of
language and MT-system independent monolin-
gual and bilingual similarity metrics between the
sentences for prediction and corpora of the lan-
guage pair under consideration. Based on standard
regression algorithm (the same used by the base-
line system), the submissions from CNGL using
such feature families topped many of the tasks.
Another interesting family of features is that
used by TCD-CNGL and TCD-DCU-CNGL for
Tasks 1.1 and 1.3. These were borrowed from
work on style or authorship identification. The as-
sumption is that low/high quality translations can
be characterised by some patterns which are fre-
quent and/or differ significantly from patterns be-
longing to the opposite category.
Like in last year?s task, the vast majority of
the participating systems used external resources
in addition to those provided for the task, par-
ticularly for linguistically-oriented features, such
as parsers, part-of-speech taggers, named entity
recognizers, etc. A novel set of syntactic fea-
tures based on Combinatory Categorial Grammar
(CCG) performed reasonably well in Task 1.2:
with six CCG-based features and no additional
features, the system outperformed the baseline
system and also a second submission where the
17 baseline features were added. This highlights
the potential of linguistically-motivated features
for the problem.
As expected, different feature sets were used
for different tasks. This is essential for Task 2,
where word-level features are certainly necessary.
For example, LIG used a number of lexical fea-
tures such as part-of-speech tag, word-posterior
probabilities, syntactic (constituent label, distance
to the constituent tree root, and target and source
polysemy count). For submissions where a se-
quence labelling algorithm such as a Conditional
Random Fields was used for prediction, the inter-
dependencies between adjacent words and labels
was also modelled though features.
Pseudo-references, i.e., scores from standard
evaluation metrics such as BLEU based on trans-
lations generated by an alternative MT system as
?reference?, featured in more than half of the sub-
missions for sentence-level tasks. This is not sur-
prising given their performance in previous work
on quality estimation.
Identify effective machine learning techniques
for all variants of the quality estimation task
For the sentence-level tasks, standard regression
methods such as SVR performed well as in the
previous edition of the shared task, topping the
results for the ranking variant of Task 1.1, both
first and second place. In fact this algorithm was
used by most submissions that outperformed the
baseline. An alternative algorithm to SVR with
very promising results and which was introduced
for the problem this year is that of Gaussian Pro-
cesses. It was used by SHEF, the winning submis-
sion in the scoring variant of Task 1.1, which also
performed well in the ranking variant, despite its
hyperparameters having been optimised for scor-
ing only. Algorithms behave similarly for Task
1.3, with SVR performing particularly well.
For Task 1.2, logistic regression performed the
best or among the best, along with SVR. One of
the most effective approach for this task, however,
appears to be one that is better tailored for the
task, namely pair-wise decomposition for ranking.
This approach benefits from transforming a k-way
ranking problem into a series of simpler, 2-way
ranking problems, which can be more accurately
solved. Another approach that shows promise is
32
that of ensemble of regressors, in which the output
is the results combining the predictions of differ-
ent regression models.
Linear-chain Conditional Random Fields are a
popular model of choice for sequence labelling
tasks and have been successfully used by several
participants in Task 2, along with discriminatively
trained Hidden Markov Models and Na??ve Bayes.
As in the previous edition, feature engineer-
ing and feature selection prior to model learning
were important components in many submissions.
However, the role of individual features is hard
to judge separately from the role of the machine
learning techniques employed.
Establish the state of the art performance All
four tasks addressed in this shared task have
achieved a dual role that is important for the re-
search community: (i) to make publicly available
new data sets that can serve to compare different
approaches and contributions; and (ii) to estab-
lish the present state-of-the-art performance in the
field, so that progress can be easily measured and
tracked. In addition, the public availability of the
scoring scripts makes evaluation and direct com-
parison straightforward.
Many participants submitted predictions for
several tasks. Comparison of the results shows
that there is little overlap between the best sys-
tems when the predicted value is varied. While
we did not formally require the participants to use
similar systems across tasks, these results indicate
that specialised systems with features selected de-
pending on the predicted variable can in fact be
beneficial.
As we mentioned before, compared to the pre-
vious edition of the task, we noticed (for Task
1.1) a larger relative improvement of scores over
the baseline system, as well as a larger propor-
tion of systems outperforming the baseline sys-
tems, which are a good indication that the field is
progressing over the years. For example, in the
scoring variant of Task 1.1, last year only 5 out of
20 systems (i.e. 25% of the systems) were able to
significantly outperform the baseline. This year, 9
out 16 systems (i.e. 56%) outperformed the same
baseline. Last year, the relative improvement of
the winning submission with respect to the base-
line system was 13%, while this year the relative
improvement is of 19%.
Overall, the tables of results presented in Sec-
tion 6.8 give a comprehensive view of the current
state-of-the-art on the data sets used for this shared
task, as well as indications on how much room
there still is for improvement via figures from ora-
cle methods. As a result, people interested in con-
tributing to research in these machine translation
quality estimation tasks will be able to do so in a
principled way, with clearly established state-of-
the-art levels and straightforward means of com-
parison.
7 Summary
As in previous incarnations of this workshop we
carried out an extensive manual and automatic
evaluation of machine translation performance,
and we used the human judgements that we col-
lected to validate automatic metrics of translation
quality. We also refined last year?s quality estima-
tion task, asking for methods that predict sentence-
level post-editing effort and time, rank translations
from alternative systems, and pinpoint words in
the output that are more likely to be wrong.
As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.19
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Google, Microsoft and Yandex.
We would also like to thank our colleagues Ma-
tous? Macha?c?ek and Martin Popel for detailed dis-
cussions.
References
Allauzen, A., Pe?cheux, N., Do, Q. K., Dinarelli,
M., Lavergne, T., Max, A., Le, H.-S., and Yvon,
F. (2013). LIMSI @ WMT13. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 60?67, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Almaghout, H. and Specia, L. (2013). A CCG-
based Quality Estimation Metric for Statistical
Machine Translation. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Ammar, W., Chahuneau, V., Denkowski, M., Han-
neman, G., Ling, W., Matthews, A., Murray,
19http://statmt.org/wmt13/results.html
33
K., Segall, N., Lavie, A., and Dyer, C. (2013).
The CMU machine translation systems at WMT
2013: Syntax, synthetic translation options, and
pseudo-references. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 68?75, Sofia, Bulgaria. Association for
Computational Linguistics.
Avramidis, E. (2012). Comparative Quality Es-
timation: Automatic Sentence-Level Ranking
of Multiple Machine Translation Outputs. In
Proceedings of 24th International Conference
on Computational Linguistics, pages 115?132,
Mumbai, India.
Avramidis, E. and Popovic, M. (2013). Selecting
feature sets for comparative and time-oriented
quality estimation of machine translation out-
put. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 327?
334, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Avramidis, E., Popovic?, M., Vilar, D., and Bur-
chardt, A. (2011). Evaluate with confidence es-
timation: Machine ranking of translation out-
puts using grammatical features. In Proceed-
ings of the Sixth Workshop on Statistical Ma-
chine Translation.
Aziz, W., Mitkov, R., and Specia, L. (2013).
Ranking Machine Translation Systems via Post-
Editing. In Proc. of Text, Speech and Dialogue
(TSD), Lecture Notes in Artificial Intelligence,
Berlin / Heidelberg. Za?padoc?eska? univerzita v
Plzni, Springer Verlag.
Bach, N., Huang, F., and Al-Onaizan, Y. (2011).
Goodness: A method for measuring machine
translation confidence. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 211?219, Portland, Ore-
gon, USA.
Beck, D., Shah, K., Cohn, T., and Specia, L.
(2013). SHEF-Lite: When less is more for
translation quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 335?340, Sofia, Bulgaria.
Association for Computational Linguistics.
Bic?ici, E., Groves, D., and van Genabith, J. (2013).
Predicting sentence translation quality using ex-
trinsic and language independent features. Ma-
chine Translation.
Bic?ici, E. and van Genabith, J. (2013). CNGL-
CORE: Referential translation machines for
measuring semantic similarity. In *SEM 2013:
The Second Joint Conference on Lexical and
Computational Semantics, Atlanta, Georgia,
USA. Association for Computational Linguis-
tics.
Bicici, E. (2013a). Feature decay algorithms
for fast deployment of accurate statistical ma-
chine translation systems. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 76?82, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Bicici, E. (2013b). Referential translation ma-
chines for quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 341?349, Sofia, Bulgaria.
Association for Computational Linguistics.
B??lek, K. and Zeman, D. (2013). CUni multilin-
gual matrix in the WMT 2013 shared task. In
Proceedings of the Eighth Workshop on Statis-
tical Machine Translation, pages 83?89, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Bojar, O., Kos, K., and Marec?ek, D. (2010). Tack-
ling Sparse Data Issue in Machine Translation
Evaluation. In Proceedings of the ACL 2010
Conference Short Papers, pages 86?91, Upp-
sala, Sweden. Association for Computational
Linguistics.
Bojar, O., Rosa, R., and Tamchyna, A. (2013).
Chimera ? three heads for English-to-Czech
translation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
90?96, Sofia, Bulgaria. Association for Compu-
tational Linguistics.
Borisov, A., Dlougach, J., and Galinskaya, I.
(2013). Yandex school of data analysis ma-
chine translation systems for WMT13. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, pages 97?101, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
34
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montre?al, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Buck, C., Turchi, M.,
and Negri, M. (2013). FBK-UEdin participa-
tion to the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 350?
356, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Charniak, E. and Johnson, M. (2005). Coarse-to-
fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual
Meeting on Association for Computational Lin-
guistics, pages 173?180. Association for Com-
putational Linguistics.
Cho, E., Ha, T.-L., Mediani, M., Niehues, J., Her-
rmann, T., Slawik, I., and Waibel, A. (2013).
The Karlsruhe Institute of Technology transla-
tion systems for the WMT 2013. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 102?106, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling Anno-
tator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality
Estimation. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (to appear).
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
de Marneffe, M.-C., MacCartney, B., and Man-
ning, C. D. (2006). Generating typed depen-
dency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Denkowski, M. and Lavie, A. (2010). Meteor-next
and the meteor paraphrase tables: improved
evaluation support for five target languages. In
Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR,
WMT ?10, pages 339?342, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.
Durgar El-Kahlout, I. and Mermer, C. (2013).
TU?btak-blgem german-english machine trans-
lation systems for w13. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 107?111, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Durrani, N., Fraser, A., Schmid, H., Sajjad, H.,
and Farkas, R. (2013a). Munich-Edinburgh-
Stuttgart submissions of OSM systems at
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
120?125, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Durrani, N., Haddow, B., Heafield, K., and Koehn,
P. (2013b). Edinburgh?s machine translation
systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 112?119, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
35
Eidelman, V., Wu, K., Ture, F., Resnik, P., and Lin,
J. (2013). Towards efficient large-scale feature-
rich statistical machine translation. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 126?131, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Fleiss, J. L. (1971). Measuring nominal scale
agreement among many raters. Psychological
Bulletin, 76(5):378?382.
Formiga, L., Costa-jussa`, M. R., Marin?o, J. B.,
Fonollosa, J. A. R., Barro?n-Ceden?o, A., and
Marquez, L. (2013a). The TALP-UPC phrase-
based translation systems for WMT13: System
combination with morphology generation, do-
main adaptation and corpus filtering. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 132?138, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Formiga, L., Gonza`lez, M., Barro?n-Ceden?o, A.,
Fonollosa, J. A. R., and Marquez, L. (2013b).
The TALP-UPC approach to system selection:
Asiya features and pairwise classification using
random forests. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 357?362, Sofia, Bulgaria. Association for
Computational Linguistics.
Formiga, L., Ma`rquez, L., and Pujantell, J.
(2013c). Real-life translation quality estimation
for mt system selection. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Galus?c?a?kova?, P., Popel, M., and Bojar, O. (2013).
PhraseFix: Statistical post-editing of TectoMT.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 139?145,
Sofia, Bulgaria. Association for Computational
Linguistics.
Gispert, A., Blackwood, G., Iglesias, G., and
Byrne, W. (2013). N-gram posterior probabil-
ity confidence measures for statistical machine
translation: an empirical study. Machine Trans-
lation, 27:85?114.
Gonza`lez, M., Gime?nez, J., and Ma`rquez, L.
(2012). A graphical interface for mt evaluation
and error analysis. In Proceedings of the ACL
2012 System Demonstrations, pages 139?144,
Jeju Island, Korea.
Green, S., Cer, D., Reschke, K., Voigt, R., Bauer,
J., Wang, S., Silveira, N., Neidert, J., and Man-
ning, C. D. (2013). Feature-rich phrase-based
translation: Stanford University?s submission to
the WMT 2013 translation task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 146?151, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hall, M., Frank, E., Holmes, G., Pfahringer,
B., Reutemann, P., and Witten, I. H. (2009).
The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Han, A. L.-F., Wong, D. F., Chao, L. S., Lu, Y., He,
L., Wang, Y., and Zhou, J. (2013). A descrip-
tion of tunable machine translation evaluation
systems in WMT13 metrics task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 412?419, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hildebrand, S. and Vogel, S. (2013). MT quality
estimation: The CMU system for WMT?13. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 371?377, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Hosmer, D. (1989). Applied logistic regression.
Wiley, New York, 8th edition.
Huet, S., Manishina, E., and Lefe`vre, F.
(2013). Factored machine translation systems
for Russian-English. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 152?155, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Hunt, E., Martin, J., and Stone, P. (1966). Experi-
ments in Induction. Academic Press, New York.
Kaplan, R., Riezler, S., King, T., Maxwell, J.,
Vasserman, A., and Crouch, R. (2004). Speed
and accuracy in shallow and deep stochastic
parsing. In Proceedings of the Human Lan-
guage Technology Conference and the 4th An-
nual Meeting of the North American Chapter of
the Association for Computational Linguistics
(HLT/NAACL 04).
36
Kauchak, D. and Barzilay, R. (2006). Paraphras-
ing for automatic evaluation. In Proceedings
of the main conference on Human Language
Technology Conference of the North American
Chapter of the Association of Computational
Linguistics, HLT-NAACL ?06, pages 455?462,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Koehn, P. (2012). Simulating human judgment in
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Kononenko, I. (1994). Estimating attributes: anal-
ysis and extensions of RELIEF. In Proceedings
of the European conference on machine learn-
ing on Machine Learning, pages 171?182, Se-
caucus, NJ, USA. Springer-Verlag New York,
Inc.
Kos, K. and Bojar, O. (2009). Evaluation of Ma-
chine Translation Metrics for Czech as the Tar-
get Language. Prague Bulletin of Mathematical
Linguistics, 92:135?147.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Langlois, D., Raybaud, S., and Sma??li, K. (2012).
Loria system for the wmt12 quality estimation
shared task. In Proceedings of the Seventh
Workshop on Statistical Machine Translation,
pages 114?119, Montre?al, Canada.
Langlois, D. and Smaili, K. (2013). LORIA sys-
tem for the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 378?
383, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Le, H. S., Oparin, I., Allauzen, A., Gauvain, J.-L.,
and Yvon, F. (2011). Structured output layer
neural network language model. In ICASSP,
pages 5524?5527.
Lin, C.-J., Weng, R. C., and Keerthi, S. S. (2007).
Trust region Newton methods for large-scale lo-
gistic regression. In Proceedings of the 24th
international conference on Machine learning
- ICML ?07, pages 561?568, New York, New
York, USA. ACM Press.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Macha?c?ek, M. and Bojar, O. (2013). Results of the
WMT13 metrics shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 43?49, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Matusov, E. and Leusch, G. (2013). Omnifluent
English-to-French and Russian-to-English sys-
tems for the 2013 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 156?161, Sofia, Bulgaria. Association for
Computational Linguistics.
McCallum, A. K. (2002). MALLET: a machine
learning for language toolkit.
Miceli Barone, A. V. and Attardi, G. (2013).
Pre-reordering for machine translation using
transition-based walks on dependency parse
trees. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 162?
167, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Moreau, E. and Rubino, R. (2013). An approach
using style classification features for quality es-
timation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
427?432, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Nadejde, M., Williams, P., and Koehn, P. (2013).
Edinburgh?s syntax-based machine translation
systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
168?174, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Okita, T., Liu, Q., and van Genabith, J. (2013).
Shallow semantically-informed PBSMT and
HPBSMT. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
175?182, Sofia, Bulgaria. Association for Com-
putational Linguistics.
O?zgu?r, A., O?zgu?r, L., and Gu?ngo?r, T. (2005). Text
37
categorization with class-based and corpus-
based keyword selection. In Proceedings of
the 20th International Conference on Computer
and Information Sciences, ISCIS?05, pages
606?615, Berlin, Heidelberg. Springer.
Peitz, S., Mansour, S., Huck, M., Freitag, M.,
Ney, H., Cho, E., Herrmann, T., Mediani,
M., Niehues, J., Waibel, A., Allauzen, A.,
Khanh Do, Q., Buschbeck, B., and Wand-
macher, T. (2013a). Joint WMT 2013 submis-
sion of the QUAERO project. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 183?190, Sofia, Bulgaria.
Association for Computational Linguistics.
Peitz, S., Mansour, S., Peter, J.-T., Schmidt, C.,
Wuebker, J., Huck, M., Freitag, M., and Ney,
H. (2013b). The RWTH aachen machine trans-
lation system for WMT 2013. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 191?197, Sofia, Bulgaria.
Association for Computational Linguistics.
Pighin, D., Formiga, L., and Ma`rquez, L.
(2012). A graph-based strategy to streamline
translation quality assessments. In Proceed-
ings of the Tenth Conference of the Associa-
tion for Machine Translation in the Americas
(AMTA?2012), San Diego, USA.
Pino, J., Waite, A., Xiao, T., de Gispert, A., Flego,
F., and Byrne, W. (2013). The University of
Cambridge Russian-English system at WMT13.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 198?203,
Sofia, Bulgaria. Association for Computational
Linguistics.
Post, M., Ganitkevitch, J., Orland, L., Weese, J.,
Cao, Y., and Callison-Burch, C. (2013). Joshua
5.0: Sparser, better, faster, server. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 204?210, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Rubino, R., Toral, A., Corte?s Va??llo, S., Xie, J.,
Wu, X., Doherty, S., and Liu, Q. (2013a). The
CNGL-DCU-Prompsit translation systems for
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
211?216, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Rubino, R., Wagner, J., Foster, J., Roturier, J.,
Samad Zadeh Kaljahi, R., and Hollowood, F.
(2013b). DCU-Symantec at the WMT 2013
quality estimation shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 390?395, Sofia, Bulgaria.
Association for Computational Linguistics.
Sajjad, H., Smekalova, S., Durrani, N., Fraser, A.,
and Schmid, H. (2013). QCRI-MES submis-
sion at WMT13: Using transliteration mining
to improve statistical machine translation. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 217?222, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Schmidt, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods
in Natural Language Processing.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An In-
vestigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings
of MT Summit XIV (to appear), Nice, France.
Singh, A. K., Wisniewski, G., and Yvon, F.
(2013). LIMSI submission for the WMT?13
quality estimation task: an experiment with n-
gram posteriors. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 396?402, Sofia, Bulgaria. Association for
Computational Linguistics.
Smith, J., Saint-Amand, H., Plamada, M., Koehn,
P., Callison-Burch, C., and Lopez, A. (2013).
Dirt cheap web-scale parallel text from the
Common Crawl. In Proceedings of the 2013
Conference of the Association for Computa-
tional Linguistics (ACL 2013), Sofia, Bulgaria.
Association for Computational Linguistics.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Snover, M., Madnani, N., Dorr, B. J., and
Schwartz, R. (2009). Fluency, adequacy, or
hter?: exploring different human judgments
with a tunable mt metric. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
38
lation, StatMT ?09, pages 259?268, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Soricut, R., Bach, N., and Wang, Z. (2012). The
SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceed-
ings of the 7th Workshop on Statistical Machine
Translation, pages 145?151.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L. (2011). Exploiting Objective Annota-
tions for Measuring Translation Post-editing Ef-
fort. In Proceedings of the 15th Conference of
the European Association for Machine Transla-
tion, pages 73?80, Leuven.
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Stolcke, A. (2002). SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the
7th International Conference on Spoken Lan-
guage Processing (ICSLP 2002), pages 901?
904.
Stone, M. and Brooks, R. J. (1990). Contin-
uum regression: cross-validated sequentially
constructed prediction embracing ordinary least
squares, partial least squares and principal com-
ponents regression. Journal of the Royal
Statistical Society Series B Methodological,
52(2):237?269.
Stymne, S., Hardmeier, C., Tiedemann, J., and
Nivre, J. (2013). Tunable distortion limits and
corpus cleaning for SMT. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 223?229, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Tantug, A. C., Oflazer, K., and El-Kahlout, I. D.
(2008). BLEU+: a Tool for Fine-Grained BLEU
Computation. In (ELRA), E. L. R. A., edi-
tor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08),
Marrakech, Morocco.
Weller, M., Kisselew, M., Smekalova, S., Fraser,
A., Schmid, H., Durrani, N., Sajjad, H., and
Farkas, R. (2013). Munich-Edinburgh-Stuttgart
submissions at WMT13: Morphological and
syntactic processing for SMT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 230?237, Sofia, Bulgaria.
Association for Computational Linguistics.
39
A Pairwise System Comparisons by Human Judges
Tables 21?30 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according bootstrap resampling (p ?
0.05). Gray lines separate clusters based on non-overlapping rank ranges.
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
M
ES
UE
DI
N
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
CU
-ZE
M
AN
CU
-TA
M
CH
YN
A
DC
U-
FD
A
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .50 .48? .43? .47? .43? .44? .38? .32? .25? .26?
ONLINE-B .50 ? .46? .48? .47? .49 .44? .40? .39? .29? .27?
MES .52? .54? ? .49 .47? .44? .45? .42? .41? .27? .25?
UEDIN .57? .52? .51 ? .51 .48? .47? .42? .39? .28? .25?
ONLINE-A .53? .53? .53? .49 ? .48 .51 .44? .42? .31? .30?
UEDIN-SYNTAX .57? .51 .56? .52? .52 ? .51 .43? .41? .29? .26?
CU-ZEMAN .56? .56? .55? .53? .49 .49 ? .45? .42? .32? .29?
CU-TAMCHYNA .62? .60? .58? .58? .56? .57? .55? ? .46? .35? .32?
DCU-FDA .68? .61? .59? .61? .58? .59? .58? .54? ? .32? .32?
JHU .75? .71? .73? .72? .69? .71? .68? .65? .68? ? .46?
SHEF-WPROA .74? .73? .75? .75? .70? .74? .71? .68? .68? .54? ?
score .60 .58 .57 .56 .54 .54 .53 .48 .45 .32 .29
rank 1 2-3 2-4 3-5 4-7 5-7 6-7 8 9 10 11
Table 21: Head to head comparison, ignoring ties, for Czech-English systems
CU
-B
OJ
AR
CU
-D
EP
FI
X
ON
LI
NE
-B
UE
DI
N
CU
-ZE
M
AN
M
ES
ON
LI
NE
-A
CU
-PH
RA
SE
FI
X
CU
-TE
CT
OM
T
CO
M
M
ER
CI
AL
-1
CO
M
M
ER
CI
AL
-2
SH
EF
-W
PR
OA
CU-BOJAR ? .51 .47? .44? .42? .43? .48 .41? .37? .39? .38? .33?
CU-DEPFIX .49 ? .48? .42? .43? .41? .47? .42? .40? .40? .39? .34?
ONLINE-B .53? .52? ? .47? .44? .44? .44? .44? .44? .41? .36? .34?
UEDIN .56? .58? .53? ? .47? .47? .48 .45? .44? .42? .43? .38?
CU-ZEMAN .58? .57? .56? .53? ? .49 .49 .48? .46? .47? .47? .35?
MES .57? .59? .56? .53? .51 ? .50 .47? .46? .43? .44? .42?
ONLINE-A .52 .53? .56? .52 .51 .50 ? .52 .47? .47? .47? .46?
CU-PHRASEFIX .59? .58? .56? .55? .52? .53? .48 ? .49 .48? .49 .42?
CU-TECTOMT .63? .60? .56? .56? .54? .54? .53? .51 ? .46? .46? .40?
COMMERCIAL-1 .61? .60? .59? .58? .53? .57? .53? .52? .54? ? .49 .42?
COMMERCIAL-2 .62? .61? .64? .57? .53? .56? .53? .51 .54? .51 ? .43?
SHEF-WPROA .67? .66? .66? .62? .65? .58? .54? .58? .60? .58? .57? ?
score .58 .57 .56 .52 .50 .50 .49 .48 .47 .45 .45 .38
rank 1-2 1-2 3 4 5-7 5-7 5-8 7-9 8-9 10-11 10-11 12
Table 22: Head to head comparison, ignoring ties, for English-Czech systems
40
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
UE
DI
N
QU
AE
RO
KI
T
M
ES
RW
TH
-JA
NE
M
ES
-SZ
EG
ED
-R
EO
RD
ER
-SP
LI
T
LI
M
SI
-N
CO
DE
-SO
UL
TU
BI
TA
K
UM
D
DC
U
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
DE
SR
T
ONLINE-B ? .48 .44? .37? .44? .41? .42? .40? .35? .37? .32? .31? .31? .27? .23? .18? .16?
ONLINE-A .52 ? .47 .45? .47 .43? .42? .41? .44? .40? .35? .36? .34? .31? .27? .25? .21?
UEDIN-SYNTAX .56? .53 ? .48 .46? .48? .46? .46? .45? .45? .35? .35? .34? .28? .25? .20? .19?
UEDIN .63? .55? .52 ? .51 .46? .47? .49 .44? .43? .39? .34? .35? .32? .28? .24? .22?
QUAERO .56? .53 .54? .49 ? .49 .52 .44? .46? .44? .39? .38? .37? .30? .31? .25? .21?
KIT .59? .57? .52? .54? .51 ? .45? .51 .43? .46? .37? .38? .41? .35? .31? .25? .21?
MES .58? .58? .54? .53? .48 .55? ? .49 .49 .46? .44? .37? .40? .34? .30? .26? .20?
RWTH-JANE .60? .59? .54? .51 .56? .49 .51 ? .46? .50 .45? .46? .47? .38? .33? .28? .20?
MES-SZEGED-REORDER-SPLIT .65? .56? .55? .56? .54? .57? .51 .54? ? .53? .44? .41? .41? .36? .34? .31? .21?
LIMSI-NCODE-SOUL .63? .60? .55? .57? .56? .54? .54? .50 .47? ? .51 .45? .43? .37? .34? .30? .22?
TUBITAK .68? .65? .65? .61? .61? .63? .56? .55? .56? .49 ? .48? .49 .39? .41? .30? .25?
UMD .69? .64? .65? .66? .62? .62? .63? .54? .59? .55? .52? ? .48? .41? .40? .33? .27?
DCU .69? .66? .66? .65? .63? .59? .60? .53? .59? .57? .51 .52? ? .41? .38? .37? .25?
CU-ZEMAN .73? .69? .72? .68? .70? .65? .66? .62? .64? .63? .61? .59? .59? ? .44? .43? .29?
JHU .77? .73? .75? .72? .69? .69? .70? .67? .66? .66? .59? .60? .62? .56? ? .43? .30?
SHEF-WPROA .82? .75? .80? .76? .75? .75? .74? .72? .69? .70? .70? .67? .63? .57? .57? ? .41?
DESRT .84? .79? .81? .78? .79? .79? .80? .80? .79? .78? .75? .73? .75? .71? .70? .59? ?
score .66 .62 .60 .58 .58 .57 .56 .54 .53 .52 .48 .46 .46 .39 .36 .31 .23
rank 1 2-3 2-3 4-5 4-5 5-7 6-7 8-9 8-10 9-10 11 12-13 12-13 14 15 16 17
Table 23: Head to head comparison, ignoring ties, for German-English systems
ON
LI
NE
-B
PR
OM
T
UE
DI
N-
SY
NT
AX
ON
LI
NE
-A
UE
DI
N
KI
T
ST
AN
FO
RD
LI
M
SI
-N
CO
DE
-SO
UL
M
ES
-R
EO
RD
ER
JH
U
CU
-ZE
M
AN
TU
BI
TA
K
UU SH
EF
-W
PR
OA
RW
TH
-JA
NE
ONLINE-B ? .55? .50 .45? .45? .34? .37? .37? .37? .32? .32? .33? .24? .21? .26?
PROMT .45? ? .48? .50 .43? .40? .39? .36? .37? .31? .31? .32? .27? .24? .27?
UEDIN-SYNTAX .50 .52? ? .57? .45? .43? .38? .41? .39? .38? .33? .33? .26? .25? .22?
ONLINE-A .55? .50 .43? ? .51 .42? .48 .41? .36? .44? .44? .38? .32? .27? .29?
UEDIN .55? .57? .55? .49 ? .52 .45? .45? .42? .43? .37? .34? .29? .27? .31?
KIT .66? .60? .57? .58? .48 ? .48 .45? .42? .36? .39? .40? .30? .29? .26?
STANFORD .63? .61? .62? .52 .55? .52 ? .50 .44? .48 .44? .43? .34? .29? .32?
LIMSI-NCODE-SOUL .63? .64? .59? .59? .55? .55? .50 ? .44? .44? .44? .47? .40? .34? .33?
MES-REORDER .63? .63? .61? .64? .58? .58? .56? .56? ? .50 .46? .49 .38? .37? .34?
JHU .68? .69? .62? .56? .57? .64? .52 .56? .50 ? .48? .45? .36? .37? .34?
CU-ZEMAN .68? .69? .67? .56? .63? .61? .56? .56? .54? .52? ? .48 .40? .33? .34?
TUBITAK .67? .68? .67? .62? .66? .60? .57? .53? .51 .55? .52 ? .38? .40? .32?
UU .76? .73? .74? .68? .71? .70? .66? .60? .62? .64? .60? .62? ? .44? .46?
SHEF-WPROA .79? .76? .75? .73? .73? .71? .71? .66? .63? .63? .67? .60? .56? ? .47?
RWTH-JANE .74? .73? .78? .71? .69? .74? .68? .67? .66? .66? .66? .68? .54? .53? ?
score .63 .63 .61 .58 .57 .55 .52 .50 .47 .47 .46 .45 .36 .32 .32
rank 1-2 1-2 3 3-5 4-6 5-6 7 8 9-11 9-11 10-12 11-12 13 14-15 14-15
Table 24: Head to head comparison, ignoring ties, for English-German systems
41
UE
DI
N-
HE
AF
IE
LD
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
ON
LI
NE
-A
M
ES
-SI
M
PL
IF
IE
DF
RE
NC
H
DC
U
RW
TH
CM
U-
TR
EE
-TO
-TR
EE
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .45? .46? .46? .42? .42? .34? .34? .29? .33? .31? .28? .24?
UEDIN .55? ? .52? .43? .45? .46? .40? .38? .33? .36? .33? .32? .23?
ONLINE-B .54? .48? ? .49 .46? .44? .45? .40? .38? .34? .36? .31? .26?
LIMSI-NCODE-SOUL .54? .57? .51 ? .52? .47 .45? .42? .38? .36? .34? .31? .28?
KIT .58? .55? .54? .48? ? .47 .46? .44? .39? .38? .37? .33? .28?
ONLINE-A .58? .54? .56? .53 .53 ? .47 .45? .40? .40? .39? .34? .32?
MES-SIMPLIFIEDFRENCH .66? .60? .55? .55? .54? .53 ? .48? .44? .40? .39? .39? .32?
DCU .66? .62? .60? .58? .56? .55? .52? ? .45? .45? .42? .41? .36?
RWTH .71? .67? .62? .62? .61? .60? .56? .55? ? .48? .47? .47? .38?
CMU-TREE-TO-TREE .67? .64? .66? .64? .62? .60? .60? .55? .52? ? .50 .48 .37?
CU-ZEMAN .69? .67? .64? .66? .63? .61? .61? .58? .53? .50 ? .47? .39?
JHU .72? .68? .69? .69? .67? .66? .61? .59? .53? .52 .53? ? .45?
SHEF-WPROA .76? .77? .74? .72? .72? .68? .68? .64? .62? .63? .61? .55? ?
score .63 .60 .59 .57 .56 .54 .51 .48 .43 .42 .42 .38 .32
rank 1 2-3 2-3 4-5 4-5 5-6 7 8 9-10 9-11 10-11 12 13
Table 25: Head to head comparison, ignoring ties, for French-English systems
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
PR
OM
T
ST
AN
FO
RD
M
ES
M
ES
-IN
FL
EC
TI
ON
RW
TH
-PH
RA
SE
-B
AS
ED
-JA
NE
ON
LI
NE
-A
DC
U
CU
-ZE
M
AN
JH
U
OM
NI
FL
UE
NT
IT
S-L
AT
L
IT
S-L
AT
L-P
E
UEDIN ? .49 .47? .48 .50 .44? .41? .40? .47? .39? .41? .35? .29? .30? .27? .24?
ONLINE-B .51 ? .46? .47? .47? .44? .49 .43? .43? .43? .38? .35? .36? .28? .25? .25?
LIMSI-NCODE-SOUL .53? .54? ? .45? .48 .48 .45? .43? .44? .45? .41? .32? .34? .30? .27? .27?
KIT .52 .53? .55? ? .48 .46? .45? .43? .45? .46? .38? .30? .33? .31? .29? .29?
PROMT .50 .53? .52 .52 ? .50 .48 .52? .45? .47 .48? .38? .36? .36? .34? .31?
STANFORD .56? .56? .52 .54? .50 ? .52 .48 .44? .49 .44? .39? .34? .36? .30? .29?
MES .59? .51 .55? .55? .52 .48 ? .52 .51 .45? .45? .36? .37? .34? .29? .29?
MES-INFLECTION .60? .57? .57? .57? .48? .52 .48 ? .54? .51 .46? .37? .35? .31? .33? .31?
RWTH-PHRASE-BASED-JANE .53? .57? .56? .55? .55? .56? .49 .46? ? .53 .49 .38? .36? .34? .35? .31?
ONLINE-A .61? .57? .55? .54? .53 .51 .55? .49 .47 ? .50 .45? .38? .38? .39? .35?
DCU .59? .62? .59? .62? .52? .56? .55? .54? .51 .50 ? .42? .40? .40? .36? .35?
CU-ZEMAN .65? .65? .68? .70? .62? .61? .64? .63? .62? .55? .58? ? .50 .42? .41? .37?
JHU .71? .64? .66? .67? .64? .66? .63? .65? .64? .62? .60? .50 ? .47? .42? .38?
OMNIFLUENT .70? .72? .70? .69? .64? .64? .66? .69? .66? .62? .60? .58? .53? ? .43? .42?
ITS-LATL .73? .75? .72? .71? .66? .70? .71? .67? .65? .61? .64? .59? .58? .57? ? .45?
ITS-LATL-PE .76? .75? .73? .71? .69? .71? .71? .69? .69? .65? .65? .63? .62? .58? .55? ?
score .60 .60 .58 .58 .55 .55 .54 .53 .53 .51 .49 .42 .40 .38 .35 .32
rank 1-2 1-3 2-4 3-4 5-7 5-8 5-8 6-9 7-10 9-11 10-11 12 13 14 15 16
Table 26: Head to head comparison, ignoring ties, for English-French systems
42
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
UE
DI
N
ON
LI
NE
-A
M
ES
LI
M
SI
-N
CO
DE
-SO
UL
DC
U
DC
U-
OK
IT
A
DC
U-
FD
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .49 .42? .45? .43? .40? .34? .43? .37? .34? .31? .15?
ONLINE-B .51 ? .49 .44? .46? .47? .42? .39? .40? .37? .37? .16?
UEDIN .58? .51 ? .55? .50 .47? .43? .42? .39? .39? .35? .14?
ONLINE-A .55? .56? .45? ? .50 .44? .45? .42? .42? .41? .37? .18?
MES .57? .54? .50 .50 ? .47? .45? .41? .41? .40? .38? .15?
LIMSI-NCODE-SOUL .60? .53? .53? .56? .53? ? .46? .45? .44? .43? .38? .18?
DCU .66? .58? .57? .55? .55? .54? ? .44? .47? .42? .41? .16?
DCU-OKITA .57? .61? .58? .58? .59? .55? .56? ? .49 .46? .46? .18?
DCU-FDA .63? .60? .61? .58? .59? .56? .53? .51 ? .48? .43? .18?
CU-ZEMAN .66? .63? .61? .59? .60? .57? .58? .54? .52? ? .43? .18?
JHU .69? .63? .65? .63? .62? .62? .59? .54? .57? .57? ? .22?
SHEF-WPROA .85? .84? .86? .82? .85? .82? .84? .82? .82? .82? .78? ?
score .62 .59 .57 .57 .56 .53 .51 .48 .48 .46 .42 .16
rank 1 2 3-5 3-5 3-5 6 7 8-9 8-9 10 11 12
Table 27: Head to head comparison, ignoring ties, for Spanish-English systems
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N
PR
OM
T
M
ES
TA
LP
-U
PC
LI
M
SI
-N
CO
DE
DC
U
DC
U-
FD
A
DC
U-
OK
IT
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
ONLINE-B ? .49 .45? .43? .38? .35? .34? .35? .37? .34? .33? .32? .23?
ONLINE-A .51 ? .49 .48 .38? .46? .42? .41? .43? .38? .38? .37? .31?
UEDIN .55? .51 ? .49 .46? .45? .43? .42? .36? .38? .38? .38? .26?
PROMT .57? .52 .51 ? .46? .48 .43? .43? .40? .37? .39? .34? .29?
MES .62? .62? .54? .54? ? .46? .44? .44? .41? .40? .43? .36? .32?
TALP-UPC .65? .54? .55? .52 .54? ? .50 .45? .44? .40? .40? .37? .32?
LIMSI-NCODE .66? .58? .57? .57? .56? .50 ? .46? .51 .48 .44? .45? .35?
DCU .65? .59? .58? .57? .56? .55? .54? ? .50 .48 .48 .45? .36?
DCU-FDA .63? .57? .64? .60? .59? .56? .49 .50 ? .53? .49 .42? .32?
DCU-OKITA .66? .62? .62? .63? .60? .60? .52 .52 .47? ? .50 .47? .36?
CU-ZEMAN .67? .62? .62? .61? .57? .60? .56? .52 .51 .50 ? .46? .40?
JHU .68? .63? .62? .66? .64? .63? .55? .55? .58? .53? .54? ? .37?
SHEF-WPROA .77? .69? .74? .71? .68? .68? .65? .64? .68? .64? .60? .63? ?
score .63 .58 .57 .56 .53 .52 .49 .47 .47 .45 .44 .41 .32
rank 1 2-4 2-4 3-4 5-6 5-6 7-8 7-9 8-10 9-11 10-11 12 13
Table 28: Head to head comparison, ignoring ties, for English-Spanish systems
43
ON
LI
NE
-B
CM
U
ON
LI
NE
-A
ON
LI
NE
-G
PR
OM
T
QC
RI
-M
ES
UC
AM
-M
UL
TI
FR
ON
TE
ND
BA
LA
GU
R
M
ES
-Q
CR
I
UE
DI
N
OM
NI
FL
UE
NT
-U
NC
NS
TR
LI
A
OM
NI
FL
UE
NT
-C
NS
TR
UM
D
CU
-K
AR
EL
CO
M
M
ER
CI
AL
-3
UE
DI
N-
SY
NT
AX
JH
U
CU
-ZE
M
AN
ONLINE-B ? .40? .42? .41? .37? .37? .41? .33? .33? .37? .33? .33? .35? .38? .34? .33? .29? .28? .14?
CMU .60? ? .50 .46? .43? .47? .42? .42? .39? .43? .41? .41? .40? .38? .36? .30? .30? .29? .17?
ONLINE-A .58? .50 ? .50 .51 .43? .47? .44? .40? .41? .43? .38? .40? .38? .38? .39? .34? .30? .19?
ONLINE-G .59? .54? .50 ? .55? .50 .51 .48 .42? .41? .44? .43? .46? .40? .44? .36? .34? .33? .19?
PROMT .63? .57? .49 .45? ? .43? .47? .43? .47? .47? .43? .39? .44? .43? .37? .41? .40? .38? .25?
QCRI-MES .63? .53? .57? .50 .57? ? .48 .46? .47? .45? .43? .45? .45? .38? .42? .37? .33? .40? .19?
UCAM-MULTIFRONTEND .59? .58? .53? .49 .53? .52 ? .47? .48 .46? .46? .42? .45? .46? .45? .40? .39? .33? .17?
BALAGUR .67? .58? .56? .52 .57? .54? .53? ? .47? .49 .45? .53? .40? .44? .44? .41? .36? .33? .23?
MES-QCRI .67? .61? .60? .58? .53? .53? .52 .53? ? .49 .47? .47? .43? .43? .44? .38? .42? .39? .17?
UEDIN .63? .57? .59? .59? .53? .55? .54? .51 .51 ? .48 .52 .44? .52 .49 .42? .43? .35? .21?
OMNIFLUENT-UNCNSTR .67? .59? .57? .56? .57? .57? .54? .55? .53? .52 ? .51 .46? .48 .48 .44? .40? .39? .25?
LIA .67? .59? .62? .57? .61? .55? .58? .47? .53? .48 .49 ? .51 .49 .48 .50 .41? .39? .20?
OMNIFLUENT-CNSTR .65? .60? .60? .54? .56? .55? .55? .60? .57? .56? .54? .49 ? .51 .48 .47? .40? .40? .25?
UMD .62? .62? .62? .60? .57? .62? .54? .56? .57? .48 .52 .51 .49 ? .53? .42? .46? .42? .19?
CU-KAREL .66? .64? .62? .56? .63? .58? .55? .56? .56? .51 .52 .52 .52 .47? ? .44? .40? .47? .24?
COMMERCIAL-3 .67? .70? .61? .64? .59? .63? .60? .59? .62? .58? .56? .50 .53? .58? .56? ? .51 .44? .32?
UEDIN-SYNTAX .71? .70? .66? .66? .60? .67? .61? .64? .58? .57? .60? .59? .60? .54? .60? .49 ? .45? .25?
JHU .72? .71? .70? .67? .62? .60? .67? .67? .61? .65? .61? .61? .60? .58? .53? .56? .55? ? .24?
CU-ZEMAN .86? .83? .81? .81? .75? .81? .83? .77? .83? .79? .75? .80? .75? .81? .76? .68? .75? .76? ?
score .65 .60 .58 .56 .56 .55 .54 .52 .51 .50 .49 .49 .48 .48 .47 .43 .41 .39 .21
rank 1 2-3 2-3 4-6 4-6 5-7 5-7 8-9 8-10 9-11 10-12 11-14 12-15 12-15 13-15 16 17 18 19
Table 29: Head to head comparison, ignoring ties, for Russian-English systems
PR
OM
T
ON
LI
NE
-B
CM
U
ON
LI
NE
-G
ON
LI
NE
-A
UE
DI
N
QC
RI
-M
ES
CU
-K
AR
EL
M
ES
-Q
CR
I
JH
U
CO
M
M
ER
CI
AL
-3
LI
A
BA
LA
GU
R
CU
-ZE
M
AN
PROMT ? .44? .39? .47 .46? .36? .37? .37? .32? .35? .28? .30? .32? .24?
ONLINE-B .56? ? .44? .41? .44? .38? .37? .35? .33? .39? .33? .31? .35? .24?
CMU .61? .56? ? .52 .49 .47? .43? .41? .39? .44? .44? .40? .35? .28?
ONLINE-G .53 .59? .48 ? .48 .50 .48 .46 .46? .42? .38? .43? .38? .36?
ONLINE-A .54? .56? .51 .52 ? .47 .49 .49 .48 .44? .38? .40? .40? .34?
UEDIN .64? .62? .53? .50 .53 ? .49 .46? .42? .39? .44? .41? .38? .29?
QCRI-MES .63? .63? .57? .52 .51 .51 ? .48 .45? .44? .42? .39? .40? .29?
CU-KAREL .63? .65? .59? .54 .51 .54? .52 ? .50 .46? .43? .40? .42? .34?
MES-QCRI .68? .67? .61? .54? .52 .58? .55? .50 ? .48? .47? .43? .45? .34?
JHU .65? .61? .56? .58? .56? .61? .56? .54? .52? ? .51 .44? .44? .33?
COMMERCIAL-3 .72? .67? .56? .62? .62? .56? .58? .57? .53? .49 ? .52 .48 .44?
LIA .70? .69? .60? .57? .60? .59? .61? .60? .57? .56? .48 ? .47? .41?
BALAGUR .68? .65? .65? .62? .60? .62? .60? .58? .55? .56? .52 .53? ? .41?
CU-ZEMAN .76? .76? .72? .64? .66? .71? .71? .66? .66? .67? .56? .59? .59? ?
score .64 .62 .55 .54 .53 .53 .52 .49 .47 .46 .43 .42 .41 .33
rank 1 2 3-4 3-6 3-7 4-7 5-7 8 9-10 9-10 11-12 11-13 12-13 14
Table 30: Head to head comparison, ignoring ties, for English-Russian systems
44
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 206?212,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joshua 5.0: Sparser, better, faster, server
Matt Post1 and Juri Ganitkevitch2 and Luke Orland1 and Jonathan Weese2 and Yuan Cao2
1Human Language Technology Center of Excellence
2Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch
Computer and Information Sciences Department
University of Pennsylvania
Abstract
We describe improvements made over the
past year to Joshua, an open-source trans-
lation system for parsing-based machine
translation. The main contributions this
past year are significant improvements in
both speed and usability of the grammar
extraction and decoding steps. We have
also rewritten the decoder to use a sparse
feature representation, enabling training of
large numbers of features with discrimina-
tive training methods.
1 Introduction
Joshua is an open-source toolkit1 for hierarchical
and syntax-based statistical machine translation
of human languages with synchronous context-
free grammars (SCFGs). The original version of
Joshua (Li et al, 2009) was a port (from Python to
Java) of the Hiero machine translation system in-
troduced by Chiang (2007). It was later extended
to support grammars with rich syntactic labels (Li
et al, 2010). Subsequent efforts produced Thrax,
the extensible Hadoop-based extraction tool for
synchronous context-free grammars (Weese et al,
2011), later extended to support pivoting-based
paraphrase extraction (Ganitkevitch et al, 2012).
Joshua 5.0 continues our yearly update cycle.
The major components of Joshua 5.0 are:
?3.1 Sparse features. Joshua now supports an
easily-extensible sparse feature implementa-
tion, along with tuning methods (PRO and
kbMIRA) for efficiently setting the weights
on large feature vectors.
1joshua-decoder.org
?3.2 Significant speed increases. Joshua 5.0 is up
to six times faster than Joshua 4.0, and also
does well against hierarchical Moses, where
end-to-end decoding (including model load-
ing) of WMT test sets is as much as three
times faster.
?3.3 Thrax 2.0. Our reengineered Hadoop-based
grammar extractor, Thrax, is up to 300%
faster while using significantly less interme-
diate disk space.
?3.4 Many other features. Joshua now includes a
server mode with fair round-robin scheduling
among and within requests, a bundler for dis-
tributing trained models, improvements to the
Joshua pipeline (for managing end-to-end ex-
periments), and better documentation.
2 Overview
Joshua is an end-to-end statistical machine trans-
lation toolkit. In addition to the decoder com-
ponent (which performs the actual translation), it
includes the infrastructure needed to prepare and
align training data, build translation and language
models, and tune and evaluate them.
This section provides a brief overview of the
contents and abilities of this toolkit. More infor-
mation can be found in the online documentation
(joshua-decoder.org/5.0/).
2.1 The Pipeline: Gluing it all together
The Joshua pipeline ties together all the infrastruc-
ture needed to train and evaluate machine transla-
tion systems for research or industrial purposes.
Once data has been segmented into parallel train-
ing, development, and test sets, a single invocation
of the pipeline script is enough to invoke this entire
infrastructure from beginning to end. Each step is
206
broken down into smaller steps (e.g., tokenizing a
file) whose dependencies are cached with SHA1
sums. This allows a reinvoked pipeline to reliably
skip earlier steps that do not need to be recom-
puted, solving a common headache in the research
and development cycle.
The Joshua pipeline is similar to other ?ex-
periment management systems? such as Moses?
Experiment Management System (EMS), a much
more general, highly-customizable tool that al-
lows the specification and parallel execution of
steps in arbitrary acyclic dependency graphs
(much like the UNIX make tool, but written with
machine translation in mind). Joshua?s pipeline
is more limited in that the basic pipeline skeleton
is hard-coded, but reduced versatility covers many
standard use cases and is arguably easier to use.
The pipeline is parameterized in many ways,
and all the options below are selectable with
command-line switches. Pipeline documentation
is available online.
2.2 Data preparation, alignment, and model
building
Data preparation involves data normalization (e.g.,
collapsing certain punctuation symbols) and tok-
enization (with the Penn treebank or user-specified
tokenizer). Alignment with GIZA++ (Och and
Ney, 2000) and the Berkeley aligner (Liang et al,
2006b) are supported.
Joshua?s builtin grammar extractor, Thrax, is
a Hadoop-based extraction implementation that
scales easily to large datasets (Ganitkevitch et al,
2013). It supports extraction of both Hiero (Chi-
ang, 2005) and SAMT grammars (Zollmann and
Venugopal, 2006) with extraction heuristics eas-
ily specified via a flexible configuration file. The
pipeline also supports GHKM grammar extraction
(Galley et al, 2006) using the extractors available
from Michel Galley2 or Moses.
SAMT and GHKM grammar extraction require
a parse tree, which are produced using the Berke-
ley parser (Petrov et al, 2006), or can be done out-
side the pipeline and supplied as an argument.
2.3 Decoding
The Joshua decoder is an implementation of the
CKY+ algorithm (Chappelier et al, 1998), which
generalizes CKY by removing the requirement
2nlp.stanford.edu/?mgalley/software/
stanford-ghkm-latest.tar.gz
that the grammar first be converted to Chom-
sky Normal Form, thereby avoiding the complex-
ities of explicit binarization schemes (Zhang et
al., 2006; DeNero et al, 2009). CKY+ main-
tains cubic-time parsing complexity (in the sen-
tence length) with Earley-style implicit binariza-
tion of rules. Joshua permits arbitrary SCFGs, im-
posing no limitation on the rank or form of gram-
mar rules.
Parsing complexity is still exponential in the
scope of the grammar,3 so grammar filtering re-
mains important. The default Thrax settings ex-
tract only grammars with rank 2, and the pipeline
implements scope-3 filtering (Hopkins and Lang-
mead, 2010) when filtering grammars to test sets
(for GHKM).
Joshua uses cube pruning (Chiang, 2007) with
a default pop limit of 100 to efficiently explore the
search space. Other decoder options are too nu-
merous to mention here, but are documented on-
line.
2.4 Tuning and testing
The pipeline allows the specification (and optional
linear interpolation) of an arbitrary number of lan-
guage models. In addition, it builds an interpo-
lated Kneser-Ney language model on the target
side of the training data using KenLM (Heafield,
2011; Heafield et al, 2013), BerkeleyLM (Pauls
and Klein, 2011) or SRILM (Stolcke, 2002).
Joshua ships with MERT (Och, 2003) and PRO
implementations. Tuning with k-best batch MIRA
(Cherry and Foster, 2012) is also supported via
callouts to Moses.
3 What?s New in Joshua 5.0
3.1 Sparse features
Until a few years ago, machine translation systems
were for the most part limited in the number of fea-
tures they could employ, since the line-based op-
timization method, MERT (Och, 2003), was not
able to efficiently search over more than tens of
feature weights. The introduction of discrimina-
tive tuning methods for machine translation (Liang
et al, 2006a; Tillmann and Zhang, 2006; Chiang
et al, 2008; Hopkins and May, 2011) has made
it possible to tune large numbers of features in
statistical machine translation systems, and open-
3Roughly, the number of consecutive nonterminals in a
rule (Hopkins and Langmead, 2010).
207
source implementations such as Cherry and Foster
(2012) have made it easy.
Joshua 5.0 has moved to a sparse feature rep-
resentation internally. First, to clarify terminol-
ogy, a feature as implemented in the decoder is
actually a template that can introduce any number
of actual features (in the standard machine learn-
ing sense). We will use the term feature function
for these templates and feature for the individual,
traditional features that are induced by these tem-
plates. For example, the (typically dense) features
stored with the grammar on disk are each separate
features contributed by the PHRASEMODEL fea-
ture function template. The LANGUAGEMODEL
template contributes a single feature value for each
language model that was loaded.
For efficiency, Joshua does not store the en-
tire feature vector during decoding. Instead, hy-
pergraph nodes maintain only the best cumulative
score of each incoming hyperedge, and the edges
themselves retain only the hyperedge delta (the in-
ner product of the weight vector and features in-
curred by that edge). After decoding, the feature
vector for each edge can be recomputed and ex-
plicitly represented if that information is required
by the decoder (for example, during tuning).
This functionality is implemented via the fol-
lowing feature function interface, presented here
in simplified pseudocode:
interface FeatureFunction:
apply(context, accumulator)
The context comprises fixed pieces of the input
sentence and hypergraph:
? the hypergraph edge (which represents the
SCFG rule and sequence of tail nodes)
? the complete source sentence
? the input span
The accumulator object?s job is to accumulate
feature (name,value) pairs fired by a feature func-
tion during the application of a rule, via another
interface:
interface Accumulator:
add(feature_name, value)
The accumulator generalization4 permits the use
of a single feature-gathering function for two ac-
cumulator objects: the first, used during decoding,
maintains only a weighted sum, and the second,
4Due to Kenneth Heafield.
used (if needed) during k-best extraction, holds
onto the entire sparse feature vector.
For tuning large sets of features, Joshua sup-
ports both PRO (Hopkins and May, 2011), an in-
house version introduced with Joshua 4.0, and k-
best batch MIRA (Cherry and Foster, 2012), im-
plemented via calls to code provided by Moses.
3.2 Performance improvements
We introduced many performance improvements,
replacing code designed to get the job done under
research timeline constraints with more efficient
alternatives, including smarter handling of locking
among threads, more efficient (non string-based)
computation of dynamic programming state, and
replacement of fixed class-based array structures
with fixed-size literals.
We used the following experimental setup to
compare Joshua 4.0 and 5.0: We extracted a large
German-English grammar from all sentences with
no more than 50 words per side from Europarl v.7
(Koehn, 2005), News Commentary, and the Com-
mon Crawl corpora using Thrax default settings.
After filtering against our test set (newstest2012),
this grammar contained 70 million rules. We then
trained three language models on (1) the target
side of our grammar training data, (2) English
Gigaword, and (3) the monolingual English data
released for WMT13. We tuned a system using
kbMIRA and decoded using KenLM (Heafield,
2011). Decoding was performed on 64-core 2.1
GHz AMD Opteron processors with 256 GB of
available memory.
Figure 1 plots the end-to-end runtime5 as a
function of the number of threads. Each point in
the graph is the minimum of at least fifteen runs
computed at different times over a period of a few
days. The main point of comparison, between
Joshua 4.0 and 5.0, shows that the current version
is up to 500% faster than it was last year, espe-
cially in multithreaded situations.
For further comparison, we took these models,
converted them to hierarchical Moses format, and
then decoded with the latest version.6 We com-
piled Moses with the recommended optimization
settings7 and used the in-memory (SCFG) gram-
5i.e., including model loading time and grammar sorting
6The latest version available on Github as of June 7, 2013
7With tcmalloc and the following compile flags:
--max-factors=1 --kenlm-max-order=5
debug-symbols=off
208
 500 1000 2000 3000 4000
 5000 10000
 2 4  8  16  32  48decoding time (seconds) thread count
Joshua 4.0 (in-memory)Moses (in-memory)Joshua 4.0 (packed)Joshua 5.0 (packed)
Figure 1: End-to-end runtime as a function of the
number of threads. Each data point is the mini-
mum of at least fifteen different runs.
 200 300 400 500 1000 2000
 3000 4000 5000
 2 4  8  16  32  48decoding time (seconds) thread count
Joshua 5.0Moses
Figure 2: Decoding time alone.
mar format. BLEU scores were similar.8 In this
end-to-end setting, Joshua is about 200% faster
than Moses at high thread counts (Figure 1).
Figure 2 furthers the Moses and Joshua com-
parison by plotting only decoding time (subtract-
ing out model loading and sorting times). Moses?
decoding speed is 2?3 times faster than Joshua?s,
suggesting that the end-to-end gains in Figure 1
are due to more efficient grammar loading.
3.3 Thrax 2.0
The Thrax module of our toolkit has undergone
a similar overhaul. The rule extraction code was
822.88 (Moses), 22.99 (Joshua 4), and 23.23 (Joshua 5).
long-term investment holding on to
det
amod
the
JJ NN VBG IN TO DT
NP
PP
VP
? ?
the long-term
?
=~sig
?
dep-det-R-investment
pos-L-TO 
pos-R-NN  
lex-R-investment 
lex-L-to 
dep-amod-R-investment
syn-gov-NP syn-miss-L-NN 
lex-L-on-to 
pos-L-IN-TO  
dep-det-R-NN dep-amod-R-NN
Figure 3: Here, position-aware lexical and part-of-
speech n-gram features, labeled dependency links,
and features reflecting the phrase?s CCG-style la-
bel NP/NN are included in the context vector.
rewritten to be easier to understand and extend, al-
lowing, for instance, for easy inclusion of alterna-
tive nonterminal labeling strategies.
We optimized the data representation used for
the underlying map-reduce framework towards
greater compactness and speed, resulting in a
300% increase in extraction speed and an equiv-
alent reduction in disk I/O (Table 1). These
gains enable us to extract a syntactically labeled
German-English SAMT-style translation grammar
from a bitext of over 4 million sentence pairs in
just over three hours. Furthermore, Thrax 2.0 is
capable of scaling to very large data sets, like
the composite bitext used in the extraction of the
paraphrase collection PPDB (Ganitkevitch et al,
2013), which counted 100 million sentence pairs
and over 2 billion words on the English side.
Furthermore, Thrax 2.0 contains a module fo-
cused on the extraction of compact distributional
signatures over large datasets. This distribu-
tional mode collects contextual features for n-
gram phrases, such as words occurring in a win-
dow around the phrase, as well as dependency-
based and syntactic features. Figure 3 illustrates
the feature space. We then compute a bit signature
from the resulting feature vector via a randomized
locality-sensitive hashing projection. This yields a
compact representation of a phrase?s typical con-
text. To perform this projection Thrax relies on
the Jerboa toolkit (Van Durme, 2012). As part of
the PPDB effort, Thrax has been used to extract
rich distributional signatures for 175 million 1-
to-4-gram phrases from the Annotated Gigaword
corpus (Napoles et al, 2012), a parsed and pro-
209
Cs-En Fr-En De-En Es-En
Rules 112M 357M 202M 380M
Space Time Space Time Space Time Space Time
Joshua 4.0 120GB 112 min 364GB 369 min 211GB 203 min 413GB 397 min
Joshua 5.0 31GB 25 min 101GB 81 min 56GB 44 min 108GB 84 min
Difference -74.1% -77.7% -72.3% -78.0% -73.5% -78.3% -73.8% -78.8%
Table 1: Comparing Hadoop?s intermediate disk space use and extraction time on a selection of Europarl
v.7 Hiero grammar extractions. Disk space was measured at its maximum, at the input of Thrax?s final
grammar aggregation stage. Runtime was measured on our Hadoop cluster with a capacity of 52 mappers
and 26 reducers. On average Thrax 2.0, bundled with Joshua 5.0, is up to 300% faster and more compact.
cessed version of the English Gigaword (Graff et
al., 2003).
Thrax is distributed with Joshua and is also
available as a separate download.9
3.4 Other features
Joshua 5.0 also includes many features designed
to increase its usability. These include:
? A TCP/IP server architecture, designed to
handle multiple sets of translation requests
while ensuring fairness in thread assignment
both across and within these connections.
? Intelligent selection of translation and lan-
guage model training data using cross-
entropy difference to rank training candidates
(Moore and Lewis, 2010; Axelrod et al,
2011) (described in detail in Orland (2013)).
? A bundler for easy packaging of trained mod-
els with all of its dependencies.
? A year?s worth of improvements to the
Joshua pipeline, including many new features
and supported options, and increased robust-
ness to error.
? Extended documentation.
4 WMT Submissions
We submitted a constrained entry for all tracks ex-
cept English-Czech (nine in total). Our systems
were constructed in a straightforward fashion and
without any language-specific adaptations using
the Joshua pipeline. For each language pair, we
trained a Hiero system on all sentences with no
more than fifty words per side in the Europarl,
News Commentary, and Common Crawl corpora.
9github.com/joshua-decoder/thrax
We built two interpolated Kneser-Ney language
models: one from the monolingual News Crawl
corpora (2007?2012), and another from the tar-
get side of the training data. For systems translat-
ing into English, we added a third language model
built on Gigaword. Language models were com-
bined linearly into a single language model using
interpolation weights from the tuning data (new-
stest2011). We tuned our systems with kbMIRA.
For truecasing, we used a monolingual translation
system built on the training data, and finally deto-
kenized with simple heuristics.
5 Summary
The 5.0 release of Joshua is the result of a signif-
icant year-long research, engineering, and usabil-
ity effort that we hope will be of service to the
research community. User-friendly packages of
Joshua are available from joshua-decoder.
org, while developers are encouraged to partic-
ipate via github.com/joshua-decoder/
joshua. Mailing lists, linked from the main
Joshua page, are available for both.
Acknowledgments Joshua?s sparse feature rep-
resentation owes much to discussions with Colin
Cherry, Barry Haddow, Chris Dyer, and Kenneth
Heafield at MT Marathon 2012 in Edinburgh.
This material is based on research sponsored
by the NSF under grant IIS-1249516 and DARPA
under agreement number FA8750-13-2-0017 (the
DEFT program). The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes. The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as representing
official policies or endorsements of DARPA or the
U.S. Government.
210
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP, pages 355?
362, Edinburgh, Scotland, UK., July.
J.C. Chappelier, M. Rajman, et al 1998. A generalized
CYK algorithm for parsing stochastic CFG. In First
Workshop on Tabulation in Parsing and Deduction
(TAPD98), pages 133?137.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of NAACL-HLT, pages 427?436, Montre?al,
Canada, June.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of
EMNLP, Waikiki, Hawaii, USA, October.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, Ann Arbor, Michigan.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
John DeNero, Adam Pauls, and Dan Klein. 2009.
Asynchronous binarization for synchronous gram-
mars. In Proceedings of ACL, Suntec, Singapore,
August.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of ACL/COLING, Sydney, Australia, July.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, PRO, and paraphrases. In Proceedings of
the Workshop on Statistical Machine Translation.
Juri Ganitkevitch, Chris Callison-Burch, and Benjamin
Van Durme. 2013. Ppdb: The paraphrase database.
In Proceedings of HLT/NAACL.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2003.
English gigaword. Linguistic Data Consortium,
Philadelphia.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of ACL, Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the
Workshop on Statistical Machine Translation, pages
187?197. Association for Computational Linguis-
tics.
Mark Hopkins and Greg Langmead. 2010. SCFG
decoding without binarization. In Proceedings of
EMNLP, pages 646?655.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Work-
shop on Statistical Machine Translation, Athens,
Greece, March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proceedings of the Workshop on
Statistical Machine Translation.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of ACL/COLING.
Percy Liang, Ben Taskar, and Dan Klein. 2006b.
Alignment by agreement. In Proceedings of
NAACL, pages 104?111, New York City, USA, June.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of ACL (short papers), pages 220?224.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of ACL, pages 440?
447, Hong Kong, China, October.
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
Sapporo, Japan.
Luke Orland. 2013. Intelligent selection of trans-
lation model training data for machine translation
with TAUS domain data: A summary. Master?s the-
sis, Johns Hopkins University, Baltimore, Maryland,
June.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of ACL,
pages 258?267, Portland, Oregon, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL,
Sydney, Australia, July.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Seventh International
Conference on Spoken Language Processing.
211
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proceedings of ACL/COLING, pages 721?728, Syd-
ney, Australia, July.
Benjamin Van Durme. 2012. Jerboa: A toolkit for ran-
domized and streaming algorithms. Technical Re-
port 7, Human Language Technology Center of Ex-
cellence, Johns Hopkins University.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the
Thrax grammar extractor. In Proceedings of the
Workshop on Statistical Machine Translation.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation, New York, New York.
212
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 262?270,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Combining Bilingual and Comparable Corpora
for Low Resource Machine Translation
Ann Irvine
Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch?
Computer and Information Science Dept.
University of Pennsylvania
Abstract
Statistical machine translation (SMT) per-
formance suffers when models are trained
on only small amounts of parallel data.
The learned models typically have both
low accuracy (incorrect translations and
feature scores) and low coverage (high
out-of-vocabulary rates). In this work, we
use an additional data resource, compa-
rable corpora, to improve both. Begin-
ning with a small bitext and correspond-
ing phrase-based SMT model, we improve
coverage by using bilingual lexicon induc-
tion techniques to learn new translations
from comparable corpora. Then, we sup-
plement the model?s feature space with
translation scores estimated over compa-
rable corpora in order to improve accu-
racy. We observe improvements between
0.5 and 1.7 BLEU translating Tamil, Tel-
ugu, Bengali, Malayalam, Hindi, and Urdu
into English.
1 Introduction
Standard statistical machine translation (SMT)
models (Koehn et al, 2003) are trained using
large, sentence-aligned parallel corpora. Unfortu-
nately, parallel corpora are not always available in
large enough quantities to train robust models (Ko-
lachina et al, 2012). In this work, we consider the
situation in which we have access to only a small
amount of bitext for a given low resource language
pair, and we wish to supplement an SMT model
with additional translations and features estimated
using comparable corpora in the source and tar-
get languages. Assuming access to a small amount
?Performed while faculty at Johns Hopkins University
of parallel text is realistic, especially considering
the recent success of crowdsourcing translations
(Zaidan and Callison-Burch, 2011; Ambati, 2011;
Post et al, 2012).
We frame the shortcomings of SMT models
trained on limited amounts of parallel text1 in
terms of accuracy and coverage. In this con-
text, coverage refers to the number of words and
phrases that a model has any knowledge of at all,
and it is low when the training text is small, which
results in a high out-of-vocabulary (OOV) rate.
Accuracy refers to the correctness of the transla-
tion pairs and their corresponding probability fea-
tures that make up the translation model. Because
the quality of unsupervised automatic word align-
ments correlates with the amount of available par-
allel text and alignment errors result in errors in
extracted translation pairs, accuracy tends to be
low in low resource settings. Additionally, esti-
mating translation probabilities2 over sparse train-
ing sets results in inaccurate feature scores.
Given these deficiencies, we begin with a base-
line SMT model learned from a small parallel cor-
pus and supplement the model to improve its ac-
curacy and coverage. We apply techniques pre-
sented in prior work that use comparable corpora
to estimate similarities between word and phrases.
In particular, we build on prior work in bilingual
lexicon induction in order to predict translations
for OOV words, improving coverage. We then use
the same corpora to estimate additional translation
feature scores, improving model accuracy. We see
improvements in translation quality between 0.5
1We consider low resource settings to be those with par-
allel datasets of fewer than 1 million words. Most standard
MT datasets contain tens or hundreds of millions of words.
2Estimating reordering probabilities over sparse data also
leads to model inaccuracies; we do not tackle that here.
262
and 1.7 BLEU points translating the following low
resource languages into English: Tamil, Telugu,
Bengali, Malayalam, Hindi, and Urdu.
2 Previous Work
Prior work shows that a variety of signals, in-
cluding distributional, temporal, topic, and string
similarity, may inform bilingual lexicon induc-
tion (Rapp, 1995; Fung and Yee, 1998; Rapp,
1999; Schafer and Yarowsky, 2002; Koehn and
Knight, 2002; Monz and Dorr, 2005; Huang
et al, 2005; Schafer, 2006; Klementiev and
Roth, 2006; Haghighi et al, 2008; Mimno et al,
2009; Mausam et al, 2010). Other work has
used decipherment techniques to learn translations
from monolingual and comparable data (Ravi and
Knight, 2011; Dou and Knight, 2012; Nuhn et al,
2012). Daume? and Jagarlamudi (2011) use con-
textual and string similarity to mine translations
for OOV words in a high resource language do-
main adaptation for a machine translation setting.
Unlike most other prior work on bilingual lexicon
induction, Daume? and Jagarlamudi (2011) use the
translations in end-to-end SMT.
More recently, Irvine and Callison-Burch
(2013) combine a variety of the techniques for
estimating word pair similarity using source and
target language comparable corpora. That work
shows that only a small amount of supervision is
needed to learn how to effectively combine simi-
larity features into a single model for doing bilin-
gual lexicon induction. In this work, because we
assume access to a small amount of bilingual data,
it is natural to take such a supervised approach to
inducing new translations, and we directly apply
that of Irvine and Callison-Burch (2013).
Klementiev et al (2012) use comparable cor-
pora to score an existing Spanish-English phrase
table extracted from the Europarl corpus. In this
work, we directly apply their technique for scor-
ing an existing phrase table. However, unlike that
work, our initial phrase tables are estimated from
small parallel corpora for genuine low resource
languages. Additionally, we include new transla-
tions discovered in comparable corpora.
Other prior work has mined supplemental paral-
lel data from comparable corpora (Munteanu and
Marcu, 2006; AbduI-Rauf and Schwenk, 2009;
Smith et al, 2010; Uszkoreit et al, 2010; Smith et
al., 2013). Such efforts are orthogonal and com-
plementary to the approach that we take.
Language Train Words (k) Dev Types Dev TokensSent Dict % OOV % OOV
Tamil 335 77 44 25
Telugu 414 41 39 21
Bengali 240 7 37 18
Malayalam 263 151 6 3
Hindi 659 n/a 34 11
Urdu 616 116 23 6
Table 1: Information about datasets released by Post et al
(2012): thousands of words in the source language parallel
sentences and dictionaries, and percent of development set
word types (unique word tokens) and word tokens that are
OOV (do not appear in either section of the training data).
Language Web Crawls Wikipedia
Tamil 0.1 4.4
Telugu 0.4 8.6
Bengali 2.7 3.3
Malayalam 0.1 3.7
Hindi 18.1 6.4
Urdu 285 2.5
Table 2: Millions of words of time-stamped web crawls and
Wikipedia text, by language.
3 Using Comparable Corpora to
Improve Accuracy and Coverage
After describing our bilingual and comparable cor-
pora, we briefly describe the techniques proposed
by Irvine and Callison-Burch (2013) and Klemen-
tiev et al (2012). The contribution of this paper
is the application and combination of these tech-
niques in truly low resource translation conditions.
3.1 Datasets
Post et al (2012) used Mechanical Turk to col-
lect small parallel corpora for the following Indian
languages and English: Tamil, Telugu, Bengali,
Malayalam, Hindi, and Urdu. They collected both
parallel sentence pairs and a dictionary of word
translations.3 We use all six datasets, which pro-
vide real low resource data conditions for six truly
low resource language pairs. Table 1 shows statis-
tics about the datasets.
Table 2 lists the amount of comparable data
that we use for each language. Following both
Klementiev et al (2012) and Irvine and Callison-
Burch (2013), we use time-stamped web crawls
as well as interlingually linked Wikipedia docu-
ments. We use the time-stamped data to estimate
temporal similarity and the interlingual Wikipedia
links, which indicate documents about the same
topic written in different languages, to estimate
3No dictionary was provided for Hindi.
263
topic similarity. We use both datasets in combina-
tion with a dictionary derived from the small par-
allel corpora to estimate contextual similarity.
3.2 Improving Coverage
In order to improve the coverage of our low re-
source translation models, we use bilingual lexi-
con induction techniques to learn translations for
words which appear in our test sets but not in our
training data (OOVs). Bilingual lexicon induction
is the task of inducing pairs of words that are trans-
lations of one another from monolingual or com-
parable corpora. Irvine and Callison-Burch (2013)
use a diverse set of features estimated over compa-
rable corpora and a small set of known translations
as supervision for training a discriminative classi-
fier, which makes predictions (translation or not a
translation) on test set words paired with all pos-
sible translations. Possible translations are taken
from the set of all target words appearing in the
comparable corpora. Candidates are ranked ac-
cording to their classification scores. They achieve
very good performance on the induction task itself
compared with an unsupervised baseline that ag-
gregates the same similarity features uniformly. In
our setting, we have access to a small parallel cor-
pus, which makes such a supervised approach to
bilingual lexicon induction a natural choice.
We use the framework described in Irvine and
Callison-Burch (2013) directly, and further details
may be found there. In particular, we use the same
feature set, which includes the temporal, contex-
tual, topic, orthographic, and frequency similarity
between a candidate translation pair. We derive
translations to serve as positive supervision from
our automatically aligned parallel text4 and, like
the prior work, use random word pairs as nega-
tive supervision. Figure 1 shows some examples
of Bengali words, their correct translations, and
the top-3 translations that this framework induces.
In our initial experiments, we add the high-
est ranked English candidate translation for each
source language OOV to our phrase tables. Be-
cause all of the OOVs appear at least once in our
comparable corpora,5 we are able to mine transla-
tions for all of them. Adding these translations by
definition improves the coverage of our MT mod-
els. Then, in additional sets of experiments, we
4GIZA++ intersection alignments over all training data.
5The Post et al (2012) datasets are crowdsourced English
translations of source Wikipedia text. Using Wikipedia as
comparable corpora, we observe all OOVs at least once.
Source Induced Translations Correct Translation
???????????
mathematical
mathematicallyequal
ganitikovabe
????? 
function
functionfunctions
variables
?????? 
made
inauguration
goal
earned
Figure 1: Examples of OOV Bengali words, our top-3
ranked induced translations, and their correct translations.
also induce translations for source language words
which are low frequency in the training data and
supplement our SMT models with top-k transla-
tions, not just the highest ranked.
3.3 Improving Accuracy
In order to improve the accuracy of our mod-
els, we use comparable corpora to estimate ad-
ditional features over the translation pairs in our
phrase tables and include those features in tuning
and decoding. This approach follows that of Kle-
mentiev et al (2012). We compute both phrasal
features and lexically smoothed features (using
word alignments, like the Moses lexical transla-
tion probabilities) for all of the following except
orthographic similarity, for which we only use lex-
ically smoothed features,6 resulting in nine addi-
tional features: temporal similarity based on time-
stamped web crawls, contextual similarity based
on web crawls and Wikipedia (separately), ortho-
graphic similarity using normalized edit distance,
and topic similarity based on inter-lingually linked
Wikipedia pages. Our hope is that by adding a di-
verse set of similarity features to the phrase tables,
our models will better distinguish between good
and bad translation pairs, improving accuracy.
4 Experiments
4.1 Experimental setup
We use the data splits given by Post et al (2012)
and, following that work, include the dictionaries
in the training data and report results on the devtest
set using case-insensitive BLEU and four refer-
ences. We use the Moses phrase-based MT frame-
work (Koehn et al, 2007). For each language, we
extract a phrase table with a phrase limit of seven.
In order to make our results comparable to those
of Post et al (2012), we follow that work and use
6Because the words within a phrase pair are often re-
ordered, phrase-level orthographic similarity is unreliable.
264
Language Top-1 Acc. Top-10 Acc.
Tamil 4.5 10.2
Telugu 32.8 47.9
Bengali 17.9 29.8
Malayalam 12.9 23.0
Hindi 44.3 57.6
Urdu 16.1 33.8
Table 3: Percent of word types in a held out portion of the
training data which are translated correctly by our bilingual
lexicon induction technique. Evaluation is over the top-1 and
top-10 outputs in the ranked lists for each source word.
the English side of the training data to train a lan-
guage model. Using a language model trained on
a larger corpus (e.g. the English side of our com-
parable corpora) may yield better results, but such
an improvement is orthogonal to the focus of this
work. Throughout our experiments, we use the
batch version of MIRA (Cherry and Foster, 2012)
for tuning the feature set.7 We rerun tuning for
all experimental conditions and report results av-
eraged over three tuning runs (Clark et al, 2011).
Our baseline uses the bilingually extracted
phrase pairs and standard translation probability
features. We supplement it with the top ranked
translation for each OOV to improve coverage (+
OOV Trans) and with additional features to im-
prove accuracy (+Features). In Section 4.2, we
make each modification separately and then to-
gether. Then we present additional experiments
where we induce translations for low frequency
words, in addition to OOVs (4.3), append top-k
translations (4.4), vary the amount of training data
used to induce the baseline model (4.5), and vary
the amount of comparable corpora used to esti-
mate features and induce translations (4.6).
4.2 Results
Before presenting end-to-end MT results, we ex-
amine the performance of the supervised bilingual
lexicon induction technique that we use for trans-
lating OOVs. In Table 3, top-1 accuracy is the per-
cent of source language words in a held out portion
of the training data8 for which the highest ranked
English candidate is a correct translation.9 Perfor-
mance is lowest for Tamil and highest for Hindi.
For all languages, top-10 accuracy is much higher
than the top-1 accuracy. In Section 4.4, we explore
7We experimented with MERT and PRO as well but saw
consistently better baseline performance using batch MIRA.
8Described in Section 3.2. We retrain with all training
data for MT experiments.
9Post et al (2012) gathered up to six translations for each
source word, so some have multiple correct translations
appending the top-k translations for OOV words to
our model instead of just the top-1.
Table 4 shows our results adding OOV transla-
tions, adding features, and then both. Additional
translation features alone, which improve our
models? accuracy, increase BLEU scores between
0.18 (Bengali) and 0.60 (Malayalam) points.
Adding OOV translations makes a big differ-
ence for some languages, such as Bengali and
Urdu, and almost no difference for others, like
Malayalam and Tamil. The OOV rate (Table 1) is
low in the Malayalam dataset and high in the Tamil
dataset. However, as Table 3 shows, the translation
induction accuracy is low for both. Since few of
the supplemental translations are correct, we don?t
observe BLEU gains. In contrast, induction ac-
curacies for the other languages are higher, OOV
rates are substantial, and we do observe moderate
BLEU improvements by supplementing phrase ta-
bles with OOV translations.
In order to compute the potential BLEU gains
that we could realize by correctly translating all
OOV words (achieving 100% accuracy in Table
3), we perform an oracle experiment. We use au-
tomatic word alignments over the test sets to iden-
tify correct translations and append those to the
phrase tables.10 The results, in Table 4, show pos-
sible gains between 4.3 (Telugu and Bengali) and
0 (Malayalam) BLEU points above the baseline.
Not surprisingly, the possible gain for Malayalam,
which has a very low OOV rate, is very low. Our
+OOV Trans. model gains between 0% (Tamil)
and 38% (Urdu) of the potential improvement.
Using comparable corpora to improve both ac-
curacy (+Features) and coverage (+OOV Trans.)
results in translations that are better than apply-
ing either technique alone for five of the six lan-
guages. BLEU gains range from 0.48 (Bengali)
to 1.39 (Urdu). We attribute the particularly good
Urdu performance to the relatively large compa-
rable corpora (Table 2). As a result, we have al-
ready begun to expand our web crawls for all lan-
guages. In Section 4.6, we present results varying
the amount of Urdu-English comparable corpora
used to induce translations and estimate additional
features.
Table 4 also shows the Hiero (Chiang, 2005)
and SAMT (Zollmann and Venugopal, 2006) re-
sults that Post et al (2012) report for the same
10Because the automatic word alignments are noisy, this
oracle is conservative.
265
Tamil Telugu Bengali Malayalam Hindi Urdu
Experiment BLEU Diff. BLEU Diff. BLEU Diff. BLEU Diff. BLEU Diff. BLEU Diff.
Baseline 9.45 11.72 12.07 13.55 15.01 20.39
+Features 9.77 +0.32 11.96 +0.24 12.25 +0.18 14.15 +0.60 15.34 +0.33 20.97 +0.58
+OOV Trans. 9.45 0.00 12.20 +0.48 12.74 +0.67 13.65 +0.10 15.59 +0.58 21.30 +0.91
+Feats & OOV 9.98 +0.53 12.25 +0.53 12.55 +0.48 14.18 +0.63 16.08 +1.07 21.78 +1.39
OOV Oracle 12.32 +2.87 16.04 +4.32 16.41 +4.34 13.55 0.00 17.72 +2.71 22.80 2.41
Hiero 9.81 12.46 12.72 13.72 15.53 19.53
SAMT 9.85 12.61 13.53 14.28 17.29 20.99
Table 4: BLEU performance gains that target coverage (+OOV Trans.) and accuracy (+Features), and both (+Feats & OOV).
OOV oracle uses OOV translations from automatic word alignments. Hiero and SAMT results are reported in Post et al (2012).
datasets. Both syntax-based models outperform
the phrase-based MT baseline for each language
except Urdu, where the phrase-based model out-
performs Hiero. Here, we extend a phrase-based
rather than a syntax-based system because it is
simpler. However, our improvements may also ap-
ply to syntactic models (future work). Because our
efforts have focused on the accuracy and cover-
age of translation pairs and have not addressed re-
ordering or syntax, we expect that combining them
with an SAMT grammar will result in state-of-the
art performance.
4.3 Translations of Low Frequency Words
Given the positive results in Section 4.2, we hy-
pothesize that mining translations for low fre-
quency words, in addition to OOV words, may im-
prove accuracy. For source words which only ap-
pear a few times in the parallel training text, the
bilingually extracted translations in the standard
phrase table are likely to be inaccurate. There-
fore, we perform additional experiments varying
the minimum source word training data frequency
for which we induce additional translations. That
is, if freq(wsrc) ? M , we induce a new transla-
tion for it and include that translation in our phrase
table. Note that in the results presented in Table 4,
M = 0. In these experiments, we include our ad-
ditional phrase table features estimated over com-
parable corpora and hope that these scores will as-
sist the model in choosing among multiple trans-
lation options for low frequency words, one or
more of which is extracted bilingually and one of
which is induced using comparable corpora. Table
5 shows the results when we vary M . As before,
we average BLEU scores over three tuning runs.
In general, modest BLEU score gains are made
as we supplement our phrase-based models with
induced translations of low frequency words. The
highest performance is achieved when M is be-
tween 5 and 50, depending on language. The
Language Base. M : trans added for freq(wsrc) ? M0 1 5 10 25 50
Tamil 9.5 10.0 9.9 10.2 10.2 9.9 10.2
Telugu 11.7 12.3 12.2 12.3 12.4 12.3 11.9
Bengali 12.1 12.6 12.8 13.0 12.9 13.1 13.0
Malayalam 13.6 14.2 14.1 14.2 14.2 13.9 13.9
Hindi 15.0 16.1 16.1 16.2 16.2 16.0 15.8
Urdu 20.4 21.8 21.8 21.8 21.9 22.1 21.8
Table 5: Varying minimum parallel training data frequency
of source words for which new translations are induced and
included in the phrase-based model. In all cases, the top-1
induced translation is added to the phrase table and features
estimated over comparable corpora are included (i.e. +Feats
& Trans model).
largest gains are 0.5 and 0.3 BLEU points for Ben-
gali and Urdu, respectively, at M = 25. This
is not surprising; we also saw the largest rela-
tive gains for those two languages when we added
OOV translations to our baseline model. With the
addition of low frequency translations, our highest
performing Urdu model achieves a BLEU score
that is 1.7 points higher than the baseline.
In different data conditions, inducing transla-
tions for low frequency words may result in better
or worse performance. For example, the size of the
training set impacts the quality of automatic word
alignments, which in turn impacts the reliability
of translations of low frequency words. However,
the experiments detailed here suggest that includ-
ing induced translations of low frequency words
will not hurt performance and may improve it.
4.4 Appending Top-K Translations
So far we have only added the top-1 induced trans-
lation for OOV and low frequency source words to
our phrase-based model. However, the bilingual
lexicon induction results in Table 3 show that ac-
curacies in the top-10 ranked translations are, on
average, nearly twice the top-1 accuracies. Here,
we explore adding the top-k induced translations.
We hope that our additional phrase table features
estimated over comparable corpora will enable the
266
Language Base. k: top-k translations added1 3 5 10 25
Tamil 9.5 10.0 10.0 9.8 10.0 10.0
Telugu 11.7 12.3 11.7 11.9 11.7 11.6
Bengali 12.1 12.6 12.6 12.6 12.7 12.8
Malayalam 13.6 14.2 14.2 14.2 14.2 14.1
Hindi 15.0 16.1 16.0 15.9 15.9 15.9
Urdu 20.4 21.8 21.8 21.7 21.5 21.6
Table 6: Adding top-k induced translations for source lan-
guage OOV words, varying k. Features estimated over com-
parable corpora are included (i.e. +Feats & Trans model).
The highest BLEU score for each language is highlighted. In
many cases differences are less than 0.1 BLEU.
decoder to correctly choose between the k trans-
lation options. We induce translations for OOV
words only (M = 0) and include all comparable
corpora features.
Table 6 shows performance as we append the
top-k ranked translations for each OOV word and
vary k. With the exception of Bengali, using a
k greater than 1 does not increase performance.
In the case of Bengali, and additional 0.2 BLEU
is observed when the top-25 translations are ap-
pended. In contrast, we see performance decrease
substantially for other languages (0.7 BLEU for
Telugu and 0.2 for Urdu) when the top-25 trans-
lations are used. Therefore, we conclude that, in
general, the models do not sufficiently distinguish
good from bad translations when we append more
than just the top-1. Although using a k greater than
1 means that more correct translations are in the
phrase table, it also increases the number of possi-
ble outputs over which the decoder must search.
4.5 Learning Curves over Parallel Data
In the experiments above, we only evaluated our
methods for improving the accuracy and coverage
of models trained on small amounts of bitext us-
ing the full parallel training corpora released by
Post et al (2012). Here, we apply the same tech-
niques but vary the amount of parallel data in order
to generate learning curves. Figure 2 shows learn-
ing cures for all six languages. In all cases, results
are averaged over three tuning runs. We sample
both parallel sentences and dictionary entries.
All six learning curves show similar trends. In
all experimental conditions, BLEU performance
increases approximately linearly with the log of
the amount of training data. Additionally, supple-
menting the baseline with OOV translations im-
proves performance more than supplementing the
baseline with additional phrase table scores based
5 10 20 50 100 200
20.0
20.5
21.0
21.5
22.0
Comparable Corpora (Millions of Tokens)
BLE
U ? ? ? ? ?
?
?
Baseline+Trans.+Feats.+Trans. & Feats.
Figure 3: English to Urdu translation results using vary-
ing amounts of comparable corpora to estimate features and
induce translations.
on comparable corpora. However, in most cases,
supplementing the baseline with both translations
and features improves performance more than ei-
ther alone. Performance gains are greatest when
very little training data is used. The Urdu learning
curve shows the most gains as well as the clean-
est trends across training data amounts. As before,
we attribute this to the relatively large comparable
corpora available for Urdu.
4.6 Learning Curves over Comparable
Corpora
In our final experiment, we consider the effect of
the amount of comparable corpora that we use
to estimate features and induce translations. We
present learning curves for Urdu-English because
we have the largest amount of comparable corpora
for that pair. We use the full amount of paral-
lel data to train a baseline model, and then we
randomly sample varying amounts of our Urdu-
English comparable corpora. Sampling is done
separately for the web crawl and Wikipedia com-
parable corpora. Figure 3 shows the results. As
before, results are averaged over three tuning runs.
The phrase table features estimated over com-
parable corpora improve end-to-end MT perfor-
mance more with increasing amounts of compa-
rable corpora. In contrast, the amount of com-
parable corpora used to induce OOV translations
does not impact the performance of the resulting
MT system as much. The difference may be due
267
500 1000 2000 5000 10000 50000
0
5
10
15
20
Telugu
Training Data
BLE
U
l l l
l l
l l
l l
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(a) Telugu
500 1000 2000 5000 10000 20000
0
5
10
15
20
Bengali
Training Data
BLE
U
l l
l
l l
l l
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(b) Bengali
500 1000 2000 5000 20000 50000 200000
0
5
10
15
20
Malayalam
Training Data
BLE
U
l l
l l l l
l
l
l l
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(c) Malayalam
5 0 1000 2000 5000 10000 50000
0
5
10
15
20
Tamil
Training Data
BLE
U
l l l
l l
l
l l l
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(d) Tamil
500 1000 2000 5000 10000 20000
0
5
10
15
20
Hindi
Training Data
BLE
U
l
l l
l l
l l
l
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(e) Hindi
500 1000 2000 5000 20000 50000
0
5
10
15
20
Urdu
Training Data
BLE
U
l
l l
l
l
l
l
l
ll
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(f) Urdu
Figure 2: Comparison of learning curves over lines of parallel training data for four SMT systems: our
baseline phrase-based model (baseline), model that supplements the baseline with translations of OOV
words induced using our supervised bilingual lexicon induction framework (+Trans), model that supple-
ments the baseline with additional phrase table features estimated over comparable corpora (+Feats), and
a system that supplements the baseline with both OOV translations and additional features (+Trans &
Feats).
268
to the fact that data sparsity is always more of an
issue when estimating features over phrase pairs
than when estimating features over word pairs be-
cause phrases appear less frequently than words
in monolingual corpora. Our comparable cor-
pora features are estimated over phrase pairs while
translations are only induced for OOV words, not
phrases. So, it makes sense that the former would
benefit more from larger comparable corpora.
5 Conclusion
As Post et al (2012) showed, it is reasonable
to assume a small parallel corpus for training an
SMT model even in a low resource setting. We
have used comparable corpora to improve the ac-
curacy and coverage of phrase-based MT models
built using small bilingual corpora for six low re-
source languages. We have shown that our meth-
ods improve BLEU score performance indepen-
dently and that their combined impact is nearly ad-
ditive. Additionally, our results show that adding
induced translations of low frequency words im-
proves performance beyond what is achieved by
inducing translations for OOVs alone. Finally, our
results show that our techniques improve relative
performance most when very little parallel train-
ing data is available.
6 Acknowledgements
This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and
by the Johns Hopkins University Human Lan-
guage Technology Center of Excellence. The
views and conclusions contained in this publica-
tion are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
References
Sadaf AbduI-Rauf and Holger Schwenk. 2009. On
the use of comparable corpora to improve smt per-
formance. In Proceedings of the Conference of the
European Association for Computational Linguis-
tics (EACL).
Vamshi Ambati. 2011. Active Learning for Machine
Translation in Scarce Data Scenarios. Ph.D. thesis,
Carnegie Mellon University.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics (ACL).
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: controlling for
optimizer instability. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011.
Domain adaptation for machine translation by min-
ing unseen words. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Fei Huang, Ying Zhang, and Stephan Vogel. 2005.
Mining key phrase translations from web cor-
pora. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics (ACL).
Alex Klementiev, Ann Irvine, Chris Callison-Burch,
and David Yarowsky. 2012. Toward statistical ma-
chine translation without parallel corpora. In Pro-
ceedings of the Conference of the European Associ-
ation for Computational Linguistics (EACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
269
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Prasanth Kolachina, Nicola Cancedda, Marc Dymet-
man, and Sriram Venkatapathy. 2012. Prediction of
learning curves in machine translation. In Proceed-
ings of the Conference of the Association for Com-
putational Linguistics (ACL).
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sam-
mer, and Jeff Bilmes. 2010. Panlingual lexical
translation via probabilistic inference. Artificial In-
telligence, 174:619?637, June.
David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Christof Monz and Bonnie J. Dorr. 2005. Iterative
translation disambiguation for cross-language infor-
mation retrieval. In Proceedings of the Conference
on Research and Developments in Information Re-
trieval (SIGIR).
Dragos Munteanu and Daniel Marcu. 2006. Extracting
parallel sub-sentential fragments from non-parallel
corpora. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the Conference of the Association for Computa-
tional Linguistics (ACL).
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of
the Workshop on Statistical Machine Translation
(WMT).
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the Conference
of the Association for Computational Linguistics
(ACL).
Sujith Ravi and Kevin Knight. 2011. Deciphering
foreign language. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Charles Schafer. 2006. Translation Discovery Using
Diverse Similarity Measures. Ph.D. thesis, Johns
Hopkins University.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Jason Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt cheap web-scale paral-
lel text from the common crawl. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of the International Conference on Computa-
tional Linguistics (COLING).
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proceedings of the Con-
ference of the Association for Computational Lin-
guistics (ACL).
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation (WMT).
270
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 160?170,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Hallucinating Phrase Translations for Low Resource MT
Ann Irvine
Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch
Computer and Information Science Dept.
University of Pennsylvania
Abstract
We demonstrate that ?hallucinating?
phrasal translations can significantly im-
prove the quality of machine translation in
low resource conditions. Our hallucinated
phrase tables consist of entries composed
from multiple unigram translations drawn
from the baseline phrase table and from
translations that are induced from mono-
lingual corpora. The hallucinated phrase
table is very noisy. Its translations are low
precision but high recall. We counter this
by introducing 30 new feature functions
(including a variety of monolingually-
estimated features) and by aggressively
pruning the phrase table. Our analysis
evaluates the intrinsic quality of our
hallucinated phrase pairs as well as their
impact in end-to-end Spanish-English and
Hindi-English MT.
1 Introduction
In this work, we augment the translation model for
a low-resource phrase-based SMT system by auto-
matically expanding its phrase table. We ?halluci-
nate? new phrase table entries by composing the
unigram translations from the baseline system?s
phrase table and translations learned from compa-
rable monolingual corpora. The composition pro-
cess yields a very large number of new phrase pair
translations, which are high recall but low preci-
sion. We filter the phrase table using a new set of
feature functions estimated from monolingual cor-
pora. We evaluate the hallucinated phrase pairs in-
trinsically as well as in end-to-end machine trans-
lation. The augmented phrase table provides more
coverage than the original phrase table, while be-
ing high quality enough to improve translation per-
formance.
We propose a four-part approach to hallucinat-
ing and using new phrase pair translations:
1. Learn potential translations for out-of-
vocabulary (OOV) words from comparable
monolingual corpora
2. ?Hallucinate? a large, noisy set of phrase
translations by composing unigram transla-
tions from the baseline model and from the
monolingually-induced bilingual dictionary
3. Use comparable monolingual corpora to
score, rank, and prune the huge number of
hallucinated translations
4. Augment the baseline phrase table with hal-
lucinated translations and new feature func-
tions estimated from monolingual corpora
We define an algorithm for generating loosely
compositional phrase pairs, which we use to hal-
lucinate new translations. In oracle experiments,
we show that such loosely compositional phrase
pairs contribute substantially to the performance
of end-to-end SMT, beyond that of component un-
igram translations. In our non-oracle experiments,
we show that adding a judiciously pruned set of
automatically hallucinated phrase pairs to an end-
to-end baseline SMT model results in a signifi-
cant improvement in translation quality for both
Spanish-English and Hindi-English.
2 Motivation
Translation models learned over small amounts
of parallel data suffer from the problem of low
coverage. That is, they do not include trans-
lations for many words and phrases. Unknown
160
words, or out-of-vocabulary (OOV) words, have
been the focus of previous work on integrating
bilingual lexicon induction and machine transla-
tion (Daum?e and Jagarlamudi, 2011; Irvine and
Callison-Burch, 2013a; Razmara et al., 2013).
Bilingual lexicon induction is the task of learning
translations from monolingual texts, and typical
approaches compare projected distributional sig-
natures of words in the source language with dis-
tributional signatures representing target language
words (Rapp, 1995; Schafer and Yarowsky, 2002;
Koehn and Knight, 2002; Haghighi et al., 2008).
If the source and target language each contain, for
example, 100, 000 words, the number of pairwise
comparisons is about 10 billion, which is signifi-
cant but computationally feasible.
In contrast to unigrams, the difficulty in induc-
ing a comprehensive set of phrase translations is
that the number of both source and target phrases
is immense. For example, there are about 83 mil-
lion unique phrases up to length three in the En-
glish Wikipedia. Pairwise comparisons of two sets
of 100 million phrases corresponds to 1 x 10
16
.
Thus, even if we limit the task to short phrases, the
number of pairwise phrase comparisons necessary
to do an exhaustive search is infeasible. However,
multi-word translation units have been shown to
improve the quality of SMT dramatically (Koehn
et al., 2003). Phrase translations allow transla-
tion models to memorize local context-dependent
translations and reordering patterns.
3 Approach
Rather than compare all source language phrases
with all target language phrases, our approach effi-
ciently proposes a smaller set of hypothesis phrase
translations for each source language phrase. Our
method builds upon the notion that many phrase
translations can be composed from the translations
of its component words and subphrases. For ex-
ample Spanish la bruja verde translates into En-
glish as the green witch. Each Spanish word cor-
responds to exactly one English word. The phrase
pair could be memorized and translated as a unit,
or the English translation could be composed from
the translations of each Spanish unigram.
Zens et al. (2012) found that only 2% of phrase
pairs in German-English, Czech-English, Spanish-
English, and French-English phrase tables consist
of multi-word source and target phrases and are
non-compositional. That is, for these languages,
the vast majority of phrase pairs in a given phrase
table could be composed from smaller units. Our
approach takes advantage of the fact that many
phrases can be translated compositionally.
We describe our approach in three parts. In Sec-
tion 3.1, we begin by inducing translations for un-
known unigrams. Then, in 3.2, we introduce our
algorithm for composing phrase translations. In
order to achieve a high recall in our set of hypoth-
esis translations, we define compositionality more
loosely than is typical. Finally, in 3.3, we use com-
parable corpora to prune the large set of hypothesis
translations for each source phrase.
3.1 Unigram Translations
In any low resource setting, many word transla-
tions are likely to be unknown. Therefore, before
moving to phrases, we use a bilingual lexicon in-
duction technique to identify translations for un-
igrams. Specifically, because we assume a set-
ting where we have some small amount of paral-
lel data, we follow our prior work on supervised
bilingual lexicon induction (Irvine and Callison-
Burch, 2013b). We take examples of good transla-
tion pairs from our word aligned training data (de-
scribed in Section 4) and use random word pairs
as negative supervision. We use this supervision
to learn a log-linear classifier that predicts whether
a given word pair is a translation or not. We pair
and score all source language unigrams in our tun-
ing and test sets with target language unigrams that
appear in our comparable corpora. Then, for each
source language unigram, we use the log-linear
model scores to rerank candidate target language
unigram translations. As in our prior work, we
include the following word pair features in our
log-linear classifier: contextual similarity, tempo-
ral similarity, topic similarity, frequency similar-
ity, and orthographic similarity.
3.2 Loosely Compositional Translations
We propose a novel technique for loosely compos-
ing phrasal translations from an existing dictio-
nary of unigram translations and stop word lists.
Given a source language phrase, our approach
considers all combinations and all permutations
of all unigram translations for each source phrase
content word. We ignore stop words in the in-
put source phrase and allow any number of stop
words anywhere in the output target phrase. In
order to make the enumeration efficient, we pre-
compute an inverted index that maps sorted target
161
casa houselinda prettylinda cutelinda handsome
la casa linda
stop words removed
casa linda
Cartesian product?of unigram translationscute, house handsome, house house, pretty
Inverted Index lookups
pretty house the pretty house a pretty house cute house house and handsome
Bilingual Dictionary:
Input Phrase:
A
B
C
D
Figure 1: Example of loosely composed translations for the
Spanish input in A, la casa linda. In B, we remove the stop
word la. Then, in C, we enumerate the cartesian product of all
unigram translations in the bilingual dictionary and sort the
words within each alphabetically. Finally, we look up each
list of words in C in the inverted index, and corresponding
target phrases are enumerated in D. The inverted index con-
tains all phrasal combinations and permutations of the word
lists in C which also appear monolingually with some fre-
quency and with, optionally, any number of stop words.
language content words to sets of phrases contain-
ing those words in any order along with, option-
ally, any number of stop words. Our algorithm for
composing candidate phrase translations is given
in Algorithm 1, and an example translation is com-
posed in Figure 1. Although in our experiments
we compose translations for source phrases up to
length three, the algorithm is generally applicable
to any set of source phrases of interest.
Algorithm 1 yields a set of target language
translations for any source language phrase for
which all content unigrams have at least one
known translation. For most phrases, the result-
ing set of hypothesis translations is very large and
the majority are incorrect. In an initial pruning
step, we add a monolingual frequency cutoff to the
composition algorithm and only add target phrases
that have a frequency of at least ?
Freq
T
to the in-
verted index. Doing so eliminates improbable tar-
get language constructions early on, for example
house handsome her or cute a house.
Input: A set of source language phrases of interest, S,
each consisting of a sequence of words
s
m
1
, s
m
2
, ...s
m
i
; A list of all target language
phrases, targetPhrases; Source and target stop
word lists, Stop
src
and Stop
trg
; Set of unigram
translations, t
s
m
i
, for all source language words
s
m
i
R Stop
src
; monolingual target language
phrase frequencies, Freq
T
; Monolingual
frequency threshold ?
Freq
T
Output: @ Sm P S, a set of candidate phrase
translations, T
m
1
, T
m
2
, ...T
m
k
Construct TargetInvertedIndex:
for T P targetPhrases do
if Freq
T
pT q ? ?
Freq
T
then
T
1 ?words t
j
P T if t
j
R Stop
trg
T
1
sorted
? sortedpT 1q
append T to TargetInvertedIndex[T
1
sorted
]
end
end
for S
m P S do
S
1 ?words sm
i
P Sm if sm
i
R Stop
src
Combs
S
1 ? t
s
1
1
?
t
s
1
2
?
...
?
t
s
1
k
T ? r s
for c
s
1 P Combs
S
1
do
c
s
1
sorted
? sortedpc
s
1q
T ? T`TargetInvertedIndexpc
s
1
sorted
q
end
T
m ? T
end
Algorithm 1: Computing a set of candidate composi-
tional phrase translations for each source phrase in the set
S. An inverted index of target phrases is constructed that
maps sorted lists of content words to phrases that contain
those content words, as well as optionally any stop words,
and have a frequency of at least ?
Freq
T
. Then, for a given
source phrase S
m
, stop words are removed from the phrase.
Next, the cartesian product of all unigram translations is
computed. Each element in the product is sorted and any
corresponding phrases in the inverted index are added to the
output.
3.3 Pruning Phrase Pairs Using Scores
Derived from Comparable Corpora
We further prune the large, noisy set of hypothe-
sized phrase translations before augmenting a seed
translation model. To do so, we use a supervised
setup very similar to that used for inducing uni-
gram translations; we estimate a variety of sig-
nals that indicate translation equivalence, includ-
ing temporal, topical, contextual, and string simi-
larity. As we showed in Klementiev et al. (2012),
such signals are effective for identifying phrase
translations as well as unigram translations. We
add ngram length, alignment, and unigram trans-
lation features to the set, listed in Appendix A.
We learn a log-linear model for combining the
features into a single score for predicting the qual-
ity of a given phrase pair. We extract training data
from the seed translation model. We rank hypoth-
esis translations for each source phrase using clas-
162
sification scores and keep the top-k. We found that
using a score threshold sometimes improves pre-
cision. However, as experiments below show, the
recall of the set of phrase pairs is more important,
and we did not observe improvements in transla-
tion quality when we used a score threshold.
4 Experimental Setup
In all of our experiments, we assume that we have
access to only a small parallel corpus. For our
Spanish experiments, we randomly sample 2, 000
sentence pairs (about 57, 000 Spanish words) from
the Spanish-English Europarl v5 parallel corpus
(Koehn, 2005). For Hindi, we use the parallel cor-
pora released by Post et al. (2012). Again, we
randomly sample 2, 000 sentence pairs from the
training corpus (about 39, 000 Hindi words). We
expect that this amount of parallel text could be
compiled for a single text domain and any pair of
modern languages. Additionally, we use approxi-
mately 2, 500 and 1, 000 single-reference parallel
sentences each for tuning and testing our Span-
ish and Hindi models, respectively. Spanish tun-
ing and test sets are newswire articles taken from
the 2010 WMT shared task (Callison-Burch et al.,
2010).
1
We use the Hindi development and testing
splits released by Post et al. (2012).
4.1 Unigram Translations
Of the 16, 269 unique unigrams in the source side
of our Spanish MT tuning and test sets, 73% are
OOV with respect to our training corpus. 21% of
unigram tokens are OOV. For Hindi, 61% of the
8, 137 unique unigrams in the tuning and test sets
are OOV with respect to our training corpus, and
18% of unigram tokens are OOV. However, be-
cause automatic word alignments estimated over
the small parallel training corpora are noisy, we
use bilingual lexicon induction to induce transla-
tions for all unigrams. We use the Wikipedia and
online news web crawls datasets that we released
in Irvine and Callison-Burch (2013b) to estimate
similarity scores. Together, the two datasets con-
tain about 900 million words of Spanish data and
about 50 million words of Hindi data. For both
languages, we limit the set of hypothesis target un-
igram translations to those that appear at least 10
times in our comparable corpora.
We use 3, 000 high probability word translation
1
news-test2008 plus news-syscomb2009 for tuning and
newstest2009 for testing.
pairs extracted from each parallel corpus as posi-
tive supervision and 9, 000 random word pairs as
negative supervision. We use Vowpal Wabbit
2
for
learning. The top-5 induced translations for each
source language word are used as both a baseline
set of new translations (Section 6.3) and for com-
posing phrase translations.
4.2 Composing and Pruning Phrase
Translations
There are about 183 and 66 thousand unique bi-
grams and trigrams in the Spanish and Hindi tun-
ing and test sets, respectively. However, many
of these phrases do not demand new hypothesis
translations. We do not translate those which con-
tain numbers or punctuation. Additionally, for
Spanish, we exclude names, which are typically
translated identically between Spanish and En-
glish.
3
We exclude phrases which are sequences of
stop words only. Additionally, we exclude phrases
that appear more than 100 times in the small train-
ing corpus because our seed phrase table likely al-
ready contains high quality translations for them.
Finally, we exclude phrases that appear fewer than
20 times in our comparable corpora as our fea-
tures are unreliable when estimated over so few
tokens. We hypothesize translations for the ap-
proximately 15 and 6 thousand Spanish and Hindi
phrases, respectively, which meet these criteria.
Our approach for inducing translations straightfor-
wardly generalizes to any set of source phrases.
In defining loosely compositional phrase trans-
lations, we use both the induced unigram dictio-
nary (Section 3.1) and the dictionary extracted
from the word aligned parallel corpus. We ex-
pand these dictionaries further by mapping uni-
grams to their five-character word prefixes. We
use monolingual corpora of Wikipedia articles
4
to
construct stop word lists, containing the most fre-
quent 300 words in each language, and indexes of
monolingual phrase frequencies. There are about
83 million unique phrases up to length three in
the English Wikipedia. However, we ignore tar-
get phrases that appear fewer than three times, re-
ducing this set to 10 million English phrases. On
2
http://hunch.net/
?
vw/, version 6.1.4. with
standard learning parameters
3
Our names list comes from page titles of Spanish
Wikipedia pages about people. We iterate through years, be-
ginning with 1AD, and extract names from Wikipedia ?born
in? category pages, e.g. ?2013 births,? or ?Nacidos en 2013.?
4
All inter-lingually linked source language and English
articles.
163
average, our Spanish model yields 7, 986 English
translations for each Spanish bigram, and 9, 231
for each trigram, or less than 0.1% of all possi-
ble candidate English phrases. Our Hindi model
yields even fewer candidate English phrases, 826
for each bigram and 1, 113 for each trigram, on
average.
We use the same comparable corpora used for
bilingual lexicon induction to estimate features
over hypothesis phrase translations. The full fea-
ture set is listed in Appendix A. We extract su-
pervision from the seed translation models by first
identifying phrase pairs with multi-word source
strings, that appear at least three times in the train-
ing corpus, and that are composeable using base-
line model unigram translations and induced dic-
tionaries. Then, for each language pair, we use
the 3, 000 that have the highest ppf |eq scores as
positive supervision. We randomly sample 9, 000
compositional phrase pairs from those not in each
phrase table as negative supervision. Again, we
use Vowpal Wabbit for learning a log linear model
to score any phrase pair.
4.3 Machine Translation
We use GIZA++ to word align each training cor-
pus. We use the Moses SMT framework (Koehn et
al., 2007) and the standard phrase-based MT fea-
ture set, including phrase and lexical translation
probabilities and a lexicalized reordering model.
When we augment our models with new transla-
tions, we use the average reordering scores over
all bilingually estimated phrase pairs. We tune
all models using batch MIRA (Cherry and Fos-
ter, 2012). We average results over three tuning
runs and use approximate randomization to mea-
sure statistical significance (Clark et al., 2011).
For Spanish, we use a 5-gram language model
trained on the English side of the complete Eu-
roparl corpus and for Hindi a 5-gram language
model trained on the English side of the com-
plete training corpus released by Post et al. (2012).
We train our language models using SRILM with
Kneser-Ney smoothing. Our baseline models use
a phrase limit of three, and we augment them with
translations of phrases up to length three in our ex-
periments.
5 Oracle Experiment
Before moving to the results of our proposed
approach for composing phrase translations, we
present an oracle experiment to answer these re-
search questions: Would a low resource transla-
tion model benefit from composing its unigram
translations into phrases? Would this be fur-
ther improved by adding unigram translations that
are learned from monolingual texts? We an-
swer these questions by starting with our low-
resource Spanish-English and Hindi-English base-
lines and augmenting each with (1) phrasal trans-
lations composed from baseline model unigram
translations, and (2) phrasal translations composed
of a mix of baseline model unigram translations
and the monolingually-induced unigrams.
Figure 2 illustrates how our hallucinated phrase-
table entries can result in improved translation
quality for Spanish to English translation. Since
the baseline model is trained from such a small
amount of data, it typically translates individual
words instead of phrases. In our augmented sys-
tem, we compose a translation of was no one from
habia nadie, since habia translates as was in the
baseline model, nadie translates as one, and no is
a stop word. We are able to monolingually-induce
translations for the OOVs centros and electorales
before composing the phrase translation polling
stations for centros electorales.
In our oracle experiments, composed transla-
tions are only added to the phrase table if they
are contained in the reference. This eliminates the
huge number of noisy translations that our com-
positional algorithm generates. We augment base-
line models with translations for the same sets of
source language phrases described in Section 4.
We use GIZA++ to word align our tuning and
test sets
5
and use a standard phrase pair extraction
heuristic
6
to identify oracle phrase translations.
We add oracle translations to each baseline model
without bilingually estimated translation scores
7
because such scores are not available for our auto-
matically induced translations. Instead, we score
the oracle phrase pairs using the 30 new phrase ta-
ble features described in Section 3.3.
Table 1 shows the results of our oracle experi-
ments. Augmenting the baselines with the subset
of oracle translations which are composed given
the unigram translations in the baseline models
themselves (i.e. in the small training sets) yields
5
For both languages, we learn an alignment over our tun-
ing and test sets and complete parallel training sets.
6
grow-diag-final
7
We use an indicator feature for distinguishing new com-
posed translations from bilingually extracted phrase pairs.
164
not having dependent on the centros electorales .
no was no one in the polling stations .
no hab?a nadie en los centros electorales .
original composeable?from original original composeable?from induced original
Baseline:
Input:
Hallucination Oracle:
Figure 2: Example output from motivating experiment: a comparison of the baseline and full oracle translations of Spanish
no hab??a nadie en los centros electorales, which translates correctly as there was nobody at the voting offices. The full oracle
is augmented with translations composed from the seed model as well as induced unigram translations. The phrase was no one
is composeable from hab??a nadie given the seed model. In contrast, the phrase polling stations is composeable from centros
electorales using induced translations. For each translation, the phrase segmentations used by the decoder are highlighted.
Experiment
BLEU
Baseline Monolingually
Features Estimated Feats.
Spanish
Low Resource Baseline 13.47 13.35
+ Composeable Oracle
14.90 15.18
from Initial Model
+ Composeable Oracle
15.47 15.94
w/ Induced Unigram Trans.
Hindi
Low Resource Baseline 8.49 8.26
+ Composeable Oracle
9.12 9.54
from Initial Model
+ Composeable Oracle
10.09 10.19
w/ Induced Unigram Trans.
Table 1: Motivating Experiment: BLEU results using the
baseline SMT model and composeable oracle translations
with and without induced unigram translations.
a BLEU score improvement of about 1.4 points
for Spanish and about 0.6 for Hindi. This find-
ing itself is noteworthy, and we investigated the
reason for it. A representative example of a com-
positional oracle translation that was added to the
Spanish model is para evitarlos, which translates
as to prevent them. In the training corpus, para
translates far more frequently as for than to. Thus,
it is useful for the translation model to know that,
in the context of evitarlos, para should translate
as to and not for. Additionally, evitarlos was ob-
served only translating as the unigram prevent.
The small model fails to align the adjoined clitic
los with its translation them. However, our loose
definition of compositionality allows the English
stop word them to appear anywhere in the target
translation.
In the first result, composeable translations do
not include those that contain new, induced word
translations. Using the baseline model and in-
duced unigram translations to compose phrase
translations results in a 2 and 1.6 BLEU point gain
for Spanish and Hindi, respectively.
The second column of Table 1 shows the results
of augmenting the baseline models with the same
oracle phrase pairs as well as the new features esti-
mated over all phrase pairs. Although the features
do not improve the performance of the baseline
models, this diverse set of scores improves perfor-
mance dramatically when new, oracle phrase pairs
are added. Adding all oracle translations and the
new feature set results in a total gain of about 2.6
BLEU points for Spanish and about 1.9 for Hindi.
These gains are the maximum that we could hope
to achieve by augmenting models with our hallu-
cinated translations and new feature set.
6 Experimental Results
6.1 Unigram Translations
Table 2 shows examples of top ranked transla-
tions for several Spanish words. Although per-
formance is generally quite good, we do observe
some instances of false cognates, for example the
top ranked translation for aburridos, which trans-
lates correctly as bored, is burritos. Using au-
tomatic word alignments as a reference, we find
that 44% of Spanish tuning set unigrams have a
correct translation in their top-10 ranked lists and
62% in the top-100. For Hindi, 31% of tuning set
unigrams have a correct translation in their top-10
ranked lists and 43% in the top-100.
6.2 Hallucinated Phrase Pairs
Before moving to end-to-end SMT experiments,
we evaluate the goodness of the hallucinated and
pruned phrase pairs themselves. In order to do so,
we use the same set of oracle phrase translations
described in Section 5.
Table 3 shows the top three English transla-
tions for several Spanish phrases along with their
model scores. Common, loose translations of
some phrases are scored higher than less common
but literal translations. For example, very obvi-
165
Spanish abdominal abejorro abril aburridos accionista aceite actriz
Top 5
English
Translations
abdominal bumblebees april burritos actionists adulterated actress
abdomen bombus march boredom actionist iooc actor
bowel xylocopa june agatean telmex olive award
appendicitis ilyitch july burrito shareholder milliliters american
acute bumble december poof antagonists canola singer
Table 2: Top five induced translations for several source words. Correct translations are bolded. aceite translates as oil.
Spanish English Score
ambos partidos
two parties 5.72
both parties 5.31
and parties 3.16
hab??a apoyado
were supported 4.80
were members 4.52
had supported 4.39
ministro neerland`es
finnish minister 4.76
finnish ministry 2.77
dutch minister 1.31
unas cuantas semanas
over a week 4.30
a few weeks 3.72
few weeks 3.22
muy evidentes
very obvious 1.88
very evident 1.87
obviously very 1.84
Table 3: Top three compositional translations for several
source phrases and their model scores. Correct translations
are bolded.
ous scores higher than very evident as a translation
of Spanish muy evidentes. Similarly, dutch minis-
ter is scored higher than netherlands minister as a
translation for ministro neerland`es.
We use model scores to rerank candidate trans-
lations for each source phrase and keep the top-
k translations. Figure 3 shows the precision and
type-based recall (the percent of source phrases
for which at least one correct translation is gen-
erated) as we vary k for each language pair. At
k ? 1, precision and recall are about 27% for
Spanish and about 25% for Hindi.
8
At k ? 200,
recall increases to 57% for Spanish and precision
drops to 2%. For Hindi, recall increases to 40%
and precision drops to 1%.
Moving from k ? 1 to k ? 200, precision
drops at about the same rate for the two source lan-
guages. However, recall increases less for Hindi
than for Spanish. We attribute this to two things.
First, Hindi and English are less related than Span-
ish and English, and fewer phrases are translated
compositionally. Our oracle experiments showed
that there is less to gain in composing phrase trans-
lations for Hindi than for Spanish. Second, the
accuracy of our induced unigram translations is
lower for Hindi than it is for Spanish. Without ac-
curate unigram translations, we are unable to com-
pose high quality phrase translations.
8
Since we are computing type-based recall, and at k=1,
we produce exactly one translation for each source phrase,
precision and recall are the same.
l
l
l
l
l
l
l
l
l
l
l l0 10 20 30 40 50 60 700
20
40
60
80
Recall
Prec
ision
13.9014.0714.30 14.50 14.57
13.47
(a) Spanish
l
l
l
l
l
l
l
0 10 20 30 40 50 60 700
10
20
30
40
50
Recall
Prec
ision 8.16
8.868.899.009.04
8.49
(b) Hindi
Figure 3: Precision Recall curve with BLEU scores for the
top-k scored hallucinated translations. k varies from 1 to 200.
Baseline model performance is shown with a red triangle.
Because we hallucinate translations for source
phrases that appear in the training data up to 100
times, our baseline models include some of the
oracle phrase translations. Not surprisingly, the
bilingually extracted phrase pairs have relatively
high precision (81% and 40% for Spanish and
Hindi, respectively) and low recall (6% and 15%
for Spanish and Hindi, respectively).
6.3 End-to-End Translation
Table 4 shows end-to-end translation BLEU score
results (Papineni et al., 2002). Our first baseline
SMT models are trained using only 2, 000 paral-
lel sentences and no new translation model fea-
tures. Our Spanish baseline achieves a BLEU
score of 13.47 and our Hindi baseline a BLEU
score of 8.49. When we add the 30 new feature
functions estimated over comparable monolingual
corpora, performance is slightly lower, 13.35 for
166
Experiment
BLEU
Spanish Hindi
Baseline 13.47 8.49
+ Mono. Scores 13.35 8.26
+ Mono. Scores & OOV Trans 14.01 8.31
+ Phrase Trans, k=1 13.90 8.16
+ Phrase Trans, k=2 14.07 8.86*
+ Phrase Trans, k=5 14.30* 8.89*
+ Phrase Trans, k=25 14.50* 9.00*
+ Phrase Trans, k=200 14.57* 9.04*
Table 4: Experimental results. First, the baseline models
are augmented with monolingual phrase table features and
then also with the top-5 induced translations for all OOV un-
igrams. Then, we append the top-k hallucinated phrase trans-
lations to the third baseline models. BLEU scores are aver-
aged over three tuning runs. We measure the statistical sig-
nificance of each +Phrase Trans model in comparison with
the highest performing (bolded) baseline for each language;
* indicates statistical significance with p ? 0.01.
Spanish and 8.26 for Hindi. Our third baselines
augment the second with unigram translations for
all OOV tuning and test set source words using the
bilingual lexicon induction techniques described
in Section 3.1. We append the top-5 translations
for each,
9
score both the original and the new
phrase pairs with the new feature set, and retune.
With these additional unigram translations, perfor-
mance increases to 14.01 for Spanish and 8.31 for
Hindi.
We append the top-k composed translations for
the source phrases described in Section 4 to the
third baseline models. Both original and new
phrase pairs are scored using the new feature set.
BLEU score results are shown at different values
of k along the precision-recall plots for each lan-
guage pair in Figure 3 as well as in Table 4. We
would expect that higher precision and higher re-
call would benefit end-to-end SMT. As usual, a
tradeoff exists between precision and recall, how-
ever, in this case, improvements in recall outweigh
the risk of a lower precision. As k increases, pre-
cision decreases but both recall and BLEU scores
increase. For both Spanish and Hindi, BLEU score
gains start to taper off at k values over 25.
In additional experiments, we found that with-
out the new features the same sets of hallucinated
phrase pairs hurt performance slightly in compar-
ison with the baseline augmented with unigram
translations, and results don?t change as we vary
k.
10
Thus, the translation models are able to ef-
fectively use the higher recall sets of new phrase
9
The same set used for composing phrase translations.
10
For all values of k between 1 and 100, without the new
features, BLEU scores are about 13.70 for Spanish
pairs because we also augmented the models with
30 new feature functions, which help them distin-
guish good from bad translations.
7 Discussion
Our results showed that including a high recall
set of ?hallucinated? translations in our augmented
phrase table successfully improved the quality of
our machine translations. The algorithm that we
proposed for hypothesizing translations is flexible,
and in future work we plan to modify it slightly
to output even more candidate translations. For
example, we could retrieve target phrases which
contain at least one source word translation instead
of all. Alternatively, we could identify candidates
using entirely different information, for example
the monolingual frequency of a source and target
word, instead of unigram translations. This type
of inverted index may improve recall in the set of
hypothesis phrase translations at the cost of gener-
ating a much bigger set for reranking.
Our new phrase table features were informa-
tive in distinguishing correct from incorrect phrase
translations, and they allowed us to make use of
noisy but high recall supplemental phrase pairs.
This is a critical result for research on identify-
ing phrase translations from non-parallel text. We
also believe that using fairly strong target (En-
glish) language models contributed to our models?
ability to discriminate between good and bad hal-
lucinated phrase pairs. We leave research on the
influence of the language model in our setting to
future work.
In this work, we experimented with two lan-
guage pairs, Spanish-English and Hindi-English.
While Spanish and English are very closely re-
lated, Hindi and English are less related. Our
oracle experiments showed potential for compos-
ing phrase translations for both language pairs,
and, indeed, in our experiments using hallucinated
phrase translations we saw significant translation
quality gains for both. We expect that improving
the quality of induced unigram translations will
yield even more performance gains.
The vast majority of prior work on low resource
MT has focused on Spanish-English (Haghighi
et al., 2008; Klementiev et al., 2012; Ravi and
Knight, 2011; Dou and Knight, 2012; Ravi, 2013;
Dou and Knight, 2013). Although such experi-
ments serve as important proofs of concept, we
found it important to also experiment with a more
167
truly low resource language pair. The success of
our approach that we have seen for Spanish and
Hindi suggests that it is worth pursuing such di-
rections for other even less related and resourced
language pairs. In addition to language pair, text
genre and the degree of looseness or literalness of
given parallel corpora may also affect the amount
of phrase translation compositionality.
8 Related Work
Phrase-based SMT models estimated over very
large parallel corpora are expensive to store and
process. Prior work has reduced the size of SMT
phrase tables in order to improve efficiency with-
out the loss of translation quality (He et al., 2009;
Johnson et al., 2007; Zens et al., 2012). Typi-
cally, the goal of pruning is to identify and re-
move phrase pairs which are likely to be inaccu-
rate, using either the scores and counts of a given
pair itself or those relative to other phrase pairs.
Our work, in contrast, focuses on low resource set-
tings, where training data is limited and provides
incomplete and unreliable scored phrase pairs. We
begin by dramatically increasing the size of our
SMT phrase table in order to expand its coverage
and then use non-parallel data to rescore and filter
the table.
In the decipherment task, translation models
are learned from comparable corpora without any
parallel text (Ravi and Knight, 2011; Dou and
Knight, 2012; Ravi, 2013). In contrast, we be-
gin with a small amount of parallel data and take
a very different approach to learning translation
models. In our prior work (Irvine and Callison-
Burch, 2013b), we showed how effective even
small amounts of bilingual data can be for learning
translations from monolingual texts.
Garera and Yarowsky (2008) pivot through
bilingual dictionaries in several language pairs to
compose translations for compound words. Zhang
and Zong (2013) construct a set of new, additional
phrase pairs for the task of domain adaptation for
machine translation. That work uses two dictio-
naries to bootstrap a set of phrase pair transla-
tions: one probabilistic dictionary extracted from
2 million words of bitext and one manually created
new-domain dictionary of 140, 000 word transla-
tions. Our approach to the construction of new
phrase pairs is somewhat similar to Zhang and
Zong (2013), but we don?t rely on a very large
manually generated dictionary. Additionally, we
focus on the low resource language pair setting,
where a large training corpus is not available.
Deng et al. (2008) work in a standard SMT set-
ting but use a discriminative framework for ex-
tracting phrase pairs from parallel corpora. That
approach yields a phrase table with higher preci-
sion and recall than the table extracted by stan-
dard world alignment based heuristics (Och and
Ney, 2003; Koehn et al., 2003). The discrimi-
native model combines features from word align-
ments and bilingual training data as well as infor-
mation theoretic features estimated over monolin-
gual data into a single log-linear model and then
the phrase pairs are filtered using a threshold on
model scores. The phrase pairs that it extracts are
limited to those that appear in pairs of sentences in
the parallel training data. Our work takes a similar
approach to that of Deng et al. (2008), however,
unlike that work, we hallucinate phrase pairs that
did not appear in training data in order to augment
the original, bilingually extracted phrase table.
Other prior work has used comparable cor-
pora to extract parallel sentences and phrases
(Munteanu and Marcu, 2006; Smith et al., 2010).
Such efforts are orthogonal to our approach. We
use parallel corpora, when available, and hallu-
cinates phrase translations without assuming any
parallel text in our comparable corpora.
9 Conclusions
We showed that ?hallucinating? phrasal transla-
tions can significantly improve machine transla-
tion performance in low resource conditions. Our
hallucinated translations are composed from uni-
gram translations. The translations are low preci-
sion but high recall. We countered this by intro-
ducing new feature functions and pruning aggres-
sively.
10 Acknowledgements
This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and
by the Johns Hopkins University Human Lan-
guage Technology Center of Excellence. The
views and conclusions contained in this publica-
tion are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
168
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Workshop on Sta-
tistical Machine Translation (WMT).
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: controlling for
optimizer instability. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Hal Daum?e, III and Jagadeesh Jagarlamudi. 2011.
Domain adaptation for machine translation by min-
ing unseen words. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Yonggang Deng, Jia Xu, and Yuqing Gao. 2008.
Phrase table training for precision and recall: What
makes a good phrase and a good phrase pair? In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Qing Dou and Kevin Knight. 2012. Large scale
decipherment for out-of-domain machine transla-
tion. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP/CoNLL).
Qing Dou and Kevin Knight. 2013. Dependency-
based decipherment for resource-limited machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Nikesh Garera and David Yarowsky. 2008. Translating
compounds by learning component gloss translation
models via multiple languages. In Proceedings of
the International Joint Conference on Natural Lan-
guage Processing (IJCNLP).
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Zhongjun He, Yao Meng, and Hao Yu. 2009. Dis-
carding monotone composed rule for hierarchical
phrase-based statistical machine translation. In Pro-
ceedings of the 3rd International Universal Commu-
nication Symposium.
Ann Irvine and Chris Callison-Burch. 2013a. Com-
bining bilingual and comparable corpora for low
resource machine translation. In Proceedings of
the Workshop on Statistical Machine Translation
(WMT).
Ann Irvine and Chris Callison-Burch. 2013b. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation quality
by discarding most of the phrasetable. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP/CoNLL).
Alex Klementiev, Ann Irvine, Chris Callison-Burch,
and David Yarowsky. 2012. Toward statistical ma-
chine translation without parallel corpora. In Pro-
ceedings of the Conference of the European Associ-
ation for Computational Linguistics (EACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Machine Translation Summit.
Prasanth Kolachina, Nicola Cancedda, Marc Dymet-
man, and Sriram Venkatapathy. 2012. Prediction of
learning curves in machine translation. In Proceed-
ings of the Conference of the Association for Com-
putational Linguistics (ACL).
Dragos Munteanu and Daniel Marcu. 2006. Extracting
parallel sub-sentential fragments from non-parallel
corpora. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
169
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the Conference of the Association for Computa-
tional Linguistics (ACL).
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of
the Workshop on Statistical Machine Translation
(WMT).
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Sujith Ravi and Kevin Knight. 2011. Deciphering
foreign language. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Sujith Ravi. 2013. Scalable decipherment for machine
translation via hash sampling. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP/CoNLL).
Jiajun Zhang and Chengqing Zong. 2013. Learning
a phrase-based translation model from monolingual
data with application to domain adaptation. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics (ACL).
Appendix A: Phrase pair filtering features
The first ten features are similar to those described
by Irvine and Callison-Burch (2013b). Stop words
are defined as the most frequent 300 words in each
language?s Wikipedia, and content words are all
non-stop words.
? Web crawl phrasal context similarity score
? Web crawl lexical context similarity score, averaged over
aligned unigrams
? Web crawl phrasal temporal similarity score
? Web crawl lexical temporal similarity score, averaged
over aligned unigrams
? Wikipedia phrasal context similarity score
? Wikipedia lexical context similarity score, averaged over
aligned unigrams
? Wikipedia phrasal topic similarity score
? Wikipedia lexical topic similarity score, averaged over
aligned unigrams
? Normalized edit distance, averaged over aligned unigrams
? Absolute value of difference between the logs of the
source and target phrase Wikipedia monolingual frequen-
cies
? Log target phrase Wikipedia monolingual frequency
? Log source phrase Wikipedia monolingual frequency
? Indicator: source phrase is longer
? Indicator: target phrase is longer
? Indicator: source and target phrases same length
? Number of source content words higher than target
? Number of target content words higher than source
? Number of source and target content words same
? Number of source stop words higher than target
? Number of target stop words higher than source
? Number of source and target stop words same
? Percent of source words aligned to at least one target word
? Percent of target words aligned to at least one source word
? Percent of source content words aligned to at least one
target word
? Percent of target content words aligned to at least one
source word
? Percent of aligned word pairs aligned in bilingual training
data
? Percent of aligned word pairs in induced dictionary
? Percent of aligned word pairs in stemmed induced dictio-
nary
170
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437?444,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Using Comparable Corpora to Adapt MT Models to New Domains
Ann Irvine
Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch
Computer and Information Science Dept.
University of Pennsylvania
Abstract
In previous work we showed that when us-
ing an SMT model trained on old-domain
data to translate text in a new-domain,
most errors are due to unseen source
words, unseen target translations, and in-
accurate translation model scores (Irvine
et al., 2013a). In this work, we target er-
rors due to inaccurate translation model
scores using new-domain comparable cor-
pora, which we mine from Wikipedia. We
assume that we have access to a large old-
domain parallel training corpus but only
enough new-domain parallel data to tune
model parameters and do evaluation. We
use the new-domain comparable corpora
to estimate additional feature scores over
the phrase pairs in our baseline models.
Augmenting models with the new features
improves the quality of machine transla-
tions in the medical and science domains
by up to 1.3 BLEU points over very strong
baselines trained on the 150 million word
Canadian Hansard dataset.
1 Introduction
Domain adaptation for machine translation is
known to be a challenging research problem that
has substantial real-world application. In this set-
ting, we have access to training data in some old-
domain of text but very little or no training data
in the domain of the text that we wish to translate.
For example, we may have a large corpus of par-
allel newswire training data but no training data in
the medical domain, resulting in low quality trans-
lations at test time due to the mismatch.
In Irvine et al. (2013a), we introduced a tax-
onomy for classifying machine translation errors
related to lexical choice. Our ?S4? taxonomy in-
cludes seen, sense, score, and search errors. Seen
errors result when a source language word or
phrase in the test set was not observed at all during
training. Sense errors occur when the source lan-
guage word or phrase was observed during train-
ing but not with the correct target language trans-
lation. If the source language word or phrase was
observed with its correct translation during train-
ing, but an incorrect alternative outweighs the cor-
rect translation, then a score error has occurred.
Search errors are due to pruning in beam search
decoding. We measured the impact of each error
type in a domain adaptation setting and concluded
that seen and sense errors are the most frequent but
that there is also room for improving errors due to
inaccurate translation model scores (Irvine et al.,
2013a). In this work, we target score errors, using
comparable corpora to reduce their frequency in a
domain adaptation setting.
We assume the setting where we have an old-
domain parallel training corpus but no new domain
training corpus.
1
We do, however, have access
to a mixed-domain comparable corpus. We iden-
tify new-domain text within our comparable cor-
pus and use that data to estimate new translation
features on the translation models extracted from
old-domain training data. Specifically, we focus
on the French-English language pair because care-
fully curated datasets exist in several domains for
tuning and evaluation. Following our prior work,
we use the Canadian Hansard parliamentary pro-
ceedings as our old-domain and adapt models to
both the medical and the science domains (Irvine
et al., 2013a). At over 8 million sentence pairs,
1
Some prior work has referred to old-domain and new-
domain corpora as out-of-domain and in-domain, respec-
tively.
437
the Canadian Hansard dataset is one of the largest
publicly available parallel corpora and provides a
very strong baseline. We give details about each
dataset in Section 4.1.
We use comparable corpora to estimate sev-
eral signals of translation equivalence. In partic-
ular, we estimate the contextual, topic, and or-
thographic similarity of each phrase pair in our
baseline old-domain translation model. In Sec-
tion 3, we describe each feature in detail. Us-
ing just 5 thousand comparable new-domain doc-
ument pairs, which we mine from Wikipedia, and
five new phrase table features, we observe perfor-
mance gains of up to 1.3 BLEU points on the sci-
ence and medical translation tasks over very strong
baselines.
2 Related Work
Recent work on machine translation domain adap-
tation has focused on either the language model-
ing component or the translation modeling com-
ponent of an SMT model. Language modeling re-
search has explored methods for subselecting new-
domain data from a large monolingual target lan-
guage corpus for use as language model training
data (Lin et al., 1997; Klakow, 2000; Gao et al.,
2002; Moore and Lewis, 2010; Mansour et al.,
2011). Translation modeling research has typi-
cally assumed that either (1) two parallel datasets
are available, one in the old domain and one in the
new, or (2) a large, mixed-domain parallel training
corpus is available. In the first setting, the goal is
to effectively make use of both the old-domain and
the new-domain parallel training corpora (Civera
and Juan, 2007; Koehn and Schroeder, 2007; Fos-
ter and Kuhn, 2007; Foster et al., 2010; Haddow
and Koehn, 2012; Haddow, 2013). In the sec-
ond setting, it has been shown that, in some cases,
training a translation model on a subset of new-
domain parallel training data within a larger train-
ing corpus can be more effective than using the
complete dataset (Mansour et al., 2011; Axelrod
et al., 2011; Sennrich, 2012; Gasc?o et al., 2012).
For many language pairs and domains, no new-
domain parallel training data is available. Wu et
al. (2008) machine translate new-domain source
language monolingual corpora and use the syn-
thetic parallel corpus as additional training data.
Daum?e and Jagarlamudi (2011), Zhang and Zong
(2013), and Irvine et al. (2013b) use new-domain
comparable corpora to mine translations for un-
seen words. That work follows a long line of re-
search on bilingual lexicon induction (e.g. Rapp
(1995), Schafer and Yarowsky (2002), Koehn and
Knight (2002), Haghighi et al. (2008), Irvine and
Callison-Burch (2013), Razmara et al. (2013)).
These efforts improve S4 seen, and, in some in-
stances, sense error types. To our knowledge,
no prior work has focused on fixing errors due
to inaccurate translation model scores in the set-
ting where no new-domain parallel training data is
available.
In Klementiev et al. (2012), we used compara-
ble corpora to estimate several features for a given
phrase pair that indicate translation equivalence,
including contextual, temporal, and topical simi-
larity. The definitions of phrasal and lexical con-
textual and topic similarity that we use here are
taken from our prior work, where we replaced
bilingually estimated phrase table features with
the new features and cited applications to low re-
source SMT. In this work we also focus on scoring
a phrase table using comparable corpora. How-
ever, here we work in a domain adaptation setting
and seek to augment, not replace, an existing set
of bilingually estimated phrase table features.
3 Phrase Table Scoring
We begin with a scored phrase table estimated us-
ing our old-domain parallel training corpus. The
phrase table contains about 201 million unique
source phrases up to length seven and about 479
million total phrase pairs. We use Wikipedia as a
source for comparable document pairs (details are
given in Section 4.1). We augment the bilingually
estimated features with the following: (1) lexical
and phrasal contextual similarity estimated over a
comparable corpus, (2) lexical and phrasal topi-
cal similarity estimated over a comparable corpus,
and (3) lexical orthographic similarity.
Contextual Similarity We estimate contextual
similarity
2
by first computing a context vector for
each source and target word and phrase in our
phrase table using the source and target sides of
our comparable corpus, respectively. We begin by
collecting vectors of counts of words that appear
in the context of each source and target phrase, p
s
and p
t
. We use a bag-of-words context consist-
ing of the two words to the left and two words to
2
Similar to distributional similarity, which is typically de-
fined monolingually.
438
the right of each occurrence of each phrase. Vari-
ous means of computing the component values of
context vectors from raw context frequency counts
have been proposed (e.g. Rapp (1999), Fung and
Yee (1998)). Following Fung and Yee (1998), we
compute the value of the k-th component of p
s
?s
contextual vector, C
p
s
, as follows:
C
p
s
k
? n
p
s
,k
? plogpn{n
k
q ` 1q
where n
p
s
,k
and n
k
are the number of times the
k-th source word, s
k
, appears in the context of p
s
and in the entire corpus, and n is the maximum
number of occurrences of any word in the data.
Intuitively, the more frequently s
k
appears with p
s
and the less common it is in the corpus in general,
the higher its component value. The context vector
for p
s
, C
p
s
, is M -dimensional, where M is the
size of the source language vocabulary. Similarly,
we compute N -dimensional context vectors for all
target language words and phrases, where N is the
size of the target language vocabulary.
We identify the most probable translation t for
each of the M source language words, s, as the
target word with the highest ppt|sq under our word
aligned old-domain training corpus. Given this
dictionary of unigram translations, we then project
each M -dimensional source language context vec-
tor into the N -dimensional target language context
vector space. To compare a given pair of source
and target context vectors, C
p
s
and C
p
t
, respec-
tively, we compute their cosine similarity, or their
dot product divided by the product of their magni-
tudes:
sim
contextual
pp
s
, p
t
q ?
C
p
s
? C
p
t
||C
p
s
||||C
p
t
||
For a given phrase pair in our phrase table, we
estimate phrasal contextual similarity by directly
comparing the context vectors of the two phrases
themselves. Because context vectors for phrases,
which tend to be less frequent than words, can be
sparse, we also compute lexical contextual simi-
larity over phrase pairs. We define lexical con-
textual similarity as the average of the contextual
similarity between all word pairs within the phrase
pair.
Topic Similarity Phrases and their translations
are likely to appear in articles written about the
same topic in two languages. We estimate topic
similarity using the distribution of words and
phrases across Wikipedia pages, for which we
have interlingual French-English links. Specif-
ically, we compute topical vectors by counting
the number of occurrences of each word and
phrase across Wikipedia pages. That is, for each
source and target phrase, p
s
and p
t
, we collect M -
dimensional topic vectors, where M is the number
of Wikipedia page pairs used (in our experiments,
M is typically 5, 000). We use Wikipedia?s inter-
lingual links to align the French and English topic
vectors and normalize each topic vector by the to-
tal count. As with contextual similarity, we com-
pare a pair of source and target topic vectors, T
p
s
and T
p
t
, respectively, using cosine similarity:
sim
topic
pp
s
, p
t
q ?
T
p
s
? T
p
t
||T
p
s
||||T
p
t
||
We estimate both phrasal and lexical topic simi-
larity for each phrase pair. As before, lexical topic
similarity is estimated by taking an average topic
similarity across all word pairs in a given phrase
pair.
Orthographic Similarity We make use of one
additional signal of translation equivalence: ortho-
graphic similarity. In this case, we do not refer-
ence comparable corpora but simply compute the
edit distance between a given pair of phrases. This
signal is often useful for identifying translations
of technical terms, which appear frequently in our
medical and science domain corpora. However,
because of word order variation, we do not mea-
sure edit distance on phrase pairs directly. For ex-
ample, French embryon humain translates as En-
glish human embryo; embryon translates as em-
bryo and humain translates as human. Although
both word pairs are cognates, the words appear
in opposite orders in the two phrases. Therefore,
directly measuring string edit distance across the
phrase pair would not effectively capture the relat-
edness of the words. Hence, we only measure lex-
ical orthographic similarity, not phrasal. We com-
pute lexical orthographic similarity by first com-
puting the edit distance between each word pair,
w
s
and w
t
, within a given phrase pair, normalized
by the lengths of the two words:
sim
orth
pw
s
, w
t
q ?
edpw
s
, w
t
q
|w
s
||w
t
|
2
We then compute the average normalized edit dis-
tance across all word pairs.
The above similarity metrics all allow for scores
of zero, which can be problematic for our log-
439
Corpus Source Words Target Words
Training
Canadian Hansard 161.7 m 144.5 m
Tune-1 / Tune-2 / Test
Medical 53k / 43k / 35k 46k / 38k / 30k
Science 92k / 120k / 120k 75k / 101k / 101k
Language Modeling and Comparable Corpus Selection
Medical - 5.9 m
Science - 3.6 m
Table 1: Summary of the size of each corpus of text used
in this work in terms of the number of source and target word
tokens.
linear translation models. We describe our ex-
periments with different minimum score cutoffs in
Section 4.2.
4 Experimental Setup
4.1 Data
We assume that the following data is available in
our translation setting:
? Large old-domain parallel corpus for training
? Small new-domain parallel corpora for tun-
ing and testing
? Large new-domain English monolingual cor-
pus for language modeling and identifying
new-domain-like comparable corpora
? Large mixed-domain comparable corpus,
which includes some text from the new-
domain
These data conditions are typical for many real-
world uses of machine translation. A summary of
the size of each corpus is given in Table 1.
Our old-domain training data is taken from
the Canadian Hansard parliamentary proceedings
dataset, which consists of manual transcriptions
and translations of meetings of the Canadian par-
liament. The dataset is substantially larger than
the commonly used Europarl corpus, containing
over 8 million sentence pairs and about 150 mil-
lion word tokens of French and English.
For tuning and evaluation, we use new-domain
medical and science parallel datasets released by
Irvine et al. (2013a). The medical texts con-
sist of documents from the European Medical
Agency (EMEA), originally released by Tiede-
mann (2009). This data is primarily taken from
prescription drug label text. The science data is
made up of translated scientific abstracts from the
fields of physics, biology, and computer science.
For both the medical and science domains, we
use three held-out parallel datasets of about 40
and 100 thousand words,
3
respectively, released
by Irvine et al. (2013a). We do tuning on dev1,
additional parameter selection on test2, and blind
testing on test1.
We use large new-domain monolingual English
corpora for language modeling and for selecting
new-domain-like comparable corpora from our
mixed domain comparable corpus. Specifically,
we use the English side of the medical and science
training datasets released by Irvine et al. (2013a).
We do not use the parallel French side of the train-
ing data at all; our data setting assumes that no
new-domain parallel data is available for training.
We use Wikipedia as a source of compara-
ble corpora. There are over half a million
pairs of inter-lingually linked French and English
Wikipedia documents.
4
We assume that we have
enough monolingual new-domain data in one lan-
guage to rank Wikipedia pages according to how
new-domain-like they are. In particular, we use
our new-domain English language modeling data
to measure new-domain-likeness. We could have
targeted our learning even more by using our new-
domain French test sets to select comparable cor-
pora. Doing so may increase the similarity be-
tween our test data and comparable corpora. How-
ever, to avoid overfitting any particular test set, we
use our large English new-domain language mod-
eling corpus instead.
For each inter-lingually linked pair of French
and English Wikipedia documents, we compute
the percent of English phrases up to length four
that are observed in the English monolingual new-
domain corpus and rank document pairs by the ge-
ometric mean of the four overlap measures. More
sophisticated ways to identify new-domain-like
Wikipedia pages (e.g. (Moore and Lewis, 2010))
may yield additional performance gains, but, qual-
itatively, the ranked Wikipedia pages seem rea-
sonable for the purposes of generating a large set
of top-k new-domain document pairs. The top-10
ranked pages for each domain are listed in Table 2.
The top ranked science domain pages are primar-
ily related to concepts from the field of physics
but also include computer science and chemistry
3
Or about 4 thousand lines each. The sentences in the
medical domain text are much shorter than those in the sci-
ence domain.
4
As of January 2014.
440
Science Medical
Diagnosis (artificial intelligence) Pregabalin
Absorption spectroscopy Cetuximab
Spectral line Fluconazole
Chemical kinetics Calcitonin
Mahalanobis distance Pregnancy category
Dynamic light scattering Trazodone
Amorphous solid Rivaroxaban
Magnetic hyperthermia Spironolactone
Photoelasticity Anakinra
Galaxy rotation curve Cladribine
Table 2: Top 10 Wikipedia articles ranked by their similar-
ity to large new-domain English monolingual corpora.
topics. The top ranked medical domain pages are
nearly all prescription drugs, which makes sense
given the content of the EMEA medical corpus.
4.2 Phrase-based Machine Translation
We word align our old-domain training corpus
using GIZA++ and use the Moses SMT toolkit
(Koehn et al., 2007) to extract a translation gram-
mar. In this work, we focus on phrase-based
SMT models, however our approach to using new-
domain comparable corpora to estimate translation
scores is theoretically applicable to any type of
translation grammar.
Our baseline models use a phrase limit of seven
and the standard phrase-based SMT feature set, in-
cluding forward and backward phrase and lexical
translation probabilities. Additionally, we use the
standard lexicalized reordering model. We exper-
iment with two 5-gram language models trained
using SRILM with Kneser-Ney smoothing on (1)
the English side of the Hansard training corpus,
and (2) the relevant new-domain monolingual En-
glish corpus. We experiment with using, first, only
the old-domain language model and, then, both the
old-domain and the new-domain language models.
Our first comparison system augments the stan-
dard feature set with the orthographic similarity
feature, which is not based on comparable cor-
pora. Our second comparison system uses both
the orthographic feature and the contextual and
topic similarity features estimated over a random
set of comparable document pairs. The third sys-
tem estimates contextual and topic similarity using
new-domain-like comparable corpora. We tune
our phrase table feature weights for each model
separately using batch MIRA (Cherry and Fos-
ter, 2012) and new-domain tuning data. Results
are averaged over three tuning runs, and we use
the implementation of approximate randomization
released by Clark et al. (2011) to measure the
statistical significance of each feature-augmented
model compared with the baseline model that uses
the same language model(s).
As noted in Section 3, the features that we
estimate from comparable corpora may be zero-
valued. We use our second tuning sets
5
to tune
a minimum threshold parameter for our new fea-
tures. We measure performance in terms of BLEU
score on the second tuning set as we vary the new
feature threshold between 1e?07 and 0.5 for each
domain. A threshold of 0.01, for example, means
that we replace all feature with values less than
0.01 with 0.01. For both new-domains, perfor-
mance drops when we use thresholds lower than
0.01 and higher than 0.25. We use a minimum
threshold of 0.1 for all experiments presented be-
low for both domains.
5 Results
Table 3 presents a summary of our results on the
test set in each domain. Using only the old-domain
language model, our baselines yield BLEU scores
of 22.70 and 21.29 on the medical and science test
sets, respectively. When we add the orthographic
similarity feature, BLEU scores increase signifi-
cantly, by about 0.4 on the medical data and 0.6 on
science. Adding the contextual and topic features
estimated over a random selection of comparable
document pairs improves BLEU scores slightly in
both domains. Finally, using the most new-domain
like document pairs to estimate the contextual and
topic features yields a 1.3 BLEU score improve-
ment over the baseline in both domains. For both
domains, this result is a statistically significant im-
provement
6
over each of the first three systems.
In both domains, the new-domain language
models contribute substantially to translation qual-
ity. Baseline BLEU scores increase by about
6 and 5 BLEU score points in the medical and
science domains, respectively, when we add the
new-domain language models. In the medical do-
main, neither the orthographic feature nor the or-
thographic feature in combination with contextual
and topic features estimated over random docu-
ment pairs results in a significant BLEU score
improvement. However, using the orthographic
feature and the contextual and topic features es-
timated over new-domain document pairs yields a
5
test2 datasets released by Irvine et al. (2013a)
6
p-value ? 0.01
441
Language Model(s) System Medical Science
Old
Baseline 22.70 21.29
+ Orthographic Feature 23.09* (`0.4) 21.86* (`0.6)
+ Orthographic & Random CC Features 23.22* (`0.5) 21.88* (`0.6)
+ Orthographic & New-domain CC Features 23.98* (`1.3) 22.55* (`1.3)
Old+New
Baseline 28.82 26.18
+ Orthographic Feature 29.02 (`0.2) 26.40* (`0.2)
+ Orthographic & Random CC Features 28.86 (`0.0) 26.52* (`0.3)
+ Orthographic & New-domain CC Features 29.16* (`0.3) 26.50* (`0.3)
Table 3: Comparison between the performance of baseline old-domain translation models and domain-adapted models in
translating science and medical domain text. We experiment with two language models: old, trained on the English side of our
Hansard old-domain training corpus and new, trained on the English side of the parallel training data in each new domain. We
use comparable corpora of 5, 000 (1) random, and (2) the most new-domain-like document pairs to score phrase tables. All
results are averaged over three tuning runs, and we perform statistical significance testing comparing each system augmented
with additional features with the baseline system that uses the same language model(s). * indicates that the BLEU scores are
statistically significant with p ? 0.01.
small but significant improvement of 0.3 BLEU.
In the science domain, in contrast, all three aug-
mented models perform statistically significantly
better than the baseline. Contextual and topic fea-
tures yield only a slight improvement above the
model that uses only the orthographic feature, but
the difference is statistically significant. For the
science domain, when we use the new domain lan-
guage model, there is no difference between esti-
mating the contextual and topic features over ran-
dom comparable document pairs and those chosen
for their similarity with new-domain data.
Differences across domains may be due to the
fact that the medical domain corpora are much
more homogenous, containing the often boiler-
plate text of prescription drug labels, than the sci-
ence domain corpora. The science domain cor-
pora, in contrast, contain abstracts from several
different scientific fields; because that data is more
diverse, a randomly chosen mixed-domain set of
comparable corpora may still be relevant and use-
ful for adapting a translation model.
We experimented with varying the number of
comparable document pairs used for estimating
contextual and topic similarity but saw no sig-
nificant gains from using more than 5, 000 in ei-
ther domain. In fact, performance dropped in the
medical domain when we used more than a few
thousand document pairs. Our proposed approach
orders comparable document pairs by how new-
domain-like they are and augments models with
new features estimated over the top-k. As a result,
using more comparable document pairs means that
there is more data from which to estimate sig-
nals, but it also means that the data is less new-
domain like overall. Using a domain similarity
threshold to choose a subset of comparable doc-
ument pairs may prove useful in future work, as
the ideal amount of comparable data will depend
on the type and size of the initial mixed-domain
comparable corpus as well as the homogeneity of
the text domain of interest.
We also experimented with using a third lan-
guage model estimated over the English side of
our comparable corpora. However, we did not see
any significant improvements in translation qual-
ity when we used this language model in combina-
tion with the old and new domain language mod-
els.
6 Conclusion
In this work, we targeted SMT errors due
to translation model scores using new-domain
comparable corpora. Our old-domain French-
English baseline model was trained on the Cana-
dian Hansard parliamentary proceedings dataset,
which, at 8 million sentence pairs, is one of the
largest publicly available parallel datasets. Our
task was to adapt this baseline to the medical and
scientific text domains using comparable corpora.
We used new-domain parallel data only to tune
model parameters and do evaluation. We mined
Wikipedia for new-domain-like comparable docu-
ment pairs, over which we estimated several addi-
tional features scores: contextual, temporal, and
orthographic similarity. Augmenting the strong
baseline with our new feature set improved the
quality of machine translations in the medical and
science domains by up to 1.3 BLEU points.
442
7 Acknowledgements
This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and
by the Johns Hopkins University Human Lan-
guage Technology Center of Excellence. The
views and conclusions contained in this publica-
tion are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Workshop on Sta-
tistical Machine Translation (WMT).
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: controlling for
optimizer instability. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Hal Daum?e, III and Jagadeesh Jagarlamudi. 2011.
Domain adaptation for machine translation by min-
ing unseen words. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of
the Workshop on Statistical Machine Translation
(WMT).
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrimi-
native instance weighting for domain adaptation in
SMT. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to sta-
tistical language modeling for chinese. ACM Trans-
actions on Asian Language Information Processing
(TALIP).
Guillem Gasc?o, Martha-Alicia Rocha, Germ?an
Sanchis-Trilles, Jes?us Andr?es-Ferrer, and Francisco
Casacuberta. 2012. Does more data always yield
better translations? In Proceedings of the Confer-
ence of the European Association for Computational
Linguistics (EACL).
Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on SMT systems. In
Proceedings of the Workshop on Statistical Machine
Translation (WMT).
Barry Haddow. 2013. Applying pairwise ranked op-
timisation to improve the interpolation of transla-
tion models. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL).
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Ann Irvine, John Morgan, Marine Carpuat, Hal Daum?e
III, and Dragos Munteanu. 2013a. Measuring ma-
chine translation errors in new domains. Transac-
tions of the Association for Computational Linguis-
tics, 1(October).
Ann Irvine, Chris Quirk, and Hal Daume III. 2013b.
Monolingual marginal matching for translation
model adaptation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
Dietrich Klakow. 2000. Selecting articles from the
language model training corpus. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP).
Alex Klementiev, Ann Irvine, Chris Callison-Burch,
and David Yarowsky. 2012. Toward statistical ma-
chine translation without parallel corpora. In Pro-
ceedings of the Conference of the European Associ-
ation for Computational Linguistics (EACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Workshop on Statistical
Machine Translation (WMT).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
443
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Ker-
Jiann Chen, and Lin-Shan Lee. 1997. Chinese lan-
guage model adaptation based on document classifi-
cation and multiple domain-specific language mod-
els. In Fifth European Conference on Speech Com-
munication and Technology.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining translation and language model
scoring for domain-specific data filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT).
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the Conference
of the Association for Computational Linguistics
(ACL).
Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the Confer-
ence of the European Association for Computational
Linguistics (EACL).
J?org Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In N. Nicolov, K. Bontcheva, G. An-
gelova, and R. Mitkov, editors, Recent Advances in
Natural Language Processing (RANLP).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the International Confer-
ence on Computational Linguistics (COLING).
Jiajun Zhang and Chengqing Zong. 2013. Learning
a phrase-based translation model from monolingual
data with application to domain adaptation. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics (ACL).
444
